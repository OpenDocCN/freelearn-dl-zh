- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Continous Action Space
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续动作空间
- en: 'This chapter kicks off the advanced reinforcement learning (RL) part of the
    book by taking a look at a problem that has only been briefly mentioned so far:
    working with environments when our action space is not discrete. Continuous action
    space problems are an important subfield of RL, both theoretically and practically,
    because they have essential applications in robotics, control problems, and other
    fields in which we communicate with physical objects. In this chapter, you will
    become familiar with the challenges that arise in such cases and learn how to
    solve them.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过探讨一个到目前为止仅简要提到过的问题——当我们的动作空间不是离散的时，如何处理环境，来开启本书的强化学习（RL）高级部分。连续动作空间问题是强化学习中的一个重要子领域，无论在理论上还是实践中，它们在机器人学、控制问题以及与物理对象互动的其他领域都有重要应用。在本章中，你将会熟悉在这种情况下出现的挑战，并学习如何解决这些问题。
- en: 'This material might be applicable even in problems and environments we’ve already
    seen. For example, in the previous chapter, when we implemented a mouse clicking
    in the browser environment, the x and y coordinates for the click position could
    be seen as two continuous variables to be predicted as actions. This might look
    a bit artificial, but such representation has a lot of sense from the environment
    perspective: it is much more compact and naturally captures possible click dispersion.
    At the end, clicking at coordinate (x,y) isn’t much different from clicking at
    the (x + 1,y + 1) position for most of the tasks.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本材料甚至可能适用于我们已经遇到的问题和环境。例如，在上一章中，当我们在浏览器环境中实现鼠标点击时，点击位置的 x 和 y 坐标可以看作是两个连续变量，作为动作进行预测。这看起来可能有点人为，但从环境的角度来看，这种表示方式是非常合理的：它更加紧凑，能够自然地捕捉到可能的点击分布。最后，在坐标
    (x,y) 上点击与在 (x + 1, y + 1) 位置上点击，对大多数任务来说并没有太大区别。
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Cover the continuous action space, why it is important, how it differs from
    the already familiar discrete action space, and the way it is implemented in the
    Gym API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍连续动作空间，解释它的重要性，如何与我们已熟悉的离散动作空间不同，并且如何在 Gym API 中实现
- en: Discuss the domain of continuous control using RL methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论使用强化学习方法（RL）解决连续控制领域的问题
- en: Check three different algorithms on the problem of a four-legged robot
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查三种不同算法在四足机器人问题上的表现
- en: Why a continuous space?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么是连续空间？
- en: All the examples that we have seen so far in the book had a discrete action
    space, so you might have the wrong impression that discrete actions dominate the
    field. This is a very biased view, of course, and just reflects the selection
    of domains that we picked our test problems from. Besides Atari games and simple,
    classic RL problems, there are many tasks that require more than just making a
    selection from a small and discrete set of things to do.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中看到的所有例子都是离散动作空间，因此你可能会产生一种错误的印象，认为离散动作主导了这个领域。当然，这是一种非常偏颇的观点，反映的只是我们选择的测试问题的领域。除了Atari游戏和简单的经典强化学习问题外，还有许多任务需要的不仅仅是从一个小而离散的动作集合中进行选择。
- en: To give you an example, just imagine a simple robot with only one controllable
    joint that can be rotated in some range of degrees. Usually, to control a physical
    joint, you have to specify either the desired position or the force applied. In
    both cases, you need to make a decision about a continuous value. This value is
    fundamentally different from a discrete action space, as the set of values on
    which you can make a decision is potentially infinite. For instance, you could
    ask the joint to move to a 13.5^∘ angle or 13.512^∘ angle, and the results could
    be different. Of course, there are always some physical limitations of the system,
    as you can’t specify the action with infinite precision, but the size of the potential
    values could be very large.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，想象一个简单的机器人，只有一个可控关节，能够在某个角度范围内旋转。通常，要控制一个物理关节，你必须指定期望的位置或施加的力。在这两种情况下，你都需要做出一个关于连续值的决策。这个值与离散动作空间本质上是不同的，因为你可以做出决策的值集合可能是无限的。例如，你可以要求关节转到
    13.5^∘ 角度或 13.512^∘ 角度，结果可能会不同。当然，系统总是存在一些物理限制，你不能以无限精度来指定动作，但潜在值的大小可能会非常大。
- en: In fact, when you need to communicate with a physical world, a continuous action
    space is much more likely than having a discrete set of actions. As an example,
    different kinds of robots control systems (such as a heating/cooling controller).
    The methods of RL could be applied to this domain, but there are some details
    that you need to take into consideration before using the advantage actor-critic
    (A2C) or deep Q-network (DQN) methods.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当你需要与物理世界进行交互时，连续动作空间比离散动作集合更为常见。例如，不同种类的机器人控制系统（如加热/冷却控制器）。强化学习的方法可以应用于这一领域，但在使用优势演员-评论员（A2C）或深度
    Q 网络（DQN）方法之前，有一些细节需要考虑。
- en: In this chapter, we will explore how to deal with this family of problems. This
    will act as a good starting point for learning about this very interesting and
    important domain of RL.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨如何处理这一系列问题。这将作为学习强化学习这一非常有趣且重要领域的良好起点。
- en: The action space
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作空间
- en: The fundamental and obvious difference with a continuous action space is its
    continuity. In contrast to a discrete action space, when the action is defined
    as a discrete, mutually exclusive set of options to choose from (for example {left,
    right}, which contains only two elements), the continuous action has a value from
    some range (for instance, [0…1], which includes infinite elements, like 0.5, ![√-
    23-](img/eq49.png), and ![ 3 πe5-](img/eq50.png)). On every time step, the agent
    needs to select the concrete value for the action and pass it to the environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与连续动作空间的根本且明显的区别在于其连续性。与离散动作空间相对，当动作被定义为一个离散的、互斥的选项集合（例如 {left, right}，仅包含两个元素）时，连续动作则是某个范围内的值（例如
    [0…1]，包含无限多个元素，如 0.5、![√- 23-](img/eq49.png) 和 ![3 πe5-](img/eq50.png)）。在每个时间步骤中，代理需要为动作选择一个具体的值并将其传递给环境。
- en: In Gym, a continuous action space is represented as the gym.spaces.Box class,
    which was described, when we talked about the observation space. You may remember
    that Box includes a set of values with a shape and bounds. For example, every
    observation from the Atari emulator was represented as Box(low=0, high=255, shape=(210,
    160, 3)), which means 100,800 values organized as a 3D tensor, with values from
    the 0…255 range. For the action space, it’s unlikely that you’ll work with such
    large numbers of actions. For example, the four-legged robot that we will use
    as a testing environment has eight continuous actions, which correspond to eight
    motors, two in every leg. For this environment, the action space will be defined
    as Box(low=-1, high=1, shape= (8, )), which means eight values from the range
    −1…1 have to be selected at every timestamp to control the robot.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gym 中，连续动作空间由 gym.spaces.Box 类表示，这在我们讨论观察空间时已有描述。你可能还记得，Box 包括一组具有形状和边界的值。例如，从
    Atari 模拟器得到的每个观察值被表示为 Box(low=0, high=255, shape=(210, 160, 3))，这意味着 100,800 个值以
    3D 张量的形式组织，值的范围为 0 到 255。对于动作空间来说，你不太可能处理如此大量的动作。例如，我们将用作测试环境的四足机器人有八个连续动作，分别对应每条腿上的两个马达。对于这个环境，动作空间将被定义为
    Box(low=-1, high=1, shape=(8,))，这意味着每个时间戳需要选择来自 −1 到 1 范围内的八个值来控制机器人。
- en: In this case, the action passed to the env.step() at every step won’t be an
    integer anymore; it will be a NumPy vector of some shape with individual action
    values. Of course, there could be more complicated cases when the action space
    is a combination of discrete and continuous actions, which may be represented
    with the gym.spaces.Tuple class.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，传递给 env.step() 的动作在每一步将不再是整数，而是具有某种形状的 NumPy 向量，包含单独的动作值。当然，也可能出现更复杂的情况，当动作空间是离散动作和连续动作的组合时，可能会用
    gym.spaces.Tuple 类来表示。
- en: Environments
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: Most of the environments that include continuous action spaces are related to
    the physical world, so physics simulations are normally used. There are lots of
    software packages that can simulate physical processes, from very simple open
    source tools to complex commercial packages that can simulate multiphysics processes
    (such as fluid, burning, and strength simulations).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数包含连续动作空间的环境与物理世界相关，因此通常使用物理仿真。现在有许多软件包可以模拟物理过程，从非常简单的开源工具到复杂的商业软件包，这些包可以模拟多物理过程（如流体、燃烧和强度仿真）。
- en: In the case of robotics, one of the most popular packages is MuJoCo, which stands
    for Multi-Joint dynamics with Contact ([https://www.mujoco.org](https://www.mujoco.org)).
    This is a physics engine in which you can define the components of the system
    and their interaction and properties. Then the simulator is responsible for solving
    the system by taking into account your intervention and finding the parameters
    (usually the location, velocities, and accelerations) of the components. This
    makes it ideal as a playground for RL environments, as you can define fairly complicated
    systems (such as multipede robots, robotic arms, or humanoids) and then feed the
    observation into the RL agent, getting actions back.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人领域，最受欢迎的一个软件包是MuJoCo，它代表了带有接触的多关节动力学（[https://www.mujoco.org](https://www.mujoco.org)）。这是一个物理引擎，你可以在其中定义系统的各个组件及其交互和属性。然后，模拟器负责根据你的干预解决系统，找到组件的参数（通常是位置、速度和加速度）。这使得它成为强化学习环境的理想测试场，因为你可以定义相当复杂的系统（如多足机器人、机械臂或人形机器人），然后将观察结果输入到强化学习代理中，获得反馈的动作。
- en: For a long time, MuJoCo was a commercial package and required an expensive license
    to be purchased. Trial licenses and education licenses existed, but they limited
    the audience for this software. But in 2022, DeepMind acquired MuJoCo and made
    the source code publicly available for everybody, which was a really great and
    generous move. Farama Gymnasium includes several MuJoCo environments ([https://gymnasium.farama.org/environments/mujoco/](https://gymnasium.farama.org/environments/mujoco/))
    out of the box; to get them working, you need to install the gymnasium[mujoco]
    package.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，MuJoCo是一个商业软件包，需要购买昂贵的许可证。虽然有试用许可证和教育许可证，但它们限制了该软件的受众。但在2022年，DeepMind收购了MuJoCo，并将源代码公开给所有人使用，这无疑是一次伟大而慷慨的举动。Farama
    Gymnasium包含了多个MuJoCo环境（[https://gymnasium.farama.org/environments/mujoco/](https://gymnasium.farama.org/environments/mujoco/)），开箱即用；要使它们正常工作，你需要安装gymnasium[mujoco]包。
- en: 'Besides MuJoCo, there are other physics simulators you can use for RL. One
    of the most popular is PyBullet ([https://pybullet.org/](https://pybullet.org/)),
    which was open source from the very beginning. In this chapter, we’ll use PyBullet
    in our experiments, and later in the book, we’ll take a look at MuJoCo as well.
    To install PyBullet, you need to execute pip install pybullet==3.2.6 in your Python
    environment. As PyBullet wasn’t updated to the Gymnasium API, we also need to
    install OpenAI Gym for compatibility:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MuJoCo外，还有其他物理模拟器可以用于强化学习。其中一个最受欢迎的是PyBullet（[https://pybullet.org/](https://pybullet.org/)），它从一开始就是开源的。在本章中，我们将使用PyBullet进行实验，稍后的章节中，我们也会介绍MuJoCo。要安装PyBullet，你需要在Python环境中执行pip
    install pybullet==3.2.6。由于PyBullet没有更新到Gymnasium API，我们还需要安装OpenAI Gym以确保兼容性：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use version 0.25.1, as later versions of OpenAI Gym are not compatible with
    the latest version of PyBullet.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用版本0.25.1，因为OpenAI Gym的后续版本与PyBullet的最新版本不兼容。
- en: 'The following code (which is available in Chapter15/01_check_env.py) allows
    you to check that PyBullet works. It looks at the action space and renders an
    image of the environment that we will use as a guinea pig in this chapter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码（位于Chapter15/01_check_env.py中）允许你检查PyBullet是否正常工作。它会查看动作空间，并渲染出我们将在本章中作为实验对象使用的环境图像：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After you start the utility, it should open the graphical user interface (GUI)
    window with our four-legged robot, shown in the following figure, that we will
    train to move:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 启动该工具后，它应当打开图形用户界面（GUI）窗口，显示我们的四足机器人，如下图所示，我们将训练它进行移动：
- en: '![PIC](img/file195.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file195.png)'
- en: 'Figure 15.1: The Minitaur environment in the PyBullet GUI (for better visualization,
    refer to https://packt.link/gbp/9781835882702 )'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：PyBullet GUI中的Minitaur环境（欲更好地可视化，参考https://packt.link/gbp/9781835882702）
- en: 'This environment provides you with 28 numbers as the observation and they correspond
    to different physical parameters of the robot: velocity, position, and acceleration.
    (You can check the source code of MinitaurBulletEnv-v0 for details.) The action
    space is eight numbers that define the parameters of the motors. There are two
    in every leg (one in every knee). The reward of this environment is the distance
    traveled by the robot minus the energy spent.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该环境提供28个数字作为观察值，它们对应机器人不同的物理参数：速度、位置和加速度。（你可以查看MinitaurBulletEnv-v0的源代码以获取详细信息。）动作空间是8个数字，定义了电机的参数。每条腿上有两个电机（每个膝盖一个）。该环境的奖励是机器人行进的距离减去消耗的能量。
- en: The A2C method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A2C方法
- en: The first method that we will apply to our walking robot problem is A2C, which
    we experimented with in Part 3 of the book. This choice of method is quite obvious,
    as A2C is very easy to adapt to the continuous action domain. As a quick refresher,
    A2C’s idea is to estimate the gradient of our policy as ∇J = ∇[𝜃] log π[𝜃](a|s)(R
    −V [𝜃](s)). The policy π[𝜃](s) is supposed to provide the probability distribution
    of actions given the observed state. The quantity V [𝜃](s) is called a critic,
    equal to the value of the state, and is trained using the mean squared error (MSE)
    loss between the critic’s return and the value estimated by the Bellman equation.
    To improve exploration, the entropy bonus L[H] = π[𝜃](s)log π[𝜃](s) is usually
    added to the loss.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用于我们的行走机器人问题的第一个方法是A2C，这是我们在本书第3部分中进行实验的内容。选择这个方法是显而易见的，因为A2C非常容易适应连续动作域。简要回顾一下，A2C的理念是估计我们策略的梯度，即∇J
    = ∇[𝜃] log π[𝜃](a|s)(R −V [𝜃](s))。策略π[𝜃](s)应该提供给定观察状态下的动作概率分布。量V [𝜃](s)称为评论员，等于状态的值，并使用评论员回报与由Bellman方程估计的值之间的均方误差（MSE）损失进行训练。为了提高探索性，通常会在损失中添加熵奖励L[H]
    = π[𝜃](s)log π[𝜃](s)。
- en: Obviously, the value head of the actor-critic will be unchanged for continuous
    actions. The only thing that is affected is the representation of the policy.
    In the discrete cases that we have seen, we had only one action with several mutually
    exclusive discrete values. For such a case, the obvious representation of the
    policy was the probability distribution over all actions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于连续动作，演员-评论员的值头将保持不变。唯一受到影响的是策略的表示。在我们之前看到的离散情况中，只有一个动作具有多个互斥的离散值。对于这种情况，策略的明显表示就是所有动作的概率分布。
- en: In a continuous case, we usually have several actions, each of which can take
    a value from some range. With that in mind, the simplest policy representation
    will be just those values returned for every action. These values should not be
    confused with the value of the state, V (s), which indicates how many rewards
    we can get from the state. To illustrate the difference, let’s imagine a simple
    car steering case in which we can only turn the wheel. The action at every moment
    will be the wheel angle (action value), but the value of every state will be the
    potential discounted reward from the state (for example, the distance the car
    can travel), which is a totally different thing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续情况下，我们通常有多个动作，每个动作都可以从某个范围内取值。考虑到这一点，最简单的策略表示将是每个动作返回的值。这些值不应与状态的值V(s)混淆，后者表示我们可以从该状态获得多少奖励。为了说明这两者的区别，我们可以想象一个简单的汽车转向案例，在该案例中，我们只能转动方向盘。每时每刻的动作将是方向盘的角度（动作值），但每个状态的值将是该状态下潜在的折扣奖励（例如，汽车可以行驶的距离），这完全是不同的概念。
- en: Returning to our action representation options, if you remember what we covered
    in the Policy representation section in Chapter [11](ch015.xhtml#x1-18200011),
    the representation of an action as a concrete value has different disadvantages,
    mostly related to the exploration of the environment. A much better choice will
    be something stochastic, for example, the network returning parameters of the
    Gaussian distribution. For N actions, those parameters will be two vectors of
    size N. The first will be the mean values, μ, and the second vector will contain
    variances, σ². In that case, our policy will be represented as a random N-dimensional
    vector of uncorrelated, normally distributed random variables, and our network
    can make a selection about the mean and the variance of every variable.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的动作表示选项，如果你记得我们在第[11章](ch015.xhtml#x1-18200011)的策略表示部分中讨论的内容，作为一个具体值表示的动作有不同的缺点，主要与环境探索有关。一个更好的选择是随机的，例如，网络返回高斯分布的参数。对于N个动作，这些参数将是两个大小为N的向量。第一个向量将是均值μ，第二个向量将包含方差σ²。在这种情况下，我们的策略将表示为一个随机的N维无相关的正态分布随机变量向量，我们的网络可以对每个变量的均值和方差做出选择。
- en: By definition, the probability density function of the Gaussian distribution
    is given by
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，Gaussian分布的概率密度函数由下式给出：
- en: '![--1-- √2πσ2-](img/eq53.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![--1-- √2πσ2-](img/eq53.png)'
- en: We could directly use this formula to get the probabilities, but to improve
    numerical stability, it is worth doing some math and simplifying the expression
    for log π[𝜃](a|s).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用这个公式来获取概率，但为了提高数值稳定性，值得进行一些数学推导并简化log π[𝜃](a|s)的表达式。
- en: 'The final result will be this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果将是这个：
- en: '![(x−μ)2 2σ2](img/eq51.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![(x−μ)2 2σ2](img/eq51.png)'
- en: The entropy of the Gaussian distribution could be obtained using the differential
    entropy definition and will be ![√ ------ 2πeσ2](img/eq52.png). Now we have everything
    we need to implement the A2C method, so let’s do this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布的熵可以通过微分熵的定义获得，其结果为 ![√ ------ 2πeσ2](img/eq52.png)。现在我们已经具备了实现A2C方法所需的一切，接下来我们开始实现。
- en: Implementation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'The complete source code is in 02_train_a2c.py, lib/model.py, and lib/common.py.
    You will be familiar with most of the code, so the following includes only the
    parts that differ. Let’s start with the model class defined in lib/model.py:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的源代码位于02_train_a2c.py、lib/model.py和lib/common.py中。你会熟悉大部分代码，所以下面只列出了不同的部分。我们从lib/model.py中定义的模型类开始：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, our network has three heads, instead of the normal two for a
    discrete variant of A2C. The first two heads return the mean value and the variance
    of the actions, while the last is the critic head returning the value of the state.
    The mean value returned has an activation function of a hyperbolic tangent, which
    is the squashed output to the range of −1…1\. The variance is transformed with
    the softplus activation function, which is log(1 + e^x) and has the shape of a
    smoothed rectified linear unit (ReLU) function. This activation helps to make
    our variance positive. The value head, as usual, has no activation function applied.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的网络有三个头，而不是A2C离散版本中的两个。前两个头返回动作的均值和方差，而最后一个是返回状态值的评论员头。返回的均值使用双曲正切激活函数，该激活函数将输出压缩到范围−1…1。方差使用softplus激活函数进行变换，该函数为log(1
    + e^x)，其形状类似于平滑的修正线性单元（ReLU）函数。这个激活函数有助于确保我们的方差为正。值头与往常一样，没有应用激活函数。
- en: 'The forward pass is obvious; we apply the common layer first, and then we calculate
    individual heads:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播是显而易见的；我们首先应用常规层，然后计算各个头：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next step is to implement the PTAN Agent class, which is used to convert
    the observation into actions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是实现PTAN代理类，它用于将观察值转换为动作：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the discrete case, we used the ptan.agent.DQNAgent and ptan.agent.PolicyAgent
    classes, but for our problem, we need to write our own, which is not complicated:
    you just need to write a class, derived from ptan.agent.BaseAgent, and override
    the __call__ method, which needs to convert observations into actions.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散情况下，我们使用了ptan.agent.DQNAgent和ptan.agent.PolicyAgent类，但对于我们的这个问题，我们需要自己编写一个，编写并不复杂：你只需要编写一个类，继承自ptan.agent.BaseAgent，并重写__call__方法，该方法需要将观察值转换为动作。
- en: In this class, we get the mean and the variance from the network and sample
    the normal distribution using NumPy functions. To prevent the actions from going
    outside of the environment’s −1…1 bounds, we use np.clip(), which replaces all
    values less than -1 with -1, and values more than 1 with 1\. The agent_states
    argument is not used, but it needs to be returned with the chosen actions, as
    our BaseAgent supports keeping the state of the agent. We don’t need this functionality
    right now, but it will be handy in the next section on deep deterministic policy
    gradients, when we will need to implement a random exploration using the Ornstein-Uhlenbeck
    (OU) process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们从网络中获取均值和方差，并使用NumPy函数对正态分布进行采样。为了防止动作超出环境的−1…1范围，我们使用np.clip()，它将所有小于-1的值替换为-1，将所有大于1的值替换为1。agent_states参数没有使用，但它需要与所选择的动作一起返回，因为我们的BaseAgent支持保持代理的状态。我们现在不需要这个功能，但它将在下一部分的深度确定性策略梯度中派上用场，届时我们将需要使用奥恩斯坦-乌伦贝克（OU）过程实现随机探索。
- en: 'With the model and the agent at hand, we can now go to the training process,
    defined in 02_train_a2c.py. It consists of the training loop and two functions.
    The first is used to perform periodical tests of our model on the separate testing
    environment. During the testing, we don’t need to do any exploration; we will
    just use the mean value returned by the model directly, without any random sampling.
    The testing function is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有了模型和代理之后，我们现在可以进入训练过程，这部分在02_train_a2c.py中定义。它包括训练循环和两个函数。第一个函数用于在独立的测试环境中定期测试我们的模型。在测试过程中，我们不需要进行任何探索；我们将直接使用模型返回的均值，不进行任何随机采样。测试函数如下所示：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The second function defined in the training module implements the calculation
    of the logarithm of the taken actions’ probabilities given the policy. The function
    is a straightforward implementation of the formula we saw earlier:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模块中定义的第二个函数实现了根据策略计算所采取动作的概率的对数。该函数是我们之前看到的公式的直接实现：
- en: '![(x−μ2)2 2σ](img/eq54.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![(x−μ2)2 2σ](img/eq54.png)'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The only tiny difference is in using the torch.clamp() function to prevent the
    division on zero when the returned variance is too small.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的微小区别是在使用torch.clamp()函数，以防当返回的方差太小时发生除零错误。
- en: 'The training loop, as usual, creates the network and the agent, and then instantiates
    the two-step experience source and optimizer. The hyperparameters used are given
    as follows. They weren’t tweaked much, so there is plenty of room for optimization:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环照常创建网络和代理，然后实例化两步经验源和优化器。使用的超参数如下所示。它们没有经过太多调整，因此仍有很大的优化空间：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code used to perform the optimization step on the collected batch is very
    similar to the A2C training that we implemented in Chapter [12](ch016.xhtml#x1-20300012).
    The difference is only in using our calc_logprob() function and a different expression
    for the entropy bonus, which is shown next:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对收集到的批次执行优化步骤的代码与我们在第[12章](ch016.xhtml#x1-20300012)中实现的A2C训练非常相似。唯一的区别是使用了我们的calc_logprob()函数以及不同的熵奖励表达式，接下来会展示：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Every TEST_ITERS frames, the model is tested, and in the case of the best reward
    obtained, the model weights are saved.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每隔TEST_ITERS帧，模型会进行一次测试，并在获得最佳奖励时保存模型权重。
- en: Results
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: In comparison to other methods that we will look at in this chapter, A2C shows
    the worst results, both in terms of the best reward and convergence speed. That’s
    likely because of the single environment used to gather experience, which is a
    weak point of the policy gradient (PG) methods. So, you may want to check the
    effect of several parallel environments on A2C.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在本章中将要探讨的其他方法相比，A2C的表现最差，无论是在最佳奖励还是收敛速度上。这很可能是因为经验收集仅使用了单一环境，而这是策略梯度（PG）方法的一个弱点。因此，你可能想要检查多个并行环境对A2C的影响。
- en: To start the training, we pass the -n argument with the run name, which will
    be used in TensorBoard and a new directory to save the models. The --dev option
    could be used to enable the GPU usage, but due to the small dimensionality of
    the input and the tiny network size, it gives only a marginal increase in speed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，我们传递-n参数以及运行名称，该名称将在TensorBoard中使用，并且会创建一个新的目录以保存模型。可以使用--dev选项启用GPU，但由于输入的维度较小且网络规模较小，它只会带来微小的速度提升。
- en: 'After 9M frames, which took 16 hours of optimization, the training process
    reached the best score of 0.35 during the testing, which is not very impressive.
    If we leave it running for a week or two, we might be able to achieve a better
    score. The reward and episode steps during the training and testing are shown
    in the following graphs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过9M帧（16小时的优化过程）后，训练过程在测试中达到了最佳得分0.35，虽然不算很出色。如果我们让它运行一两周，可能能获得更好的分数。训练和测试中的奖励以及回合步骤如下图所示：
- en: '![PIC](img/B22150_15_02.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_02.png)'
- en: 'Figure 15.2: The reward (left) and steps (right) for training episodes'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：训练回合的奖励（左）和步骤（右）
- en: '![PIC](img/B22150_15_03.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_03.png)'
- en: 'Figure 15.3: The reward (left) and steps (right) for testing episodes'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：测试回合的奖励（左）和步骤（右）
- en: The episode steps charts (right plots on both figures) shows the average count
    of steps performed in the episode before the end. The time limit of the environment
    is 1,000 steps, so everything lower than 1,000 indicates that the episode was
    stopped due to environment checks. For most of the PyBullet environments, special
    checks for self-damage are implemented internally, which stop the simulation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该图中的回合步骤图（右侧的图表）显示了回合结束前每个回合执行的平均步数。环境的时间限制为1,000步，因此低于1,000的值表示回合因环境检查而停止。对于大多数PyBullet环境，内部实现了自我损害检查，这会停止仿真。
- en: Using models and recording videos
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型和录制视频
- en: As you have seen before, the physical simulator can render the state of the
    environment, which makes it possible to see how our trained model behaves. To
    do that for our A2C models, there is a utility, 03_play_a2c.py. Its logic is the
    same as in the test_net() function, so its code is not shown here.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前所看到的，物理模拟器可以呈现环境的状态，这使得我们可以观察训练后的模型行为。为了实现这一点，对于我们的A2C模型，提供了一个工具：03_play_a2c.py。其逻辑与test_net()函数相同，因此这里不展示代码。
- en: To start it, you need to pass the -m option with the model file and optional
    parameter -r with a directory name, which will be used to save the video using
    the RecordVideo wrapper we discussed in Chapter [2](ch006.xhtml#x1-380002).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动它，你需要传递 -m 选项和模型文件，以及可选的 -r 参数，后者是一个目录名称，将用于保存视频，使用我们在第[2](ch006.xhtml#x1-380002)章中讨论的RecordVideo包装器。
- en: 'At the end of the simulation, the utility shows the number of steps and accumulated
    reward. For example, the best A2C model from my training was able to get the reward
    0.312 and the video is just 2 seconds long (you can find it here: [https://youtu.be/s9BReDUtpQs](https://youtu.be/s9BReDUtpQs)).
    Figure [15.4](#x1-279005r4) shows the last frame of the video and it looks like
    our model had problems keeping the balance.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟结束时，效用显示了步骤数和累积奖励。例如，我的训练中最好的A2C模型能够获得0.312的奖励，而视频只有2秒长（你可以在这里找到它：[https://youtu.be/s9BReDUtpQs](https://youtu.be/s9BReDUtpQs)）。图[15.4](#x1-279005r4)显示了视频的最后一帧，看起来我们的模型在保持平衡方面遇到了一些问题。
- en: '![PIC](img/file200.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file200.png)'
- en: 'Figure 15.4: Last frame of A2C model simulation'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：A2C模型模拟的最后一帧
- en: Deep deterministic policy gradients
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度（DDPG）
- en: The next method that we will take a look at is called deep deterministic policy
    gradients (DDPG), which is an actor-critic method but has a very nice property
    of being off-policy. The following is a simplified interpretation of the strict
    proofs. If you are interested in understanding the core of this method deeply,
    you can always refer to the article by Silver et al. called Deterministic policy
    gradient algorithms [[Sil+14](#)], published in 2014, and the paper by Lillicrap
    et al. called Continuous control with deep reinforcement learning [[Lil15](#)],
    published in 2015.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看看的一种方法叫做深度确定性策略梯度（DDPG），它是一种演员-评论员方法，但有一个非常好的特性——它是脱离策略的。以下是对严格证明的简化解释。如果你有兴趣深入理解这种方法的核心，可以随时参考Silver等人于2014年发表的名为《确定性策略梯度算法》[[Sil+14](#)]的文章，以及Lillicrap等人于2015年发表的名为《使用深度强化学习进行连续控制》[[Lil15](#)]的论文。
- en: The simplest way to illustrate the method is through comparison with the already
    familiar A2C method. In this method, the actor estimates the stochastic policy,
    which returns the probability distribution over discrete actions or, as we have
    just covered in the previous section, the parameters of normal distribution. In
    both cases, our policy is stochastic, so, in other words, our action taken is
    sampled from this distribution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 说明这种方法最简单的方式是与我们已经熟悉的A2C方法进行比较。在这种方法中，演员估计的是随机策略，该策略返回离散动作的概率分布，或者像我们在上一节中讲解的那样，正态分布的参数。在这两种情况下，我们的策略都是随机的，换句话说，我们采取的动作是从这个分布中采样的。
- en: Deterministic policy gradients also belong to the A2C family, but the policy
    is deterministic, which means that it directly provides us with the action to
    take from the state. This makes it possible to apply the chain rule to the Q-value,
    and by maximizing the Q, the policy will be improved as well. To understand this,
    let’s look at how the actor and critic are connected in a continuous action domain.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性策略梯度也属于A2C家族，但其策略是确定性的，这意味着它直接提供我们在某一状态下应采取的动作。这使得可以对Q值应用链式法则，通过最大化Q，策略也会得到改进。为了理解这一点，我们来看一下演员和评论员在连续动作域中是如何连接的。
- en: Let’s start with the actor, as it is the simpler of the two. What we want from
    it is the action to take for every given state. In a continuous action domain,
    every action is a number, so the actor network will take the state as an input
    and return N values, one for every action. This mapping will be deterministic,
    as the same network always returns the same output if the input is the same. (We’re
    not going to use dropout or anything adding stochasticity to the inference; we’re
    just going to use an ordinary feed-forward network.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从演员（actor）开始，因为它是两者中更简单的一个。我们从它这里需要的是在每个给定状态下采取的动作。在连续动作域中，每个动作都是一个数字，因此演员网络会将状态作为输入，并返回N个值，每个值对应一个动作。这个映射是确定性的，因为相同的网络如果输入相同，始终返回相同的输出。（我们不会使用dropout或任何增加推断随机性的技术；我们只会使用普通的前馈网络。）
- en: 'Now let’s look at the critic. The role of the critic is to estimate the Q-value,
    which is a discounted reward of the action taken in some state. However, our action
    is a vector of numbers, so our critic network now accepts two inputs: the state
    and the action. The output from the critic will be the single number, which corresponds
    to the Q-value. This architecture is different from the DQN, when our action space
    was discrete and, for efficiency, we returned values for all actions in one pass.
    This mapping is also deterministic.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下评论员。评论员的作用是估算Q值，即在某个状态下采取的动作的折扣奖励。然而，我们的动作是一个数字向量，所以我们的评论员网络现在接受两个输入：状态和动作。评论员的输出将是一个数字，表示Q值。这个架构不同于DQN，当时我们的动作空间是离散的，并且为了提高效率，我们在一次传递中返回所有动作的值。这个映射也是确定性的。
- en: 'So, we have two functions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有两个函数：
- en: The actor, let’s call it μ(s), which converts the state into the action
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员，我们称之为μ(s)，将状态转换为动作
- en: 'The critic, which, through the state and the action, gives us the Q-value:
    Q(s,a)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论员通过状态和动作，给我们Q值：Q(s,a)
- en: 'We can substitute the actor function into the critic and get the expression
    with only one input parameter of our state: Q(s,μ(s)). In the end, neural networks
    are just functions.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将演员函数代入评论员，并得到只有一个输入参数的表达式：Q(s,μ(s))。最终，神经网络只是函数。
- en: 'Now, the output of the critic gives us the approximation of the entity that
    we’re interested in maximizing in the first place: the discounted total reward.
    This value depends not only on the input state but also on the parameters of the
    𝜃[μ] actor and the 𝜃[Q] critic networks. At every step of our optimization, we
    want to change the actor’s weights to improve the total reward that we get. In
    mathematical terms, we want the gradient of our policy.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，评论员的输出给出了我们最初想要最大化的实体的近似值：折扣总奖励。这个值不仅依赖于输入状态，还依赖于𝜃[μ]演员和𝜃[Q]评论员网络的参数。在我们优化的每一步中，我们都希望改变演员的权重，以提高我们获得的总奖励。从数学角度看，我们希望得到我们策略的梯度。
- en: 'In his deterministic policy gradient theorem, Silver et al. proved that the
    stochastic policy gradient is equivalent to the deterministic policy gradient.
    In other words, to improve the policy, we just need to calculate the gradient
    of the Q(s,μ(s)) function. By applying the chain rule, we get the gradient: ∇[a]Q(s,a)∇[𝜃[μ]]μ(s).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的确定性策略梯度定理中，Silver等人证明了随机策略梯度等同于确定性策略梯度。换句话说，想要改进策略，我们只需要计算Q(s,μ(s))函数的梯度。通过应用链式法则，我们得到梯度：∇[a]Q(s,a)∇[𝜃[μ]]μ(s)。
- en: Note that, despite both the A2C and DDPG methods belonging to the A2C family,
    the way that the critic is used is different. In A2C, we use the critic as a baseline
    for a reward from the experienced trajectories, so the critic is an optional piece
    (without it, we will get the REINFORCE method) and is used to improve the stability.
    This happens as the policy in A2C is stochastic, which builds a barrier in our
    backpropagation capabilities (we have no way of differentiating the random sampling
    step).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管A2C和DDPG方法都属于A2C家族，但评论员的使用方式是不同的。在A2C中，我们使用评论员作为经验轨迹中奖励的基准，因此评论员是一个可选部分（没有它，我们将得到REINFORCE方法），并用于提高稳定性。这是因为A2C中的策略是随机的，这在我们的反向传播能力中建立了一个屏障（我们无法区分随机采样步骤）。
- en: In DDPG, the critic is used in a different way. As our policy is deterministic,
    we can now calculate the gradients from Q, which is obtained from the critic network,
    which uses actions produced by the actor (check Figure [15.5](#x1-282003r5)),
    so the whole system is differentiable and could be optimized end to end with stochastic
    gradient descent (SGD). To update the critic network, we can use the Bellman equation
    to find the approximation of Q(s,a) and minimize the MSE objective.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPG中，评论员以不同的方式使用。由于我们的策略是确定性的，我们现在可以从Q中计算梯度，Q是从评论员网络获得的，评论员使用由演员产生的动作（见图[15.5](#x1-282003r5)），因此整个系统是可微的，并且可以通过随机梯度下降（SGD）进行端到端优化。为了更新评论员网络，我们可以使用贝尔曼方程来找到Q(s,a)的近似值并最小化均方误差（MSE）目标。
- en: 'All this may look a bit cryptic, but behind it stands a quite simple idea:
    the critic is updated as we did in A2C, and the actor is updated in a way to maximize
    the critic’s output. The beauty of this method is that it is off-policy, which
    means that we can now have a huge replay buffer and other tricks that we used
    in DQN training. Nice, right?'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些可能看起来有些晦涩，但背后是一个相当简单的思想：评论员像我们在 A2C 中做的那样进行更新，演员则通过最大化评论员输出的方式进行更新。这种方法的优点在于它是脱离策略的，这意味着我们现在可以拥有一个巨大的重放缓冲区，以及在
    DQN 训练中使用的其他技巧。不错吧？
- en: Exploration
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索
- en: 'The price we have to pay for all this goodness is that our policy is now deterministic,
    so we have to explore the environment somehow. We can do this by adding noise
    to the actions returned by the actor before we pass them to the environment. There
    are several options here. The simplest method is just to add the random noise
    to the actions: μ(s) + 𝜖𝒩. We will use this in the next method that we will consider
    in this chapter.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为所有这些好处付出的代价是我们的策略现在是确定性的，因此我们必须以某种方式探索环境。我们可以通过在将动作传递给环境之前，向演员返回的动作中添加噪声来实现这一点。这里有几种选择。最简单的方法就是直接将随机噪声添加到动作中：μ(s)
    + 𝜖𝒩。我们将在本章中考虑的下一个方法中使用这种方式。
- en: 'A more advanced (and sometimes giving better results) approach to the exploration
    is to use the previously mentioned Ornstein-Uhlenbeck process, which is very popular
    in the financial world and other domains dealing with stochastic processes. This
    process models the velocity of a massive Brownian particle under the influence
    of friction and is defined by this stochastic differential equation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更先进的（有时能获得更好结果的）探索方法是使用之前提到的 Ornstein-Uhlenbeck 过程，这在金融领域及其他处理随机过程的领域中非常流行。这个过程模拟了一个大质量布朗粒子在摩擦力作用下的速度，并通过以下随机微分方程定义：
- en: ∂x[t] = 𝜃(μ −x[t])∂t + σ∂W,
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x[t] = 𝜃(μ −x[t])∂t + σ∂W，
- en: where 𝜃, μ, and σ are parameters of the process and W[t] is the Wiener process.
    In a discrete-time case, the OU process could be written as
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝜃、μ 和 σ 是过程的参数，W[t] 是维纳过程。在离散时间的情况下，OU 过程可以写作：
- en: x[t+1] = x[t] + 𝜃(μ −x) + σ𝒩.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: x[t+1] = x[t] + 𝜃(μ −x) + σ𝒩。
- en: This equation expresses the next value generated by the process via the previous
    value of the noise, adding normal noise, 𝒩. In our exploration, we will add the
    value of the OU process to the action returned by the actor.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表示通过加入正态噪声 𝒩 来生成过程的下一个值。在我们的探索中，我们将把 OU 过程的值添加到演员返回的动作中。
- en: Implementation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'This example consists of three source files:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子由三个源文件组成：
- en: lib/model.py contains the model and the PTAN agent
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/model.py 包含模型和 PTAN 代理
- en: lib/common.py has a function used to unpack the batch
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/common.py 中有一个用于解包批次的函数
- en: 04_train_ddpg.py has the startup code and the training loop
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 04_train_ddpg.py 包含启动代码和训练循环
- en: Here, I will show only the significant pieces of the code. The model consists
    of two separate networks for the actor and critic, and it follows the architecture
    from the paper by Lillicrap et al. [[Lil15](#)] mentioned earlier. The actor is
    extremely simple and is a feed-forward network with two hidden layers. The input
    is an observation vector, whereas the output is a vector with N values, one for
    each action. The output actions are transformed with hyperbolic tangent nonlinearity
    to squeeze the values to the −1…1 range.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将只展示代码中的重要部分。模型由两个独立的网络组成，分别是演员和评论员，它遵循 Lillicrap 等人论文中的架构[[Lil15](#)]。演员非常简单，是一个具有两个隐藏层的前馈网络。输入是一个观察向量，而输出是一个包含
    N 个值的向量，每个动作对应一个值。输出的动作经过双曲正切非线性变换，将值压缩到 −1…1 范围内。
- en: 'The critic is a bit unusual, as it includes two separate paths for the observation
    and the actions, and those paths are concatenated together to be transformed into
    the critic output of one number. Figure [15.5](#x1-282003r5) shows the structure
    of both networks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员有些不寻常，因为它包括观察和动作的两个独立路径，这些路径被连接在一起并转化为评论员的输出结果——一个数字。图[15.5](#x1-282003r5)展示了两个网络的结构：
- en: '![PIC](img/file201.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file201.png)'
- en: 'Figure 15.5: The DDPG actor and critic networks'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5：DDPG 演员和评论员网络
- en: 'The code for the actor includes a three-layer network that produces the action
    value:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 演员的代码包括一个三层网络，用于产生动作值：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, the following is the code used for the critic:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，以下是用于评论员的代码：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The forward() function of the critic first transforms the observations with
    its small network and then concatenates the output and given actions to transform
    them into one single value of Q. To use the actor network with the PTAN experience
    source, we need to define the agent class that has to transform the observations
    into actions. This class is the most convenient place to put our OU exploration
    process, but to do this properly, we should use the functionality of the PTAN
    agents that we haven’t used so far: optional statefulness.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: critic的forward()函数首先通过其小型网络转化观察结果，然后将输出和给定的动作拼接起来，转化为一个Q值。为了使用PTAN经验源中的演员网络，我们需要定义一个智能体类，该类必须将观察结果转化为动作。这个类是实现OU探索过程的最方便的地方，但为了正确实现这一点，我们应该使用PTAN智能体尚未使用的功能：可选的状态性。
- en: 'The idea is simple: our agent transforms the observations into actions. But
    what if it needs to remember something between the observations? All our examples
    have been stateless so far, but sometimes this is not enough. The issue with OU
    is that we have to track the OU values between the observations.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：我们的智能体将观察转化为动作。但是，如果它需要在观察之间记住某些信息呢？到目前为止，我们的所有示例都是无状态的，但有时这并不够。OU的问题在于我们需要在观察之间跟踪OU值。
- en: Another very useful use case for stateful agents is a partially observable Markov
    decision process (POMDP), which was briefly mentioned in Chapter [6](#) and Chapter [14](ch018.xhtml#x1-24700014).
    The POMDP is a Markov decision process when the state observed by the agent doesn’t
    comply with the Markov property and doesn’t include the full information to distinguish
    one state from another. In that case, our agent needs to track the state along
    the trajectory to be able to take the action.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非常有用的状态智能体应用场景是部分可观察的马尔可夫决策过程（POMDP），该过程在第[6](#)章和第[14](ch018.xhtml#x1-24700014)章中简要提及。POMDP是一个马尔可夫决策过程，其中智能体观察到的状态不符合马尔可夫性质，且不包含区分一个状态与另一个状态所需的完整信息。在这种情况下，我们的智能体需要跟踪状态以便能够采取适当的行动。
- en: 'So, the code for the agent that implements the OU for exploration is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实现OU探索的智能体代码如下：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The constructor accepts a lot of parameters, most of which are the default hyperparameters
    of the OU process taken from the paper Continuous Control with Deep Reinforcement
    Learning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受很多参数，其中大部分是来自论文《Continuous Control with Deep Reinforcement Learning》的OU过程的默认超参数。
- en: The initial_state() method is derived from the BaseAgent class and has to return
    the initial state of the agent when a new episode is started. As our initial state
    has to have the same dimension as the actions (we want to have individual exploration
    trajectories for every action of the environment), we postpone the initialization
    by returning None as the initial state.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: initial_state()方法源自BaseAgent类，当一个新回合开始时，它必须返回智能体的初始状态。由于我们的初始状态必须与动作具有相同的维度（我们希望为环境的每个动作拥有独立的探索轨迹），因此我们通过返回None作为初始状态来推迟初始化。
- en: 'In the __call__ method, we’ll take this into account:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在__call__方法中，我们会考虑到这一点：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method is the core of the agent and the purpose of it is to convert the
    observed state and internal agent state into the action. As the first step, we
    convert the observations into the appropriate form and ask the actor network to
    convert them into deterministic actions. The rest of the method is for adding
    the exploration noise by applying the OU process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法是智能体的核心，目的是将观察到的状态和内部智能体状态转化为动作。作为第一步，我们将观察结果转化为适当的形式，并请求演员网络将其转化为确定性动作。该方法的其余部分是通过应用OU过程来添加探索噪声。
- en: 'In this loop, we iterate over the batch of observations and the list of the
    agent states from the previous call, and we update the OU process value, which
    is a straightforward implementation of the already shown formula:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，我们遍历观察的批次和来自上次调用的智能体状态列表，并更新OU过程值，这是对已展示公式的简单实现：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To finalize the loop, we add the noise from the OU process to our actions and
    save the noise value for the next step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个循环，我们将OU过程中的噪声添加到我们的动作中，并将噪声值保存到下一个步骤。
- en: 'Finally, we clip the actions to enforce them to fall into the −1…1 range; otherwise,
    PyBullet will throw an exception:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对动作进行裁剪，确保它们落入−1到1的范围内；否则，PyBullet将抛出异常：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The final piece of the DDPG implementation is the training loop in the 04_train_ddpg.py
    file. To improve the stability, we use the replay buffer with 100,000 transitions
    and target networks for both the actor and the critic (we discussed both in Chapter [6](#)):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 实现的最后一部分是 04_train_ddpg.py 文件中的训练循环。为了提高稳定性，我们使用了具有 100,000 个转换的重放缓冲区，并为演员和评论员都使用了目标网络（我们在第
    [6](#) 章中讨论了这两者）。
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We also use two different optimizers to simplify the way that we handle gradients
    for the actor and critic training steps. The most interesting code is inside the
    training loop. On every iteration, we store the experience into the replay buffer
    and sample the training batch:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了两种不同的优化器，以简化我们处理演员和评论员训练步骤的梯度方式。最有趣的代码在训练循环内部。在每次迭代时，我们将经验存储到重放缓冲区，并抽取训练批次：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, two separate training steps are performed. To train the critic, we need
    to calculate the target Q-value using the one-step Bellman equation, with the
    target critic network as the approximation of the next state:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，执行两个独立的训练步骤。为了训练评论员，我们需要使用一步贝尔曼方程计算目标 Q 值，其中目标评论员网络作为下一个状态的近似：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When we have got the reference, we can calculate the MSE loss and ask the critic’s
    optimizer to tweak the critic’s weights. The whole process is similar to the training
    for the DQN, so nothing is really new here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们得到参考后，我们可以计算 MSE 损失，并请求评论员的优化器调整评论员的权重。整个过程类似于 DQN 的训练，所以这里没有什么新东西：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'On the actor’s training step, we need to update the actor’s weights in a direction
    that will increase the critic’s output. As both the actor and critic are represented
    as differentiable functions, what we need to do is just pass the actor’s output
    to the critic and then minimize the negated value returned by the critic:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在演员的训练步骤中，我们需要更新演员的权重，朝着增加评论员输出的方向进行。由于演员和评论员都表示为可微函数，我们需要做的只是将演员的输出传递给评论员，然后最小化评论员返回的负值：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This negated output of the critic could be used as a loss to backpropagate
    it to the critic network and, finally, the actor. We don’t want to touch the critic’s
    weights, so it’s important to ask only the actor’s optimizer to do the optimization
    step. The weights of the critic will still keep the gradients from this call,
    but they will be discarded on the next optimization step:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员的负输出可以作为损失，用于反向传播到评论员网络，最终再传播到演员。我们不希望触碰评论员的权重，因此只要求演员的优化器执行优化步骤。这时，评论员的权重仍然保持着来自此调用的梯度，但它们会在下一次优化步骤中被丢弃：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As the last step of the training loop, we perform the update of the target
    networks in an unusual way:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作为训练循环的最后一步，我们以一种不寻常的方式更新目标网络：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Previously, we synced the weights from the optimized network into the target
    periodically. In continuous action problems, such syncing works worse than so-called
    “soft sync.” The soft sync is carried out on every step, but only a small ratio
    of the optimized network’s weights are added to the target network. This makes
    a smooth and slow transition from the old weight to the new ones.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们定期将优化后的网络权重同步到目标网络。在连续动作问题中，这种同步比所谓的“软同步”效果差。软同步在每一步进行，但仅将优化网络权重的一小部分添加到目标网络中。这使得从旧权重到新权重的过渡变得平滑而缓慢。
- en: Results and video
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果与视频
- en: 'The code can be started in the same way as the A2C example: you need to pass
    the run name and optional --dev flag. My experiments have shown ≈ 30% speed increase
    from a GPU, so if you’re in a hurry, using CUDA may be a good idea, but the increase
    is not that dramatic, as we have seen in the case of Atari games.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以像 A2C 示例那样启动：你需要传递运行名称和可选的 --dev 标志。我的实验表明，使用 GPU 可以提高约 30% 的速度，因此如果你赶时间，使用
    CUDA 可能是个好主意，但增速并不像在 Atari 游戏中看到的那样剧烈。
- en: After 5M observations, which took about 20 hours, the DDPG algorithm was able
    to reach the mean reward of 4.5 on 10 test episodes, which is an improvement over
    the A2C result. The training dynamics are shown in Figure [15.6](#x1-283002r6)
    and Figure [15.7](#x1-283003r7).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 5M 次观察之后，耗时大约 20 小时，DDPG 算法能够在 10 次测试中达到 4.5 的平均奖励，这比 A2C 的结果有所提升。训练动态显示在图
    [15.6](#x1-283002r6) 和图 [15.7](#x1-283003r7) 中。
- en: '![PIC](img/B22150_15_06.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_15_06.png)'
- en: 'Figure 15.6: The reward (left) and steps (right) for training episodes'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6：训练回合的奖励（左）和步数（右）
- en: '![PIC](img/B22150_15_07.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_15_07.png)'
- en: 'Figure 15.7: Actor loss (left) and critic loss (right) during the training'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：训练过程中的演员损失（左）和评论员损失（右）
- en: The “Episode steps” plot shows the mean length of the episodes that we used
    for training. The critic loss is an MSE loss and should be low, but the actor
    loss, as you will remember, is the negated critic’s output, so the smaller it
    is, the better the reward that the actor can (potentially) achieve.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: “Episode steps”图显示了我们用于训练的实验平均步数。评论者损失是均方误差（MSE）损失，应该较低，而演员损失，正如你所记得的，是评论者输出的负值，因此它越小，演员能（潜在地）获得的奖励就越好。
- en: In Figure [15.8](#x1-283004r8), the shown values were obtained during the testing
    (which are average values obtained for 10 episodes).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[15.8](#x1-283004r8)中，所示的数值是在测试过程中获得的（这些是10次实验的平均值）。
- en: '![PIC](img/B22150_15_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_08.png)'
- en: 'Figure 15.8: The reward (left) and steps (right) for testing episodes'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：测试实验的奖励（左）和步数（右）
- en: 'To test the saved model and record the video the same way we did for the A2C
    model, you can use the utility 05_play_ddpg.py. It uses the same command-line
    options, but is supposed to load DDPG models. In figure Figure [15.9](#x1-283006r9),
    the last frame of my video is shown:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试保存的模型并像我们对A2C模型所做的那样记录视频，你可以使用实用程序05_play_ddpg.py。它使用相同的命令行选项，但应该加载DDPG模型。在图[15.9](#x1-283006r9)中，展示了我的视频的最后一帧：
- en: '![PIC](img/file208.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file208.png)'
- en: 'Figure 15.9: Last frame of DDPG model simulation'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：DDPG模型仿真最后一帧
- en: The score during the testing was 3.033 and the video is avaliable at [https://youtu.be/vVnd0Nu1d9s](https://youtu.be/vVnd0Nu1d9s).
    Now the video is 11 seconds long and the model fails after falling forward.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 测试过程中的得分为3.033，视频可以在[https://youtu.be/vVnd0Nu1d9s](https://youtu.be/vVnd0Nu1d9s)观看。现在该视频时长为11秒，模型在前倾后失败。
- en: Distributional policy gradients
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式策略梯度
- en: As the last method of this chapter, we will take a look at the paper by Barth-Maron
    et al., called Distributed distributional deterministic policy gradients [[Bar+18](#)],
    published in 2018.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的最后一种方法，我们将看看Barth-Maron等人于2018年发布的论文《Distributed distributional deterministic
    policy gradients》[[Bar+18](#)]。
- en: The full name of the method is Distributed Distributional Deep Deterministic
    Policy Gradients or D4PG for short. The authors proposed several improvements
    to the DDPG method to improve stability, convergence, and sample efficiency.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的全名是分布式分布式深度确定性策略梯度，简称D4PG。作者对DDPG方法提出了几项改进，以提高稳定性、收敛性和样本效率。
- en: First, they adapted the distributional representation of the Q-value proposed
    in the paper by Bellemare et al. called A distributional perspective on reinforcement
    learning, published in 2017 [[BDM17](#)]. We discussed this approach in Chapter [8](ch012.xhtml#x1-1240008),
    when we talked about DQN improvements, so refer to it or to the original Bellemare
    paper for details. The core idea is to replace a single Q-value from the critic
    with a probability distribution. The Bellman equation is replaced with the Bellman
    operator, which transforms this distributional representation in a similar way.
    The second improvement was the usage of the n-step Bellman equation, unrolled
    to speed up the convergence. We also discussed this in detail in Chapter [8](ch012.xhtml#x1-1240008).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们采用了Bellemare等人于2017年提出的Q值分布表示方法，该方法在论文《A distributional perspective on
    reinforcement learning》中进行了阐述[[BDM17](#)]。我们在第[8](ch012.xhtml#x1-1240008)章讨论了这种方法，讲解了DQN的改进，因此可以参考该章或Bellemare的原始论文以获取更多细节。核心思想是将评论者的单一Q值替换为一个概率分布。Bellman方程被Bellman算子所取代，它以类似的方式转化这种分布式表示。第二个改进是使用n步Bellman方程，通过展开加速收敛。我们在第[8](ch012.xhtml#x1-1240008)章也详细讨论了这个问题。
- en: 'Another difference versus the original DDPG method was the usage of the prioritized
    replay buffer instead of the uniformly sampled buffer. So, strictly speaking,
    the authors took relevant improvements from the paper by Hassel et al., called
    Rainbow: Combining Improvements in Deep Reinforcement Learning, which was published
    in 2017 [[Hes+18](#)], and adapted them to the DDPG method. The result was impressive:
    this combination showed state-of-the-art results on the set of continuous control
    problems. Let’s try to reimplement the method and check it ourselves.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '与原始DDPG方法的另一个不同之处是使用了优先重放缓冲区，而不是均匀采样缓冲区。因此，严格来说，作者从Hassel等人的论文《Rainbow: Combining
    Improvements in Deep Reinforcement Learning》中吸取了相关的改进，并将其适配到DDPG方法中，该论文于2017年发布[[Hes+18](#)]。结果令人印象深刻：这种组合在一系列连续控制问题上展现了最先进的结果。让我们尝试重新实现这个方法，并自己检查一下。'
- en: Architecture
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: The most notable change between D4PG and DDPG is the critic’s output. Instead
    of returning the single Q-value for the given state and the action, it now returns
    N_ATOMS values, corresponding to the probabilities of values from the predefined
    range. In my code, I used N_ATOMS=51 and the distribution range of Vmin=-10 and
    Vmax=10, so the critic returned 51 numbers, representing the probabilities of
    the discounted reward falling into bins with bounds in [−10,−9.6,−9.2,…,9.6,10].
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG 和 DDPG 之间最显著的变化是评论员的输出。评论员不再返回给定状态和动作的单一 Q 值，而是返回 N_ATOMS 个值，这些值对应于预定义范围内各个值的概率。在我的代码中，我使用了
    N_ATOMS=51 和分布范围 Vmin=-10，Vmax=10，因此评论员返回了 51 个数字，表示折扣奖励落入[−10,−9.6,−9.2,…,9.6,10]区间的概率。
- en: Another difference between D4PG and DDPG is the exploration. DDPG used the OU
    process for exploration, but according to the D4PG authors, they tried both OU
    and adding simple random noise to the actions, and the result was the same. So,
    they used a simpler approach for exploration in the paper.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG 和 DDPG 之间的另一个区别是探索。DDPG 使用 OU 过程进行探索，但根据 D4PG 论文的作者所述，他们尝试了 OU 和向动作中添加简单随机噪声两种方式，结果是相同的。因此，他们在论文中使用了更简单的探索方法。
- en: The last significant difference in the code is related to the training, as D4PG
    uses cross-entropy loss to calculate the difference between two probability distributions
    (returned by the critic and obtained as a result of the Bellman operator). To
    make both distributions aligned to the same supporting atoms, distribution projection
    is used in the same way as in the original paper by Bellemare et al.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的最后一个显著差异与训练有关，因为 D4PG 使用交叉熵损失来计算两个概率分布之间的差异（一个是评论员返回的，另一个是通过贝尔曼算子得到的）。为了使这两个分布对齐到相同的支持原子，使用了分布投影，方法与
    Bellemare 等人在原始论文中使用的相同。
- en: Implementation
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'The complete source code is in 06_train_d4pg.py, lib/model.py, and lib/common.py.
    As before, we start with the model class. The actor class has exactly the same
    architecture as in DDPG, so during the training class, DDPGActor is used. The
    critic has the same size and count of hidden layers; however, the output is not
    a single number, but N_ATOMS:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的源代码位于 06_train_d4pg.py、lib/model.py 和 lib/common.py 中。如前所述，我们首先从模型类开始。演员类的架构与
    DDPG 完全相同，因此在训练类中，使用了 DDPGActor。评论员的隐藏层数量和大小与之前相同；然而，输出不是一个数字，而是 N_ATOMS：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We also create a helper PyTorch buffer with reward supports, which will be
    used to get from the probability distribution to the single mean Q-value:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个带有奖励支持的辅助 PyTorch 缓冲区，它将用于从概率分布中获取单一的均值 Q 值：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, softmax() application is not part of the network’s forward()
    method, as we’re going to use the more stable log_softmax() function during the
    training. Due to this, softmax() needs to be applied when we want to get actual
    probabilities.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，softmax() 的应用不是网络 forward() 方法的一部分，因为在训练期间我们将使用更稳定的 log_softmax() 函数。因此，softmax()
    需要在我们想要获取实际概率时应用。
- en: 'The agent class is much simpler for D4PG and has no state to track:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 D4PG，智能体类要简单得多，并且没有需要跟踪的状态：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For every state to be converted to actions, the agent applies the actor network
    and adds Gaussian noise to the actions, scaled by the epsilon value. In the training
    code, we have the following hyperparameters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将每个状态转换为动作，智能体应用演员网络并向动作中添加高斯噪声，噪声大小由 epsilon 值缩放。在训练代码中，我们有以下超参数：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: I used a smaller replay buffer of 100,000, and it worked fine. (In the D4PG
    paper, the authors used 1M transitions in the buffer.) The buffer is prepopulated
    with 10,000 samples from the environment, and then the training starts.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了一个较小的回放缓冲区，大小为 100,000，效果良好。（在 D4PG 论文中，作者使用了 1M 的过渡数据存储在缓冲区中。）缓冲区预先填充了来自环境的
    10,000 个样本，然后开始训练。
- en: 'For every training loop, we perform the same two steps as before: we train
    the critic and the actor. The difference is in the way that the loss for the critic
    is calculated:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个训练循环，我们执行与之前相同的两个步骤：训练评论员和演员。区别在于评论员损失计算的方式：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As the first step in the critic’s training, we ask it to return the probability
    distribution for the states and actions taken. This probability distribution will
    be used as an input in the cross-entropy loss calculation. To get the target probability
    distribution, we need to calculate the distribution from the last states in the
    batch and then perform the Bellman projection of the distribution:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 作为评论者训练的第一步，我们要求它返回状态和采取的动作的概率分布。这个概率分布将作为输入用于交叉熵损失的计算。为了获得目标概率分布，我们需要计算批次中最后状态的分布，然后执行分布的贝尔曼投影：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This projection function is a bit complicated and is exactly the same as the
    implementation explained in detail in Chapter [8](ch012.xhtml#x1-1240008). As
    a quick reminder, it calculates the transformation of the last_states probability
    distribution, which is shifted according to the immediate reward and scaled to
    respect the discount factor. The result is the target probability distribution
    that we want our network to return. As there is no general cross-entropy loss
    function in PyTorch, we calculate it manually by multiplying the logarithm of
    the input probability by the target probabilities:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个投影函数有点复杂，完全与第 [8](ch012.xhtml#x1-1240008) 章中详细解释的实现相同。简要地说，它计算最后状态的概率分布的变换，该分布根据即时奖励进行了偏移，并按折扣因子进行缩放。结果是我们希望网络返回的目标概率分布。由于
    PyTorch 中没有通用的交叉熵损失函数，我们通过将输入概率的对数与目标概率相乘来手动计算它：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The actor’s training is much simpler, and the only difference from the DDPG
    method is the use of the distr_to_q() method of the model to convert from the
    probability distribution to the single mean Q-value using support atoms:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 演员的训练要简单得多，唯一与 DDPG 方法的区别是使用模型的 distr_to_q() 方法，将概率分布转换为单一的均值 Q 值，使用支持原子：
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Results
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: The D4PG method showed the best results in both convergence speed and the reward
    obtained. Following 20 hours of training, after about 3.5M observations, it was
    able to reach the mean test reward of 17.912\. Given that “gym environment threshold”
    is 15.0 (which is a score when the environment considered it solved), that is
    a great result. And this result could be improved, as the count of steps is less
    than 1,000 (which is a time limit for the environment). This means that our model
    is being terminated prematurely because of internal environment checks. In Figure [15.10](#x1-287002r10)
    and Figure [15.11](#x1-287003r11), we have the train and test metrics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: D4PG 方法在收敛速度和获得的奖励方面都表现最好。经过 20 小时的训练，大约 350 万次观测后，它能够达到 17.912 的平均测试奖励。鉴于“gym
    环境阈值”是 15.0（这是环境认为任务已解决时的分数），这是一个很棒的结果。而且这个结果还可以进一步提高，因为步骤数不到 1,000（这是环境的时间限制）。这意味着我们的模型因内部环境检查而被提前终止。在图
    [15.10](#x1-287002r10) 和图 [15.11](#x1-287003r11) 中，我们展示了训练和测试的指标。
- en: '![PIC](img/B22150_15_10.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_10.png)'
- en: 'Figure 15.10: The reward (left) and steps (right) for training episodes'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：训练回合的奖励（左）和步骤（右）
- en: '![PIC](img/B22150_15_11.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_11.png)'
- en: 'Figure 15.11: The reward (left) and steps (right) for testing episodes'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：测试回合的奖励（左）和步骤（右）
- en: To compare the implemented methods, Figure [15.12](#x1-287004r12) contains test
    episode metrics from all three methods.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较实现的方法，图 [15.12](#x1-287004r12) 包含了三种方法的测试回合指标。
- en: '![PIC](img/B22150_15_12.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_12.png)'
- en: 'Figure 15.12: The reward (left) and steps (right) for test episodes'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：测试回合的奖励（左）和步骤（右）
- en: 'To check the model “in action,” you can use the same tool, 05_play_ddpg.py
    (as actor has the same network structure as in DDPG). Now the video produced by
    the best model lasts 33 seconds and the final score was 17.827\. You can watch
    it here: [https://youtu.be/XZdVrGPaI0M](https://youtu.be/XZdVrGPaI0M).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查模型的“实际表现”，你可以使用相同的工具 05_play_ddpg.py（因为演员网络结构与 DDPG 中的相同）。现在，最佳模型生成的视频时长为
    33 秒，最终得分为 17.827。你可以在这里观看：[https://youtu.be/XZdVrGPaI0M](https://youtu.be/XZdVrGPaI0M)。
- en: Things to try
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要尝试的事项
- en: 'Here is a list of things you can do to improve your understanding of the topic:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以帮助你提升对该主题理解的事项：
- en: In the D4PG code, I used a simple replay buffer, which was enough to get good
    improvement over DDPG. You can try to switch the example to the prioritized replay
    buffer in the same way as we did in Chapter [8](ch012.xhtml#x1-1240008).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 D4PG 代码中，我使用了一个简单的重放缓冲区，这足以比 DDPG 获得更好的改进。你可以尝试将示例切换为优先重放缓冲区，就像我们在第 [8](ch012.xhtml#x1-1240008)
    章中做的那样。
- en: There are lots of interesting and challenging environments around. For example,
    you can start with other PyBullet environments, but there is also the DeepMind
    Control Suite (Tassa et al., DeepMind Control Suite, arXiv abs/1801.00690 (2018)),
    MuJoCo-based environments in Gym, and many others.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周围有很多有趣且具有挑战性的环境。例如，你可以从其他PyBullet环境开始，但也有DeepMind控制套件（Tassa等人，DeepMind控制套件，arXiv
    abs/1801.00690（2018）），Gym中的基于MuJoCo的环境，以及其他许多环境。
- en: You can play with the very challenging Learning to Run competition from NIPS-2017
    (which also took place in 2018 and 2019 with more challenging problems), where
    you are given a simulator of the human body and your agent needs to figure out
    how to move it around.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以尝试参加非常具有挑战性的“学习跑步”竞赛，该竞赛源自NIPS-2017（并且在2018年和2019年也有举办，问题更具挑战性），在这个竞赛中，你将获得一个人体仿真器，而你的智能体需要弄清楚如何使其运动。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we quickly skimmed through the very interesting domain of
    continuous control using RL methods, and we checked three different algorithms
    on one problem of a four-legged robot. In our training, we used an emulator, but
    there are real models of this robot made by the Ghost Robotics company. (You can
    check out the cool video on YouTube: [https://youtu.be/bnKOeMoibLg](https://youtu.be/bnKOeMoibLg).)
    We applied three training methods to this environment: A2C, DDPG, and D4PG (which
    showed the best results).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们快速浏览了使用强化学习方法进行连续控制的非常有趣的领域，并在四足机器人这一问题上检查了三种不同的算法。在我们的训练中，我们使用了一个仿真器，但Ghost
    Robotics公司制造了这种机器人的真实模型。（你可以在YouTube上查看这个很酷的视频：[https://youtu.be/bnKOeMoibLg](https://youtu.be/bnKOeMoibLg)。）我们将三种训练方法应用于这个环境：A2C，DDPG和D4PG（后者显示出了最好的结果）。
- en: 'In the next chapter, we will continue exploring the continuous action domain
    and check a different set of improvements: trust region extension.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将继续探索连续动作领域，并查看一组不同的改进方法：信任域扩展。
