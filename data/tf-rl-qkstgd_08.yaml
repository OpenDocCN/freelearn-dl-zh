- en: Deep RL Applied to Autonomous Driving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习应用于自动驾驶
- en: Autonomous driving is one of the hottest technological revolutions in development
    as of the time of writing this. It will dramatically alter how humanity looks
    at transportation in general, and will drastically reduce travel costs as well
    as increase safety. Several state-of-the-art algorithms are used by the autonomous
    vehicle development community to this end. These include, but are not limited
    to, perception, localization, path planning, and control. Perception deals with
    the identification of the environment around an autonomous vehicle—pedestrians,
    cars, bicycles, and so on. Localization involves the identification of the exact
    location—or pose to be more precise—of the vehicle in a precomputed map of the
    environment. Path planning, as the name implies, is the process of planning the
    path of the autonomous vehicle, both in the long term (say, from point *A* to
    point *B*) as well as the shorter term (say, the next 5 seconds). Control is the
    actual execution of the desired path, including evasive maneuvers. In particular,
    **reinforcement learning** (**RL**) is widely used in the path planning and control
    of the autonomous vehicle, both for urban as well as highway driving.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶是目前正在开发的最热门的技术革命之一。它将极大地改变人类对交通的看法，显著降低旅行成本并提高安全性。自动驾驶汽车开发社区已经使用了若干前沿算法来实现这一目标。这些算法包括但不限于感知、定位、路径规划和控制。感知涉及识别自动驾驶汽车周围的环境——行人、汽车、自行车等。定位是指在预先计算好的环境地图中识别汽车的确切位置（或者更精确地说，姿态）。路径规划，顾名思义，是规划自动驾驶汽车路径的过程，包括长期路径（比如从*A*点到*B*点）以及短期路径（比如接下来的5秒）。控制则是实际执行所需路径的过程，包括避让操作。特别地，**强化学习**（**RL**）在自动驾驶的路径规划和控制中被广泛应用，适用于城市驾驶和高速公路驾驶。
- en: In this chapter, we will use the **The Open Racing Car Simulator** (**TORCS**)
    simulator to train an RL agent to learn to successfully drive on a racetrack.
    While the CARLA simulator is more robust and has realistic rendering, TORCS is
    easier to use and so is a good first option. The interested reader is encouraged
    to try out training RL agents on the CARLA simulator after completing this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用**开放赛车模拟器**（**TORCS**）来训练RL智能体，学习如何在赛道上成功驾驶。尽管CARLA模拟器更强大且具有逼真的渲染效果，但TORCS更易于使用，因此是一个很好的入门选择。完成本书后，鼓励感兴趣的读者尝试在CARLA模拟器上训练RL智能体。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Learning to use TORCS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习使用TORCS
- en: Training a **Deep Deterministic Policy Gradient** (**DDPG**) agent to learn
    to drive
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练**深度确定性策略梯度**（**DDPG**）智能体来学习驾驶
- en: Training a **Proximal Policy Optimization** (**PPO**) agent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练**近端策略优化**（**PPO**）智能体
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, we will need the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习需要以下工具：
- en: Python (version 2 or 3)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（版本2或3）
- en: NumPy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: TensorFlow (version 1.4 or higher)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本1.4或更高）
- en: TORCS racing car simulator
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TORCS赛车模拟器
- en: Car driving simulators
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汽车驾驶模拟器
- en: 'Applying RL in autonomous driving necessitates the use of robust car-driving
    simulators, as the RL agent cannot be trained on the road directly. To this end,
    several open source car-driving simulators have been developed by the research
    community, with each having its own pros and cons. Some of the open source car
    driving simulators are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶中应用强化学习（RL）需要使用强大的汽车驾驶模拟器，因为RL智能体不能直接在道路上进行训练。为此，研究社区开发了几款开源汽车驾驶模拟器，每款都有其优缺点。一些开源汽车驾驶模拟器包括：
- en: CARLA
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CARLA
- en: '[http://vladlen.info/papers/carla.pdf](http://vladlen.info/papers/carla.pdf)'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://vladlen.info/papers/carla.pdf](http://vladlen.info/papers/carla.pdf)'
- en: Developed at Intel labs
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在英特尔实验室开发
- en: Suited to urban driving
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于城市驾驶
- en: TORCS
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TORCS
- en: '[http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)'
- en: Racing car
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赛车
- en: DeepTraffic
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepTraffic
- en: '[https://selfdrivingcars.mit.edu/deeptraffic/](https://selfdrivingcars.mit.edu/deeptraffic/)'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://selfdrivingcars.mit.edu/deeptraffic/](https://selfdrivingcars.mit.edu/deeptraffic/)'
- en: Developed at MIT
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MIT开发
- en: Suited to highway driving
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于高速公路驾驶
- en: Learning to use TORCS
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习使用TORCS
- en: 'We will first learn how to use the TORCS racing car simulator, which is an
    open source simulator. You can obtain the download instructions from [http://torcs.sourceforge.net/index.php?name=Sections&op=viewarticle&artid=3](http://torcs.sourceforge.net/index.php?name=Sections&op=viewarticle&artid=3)
    but the salient steps are summarized as follows for Linux:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先学习如何使用 TORCS 赛车模拟器，它是一个开源模拟器。你可以从 [http://torcs.sourceforge.net/index.php?name=Sections&op=viewarticle&artid=3](http://torcs.sourceforge.net/index.php?name=Sections&op=viewarticle&artid=3)
    获取下载说明，但以下是 Linux 系统下的关键步骤总结：
- en: Download the `torcs-1.3.7.tar.bz2` file from [https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download](https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download)
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download](https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download)
    下载 `torcs-1.3.7.tar.bz2` 文件
- en: Unpack the package with `tar xfvj torcs-1.3.7.tar.bz2`
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tar xfvj torcs-1.3.7.tar.bz2` 解包该包
- en: 'Run the following commands:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令：
- en: '`cd torcs-1.3.7`'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd torcs-1.3.7`'
- en: '`./configure`'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`./configure`'
- en: '`make`'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make`'
- en: '`make install`'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make install`'
- en: '`make datainstall`'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make datainstall`'
- en: 'The default installation directories are:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认安装目录为：
- en: '`/usr/local/bin`: TORCS command (directory should be in your `PATH`)'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/usr/local/bin`：TORCS 命令（该目录应包含在你的 `PATH` 中）'
- en: '`/usr/local/lib/torcs`: TORCS dynamic `libs` (directory MUST be in your `LD_LIBRARY_PATH`
    if you don''t use the TORCS shell)'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/usr/local/lib/torcs`：TORCS 动态 `libs`（如果不使用 TORCS shell，目录必须包含在你的 `LD_LIBRARY_PATH`
    中）'
- en: '`/usr/local/share/games/torcs`: TORCS data files'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/usr/local/share/games/torcs`：TORCS 数据文件'
- en: By running the `torcs` command (the default location is `/usr/local/bin/torcs`),
    you can now see the TORCS simulator open. The desired settings can then be chosen,
    including the choice of the car, racetrack, and so on. The simulator can also
    be played as a video game, but we are interested in using it to train an RL agent.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行 `torcs` 命令（默认位置是 `/usr/local/bin/torcs`），你现在可以看到 TORCS 模拟器开启。然后可以选择所需的设置，包括选择汽车、赛道等。模拟器也可以作为视频游戏来玩，但我们更感兴趣的是用它来训练
    RL 智能体。
- en: State space
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态空间
- en: 'We will next define the state space for TORCS. *Table 2: Description of the
    available sensors (part II). Ranges are reported with their unit of measure (where
    defined)* of the *Simulated Car Racing Championship: Competition Software Manual*
    document at [https://arxiv.org/pdf/1304.1672.pdf](https://arxiv.org/pdf/1304.1672.pdf)
    provides a summary of the state parameters that are available for the simulator.
    We will use the following entries as our state space; the number in brackets identifies
    the size of the entry:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义 TORCS 的状态空间。*表 2：可用传感器描述（第二部分）。范围与其单位（如果定义了单位）一起报告*，来自 *模拟汽车赛车锦标赛：竞赛软件手册*
    文档，在 [https://arxiv.org/pdf/1304.1672.pdf](https://arxiv.org/pdf/1304.1672.pdf)
    中提供了一个模拟器可用状态参数的总结。我们将使用以下条目作为我们的状态空间；方括号中的数字表示条目的大小：
- en: '`angle`: Angle between the car direction and the track (1)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`angle`：汽车方向与赛道之间的角度（1）'
- en: '`track`: This will give us the end of the track measured every 10 degrees from
    -90 to +90 degrees; it has 19 real values, counting the end values (19)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`track`：这将给我们每 10 度测量一次的赛道末端，从 -90 到 +90 度；它有 19 个实数值，包括端点值（19）'
- en: '`trackPos`: Distance between the car and the track axis (1)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trackPos`：汽车与赛道轴之间的距离（1）'
- en: '`speedX`: Speed of the car in the longitudinal direction (1)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speedX`：汽车在纵向方向的速度（1）'
- en: '`speedY`: Speed of the car in the transverse direction (1)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speedY`：汽车在横向方向的速度（1）'
- en: '`speedZ`: Speed of the car in the *Z*-direction; we don''t need this actually,
    but we retain it for now (1)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speedZ`：汽车在 *Z* 方向的速度；其实我们并不需要这个，但现在先保留它（1）'
- en: '`wheelSpinVel`: The rotational speed of the four wheels of the car (4)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wheelSpinVel`：汽车四个轮子的旋转速度（4）'
- en: '`rpm`: The car engine''s rpm (1)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpm`：汽车引擎的转速（1）'
- en: 'See the previously mentioned document for a better understanding of the preceding
    variables, including their permissible ranges. Summing up the number of real valued
    entries, we note that our state space is a real valued vector of size 1+19+1+1+1+1+4+1
    = 29\. Our action space is of size *3*: the steering, acceleration, and brake.
    Steering is in the range [*-1,1*], and acceleration is in the range [*0,1*], as
    is the brake.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考之前提到的文档以更好地理解前述变量，包括它们的允许范围。总结所有实数值条目的数量，我们注意到我们的状态空间是一个实数值向量，大小为 1+19+1+1+1+1+4+1
    = 29。我们的动作空间大小为*3*：转向、加速和刹车。转向范围为 [*-1,1*]，加速范围为 [*0,1*]，刹车也是如此。
- en: Support files
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持文件
- en: 'The open source community has also developed two Python files to interface
    TORCS with Python so that we can call TORCS from Python commands. In addition,
    to automatically start TORCS, we need another `sh` file. These three files are
    summarized as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 开源社区还开发了两个Python文件，将TORCS与Python接口，以便我们能够从Python命令调用TORCS。此外，为了自动启动TORCS，我们需要另一个`sh`文件。这三个文件总结如下：
- en: '`gym_torcs.py`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gym_torcs.py`'
- en: '`snakeoil3_gym.py`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`snakeoil3_gym.py`'
- en: '`autostart.sh`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`autostart.sh`'
- en: 'These files are included in the code files for the chapter ([https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide](https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide)),
    but can also be obtained from a Google search. In lines ~130-160 of `gym_torcs.py`,
    the reward function is set. You can see the following lines, which convert the
    raw simulator states to NumPy arrays:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件包含在本章的代码文件中（[https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide](https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide)），但也可以通过Google搜索获取。在`gym_torcs.py`的第~130-160行中，设置了奖励函数。你可以看到以下行，这些行将原始的模拟器状态转换为NumPy数组：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The reward function is then set as follows. Note that we give rewards for higher
    longitudinal speed along the track (the cosine of the angle term), and penalize
    lateral speed (the sine of the angle term). Track position is also penalized.
    Ideally, if this were zero, we would be at the center of the track, and values
    of *+1* or *-1* imply that we are at the edges of the track, which is not desired
    and hence penalized:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，奖励函数被设置如下。请注意，我们根据沿轨道的纵向速度（角度项的余弦值）给予奖励，并惩罚横向速度（角度项的正弦值）。轨道位置也会被惩罚。理想情况下，如果这是零，我们将处于轨道的中心，而*+1*或*-1*的值表示我们在轨道的边缘，这是不希望发生的，因此会受到惩罚：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We terminate the episode if the car is out of the track and/or the progress
    of the agent is stuck using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果汽车偏离轨道或智能体的进度卡住，我们使用以下代码终止该回合：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are now ready to train an RL agent to drive a car in TORCS successfully.
    We will use a DDPG agent first.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好训练一个RL智能体，让它在TORCS中成功驾驶。我们将首先使用DDPG智能体。
- en: Training a DDPG agent to learn to drive
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练DDPG智能体学习驾驶
- en: Most of the DDPG code is the same as we saw earlier in [Chapter 5](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml), *Deep
    Deterministic Policy Gradients (DDPG)*; only the differences will be summarized
    here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分DDPG代码与我们在[第5章](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml)中看到的*深度确定性策略梯度（DDPG）*相同；这里只总结其中的差异。
- en: Coding ddpg.py
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写 ddpg.py
- en: 'Our state dimension for TORCS is `29` and the action dimension is `3`; these
    are set in `ddpg.py` as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的TORCS状态维度是`29`，动作维度是`3`；这些在`ddpg.py`中设置，如下所示：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Coding AandC.py
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写 AandC.py
- en: 'The actor and critic file, `AandC.py`, also needs to be modified. In particular,
    the `create_actor_network` in the `ActorNetwork` class is edited to have two hidden
    layers with `400` and `300` neurons, respectively. Also, the output consists of
    three actions: `steering`, `acceleration`, and `brake`. Since steering is in the
    [*-1,1*] range, the `tanh` activation function is used; `acceleration` and `brake`
    are in the [*0,1*] range, and so the `sigmoid` activation function is used. We
    then `concat` them along axis dimension `1`, and this is the output of our actor''s
    policy:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: actor 和 critic 文件`AandC.py`也需要进行修改。特别地，`ActorNetwork`类中的`create_actor_network`被修改为拥有两个隐藏层，分别包含`400`和`300`个神经元。此外，输出由三个动作组成：`steering`（转向）、`acceleration`（加速）和`brake`（刹车）。由于转向在[*-1,1*]范围内，因此使用`tanh`激活函数；加速和刹车在[*0,1*]范围内，因此使用`sigmoid`激活函数。然后，我们在轴维度`1`上将它们`concat`（连接），这就是我们actor策略的输出：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Likewise, the `CriticNetwork` class `create_critic_network()` function is edited
    to have two hidden layers for the neural network, with `400` and `300` neurons,
    respectively. This is shown in the following code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，`CriticNetwork`类的`create_critic_network()`函数被修改为拥有两个隐藏层，分别包含`400`和`300`个神经元。这在以下代码中显示：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The other changes to be made are in `TrainOrTest.py`, which we will look into next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行的其他更改在`TrainOrTest.py`中，我们接下来将查看这些更改。
- en: Coding TrainOrTest.py
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写 TrainOrTest.py
- en: 'Import the TORCS environment from `gym_torcs` so that we can train the RL agent
    on it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从`gym_torcs`导入TORCS环境，以便我们能够在其上训练RL智能体：
- en: '**Import TORCS**: Import the TORCS environment from `gym_torcs` as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入TORCS**：从`gym_torcs`导入TORCS环境，代码如下：'
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**The** `env` **variable**: Create a TORCS environment variable using the following
    command:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`env`变量**：使用以下命令创建TORCS环境变量：'
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Relaunch TORCS**: Since TORCS is known to have a memory leak error, reset
    the environment every `100` episodes using `relaunch=True`; otherwise reset without
    any arguments as follows:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重新启动TORCS**：由于TORCS已知存在内存泄漏错误，每`100`个回合后使用`relaunch=True`重新启动环境；否则，可以按如下方式在不带参数的情况下重置环境：'
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Stack up state space**: Use the following command to stack up the 29-dimension
    space:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**堆叠状态空间**：使用以下命令堆叠29维的状态空间：'
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Number of time steps per episode**: Choose the number of time steps `msteps` to
    run per episode. For the first `100` episodes, the agent has not learned much,
    and so you can choose `100` time steps per episode; we gradually increase this
    linearly for later episodes up to the upper limit of `max_steps`.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每回合的时间步数**：选择每个回合运行的时间步数`msteps`。在前`100`个回合中，代理学习尚未深入，因此你可以选择每回合`100`个时间步；对于后续回合，我们将其线性增加，直到达到`max_steps`的上限。'
- en: This step is not critical and the agent's learning is not dependent on the number
    of steps we choose per episode. Feel free to experiment with how `msteps` is set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤并不是关键，代理的学习并不依赖于我们为每个回合选择的步数。可以自由尝试设置`msteps`。
- en: 'Choose the number of time steps as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 选择每回合的时间步数，如下所示：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Full throttle**: For the first `10` episodes, we apply full throttle to warm
    up the neural network parameters. Only after that, do we start using the actor''s
    policy. Note that TORCS typically learns in about ~1,500–2,000 episodes, so the
    first `10` episodes will not really have much influence later on in the learning.
    Apply full throttle to warm up the neural network parameters as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全速前进**：在前`10`个回合中，我们施加全速前进以预热神经网络参数。只有在此之后，我们才开始使用演员的策略。需要注意的是，TORCS通常在大约1,500到2,000个回合后学习完成，所以前`10`个回合对后期的学习影响不大。通过以下方式应用全速前进来预热神经网络参数：'
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'That''s it for the changes that need to be made to the code for the DDPG to
    play TORCS. The rest of the code is the same as that covered in [Chapter 5](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml),
    *Deep Deterministic Policy Gradients (DDPG)*. We can train the agent using the
    following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为了让DDPG代理在TORCS中玩的代码需要做的更改。其余的代码与[第5章](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml)中讲解的*深度确定性策略梯度（DDPG）*相同。我们可以使用以下命令来训练代理：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Enter `1` for training; `0` is for testing a pretrained agent. Training can
    take about 2–5 days depending on the speed of the computer used. But this is a
    fun problem and is worth the effort. The number of steps experienced per episode,
    as well as the rewards, are stored in `analysis_file.txt`, which we can plot.
    The number of time steps per episode is plotted as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`1`进行训练；输入`0`是为了测试一个预训练的代理。训练可能需要2到5天，具体取决于使用的计算机速度。但这是一个有趣的问题，值得花费时间和精力。每个回合的步数以及奖励都会存储在`analysis_file.txt`文件中，我们可以用它来绘制图表。每个回合的时间步数如下图所示：
- en: '![](img/3a0210b4-3bb8-4c79-9db0-22b783f78002.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a0210b4-3bb8-4c79-9db0-22b783f78002.png)'
- en: 'Figure 1: Number of time steps per episode of TORCS (training mode)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：TORCS每回合的时间步数（训练模式）
- en: We can see that the car has learned to drive reasonably well after ~600 episodes,
    with more efficient driving after ~1,500 episodes. Approximately ~300 time steps
    correspond to one lap of the racetrack. Thus, the agent is able to drive more
    than seven to eight laps without terminating toward the end of the training. For
    a cool video of the DDPG agent driving, see the following YouTube link: [https://www.youtube.com/watch?v=ajomz08hSIE](https://www.youtube.com/watch?v=ajomz08hSIE).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过大约600次训练后，汽车已经学会了合理地驾驶，而在大约1500次训练后，驾驶更加高效。大约300个时间步对应赛道的一圈。因此，代理在训练结束时能够驾驶七到八圈以上而不会中途终止。关于DDPG代理驾驶的精彩视频，请参考以下YouTube链接：[https://www.youtube.com/watch?v=ajomz08hSIE](https://www.youtube.com/watch?v=ajomz08hSIE)。
- en: Training a PPO agent
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练PPO代理
- en: 'We saw previously how to train a DDPG agent to drive a car on TORCS. How to
    use a PPO agent is left as an exercise for the interested reader. This is a nice
    challenge to complete. The PPO code from [Chapter 7](7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml),
    *Trust Region Policy Optimization and Proximal Policy Optimization*, can be reused,
    with the necessary changes made to the TORCS environment. The PPO code for TORCS
    is also supplied in the code repository ([https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide](https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide)),
    and the interested reader can peruse it. A cool video of a PPO agent driving a
    car in TORCS is in the following YouTube video at: [https://youtu.be/uE8QaJQ7zDI](https://youtu.be/uE8QaJQ7zDI)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前展示了如何训练一个DDPG智能体在TORCS中驾驶汽车。如何使用PPO智能体则留作有兴趣的读者的练习。这是一个不错的挑战。来自[第7章](7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml)的PPO代码，*信任区域策略优化与近端策略优化*，可以复用，只需对TORCS环境做必要的修改。TORCS的PPO代码也已提供在代码库中（[https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide](https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide)），有兴趣的读者可以浏览它。TORCS中PPO智能体驾驶汽车的酷炫视频可以在以下YouTube视频中查看：[https://youtu.be/uE8QaJQ7zDI](https://youtu.be/uE8QaJQ7zDI)
- en: Another challenge for the interested reader is to use **Trust Region Policy
    Optimization** (**TRPO**) for the TORCS racing car problem. Try this too, if interested!
    This is one way to master RL algorithms.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是对于有兴趣的读者来说，使用**信任区域策略优化**（**TRPO**）解决TORCS赛车问题。如果有兴趣，也可以尝试一下！这是掌握RL算法的一种方法。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to apply RL algorithms to train an agent to learn
    to drive a car autonomously. We installed the TORCS racing-car simulator and also
    learned how to interface it with Python, so that we can train RL agents. We also
    did a deep dive into the state space for TORCS and the meaning of each of these
    terms. The DDPG algorithm was then used to train an agent to learn to drive successfully
    in TORCS. The video rendering in TORCS is really cool! The trained agent was able
    to drive more than seven to eight laps around the racetrack successfully. Finally,
    the use of PPO for the same problem of driving a car autonomously was also explored
    and left as an exercise for the interested reader; code for this is supplied in
    the book's repository.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们展示了如何应用强化学习（RL）算法训练一个智能体，使其学会自主驾驶汽车。我们安装了TORCS赛车模拟器，并学习了如何将其与Python接口，以便我们可以训练RL智能体。我们还深入探讨了TORCS的状态空间以及这些术语的含义。随后，使用DDPG算法训练一个智能体，成功学会在TORCS中驾驶。TORCS中的视频渲染效果非常酷！训练后的智能体能够成功驾驶超过七到八圈。最后，我们也探索了使用PPO算法解决同样的自主驾驶问题，并将其作为练习留给有兴趣的读者；相关代码已提供在本书的代码库中。
- en: This concludes this chapter as well as the book. Feel free to read upon more
    material online on the application of RL for autonomous driving and robotics.
    This is now a very hot area of both academic and industry research, and is well
    funded, with several job openings in these areas. Wishing you the best!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这也就结束了本章以及本书的内容。可以随时在线阅读更多关于RL在自主驾驶和机器人学应用的材料。这是目前学术界和工业界研究的热门领域，并且得到了充分的资金支持，相关领域也有许多职位空缺。祝你一切顺利！
- en: Questions
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why can you not use DQN for the TORCS problem?
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么不能使用DQN来解决TORCS问题？
- en: We used the Xavier weights initializer for the neural network weights. What
    other weight initializers are you aware of, and how well will the trained agent
    perform with them?
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了Xavier权重初始化器来初始化神经网络的权重。你知道还有哪些其他的权重初始化方法？使用它们训练的智能体表现如何？
- en: Why is the `abs()` function used in the reward function, and why is it used
    for the last two terms but not for the first term?
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在奖励函数中使用`abs()`函数？为什么它用于最后两个项而不是第一个项？
- en: How can you ensure smoother driving than what was observed in the video?
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何确保比视频中观察到的驾驶更平稳？
- en: Why is a replay buffer used in DDPG but not in PPO?
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在DDPG中使用重放缓冲区而在PPO中不使用？
- en: Further reading
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '*Continuous control with deep reinforcement learning*, Timothy P. Lillicrap,
    Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
    Silver, Daan Wierstra, arXiv:1509.02971 (DDPG paper): [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用深度强化学习进行连续控制*，Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra，arXiv:1509.02971（DDPG论文）：[https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)'
- en: '*Proximal Policy Optimization Algorithms*, John Schulman, Filip Wolski, Prafulla
    Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347 (PPO paper): [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*近端策略优化算法*，John Schulman，Filip Wolski，Prafulla Dhariwal，Alec Radford，Oleg Klimov，arXiv:1707.06347
    (PPO 论文)：[https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)'
- en: 'TORCS: [http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TORCS：[http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)
- en: '*Deep Reinforcement Learning Hands-On*, by *Maxim Lapan*, *Packt Publishing*:
    [https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习实战*，*Maxim Lapan* 著，*Packt Publishing* 出版：[https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands)'
