- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Neural Networks for Neural Search
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与神经搜索
- en: Search has always been a crucial part of all information systems; getting the
    right information to the right user is integral. This is because a user query,
    as in a set of keywords, cannot fully represent a user’s information needs. Traditionally,
    symbolic search has been developed to allow users to perform keyword-based searches.
    However, such search applications were bound to a text-based search box. With
    the recent developments in deep learning and artificial intelligence, we can encode
    any kind of data into vectors and measure the similarities between two vectors.
    This allows users to create a query with any kind of data and get any kind of
    search result.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索一直是所有信息系统中的关键组成部分；将正确的信息提供给正确的用户是至关重要的。这是因为用户查询（如一组关键词）不能完全代表用户的信息需求。传统上，符号搜索被开发出来，允许用户进行基于关键词的搜索。然而，这类搜索应用被局限于基于文本的搜索框中。随着深度学习和人工智能的最新发展，我们可以将任何类型的数据编码为向量，并衡量两个向量之间的相似度。这使得用户能够用任何类型的数据创建查询，并获得任何类型的搜索结果。
- en: In this chapter, we will review important concepts regarding information retrieval
    and neural search, as well as looking at the benefits that neural search provides
    to developers. Before we start introducing neural search, we will first introduce
    the drawbacks of the traditional symbolic-based search. Then, we’ll move on to
    looking at how to use neural networks in order to build a cross/multi-modality
    search. This will include looking at its major applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾信息检索和神经搜索的相关重要概念，同时探讨神经搜索为开发者带来的好处。在介绍神经搜索之前，我们将首先介绍基于传统符号的搜索的缺点。接着，我们将探讨如何利用神经网络构建跨/多模态搜索，这将包括其主要应用。
- en: 'In this chapter, we’re going to cover the following main topics in particular:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍以下几个主题：
- en: Legacy search versus neural search
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统搜索与神经搜索
- en: Machine learning for search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索中的机器学习
- en: Practical applications for neural search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经搜索的实际应用
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具有以下技术要求：
- en: '**Hardware**: A desktop or laptop computer with a minimum of 4 GB of RAM; 8
    GB is suggested'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件**：具有至少4 GB内存的台式机或笔记本电脑；建议使用8 GB内存'
- en: '**Operating system**: A Unix-like operating system such as macOS, or any Linux-based
    distribution, such as Ubuntu'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作系统**：类似Unix的操作系统，如macOS，或任何基于Linux的发行版，如Ubuntu'
- en: '**Programming Language**: Python 3.7 or higher, and Python Package Installer,
    or pip'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程语言**：Python 3.7或更高版本，以及Python包管理器或pip'
- en: Legacy search versus neural search
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统搜索与神经搜索
- en: This section will guide you through the fundamentals of symbolic search systems,
    the different types of search applications, and their importance. This is followed
    by a brief description of how the symbolic search system works, with some code
    written in Python. Last but not least, we’ll summarize the pros and cons of the
    traditional symbolic search versus neural search. This will help us to understand
    how a neural search can better bridge the gap between a user’s intent and the
    retrieved documents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将引导你了解符号搜索系统的基本原理、不同类型的搜索应用及其重要性。接下来会简要描述符号搜索系统的工作原理，并提供一些用Python编写的代码。最后，我们将总结传统符号搜索与神经搜索的优缺点。这将帮助我们理解神经搜索如何更好地弥合用户意图与检索文档之间的差距。
- en: Exploring various data types and search scenarios
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索各种数据类型和搜索场景
- en: In today’s society, governments, enterprises, and individuals create a huge
    amount of data by using various platforms every day. We live in the era of big
    data, where things such as texts, images, videos, and audio files play a significant
    role in society and the fulfillment of daily tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今社会，政府、企业和个人每天通过各种平台产生大量数据。我们生活在大数据时代，文本、图像、视频和音频文件等在社会中发挥着重要作用，并且是日常任务完成的重要组成部分。
- en: 'Generally speaking, there are three types of data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，数据分为三种类型：
- en: '**Structured data:** This includes data that is logically expressed and realized
    by a two-dimensional table structure. Structured data strictly follows a specific
    data format and length specifications and is mainly stored and managed using relational
    databases.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化数据**：指以二维表格结构表达和实现的数据。结构化数据严格遵循特定的数据格式和长度规格，主要通过关系型数据库进行存储和管理。'
- en: '**Unstructured data:** This has neither a regular or complete structure nor
    a predefined data model. This type of data is not appropriately managed by representing
    the data using a two-dimensional logical table used in databases. This includes
    office documents, text, pictures, hypertext markup language (HTML), various reports,
    images, and audio and video information in all formats.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非结构化数据：** 这种数据既没有规律或完整的结构，也没有预定义的数据模型。通过使用数据库中常见的二维逻辑表来表示数据，这种数据难以得到有效管理。非结构化数据包括办公文档、文本、图片、超文本标记语言（HTML）、各种报告以及所有格式的图像、音频和视频信息。'
- en: '**Semi-structured data:** This falls somewhere between structured and unstructured
    data. It includes log files, **Extensible Markup Language** (**XML**), and **Javascript
    Object Notation** (**JSON**). Semi-structured data does not conform to the data
    model structure associated with relational databases or other data tables, but
    it contains relevant tags that can be used to separate semantic elements that
    are used to stratify records and fields.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半结构化数据：** 这种数据介于结构化数据和非结构化数据之间。它包括日志文件、**可扩展标记语言**（**XML**）和**JavaScript
    对象表示法**（**JSON**）。半结构化数据不符合与关系数据库或其他数据表相关的数据模型结构，但它包含相关的标签，可以用来分隔语义元素，以便对记录和字段进行分层。'
- en: 'Search indices are widely used to hunt for unstructured and semi-structured
    data within a massive data collection to meet the information needs of users.
    Based on the levels and applications of the document collection, searches can
    be further divided into three types: web search, enterprise search, and personal
    search.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索索引广泛用于在庞大的数据集中搜索非结构化和半结构化数据，以满足用户的信息需求。根据文档集合的层级和应用，搜索可以进一步分为三种类型：网络搜索、企业搜索和个人搜索。
- en: In a **web search**, the search engine first needs to index hundreds of millions
    of documents. The search results are then returned to users in an efficient manner
    while the system is continuously optimized. Typical examples of web search applications
    are Google, Bing, and Baidu.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在**网络搜索**中，搜索引擎首先需要对数以亿计的文档进行索引。然后，搜索结果会以高效的方式返回给用户，同时系统也在不断优化。网络搜索应用的典型例子包括
    Google、Bing 和百度。
- en: In addition to web search, as a software development engineer, you are likely
    to encounter *enterprise* and *personal search* operations. In enterprise search
    scenarios, the search engine indexes internal documents of an enterprise to serve
    the employees and customers of the business, such as an internal patent search
    index of a company, or the search index of a music platform, such as SoundCloud.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了网络搜索之外，作为软件开发工程师，你很可能会遇到*企业*和*个人搜索*操作。在企业搜索场景中，搜索引擎对企业的内部文档进行索引，以服务企业的员工和客户，例如公司内部的专利搜索索引，或音乐平台如
    SoundCloud 的搜索索引。
- en: If you are developing an email application and intend to allow users to search
    for historical emails, this constitutes a typical example of a personal search.
    This book focuses on enterprise and personal types of search operations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在开发一个电子邮件应用，并计划允许用户搜索历史邮件，那么这就构成了典型的个人搜索示例。本书主要关注企业和个人类型的搜索操作。
- en: Important Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure you understand the difference between search and match. Search, in
    most cases, is done in documents organized in an unstructured or semi-structured
    format, while match (such as an SQL-like query) takes place on structured data,
    such as tabular data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你理解搜索与匹配之间的区别。搜索通常是在组织成非结构化或半结构化格式的文档中进行的，而匹配（如 SQL 类似的查询）则发生在结构化数据上，例如表格数据。
- en: As for different data types, the concept of modality plays an important role
    in a search system. Modality refers to the form of information such as text, images,
    video, and audio files. Cross-modality search (also known as *cross-media search*)
    refers to retrieving samples from different modes with similar semantics by exploring
    the relationship between different modalities and employing a certain modal sample.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同的数据类型，模态的概念在搜索系统中起着重要作用。模态指的是信息的形式，如文本、图像、视频和音频文件。跨模态搜索（也称为*跨媒体搜索*）是指通过探索不同模态之间的关系，并利用某种模态样本，来检索具有相似语义的不同模态样本。
- en: For example, when we enter a keyword in an email inbox application, we can find
    the appropriate email returns as a result of a unimodal search – searching text
    by text. When you enter a keyword on a page for image retrieval, the search engine
    will return appropriate images as a result of a cross-modal search, searching
    images by text.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们在电子邮件收件箱应用中输入一个关键词时，我们可以通过单模搜索——逐字搜索文本，找到相应的邮件。当你在页面中输入一个关键词进行图片检索时，搜索引擎会通过跨模态搜索，将文本作为依据返回相关的图片。
- en: Of course, a unimodal search is not limited to searching text by text. The app
    known as Shazam, which is popular in the App Store, helps users to identify music
    and returns a track’s title to users in a short time. This can be seen as an application
    of unimodal search. Here, the concept of modality no longer refers to text, but
    to audio. On Pinterest, users can locate similar images through an image search,
    where the modality refers to an image. Likewise, the scope of a cross-modal search
    covers far more than searching for images by text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，单模搜索不仅限于逐字搜索文本。像Shazam这样的应用，在App Store中非常流行，它帮助用户识别音乐，并在短时间内返回歌曲的标题。这可以视为单模搜索的应用。在这里，模态的概念不再指文本，而是指音频。在Pinterest上，用户可以通过图片搜索定位相似的图片，模态指的是图片。同样，跨模态搜索的范围远不止通过文本搜索图片。
- en: Let’s consider this from another perspective. Is it possible for us to search
    across multiple modalities? Of course, the answer is “Yes!” Imagine a search scenario
    where a user uploads a photo of clothes and wants to look for similar types of
    clothing (we usually call this type of application “shop the look”), and at the
    same time enters a paragraph that describes the clothes in the search box to improve
    the accuracy of the search. In this way, our search keywords span two modalities
    (text and images). We refer to this search scenario as a multi-modal search.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从另一个角度来考虑这个问题。我们能否进行跨多个模态的搜索呢？当然，答案是“可以！”想象一下这样的搜索场景：用户上传一张衣物的照片，并希望找到相似类型的衣服（我们通常称这种应用为“shop
    the look”），同时在搜索框中输入描述衣物的段落，以提高搜索的准确性。这样，我们的搜索关键词跨越了两种模态（文本和图片）。我们称这种搜索场景为多模态搜索。
- en: Now that we have a grasp of the concept of modality, we will elaborate on the
    working principles, advantages, and disadvantages of symbolic search systems.
    By the end of this section, you will understand, that symbolic search systems
    cannot deal with different modalities.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了模态的概念，接下来我们将详细阐述符号搜索系统的工作原理、优缺点。到本节结束时，你将理解，符号搜索系统无法处理不同的模态。
- en: How does the traditional search system work?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统搜索系统是如何工作的？
- en: As a developer, you may have used Elasticsearch or Apache Solr to build a search
    system in web applications. These two widely used search frameworks were developed
    based on Apache Lucene. We’ll take Lucene as a case in point to introduce the
    components of a search system. Imagine you intend to search for a keyword in thousands
    of text documents (`txt`). How will you complete this task?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，你可能曾经使用过Elasticsearch或Apache Solr来构建网页应用中的搜索系统。这两个广泛使用的搜索框架是基于Apache Lucene开发的。我们以Lucene为例，介绍搜索系统的组成部分。假设你打算在成千上万的文本文件（`txt`）中搜索一个关键词，你会如何完成这个任务？
- en: 'The easiest solution is to traverse all text documents from a path and read
    through the contents of these documents. If the keyword is in the file, the name
    of the document will be returned:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是遍历从路径中获取的所有文本文件，读取这些文件的内容。如果文件中包含该关键词，文档名称将被返回：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The code fulfills the simplest search function by traversing all files with
    the extension `.txt` in the current directory and then opening those files in
    turn. If the keyword `hello` `jina` used for the query is available, the filename
    will be printed with all the matching files. Although these lines of code allow
    you to conduct a basic search, the process has many flaws:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码通过遍历当前目录下所有扩展名为`.txt`的文件，并依次打开这些文件，来实现最简单的搜索功能。如果查询使用的关键词`hello`或`jina`存在，匹配的文件名将被打印出来。虽然这些代码允许你进行基本的搜索，但其过程有很多缺陷：
- en: '**Poor scalability**: In a production environment, there may be millions of
    files to be retrieved. Meanwhile, users of the retrieval system expect to obtain
    retrieval results in the shortest possible time, posing stringent requirements
    for the performance of the search system.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性差**：在生产环境中，可能有数百万个文件需要检索。与此同时，检索系统的用户期望在最短的时间内获得检索结果，这对搜索系统的性能提出了严格的要求。'
- en: '**Lack of a relevance measurement**: The code helps you achieve the most basic
    Boolean retrieval, which is to return the result of a match or mismatch. In a
    real-world scenario, users need a score to measure the relevance degree from a
    search system that is sorted in descending order, with more relevant files being
    returned to users first. Obviously, the aforementioned code snippets are unable
    to fulfill this function.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏相关性度量**：该代码帮助您实现最基本的布尔检索，即返回匹配或不匹配的结果。在现实世界的场景中，用户需要一个评分来衡量来自搜索系统的相关性程度，且搜索结果会按降序排列，相关性更高的文件会优先返回给用户。显然，前述代码片段无法实现这一功能。'
- en: To address these issues, we need to *index* the files to be retrieved. **Indexing**
    refers to a process of converting a file type that allows a rapid search and skipping
    the continuous scanning of all files.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们需要对待检索的文件进行*索引*。**索引**是指将文件类型转换为允许快速搜索并跳过对所有文件的连续扫描的过程。
- en: As an important part of our daily lives, indexing is comparable to consulting
    a dictionary and visiting a library. We’ll use the most widely used search library,
    Lucene, to illustrate the idea.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们日常生活中的重要部分，索引类似于查阅字典或访问图书馆。我们将使用最广泛使用的搜索库Lucene来说明这个概念。
- en: Lucene Core ([https://lucene.apache.org/](https://lucene.apache.org/)) is a
    Java library providing powerful indexing and search features, as well as spellchecking,
    hit highlighting, and advanced analysis/tokenization capabilities. Apache Lucene
    sets the standard for search and indexing performance. It is the search core of
    both Apache Solr and Elasticsearch.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene Core ([https://lucene.apache.org/](https://lucene.apache.org/))是一个Java库，提供强大的索引和搜索功能，以及拼写检查、命中高亮显示和高级分析/分词能力。Apache
    Lucene为搜索和索引性能设定了标准。它是Apache Solr和Elasticsearch的搜索核心。
- en: In Lucene, after all collections of files to be retrieved are loaded, you may
    extract texts from such files and convert them to Lucene Documents, which generally
    contain the title, body, abstract, author, and URL of a file.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lucene中，在所有待检索文件集合加载完毕后，您可以从这些文件中提取文本，并将它们转换为Lucene文档，这些文档通常包含文件的标题、正文、摘要、作者和URL。
- en: 'Next, your file will be analyzed by Lucene’s *text analyzer*, which generally
    includes the following processes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您的文件将由Lucene的*文本分析器*进行分析，通常包括以下过程：
- en: '**Tokenizer**: This splits the raw input paragraphs into tokens that cannot
    be further decomposed.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词器**：这会将原始输入段落分割成不能进一步分解的词素。'
- en: '**Decomposing** **compound** **words**: In languages such as German, words
    composed of two or more tokens are called compound words.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**分解** **复合** **词**：在德语等语言中，由两个或多个词素组成的单词被称为复合词。'
- en: '**Spell correction**: Lucene allows users to conduct spellchecking to enhance
    the accuracy of retrieval.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**拼写修正**：Lucene允许用户进行拼写检查，以提高检索的准确性。'
- en: '**Synonym analysis:** This enables users to manually add synonyms in Lucene
    to improve the recall rate of the search system (note: the accuracy rate and recall
    rate will be elaborated upon shortly).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**同义词分析**：这使得用户可以手动在Lucene中添加同义词，以提高搜索系统的召回率（注意：准确率和召回率将稍后详细说明）。'
- en: '**Stemming and lemmatization:** The former enables users to derive the root
    by removing the suffix of a word (for example, *play*, the root form, is derived
    from the words *plays*, *playing*, and *played*), while the latter helps users
    convert words into basic forms, such as *is*, *are*, and *been*, which are converted
    to *be*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取与词形还原**：前者通过去除词汇的后缀来派生词根（例如，*play*作为词根是从单词*plays*、*playing*和*played*中派生出来的），而后者则帮助用户将单词转换为基本形式，例如，*is*、*are*和*been*都可以转换为*be*。'
- en: Let’s attempt to preprocess some texts using **NLTK**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用**NLTK**预处理一些文本。
- en: Important Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: NLTK is a leading platform for building Python programs to work with human language
    data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK是一个领先的平台，用于构建与人类语言数据交互的Python程序。它提供了一个易于使用的接口，访问50多个语料库和词汇资源。
- en: 'First, install a Python package called `nltk` with this command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过以下命令安装一个名为`nltk`的Python包：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We preprocess the text `Jina is a neural search framework built with cutting-edge
    technology called deep learning`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预处理文本`Jina is a neural search framework built with cutting-edge technology
    called deep learning`：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code enables us to carry out two operations on a sentence: tokenizing
    and stemming. The results of each are printed respectively. The raw input strings
    are parsed into a list of strings in Python, and finally each parsed token is
    lemmatized to its basic form. For instance, `cutting` and `called` are respectively
    converted to `cut` and `call`. For more operations, please refer to the official
    documentation of NLTK ([https://www.nltk.org/](https://www.nltk.org/)).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使我们能够对一个句子进行两项操作：分词和词干提取。每个操作的结果分别打印出来。原始输入字符串被解析为Python中的字符串列表，最终每个解析后的词元被词形还原为其基本形式。例如，`cutting`和`called`分别被转换为`cut`和`call`。有关更多操作，请参阅NLTK的官方文档（[https://www.nltk.org/](https://www.nltk.org/)）。
- en: After files are processed with the Lucene Document, the *clean* files will be
    indexed. Generally, in a traditional search system, all files are indexed using
    an **inverted index**. An inverted index (also referred to as a **postings file**
    or **inverted file**) is an index data structure storing a map of content, such
    as words or numbers, to its locations in a database file, or in a document or
    a set of documents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Lucene文档处理文件后，*清理过的*文件将被索引。通常，在传统的搜索系统中，所有文件都使用**倒排索引**进行索引。倒排索引（也称为**文档列表文件**或**倒排文件**）是一种索引数据结构，它将内容（如单词或数字）映射到其在数据库文件或文档集中的位置。
- en: 'Simply put, an inverted index consists of two parts: a **term dictionary**,
    and **postings**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，倒排索引由两部分组成：**术语词典**和**文档列表**。
- en: Tokens, their IDs, and the document frequency (the frequency of such tokens
    appearing in the entire collection of documents to be retrieved) are stored in
    the term dictionary. A collection of all tokens is called a vocabulary. All tokens
    are sorted in alphabetical order in the dictionary.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 词元、它们的ID和文档频率（即该词元在整个待检索文档集合中出现的频率）存储在术语词典中。所有词元的集合称为词汇表。词典中的所有词元按字母顺序排列。
- en: 'In the postings, we save the token ID and the document IDs where the token
    occurred. Assuming that in the aforesaid example, the token `jina` from our query
    keyword `hello jina` appears three times in the entire collection of documents
    (in `1.txt`, `3.txt`, and `11.txt`), then the token is “jina” and the document
    frequency is 3\. Meanwhile, the names of the three text documents, `1.txt`, `3.txt`,
    and `11.txt`, are saved in the posting. Then, the indexing of the text file is
    completed as shown in the following figure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档列表中，我们保存了词元ID和该词元出现的文档ID。假设在上述示例中，查询关键词`hello jina`中的词元`jina`在整个文档集合中出现了三次（分别出现在`1.txt`、`3.txt`和`11.txt`中），那么该词元为“jina”，文档频率为3。同时，这三个文本文件的名称`1.txt`、`3.txt`和`11.txt`将被保存在文档列表中。随后，文本文件的索引就完成了，如下图所示：
- en: '![Figure 1.1 – Data structure of inverted index ](img/Figure_1.1_B17488.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 倒排索引的数据结构](img/Figure_1.1_B17488.jpg)'
- en: Figure 1.1 – Data structure of inverted index
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 倒排索引的数据结构
- en: When a user makes a query, keywords used for the query are generally shorter
    than the collection of documents to be retrieved. Lucene can perform the same
    preprocessing for such keywords (such as tokenization, decomposition, and spelling
    correction).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户发出查询时，用于查询的关键词通常比待检索的文档集合要短。Lucene可以对这些关键词执行相同的预处理操作（如分词、分解和拼写纠正）。
- en: The processed tokens are mapped to the postings through the term dictionary
    in the inverted index so that matched files can be quickly found. Finally, Lucene’s
    scoring starts to work and scores each related file discovered according to a
    vector space model. Our index file is stored in an inverted index, which may be
    represented as a vector.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的词元通过倒排索引中的术语词典映射到文档列表，从而可以快速找到匹配的文件。最后，Lucene的打分机制开始工作，并根据向量空间模型对每个相关文件进行打分。我们的索引文件存储在倒排索引中，可以表示为一个向量。
- en: Assuming that our query keyword is `jina`, we map it to the vector of the inverted
    index and have it represented by `-` when it does not appear in the file; then
    the query vector `[-,'jina',-,-, ...]` can be obtained. This is how we represent
    a *query*, as a **vector space model**, in a traditional search engine.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的查询关键词是`jina`，我们将其映射到倒排索引的向量中，并在该词元未出现在文件中时用`-`表示；此时可以获得查询向量`[-,'jina',-,-,...]`。这就是我们在传统搜索引擎中以**向量空间模型**表示一个*查询*的方式。
- en: '![Figure 1.2 – Term occurrence in the vector space model ](img/Figure_1.2_B17488.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 向量空间模型中的术语出现](img/Figure_1.2_B17488.jpg)'
- en: Figure 1.2 – Term occurrence in the vector space model
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 向量空间模型中的术语出现
- en: Next, in order to derive the ranking, we need to numerically represent the token
    of the space vector model. Generally, **tf-idf** is regarded as a simple approach.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了得出排名，我们需要对空间向量模型的词汇进行数值表示。通常，**tf-idf**被认为是一种简单的方法。
- en: With this algorithm, we grant a higher weight to any token that appears relatively
    frequently. If such a token appears multiple times in many documents, we believe
    that the token is weakly representative, and the weight of the token will be reduced
    again. If the token does not appear in the documents, its weight is 0\.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此算法，我们赋予任何相对频繁出现的词汇更高的权重。如果一个词在多个文档中多次出现，我们认为该词代表性较弱，因此它的权重会再次降低。如果该词在文档中没有出现，它的权重为0\。
- en: 'In Lucene, an algorithm called **bm-25** is employed more frequently, which
    further optimizes tf-idf. After numerical calculation, the vector is expressed
    as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lucene中，更常用的算法是**bm-25**，它进一步优化了tf-idf。在数值计算后，向量表示如下：
- en: '![Figure 1.3 – Vector space representation ](img/Figure_1.3_B17488.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 向量空间表示](img/Figure_1.3_B17488.jpg)'
- en: Figure 1.3 – Vector space representation
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 向量空间表示
- en: As shown in the preceding figure, because the word **a** appears too frequently,
    it appears in document 1 and document 2 and has a low weight score. The token
    *jina*, a relatively uncommon word (appearing in document 2), has been granted
    a higher weight.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，由于词汇**a**出现过于频繁，它出现在文档1和文档2中，并且权重较低。而*Jina*这个相对不常见的词（出现在文档2中）则被赋予了更高的权重。
- en: In the query vector, because the query keyword only has one word, *jina*, its
    weight is set as 1 and the weights of other tokens that do not appear are set
    as 0\. Afterward, we multiply the query vector and the document vector element
    by element and add up the results to obtain the score of each document corresponding
    to the query keyword. Then, reverse sorting is performed so that the sorted documents
    can be returned to the user according to the score sorted in an inverted order
    (from high to low scores).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询向量中，由于查询关键词只有一个词，*Jina*，它的权重被设为1，其他未出现的词汇权重设为0。然后，我们将查询向量与文档向量按元素相乘并将结果相加，从而获得每个文档对应查询关键词的分数。随后，进行倒序排序，按照分数从高到低排序，最终将排序后的文档返回给用户。
- en: In short, if the keyword used for a query appears more frequently in a particular
    file and less frequently in the vocabulary file, its relative score will be higher
    and returned to the user with a higher priority. Of course, Lucene also grants
    different weights to various parts of a file. For example, the title and keywords
    of the file will have a higher scoring weight than the body would. Given the fact
    this book is about neural search, this aspect will not be elaborated upon further
    here.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果查询使用的关键字在特定文件中出现得更频繁，而在词汇文件中出现得较少，那么它的相对分数会更高，并且会以更高的优先级返回给用户。当然，Lucene也会根据文件的不同部分赋予不同的权重。例如，文件的标题和关键词的得分权重通常会高于正文部分。鉴于本书讨论的是神经搜索，关于这一方面不会再进一步展开。
- en: Pros and cons of the traditional search system
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统搜索系统的优缺点
- en: 'In the previous section, we briefly revisited traditional symbolic search.
    Perhaps you have noticed that both the Lucene we introduced previously and the
    Lucene-based search frameworks, such as Elasticsearch and Solr, are based on text
    retrieval. This has quite a few advantages in the application scenarios of searching
    text by text:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们简要回顾了传统的符号搜索。也许你已经注意到，我们之前介绍的Lucene，以及基于Lucene的搜索框架，如Elasticsearch和Solr，都是基于文本检索的。这在基于文本搜索的应用场景中有不少优势：
- en: '**Mature technology**: Since research and development were done in 1999, the
    Lucene and Lucene-based search systems have existed for over 20 years and have
    been widely used in various web applications.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**成熟的技术**：由于研究和开发始于1999年，基于Lucene的搜索系统已经存在超过20年，并且在各种网页应用中得到了广泛应用。'
- en: '**Easy integration**: As users, developers of a web application do not need
    to have a deep understanding of Elasticsearch, Solr, or the operating logic of
    Lucene; only a small amount of code is required to integrate a high-performing,
    extensible search system into web applications.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**易于集成**：作为用户，网页应用的开发人员无需深入理解Elasticsearch、Solr或Lucene的操作逻辑；只需少量代码即可将高性能、可扩展的搜索系统集成到网页应用中。'
- en: '**Well-developed ecosystem**: Thanks to the operation of Elastic Company, Elasticsearch
    has extended its search system functionality significantly. Currently, it is not
    only a search framework, but also a platform equipped with user management, a
    restful interface, data backup and restoration, and security management such as
    single sign-in, log audit, and other functions. Meanwhile, the Elasticsearch community
    has contributed a variety of plugins and integrations.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**完善的生态系统**：得益于Elastic公司运营，Elasticsearch显著扩展了其搜索系统功能。目前，它不仅是一个搜索框架，还配备了用户管理、RESTful接口、数据备份与恢复、以及包括单点登录、日志审计等在内的安全管理功能。同时，Elasticsearch社区也贡献了各种插件和集成。'
- en: At the same time, you have probably realized that both Elasticsearch and Solr
    with Lucene at the core have unavoidable flaws.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，您可能已经意识到，无论是基于Lucene的Elasticsearch还是Solr，都有无法避免的缺陷。
- en: In the previous section, we introduced the concept of modality. Lucene and Elasticsearch,
    which is built on top of it, are inherently unable to support cross-modal and
    multi-modal search options. Let’s take a moment to review the operating principle
    of Lucene, as Lucene has powered most of the search systems users are using on
    a daily basis. When texts are preprocessed in the first place, the search keyword
    must be text. When a data collection to be retrieved is preprocessed and indexed,
    likewise the index result is also the text stored in the inverted index.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面一节中，我们介绍了模态的概念。基于Lucene构建的Elasticsearch，本质上无法支持跨模态和多模态搜索选项。让我们稍作回顾Lucene的工作原理，因为Lucene为用户日常使用的大多数搜索系统提供了支持。当文本首先被预处理时，搜索关键字必须是文本。当要检索的数据集经过预处理和索引时，相应的索引结果也是存储在倒排索引中的文本。
- en: 'In this way, the Lucene-based search platform can only rely on the text modality
    and retrieve data in the text modality. If objects to be retrieved are images,
    audio, or video files, how can they be found using a traditional search system?
    It is quite simple; two main methods are employed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，基于Lucene的搜索平台只能依赖文本模态进行数据检索。如果要检索的对象是图片、音频或视频文件，传统搜索系统如何找到它们呢？其实很简单，采用了两种主要方法：
- en: '**Manual tagging and adding metadata**: For example, when a user uploads a
    song to a music platform, they may manually tag the author, album, music type,
    release time, and other data. Doing so ensures that users are able to retrieve
    music using text.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工标注和添加元数据**：例如，当用户上传一首歌曲到音乐平台时，他们可能会手动标记作者、专辑、音乐类型、发布时间等数据。这样做确保了用户可以通过文本来检索音乐。'
- en: '**Hypothesis of the surrounding text**: If an image, in the absence of user
    tagging, appears in an article, it will be assumed by the traditional search system
    to be more closely associated with its surrounding text. Accordingly, when a user’s
    query keyword matches the surrounding text of the image, the latter will be matched.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**周围文本的假设**：如果一张图片在没有用户标注的情况下出现在一篇文章中，传统搜索系统会假设该图片与其周围的文本有较强的关联。因此，当用户查询的关键字与图片周围的文本匹配时，图片就会被检索到。'
- en: The essence of the two methods is to convert the document of non-text modality
    into a text modality so as to effectively use the current retrieval technology.
    However, this modal conversion process either relies on a large amount of manual
    tagging, or is done at the cost of query accuracy, which greatly undermines the
    user’s search experience.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的本质是将非文本模态的文档转换为文本模态，从而有效利用当前的检索技术。然而，这一模态转换过程要么依赖大量的人工标注，要么以查询准确性为代价，从而大大削弱了用户的搜索体验。
- en: Likewise, this type of search mode limits the user’s search habits to a keyword
    search and cannot be extended to a real cross-modal or even multi-modal search.
    For deeper insight into this issue, we may use a vector space to represent keywords
    of a paragraph and use another vector space to denote a text document to be retrieved.
    However, due to the restrictions of the technology back in the days when we had
    to rely on traditional search systems, we were unable to use the space vector
    to represent a piece of music, image, or video. It is also impossible to map two
    documents of different modalities to the same space vector to compare their similarities.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这种搜索模式将用户的搜索习惯局限于关键词搜索，无法扩展到真正的跨模态甚至多模态搜索。为了更深入地理解这个问题，我们可以使用向量空间来表示一段文字的关键词，并使用另一个向量空间来表示待检索的文本。然而，由于当时我们不得不依赖传统搜索系统的技术限制，我们无法使用空间向量来表示一段音乐、图像或视频。同样，也无法将不同模态的两个文档映射到同一个空间向量中以比较它们的相似性。
- en: With the research and development on (statistical) machine learning techniques,
    more and more researchers and engineers have started to empower their search system
    using machine learning algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随着（统计）机器学习技术的研究和发展，越来越多的研究人员和工程师开始通过使用机器学习算法来增强他们的搜索系统。
- en: Machine learning for search
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于搜索的机器学习
- en: As a cross-disciplinary task, neural search has gone beyond the boundaries of
    information retrieval. It requires a general understanding of the concepts of
    machine learning, deep learning, and how we can apply these techniques to improve
    a search task. In this section, we will give a brief introduction to machine learning
    and how it can be applied to search systems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项跨学科的任务，神经搜索已超越了信息检索的边界。它需要对机器学习、深度学习的概念有一个基本的了解，并理解如何应用这些技术来改进搜索任务。在本节中，我们将简要介绍机器学习以及它如何应用于搜索系统。
- en: Understanding machine learning and artificial intelligence
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解机器学习与人工智能
- en: '**Machine learning** refers to a technique that teaches computers to make decisions
    in a way that comes naturally to humans by enabling computers to learn the inherent
    laws of data and acquire new experience and knowledge, thus improving their intelligence.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**是指一种技术，通过使计算机学习数据的内在规律，并获取新的经验和知识，从而提高计算机的智能，使其能够以类似人类自然的方式做出决策。'
- en: Because various industries require an increased level of efficiency during data
    processing and analysis due to their growing demand for data, a large number of
    machine learning algorithms have emerged. The concept of statistical machine learning
    algorithms primarily refers to the steps and processes of solving optimization
    problems through mathematical and statistical methods.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各行业对数据处理和分析效率的需求不断增加，大量的机器学习算法应运而生。统计机器学习算法的概念主要是指通过数学和统计方法解决优化问题的步骤和过程。
- en: With respect to different data and model requirements, appropriate machine learning
    algorithms are selected and employed to tackle practical issues in a more efficient
    manner. Machine learning has achieved great success in many fields, such as natural
    language understanding, computer vision, machine translation, and expert systems.
    It is fair to say that whether a system has a *learning* function or not has become
    a hallmark of it possessing intelligence.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同的数据和模型需求，选择并使用适当的机器学习算法，以更高效的方式解决实际问题。机器学习在许多领域取得了巨大的成功，如自然语言理解、计算机视觉、机器翻译和专家系统。可以说，是否具备*学习*功能，已经成为判断一个系统是否具备智能的标志。
- en: Hinton et al. (2006) proposed the concept of **deep learning** (deep learning/deep
    neural networks). In 2009, Hinton introduced deep neural networks to scholars
    specialized in voices. Hence, in 2010, this field of research witnessed a remarkable
    breakthrough in speech recognition. In the next 11 years, **convolutional neural
    networks** (**CNNs**) were applied in the field of image recognition, leading
    to significant achievements.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton 等人（2006）提出了**深度学习**（深度学习/深度神经网络）的概念。2009年，Hinton 将深度神经网络引入了专注于语音的学者中。因此，2010年，这一研究领域在语音识别方面取得了显著突破。在接下来的11年中，**卷积神经网络**（**CNNs**）被广泛应用于图像识别领域，并取得了显著成就。
- en: Three founders of neural networks, LeCun, Bengio, and Hinton (2015), published
    a review titled *Deep Learning* in Nature. This shows that deep neural networks
    have not only been accepted by academia, but also the industrial field. Furthermore,
    in 2016 and 2017, the world witnessed a general expansion in deep learning. AlphaGo
    and AlphaZero were invented by Google after a short learning period and won a
    landslide victory over the top three Go players in the world. The intelligent
    voice system launched by iFLYTEK boasts a recognition accuracy rate of over 97%
    and stands at the forefront of AI worldwide; the autonomous driving systems developed
    by companies such as Google and Tesla have passed a milestone of testing on the
    road. These achievements have unveiled the value and charm of neural networks
    to humans again.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的三位创始人LeCun、Bengio和Hinton（2015年）在《自然》杂志上发表了一篇名为*深度学习*的综述文章。这表明深度神经网络不仅被学术界接受，而且在工业领域也得到了广泛应用。此外，2016年和2017年，深度学习迎来了全球范围的扩展。AlphaGo和AlphaZero在谷歌经过短期学习后发明，并以压倒性胜利战胜了世界排名前三的围棋选手。科大讯飞推出的智能语音系统，识别准确率超过97%，处于全球人工智能的前沿；谷歌和特斯拉等公司开发的自动驾驶系统，已经达到了路测的里程碑。这些成就再次揭示了神经网络的价值与魅力。
- en: 'Machine learning has been applied to various industries, so maybe we can ask
    ourselves: can we apply machine learning to search applications? The answer here
    is “Yes.” In the next section, we’ll give a brief overview of different types
    of machine learning and how search can benefit from it.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习已经应用于各个行业，那么或许我们可以问自己：能否将机器学习应用到搜索应用中？答案是“可以”。在接下来的部分，我们将简要概述不同类型的机器学习以及搜索如何从中受益。
- en: Machine learning and learning-to-rank
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习与排序学习
- en: 'Imagine a scenario where you intend to train a model capable of evaluating
    the price of a new apartment or house based on the collected data related to local
    real estate information and prices. This is one of the most important tasks of
    machine learning: **regression**.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样一个场景，你打算训练一个能够根据收集到的与本地房地产信息和价格相关的数据，评估新公寓或房屋价格的模型。这是机器学习中最重要的任务之一：**回归**。
- en: Before the popularization of the deep learning technique, data analysts would
    have had to clean this data, use business logic to perform feature engineering,
    and design features of a real estate price predictor, such as the floor area,
    construction time, and type of apartments or houses, as well as the average prices
    of surrounding apartments or houses, and so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习技术普及之前，数据分析师需要清洗这些数据，利用业务逻辑进行特征工程，并设计房地产价格预测器的特征，例如建筑面积、建造时间、房屋或公寓的类型，以及周围房屋或公寓的平均价格等。
- en: 'After feature engineering has been completed, raw data will be used to form
    a two-dimensional data table similar to Excel. The horizontal axis represents
    each house record, and the vertical axis represents each feature. The data is
    usually divided into two to three parts again: the majority of the data is used
    for *model training*, while a small amount of the data is used for *model evaluation.*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程完成后，原始数据将被用于形成一个类似于Excel的二维数据表。横轴代表每个房产记录，纵轴代表每个特征。数据通常会再次分为两到三部分：大多数数据用于*模型训练*，而少量数据用于*模型评估*。
- en: Next, machine learning engineers will select one or more appropriate algorithms
    from the machine learning toolkit to train the model and evaluate the performance
    of the model in the test data. Finally, the model with the best performance will
    be deployed in the production environment to serve customers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，机器学习工程师将从机器学习工具包中选择一个或多个适合的算法，进行模型训练，并评估模型在测试数据上的表现。最后，表现最佳的模型将被部署到生产环境中，为客户提供服务。
- en: 'Imagine another scenario where many landmark pictures have been collected from
    social networks. When a user uploads a new landmark picture, you expect your system
    to automatically recognize the name of the site. This is another important task
    of machine learning: **classification**.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 再想象一个场景，你收集了许多来自社交网络的地标图片。当用户上传一张新的地标图片时，你希望你的系统能够自动识别该地点的名称。这是机器学习中的另一个重要任务：**分类**。
- en: In the field of traditional machine learning and computer vision, some features,
    such as SIFT, SURF, and HOG, are employed to develop a **Bag-of-Visual-Words**
    (**BoW**)through which a vector representation of this photo is established. Moreover,
    models are used to predict the classification. Nowadays, deep learning serves
    as the model to extract visual features from images without the requirement of
    feature engineering.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统机器学习和计算机视觉领域，采用一些特征，如SIFT、SURF和HOG，来开发**视觉词袋**（**BoW**），通过它建立该照片的向量表示。此外，模型被用来预测分类。如今，深度学习作为模型，无需特征工程即可从图像中提取视觉特征。
- en: Let’s take a moment to look at our two examples. During the training process
    of predicting prices of houses (apartments), models are trained using *feature
    engineering*. All the training data is ground-truth, i.e., the house (apartment)
    prices and landmark names are documented. Such tasks are collectively referred
    to as supervised machine learning in the field of machine learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间看一下我们的两个例子。在预测房价（公寓）价格的训练过程中，模型通过*特征工程*进行训练。所有的训练数据都是实际值，即房价（公寓价格）和地标名称都有文档记录。这样的任务统称为机器学习中的监督式学习。
- en: Since we can perform regression analysis and classification of data through
    supervised machine learning, is it possible to apply supervised machine learning
    in the search? The answer is yes, of course.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以通过监督式机器学习进行数据的回归分析和分类，那么是否可以将监督式机器学习应用到搜索中呢？答案是肯定的，当然可以。
- en: Assuming that our task is to optimize the search system, the goal is to predict
    the user’s click rate for the document and return documents with a higher predicted
    click rate to the user first. This is called **learning-to-rank** (first stage)
    and **neural information retrieval** (second stage). The concept of learning-to-rank
    (based on statistical machine learning), as proposed by academia in the early
    1990s, evolved for nearly 20 years before experiencing a downturn since the emergence
    of deep learning in 2010, when neural information retrieval was at its peak.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的任务是优化搜索系统，目标是预测用户点击文档的概率，并将预测点击率更高的文档首先返回给用户。这就是**学习排序**（第一阶段）和**神经信息检索**（第二阶段）。学习排序的概念（基于统计机器学习）由学术界在1990年代初提出，经过近20年的发展后，在2010年深度学习的出现导致其进入低谷，神经信息检索达到顶峰。
- en: Just like the prediction of an apartment (or house) price, or landmark recognition,
    engineers first performed data engineering after collecting such data. Common
    features include the number of query keywords in the document title/body, the
    percentage of the document title body that contains the query keywords, the tf-idf
    score, and the bm-25 score, among others. It follows that the final score of a
    traditional search system is used as a numerical feature during the training of
    models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就像预测公寓（或房屋）价格，或地标识别一样，工程师们在收集数据后首先进行数据工程。常见的特征包括文档标题/正文中查询关键词的数量、文档标题/正文中包含查询关键词的百分比、tf-idf评分、bm-25评分等。因此，传统搜索系统的最终得分作为训练模型时的数值特征。
- en: In a real-world scenario, Microsoft’s Bing search platform was designed with
    its own *Microsoft Learning to Rank Datasets*, which contain 136 features. Besides,
    they published a contest for learning-to-rank, calling for the use of these datasets
    as a basic training model for predicting the matching degree of web pages. After
    that, a trained model is applied in the production environment of a Bing search,
    which has improved the search effect to a certain extent.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，微软的Bing搜索平台设计了自己的*Microsoft Learning to Rank Datasets*，其中包含136个特征。此外，他们还发布了一场学习排序的竞赛，呼吁使用这些数据集作为预测网页匹配度的基本训练模型。之后，经过训练的模型被应用到Bing搜索的生产环境中，搜索效果得到了某种程度的提升。
- en: At the same time, search companies such as Google, Yahoo, and Baidu have also
    conducted a large amount of research and have partially deployed their research
    results into the production environment.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，谷歌、雅虎和百度等搜索公司也进行了大量的研究，并将其部分研究成果部分投入到生产环境中。
- en: In the fields of enterprise and personal search, Elastic has developed its learning-to-rank
    plugin named ElasticSearch LTR, which can be plug-in into your ES-powered search
    system. As a user, you still need to use a familiar machine learning framework
    to design features, train the learning-to-rank model, evaluate the model performance,
    and select models. Elasticsearch’s support for learning-to-rank can be plugged
    into the existence search system and get a new predicted ranking score based on
    model output. Although machine learning can be used to design models for multi-modal
    data, Elasticsearch places more emphasis on text-to-text search. Figure 1.4 demonstrates
    how learning-to-rank works in a search system.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业和个人搜索领域，Elastic 开发了名为 ElasticSearch LTR 的学习排序插件，可以插入到你的 ES 驱动的搜索系统中。作为用户，你仍然需要使用一个熟悉的机器学习框架来设计特征、训练学习排序模型、评估模型性能和选择模型。Elasticsearch
    对学习排序的支持可以插件化地加入现有的搜索系统，并基于模型输出获得新的预测排名分数。虽然机器学习可以用来为多模态数据设计模型，但 Elasticsearch
    更侧重于文本到文本的搜索。图 1.4 展示了学习排序如何在搜索系统中工作。
- en: '![Figure 1.4 – Learning-to-rank ](img/Figure_1.4_B17488.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 学习排序](img/Figure_1.4_B17488.jpg)'
- en: Figure 1.4 – Learning-to-rank
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 学习排序
- en: This book will focus on search powered by deep neural networks, namely **neural
    information retrieval**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将重点介绍由深度神经网络驱动的搜索，即**神经信息检索**。
- en: The advantage of neural information retrieval is that users do not have to *design*
    features by themselves. Normally, we leverage two independent deep learning models
    (neural nets) as feature extractors to extract vectors from queries and documents
    respectively. Then, we measure the similarity between two vectors using metrics,
    such as cosine similarity. At this stage, neural-network-powered search has become
    very promising for industrial use cases. In the next section, we will introduce
    some of the potential applications for neural-network-powered search.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 神经信息检索的优势在于用户不需要*自行设计*特征。通常，我们利用两个独立的深度学习模型（神经网络）作为特征提取器，分别从查询和文档中提取向量。然后，我们使用如余弦相似度等度量标准来衡量两个向量之间的相似性。在这个阶段，神经网络驱动的搜索已经成为工业应用案例中非常有前景的技术。在下一节中，我们将介绍一些神经网络驱动搜索的潜在应用。
- en: Practical applications powered by neural search
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经搜索驱动的实际应用
- en: The previous section provided an overview of the representation and principles
    of dense vectors. This section will focus on the application of these vectors.
    During our daily work and study, all files will have a unique modality, such as
    a text, image, audio, or video file, and so on. If documents of any modality can
    be represented by dense vectors and then mapped to the same vector space, it is
    possible to compare the cross-modal similarity. This also allows us to use one
    modality to search for data in another modality.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节提供了关于稠密向量的表示和原理的概述。本节将重点介绍这些向量的应用。在我们的日常工作和学习中，所有文件都会有一个独特的模态，例如文本、图像、音频或视频文件等。如果任何模态的文档都可以通过稠密向量表示，并映射到相同的向量空间中，那么就可以比较跨模态的相似性。这也使得我们能够使用一种模态来搜索另一种模态中的数据。
- en: This scenario was first extensively put into practice in the field of e-commerce
    with the common use of *image search*, for example. Its major application in this
    field includes having a product photo and hunting for related or similar products
    offline and online.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这一场景首次在电子商务领域得到广泛应用，举例来说就是常见的*图像搜索*。在该领域的主要应用之一是获取产品照片，然后在网上和线下寻找相关或相似的产品。
- en: 'The e-commerce search primarily consists of steps such as the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 电子商务搜索主要包括以下几个步骤：
- en: '**Preprocessing**'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**'
- en: '**Feature extraction and fusion**'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征提取与融合**'
- en: '**Large-scale similarity search**'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**大规模相似性搜索**'
- en: During preprocessing, techniques such as **resizing**, **normalization**, and
    **semantic** **segmentation** may first be employed to process images. Resizing
    and normalization enable the input image to match the input format of the pre-trained
    neural network. Semantic segmentation has the function of removing background
    noise from the image and leaving only the product itself. Of course, we need to
    pre-train a neural pathway for feature extraction, which will be elaborated on
    shortly. By the same token, if the dataset of an e-commerce product to be retrieved
    has a large amount of noise, such as a large number of buildings, pedestrians,
    and so on in the background of fashion photos, it will be necessary to train a
    semantic segmentation model that can help us accurately extract the product profile
    from photos.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理过程中，可能首先会采用**调整大小**、**归一化**和**语义****分割**等技术来处理图像。调整大小和归一化使输入图像能够与预训练神经网络的输入格式匹配。语义分割则具有去除图像背景噪声，仅保留产品本身的功能。当然，我们需要预训练一个神经网络路径用于特征提取，稍后将详细讲解。同样，如果待检索的电商产品数据集中存在大量噪声，例如时尚照片背景中有大量建筑物、行人等，就需要训练一个语义分割模型，帮助我们准确从照片中提取产品轮廓。
- en: During feature extraction, a **fully connected** (**FC**) layer of deep learning
    is generally used as a feature extractor. The common backbone models of deep learning
    are AlexNet, VGGNet, Inception, and ResNet. These models are usually pre-trained
    on a large-scale dataset (such as the ImageNet dataset) to complete classification
    tasks. Transfer learning is carried out with the dataset in the e-commerce field
    in a bid to make the feature extractor suitable for the field, such as the feature
    extraction of fashion. Currently, a feature extractor with deep learning techniques
    at its core can be regarded as a *global feature extractor*. In some applications,
    traditional computer vision features, such as SIFT or VLAD, are employed for the
    extraction of local features and fusion with global features to enhance vector
    representation. The global feature will transform the preprocessed image into
    a dense vector representation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征提取过程中，通常使用深度学习的**全连接**（**FC**）层作为特征提取器。深度学习的常见骨干网络模型有 AlexNet、VGGNet、Inception
    和 ResNet。这些模型通常会在大规模数据集（如 ImageNet 数据集）上进行预训练，以完成分类任务。随后，通过迁移学习使用电商领域的数据集，以使特征提取器适用于该领域，例如时尚类产品的特征提取。目前，以深度学习技术为核心的特征提取器可以视为*全局特征提取器*。在一些应用中，传统的计算机视觉特征，如
    SIFT 或 VLAD，常用于提取局部特征，并与全局特征融合，以增强向量表示。全局特征将把预处理后的图像转换为密集的向量表示。
- en: When users make a query based on the search for images with images, the keyword
    used for the query is also an image. The system will generate a dense vector representation
    of that image. Then, users will be able to find the most similar image by comparing
    the dense vector of the image to be queried against those of all images in the
    library. This is feasible in theory. However, in reality, with the rapid increase
    in the number of commodities, there may be tens of millions of dense vectors of
    indexed images. As a result, the comparison of vectors in a pair-wise manner will
    fail to meet the user’s requirements for a quick response from the retrieval system.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户基于图像进行查询时，查询所用的关键词也是一张图像。系统将生成该图像的密集向量表示。然后，用户可以通过将待查询图像的密集向量与库中所有图像的向量进行比较，从而找到最相似的图像。理论上这是可行的。然而，实际上，随着商品数量的快速增加，可能会有数千万个已索引图像的密集向量。因此，逐对比较向量将无法满足用户对检索系统快速响应的需求。
- en: Therefore, *large-scale similarity search techniques*, such as product quantization,
    are generally used to divide the vector to be searched into multiple buckets and
    perform a quick match based on the buckets by minimizing the recall rate and greatly
    speeding up the vector-matching process. Therefore, this technique is also commonly
    referred to as *approximate nearest neighbor*, or *ANN retrieval*. Commonly used
    ANN libraries include the FAISS, which is maintained by Facebook, and Annoy, maintained
    by Spotify.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*大规模相似性搜索技术*，如产品量化，通常用于将待搜索的向量划分为多个桶，并通过最小化召回率在桶之间进行快速匹配，从而大大加速向量匹配过程。因此，这项技术通常被称为*近似最近邻*，或*ANN
    检索*。常用的 ANN 库包括由 Facebook 维护的 FAISS 和由 Spotify 维护的 Annoy。
- en: Likewise, the search for images by images in an e-commerce scenario is also
    applicable to other scenarios, such as *Tourism Landmark Retrieval* (using pictures
    of tourist attractions to quickly locate other pictures of that attraction or
    similar tourist attractions), or *Celebrity Retrieval* (used to find photos of
    celebrities and retrieve their pictures). In the field of search engines, there
    are many such applications, which are collectively referred to as *reverse image
    search*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在电子商务场景中通过图像搜索图像，也适用于其他场景，比如*旅游景点检索*（通过旅游景点的图片快速定位该景点或类似旅游景点的其他图片），或*名人检索*（用于寻找名人的照片并检索其图片）。在搜索引擎领域，存在许多此类应用，这些应用统称为*反向图像搜索*。
- en: Another interesting application is **question answering**. Neural-network-based
    search systems could be powerful when building a question-answering (QA) system
    on different tasks. First, the questions and answers that are currently available
    are taken as a training dataset on which to develop a pre-trained model of texts.
    When the user enters a question, the pre-trained model is employed to encode the
    question into a dense vector representation, conduct similarity matching in the
    dense vector representation of the existing repository of answers, and quickly
    help users find the answer to a question. Second, many question-answering systems,
    such as Quora, StackOverflow, and Zhihu, already have a large number of previously
    asked questions. When a user wants to ask a question, the question-answering system
    first determines whether the question has already been asked by someone else.
    If so, the user will be advised to click and check the answers to similar questions
    instead of repeating the query. This also involves similarity match, which is
    normally referred to as *deduplication* or *paraphrase identification*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的应用是**问答系统**。基于神经网络的搜索系统在构建不同任务的问答（QA）系统时可能非常强大。首先，当前可用的问答对作为训练数据集，用于开发文本的预训练模型。当用户输入问题时，预训练模型被用来将问题编码为一个密集的向量表示，在现有的答案库的密集向量表示中进行相似度匹配，快速帮助用户找到问题的答案。其次，许多问答系统，如Quora、StackOverflow和知乎，已经拥有大量先前提出的问题。当用户想要提问时，问答系统首先判断该问题是否已经有人提过。如果是，用户将被建议点击并查看类似问题的答案，而不是重复提问。这也涉及相似度匹配，通常称为*去重*或*释义识别*。
- en: Meanwhile, in the real world, a large number of unexplored applications can
    be completed using neural information retrieval. For instance, if you employ text
    to search for untagged music, it is necessary to map the text and music representation
    to the same vector space. Then, the appearance time of scenarios in the video
    can be located using images. Conversely, when a user is watching a video, a product
    that appears in the video is retrieved and the purchase can be completed. Alternatively,
    deep learning can be carried out for specialized data retrieval, such as source
    code retrieval, DNA sequence retrieval, and more!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，在现实世界中，许多尚未探索的应用可以通过神经信息检索来实现。例如，如果你使用文本搜索未标记的音乐，就需要将文本和音乐的表示映射到相同的向量空间中。然后，视频中场景出现的时间可以通过图像来定位。相反，当用户观看视频时，可以检索视频中出现的产品并完成购买。或者，可以对专业数据进行深度学习检索，比如源代码检索、DNA序列检索等！
- en: New terms learned in this chapter
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章学习的新术语
- en: '**Traditional search**: Mostly applied to text retrieval. Measures the similarity
    by the weighted score of occurrences of a set of tokens from a query and documents.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统搜索**：主要应用于文本检索。通过查询和文档中一组标记出现的加权分数来衡量相似度。'
- en: '**Indexing**: The process of converting files that allow a rapid search and
    skipping the continuous scanning of all files.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：将文件转换为可以快速搜索并跳过连续扫描所有文件的过程。'
- en: '**Searching**: The process of conducting similarity score computation against
    a user query and indexed documents inside the document store and returning the
    top-k matches.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：对用户查询和文档库中索引的文档进行相似度分数计算，并返回前k个匹配结果的过程。'
- en: '**Vector space model**: A way to represent a document numerically. The dimension
    of the VSM is the number of distinct tokens in all documents. The value of each
    dimension is the weight of each term.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量空间模型**：一种以数字形式表示文档的方法。VSM的维度是所有文档中不同标记的数量。每个维度的值是每个术语的权重。'
- en: '**TF-IDF**: Term-Frequency Inverse Document Frequency is an algorithm that
    is intended to reflect how important a word is to a document in a collection of
    documents that are to be indexed.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-IDF**：词频-逆文档频率（TF-IDF）是一种算法，旨在反映某个单词在一组需要被索引的文档中的重要性。'
- en: '**Machine learning**: This refers to a technique that teaches computers to
    make decisions in a way that comes naturally to humans by enabling computers to
    learn the distribution of data and acquire new experience and knowledge.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：这是一种技术，通过使计算机学习数据分布并获得新的经验和知识，教计算机以类似人类的方式做出决策。'
- en: '**Deep neural networks**: A **deep neural network** (**DNN**) is an **artificial
    neural network** (**ANN**) with multiple layers between the input and output layers
    that aims to predict, classify, or learn a compact representation (dense vector)
    of a piece of data.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度神经网络**：**深度神经网络**（**DNN**）是一种具有多个输入层和输出层之间的层次结构的**人工神经网络**（**ANN**），旨在预测、分类或学习数据的紧凑表示（密集向量）。'
- en: '**Neural search**: Unlike symbolic search, neural search makes use of the representation
    (a dense vector) generated by DNNs and measures the similarity between a query
    vector and a document vector, returning the top-k matches based on certain metrics.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经搜索**：与符号搜索不同，神经搜索利用DNN生成的表示（密集向量），并衡量查询向量与文档向量之间的相似度，根据特定的度量标准返回前k个匹配项。'
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you have learned about the key concepts of searching and matching.
    We have also covered the difference between legacy search and neural-network-based
    search. We saw how neural networks can help us tackle the issues traditional search
    cannot solve, such as cross-modality or multi-modality search.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你已经了解了搜索和匹配的关键概念。我们还介绍了传统搜索与基于神经网络的搜索之间的区别。我们看到，神经网络可以帮助我们解决传统搜索无法解决的问题，例如跨模态或多模态搜索。
- en: Neural networks are able to encode different types of information into a common
    embedding space and make different pieces of information comparable, and that’s
    why deep learning and neural networks have the potential to better fulfill a user’s
    information needs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络能够将不同类型的信息编码到一个共同的嵌入空间，并使不同的信息片段具有可比性，这也是深度学习和神经网络有潜力更好地满足用户信息需求的原因。
- en: We have introduced several possible applications using deep-learning-powered
    search systems, for instance, vision-based product search in fashion or tourism,
    or text-based search for question answering and text deduplication. More kinds
    of application are still to be explored!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了几种基于深度学习搜索系统的可能应用，例如，时尚或旅游领域的基于视觉的产品搜索，或基于文本的问答搜索和文本去重。还有更多应用有待探索！
- en: 'You should now understand the core idea behind neural search: neural search
    has the ability to encode any kind of data into an expressive representation,
    namely an **embedding**. Creating a quality embedding is crucial to a search application
    powered by deep learning, since it determines the quality of the final search
    result.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该理解神经搜索背后的核心思想：神经搜索能够将任何类型的数据编码成富有表现力的表示，也就是**嵌入**。创建高质量的嵌入对深度学习驱动的搜索应用至关重要，因为它决定了最终搜索结果的质量。
- en: In the next chapter, we will introduce the foundations of embeddings, such as
    how to encode information into embeddings, how to measure the distance between
    different embeddings, and some of the most important models we can use to encode
    different modalities of data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍嵌入的基础知识，例如如何将信息编码为嵌入，如何衡量不同嵌入之间的距离，以及我们可以用来编码不同数据模态的一些最重要的模型。
