- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Reinforcement Learning Frontiers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习前沿
- en: Congratulations! You have made it to the final chapter. We have come a long
    way. We started off with the fundamentals of reinforcement learning and gradually
    we learned about the state-of-the-art deep reinforcement learning algorithms.
    In this chapter, we will look at some exciting and promising research trends in
    reinforcement learning. We will start the chapter by learning what meta learning
    is and how it differs from other learning paradigms. Then, we will learn about
    one of the most used meta-learning algorithms, called **Model-Agnostic Meta Learning**
    (**MAML**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经来到了最后一章。我们走了很长一段路。我们从强化学习的基础开始，逐渐学习了最先进的深度强化学习算法。在本章中，我们将讨论一些激动人心且前景广阔的强化学习研究趋势。我们将从学习元学习是什么以及它与其他学习范式的区别开始。然后，我们将学习一个最常用的元学习算法——**模型无关元学习**（**MAML**）。
- en: We will understand MAML in detail, and then we will see how to apply it in a
    reinforcement learning setting. Following this, we will learn about hierarchical
    reinforcement learning, and we look into a popular hierarchical reinforcement
    learning algorithm called MAXQ value function decomposition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细了解 MAML，然后看看如何将其应用于强化学习环境。接着，我们将学习层次化强化学习，并深入研究一种流行的层次化强化学习算法——MAXQ 值函数分解。
- en: At the end of the chapter, we will look at an interesting algorithm called **Imagination Augmented
    Agents** (**I2As**), which makes use of both model-based and model-free learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将介绍一个有趣的算法——**想象增强型智能体**（**I2As**），它同时利用基于模型和无模型学习。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将学习以下内容：
- en: Meta reinforcement learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元强化学习
- en: Model-agnostic meta learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型无关元学习
- en: Hierarchical reinforcement learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次化强化学习
- en: MAXQ value function decomposition
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAXQ 值函数分解
- en: Imagination Augmented Agents
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象增强型智能体
- en: Let's begin the chapter by understanding meta reinforcement learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解元强化学习开始本章内容。
- en: Meta reinforcement learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元强化学习
- en: In order to understand how meta reinforcement learning works, first let's understand
    meta learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解元强化学习是如何工作的，首先让我们了解元学习。
- en: Meta learning is one of the most promising and trending research areas in the
    field of artificial intelligence. It is believed to be a stepping stone for attaining
    **Artificial General Intelligence** (**AGI**). What is meta learning? And why
    do we need meta learning? To answer these questions, let's revisit how deep learning
    works.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习是人工智能领域最有前景和最热门的研究方向之一。人们相信它是实现**人工通用智能**（**AGI**）的垫脚石。什么是元学习？我们为什么需要元学习？为了回答这些问题，让我们回顾一下深度学习是如何工作的。
- en: We know that in deep learning, we train a deep neural network to perform a task.
    But the problem with deep neural networks is that we need to have a large training
    dataset to train our network, as it will fail to learn when we have only a few
    data points.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在深度学习中，我们训练一个深度神经网络来执行某个任务。但深度神经网络的问题在于，我们需要一个大量的训练数据集来训练我们的网络，因为当数据点较少时，它无法学习。
- en: Let's say we trained a deep learning model to perform task **A**. Suppose we
    have a new task **B**, which is closely related to task **A**. Although task **B**
    is closely related to task **A**, we can't use the model we trained for task **A**
    to perform task **B**. We need to train a new model from scratch for task **B**.
    So, for each task, we need to train a new model from scratch although they might
    be related. But is this really true AI? Not really. How do we humans learn? We
    generalize our learning to multiple concepts and learn from there. But current
    learning algorithms master only one task. So, here is where meta learning comes
    in.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们训练了一个深度学习模型来执行任务**A**。现在假设有一个新的任务**B**，它与任务**A**密切相关。尽管任务**B**与任务**A**密切相关，我们不能直接用为任务**A**训练的模型来执行任务**B**。我们需要从头开始为任务**B**训练一个新模型。因此，对于每个任务，我们都需要从头开始训练一个新模型，尽管它们可能是相关的。但这真的算是人工智能吗？其实不算。那么，我们人类是如何学习的呢？我们将学习概括到多个概念中，从中学习。然而，当前的学习算法只能掌握一个任务。所以，这就是元学习发挥作用的地方。
- en: Meta learning produces a versatile AI model that can learn to perform various
    tasks without having to be trained from scratch. We train our meta-learning model
    on various related tasks with few data points, so for a new related task, it can
    make use of the learning achieved in previous tasks. Many researchers and scientists
    believe that meta learning can get us closer to achieving AGI. Learning to learn
    is the key focus of meta learning. We will understand how exactly meta learning
    works by looking at a popular meta learning algorithm called MAML in the next
    section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习生成了一个多功能的AI模型，它可以学习执行各种任务，而无需从头开始训练。我们在各种相关任务上训练我们的元学习模型，并且使用很少的数据点，因此对于一个新的相关任务，它可以利用在之前任务中获得的学习成果。许多研究人员和科学家认为，元学习可以让我们更接近实现通用人工智能（AGI）。学习如何学习是元学习的核心焦点。我们将在下一部分通过了解一种流行的元学习算法——MAML，来深入理解元学习到底是如何工作的。
- en: Model-agnostic meta learning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型无关元学习
- en: '**Model-Agnostic Meta Learning** (**MAML**) is one of the most popular meta-learning
    algorithms and it has been a major breakthrough in meta-learning research. The
    basic idea of MAML is to find a better initial model parameter so that with a good
    initial parameter, a model can learn quickly on new tasks with fewer gradient steps.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型无关元学习**（**MAML**）是最受欢迎的元学习算法之一，它在元学习研究中取得了重大突破。MAML的基本思想是找到更好的初始模型参数，这样通过一个好的初始参数，模型可以在新的任务上通过较少的梯度步骤快速学习。'
- en: So, what do we mean by that? Let's say we are performing a classification task
    using a neural network. How do we train the network? We start off by initializing
    random weights and train the network by minimizing the loss. How do we minimize
    the loss? We minimize the loss using gradient descent. Okay, but how do we use
    gradient descent to minimize the loss? We use gradient descent to find the optimal
    weights that will give us the minimal loss. We take multiple gradient steps to
    find the optimal weights so that we can reach convergence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这意味着什么呢？假设我们正在执行一个使用神经网络的分类任务。我们如何训练网络？我们首先初始化随机权重，并通过最小化损失来训练网络。我们如何最小化损失？我们通过梯度下降最小化损失。好吧，如何使用梯度下降来最小化损失呢？我们使用梯度下降来找到能够给我们最小损失的最优权重。我们进行多次梯度步骤，以找到最优权重，从而达到收敛。
- en: In MAML, we try to find these optimal weights by learning from the distribution
    of similar tasks. So, for a new task, we don't have to start with randomly initialized
    weights; instead, we can start with optimal weights, which will take fewer gradient
    steps to reach convergence and doesn't require more data points for training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在MAML中，我们通过从相似任务的分布中学习来找到这些最优权重。因此，对于一个新任务，我们不必从随机初始化的权重开始；相反，我们可以从最优权重开始，这样只需较少的梯度步骤就能达到收敛，并且不需要更多的数据点来进行训练。
- en: 'Let''s understand how MAML works in simple terms. Let''s suppose we have three
    related tasks: *T*[1], *T*[2], and *T*[3].'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用简单的术语来理解MAML是如何工作的。假设我们有三个相关的任务：*T*[1]、*T*[2] 和 *T*[3]。
- en: First, we randomly initialize our model parameter (weight), ![](img/B15558_09_118.png).
    We train our network on task *T*[1]. Then, we try to minimize the loss *L* by
    gradient descent. We minimize the loss by finding the optimal parameter. Let ![](img/B15558_17_002.png)
    be the optimal parameter for the task *T*[1]. Similarly, for tasks *T*[2] and
    *T*[3], we will start off with a randomly initialized model parameter ![](img/B15558_09_118.png)
    and minimize the loss by finding the optimal parameters by gradient descent. Let
    ![](img/B15558_12_208.png) and ![](img/B15558_17_005.png) be the optimal parameters
    for tasks *T*[2] and *T*[3], respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们随机初始化我们的模型参数（权重），![](img/B15558_09_118.png)。我们在任务 *T*[1] 上训练我们的网络。然后，我们通过梯度下降来最小化损失
    *L*。我们通过找到最优参数来最小化损失。令 ![](img/B15558_17_002.png) 为任务 *T*[1] 的最优参数。类似地，对于任务 *T*[2]
    和 *T*[3]，我们将从随机初始化的模型参数 ![](img/B15558_09_118.png) 开始，并通过梯度下降找到最优参数来最小化损失。令 ![](img/B15558_12_208.png)
    和 ![](img/B15558_17_005.png) 分别为任务 *T*[2] 和 *T*[3] 的最优参数。
- en: 'As we can see in the following figure, we start off each task with the randomly
    initialized parameter ![](img/B15558_09_054.png) and minimize the loss by finding
    the optimal parameters ![](img/B15558_17_007.png), ![](img/B15558_17_008.png),
    and ![](img/B15558_17_009.png) for the tasks *T*[1], *T*[2], and *T*[3] respectively:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们从随机初始化的参数 ![](img/B15558_09_054.png) 开始每个任务，并通过找到任务 *T*[1]、*T*[2] 和
    *T*[3] 的最优参数 ![](img/B15558_17_007.png)、![](img/B15558_17_008.png) 和 ![](img/B15558_17_009.png)
    来最小化损失：
- en: '![](img/B15558_17_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_01.png)'
- en: 'Figure 17.1: ![](img/B15558_17_010.png) is initialized at a random position'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1： ![](img/B15558_17_010.png) 被初始化在一个随机位置
- en: However, instead of initializing ![](img/B15558_09_098.png) in a random position,
    that is, with random values, if we initialize ![](img/B15558_09_123.png) in a
    position that is common to all three tasks, then we don't need to take many gradient
    steps and it will take us less time to train. MAML tries to do exactly this. MAML
    tries to find this optimal parameter ![](img/B15558_09_087.png) that is common
    to many of the related tasks, so we can train a new task relatively quick with
    few data points without having to take many gradient steps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不同于在随机位置初始化 ![](img/B15558_09_098.png)，即使用随机值，如果我们在一个所有任务共同的位置初始化 ![](img/B15558_09_123.png)，那么我们就不需要进行太多的梯度步骤，训练所需的时间会更少。MAML
    就是为了实现这一目标。MAML 尝试找到这个最优参数 ![](img/B15558_09_087.png)，它对多个相关任务都适用，这样我们就能以更少的数据点更快速地训练新任务，而不需要进行过多的梯度步骤。
- en: 'As *Figure 17.2* shows, we shift ![](img/B15558_09_054.png) to a position that
    is common to all different optimal ![](img/B15558_09_107.png) values:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 17.2* 所示，我们将 ![](img/B15558_09_054.png) 调整到一个所有不同最优 ![](img/B15558_09_107.png)
    值共同的位置：
- en: '![](img/B15558_17_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_02.png)'
- en: 'Figure 17.2: ![](img/B15558_17_016.png) is initialized at the optimal position'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2： ![](img/B15558_17_016.png) 被初始化在最优位置
- en: So, for a new related task, say, *T*[4], we don't have to start with a randomly
    initialized parameter, ![](img/B15558_09_087.png). Instead, we can start with
    the optimal ![](img/B15558_09_123.png) value (shifted ![](img/B15558_10_037.png))
    so that it will take fewer gradient steps to attain convergence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于一个新的相关任务，假设是 *T*[4]，我们不必从一个随机初始化的参数开始，![](img/B15558_09_087.png)。相反，我们可以从最优的
    ![](img/B15558_09_123.png) 值（已偏移的 ![](img/B15558_10_037.png)）开始，这样就能减少梯度下降的步数，加快收敛。
- en: Thus, in MAML, we try to find this optimal ![](img/B15558_09_098.png) value
    that is common to related tasks to help us learn from fewer data points and minimize
    our training time. MAML is model-agnostic, meaning that we can apply MAML to any
    models that are trainable with gradient descent. But how exactly does MAML work?
    How do we shift the model parameters to an optimal position? Now that we have
    a basic understanding of MAML, we will address all these questions in the next
    section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 MAML 中，我们试图找到这个对相关任务通用的最优 ![](img/B15558_09_098.png) 值，帮助我们通过更少的数据点进行学习，并减少训练时间。MAML
    是与模型无关的，这意味着我们可以将 MAML 应用到任何可以通过梯度下降训练的模型上。那么，MAML 究竟是如何工作的呢？我们如何将模型参数调整到最优位置？现在我们对
    MAML 有了基本的了解，接下来我们将解决这些问题。
- en: Understanding MAML
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 MAML
- en: 'Suppose we have a model *f* parameterized by ![](img/B15558_09_118.png), that
    is, ![](img/B15558_17_022.png), and we have a distribution over tasks, *p(T)*.
    First, we initialize our parameter ![](img/B15558_09_056.png) with some random
    values. Next, we sample a batch of tasks *T*[i] from a distribution over tasks―that
    is, *T*[i]* ~ p(T)*. Let''s say we have sampled five tasks: *T*[1], *T*[2], *T*[3],
    *T*[4], *T*[5].'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由 ![](img/B15558_09_118.png) 参数化的模型 *f*，即 ![](img/B15558_17_022.png)，并且我们有一个任务分布
    *p(T)*。首先，我们用一些随机值初始化我们的参数 ![](img/B15558_09_056.png)。接下来，我们从任务分布中抽取一批任务 *T*[i]―即
    *T*[i]* ~ p(T)*。假设我们抽取了五个任务：*T*[1]、*T*[2]、*T*[3]、*T*[4]、*T*[5]。
- en: 'Now, for each task *T*[i], we sample *k* number of data points and train the
    model *f* parameterized by ![](img/B15558_09_098.png), that is, ![](img/B15558_17_025.png).
    We train the model by computing the loss ![](img/B15558_17_026.png) and we minimize
    the loss using gradient descent and find the optimal parameter ![](img/B15558_17_027.png).
    The parameter update rule using gradient descent is given as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务 *T*[i]，我们抽取 *k* 个数据点并训练由 ![](img/B15558_09_098.png) 参数化的模型 *f*，即 ![](img/B15558_17_025.png)。我们通过计算损失
    ![](img/B15558_17_026.png) 来训练模型，并使用梯度下降最小化损失，找到最优参数 ![](img/B15558_17_027.png)。使用梯度下降的参数更新规则如下：
- en: '![](img/B15558_17_028.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_028.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，以下内容适用：
- en: '![](img/B15558_17_027.png) is the optimal parameter for a task *T*[i]'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_17_027.png) 是任务 *T*[i] 的最优参数'
- en: '![](img/B15558_17_030.png) is the initial parameter'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_17_030.png) 是初始参数'
- en: '![](img/B15558_07_025.png) is the learning rate'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_07_025.png) 是学习率'
- en: '![](img/B15558_17_032.png) is the gradient of loss for a task *T*[i] with the
    model parameterized as ![](img/B15558_17_033.png)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_17_032.png) 是任务 *T*[i] 的损失梯度，模型参数化为 ![](img/B15558_17_033.png)'
- en: So, after the preceding parameter update using gradient descent, we will have
    optimal parameters for all five tasks that we have sampled. That is, for the tasks
    *T*[1], *T*[2], *T*[3], *T*[4], *T*[5], we will have the optimal parameters ![](img/B15558_17_034.png),
    respectively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在使用梯度下降进行前述参数更新后，我们将获得所有五个任务的最优参数。也就是说，对于任务 *T*[1]、*T*[2]、*T*[3]、*T*[4]、*T*[5]，我们将分别得到最优参数
    ![](img/B15558_17_034.png)。
- en: Now, before the next iteration, we perform a meta update or meta optimization.
    That is, in the previous step, we found the optimal parameter ![](img/B15558_17_035.png)
    by training on each of the tasks, *T*[i]. Now we take some new set of tasks and
    for each of these new tasks *T*[i], we don't have to start from the random position
    ![](img/B15558_09_098.png); instead, we can start from the optimal position ![](img/B15558_17_037.png)
    to train the model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在下一次迭代之前，我们进行一次元更新或元优化。也就是说，在前一步中，我们通过在每个任务 *T*[i] 上训练，找到了最优参数 ![](img/B15558_17_035.png)。现在我们拿到一组新的任务，对于这些新任务
    *T*[i]，我们不必从随机位置 ![](img/B15558_09_098.png) 开始；相反，我们可以从最优位置 ![](img/B15558_17_037.png)
    开始训练模型。
- en: 'That is, for each of the new tasks *T*[i], instead of using the randomly initialized
    parameter ![](img/B15558_09_098.png), we use the optimal parameter ![](img/B15558_17_039.png).
    This implies that we train the model *f* parameterized by ![](img/B15558_17_040.png),
    that is, ![](img/B15558_17_041.png) instead of using ![](img/B15558_17_025.png).
    Then, we calculate the loss ![](img/B15558_17_043.png), compute the gradients,
    and update the parameter ![](img/B15558_09_054.png). This makes our randomly initialized
    parameter ![](img/B15558_09_087.png) move to an optimal position where we don''t
    have to take many gradient steps. This step is called a meta update, meta optimization,
    or meta training. It can be expressed as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，对于每个新任务 *T*[i]，我们并不是使用随机初始化的参数 ![](img/B15558_09_098.png)，而是使用最优参数 ![](img/B15558_17_039.png)。这意味着我们训练的是由
    ![](img/B15558_17_040.png) 参数化的模型 *f*，也就是 ![](img/B15558_17_041.png)，而不是使用 ![](img/B15558_17_025.png)。然后，我们计算损失
    ![](img/B15558_17_043.png)，计算梯度，并更新参数 ![](img/B15558_09_054.png)。这使得我们随机初始化的参数
    ![](img/B15558_09_087.png) 移动到一个最优位置，避免了需要进行多次梯度更新。这一步称为元更新、元优化或元训练。可以表示为：
- en: '![](img/B15558_17_046.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_046.png)'
- en: 'In equation (2), the following applies:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 (2) 中，以下公式适用：
- en: '![](img/B15558_14_004.png) is the initial parameter'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_14_004.png) 是初始参数'
- en: '![](img/B15558_09_152.png) is the learning rate'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_09_152.png) 是学习率'
- en: '![](img/B15558_17_049.png) is the gradient of loss for each of the new tasks
    *T*[i], with the model parameterized as ![](img/B15558_17_041.png)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_17_049.png) 是每个新任务 *T*[i] 的损失梯度，模型的参数化为 ![](img/B15558_17_041.png)'
- en: If you look at our previous meta update equation (2) closely, we can see that
    we are updating our model parameter ![](img/B15558_09_118.png) by merely taking
    an average of gradients of each new task *T*[i] with the model *f* parameterized
    by ![](img/B15558_17_037.png).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察我们之前的元更新方程 (2)，我们可以看到，我们通过仅仅对每个新任务 *T*[i] 的梯度取平均，来更新我们的模型参数 ![](img/B15558_09_118.png)，而这个模型
    *f* 是由 ![](img/B15558_17_037.png) 参数化的。
- en: 'Figure 17.3 helps us to understand the MAML algorithm better. As we can observe,
    our MAML algorithm has two loops—an **inner loop** where we find the optimal parameter
    ![](img/B15558_17_053.png) for each of the tasks *T*[i] using the model *f* parameterized
    by initial parameter ![](img/B15558_09_054.png), that is, ![](img/B15558_17_055.png),
    and an **outer loop** where we use the model *f* parameterized by the optimal
    parameter ![](img/B15558_17_056.png) obtained in the previous step, that is ![](img/B15558_17_057.png),
    and train the model on the new set of tasks, calculate the loss, compute the gradient
    of the loss, and update the randomly initialized model parameter ![](img/B15558_09_098.png):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3 有助于我们更好地理解 MAML 算法。正如我们所观察到的，MAML 算法有两个循环——**内部循环**，我们在这个循环中使用初始参数 ![](img/B15558_09_054.png)，也就是
    ![](img/B15558_17_055.png)，通过在每个任务 *T*[i] 上训练来找出最优参数 ![](img/B15558_17_053.png)；**外部循环**，我们在这个循环中使用在前一步获得的最优参数
    ![](img/B15558_17_056.png)，也就是 ![](img/B15558_17_057.png)，并在新的一组任务上训练模型，计算损失，计算损失的梯度，并更新随机初始化的模型参数
    ![](img/B15558_09_098.png)：
- en: '![](img/B15558_17_03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_03.png)'
- en: 'Figure 17.3: The MAML algorithm'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3：MAML 算法
- en: Note that we should not use the same set of tasks we used to find the optimal
    parameter ![](img/B15558_17_059.png) when updating the model parameter ![](img/B15558_09_098.png)
    in the outer loop.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在更新模型参数 ![](img/B15558_09_098.png) 时，我们不应该使用与找到最优参数时相同的一组任务 ![](img/B15558_17_059.png)，这些任务是在外循环中使用的。
- en: In a nutshell, in MAML, we sample a batch of tasks and for each task *T*[i]
    in the batch, we minimize the loss using gradient descent and get the optimal
    parameter ![](img/B15558_17_027.png). Then, we update our randomly initialized
    model parameter ![](img/B15558_09_054.png) by calculating gradients for each new
    task *T*[i] with the model parameterized as ![](img/B15558_17_041.png).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在 MAML 中，我们采样一批任务，对于批次中的每个任务 *T*[i]，我们使用梯度下降最小化损失并得到最优参数 ![](img/B15558_17_027.png)。然后，我们通过为每个新任务
    *T*[i] 计算梯度，并以 ![](img/B15558_17_041.png) 为参数化模型，来更新我们随机初始化的模型参数 ![](img/B15558_09_054.png)。
- en: Still not clear how exactly MAML works? Worry not! Let's look in even more detail
    at the steps and understand how MAML works in a supervised learning setting in
    the next section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然不清楚 MAML 到底是如何工作的？不用担心！接下来，我们将更详细地查看步骤，理解 MAML 在监督学习中的运作方式。
- en: MAML in a supervised learning setting
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习中的 MAML
- en: As we learned, MAML is model-agnostic and so we can apply MAML to any model
    that can be trained with gradient descent. In this section, let's learn how to
    apply the MAML algorithm in a supervised learning setting. Before going ahead,
    let's define our loss function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学，MAML 是与模型无关的，因此我们可以将 MAML 应用于任何可以通过梯度下降训练的模型。在这一节中，让我们学习如何在监督学习环境中应用
    MAML 算法。在继续之前，我们先定义我们的损失函数。
- en: 'If we are performing regression, then we can use mean squared error as our
    loss function:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进行回归任务，那么可以使用均方误差作为我们的损失函数：
- en: '![](img/B15558_17_064.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_064.png)'
- en: 'If we are performing classification, then we can use cross-entropy loss as
    our loss function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进行分类任务，那么可以使用交叉熵损失作为我们的损失函数：
- en: '![](img/B15558_17_065.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_065.png)'
- en: Now let's see step by step how exactly MAML is used in supervised learning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们一步步地看看 MAML 是如何在监督学习中使用的。
- en: Let's say we have a model *f* parameterized by a parameter ![](img/B15558_17_066.png),
    and we have a distribution over tasks *p(T)*. First, we randomly initialize the
    model parameter ![](img/B15558_09_008.png).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由参数 ![](img/B15558_17_066.png) 参数化的模型 *f*，并且我们有一个任务分布 *p(T)*。首先，我们随机初始化模型参数
    ![](img/B15558_09_008.png)。
- en: Next, we sample a batch of tasks *T*[i] from a distribution of tasks, that is,
    *T*[i] *~ p(T)*. Let's say we have sampled three tasks; then, we have *T*[1],
    *T*[2], *T*[3].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从任务分布中采样一批任务 *T*[i]，即 *T*[i] *~ p(T)*。假设我们已经采样了三个任务，那么我们将得到 *T*[1]、*T*[2]、*T*[3]。
- en: '**Inner loop:** For each task *T*[i], we sample *k* data points and prepare
    our training and test datasets:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**内循环：** 对于每个任务 *T*[i]，我们采样 *k* 个数据点，并准备训练和测试数据集：'
- en: '![](img/B15558_17_068.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_068.png)'
- en: Wait! What are the training and test datasets? We use the training dataset in
    the inner loop for finding the optimal parameter ![](img/B15558_17_069.png) and
    the test set in the outer loop for finding the optimal parameter ![](img/B15558_09_087.png).
    The test dataset does not mean that we are checking the model's performance. It
    basically acts as a training set in the outer loop. We can also call our test
    set a meta-training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！训练数据集和测试数据集是什么？我们在内循环中使用训练数据集来寻找最优参数 ![](img/B15558_17_069.png)，而在外循环中使用测试集来寻找最优参数
    ![](img/B15558_09_087.png)。测试数据集并不意味着我们在检查模型的性能，它基本上在外循环中作为训练集使用。我们也可以将测试集称为元训练集。
- en: Now, we train the model ![](img/B15558_17_055.png) on the training dataset ![](img/B15558_17_072.png),
    calculate the loss, minimize the loss using gradient descent, and get the optimal
    parameter ![](img/B15558_17_059.png) as ![](img/B15558_17_074.png).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在训练数据集 ![](img/B15558_17_072.png) 上训练模型 ![](img/B15558_17_055.png)，计算损失，使用梯度下降最小化损失，并得到最优参数
    ![](img/B15558_17_059.png)，其值为 ![](img/B15558_17_074.png)。
- en: That is, for each of the tasks *T*[i], we sample *k* data points and prepare
    ![](img/B15558_17_075.png) and ![](img/B15558_17_076.png). Next, we minimize the
    loss on the training dataset ![](img/B15558_17_077.png) and get the optimal parameter
    ![](img/B15558_17_078.png). As we sampled three tasks, we will have three optimal
    parameters, ![](img/B15558_17_079.png).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，对于每个任务 *T*[i]，我们采样 *k* 个数据点，并准备 ![](img/B15558_17_075.png) 和 ![](img/B15558_17_076.png)。接下来，我们在训练数据集
    ![](img/B15558_17_077.png) 上最小化损失并得到最优参数 ![](img/B15558_17_078.png)。由于我们采样了三个任务，所以我们将得到三个最优参数，![](img/B15558_17_079.png)。
- en: '**Outer loop**: Now, we perform meta optimization on the test set (meta-training
    set); that is, we try to minimize the loss in the test set ![](img/B15558_17_080.png).
    Here, we parameterize our model *f* by the optimal parameter ![](img/B15558_17_069.png)
    calculated in the previous step. So, we compute the loss of the model ![](img/B15558_17_082.png)
    and the gradients of the loss and update our randomly initialized parameter ![](img/B15558_09_087.png)
    using our test dataset (meta-training dataset) as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**外部循环**：现在，我们对测试集（元训练集）进行元优化；也就是说，我们尝试最小化测试集上的损失![](img/B15558_17_080.png)。在这里，我们使用之前步骤中计算出的最优参数![](img/B15558_17_069.png)对我们的模型*f*进行参数化。所以，我们计算模型的损失![](img/B15558_17_082.png)和损失的梯度，并使用我们的测试数据集（元训练数据集）更新我们随机初始化的参数![](img/B15558_09_087.png)，公式如下:'
- en: '![](img/B15558_17_084.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_084.png)'
- en: We repeat the preceding steps for several iterations to find the optimal parameter.
    For a clear understanding of how MAML works in supervised learning, let's look
    into the algorithm in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复前面的步骤进行多次迭代，以找到最优参数。为了更清楚地理解MAML如何在监督学习中工作，接下来我们将深入研究该算法。
- en: Algorithm – MAML in supervised learning
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法——MAML在监督学习中的应用
- en: 'The algorithm of MAML in a supervised learning setting is given as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MAML在监督学习中的算法如下所示：
- en: Say that we have a model *f* parameterized by a parameter ![](img/B15558_09_056.png)
    and we have a distribution over tasks *p(T)*. First, we randomly initialize the
    model parameter ![](img/B15558_09_054.png).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个由参数![](img/B15558_09_056.png) 参数化的模型*f*，并且我们有一个关于任务的分布*p(T)*。首先，我们随机初始化模型参数![](img/B15558_09_054.png)。
- en: Sample a batch of tasks *T*[i] from a distribution of tasks, that is, *T*[i]
    *~ p(T)*.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从任务分布中采样一批任务*T*[i]，即*T*[i] *~ p(T)*。
- en: 'For each task *T*[i]:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个任务*T*[i]：
- en: Sample *k* data points and prepare our training and test datasets:![](img/B15558_17_068.png)
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样*k*个数据点，并准备我们的训练和测试数据集:![](img/B15558_17_068.png)
- en: Train the model ![](img/B15558_17_088.png) on the training dataset ![](img/B15558_17_089.png)
    and compute the loss.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集![](img/B15558_17_089.png)上训练模型![](img/B15558_17_088.png)，并计算损失。
- en: Minimize the loss using gradient descent and get the optimal parameter ![](img/B15558_17_053.png)
    as ![](img/B15558_17_091.png).
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降最小化损失，并获得最优参数![](img/B15558_17_053.png)，其公式为![](img/B15558_17_091.png)。
- en: Now, minimize the loss on the test set ![](img/B15558_17_092.png). Parameterize
    the model *f* with the optimal parameter ![](img/B15558_17_069.png) calculated
    in the previous step, compute loss ![](img/B15558_17_094.png). Calculate gradients
    of the loss and update our randomly initialized parameter ![](img/B15558_17_066.png)
    using our test (meta-training) dataset as:![](img/B15558_17_084.png).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，最小化测试集上的损失![](img/B15558_17_092.png)。使用之前步骤中计算出的最优参数![](img/B15558_17_069.png)对模型*f*进行参数化，计算损失![](img/B15558_17_094.png)。计算损失的梯度，并使用我们的测试（元训练）数据集更新我们随机初始化的参数![](img/B15558_17_066.png)，公式如下:![](img/B15558_17_084.png)。
- en: Repeat *steps 2* to *4* for several iterations.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行*步骤 2*到*步骤 4*，进行多次迭代。
- en: 'The following figure gives us an overview of how the MAML algorithm works in
    supervised learning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了MAML算法在监督学习中的工作原理：
- en: '![](img/B15558_17_04.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_04.png)'
- en: 'Figure 17.4: Overview of MAML'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4：MAML概览
- en: Now that we have learned how to use MAML in a supervised learning setting, in
    the next section, we will see how to use MAML in a reinforcement learning setting.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在监督学习中使用MAML，在接下来的部分中，我们将探讨如何在强化学习中使用MAML。
- en: MAML in a reinforcement learning setting
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MAML在强化学习中的应用
- en: Now, let's learn how to apply the MAML algorithm in a reinforcement learning
    setting. We know that the objective of reinforcement learning is to find the optimal
    policy, that is, the policy that gives the maximum return. We've learned about
    several reinforcement learning algorithms for finding the optimal policy, and
    we've also learned about several deep reinforcement learning algorithms for finding
    the optimal policy, where we used the neural network parameterized by ![](img/B15558_10_095.png).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解如何在强化学习环境中应用MAML算法。我们知道强化学习的目标是找到最优策略，也就是能够提供最大回报的策略。我们已经了解了几种用于寻找最优策略的强化学习算法，并且也了解了几种深度强化学习算法，在这些算法中，我们使用了由![](img/B15558_10_095.png)参数化的神经网络。
- en: We can apply MAML to any algorithm that can be trained with gradient descent.
    For instance, let's take the policy gradient method. In the policy gradient method,
    we use a neural network parameterized by ![](img/B15558_09_106.png) to find the
    optimal policy and we train our network using gradient descent. So, we can apply
    the MAML algorithm to the policy gradient method.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将MAML应用于任何可以通过梯度下降进行训练的算法。例如，假设我们使用策略梯度方法。在策略梯度方法中，我们使用由![](img/B15558_09_106.png)参数化的神经网络来找到最优策略，并使用梯度下降来训练我们的网络。因此，我们可以将MAML算法应用于策略梯度方法。
- en: Let's understand how MAML works in reinforcement learning step by step.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步理解MAML如何在强化学习中工作。
- en: Let's say we have a model (policy network) *f* parameterized by a parameter
    ![](img/B15558_09_118.png). The model (policy network) *f* tries to find the optimal
    policy by learning the optimal parameter ![](img/B15558_09_087.png). Suppose,
    we have a distribution over tasks *p(T)*. First, we randomly initialize the model
    parameter ![](img/B15558_09_054.png).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由参数![](img/B15558_09_118.png)参数化的模型（策略网络）*f*。该模型（策略网络）*f*试图通过学习最优参数![](img/B15558_09_087.png)来寻找最优策略。假设我们有一个任务分布*p(T)*。首先，我们随机初始化模型参数![](img/B15558_09_054.png)。
- en: Next, we sample a batch of tasks *T*[i] from a distribution of tasks, that is,
    *T*[i] *~ p(T)*. Let's say we have sampled three tasks; then, we have *T*[1],
    *T*[2], *T*[3].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从任务分布中抽样一批任务*T*[i]，即*T*[i] *~ p(T)*。假设我们抽样了三个任务，那么我们有*T*[1]，*T*[2]，*T*[3]。
- en: '**Inner loop**: For each task *T*[i], we prepare our train dataset ![](img/B15558_17_089.png).
    Okay, how can we create the training dataset in a reinforcement learning setting?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**内部循环**：对于每个任务*T*[i]，我们准备我们的训练数据集![](img/B15558_17_089.png)。好吧，我们如何在强化学习设置中创建训练数据集呢？'
- en: 'We have the model (policy network) ![](img/B15558_17_025.png). So, we generate
    *k* number of trajectories using our model ![](img/B15558_17_025.png). We know
    that the trajectories consist of a sequence of state-action pairs. So, we have:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个模型（策略网络）![](img/B15558_17_025.png)。因此，我们使用我们的模型![](img/B15558_17_025.png)生成*k*条轨迹。我们知道这些轨迹由一系列状态-动作对组成。所以，我们有：
- en: '![](img/B15558_17_105.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_105.png)'
- en: Now, we compute the loss and minimize it using gradient descent and get the
    optimal parameter ![](img/B15558_17_053.png) as ![](img/B15558_17_074.png).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算损失并通过梯度下降最小化它，得到最优参数![](img/B15558_17_053.png)，即![](img/B15558_17_074.png)。
- en: That is, for each of the tasks *T*[i], we sample *k* trajectories and prepare
    the training dataset ![](img/B15558_17_108.png). Next, we minimize the loss on
    the training dataset ![](img/B15558_17_109.png) and get the optimal parameter
    ![](img/B15558_17_053.png). As we sampled three tasks, we will have three optimal
    parameters, ![](img/B15558_17_111.png).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，对于每个任务*T*[i]，我们抽取*k*条轨迹并准备训练数据集![](img/B15558_17_108.png)。接下来，我们最小化训练数据集![](img/B15558_17_109.png)上的损失并得到最优参数![](img/B15558_17_053.png)。由于我们抽样了三个任务，因此我们将得到三个最优参数，![](img/B15558_17_111.png)。
- en: 'We also need the test dataset ![](img/B15558_17_112.png), which we use in the
    outer loop. How do we prepare our test dataset? Now, we use our model *f* parameterized
    by the optimal parameter ![](img/B15558_17_056.png); that is, we use ![](img/B15558_17_057.png)
    and generate *k* number of trajectories. So, we have:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要测试数据集![](img/B15558_17_112.png)，它将在外部循环中使用。我们如何准备我们的测试数据集？现在，我们使用由最优参数![](img/B15558_17_056.png)参数化的模型*f*；也就是说，我们使用![](img/B15558_17_057.png)并生成*k*条轨迹。所以，我们有：
- en: '![](img/B15558_17_115.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_115.png)'
- en: Remember that ![](img/B15558_17_116.png) is created by ![](img/B15558_17_117.png)
    and the test (meta-training) dataset ![](img/B15558_17_118.png) is created by
    ![](img/B15558_17_119.png).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，![](img/B15558_17_116.png)是由![](img/B15558_17_117.png)创建的，测试（元训练）数据集![](img/B15558_17_118.png)是由![](img/B15558_17_119.png)创建的。
- en: '**Outer loop**: Now, we perform meta optimization on the test (meta-training)
    dataset; that is, we try to minimize the loss in the test dataset ![](img/B15558_17_120.png).
    Here, we parameterize our model *f* by the optimal parameter ![](img/B15558_17_053.png)
    calculated in the previous step. So, we compute the loss of the model ![](img/B15558_17_043.png)
    and the gradients of the loss and update our randomly initialized parameter ![](img/B15558_09_098.png)
    using our test (meta-training) dataset as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**外部循环**：现在，我们在测试（元训练）数据集上执行元优化；也就是说，我们尝试最小化测试数据集![](img/B15558_17_120.png)中的损失。在这里，我们通过在前一步计算的最优参数![](img/B15558_17_053.png)参数化我们的模型*f*。因此，我们计算模型![](img/B15558_17_043.png)的损失及其梯度，并使用测试（元训练）数据集来更新我们随机初始化的参数![](img/B15558_09_098.png)，公式如下：'
- en: '![](img/B15558_17_084.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_084.png)'
- en: We repeat the preceding step for several iterations to find the optimal parameter.
    For a clear understanding of how MAML works in reinforcement learning, let's look
    into the algorithm in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复前面的步骤若干次，以找到最优参数。为了清楚理解MAML在强化学习中的工作原理，接下来我们将研究算法的细节。
- en: Algorithm – MAML in reinforcement learning
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法——强化学习中的MAML
- en: 'The algorithm of MAML in a reinforcement learning setting is given as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的MAML算法如下所示：
- en: Say, we have a model *f* parameterized by a parameter ![](img/B15558_10_066.png)
    and we have a distribution over tasks *p(T)*. First, we randomly initialize the
    model parameter ![](img/B15558_09_087.png).
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个由参数![](img/B15558_10_066.png)参数化的模型*f*，并且我们有一个任务分布*p(T)*。首先，我们随机初始化模型参数![](img/B15558_09_087.png)。
- en: Sample a batch of tasks *T*[i] from a distribution of tasks, that is, *T*[i]
    *~ p(T).*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从任务分布中采样一批任务*T*[i]，即*T*[i] *~ p(T)。
- en: 'For each task *T*[i]:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个任务 *T*[i]：
- en: 'Sample *k* trajectories using ![](img/B15558_17_025.png) and prepare the training
    dataset: ![](img/B15558_17_128.png).'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用![](img/B15558_17_025.png)采样*k*轨迹并准备训练数据集：![](img/B15558_17_128.png)。
- en: Train the model ![](img/B15558_17_129.png) on the training dataset ![](img/B15558_17_130.png)
    and compute the loss.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集![](img/B15558_17_130.png)上训练模型![](img/B15558_17_129.png)，并计算损失。
- en: Minimize the loss using gradient descent and get the optimal parameter ![](img/B15558_17_131.png)
    as ![](img/B15558_17_132.png).
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降法最小化损失，并得到最优参数![](img/B15558_17_131.png)，即![](img/B15558_17_132.png)。
- en: 'Sample *k* trajectories using ![](img/B15558_17_133.png) and prepare the test
    dataset: ![](img/B15558_17_115.png).'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用![](img/B15558_17_133.png)采样*k*轨迹并准备测试数据集：![](img/B15558_17_115.png)。
- en: Now, we minimize the loss on the test dataset ![](img/B15558_17_135.png). Parameterize
    the model *f* with the optimal parameter ![](img/B15558_17_136.png) calculated
    in the previous step and compute the loss ![](img/B15558_17_137.png). Calculate
    the gradients of the loss and update our randomly initialized parameter ![](img/B15558_17_066.png)
    using our test (meta-training) dataset as:![](img/B15558_17_139.png)
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们在测试数据集![](img/B15558_17_135.png)上最小化损失。用之前步骤中计算出的最优参数![](img/B15558_17_136.png)对模型*f*进行参数化，并计算损失![](img/B15558_17_137.png)。计算损失的梯度，并使用我们的测试（元训练）数据集更新我们随机初始化的参数![](img/B15558_17_066.png)，计算公式为:![](img/B15558_17_139.png)
- en: Repeat *steps 2* to *4* for several iterations.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*到*步骤4*若干次。
- en: That's it! Meta learning is a growing field of research. Now that we have a
    basic idea of meta learning, you can explore more about meta learning and see
    how meta learning is used in reinforcement learning. In the next section, we will
    learn about hierarchical reinforcement learning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！元学习是一个不断发展的研究领域。现在我们对元学习有了一个基本的了解，你可以进一步探索元学习，并了解元学习在强化学习中的应用。在下一部分，我们将学习层次化强化学习。
- en: Hierarchical reinforcement learning
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化强化学习
- en: The problem with reinforcement learning is that it cannot scale well with a
    large number of state spaces and actions, which ultimately leads to the problem
    called curse of dimensionality. **Hierarchical reinforcement learning** (**HRL**)
    is proposed to solve the curse of dimensionality, where we decompose large problems
    into small subproblems in a hierarchy. Let's suppose the goal of our agent is
    to reach home from school. Now, our goal is split into a set of subgoals, such
    as going out of the school gate, booking a cab, and so on.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的问题在于，当状态空间和动作空间的数量较大时，它无法很好地扩展，最终会导致所谓的“维度灾难”问题。**层次化强化学习**（**HRL**）被提出用来解决维度灾难问题，在该方法中，我们将大型问题分解为层次结构中的小子问题。假设我们的智能体的目标是从学校回家。现在，我们的目标被分解为一组子目标，比如走出学校大门、叫出租车等等。
- en: There are different methods used in HRL, such as state-space decomposition,
    state abstraction, and temporal abstraction. In state-space decomposition, we
    decompose the state space into different subspaces and try to solve the problem
    in a smaller subspace. Breaking down the state space also allows faster exploration,
    as the agent does not want to explore the entire state space. In state abstraction,
    the agent ignores the variables that are irrelevant to achieving the current subtasks
    in the current state space. In temporal abstraction, the action sequence and action
    sets are grouped, which divides the single step into multiple steps.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在HRL中使用了不同的方法，例如状态空间分解、状态抽象和时间抽象。在状态空间分解中，我们将状态空间分解为不同的子空间，并尝试在较小的子空间中解决问题。分解状态空间还可以加快探索速度，因为代理不需要探索整个状态空间。在状态抽象中，代理忽略当前状态空间中与完成当前子任务无关的变量。在时间抽象中，动作序列和动作集合被分组，将单步动作划分为多个步骤。
- en: We will now look into one of the most commonly used algorithms in HRL, called
    MAXQ value function decomposition.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨HRL中最常用的算法之一，叫做MAXQ值函数分解。
- en: MAXQ value function Decomposition
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MAXQ值函数分解
- en: 'MAXQ value function decomposition is one of the most frequently used algorithms
    in HRL. In this section, let''s get a basic idea and overview of how MAXQ value
    function decomposition works. Let''s understand how MAXQ value function decomposition
    works with an example. Let''s take a taxi environment as shown in *Figure 17.5*:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MAXQ值函数分解是HRL中最常用的算法之一。在本节中，让我们基本了解MAXQ值函数分解是如何工作的。我们通过一个例子来理解MAXQ值函数分解的工作原理。我们以*图
    17.5*所示的出租车环境为例：
- en: '![](img/B15558_17_05.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_05.png)'
- en: 'Figure 17.5: Taxi environment'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5：出租车环境
- en: Let's suppose our agent is driving a taxi. As *Figure 17.5* shows, the tiny,
    yellow-colored rectangle is the taxi driven by our agent. The letters (**R**,
    **G**, **Y**, **B**) represent the different locations. Thus we have four locations
    in total, and the agent has to pick up a passenger at one location and drop them
    off at another location. The agent will receive +20 points as a reward for a successful
    drop-off and -1 point for every time step it takes. The agent will also lose -10
    points for illegal pickups and drop-offs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的代理正在驾驶一辆出租车。如*图 17.5*所示，黄色的矩形代表由我们代理驾驶的出租车。字母（**R**、**G**、**Y**、**B**）表示不同的位置。因此，我们总共有四个位置，代理需要在一个位置接乘客并在另一个位置将其放下。代理成功放下乘客后将获得+20分，每个时间步骤消耗1分。如果代理进行非法接送，将失去-10分。
- en: So the goal of our agent is to learn to pick up and drop off passengers at the
    correct location in a short time without adding illegal passengers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的代理的目标是在短时间内将乘客接送到正确的位置，并且不添加非法乘客。
- en: 'Now we break the goal of our agent into four subtasks as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把代理的目标分解为以下四个子任务：
- en: '**Navigate**: In the Navigate subtask, the goal of our agent is to drive the
    taxi from the current location to one of the target locations. The Navigate(t)
    subtask will use the four primitive actions: *north*, *south*, *east*, and *west*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Navigate**：在Navigate子任务中，我们的代理的目标是将出租车从当前位置开到一个目标位置。Navigate(t)子任务将使用四个原始动作：*north*、*south*、*east*和*west*。'
- en: '**Get**:In the Get subtask, the goal of our agent is to drive the taxi from
    its current location to the passenger''s location and pick up the passenger.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Get**：在Get子任务中，我们的代理的目标是将出租车从当前位置开到乘客所在的位置并接载乘客。'
- en: '**Put**:In the Put subtask, the goal of our agent is to drive the taxi from
    its current location to the passenger''s destination and drop off the passenger.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Put**：在Put子任务中，我们的代理的目标是将出租车从当前位置开到乘客的目的地并将乘客放下。'
- en: '**Root**:Root is the whole task.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Root**：Root是整个任务。'
- en: 'We can represent all these subtasks in a directed acyclic graph called a task
    graph, as *Figure 17.6* shows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个有向无环图来表示所有这些子任务，这个图被称为任务图，如*图 17.6*所示：
- en: '![](img/B15558_17_06.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_06.png)'
- en: 'Figure 17.6: Task graph'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.6：任务图
- en: 'As we can observe from the preceding figure, all the subtasks are arranged
    hierarchically. Each node represents a subtask or primitive action and each edge
    connects in such a way that a subtask can call its child subtask. As shown, the
    **Navigate(t)** subtask has four primitive actions: **East**, **West**, **North**,
    and **South**. The **Get** subtask has a **Pickup** primitive action and a **Navigate(t)**
    subtask. Similarly, the **Put** subtask has a **Putdown** (drop) primitive action
    and **Navigate(t)** subtask.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中我们可以观察到，所有的子任务都是按层次排列的。每个节点代表一个子任务或原始动作，每条边连接的方式使得一个子任务可以调用它的子子任务。如所示，**Navigate(t)**子任务有四个原始动作：**East**、**West**、**North**和**South**。**Get**子任务有一个**Pickup**原始动作和一个**Navigate(t)**子任务。类似地，**Put**子任务有一个**Putdown**（放下）原始动作和一个**Navigate(t)**子任务。
- en: 'In MAXQ value function decomposition, we decompose the value function into
    a set of value functions for each of the subtasks. For the efficient designing
    and debugging of MAXQ decompositions, we can redesign our task graphs as *Figure 17.7 *shows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在MAXQ值函数分解中，我们将值函数分解为每个子任务的一组值函数。为了高效设计和调试MAXQ分解，我们可以像*图17.7*所示那样重新设计我们的任务图：
- en: '![](img/B15558_17_07.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_07.png)'
- en: 'Figure 17.7: Task graph'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7：任务图
- en: 'As we can observe from *Figure 17.7*, our redesigned graph contains two special
    types of nodes: max nodes and Q nodes. The max nodes define the subtasks in the
    task decomposition and the Q nodes define the actions that are available for each
    subtask.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图17.7*中我们可以观察到，我们重新设计的图包含两种特殊类型的节点：最大节点和Q节点。最大节点定义了任务分解中的子任务，而Q节点定义了每个子任务可用的行动。
- en: Thus, in this section, we got a basic idea of MaxQ value function decomposition.
    In the next section, we will learn about I2A.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这一节中，我们对MaxQ值函数分解有了基本的了解。在下一节中，我们将学习I2A。
- en: Imagination augmented agents
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 想象增强代理
- en: Are you a fan of chess? If I asked you to play chess, how would you play it?
    Before moving any chess piece on the chessboard, you might imagine the consequences
    of moving a chess piece and move the chess piece that you think would help you
    to win the game. So, basically, before taking any action, we imagine the consequence
    and, if it is favorable, we proceed with that action, else we refrain from performing
    that action.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你是国际象棋的爱好者吗？如果我让你下棋，你会怎么下？在移动棋盘上的任何棋子之前，你可能会想象移动某个棋子的后果，并移动你认为能够帮助你赢得游戏的棋子。因此，基本上，在采取任何行动之前，我们会想象后果，如果后果是有利的，我们就会执行这个行动，否则我们会避免执行该行动。
- en: 'Similarly, **Imagination Augmented Agents** (**I2As**) are augmented with imagination.
    Before taking any action in an environment, the agent imagines the consequences
    of taking the action and if they think the action will provide a good reward,
    they will perform the action. The I2A takes advantage of both model-based and
    model-free learning. *Figure 17.8* shows the architecture of I2As:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，**想象增强代理**（**I2As**）通过想象得到了增强。在环境中采取任何行动之前，代理会想象采取该行动的后果，如果他们认为该行动会带来好的奖励，他们就会执行这个行动。I2A利用了基于模型和无模型学习的优势。*图17.8*展示了I2A的架构：
- en: '![](img/B15558_17_08.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_08.png)'
- en: 'Figure 17.8: I2A architecture'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8：I2A架构
- en: As we can observe from *Figure 17.8*, I2A architecture has both model-based
    and model-free paths. Thus, the action the agent takes is the result of both the
    model-based and model-free paths. In the model-based path, we have rollout encoders.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图17.8*中我们可以观察到，I2A架构既有基于模型的路径，也有无模型的路径。因此，代理采取的行动是基于模型路径和无模型路径的结果。在基于模型的路径中，我们有回滚编码器。
- en: 'These rollout encoders are where the agent performs imagination tasks, so let''s
    take a closer look at the rollout encoders. *Figure 17.9* shows a single rollout
    encoder:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这些回滚编码器是代理执行想象任务的地方，让我们仔细看看回滚编码器。*图17.9*展示了一个单一的回滚编码器：
- en: '![](img/B15558_17_09.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_09.png)'
- en: 'Figure 17.9: Single imagination rollout'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9：单一的想象回滚
- en: 'From *Figure 17.9*, we can observe that the rollout encoders have two layers:
    the imagine future layer and the encoder layer. The imagine future layer is where
    the imagination happens. The imagine future layer consists of the imagination
    core.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图17.9*中我们可以观察到，回滚编码器有两层：想象未来层和编码器层。想象未来层是进行想象的地方。想象未来层由想象核心组成。
- en: When we feed the state *s*[t] to the imagination core, we get the next state
    ![](img/B15558_17_140.png) and the reward ![](img/B15558_17_141.png), and when
    we feed this next state ![](img/B15558_17_140.png) to the next imagination core,
    we get the next state ![](img/B15558_17_143.png) and reward ![](img/B15558_17_144.png).
    If we repeat these for *n* steps, we get a rollout, which is basically a pair
    of states and rewards, and then we use encoders such as **Long Short-Term Memory**
    (**LSTM**) to encode this rollout. As a result, we get rollout encodings. These
    rollout encodings are actually the embeddings describing the future imagined path.
    We will have multiple rollout encoders for different future imagined paths, and
    we use an aggregator to aggregate this rollout encoder.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将状态*s*[t]输入到想象核心时，我们得到下一个状态 ![](img/B15558_17_140.png) 和奖励 ![](img/B15558_17_141.png)，然后当我们将这个下一个状态
    ![](img/B15558_17_140.png) 输入到下一个想象核心时，我们得到下一个状态 ![](img/B15558_17_143.png) 和奖励
    ![](img/B15558_17_144.png)。如果我们重复这些操作 *n* 步，我们得到一个展开（rollout），它基本上是一个状态和奖励的对，然后我们使用编码器，如**长短时记忆网络**（**LSTM**），来编码这个展开。结果，我们得到展开编码。这些展开编码实际上是描述未来想象路径的嵌入。我们将为不同的未来想象路径拥有多个展开编码器，并且我们使用一个聚合器来聚合这个展开编码器。
- en: 'Okay, but how exactly does the imagination happen in the imagination core?
    What is actually in the imagination core? *Figure 17.10* shows a single imagination
    core:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但在想象核心中，想象究竟是如何发生的？想象核心中到底包含了什么？*图 17.10* 显示了一个单一的想象核心：
- en: '![](img/B15558_17_10.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_10.png)'
- en: 'Figure 17.10: The imagination core'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.10：想象核心
- en: As we can observe from *Figure 17.10*, the imagination core consists of a policy
    network and an environment model. The environment model learns from all the actions
    that the agent has performed so far. It takes information about the state ![](img/B15558_17_145.png),
    imagines all the possible futures considering the experience, and chooses the
    action ![](img/B15558_17_146.png) that gives a high reward.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从*图 17.10*中观察到的，想象核心由策略网络和环境模型组成。环境模型从代理迄今为止执行的所有动作中学习。它接收关于状态的信息 ![](img/B15558_17_145.png)，结合经验想象所有可能的未来，并选择能够获得高奖励的动作
    ![](img/B15558_17_146.png)。
- en: '*Figure 17.11* shows the complete architecture of I2As with all components
    expanded:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17.11* 显示了 I2A 的完整架构，所有组件都已展开：'
- en: '![](img/B15558_17_11.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_11.png)'
- en: 'Figure 17.11: Full I2A architecture'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.11：完整的 I2A 架构
- en: 'Have you played Sokoban before? Sokoban is a classic puzzle game where the
    player has to push boxes to a target location. The rules of the game are very
    simple: boxes can only be pushed and cannot be pulled. If we push a box in the
    wrong direction then the puzzle becomes unsolvable:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你玩过推箱子游戏吗？推箱子是一个经典的益智游戏，玩家需要将箱子推到目标位置。游戏规则非常简单：箱子只能被推，不能被拉。如果我们把箱子推错了方向，那么谜题就变得无法解决。
- en: '![](img/B15558_17_12.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_17_12.png)'
- en: 'Figure 17.12: Sokoban environment'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.12：推箱子环境
- en: The I2A architecture provides good results in these kinds of environments, where
    the agent has to plan in advance before taking an action. The authors of the paper
    tested I2A performance on Sokoban and achieved great results.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: I2A 架构在这些环境中提供了良好的结果，这些环境中代理必须在采取行动之前进行提前规划。论文的作者在推箱子游戏（Sokoban）上测试了 I2A 的性能，并取得了出色的结果。
- en: There are various exciting research advancements happening around deep reinforcement
    learning. Now that you have finished reading the book, you can start exploring
    such advancements and experiment with various projects. Learn and reinforce!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习领域正在发生各种令人兴奋的研究进展。现在你已经读完了这本书，你可以开始探索这些进展并尝试各种项目。学习并巩固！
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by understanding what meta learning is. We learned that
    with meta learning, we train our model on various related tasks with a few data
    points, such that for a new related task, our model can make use of the learning
    obtained from the previous tasks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解元学习（meta learning）开始这一章。我们了解到，通过元学习，我们可以在各种相关任务上用少量数据点训练模型，这样对于一个新的相关任务，我们的模型就可以利用从之前任务中获得的学习成果。
- en: Next, we learned about a popular meta-learning algorithm called MAML. In MAML,
    we sample a batch of tasks and for each task *T*[i] in the batch, we minimize
    the loss using gradient descent and get the optimal parameter ![](img/B15558_17_053.png).
    Then, we update our randomly initialized model parameter ![](img/B15558_09_054.png)
    by calculating the gradients for each of the new tasks *T*[i] with the model parameterized
    as ![](img/B15558_17_041.png).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了一种流行的元学习算法——MAML。在MAML中，我们从一批任务中采样，对于批次中的每个任务 *T*[i]，我们通过梯度下降最小化损失并获得最优参数
    ![](img/B15558_17_053.png)。然后，通过计算每个新任务 *T*[i] 的梯度并用模型参数化！[](img/B15558_17_041.png)，我们更新随机初始化的模型参数！[](img/B15558_09_054.png)。
- en: Moving on, we learned about HRL, where we decompose large problems into small
    subproblems in a hierarchy. We also looked into the different methods used in
    HRL, such as state-space decomposition, state abstraction, and temporal abstraction.
    Next, we got an overview of MAXQ value function decomposition, where we decompose
    the value function into a set of value functions for each of the subtasks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了HRL，在HRL中我们将大问题分解为层次化的小子问题。我们还研究了HRL中使用的不同方法，如状态空间分解、状态抽象和时间抽象。接下来，我们概述了MAXQ价值函数分解，我们将价值函数分解为每个子任务的价值函数集。
- en: At the end of the chapter, we learned about I2As, which are augmented with imagination.
    Before taking any action in an environment, the agent imagines the consequences
    of taking the action, and if they think the action will provide a good reward,
    they will perform the action.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们学习了带有想象增强的I2A。在采取任何行动之前，代理会想象采取该行动的后果，如果他们认为该行动会带来好的奖励，他们就会执行该行动。
- en: Deep reinforcement learning is evolving every day with interesting advancements.
    Now that you have learned about the various state-of-the-art deep reinforcement
    learning algorithms, you can start building interesting projects and also contribute
    to deep reinforcement learning research.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习每天都在发展，取得了有趣的进展。现在你已经了解了各种最先进的深度强化学习算法，可以开始构建有趣的项目，并为深度强化学习研究做出贡献。
- en: Questions
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s test the knowledge you gained in this chapter; try answering the following
    questions:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下你在本章中获得的知识；尝试回答以下问题：
- en: Why do we need meta learning?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要元学习？
- en: What is MAML?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是MAML？
- en: What is the meta objective?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是元目标？
- en: What is the meta training set?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是元训练集？
- en: Define HRL.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义HRL。
- en: How does an I2A work?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: I2A是如何工作的？
- en: Further reading
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'For more information, we can refer to the following papers:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，我们可以参考以下论文：
- en: '**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks** by *Chelsea
    Finn*, *Pieter Abbeel, Sergey Levine*, [https://arxiv.org/pdf/1703.03400.pdf](https://arxiv.org/pdf/1703.03400.pdf)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面向模型无关的元学习：深度网络的快速适应** 由 *Chelsea Finn*、*Pieter Abbeel, Sergey Levine* 著，[https://arxiv.org/pdf/1703.03400.pdf](https://arxiv.org/pdf/1703.03400.pdf)'
- en: '**Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition**
    by *Thomas G. Dietterich*, [https://arxiv.org/pdf/cs/9905014.pdf](https://arxiv.org/pdf/cs/9905014.pdf)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAXQ价值函数分解的层次强化学习** 由 *Thomas G. Dietterich* 著，[https://arxiv.org/pdf/cs/9905014.pdf](https://arxiv.org/pdf/cs/9905014.pdf)'
- en: '**Imagination-Augmented Agents for Deep Reinforcement Learning** by *Théophane
    Weber*, *et al*.,[https://arxiv.org/pdf/1707.06203.pdf](https://arxiv.org/pdf/1707.06203.pdf)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**想象增强的深度强化学习代理** 由 *Théophane Weber*、*等* 著，[https://arxiv.org/pdf/1707.06203.pdf](https://arxiv.org/pdf/1707.06203.pdf)'
