- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Introducing Graph Convolutional Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍图卷积网络
- en: The **Graph Convolutional Network** (**GCN**) architecture is the blueprint
    of what a GNN looks like. Introduced by Kipf and Welling in 2017 [1], it is based
    on the idea of creating an efficient variant of **Convolutional Neural Networks**
    (**CNNs**) applied to graphs. More accurately, it is an approximation of a graph
    convolution operation in graph signal processing. Thanks to its versatility and
    ease of use, the GCN has become the most popular GNN in scientific literature.
    More generally, it is the architecture of choice to create a solid baseline when
    dealing with graph data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**图卷积网络**（**GCN**）架构是GNN的蓝图。它由Kipf和Welling于2017年提出[1]，基于创建一个有效的**卷积神经网络**（**CNN**）变体，应用于图结构。更准确地说，它是图信号处理中图卷积操作的近似。由于其多功能性和易用性，GCN已成为科学文献中最受欢迎的GNN。更广泛地说，它是处理图数据时建立坚实基准的首选架构。'
- en: In this chapter, we’ll talk about the limitations of our previous vanilla GNN
    layer. This will help us to understand the motivation behind GCNs. We’ll detail
    how the GCN layer works and why it performs better than our solution. We’ll test
    this statement by implementing a GCN on the `Cora` and `Facebook Page-Page` datasets
    using PyTorch Geometric. This should improve our results even further.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论之前基础GNN层的局限性。这将帮助我们理解GCN的动机。我们将详细介绍GCN层的工作原理以及它为什么比我们的解决方案表现更好。我们将通过使用PyTorch
    Geometric在`Cora`和`Facebook Page-Page`数据集上实现GCN来验证这一点。这应该能进一步改善我们的结果。
- en: 'The last section is dedicated to a new task: **node regression**. This is not
    a very common task when it comes to GNNs, but it is particularly useful when you’re
    working with tabular data. If you have the opportunity to transform your tabular
    dataset into a graph, this will enable you to perform regression in addition to
    classification.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节介绍了一个新任务：**节点回归**。这是GNN中不太常见的任务，但在处理表格数据时特别有用。如果你有机会将表格数据集转化为图结构，那么这将使你能够进行回归任务，除了分类任务之外。
- en: By the end of this chapter, you will be able to implement a GCN in PyTorch Geometric
    for classification or regression tasks. Thanks to linear algebra, you’ll understand
    why this model performs better than our vanilla GNN. Finally, you’ll know how
    to plot node degrees and the density distribution of a target variable.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够在PyTorch Geometric中实现一个GCN，用于分类或回归任务。通过线性代数，你将理解为什么这个模型比我们的基础GNN表现更好。最后，你将学会如何绘制节点度数和目标变量的密度分布。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Designing the graph convolutional layer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计图卷积层
- en: Comparing graph convolutional and graph linear layers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较图卷积层与图线性层
- en: Predicting web traffic with node regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点回归预测网络流量
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例可以在GitHub上找到，链接为[https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06)。
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* section of this book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*前言*部分，你可以找到在本地计算机上运行代码所需的安装步骤。
- en: Designing the graph convolutional layer
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计图卷积层
- en: 'First, let’s talk about a problem we did not anticipate in the previous chapter.
    Unlike tabular or image data, nodes do not always have the same number of neighbors.
    For instance, in *Figure 6**.1*, node ![](img/Formula_B19153_06_001.png) has 3
    neighbors while node ![](img/Formula_B19153_06_002.png) only has 1:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论一个我们在上一章没有预见到的问题。与表格数据或图像数据不同，节点的邻居数并不总是相同。例如，在*图6.1*中，节点![](img/Formula_B19153_06_001.png)有3个邻居，而节点![](img/Formula_B19153_06_002.png)只有1个：
- en: '![Figure 6.1 – Simple graph where nodes have different numbers of neighbors](img/B19153_06_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 简单图，其中节点的邻居数不同](img/B19153_06_001.jpg)'
- en: Figure 6.1 – Simple graph where nodes have different numbers of neighbors
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 简单图，其中节点的邻居数不同
- en: 'However, if we look at our GNN layer, we don’t take into account this difference
    in the number of neighbors. Our layer consists of a simple sum without any normalization
    coefficient. Here is how we calculated the embedding of a node, ![](img/Formula_B19153_06_003.png):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们观察我们的 GNN 层，我们并没有考虑邻居数量的差异。我们的层由一个简单的求和构成，没有任何归一化系数。下面是我们计算节点嵌入的方式，![](img/Formula_B19153_06_003.png)：
- en: '![](img/Formula_B19153_06_004.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_004.jpg)'
- en: 'Imagine that node ![](img/Formula_B19153_06_005.png) has 1,000 neighbors and
    node ![](img/Formula_B19153_06_006.png) only has 1: the embedding ![](img/Formula_B19153_06_007.png)
    will have much larger values than ![](img/Formula_B19153_06_008.png). This is
    an issue because we want to compare these embeddings. How are we supposed to make
    meaningful comparisons when their values are so vastly different?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设节点 ![](img/Formula_B19153_06_005.png) 有 1000 个邻居，而节点 ![](img/Formula_B19153_06_006.png)
    只有 1 个：嵌入 ![](img/Formula_B19153_06_007.png) 的值将比 ![](img/Formula_B19153_06_008.png)
    大得多。这是一个问题，因为我们想要比较这些嵌入。当它们的值差异如此之大时，我们该如何进行有意义的比较呢？
- en: 'Fortunately, there is a simple solution: dividing the embedding by the number
    of neighbors. Let’s write ![](img/Formula_B19153_06_009.png), the degree of node
    ![](img/Formula_B19153_06_010.png). Here is the new formula for the GNN layer:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个简单的解决方法：通过邻居数量来除以嵌入。让我们写出 ![](img/Formula_B19153_06_009.png)，节点 ![](img/Formula_B19153_06_010.png)
    的度。这里是 GNN 层的新公式：
- en: '![](img/Formula_B19153_06_011.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_011.jpg)'
- en: 'But how do we translate it into a matrix multiplication? As a reminder, this
    was what we obtained for our vanilla GNN layer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何将其转化为矩阵乘法呢？提醒一下，这是我们为普通 GNN 层获得的结果：
- en: '![](img/Formula_B19153_06_012.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_012.jpg)'
- en: Here, ![](img/Formula_B19153_06_013.png).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 ![](img/Formula_B19153_06_013.png)。
- en: 'The only thing that is missing from this formula is a matrix to give us the
    normalization coefficient, ![](img/Formula_B19153_06_020.png). This is something
    that can be obtained thanks to the degree matrix ![](img/Formula_B19153_06_015.png),
    which counts the number of neighbors for each node. Here is the degree matrix
    for the graph shown in *Figure 6**.1*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式中唯一缺少的是一个矩阵，用来给我们提供归一化系数，![](img/Formula_B19153_06_020.png)。这一点可以通过度矩阵 ![](img/Formula_B19153_06_015.png)
    获得，它计算每个节点的邻居数量。下面是图 6.1 中所示图形的度矩阵：
- en: '![](img/Formula_B19153_06_016.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_016.jpg)'
- en: 'Here is the same matrix in NumPy:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相同的矩阵在 NumPy 中的表示：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'By definition, ![](img/Formula_B19153_06_017.png) gives us the degree of each
    node, ![](img/Formula_B19153_06_018.png). Therefore, the inverse of this matrix
    ![](img/Formula_B19153_06_019.png) directly gives us the normalization coefficients,
    ![](img/Formula_B19153_06_0201.png):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，![](img/Formula_B19153_06_017.png) 给出了每个节点的度，![](img/Formula_B19153_06_018.png)。因此，这个矩阵的逆
    ![](img/Formula_B19153_06_019.png) 直接给我们归一化系数，![](img/Formula_B19153_06_0201.png)：
- en: '![](img/Formula_B19153_06_021.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_021.jpg)'
- en: 'The inverse of a matrix can directly be calculated using the `numpy.linalg.inv()`function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的逆可以直接通过 `numpy.linalg.inv()` 函数计算：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is exactly what we were looking for. To be even more accurate, we added
    self-loops to the graph, represented by ![](img/Formula_B19153_06_0131.png). Likewise,
    we should add self-loops to the degree matrix, ![](img/Formula_B19153_06_023.png).
    The final matrix we are actually interested in is ![](img/Formula_B19153_06_024.png):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们所寻找的。为了更加准确，我们在图中添加了自环，用 ![](img/Formula_B19153_06_0131.png) 表示。同样，我们应该在度矩阵中添加自环，![](img/Formula_B19153_06_023.png)。我们真正感兴趣的最终矩阵是
    ![](img/Formula_B19153_06_024.png)：
- en: '![](img/Formula_B19153_06_025.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_025.jpg)'
- en: 'NumPy has a specific function, `numpy.identity(n)`, to quickly create an identity
    matrix ![](img/Formula_B19153_06_026.png) of ![](img/Formula_B19153_06_027.png)
    dimensions. In this example, we have four dimensions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 有一个特定的函数，`numpy.identity(n)`，可以快速创建一个单位矩阵 ![](img/Formula_B19153_06_026.png)，它的维度是
    ![](img/Formula_B19153_06_027.png)。在这个例子中，我们有四个维度：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have our matrix of normalization coefficients, where should we
    put it in the formula? There are two options:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了归一化系数矩阵，我们应该把它放在哪里呢？有两个选项：
- en: '![](img/Formula_B19153_06_028.png) will normalize every row of features.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_B19153_06_028.png) 将对每一行特征进行归一化。'
- en: '![](img/Formula_B19153_06_029.png)will normalize every column of features.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_B19153_06_029.png) 将对每一列特征进行归一化。'
- en: We can verify this experimentally by calculating ![](img/Formula_B19153_06_030.png)
    and ![](img/Formula_B19153_06_031.png).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过实验验证这一点，计算 ![](img/Formula_B19153_06_030.png) 和 ![](img/Formula_B19153_06_031.png)。
- en: '![](img/Formula_B19153_06_032.jpg)![](img/Formula_B19153_06_033.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_032.jpg)![](img/Formula_B19153_06_033.jpg)'
- en: Indeed, in the first case, the sum of every row is equal to 1\. In the second
    case, the sum of every column is equal to 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，在第一个案例中，每一行的和等于 1。在第二个案例中，每一列的和等于 1。
- en: 'Matrix multiplications can be performed using the `numpy.matmul()` function.
    Even more conveniently, Python has had its own matrix multiplication operator,
    `@`, since version 3.5\. Let’s define the adjacency matrix ![](img/Formula_B19153_06_034.png)
    and use this operator to compute our matrix multiplications:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法可以使用 `numpy.matmul()` 函数进行。更方便的是，Python 从版本 3.5 开始就有了自己的矩阵乘法运算符 `@`。让我们定义邻接矩阵
    ![](img/Formula_B19153_06_034.png) 并使用这个运算符来计算矩阵乘法：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We obtain the same results as with manual matrix multiplication.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了与手动矩阵乘法相同的结果。
- en: So, which option should we use? Naturally, the first option looks more appealing
    because it nicely normalizes neighboring node features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们应该使用哪个选项呢？自然，第一个选项看起来更具吸引力，因为它很好地对邻居节点特征进行了归一化。
- en: 'However, Kipf and Welling [1] noticed that features from nodes with a lot of
    neighbors spread very easily, unlike features from more isolated nodes. In the
    original GCN paper, the authors proposed a hybrid normalization to counterbalance
    this effect. In practice, they assign higher weights to nodes with few neighbors
    using the following formula:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kipf 和 Welling [1] 注意到，具有很多邻居的节点的特征传播非常容易，这与更孤立节点的特征不同。在原始 GCN 论文中，作者提出了一种混合归一化方法来对抗这种效果。实际上，他们通过以下公式给具有较少邻居的节点分配更高的权重：
- en: '![](img/Formula_B19153_06_035.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_035.jpg)'
- en: 'In terms of individual embeddings, this operation can be written as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在个体嵌入方面，这个操作可以写成如下形式：
- en: '![](img/Formula_B19153_06_036.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_036.jpg)'
- en: Those are the original formulas to implement a graph convolutional layer. As
    with our vanilla GNN layer, we can stack these layers to create a GCN. Let’s implement
    a GCN and verify that it performs better than our previous approaches.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是实现图卷积层的原始公式。与我们基本的 GNN 层一样，我们可以堆叠这些层来创建 GCN。让我们实现一个 GCN 并验证它是否优于我们之前的方法。
- en: Comparing graph convolutional and graph linear layers
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较图卷积层和图线性层
- en: In the previous chapter, our vanilla GNN outperformed the Node2Vec model, but
    how does it compare to a GCN? In this section, we will compare their performance
    on the Cora and Facebook Page-Page datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们的基本 GNN 超越了 Node2Vec 模型，但它与 GCN 比如何呢？在本节中，我们将比较它们在 Cora 和 Facebook Page-Page
    数据集上的表现。
- en: Compared to the vanilla GNN, the main feature of the GCN is that it considers
    node degrees to weigh its features. Before the real implementation, let’s analyze
    the node degrees in both datasets. This information is relevant since it is directly
    linked to the performance of the GCN.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于基本 GNN，GCN 的主要特点是它考虑节点的度数来加权其特征。在实际实现之前，让我们先分析这两个数据集中的节点度数。这些信息是相关的，因为它与
    GCN 的性能直接相关。
- en: 'From what we know about this architecture, we expect it to perform better when
    node degrees vary greatly. If every node has the same number of neighbors, these
    architectures are equivalent: (![](img/Formula_B19153_06_037.png)):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '根据我们对这个架构的了解，我们预计当节点度数变化很大时，它的表现会更好。如果每个节点的邻居数量相同，这些架构是等效的：(![](img/Formula_B19153_06_037.png)):'
- en: 'We import the `Planetoid` class from PyTorch Geometric. To visualize the node
    degrees, we also import `matplotlib` and two additional classes: `degree` to get
    the number of neighbors of each node and `Counter` to count the number of nodes
    for each degree:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 PyTorch Geometric 导入 `Planetoid` 类。为了可视化节点度数，我们还导入了 `matplotlib` 和另外两个类：`degree`
    用于获取每个节点的邻居数量，`Counter` 用于统计每个度数的节点数量：
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Cora dataset is imported and its graph is stored in `data`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cora 数据集已导入，其图被存储在 `data` 中：
- en: '[PRE5]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We compute the number of neighbors of each node in the graph:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算图中每个节点的邻居数量：
- en: '[PRE6]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To produce a more natural visualization, we count the number of nodes for each
    degree:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成更自然的可视化，我们统计每个度数的节点数量：
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s plot this result using a bar plot:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用条形图来绘制这个结果：
- en: '[PRE8]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That gives us the following plot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了如下的图表。
- en: '![Figure 6.2 – Number of nodes with specific node degrees in the Cora dataset](img/B19153_06_002.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Cora 数据集中具有特定节点度数的节点数量](img/B19153_06_002.jpg)'
- en: Figure 6.2 – Number of nodes with specific node degrees in the Cora dataset
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – Cora 数据集中具有特定节点度数的节点数量
- en: 'This distribution looks exponential with a heavy tail: it ranges from 1 neighbor
    (485 nodes) to 168 neighbors (1 node)! This is exactly the kind of dataset where
    we want a normalization process to consider this disbalance.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布看起来是指数型的，并且有重尾：它的邻居数量从1个（485个节点）到168个（1个节点）不等！这正是我们希望进行归一化处理来考虑这种不平衡的数据集。
- en: 'The same process is repeated with the Facebook Page-Page dataset with the following
    result:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的过程在Facebook页面-页面数据集上重复，得到以下结果：
- en: '![Figure 6.3 – Number of nodes with specific node degrees in the Facebook Page-Page
    dataset](img/B19153_06_003.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – Facebook页面-页面数据集中具有特定节点度数的节点数量](img/B19153_06_003.jpg)'
- en: Figure 6.3 – Number of nodes with specific node degrees in the Facebook Page-Page
    dataset
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – Facebook页面-页面数据集中具有特定节点度数的节点数量
- en: This distribution of node degrees looks even more skewed, with a number of neighbors
    that ranges from 1 to 709\. For the same reason, the Facebook Page-Page dataset
    is also a good case in which to apply a GCN.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种节点度数分布看起来更加偏斜，邻居数量从1到709不等。出于同样的原因，Facebook页面-页面数据集也是一个很好的应用GCN的案例。
- en: 'We could build our own graph layer but, conveniently enough, PyTorch Geometric
    already has a predefined GCN layer. Let’s implement it on the Cora dataset first:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建自己的图层，但幸运的是，PyTorch Geometric已经预定义了一个GCN层。让我们先在Cora数据集上实现它：
- en: 'We import PyTorch and the GCN layer from PyTorch Geometric:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从PyTorch Geometric导入PyTorch和GCN层：
- en: '[PRE9]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We create a function to calculate the accuracy score:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个函数来计算准确率：
- en: '[PRE10]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We create a GCN class with a `__init_()` function that takes three parameters
    as input: the number of input dimensions, `dim_in`, the number of hidden dimensions,
    `dim_h`, and the number of output dimensions, `dim_out`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个GCN类，其中包含一个`__init_()`函数，该函数接受三个参数作为输入：输入维度的数量`dim_in`，隐藏维度的数量`dim_h`，以及输出维度的数量`dim_out`：
- en: '[PRE11]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `forward` method is identical, and has two GCN layers. A log `softmax`
    function is applied to the result for classification:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forward`方法是相同的，并且有两个GCN层。对结果应用一个对数`softmax`函数进行分类：'
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `fit()` method is the same, with the exact same parameters for the `Adam`
    optimizer (a learning rate of 0.1 and L2 regularization of 0.0005):'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit()`方法相同，使用相同的`Adam`优化器参数（学习率为0.1，L2正则化为0.0005）：'
- en: '[PRE13]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We implement the same `test()` method:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实现相同的`test()`方法：
- en: '[PRE14]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s instantiate and train our model for `100` epochs:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实例化并训练我们的模型`100`个周期：
- en: '[PRE15]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output of the training:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是训练的输出：
- en: '[PRE16]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, let’s evaluate it on the test set:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们在测试集上评估一下：
- en: '[PRE17]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If we repeat this experiment 100 times, we obtain an average accuracy score
    of 80.17% (± 0.61%), which is significantly higher than the 74.98% (± 1.50%) obtained
    by our vanilla GNN.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重复进行100次这个实验，我们得到的平均准确率为80.17%（± 0.61%），显著高于我们原始GNN得到的74.98%（± 1.50%）。
- en: 'The exact same model is then applied to the Facebook Page-Page dataset, where
    it obtains an average accuracy of 91.54% (± 0.28%). Once again, it is significantly
    higher than the result obtained by the vanilla GNN, with only 84.85% (± 1.68%).
    The following table summarizes the accuracy scores with standard deviation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完全相同的模型应用于Facebook页面-页面数据集，取得了91.54%（± 0.28%）的平均准确率。再次，它显著高于原始GNN得到的84.85%（±
    1.68%）的结果。以下表格总结了带有标准偏差的准确率：
- en: '|  | **MLP** | **GNN** | **GCN** |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | **MLP** | **GNN** | **GCN** |'
- en: '| **Cora** | 53.47%(±1.81%) | 74.98%(±1.50%) | 80.17%(±0.61%) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **Cora** | 53.47%(±1.81%) | 74.98%(±1.50%) | 80.17%(±0.61%) |'
- en: '| **Facebook** | 75.21%(±0.40%) | 84.85%(±1.68%) | 91.54%(±0.28%) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **Facebook** | 75.21%(±0.40%) | 84.85%(±1.68%) | 91.54%(±0.28%) |'
- en: Figure 6.4 – Summary of accuracy scores with standard deviation
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 带有标准偏差的准确率汇总
- en: We can attribute these high scores to the wide range of node degrees in these
    two datasets. By normalizing features and considering the number of neighbors
    of the central node and its own neighbors, the GCN gains a lot of flexibility
    and can work well with various types of graphs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些高分归因于这两个数据集节点度数的广泛范围。通过对特征进行归一化，并考虑中心节点及其邻居的数量，GCN获得了很大的灵活性，并能很好地处理各种类型的图。
- en: Nonetheless, node classification is not the only task that GNNs can perform.
    In the next section, we’ll see a new type of application that is rarely covered
    in the literature.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，节点分类并不是GNN能够执行的唯一任务。在接下来的部分，我们将看到一种文献中很少涉及的新型应用。
- en: Predicting web traffic with node regression
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用节点回归预测网页流量
- en: In machine learning, **regression** refers to the prediction of continuous values.
    It is often contrasted with **classification**, where the goal is to find the
    correct categories (which are not continuous). In graph data, their counterparts
    are node classification and node regression. In this section, we will try to predict
    a continuous value instead of a categorical variable for each node.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**回归**指的是预测连续值。它通常与**分类**对比，后者的目标是找到正确的类别（这些类别是离散的，不是连续的）。在图数据中，它们的对应关系是节点分类和节点回归。在这一部分，我们将尝试预测每个节点的连续值，而不是类别变量。
- en: 'The dataset we will use is the Wikipedia Network (GNU General Public License
    v3.0), introduced by Rozemberckzi et al. in 2019 [2]. It is composed of three
    page-page networks: chameleons (2,277 nodes and 31,421 edges), crocodiles (11,631
    nodes and 170,918 edges), and squirrels (5,201 nodes and 198,493 edges). In these
    datasets, nodes represent articles and edges are mutual links between them. Node
    features reflect the presence of particular words in the articles. Finally, the
    goal is to predict the log average monthly traffic of December 2018.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是维基百科网络（GNU通用公共许可证v3.0），由Rozemberckzi等人于2019年提出[2]。它由三个页面-页面网络组成：变色龙（2,277个节点和31,421条边），鳄鱼（11,631个节点和170,918条边），以及松鼠（5,201个节点和198,493条边）。在这些数据集中，节点表示文章，边表示它们之间的相互链接。节点特征反映了文章中特定词汇的出现情况。最后，目标是预测2018年12月的对数平均月流量。
- en: 'In this section, we will apply a GCN to predict this traffic on the chameleon
    dataset:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将应用GCN来预测变色龙数据集上的流量：
- en: 'We import the Wikipedia Network and download the chameleon dataset. We apply
    the `transform` function, `RandomNodeSplit()`, to randomly create an evaluation
    mask and a test mask:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入了维基百科网络并下载了变色龙数据集。我们应用了`transform`函数，`RandomNodeSplit()`，来随机创建评估掩码和测试掩码：
- en: '[PRE18]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We print information about this dataset:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印了该数据集的信息：
- en: '[PRE19]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is the output we obtain:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们获得的输出：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'There is a problem with our dataset: the output says that we have five classes.
    However, we want to perform node regression, not classification. So what happened?'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的数据集存在问题：输出显示我们有五个类别。然而，我们希望进行节点回归，而不是分类。那么发生了什么呢？
- en: 'In fact, these five classes are bins of the continuous values we want to predict.
    Unfortunately, these labels are not the ones we want: we have to change them manually.
    First, let’s download the `wikipedia.zip` file from the following page: [https://snap.stanford.edu/data/wikipedia-article-networks.xhtml](https://snap.stanford.edu/data/wikipedia-article-networks.xhtml).
    After unzipping the file, we import `pandas` and use it to load the targets:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这五个类别是我们想要预测的连续值的区间。不幸的是，这些标签不是我们需要的：我们必须手动更改它们。首先，让我们从以下页面下载`wikipedia.zip`文件：[https://snap.stanford.edu/data/wikipedia-article-networks.xhtml](https://snap.stanford.edu/data/wikipedia-article-networks.xhtml)。解压文件后，我们导入`pandas`并使用它来加载目标值：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We apply a log function to the target values using `np.log10()` because the
    goal is to predict the log average monthly traffic:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对目标值应用了对数函数，使用`np.log10()`，因为目标是预测对数平均月流量：
- en: '[PRE22]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We redefine `data.y` as a tensor of the continuous values from the previous
    step. Note that these values are not normalized in this example, which is a good
    practice that is usually implemented. We will not perform it here for ease of
    exposition:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重新定义`data.y`为从上一步得到的连续值的张量。请注意，这些值在本例中没有标准化，通常标准化是一个好的做法，但为了便于说明，我们这里不进行标准化：
- en: '[PRE23]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once again, it is a good idea to visualize the node degrees as we did for the
    two previous datasets. We use the exact same code to produce the following figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，像之前的两个数据集一样，最好可视化节点度。我们使用完全相同的代码来生成以下图形：
- en: '![Figure 6.5 – Number of nodes with specific node degrees in the Wikipedia
    Network](img/B19153_6_0005.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 维基百科网络中具有特定节点度的节点数量](img/B19153_6_0005.jpg)'
- en: Figure 6.5 – Number of nodes with specific node degrees in the Wikipedia Network
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 维基百科网络中具有特定节点度的节点数量
- en: 'This distribution has a shorter tail than the previous ones but keeps a similar
    shape: most nodes have one or a few neighbors, but some of them act as “hubs”
    and can connect more than 80 nodes.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布的尾部比之前的分布更短，但保持了类似的形状：大多数节点只有一个或几个邻居，但其中一些节点作为“枢纽”，可以连接超过80个节点。
- en: 'In the case of node regression, the distribution of node degrees is not the
    only type of distribution we should check: the distribution of our target values
    is also essential. Indeed, non-normal distribution (such as node degrees) tends
    to be harder to predict. We can use the Seaborn library to plot the target values
    and compare them to a normal distribution provided by `scipy.stats.norm`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点回归的情况下，节点度分布不是我们应该检查的唯一分布类型：我们的目标值分布同样至关重要。实际上，非正态分布（如节点度）往往更难预测。我们可以使用 Seaborn
    库绘制目标值，并将其与 `scipy.stats.norm` 提供的正态分布进行比较：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This gives us the following plot:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了如下图表：
- en: '![Figure 6.6 – Density plot of target values from the Wikipedia Network](img/B19153_06_006.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6.6 – Wikipedia 网络的目标值密度图](img/B19153_06_006.jpg)'
- en: Figure 6.6 – Density plot of target values from the Wikipedia Network
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – Wikipedia 网络的目标值密度图
- en: This distribution is not exactly normal, but it is not exponential like the
    node degrees either. We can expect our model to perform well to predict these
    values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布不完全是正态分布，但也不像节点度那样是指数分布。我们可以预期我们的模型在预测这些值时会表现良好。
- en: 'Let’s implement it step by step with PyTorch Geometric:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 PyTorch Geometric 一步步实现：
- en: 'We define the GCN class and the `__init__()` function. This time, we have three
    `GCNConv` layers with a decreasing number of neurons. The idea behind this encoder
    architecture is to force the model to select the most relevant features to predict
    the target values. We also added a linear layer to output a prediction that is
    not limited to a number between 0 or -1 and 1:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了 GCN 类和 `__init__()` 函数。这一次，我们有三个 `GCNConv` 层，且每一层的神经元数量逐渐减少。这个编码器架构的目的是迫使模型选择最相关的特征来预测目标值。我们还添加了一个线性层，以输出一个不局限于
    0 或 -1 和 1 之间的预测值：
- en: '[PRE25]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `forward()` method includes the new `GCNConv` and `nn.Linear` layers. There
    is no need for a log `softmax` function here since we’re not predicting a class:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forward()` 方法包括了新的 `GCNConv` 和 `nn.Linear` 层。这里不需要使用对数 `softmax` 函数，因为我们并不预测类别：'
- en: '[PRE26]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The main change in the `fit()` method is the `F.mse_loss()` function, which
    replaces the cross-entropy loss used in classification tasks. The **Mean Squared
    Error** (**MSE**) will be our main metric. It corresponds to the average of the
    squares of the errors and can be defined as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit()` 方法的主要变化是 `F.mse_loss()` 函数，它取代了用于分类任务的交叉熵损失。**均方误差**（**MSE**）将成为我们的主要评估指标。它对应于误差的平方的平均值，可以定义如下：'
- en: '![](img/Formula_B19153_06_038.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_038.jpg)'
- en: 'In code, this is how it is implemented:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码中，具体实现如下：
- en: '[PRE27]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The MSE is also included in the `test()` method:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MSE 也包含在 `test()` 方法中：
- en: '[PRE28]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We instantiate the model with `128` hidden dimensions and only `1` output dimension
    (the target value). It is trained on `200` epochs:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化模型，设置 `128` 个隐藏维度，并且只有 `1` 个输出维度（目标值）。模型将在 `200` 个周期内进行训练：
- en: '[PRE29]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We test it to obtain the MSE on the test set:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在测试集上测试模型，计算 MSE：
- en: '[PRE30]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This MSE loss is not the most interpretable metric by itself. We can get more
    meaningful results using the two following metrics:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 MSE 损失本身并不是最容易解释的指标。我们可以使用以下两个指标来获得更有意义的结果：
- en: 'The RMSE, which measures the average magnitude of the error:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE 衡量的是误差的平均大小：
- en: '![](img/Formula_B19153_06_039.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_06_039.jpg)'
- en: The **Mean Absolute Error** (**MAE**), which gives the mean absolute difference
    between the predicted and real values:![](img/Formula_B19153_06_040.jpg)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**），它给出了预测值与实际值之间的平均绝对差异：![](img/Formula_B19153_06_040.jpg)'
- en: 'Let’s implement them in Python:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Python 中一步步实现：
- en: 'We can directly import the MSE and the MAE from the scikit-learn library:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以直接从 scikit-learn 库中导入 MSE 和 MAE：
- en: '[PRE31]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We convert the PyTorch tensors for the predictions into the NumPy arrays given
    by the model using `.detach().numpy()`:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过 `.detach().numpy()` 将 PyTorch 张量的预测结果转换为模型给出的 NumPy 数组：
- en: '[PRE32]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We compute the MSE and the MAE with their dedicated function. The RMSE is calculated
    as the square root of MSE using `np.sqrt()`:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用专用函数计算 MSE 和 MAE。RMSE 通过 `np.sqrt()` 计算 MSE 的平方根：
- en: '[PRE33]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: These metrics are useful for comparing different models, but it can be difficult
    to interpret the MSE and the RMSE.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估指标对于比较不同模型非常有用，但解释 MSE 和 RMSE 可能比较困难。
- en: 'The best tool to visualize the results of our model is a scatter plot, where
    the horizontal axis represents our predictions and the vertical axis represents
    the real values. Seaborn has a dedicated function (`regplot()`) for this type
    of visualization:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化我们模型结果的最佳工具是散点图，其中横轴表示我们的预测值，纵轴表示真实值。Seaborn 提供了一个专用函数（`regplot()`）来进行这种类型的可视化：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Figure 6.7 – Ground truth test values (x-axis) versus. predicted test values
    (y-axis)](img/B19153_06_007.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 实际测试值（x 轴）与预测测试值（y 轴）](img/B19153_06_007.jpg)'
- en: Figure 6.7 – Ground truth test values (x-axis) versus. predicted test values
    (y-axis)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 实际测试值（x 轴）与预测测试值（y 轴）
- en: We don’t have a baseline to work with in this example, but this is a decent
    prediction with few outliers. It would work in a lot of applications, despite
    a minimalist dataset. If we wanted to improve these results, we could tune the
    hyperparameters and do more error analysis to understand where the outliers come
    from.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中我们没有基准线可以参考，但这是一个相当不错的预测，只有少量的异常值。尽管数据集较为简约，但在许多应用中都会有效。如果我们想改善这些结果，可以调整超参数并进行更多的错误分析，以了解异常值的来源。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we improved our vanilla GNN layer to correctly normalize features.
    This enhancement introduced the GCN layer and smart normalization. We compared
    this new architecture to Node2Vec and our vanilla GNN on the Cora and Facebook
    Page-Page datasets. Thanks to this normalization process, the GCN obtained the
    highest accuracy scores by a large margin in both cases. Finally, we applied it
    to node regression with the Wikipedia Network and learned how to handle this new
    task.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们改进了我们的基础 GNN 层，以正确归一化特征。这个改进引入了 GCN 层和智能归一化。我们将这种新架构与 Node2Vec 和我们基础的
    GNN 在 Cora 和 Facebook Page-Page 数据集上进行了比较。由于这一归一化过程，GCN 在这两种情况下都以较大优势获得了最高的准确率。最后，我们将其应用于
    Wikipedia 网络的节点回归，并学习如何处理这一新任务。
- en: In [*Chapter 7*](B19153_07.xhtml#_idTextAnchor082), *Graph Attention Networks*,
    we will go a step further by discriminating neighboring nodes based on their importance.
    We will see how to automatically weigh node features through a process called
    self-attention. This will improve our performance, as we will see by comparing
    it to the GCN architecture.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B19153_07.xhtml#_idTextAnchor082)，*图注意力网络*中，我们将进一步发展，通过根据邻居节点的重要性来进行区分。我们将看到如何通过一种叫做自注意力的过程自动地对节点特征进行加权。这将提高我们的性能，正如我们将通过与
    GCN 架构的比较看到的那样。
- en: Further reading
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[1] T. N. Kipf and M. Welling, *Semi-Supervised Classification with Graph Convolutional
    Networks*. arXiv, 2016\. DOI: 10.48550/ARXIV.1609.02907\. Available: [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. N. Kipf 和 M. Welling，*图卷积网络的半监督分类*。arXiv，2016年。DOI: 10.48550/ARXIV.1609.02907。可用：[https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)。'
- en: '[2] B. Rozemberczki, C. Allen, and R. Sarkar, *Multi-scale Attributed Node
    Embedding*. arXiv, 2019\. DOI: 10.48550/ARXIV.1909.13021\. Available: [https://arxiv.org/abs/1909.13021](https://arxiv.org/abs/1909.13021).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Rozemberczki, C. Allen 和 R. Sarkar，*多尺度属性节点嵌入*。arXiv，2019年。DOI: 10.48550/ARXIV.1909.13021。可用：[https://arxiv.org/abs/1909.13021](https://arxiv.org/abs/1909.13021)。'
