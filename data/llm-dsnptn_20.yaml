- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '20'
- en: Chain-of-Thought Prompting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维链提示
- en: '**Chain-of-thought** (**CoT**) **prompting** originated from a research paper
    titled *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*,
    published by Google researchers Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
    Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou in 2022.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**思维链**（**CoT**）**提示**起源于一篇名为《思维链提示引发大型语言模型中的推理》的研究论文，该论文由谷歌研究人员Jason Wei、Xuezhi
    Wang、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei Xia、Ed Chi、Quoc Le和Denny Zhou于2022年发表。'
- en: The key innovation of CoT prompting was encouraging language models to break
    down complex reasoning problems into intermediate steps before arriving at a final
    answer. This was done by including demonstrations where the model is shown examples
    of step-by-step reasoning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: CoT提示的关键创新在于鼓励语言模型在得出最终答案之前将复杂的推理问题分解成中间步骤。这是通过包含模型逐步推理的示例来实现的。
- en: The researchers demonstrated that by prompting LLMs with a few examples of reasoning
    chains (such as “Let’s think step by step”), the models could significantly improve
    their performance on complex tasks requiring multi-step reasoning, such as arithmetic,
    commonsense, and symbolic reasoning problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员证明了通过用几个推理链的示例（例如“让我们一步步思考”）提示LLM，模型可以显著提高其在需要多步推理的复杂任务上的性能，例如算术、常识和符号推理问题。
- en: Before CoT, most prompting techniques focused on getting direct answers. CoT
    showed that explicitly encouraging models to demonstrate their reasoning process
    led to more accurate results, especially for problems requiring several logical
    steps. CoT is beneficial in promoting transparency and ensuring accuracy by guiding
    the model through logical steps, whereas direct answering, while quicker, can
    miss intermediate steps that could clarify or validate the reasoning behind the
    answer.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在CoT之前，大多数提示技术都集中在获取直接答案上。CoT表明，明确鼓励模型展示其推理过程可以导致更准确的结果，尤其是在需要多个逻辑步骤的问题上。CoT通过引导模型通过逻辑步骤来促进透明度和确保准确性，而直接回答虽然更快，但可能会错过澄清或验证答案背后推理的中间步骤。
- en: This research was particularly significant because it showed that reasoning
    abilities could emerge primarily through scale and prompting rather than requiring
    architectural changes to the models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究特别有意义，因为它表明推理能力主要通过规模和提示而不是需要改变模型架构来产生。
- en: In this chapter, you’ll learn to leverage CoT prompting to improve your LLM’s
    performance on complex reasoning tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何利用CoT提示来提高你的LLM在复杂推理任务上的性能。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Designing effective CoT prompts
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计有效的CoT提示
- en: Using CoT prompting for problem solving
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CoT提示进行问题解决
- en: Combining CoT prompting with other techniques
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将CoT提示与其他技术相结合
- en: Evaluating CoT prompting outputs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估CoT提示输出
- en: Limitations of CoT prompting
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoT提示的局限性
- en: Future directions
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来方向
- en: Designing effective CoT prompts
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计有效的CoT提示
- en: 'The process of creating effective CoT prompts helps in fostering clarity, logical
    progression, and structured reasoning, which in turn ensures more accurate and
    coherent outputs. By providing a well-defined problem statement, breaking the
    task into smaller steps, using explicit markers to guide the reasoning, and including
    a sample CoT response, the model is better equipped to follow a systematic approach
    that aligns with human problem-solving methods, leading to clear and rational
    conclusions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 创建有效CoT提示的过程有助于培养清晰度、逻辑进展和结构化推理，从而确保更准确和连贯的输出。通过提供明确的问题陈述，将任务分解成更小的步骤，使用明确的标记来引导推理，以及包括一个样本CoT响应，模型将更好地遵循与人类问题解决方法相一致的系统方法，从而得出清晰和理性的结论：
- en: '**Provide a clear problem statement**: A precise problem statement directs
    the reasoning toward a specific goal, eliminating ambiguity and ensuring that
    the model understands exactly what is being asked. This helps prevent misinterpretations
    and guides the entire reasoning process in the right direction.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提供明确的问题陈述**：精确的问题陈述将推理引导到特定的目标，消除歧义并确保模型确切地理解被要求做什么。这有助于防止误解并引导整个推理过程走向正确的方向。'
- en: '**Break down the problem into logical steps**: Dividing a complex task into
    smaller, manageable steps helps in organizing the reasoning and makes the overall
    problem easier to tackle. This breakdown aids in focusing on one aspect at a time,
    promoting clarity and reducing the risk of missing important details.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将问题分解为逻辑步骤**：将复杂任务分解为更小、更易管理的步骤有助于组织推理，并使整体问题更容易解决。这种分解有助于一次关注一个方面，提高清晰度并降低遗漏重要细节的风险。'
- en: '**Use explicit reasoning markers**: Markers such as “First,” “Next,” and “Finally”
    act as signposts for the logical flow of the reasoning process. They help structure
    the thought process in a clear sequence, ensuring that each part of the problem
    is addressed in the right order, which increases the overall coherence of the
    response.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用明确的推理标记**：例如“首先”、“接下来”和“最后”等标记作为推理过程逻辑流的标志。它们有助于以清晰的顺序结构化思维过程，确保问题各部分按正确顺序解决，从而提高整体回答的连贯性。'
- en: '**Include a sample CoT response in the prompt**: Providing an example helps
    establish a standard for the reasoning format and sets clear expectations for
    the process. It also serves as a reference point, guiding the model in how to
    structure its response and making it easier to generate consistent and logically
    sound outputs.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在提示中包含一个CoT示例响应**：提供示例有助于建立推理格式的标准，并为过程设定明确的期望。它还作为参考点，指导模型如何构建其响应，并使其更容易生成一致且逻辑上合理的输出。'
- en: 'Here’s an example of implementing a CoT prompt:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个实现CoT提示的示例：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This function generates a CoT prompt for a given problem (`If a train travels
    120 km in 2 hours, what is its average speed in km/h?`), providing a structure
    for step-by-step reasoning. Here are the sample steps using CoT:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数为给定的问题（`如果一列火车以2小时行驶120公里，其平均速度是多少km/h？`）生成一个CoT提示，提供逐步推理的结构。以下是使用CoT的样本步骤：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Therefore, the answer is 60 km/h.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，答案是60 km/h。
- en: CoT prompting can be applied to various problem-solving scenarios. Let’s see
    one such scenario next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CoT提示可以应用于各种问题解决场景。让我们看看下一个场景。
- en: Using CoT prompting for problem solving
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CoT提示进行问题解决
- en: 'Let’s implement a function that uses CoT for mathematical word problems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个使用CoT解决数学文字问题的函数：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This function applies CoT prompting to solve a mathematical word problem (for
    example, `If a recipe calls for 2 cups of flour for 8 servings, how many cups
    of flour are needed for 12 servings?`), guiding the LLM through a step-by-step
    reasoning process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应用CoT提示来解决数学文字问题（例如，`如果一份食谱需要2杯面粉制作8份，那么制作12份需要多少杯面粉？`），引导LLM通过逐步推理过程。
- en: In addition to using CoT prompting for problem solving, we can also combine
    it with other techniques to improve LLM performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用CoT提示进行问题解决外，我们还可以将其与其他技术结合，以提高LLM的性能。
- en: Combining CoT prompting with other techniques
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将CoT提示与其他技术结合
- en: 'CoT can be combined with other prompting techniques to further enhance LLM
    performance. Let’s implement a function that combines CoT with **few-shot** **learning**
    (**FSL**):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CoT可以与其他提示技术结合，以进一步提高LLM的性能。让我们实现一个结合CoT与**少样本学习**（**FSL**）的函数：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function combines FSL with CoT prompting, providing examples of step-by-step
    solutions to guide the LLM in solving a new problem (see the code example for
    `If a train travels 180 km in 3 hours, what is its average speed in km/h?`). Combining
    methods such as CoT + FSL has been shown to improve performance in recent benchmarks
    ([https://aclanthology.org/2023.emnlp-main.782.pdf](https://aclanthology.org/2023.emnlp-main.782.pdf)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数结合FSL与CoT提示，提供逐步解决方案的示例，以指导LLM解决新问题（请参阅`如果一列火车以3小时行驶180公里，其平均速度是多少km/h？`的代码示例）。最近的研究表明，将CoT
    + FSL等方法相结合可以提高基准测试中的性能（[https://aclanthology.org/2023.emnlp-main.782.pdf](https://aclanthology.org/2023.emnlp-main.782.pdf)）。
- en: Next, let’s see how we can evaluate the quality of CoT prompts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何评估CoT提示的质量。
- en: Evaluating CoT prompting outputs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估CoT提示输出
- en: 'Evaluating the outputs of CoT prompts involves assessing both the final answer
    and the reasoning process. Let’s implement a simple evaluation function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 评估CoT提示的输出涉及评估最终答案和推理过程。让我们实现一个简单的评估函数：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This evaluation function assesses both the correctness of the final answer and
    the quality of the reasoning steps in the CoT output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此评估函数评估了CoT输出中最终答案的正确性和推理步骤的质量。
- en: Limitations of CoT prompting
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CoT提示的限制
- en: 'While CoT prompting is powerful, it has some limitations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CoT提示功能强大，但它也有一些限制：
- en: High token usage and computation time
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高token使用量和计算时间
- en: Potential for error propagation in multi-step reasoning
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多步推理中可能存在错误传播
- en: Dependence on the quality of the initial prompt
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于初始提示的质量
- en: May not be suitable for all types of problems
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不适用于所有类型的问题
- en: 'To address some of these limitations, consider implementing a dynamic CoT approach:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些局限性，考虑实施动态CoT方法：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `dynamic_cot` function implements a dynamic CoT approach to break down and
    solve a problem step by step using a language model. It starts by creating an
    initial prompt that introduces the problem and instructs the model to solve it
    incrementally. The function then enters a loop, iterating up to `max_steps` times
    (default is `5`), where in each iteration, it feeds the model a growing prompt
    that includes all the steps generated so far. The model processes this prompt,
    generates the next step in the reasoning process, and appends it to the prompt.
    The new step is decoded from tokenized outputs and added to the prompt string.
    The function checks for the phrase `Therefore, the final answer is` in the generated
    step, signaling that the model has reached a conclusion and should stop. If this
    phrase is found, the loop breaks early; otherwise, it continues until the maximum
    steps are reached. Finally, the function returns the complete prompt, which includes
    all the reasoning steps leading to the solution. However, in real-world use, token
    limitations of the model may impact long multi-step prompts. As the prompt grows
    with each new step, it might exceed the model’s maximum token limit, which could
    result in truncated inputs, loss of earlier context, or failure to generate accurate
    steps, especially in complex or lengthy problems. This is a significant consideration
    when dealing with problems requiring many steps or substantial context.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`dynamic_cot`函数实现了一种动态CoT方法，通过语言模型逐步分解和解决问题。它首先创建一个初始提示，介绍问题并指导模型逐步解决它。然后，函数进入一个循环，最多迭代`max_steps`次（默认为`5`），在每次迭代中，它向模型提供一个不断增长的提示，其中包括迄今为止生成的所有步骤。模型处理这个提示，生成推理过程中的下一步，并将其附加到提示中。新步骤从标记化输出中解码并添加到提示字符串中。函数检查生成的步骤中是否存在短语`Therefore,
    the final answer is`，这表明模型已得出结论并应停止。如果找到这个短语，循环提前中断；否则，它继续进行，直到达到最大步骤数。最后，函数返回完整的提示，其中包含导致解决方案的所有推理步骤。然而，在实际应用中，模型的token限制可能会影响长多步提示。随着提示的每一步增长，它可能会超过模型的最大token限制，这可能导致输入截断、早期上下文丢失或无法生成准确的步骤，尤其是在复杂或长问题中。当处理需要许多步骤或大量上下文的问题时，这是一个重要的考虑因素。'
- en: Future directions
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来方向
- en: 'As CoT prompting continues to evolve, several promising directions emerge:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 随着CoT提示的持续发展，出现了一些有希望的方向：
- en: '**Adaptive CoT**: Dynamically adjusting the reasoning process based on problem
    complexity'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应CoT**：根据问题复杂度动态调整推理过程'
- en: '**Multi-modal CoT**: Incorporating visual or auditory information in the reasoning
    process ([https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923))'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态CoT**：在推理过程中结合视觉或听觉信息([https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923))'
- en: '**Collaborative CoT**: Combining insights from multiple LLMs or human-AI collaboration
    ([https://arxiv.org/html/2409.07355v1](https://arxiv.org/html/2409.07355v1))'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协作CoT**：结合多个LLM的见解或人机协作([https://arxiv.org/html/2409.07355v1](https://arxiv.org/html/2409.07355v1))'
- en: '**Meta-learning for CoT**: Meta-learning and CoT approaches have emerged as
    powerful techniques for addressing the challenges of few-shot relation extraction
    ([https://arxiv.org/abs/2311.05922](https://arxiv.org/abs/2311.05922))'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CoT的元学习**：元学习和CoT方法已成为解决少样本关系抽取挑战的有力技术([https://arxiv.org/abs/2311.05922](https://arxiv.org/abs/2311.05922))'
- en: 'Here’s a conceptual implementation of adaptive CoT:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种自适应CoT的概念性实现：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This adaptive CoT approach assesses problem complexity and chooses an appropriate
    solving strategy, balancing efficiency and reasoning depth.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自适应CoT方法评估问题复杂度并选择合适的解决策略，平衡效率和推理深度。
- en: The `adaptive_cot` function adapts the CoT approach based on the complexity
    of the problem. It first assesses the problem’s complexity by calling the `assess_problem_complexity`
    function, which could involve analyzing keywords, sentence structure, or other
    features to determine how complex the problem is (though the logic for this is
    yet to be implemented). If the complexity score exceeds a predefined threshold
    (`complexity_threshold`), the function uses a detailed CoT approach via the `detailed_cot`
    function, which would generate a more elaborate, step-by-step solution. For simpler
    problems, it uses a straightforward solving method via the `simple_solve` function,
    which provides a direct answer without breaking down the problem into multiple
    steps. The result is returned based on which approach is deemed appropriate for
    the given problem. This dynamic approach allows the model to choose the most efficient
    method of solving a problem based on its complexity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`adaptive_cot`函数根据问题的复杂性调整CoT方法。它首先通过调用`assess_problem_complexity`函数评估问题的复杂性，这可能涉及分析关键词、句子结构或其他特征以确定问题的复杂程度（尽管这一逻辑尚未实现）。如果复杂性评分超过预定义的阈值（`complexity_threshold`），则函数通过`detailed_cot`函数使用详细的CoT方法，这将生成更详细、分步骤的解决方案。对于简单问题，它通过`simple_solve`函数使用直接解决方法，该函数提供直接答案而不将问题分解成多个步骤。结果基于哪种方法被认为适用于给定问题而返回。这种动态方法允许模型根据其复杂性选择解决问题的最有效方法。'
- en: Summary
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to design effective CoT prompts that guide
    LLMs through step-by-step reasoning processes. We covered applications of this
    technique in various problem-solving scenarios and discussed how to combine it
    with other prompting strategies. You also learned how to evaluate the quality
    of CoT outputs and understood the limitations of this approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何设计有效的CoT提示，引导LLM通过逐步推理过程。我们讨论了该技术在各种问题解决场景中的应用，并讨论了如何将其与其他提示策略相结合。您还学习了如何评估CoT输出的质量，并理解了这种方法的优势。
- en: By implementing the strategies and considerations discussed in this chapter,
    you can significantly improve your LLM’s performance on complex problem-solving
    tasks, while also gaining insights into the model’s reasoning process.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施本章讨论的策略和考虑因素，您可以显著提高您的LLM在复杂问题解决任务上的性能，同时深入了解模型的推理过程。
- en: In the next chapter, we will investigate **tree-of-thoughts** (**ToT**) prompting,
    an advanced technique that extends the concepts of CoT to create even more sophisticated
    reasoning structures.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨**思维树（ToT**）提示，这是一种高级技术，它扩展了CoT的概念，以创建更加复杂的推理结构。
