- en: '*Chapter 9:* Convolutional Neural Networks for Image Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第九章:* 卷积神经网络在图像分类中的应用'
- en: In the previous chapters, we talked about **Recurrent Neural Networks** (**RNNs**)
    and how they can be applied to different types of sequential data and use cases.
    In this chapter, we want to talk about another family of neural networks, called
    **Convolutional Neural Networks** (**CNNs**). CNNs are especially powerful when
    used on data with grid-like topology and spatial dependencies, such as images
    or videos.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了**递归神经网络**（**RNN**）及其如何应用于不同类型的序列数据和使用场景。本章我们将讨论另一类神经网络——**卷积神经网络**（**CNN**）。当应用于具有网格状拓扑和空间依赖的数据时，如图像或视频，CNN尤其强大。
- en: We will start with a general introduction to CNNs, explaining the basic idea
    behind a convolution layer and introducing some related terminology such as padding,
    pooling, filters, and stride.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先进行CNN的基本介绍，解释卷积层背后的基本理念，并介绍一些相关术语，如填充、池化、滤波器和步幅。
- en: 'Afterward, we will build and train a CNN for image classification from scratch.
    We will cover all required steps: from reading and preprocessing of the images
    to defining, training, and applying the CNN.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将从零开始构建并训练一个用于图像分类的CNN。我们将涵盖所有必要的步骤：从读取和预处理图像到定义、训练和应用CNN。
- en: To train a neural network from scratch, a huge amount of labeled data is usually
    required. For some specific domains, such as images or videos, such a large amount
    of data might not be available, and the training of a network might become impossible.
    Transfer learning is a proposed solution to handle this problem. The idea behind
    transfer learning consists of using a state-of-the-art neural network trained
    for a task A as a starting point for another, related, task B.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要从零开始训练一个神经网络，通常需要大量的标注数据。对于一些特定领域，如图像或视频，可能没有足够的数据，网络训练可能变得不可行。迁移学习是为了解决这个问题而提出的解决方案。迁移学习的理念是利用已经为任务A训练好的最先进神经网络作为任务B的起点，任务B是与A相关的另一个任务。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to CNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs简介
- en: Classifying Images with CNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN进行图像分类
- en: Introduction to Transfer Learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习简介
- en: Applying Transfer Learning for Cancer Type Prediction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用迁移学习进行癌症类型预测
- en: Introduction to CNNs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs简介
- en: CNNs are commonly used in image processing and have been the winning models
    in several image-processing competitions. They are often used, for example, for
    image classification, object detection, and semantic segmentation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 常用于图像处理，并且在多个图像处理竞赛中获得了优胜。它们通常用于图像分类、物体检测和语义分割等任务。
- en: 'Sometimes, CNNs are also used for non-image-related tasks, such as recommendation
    systems, videos, or time-series analysis. Indeed, CNNs are not only applied to
    two-dimensional data with a grid structure but can also work when applied to one-
    or three-dimensional data. In this chapter, however, we focus on the most common
    CNN application area: **image processing**.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，CNN 也用于与图像无关的任务，如推荐系统、视频或时间序列分析。事实上，CNN不仅适用于具有网格结构的二维数据，还可以在一维或三维数据中应用。然而，本章我们将专注于CNN最常见的应用领域：**图像处理**。
- en: A CNN is a neural network with at least one **convolution layer**. As the name
    states, convolution layers perform a convolution mathematical transformation on
    the input data. Through such a mathematical transformation, convolution layers
    acquire the ability to detect and extract a number of features from an image,
    such as edges, corners, and shapes. Combinations of such extracted features are
    used to classify images or to detect specific objects within an image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是一种至少包含一个**卷积层**的神经网络。顾名思义，卷积层对输入数据执行卷积数学变换。通过这种数学变换，卷积层获得了从图像中检测和提取多个特征的能力，如边缘、角落和形状。这些提取特征的组合被用于对图像进行分类或检测图像中的特定物体。
- en: A convolution layer is often found together with a **pooling layer**, also commonly
    used in the feature extraction part of image processing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层通常与**池化层**一起出现，池化层也常用于图像处理中的特征提取部分。
- en: The goal of this section is thus to explain how convolution layers and pooling
    layers work separately and together and to detail the different setting options
    for the two layers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是解释卷积层和池化层如何单独以及共同工作，并详细说明这两种层的不同设置选项。
- en: As mentioned, in this chapter we will focus on CNNs for image analysis. So,
    before we dive into the details of CNNs, let’s quickly review how images are stored.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在本章中我们将专注于图像分析的卷积神经网络（CNN）。因此，在深入探讨CNN的细节之前，让我们快速回顾一下图像是如何存储的。
- en: How are Images Stored?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像是如何存储的？
- en: A grayscale image can be stored as a matrix, where each cell represents one
    pixel of the image and the cell value represents the gray level of the pixel.
    For example, a black and white image, with size ![](img/Formula_B16391_09_001.png)
    pixels, can be represented as a matrix with dimensions ![](img/Formula_B16391_09_002.png),
    where each value of the matrix ranges between ![](img/Formula_B16391_09_003.png)
    and ![](img/Formula_B16391_09_004.png). ![](img/Formula_B16391_09_005.png) is
    a black pixel, ![](img/Formula_B16391_09_006.png) is a white pixel, and a value
    in between corresponds to a level of gray in the grayscale.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 灰度图像可以存储为矩阵，其中每个单元格表示图像的一个像素，单元格的值表示该像素的灰度级。例如，一张黑白图像，大小为![](img/Formula_B16391_09_001.png)像素，可以表示为维度为![](img/Formula_B16391_09_002.png)的矩阵，其中矩阵中的每个值介于![](img/Formula_B16391_09_003.png)和![](img/Formula_B16391_09_004.png)之间。![](img/Formula_B16391_09_005.png)表示一个黑色像素，![](img/Formula_B16391_09_006.png)表示一个白色像素，矩阵中的中间值表示灰度级。
- en: '*Figure 9.1* here depicts an example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.1*这里展示了一个例子：'
- en: '![Figure 9.1 – Matrix representation of a grayscale 5 x 5 image](img/B16391_09_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 灰度5 x 5图像的矩阵表示](img/B16391_09_001.jpg)'
- en: Figure 9.1 – Matrix representation of a grayscale 5 x 5 image
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 灰度5 x 5图像的矩阵表示
- en: 'As each pixel is represented by one gray value only, one **channel** (matrix)
    is sufficient to represent this image. For color images, on the other hand, more
    than one value is needed to define the color of each pixel. One option is to use
    the three values specifying the intensity of red, green, and blue to define the
    pixel color. In the following screenshot, to represent a color image, three channels
    are used instead of one: (*Figure 9.2*):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个像素仅由一个灰度值表示，一个**通道**（矩阵）就足以表示这张图像。另一方面，对于彩色图像，需要多个值来定义每个像素的颜色。一种选择是使用三个值来指定红色、绿色和蓝色的强度，从而定义像素颜色。在以下截图中，表示彩色图像时，使用了三个通道而不是一个：(*图9.2*)：
- en: '![Figure 9.2 – Representing a 28 x 28 color image using three channels for
    RGB](img/B16391_09_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 使用三个通道表示28 x 28彩色图像（RGB）](img/B16391_09_002.jpg)'
- en: Figure 9.2 – Representing a 28 x 28 color image using three channels for RGB
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 使用三个通道表示28 x 28彩色图像（RGB）
- en: Moving from a grayscale image to a **red, green, and blue** (**RGB**) image,
    the more general concept of **tensor**—instead of a simple matrix—becomes necessary.
    In this way, the grayscale image can be described as a tensor of ![](img/Formula_B16391_09_007.png),
    while a color image with ![](img/Formula_B16391_09_008.png) pixels can be represented
    with a ![](img/Formula_B16391_09_009.png) tensor.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从灰度图像到**红色、绿色和蓝色**（**RGB**）图像时，更为通用的**张量**概念——而不是简单的矩阵——变得必不可少。这样，灰度图像可以被描述为![](img/Formula_B16391_09_007.png)的张量，而具有![](img/Formula_B16391_09_008.png)像素的彩色图像则可以用一个![](img/Formula_B16391_09_009.png)张量表示。
- en: In general, a tensor representing an image with ![](img/Formula_B16391_09_010.png)
    pixels height, ![](img/Formula_B16391_09_011.png) pixels width, and ![](img/Formula_B16391_09_012.png)
    channels has the dimension ![](img/Formula_B16391_09_013.png) x ![](img/Formula_B16391_09_014.png)
    x ![](img/Formula_B16391_09_015.png).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，一个表示图像的张量，其高度为![](img/Formula_B16391_09_010.png)像素，宽度为![](img/Formula_B16391_09_011.png)像素，通道数为![](img/Formula_B16391_09_012.png)，其维度为![](img/Formula_B16391_09_013.png)
    x ![](img/Formula_B16391_09_014.png) x ![](img/Formula_B16391_09_015.png)。
- en: But why do we need special networks to analyze images? Couldn’t we just **flatten**
    the image, represent each image as a long vector, and train a standard fully connected
    feedforward neural network?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们需要专门的网络来分析图像呢？我们不能只是**展平**图像，将每个图像表示为一个长向量，然后训练一个标准的全连接前馈神经网络吗？
- en: Important note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The process of transforming a matrix representation of an image into a vector
    is called **flattening**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像的矩阵表示转换为向量的过程称为**展平**。
- en: Why do we need CNNs?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们需要卷积神经网络（CNN）？
- en: For basic binary images, flattening and fully connected feedforward networks
    might yield acceptable performance. However, with more complex images, with strong
    pixel dependencies throughout the image, the combination of flattening and feedforward
    neural networks usually fails.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基本的二值图像，展平和全连接前馈网络可能会得到可接受的性能。然而，对于更复杂的图像，图像中的像素依赖性较强，展平和前馈神经网络的组合通常无法有效工作。
- en: Indeed, the spatial dependency is lost when the image is flattened into a vector.
    As a result, fully connected feedforward networks are not translation-invariant.
    This means that they produce different results for shifted versions of the same
    image. For example, a network might learn to identify a cat in the upper-left
    corner of an image, but the same network is not able to detect a cat in the lower-right
    corner of the same image.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，当图像被展平为一个向量时，空间依赖性会丧失。因此，完全连接的前馈网络不是平移不变的。这意味着它们对于同一图像的不同位移版本会产生不同的结果。例如，网络可能会学习在图像的左上角识别猫，但同一个网络无法检测图像右下角的猫。
- en: In addition, the flattening of an image produces a very long vector, and therefore
    it requires a very large fully connected feedforward network with many weights.
    For example, for a ![](img/Formula_B16391_09_016.png) pixel image with three channels,
    the network needs ![](img/Formula_B16391_09_017.png) inputs. If the next layer
    has ![](img/Formula_B16391_09_018.png) neurons, we would need to train ![](img/Formula_B16391_09_019.png)
    weights only in the first layer. You see that the number of weights can quickly
    become unmanageable, likely leading to overfitting during training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，展平图像会生成一个非常长的向量，因此它需要一个非常大的完全连接的前馈网络，包含大量权重。例如，对于一个![](img/Formula_B16391_09_016.png)像素、三个通道的图像，网络需要![](img/Formula_B16391_09_017.png)个输入。如果下一层有![](img/Formula_B16391_09_018.png)个神经元，我们将仅在第一层训练![](img/Formula_B16391_09_019.png)个权重。你会发现，权重的数量可能会迅速变得难以管理，导致训练过程中出现过拟合。
- en: Convolution layers, which are the main building block of a CNN, allow us to
    solve this problem by exploiting the spatial properties of the image. So, let’s
    find out how a convolution layer works.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是CNN的主要构建块，它利用图像的空间属性来解决这个问题。那么，让我们来看看卷积层是如何工作的。
- en: How does a Convolution Layer work?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层是如何工作的？
- en: The idea of CNNs is to use filters to detect patterns—also called features—such
    as corners, vertical edges, and horizontal edges, in different parts of an image.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的思路是使用滤波器来检测图像不同部分的模式——也叫特征——如角落、垂直边缘和水平边缘。
- en: For an image with one channel a **filter** is a small matrix, often of size
    ![](img/Formula_B16391_09_020.png) or ![](img/Formula_B16391_09_021.png), called
    a **kernel**. Different kernels—that is, matrices with different values—filter
    different patterns. A kernel moves across an image and performs a convolution
    operation. That convolution operation gives a name to the layer. The output of
    such a convolution is called a **feature map**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个单通道图像，**滤波器**是一个小矩阵，通常大小为![](img/Formula_B16391_09_020.png)或![](img/Formula_B16391_09_021.png)，称为**卷积核**。不同的卷积核——即具有不同数值的矩阵——过滤不同的模式。卷积核在图像上滑动并执行卷积操作。这个卷积操作赋予了该层一个名称。这样的卷积输出称为**特征图**。
- en: Important note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: For an input image with three channels (for example, an input tensor with shape
    ![](img/Formula_B16391_09_022.png)), a kernel with kernel size 2 has the shape
    ![](img/Formula_B16391_09_023.png). This means the kernel can incorporate information
    from all channels but only within a small (2 x 2, in this example) region of the
    input image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具有三个通道的输入图像（例如，输入张量形状为![](img/Formula_B16391_09_022.png)），大小为2的卷积核的形状为![](img/Formula_B16391_09_023.png)。这意味着卷积核可以从所有通道获取信息，但仅限于输入图像的一个小区域（例如，在本例中为2
    x 2）。
- en: '*Figure 9.3* here shows an example of how a convolution is calculated for an
    image of size ![](img/Formula_B16391_09_024.png) and a kernel with size ![](img/Formula_B16391_09_025.png):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.3* 这里展示了如何计算卷积，图像大小为![](img/Formula_B16391_09_024.png)，卷积核大小为![](img/Formula_B16391_09_025.png)：'
- en: '![Figure 9.3 – Example of a convolution obtained by applying a 3 x 3 kernel
    to a 4 x 4 image](img/B16391_09_003.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 通过将一个3 x 3的卷积核应用于一个4 x 4图像得到的卷积示例](img/B16391_09_003.jpg)'
- en: Figure 9.3 – Example of a convolution obtained by applying a 3 x 3 kernel to
    a 4 x 4 image
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 通过将一个3 x 3的卷积核应用于一个4 x 4图像得到的卷积示例
- en: 'In this example, we start by applying the kernel to the upper-left ![](img/Formula_B16391_09_026.png)
    region of the image. The image values are elementwise multiplied with the kernel
    values and then summed up, as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先将卷积核应用于图像的左上角![](img/Formula_B16391_09_026.png)区域。图像的值与卷积核的值按元素相乘，然后求和，计算过程如下：
- en: '![](img/Formula_B16391_09_027.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_09_027.png)'
- en: The result of this elementwise multiplication and sum is the first value, in
    the upper-left corner, in the output feature map. The kernel is then moved across
    the whole image to calculate all other values of the output feature map.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逐元素相乘并求和的结果就是输出特征图左上角的第一个值。接着，卷积核会在整张图像上滑动，计算输出特征图的所有其他值。
- en: Important note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The convolution operation is denoted with a * and is different from a matrix
    multiplication. Even though the layer is called convolution, most neural network
    libraries actually implement a related function called **cross-correlation**.
    To perform a correct convolution, according to its mathematical definition, the
    kernel in addition must be flipped. For CNNs this doesn’t make a difference because
    the weights are learned anyway.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作用*表示，且与矩阵乘法不同。尽管这一层叫做卷积层，但大多数神经网络库实际上实现的是一个相关的函数，叫做**交叉相关（cross-correlation）**。为了执行正确的卷积，按照其数学定义，核还必须翻转。对于卷积神经网络（CNN），这并不影响结果，因为权重无论如何都会被学习。
- en: In a convolution layer, a large number of filters (kernels) are trained in parallel
    on the input dataset and for the required task. That is, the weights in the kernel
    are not set manually but are adjusted automatically as weights during the network
    training procedure. During execution, all trained kernels are applied to calculate
    the feature map.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层中，大量的滤波器（核）会并行训练输入数据集，并针对所需任务进行优化。也就是说，核中的权重不是手动设置的，而是在网络训练过程中作为权重自动调整。在执行时，所有训练过的核都会应用于计算特征图。
- en: The dimension of the feature map is then a tensor of size ![](img/Formula_B16391_09_028.png).
    In the example in *Figure 9.3*, we applied only one kernel, and the dimension
    of the feature map is ![](img/Formula_B16391_09_029.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图的维度随后变成一个大小为![](img/Formula_B16391_09_028.png)的张量。在*图 9.3* 的示例中，我们只应用了一个核，特征图的维度为![](img/Formula_B16391_09_029.png)。
- en: 'Historically, kernels were designed manually for selected tasks. For example,
    the kernel in *Figure 9.3* detects vertical lines. *Figure 9.4* here shows you
    the impact of some other handcrafted kernels:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，核是手工设计的，用于特定任务。例如，*图 9.3* 中的核用于检测垂直线。*图 9.4* 显示了其他一些手工设计的核的影响：
- en: '![Figure 9.4 – Impact of some hand-crafted kernels on the original image](img/B16391_09_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 一些手工设计的核对原始图像的影响](img/B16391_09_004.jpg)'
- en: Figure 9.4 – Impact of some hand-crafted kernels on the original image
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 一些手工设计的核对原始图像的影响
- en: The convolution operation is just a part of the convolution layer. After that,
    a bias and a non-linear activation function are applied to each entry in the feature
    map. For example, we can add a bias value to each value in the feature map and
    then apply **rectified liner unit** (**ReLU**) as an activation function to set
    all values below the bias to 0.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作只是卷积层的一部分。之后，偏置值和非线性激活函数会应用于特征图中的每个条目。例如，我们可以向特征图中的每个值添加偏置值，然后应用**整流线性单元（ReLU）**作为激活函数，将所有小于偏置的值设置为
    0。
- en: Important note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073), *Getting Started
    with Neural Networks*, we introduced dense layers. In a dense layer, the weighted
    sum of the input is first calculated; then, a bias value is added to the sum,
    and the activation function is applied. In a convolutional layer, the weighted
    sum of the dense layer is replaced by the convolution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 3 章*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073)，*神经网络入门*一章中，我们介绍了密集层。在密集层中，首先计算输入的加权和；然后，将偏置值添加到和中，最后应用激活函数。在卷积层中，密集层的加权和被卷积所取代。
- en: 'A convolution layer has multiple setting options. We have already introduced
    three of them along the way, and they are listed here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层有多个设置选项。我们已经在前面的内容中介绍了其中三个，这里列出它们：
- en: The kernel size, which is often ![](img/Formula_B16391_09_030.png)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核的大小，通常是![](img/Formula_B16391_09_030.png)
- en: The number of filters
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器的数量
- en: The activation function, where ReLU is the one most commonly used
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数，其中ReLU是最常用的函数。
- en: 'There are three more setting options: padding, stride, and dilation rate. Let’s
    continue with padding.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有三个设置选项：填充（padding）、步幅（stride）和扩张率（dilation rate）。我们继续讨论填充（padding）。
- en: Introducing Padding
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入填充（Padding）
- en: When we applied the filter in the example in *Figure 9.3*, the dimension of
    the feature map shrunk compared to the dimension of the input image. The input
    image had a size of ![](img/Formula_B16391_09_031.png) and the feature map a size
    of ![](img/Formula_B16391_09_032.png).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在*图 9.3*中的示例中应用滤波器时，特征图的维度相比输入图像的维度有所缩小。输入图像的大小为 ![](img/Formula_B16391_09_031.png)，而特征图的大小为
    ![](img/Formula_B16391_09_032.png)。
- en: In addition, by looking at the feature map, we can see that pixels in the inner
    part of the input image (cells with values f, g, j, and k) are more often considered
    in the convolution than pixels at corners and borders. This implies that inner
    values will get a higher weight in further analysis. To overcome this issue, images
    can be zero-padded by adding zeros in additional external cells (*Figure 9.5*).
    This is a process called **padding**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过查看特征图，我们可以看到输入图像内部的像素（值为 f, g, j, k 的单元格）在卷积过程中比角落和边缘的像素更常被考虑。这意味着内部值在后续分析中将获得更高的权重。为了解决这个问题，可以通过在额外的外部单元格中添加零来对图像进行零填充（*图
    9.5*）。这就是所谓的**填充**过程。
- en: '*Figure 9.5* here shows you an example of a zero-padded input:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.5* 这里展示了一个零填充输入的示例：'
- en: '![Figure 9.5 – Example of a zero-padded image](img/B16391_09_005.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 零填充图像示例](img/B16391_09_005.jpg)'
- en: Figure 9.5 – Example of a zero-padded image
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 零填充图像示例
- en: Here, two cells with value zero have been added to each row and column, all
    around the original image. If a kernel of size ![](img/Formula_B16391_09_033.png)
    is now applied to this padded image, the output dimension of the feature map would
    be the same as the dimension of the original image. The number of cells to use
    for zero padding is one more setting available in convolution layers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，在原始图像的每一行和每一列周围都添加了两个零值单元格。如果现在对这个零填充的图像应用大小为 ![](img/Formula_B16391_09_033.png)
    的卷积核，特征图的输出维度将与原始图像的维度相同。用于零填充的单元格数量是卷积层中的另一个可配置参数。
- en: Two other settings that influence the output size, if no padding is used, are
    called **stride** and **dilation rate**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个影响输出大小的设置，如果不使用填充，分别叫做**步幅**和**膨胀率**。
- en: Introducing Stride and Dilation Rate
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍步幅和膨胀率
- en: In the example in *Figure 9.3*, we applied the filter to every pixel. For images
    of a large size, it is not always necessary to perform the convolution on every
    single pixel. Instead of always shifting the kernel by one pixel, we could shift
    it by more than one horizontal or vertical pixel.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.3*的示例中，我们将滤波器应用于每一个像素。对于大尺寸的图像，通常不需要对每个像素进行卷积。我们可以将卷积核按水平或垂直方向移动多个像素，而不是每次只移动一个像素。
- en: The number of pixels used for the kernel shift is called **stride**. The stride
    is normally defined by a tuple, specifying the number of cells for the shift in
    the horizontal and vertical direction. A higher stride value, without padding,
    leads to a downsampling of the input image.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用于卷积核滑动的像素数量称为**步幅**。步幅通常由一个元组定义，指定水平和垂直方向上滑动的单元格数量。较高的步幅值，在没有填充的情况下，会导致输入图像的下采样。
- en: The top part of *Figure 9.6* shows how a kernel of size 3 x 3 moves across an
    image with stride 2, 2.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.6* 的顶部部分展示了一个大小为 3 x 3 的卷积核如何以步幅 2, 2 在图像上滑动。'
- en: Another setting option for a convolution layer is the **dilation rate**. The
    dilation rate indicates that only one cell out of ![](img/Formula_B16391_09_034.png)
    consecutive cells in the input image is used for the convolution operation. A
    dilation rate of ![](img/Formula_B16391_09_035.png) uses only one every two pixels
    from the input image for the convolution. A dilation rate of ![](img/Formula_B16391_09_036.png)
    uses one of three consecutive pixels. As for the stride, a dilation rate is a
    tuple of values for the horizontal and vertical direction. When using a dilation
    rate higher than ![](img/Formula_B16391_09_037.png), the kernel gets dilated to
    a larger field of view on the original image. So, a 3 x 3 kernel with dilation
    rate ![](img/Formula_B16391_09_038.png) explores a field of view of size 5 x 5
    in the input image, while using only nine convolution parameters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的另一个设置选项是**膨胀率**。膨胀率表示输入图像中每 ![](img/Formula_B16391_09_034.png) 个连续单元格中只使用其中一个进行卷积操作。膨胀率为
    ![](img/Formula_B16391_09_035.png) 时，仅使用输入图像中的每两个像素进行卷积。膨胀率为 ![](img/Formula_B16391_09_036.png)
    时，使用三个连续像素中的一个。就像步幅一样，膨胀率是一个水平方向和垂直方向的值元组。当使用高于 ![](img/Formula_B16391_09_037.png)
    的膨胀率时，核会膨胀成在原始图像上更大的视野。因此，一个3 x 3的核，膨胀率为 ![](img/Formula_B16391_09_038.png) 时，探索输入图像中大小为5
    x 5的视野，同时仅使用九个卷积参数。
- en: 'For a ![](img/Formula_B16391_09_039.png) kernel and a dilation rate of ![](img/Formula_B16391_09_040.png),
    the kernel scans an area of ![](img/Formula_B16391_09_041.png) on the input image
    using only its corner values (see the lower part of *Figure 9.6*). This means
    for a dilation rate of ![](img/Formula_B16391_09_042.png), we have a gap of size
    1\. For a dilation rate of ![](img/Formula_B16391_09_043.png), we would have a
    gap size of 2, and so on:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 ![](img/Formula_B16391_09_039.png) 核心和一个膨胀率为 ![](img/Formula_B16391_09_040.png)
    的情况，核心仅使用其角值扫描输入图像上的 ![](img/Formula_B16391_09_041.png) 区域（见 *图9.6* 下部）。这意味着对于膨胀率为
    ![](img/Formula_B16391_09_042.png) 的情况，我们有一个大小为1的间隙；对于膨胀率为 ![](img/Formula_B16391_09_043.png)
    的情况，我们会有一个大小为2的间隙，依此类推：
- en: '![Figure 9.6 – Impact of different stride and dilation rate values on the output
    feature map](img/B16391_09_006.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 不同步幅和膨胀率值对输出特征图的影响](img/B16391_09_006.jpg)'
- en: Figure 9.6 – Impact of different stride and dilation rate values on the output
    feature map
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 不同步幅和膨胀率值对输出特征图的影响
- en: Another commonly used layer in CNNs is the pooling layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络中另一个常用的层是池化层。
- en: Introducing Pooling
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入池化
- en: 'The idea of **pooling** is to replace an area of the feature map with summary
    statistics. For example, pooling can replace each ![](img/Formula_B16391_09_044.png)
    area of the feature map with its maximum value, called **max pooling**, or its
    average value, called **average pooling** (*Figure 9.7*):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**池化**的思想是用汇总统计值替代特征图中的一块区域。例如，池化可以用每个 ![](img/Formula_B16391_09_044.png) 区域的最大值来替代该区域，称为**最大池化**，或者用其平均值来替代，称为**平均池化**（*图9.7*）：'
- en: '![Figure 9.7 – Results of max and average pooling](img/B16391_09_007.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 最大池化和平均池化的结果](img/B16391_09_007.jpg)'
- en: Figure 9.7 – Results of max and average pooling
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 最大池化和平均池化的结果
- en: A pooling layer reduces the dimension of the input image in a more efficient
    way and allows the extraction of dominant, rotational, and positional-invariant
    features.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层以更高效的方式减少输入图像的维度，并允许提取主导的、旋转不变和位置不变的特征。
- en: As with a filter, in pooling we need to define the size of the explored area
    for which to calculate the summary statistics. A commonly used setting is a pooling
    size of ![](img/Formula_B16391_09_045.png) pixels and a stride of two pixels in
    each direction. This setting halves the image dimension.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 和滤波器一样，在池化中我们需要定义要计算汇总统计的区域大小。一个常用的设置是池化大小为 ![](img/Formula_B16391_09_045.png)
    像素，且每个方向的步幅为两个像素。这个设置将图像的维度减半。
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Pooling layers don’t have any weights, and all settings are defined during the
    configuration of the layer. They are static layers, and their parameters do not
    get trained like the other weights in the network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层没有任何权重，所有设置都是在层配置期间定义的。它们是静态层，其参数不会像网络中的其他权重一样进行训练。
- en: Pooling layers are normally used after one convolution layer or multiple-stacked
    convolution layers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层通常在一个卷积层或多个堆叠卷积层之后使用。
- en: Convolution layers can be applied to input images as well as to feature maps.
    Indeed, multiple convolution layers are often stacked on top of each other in
    a CNN. In such a hierarchy, the first convolution layer may extract low-level
    features, such as edges. The filters in the next layer then work on top of the
    extracted features and may learn to detect shapes, and so on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层既可以应用于输入图像，也可以应用于特征图。事实上，多个卷积层通常会堆叠在一起形成 CNN。在这种层级结构中，第一层卷积可能会提取低级特征，如边缘。接下来的层中的滤波器将基于提取的特征进行处理，可能会学习检测形状，等等。
- en: The final extracted features can then be used for different tasks. In the case
    of image classification, the feature map—resulting from the stacking of multiple
    convolution layers—is flattened, and a classifier network is applied on top of
    it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最终提取的特征可以用于不同的任务。在图像分类的情况下，特征图——由多个卷积层堆叠而成——被平展，然后在其上应用分类网络。
- en: To summarize, a standard CNN for image classification first uses a series of
    convolution and pooling layers, then a flattened layer, and then a series of dense
    layers for the final classification.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，标准的图像分类 CNN 首先使用一系列卷积层和池化层，然后是一个平展层，最后是一系列全连接层用于最终分类。
- en: Now that we are familiar with convolutional layers and pooling layers, let’s
    see how they can be introduced inside a network for image classification.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了卷积层和池化层，让我们看看如何将它们引入到图像分类网络中。
- en: Classifying Images with CNNs
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNN 进行图像分类
- en: In this section, we will see how to build and train from scratch a CNN for image
    classification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何从零开始构建和训练用于图像分类的 CNN。
- en: 'The goal is to classify handwritten digits between 0 and 9 with the data from
    the **MNIST database**, a large database of handwritten digits commonly used for
    training various image-processing applications. The MNIST database contains 60,000
    training images and 10,000 testing images of handwritten digits and can be downloaded
    from this website: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是使用来自**MNIST 数据库**的数据，将手写数字进行分类，MNIST 是一个常用的手写数字数据库，通常用于训练各种图像处理应用。MNIST 数据库包含60,000张训练图像和10,000张测试图像，数字为手写体，可以从以下网站下载：[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。
- en: To read and preprocess images, KNIME Analytics Platform offers a set of dedicated
    nodes and components, available after installing the **KNIME Image Processing
    Extension**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取和预处理图像，KNIME 分析平台提供了一组专用的节点和组件，在安装**KNIME 图像处理扩展**后可用。
- en: Tip
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: The KNIME Image Processing Extension ([https://www.knime.com/community/image-processing](https://www.knime.com/community/image-processing))
    allows you to read in more than 140 different format types of images (thanks to
    the Bio-Formats **Application Processing Interface** (**API**)). In addition,
    it can be used to apply well-known image-processing techniques such as segmentation,
    feature extraction, tracking, and classification, taking advantage of the graphical
    user interface within KNIME Analytics Platform.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**KNIME 图像处理扩展**（[https://www.knime.com/community/image-processing](https://www.knime.com/community/image-processing)）允许你读取超过140种不同格式的图像（得益于生物格式**应用处理接口**（**API**））。此外，它还可以用来应用众所周知的图像处理技术，如分割、特征提取、跟踪和分类，利用
    KNIME 分析平台中的图形用户界面。'
- en: In general, the nodes operate on multi-dimensional image data (for example,
    videos, 3D images, multi-channel images, or even a combination of these), via
    the internal library `ImgLib2-API`. Several nodes calculate image features (for
    example, Zernike, texture, or histogram features) for segmented images (for example,
    a single cell). Machine learning algorithms are applied on the resulting feature
    vectors for the final classification.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些节点操作多维图像数据（例如，视频、3D 图像、多通道图像，甚至这些的组合），通过内部库 `ImgLib2-API`。几个节点计算分割图像（例如，单个细胞）的图像特征（例如，Zernike、纹理或直方图特征）。机器学习算法应用于结果特征向量，用于最终分类。
- en: 'To apply and train neural networks on images, we need one further extension:
    the **KNIME Image Processing - Deep Learning Extension**. This extension introduces
    a number of useful image operations—for example, some conversions necessary for
    image data to feed the **Keras Network Learner** node.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在图像上应用和训练神经网络，我们需要一个额外的扩展：**KNIME 图像处理 - 深度学习扩展**。这个扩展引入了一些有用的图像操作——例如，一些必要的转换，用于将图像数据输入到**Keras
    网络学习器**节点中。
- en: Important note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'To train and apply neural networks on images, you need to install the following
    extensions:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在图像上训练和应用神经网络，您需要安装以下扩展：
- en: KNIME Image Processing ([https://www.knime.com/community/image-processing](https://www.knime.com/community/image-processing))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 图像处理 ([https://www.knime.com/community/image-processing](https://www.knime.com/community/image-processing))
- en: KNIME Image Processing – Deep Learning Extension ([https://hub.knime.com/bioml-konstanz/extensions/org.knime.knip.dl.feature/latest](https://hub.knime.com/bioml-konstanz/extensions/org.knime.knip.dl.feature/latest))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 图像处理 – 深度学习扩展 ([https://hub.knime.com/bioml-konstanz/extensions/org.knime.knip.dl.feature/latest](https://hub.knime.com/bioml-konstanz/extensions/org.knime.knip.dl.feature/latest))
- en: Let’s get started with reading and preprocessing the handwritten digits.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始读取和预处理手写数字。
- en: Reading and Preprocessing Images
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和预处理图像
- en: 'For this case study, we use a subset of the MNIST dataset: 10,000 image samples
    for training and 1,500 for testing. Each image has ![](img/Formula_B16391_09_046.png)
    pixels and only one channel. The training and testing images are saved in two
    different folders, with progressive numbers as filenames. In addition, we have
    a table with the image labels, sorted by the order of the image filenames.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本案例研究，我们使用MNIST数据集的一个子集：10,000张图像样本用于训练，1,500张用于测试。每张图像有![](img/Formula_B16391_09_046.png)像素且只有一个通道。训练和测试图像保存在两个不同的文件夹中，文件名为递增数字。此外，我们有一个包含图像标签的表格，按图像文件名的顺序进行排序。
- en: 'The goal of the reading and preprocessing workflow is to read the images and
    to match them with their labels. Therefore, the following steps are implemented
    (also shown in *Figure 9.8*):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和预处理工作流的目标是读取图像并将它们与标签匹配。因此，实施了以下步骤（也见于*图 9.8*）：
- en: Read and sort the images for training.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取并排序用于训练的图像。
- en: Import the digit labels for the training images.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入训练图像的数字标签。
- en: Match the labels with the images.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签与图像匹配。
- en: Transform the pixel type from unsigned byte to float.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将像素类型从无符号字节转换为浮动。
- en: Convert the labels into a collection cell.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签转换为集合单元格。
- en: 'These steps are performed by the workflow shown in the following screenshot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤由以下截图中所示的工作流执行：
- en: '![Figure 9.8 – This workflow reads a subset of the MNIST dataset, adds the
    corresponding labels, and transforms the pixel type from unsigned byte to float](img/B16391_09_008.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – 此工作流读取MNIST数据集的一个子集，添加对应的标签，并将像素类型从无符号字节转换为浮动](img/B16391_09_008.jpg)'
- en: Figure 9.8 – This workflow reads a subset of the MNIST dataset, adds the corresponding
    labels, and transforms the pixel type from unsigned byte to float
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 此工作流读取MNIST数据集的一个子集，添加对应的标签，并将像素类型从无符号字节转换为浮动
- en: 'To read the images, we use the **Image Reader (Table)** node. This node expects
    an input column with the **Uniform Resource Locator** (**URL**) paths to the image
    files. To create the sorted list of URLs, the **List Files** node first gets all
    paths to the image files in the training folder. Then, the **Sort images** metanode
    is used. *Figure 9.9* here shows you the inside of the metanode:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取图像，我们使用**图像读取器（表格）**节点。该节点需要输入一列包含指向图像文件的**统一资源定位符**（**URL**）路径。为了创建排序后的URL列表，**列出文件**节点首先获取训练文件夹中所有图像文件的路径。然后，使用**排序图像**元节点。这里的*图
    9.9*展示了元节点内部的情况：
- en: '![Figure 9.9 – Inside of the Sort images metanode](img/B16391_09_009.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 排序图像元节点内部](img/B16391_09_009.jpg)'
- en: Figure 9.9 – Inside of the Sort images metanode
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 排序图像元节点内部
- en: The metanode extracts the image number from the filename with a **String Manipulation**
    node and sorts them with a **Sorter** node. The **Image Reader (Table)** node
    then reads the images.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 元节点通过**字符串操作**节点从文件名中提取图像编号，并通过**排序器**节点对其进行排序。然后，**图像读取器（表格）**节点读取图像。
- en: The **File Reader** node, in the lower branch, reads the table with the image
    labels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**文件读取器**节点，在下部分支中，读取包含图像标签的表格。'
- en: 'In the next step, the **Column Appender** node appends the correct label to
    each image. Since images have been sorted as to match their corresponding label,
    a simple appending operation is sufficient. *Figure 9.10* here shows a subset
    of the output of the **Column Appender** node:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，**列附加器**节点将正确的标签附加到每个图像上。由于图像已经被排序以匹配相应的标签，简单的附加操作就足够了。这里的*图 9.10*展示了**列附加器**节点输出的一个子集：
- en: '![Figure 9.10 – Output of the Column Appender node, with the digit image and
    the corresponding label](img/B16391_09_010.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – 列附加器节点的输出，包含数字图像和相应标签](img/B16391_09_010.jpg)'
- en: Figure 9.10 – Output of the Column Appender node, with the digit image and the
    corresponding label
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 列追加节点的输出，包含数字图像及其对应标签。
- en: Next, the **Image Calculator** node changes the pixel type from *unsigned byte*
    to *float*, by dividing each pixel value by 255.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**图像计算器** 节点通过将每个像素值除以 255，将像素类型从 *无符号字节* 改为 *浮动*。
- en: Finally, the **Create Collection Column** node creates a collection cell for
    each label. The collection cell is required to create the one-hot vector-encoded
    classes, to use during training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**创建集合列** 节点为每个标签创建一个集合单元。这个集合单元在训练过程中用于创建 one-hot 向量编码的类别。
- en: Now that we have read and preprocessed the training images, we can design the
    network structure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经读取并预处理了训练图像，可以设计网络结构了。
- en: Designing the Network
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计网络结构
- en: In this section, you will learn how to define a classical CNN for image classification.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何定义一个经典的 CNN 用于图像分类。
- en: 'A classical CNN for image classification consists of two parts, which are trained
    together in an end-to-end fashion, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的 CNN 用于图像分类，通常包含两部分，它们以端到端的方式共同训练，具体如下：
- en: '**Feature Extraction**: The first part performs the feature extraction of the
    images, by training a number of filters.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：第一部分通过训练若干滤波器来进行图像的特征提取。'
- en: '**Classification**: The second part trains a classification network on the
    extracted features, available in the flattened feature map resulting from the
    feature extraction part.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：第二部分在提取到的特征上训练分类网络，提取后的特征存在于从特征提取部分得到的扁平化特征图中。'
- en: We start with a simple network structure with only one convolution layer, followed
    by a pooling layer for the feature extraction part. The resulting feature maps
    are then flattened, and a simple classifier network, with just one hidden layer
    with the ReLU activation function, is trained on them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的网络结构开始，只有一个卷积层，后面接着一个池化层用于特征提取部分。得到的特征图会被扁平化，随后在其上训练一个简单的分类网络，这个网络只有一个隐藏层，并使用
    ReLU 激活函数。
- en: 'The workflow here in *Figure 9.11* shows this network structure:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的工作流在*图 9.11*中展示了这个网络结构：
- en: '![Figure 9.11 – This workflow snippet builds a simple CNN for the classification
    of the MNIST dataset](img/B16391_09_011.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 这个工作流片段构建了一个简单的 CNN，用于 MNIST 数据集的分类](img/B16391_09_011.jpg)'
- en: Figure 9.11 – This workflow snippet builds a simple CNN for the classification
    of the MNIST dataset
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 这个工作流片段构建了一个简单的 CNN，用于 MNIST 数据集的分类。
- en: The workflow starts with a **Keras Input Layer** node to define the input shape.
    The images of the MNIST dataset have ![](img/Formula_B16391_09_047.png) pixels
    and only one channel, as they are grayscale images. Thus, the input is a tensor
    of shape ![](img/Formula_B16391_09_048.png), and therefore the input shape is
    set to ![](img/Formula_B16391_09_049.png).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流从 **Keras 输入层** 节点开始，用于定义输入形状。MNIST 数据集的图像具有 ![](img/Formula_B16391_09_047.png)
    像素，并且只有一个通道，因为它们是灰度图像。因此，输入是形状为 ![](img/Formula_B16391_09_048.png) 的张量，输入形状设置为
    ![](img/Formula_B16391_09_049.png)。
- en: 'Next, the convolutional layer is implemented with a **Keras Convolution 2D
    Layer** node. *Figure 9.12* here shows you the configuration window of the node:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，卷积层通过 **Keras 卷积 2D 层** 节点实现。*图 9.12* 展示了该节点的配置窗口：
- en: '![Figure 9.12 – Keras Convolution 2D Layer node and its configuration window](img/B16391_09_012.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – Keras 卷积 2D 层节点及其配置窗口](img/B16391_09_012.jpg)'
- en: Figure 9.12 – Keras Convolution 2D Layer node and its configuration window
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – Keras 卷积 2D 层节点及其配置窗口。
- en: The setting named **Filters** sets the number of filters to apply. This will
    be the last dimension of the feature map. In this example, we decided to train
    *32* filters.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 名为 **Filters** 的设置决定了要应用的滤波器数量。这将是特征图的最后一个维度。在这个例子中，我们决定训练 *32* 个滤波器。
- en: Next, you can set the **Kernel size** option in pixels—that is, an integer tuple
    defining the height and width of each kernel. For the MNIST dataset, we use a
    kernel size of ![](img/Formula_B16391_09_050.png). This means the setting is ![](img/Formula_B16391_09_051.png).50.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以在像素中设置 **Kernel size** 选项，也就是一个整数元组，定义每个卷积核的高度和宽度。对于 MNIST 数据集，我们使用的卷积核大小是
    ![](img/Formula_B16391_09_050.png)。这意味着设置为 ![](img/Formula_B16391_09_051.png)。50。
- en: Next, you can set the `dilation_rate` greater than 1.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以将 `dilation_rate` 设置为大于 1 的值。
- en: Next, you can select whether you want to use zero padding or not. The **Padding**
    option allows you to select between **Valid** and **Same**. **Valid** means no
    padding is performed. **Same** means zero padding is performed, such that the
    output dimension of the feature map is the same as the input dimension. As we
    have mainly black pixels on the border of the images, we decided not to zero-pad
    the images and selected **Valid**.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以选择是否使用零填充。**填充**选项让你在**有效**和**相同**之间进行选择。**有效**表示不执行填充操作。**相同**表示执行零填充，使得特征图的输出维度与输入维度相同。由于图像边缘主要是黑色像素，我们决定不对图像进行零填充，选择了**有效**。
- en: Next, you can select the **Dilation rate** option, as an integer tuple. Currently,
    specifying any dilation rate value greater than 1 is incompatible with specifying
    any stride value greater than 1\. A dilation rate of ![](img/Formula_B16391_09_052.png)
    means that no pixels are skipped. A dilation rate of ![](img/Formula_B16391_09_053.png)
    means every second pixel is used. This means a gap size of 1\. We use ![](img/Formula_B16391_09_054.png)
    for the dilation rate .52.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以选择**膨胀率**选项，作为一个整数元组。目前，指定任何大于1的膨胀率值与指定任何大于1的步幅值是不兼容的。膨胀率为![](img/Formula_B16391_09_052.png)表示没有像素被跳过。膨胀率为![](img/Formula_B16391_09_053.png)表示每隔一个像素使用一个像素，这意味着间隔大小为1。我们使用![](img/Formula_B16391_09_054.png)表示膨胀率为.52。
- en: 'Last, the **Activation function** option must be selected. For this case study,
    we went for the most commonly used activation function for convolutional layers:
    **ReLU**.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，必须选择**激活函数**选项。对于这个案例研究，我们选择了卷积层中最常用的激活函数：**ReLU**。
- en: The output tensor of the convolutional layer (that is, our feature map) has
    the dimension ![](img/Formula_B16391_09_055.png), as we have ![](img/Formula_B16391_09_056.png)
    filters and we don’t use padding.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出张量（即我们的特征图）具有维度![](img/Formula_B16391_09_055.png)，因为我们有![](img/Formula_B16391_09_056.png)个滤波器，并且没有使用填充。
- en: Next, a **Keras Max Pooling 2D Layer** node is used to apply max pooling on
    the two dimensions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用**Keras Max Pooling 2D Layer**节点对两个维度进行最大池化。
- en: '*Figure 9.13* here shows you the configuration window of the node:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.13* 显示了节点的配置窗口：'
- en: '![Figure 9.13 – Keras Max Pooling 2D Layer node and its configuration window](img/B16391_09_013.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – Keras Max Pooling 2D Layer节点及其配置窗口](img/B16391_09_013.jpg)'
- en: Figure 9.13 – Keras Max Pooling 2D Layer node and its configuration window
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – Keras Max Pooling 2D Layer节点及其配置窗口
- en: In the configuration window of the **Keras Max Pooling 2D Layer** node, you
    can define the **Pool size**. Again, this is an integer tuple defining the pooling
    window. Remember, the idea of max pooling is to represent each area of the size
    of the pooling window with the maximum value in the area.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Keras Max Pooling 2D Layer**节点的配置窗口中，你可以定义**池化大小**。这同样是一个整数元组，定义了池化窗口。请记住，最大池化的理念是用池化窗口中的最大值来表示每个区域。
- en: The **stride** is again an integer tuple, setting the step size to shift the
    pooling window.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**步幅**再次是一个整数元组，用于设置池化窗口的步长。'
- en: Lastly, you can select whether to apply zero padding by selecting **Valid**
    for no padding, and **Same** to apply padding.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以选择是否应用零填充，选择**有效**表示不填充，选择**相同**表示应用填充。
- en: For this MNIST example, we set **Pool size** as ![](img/Formula_B16391_09_057.png),
    **Strides** as ![](img/Formula_B16391_09_058.png), and applied no padding. Therefore,
    the dimension of output of the pooling layer is ![](img/Formula_B16391_09_059.png).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个 MNIST 示例，我们将**池化大小**设置为![](img/Formula_B16391_09_057.png)，**步幅**设置为![](img/Formula_B16391_09_058.png)，并且没有应用填充。因此，池化层的输出维度为![](img/Formula_B16391_09_059.png)。
- en: Next, a **Keras Flatten Layer** node is used to transform the feature map into
    a vector, with dimension ![](img/Formula_B16391_09_060.png).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用一个**Keras Flatten Layer**节点将特征图转换为向量，维度为![](img/Formula_B16391_09_060.png)。
- en: After the **Keras Flatten Layer** node, we build a simple classification network
    with one hidden layer and one output layer. The hidden layer with the ReLU activation
    function and 100 units is implemented by the first **Keras Dense Layer** node
    in *Figure 9.11*, while the output layer is implemented by the second (and last)
    **Keras Dense Layer** node in *Figure 9.11*. As it is a multiclass classification
    problem with 10 different classes, here the softmax activation function with 10
    units is used. In addition, the **Name prefix** *output* is used so that we can
    identify the output layer more easily when applying the network to new data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Keras 扁平化层**节点之后，我们构建了一个简单的分类网络，包含一个隐藏层和一个输出层。隐藏层使用 ReLU 激活函数和 100 个单元，由*图
    9.11*中的第一个**Keras Dense 层**节点实现，而输出层由*图 9.11*中的第二个（也是最后一个）**Keras Dense 层**节点实现。由于这是一个有
    10 个不同类别的多类分类问题，因此这里使用了带有 10 个单元的 softmax 激活函数。此外，使用**名称前缀** *output*，这样在将网络应用于新数据时可以更容易地识别输出层。
- en: Now that we have defined the network structure, we can move on to train the
    CNN.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了网络结构，可以开始训练 CNN。
- en: Training and Applying the Network
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和应用网络
- en: 'To train the CNN built in the previous section, we again use the **Keras Network
    Learner** node. In the previous chapters, we already saw that this node offers
    many conversion types for input and target data (such as, for example, the **From
    Collection of Number (integers) to One-Hot Tensor** option). Installing the **KNIME
    Image Processing – Deep Learning Extension** adds one more conversion option:
    **From Image (Auto-mapping)**. This new conversion option allows us to select
    an image column from the input table and to automatically create the tensor to
    feed into the network.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练上一节中构建的 CNN，我们再次使用**Keras 网络学习器**节点。在前面的章节中，我们已经看到这个节点提供了许多输入和目标数据的转换类型（例如，**从数字集合（整数）到独热张量**的选项）。安装**KNIME
    图像处理 – 深度学习扩展**后，新增了一个转换选项：**从图像（自动映射）**。这个新的转换选项允许我们从输入表中选择一个图像列，并自动创建张量以输入到网络中。
- en: '*Figure 9.14* here shows the **Input Data** tab of the configuration window
    of the **Keras Network Learner** node, including this additional conversion option:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.14* 显示了**输入数据**标签，来自**Keras 网络学习器**节点的配置窗口，其中包括这个额外的转换选项：'
- en: '![Figure 9.14 – Input Data tab of the configuration window of the Keras Network
    Learner node with the additional conversion option, From Image (Auto-mapping)](img/B16391_09_014.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – Keras 网络学习器节点配置窗口的输入数据标签，带有额外的转换选项：从图像（自动映射）](img/B16391_09_014.jpg)'
- en: Figure 9.14 – Input Data tab of the configuration window of the Keras Network
    Learner node with the additional conversion option, From Image (Auto-mapping)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – Keras 网络学习器节点配置窗口的输入数据标签，带有额外的转换选项：从图像（自动映射）
- en: In the **Target Data** tab, the conversion option from **From Collection of
    Number (integer) to One-Hot Tensor** is selected for the column with the collection
    cell of the image label.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在**目标数据**标签中，选择了将**从数字集合（整数）到独热张量**的转换选项，适用于图像标签的集合单元列。
- en: On the bottom, the *Categorical cross entropy* activation function is selected,
    as the problem is a multiclass classification problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 底部选择了*类别交叉熵*激活函数，因为这是一个多类分类问题。
- en: 'In the **Options** tab, the following training parameters are set:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在**选项**标签中，设置了以下训练参数：
- en: '`10`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`10`'
- en: '`200`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`200`'
- en: '`Adadelta with the default settings`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Adadelta 默认设置`'
- en: '*Figure 9.15* here shows the progress of the training procedure in the **Learning
    Monitor** view of the **Keras Network Learner** node after node execution:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.15* 显示了**Keras 网络学习器**节点执行后的**学习监视器**视图，显示训练过程的进展：'
- en: '![Figure 9.15 – The Learning Monitor view shows the training progress of the
    network](img/B16391_09_015.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – 学习监视器视图显示网络的训练进度](img/B16391_09_015.jpg)'
- en: Figure 9.15 – The Learning Monitor view shows the training progress of the network
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – 学习监视器视图显示网络的训练进度
- en: The **Learning Monitor** view shows the progress of the network during training
    over the many training batches. On the right-hand side, you can see the accuracy
    for the last few batches. **Current Value** shows you the accuracy for the last
    batch, which is in this case **0.995**.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习监视器**视图显示了在多个训练批次中的网络训练进度。在右侧，你可以看到最后几个批次的准确率。**当前值**显示的是最后一个批次的准确率，在这个例子中是
    **0.995**。'
- en: Now that we have a trained CNN satisfactorily performing on the training set,
    we can apply it to the test set. Here, the same reading and preprocessing steps
    as for the training set must also be applied on the test set.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练出了一个在训练集上表现良好的CNN，可以将其应用于测试集。在此，必须对测试集执行与训练集相同的读取和预处理步骤。
- en: The **Keras Network Executor** node applies the trained network on the images
    in the test set. In the configuration window, the last layer, producing the probability
    distribution of the different digits, is selected as output.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras网络执行器**节点将训练好的网络应用于测试集中的图像。在配置窗口中，选择产生不同数字概率分布的最后一层作为输出。'
- en: At this point, a bit of postprocessing is required in order to extract the final
    prediction from the network output.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，需要进行一些后处理，以便从网络输出中提取最终的预测结果。
- en: Prediction Extraction and Model Evaluation
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测提取与模型评估
- en: 'The output of the **Keras Network Executor** node is a table with 12 columns,
    comprising the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras网络执行器**节点的输出是一个包含12列的表，包含以下内容：'
- en: The image column
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像列
- en: The true class value, named **Actual Value**
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实类别值，命名为**实际值**
- en: '10 columns with the probability values for the image classes with the column
    headers: `output/Softmax:0_x`, where `x` is a number between 0 and 9 encoding
    the class'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10列包含图像类别的概率值，列标题为：`output/Softmax:0_x`，其中`x`是介于0到9之间的数字，用来表示类别。
- en: 'The goal of the postprocessing is to extract the class with the highest probability
    and then to evaluate the network performance. This is implemented by the workflow
    snippet shown here in *Figure 9.16*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理的目标是提取概率最高的类别，然后评估网络性能。这是通过*图 9.16*中所示的工作流片段实现的：
- en: '![Figure 9.16 – This workflow snippet extracts the digit class with the highest
    probability and evaluates the network performance on the test set](img/B16391_09_016.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.16 – 这个工作流片段提取概率最高的数字类别，并在测试集上评估网络性能](img/B16391_09_016.jpg)'
- en: Figure 9.16 – This workflow snippet extracts the digit class with the highest
    probability and evaluates the network performance on the test set
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – 这个工作流片段提取概率最高的数字类别，并在测试集上评估网络性能
- en: The **Many to One** node extracts the column header of the column with the highest
    probability in each row.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**一对多**节点提取每一行中概率最高的列的列标题。'
- en: Then, the **Column Expression** node extracts the class from the column header.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**列表达式**节点从列标题中提取类别。
- en: Tip
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The **Column Expression** node is a very powerful node. It provides the possibility
    to append an arbitrary number of new columns or modify existing columns using
    expressions.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表达式**节点是一个非常强大的节点。它提供了通过表达式附加任意数量的新列或修改现有列的可能性。'
- en: For each column to be appended or modified, a separate expression can be defined.
    These expressions can be simply created using predefined functions, similarly
    to the `=`).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个需要附加或修改的列，可以定义一个单独的表达式。这些表达式可以像使用`=`一样简单地通过预定义函数创建。
- en: Available flow variables and columns of the input table can be accessed via
    the provided access functions variable ("`variableName`") and column ("`columnName`").
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过提供的访问函数变量（"`variableName`"）和列（"`columnName`"）来访问可用的流程变量和输入表的列。
- en: '*Figure 9.17* here shows you the configuration window of the **Column Expression**
    node, with the expression used in the workflow snippet in *Figure 9.16* to extract
    the class information. In this case, the expression extracts the last character
    from the strings in the column named **Detected Digit**:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.17*这里显示了**列表达式**节点的配置窗口，其中包含工作流片段*图 9.16*中用于提取类别信息的表达式。在此案例中，表达式从名为**检测到的数字**的列中的字符串中提取最后一个字符：'
- en: '![Figure 9.17 – The Column Expression node and its configuration window](img/B16391_09_017.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.17 – 列表达式节点及其配置窗口](img/B16391_09_017.jpg)'
- en: Figure 9.17 – The Column Expression node and its configuration window
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – 列表达式节点及其配置窗口
- en: Next, the data type of the predicted class is converted from `String` to `Integer`
    with the **String to Number** node, and the network performance is evaluated on
    the test set with the **Scorer** node.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，预测类别的数据类型从`String`转换为`Integer`，通过**字符串转数字**节点实现，接着用**评分器**节点在测试集上评估网络性能。
- en: '*Figure 9.18* here shows the view produced by the **Scorer** node:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.18*这里显示了**评分器**节点产生的视图：'
- en: '![Figure 9.18 – View of the Scorer node, showing the performance of the network
    on the test set](img/B16391_09_018.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – Scorer 节点视图，展示了网络在测试集上的表现](img/B16391_09_018.jpg)'
- en: Figure 9.18 – View of the Scorer node, showing the performance of the network
    on the test set
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – Scorer 节点视图，展示了网络在测试集上的表现
- en: 'As you can see, this simple CNN has already reached an accuracy of 94% and
    a Cohen’s kappa of 0.934 on the test set. The complete workflow is available on
    the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个简单的卷积神经网络已经在测试集上达到了94%的准确率和0.934的Cohen’s kappa值。完整的工作流可以在 KNIME Hub 上找到：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/)。
- en: 'In this section, we built and trained from scratch a simple CNN, reaching an
    acceptable performance for this rather simple image classification task. Of course,
    we could try to further improve the performance of this network by doing the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从头开始构建并训练了一个简单的卷积神经网络（CNN），并在这个相对简单的图像分类任务中达到了可接受的性能。当然，我们可以通过以下方法进一步提高该网络的性能：
- en: Increasing the number of training epochs
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加训练周期数
- en: Adding a second convolutional layer together with a pooling layer
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加第二个卷积层，并结合池化层
- en: Using batch normalization for training
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批归一化进行训练
- en: Using augmentation
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强
- en: Using dropout
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用丢弃法（dropout）
- en: We leave this up to you, and continue with another way of network learning,
    called transfer learning.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此留给你去完成，接下来介绍另一种网络学习方式，称为迁移学习。
- en: Introduction to transfer learning
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习简介
- en: The general idea of **transfer learning** is to reuse the knowledge gained by
    a network trained for task **A** on another related task **B**. For example, if
    we train a network to recognize sailing boats (task A), we can use this network
    as a starting point to train a new model to recognize motorboats (task B). In
    this case, task A is called the *source task* and task B the *target task*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**的基本理念是将用于任务**A**的网络在另一个相关任务**B**上重新使用。例如，如果我们训练一个网络来识别帆船（任务A），我们可以将这个网络作为起点，训练一个新模型来识别摩托艇（任务B）。在这种情况下，任务A被称为*源任务*，任务B则是*目标任务*。'
- en: 'Reusing a trained network as the starting point to train a new network is different
    from the traditional way of training networks, whereby neural networks are trained
    on their own for specific tasks on specific datasets. *Figure 9.19* here visualizes
    the traditional way of network training, whereby different systems are trained
    for different tasks and domains:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个训练过的网络作为起点来训练一个新网络，与传统的网络训练方式不同，传统方法通常是将神经网络针对特定任务和数据集单独进行训练。*图 9.19* 这里可视化了传统的网络训练方式，其中不同的系统为不同的任务和领域进行训练：
- en: '![Figure 9.19 – Traditional way of training machine learning models and neural
    networks](img/B16391_09_019.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – 传统的机器学习模型和神经网络训练方式](img/B16391_09_019.jpg)'
- en: Figure 9.19 – Traditional way of training machine learning models and neural
    networks
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 传统的机器学习模型和神经网络训练方式
- en: But why should we use transfer learning instead of training models in the traditional,
    isolated way?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们要使用迁移学习，而不是以传统、孤立的方式训练模型呢？
- en: Why use Transfer Learning?
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用迁移学习？
- en: Current state-of-the-art neural networks have shown amazing performance in tackling
    specific complex tasks. Sometimes, these models are even better than humans, beating
    world champions at board games or at detecting objects in images. To train these
    successful networks, usually a huge amount of labeled data is required, as well
    as a vast amount of computational resources and time.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当前最先进的神经网络在处理特定复杂任务方面表现出了惊人的性能。有时，这些模型甚至超过了人类，在棋盘游戏中击败世界冠军，或在图像中检测物体。为了训练这些成功的网络，通常需要大量的标注数据，以及大量的计算资源和时间。
- en: To get a comprehensive labeled dataset for a new domain, in order to be able
    to train a network to reach state-of-art-performance, can be difficult or even
    impossible. As an example, the often-used **ImageNet database**, which is used
    to train state-of-the-art models, has been developed over the course of many years.
    It would take time to create a similar new dataset for a new image domain. However,
    when these state-of-the-art models are applied to other related domains, they
    often suffer a considerable loss in performance, or, even worse, they break down.
    This happens due to the model bias toward the training data and domain.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一个全面标注的数据集用于一个新领域，以便能够训练一个网络达到最先进的性能，可能是困难的，甚至是不可能的。举个例子，常用的**ImageNet数据库**，用于训练最先进的模型，已经在多年的时间里开发出来。要为一个新的图像领域创建一个类似的新数据集需要时间。然而，当这些最先进的模型应用到其他相关领域时，它们往往会遭遇显著的性能下降，甚至更糟糕的是，模型可能会崩溃。这是因为模型对训练数据和领域存在偏倚。
- en: Transfer learning allows us to use the knowledge gained during training on a
    task and domain where sufficient labeled data was available as a starting point,
    to train new models in domains where not enough labeled data is yet available.
    This approach has shown great results in many computer vision and **natural language
    processing** (**NLP**) tasks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习使我们能够利用在一个任务和领域中训练时获得的知识（该领域有足够的标注数据）作为起点，在数据不足的新领域中训练新的模型。这种方法在许多计算机视觉和**自然语言处理**（**NLP**）任务中取得了显著的成果。
- en: '*Figure 9.20* here visualizes the idea behind transfer learning:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.20* 在这里可视化了转移学习的基本概念：'
- en: '![Figure 9.20 – Idea behind transfer learning](img/B16391_09_020.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.20 – 转移学习的基本概念](img/B16391_09_020.jpg)'
- en: Figure 9.20 – Idea behind transfer learning
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – 转移学习的基本概念
- en: Before we talk about how we can apply transfer learning when training a neural
    network, let’s have a quick look at the formal definition of transfer learning
    and the many scenarios in which it can be applied.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何在训练神经网络时应用转移学习之前，先快速浏览一下转移学习的正式定义以及它可以应用的多种场景。
- en: Formal Definition of Transfer Learning
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习的正式定义
- en: A formal definition of transfer learning and of the related scenarios can be
    found in the paper by Sinno Jialin Pan and Qiang Yang, *A Survey on Transfer Learning*,
    IEEE Transactions on Knowledge and Data Engineering, 2009([https://ieeexplore.ieee.org/abstract/document/5288526](https://ieeexplore.ieee.org/abstract/document/5288526)).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习的正式定义及相关场景可以在Sinno Jialin Pan和Qiang Yang的论文《转移学习综述》中找到，IEEE Transactions
    on Knowledge and Data Engineering, 2009（[https://ieeexplore.ieee.org/abstract/document/5288526](https://ieeexplore.ieee.org/abstract/document/5288526)）。
- en: The definition involves the concept of a **domain** and a **task**.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义涉及到**领域**和**任务**的概念。
- en: In the paper, a **domain** ![](img/Formula_B16391_09_061.png) is introduced
    as tuple {![](img/Formula_B16391_09_062.png)where ![](img/Formula_B16391_09_063.png)
    is the feature space and a ![](img/Formula_B16391_09_064.png) the marginal probability
    distribution for ![](img/Formula_B16391_09_065.png).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，引入了**领域** ![](img/Formula_B16391_09_061.png)，其定义为元组 {![](img/Formula_B16391_09_062.png)其中
    ![](img/Formula_B16391_09_063.png) 是特征空间， ![](img/Formula_B16391_09_064.png) 是
    ![](img/Formula_B16391_09_065.png) 的边际概率分布。
- en: 'For a given domain, ![](img/Formula_B16391_09_066.png), a task ![](img/Formula_B16391_09_067.png)
    consists of the following two components as well:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个给定的领域 ![](img/Formula_B16391_09_066.png)，一个任务 ![](img/Formula_B16391_09_067.png)
    由以下两个组成部分构成：
- en: A label space ![](img/Formula_B16391_09_068.png)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签空间 ![](img/Formula_B16391_09_068.png)
- en: A predictive function ![](img/Formula_B16391_09_069.png)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测函数 ![](img/Formula_B16391_09_069.png)
- en: Here, the predictive function ![](img/Formula_B16391_09_070.png) could be the
    conditional probability distribution ![](img/Formula_B16391_09_071.png) In general,
    the predictive function is a function trained on the labeled training data to
    predict the label ![](img/Formula_B16391_09_072.png) for any sample ![](img/Formula_B16391_09_073.png)
    in the feature space.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，预测函数 ![](img/Formula_B16391_09_070.png) 可以是条件概率分布 ![](img/Formula_B16391_09_071.png)。一般来说，预测函数是基于标注的训练数据训练出来的，用来预测任何样本
    ![](img/Formula_B16391_09_073.png) 在特征空间中的标签 ![](img/Formula_B16391_09_072.png)。
- en: 'Using this terminology, **transfer learning** is defined by Sinno Jialin Pan
    and Qiang Yang in the following way:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这一术语，**转移学习**由Sinno Jialin Pan和Qiang Yang以以下方式定义：
- en: '*"Given a source domain* ![](img/Formula_B16391_09_074.png) *and learning task*
    ![](img/Formula_B16391_09_075.png)*, a target domain* ![](img/Formula_B16391_09_076.png)
    *and learning task* ![](img/Formula_B16391_09_077.png) *, transfer learning aims
    to help improve the learning of the target predictive function* ![](img/Formula_B16391_09_078.png)
    *in* ![](img/Formula_B16391_09_079.png) *using the knowledge in* ![](img/Formula_B16391_09_080.png)
    *and* ![](img/Formula_B16391_09_081.png)*, where* ![](img/Formula_B16391_09_082.png)*,
    or* ![](img/Formula_B16391_09_083.png)."'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*“给定一个源领域* ![](img/Formula_B16391_09_074.png) *和学习任务* ![](img/Formula_B16391_09_075.png)*，一个目标领域*
    ![](img/Formula_B16391_09_076.png) *和学习任务* ![](img/Formula_B16391_09_077.png)
    *，迁移学习旨在帮助改善目标预测函数的学习* ![](img/Formula_B16391_09_078.png) *，使用在* ![](img/Formula_B16391_09_079.png)
    *中的知识* ![](img/Formula_B16391_09_080.png) *和* ![](img/Formula_B16391_09_081.png)*，其中*
    ![](img/Formula_B16391_09_082.png)*，或* ![](img/Formula_B16391_09_083.png)*。”'
- en: 'Sebastian Ruder uses this definition in his article, *Transfer Learning - Machine
    Learning’s Next Frontier*, *2017* ( [https://ruder.io/transfer-learning/](https://ruder.io/transfer-learning/))
    to describe the following *four scenarios* in which transfer learning can be used:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian Ruder在他的文章《迁移学习——机器学习的下一个前沿》*（2017）*中使用了这个定义（[https://ruder.io/transfer-learning/](https://ruder.io/transfer-learning/)），描述了迁移学习可以应用的以下*四种情境*：
- en: 'Different feature spaces: ![](img/Formula_B16391_09_084.png)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的特征空间：![](img/Formula_B16391_09_084.png)
- en: An example in the paper is cross-lingual adaptation, where we have documents
    in different languages.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文中的一个例子是跨语言适配，我们有不同语言的文档。
- en: 'Different marginal probabilities: ![](img/Formula_B16391_09_085.png)'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的边际概率：![](img/Formula_B16391_09_085.png)
- en: An example comes in the form of documents that discuss different topics. This
    scenario is called *domain adaption*.
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个例子是讨论不同主题的文档。这个情境称为*领域适配*。
- en: 'Different label spaces: ![](img/Formula_B16391_09_086.png)'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的标签空间：![](img/Formula_B16391_09_086.png)
- en: (for example, if we have documents with different labels).
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （例如，如果我们有不同标签的文档）。
- en: Different conditional probabilities ![](img/Formula_B16391_09_087.png) This
    usually occurs together with scenario 3.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的条件概率 ![](img/Formula_B16391_09_087.png) 这通常与情境3一起发生。
- en: Now that we have a basic understanding of transfer learning, let’s find out
    next how transfer learning can be applied to the field of deep learning.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对迁移学习有了基本的理解，接下来让我们探讨迁移学习如何应用于深度学习领域。
- en: Applying Transfer Learning
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用迁移学习
- en: In a neural network, the knowledge gained during training is stored in the weights
    of the layers. For example, in the case of CNNs, a number of filters are trained
    to extract a number of features. Thus, the knowledge of how to extract such features
    from an image is stored in the weights of the kernels for the implemented filters.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，训练过程中获得的知识存储在各层的权重中。例如，在CNN中，许多滤波器被训练以提取一系列特征。因此，从图像中提取这些特征的知识被存储在实现滤波器的卷积核权重中。
- en: In a stacked CNN for image classification, the initial convolution layers are
    responsible for extracting low-level features such as edges, while the next convolution
    layers extract higher-level features such as body parts, animals, or faces. The
    last layers are trained to classify the images, based on the extracted features.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于图像分类的堆叠卷积神经网络（CNN）中，最初的卷积层负责提取低级特征，如边缘，而接下来的卷积层提取更高级的特征，如身体部位、动物或面部。最后的层被训练用于根据提取的特征对图像进行分类。
- en: So, if we want to train a CNN for a different image-classification task, on
    different images and with different labels, we must not train the new filters
    from scratch, but we can use the previously trained convolution layers in a state-of-the-art
    network as the starting point. Hopefully, the new training procedure will be faster
    and will require a smaller amount of data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想为不同的图像分类任务训练一个CNN，处理不同的图像并使用不同的标签，我们就不能从零开始训练新的滤波器，而应该使用在最先进网络中训练过的卷积层作为起点。希望新的训练过程能够更快，且所需的数据量更少。
- en: 'To use the trained layers from another network as the training starting point,
    we need to extract the convolution layers from the original network and then build
    some new layers on top. To do so, we have the following two options:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用另一个网络训练过的层作为训练起点，我们需要从原始网络中提取卷积层，然后在其上构建一些新的层。为此，我们有以下两个选择：
- en: We freeze the weights of the trained layers and just train the added layers
    based on the output of the frozen layers. This approach is often used in NLP applications,
    where trained embeddings are reused.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们冻结已经训练好的层的权重，只根据被冻结层的输出训练新增的层。这种方法在NLP应用中经常使用，其中已训练的词向量会被重用。
- en: We use the trained weights to initialize new convolution layers in the network
    and then fine-tune them while training the added layers. In this case, a small
    training rate is used to not unlearn the learned knowledge from the source task.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用训练好的权重来初始化网络中的新卷积层，然后在训练新增的层时对其进行微调。在这种情况下，会使用较小的学习率，以避免遗忘源任务中已经学到的知识。
- en: For the last case study of this book, we want to train a neural network to predict
    cancer type from histopathology slide images. To speed up the learning process
    and considering the relatively small dataset we have, we will apply transfer learning
    starting from the convolution layers in the popular VGG16 network used here as
    the source network.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本书的最后一个案例研究，我们希望训练一个神经网络，通过组织病理学切片图像预测癌症类型。为了加快学习过程，并考虑到我们所拥有的相对较小的数据集，我们将应用迁移学习，从这里作为源网络的流行VGG16网络的卷积层开始。
- en: Applying Transfer Learning for Cancer Type Prediction
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用迁移学习进行癌症类型预测
- en: We will introduce here a new (and final) case study. We will start from the
    state-of-the-art VGG16 network as a source network to train a new target network
    on a dataset of images describing three different subtypes of lymphoma, which
    are **chronic lymphocytic leukemia** (**CLL**), **follicular lymphoma** (**FL**),
    and **mantle cell lymphoma** (**MCL**).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里介绍一个新的（也是最后的）案例研究。我们将从最先进的VGG16网络作为源网络，训练一个新的目标网络，该目标网络将基于描述三种不同类型淋巴瘤的图像数据集进行训练，这三种类型分别是**慢性淋巴细胞白血病**（**CLL**）、**滤泡性淋巴瘤**（**FL**）和**外套细胞淋巴瘤**（**MCL**）。
- en: A typical task for a pathologist in a hospital is to look at histopathology
    slide images and make a decision about the type of lymphoma. Even for experienced
    pathologists this is a difficult task and, in many cases, follow-up tests are
    required to confirm the diagnosis. An assistive technology that can guide pathologists
    and speed up their job would be of great value.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 病理学家在医院的典型任务之一是查看组织病理学切片图像，并判断淋巴瘤的类型。即使是经验丰富的病理学家，这也是一项困难的任务，而且在许多情况下，仍需要后续检查来确认诊断。如果能有一种辅助技术来指导病理学家并加速他们的工作，那将具有极大的价值。
- en: VGG16 is one of the winner models on the ImageNet Challenge from 2014\. It is
    a stacked CNN network, using kernels of size ![](img/Formula_B16391_09_026.png)
    with an increasing depth—that is, with an increasing number of filters. The original
    network was trained on the ImageNet dataset, containing images ![](img/Formula_B16391_09_089.png),
    referring to more than 1,000 classes.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16是2014年ImageNet挑战赛中的获胜模型之一。它是一个堆叠的卷积神经网络（CNN），使用大小为 ![](img/Formula_B16391_09_026.png)
    的卷积核，并且具有越来越深的结构——即，过滤器的数量逐渐增加。原始网络是在ImageNet数据集上训练的，该数据集包含了 ![](img/Formula_B16391_09_089.png)
    图像，涵盖了超过1,000个类别。
- en: '*Figure 9.21* shows you the network structure of the VGG16 model.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.21*展示了VGG16模型的网络结构。'
- en: 'It starts with two convolution layers, each with 64 filters. After a max pooling
    layer, again two convolution layers are used, this time each with 128 filters.
    Then, another max pooling layer is followed by three convolution layers, each
    with 256 filters. After one more max pooling layer, there are again three convolution
    layers, each with 512 filters, followed by another pooling layer and three convolution
    layers each with 512 filters. After one last pooling layer, three dense layers
    are used:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 它从两个卷积层开始，每个卷积层有64个过滤器。经过一个最大池化层后，再使用两个卷积层，这次每个卷积层有128个过滤器。然后，另一个最大池化层后跟着三个卷积层，每个卷积层有256个过滤器。再经过一个最大池化层后，又有三个卷积层，每个卷积层有512个过滤器，接着是另一个池化层和三个卷积层，每个卷积层有512个过滤器。最后经过一个池化层后，使用了三层全连接层：
- en: '![Figure 9.21 – Network structure of the VGG16 model](img/B16391_09_021.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图9.21 – VGG16模型的网络结构](img/B16391_09_021.jpg)'
- en: Figure 9.21 – Network structure of the VGG16 model
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21 – VGG16模型的网络结构
- en: In this case study, we would like to reuse the trained convolution layers of
    the VGG16 model and add some layers on top for the cancer cell classification
    task. During training, the convolution layers will be frozen and only the added
    layers will be trained.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们希望重用VGG16模型训练好的卷积层，并在其上添加一些新的层来进行癌细胞分类任务。在训练过程中，卷积层将被冻结，只有新增的层会被训练。
- en: 'To do so, we build three separate sub-workflows: one workflow to download the
    data, one workflow to preprocess the images, and a third workflow to train the
    neural network, using transfer learning. You can download the workflow with the
    three sub-workflows from the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/).
    Let’s start with the workflow to download the data.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们构建了三个独立的子工作流：一个工作流用于下载数据，一个工作流用于预处理图像，第三个工作流用于训练神经网络，使用迁移学习。你可以从KNIME Hub下载包含这三个子工作流的工作流：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/)。让我们从下载数据的工作流开始。
- en: Downloading the Dataset
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据集
- en: 'The full dataset with images of cancer cells is available as a single `tar.gz`
    file containing 374 images: [https://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html](https://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html).
    The workflow shown in *Figure 9.22* downloads the file and creates a table with
    the file path and the class information for each image:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 包含癌细胞图像的完整数据集以单个`tar.gz`文件的形式提供，包含 374 张图像：[https://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html](https://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html)。如*图
    9.22*所示的工作流下载该文件，并创建一个包含每个图像文件路径和类别信息的表格：
- en: '![Figure 9.22 – This workflow downloads the full labeled dataset of images
    of cancer cells](img/B16391_09_022.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – 这个工作流下载了完整的标注图像数据集，包含癌细胞图像](img/B16391_09_022.jpg)'
- en: Figure 9.22 – This workflow downloads the full labeled dataset of images of
    cancer cells
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – 这个工作流下载了完整的标注图像数据集，包含癌细胞图像
- en: Therefore, the workflow first defines a directory for the downloaded data using
    the `tar.gz` file into the created directory. The `.table` file.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，工作流首先使用`tar.gz`文件定义一个目录来存储下载的数据，并将其解压到创建的目录中。`.table`文件。
- en: The next step is to preprocess the images.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是预处理图像。
- en: Reading and Preprocessing the Images
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和预处理图像
- en: In the next step, the table created by the workflow in *Figure 9.22* is read
    and the images are preprocessed. Each image has dimensions 1388px, 1040px, and
    three-color channels; this means ![](img/Formula_B16391_09_090.png). To reduce
    the spatial complexity of computation, we use a similar approach as that taken
    in the paper *Histology Image Classification Using Supervised Classification and
    Multimodal Fusion* ([https://ieeexplore.ieee.org/document/5693834](https://ieeexplore.ieee.org/document/5693834)),
    where each image is chopped into 25 blocks. For this use case, we decided to chop
    each image into blocks of size ![](img/Formula_B16391_09_091.png).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，将读取*图 9.22*中工作流创建的表格，并对图像进行预处理。每张图像的尺寸为 1388px x 1040px，具有三个颜色通道；这意味着
    ![](img/Formula_B16391_09_090.png)。为了减少计算的空间复杂度，我们采用与论文*Histology Image Classification
    Using Supervised Classification and Multimodal Fusion*（[https://ieeexplore.ieee.org/document/5693834](https://ieeexplore.ieee.org/document/5693834)）中类似的方法，将每张图像切割成
    25 个块。对于这个用例，我们决定将每张图像切割成大小为 ![](img/Formula_B16391_09_091.png) 的块。
- en: 'The loading and preprocessing steps are performed by the workflow shown here
    in *Figure 9.23*:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 加载和预处理步骤由此处*图 9.23*所示的工作流执行：
- en: '![Figure 9.23 – This workflow loads and preprocesses the image](img/B16391_09_023.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.23 – 这个工作流加载并预处理图像](img/B16391_09_023.jpg)'
- en: Figure 9.23 – This workflow loads and preprocesses the image
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – 这个工作流加载并预处理图像
- en: This second workflow starts with reading the table created in the first workflow,
    including the paths to the images as well as the class information. Next, the
    **Category To Number** node encodes the different nominal class values (FL, MCL,
    and CLL) with an index, before the dataset is split into a training set and a
    test set using the **Partitioning** node. For this case study, we decided to use
    60% of the data for training and 40% of the data for testing, using stratified
    sampling on the **class** column.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个工作流从读取第一个工作流中创建的表格开始，其中包含图像路径以及类别信息。接下来，**类别到数字**节点用索引编码不同的类别值（FL、MCL 和 CLL），然后使用**分割**节点将数据集分为训练集和测试集。对于这个案例研究，我们决定使用
    60% 的数据用于训练，40% 的数据用于测试，并在**类别**列上使用分层抽样。
- en: In the **Load and preprocess images (Local Files)** component, the images are
    uploaded and preprocessed.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在**加载和预处理图像（本地文件）**组件中，图像被上传并预处理。
- en: '*Figure 9.24* here shows you the inside of this component:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.24* 显示了这个组件的内部结构：'
- en: '![Figure 9.24 – Inside of the Load and preprocess images (Local Files) component](img/B16391_09_024.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图9.24 – 加载和预处理图像（本地文件）组件的内部](img/B16391_09_024.jpg)'
- en: Figure 9.24 – Inside of the Load and preprocess images (Local Files) component
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24 – 加载和预处理图像（本地文件）组件的内部
- en: The component uses a loop to load and preprocess one image after the other.
    The **Chunk Loop Start** node, with one row per chunk, starts the loop, while
    the **Loop End** node, concatenating the resulting rows from the loop iterations,
    ends the loop.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 该组件使用循环来依次加载和预处理每一张图像。**Chunk Loop Start**节点以每个块一行的方式启动循环，而**Loop End**节点则在循环迭代结束时将结果行连接起来，结束循环。
- en: In the loop body, one image is always loaded with the **Image Reader (Table)**
    node. The image is then normalized using the **Image Calculator** node, dividing
    each pixel value by 255.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环体内，**Image Reader (Table)**节点始终加载一张图像。然后，使用**Image Calculator**节点将图像归一化，每个像素值除以255。
- en: Next, the **Image Cropper** node is used to crop the image to a size that is
    dividable by 64\. Since the original size of the images is 1388px 1040px, the
    first 44 pixels of the left side and the top 16 pixels of each image are cropped.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用**Image Cropper**节点裁剪图像到可被64整除的大小。由于原始图像的尺寸为1388px x 1040px，因此每个图像的左侧裁剪掉前44个像素，顶部裁剪掉16个像素。
- en: '*Figure 9.25* here shows you the configuration window of the node:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的*图9.25*展示了该节点的配置窗口：
- en: '![Figure 9.25 – Image Cropper node and its configuration window](img/B16391_09_025.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图9.25 – 图像裁剪器节点及其配置窗口](img/B16391_09_025.jpg)'
- en: Figure 9.25 – Image Cropper node and its configuration window
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25 – 图像裁剪器节点及其配置窗口
- en: 'Next, the **Splitter** node splits each image into 336 images of size 64 x
    64 pixels, storing each new sub-image in a new column, for a total of ~75,000
    patches. *Figure 9.26* here shows you the **Advanced** tab of the configuration
    window of the **Splitter** node, where the maximum size for each dimension of
    the resulting images has been set:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**Splitter**节点将每个图像拆分成336个64 x 64像素的小图像，将每个新子图像存储在新的一列中，共生成约75,000个补丁。此处的*图9.26*展示了**Splitter**节点配置窗口中的**高级**标签，已设置生成图像每个维度的最大尺寸：
- en: '![Figure 9.26 – The Splitter node and its configuration window](img/B16391_09_026.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图9.26 – Splitter节点及其配置窗口](img/B16391_09_026.jpg)'
- en: Figure 9.26 – The Splitter node and its configuration window
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26 – Splitter节点及其配置窗口
- en: Next, the table is transposed into one column and renamed, before the class
    information is added to each image with the **Cross Joiner** node.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，表格被转置为一列并重命名，然后通过**Cross Joiner**节点将类别信息添加到每个图像中。
- en: Now that we have the prepared images, we can continue with the last workflow.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好图像，可以继续执行最后的工作流。
- en: Training the Network
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练网络
- en: The first step of the training workflow is to define the network structure using
    *VGG16’s convolution layers* as a starting point.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工作流的第一步是定义网络结构，使用*VGG16的卷积层*作为起点。
- en: The VGG16 model was originally trained to predict the classes in the ImageNet
    dataset. Despite the 1,000 classes in the dataset, none of them matches the three
    cancer types for this study. Therefore, we recycle only the trained convolution
    layers of the VGG16 network. We will then add some new neural layers on top for
    the classification task, and finally fine-tune the resulting network to our task.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16模型最初是为了预测ImageNet数据集中的类别而训练的。尽管该数据集包含1,000个类别，但其中没有任何类别与本研究中的三种癌症类型相匹配。因此，我们只回收VGG16网络中已训练好的卷积层。然后，我们会在其上添加一些新的神经层用于分类任务，并最终对得到的网络进行微调，以适应我们的任务。
- en: 'To train the final network, we will use the **Keras Network Learner** node
    and the ~75,000 patches created from the training set images. These steps are
    performed by the workflow shown here in *Figure 9.27*:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练最终的网络，我们将使用**Keras Network Learner**节点和从训练集图像中创建的约75,000个补丁。以下步骤由*图9.27*所示的工作流执行：
- en: '![Figure 9.27 – Training workflow to train the new network to classify images
    of cancer cells](img/B16391_09_027.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图9.27 – 训练工作流以训练新的网络来分类癌细胞图像](img/B16391_09_027.jpg)'
- en: Figure 9.27 – Training workflow to train the new network to classify images
    of cancer cells
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.27 – 训练工作流以训练新的网络来分类癌细胞图像
- en: The workflow first reads the VGG16 network with the `.h5` file with the complete
    network structure and weights, or networks are saved in `.json` or `.yaml` files
    with just the network structure.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流首先读取包含完整网络结构和权重的VGG16网络的`.h5`文件，或者网络保存在仅包含网络结构的`.json`或`.yaml`文件中。
- en: In this case, we read the `.h5` file of the trained VGG16 network because we
    aim to use all of the knowledge embedded inside the network.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们读取了训练好的VGG16网络的`.h5`文件，因为我们打算利用网络中嵌入的所有知识。
- en: The output tensor of the VGG16 network has dimensions ![](img/Formula_B16391_09_092.png),
    which is the size of the output of the last max pooling layer. Before we can add
    some dense layers for the classification task, we flatten the output using the
    **Keras Flatten Layer** node.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16网络的输出张量的尺寸为![](img/Formula_B16391_09_092.png)，这是最后一个最大池化层的输出大小。在我们能够为分类任务添加一些全连接层之前，我们使用**Keras展平层**节点将输出展平。
- en: Now, a dense layer with **ReLU** activation and 64 neurons is added using a
    **Keras Dense Layer** node. Next, a **Dropout Layer** node is introduced, with
    a dropout rate of ![](img/Formula_B16391_09_093.png) Finally, one last **Keras
    Dense Layer** node defines the output of the network. As we are dealing with a
    classification problem with three different classes, the **softmax** activation
    function with three units is adopted.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用**Keras全连接层**节点添加一个具有**ReLU**激活函数和64个神经元的全连接层。接下来，引入了一个**Dropout层**节点，设置丢弃率为![](img/Formula_B16391_09_093.png)。最后，最后一个**Keras全连接层**节点定义了网络的输出。由于我们正在处理一个有三个不同类别的分类问题，因此采用了具有三个单元的**softmax**激活函数。
- en: If we were to connect the output of the last **Keras Dense Layer** node to a
    **Keras Network Learner** node, we would fine-tune all layers, including the trained
    convolution layers from the VGG16 model. We do not want to lose all that knowledge!
    So, we decided to not fine-tune the layers of the VGG16 model but to train only
    the newly added layers. Therefore, the layers of the VGG16 model must be frozen.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将最后一个**Keras全连接层**节点的输出连接到**Keras网络学习器**节点，我们将微调所有层，包括从VGG16模型中训练出来的卷积层。但我们不希望丢失所有这些知识！因此，我们决定不微调VGG16模型的层，而只训练新增的层。因此，VGG16模型的层必须被冻结。
- en: 'To freeze layers of a network, we use the **Keras Freeze Layers** node. *Figure
    9.28* here shows you the configuration window of this node:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了冻结网络的层，我们使用**Keras冻结层**节点。*图9.28*展示了该节点的配置窗口：
- en: '![Figure 9.28 – The Keras Freeze Layers node and its configuration window](img/B16391_09_028.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图9.28 – Keras冻结层节点及其配置窗口](img/B16391_09_028.jpg)'
- en: Figure 9.28 – The Keras Freeze Layers node and its configuration window
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.28 – Keras冻结层节点及其配置窗口
- en: In the configuration window, you can select the layer(s) to freeze. Later on,
    when training the network, the weights of the selected layers will not be updated.
    All other layers will be trained. We froze every layer except the ones we added
    at the end of the VGG16 network.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置窗口中，您可以选择要冻结的层。在之后训练网络时，所选层的权重将不会更新，其他所有层将会被训练。我们冻结了除了VGG16网络末端添加的层以外的所有层。
- en: In the lower branch of the workflow, we read the training data using the **Table
    Reader** node and we one-hot encode the class using the **One to Many** node.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流的下半部分，我们使用**表格读取器**节点读取训练数据，并使用**一对多**节点对类别进行一热编码。
- en: Now that we have the training data and the network structure, we can fine-tune
    it with the **Keras Network Learner** node.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练数据和网络结构，我们可以使用**Keras网络学习器**节点对其进行微调。
- en: As with all other case studies in this book, the columns for the input data
    and target data are selected in the configuration window of the **Keras Network
    Learner** node, together with the required conversion type. In this case, the
    **From Image** conversion for the input column and from Number (double) for the
    target column have been selected. Because this is a multiclass classification
    task, the **Categorical cross entropy** loss function has been adopted. To fine-tune
    this network, it has been trained for 5 epochs using a training batch size of
    64 and RMSProp with the default settings as optimizer.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的所有其他案例研究一样，输入数据和目标数据的列会在**Keras网络学习器**节点的配置窗口中选择，并选择所需的转换类型。在本例中，输入列选择了**从图像**转换，目标列选择了从数字（双精度）转换。因为这是一个多类别分类任务，所以采用了**类别交叉熵**损失函数。为了微调该网络，它已使用训练批次大小为64、优化器为RMSProp（默认设置）训练了5个epoch。
- en: Once the network has been fine-tuned, we evaluate its performance on the test
    images. The preprocessed test images, as patches of 64 x 64 px, are read with
    a **Table Reader** node. To predict the class of an image, we generate predictions
    for each of the 64 x 64px patches using the **Keras Network Executor** node. Then,
    all predictions are combined using a simple majority voting scheme, implemented
    in the **Extract Prediction** metanode.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络经过微调，我们就可以评估其在测试图像上的表现。经过预处理的测试图像，以64 x 64像素的小块形式，通过**Table Reader**节点读取。为了预测图像的类别，我们使用**Keras
    Network Executor**节点为每个64 x 64像素的小块生成预测。然后，所有预测结果通过一个简单的多数投票方案结合起来，这个方案在**Extract
    Prediction**元节点中实现。
- en: Finally, the network is evaluated using the **Scorer** node. The classifier
    has achieved 96% accuracy (fine-tuning for a few more epochs can push the accuracy
    to 98%).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用**Scorer**节点评估网络。分类器已经达到了96%的准确率（再微调几个周期可以将准确率提高到98%）。
- en: Tip
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In this use case, the VGG16 model is only used for feature extraction. Therefore,
    another approach is to apply the convolutional layers of the VGG16 model to extract
    the features beforehand and to feed them as input into a classic feedforward neural
    network. This has the advantage that the forward pass through VGG16 would be done
    only once per image, instead of doing it in every batch update.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个使用案例中，VGG16模型仅用于特征提取。因此，另一种方法是提前应用VGG16模型的卷积层来提取特征，并将其作为输入馈送到经典的前馈神经网络中。这样做的好处是，VGG16的前向传播仅需对每张图像进行一次，而不是在每次批处理更新时都进行。
- en: We could now save the network and deploy it to allow a pathologist to access
    those predictions via a web browser, for example. How this can be done using KNIME
    Analytics Platform and KNIME Server is shown in the next chapter.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以保存网络并将其部署，例如，让病理学家通过网页浏览器访问这些预测。如何使用KNIME Analytics Platform和KNIME Server来实现这一点将在下一章中展示。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored CNNs, focusing on image data.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了CNN，重点讨论了图像数据。
- en: We started with an introduction to convolution layers, which motivates the name
    of this new family of neural networks. In this introduction, we explained why
    CNNs are so commonly used for image data, how convolutional networks work, and
    the impact of the many setting options. Next, we discussed pooling layers, commonly
    used in CNNs to efficiently downsample the data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从卷积层的介绍开始，这也促成了这一新型神经网络家族的命名。在这个介绍中，我们解释了为什么CNNs如此常用于图像数据，卷积网络是如何工作的，以及各种设置选项的影响。接着，我们讨论了池化层，它通常用于CNNs中以高效地下采样数据。
- en: Finally, we put all this knowledge to work by building and training from scratch
    a CNN to classify images of digits between 0 and 9 from the MNIST dataset. Afterward,
    we discussed the concept of transfer learning, introduced four scenarios in which
    transfer learning can be applied, and showed how we can use transfer learning
    in the field of neural networks.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有这些知识付诸实践，通过从头开始构建并训练一个CNN来对MNIST数据集中的数字0到9的图像进行分类。之后，我们讨论了迁移学习的概念，介绍了四种迁移学习应用场景，并展示了如何在神经网络领域应用迁移学习。
- en: In the last section, we applied transfer learning to train a CNN to classify
    histopathology slide images. Instead of training it from scratch, this time we
    reused the convolutional layers of a trained VGG16 model for the feature extraction
    of the images.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们应用迁移学习训练了一个CNN来分类组织病理学切片图像。这一次，我们没有从头开始训练，而是重新使用了经过训练的VGG16模型的卷积层来提取图像的特征。
- en: Now that we have covered the many different use cases, we will move on to the
    next step, which is the deployment of the trained neural networks. In the next
    chapter, you will learn about different deployment options with KNIME software.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了许多不同的使用案例，接下来我们将进入下一步，即训练好的神经网络的部署。在下一章中，你将了解使用KNIME软件的不同部署选项。
- en: Questions and Exercises
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与练习
- en: What is the kernel size in a convolutional layer?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层中的核大小是什么？
- en: a) The area summarized by a statistical value
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 用统计值总结的区域
- en: b) The size of the matrix moving across an image
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 图像中移动矩阵的大小
- en: c) The number of pixels to shift the matrix
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 矩阵移动的像素数量
- en: d) The size of the area used by a layer
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 层使用的区域大小
- en: What is a pooling layer?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是池化层？
- en: a) A pooling layer is a commonly used layer in RNNs
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 池化层是RNN中常用的层
- en: b) A pooling layer summarizes an area with a statistical value
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 池化层用统计值总结一个区域
- en: c) A pooling layer is a commonly used layer in feedforward networks
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 池化层是前馈网络中常用的层
- en: d) A pooling layer can be used to upsample images
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 池化层可以用来对图像进行上采样
- en: When is transfer learning helpful?
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转移学习在什么时候有帮助？
- en: a) To transfer data to another system
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 将数据转移到另一个系统
- en: b) If no model is available
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 如果没有可用的模型
- en: c) If not enough labeled data is available
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 如果没有足够的标注数据
- en: d) To compare different models
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 用于比较不同的模型
