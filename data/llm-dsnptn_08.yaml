- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Hyperparameter Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: In this chapter, you’ll learn about the hyperparameters in LLMs and strategies
    for optimizing them efficiently. We’ll explore both manual and automated tuning
    approaches, including grid search, random search, and more advanced methods, such
    as Bayesian optimization and population-based training. You’ll also gain insights
    into handling multi-objective optimization scenarios common in LLM development.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解LLM中的超参数以及优化它们的策略。我们将探讨手动和自动调整方法，包括网格搜索、随机搜索以及更高级的方法，如贝叶斯优化和基于群体的训练。你还将深入了解在LLM开发中常见的多目标优化场景的处理。
- en: By the end, you’ll be equipped with practical tools and techniques to fine-tune
    your LLMs for optimal performance across various tasks and domains.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到最后，你将掌握实用的工具和技术，以优化你的LLM在各种任务和领域中的表现。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding hyperparameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解超参数
- en: Manual versus automated tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动与自动调整
- en: Grid and random search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格和随机搜索
- en: Bayesian optimization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Population-based methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于群体的方法
- en: Multi-objective hyperparameter optimization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多目标超参数优化
- en: Hyperparameter tuning at scale – challenges and solutions
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模化超参数调整——挑战与解决方案
- en: Understanding hyperparameters
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解超参数
- en: Hyperparameters are settings that are set before the machine learning training
    process begins and are not learned from the data. They control various aspects
    of the learning algorithm itself, such as the model’s complexity, learning rate,
    and the overall training process. Data scientists manually choose and tune these
    hyperparameters to optimize the model’s performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是在机器学习训练过程开始之前设置的参数，它们不是从数据中学习的。它们控制学习算法本身的各个方面，例如模型的复杂性、学习速率以及整体训练过程。数据科学家手动选择和调整这些超参数以优化模型的表现。
- en: 'Hyperparameters in LLMs can be broadly categorized into three groups: architectural,
    optimization, and regularization hyperparameters:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM中的超参数可以大致分为三类：架构、优化和正则化超参数：
- en: '**Architectural hyperparameters**: These define the design and structure of
    the model, determining how it processes and represents data. They are critical
    because they directly influence the model’s capacity to learn complex patterns
    and relationships in the data. The right architecture balances computational efficiency
    with performance, enabling the model to generalize well to unseen data.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构超参数**：这些定义了模型的设计和结构，决定了模型如何处理和表示数据。它们至关重要，因为它们直接影响模型学习数据中的复杂模式和关系的能力。正确的架构在计算效率和性能之间取得平衡，使模型能够很好地泛化到未见数据。'
- en: 'Parameters within this category include the following:'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本类别中的参数包括以下内容：
- en: Number of layers
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数数量
- en: Hidden size
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层大小
- en: Number of attention heads
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力头数
- en: Feed-forward dimension
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈维度
- en: Vocabulary size
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量大小
- en: '**Optimization hyperparameters**: These govern how the model learns during
    training by adjusting the parameters to minimize the loss function. They are important
    because they control the rate and manner of updates, affecting convergence speed,
    stability, and the model’s ability to reach an optimal solution. Proper tuning
    ensures efficient training without divergence or underfitting.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化超参数**：这些通过调整参数以最小化损失函数来控制模型在训练过程中的学习方式。它们很重要，因为它们控制更新速率和方式，影响收敛速度、稳定性和模型达到最优解的能力。适当的调整确保了高效的训练，避免了发散或欠拟合。'
- en: 'Parameters within this category include the following (we covered these in
    [*Chapter 7*](B31249_07.xhtml#_idTextAnchor108)):'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本类别中的参数包括以下内容（我们在[*第7章*](B31249_07.xhtml#_idTextAnchor108)中讨论过）：
- en: Learning rate
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习速率
- en: Batch size
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小
- en: Number of training steps
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练步数
- en: Warmup steps
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预热步骤
- en: Learning rate schedule
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习速率调度
- en: '**Regularization hyperparameters**: These introduce mechanisms to prevent the
    model from overfitting to the training data, ensuring it generalizes to new data.
    They are crucial because models with high capacity can easily memorize the training
    data, leading to poor performance on unseen data. Regularization techniques enforce
    constraints that encourage simplicity and robustness.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化超参数**：这些引入机制以防止模型过度拟合训练数据，确保其泛化到新数据。它们至关重要，因为高容量模型可以轻易记住训练数据，导致在未见数据上的表现不佳。正则化技术强制执行约束，鼓励简单性和鲁棒性。'
- en: 'Parameters within this category include the following (see [*Chapter 9*](B31249_09.xhtml#_idTextAnchor141)
    for more):'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此类别中的参数包括以下内容（更多内容请参阅[*第9章*](B31249_09.xhtml#_idTextAnchor141)）：
- en: Dropout rate
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout率
- en: Weight decay
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重衰减
- en: Label smoothing
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签平滑
- en: 'Let’s implement a function to create an LLM with configurable hyperparameters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个函数来创建具有可配置超参数的LLM：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this code, we define a function, `create_llm`, that allows us to easily
    create LLMs with different architectural hyperparameters. The function takes the
    following parameters:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们定义了一个函数`create_llm`，它允许我们轻松地创建具有不同架构超参数的LLMs。该函数接受以下参数：
- en: '`num_layers`: The number of transformer layers in the model. More layers can
    capture more complex patterns, but they increase computational requirements.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers`：模型中transformer层的数量。更多的层可以捕捉更复杂的模式，但它们会增加计算需求。'
- en: '`hidden_size`: The dimension of the hidden states throughout the model. This
    affects the model’s capacity to capture information.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`：模型中隐藏状态的维度。这影响模型捕捉信息的能力。'
- en: '`num_heads`: The number of attention heads in each layer. Multiple heads allow
    the model to focus on different aspects of the input simultaneously.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads`：每个层中注意力头的数量。多个头允许模型同时关注输入的不同方面。'
- en: '`ff_dim`: The dimension of the feed-forward layer in each transformer block.
    This is typically set to four times the `hidden_size`.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ff_dim`：每个transformer块中前馈层的维度。这通常设置为`hidden_size`的四倍。'
- en: '`vocab_size`: The size of the model’s vocabulary. This determines the size
    of the embedding layer and the output layer.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`：模型词汇表的大小。这决定了嵌入层和输出层的大小。'
- en: We use these parameters to create a `GPT2Config` object, which is then used
    to initialize a `GPT2LMHeadModel`. This approach allows us to easily experiment
    with different model architectures.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些参数来创建一个`GPT2Config`对象，然后使用该对象初始化一个`GPT2LMHeadModel`。这种方法允许我们轻松地实验不同的模型架构。
- en: Manual versus automated tuning
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动与自动化调整
- en: '**Manual tuning** involves adjusting hyperparameters based on intuition, experience,
    and gradual experimentation. Manual tuning allows you to leverage domain knowledge
    to explore tailored configurations systematically, but it is time-intensive, prone
    to suboptimal results, and inefficient in exploring large hyperparameter spaces.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**手动调整**涉及根据直觉、经验和逐步实验调整超参数。手动调整允许您利用领域知识系统地探索定制配置，但它是时间密集型的，容易产生次优结果，并且在探索大超参数空间方面效率低下。'
- en: '**Automated tuning**, on the other hand, uses algorithms to systematically
    explore the hyperparameter space. Automated tuning efficiently explores large
    hyperparameter spaces using algorithms to optimize performance, saving time and
    effort compared to manual tuning, but it can be computationally expensive and
    may require expertise to configure properly.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动化调整**，另一方面，使用算法系统地探索超参数空间。自动化调整通过算法优化性能，有效地探索大超参数空间，与手动调整相比，可以节省时间和精力，但可能计算成本较高，并且可能需要专业知识来正确配置。'
- en: Manual tuning is useful when domain knowledge or intuition can guide a small,
    targeted search space, especially in resource-constrained settings or for simpler
    models. Automated tuning is better for large, complex hyperparameter spaces where
    systematic exploration and optimization are required, as it can find better configurations
    more efficiently despite higher computational costs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当领域知识或直觉可以引导一个小型、有针对性的搜索空间时，手动调整很有用，尤其是在资源受限的环境或简单的模型中。自动化调整更适合大型、复杂的超参数空间，因为需要系统探索和优化，尽管计算成本较高，但它可以更有效地找到更好的配置。
- en: Let’s implement both approaches.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现两种方法。
- en: Manual tuning
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动调整
- en: 'First, we’ll implement manual tuning:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现手动调整：
- en: 'Start with the imports:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始导入：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Load a sample dataset:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个样本数据集：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Set up the manual tuning hyperparameters:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置手动调整的超参数：
- en: '[PRE3]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Conduct training with `manual_hyperparameters`:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`manual_hyperparameters`进行训练：
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Evaluate the model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型：
- en: '[PRE5]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this manual tuning example, we define a list of hyperparameter configurations
    to try. We then iterate through these configurations, creating a model for each,
    training it, and evaluating its performance. This approach allows us to systematically
    explore different model sizes and architectures.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个手动调整示例中，我们定义了一个要尝试的超参数配置列表。然后我们遍历这些配置，为每个配置创建一个模型，对其进行训练，并评估其性能。这种方法允许我们系统地探索不同的模型大小和架构。
- en: 'The manual tuning process can be guided by domain knowledge and intuition.
    For example, we might start with a small model (6 layers, 512 hidden size) and
    gradually increase the size to see how it affects performance. We choose these
    specific configurations based on common practices in transformer-based models:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 手动调优过程可以由领域知识和直觉指导。例如，我们可能从一个小的模型（6层，512隐藏大小）开始，逐渐增加大小以查看它如何影响性能。我们选择这些特定的配置是基于基于转换器模型的常见实践：
- en: The smallest configuration (6 layers, 512 hidden size) represents a compact
    model suitable for faster training and deployment
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小配置（6层，512隐藏大小）代表一个紧凑的模型，适合快速训练和部署
- en: The medium configuration (12 layers, 768 hidden size) is similar to the base
    GPT-2 model, known to perform well on many tasks
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中等配置（12层，768隐藏大小）与已知的在许多任务上表现良好的基础GPT-2模型相似
- en: The largest configuration (24 layers, 1,024 hidden size) represents a more powerful
    model that might capture more complex patterns but requires more computational
    resources
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大配置（24层，1,024隐藏大小）代表一个更强大的模型，可能能够捕捉更复杂的模式，但需要更多的计算资源
- en: Automated tuning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动调优
- en: 'Now, let’s implement a simple automated tuning approach using random search
    (we will show a more advanced random search in the next section):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用随机搜索实现一个简单的自动化调优方法（我们将在下一节中展示更高级的随机搜索）：
- en: 'Add the `import` statement and set up the random parameters:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加`import`语句并设置随机参数：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Conduct training:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行训练：
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Evaluate and print out the results:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估并打印结果：
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This random search implementation randomly selects hyperparameters from predefined
    options for each trial (no manual intervention during trials). Predefined options
    refer to the specified ranges, sets, or distributions from which random values
    for hyperparameters are sampled during the search process. For example, discrete
    hyperparameters such as the number of layers might be chosen from a set `[6, 12,
    24]`, while continuous hyperparameters such as learning rate could be sampled
    from a uniform or log-uniform distribution, such as `10^(-5)` to `10^(-3)`. These
    options define the boundaries and possible values for each hyperparameter, guiding
    the random sampling process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种随机搜索实现从预定义的选项中随机选择每个试验的超参数（试验期间没有手动干预）。预定义选项是指搜索过程中从指定的范围、集合或分布中抽取超参数随机值的指定范围、集合或分布。例如，离散超参数（如层数）可能从集合`[6,
    12, 24]`中选择，而连续超参数（如学习率）可能从均匀分布或对数均匀分布中抽取，例如从`10^(-5)`到`10^(-3)`。这些选项定义了每个超参数的边界和可能值，指导随机抽样过程。
- en: We choose to search over a discrete set of values for each hyperparameter to
    limit the search space and ensure that we’re exploring configurations that are
    known to work well for transformer models. The number of trials (10 in this case)
    is a balance between exploration and computational resources. More trials increase
    the chance of finding a good configuration but also increase the computational
    cost.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择在每个超参数上搜索一组离散值以限制搜索空间，并确保我们正在探索已知对转换器模型表现良好的配置。试验次数（本例中为10次）是探索和计算资源之间的平衡。更多的试验增加了找到良好配置的机会，但也增加了计算成本。
- en: In the subsequent sections, we will introduce other automated turning techniques
    such as grid search and more advanced random search, Bayesian optimization, the
    population-based method, and multi-objective hyperparameter optimization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍其他自动化调优技术，例如网格搜索和更高级的随机搜索、贝叶斯优化、基于群体的方法和多目标超参数优化
- en: Grid and random search
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索和随机搜索
- en: '**Grid search** and **random search** are two common methods for hyperparameter
    tuning. We covered random search in the previous section. In this section, we
    implement grid search and a more advanced version of random search.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格搜索**和**随机搜索**是两种常见的超参数调优方法。我们在上一节中介绍了随机搜索。在本节中，我们实现网格搜索和更高级的随机搜索版本。'
- en: 'Add the import and set up the grid search parameters:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加导入并设置网格搜索参数：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Train the model with the defined hyperparameters:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用定义的超参数训练模型：
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Evaluate and print out the results:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估并打印结果：
- en: '[PRE11]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Grid search exhaustively explores all combinations of hyperparameters. This
    approach is thorough but can be computationally expensive, especially for LLMs
    with many hyperparameters. In this implementation, we’re exploring `3^4` = `81`
    different configurations, which could take a significant amount of time and resources.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索全面探索所有超参数组合。这种方法非常彻底，但可能计算成本高昂，尤其是对于具有许多超参数的LLM。在本实现中，我们正在探索`3^4` = `81`种不同的配置，这可能需要大量的时间和资源。
- en: The hyperparameter ranges are chosen to cover a reasonable space of model sizes,
    from relatively small (6 layers, 512 hidden size) to quite large (24 layers, 1,024
    hidden size). This allows us to explore the trade-off between model size and performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数范围的选择旨在覆盖合理的模型大小空间，从相对较小（6层，512个隐藏层大小）到相当大（24层，1,024个隐藏层大小）。这使我们能够探索模型大小和性能之间的权衡。
- en: 'Now, let’s implement a more sophisticated random search that also includes
    optimization hyperparameters:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个更复杂的随机搜索，它还包括优化超参数：
- en: 'Add the `import` statement and set up the `advanced_random_search` hyperparameters:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加`import`语句并设置`advanced_random_search`超参数：
- en: '[PRE12]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Conduct the training:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行训练：
- en: '[PRE13]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Evaluate and print out the results:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估并打印结果：
- en: '[PRE14]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This advanced random search includes both architectural and optimization hyperparameters.
    We use `random.uniform` for `0.001`, `0.0015`, or `0.002`) such as learning rate
    and weight decay, and `random.choice` or `random.randint` for `32`, `64`, and
    `128` for batch size or selecting from a fixed set of options).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高级随机搜索包括架构和优化超参数。我们使用`random.uniform`来设置`0.001`、`0.0015`或`0.002`等，例如学习率和权重衰减，以及`random.choice`或`random.randint`来设置`32`、`64`和`128`作为批量大小或从一组固定的选项中进行选择）。
- en: 'The ranges for each hyperparameter are chosen based on common practices in
    LLM training (see also [*Chapter 7*](B31249_07.xhtml#_idTextAnchor108)):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个超参数的范围是基于LLM训练中的常见做法选择的（也请参阅[*第7章*](B31249_07.xhtml#_idTextAnchor108)）：
- en: '`1e-5` and `1e-3`, as learning rates for LLMs are typically in this range'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1e-5`和`1e-3`，因为LLM的学习率通常在这个范围内'
- en: '`8`, `16`, and `32`, which are common batch sizes that balance between computational
    efficiency and stability'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`8`、`16`和`32`，这些是常见的批量大小，在计算效率和稳定性之间取得平衡'
- en: '`2` to `5` epochs, as LLMs often converge within a few epochs on large datasets'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2`到`5`个周期，因为LLM通常在大数据集上只需要几个周期就会收敛'
- en: '`100` and `1,000` steps, which can help stabilize early training'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`100`步和`1,000`步，这有助于稳定早期训练'
- en: '`0` and `0.2`, as small amounts of weight decay can help prevent overfitting'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`和`0.2`，因为少量的权重衰减可以帮助防止过拟合'
- en: Advanced random search is better than grid search because it explores the hyperparameter
    space more efficiently by sampling randomly instead of exhaustively evaluating
    every possible combination. This flexibility allows it to focus on key hyperparameters
    that significantly impact performance, preventing redundant evaluations of less
    impactful ones. It can handle continuous parameters directly by sampling from
    distributions, unlike grid search, which requires discretization and exponentially
    grows in computational cost as the parameter space increases. By limiting the
    number of trials to a predefined budget, advanced random search can discover effective
    configurations faster and with less computational expense, making it more practical
    for large and complex models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 高级随机搜索比网格搜索更好，因为它通过随机采样而不是全面评估每个可能的组合来更有效地探索超参数空间。这种灵活性允许它专注于对性能有显著影响的键超参数，防止对影响较小的超参数进行冗余评估。它可以直接通过从分布中采样来处理连续参数，而网格搜索则需要离散化，并且随着参数空间的增加，计算成本呈指数增长。通过将试验次数限制在预定义的预算内，高级随机搜索可以更快地发现有效的配置，并且计算成本更低，使其更适合大型和复杂模型。
- en: Bayesian optimization
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: '**Bayesian optimization** is a more sophisticated approach to hyperparameter
    tuning that can be particularly effective for LLMs. It uses a probabilistic model
    to predict the performance of different hyperparameter configurations and intelligently
    selects the next configuration to try.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化**是一种更高级的超参数调整方法，对于LLM特别有效。它使用概率模型来预测不同超参数配置的性能，并智能地选择下一个要尝试的配置。'
- en: 'Let’s implement Bayesian optimization using the `optuna` library. **Optuna**
    is an open source hyperparameter optimization framework for automating the process
    of finding optimal parameters for algorithms and models. It employs advanced Bayesian
    optimization techniques, primarily the **Tree-structured Parzen Estimator** (**TPE**)
    algorithm, to efficiently search complex parameter spaces:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `optuna` 库实现贝叶斯优化。**Optuna** 是一个开源的超参数优化框架，用于自动化寻找算法和模型最优参数的过程。它采用先进的贝叶斯优化技术，主要使用
    **树结构帕累托估计器**（**TPE**）算法，以有效地搜索复杂的参数空间：
- en: 'Import optuna and set up the hyperparameters:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 optuna 并设置超参数：
- en: '[PRE15]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Conduct training:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行训练：
- en: '[PRE16]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run the optimization:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行优化：
- en: '[PRE17]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this implementation, we define an `objective` function that Optuna will optimize.
    The function creates and trains a model with the hyperparameters suggested by
    Optuna and then returns the evaluation loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中，我们定义了一个 `objective` 函数，Optuna 将对其进行优化。该函数使用 Optuna 建议的超参数创建和训练模型，然后返回评估损失。
- en: 'We use Optuna’s suggestion methods to define the search space:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Optuna 的建议方法来定义搜索空间：
- en: '`suggest_int` for integer hyperparameters such as `num_layers` and `num_epochs`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suggest_int` 用于整数超参数，如 `num_layers` 和 `num_epochs`'
- en: '`suggest_categorical` for hyperparameters with discrete options such as `hidden_size`
    and `num_heads`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suggest_categorical` 用于具有离散选项的超参数，例如 `hidden_size` 和 `num_heads`'
- en: '`suggest_loguniform` for the learning rate, as we want to search this space
    logarithmically'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suggest_loguniform` 用于学习率，因为我们想以对数方式搜索这个空间'
- en: '`suggest_uniform` for weight decay, as we want to search this space uniformly'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suggest_uniform` 用于权重衰减，因为我们想在这个空间内均匀搜索'
- en: The ranges for each hyperparameter are similar to those in our random search
    implementation based on common practices in LLM training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个超参数的范围与我们在基于随机搜索的实现中使用的范围类似，基于 LLM 训练中的常见实践。
- en: Bayesian optimization can be more efficient than grid or random search, especially
    for expensive-to-evaluate functions such as training LLMs. It uses the results
    of previous trials to inform the selection of future trials, potentially finding
    good configurations more quickly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化可能比网格搜索或随机搜索更有效，尤其是在评估成本高昂的函数，如训练 LLM 时。它使用先前试验的结果来指导未来试验的选择，可能更快地找到好的配置。
- en: Population-based methods
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于群体的方法
- en: '**Population-based training** (**PBT**) is a powerful technique that combines
    parallel search with adaptive hyperparameter tuning during the training process.
    PBT is particularly effective for problems where training can be paused and resumed
    efficiently. This is because PBT periodically evaluates and updates hyperparameters
    and model weights across a population, requiring seamless pause-and-resume capabilities.
    This adaptability ensures optimal use of computational resources and makes PBT
    ideal for tasks such as neural architecture search, reinforcement learning, and
    hyperparameter tuning, where iterative optimization is computationally intensive.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于群体的训练**（**PBT**）是一种强大的技术，它将并行搜索与训练过程中的自适应超参数调整相结合。PBT 特别适用于可以高效暂停和恢复训练的问题。这是因为
    PBT 定期评估和更新群体中的超参数和模型权重，需要无缝的暂停和恢复功能。这种适应性确保了计算资源的最佳利用，使 PBT 成为神经架构搜索、强化学习和超参数调整等任务的理想选择，在这些任务中，迭代优化计算密集。'
- en: Here, we’ll implement a simplified version of PBT to illustrate its core concepts
    and functionality.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将实现 PBT 的简化版本，以说明其核心概念和功能。
- en: 'We’ll start by creating a `SimplePBT` class that encapsulates the core functionality
    of the PBT algorithm. Let’s break down the implementation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建一个 `SimplePBT` 类，它封装了 PBT 算法的核心功能。让我们分解实现过程：
- en: 'First, initialize the class:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，初始化类：
- en: '[PRE18]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `SimplePBT` class is initialized with two main parameters:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`SimplePBT` 类使用两个主要参数进行初始化：'
- en: '`population_size`: The number of different hyperparameter configurations to
    maintain (the default is `4`)'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`population_size`: 维护的不同超参数配置的数量（默认为 `4`）'
- en: '`num_generations`: The number of iterations the PBT algorithm will run (the
    default is `5`)'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_generations`: PBT 算法将运行的迭代次数（默认为 `5`）'
- en: The `population` list will store dictionaries representing each individual in
    the population, containing hyperparameters and their corresponding performance
    scores.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`population` 列表将存储代表群体中每个个体的字典，包含超参数及其相应的性能分数。'
- en: 'Initialize the population: The `initialize_population` method creates the initial
    set of hyperparameter configurations:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化人口：`initialize_population` 方法创建初始的超参数配置集：
- en: '[PRE19]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For each individual in the population, do the following:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于种群中的每个个体，执行以下操作：
- en: '`num_layers`, `hidden_size`), are randomly selected from predefined options.
    These hyperparameters are categorical because they represent distinct, individual
    choices rather than values along a continuum.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` 和 `hidden_size` 从预定义选项中随机选择。这些超参数是分类的，因为它们代表的是离散的、个体的选择，而不是连续值。'
- en: '`learning_rate`, `weight_decay`) are sampled from specified ranges.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate` 和 `weight_decay` 从指定的范围内采样。'
- en: Each configuration is added to the `population` list with an initial score of
    `None`.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个配置都添加到 `population` 列表中，初始得分为 `None`。
- en: 'Train and evaluate: The `train_and_evaluate` method is responsible for creating
    an LLM with the given hyperparameters, setting up training arguments, initializing
    a trainer with the model and arguments, training the model, evaluating the model,
    and returning the evaluation loss:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和评估：`train_and_evaluate` 方法负责创建具有给定超参数的 LLM，设置训练参数，使用模型和参数初始化训练器，训练模型，评估模型，并返回评估损失：
- en: '[PRE20]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This method assumes the existence of `create_llm`, `TrainingArguments`, and
    `Trainer` classes, which would typically be provided by a deep learning framework
    such as Hugging Face Transformers.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此方法假设存在 `create_llm`、`TrainingArguments` 和 `Trainer` 类，这些类通常由深度学习框架（如 Hugging
    Face Transformers）提供。
- en: 'Exploit and explore: The `exploit_and_explore` method implements the core PBT
    algorithm:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用和探索：`exploit_and_explore` 方法实现了核心的 PBT 算法：
- en: '[PRE21]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It sorts the population based on their scores (a lower score indicates less
    loss). The bottom-performing half of the population is replaced with mutated versions
    of the top-performing half. This approach balances `mutate` method introduces
    variations in the hyperparameters:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它根据得分对人口进行排序（得分越低表示损失越小）。表现最差的种群下半部分被表现最好的种群变异版本所取代。这种方法平衡了 `mutate` 方法在超参数中引入的变化：
- en: '[PRE22]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It randomly selects one hyperparameter to mutate. For categorical parameters,
    it chooses a new value from predefined options. For continuous parameters like
    learning rate, it perturbs the current value within a certain range. For weight
    decay, it adds a small random value while keeping it within [`0, 0.2`].
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它随机选择一个超参数进行变异。对于分类参数，它从预定义选项中选择一个新值。对于像学习率这样的连续参数，它在一定范围内扰动当前值。对于权重衰减，它在保持其在
    `[0, 0.2]` 范围内的同时添加一个小的随机值。
- en: This mutation strategy allows both small and large changes in the hyperparameters,
    promoting diverse exploration of the hyperparameter space.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种变异策略允许对超参数进行小到大的变化，从而促进对超参数空间的多样化探索。
- en: 'Run the PBT process:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 PBT 过程：
- en: '[PRE23]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `run` method orchestrates the entire PBT process:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`run` 方法协调整个 PBT 过程：'
- en: It initializes the population.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它初始化人口。
- en: For each generation, it trains and evaluates each individual in the population
    and it performs exploitation and exploration to update the population.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一代，它训练和评估种群中的每个个体，并执行利用和探索以更新种群。
- en: After all generations, it prints the best hyperparameters and the score found.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有代数完成后，它打印出找到的最佳超参数和得分。
- en: 'Use the `SimplePBT` class: To use the `SimplePBT` class, you can simply create
    an instance and run it:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SimplePBT` 类：要使用 `SimplePBT` 类，您可以简单地创建一个实例并运行它：
- en: '[PRE24]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will start the PBT process with the default population size of `4` and
    `5` generations. You can adjust these parameters when creating the `SimplePBT`
    instance to suit your specific needs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动 PBT 过程，默认人口大小为 `4` 和 `5` 代。您可以在创建 `SimplePBT` 实例时调整这些参数以适应您的特定需求。
- en: Multi-objective hyperparameter optimization
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多目标超参数优化
- en: 'In LLM development, we often need to balance multiple objectives, such as model
    performance, inference speed, and model size. Let’s implement multi-objective
    optimization using Optuna:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 开发中，我们经常需要平衡多个目标，例如模型性能、推理速度和模型大小。让我们使用 Optuna 实现多目标优化：
- en: 'Add the `import` statement and set up the hyperparameters:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 `import` 语句并设置超参数：
- en: '[PRE25]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Conduct the training:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行训练：
- en: '[PRE26]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Carry out the evaluation:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行评估：
- en: '[PRE27]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run the multi-objective optimization:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行多目标优化：
- en: '[PRE28]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In this multi-objective optimization, we’re trying to minimize three objectives
    simultaneously:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个多目标优化中，我们试图同时最小化三个目标：
- en: Evaluation loss (model performance)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估损失（模型性能）
- en: Model size (in MB)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型大小（以 MB 计）。
- en: Inference time (simulated based on model architecture)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理时间（基于模型架构模拟）
- en: We use Optuna’s multi-objective optimization capability by specifying multiple
    directions in `create_study`. The optimization process will try to find the **Pareto
    front** (the set of solutions where improving any one objective necessitates degrading
    at least one other objective) of these objectives’ configurations, where improving
    one objective would necessarily worsen another.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在`create_study`中指定多个方向来使用Optuna的多目标优化能力。优化过程将尝试找到这些目标配置的**帕累托前沿**（任何改进一个目标都需要至少降低另一个目标的一组解决方案），其中提高一个目标必然会导致另一个目标恶化。
- en: The `objective` function now returns three values corresponding to our three
    objectives. For model size, we calculate the total number of parameters and convert
    it to MB. For inference time, we use a simple heuristic based on the model’s architecture
    in a real scenario, you would want to measure this.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`objective`函数返回三个值，对应我们的三个目标。对于模型大小，我们计算参数总数并将其转换为MB。对于推理时间，我们使用基于模型架构的简单启发式方法，在实际场景中，你可能会想测量这个值。
- en: This approach allows us to explore trade-offs between model performance, size,
    and speed. It’s particularly useful for LLM development, where we often need to
    balance these factors for different deployment scenarios.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使我们能够探索模型性能、大小和速度之间的权衡。对于LLM开发尤其有用，因为我们经常需要在不同部署场景中平衡这些因素。
- en: Hyperparameter tuning at scale – challenges and solutions
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整的规模——挑战和解决方案
- en: 'When tuning hyperparameters for LLMs, we face several challenges:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当调整LLM的超参数时，我们面临几个挑战：
- en: '**Computational cost**: Training LLMs is expensive, limiting the number of
    trials we can run'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：训练LLM很昂贵，限制了我们可以运行的试验数量'
- en: '**Long training times**: Each trial can take days or weeks, making the entire
    process very time-consuming'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长训练时间**：每个试验可能需要几天或几周，使整个过程非常耗时'
- en: '**Large search space**: LLMs have many hyperparameters, creating a vast search
    space'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大搜索空间**：LLM有很多超参数，创建了一个庞大的搜索空间'
- en: '**Sensitivity to initialization**: LLM performance can vary significantly with
    different random seeds'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对初始化的敏感性**：LLM的性能可能因不同的随机种子而有很大差异'
- en: 'To address these challenges, we can employ several strategies:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们可以采用几种策略：
- en: '**Use smaller proxy tasks**: Instead of tuning on the full task, use a smaller
    dataset or fewer training steps to get a quick estimate of performance'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用较小的代理任务**：不是在完整任务上调整，而是使用较小的数据集或更少的训练步骤来快速估计性能'
- en: '**Leverage pre-trained models**: Start from pre-trained weights and focus on
    tuning fine-tuning hyperparameters'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用预训练模型**：从预训练权重开始，专注于调整微调超参数'
- en: '**Use multi-fidelity optimization**: Start with low-fidelity evaluations (e.g.,
    few training steps) and gradually increase fidelity for promising configurations'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用多保真度优化**：从低保真度评估（例如，少量训练步骤）开始，并逐渐增加有希望配置的保真度'
- en: '**Distributed hyperparameter tuning**: Use multiple machines to explore different
    hyperparameters in parallel'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式超参数调整**：使用多台机器并行探索不同的超参数'
- en: 'Let’s implement a simple multi-fidelity optimization approach:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个简单的多保真度优化方法：
- en: 'Add the `import` statement and set up the hyperparameters:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加`import`语句并设置超参数：
- en: '[PRE29]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use a multi-fidelity strategy to train, starting with a small number of steps:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多保真度策略进行训练，从少量步骤开始：
- en: '[PRE30]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Conduct evaluation:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行评估：
- en: '[PRE31]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Prune the unpromising trials:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝没有希望的试验：
- en: '[PRE32]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Run the multi-fidelity optimization:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行多保真度优化：
- en: '[PRE33]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'There are a few aspects of this multi-fidelity approach that we should look
    at:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该关注这个多保真度方法的一些方面：
- en: We start by training each model configuration for only `100` steps, which gives
    a quick initial estimate of performance
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先只对每个模型配置进行`100`步的训练，这给出了性能的快速初始估计
- en: We then increase the number of training steps to `500` and then `2,000` for
    promising configurations
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将训练步骤的数量增加到`500`，然后对于有希望的配置增加到`2,000`
- en: We use Optuna’s pruning mechanism to early-stop unpromising trials, saving computational
    resources
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Optuna的剪枝机制来提前终止没有希望的试验，节省计算资源
- en: '`MedianPruner` stops a trial if its performance is worse than the median of
    previous trials at the same step. This allows us to focus our computational resources
    on the most promising hyperparameter configurations.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`MedianPruner`如果试验的性能低于同一步骤之前试验的中位数，则停止试验。这使我们能够将计算资源集中在最有希望的参数配置上。'
- en: 'This approach helps to address the challenges of hyperparameter tuning at scale:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有助于解决大规模超参数调整的挑战：
- en: It reduces the computational cost by quickly eliminating poor configurations
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过快速消除不良配置来降低计算成本
- en: It shortens the overall tuning time by using shorter training runs for initial
    evaluation
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过使用较短的训练运行进行初始评估来缩短整体调整时间
- en: It allows us to explore a larger search space by running more trials in the
    same amount of time
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过在相同的时间内运行更多试验来允许我们探索更大的搜索空间
- en: However, there are still limitations to this approach. The performance after
    a small number of steps may not always correlate well with the final performance,
    especially for LLMs that often require long training times to converge.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法仍然存在局限性。经过少量步骤后的性能可能并不总是与最终性能很好地相关，特别是对于需要长时间训练才能收敛的LLMs（大型语言模型）。
- en: 'To further improve hyperparameter tuning at scale, consider the following advanced
    techniques:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步改进大规模超参数调整，考虑以下高级技术：
- en: '**Distributed hyperparameter tuning**: This setup allows multiple machines
    to contribute to the same hyperparameter search, greatly speeding up the process:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式超参数调整**：这种设置允许多台机器共同参与同一超参数搜索，大大加快了过程：'
- en: '[PRE34]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Leveraging pre-trained models**: This approach starts from pre-trained models
    and focuses on tuning the fine-tuning hyperparameters and model size, which can
    be more efficient than training from scratch:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用预训练模型**：这种方法从预训练模型开始，专注于调整微调的超参数和模型大小，这比从头开始训练更有效率：'
- en: '[PRE35]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**Bayesian optimization with Gaussian processes**: For problems where we can
    only afford a small number of trials, Gaussian process-based Bayesian optimization
    can be more sample-efficient than tree-based methods such as TPE (which is Optuna’s
    default):'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于高斯过程的贝叶斯优化**：对于只能进行少量试验的问题，基于高斯过程的贝叶斯优化比基于树的TPE（Optuna的默认方法）等方法更具有样本效率：'
- en: '[PRE36]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This approach can be particularly useful for LLM tuning where each trial is
    very expensive.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种方法对于LLM调整特别有用，因为每次试验的成本都非常高。
- en: '**Asynchronous Successive Halving Algorithm** (**ASHA**): ASHA is a bandit-based
    algorithm that can be more efficient than simple pruning methods:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步逐次减半算法**（**ASHA**）：ASHA是一种基于bandit的算法，其效率可能高于简单的剪枝方法：'
- en: '[PRE37]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ASHA is particularly well-suited for large-scale hyperparameter optimization
    as it can handle asynchronous parallel optimization efficiently.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ASHA特别适合大规模超参数优化，因为它可以有效地处理异步并行优化。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Hyperparameter tuning for LLMs presents unique challenges due to the scale and
    complexity of these models. By leveraging techniques such as multi-fidelity optimization,
    distributed tuning, and advanced algorithms such as Bayesian optimization and
    ASHA, we can make this process more efficient and effective. However, it’s important
    to remember that there’s often no one-size-fits-all solution, and the best approach
    may depend on your specific use case, available resources, and the characteristics
    of your LLM task.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs的规模和复杂性，超参数调整面临着独特的挑战。通过利用多保真优化、分布式调整和贝叶斯优化、ASHA等高级算法，我们可以使这个过程更加高效和有效。然而，重要的是要记住，通常没有一种适合所有情况的解决方案，最佳方法可能取决于您的具体用例、可用资源和您的LLM任务的特性。
- en: In the next chapter, we’ll focus on LLM regularization.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点关注LLM正则化。
