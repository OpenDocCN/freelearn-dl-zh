- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Architecting LLM Solutions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 LLM 解决方案
- en: '**Large language models** (**LLMs**) have revolutionized the field of **natural
    language processing** (**NLP**) and **artificial intelligence** (**AI**), offering
    remarkable versatility in tackling a variety of tasks. However, realizing their
    full potential requires addressing certain challenges and developing effective
    LLM solutions. In this chapter, we’ll demystify the process of architecting LLM
    solutions, focusing on essential aspects such as memory, problem-solving capabilities,
    autonomous agents, and advanced tools for enhanced performance. We will be focusing
    on retrieval-augmented language models, which provide contextually relevant information,
    their practical applications, and methods to improve them further. Additionally,
    we’ll uncover the challenges, best practices, and evaluation methods to ensure
    the success of an LLM solution.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型** (**LLMs**) 已经彻底改变了 **自然语言处理** (**NLP**) 和 **人工智能** (**AI**) 领域，展现出在处理各种任务时的卓越多功能性。然而，要实现它们的全部潜力，需要解决一些挑战并开发有效的
    LLM 解决方案。本章将为您揭开构建 LLM 解决方案的神秘面纱，重点介绍内存、问题解决能力、自主代理和增强性能的高级工具等关键方面。我们将专注于基于检索增强的语言模型，这些模型能够提供上下文相关的信息，探讨它们的实际应用以及进一步改进它们的方法。此外，我们还将揭示挑战、最佳实践和评估方法，以确保
    LLM 解决方案的成功。'
- en: Building upon these foundational concepts, this chapter will equip you with
    the knowledge and techniques necessary to create powerful LLM solutions tailored
    to your specific needs. By mastering the art of architecting LLM solutions, you
    will be better prepared to tackle complex challenges, optimize performance, and
    unlock the true potential of these versatile models in a wide range of real-world
    applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基础概念的基础上，本章将为您提供必要的知识和技巧，帮助您创建强大的 LLM 解决方案，满足您的特定需求。通过掌握构建 LLM 解决方案的艺术，您将更好地准备好应对复杂的挑战，优化性能，并在各种现实应用中释放这些多功能模型的真正潜力。
- en: 'Specifically, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主题：
- en: Overview of LLM solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 解决方案概述
- en: Handling knowledge for LLM solutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理 LLM 解决方案中的知识
- en: Evaluating LLM solutions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 LLM 解决方案
- en: Identifying challenges with LLM solutions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别 LLM 解决方案中的挑战
- en: Tackling challenges with LLM solutions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应对 LLM 解决方案的挑战
- en: Leveraging LLMs to build autonomous agents
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 LLM 构建自主代理
- en: Exploring LLM solution use cases
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 LLM 解决方案的应用场景
- en: Overview of LLM solutions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 解决方案概述
- en: LLMs excel in diverse tasks such as answering questions, machine translation,
    language modeling, sentiment analysis, and text summarization. They generate unstructured
    text but can be guided to produce structured output. LLM solutions harness this
    ability and leverage custom data from knowledge bases to create targeted, valuable
    outcomes for organizations and individuals. By properly streamlining processes
    and enhancing output quality objectively, an LLM solution can unlock the true
    potential of LLM-generated content, making it more powerful and practical across
    applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 在回答问题、机器翻译、语言建模、情感分析和文本摘要等多种任务中表现出色。它们能够生成非结构化文本，但可以被引导生成结构化输出。LLM 解决方案利用这一能力，并通过知识库中的自定义数据来创造有针对性的、有价值的结果，服务于组织和个人。通过合理优化流程和客观提升输出质量，LLM
    解决方案可以释放 LLM 生成内容的真正潜力，使其在各类应用中更加强大和实用。
- en: The increasing accessibility of LLMs with pre-trained world knowledge has played
    a significant role in making these benefits more attainable for a broader audience.
    Thanks to various LLM providers and open source platforms, organizations and developers
    can now more easily adopt and integrate LLMs into their workflows. Prominent LLM
    providers, such as OpenAI (GPT-4 or GPT-3.5), Microsoft Azure, Google, and Amazon
    Bedrock, offer pre-trained models and APIs that can be seamlessly integrated into
    diverse applications. Additionally, the Hugging Face platform has made LLMs even
    more accessible by offering an extensive collection of open source models. Hugging
    Face provides a wide selection of pre-trained models and fine-tuning techniques
    while fostering an active community that continually contributes to enhancing
    LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练世界知识的LLM逐渐变得更加普及，这在使更多人群能够获得这些好处方面发挥了重要作用。得益于各种LLM提供商和开源平台，组织和开发者现在可以更轻松地采纳并将LLM集成到他们的工作流程中。知名的LLM提供商，如OpenAI（GPT-4或GPT-3.5）、Microsoft
    Azure、Google和Amazon Bedrock，提供预训练模型和API，能够无缝集成到各种应用程序中。此外，Hugging Face平台通过提供丰富的开源模型库，使LLM更加易于访问。Hugging
    Face不仅提供了广泛的预训练模型和微调技术，还培养了一个积极的社区，持续贡献于LLM的改进。
- en: As organizations and individuals harness the power of LLMs for their typical
    tasks and use cases, it is crucial to determine how to leverage custom knowledge
    effectively. This consideration ensures that LLMs are optimally utilized to address
    specific needs; this will be explored further in the *Handling knowledge for LLM
    solutions* section. By taking advantage of the increased accessibility and versatility
    of LLMs, organizations and individuals can unlock the full potential of these
    powerful models to drive innovation and improve outcomes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织和个人利用LLM的强大功能来完成日常任务和用例，至关重要的是要确定如何有效利用定制知识。这一考虑确保了LLM能够针对特定需求得到最优利用；这一点将在*LLM解决方案中的知识处理*部分进一步探讨。通过充分利用LLM日益增加的可达性和多功能性，组织和个人能够释放这些强大模型的全部潜力，推动创新并改善结果。
- en: 'Despite their impressive capabilities, LLMs face some limitations when it comes
    to solving more complex problems that they were not made to account for. Some
    of these limitations are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）具有令人印象深刻的能力，但在解决它们未被设计用来处理的复杂问题时，仍面临一些限制。以下是其中的一些限制：
- en: Inability to access up-to-date information on recent events
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法访问最新事件的信息
- en: Tendency to hallucinate facts or generate imitative falsehoods
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于虚构事实或生成模仿的虚假信息
- en: Difficulties in understanding low-resource languages
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解低资源语言的困难
- en: Lack of mathematical skills for precise calculations
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏进行精确计算的数学能力
- en: Unawareness of the progression of time
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对时间流逝的无意识
- en: 'To overcome these limitations and enhance LLMs’ problem-solving capabilities,
    advanced solutions can be developed by incorporating the following components:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制并增强LLM的解决问题能力，可以通过加入以下组件来开发先进的解决方案：
- en: '**Real-time data integration**: By connecting LLMs to real-time data sources
    such as APIs, databases, or web services, the model can access up-to-date information
    and provide more accurate responses.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时数据集成**：通过将LLM连接到实时数据源，如API、数据库或网络服务，模型可以获取最新的信息并提供更准确的回应。'
- en: '**Existing tool integration**: Incorporating existing tools and APIs into the
    LLM architecture can extend its capabilities, allowing it to perform tasks that
    would otherwise be impossible or challenging for a standalone model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有工具集成**：将现有工具和API集成到LLM架构中，可以扩展其能力，使其执行一些单独模型难以或无法完成的任务。'
- en: '**Multiple agents with different personas and contexts**: Developing a multi-agent
    system where each agent possesses a unique persona and context can help address
    the challenges of diverse problem-solving scenarios. These agents can collaborate,
    share information, and provide more comprehensive and reliable solutions.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有不同人格和背景的多个代理**：开发一个多代理系统，每个代理具有独特的人格和背景，可以帮助解决不同问题情境下的挑战。这些代理可以协作、共享信息并提供更全面、可靠的解决方案。'
- en: '*Figure 19**.1* shows an architecture that depicts the different approaches
    and methods that can be applied in an LLM solution and will be introduced in this
    chapter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.1* 显示了一种架构，展示了可以在LLM解决方案中应用的不同方法和技巧，本章将介绍这些内容：'
- en: '![Figure 19.1 – LLM solution architecture](img/B18187_19_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图19.1 – LLM解决方案架构](img/B18187_19_1.jpg)'
- en: Figure 19.1 – LLM solution architecture
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.1 – LLM 解决方案架构
- en: In the next few sections, we will dive into the individual components listed
    in this LLM solution architecture and LLM solutions in general more comprehensively.
    We will start with how knowledge is handled for LLM solutions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个部分，我们将更全面地深入探讨此 LLM 解决方案架构中列出的各个组件以及 LLM 解决方案的整体内容。我们将从如何处理 LLM 解决方案中的知识开始。
- en: Handling knowledge for LLM solutions
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理 LLM 解决方案中的知识
- en: Domain knowledge is key in creating LLM solutions as it provides the background
    information and understanding they need to solve specific problems. This ultimately
    ensures the answers or actions the solutions come up with are on point and helpful.
    Domain knowledge needs to be included as context either as part of parametric
    memory, non-parametric memory, or a combination of both. Parametric memory refers
    to the parameters that are learned in an LLM. Non-parametric memory refers to
    an external library of knowledge, such as a list of documents, articles, or excerpts,
    that can be selectively chosen to be injected as part of the LLM context. This
    process is also referred to as an in-context learning method, knowledge retrieval,
    or information retrieval.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 域知识是创建 LLM 解决方案的关键，它提供了解决特定问题所需的背景信息和理解。这最终确保了解决方案得出的答案或行动是准确的且有帮助的。域知识需要作为上下文包含在内，作为参数化记忆、非参数化记忆的一部分，或两者的结合。参数化记忆指的是在
    LLM 中学习到的参数。非参数化记忆指的是外部知识库，如文档、文章或摘录列表，可以根据需要选择注入为 LLM 上下文的一部分。这个过程也被称为上下文学习方法、知识检索或信息检索。
- en: 'Non-parametric external knowledge can be provided to an LLM through either
    of the following options:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数化的外部知识可以通过以下任一方式提供给 LLM：
- en: '**As a latent conditioner in the cross-attention mechanism**: Latent conditioning
    involves generating latent feature vectors from the external knowledge and feeding
    it as part of the key and value vectors in the attention mechanism that were introduced
    in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092), *Understanding Neural Network
    Transformers*, while the original input is passed in as the query vector. This
    approach typically requires some form of fine-tuning the decoder part of the network
    in an encoder-decoder transformer architecture. Ideally, the fine-tuning process
    will build a decoder that can generalize to the domain of the intended external
    latent features and can attend to a variety of information. This approach allows
    the inclusion of any data modality as external knowledge. Notably, the **Retrieval
    Augmented Generation** (**RAG**) and **Retrieval-Enhanced Transformer** (**RETRO**)
    methods from published research papers [1][2] use this approach.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为交叉注意力机制中的潜在条件**：潜在条件涉及从外部知识生成潜在特征向量，并将其作为键值向量的一部分输入到注意力机制中，该机制在[*第 6 章*](B18187_06.xhtml#_idTextAnchor092)《理解神经网络转换器》中已介绍，同时原始输入作为查询向量传入。此方法通常需要对网络的解码器部分进行某种形式的微调，尤其是在编码器-解码器的转换器架构中。理想情况下，微调过程将构建一个解码器，使其能够推广到目标外部潜在特征的领域，并能够关注各种信息。此方法允许将任何数据模态作为外部知识进行包含。值得注意的是，发布的研究论文中的**检索增强生成**（**RAG**）和**检索增强转换器**（**RETRO**）方法[1][2]采用了这种方法。'
- en: '**As part of an LLM’s input prompt**: This is a straightforward process that
    doesn’t require any fine-tuning but can still benefit from it. This approach brings
    the lowest barrier of entry to leverage any custom domain knowledge in LLMs. However,
    this approach only supports knowledge represented in data modalities that can
    be effectively represented as textual data, such as text, numerical, categorical,
    and date data. Notably, the **Retrieval-Augmented Language Model Pre-Training**
    (**REALM**) method, as part of a published research paper [3], uses this approach
    for pre-training specifically and doesn’t use it as part of the final trained
    model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为 LLM 输入提示的一部分**：这是一个简单的过程，不需要任何微调，但仍然可以从中受益。这种方法为将任何自定义领域知识应用于 LLM 提供了最低的入门门槛。然而，这种方法仅支持以能够有效表示为文本数据的数据模态表示的知识，如文本、数值、类别和日期数据。值得注意的是，发布的研究论文中的**检索增强语言模型预训练**（**REALM**）方法[3]，特别用于预训练，并未作为最终训练模型的一部分使用。'
- en: 'Both methods require a knowledge base to be established, as depicted in *Figure
    19**.2*, and a knowledge retrieval component that retrieves information from the
    knowledge base, as depicted in *Figure 19**.1*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法都需要建立一个知识库，如*图 19.2*所示，并且需要一个知识检索组件来从知识库中检索信息，如*图 19.1*所示：
- en: '![Figure 19.2 – Establishing a knowledge base](img/B18187_19_2.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 19.2 – 构建知识库](img/B18187_19_2.jpg)'
- en: Figure 19.2 – Establishing a knowledge base
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.2 – 构建知识库
- en: 'A short tabular summary of the REALM, RETRO, and RAG methods is presented in
    *Table 19.1*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 19.1* 简要总结了REALM、RETRO和RAG方法：'
- en: '| **Knowledge** **retrieval methods** | **Retriever training** | **Retrieval
    integration** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **知识** **检索方法** | **检索器训练** | **检索集成** |'
- en: '| RAG | Fine-tune with a frozen base network | Latent conditioner with cross-attention
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| RAG | 使用冻结的基础网络进行微调 | 带有交叉注意力的潜在调节器 |'
- en: '| REALM | Full end-to-end training | Prepend to prompt specifically without
    a template |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| REALM | 完整的端到端训练 | 特别地将其预先加入提示中，不使用模板 |'
- en: '| RETRO | Fine-tune with a frozen base network | Latent conditioner with cross-attention
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| RETRO | 使用冻结的基础网络进行微调 | 带有交叉注意力的潜在调节器 |'
- en: Table 19.1 – Short overview of retrieval integration with LLM methods
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19.1 – 与LLM方法的检索集成简短概述
- en: A knowledge base requires that text data is pre-processed into appropriate logical
    chunks or segments. These segments are then transformed into embedding vectors
    using trained transformer models. Finally, a nearest neighbor index is constructed,
    enabling efficient retrieval of relevant information from the knowledge base.
    The nearest neighbor index can either be a simple KNN algorithm that computes
    raw distances between the prompt embedding vector, or an approximate KNN algorithm
    that approximates the distance computations. Both the index and the logical text
    chunks will then serve as the knowledge base, which can be used for retrieval.
    The method to perform retrieval can vary with different strategies, but the simplest
    form involves simply generating embedding from the prompt and returning the top
    *k* closest text chunks from the knowledge base using the index. These top *k*
    closest text chunks can then be included as part of the LLM prompt or as a latent
    conditioner.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库要求将文本数据预处理成适当的逻辑块或段落。这些段落随后通过训练好的变换器模型转化为嵌入向量。最后，构建最近邻索引，使得能够高效地从知识库中检索相关信息。最近邻索引可以是简单的KNN算法，用来计算提示嵌入向量之间的原始距离，或者是近似KNN算法，近似计算距离。然后，索引和逻辑文本段落将作为知识库的一部分，可以用于检索。执行检索的方法可以根据不同策略有所不同，但最简单的形式是通过生成提示的嵌入向量，并使用索引返回知识库中与之最接近的前*k*个文本段落。这些前*k*个最接近的文本段落可以作为LLM提示的一部分或作为潜在调节器包含进去。
- en: For the approach of including the most relevant text chunks as part of the prompt,
    crafting a prompt template that can allow a specific spot to be inserted is the
    standard and can help organize information in a prompt properly. This can be as
    simple as using leading text such as `Context:`, following up with the retrieved
    relevant text chunks, and having new line separation before and after the context
    part of the prompt template.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将最相关的文本切块作为提示的一部分的方式，制作一个可以插入特定内容的提示模板是标准做法，并有助于合理地组织提示中的信息。这可以像使用引导性文本`Context:`一样简单，接着是检索到的相关文本切块，并在提示模板的上下文部分之前和之后插入换行符。
- en: While research papers often present published methods that encompass various
    aspects of the retrieval process in a single method, it is helpful to consider
    each component of building and using the knowledge base as separate, interchangeable
    parts. This allows for greater flexibility in selecting the most suitable components
    for specific situations. Moreover, although there is a published method known
    as RAG, it is worth noting that, in practice, the term RAG is commonly used to
    describe the general approach of integrating knowledge retrieval with LLMs, rather
    than referring solely to that specific method. Let’s briefly go through the three
    key method-based components that can be freely modified according to the use case.
    We will also choose orchestrator tools that help streamline the implementation
    of these components.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究论文通常会提出涵盖检索过程各个方面的已发布方法，但将构建和使用知识库的每个组件视为独立的、可互换的部分是很有帮助的。这允许在特定情况下选择最合适的组件，从而具有更大的灵活性。此外，尽管存在一种已发布的方法被称为RAG，但值得注意的是，在实践中，RAG这一术语通常用来描述将知识检索与LLM集成的通用方法，而不仅仅指该特定方法。接下来，我们将简要介绍可以根据用例自由修改的三大基于方法的组件。我们还将选择帮助简化这些组件实现的协调工具。
- en: Exploring chunking methods
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索文本切块方法
- en: 'The text chunking process affects the efficiency of LLM context utilization
    and the quality of the resulting LLM generation. Choosing an appropriate chunking
    method depends on the following factors:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文本切块过程会影响LLM上下文利用效率和生成结果的质量。选择合适的切块方法取决于以下因素：
- en: '**The embedding model used for embedding vector generation**: Different pre-trained
    embedding models may have different requirements or limitations when it comes
    to text chunking. Two such requirements are the supported context size and the
    typical text context size that was used during pre-training.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于生成嵌入向量的嵌入模型**：不同的预训练嵌入模型在文本切块时可能有不同的要求或限制。两个这样的要求是支持的上下文大小和预训练期间使用的典型文本上下文大小。'
- en: '**The granularity of information needed for the expected prompts that will
    be made**: The level of detail or granularity required for the prompts can impact
    the choice of text chunking method. Depending on the specific use case, the method
    should be able to chunk the text into appropriate and concise segments that provide
    the necessary information for the LLM to generate accurate, concise, and relevant
    responses.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预期提示所需信息的粒度**：预期提示所需的详细程度或粒度可能会影响文本切块方法的选择。根据具体的使用案例，方法应能够将文本切分成合适且简洁的片段，从而为LLM提供必要的信息，以生成准确、简洁且相关的响应。'
- en: '**The nature of the text data used to build a knowledge base**: The characteristics
    of the text data itself can also influence the choice of text chunking method.
    For example, if the text data consists of long paragraphs or documents, a method
    that breaks the text into smaller chunks or sections may be more suitable. On
    the other hand, if the text data is already organized into logical segments, a
    method that preserves these segments may be preferred. Also, if the text data
    is Python code, it can be suitable to chunk the text by code methods.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于构建知识库的文本数据的性质**：文本数据本身的特征也可能影响文本切块方法的选择。例如，如果文本数据由长段落或文档组成，可能更适合采用将文本拆分成较小切块或章节的方法。另一方面，如果文本数据已经被组织成逻辑段落，则可能更倾向于选择能够保留这些段落的方法。此外，如果文本数据是Python代码，使用代码方法对文本进行切块可能更为合适。'
- en: There are several methods for chunking text, including sentence, paragraph,
    entity, topic, and section chunking. These methods help organize the text into
    meaningful units that can be processed by the LLM. One notable method that is
    useful and readily available is the recursive chunking method from the LangChain
    library. This method allows you to adjust the granularity of the chunks by recursively
    splitting the text using an ordered list of text separators, a maximum chunk size,
    and the percent of overlap between chunks. The maximum chunk size should be tailored
    to the context size supported by the embedding model, ensuring that the generated
    chunks can be effectively processed. Meanwhile, incorporating an overlap percentage
    helps minimize the risk of missing critical information that could be located
    at the boundaries of chunks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以进行文本切块，包括句子、段落、实体、主题和章节切块。这些方法帮助将文本组织成可以被LLM处理的有意义单元。一个值得注意且易于使用的方法是来自LangChain库的递归切块方法。该方法通过递归地使用有序文本分隔符列表、最大切块大小和切块之间的重叠百分比来调整切块的粒度。最大切块大小应根据嵌入模型支持的上下文大小量身定制，以确保生成的切块能够有效处理。与此同时，加入重叠百分比有助于最小化错过关键信息的风险，特别是那些可能位于切块边界的信息。
- en: Many document-specific chunking methods are created based on this recursive
    chunking method by specifying the appropriate ordered list of text separators.
    Specifically, as of `langchain==0.0.314`, recursive methods have been created
    for Python code with `PythonCodeTextSplitter`, markdown documents with the `MarkdownTextSplitter`
    class, and LaTeX-formatted text with the `LatexTextSplitter` class.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于递归切块方法的文档特定切块方法是通过指定适当的文本分隔符的有序列表创建的。具体来说，截至`langchain==0.0.314`，已经为Python代码创建了递归方法（`PythonCodeTextSplitter`），为Markdown文档创建了`MarkdownTextSplitter`类的递归方法，以及为LaTeX格式文本创建了`LatexTextSplitter`类的递归方法。
- en: Next, let’s dive into embedding model choices.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨嵌入模型的选择。
- en: Exploring embedding models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索嵌入模型
- en: Embedding models play a crucial role in generating a knowledge base for LLM
    solutions. These models are responsible for encoding the semantic information
    of text into vector representations, which are then used to retrieve relevant
    information from the knowledge base.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型在为 LLM 解决方案生成知识库中起着至关重要的作用。这些模型负责将文本的语义信息编码为向量表示，然后这些向量用于从知识库中检索相关信息。
- en: One benchmark that provides insights into the performance of text embedding
    models is the `text-embedding-ada-002`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提供文本嵌入模型性能洞察的基准是 `text-embedding-ada-002`。
- en: When selecting an embedding model for knowledge base generation, it is essential
    to consider factors such as model size, embedding dimensions, and sequence length.
    Traditional embedding models such as GloVe offer high speed but may lack context
    awareness, resulting in lower average scores. On the other hand, models such as
    `all-mpnet-base-v2` and `all-MiniLM-L6-v2` strike a balance between speed and
    performance, providing satisfactory results. For maximum performance, larger models
    such as `bge-large-en-v1.5`, `ember-v1`, and `e5-large-v2` dominate the MTEB leaderboard,
    all with a 1.34 GB model size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择用于生成知识库的嵌入模型时，必须考虑模型大小、嵌入维度和序列长度等因素。传统的嵌入模型，如 GloVe，提供高速度，但可能缺乏上下文意识，导致较低的平均得分。另一方面，像
    `all-mpnet-base-v2` 和 `all-MiniLM-L6-v2` 这样的模型在速度和性能之间取得了平衡，提供了令人满意的结果。为了获得最佳性能，像
    `bge-large-en-v1.5`、`ember-v1` 和 `e5-large-v2` 这样的更大模型主导了 MTEB 排行榜，所有这些模型的大小为
    1.34 GB。
- en: It’s important to note that the choice of embedding model depends on the specific
    task and dataset being used. Therefore, thoroughly exploring the various tabs
    of the MTEB leaderboard and considering the requirements of the knowledge base
    generation process can help in selecting the most suitable embedding model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，嵌入模型的选择取决于具体的任务和使用的数据集。因此，深入探索 MTEB 排行榜的不同选项，并考虑知识库生成过程的要求，有助于选择最合适的嵌入模型。
- en: MTEB, with its extensive collection of datasets and evaluation metrics, serves
    as a valuable resource for researchers and practitioners in the field of NLP.
    By leveraging the insights provided by MTEB, developers can make informed decisions
    when choosing an embedding model for knowledge base generation in LLM solutions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MTEB 通过其丰富的数据集和评估指标集合，为 NLP 领域的研究人员和实践者提供了宝贵的资源。通过利用 MTEB 提供的洞察，开发者可以在为 LLM
    解决方案选择嵌入模型时做出明智的决策。
- en: Before we delve into exploring the knowledge base index types, it’s essential
    to remember that the choice of chunking method and embedding model shapes the
    construction of your knowledge base. Both components play a crucial part in how
    effectively the LLM can retrieve and utilize knowledge. Now, let’s dive deeper
    into the world of knowledge base index types and learn how they contribute to
    the efficiency of LLM solutions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探索知识库索引类型之前，必须记住，分块方法和嵌入模型的选择决定了知识库的构建方式。这两个组成部分在 LLM 能够高效地检索和利用知识方面发挥着重要作用。现在，让我们更深入地了解知识库索引类型，并学习它们如何提升
    LLM 解决方案的效率。
- en: Exploring the knowledge base index types
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索知识库索引类型
- en: The knowledge base index is the backbone of retrieval mechanisms in LLM solutions.
    It is the component that facilitates the efficient lookup of relevant information.
    While there are several ways of implementing this index, they all aim to provide
    a fast and efficient way of retrieving the most relevant text chunks from the
    knowledge base based on the input prompt.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库索引是大规模语言模型（LLM）解决方案中检索机制的支柱。它是实现高效查找相关信息的组成部分。虽然实现这个索引的方法有很多种，但它们的目标都是提供一种快速高效的方式，从知识库中基于输入提示检索最相关的文本片段。
- en: 'Many options are available for building a knowledge base index. They range
    from manual code implementations to using various vector database libraries, service
    providers, and plugins. Some of these options are listed here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 构建知识库索引有多种选择。它们从手动代码实现，到使用各种向量数据库库、服务提供商和插件都有。以下是一些可选方案：
- en: '`faiss`, a library for efficient similarity search of dense vectors, and `scipy`,
    a library for pairwise distance computations. This allows for customization but
    may require more effort and expertise while requiring bigger RAM allocations.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`faiss` 是一个用于高效相似度搜索稠密向量的库，`scipy` 是一个用于计算成对距离的库。这允许自定义，但可能需要更多的努力和专业知识，并且需要更大的内存分配。'
- en: '**Service providers**: Various cloud providers offer vector database services.
    These include Pinecone, Chroma, Vespa, and Weaviate. These services handle the
    complexities of managing a vector database, providing scalable and robust solutions
    that can be easily integrated into your LLM architecture and solution.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务提供商**：各种云服务提供商提供向量数据库服务，包括Pinecone、Chroma、Vespa和Weaviate。这些服务处理了管理向量数据库的复杂性，提供了可扩展且强大的解决方案，可以轻松地集成到你的LLM架构和解决方案中。'
- en: '**Database tools with vector computation support**: Traditional database tools
    such as MongoDB, Neo4j, Redis, and PostgreSQL provide vector computation support
    through plugins. This can be a good option if you’re already using these tools
    in your tech stack and want to leverage their capabilities for your knowledge
    base.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量计算的数据库工具**：传统的数据库工具，如MongoDB、Neo4j、Redis和PostgreSQL，通过插件提供向量计算支持。如果你已经在技术栈中使用这些工具，并希望利用它们的功能来构建知识库，这可能是一个不错的选择。'
- en: '**Plugins**: There are also plugins available, directly from LLM service providers,
    such as ChatGPT, that can help with the construction and maintenance of a knowledge
    base.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：还有一些插件可以直接从LLM服务提供商获取，例如ChatGPT，它们可以帮助构建和维护知识库。'
- en: Choosing the right knowledge base index type depends on your specific requirements,
    such as the size of your knowledge base, the complexity of your retrieval needs,
    and the resources you have available. Consider factors such as scalability, ease
    of integration, cost, and the level of control you need over your knowledge base
    when making your choice. A recommendation is to only consider vector databases
    that are using the actual database technology or claim to do so when your knowledge
    base is big enough to matter. If your knowledge base is small, let’s say in the
    six-digit range, raw distance computations take less than 1 second if you make
    one prompt per compute in Python! Next, we will briefly discover orchestrator
    libraries for LLM solutions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的知识库索引类型取决于你的具体需求，比如知识库的大小、检索需求的复杂性以及你可用的资源。在做出选择时，要考虑可扩展性、集成的简便性、成本以及对知识库控制的程度。如果你的知识库已经足够大，建议只考虑使用实际数据库技术或声称使用这种技术的向量数据库。如果你的知识库比较小，例如六位数范围内，通过Python计算单次提示的原始距离计算时间少于1秒！接下来，我们将简要探索LLM解决方案的协调库。
- en: Exploring orchestrator tools for LLM solutions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索LLM解决方案的协调工具
- en: The process of architecting LLM solutions can be streamlined with the use of
    specific tools. Open sourced orchestrator libraries such as LangChain and LlamaIndex
    play a pivotal role in this context. Both tools simplify tasks such as setting
    up the knowledge base, integrating an LLM, and managing retrieval mechanisms.
    In general, an orchestrator significantly reduces the complexity and development
    time of LLM solutions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 构建LLM解决方案的过程可以通过使用特定工具进行简化。开源的协调库，如LangChain和LlamaIndex，在这个过程中发挥了重要作用。这两种工具简化了设置知识库、集成LLM和管理检索机制等任务。总的来说，协调工具显著减少了LLM解决方案的复杂性和开发时间。
- en: In addition to open sourced orchestrator tools, there are also paid options
    available that provide advanced features and support. Some of these DataRobot,
    Microsoft Azure, IBM Watson, LangSmith, OpenAI, and Google Vertex AI. These platforms
    offer a wide range of pre-built models, integrations, and tools that streamline
    the entire pipeline, from data ingestion to model deployment and monitoring.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开源的协调工具外，还有一些付费选项可供选择，这些选项提供高级功能和支持。包括DataRobot、Microsoft Azure、IBM Watson、LangSmith、OpenAI
    和 Google Vertex AI。这些平台提供了广泛的预构建模型、集成和工具，能够简化整个流程，从数据摄取到模型部署和监控。
- en: As you continue to explore these tools and methods, it’s crucial to establish
    robust evaluation methods to measure the impact of these components on your solution,
    ensuring it meets its intended objectives. We’ll delve deeper into these methods
    in the upcoming section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续探索这些工具和方法的同时，建立强大的评估方法至关重要，以衡量这些组件对你的解决方案的影响，确保它达成预期目标。我们将在接下来的部分深入探讨这些方法。
- en: Evaluating LLM solutions
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LLM解决方案
- en: Evaluating LLM solutions is a crucial step in harnessing their full potential
    and ensuring their effectiveness in various applications. By implementing a comprehensive
    set of evaluation approaches, organizations can better assess the performance,
    accuracy, and overall quality of the results from an LLM solution, while also
    considering the associated costs, adherence to safety standards, and potential
    negative impact on users. In other words, doing this provides you with valuable
    insights to help make any informed decisions. To achieve a comprehensive evaluation,
    we can view evaluation methods as part of either a quantitative measure or a qualitative
    measure. Let’s dive into evaluation methods by these groups.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM解决方案是充分发挥其潜力并确保其在各种应用中有效性的关键步骤。通过实施一套全面的评估方法，组织可以更好地评估LLM解决方案的性能、准确性和整体质量，同时考虑相关的成本、安全标准的遵守情况，以及对用户的潜在负面影响。换句话说，这样做为你提供了有价值的见解，帮助做出有根据的决策。为了实现全面评估，我们可以将评估方法视为定量衡量或定性衡量的一部分。让我们按这些组别深入探讨评估方法。
- en: Evaluating LLM solutions through quantitative metrics
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过定量指标评估LLM解决方案
- en: 'Quantitative metrics can be aggregated throughout a provided evaluation dataset
    and can provide a more quick, comprehensive, and objective measure to compare
    multiple LLM solution setups. Here are some examples of quantitative metrics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定量指标可以在提供的评估数据集中进行汇总，能够提供一种更快速、全面和客观的衡量方式，用于比较多个LLM解决方案设置。以下是一些定量指标的例子：
- en: '**Comprehension and fluency-based metrics**: Flesch Reading Ease, Coleman Liau
    Index, and SMOG readability.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解和流利度相关的指标**：Flesch阅读难易度、Coleman Liau指数和SMOG可读性。'
- en: '**Facts-based metrics**: Any metrics that use the facts provided by a knowledge
    base for inference:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于事实的指标**：任何使用知识库提供的事实进行推理的指标：'
- en: '**Factual consistency**: This refers to comparing generated text with facts
    stated in the knowledge base. It is important to note that relevant facts might
    not always be available in the knowledge base. This metric is also known as the
    extractiveness metric. To measure factual consistency, you can use either semantic
    similarity, which focuses on differences in the meaning of the text, or lexical
    similarity, which emphasizes matching words in the text.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实一致性**：指的是将生成的文本与知识库中陈述的事实进行比较。需要注意的是，相关的事实可能并不总是出现在知识库中。这个指标也被称为抽取性指标。要衡量事实一致性，可以使用语义相似度，侧重于文本含义上的差异，或者使用词汇相似度，强调文本中词汇的匹配。'
- en: '**Factual relevance**: This is about how relevant the provided facts are, without
    considering the LLM generation. This is possible when you have ranked relevant
    document labels.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实相关性**：这是指所提供的事实的相关性，而不考虑LLM生成的内容。只有在你有排名的相关文档标签时，才可以实现这一点。'
- en: '**Generated text relevance/accuracy metric**: This metric evaluates the relevance
    and accuracy of the text generated by an LLM in comparison to an ideal ground
    truth. It can be computed using similarity metrics or self-evaluation techniques.
    Self-evaluation can be further broken down into the following areas:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成文本的相关性/准确性指标**：该指标评估由大语言模型（LLM）生成的文本与理想真实情况的相关性和准确性。可以通过相似度度量或自我评估技术来计算。自我评估可以进一步细分为以下几个方面：'
- en: '**With access to token probabilities**: The average of log probabilities is
    used to assess the quality of the generated text. Higher log probabilities indicate
    that the model is more confident in its output, suggesting greater relevance and
    accuracy.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有访问令牌概率**：通过计算对数概率的平均值来评估生成文本的质量。较高的对数概率表明模型对其输出更有信心，意味着相关性和准确性较高。'
- en: '**Without access to token probabilities**: **SelfCheckGPT** is a method that
    can be employed to evaluate the generated text without relying on token probabilities.
    This approach leverages the LLM’s capabilities to assess the quality of its generated
    content, providing an alternative measure of relevance and accuracy.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有访问到令牌概率**：**SelfCheckGPT**是一种可以评估生成文本的方法，无需依赖令牌概率。该方法利用LLM的能力评估其生成内容的质量，提供了一种替代的相关性和准确性衡量方式。'
- en: '**Runtime metrics**: The time taken to generate text, the number of tokens
    processed, and so on.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时指标**：生成文本所需的时间、处理的令牌数等。'
- en: '**Cost metrics**: The number of tokens generated, API call costs, hosting costs,
    and so on.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本指标**：生成的令牌数、API调用成本、托管成本等。'
- en: '**Guardrail violations metrics**: The percentage of outputs that violate predefined
    standards. Examples of guardrails are toxicity levels and hate speech degree.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防护措施违规指标**：违反预定义标准的输出百分比。防护措施的例子包括有毒内容水平和仇恨言论程度。'
- en: '**Adversarial performance metrics**: The performance measures in handling adversarial
    inputs. These were introduced more comprehensively in [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206),
    *Analyzing* *Adversarial Performance*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗性性能指标**：衡量处理对抗性输入的性能。这些指标在 [*第14章*](B18187_14.xhtml#_idTextAnchor206) 中有更为详细的介绍，*分析*
    *对抗性性能*。'
- en: '**Bias and fairness metrics**: Quantitative measures for assessing biases in
    the generated text. These were introduced more comprehensively in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见和公平性指标**：用于评估生成文本中的偏见的定量度量。这些在 [*第13章*](B18187_13.xhtml#_idTextAnchor196)
    中有更为详细的介绍，*探索偏见* *与公平性*。'
- en: '**Any supervised classification or regression metrics**: This can be applied
    to the results or resulting actions from an LLM solution.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任何监督式分类或回归指标**：可以应用于 LLM 解决方案的结果或结果行动。'
- en: Note that quantitative methods such as generated text relevance and factual
    consistency metrics, which use similarity metrics to compare two sets of text,
    are not as reliable as supervised model metrics such as accuracy. These metrics
    should be taken with a grain of salt. Additionally, a nice bonus with quantitative
    metrics is that they can be used for monitoring a deployed model programmatically.
    Next, we will dive into qualitative manual evaluations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，像生成文本相关性和事实一致性度量这样的定量方法，通过使用相似度度量来比较两组文本，并不像监督模型的度量（如准确度）那样可靠。这些指标应谨慎对待。此外，定量指标的一个附加好处是，它们可以用于程序化地监控已部署的模型。接下来，我们将深入探讨定性手动评估。
- en: Evaluating LLM solutions through qualitative evaluation methods
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过定性评估方法评估 LLM 解决方案
- en: 'Qualitative methods, which involve human feedback and manual assessments, complement
    quantitative measures and provide a comprehensive understanding of LLM performance.
    It can also sometimes be the only way for evaluation when there are no reference
    ground truth datasets. Here are some examples of qualitative LLM solution evaluation
    methods:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定性方法涉及人类反馈和手动评估，补充了定量度量，并提供了对 LLM 性能的全面理解。当没有参考的真实数据集时，它有时也是唯一可用的评估方法。以下是一些定性
    LLM 解决方案评估方法的示例：
- en: '**Human feedback scores**: These are the users’ ratings or rankings of generated
    responses to gauge effectiveness and relevance. Examples include grammar and coherence
    of text.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈评分**：这是用户对生成响应的评分或排名，用于评估其有效性和相关性。示例包括文本的语法和连贯性。'
- en: '**Generated text relevance evaluation**: This involves manually assessing the
    generated text’s relevance to the given context or prompt.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成文本相关性评估**：这涉及手动评估生成文本与给定上下文或提示的相关性。'
- en: '**Prediction explanations**: These assess the reasoning behind the generated
    text or predictions, which can help identify potential biases or faulty logic
    in the LLM solution.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测解释**：评估生成文本或预测背后的推理过程，这有助于识别 LLM 解决方案中的潜在偏见或错误逻辑。'
- en: '**Ethical and legal compliance**: This ensures that the generated text adheres
    to ethical and legal guidelines through manual review.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理与法律合规性**：通过手动审查确保生成的文本符合伦理和法律指南。'
- en: By employing a combination of quantitative metrics and qualitative manual evaluations,
    organizations can gain a deeper understanding of LLM performance and identify
    potential areas for improvement. In general, try to treat LLM solutions as no
    different from any supervised machine learning projects and evaluate them vigorously,
    similarly to how you would in a supervised machine learning project. This holistic
    approach to evaluating LLM solutions not only ensures consistent performance and
    compliance but also helps in aligning these powerful models with specific needs
    and objectives, driving innovation and improving outcomes in various applications.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合定量指标和定性手动评估，组织可以更深入地了解 LLM 的表现，并识别潜在的改进领域。通常，应该将 LLM 解决方案视为与任何监督式机器学习项目没有区别，并像评估监督式机器学习项目一样，严格评估它们。这种全面评估
    LLM 解决方案的方法不仅确保了一致的表现和合规性，还帮助将这些强大的模型与特定的需求和目标对齐，从而推动创新并改善各种应用中的成果。
- en: Identifying challenges with LLM solutions
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别 LLM 解决方案中的挑战
- en: 'Despite their impressive capabilities, LLMs face challenges when solving complex
    real-world problems. In this section, we will explore some of the challenges faced
    by LLM solutions and discuss possible ways to tackle them. We will explore challenges
    by high-level groups, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM具有令人印象深刻的能力，但在解决复杂的现实世界问题时，它们面临许多挑战。在本节中，我们将探讨LLM解决方案面临的一些挑战，并讨论可能的应对方式。我们将按高层次的群组来探讨这些挑战，如下所示：
- en: '**Output and** **input limitations**:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出和** **输入限制**：'
- en: '**LLMs just produce text**: Text output can help provide value for a lot of
    businesses. However, many other use cases require predictions and recommendations
    in entirely different formats.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM只是生成文本**：文本输出可以为许多企业提供价值。然而，许多其他用例需要以完全不同的格式进行预测和推荐。'
- en: '**The context size of an LLM is limited**: The issue is that with a large input
    size, you need exponentially more compute resources to train and predict. So,
    context size usually stays in a token range of one to three thousand. This issue
    should be prevalent only for use cases that require long context, as a few thousand
    context sizes should be enough for most use cases.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM的上下文大小有限**：问题在于，随着输入大小的增大，训练和预测所需的计算资源会呈指数级增加。因此，上下文大小通常保持在一到三千个标记的范围内。这个问题应该只在需要长上下文的用例中普遍存在，因为对于大多数用例，几千个上下文大小应该足够。'
- en: '**An LLM is a text-specific model**: Other data modalities are not supported
    by default.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM是文本特定模型**：默认情况下，其他数据模式不被支持。'
- en: '**Repetitive retrieved information**: The information that’s retrieved from
    a knowledge base can be highly relevant but repetitive and numerous. As the context
    size of an LLM is limited, a risk arises when multiple pieces of information are
    placed as the context is repetitive and takes up most of the context limit quota.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复检索信息**：从知识库中检索到的信息可能高度相关，但却是重复且数量庞大的。由于LLM的上下文大小是有限的，当多个信息片段作为上下文时，可能会因为重复而占用大部分上下文限制配额，从而引发风险。'
- en: '**Knowledge and** **information-related challenges**:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识和** **信息相关挑战**：'
- en: '**Inability to access up-to-date information**: LLMs may not know about recent
    events or developments, leading to outdated or inaccurate information being provided
    in their responses.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法访问最新信息**：LLM可能不了解最近的事件或发展，导致它们提供的信息过时或不准确。'
- en: '**Handling low-resource languages**: LLMs can struggle with understanding and
    processing languages with limited data or resources.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理低资源语言**：LLM在理解和处理数据或资源有限的语言时可能会遇到困难。'
- en: '**Unawareness of the progression of time**: LLMs may not understand the concept
    of time, leading to confusion when dealing with time-sensitive information.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对时间进程的无知**：LLM可能无法理解时间的概念，这会在处理与时间敏感的信息时引发混淆。'
- en: '**Information loss**: LLMs are shown to look at the beginning and end of sentences,
    but not so much the middle, and thus lose the most information placed in the middle.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息丧失**：研究表明，LLM更倾向于关注句子的开头和结尾，而不是中间部分，因此会丧失放在中间的最重要信息。'
- en: '**Single index failures**: This challenge arises when an LLM lacks sufficient
    knowledge about a specific topic or area due to limitations in its training data.
    For instance, if you ask an LLM about a newly opened local restaurant that wasn’t
    covered in its training data, the LLM may provide limited or irrelevant information.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单一索引失败**：当LLM因其训练数据的限制而缺乏关于特定主题或领域的足够知识时，就会出现这个挑战。例如，如果你询问LLM关于一个新开张的本地餐馆，而这个餐馆没有出现在它的训练数据中，LLM可能会提供有限或不相关的信息。'
- en: '**Incomplete content retrieval from documents:** When retrieval of a chunked
    sentence gets the right document, but the actual content needed is below the retrieved
    chunk in the same document, LLMs may not provide the complete or accurate information
    required by the user.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从文档中检索不完整的内容**：当从分段句子中检索到正确的文档，但实际所需的内容位于同一文档中检索到的部分下方时，LLM可能无法提供用户所需的完整或准确的信息。'
- en: 'Example: In a documentation search for a software''s installation process,
    the LLM retrieves a section mentioning the installation, but the actual step-by-step
    instructions are located in the following section of the document. Consequently,
    the user only receives an overview without the necessary details for proper installation.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：在一个关于软件安装过程的文档搜索中，LLM检索到了提及安装的部分，但实际的逐步说明位于文档的下一部分。因此，用户只获得了一个概述，而没有得到进行正确安装所需的详细信息。
- en: '**The use irrelevant information in the context:** LLMs may use irrelevant
    information from their context as a basis for their output, essentially mimicking
    or echoing opinions found in the context even if they are not applicable or appropriate
    for the given situation. This phenomenon, referred to as sycophancy, can lead
    to misleading or unhelpful responses.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在上下文中使用无关信息**：LLM可能会将其上下文中的无关信息作为其输出的依据，本质上模仿或回响在上下文中出现的观点，即使这些观点不适用于当前的情况或不合适。这种现象被称为“拍马屁”，可能导致误导或无用的回应。'
- en: '**The global knowledge base summarization task can’t be executed accurately**:
    A retrieval process is unaware of the type of knowledge base it requests and thus
    can’t execute the global summarization task effectively.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球知识库总结任务无法准确执行**：检索过程无法了解其请求的知识库类型，因此无法有效地执行全球总结任务。'
- en: '**Accuracy** **and reliability**:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性** **和可靠性**：'
- en: '**Hallucinations**: LLMs can generate false or misleading information that
    may appear plausible but is not based on facts. This phenomenon is known as hallucination.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉**：LLM可能会生成看似合理但并非基于事实的虚假或误导性信息。这一现象被称为幻觉。'
- en: '**Lack of mathematical skills**: LLMs often cannot perform precise calculations
    or solve complex mathematical problems. This issue is more widely known as it
    is slightly controversial, depending on how you look at it.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏数学技能**：LLM通常无法进行精确计算或解决复杂的数学问题。这一问题相对广为人知，且根据不同的视角来看，稍显有争议。'
- en: '**Imitative falsehoods**: These are false statements that LLMs generate because
    they mimic common misconceptions found in the training data. Since the model learns
    from the data it’s trained on, it might inadvertently reproduce widely held but
    incorrect beliefs. For example, if many people believe that a specific food causes
    a particular illness, an LLM might generate a similar statement, even if it’s
    not scientifically accurate.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模仿虚假**：这些是LLM生成的虚假陈述，因为它们模仿了训练数据中常见的误解。由于模型从其训练数据中学习，它可能无意中再现广泛存在但错误的信念。例如，如果许多人认为某种特定的食物会引发某种特定的疾病，LLM可能会生成类似的陈述，尽管从科学角度来看并不准确。'
- en: '**Non-imitative falsehoods**: These are false statements that arise due to
    the model’s inability to fully achieve its training objective. This includes hallucinations,
    which are statements that seem plausible but are incorrect. For instance, an LLM
    might generate a statement about a historical event that never occurred, but the
    statement may sound convincing to someone who is not knowledgeable about that
    specific event.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非模仿虚假**：这些是由于模型未能完全实现其训练目标而产生的虚假陈述，包括幻觉现象，即看似合理但实际上不正确的陈述。例如，一个LLM可能会生成关于某个历史事件的陈述，而该事件根本未曾发生，但对于不了解该事件的某些人来说，这个陈述听起来可能很有说服力。'
- en: '**Runtime performance issues**: An LLM’s runtime can be slow. Additionally,
    by adding a knowledge base to it, the entire process can become slower than it
    already is.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时性能问题**：LLM的运行时间可能较慢。此外，向其添加知识库后，整个过程可能变得比原来更慢。'
- en: '**Ethical implications and societal impacts**: The widespread adoption and
    deployment of LLMs comes with several ethical implications and societal impacts.
    As these models learn from vast amounts of data, they may inadvertently inherit
    biases present in the training data, leading to biased outputs, perpetuating stereotypes,
    or promoting misinformation. Furthermore, LLMs can generate content that may inadvertently
    promote harmful behavior, hate speech, or violate privacy concerns. The following
    ethical challenges are involved in the usage of an LLM solution:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理影响和社会影响**：LLM（大规模语言模型）的广泛采用和部署带来了若干伦理影响和社会影响。由于这些模型从大量数据中学习，它们可能无意中继承训练数据中的偏见，导致偏颇的输出，延续刻板印象，或传播虚假信息。此外，LLM可能生成内容，这些内容可能无意中促进有害行为、仇恨言论或侵犯隐私问题。使用LLM解决方案时涉及以下伦理挑战：'
- en: '**Bias and fairness**: Ensuring that the LLM does not exhibit biased behavior
    or discriminate against specific user groups based on their race, gender, age,
    or other protected attributes. Consider the case that a bank uses an LLM to analyze
    loan applications and determine creditworthiness. The LLM has been trained on
    historical data, which may contain biases against certain ethnic groups. As a
    result, the LLM might reject loan applications from these groups at a higher rate,
    even when the applicants have good credit scores.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见与公平性**：确保LLM在分析贷款申请和确定信用资质时不表现出偏见或歧视特定用户群体，如种族、性别、年龄或其他受保护属性。考虑这样一个情况，银行使用LLM分析贷款申请并确定信用资质。LLM是基于历史数据训练的，这些数据可能对某些族裔群体存在偏见。因此，LLM可能会以更高的比率拒绝这些群体的贷款申请，即使申请人有良好的信用评分。'
- en: '**Privacy concerns**: LLMs may inadvertently generate **personally identifiable
    information** (**PII**) or sensitive data in their outputs, which raises privacy
    concerns and potential legal issues. Consider the case where a healthcare organization
    uses an LLM to generate personalized health recommendations for its clients. The
    LLM can inadvertently include specific patient names and medical conditions in
    the generated advice, which then gets shared publicly, violating patient privacy.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私问题**：LLM可能在其输出中无意生成个人可识别信息（PII）或敏感数据，这引发隐私问题和潜在的法律问题。考虑这样一个情况，一个医疗机构使用LLM为其客户生成个性化的健康建议。LLM可能会在生成的建议中无意包含特定患者的姓名和医疗状况，然后这些信息被公开分享，违反了患者的隐私。'
- en: '**Misinformation and disinformation**: LLMs can potentially generate misleading
    or false information, which can contribute to the spread of misinformation and
    disinformation. Consider the case where an LLM is used by a news agency to automatically
    summarize and publish news articles. The model unintentionally generates a summary
    that misrepresents the original story, leading to the spread of misinformation
    about a crucial business merger.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误信息和虚假信息**：LLM可能会生成误导性或虚假信息，这可能导致错误信息和虚假信息的传播。考虑这样一个情况，新闻机构使用LLM自动总结和发布新闻文章。模型无意中生成了一个误传原始故事的摘要，导致关于重要商业并购的错误信息传播。'
- en: '**Safety**: Ensuring that the content generated by LLMs adheres to ethical
    guidelines, legal regulations, and community standards while avoiding promoting
    harmful or offensive content. Consider the case where an e-commerce platform uses
    an LLM to generate product descriptions for sellers. The LLM can create a description
    that promotes a potentially harmful product, such as a recalled item or an item
    that violates safety regulations, exposing the platform to legal and ethical issues.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：确保由LLM生成的内容符合道德准则、法律法规和社区标准，同时避免推广有害或冒犯性内容。考虑这样一种情况，电商平台使用LLM为卖家生成产品描述。LLM可能会创建促销潜在有害产品的描述，如召回的商品或违反安全规定的商品，从而使平台面临法律和道德问题。'
- en: '**Transparency and explainability**: Ensuring that the decisions made by LLMs
    are transparent, understandable, and justifiable to users and stakeholders. Consider
    the case where an insurance company uses an LLM to assess risk and determine premiums
    for customers. A customer receives a significantly higher premium and requests
    an explanation for the increase. The LLM’s decision-making process is, by itself,
    opaque and difficult to understand, making it challenging for the company to provide
    a clear and justifiable explanation.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度与可解释性**：确保LLM的决策对用户和利益相关者是透明、可理解和可证明的。考虑这样一个情况，保险公司使用LLM评估风险并确定客户的保费。客户收到显著提高的保费并要求解释增加的原因。LLM的决策过程本身是不透明且难以理解的，这使得公司难以提供清晰和合理的解释。'
- en: Now that we have identified these challenges, let’s move on to the next section,
    where we will explore potential solutions and strategies to overcome these limitations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了这些挑战，让我们继续下一节，我们将探讨克服这些限制的潜在解决方案和策略。
- en: Tackling challenges with LLM solutions
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理LLM解决方案中的挑战
- en: Tackling the pesky challenges that LLMs face is key to unlocking their full
    potential and making them our trusty tools or sidekicks in solving real-world
    problems. Only by tackling these challenges can an LLM solution be formed objectively
    and effectively. In this section, we’ll dive into various complementary strategies
    that can help us tackle these challenges and boost the performance of LLMs by
    its high-level issue type. We will start with output and input limitations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 解决LLM面临的棘手挑战对于释放其全部潜力并使其成为解决现实问题的可靠工具或助手至关重要。只有通过解决这些挑战，LLM的解决方案才能客观有效地形成。在本节中，我们将深入探讨各种互补策略，帮助我们解决这些挑战并提升LLM在高级问题类型中的性能。我们将从输出和输入限制开始。
- en: Tackling the output and input limitation challenge
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决输出和输入限制挑战
- en: 'Navigating the output and input limitation challenges is vital for unlocking
    the full potential of LLMs, allowing them to efficiently process diverse data
    types, formats, and context sizes while delivering accurate and reliable results.
    The solutions are as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 导航输出和输入限制挑战对于释放LLM的全部潜力至关重要，使它们能够高效处理多样的数据类型、格式和上下文大小，同时提供准确可靠的结果。解决方案如下：
- en: '**Customized pre-processing**: Design tailored pre-processing techniques to
    transform non-text data into a format that can be efficiently processed by LLMs.
    For example, design a structure that places structured tabular data as the LLM
    prompt.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制化预处理**：设计定制化的预处理技术，将非文本数据转换为能够被LLM高效处理的格式。例如，设计一个结构，将结构化的表格数据放置在LLM提示符中。'
- en: '**Use context limit expansion neural network components**: Implement advanced
    neural network components such as LongLORA, which requires you to fine-tune an
    existing model, to expand the context window size, allowing LLMs to process larger
    amounts of information. However, it is essential to note that this option might
    not be available for external LLM providers and might only be feasible if you
    are considering hosting your own LLM model.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用上下文限制扩展神经网络组件**：实施先进的神经网络组件，如LongLORA，需要对现有模型进行微调，以扩展上下文窗口大小，使LLM能够处理更多的信息量。然而，需要注意的是，这个选项可能不适用于外部LLM提供商，只有在考虑托管自己的LLM模型时才可行。'
- en: '**LLM context optimization**: Any wasted space or repetitive content limits
    the depth and breadth of the answers we can extract and generate. There are three
    possible methods here:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM上下文优化**：任何浪费空间或重复内容都会限制我们从中提取和生成答案的深度和广度。这里有三种可能的方法：'
- en: Select only the most relevant and unique information to be included in the LLM’s
    context window. The maximal marginal relevance algorithm can be used to find a
    set of both relevant and unique sets of information from the distance scores.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅选择最相关和独特的信息包含在LLM的上下文窗口中。最大边缘相关性算法可用于从距离分数中找到一组既相关又独特的信息集。
- en: Consider compressing and summarizing the information provided, which can also
    be done by an LLM, and then use the summarized information as context in the main
    LLM prompt.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑压缩和总结提供的信息，LLM也可以执行这一任务，然后使用总结的信息作为主LLM提示中的上下文。
- en: Apply knowledge retrieval on demand instead of by default. This on-demand behavior
    can be enforced by treating the RAG as a tool and either teaching the LLM to use
    it via fine-tuning or in-context learning.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需求应用知识检索，而不是默认的方式。这种按需行为可以通过微调或上下文学习来实现，将RAG作为工具来教导LLM使用。
- en: Next, we will tackle the challenges with knowledge and information.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过知识和信息来解决挑战。
- en: Tackling the knowledge- and information-related challenge
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决与知识和信息相关的挑战
- en: 'Addressing the output and input limitation challenges is crucial for enhancing
    the versatility and effectiveness of LLMs in solving a wider range of real-world
    problems across various data modalities and context sizes. The solutions are as
    follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 解决输出和输入限制挑战对于增强LLM在解决各种数据模态和上下文大小的实际问题中的多功能性和有效性至关重要。解决方案如下：
- en: '**Real-time data integration**: Connecting LLMs to real-time data sources such
    as APIs, databases, or web services can help them access up-to-date information
    and provide more accurate responses. Incorporating relevant information from knowledge
    bases, using the RAG approach, is part of this solution. RAG can also help reduce
    hallucinations compared to if a model is fine-tuned with custom data if a rigorous
    strict prompt is made to instruct the LLM to not deviate from the context provided
    in the prompt.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时数据集成**：将LLM与实时数据源（如API、数据库或网络服务）连接，可以帮助它们访问最新信息并提供更准确的响应。将来自知识库的相关信息融入其中，使用RAG方法是该解决方案的一部分。与通过自定义数据对模型进行微调相比，RAG还可以帮助减少幻觉现象，前提是严格的提示词要求LLM不偏离提示中提供的上下文。'
- en: '**Tool integration**: Enhancing LLM architecture by integrating existing tools,
    APIs, and specialized algorithms can significantly extend their capabilities,
    allowing them to tackle tasks that are challenging or impossible for standalone
    models. Tools can be used to retrieve extra input context needed for the generation
    process. Alternatively, they can be used to accomplish specific tasks that the
    generated text tells them to do. Examples include leveraging external search engines,
    domain-specific APIs, and computational libraries to provide accurate responses,
    solve complex mathematical problems, or address queries related to real-time data.
    For LLMs such as GPT-3.5, which have API access, this can be achieved through
    effective few-shot prompting, while advanced models such as **Toolformer** and
    **WebGPT** by OpenAI showcase the potential of integrating external tools seamlessly
    into the LLM’s parametric memory and framework. WebGPT can browse the internet
    by detecting the Bing search engine identifier it generates and subsequently execute
    the search before continuing the generation it’s appended. Toolformer, on the
    other hand, is an LLM that can autonomously select and utilize APIs, integrating
    tools such as calculators, Q&A systems, search engines, translators, and calendars
    for improved generation. This is a key functionality of transforming an LLM into
    an agent that can accomplish real-world tasks.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具集成**：通过集成现有的工具、API和专门的算法来增强LLM架构，可以显著扩展其能力，使其能够处理独立模型难以完成或无法完成的任务。工具可以用于检索生成过程中所需的额外输入上下文，或者用于执行生成文本所要求的特定任务。例如，利用外部搜索引擎、特定领域API和计算库，提供准确的响应，解决复杂的数学问题，或回答与实时数据相关的查询。对于具有API访问权限的LLM（如GPT-3.5），可以通过有效的少量示例提示实现这一目标，而像**Toolformer**和OpenAI的**WebGPT**等高级模型则展示了将外部工具无缝集成到LLM参数记忆和框架中的潜力。WebGPT可以通过检测其生成的Bing搜索引擎标识符来浏览互联网，并在继续生成文本前执行搜索。另一方面，Toolformer是一种LLM，能够自主选择并利用API，集成如计算器、问答系统、搜索引擎、翻译器和日历等工具，以改进生成过程。这是将LLM转变为能够完成现实世界任务的智能体的关键功能。'
- en: '`[1,` `3,` `5,` `7,` `9,` `10,` `8,` `6,` `4,` `2]`, the LLM is encouraged
    to pay equal attention to all parts of the text, reducing the likelihood of missing
    valuable information placed in the middle.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[1,` `3,` `5,` `7,` `9,` `10,` `8,` `6,` `4,` `2]`，LLM被鼓励平等关注文本的所有部分，从而减少遗漏中间部分有价值信息的可能性。'
- en: '**Utilizing the surrounding information from the same document in LLM context:**
    This solution enhances the LLM''s understanding by incorporating additional information
    from the source document. Expanding the scope of retrieval to include surrounding
    text or metadata helps the LLM generate more accurate and comprehensive responses,
    ensuring it considers the broader context. This approach improves the LLM''s ability
    to address complex questions and provide well-informed responses, which effectively
    solves the documentation search use case issue.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用LLM上下文中的周围信息**：该解决方案通过结合来自源文档的额外信息，增强了LLM的理解能力。扩大检索范围，包含周围的文本或元数据，帮助LLM生成更准确和全面的响应，确保其考虑更广泛的上下文。这种方法提高了LLM处理复杂问题并提供充分信息响应的能力，能有效解决文档搜索的使用案例问题。'
- en: '**Filtering out irrelevant context using the LLM:** Before proceeding with
    the generation task, the LLM is employed to identify and remove any irrelevant
    context. This refined context is then used for generating responses. This seemingly
    simple and logical method has demonstrated its effectiveness in most cases as
    introduced in the paper [https://arxiv.org/abs/2311.11829v1](https://arxiv.org/abs/2311.11829v1).
    Moreover, the black box nature of this technique allows for easy implementation,
    contributing to more intuitive and natural LLM-generated content.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 LLM 过滤掉无关的上下文**：在进行生成任务之前，LLM 会被用来识别并移除任何无关的上下文。然后，经过精炼的上下文将用于生成响应。这种看似简单且合乎逻辑的方法在大多数案例中已证明其有效性，如论文
    [https://arxiv.org/abs/2311.11829v1](https://arxiv.org/abs/2311.11829v1) 中所介绍的那样。此外，这种技术的黑箱特性使其易于实现，从而促进了更直观、自然的
    LLM 生成内容。'
- en: '**Regularly building an up-to-date knowledge base**: To address the issue of
    single index failures, it is essential to maintain and update the LLM’s knowledge
    base regularly. This ensures that the LLM stays current with recent developments
    and can provide accurate information across a wide range of subjects, ultimately
    enhancing its reliability and effectiveness in solving real-world problems.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期构建最新的知识库**：为了解决单一索引失败的问题，定期维护和更新 LLM 的知识库是至关重要的。这确保了 LLM 能够跟上最新的发展，并提供准确的跨学科信息，最终提高其在解决现实世界问题时的可靠性和有效性。'
- en: '**Treat RAG as a tool an LLM can use dynamically based on its generation**:
    This will help solve the problem of not being able to perform summarization at
    the global level of a knowledge base. Similar to how Deadpool is aware of being
    a comic book character, we need the retrieval process to be aware of the type
    of knowledge base it is retrieving from, along with a special handler for summarization
    tasks. A bonus here is to allow the LLM to configure how many rows of relevant
    text to return to the scope of summarization that can be expanded and shrunk as
    required.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将 RAG 视为 LLM 可以基于其生成动态使用的工具**：这有助于解决无法在知识库的全局层面进行总结的问题。类似于死侍意识到自己是漫画书中的角色，我们需要检索过程能够意识到它正在从哪种类型的知识库中进行检索，并为总结任务配置一个特殊的处理程序。这里的一个额外好处是，允许
    LLM 配置返回多少行相关文本，以便在需要时可以根据范围进行扩展和收缩。'
- en: '**Multi-index retrieval**: To address the issue of single-index failures, a
    multi-index retrieval approach can be employed. This solution involves decomposing
    – or in other words, chunking – the user’s query into multiple components and
    retrieving information from various sources or knowledge indexes. This multi-faceted
    search strategy helps gather more diverse and comprehensive information, reducing
    the likelihood of overlooking relevant details due to a single index’s limitations.
    Consider a user asking about a rare bird species. Using a single index might yield
    limited information. With a multi-index retrieval approach, the LLM would do the
    following:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多索引检索**：为了解决单一索引失败的问题，可以采用多索引检索方法。该方案涉及将用户的查询分解，或者换句话说，拆分成多个组件，并从多个来源或知识索引中检索信息。这种多角度的搜索策略有助于收集更广泛和全面的信息，减少由于单一索引的局限性而忽视相关细节的可能性。假设用户询问某种稀有鸟类。使用单一索引可能获得的信息有限，而采用多索引检索方法，LLM
    将执行以下操作：'
- en: Decompose the query into components (for example, habitat, diet, and appearance).
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询分解成多个组件（例如，栖息地、饮食和外貌）。
- en: Retrieve data from various sources (for example, ornithology databases, nature
    websites, and social media).
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从多个来源检索数据（例如，鸟类学数据库、自然网站和社交媒体）。
- en: Aggregate and synthesize the data to generate a comprehensive response.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汇总并综合数据，生成全面的响应。
- en: '**Set up directed acyclic graph (DAG) workflows**: Setting up DAG workflows
    involves organizing a series of tasks or processes in a structured, non-circular
    sequence to efficiently process multiple sources of information and extend an
    LLM’s functionality. In the context of LLMs, a DAG workflow can be manually designed
    to connect various tools, APIs, and algorithms while addressing the challenges
    related to real-time data integration, tool integration, and multi-index retrieval.
    Let’s consider a use case where a user wants to plan a trip and needs information
    on various aspects of the destination, such as weather, attractions, and local
    cuisine. An LLM could use the DAG workflow to address this complex query efficiently.
    Here’s an example of a DAG workflow for LLMs:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置有向无环图 (DAG) 工作流**：设置 DAG 工作流涉及将一系列任务或过程组织成结构化的、非循环的顺序，以高效处理多个信息来源并扩展 LLM
    的功能。在 LLM 的上下文中，可以手动设计一个 DAG 工作流，连接各种工具、API 和算法，同时解决与实时数据集成、工具集成和多索引检索相关的问题。我们来考虑一个用例，假设用户想要规划一次旅行，并需要了解目的地的各个方面，如天气、景点和当地美食。LLM
    可以使用 DAG 工作流高效地解决这个复杂的查询。以下是 LLM 的 DAG 工作流示例：'
- en: Decompose the user’s query into sub-queries or components, specifically, weather
    forecast, top attractions, and cuisines topics.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户的查询分解为子查询或组件，具体来说是天气预报、热门景点和美食主题。
- en: For each subquery, identify relevant tools, APIs, or data sources. For weather
    forecast, we will retrieve data from a weather API. For top attractions, we will
    extract information from travel website knowledge base. For local cuisine, we
    will gather data from restaurant review website APIs.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个子查询，确定相关的工具、API 或数据源。对于天气预报，我们将从天气 API 获取数据。对于热门景点，我们将从旅游网站知识库中提取信息。对于当地美食，我们将从餐厅评价网站
    API 获取数据。
- en: Apply summarization to each fact separately before using it as part of the LLM’s
    input context.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将每个事实作为 LLM 输入上下文的一部分之前，先对其进行总结。
- en: Execute the LLM generation process with the user query and the summarized facts.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用用户查询和总结的事实执行 LLM 生成过程。
- en: Publish the results on a website through an API.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 API 将结果发布到网站上。
- en: 'This DAG is depicted in *Figure 19**.3*:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该 DAG 如 *图 19.3* 所示：
- en: '![Figure 19.3 – An example LLM DAG workflow](img/B18187_19_3.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 19.3 – 一个示例的 LLM DAG 工作流](img/B18187_19_3.jpg)'
- en: Figure 19.3 – An example LLM DAG workflow
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.3 – 一个示例的 LLM DAG 工作流
- en: By setting up a manual DAG workflow, an LLM can efficiently process information
    from multiple sources, leverage external tools and APIs, and provide accurate
    and reliable responses to a wide range of real-world problems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置手动 DAG 工作流，LLM 可以高效地处理来自多个来源的信息，利用外部工具和 API，提供准确可靠的回答，以应对各种现实世界中的问题。
- en: This strategy helps the LLM provide a more accurate and detailed response, even
    when information is scarce or not readily available in a single index. The type
    of problem this solves is more commonly known as multi-hop question answering.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略帮助 LLM 提供更准确、更详细的回答，即使在信息稀缺或无法在单一索引中轻易获取的情况下。这类问题通常被称为多跳问答。
- en: Next, we will tackle the challenges of accuracy and reliability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解决准确性和可靠性的问题。
- en: Tackling the challenges of accuracy and reliability
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决准确性和可靠性的问题
- en: 'Ensuring the accuracy and reliability of LLMs is crucial for building trust
    in their abilities and making them effective problem-solving tools. The solutions
    that can help solve accuracy and reliability-related challenges are as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 LLM 的准确性和可靠性对于建立对其能力的信任以及使其成为有效的解决问题工具至关重要。解决准确性和可靠性挑战的解决方案如下：
- en: '**Treat the LLM solution as any modeling experiment**: Pair the LLM with a
    knowledge base, evaluate its performance using relevant metrics, and gather insights
    to fine-tune its capabilities iteratively according to the deep learning life
    cycle. This will help you choose a model that at least produces fewer hallucinations
    and can help you understand its effectiveness for your use case.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将 LLM 解决方案视为任何建模实验**：将 LLM 与知识库配对，使用相关指标评估其性能，并收集见解，以根据深度学习生命周期迭代地调整其能力。这将帮助你选择一个至少能产生较少幻觉并帮助你理解其在特定应用中的有效性的模型。'
- en: '**Fine-tune retriever embedding models**: This is instead of just depending
    on pre-trained embedding models or embedding model providers. This can improve
    the retrieval accuracy, thereby boosting the quality of LLM-generated responses.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调检索嵌入模型**：这不是仅仅依赖于预训练的嵌入模型或嵌入模型提供者。这样可以提高检索的准确性，从而提升LLM生成响应的质量。'
- en: '**Prompt engineering**: Prompt engineering is the process of crafting effective
    and targeted prompts to guide a language model’s response, thereby improving its
    accuracy, relevance, and overall performance. Consider implementing the following
    techniques:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**：提示工程是制定有效且有针对性的提示，以引导语言模型的响应，从而提高其准确性、相关性和整体表现的过程。可以考虑实施以下技术：'
- en: '**Chain-of-thought (CoT)**: The method encourages LLMs to generate step-by-step
    reasoning traces, leading to more accurate and structured responses for tasks
    involving arithmetic, commonsense reasoning, and other problem-solving scenarios.
    By guiding the LLM through a series of reasoning steps, CoT helps reduce issues
    such as fact hallucination while enhancing the overall quality and coherence of
    the generated content.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维链（CoT）**：该方法鼓励LLM生成逐步推理过程，从而在涉及算术、常识推理和其他问题解决场景的任务中产生更准确和结构化的响应。通过引导LLM经过一系列推理步骤，CoT有助于减少诸如事实幻觉等问题，同时提高生成内容的整体质量和连贯性。'
- en: '**ReAct**: This method is a framework that interleaves reasoning traces and
    task-specific actions, enabling LLMs to generate more reliable and factual responses.
    By incorporating dynamic reasoning and interaction with external sources, ReAct
    effectively addresses issues such as fact hallucination and error propagation,
    resulting in improved human interpretability and trustworthiness of LLMs.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReAct**：这种方法是一个框架，它交替进行推理过程和任务特定的操作，使得LLM能够生成更可靠和真实的响应。通过结合动态推理和与外部来源的互动，ReAct有效地解决了事实幻觉和错误传播等问题，从而提高了LLM的可解释性和可信度。'
- en: '**Prompt tuning**: Prompt tuning is a technique for refining LLM behavior by
    optimizing the input prompts using gradient-based methods, which allows for better
    control over the model’s responses, and leads to improved accuracy and relevance
    in various problem-solving tasks. By fine-tuning prompts, users can effectively
    guide the LLM to generate more desirable and context-specific outputs. This only
    applies to LLMs you can host yourself, however.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示调优**：提示调优是一种通过使用基于梯度的方法优化输入提示来细化LLM行为的技术，这使得更好地控制模型的响应，并提高在各种问题解决任务中的准确性和相关性。通过微调提示，用户可以有效地引导LLM生成更理想和更具上下文特定的输出。然而，这仅适用于你可以自己托管的LLM。'
- en: '**Relying on well-engineered prompts**: Leverage published well-engineered
    prompts instead of crafting one of your own. This is a technique used by Langchain
    and Auto-GPT. AutoGPT is an open source Python application based on GPT-4\. It
    automates the execution of tasks without requiring multiple prompts, using AI
    agents to access the web and perform actions with minimal guidance. Unlike ChatGPT,
    AutoGPT can execute larger tasks such as creating websites and developing marketing
    strategies without needing step-by-step instructions. It has various applications,
    such as generating content, designing logos, and developing chatbots.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖精心设计的提示**：利用已发布的精心设计的提示，而不是自己设计一个。这是Langchain和Auto-GPT使用的一种技术。AutoGPT是一个基于GPT-4的开源Python应用程序。它无需多次提示即可自动执行任务，使用AI代理访问网页并执行操作，所需的指导最小。与ChatGPT不同，AutoGPT可以执行更大的任务，例如创建网站和制定营销策略，而不需要逐步的指令。它有多种应用，例如生成内容、设计徽标和开发聊天机器人。'
- en: '**Rejection sampling (best-of-n) reference**: Use rejection sampling techniques
    to improve the quality of generated responses by selecting the best response from
    multiple attempts. The best response can be evaluated through a chosen metric.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拒绝采样（best-of-n）参考**：使用拒绝采样技术通过从多个尝试中选择最佳响应来提高生成响应的质量。最佳响应可以通过选定的度量进行评估。'
- en: '**Re-ranking relevance distance scores from knowledge retrieval**: Knowledge
    retrieval is in the domain of recommendation systems. A common technique that’s
    used is to implement proper regression-based recommendation models to re-rank
    relevance distance scores. This can help provide more accurate and potentially
    more personalized relevant information with more contextual data. This technique
    is used by most real-world large-scale recommendation-based products, such as
    YouTube.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新排名知识检索的相关性距离分数**：知识检索属于推荐系统领域。常用的一种技术是实现适当的回归基础推荐模型来重新排名相关性距离分数。这有助于提供更准确且潜在的个性化的相关信息，伴随更多的上下文数据。大多数现实世界的大规模推荐产品，如YouTube，都采用了这种技术。'
- en: '**Iterative retrieval and generation**: Use techniques such as self-ask, Active
    RAG, and ITER-RETGEN to generate temporary responses, evaluate their quality,
    and iteratively refine them using retrieved knowledge. This approach can reduce
    hallucinations and improve the quality of LLM-generated content.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代检索与生成**：使用自问、自我激活RAG、ITER-RETGEN等技术生成临时响应，评估其质量，并通过检索的知识进行迭代优化。这种方法有助于减少幻觉现象，提高LLM生成内容的质量。'
- en: '**Multi-agent systems**: Develop a multi-agent system composed of agents with
    unique personas and contexts to address diverse problem-solving scenarios. These
    agents can collaborate, share information, and provide more comprehensive and
    reliable solutions. An example of this is **AutoAgents**. AutoAgents is an innovative
    framework that adaptively generates and coordinates multiple specialized agents
    to build an AI team according to different tasks. The framework consists of two
    stages: drafting and execution. In the drafting stage, an agent team and execution
    plan are generated based on the input task, while the execution stage refines
    the plan through inter-agent collaboration and feedback to produce the outcome.
    AutoAgents can dynamically synthesize and coordinate multiple expert agents to
    form customized AI teams for diverse tasks. Experiments on open-ended question-answering
    and trivia creative writing tasks demonstrate the effectiveness of AutoAgents
    compared to existing methods. AutoAgents offers new perspectives for tackling
    complex tasks by assigning different roles to different tasks and promoting team
    cooperation.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多智能体系统**：开发由具有独特个性和背景的智能体组成的多智能体系统，以解决多样化的难题。这些智能体可以协作、共享信息，并提供更全面、更可靠的解决方案。一个例子是**AutoAgents**。AutoAgents是一个创新框架，能够根据不同任务自适应地生成和协调多个专门的智能体，构建一个AI团队。该框架由两个阶段组成：草拟阶段和执行阶段。在草拟阶段，根据输入任务生成智能体团队和执行计划，而在执行阶段，通过智能体间的协作和反馈来优化计划，最终产生结果。AutoAgents能够动态合成和协调多个专家智能体，为不同任务构建定制化的AI团队。关于开放式问答和创意写作任务的实验表明，与现有方法相比，AutoAgents在效果上具有优势。AutoAgents为解决复杂任务提供了新的视角，通过为不同任务分配不同的角色并促进团队合作。'
- en: Next, we will dive into solutions for the runtime performance challenge.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨应对运行时性能挑战的解决方案。
- en: Tackling the runtime performance challenge
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应对运行时性能挑战
- en: 'The runtime performance challenge is a critical issue that can significantly
    impact the efficiency and effectiveness of language models. As LLMs continue to
    grow in complexity and scale, optimizing their runtime performance becomes more
    crucial than ever. The solutions to solve this issue are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时性能挑战是一个关键问题，它会显著影响语言模型的效率和效果。随着LLM的复杂性和规模不断增长，优化其运行时性能变得比以往任何时候都更加重要。解决这一问题的方案如下：
- en: '**Caching outputs**: Temporarily store results to avoid recomputing information,
    enabling faster response times and improved performance. This approach is particularly
    useful when dealing with repetitive or similar queries.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存输出**：临时存储结果以避免重复计算信息，从而提高响应速度和性能。此方法在处理重复或相似查询时尤为有效。'
- en: '**GPUs and GPU inference accelerators**: This is only applicable for LLMs you
    host yourself. LLMs need to run with these components to run in a reasonable time.
    These were introduced in more detail in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models* *to Production*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU 和 GPU 推理加速器**：这仅适用于您自己托管的LLM。LLM需要依赖这些组件才能在合理的时间内运行。在[*第15章*](B18187_15.xhtml#_idTextAnchor217)，*将深度学习模型*
    *部署到生产环境* 中，这些内容已有更详细的介绍。'
- en: '`scann` algorithm and the `faiss` *IVFPQFS* algorithm provide a good balance
    between index build time, index size, retrieval recall, and retrieval runtime.
    However, an approximate KNN algorithm is only required with large knowledge bases
    since the retrieval speed of a small knowledge base is already fast, which is
    less than 1 second. Typically, suitable data dimensions lie in the range of three-digit
    vector column sizes, and seven-digit vector row sizes.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scann` 算法和 `faiss` *IVFPQFS* 算法在索引构建时间、索引大小、检索召回率和检索运行时之间提供了良好的平衡。然而，仅在大规模知识库中才需要使用近似
    KNN 算法，因为小型知识库的检索速度已经非常快，通常小于 1 秒。通常，适合的数据维度范围在三位数的向量列大小和七位数的向量行大小之间。'
- en: Tackling the challenge of ethical implications and societal impacts
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决伦理影响和社会影响的挑战
- en: Addressing the ethical implications and societal impacts of LLM solutions is
    crucial for ensuring their responsible and sustainable deployment across various
    applications. By considering the ethical and societal consequences of LLM-generated
    content, developers can create models that respect user values, adhere to legal
    guidelines, and contribute positively to society.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 LLM 解决方案的伦理影响和社会影响对于确保其在各种应用中的负责任和可持续部署至关重要。通过考虑 LLM 生成内容的伦理和社会后果，开发者可以创建尊重用户价值观、遵循法律指南并对社会产生积极贡献的模型。
- en: 'The strategies to tackle these challenges are as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战的策略如下：
- en: '**Bias and fairness mitigation**: Consider the following strategy in the context
    of the methods that were introduced in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见和公平性缓解**：在 [*第 13 章*](B18187_13.xhtml#_idTextAnchor196)中介绍的方法的背景下，考虑以下策略，*探索偏见*
    *与公平性*：'
- en: '**Data collection and preparation**: Ensure a diverse and representative dataset
    for fine-tuning LLM models. Balance sensitive attributes in the data, and eliminate
    or control potential biases that may arise from these attributes. Additionally,
    you can instruct the LLM to specifically not perpetuate bias in natural language
    as part of the input context. Better yet, empower users to define their preferences,
    values, and ethical guidelines, enabling LLMs to generate content that aligns
    with individual user needs and values.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集与准备**：确保用于微调 LLM 模型的数据集多样且具有代表性。平衡数据中的敏感属性，并消除或控制可能由这些属性引发的偏见。此外，你可以指示
    LLM 在自然语言的输入上下文中明确不传播偏见。更好的是，赋予用户定义偏好、价值观和伦理指导方针的权力，使 LLM 能够生成符合个别用户需求和价值观的内容。'
- en: '**Bias mitigation during fine-tuning**: Implement techniques such as counterfactual
    data augmentation, adversarial training, or re-sampling during the fine-tuning
    process to reduce the impact of biased features and improve fairness.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调过程中的偏见缓解**：在微调过程中实施对抗性数据增强、对抗训练或重采样等技术，以减少偏见特征的影响并提高公平性。'
- en: '**Post-processing**: Modify LLM-generated content using techniques such as
    equalized odds post-processing to ensure fairness in the outputs. This can be
    applied when using LLM providers such as OpenAI GPT-4 or fine-tuned open source
    models.'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**：使用平衡后处理等技术修改 LLM 生成的内容，确保输出结果的公平性。这可以应用于使用像 OpenAI GPT-4 或微调的开源模型等
    LLM 提供商时。'
- en: '**Monitoring and evaluation**: Continuously monitor LLM-generated content for
    potential biases using bias and fairness metrics, and adjust the model as needed
    to ensure compliance with ethical guidelines and fairness requirements.'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控与评估**：持续监控 LLM 生成的内容，使用偏见和公平性指标识别潜在偏见，并根据需要调整模型，确保符合伦理指南和公平性要求。'
- en: '**Privacy-preserving techniques**: Adopt privacy-preserving approaches, such
    as differential privacy, federated learning, and homomorphic encryption, to protect
    sensitive information in the training data and generated content. Implement policies
    and guidelines to prevent the inadvertent disclosure of **Personal Identifiable
    Information** (**PII**) in LLM-generated content.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私保护技术**：采用隐私保护方法，如差分隐私、联邦学习和同态加密，保护训练数据和生成内容中的敏感信息。实施政策和指导方针，以防止在 LLM 生成内容中无意泄露**个人可识别信息**（**PII**）。'
- en: '**Fact-checking and credibility assessment**: Incorporate fact-checking and
    credibility assessment mechanisms into LLM solutions to reduce the risk of generating
    misleading or false information. This can involve integrating LLMs with external
    knowledge sources, such as knowledge databases, to verify the accuracy of generated
    content. You can also instruct the LLM to be humble and not return a statement
    if no facts can be used for verification.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实核查和可信度评估**：将事实核查和可信度评估机制整合到LLM解决方案中，以减少生成误导性或虚假信息的风险。这可以通过将LLM与外部知识源（如知识数据库）集成，验证生成内容的准确性来实现。你还可以指示LLM保持谦逊，在无法进行事实验证时不返回任何声明。'
- en: '**Content moderation and guardrails**: Implement content moderation techniques,
    such as keyword filtering, machine learning-based classifiers, and human-in-the-loop
    review processes, to prevent the generation of harmful or offensive content. Establish
    guardrails, such as toxicity thresholds or ethical guidelines, to ensure that
    LLM-generated content adheres to community standards and legal regulations.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容审查和防护措施**：实施内容审查技术，如关键词过滤、基于机器学习的分类器以及人工审核流程，防止生成有害或冒犯性内容。建立防护措施，如有毒性阈值或伦理指南，以确保LLM生成的内容符合社区标准和法律规定。'
- en: '**Transparency and explainability**: Develop methods for enhancing the transparency
    and explainability of LLM-generated content, such as providing reasoning traces,
    saliency maps, or counterfactual explanations. The concepts that were introduced
    in *Chapter 11**, Explaining Neural Network Predictions*, and [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184)*,
    Interpreting Neural Networks*, can be applied to an LLM.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度和可解释性**：开发方法，增强LLM生成内容的透明度和可解释性，例如提供推理痕迹、显著性图或反事实解释。在*第11章*《解释神经网络预测》以及[*第12章*](B18187_12.xhtml#_idTextAnchor184)《解释神经网络》中介绍的概念可以应用于LLM。'
- en: By implementing these strategies, developers can create LLM solutions that not
    only respect user values and adhere to legal guidelines but also contribute positively
    to society. Addressing the ethical implications and societal impacts of LLMs is
    an essential step toward building trust in the technology and ensuring its responsible
    and sustainable deployment across various applications.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些策略，开发者可以创建符合用户价值观并遵守法律指南的LLM解决方案，同时也能对社会做出积极贡献。解决LLM的伦理影响和社会影响是建立对技术信任的关键一步，确保其在各类应用中的负责任和可持续部署。
- en: With a comprehensive understanding of the challenges associated with LLMs mentioned,
    as well as their potential solutions, we can now turn our attention to addressing
    the overarching challenge of LLM solution adoption across organizations and industries.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在全面理解了与LLM相关的挑战及其潜在解决方案之后，我们现在可以将注意力转向解决LLM解决方案在各组织和行业中推广的重大挑战。
- en: Tackling the overarching challenge of LLM solution adoption
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决LLM解决方案采纳的重大挑战
- en: One overarching challenge in realizing the full potential of LLM solutions lies
    in their adoption across organizations and industries. Similar to the adoption
    of any machine learning or deep learning solution, the key factor driving the
    adoption of LLMs is confidence. Confidence in the technology’s capabilities, its
    effectiveness in addressing specific use cases, and its ability to deliver tangible
    results are essential for widespread adoption.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 实现LLM解决方案全部潜力的一个重大挑战在于其在各个组织和行业中的采纳。与任何机器学习或深度学习解决方案的采纳类似，推动LLM采纳的关键因素是信任。对技术能力的信任、其在解决特定使用案例中的有效性以及其交付实际成果的能力是广泛采纳的必要条件。
- en: To overcome this challenge, it is crucial to systematically educate organizations
    about the power of LLMs, their diverse applications, and how they can be tailored
    to meet specific needs. This includes demonstrating the benefits of LLMs through
    real-world success stories, providing practical guidance on implementing LLM solutions,
    and offering support for organizations navigating the complexities of integrating
    LLMs into their workflows.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，至关重要的是系统性地教育各组织了解LLM的强大能力、其多样的应用及如何根据特定需求量身定制LLM。这包括通过真实的成功案例展示LLM的优势，提供实施LLM解决方案的实际指导，并为各组织在将LLM集成到工作流中的复杂性提供支持。
- en: Building confidence in LLM solutions involves thoroughly evaluating their performance,
    addressing the challenges discussed earlier in this chapter, and ensuring that
    the solutions meet their intended objectives. By implementing a comprehensive
    set of evaluation approaches, including quantitative metrics and qualitative manual
    evaluations, organizations can better assess the performance, accuracy, and overall
    quality of the results from an LLM solution. These evaluations should be conducted
    iteratively, allowing for ongoing refinement and improvement of the LLM solution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 建立对LLM解决方案的信心需要全面评估其性能，解决本章前面讨论的挑战，并确保解决方案能够实现预期目标。通过实施包括定量指标和定性人工评估在内的全面评估方法，组织可以更好地评估LLM解决方案的性能、准确性和整体质量。这些评估应该是迭代进行的，以便持续改进LLM解决方案。
- en: Additionally, addressing the challenges identified in this chapter, such as
    output and input limitations, knowledge and information-related challenges, accuracy
    and reliability issues, and runtime performance challenges, is essential for building
    confidence in LLM solutions. By leveraging the strategies and techniques discussed
    in this chapter, organizations can optimize the performance of LLMs and ensure
    their effectiveness in various applications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，解决本章中提到的挑战，如输出和输入限制、与知识和信息相关的挑战、准确性和可靠性问题以及运行时性能挑战，对于建立对LLM解决方案的信心至关重要。通过利用本章讨论的策略和技术，组织可以优化LLM的性能，确保其在各种应用中的有效性。
- en: Another essential aspect of building confidence in LLM solutions is effective
    communication and collaboration with stakeholders. This includes sharing evaluation
    results, discussing the benefits and potential limitations of LLMs, and addressing
    any concerns that stakeholders may have regarding the adoption of LLM solutions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 建立对LLM解决方案的信心的另一个关键方面是与利益相关者进行有效的沟通与协作。这包括共享评估结果、讨论LLM的优点和潜在局限性，以及解决利益相关者对LLM解决方案采用可能存在的任何疑虑。
- en: In conclusion, adopting LLM solutions successfully requires a combination of
    rigorous evaluation, addressing challenges, and effective communication with stakeholders.
    By treating the adoption of LLM solutions with the same level of care as any machine
    learning or deep learning solution, organizations can build confidence in the
    capabilities and performance of LLMs, unlocking their full potential in a wide
    range of real-world applications. And with that, we have explored the challenges
    that plague LLMs and their solutions in detail.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，成功采用LLM解决方案需要结合严格的评估、解决挑战和与利益相关者的有效沟通。通过以与任何机器学习或深度学习解决方案相同的关注度对待LLM解决方案的采用，组织能够建立对LLM能力和性能的信心，释放其在广泛现实应用中的全部潜力。至此，我们已经详细探讨了困扰LLM的挑战及其解决方案。
- en: In the following section, we will dive deeper into leveraging LLMs to build
    autonomous agents, which can significantly expand and improve our problem-solving
    skills.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将深入探讨如何利用LLM构建自主代理，这可以显著扩展和提升我们的解决问题能力。
- en: Leveraging LLM to build autonomous agents
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用LLM构建自主代理
- en: One promising area in which LLMs can be harnessed is the development of autonomous
    agents that can efficiently solve complex problems and interact with their environment.
    This section will focus on leveraging LLMs to build such agents and discuss the
    key aspects that contribute to their effectiveness.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以在一个有前景的领域中得到应用——开发能够高效解决复杂问题并与环境互动的自主代理。本节将重点讨论如何利用LLM构建这样的代理，并讨论其有效性的关键因素。
- en: 'Autonomous agents are AI-driven entities that can perform tasks, make decisions,
    and interact with their environment independently. By incorporating LLMs into
    these agents, developers can create versatile and adaptive systems that can tackle
    a wide range of challenges. Here are some essential components of LLM-powered
    autonomous agents:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理是由AI驱动的实体，能够独立执行任务、做出决策并与环境互动。通过将LLM融入这些代理，开发者可以创建出多功能且具有适应性的系统，能够应对各种挑战。以下是LLM驱动的自主代理的一些关键组成部分：
- en: '**Planning and decision making**: LLMs can be utilized to generate plans and
    strategies that guide the agent’s actions, taking into account the context and
    goals.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划与决策**：LLM可以用来生成引导代理行动的计划和策略，考虑到上下文和目标。'
- en: '**Observing and learning from the environment**: LLMs can be trained to observe
    and interpret the environment, learning from past experiences and adjusting their
    behavior accordingly.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察和从环境中学习**：LLM可以训练观察并解读环境，从过去的经验中学习，并相应地调整其行为。'
- en: '**Collaborative problem-solving**: Multi-agent systems can be developed, where
    each agent has a unique persona and context. These agents can collaborate, share
    information, and provide more comprehensive and reliable solutions.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协作问题解决**：可以开发多代理系统，每个代理都有独特的个性和背景。这些代理可以协作、共享信息，并提供更全面、更可靠的解决方案。'
- en: '**Self-refinement**: Autonomous agents can use LLMs to analyze their performance,
    identify areas for improvement, and refine their strategies and behaviors over
    time.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自我完善**：自主代理可以利用LLM分析其表现，识别需要改进的领域，并随着时间推移完善其策略和行为。'
- en: Agents encompass parts and pieces of the solutions to the challenges identified
    in the *Identifying challenges with LLM solutions* section of this chapter. Additionally,
    the solutions that were introduced here can be combined to expand the scope of
    problems the overall architected LLM solution can cover. Examples of published
    agent methods that were also introduced earlier in this chapter are WebGPT, Toolformer,
    Auto-GPT, and AutoAgents. Autonomous agents that leverage LLMs are the key to
    making powerful LLM solutions. By combining the strengths of LLMs with the adaptability
    and decision-making capabilities of agents, developers can create groundbreaking
    systems that can revolutionize various domains and industries.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 代理包含了本章*识别LLM解决方案中的挑战*部分中提出的挑战解决方案的各个部分。此外，这里介绍的解决方案可以组合起来，以扩展整体架构化LLM解决方案所能覆盖的问题范围。本章早些时候介绍的已发布代理方法示例包括WebGPT、Toolformer、Auto-GPT和AutoAgents。利用LLM的自主代理是实现强大LLM解决方案的关键。通过将LLM的优势与代理的适应性和决策能力相结合，开发者可以创造出革命性的系统，彻底改变各个领域和行业。
- en: With a comprehensive understanding of LLM solutions and their potential applications,
    let’s explore some specific use cases where they can be employed effectively.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过全面了解LLM解决方案及其潜在应用，让我们探索一些能够有效应用这些解决方案的具体案例。
- en: Exploring LLM solution use cases
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLM解决方案的应用案例
- en: 'In this section, we will explore some fascinating real-world applications where
    LLM solutions can truly shine. This will give you a sense of how revolutionary
    LLM solutions are. The use cases are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将探索一些令人着迷的现实世界应用案例，展示LLM解决方案如何真正发挥作用。这将让你感受到LLM解决方案的革命性。以下是这些应用案例：
- en: '**Travel itinerary planner**: LLMs can be employed to develop advanced travel
    itinerary planners that generate personalized trip plans based on user preferences
    and constraints. By integrating LLMs with travel APIs, such as flight, hotel,
    and attraction databases, as well as real-time data sources such as weather and
    traffic information, these planners can provide context-aware recommendations
    tailored to individual traveler needs. Notably, companies such as Booking.com
    and Expedia integrated this into their products, and Agoda announced that they
    will be working on it.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旅行行程规划器**：LLM可以用于开发先进的旅行行程规划器，根据用户的偏好和约束生成个性化的旅行计划。通过将LLM与旅行API（如航班、酒店和景点数据库）以及实时数据源（如天气和交通信息）集成，这些规划器能够提供针对个别旅行者需求的上下文感知推荐。值得注意的是，Booking.com和Expedia等公司已将此功能整合到他们的产品中，Agoda也宣布将会进行相关工作。'
- en: '**Intelligent tutoring systems**: LLMs can be used to develop intelligent tutoring
    systems that offer personalized learning experiences for students. By integrating
    LLMs with educational content, assessment tools, and learner data, these systems
    can generate targeted learning materials, provide real-time feedback, and adapt
    to individual learning needs. This enables a more efficient and engaging learning
    experience for students. Notably, Duolingo is a company that implemented such
    a solution in their gamified language learning product.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能辅导系统**：LLM可以用于开发智能辅导系统，为学生提供个性化的学习体验。通过将LLM与教育内容、评估工具和学习者数据集成，这些系统能够生成有针对性的学习材料、提供实时反馈，并根据个别学习需求进行适应。这为学生提供了更高效、更有趣的学习体验。值得注意的是，Duolingo是一家在其游戏化语言学习产品中实现了此类解决方案的公司。'
- en: '**Automated email responses**: LLM solutions can be employed to develop automated
    email response systems that handle various types of inquiries, such as customer
    support, sales inquiries, or general information requests. By integrating LLMs
    with email APIs, CRM systems, and relevant knowledge bases, email responses can
    be personalized, accurate, and contextually relevant. This helps businesses streamline
    their customer communication and provide efficient support. Notably, Nanonets
    AI is a company that implemented this as part of their product.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化电子邮件回复**：LLM解决方案可以用于开发自动化电子邮件回复系统，处理各种类型的咨询，如客户支持、销售询问或一般信息请求。通过将LLMs与电子邮件API、CRM系统和相关知识库集成，电子邮件回复可以个性化、准确并具有上下文相关性。这有助于企业简化客户沟通并提供高效的支持。值得注意的是，Nanonets
    AI公司在其产品中实现了这一解决方案。'
- en: '**Code generation**: LLMs can be used to generate code snippets, algorithms,
    or entire software programs based on user input or specific requirements. Solutions
    such as GitHub Copilot leverage LLMs to assist developers in writing code, suggesting
    relevant code snippets, and completing sections of code based on context. By integrating
    LLMs with code repositories, programming language APIs, and domain-specific knowledge
    bases, code generation can be tailored to specific programming languages, frameworks,
    and use cases, improving developer productivity.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成**：LLMs可以根据用户输入或特定需求生成代码片段、算法或整个软件程序。像GitHub Copilot这样的解决方案利用LLMs帮助开发者编写代码，建议相关的代码片段，并根据上下文完成代码的部分内容。通过将LLMs与代码仓库、编程语言API和特定领域的知识库结合，代码生成可以根据特定的编程语言、框架和使用场景量身定制，从而提升开发者的生产力。'
- en: '**Customer support chatbots**: LLM solutions can be employed to develop advanced,
    context-aware chatbots that can handle customer inquiries and support requests
    more effectively. By integrating LLMs with **customer relationship management**
    (**CRM**) systems and knowledge bases, chatbots can provide personalized and accurate
    responses to customer queries. This helps businesses improve their customer support
    services, reduce response times, and increase customer satisfaction. Companies
    such as forethought.ai, Ada, and EBI.AI provide such a solution in their product.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户支持聊天机器人**：LLM解决方案可以用于开发先进的、具有上下文感知的聊天机器人，更有效地处理客户咨询和支持请求。通过将LLMs与**客户关系管理**（**CRM**）系统和知识库结合，聊天机器人能够为客户查询提供个性化且准确的回答。这有助于企业提升客户支持服务、减少响应时间，并提高客户满意度。像forethought.ai、Ada和EBI.AI等公司在其产品中提供了类似的解决方案。'
- en: '**Medical diagnosis and treatment suggestions**: LLMs can be employed to develop
    advanced diagnostic tools that analyze patient symptoms, medical history, and
    relevant medical literature to suggest potential diagnoses and treatment options.
    By integrating LLMs with **electronic health record** (**EHR**) systems, medical
    databases, and domain-specific knowledge bases, these tools can help healthcare
    professionals make more informed decisions and improve patient outcomes. Notably,
    Harman, a Samsung company, implemented and offered such a solution as part of
    their offered services.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗诊断和治疗建议**：大语言模型（LLMs）可以用于开发先进的诊断工具，通过分析患者的症状、病史和相关医学文献，建议潜在的诊断和治疗方案。通过将LLMs与**电子健康记录**（**EHR**）系统、医学数据库以及特定领域的知识库结合，这些工具可以帮助医疗专业人员做出更为明智的决策，改善患者的治疗效果。值得注意的是，三星公司Harman已经实施并提供了这样的解决方案，作为其服务的一部分。'
- en: '**Personal finance management**: LLMs can be used to develop intelligent personal
    finance management applications that provide tailored financial advice, budgeting
    suggestions, and investment recommendations based on user-specific financial goals
    and risk tolerance. By integrating LLMs with banking APIs, stock market data,
    and financial knowledge bases, these applications can offer context-aware financial
    planning and guidance to users. Although not exactly a service or product, Bloomberg
    has developed BloombergGPT, a 50-billion parameter large language model designed
    specifically for finance, which showcases the potential of LLMs in the financial
    domain.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人财务管理**：LLMs可以用于开发智能的个人财务管理应用，根据用户特定的财务目标和风险承受能力，提供量身定制的财务建议、预算建议和投资推荐。通过将LLMs与银行API、股市数据和财务知识库集成，这些应用能够为用户提供具有上下文感知的财务规划和指导。虽然不完全是一个服务或产品，但彭博公司开发了BloombergGPT，这是一款专为金融领域设计的50亿参数的大型语言模型，展示了LLMs在金融领域的潜力。'
- en: '**Creative content generation**: LLMs can be employed to generate creative
    content, such as stories, poetry, or music, based on user inputs, preferences,
    or inspirations. By integrating LLMs with databases of literary works, music libraries,
    and knowledge bases on creative techniques and styles, these applications can
    produce unique and engaging content that caters to individual artistic tastes
    and needs. Notably, Jasper built a platform to account for this use case.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创意内容生成**：LLMs 可用于根据用户输入、偏好或灵感生成创意内容，如故事、诗歌或音乐。通过将 LLM 与文学作品数据库、音乐库和创作技巧与风格的知识库集成，这些应用程序可以生成独特且吸引人的内容，迎合个人艺术品味和需求。值得注意的是，Jasper
    构建了一个平台来应对这一用例。'
- en: '**Legal document analysis and drafting**: LLMs can be used to develop advanced
    legal document analysis and drafting tools that assist legal professionals in
    reviewing contracts, identifying potential issues, and generating legal documents
    based on specific requirements. By integrating LLMs with legal databases, contract
    templates, and domain-specific knowledge bases, these tools can help streamline
    legal work and improve efficiency in the legal industry. Notably, netdocuments
    implemented this use case with their product.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律文档分析与草拟**：LLMs 可用于开发先进的法律文档分析与草拟工具，帮助法律专业人士审查合同、识别潜在问题，并根据特定需求生成法律文档。通过将
    LLM 与法律数据库、合同模板及领域特定知识库集成，这些工具可以帮助简化法律工作，并提高法律行业的工作效率。值得注意的是，netdocuments 通过其产品实现了这一用例。'
- en: '**Smart home automation**: LLMs can be employed to develop intelligent home
    automation systems that understand natural language commands and adapt to user
    preferences and routines. By integrating LLMs with smart home devices, APIs, and
    user behavior data, these systems can provide a more intuitive and personalized
    home automation experience, enabling users to control their home environment with
    ease and convenience. Amazon Alexa is a prime example of this use case.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能家居自动化**：LLMs 可用于开发智能家居自动化系统，这些系统可以理解自然语言命令，并适应用户的偏好和日常作息。通过将 LLM 与智能家居设备、API
    以及用户行为数据集成，这些系统可以提供更加直观和个性化的家居自动化体验，使用户能够轻松便捷地控制家居环境。Amazon Alexa 就是这一用例的典型代表。'
- en: In each of these use cases, integrating LLMs with the relevant tools, APIs,
    and data sources ensures that the generated content, recommendations, and responses
    are accurate, contextually relevant, and tailored to specific needs, enhancing
    user experiences and providing valuable support in various domains.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些用例中，将 LLM 与相关工具、API 和数据源集成，可以确保生成的内容、推荐和响应是准确的、上下文相关的，并且根据特定需求量身定制，从而提升用户体验并在各个领域提供有价值的支持。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored LLMs and their potential to address real-world
    problems and create value across various applications. We discussed the key aspects
    of architecting LLM solutions, such as handling knowledge, interacting with real-time
    data and tools, evaluating LLM solutions, identifying and addressing challenges,
    and leveraging LLMs to build autonomous agents. We also emphasized the importance
    of retrieval-augmented language models for providing contextually relevant information
    and examined various techniques and libraries to improve LLM solutions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了大语言模型（LLMs）及其解决现实问题和在各种应用中创造价值的潜力。我们讨论了架构 LLM 解决方案的关键方面，例如处理知识、与实时数据和工具交互、评估
    LLM 解决方案、识别并解决挑战，以及利用 LLM 构建自主代理。我们还强调了检索增强语言模型在提供上下文相关信息方面的重要性，并考察了多种技术和库，以改善
    LLM 解决方案。
- en: We also discussed the limitations of LLMs, such as output and input limitations,
    knowledge and information-related challenges, accuracy and reliability issues,
    runtime performance challenges, ethical implications and societal impacts, and
    the overarching challenge of LLM solution adoption. To tackle these limitations,
    we presented various complementary strategies, such as real-time data integration,
    tool integration, prompt engineering, rejection sampling, multi-agent systems,
    runtime optimization techniques, bias and fairness mitigation, content moderation,
    and enabling LLM transparency and explainability. Lastly, we discussed leveraging
    LLMs to build autonomous agents, which can significantly expand and improve problem-solving
    abilities in diverse applications.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了LLM的局限性，例如输出和输入的限制、知识和信息相关的挑战、准确性和可靠性问题、运行时性能问题、伦理影响和社会影响，以及LLM解决方案采用的总体挑战。为了解决这些局限性，我们提出了多种互补策略，如实时数据集成、工具集成、提示工程、拒绝采样、多代理系统、运行时优化技术、偏见和公平性缓解、内容审查，以及实现LLM透明性和可解释性。最后，我们讨论了利用LLM构建自主代理，这可以显著扩展和提升在各种应用中的问题解决能力。
- en: By understanding the intricacies of LLM solutions and applying the strategies
    and techniques discussed in this chapter, organizations and individuals can harness
    the full potential of LLMs to drive innovation and improve outcomes across various
    applications.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解LLM解决方案的复杂性，并应用本章讨论的策略和技术，组织和个人可以充分发挥LLM的潜力，推动创新并改善各类应用中的成果。
- en: 'By reading *The Deep Learning Architect Handbook*, you have gained invaluable
    insights into the various stages of the deep learning life cycle, exploring critical
    aspects from planning and data preparation to model deployment and governance.
    By reaching the end of this enlightening journey, you are now armed with the knowledge
    and skills to design, develop, and deploy effective deep learning solutions. To
    build upon this strong foundation, consider taking the following next steps:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读*《深度学习架构手册》*，你获得了对深度学习生命周期各个阶段的宝贵洞察，探索了从规划、数据准备到模型部署和治理的关键方面。在完成这段启发性的学习旅程后，你现在已经具备了设计、开发和部署有效深度学习解决方案的知识和技能。为了在这一坚实的基础上继续进步，考虑采取以下下一步行动：
- en: Apply your newfound knowledge to real-world projects, either in your professional
    field or through open source contributions, to gain practical experience and deepen
    your understanding
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你新学到的知识应用到实际项目中，无论是在你的专业领域，还是通过开源贡献，获取实践经验并加深理解。
- en: Stay up-to-date with the latest research, trends, and breakthroughs in deep
    learning by attending conferences, following influential researchers, and reading
    research papers
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过参加会议、关注有影响力的研究人员以及阅读研究论文，保持对深度学习领域最新研究、趋势和突破的了解。
- en: Explore specialized areas within deep learning that interest you, such as reinforcement
    learning, generative adversarial networks, or few-shot learning, to further expand
    your expertise
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习中你感兴趣的专业领域，如强化学习、生成对抗网络或少样本学习，进一步拓展你的专业知识。
- en: Collaborate with fellow deep learning enthusiasts and professionals, joining
    communities, discussion forums, and social media groups to exchange ideas, share
    experiences, and learn from each other
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他深度学习爱好者和专业人士合作，加入社区、讨论论坛和社交媒体群组，交流思想、分享经验、互相学习。
- en: Consider pursuing advanced courses, certifications, or even a degree in deep
    learning or a related field to enhance your education and qualifications
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑攻读深度学习或相关领域的进阶课程、认证，甚至是学位，以提升你的教育水平和资质。
- en: Embrace the challenges and triumphs that lie ahead, for with the mastery of
    building deep learning models with intricate deep learning architectures, a keen
    understanding of bias and fairness, and the ability to monitor and maintain model
    performance, you are well-prepared to unleash the full potential of deep learning
    and drive innovation across a vast array of applications. Here’s to your continued
    success and growth in the world of deep learning!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 拥抱未来的挑战与胜利，因为通过掌握构建深度学习模型和复杂深度学习架构的技能，深刻理解偏见和公平性，以及能够监控和维持模型性能，你已经为释放深度学习的全部潜力并推动广泛应用中的创新做好了充分准备。祝你在深度学习的世界中继续取得成功和成长！
- en: Further reading
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '*RAG*: [https://doi.org/10.48550/arXiv.2005.11401](https://doi.org/10.48550/arXiv.2005.11401)'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*RAG*: [https://doi.org/10.48550/arXiv.2005.11401](https://doi.org/10.48550/arXiv.2005.11401)'
- en: '*RETRO*: [https://arxiv.org/pdf/2112.04426.pdf](https://arxiv.org/pdf/2112.04426.pdf)'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*RETRO*: [https://arxiv.org/pdf/2112.04426.pdf](https://arxiv.org/pdf/2112.04426.pdf)'
- en: '*REALM*: [https://doi.org/10.48550/arXiv.2002.08909](https://doi.org/10.48550/arXiv.2002.08909)'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*REALM*: [https://doi.org/10.48550/arXiv.2002.08909](https://doi.org/10.48550/arXiv.2002.08909)'
