- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Harnessing Whisper for Personalized Voice Synthesis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用Whisper进行个性化语音合成
- en: Welcome to [*Chapter 9*](B21020_09.xhtml#_idTextAnchor207), where we’ll delve
    into **personalized voice synthesis** (**PVS**). This field encompasses various
    applications and technologies that create synthetic voices tailored to individual
    preferences or needs. PVS is a versatile process that can be customized for various
    purposes, including assistive technologies, virtual assistant development, and
    digital content creation. In this context, OpenAI’s Whisper tool enables voice
    synthesis by providing accurate speech data transcriptions during preprocessing
    and registration.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到[*第9章*](B21020_09.xhtml#_idTextAnchor207)，在这一章中，我们将深入探讨**个性化语音合成**（**PVS**）。这一领域涵盖了许多应用和技术，旨在根据个人偏好或需求创建合成语音。PVS是一个多功能的过程，可以根据不同的目的进行定制，包括辅助技术、虚拟助手开发和数字内容创作。在这个背景下，OpenAI的Whisper工具通过在预处理和注册过程中提供准确的语音数据转录，来实现语音合成。
- en: As we begin, it’s crucial to distinguish between voice cloning and PVS. Voice
    cloning involves creating a digital replica of a natural person’s voice. While
    this technology has valid applications, it also raises significant ethical concerns.
    PVS, however, focuses on creating unique voices inspired by specific characteristics
    without directly copying an individual’s voice. This distinction is vital in discussions
    about the ethical use of voice synthesis technologies. In this chapter, we will
    guide you on harnessing Whisper’s power to create PVS models, ensuring you have
    the knowledge to use this technology responsibly.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要区分语音克隆与PVS。语音克隆涉及创建自然人的语音数字复制品。虽然这一技术有其有效的应用场景，但也带来了重大伦理问题。然而，PVS专注于根据特定特征创造独特的声音，而不是直接复制某个人的声音。这一区别在讨论语音合成技术的伦理使用时至关重要。在本章中，我们将指导您如何利用Whisper的强大功能创建PVS模型，确保您掌握如何负责任地使用这项技术。
- en: We’ll begin by exploring **speech synthesis** and **text-to-speech** (**TTS**)
    fundamentals. You will gain insights into the role of neural networks, audio processing,
    and voice synthesis in this domain. Building on this foundation, we will guide
    you through converting audio files to the **LJSpeech** format, a standardized
    dataset structure commonly used in TTS tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探索**语音合成**和**文本转语音**（**TTS**）的基础知识。您将深入了解神经网络、音频处理和语音合成在这一领域中的作用。在此基础上，我们将指导您如何将音频文件转换为**LJSpeech**格式，这是一种在TTS任务中常用的标准化数据集结构。
- en: Next, we will introduce you to the **Deep Learning Art School** (**DLAS**) toolkit,
    a robust framework for fine-tuning PVS models. This is where your learning journey
    will truly begin. You will discover how to set up the training environment, prepare
    the dataset, and configure the model architecture. By leveraging the power of
    Whisper’s accurate transcriptions, you can align audio segments with their corresponding
    text, creating a dataset suitable for training PVS models. This tutorial is not
    just a guide but your gateway to mastering the art of PVS with Whisper. Get ready
    to be inspired and motivated!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向您介绍**深度学习艺术学校**（**DLAS**）工具包，这是一个强大的框架，用于微调PVS模型。在这里，您的学习旅程将真正开始。您将了解如何设置训练环境、准备数据集和配置模型架构。通过利用Whisper准确的转录结果，您可以将音频片段与相应的文本对齐，创建一个适合用于训练PVS模型的数据集。本教程不仅是一个指南，更是您掌握使用Whisper进行PVS艺术创作的入门通道。准备好迎接启发与激励吧！
- en: Hands-on examples and code snippets will give you practical experience fine-tuning
    a pre-trained PVS model using your LJSpeech dataset. You will discover how to
    customize the training process, select appropriate hyperparameters, and evaluate
    the model’s performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实际示例和代码片段，您将获得微调预训练PVS模型的实战经验，使用您的LJSpeech数据集。您将学习如何定制训练过程、选择适当的超参数并评估模型的性能。
- en: Finally, we will test your fine-tuned PVS model by synthesizing realistic and
    expressive speech. You will learn how to generate natural-sounding speech by providing
    text input to the model, bringing the PVS voice to life.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将测试您微调后的PVS模型，合成真实且富有表现力的语音。您将学习如何通过向模型提供文本输入来生成自然的语音，使PVS语音栩栩如生。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Understanding TTS in PVS
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解PVS中的TTS
- en: Converting audio files into LJSpeech format
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将音频文件转换为LJSpeech格式
- en: Fine-tuning a PVS model using the DLAS toolkit
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DLAS工具包微调PVS模型
- en: Synthesizing speech using a fine-tuned PVS model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微调后的PVS模型合成语音
- en: By the end of this chapter, you will have a comprehensive understanding of how
    to utilize Whisper for PVS. You will possess the knowledge and skills to preprocess
    audio data, fine-tune voice models, and generate realistic speech using PVS frameworks.
    Whether you are a researcher, developer, or enthusiast in speech technology, this
    chapter will equip you with valuable insights and practical techniques to unlock
    the potential of PVS using OpenAI’s Whisper.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将全面了解如何利用 Whisper 实现 PVS。你将具备预处理音频数据、微调语音模型并使用 PVS 框架生成真实语音的知识和技能。无论你是语音技术领域的研究人员、开发者还是爱好者，本章将为你提供宝贵的见解和实用技巧，帮助你释放
    OpenAI Whisper 在 PVS 中的潜力。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To harness the capabilities of OpenAI’s Whisper for advanced applications, this
    chapter leverages Python and Google Colab for ease of use and accessibility. The
    Python environment setup includes the Whisper library for transcription tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 OpenAI 的 Whisper 实现高级应用，本章使用 Python 和 Google Colab 以便于使用和访问。Python 环境的设置包括用于转录任务的
    Whisper 库。
- en: '**Key requirements**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要求**：'
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If the **T4 GPU** runtime type is available,
    select it for better performance.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab 笔记本**：这些笔记本已设置为以最低所需内存和能力运行我们的 Python 代码。如果有 **T4 GPU** 运行类型可用，选择它可以获得更好的性能。'
- en: '**Python environment**: Each notebook contains directives to load the required
    Python libraries.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 环境**：每个笔记本都包含加载所需 Python 库的指令。'
- en: '**Hugging Face account**: Some notebooks require a Hugging Face account and
    login API key. The Colab notebooks include information about this topic.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face 账户**：某些笔记本需要 Hugging Face 账户和登录 API 密钥。Colab 笔记本中包含关于此主题的信息。'
- en: '**Audacity**: Audacity is a free and open source digital audio editor and recording
    application available for Windows, macOS, Linux, and other Unix-like operating
    systems. It is an excellent choice if you want to synthesize your voice.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Audacity**：Audacity 是一个免费的开源数字音频编辑和录音应用程序，支持 Windows、macOS、Linux 和其他类 Unix
    操作系统。如果你想合成自己的声音，它是一个非常好的选择。'
- en: '**Microphone and speakers**: Some notebooks implement audio with voice recording
    and audio playback. A microphone and speakers connected to your computer might
    help you experience the interactive voice features.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**麦克风和扬声器**：一些笔记本实现了语音录音和音频播放功能。连接到计算机的麦克风和扬声器可能会帮助你体验互动语音功能。'
- en: '**GitHub repository access**: All Python code, including examples, is available
    in this chapter’s GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09)).
    These Colab notebooks are ready to run, providing a practical and hands-on approach
    to learning.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 仓库访问**：所有 Python 代码，包括示例，均可在本章的 GitHub 仓库中找到（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09)）。这些
    Colab 笔记本已经准备好运行，提供了一种实用的动手学习方法。'
- en: By meeting these technical requirements, you will be prepared to explore Whisper
    in different contexts while enjoying the streamlined experience of Google Colab
    and the comprehensive resources available on GitHub.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过满足这些技术要求，你将能够在不同的环境中探索 Whisper，同时享受 Google Colab 提供的简化体验和 GitHub 上的丰富资源。
- en: Understanding text-to-speech in voice synthesis
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解语音合成中的文本到语音
- en: 'TTS is a crucial component in the voice synthesis process, enabling speech
    to be generated from written text using the synthesized voice. Understanding the
    fundamentals of TTS is essential to grasp how voice synthesizing works and how
    it can be applied in various scenarios. *Figure 9**.1* illustrates a high-level
    overview of how TTS works in the context of voice synthesis without delving too
    deeply into technical specifics:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TTS 是语音合成过程中的关键组成部分，它可以将书面文本转化为语音。了解 TTS 的基本原理对于理解语音合成的工作原理以及如何在各种场景中应用它至关重要。*图
    9.1* 展示了 TTS 在语音合成中的工作原理的高层次概述，并没有深入探讨技术细节：
- en: '![Figure 9.1 – The TTS voice synthesis pipeline](img/Figure_9.1_B21020.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – TTS 语音合成流程](img/Figure_9.1_B21020.jpg)'
- en: Figure 9.1 – The TTS voice synthesis pipeline
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – TTS 语音合成流程
- en: 'There are five components in the TTS voice synthesis pipeline:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TTS 语音合成流程中有五个组件：
- en: '**Text preprocessing**:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本预处理**：'
- en: The input text is first normalized and preprocessed.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入文本首先会进行标准化和预处理。
- en: Numbers, abbreviations, and special characters are expanded into full words.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数字、缩写和特殊字符会被展开成完整的单词。
- en: The text is divided into individual sentences, words, and phonemes (distinct
    sound units).
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本被划分为单独的句子、单词和音素（独立的音频单位）。
- en: '**Text-to-spectrogram**:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本到频谱图**：'
- en: The normalized text is converted into a sequence of linguistic features and
    encoded into a vector representation.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 规范化的文本被转化为一系列语言特征，并编码成向量表示。
- en: A spectrogram generator model, usually a deep learning model, takes this encoded
    text and generates a spectrogram.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个频谱图生成模型，通常是一个深度学习模型，接收这个编码的文本并生成频谱图。
- en: The spectrogram visually represents the frequencies and intensities of the speech
    sounds over time.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 频谱图直观地表示了语音声音的频率和强度随时间的变化。
- en: '**Spectrogram-to-waveform**:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从频谱图到波形**：'
- en: The spectrogram is then fed into a vocoder model. The vocoder is a generative
    model trained to convert spectrograms into audible waveforms. It reconstructs
    the speech signal from the frequency information in the spectrogram.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，频谱图被输入到声码器模型中。声码器是一个生成模型，经过训练能够将频谱图转化为可听的波形。它根据频谱图中的频率信息重建语音信号。
- en: '**Voice synthesis**:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语音合成**：'
- en: To synthesize a specific person’s voice, the TTS models are fine-tuned on a
    dataset of that person’s speech. This allows the models to learn their voices’
    unique characteristics, tone, and prosody. With sufficient training data, the
    generated speech will mimic the target voice.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了合成特定人物的声音，TTS 模型会在该人物的语音数据集上进行微调。这使得模型能够学习该人物声音的独特特征、语气和韵律。通过足够的训练数据，生成的语音将模仿目标声音。
- en: '**Synthesis**:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**合成**：'
- en: Finally, the generated waveform is output as audible synthetic speech. The result
    is a synthesized voice that speaks the original input text.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终，生成的波形被输出为可听的合成语音。结果是一个合成的声音，能够朗读原始输入文本。
- en: Modern TTS systems can produce highly natural-sounding speech with appropriate
    intonation and expressiveness. The TTS pipeline, with its complex interplay of
    text processing, acoustic modeling, and speech synthesis, forms the foundation
    of the PVS transformative technology. As we explore the intricacies of voice synthesis,
    it is essential to understand how TTS systems can be leveraged to create personalized
    voices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 TTS 系统能够生成非常自然的语音，具备适当的语调和表现力。TTS 流程，通过文本处理、声学建模和语音合成的复杂互动，构成了 PVS 转型技术的基础。在我们探索语音合成的细节时，理解如何利用
    TTS 系统创造个性化语音至关重要。
- en: One such robust TTS implementation is **TorToiSe-TTS-Fast**, a high-performance
    TTS system that harnesses the power of neural networks to generate realistic and
    expressive speech. The following sections will delve into TorToiSe-TTS-Fast’s
    capabilities and demonstrate how it can synthesize voices with remarkable accuracy
    and naturalness.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强大的 TTS 实现是 **TorToiSe-TTS-Fast**，它是一个高性能的 TTS 系统，利用神经网络的力量生成真实且富有表现力的语音。接下来的部分将深入探讨
    TorToiSe-TTS-Fast 的功能，并展示它如何以惊人的准确性和自然感合成声音。
- en: Introducing TorToiSe-TTS-Fast
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 TorToiSe-TTS-Fast
- en: In [*Chapter 5*](B21020_05.xhtml#_idTextAnchor142), we used the gTTS Python
    library, an interface to Google Translate’s TTS API. gTTS lets you generate spoken
    audio from text using Google’s TTS engine. This time, we will explore the TorToiSe-TTS-Fast
    project, a high-performance TTS system that leverages neural networks to synthesize
    realistic speech without fine-tuning. Next, we will learn how to initialize the
    `TextToSpeech` model, which is the core component of the TTS system. We will explore
    the `TextToSpeech` class and understand its role in converting text into speech.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第五章*](B21020_05.xhtml#_idTextAnchor142) 中，我们使用了 gTTS Python 库，它是 Google
    翻译 TTS API 的接口。gTTS 允许你使用 Google 的 TTS 引擎将文本转化为语音。这一次，我们将探索 TorToiSe-TTS-Fast
    项目，这是一个高性能的 TTS 系统，利用神经网络合成真实语音，而无需微调。接下来，我们将学习如何初始化 `TextToSpeech` 模型，它是 TTS
    系统的核心组件。我们将深入了解 `TextToSpeech` 类及其在将文本转化为语音中的作用。
- en: 'One of the exciting features of the TorToiSe-TTS-Fast project is its ability
    to generate speech using different audio clip samples of a given voice. The project
    provides a collection of pre-packaged voices as audio clips organized in separate
    folders. These audio clips are used to determine many properties of the voice
    synthesized output, such as the pitch and tone of the voice, speaking speed, and
    even speaking defects, such as a lisp or stuttering. We will delve into selecting
    a voice from that collection of pre-existing voice samples. *Figure 9**.2* shows
    the TorToiSe-TTS-Fast voice processing:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: TorToiSe-TTS-Fast项目的一个令人兴奋的特点是能够使用给定声音的不同音频片段生成语音。该项目提供了一系列预打包的声音，这些声音作为音频片段组织在单独的文件夹中。这些音频片段用于确定声音合成输出的许多属性，如声音的音高和音调、说话速度，甚至说话缺陷，如口吃或结巴。我们将深入选择该预存在的声音样本集中的一个声音。*图
    9.2*展示了TorToiSe-TTS-Fast语音处理：
- en: '![Figure 9.2 – TorToiSe-TTS-Fast voice processing pipeline](img/Figure_9.2_B21020.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – TorToiSe-TTS-Fast 语音处理流水线](img/Figure_9.2_B21020.jpg)'
- en: Figure 9.2 – TorToiSe-TTS-Fast voice processing pipeline
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – TorToiSe-TTS-Fast语音处理流水线
- en: 'By following the steps in *Figure 9**.2*, you can incorporate additional voices
    into TorToiSe and enhance its versatility:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循*图 9.2*的步骤，您可以将额外的声音集成到TorToiSe中，增强其多功能性：
- en: Collect audio samples featuring the desired voice(s). Interviews on YouTube
    (which can be downloaded using `youtube-dl` or the `pytube` Python library, as
    we did in [*Chapter 6*](B21020_06.xhtml#_idTextAnchor160)), audiobooks, and podcasts
    are excellent sources. I recommend the **Audacity** tool as a viable option for
    recording your voice and processing audio files.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集包含所需语音的音频样本。YouTube上的访谈（可以使用`youtube-dl`或`pytube` Python库下载，就像我们在[*第6章*](B21020_06.xhtml#_idTextAnchor160)中所做的那样）、有声读物和播客都是很好的信息源。我推荐**Audacity**工具作为录制您的声音和处理音频文件的可行选择。
- en: Divide the collected audio into segments of approximately 10 seconds each. A
    minimum of 3 clips is required, but more clips are recommended for better results.
    During testing, I experimented with up to 5 clips.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将收集的音频分割成大约每个10秒的片段。至少需要3个片段，但建议使用更多片段以获得更好的结果。在测试过程中，我尝试了多达5个片段。
- en: Convert the audio segments into WAV format with floating-point encoding and
    a sample rate of 22,050 Hz.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将音频片段转换为浮点编码和22,050 Hz的WAV格式。
- en: Once you run the `LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb`
    notebook later in this chapter, you will see a directory structure called `/tortoise/voices/`
    with audio clip samples in it. This is the default folder TorToiSe uses to store
    and retrieve audio samples. If you create your samples, create a folder in that
    `/tortoise/voices/` directory and save your files there. For example, I made the
    `/tortoise/voices/josue` folder to store my audio files.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稍后在本章中运行`LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb`笔记本后，您将看到一个名为`/tortoise/voices/`的目录结构，其中包含音频片段样本。这是TorToiSe用来存储和检索音频样本的默认文件夹。如果您创建自己的样本，请在该`/tortoise/voices/`目录中创建一个文件夹并将文件保存在那里。例如，我创建了`/tortoise/voices/josue`文件夹来存储我的音频文件。
- en: Transfer the processed audio segments into the newly created subdirectory.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将处理过的音频片段转移到新创建的子目录中。
- en: To utilize the new voice, execute the `tortoise` utilities with the `--voice`
    flag, followed by the name of your subdirectory.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要利用新的语音，请执行`tortoise`工具，并使用`--voice`标志，后面跟上您子目录的名称。
- en: After exploring the TorToiSe-TTS-Fast pipeline, it should be clear that high-quality
    audio data is foundational to creating convincing, natural-sounding synthesized
    voices. Preparing this audio data involves creating new recordings or manipulating
    existing audio files to ensure they are suitable for voice synthesis. This is
    where Audacity comes into play as a powerful tool for audio creation, editing,
    and refinement. Of course, I encourage you to use other tools you are already
    using for audio processing; Audacity and creating an audio file is optional.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索TorToiSe-TTS-Fast流水线之后，可以明显感觉到高质量的音频数据是创造令人信服和自然音色的合成语音的基础。准备这些音频数据涉及创建新的录音或处理现有音频文件，以确保它们适合语音合成。这就是Audacity发挥作用的地方，作为音频创建、编辑和细化的强大工具。当然，我鼓励您使用其他您已经在音频处理中使用的工具；Audacity和创建音频文件是可选的。
- en: Audacity is a versatile tool for creating, editing, and manipulating audio files,
    an essential step in the voice synthesis pipeline. It allows you to record your
    voice samples, trim and split audio clips, adjust audio properties such as pitch
    and speed, and export files in various formats compatible with voice synthesis
    tools. By leveraging Audacity’s capabilities, you can prepare high-quality audio
    data tailored to your voice synthesis requirements.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Audacity 是一个多功能工具，用于创建、编辑和处理音频文件，是语音合成流程中的关键步骤。它允许你录制语音样本、剪辑和分割音频片段、调整音频属性，如音调和速度，并导出多种兼容语音合成工具的格式。利用
    Audacity 的功能，你可以准备适合语音合成要求的高质量音频数据。
- en: Using Audacity for audio processing
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Audacity 进行音频处理
- en: At its core, Audacity is a multitrack audio editor and recorder that supports
    many operating systems, including Windows, macOS, GNU/Linux, and other Unix-like
    systems. Its open source nature ensures it remains free for all users and fosters
    a vibrant community of developers and audio enthusiasts who continuously contribute
    to its development and enhancement. This collaborative effort has equipped Audacity
    with various capabilities, from basic recording and editing to more advanced features
    such as noise reduction, spectral analysis, and support for different audio formats.
    If you prefer another audio editor, go for it. The use of Audacity is optional.
    If you want to install it, here is a step-by-step guide.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Audacity 的核心是一个多轨音频编辑器和录音机，支持许多操作系统，包括 Windows、macOS、GNU/Linux 和其他类 Unix 系统。它的开源特性确保它对所有用户免费，并培养了一个充满活力的开发者和音频爱好者社区，大家持续贡献其开发和改进。这种协作使
    Audacity 拥有了各种功能，从基本的录音和编辑到更高级的特性，如噪声消除、频谱分析和支持多种音频格式。如果你偏好其他音频编辑软件，也可以选择它，使用
    Audacity 是可选的。如果你决定安装，它这里有一步步的安装指南。
- en: The installation process of Audacity is straightforward, regardless of your
    operating system. Detailed instructions are available on the Audacity website
    ([https://support.audacityteam.org/basics/downloading-and-installing-audacity](https://support.audacityteam.org/basics/downloading-and-installing-audacity)).
    Here, we’ll cover the basic steps to get Audacity up and running on your machine.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Audacity 的安装过程非常简单，无论你的操作系统是什么。详细的安装说明可以在 Audacity 网站上找到 ([https://support.audacityteam.org/basics/downloading-and-installing-audacity](https://support.audacityteam.org/basics/downloading-and-installing-audacity))。在这里，我们将介绍将
    Audacity 安装并运行在你的机器上的基本步骤。
- en: Installing Audacity for Windows
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 Audacity for Windows
- en: 'Follow these steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: '**Download the installer**: Navigate to the official Audacity website ([https://www.audacityteam.org/](https://www.audacityteam.org/))
    and click on the download link for the Windows version. The site will automatically
    detect your operating system, but you can manually select the version if needed.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载安装程序**：访问 Audacity 官方网站 ([https://www.audacityteam.org/](https://www.audacityteam.org/))，点击
    Windows 版本的下载链接。网站会自动检测你的操作系统，但如果需要，你也可以手动选择版本。'
- en: '`Downloads` folder) and double-click to initiate installation. You might encounter
    a security prompt asking for permission to allow the installer to change your
    system; click **Yes** to proceed.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Downloads` 文件夹中）并双击以启动安装。你可能会遇到一个安全提示，询问是否允许安装程序更改你的系统；点击**是**继续。'
- en: '**Follow the installation wizard**: The installer will guide you through several
    steps. You’ll select your preferred language, agree to the license terms, choose
    the installation directory, and decide on additional tasks, such as creating a
    desktop shortcut.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**按照安装向导操作**：安装程序会引导你完成几个步骤。你将选择首选语言，同意许可协议，选择安装目录，并决定是否执行附加任务，如创建桌面快捷方式。'
- en: '**Complete the installation**: After configuring your preferences, click **Install**
    to begin the installation. Once completed, you can launch Audacity directly from
    the installer or find it in your Start menu.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完成安装**：配置好你的偏好设置后，点击**安装**开始安装。安装完成后，你可以直接从安装程序启动 Audacity，或者在开始菜单中找到它。'
- en: Installing Audacity for macOS
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 Audacity for macOS
- en: 'Follow these steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: '**Download the DMG file**: Visit the Audacity website and download the macOS
    version. The site should automatically provide the correct version of your system.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载 DMG 文件**：访问 Audacity 官网并下载 macOS 版本。网站应自动提供与你系统匹配的版本。'
- en: '`Applications` folder to install the software. You might need to authenticate
    using your administrator password.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将软件拖入`Applications` 文件夹进行安装。你可能需要使用管理员密码进行身份验证。
- en: '`Applications` folder. macOS might prompt you to confirm that you trust the
    application, especially if you’re running it for the first time.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Applications`文件夹。macOS可能会提示您确认是否信任该应用程序，特别是在第一次运行时。'
- en: Installing Audacity for Linux
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Linux上安装Audacity
- en: 'Linux users can download AppImage from the Audacity website or install Audacity
    using their distribution’s package manager. For AppImage, follow these steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Linux用户可以从Audacity网站下载AppImage，或者使用其发行版的包管理器安装Audacity。对于AppImage，请按照以下步骤操作：
- en: '**Make the AppImage executable**: After downloading, right-click the file,
    navigate to **Properties** | **Permissions**, and check the option to make the
    file executable.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使AppImage文件可执行**：下载后，右键单击文件，导航至**属性** | **权限**，并勾选使文件可执行的选项。'
- en: '**Run Audacity**: Double-click AppImage to launch Audacity.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行Audacity**：双击AppImage文件以启动Audacity。'
- en: Alternatively, use commands such as `sudo apt install audacity` for Debian-based
    distributions or `sudo yum install audacity` for Fedora/RHEL to install Audacity
    through the Terminal.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，使用命令如`sudo apt install audacity`（适用于基于Debian的发行版）或`sudo yum install audacity`（适用于Fedora/RHEL）通过终端安装Audacity。
- en: Running the notebook with TorToiSe-TTS-Fast
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TorToiSe-TTS-Fast运行笔记本
- en: With a more detailed understanding of Audacity as an audio creation, manipulation,
    and management tool, let’s do some hands-on work with TorToiSe-TTS-Fast. Please
    find and open the Colab notebook called `LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb)).
    This notebook is based on the TorToiSe-TTS-Fast ([https://github.com/152334H/tortoise-tts-fast](https://github.com/152334H/tortoise-tts-fast))
    TTS project, which drastically boosts the performance of `TorToiSe` ([https://github.com/neonbjb/tortoise-tts](https://github.com/neonbjb/tortoise-tts)),
    without modifying the base models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在更详细了解Audacity作为音频创建、编辑和管理工具后，让我们动手使用TorToiSe-TTS-Fast进行操作。请找到并打开名为`LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb`的Colab笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb)）。该笔记本基于TorToiSe-TTS-Fast（[https://github.com/152334H/tortoise-tts-fast](https://github.com/152334H/tortoise-tts-fast)）TTS项目，该项目在不修改基础模型的情况下大幅提升了`TorToiSe`（[https://github.com/neonbjb/tortoise-tts](https://github.com/neonbjb/tortoise-tts)）的性能。
- en: Using the notebook, we will develop speech from a given text with the `TextToSpeech`
    model initialized and a chosen voice. Furthermore, we will investigate the flexibility
    of the TorToiSe-TTS-Fast project by generating speech using random and even custom
    voices. We can create personalized voices for speech synthesis by uploading and
    preprocessing our WAV files.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该笔记本，我们将通过初始化`TextToSpeech`模型和选择语音来从给定的文本中生成语音。此外，我们还将通过使用随机或自定义语音来探索TorToiSe-TTS-Fast项目的灵活性。我们可以通过上传和预处理WAV文件来为语音合成创建个性化的语音。
- en: Lastly, we will explore the fascinating capability of combining multiple voices
    to generate speech with blended characteristics. We can create unique and intriguing
    voice combinations by loading voice samples and conditioning latents from different
    voices.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探索结合多种语音生成具有融合特征的语音的迷人能力。通过加载不同语音的样本和条件潜变量，我们可以创建独特而有趣的语音组合。
- en: By the end of this section, you will have a solid understanding of TTS in the
    context of voice synthesis. You will have the knowledge and practical skills to
    set up the environment, initialize the `TextToSpeech` model, select voices, generate
    speech, and create custom and combined voices using the TorToiSe-TTS-Fast project.
    This understanding will serve as a foundation for further exploring the potential
    of voice synthesis and its applications in various domains.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，您将对语音合成中的TTS有一个坚实的理解。您将掌握设置环境、初始化`TextToSpeech`模型、选择语音、生成语音以及使用TorToiSe-TTS-Fast项目创建自定义和组合语音的知识和实践技能。这个理解将为进一步探索语音合成的潜力及其在各个领域的应用打下基础。
- en: 'Let’s open the notebook and run the cells to better understand the voice synthesis
    pipeline in the TorToiSe-TTS-Fast project:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开笔记本并运行单元，以更好地理解TorToiSe-TTS-Fast项目中的语音合成流程：
- en: '**Setting up the environment**: Here, we will install and instantiate several
    libraries, each serving a distinct purpose in the project setup:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境设置**：在这里，我们将安装并实例化几个库，每个库在项目设置中扮演着不同的角色：'
- en: '[PRE0]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s briefly review each library:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下每个库的作用：
- en: '`torch`: This is the PyTorch library, a popular open source machine learning
    library for computer vision and natural language processing applications. In the
    context of this project, PyTorch provides the foundational framework for building
    and training the neural networks that underpin the voice synthesis capabilities
    of TorToiSe-TTS-Fast.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`：这是 PyTorch 库，一个广泛使用的开源机器学习库，主要用于计算机视觉和自然语言处理应用。在本项目中，PyTorch 提供了构建和训练神经网络的基础框架，这些神经网络支撑了
    TorToiSe-TTS-Fast 的语音合成功能。'
- en: '`torchaudio`: As an extension to PyTorch, `torchaudio` offers easy access to
    audio processing tools within the PyTorch framework. It is used for loading and
    saving audio files and performing transformations and augmentations on audio data,
    which are essential tasks in voice synthesis.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchaudio`：作为 PyTorch 的扩展，`torchaudio` 提供了在 PyTorch 框架内轻松访问音频处理工具的能力。它用于加载和保存音频文件，并对音频数据进行转换和增强，这些都是语音合成中的关键任务。'
- en: '`huggingface_hub`: This library from Hugging Face allows users to easily download
    and upload models and other files to the Hugging Face Hub, which may include the
    pre-trained models or components required by the `TextToSpeech` class for voice
    synthesis. The `huggingface_hub` library also provides a function for authenticating
    with the Hugging Face Hub via `notebook_login()` and managing user information
    via `whoami()`, facilitating access to the models and resources stored on the
    Hub required for voice synthesis.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_hub`：这是 Hugging Face 提供的库，允许用户轻松地将模型和其他文件下载或上传到 Hugging Face Hub，其中可能包含
    `TextToSpeech` 类所需的预训练模型或组件。`huggingface_hub` 库还提供了通过 `notebook_login()` 进行 Hub
    认证的功能，并通过 `whoami()` 管理用户信息，方便访问 Hub 上存储的用于语音合成的模型和资源。'
- en: '`transformers (version 4.29.2)`: The `transformers` library, also from Hugging
    Face, provides thousands of pre-trained models for various natural language processing
    tasks, including TTS. This library supports the underlying NLP and TTS functionalities
    of the TorToiSe-TTS-Fast project by providing access to state-of-the-art models
    and utilities.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers (版本 4.29.2)`：这是 Hugging Face 提供的 `transformers` 库，提供了成千上万的预训练模型，涵盖各种自然语言处理任务，包括
    TTS。此库支持 TorToiSe-TTS-Fast 项目中的 NLP 和 TTS 功能，提供访问最先进模型和工具的能力。'
- en: '`voicefixer (version 0.1.2)`: This tool is designed for repairing and enhancing
    human voice recordings. `voicefixer` improves the quality of voice samples before
    they are processed by the voice synthesis system, ensuring higher fidelity in
    the synthesized voices.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`voicefixer (版本 0.1.2)`：该工具旨在修复和增强人声录音。`voicefixer` 在语音合成系统处理前改善语音样本的质量，确保合成语音的更高保真度。'
- en: '`BigVGAN`: The TTS model uses a `BigVGAN` library plays a role in the voice
    synthesis process, enhancing the realism or quality of the generated voices.'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BigVGAN`：TTS 模型使用 `BigVGAN` 库，在语音合成过程中发挥作用，增强生成语音的真实感或质量。'
- en: Each of these libraries contributes to the overall functionality of the TorToiSe-TTS-Fast
    project by providing essential tools and frameworks for machine learning, audio
    processing, model management, and voice enhancement. These enable the efficient
    and practical synthesis of human voices.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些库每个都为 TorToiSe-TTS-Fast 项目的整体功能做出贡献，提供了机器学习、音频处理、模型管理和语音增强的基础工具和框架。这些工具使得人声合成的高效与实用成为可能。
- en: '`TextToSpeech` model that will be used for synthesizing voices. The steps involved
    in this section are designed to initialize the TTS model so that it is ready to
    process text input and generate corresponding speech output using the selected
    voice:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于合成语音的 `TextToSpeech` 模型。本节中的步骤旨在初始化 TTS 模型，使其能够处理文本输入并生成对应的语音输出，使用所选的声音：
- en: '[PRE1]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here are the steps that are outlined in the preceding code:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码中概述的步骤：
- en: '`TextToSpeech``TextToSpeech` class from the `tortoise.api` module. This class
    is the primary interface for the TTS functionality provided by the TorToiSe-TTS-Fast
    project.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextToSpeech``TextToSpeech` 类来自 `tortoise.api` 模块。该类是 TorToiSe-TTS-Fast 项目提供的
    TTS 功能的主要接口。'
- en: '`TextToSpeech`: After importing the class, an instance of `TextToSpeech` is
    created by simply calling the class constructor without any arguments. This instance
    is assigned to the `tts` variable.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextToSpeech`：导入该类后，通过简单调用类构造函数（不带任何参数）创建一个 `TextToSpeech` 实例。这个实例被分配给 `tts`
    变量。'
- en: '`TextToSpeech`, the necessary models are automatically downloaded from the
    Hugging Face Hub. This step ensures that all the required components for voice
    synthesis are available locally.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextToSpeech`，所需的模型会自动从 Hugging Face Hub 下载。此步骤确保所有用于语音合成的必要组件都已本地化。'
- en: The `TextToSpeech` class encapsulates the functionality needed to convert text
    into speech. Initializing it is critical in preparing the system for voice synthesizing
    tasks. Once the model has been initialized, it can be used in subsequent steps
    to generate speech from text using various voices.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextToSpeech` 类封装了将文本转换为语音所需的功能。初始化它是为语音合成任务做好准备的关键步骤。一旦模型初始化完成，它可以在后续步骤中用于使用各种语音从文本生成语音。'
- en: '**Selecting a voice**: This section is crucial for personalizing the voice
    synthesis process by allowing you to choose from various pre-existing voice samples
    or uploaded voice clips:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择语音**：此部分对于通过允许你选择各种预先存在的语音样本或上传的语音片段来个性化语音合成过程至关重要。'
- en: '[PRE2]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The steps involved in guiding you through the process of selecting a specific
    voice to synthesize are as follows:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择特定语音进行合成的步骤如下：
- en: '`os` module to interact with the operating system and lists all the available
    voice folders in the `tortoise/voices` directory. This step is essential for identifying
    which voices are available for synthesis.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`os` 模块用于与操作系统交互，并列出 `tortoise/voices` 目录下所有可用的语音文件夹。此步骤对于识别哪些语音可供合成至关重要。'
- en: '`Dropdown` class from the `ipywidgets` library. This widget allows you to select
    a voice folder from the list of available folders. The `Dropdown` widget is configured
    with options populated from the list of voice folders, a description prompt (`"Select
    a voice:"`), and other settings to ensure usability.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dropdown` 类来自 `ipywidgets` 库。这个小部件允许你从可用的文件夹列表中选择一个语音文件夹。`Dropdown` 小部件配置了由语音文件夹列表填充的选项、描述提示（`"Select
    a voice:"`）以及其他设置，确保其可用性。'
- en: '`os.listdir` function, and the `Dropdown` widget is again used to present these
    options to you.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `os.listdir` 函数，以及 `Dropdown` 小部件再次向你展示这些选项。
- en: '`IPython.display.Audio` class plays the voice file chosen directly in the Colab
    notebook. This feature provides immediate auditory feedback, enabling you to confirm
    that the selected voice is the desired one for synthesis.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IPython.display.Audio` 类直接在 Colab 笔记本中播放选定的语音文件。此功能提供即时的听觉反馈，使你能够确认所选语音是否为期望的合成语音。'
- en: These steps collectively enable a user-friendly and interactive approach to
    selecting a voice for synthesis. They ensure that you can easily navigate the
    available options and make an informed choice based on your preferences or project
    requirements.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些步骤共同提供了一种用户友好且互动性强的方法，帮助你选择合成语音。它们确保你能够轻松浏览可用选项，并根据你的偏好或项目需求做出明智的选择。
- en: '**Generating speech with a selected voice**: This section of the notebook generates
    speech audio from text using the voice previously specified by you:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成带有选定语音的语音**：此笔记本部分根据你之前指定的语音从文本生成语音：'
- en: '[PRE3]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The steps in this section are designed to convert your input text into spoken
    words in the chosen voice’s style. Here are the steps outlined in the code:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本节的步骤旨在将你的输入文本转换为所选语音风格的口语。以下是代码中概述的步骤：
- en: '`text`. This text will be synthesized into speech.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`。该文本将被合成成语音。'
- en: '`"ultra_fast"`, `"fast"`, `"standard"`, and `"high_quality"`. This determines
    the trade-off between generation speed and audio quality.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"ultra_fast"`、`"fast"`、`"standard"` 和 `"high_quality"`。这些选项决定了生成速度与音频质量之间的权衡。'
- en: '`load_voice` function from `tortoise.utils.audio` is used to load the selected
    voice. This function returns two items: `voice_samples` and `conditioning_latents`.
    These condition the TTS model to generate speech in the selected voice’s style.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_voice` 函数来自 `tortoise.utils.audio`，用于加载选定的语音。此函数返回两个项：`voice_samples`
    和 `conditioning_latents`。这些项将调整 TTS 模型，以生成符合所选语音风格的语音。'
- en: '`tts_with_preset` method of the `tts` object (an instance of the `TextToSpeech`
    class) is called with the text, voice samples, conditioning latents, and preset.
    This method synthesizes the speech based on the given parameters.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用 `tts` 对象（`TextToSpeech` 类的实例）中的 `tts_with_preset` 方法，并传入文本、语音样本、条件潜变量和预设。这一方法根据给定参数合成语音。
- en: '`torchaudio.save` function. The file is then played using `IPython.display.Audio`
    to allow you to hear the synthesized speech.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `torchaudio.save` 函数。然后通过 `IPython.display.Audio` 播放文件，允许你听到合成的语音。
- en: These steps enable you to create a spoken version of their text using the specific
    characteristics of the chosen voice, effectively using the PVS model for speech
    synthesis.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些步骤使你能够使用所选语音的特定特征创建文本的语音版本，实质上使用 PVS 模型进行语音合成。
- en: '`voice_samples` and `conditioning_latents` set to `None`, which generates speech
    using a random voice:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `voice_samples` 和 `conditioning_latents` 设置为 `None`，这将使用随机语音生成语音：
- en: '[PRE4]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Using a custom voice**: The following code allows users to upload their WAV
    files (6-10 seconds long) to create a custom voice:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用自定义语音**：以下代码允许用户上传他们的WAV文件（时长6-10秒）以创建自定义语音：'
- en: '[PRE5]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It creates a custom voice folder using `os.makedirs` and saves the uploaded
    files in that folder. The custom voice is then loaded and used to generate speech,
    similar to *steps 4* and *5*.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它使用 `os.makedirs` 创建一个自定义语音文件夹，并将上传的文件保存在该文件夹中。然后加载自定义语音并用它来生成语音，类似于 *步骤 4*
    和 *步骤 5*。
- en: '`load_voices` function loads multiple voices (in this case, `''pat''` and `''william''`).
    The `tts_with_preset` method combines voice samples and conditioning latents to
    generate speech with traits from both voices:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_voices` 函数加载多个语音（在本例中为 `''pat''` 和 `''william''`）。`tts_with_preset` 方法结合语音样本和条件潜变量，生成具有两种语音特征的语音：'
- en: '[PRE6]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Having gained a foundational understanding of TTS in voice synthesis and explored
    the powerful capabilities of the TorToiSe-TTS-Fast project, we’ll turn our attention
    to a crucial step in preparing our data for the voice synthesizing process: converting
    audio files into the LJSpeech format.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在对语音合成中的 TTS 基础知识有了初步了解，并探索了 TorToiSe-TTS-Fast 项目的强大功能后，我们将把注意力集中在为语音合成过程准备数据的关键步骤上：将音频文件转换为
    LJSpeech 格式。
- en: PVS step 1 – Converting audio files into LJSpeech format
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PVS 步骤 1 – 将音频文件转换为 LJSpeech 格式
- en: This section and the accompanying notebook, `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`,
    represent the initial step in the three-step PVS process outlined in this chapter.
    This step takes an audio sample of the target voice as input and processes it
    into the LJSpeech dataset format. The notebook demonstrates using the OZEN Toolkit
    and OpenAI’s Whisper to extract speech, transcribe it, and organize the data according
    to the LJSpeech structure. The resulting LJSpeech-formatted dataset, consisting
    of segmented audio files and corresponding transcriptions, serves as the input
    for the second step, *PVS step 2 – Fine-tuning a discrete variational autoencoder
    using the DLAS toolkit*, where a PVS model will be fine-tuned using this dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节及随附的笔记本 `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`
    代表了本章中 PVS 三步过程的初步步骤。此步骤以目标语音的音频样本为输入，并将其处理成 LJSpeech 数据集格式。该笔记本演示了如何使用 OZEN 工具包和
    OpenAI 的 Whisper 来提取语音、转录并根据 LJSpeech 结构整理数据。最终得到的 LJSpeech 格式数据集，由分段的音频文件和相应的转录组成，作为第二步
    *PVS 步骤 2 – 使用 DLAS 工具包微调离散变分自编码器* 的输入，在这一过程中，PVS 模型将使用该数据集进行微调。
- en: An LJSpeech-formatted dataset is crucial in TTS models as it provides a standardized
    structure for organizing audio files and their corresponding transcriptions. By
    following the LJSpeech format, researchers and developers can ensure compatibility
    with various TTS tools and facilitate training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LJSpeech 格式的数据集在 TTS 模型中至关重要，因为它为组织音频文件及其相应的转录提供了标准化结构。通过遵循 LJSpeech 格式，研究人员和开发者可以确保与各种
    TTS 工具兼容，并促进训练。
- en: 'An LJSpeech-formatted dataset refers to a specific structure and organization
    of audio files and their corresponding transcriptions modeled after the **LJSpeech
    dataset** ([https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)).
    The LJSpeech dataset is a public domain speech dataset that includes 13,100 short
    audio clips of a single speaker reading passages from seven non-fiction books,
    with a transcription of each clip. The audio clips vary in length and have a total
    duration of approximately 24 hours. When formatting a dataset for training a TTS
    model in the style of LJSpeech, the following structure is recommended:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LJSpeech 格式的数据集指的是按照**LJSpeech 数据集**（[https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)）的结构和组织方式安排音频文件及其对应转录的特定结构。LJSpeech
    数据集是一个公共领域的语音数据集，包含13,100个由单一发言人朗读七本非虚构书籍中的段落的短音频片段，每个片段都有相应的转录文本。这些音频片段的长度不一，总时长约为24小时。在为TTS模型训练准备LJSpeech格式的数据集时，建议使用以下结构：
- en: Audio clips should be divided into separate files with a corresponding transcription.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频片段应分割成单独的文件，并为每个文件提供相应的转录内容。
- en: The WAV file format should be used for the audio to avoid compression artifacts.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频应使用WAV文件格式，以避免压缩伪影。
- en: The audio clips and their transcriptions are collected in a folder named `wavs`.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频片段及其转录内容会被保存在名为`wavs`的文件夹中。
- en: A metadata text file maps each audio clip to its transcription. This file should
    have columns delimited by a special character, typically a pipe (`|`), to separate
    the audio filename, the transcription, and the normalized transcription.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个元数据文本文件将每个音频片段映射到其转录内容。该文件应具有通过特殊字符（通常是管道符号`|`）分隔的列，以区分音频文件名、转录文本和标准化转录文本。
- en: The delimiter used in the metadata file should not appear in the transcription
    text itself.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据文件中使用的分隔符不应出现在转录文本中。
- en: If normalized transcriptions are unavailable, the same transcription can be
    used for both columns, with normalization applied later in the pipeline.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标准化转录不可用，则可以在两个列中使用相同的转录内容，后续流程中再进行标准化。
- en: 'The folder structure for an LJSpeech-formatted dataset would look like this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: LJSpeech格式数据集的文件夹结构如下所示：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the text files, entries would be formatted as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件中的条目将按如下格式进行组织：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The LJSpeech format is widely used because it is supported by various TTS tools,
    such as TorToiSe, which provides tooling for the LJSpeech dataset. Formatting
    a dataset this way allows for immediate model training without additional formatting
    steps.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: LJSpeech格式被广泛使用，因为它受多种TTS工具的支持，例如TorToiSe，它为LJSpeech数据集提供了相关工具。以这种方式格式化数据集，可以立即开始模型训练，而无需额外的格式化步骤。
- en: Now that we understand the LJSpeech format and why it’s used, let’s convert
    our audio files into this format. By doing so, we’ll ensure that our dataset is
    compatible with various TTS tools and ready for training our PVS models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了LJSpeech格式及其应用原因，接下来让我们将音频文件转换为这种格式。这样做将确保我们的数据集与各种TTS工具兼容，并为训练PVS模型做好准备。
- en: Once you have a recording of the voice you would like to synthesize, the next
    step is to preprocess the audio files using the OZEN Toolkit. This toolkit simplifies
    extracting speech, transcribing it using Whisper, and saving the results in the
    LJSpeech format. It can handle both single audio files and entire folders of audio
    files.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了想要合成的语音录音，下一步就是使用OZEN工具包预处理音频文件。该工具包简化了提取语音、使用Whisper转录并将结果保存为LJSpeech格式的过程。它可以处理单个音频文件或整个音频文件夹。
- en: Leveraging the OZEN Toolkit and Whisper allows us to efficiently convert our
    audio data into the LJSpeech format. The toolkit automates segmenting audio files,
    generating corresponding WAV files, and creating the necessary metadata files
    (`train.txt` and `valid.txt`) that map the audio files to their transcriptions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 利用OZEN工具包和Whisper，我们可以高效地将音频数据转换为LJSpeech格式。该工具包可以自动分割音频文件，生成相应的WAV文件，并创建必要的元数据文件（`train.txt`
    和 `valid.txt`），将音频文件与它们的转录内容一一对应。
- en: Converting audio files into the LJSpeech format is a crucial skill in the voice
    synthesis pipeline as it ensures data compatibility and facilitates the training
    process. Mastering this technique will prepare you to tackle the subsequent steps,
    such as fine-tuning PVS models and synthesizing speech.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 将音频文件转换为LJSpeech格式是语音合成流程中的一项关键技能，因为它确保了数据兼容性并促进了训练过程。掌握此技术将为你解决后续步骤，如微调PVS模型和语音合成，做好准备。
- en: 'Please find and open the Colab notebook called `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb)).
    This notebook is based on the OZEN Toolkit project ([https://github.com/devilismyfriend/ozen-toolkit](https://github.com/devilismyfriend/ozen-toolkit)).
    Given a folder of files or a single audio file, it will extract the speech, transcribe
    using Whisper, and save it in LJ format (segmented audio files in WAV format go
    in the `wavs` folder, while transcriptions go in the `train` and `valid` folders).
    Let’s walk through the code while explaining the steps and providing code samples:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请查找并打开名为`LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`的Colab笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb)）。该笔记本基于OZEN
    Toolkit项目（[https://github.com/devilismyfriend/ozen-toolkit](https://github.com/devilismyfriend/ozen-toolkit)）。给定一个文件夹或单个音频文件，它将提取语音，使用Whisper进行转录，并以LJ格式保存（分段音频文件以WAV格式存放在`wavs`文件夹中，转录内容存放在`train`和`valid`文件夹中）。让我们逐步解析代码，并提供代码示例：
- en: '**Cloning the OZEN Toolkit repository**: The following command clones the OZEN
    Toolkit repository from GitHub, which contains the necessary scripts and utilities
    for processing audio files:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**克隆OZEN Toolkit仓库**：以下命令从GitHub克隆OZEN Toolkit仓库，其中包含处理音频文件所需的脚本和工具：'
- en: '[PRE9]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Installing the required libraries**: The following commands install the necessary
    libraries for audio processing, speech recognition, and text formatting. After
    installing the dependencies, restarting the session is recommended to ensure the
    installed packages are initialized adequately:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装所需的库**：以下命令将安装音频处理、语音识别和文本格式化所需的库。安装完依赖项后，建议重启会话，以确保已安装的包正确初始化：'
- en: '[PRE10]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '%cd ozen-toolkit'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '%cd ozen-toolkit'
- en: '[PRE11]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Downloading a sample audio file**: If you do not have an audio file for synthesis,
    this command downloads a sample audio file from the specified URL for demonstration
    purposes:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载示例音频文件**：如果你没有合成用的音频文件，可以使用此命令从指定的URL下载一个示例音频文件进行演示：'
- en: '[PRE12]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: import os
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import os
- en: from google.colab import files
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from google.colab import files
- en: custom_voice_folder = "./myaudiofile"
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: custom_voice_folder = "./myaudiofile"
- en: os.makedirs(custom_voice_folder, exist_ok=True)  # Create the directory if it
    doesn't exist
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: os.makedirs(custom_voice_folder, exist_ok=True)  # 如果目录不存在，则创建
- en: 'for filename, file_data in files.upload().items():'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'for filename, file_data in files.upload().items():'
- en: 'with open(os.path.join(custom_voice_folder, filename), ''wb'') as f:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`with open(os.path.join(custom_voice_folder, filename), 'wb') as f:`
- en: f.write(file_data)
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f.write(file_data)
- en: '%ls -l "$PWD"/{*,.*}'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '%ls -l "$PWD"/{*,.*}'
- en: '[PRE13]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`"config.ini"` using the `configparser` library. It defines various settings,
    such as the Hugging Face API key, Whisper model, device, diarization and segmentation
    models, validation ratio, and segmentation parameters:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`configparser`库处理`"config.ini"`文件。它定义了各种设置，如Hugging Face API密钥、Whisper模型、设备、分段和分割模型、验证比例以及分段参数：
- en: '[PRE14]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`ozen.py` script with the sample audio file as an argument (or the file you
    uploaded):'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ozen.py`脚本并将示例音频文件作为参数（或上传的文件）：
- en: '[PRE15]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 9.3 – The pyannote/segmentation gated model on Hugging Face](img/Figure_9.3_B21020.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – Hugging Face上的pyannote/segmentation门控模型](img/Figure_9.3_B21020.jpg)'
- en: Figure 9.3 – The pyannote/segmentation gated model on Hugging Face
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – Hugging Face上的pyannote/segmentation门控模型
- en: 'Run the cell after ensuring you have access to the pyannote/segmentation model.
    The `ozen.py` script processes the audio file, extracts speech, transcribes it
    using Whisper, and saves the output in the LJSpeech format. The script saves the
    DJ format files in a folder called `ozen-toolkit/output/<audio file name + timestamp>/`.
    Here is an example of the expected file structure:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在确保已访问pyannote/segmentation模型后运行该单元格。`ozen.py`脚本处理音频文件，提取语音，使用Whisper进行转录，并以LJSpeech格式保存输出。该脚本将DJ格式文件保存在名为`ozen-toolkit/output/<音频文件名+时间戳>/`的文件夹中。以下是预期文件结构的示例：
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Mounting Google Drive**: The following lines mount your Google Drive to the
    Colab environment, allowing access to the drive for saving checkpoints and loading
    datasets:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挂载Google Drive**：以下代码行将你的Google Drive挂载到Colab环境中，允许访问Google Drive以保存检查点和加载数据集：'
- en: '[PRE17]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`ozen-toolkit/output` directory to your Google Drive. After running the cell,
    go to your Google Drive using a web browser, as shown in *Figure 9**.4*; you will
    see a directory called `output` with the DJ format dataset files in it:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `ozen-toolkit/output` 目录保存到 Google Drive。运行单元格后，使用网页浏览器打开 Google Drive，如 *图
    9.4* 所示；你会看到一个名为 `output` 的目录，其中包含 DJ 格式的数据集文件：
- en: '[PRE18]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here’s the output:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 9.4 – Identifying the location of the DJ format files from the ozen.py
    script](img/Figure_9.4_B21020.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 从 ozen.py 脚本中识别 DJ 格式文件的位置](img/Figure_9.4_B21020.jpg)'
- en: Figure 9.4 – Identifying the location of the DJ format files from the ozen.py
    script
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 从 ozen.py 脚本中识别 DJ 格式文件的位置
- en: 'After running the cell, go to your Google Drive using a web browser; you will
    see a directory called `output` with the DJ format dataset files in it:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格后，使用网页浏览器打开 Google Drive；你会看到一个名为 `output` 的目录，其中包含 DJ 格式的数据集文件：
- en: '![Figure 9.5 – Example of the DJ format output folder in Google Colab](img/Figure_9.5_B21020.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – Google Colab 中 DJ 格式输出文件夹示例](img/Figure_9.5_B21020.jpg)'
- en: Figure 9.5 – Example of the DJ format output folder in Google Colab
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – Google Colab 中 DJ 格式输出文件夹示例
- en: The Python code in the `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`
    notebook demonstrates setting up the environment, installing dependencies, configuring
    the OZEN Toolkit, processing audio files using Whisper, and saving the output
    in the LJSpeech format. It provides a streamlined workflow for preparing audio
    data for further analysis or use in downstream tasks.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb` 笔记本中的
    Python 代码演示了如何设置环境、安装依赖项、配置 OZEN 工具包、使用 Whisper 处理音频文件并将输出保存为 LJSpeech 格式。它提供了一个简化的工作流程，用于为进一步分析或下游任务准备音频数据。'
- en: 'With our audio data now converted into the LJSpeech format, we are well-prepared
    to embark on the following critical stage of the voice synthesis journey: fine-tuning
    a PVS model using the powerful DLAS toolkit. The `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    notebook will cover this process in detail in the next section. By leveraging
    the DLAS toolkit’s comprehensive features and the structured LJSpeech dataset,
    we can create a personalized voice model that captures the unique characteristics
    of our target voice with remarkable accuracy and naturalness.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的音频数据已转换为 LJSpeech 格式，我们已经做好了准备，进入语音合成过程的下一个关键阶段：使用强大的 DLAS 工具包微调 PVS 模型。`LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    笔记本将在下一节详细介绍这个过程。通过利用 DLAS 工具包的全面功能和结构化的 LJSpeech 数据集，我们可以创建一个个性化的语音模型，精准且自然地捕捉目标语音的独特特征。
- en: PVS step 2 – Fine-tuning a PVS model with the DLAS toolkit
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PVS 步骤 2 – 使用 DLAS 工具包微调 PVS 模型
- en: Fine-tuning a PVS model is a critical step in creating personalized voices that
    capture the unique characteristics of a voice. To achieve high-quality results,
    utilizing a robust framework that leverages state-of-the-art techniques and provides
    flexibility in customizing the training process is essential. The DLAS toolkit
    emerges as a comprehensive solution for fine-tuning PVS models, offering a range
    of features and capabilities.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 微调 PVS 模型是创建个性化语音、捕捉语音独特特征的关键步骤。为了获得高质量的结果，必须使用一个强大的框架，利用最先进的技术，并提供灵活性来定制训练过程。DLAS
    工具包成为微调 PVS 模型的综合解决方案，提供了多种功能和能力。
- en: Before starting the fine-tuning process, ensuring that the necessary components
    and resources are in place is crucial. This includes setting up a suitable training
    environment, such as Google Colab, which provides access to powerful GPUs and
    sufficient RAM to handle the computational demands of PVS models. Checking the
    availability and compatibility of NVIDIA GPUs is vital to ensuring optimal performance
    during training.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始微调过程之前，确保必要的组件和资源到位至关重要。这包括设置合适的训练环境，比如 Google Colab，它提供了强大的 GPU 和足够的 RAM
    来处理 PVS 模型的计算需求。检查 NVIDIA GPU 的可用性和兼容性对于确保训练期间的最佳性能至关重要。
- en: The dataset preparation phase is another essential aspect of fine-tuning a PVS
    model. The DLAS toolkit requires a specific repository structure and dependencies,
    which must be cloned and installed before proceeding. Additionally, pre-trained
    model checkpoints, such as the **discrete variational autoencoder** (**dVAE**),
    play a crucial role in learning a discrete latent representation of the speech
    data. Verifying the integrity of these checkpoints is necessary to accelerate
    the fine-tuning process and achieve better results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备阶段是微调 PVS 模型的另一个重要方面。DLAS 工具包要求具有特定仓库结构和依赖关系的数据集，必须在继续之前克隆和安装。此外，预训练模型检查点，如**离散变分自编码器**（**dVAE**），在学习语音数据的离散潜在表示中发挥关键作用。验证这些检查点的完整性对加速微调过程和获得更好结果至关重要。
- en: Selecting appropriate hyperparameters based on the dataset’s size is critical
    in fine-tuning a PVS model. The DLAS toolkit offers intelligent suggestions for
    hyperparameters, such as batch sizes, learning rate decay steps, and validation
    frequencies, all of which consider the specific characteristics of the dataset.
    Understanding how these hyperparameters are calculated and their impact on the
    training process is essential for achieving optimal results.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据集大小选择适当的超参数对于微调 PVS 模型至关重要。DLAS 工具包为超参数提供智能建议，如批量大小、学习率衰减步数和验证频率，所有这些建议都考虑了数据集的特定特征。理解这些超参数如何计算以及它们对训练过程的影响对于实现最佳结果至关重要。
- en: Customization is another critical aspect of fine-tuning a PVS model using the
    DLAS toolkit. Researchers and developers often have specific requirements and
    preferences for training settings, such as experiment names, dataset names, and
    turning certain features on or off. The DLAS toolkit provides flexibility in modifying
    these settings, allowing for tailored fine-tuning processes that align with specific
    needs and goals.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 定制化是使用 DLAS 工具包微调 PVS 模型的另一个关键方面。研究人员和开发人员通常对训练设置有特定的需求和偏好，例如实验名称、数据集名称以及开启或关闭某些功能。DLAS
    工具包提供了修改这些设置的灵活性，允许根据具体需求和目标进行定制化微调过程。
- en: The DLAS toolkit utilizes a YAML configuration file to ensure the fine-tuning
    process is configured according to the desired specifications. This file serves
    as a blueprint for the training process, specifying various parameters and settings.
    The toolkit applies the customized training settings to the YAML file using sophisticated
    `sed` commands, ensuring that the fine-tuning process is tailored to the specific
    requirements and enables the reproducibility of the experiments (`sed` stands
    for Stream Editor, a powerful command-line utility that’s used for parsing and
    transforming text using a simple, compact programming language).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: DLAS 工具包利用 YAML 配置文件来确保微调过程根据所需规范进行配置。该文件作为训练过程的蓝图，指定了各种参数和设置。工具包使用复杂的 `sed`
    命令将定制的训练设置应用于 YAML 文件，确保微调过程按特定要求进行，并实现实验的可重现性（`sed` 代表流编辑器，是一种强大的命令行实用程序，用于使用简单、紧凑的编程语言解析和转换文本）。
- en: With the configuration file ready, the training process can be initiated by
    running the `train.py` script provided by the DLAS toolkit. This script leverages
    the power of GPUs to efficiently fine-tune the PVS model, utilizing optimization
    algorithms and loss functions to guide the learning process. Monitoring the training
    progress and evaluating the model’s performance using appropriate metrics is crucial
    for ensuring the quality of the fine-tuned PVS model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件准备就绪后，可以通过运行 DLAS 工具包提供的 `train.py` 脚本启动训练过程。此脚本利用 GPU 的强大计算能力有效地微调 PVS
    模型，利用优化算法和损失函数指导学习过程。监控训练进度并使用适当的指标评估模型的性能对于确保微调 PVS 模型的质量至关重要。
- en: Finally, saving and exporting the fine-tuned PVS model is essential for future
    use and deployment. The DLAS toolkit provides convenient methods to store the
    trained model checkpoints and experiment files, ensuring data persistence and
    facilitating research collaboration. Proper management and organization of the
    fine-tuned models are critical for seamless integration into various applications,
    such as virtual assistants, audiobook narration, and personalized voice interfaces.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，保存和导出经过微调的 PVS 模型对于将来的使用和部署至关重要。DLAS 工具包提供了便捷的方法来存储训练模型的检查点和实验文件，确保数据的持久性，并促进研究协作。对微调模型的适当管理和组织对于无缝集成到各种应用程序中至关重要，如虚拟助手、有声读物朗读和个性化语音界面。
- en: Researchers and developers can create personalized voices that capture the nuances
    and characteristics by understanding the components, processes, and considerations
    involved in fine-tuning a PVS model using the DLAS toolkit. The ability to customize
    the training process, select appropriate hyperparameters, and leverage pre-trained
    checkpoints empowers users to achieve high-quality results and explore exciting
    possibilities in voice synthesis.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员和开发人员可以通过理解细化 PVS 模型时所涉及的组件、过程和注意事项，利用 DLAS 工具包创建个性化的语音，捕捉其中的细微差别和特点。定制训练过程、选择合适的超参数以及利用预训练的检查点，使用户能够实现高质量的结果，并探索语音合成中的令人兴奋的可能性。
- en: Please find and open the Colab notebook called `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请查找并打开名为 `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb` 的 Colab 笔记本 ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb))。
- en: This notebook fine-tunes a PVS model using the DLAS toolkit. It is based on
    the *TorToiSe fine-tuning with DLAS* project by James Betker ([https://github.com/152334H/DL-Art-School](https://github.com/152334H/DL-Art-School)).
    I cloned and modified the code to run on Google Colab and leveraged an NVIDIA
    GPU for training.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本使用 DLAS 工具包对 PVS 模型进行微调。它基于 *TorToiSe fine-tuning with DLAS* 项目，由 James
    Betker 开发 ([https://github.com/152334H/DL-Art-School](https://github.com/152334H/DL-Art-School))。我克隆并修改了代码，使其能在
    Google Colab 上运行，并利用 NVIDIA GPU 进行训练。
- en: 'Let’s walk through the steps in the `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    notebook:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步浏览 `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb` 笔记本中的步骤：
- en: '`nvidia-smi` command. It prints out the GPU’s information if it’s connected:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nvidia-smi` 命令。如果连接了 GPU，它会打印出 GPU 的信息：'
- en: '[PRE19]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`psutil` library. It prints a message if it’s using a high-RAM runtime:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`psutil` 库。如果它使用的是高内存的运行时，它会打印出一条消息：'
- en: '[PRE20]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Mounting Google Drive**: The following code mounts your Google Drive to save
    trained checkpoints and load the dataset:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挂载 Google Drive**：以下代码将您的 Google Drive 挂载，以便保存训练的检查点并加载数据集：'
- en: '[PRE21]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Installing the requirements**: It clones the DLAS repository, downloads pre-trained
    model checkpoints, and installs the required dependencies:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装依赖项**：它克隆了 DLAS 仓库，下载了预训练的模型检查点，并安装了所需的依赖项：'
- en: '[PRE22]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Figure 9.6 – Example of searching for the DJ format dataset in the output
    directory created in the previous notebook](img/Figure_9.6_B21020.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 在前一个笔记本创建的输出目录中搜索 DJ 格式数据集的示例](img/Figure_9.6_B21020.jpg)'
- en: Figure 9.6 – Example of searching for the DJ format dataset in the output directory
    created in the previous notebook
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 在前一个笔记本创建的输出目录中搜索 DJ 格式数据集的示例
- en: 'The following is the entire script that performs the hyperparameter calculation.
    An explanation of how it works is provided after the code listing:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行超参数计算的完整脚本。代码列出后将提供其工作原理的解释：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s break down the purpose and steps in this section:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这一部分的目的和步骤：
- en: 'The code imports the necessary libraries: `Path` from `pathlib` to work with
    files and directories, and `ceil`, a function in the built-in `math` module for
    rounding numbers up to the nearest integer.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码导入了必要的库：`pathlib` 中的 `Path` 用于处理文件和目录，`ceil` 是内建 `math` 模块中的一个函数，用于将数字向上舍入到最接近的整数。
- en: 'It defines default training and validation batch size values: `DEFAULT_TRAIN_BS
    = 64 and DEFAULT_VAL_BS =` `32`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它定义了默认的训练和验证批量大小值：`DEFAULT_TRAIN_BS = 64 和 DEFAULT_VAL_BS =` `32`。
- en: 'You are prompted to provide the paths for the training and validation datasets:
    `Dataset_Training_Path` and `ValidationDataset_Training_Path`.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要提供训练和验证数据集的路径：`Dataset_Training_Path` 和 `ValidationDataset_Training_Path`。
- en: The code checks whether the training and validation dataset paths are identical.
    If they are, it prints a warning message indicating that validation metrics will
    be useless and the training rate will be substantially slowed.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码检查训练和验证数据集路径是否相同。如果相同，它将打印出警告信息，指出验证指标将无用，并且训练速度将显著变慢。
- en: The code defines a helper function, `txt_file_lines`, that takes a file path
    as input and returns the number of lines in the file.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码定义了一个辅助函数 `txt_file_lines`，它以文件路径作为输入，并返回文件中的行数。
- en: It calculates the training and validation samples by calling `txt_file_lines`
    with the respective dataset paths.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过调用 `txt_file_lines` 并传入相应的数据集路径来计算训练和验证样本。
- en: 'The code prints warning messages if the dataset sizes are small: less than
    128 for training and less than 20 for validation.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据集的大小较小，代码会打印警告信息：训练样本小于 128，验证样本小于 20。
- en: It defines a helper function called `div_spillover` that takes the number of
    samples (`n`) and batch size (`bs`) as input and returns an adjusted batch size
    to minimize the number of leftover samples in each epoch.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它定义了一个辅助函数 `div_spillover`，该函数以样本数量（`n`）和批次大小（`bs`）作为输入，并返回调整后的批次大小，以最小化每个 epoch
    中剩余样本的数量。
- en: The code calculates the training batch size (`train_bs`) based on the number
    of training samples. If the number of training samples is smaller than `DEFAULT_TRAIN_BS`,
    it sets `train_bs` to the number of training samples and prints a warning message.
    Otherwise, it calls `div_spillover` with the number of training samples and `DEFAULT_TRAIN_BS`
    to calculate an adjusted batch size.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码根据训练样本的数量计算训练批次大小（`train_bs`）。如果训练样本数量小于 `DEFAULT_TRAIN_BS`，则将 `train_bs`
    设置为训练样本数量并打印警告信息。否则，它会调用 `div_spillover`，并使用训练样本数量和 `DEFAULT_TRAIN_BS` 来计算调整后的批次大小。
- en: Similarly, it calculates the validation batch size (`val_bs`) based on the number
    of validation samples. If the number of validation samples is smaller than `DEFAULT_VAL_BS`,
    it sets `val_bs` to the number of validation samples. Otherwise, it calls `div_spillover`
    with the number of validation samples and `DEFAULT_VAL_BS` to calculate an adjusted
    batch size.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，它根据验证样本的数量计算验证批次大小（`val_bs`）。如果验证样本数量小于 `DEFAULT_VAL_BS`，则将 `val_bs` 设置为验证样本数量。否则，它会调用
    `div_spillover`，并使用验证样本数量和 `DEFAULT_VAL_BS` 来计算调整后的批次大小。
- en: The code calculates the number of steps per epoch (`steps_per_epoch`) by dividing
    the number of training samples by the training batch size.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码通过将训练样本数量除以训练批次大小来计算每个 epoch 的步数（`steps_per_epoch`）。
- en: It defines the epochs at which learning rate decay should occur via `lr_decay_epochs
    = [20, 40,` `56, 72]`.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过 `lr_decay_epochs = [20, 40,` `56, 72]` 定义学习率衰减应该发生的 epochs。
- en: The code calculates the corresponding steps for learning rate decay (`lr_decay_steps`)
    by multiplying `steps_per_epoch` with each value in `lr_decay_epochs`.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码通过将 `steps_per_epoch` 与 `lr_decay_epochs` 中的每个值相乘来计算学习率衰减的相应步数（`lr_decay_steps`）。
- en: It calculates the frequency for printing training progress (`print_freq`) based
    on the number of steps per epoch, with a minimum of 20 and a maximum of 100.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它根据每个 epoch 的步数计算打印训练进度的频率（`print_freq`），最小为 20，最大为 100。
- en: The code sets the frequency for validation and saving checkpoints (`val_freq`
    and `save_checkpoint_freq`) to three times the `print_freq` value.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码将验证频率和保存检查点的频率（`val_freq` 和 `save_checkpoint_freq`）设置为 `print_freq` 值的三倍。
- en: 'Finally, it prints the calculated settings: `train_bs`, `val_bs`, `val_freq`,
    `lr_decay_steps`, `print_freq`, and `save_checkpoint_freq`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，代码打印计算出的设置：`train_bs`、`val_bs`、`val_freq`、`lr_decay_steps`、`print_freq` 和
    `save_checkpoint_freq`。
- en: 'From the creator of the DLAS trainer, the values of `print_freq`, `val_freq`,
    and `save_checkpoint_freq` should all be adjusted to the dataset’s size. The Python
    code states the recommended values: `val_freq == save_checkpoint_freq == print_freq*3`;
    `print_freq == min(epoch_steps,100)`. Again, these are recommendations; I encourage
    you to experiment with different ones and compare results for optimal hyperparameter
    settings.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 DLAS 训练器的创建者，`print_freq`、`val_freq` 和 `save_checkpoint_freq` 的值应该根据数据集的大小进行调整。Python
    代码给出了推荐值：`val_freq == save_checkpoint_freq == print_freq*3`；`print_freq == min(epoch_steps,100)`。再次强调，这些是建议值；我鼓励你尝试不同的值并比较结果，以找到最佳的超参数设置。
- en: By calculating these hyperparameters, the code aims to provide reasonable default
    values that can be used to train the PVS model. However, we can override these
    calculated values in the subsequent sections if needed.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算这些超参数，代码旨在提供可以用于训练 PVS 模型的合理默认值。然而，如果需要，我们可以在后续的部分覆盖这些计算出的值。
- en: '**Training settings**: This section allows us to customize the training settings
    according to their requirements and available resources. It provides flexibility
    in naming the experiment, specifying dataset names, turning certain features on
    or off, and overriding calculated settings. The code also includes notes and warnings
    to guide you in making appropriate choices based on the system’s storage and computational
    capabilities:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练设置**：这一部分允许我们根据需求和可用资源自定义训练设置。它提供了灵活性，可以命名实验、指定数据集名称、开启或关闭某些功能，并覆盖计算得出的设置。代码中还包括了注释和警告，指导你根据系统的存储和计算能力做出合适的选择：'
- en: '[PRE24]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s break down the purpose and steps in this section:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们来分解这一部分的目的和步骤：
- en: 'You can specify the following training settings:'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以指定以下训练设置：
- en: '`Experiment_Name`: A string to name the experiment.'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Experiment_Name`: 用于命名实验的字符串。'
- en: '`Dataset_Training_Name`: A string to name the training dataset.'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset_Training_Name`: 用于命名训练数据集的字符串。'
- en: '`ValidationDataset_Name`: A string to name the validation dataset.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidationDataset_Name`: 用于命名验证数据集的字符串。'
- en: '`SaveTrainingStates`: A Boolean to indicate whether to save training states.'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SaveTrainingStates`: 一个布尔值，用于指示是否保存训练状态。'
- en: '`Keep_Last_N_Checkpoints`: An integer slider to specify the number of checkpoints
    to keep. Setting it to 0 means keeping all saved models.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Keep_Last_N_Checkpoints`: 一个整数滑块，用于指定要保留的检查点数量。将其设置为0表示保留所有保存的模型。'
- en: 'The code provides notes and warnings:'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码提供了注释和警告：
- en: It mentions that keeping all saved models (setting `Keep_Last_N_Checkpoints`
    to 0) could potentially cause out-of-storage issues.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提到，如果保持所有保存的模型（将`Keep_Last_N_Checkpoints`设置为0），可能会导致存储不足的问题。
- en: 'Without training states, `Fp16`: A Boolean to turn 16-bit floating-point precision
    on or off.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '没有训练状态时，`Fp16`: 一个布尔值，用于开启或关闭16位浮点精度。'
- en: '`Use8bit`: A Boolean to turn 8-bit precision on or off.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Use8bit`: 一个布尔值，用于开启或关闭8位精度。'
- en: '`TrainingRate`: A string to specify the learning rate.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TrainingRate`: 用于指定学习率的字符串。'
- en: '`TortoiseCompat`: A Boolean to turn compatibility with the TorToiSe model on
    or off. Enabling it to introduce breaking changes to the training process and
    then disabling it is recommended to reproduce older models.'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TortoiseCompat`: 一个布尔值，用于开启或关闭与TorToiSe模型的兼容性。建议在训练过程中开启它以引入破坏性更改，然后禁用它以重现旧的模型。'
- en: 'Calculated settings override:'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算得出的设置覆盖：
- en: You can manually override the calculated settings from the previous cell by
    specifying values for `TrainBS`, `ValBS`, `ValFreq`, `LRDecaySteps`, `PrintFreq`,
    and `SaveCheckpointFreq`
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过指定`TrainBS`、`ValBS`、`ValFreq`、`LRDecaySteps`、`PrintFreq`和`SaveCheckpointFreq`的值来手动覆盖前一单元格中计算得出的设置。
- en: If left blank, the calculated defaults from the previous cell will be used
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为空，则使用前一单元格中计算得出的默认值。
- en: The code defines a `take` function to override the calculated settings. If the
    override is an empty string, it returns the original value; otherwise, it returns
    the overridden value.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码定义了一个`take`函数，用于覆盖计算得出的设置。如果覆盖值为空字符串，它将返回原始值；否则，返回覆盖后的值。
- en: 'The code assigns the overridden or default values to the corresponding variables:
    `train_bs`, `val_bs`, `val_freq`, `lr_decay_steps`, `print_freq`, and `save_checkpoint_freq`.'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码将覆盖后的值或默认值分配给相应的变量：`train_bs`、`val_bs`、`val_freq`、`lr_decay_steps`、`print_freq`和`save_checkpoint_freq`。
- en: Finally, the code prompts you to run the cell after editing the settings.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，代码提示你在编辑设置后运行该单元格。
- en: '`sed` commands:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sed`命令：'
- en: '[PRE25]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s break down the purpose and steps in this section:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们来分解这一部分的目的和步骤：
- en: The code changes the current directory to `/content/DL-Art-School` using the
    `%cd` magic command
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码使用`%cd`魔法命令将当前目录更改为`/content/DL-Art-School`。
- en: It downloads a fresh YAML configuration file named `EXAMPLE_gpt.yml` from the
    GitHub repository, `152334H/DL-Art-School`, using the `wget` command and saves
    it in the `experiments` directory
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用`wget`命令从GitHub仓库`152334H/DL-Art-School`下载一个名为`EXAMPLE_gpt.yml`的全新YAML配置文件，并将其保存在`experiments`目录中。
- en: 'The code then uses a series of `sed` commands to modify the values in the `EXAMPLE_gpt.yml`
    file based on the user-defined settings:'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码接着使用一系列`sed`命令，根据用户定义的设置修改`EXAMPLE_gpt.yml`文件中的值：
- en: It replaces the `batch_size` values for training and validation with the values
    stored in the `$train_bs` and `$val_bs` variables, respectively
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它用分别存储在`$train_bs`和`$val_bs`变量中的值替代训练和验证的`batch_size`值。
- en: It updates the `val_freq` value with the value stored in `$val_freq`
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将`val_freq`值更新为存储在`$val_freq`中的值。
- en: It replaces the learning rate decay steps with the values stored in `$gen_lr_steps`
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它用 `$gen_lr_steps` 中存储的值替换学习率衰减步骤。
- en: It updates the `print_freq` and `save_checkpoint_freq` values with the corresponding
    values stored in the `$print_freq` and `$``save_checkpoint_freq` variables
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它用 `$print_freq` 和 `$save_checkpoint_freq` 中存储的相应值更新 `print_freq` 和 `save_checkpoint_freq`
    的值。
- en: The code replaces placeholders in the YAML file with the user-defined values
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该代码将 YAML 文件中的占位符替换为用户定义的值。
- en: Finally, if `TrainingRate` is not equal to the default value of `1e-5`, the
    code uses `sed` to replace the placeholder of `CHANGEME:` with the user-defined
    `TrainingRate` value in the YAML file
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果 `TrainingRate` 不等于默认值 `1e-5`，代码会使用 `sed` 将 YAML 文件中的 `CHANGEME:` 占位符替换为用户定义的
    `TrainingRate` 值。
- en: The training process can be customized according to your requirements by modifying
    the YAML file with the user-specified values. This ensures the training process
    is configured based on your preferences and dataset specifications.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过修改 YAML 文件中的用户指定值，训练过程可以根据您的需求进行定制。这确保了训练过程是基于您的偏好和数据集规格配置的。
- en: '`train.py` script with the configured YAML file. Press the stop button for
    this cell when you are satisfied with the results and have seen the following
    output:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用配置好的 YAML 文件的 `train.py` 脚本。如果您对结果满意并看到以下输出，按下该单元格的停止按钮：
- en: '[PRE26]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Figure 9**.7* shows an example of the output and the `60_gtp.pht` checkpoint
    as it looks in Google Colab’s **Files** interface:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图 9.7* 显示了输出示例和 Google Colab **文件**界面中 `60_gtp.pht` 检查点的样子：'
- en: '![Figure 9.7 – Example of the checkpoint file at the 60-epoch mark](img/Figure_9.7_B21020.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 60 轮训练时检查点文件的示例](img/Figure_9.7_B21020.jpg)'
- en: Figure 9.7 – Example of the checkpoint file at the 60-epoch mark
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 60 轮训练时检查点文件的示例
- en: 'If your training run saves many models, you might exceed the storage limits
    on the Google Colab runtime. To prevent this, try to delete old checkpoints in
    `/content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/`
    via the file explorer panel as the training runs. Resuming training after a crash
    requires config editing, so try not to let that happen:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的训练过程保存了很多模型，可能会超过 Google Colab 运行时的存储限制。为防止这种情况，请尝试在训练过程中通过文件资源管理器面板删除 `/content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/`
    中的旧检查点。训练崩溃后恢复训练需要编辑配置文件，因此尽量避免这种情况发生：
- en: '[PRE27]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`experiments` folder to Google Drive for persistence:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `experiments` 文件夹上传到 Google Drive 以确保数据持久性：
- en: '[PRE28]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 9.8 – Example of the folder in Google Drive containing DLAS checkpoint
    files](img/Figure_9.8_B21020.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – Google Drive 中包含 DLAS 检查点文件的文件夹示例](img/Figure_9.8_B21020.jpg)'
- en: Figure 9.8 – Example of the folder in Google Drive containing DLAS checkpoint
    files
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – Google Drive 中包含 DLAS 检查点文件的文件夹示例
- en: 'You will find the model checkpoints in the `<Experiment_Name>/models` folder
    – that is, the files with the `.``pth` extension:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在 `<Experiment_Name>/models` 文件夹中找到模型检查点——也就是带有 `.pth` 扩展名的文件：
- en: '![Figure 9.9 – Example of the DLAS checkpoint file. In the next step, you will
    need a checkpoint file to synthesize a fine-tuned PVS model](img/Figure_9.9_B21020.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – DLAS 检查点文件示例。在接下来的步骤中，您将需要一个检查点文件来合成经过微调的 PVS 模型](img/Figure_9.9_B21020.jpg)'
- en: Figure 9.9 – Example of the DLAS checkpoint file. In the next step, you will
    need a checkpoint file to synthesize a fine-tuned PVS model
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – DLAS 检查点文件示例。在接下来的步骤中，您将需要一个检查点文件来合成经过微调的 PVS 模型。
- en: This concludes our overview of the PVS fine-tuning process using the DLAS toolkit.
    That `.pth` file is the fine-tuned PVS model we just created with DLAS. In the
    next step, we will use that file to synthesize the voice using TorToiSe-TTS-Fast.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对使用 DLAS 工具包微调 PVS 的概述。那个 `.pth` 文件就是我们刚刚用 DLAS 创建的微调 PVS 模型。在接下来的步骤中，我们将使用该文件通过
    TorToiSe-TTS-Fast 合成语音。
- en: Fine-tuning PVS models – Hyperparameters versus dataset size
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 微调 PVS 模型 – 超参数与数据集大小
- en: When fine-tuning PVS models, it’s essential to consider the relationship between
    hyperparameters and dataset size. The Google Colab training notebook automatically
    suggests appropriate hyperparameters based on the provided dataset size to ensure
    optimal performance and results.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调 PVS 模型时，考虑超参数与数据集大小之间的关系至关重要。Google Colab 训练笔记本会根据提供的数据集大小自动建议合适的超参数，以确保最佳性能和结果。
- en: One key aspect to remember is that the number of steps per epoch (`epoch_steps`)
    is calculated as `dataset_size // batch_size`. The trainer discards partial batches,
    so selecting a batch size that evenly divides the dataset is crucial.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一个关键点是每个epoch的步数（`epoch_steps`）是根据`dataset_size // batch_size`计算得出的。训练器会丢弃部分批次，因此选择一个能够整除数据集的批次大小非常重要。
- en: 'If your dataset is relatively small (50-500 samples), consider making the following
    adjustments to the hyperparameters:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集相对较小（50-500个样本），请考虑调整以下超参数：
- en: '**- Reduce the batch size**: To minimize discarded samples, choose a batch
    size that is a clean divisor of your dataset size.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**- 减小批次大小**：为了最小化丢弃样本，选择一个能够整除数据集大小的批次大小。'
- en: '`gen_lr_steps``gen_lr_steps` should be less than `epoch_steps * 10`. Experiments
    show that the loss may increase if there is no decay by epoch 20.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`gen_lr_steps`应小于`epoch_steps * 10`。实验表明，如果在第20个epoch没有衰减，损失可能会增加。'
- en: '`print_freq`, `val_freq`, and `save_checkpoint_freq` based on the dataset’s
    size. A recommended setting is `val_freq == save_checkpoint_freq == print_freq*3`;
    `print_freq ==` `min(epoch_steps,100)`.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集的大小调整`print_freq`、`val_freq`和`save_checkpoint_freq`。推荐的设置是`val_freq ==
    save_checkpoint_freq == print_freq*3`；`print_freq == min(epoch_steps,100)`。
- en: By carefully tuning these hyperparameters according to your dataset’s size,
    you can optimize the fine-tuning process and achieve better results with your
    PVS models.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过根据数据集大小精确调节这些超参数，您可以优化微调过程，并在PVS模型中获得更好的结果。
- en: Having successfully fine-tuned our PVS model using the DLAS toolkit, we are
    now ready to test our personalized voice by synthesizing speech that captures
    the essence of our target voice. In the next section, we will explore generating
    realistic and expressive speech using our fine-tuned model, bringing the synthesized
    voice to life and unlocking the potential for a wide range of exciting applications.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 成功使用DLAS工具包微调我们的PVS模型后，我们现在准备通过合成语音来测试我们的个性化语音，以捕捉目标语音的精髓。在下一部分中，我们将探索使用微调后的模型生成真实且富有表现力的语音，将合成的语音赋予生命，并开启各种令人兴奋的应用潜力。
- en: PVS step 3 – Synthesizing speech using a fine-tuned PVS model
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PVS步骤3 – 使用微调后的PVS模型合成语音
- en: Synthesizing speech using a fine-tuned PVS model is the culmination of the voice
    synthesizing process, where the personalized voice is brought to life. It is the
    stage where the fine-tuned model is tested, generating realistic and natural-sounding
    speech. The ability to synthesize speech using a fine-tuned PVS model opens up
    various applications, from creating virtual assistants and audiobook narration
    to personalized voice interfaces.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微调后的PVS模型合成语音是语音合成过程的高潮，此时个性化的语音被赋予生命。它是微调模型进行测试的阶段，生成真实且自然的语音。能够使用微调后的PVS模型合成语音，打开了各种应用场景，从创建虚拟助手、电子书旁白到个性化语音界面。
- en: Several key components and considerations come into play when embarking on the
    journey of speech synthesis. Firstly, it is essential to have a suitable computing
    environment that can handle the computational demands of speech synthesis. This
    often involves leveraging the power of GPUs, particularly NVIDIA GPUs, which can
    significantly accelerate the synthesis process. Checking the availability and
    compatibility of the GPU is crucial to ensure smooth and efficient speech generation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始语音合成之旅时，有几个关键组件和考虑因素需要注意。首先，必须具备适合的计算环境，能够处理语音合成的计算需求。这通常涉及到利用GPU的强大计算能力，特别是NVIDIA
    GPU，这可以显著加速合成过程。检查GPU的可用性和兼容性对于确保语音生成的顺利和高效至关重要。
- en: In addition to the hardware requirements, the speech synthesis process relies
    on a robust software stack. The TorToiSe-TTS-Fast project, a high-performance
    TTS system, emerges as a powerful tool. To utilize TorToiSe-TTS-Fast, it is necessary
    to clone the project repository and install the required dependencies, ensuring
    that all the necessary libraries and packages are available.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件要求外，语音合成过程还依赖于强大的软件堆栈。TorToiSe-TTS-Fast项目是一个高性能的TTS系统，成为了一个强大的工具。为了使用TorToiSe-TTS-Fast，必须克隆项目仓库并安装所需的依赖项，确保所有必要的库和包都已准备就绪。
- en: Loading the fine-tuned PVS model is a critical step in speech synthesis. During
    the fine-tuning phase, we stored the model in **Google Drive** as a checkpoint
    file or a serialized model object. The location and format of the fine-tuned model
    may vary depending on the specific locations you used during the fine-tuning process.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 加载经过微调的PVS模型是语音合成过程中的关键步骤。在微调阶段，我们将模型存储在**Google Drive**中作为检查点文件或序列化模型对象。微调模型的存储位置和格式可能会根据您在微调过程中使用的具体位置有所不同。
- en: With the fine-tuned PVS model loaded, the next step is to prepare the text input
    that will be synthesized into speech. Depending on the desired output, this text
    input can be a single sentence, a paragraph, or a script. It is essential to ensure
    that the text input is formatted correctly and free of any errors or inconsistencies
    that could impact the quality of the synthesized speech.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载完经过微调的PVS模型后，下一步是准备将要合成成语音的文本输入。根据所需的输出，文本输入可以是一个单句、一个段落或一份脚本。确保文本输入格式正确，并且没有任何可能影响合成语音质量的错误或不一致性是至关重要的。
- en: The speech synthesis process involves feeding the text input into the fine-tuned
    PVS model and generating the corresponding audio output. This is where the magic
    happens as the model applies its learned knowledge to convert the text into speech
    that mimics the voice we used as the foundation. The synthesis process may involve
    various techniques, such as neural vocoding, to generate high-quality audio waveforms.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 语音合成过程包括将文本输入馈送到经过微调的PVS模型，并生成相应的音频输出。在这个过程中，模型运用其学到的知识将文本转化为模仿我们作为基础使用的声音的语音。合成过程可能涉及多种技术，如神经声码器，以生成高质量的音频波形。
- en: Depending on the specific requirements and preferences, various parameters and
    settings can be adjusted during speech synthesis. These can include factors such
    as the speaking rate, pitch, volume, and emotional tone of the generated speech.
    Fine-tuning these parameters allows greater control over the final output and
    can help achieve the desired expressiveness and naturalness in the synthesized
    speech.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 根据具体要求和偏好，在语音合成过程中可以调整各种参数和设置。这些参数可能包括说话速率、音调、音量和生成语音的情感语气。微调这些参数可以更好地控制最终输出，并有助于实现合成语音的期望表现力和自然度。
- en: Once the speech synthesis process is complete, the generated audio can be saved
    to a file format suitable for playback, such as WAV or MP3\. This allows us to
    integrate synthesized speech into various applications and platforms easily. Consider
    the desired audio quality and compatibility when choosing the output file format
    – that is, `"ultra_fast"`, `"fast" (default`), `"standard"`, or `"high_quality"`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦语音合成过程完成，生成的音频可以保存为适合播放的文件格式，如WAV或MP3。这使我们能够轻松地将合成语音集成到各种应用和平台中。在选择输出文件格式时，需要考虑期望的音频质量和兼容性——即，`"ultra_fast"`、`"fast"`（默认）、`"standard"`或`"high_quality"`。
- en: Finally, evaluating the quality and naturalness of the synthesized speech is
    a crucial step in the process. This can involve subjective assessments, such as
    listening tests conducted by human evaluators, and objective metrics that measure
    various aspects of the generated speech, such as intelligibility, naturalness,
    and similarity to the target voice. Iterative refinement and fine-tuning based
    on the evaluation results can help improve the overall quality of the synthesized
    speech.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，评估合成语音的质量和自然性是过程中的关键步骤。这可以包括主观评估，例如由人工评估人员进行的听力测试，以及衡量生成语音各个方面的客观指标，如清晰度、自然性和与目标声音的相似度。根据评估结果进行迭代优化和微调，可以帮助提高合成语音的整体质量。
- en: Researchers and developers can unlock the potential of personalized speech generation
    by understanding the components, considerations, and steps involved in synthesizing
    speech using a fine-tuned PVS model. The ability to create realistic and expressive
    speech that captures the essence of a voice opens exciting possibilities in various
    domains, from entertainment and education to accessibility and beyond. With the
    right tools, techniques, and attention to detail, the speech synthesis process
    using fine-tuned PVS models can be a powerful and transformative technology in
    speech and audio processing.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员和开发者通过了解合成语音所涉及的组件、考虑事项和步骤，可以释放个性化语音生成的潜力。创造出能够捕捉声音本质的逼真且富有表现力的语音，为多个领域带来了激动人心的可能性，从娱乐和教育到无障碍技术及更多。通过正确的工具、技巧和对细节的关注，使用微调
    PVS 模型的语音合成过程可以成为语音和音频处理领域中一项强大而具有变革性的技术。
- en: Please find and open the Colab notebook called `LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb)).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请找到并打开名为 `LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb`
    的 Colab 笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb)）。
- en: 'This notebook demonstrates how to check the GPU and RAM, install the necessary
    libraries, load a fine-tuned PVS model, synthesize speech using the model, and
    play the generated audio. The code relies on the tortoise-TTS-Fast project to
    achieve high-performance voice synthesis. Let’s walk through the code in the `LOAIW_ch09_4_Synthetizing_speech_using_fine-tuned_PVS_models.ipynb`
    file, with explanations and code samples:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本演示了如何检查 GPU 和 RAM，安装必要的库，加载微调的 PVS 模型，使用该模型合成语音，并播放生成的音频。代码依赖于 tortoise-TTS-Fast
    项目来实现高性能的语音合成。接下来我们将逐步讲解 `LOAIW_ch09_4_Synthetizing_speech_using_fine-tuned_PVS_models.ipynb`
    文件中的代码，包括解释和代码示例：
- en: '`nvidia-smi` command. It prints the GPU information if connected. Otherwise,
    it indicates that no GPU is connected:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nvidia-smi` 命令。它会打印 GPU 信息，如果连接了 GPU，则显示相关信息；否则，会提示没有连接 GPU：'
- en: '[PRE29]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`psutil` library. It prints the amount of available RAM in GB and indicates
    if a high-RAM runtime is being used:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`psutil` 库。它会打印可用的 RAM 数量（单位为 GB），并指示是否使用高内存运行时：'
- en: '[PRE30]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`tortoise-tts-fast` repository from GitHub and installs the required dependencies
    using `pip3`:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 GitHub 获取 `tortoise-tts-fast` 仓库并使用 `pip3` 安装所需的依赖项：
- en: '[PRE31]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '!pip3 install transformers==4.29.2'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '!pip3 install transformers==4.29.2'
- en: '!pip3 uninstall voicefixer'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '!pip3 uninstall voicefixer'
- en: '!pip3 install voicefixer==0.1.2'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '!pip3 install voicefixer==0.1.2'
- en: '!pip3 install git+https://github.com/152334H/BigVGAN.git'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '!pip3 install git+https://github.com/152334H/BigVGAN.git'
- en: '[PRE32]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Mounting Google Drive**: We must mount Google Drive to load the fine-tuned
    PVS model we created earlier:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挂载 Google Drive**：我们必须挂载 Google Drive 以加载之前创建的微调 PVS 模型：'
- en: '[PRE33]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`gpt_path`) and the text to be synthesized (`text`):'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gpt_path`) 和需要合成的文本（`text`）：'
- en: '[PRE34]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Figure 9.10 – Example of a checkpoint file created by DLAS in Google Colab](img/Figure_9.10_B21020.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – 在 Google Colab 中通过 DLAS 创建的检查点文件示例](img/Figure_9.10_B21020.jpg)'
- en: Figure 9.10 – Example of a checkpoint file created by DLAS in Google Colab
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 在 Google Colab 中通过 DLAS 创建的检查点文件示例
- en: '`tortoise_tts.py` script with the specified arguments, including the `--preset`
    option for inference speed, the `--ar_checkpoint` option for the fine-tuned model
    path, the `-o` option for an output filename, and the text to be synthesized:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的参数运行 `tortoise_tts.py` 脚本，包括用于推理速度的 `--preset` 选项、用于微调模型路径的 `--ar_checkpoint`
    选项、用于输出文件名的 `-o` 选项，以及需要合成的文本：
- en: '[PRE35]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: import IPython
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import IPython
- en: IPython.display.Audio('/content/tortoise-tts-fast/scripts/results/random_00_00.wav')
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IPython.display.Audio('/content/tortoise-tts-fast/scripts/results/random_00_00.wav')
- en: '[PRE36]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Figure 9.11 – Example of audio files created with TorToiSe-TTS-Fast using
    a fine-tuned PVS model’s checkpoint .pth file](img/Figure_9.11_B21020.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 使用微调的 PVS 模型检查点 .pth 文件通过 TorToiSe-TTS-Fast 创建的音频文件示例](img/Figure_9.11_B21020.jpg)'
- en: Figure 9.11 – Example of audio files created with TorToiSe-TTS-Fast using a
    fine-tuned PVS model’s checkpoint .pth file
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 使用微调的 PVS 模型检查点 .pth 文件通过 TorToiSe-TTS-Fast 创建的音频文件示例
- en: I encourage you to examine and run all the notebooks in this chapter. They provide
    an end-to-end practical understanding of leveraging fine-tuned PVS models for
    high-quality, efficient TTS synthesis. This knowledge will enable you to create
    PVS applications and explore the potential of personalized speech generation using
    the TorToiSe-TTS-Fast project in conjunction with the techniques you’ve learned
    throughout this book.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你仔细检查并运行本章中的所有笔记本。它们提供了利用微调PVS模型进行高质量、高效TTS合成的端到端实践理解。这些知识将使你能够创建PVS应用，并通过本书中学到的技术与TorToiSe-TTS-Fast项目相结合，探索个性化语音生成的潜力。
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored PVS using OpenAI’s Whisper. We discovered how to
    harness its power to create customized voice models that capture the unique characteristics
    of a voice or entirely new voices, opening a wide range of exciting applications.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了使用OpenAI的Whisper进行PVS。我们发现了如何利用其强大功能，创建捕捉声音独特特征或完全新声音的定制语音模型，开辟了广泛的激动人心的应用场景。
- en: We began by exploring the fundamentals of TTS in voice synthesis, gaining insights
    into the role of neural networks, audio processing, and voice synthesis. We learned
    how to convert audio files into the LJSpeech format, a standardized dataset structure
    commonly used in TTS tasks, using the OZEN Toolkit and Whisper. This hands-on
    experience provided a solid foundation for the subsequent steps in the voice synthesizing
    process.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先探索了语音合成中的TTS基础，深入了解了神经网络、音频处理和语音合成的作用。我们学习了如何使用OZEN工具包和Whisper将音频文件转换为LJSpeech格式，这是一种在TTS任务中常用的标准化数据集结构。这一实践经验为语音合成过程中的后续步骤奠定了坚实的基础。
- en: Next, we delved into the DLAS toolkit, a robust framework for fine-tuning PVS
    models. We learned how to set up the training environment, prepare the dataset,
    and configure the model architecture. By leveraging Whisper’s accurate transcriptions,
    we aligned audio segments with their corresponding text, creating a dataset suitable
    for training personalized PVS models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入研究了DLAS工具包，这是一个强大的框架，用于微调PVS模型。我们学习了如何设置训练环境、准备数据集和配置模型架构。通过利用Whisper的精准转录，我们将音频片段与相应的文本对齐，创建了一个适合训练个性化PVS模型的数据集。
- en: Through practical examples and code snippets, we gained hands-on experience
    fine-tuning a pre-trained PVS model using our LJSpeech dataset. We discovered
    how to customize the training process, select appropriate hyperparameters, and
    evaluate the model’s performance. This experience gave us the knowledge and skills
    to create high-quality personalized PVS models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实践示例和代码片段，我们获得了微调预训练PVS模型的实战经验，使用了我们的LJSpeech数据集。我们发现了如何定制训练过程、选择合适的超参数并评估模型的性能。这一经验使我们具备了创建高质量个性化PVS模型的知识和技能。
- en: Finally, we tested our fine-tuned PVS model by synthesizing realistic and expressive
    speech. We learned how to generate natural-sounding speech by providing text input
    to the model, bringing our synthesized voice to life. The ability to create personalized
    speech opened a wide range of applications, from virtual assistants and audiobook
    narration to personalized voice interfaces.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过合成逼真且富有表现力的语音来测试微调后的PVS模型。我们学习了如何通过向模型提供文本输入来生成自然的语音，赋予我们合成的声音生命。创建个性化语音的能力打开了广泛的应用场景，从虚拟助手和有声书解说到个性化语音界面。
- en: As we conclude this chapter, we look ahead to [*Chapter 10*](B21020_10.xhtml#_idTextAnchor221),
    *Shaping the Future with Whisper*. In this final chapter, we will explore the
    evolving landscape of ASR and Whisper’s role in shaping its future. We will delve
    into upcoming trends, anticipated features, ethical considerations, and the general
    direction of voice technologies, including advanced voice TTS fine-tuning techniques.
    This forward-looking perspective will provide us with the knowledge and foresight
    to prepare for and adapt to the future of ASR and voice technology.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的结束，我们展望了[*第10章*](B21020_10.xhtml#_idTextAnchor221)，*与Whisper共同塑造未来*。在这一最终章中，我们将探索ASR领域的演变，以及Whisper在塑造未来中的作用。我们将深入研究未来趋势、预期功能、伦理考量以及语音技术的整体发展方向，包括先进的语音TTS微调技术。这一前瞻性的视角将为我们提供知识和远见，以为ASR和语音技术的未来做好准备和适应。
