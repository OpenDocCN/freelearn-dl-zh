- en: 8\. The Multi-Armed Bandit Problem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 多臂老虎机问题
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will introduce the popular Multi-Armed Bandit problem and
    some common algorithms used to solve it. We will learn how to implement some of
    these algorithms, such as Epsilon Greedy, Upper Confidence Bound, and Thompson
    Sampling, in Python via an interactive example. We will also learn about contextual
    bandits as an extension of the general Multi-Armed Bandit problem. By the end
    of this chapter, you will have a deep understanding of the general Multi-Armed
    Bandit problem and the skill to apply some common ways to solve it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍流行的多臂老虎机问题及其一些常用的解决算法。我们将通过一个互动示例学习如何用Python实现这些算法，如Epsilon贪心算法、上置信界算法和汤普森采样。我们还将了解作为一般多臂老虎机问题扩展的上下文老虎机问题。通过本章的学习，你将深入理解一般的多臂老虎机问题，并具备应用一些常见方法来解决该问题的技能。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we discussed the technique of temporal difference learning,
    a popular model-free reinforcement learning algorithm that predicts a quantity
    via the future values of a signal. In this chapter, we will focus on another common
    topic, not only in reinforcement learning but also in artificial intelligence
    and probability theory – the **Multi-Armed Bandit** (**MAB**) problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了时间差分学习方法，这是一种流行的无模型强化学习算法，它通过信号的未来值来预测一个量。在本章中，我们将关注另一个常见话题，这不仅在强化学习中应用广泛，在人工智能和概率论中也具有重要地位——**多臂老虎机**（**MAB**）问题。
- en: Framed as a sequential decision-making problem to maximize the reward while
    playing at the slot machines in a casino, the MAB problem is highly applicable
    for any situation where sequential learning under uncertainty is needed, such
    as A/B testing or designing recommender systems. In this chapter, we will be introduced
    to the formalization of the problem, learn about the different common algorithms
    as solutions to the problem (namely Epsilon Greedy, Upper Confidence Bound, and
    Thompson Sampling), and finally implement them in Python.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个顺序决策问题，旨在通过在赌场老虎机上游戏来最大化奖励，多臂老虎机问题广泛适用于任何需要在不确定性下进行顺序学习的情况，如A/B测试或设计推荐系统。本章将介绍该问题的形式化方法，了解几种常见的解决算法（即Epsilon贪心算法、上置信界算法和汤普森采样），并最终在Python中实现它们。
- en: Overall, this chapter will offer you a deep understanding of the MAB problem
    in different contexts of sequential decision-making and offer you the opportunity
    to apply that knowledge to solve a variation of the problem called the queueing
    bandit.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，本章将让你深入理解多臂老虎机问题在不同的顺序决策场景中的应用，并为你提供将这一知识应用于解决一种变体问题——排队老虎机问题的机会。
- en: First, let's begin by discussing the background and the theoretical formulation
    of the problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从讨论问题的背景和理论表述开始。
- en: Formulation of the MAB Problem
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机问题的表述
- en: In its most simple form, the MAB problem consists of multiple slot machines
    (casino gambling machines), each of which can return a stochastic reward to the
    player each time it is played (specifically, when its arm is pulled). The player,
    who would like to maximize their total reward at the end of a fixed number of
    rounds, does not know the probability distribution or the average reward that
    they will obtain from each slot machine. The problem, therefore, boils down to
    the design of a learning strategy where the player needs to explore what possible
    reward values each slot machine can return and from there, quickly identify the
    one that is most likely to return the greatest expected reward.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单形式的多臂老虎机（MAB）问题由多个老虎机（赌场赌博机）组成，每次玩家玩一个老虎机时（具体来说，当它的臂被拉动时），老虎机会随机地给玩家一个奖励。玩家希望在固定轮次结束时最大化自己的总奖励，但他们不知道每个老虎机的概率分布或平均奖励。因此，这个问题的核心就是设计一个学习策略，在这个策略中，玩家需要探索每个老虎机可能返回的奖励值，并从中快速识别出最有可能返回最大期望奖励的那个老虎机。
- en: In this section, we will briefly explore the background of the problem and establish
    the notation and terminology that we will be using throughout this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要探讨问题的背景，并建立本章中将使用的符号和术语。
- en: Applications of the MAB Problem
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多臂老虎机问题的应用
- en: The slot machines we mentioned earlier are just a simplification of our settings.
    In the general case of a MAB problem, we are faced with a set of multiple decisions
    that we can choose from at each step, and we need to sufficiently explore each
    of the decisions so that we become more informed about the environment we are
    in, all while making sure we converge on the optimal decision soon so that our
    total reward is maximized at the end of the process. This is the classic trade-off
    between exploration and exploitation we are faced with in common reinforcement
    learning problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的老虎机只是我们设定的一个简化版。在一般情况下的MAB问题中，我们在每一步面临一组可选择的多个决策，并且我们需要充分探索每个决策，以便更加了解我们所处的环境，同时确保尽早收敛到最优决策，以使得最终的总奖励最大化。这就是我们在常见的强化学习问题中面临的经典探索与利用的权衡。
- en: 'Popular applications of the MAB problem include recommender systems, clinical
    trials, network routing, and as we will see at the end of this chapter, queueing
    theory. Each of these applications contains the quintessential characteristics
    that define the MAB problem: at each step of a sequential process, a decision-maker
    needs to select from a predetermined set of possible choices and, depending on
    the past observations, the decision-maker needs to find a balance between exploring
    different choices and exploiting the one that they believe is the most beneficial.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: MAB问题的常见应用包括推荐系统、临床试验、网络路由，以及如我们将在本章末尾看到的排队理论。这些应用都包含了定义MAB问题的典型特征：在每一个顺序决策过程中，决策者需要从预定的可选项中进行选择，并且根据过去的观察，决策者需要在探索不同选择和利用认为最有利的选择之间找到平衡。
- en: As an example, one of the goals of recommender systems is to display the products
    that their customers are most likely to consider/buy. When a new customer logs
    into a system such as a shopping website or an online streaming service, the recommender
    system can observe the customer's past behaviors and choices and make a decision
    regarding what kind of product advertisement should be shown to the customer.
    It does this so that the probability that they click on the advertisement is maximized.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，推荐系统的目标之一是展示客户最可能考虑或购买的产品。当一个新客户登录像购物网站或在线流媒体服务这样的系统时，推荐系统可以观察客户的过去行为和选择，并根据这些信息决定应向客户展示什么样的产品广告。它这样做是为了最大化客户点击广告的概率。
- en: As another example, which we will see in more detail later on, in a queueing
    system consisting of multiple customer classes, each is characterized by an unknown
    service rate. The queue coordinator needs to figure out how to best order these
    customers so that a given objective, such as the cumulative waiting time of the
    whole queue, is optimized.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子，稍后我们将更详细地讨论，是在一个由多个客户类别组成的排队系统中，每个类别都具有一个未知的服务速率。排队协调员需要弄清楚如何最好地安排这些客户，以优化某个目标，例如整个队列的累计等待时间。
- en: Overall, MAB is an increasingly ubiquitous problem in artificial intelligence
    and, specifically, reinforcement learning that has many interesting applications.
    In the next section, we will officially formalize the problem and the terminologies
    that we will be using throughout this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，MAB问题是人工智能领域，尤其是强化学习中的一个日益普遍的问题，它有许多有趣的应用。在接下来的章节中，我们将正式化该问题并介绍本章将使用的术语。
- en: Background and Terminology
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景与术语
- en: 'A MAB problem is characterized by the following elements:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MAB问题的特征如下：
- en: A set of "K" actions to choose from. Each of these actions is called an arm,
    following the colloquial terminology with respect to slot machines.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组可以选择的“K”个动作。每个动作称为“臂”，这是根据传统的老虎机术语来命名的。
- en: A central decision-maker who needs to choose between this set of actions at
    each step of the process. We call the act of choosing an action pulling an arm,
    and the decision-maker the player.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位中央决策者需要在每一步从这组动作中做出选择。我们将选择动作的行为称为“拉臂”，而决策者则称为“玩家”。
- en: When one of the "K" available arms is pulled, the player receives a stochastic,
    or random, reward drawn from a probability distribution that is specific to that
    arm. It is important that the rewards are randomly chosen from their respective
    distributions; if they were otherwise fixed, the player could identify the arm
    that will return the highest reward quickly and the problem would become less
    interesting.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当拉动其中一个“K”个可用臂时，玩家会从该臂特定的概率分布中随机抽取一个随机奖励。重要的是，奖励是从各自的分布中随机选择的；如果奖励是固定的，玩家就能快速识别出能够提供最高回报的臂，这样问题就不再有趣。
- en: The goal of the player is, again, to choose one of the "K" arms during each
    step of a running process so that their reward is maximized at the end. The number
    of steps in the process is called the horizon, which may or may not be known by
    the player beforehand.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家目标仍然是，在每个步骤中从“K”个臂中选择一个，以便在最后最大化奖励。过程中的步骤数称为视野，玩家可能知道也可能不知道。
- en: In most cases, each arm can be pulled infinitely. When the player is certain
    that a specific arm is the optimal one, they can keep choosing that arm for the
    rest of the process without deviating. However, in various settings, the number
    of times an arm can be pulled is finite, thus increasing the complexity of the
    problem.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，每个臂可以被拉动无限次。当玩家确定某个特定臂是最优的时，他们可以继续选择该臂进行后续操作而不偏离。然而，在不同的环境下，某个臂被拉动的次数是有限的，从而增加了问题的复杂性。
- en: The following diagram visualizes an iterative step in the environment that we
    are working with, where we have four arms whose success rates are estimated as
    70%, 30%, 55%, and 40%.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们所使用环境中的一个迭代步骤，其中有四个臂，其成功率分别估计为70%、30%、55%和40%。
- en: '![Figure 8.1: A typical MAB iteration'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1：典型的MAB迭代'
- en: '](img/B16182_08_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_01.jpg)'
- en: 'Figure 8.1: A typical MAB iteration'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：典型的MAB迭代
- en: 'At each step, we need to make a decision about which arm we should choose to
    pull next:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步中，我们需要决定应该选择哪个臂来拉动：
- en: 'Opposite to the language of reward and the corresponding maximization objective,
    a MAB problem can also be framed in the context of a cost minimization objective.
    The queueing example can, once again, be used: the cumulative waiting time of
    the whole queue is a negative quantity, or in other words, the cost that needs
    to be minimized.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与奖励的语言和相应的最大化目标相对，一个MAB问题也可以从成本最小化目标的角度来框定。排队的例子可以再次使用：整个队列的累计等待时间是一个负值，换句话说，这就是需要最小化的成本。
- en: It is common to compare the performance of a strategy to the optimal strategy
    or the genie strategy, which knows ahead of time what the optimal arm to pull
    is and always pulls that arm at every step of the process. Of course, it is highly
    improbable that any real learning strategy can simulate the performance of the
    genie strategy, but it does provide us with a fixed metric to compare our approaches
    against. The difference in performance of a given strategy and the genie strategy
    is known as the regret, which is to be minimized.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常将一个策略的表现与最优策略或天才策略进行比较，天才策略事先知道哪个臂是最优的，并在每一步都拉动那个臂。当然，任何现实中的学习策略都不太可能模拟天才策略的表现，但它为我们提供了一个固定的度量标准来与我们的策略进行比较。给定策略与天才策略的表现差异称为遗憾，目标是最小化这个遗憾。
- en: The central question of a MAB problem is how to identify the arm with the greatest
    *expected* reward (or lowest *expected* cost) with minimal exploration (pulling
    the suboptimal arms). This is because the more the player explores, the less frequent
    their choice of the optimal arm becomes, and the more their final reward decreases.
    However, if the player does not sufficiently explore all the arms, chances are
    they will misidentify the optimal arm and their total reward will be negatively
    affected in the end.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MAB问题的核心问题是如何以最小的探索（拉动次优臂）识别出具有最大*期望*奖励（或最小*期望*成本）的臂。这是因为玩家的探索越多，选择最优臂的频率就越低，最终的奖励也会随之减少。然而，如果玩家没有充分探索所有的臂，他们可能会错误地识别最优臂，最终导致总奖励受到负面影响。
- en: These situations arise when the stochastic rewards of the true optimal arm appear
    to be lower than those from other arms in the first few examples (due to randomness),
    causing the player to misidentify the optimal arm. Depending on the actual reward
    distribution that each arm has, this event can be quite likely to happen.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当真实最优臂的随机奖励在前几次实验中看起来低于其他臂的奖励（由于随机性）时，可能会导致玩家错误地识别最优臂。根据每个臂的实际奖励分布，这种情况发生的可能性较高。
- en: So, that is the general problem we set out to solve in this chapter. We now
    need to briefly consider the concept of probability distributions of reward in
    the context of MAB in order to fully understand the problem we are trying to solve.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是我们在本章中要解决的总体问题。我们现在需要简要考虑多臂赌博机背景下的奖励概率分布的概念，以便充分理解我们正在尝试解决的问题。
- en: MAB Reward Distributions
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多臂赌博机奖励分布
- en: 'In the traditional MAB problem, the reward from each arm of the bandit is associated
    with a Bernoulli distribution. Each Bernoulli distribution is, in turn, parameterized
    by a non-negative number, *p*, that is, at most, 1\. When a number is drawn from
    a Bernoulli distribution, it can take on two possible values: 1, which has a probability
    of *p*, and 0, which consequently has a probability of *1 - p*. A high value of
    *p* therefore corresponds to a good arm for the player to pull. This is because
    the player is more likely to receive 1 as their reward. Of course, a high value
    of *p* does not guarantee that the reward obtained from a specific arm is always
    1, and chances are, out of many pulls from even the arm with the highest value
    of *p* (in other words, the optimal arm), some of the rewards will be 0.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的多臂赌博机问题中，每个臂的奖励都与伯努利分布相关联。每个伯努利分布又由一个非负数*p*来参数化，*p*的最大值为1。当从伯努利分布中抽取一个数时，它可以取两个可能的值：1，其概率为*p*，以及0，其概率为*1
    - p*。因此，*p*的较高值对应玩家应当拉动的较好臂。这是因为玩家更有可能收到1作为奖励。当然，*p*的高值并不保证从特定臂获得的奖励始终为1，事实上，即使是从*p*值最高的臂（也就是最优臂）拉取，某些奖励也可能为0。
- en: 'The following diagram is an example of a Bernoulli bandit setting:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是伯努利赌博机设置的一个示例：
- en: '![Figure 8.2: Sample Bernoulli MAB problem'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2：样本伯努利多臂赌博机问题'
- en: '](img/B16182_08_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_02.jpg)'
- en: 'Figure 8.2: Sample Bernoulli MAB problem'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：样本伯努利多臂赌博机问题
- en: 'Each arm has its own reward distribution: the first has a true probability
    of 75% of returning 1 and 25% of returning 0, the second has 25% for 1 and 75%
    for 0, and so on. Note that the rates that we empirically observe do not always
    match the true rates.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个臂都有自己独特的奖励分布：第一个臂返回1的概率为75%，返回0的概率为25%；第二个臂返回1的概率为25%，返回0的概率为75%，依此类推。请注意，我们经验上观察到的比例并不总是与真实比例匹配。
- en: 'From here, we can generalize a MAB problem where the reward of an arm follows
    any probability distribution. While the inner workings of these distributions
    are different, the goal of a MAB algorithm remains constant: identifying the arm
    associated with the distribution with the highest expectation in order to maximize
    the final cumulative reward.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以将多臂赌博机问题推广到奖励遵循任何概率分布的情况。虽然这些分布的内部机制不同，但多臂赌博机算法的目标保持不变：识别与期望值最高的分布相关联的臂，以最大化最终的累计奖励。
- en: Throughout this chapter, we will be working with Bernoulli-distributed rewards,
    as they are among the most natural and intuitive reward distributions and will
    provide us with the context in which we can study various MAB algorithms. Finally,
    before we consider the different algorithms that will be covered in this chapter,
    let's take a moment to familiarize ourselves with the programming interface that
    we will be working with.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用伯努利分布的奖励，因为它们是最自然和直观的奖励分布之一，并且为我们提供了一个可以研究各种多臂赌博机算法的背景。最后，在我们考虑本章将涉及的不同算法之前，先花点时间熟悉我们将要使用的编程接口。
- en: The Python Interface
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python接口
- en: 'The Python environment that will help facilitate our discussions of MAB algorithms
    is included in the `utils.py` file of this chapter''s code repository on GitHub:
    https://packt.live/3cWiZ8j.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助我们讨论多臂赌博机算法的Python环境包含在本章代码库的`utils.py`文件中，代码库地址为：https://packt.live/3cWiZ8j。
- en: 'From this file, we can import the `Bandit` class into a separate script or
    a Jupyter script. This class is the interface we will use to create, interact,
    and solve various MAB problems. If the code we are working with is in the same
    directory as this file, we can import the `Bandit` class by simply using the following
    code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个文件中将`Bandit`类导入到一个独立的脚本或 Jupyter 脚本中。这个类是我们用来创建、互动和解决各种 MAB 问题的接口。如果我们正在使用的代码与该文件位于同一目录下，我们可以通过以下代码简单导入`Bandit`类：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we can declare an MAB problem as an instance of a `Bandit` object:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将 MAB 问题声明为`Bandit`对象的实例：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since we are not passing any arguments to this declaration, this `Bandit` instance
    takes on its default value: an MAB problem with two Bernoulli arms with probabilities
    of 0.7 and 0.3 (although our algorithms technically cannot know this).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有向此声明传递任何参数，因此此`Bandit`实例采用其默认值：一个具有 0.7 和 0.3 概率的两个伯努利臂的 MAB 问题（尽管我们的算法在技术上并不知道这一点）。
- en: The most integral method of the `Bandit` class that we need to be aware of is
    `pull()`. This method takes in an integer as an argument, denoting the index of
    the arm we would like to pull at a given step, and returns a number representing
    the stochastic reward drawn from the distribution associated with that same arm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要注意的`Bandit`类中最核心的方法是`pull()`。该方法接受一个整数作为参数，表示我们希望在给定步骤拉取的臂的索引，并返回一个数字，表示从与该臂相关的分布中抽取的随机奖励。
- en: 'For example, in the following code snippet, we call this `pull()` method with
    the `0` parameter to pull the first arm and record the returned reward, like so:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在以下代码片段中，我们调用`pull()`方法，并传递`0`参数来拉取第一个臂并记录返回的奖励，代码如下：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, you might see the number `0` or the number `1` printed out, which denotes
    the reward that you receive by pulling arm 0\. Say we''d like to pull arm 1 once;
    the same API can be used:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可能会看到数字`0`或数字`1`被打印出来，这表示通过拉取臂 0 获得的奖励。假设我们想要拉取臂 1 一次，可以使用相同的 API：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Again, the output might be `0` or `1` since we are drawing from a Bernoulli
    distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由于我们从伯努利分布中抽取，输出可能是`0`或`1`。
- en: 'Say we''d like to inspect what the reward distribution of each arm might look
    like, or more specifically, which out of the two arms is the one more likely to
    return more reward. To do this, we pull from each arm 10 times and record the
    returned reward at each step:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要检查每个臂的奖励分布是什么样的，或者更具体地说，想知道这两个臂中哪个更有可能返回更多的奖励。为此，我们从每个臂拉取 10 次并记录每一步返回的奖励：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code produces the following output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会产生以下输出：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Again, due to randomness, you might get a different output. Considering the
    preceding output, we can see that arm 0 returned a positive reward 6 out of 10
    pulls, while arm 1 returned a positive reward 5 times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机性的原因，您可能会得到不同的输出。根据前述的输出，我们可以看到，臂 0 在 10 次拉取中返回了 6 次正奖励，而臂 1 返回了 5 次正奖励。
- en: 'We''d also like to plot the cumulative reward throughout the process of 20
    steps from each arm. Here, we can use the `np.cumsum()` function from the NumPy
    library to compute that quantity and plot it using the Matplotlib library, like
    so:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望绘制每个臂在 20 步过程中累计奖励的变化图。在这里，我们可以使用 NumPy 库中的`np.cumsum()`函数来计算这一量，并通过 Matplotlib
    库进行绘制，代码如下：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following graph will then be produced:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接着会生成如下图表：
- en: '![Figure 8.3: Sample graph of cumulative reward'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3：累计奖励的示例图'
- en: '](img/B16182_08_03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_03.jpg)'
- en: 'Figure 8.3: Sample graph of cumulative reward'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：累计奖励的示例图
- en: This graph allows us to visually inspect how fast the cumulative reward we receive
    from each arm grew throughout the process of 10 pulls. We can also see that the
    cumulative reward from arm 0 is always greater than that from arm 1, indicating
    that out of the two arms, arm 0 is the optimal one. This is consistent with the
    fact that arm 0 was initialized with a Bernoulli reward distribution where *p
    = 0.7*, and arm 1 has one where *p = 0.3*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表允许我们直观地检查从每个臂上获得的累计奖励在 10 次拉取过程中增长的速度。我们还可以看到，臂 0 的累计奖励始终大于臂 1 的累计奖励，这表明在这两个臂中，臂
    0 是最优的。这与臂 0 被初始化为具有*p = 0.7*的伯努利奖励分布的事实一致，而臂 1 的奖励分布则是*p = 0.3*。
- en: 'The `pull()` method is the lower-level API that facilitates processing at each
    step. However, when we design various MAB algorithms, we will be allowing the
    algorithms to interact with the bandit problem automatically, on their own, without
    any human interference. This leads us to the second method from the `Bandit` class,
    which we will be using to test out our algorithms: `automate()`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`pull()`方法是更底层的API，用于在每一步中促进处理。然而，在设计各种MAB算法时，我们将允许这些算法自动与赌博机问题进行交互，而无需人工干预。这就引出了`Bandit`类的第二个方法，我们将用它来测试我们的算法：`automate()`。'
- en: 'As we will see in the next section, this method takes in an algorithm object
    implementation and streamlines the testing process for us. Specifically, this
    method will call the algorithm object, record its decisions, and return the corresponding
    rewards in an automatic manner. Aside from the algorithm object, it also takes
    in two other optimal parameters: `n_rounds`, which is used to specify the number
    of times we can interact with the bandit, and `visualize_regret`, which is a Boolean
    flag indicating whether we would like to plot the regret of the algorithm we are
    considering against the genie algorithm.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一节中看到的，这个方法接收一个算法对象的实现，并简化了我们的测试过程。具体来说，这个方法将调用算法对象，记录其决策，并以自动化的方式返回相应的奖励。除了算法对象外，它还接收其他两个优化参数：`n_rounds`，用于指定我们与赌博机互动的次数，以及`visualize_regret`，这是一个布尔标志，指示我们是否希望绘制所考虑算法与精灵算法之间的后悔值。
- en: This whole process is called an experiment, where an algorithm that does not
    have any prior knowledge is tested against an MAB problem. To fully analyze the
    performance of a given algorithm, we need to put that algorithm through many experiments
    and study its performance in the general case across all experiments. This is
    because a specific initialization of the MAB problem might favor one algorithm
    over another; by comparing the performance of different algorithms across multiple
    experiments, our resulting insight regarding which algorithms are better will
    be more robust.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个整个过程被称为实验，其中一个没有任何先验知识的算法会被测试用于解决多臂赌博机（MAB）问题。为了全面分析给定算法的性能，我们需要通过多个实验来验证该算法，并研究它在所有实验中的总体表现。这是因为MAB问题的特定初始化可能会使某个算法优于其他算法；通过在多个实验中比较不同算法的表现，我们对哪种算法更优的最终见解将更加稳健。
- en: This is where the `repeat()` method of the `Bandit` class comes in. This method
    takes in an algorithm class' implementation (as opposed to an object implementation)
    and repeatedly calls the `automate()` method described previously on the instances
    of the algorithm class. Doing this facilitates multiple experiments on the algorithm
    we are considering and, again, will give us a more holistic view of its performance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，`Bandit`类的`repeat()`方法派上了用场。该方法接收一个算法类的实现（与对象实现相对），并重复调用之前描述的`automate()`方法来操作算法类的实例。通过这样做，可以对我们考虑的算法进行多次实验，并且能给我们提供该算法表现的更全面视角。
- en: 'In order to interact with the methods of this `Bandit` class, we will be implementing
    our MAB algorithms as Python classes. The `pull()` method, and therefore both
    the `automate()` and `repeat()` methods as well, require these algorithm class
    implementations to have two distinct methods: `decide()`, which should return
    the index of the arm that the algorithm thinks should be pulled next at any given
    time, and `update()`, which takes in an arm index and a new reward that was just
    returned from that arm of the bandit. We will be keeping these two methods in
    mind while writing our algorithms later in this chapter.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与`Bandit`类的方法进行交互，我们将把MAB算法实现为Python类。`pull()`方法，因此包括`automate()`和`repeat()`方法，要求这些算法类实现有两个独立的方法：`decide()`，该方法应该返回算法认为应该在任意时刻拉动的臂的索引；以及`update()`，该方法接收一个臂的索引和刚从该臂获得的新奖励。在本章后续编写算法时，我们将牢记这两个方法。
- en: 'As a final note about the bandit API, due to randomness, it is entirely possible
    that, in your own implementation, you will obtain different results from the results
    shown in this chapter. For better reproducibility, we have fixed the random seed
    number of all the scripts in this chapter to `0` so that it will be possible for
    you to run the code and obtain the same results, which can be done by taking any
    of the Jupyter Notebooks from this book''s GitHub repository and running the code
    using the option shown in the following screenshot:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 bandit API 的最后说明，由于随机性，在你自己的实现中，完全可能得到与本章中显示的结果不同的结果。为了更好的可复现性，我们已经将本章所有脚本的随机种子固定为
    `0`，这样你就可以通过从本书的 GitHub 仓库获取任何 Jupyter Notebook，并使用以下截图中显示的选项运行代码，从而获得相同的结果：
- en: '![Figure 8.4: Reproducing results with Jupyter Notebooks'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4：使用 Jupyter Notebooks 重现结果'
- en: '](img/B16182_08_04.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_04.jpg)'
- en: 'Figure 8.4: Reproducing results with Jupyter Notebooks'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：使用 Jupyter Notebooks 重现结果
- en: With that said, even with randomness, we will see that some algorithms are better
    than others at solving the MAB problem. This is also why we will be analyzing
    the performance of our algorithms via many repeated experiments, ensuring that
    any performance superiority is robust to randomness.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，即使有随机性，我们也会看到一些算法在解决 MAB 问题时比其他算法表现得更好。这也是我们将通过许多重复实验分析算法性能的原因，确保任何性能上的优势在面对随机性时是稳健的。
- en: And that is all the background information we need in order to understand the
    MAB problem. We are now ready to begin discussing the approaches that are commonly
    employed on this problem, starting with the Greedy algorithm.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们理解 MAB 问题所需的所有背景信息。现在，我们准备开始讨论解决该问题时常用的方法，首先从贪婪算法开始。
- en: The Greedy Algorithm
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贪婪算法
- en: Recall the brief interaction we had with the `Bandit` instance in the previous
    section, in which we pulled the first arm 10 times and the second 10 times. This
    might not be the best strategy to maximize our cumulative reward as we are spending
    10 rounds pulling a sub-optimal arm, whichever it is among the two. The naïve
    approach is, therefore, to simply pull both (or all) of the arms once and greedily
    commit to the one that returns a positive reward.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在上一节中与`Bandit`实例的简短互动，我们拉取了第一个臂 10 次，第二个臂 10 次。这可能不是最大化累积奖励的最佳策略，因为我们在花费
    10 轮拉取一个次优臂时，无论它是哪一个，都不是最优选择。因此，幼稚的做法是简单地将每个臂（或所有臂）拉一次，然后贪婪地选择返回正奖励的臂。
- en: A generalization of this strategy is the Greedy algorithm, in which we maintain
    the list of reward averages across all available arms and at each step, we choose
    to pull the arm with the highest average. While the intuition is simple, it follows
    the probabilistic rationale that after a large number of samples, the empirical
    mean (the average of the samples) is a good approximation of the actual expectation
    of the distribution. If the reward average of an arm is larger than that of any
    other arm, the probability that that given arm is indeed the optimal arm should
    not be low.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略的一般化形式是贪婪算法，在该算法中，我们会保持一个奖励均值列表，包含所有可用臂的奖励均值，并在每一步选择拉取具有最高均值的臂。虽然直觉上很简单，但它遵循了一个概率推理：在经过大量样本后，经验均值（样本的平均值）是实际分布期望的一个良好近似。如果一个臂的奖励均值比其他任何臂都大，那么该臂实际上是最优臂的概率应该不低。
- en: Implementing the Greedy Algorithm
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现贪婪算法
- en: 'Now, let''s try implementing this algorithm. As explained in the previous section,
    we will be writing our MAB algorithms as Python classes to interact with the bandit
    API that is provided in this book. Here, we will require this algorithm class
    to have two attributes: the number of available arms to pull and the list of rewards
    that the algorithm has observed from each arm:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试实现这个算法。如前一节所述，我们将把 MAB 算法写成 Python 类，以与本书提供的 bandit API 进行交互。在这里，我们要求该算法类具有两个属性：可拉取的臂的数量和算法从每个臂观察到的奖励列表：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, `reward_history` is a list of lists, where each sub-list contains the
    past rewards returned from a given arm. The data stored in this attribute will
    be used to drive the decision of our MAB algorithms.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`reward_history` 是一个包含多个子列表的列表，每个子列表包含给定臂返回的历史奖励。这个属性中存储的数据将用于驱动我们的 MAB
    算法的决策。
- en: 'Recall that an algorithm class implementation needs two specific methods to
    interact with the bandit API, `decide()` and `update()`, the latter of which is
    simpler and is implemented here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，算法类实现需要两个特定的方法来与老虎机API交互，分别是`decide()`和`update()`，后者较为简单，已在此实现：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Again, this `update()` method needs to take in two arguments: an arm index
    (corresponding to the `arm_id` variable) and a number representing the most recent
    reward we obtain by pulling that arm (the `reward` variable). In this method,
    we simply need to store this information in the `reward_history` attribute by
    appending the number to the corresponding sub-list of rewards.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，`update()`方法需要接收两个参数：一个臂的索引（对应`arm_id`变量）和一个数字，表示通过拉动该臂所获得的最新奖励（`reward`变量）。在此方法中，我们只需要将此信息通过将数字附加到`reward_history`属性中对应的奖励子列表来存储。
- en: 'As for the `decide()` method, we need to implement the greedy logic that we
    described previously: the reward averages across all the arms are to be computed,
    and the arm with the highest average should be returned. However, before that,
    we need to handle the first few rounds where the algorithm has not observed any
    reward from any arm. The convention here is to simply force the algorithm to pull
    each arm at least once, which is implemented by the conditional given at the beginning
    of the code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`decide()`方法，我们需要实现之前描述的贪心算法逻辑：计算所有臂的奖励平均值，并返回平均值最高的臂。然而，在此之前，我们需要处理前几轮，算法尚未从任何臂中观察到奖励的情况。这里的惯例是强制算法至少拉动每个臂一次，这通过代码开头的条件来实现：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, we first find out whether any reward sub-list has a length of
    0, indicating that the corresponding arm has not been pulled by the algorithm.
    If this is the case, we simply return the index of that arm.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们首先检查是否有任何奖励子列表的长度为0，这意味着算法未曾拉动过该臂。如果是这种情况，我们直接返回该臂的索引。
- en: 'Otherwise, we compute the reward averages with the `mean_rewards` variable:
    the `np.mean()` method computes the mean of each sub-list that is stored in the
    `reward_history` attribute, which we iterate through using list comprehension.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们使用`mean_rewards`变量来计算奖励的平均值：`np.mean()`方法计算存储在`reward_history`属性中的每个子列表的均值，我们通过列表推导式遍历它们。
- en: 'Finally, we find the arm index with the highest average, which is computed
    using `np.max(mean_rewards)`. A subtle point about the algorithm we''re implemented
    here is the `np.random.choice()` function: there will be scenarios where multiple
    arms have the same highest value of reward average, in which case the algorithm
    should randomly choose among these arms without biasing any of them. The hope
    here is that if a suboptimal arm is chosen, future rewards will reveal that the
    arm is indeed less likely to yield a positive reward, and we will be converging
    to the optimal arm anyway.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们找到奖励平均值最高的臂索引，这是通过`np.max(mean_rewards)`计算的。关于我们在此实现的算法有一个微妙的要点：`np.random.choice()`函数：在某些情况下，多个臂可能有相同的最高奖励平均值，这时算法应随机选择其中的一个臂，而不会偏向任何一个。这里的期望是，如果选择了一个次优臂，未来的奖励将表明该臂确实不太可能获得正奖励，而我们最终仍然会收敛到最优臂。
- en: 'And that is all there is to it. As noted earlier, the Greedy algorithm is fairly
    straightforward but also makes intuitive sense. Now, we want to see the algorithm
    in action by having it interact with our bandit API. First, we need to create
    a new instance of a MAB problem:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。如前所述，贪心算法相当简单且符合直觉。现在，我们希望通过与我们的老虎机API互动来查看算法的实际效果。首先，我们需要创建一个新的MAB问题实例：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, our MAB problem has three arms whose rewards all follow Bernoulli distributions
    (implemented by the `np.random.binomial` random function from NumPy). The first
    arm has a reward probability of *p = 0.9*, while it has *p = 0.8* in the second
    arm and *p = 0.7* in the third; the first arm is, therefore, the optimal arm that
    our algorithms have to identify.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的MAB问题有三个臂，它们的奖励都遵循伯努利分布（由NumPy中的`np.random.binomial`随机函数实现）。第一个臂的奖励概率为*p
    = 0.9*，第二个臂为*p = 0.8*，第三个臂为*p = 0.7*；因此，第一个臂是最优臂，我们的算法需要识别出来。
- en: (As a side note, to draw from a Bernoulli distribution with the *p* parameter,
    we call `np.random.binomial(1, p)`, so that is why we are pairing each value of
    *p* with the number `1` in the preceding code snippet.)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: （顺便提一下，要从伯努利分布中抽取参数为*p*的值，我们使用`np.random.binomial(1, p)`，所以这就是为什么我们在前面的代码片段中将每个*p*的值与数字`1`配对的原因。）
- en: 'Now, we declare an instance of our Greedy algorithm with the appropriate number
    of arms and call the `automate()` method of the bandit problem to have the algorithm
    interact with the bandit for 500 rounds, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们声明一个适当数量臂的贪心算法实例，并调用bandit问题的`automate()`方法，让算法与bandit进行500轮交互，具体如下：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we can see, the `automate()` method returns three objects in a tuple: `history`,
    which is the sequential list of arms chosen by the algorithm throughout the process;
    `rewards`, the corresponding reward obtained by pulling those arms; and `optimal_rewards`,
    which is a list of what our rewards would be, had we chosen the optimal arm at
    every step throughout the process (in other words, this is the reward list of
    the genie algorithm). The tuple is visualized by the following plot, which is
    the actual output for the preceding code.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`automate()`方法返回一个包含三个对象的元组：`history`，即算法在整个过程中选择的臂的顺序列表；`rewards`，是通过拉动这些臂获得的对应奖励；以及`optimal_rewards`，是如果在每一步都选择最优臂时我们将获得的奖励列表（换句话说，这是精灵算法的奖励列表）。该元组通过以下图表可视化，这是前面代码的实际输出。
- en: 'From within the `automate()` method, we also have the option to visualize the
    difference in cumulative sum between the two lists, `rewards` and `optimal_rewards`,
    specified by the `visualize_regret` parameter. Essentially, the option will plot
    out the cumulative regret of our algorithm as a function of a round number. Since
    we are enabling this option in our call, the following plot will be generated:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在`automate()`方法中，我们还有一个选项来可视化`rewards`和`optimal_rewards`这两个列表之间的累计和差异，这由`visualize_regret`参数指定。实质上，这个选项将绘制出我们算法的累计遗憾与轮次号之间的关系图。由于我们在调用中启用了此选项，将生成以下图表：
- en: '![Figure 8.5: Sample cumulative regret, plotted by automate()'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5：由automate()绘制的样本累计遗憾](img/B16182_08_05.jpg)'
- en: '](img/B16182_08_05.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_05.jpg)'
- en: 'Figure 8.5: Sample cumulative regret, plotted by automate()'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：由automate()绘制的样本累计遗憾
- en: 'While we don''t have any other algorithm to compare it with, from this graph,
    we can see that our Greedy algorithm did significantly well as it was able to
    keep the cumulative regret no higher than 2 at all times throughout the 500 rounds.
    Another way to inspect the performance of our algorithm is to consider the `history`
    list, which, again, contains the arms that the algorithm chose to pull:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有其他算法进行比较，但从这张图中可以看到，我们的贪心算法表现得相当好，因为它能够在整个500轮中始终保持累计遗憾不超过2。另一个评估我们算法表现的方法是查看`history`列表，该列表包含了算法选择拉动的臂：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will print out the list in the following format:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将以以下格式打印出列表：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, after the three rounds of exploration at the beginning, when
    it pulled each arm once, the algorithm vacillated a bit between the arms but then
    quickly converged to choosing arm 0, the actual optimal arm, for the rest of the
    rounds. This is why the resulting cumulative regret of the algorithm is so low.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在最初的三轮探索之后，当算法每个臂拉动一次时，它在臂之间稍微摇摆了一下，但很快就收敛到选择臂0，即实际的最优臂，来进行剩余的所有轮次。这就是为什么算法最终的累计遗憾如此之低的原因。
- en: With that said, this is simply one single experiment. As mentioned previously,
    to fully benchmark the performance of our algorithms, we need to repeat this experiments
    many times, making sure that the single experiment we are considering is not an
    outlier where the algorithm does especially well or badly due to randomness.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，这仅仅是一次实验。如前所述，为了充分评估我们算法的性能，我们需要多次重复这个实验，确保我们考虑的单个实验不是由于随机性导致算法表现特别好或特别差的异常情况。
- en: 'To facilitate repeated experiments, we utilize the `repeat()` method of the
    bandit API, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于反复实验，我们利用了bandit API的`repeat()`方法，具体如下：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Remember that the `repeat()` method takes in the class implementation of a given
    algorithm, as opposed to simply an instance of the algorithm as `automate()` does.
    This is why we are passing the whole `Greedy` class to the method. Additionally,
    with the second argument of the method, we can specify whatever arguments the
    class implementation of our algorithm takes in. In this case, it is simply the
    number of arms available to be pulled, but we will have different parameters with
    different algorithms in later sections.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`repeat()`方法接受的是给定算法的类实现，而不是像`automate()`那样仅接受算法的实例。这就是为什么我们将整个`Greedy`类传递给该方法的原因。此外，通过该方法的第二个参数，我们可以指定算法类实现所需的任何参数。在这个例子中，它仅仅是可拉动的臂的数量，但在后续章节中，我们会使用不同算法时有不同的参数。
- en: Here, we are putting our Greedy algorithm through 100 experiments with the same
    bandit problem of the three Bernoulli arms we declared previously, specified by
    the `n_experiments` parameter. To save time, we only require that each experiment
    lasts for 300 rounds with the `n_rounds` parameter. Finally, we specify `visualize_regret_dist`
    to be `True`, which will help us plot the distribution of the cumulative regret
    obtained by the algorithm at the end of each experiment.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过`n_experiments`参数对Greedy算法进行100次实验，每次实验使用我们之前声明的三个伯努利臂的相同赌博机问题。为了节省时间，我们只要求每个实验持续300轮，使用`n_rounds`参数。最后，我们将`visualize_regret_dist`设置为`True`，这将帮助我们绘制每次实验结束时算法的累积遗憾分布图。
- en: 'Indeed, when this code finishes running, the following plot will be produced:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，当这段代码运行完成时，以下图形将被生成：
- en: '![Figure 8.6: Distribution of cumulative regret by the Greedy algorithm'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6：Greedy算法的累积遗憾分布'
- en: '](img/B16182_08_06.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_06.jpg)'
- en: 'Figure 8.6: Distribution of cumulative regret by the Greedy algorithm'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：Greedy算法的累积遗憾分布
- en: Here, we can see that in most cases, the Greedy algorithm does sufficiently
    well, keeping the cumulative regret below `10`. However, there are instances where
    the cumulative regret gets as high as `60`. We speculate that these are the situations
    where the algorithm misestimates the true expected reward from each arm and commits
    too early.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，在大多数情况下，Greedy算法表现得足够好，保持累积遗憾低于`10`。然而，也有一些情况下，累积遗憾高达`60`。我们推测这些是算法错误估计每个臂的真实期望奖励并过早做出决策的情况。
- en: 'As the final way to gauge how well an algorithm performs, we consider the mean
    and the max cumulative regret across these experiments, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作为衡量算法表现的最终方式，我们考虑这些实验的平均累积遗憾和最大累积遗憾，具体如下：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In our current experiment, the following numbers will be printed out:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的实验中，以下数字将被打印出来：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is consistent with the distribution that we have here: most of the regrets
    are low enough, causing the mean to be relatively low (`8.66`), but the maximum
    regret can get as high as `62`.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们这里看到的分布一致：大多数遗憾都足够低，使得平均值相对较低（`8.66`），但最大遗憾可能高达`62`。
- en: And that is the end of our discussion on the Greedy algorithm. For the rest
    of this section, we will discuss two popular variations of the algorithm, namely
    Explore-then-commit and ε-Greedy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于Greedy算法讨论的结束。接下来的部分，我们将讨论两种流行的算法变种，即探索后再决策（Explore-then-commit）和ε-Greedy算法。
- en: The Explore-then-Commit Algorithm
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索后再决策算法
- en: We mentioned that a potential reason for poor performance of the Greedy algorithm
    in some cases is committing too early when a sufficient number of sample rewards
    from each arm have not been observed. The Explore-then-commit algorithm attempts
    to address this problem by formalizing the number of rounds that should be spent
    exploring each arm at the beginning of the process.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，Greedy算法在某些情况下表现不佳的潜在原因是过早做出决策，而没有足够观察到每个臂的样本奖励。探索后再决策算法试图通过明确规定在过程开始时应花费多少轮来探索每个臂，从而解决这个问题。
- en: Specifically, each Explore-then-commit algorithm is parameterized by a number,
    *T*. In each bandit problem, an Explore-then-commit algorithm will spend exactly
    *T* rounds pulling each of the available arms. Only after these forced exploration
    rounds does the algorithm start choosing the arm with the greatest reward average.
    Greedy is a special case of the Explore-then-commit algorithm where *T* is set
    to 1\. This general algorithm, therefore, gives us the option to customize this
    parameter and set it appropriately, depending on the situation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，每个探索后决策（Explore-then-commit）算法由一个数字* T * 参数化。在每个多臂赌博机问题中，探索后决策算法将花费正好*T*轮来拉取每个可用的臂。只有在这些强制探索轮之后，算法才会开始选择奖励平均值最大的臂。贪婪算法是探索后决策算法的特例，其中*T*被设定为1。因此，这个通用算法使我们能够根据情况定制此参数并进行适当设置。
- en: 'The implementation of this algorithm is mostly similar to what we have for
    Greedy, so we will not consider it here. In short, instead of the conditional
    used to ensure the Greedy algorithm pulls each arm at least once, we can modify
    the conditional like so in its `decide()` method, given that a value for the `T`
    variable has been set:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的实现与贪婪算法非常相似，所以我们在这里不再讨论。简而言之，在确保贪婪算法每个臂至少拉取一次的条件下，我们可以在其`decide()`方法中修改条件，如下所示，前提是已经设置了`T`变量的值：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: While Explore-then-commit is a more flexible version of Greedy, it does leave
    open the question of how to choose the value for *T*. Indeed, it is not obvious
    how we should set *T* for a specific bandit problem without any prior knowledge
    about the problem. Most of the time, *T* is set with respect to the horizon if
    it is known beforehand; common values for *T* could range from 3, 5, 10, or even
    20.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管探索后决策算法是贪婪算法的更灵活版本，但它仍然留出了如何选择*T*值的问题。实际上，如果没有关于问题的先验知识，如何为特定的多臂赌博机问题设置*T*并不明显。通常，*T*会根据已知的时间范围进行设置；*T*的常见值可能是3、5、10，甚至20。
- en: The ε-Greedy Algorithm
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ε-贪婪算法
- en: Another variation of the Greedy algorithm is the ε-Greedy algorithm. For Explore-then-commit,
    the amount of forced exploration depends on the settable parameter, *T*, which
    again gives rise to the question of how to best set it. For ε-Greedy, we do not
    explicitly require the algorithm to explore more than one round for each arm.
    Instead, we leave it to chance to determine when the algorithm should carry on
    exploitation, and when it should explore a seemingly suboptimal arm.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪算法的另一个变种是ε-贪婪算法。对于探索后决策，强制探索的次数取决于可设置的参数*T*，这再次引出了如何最好地设置它的问题。对于ε-贪婪算法，我们不明确要求算法在每个臂上探索超过一轮。相反，我们将探索何时发生以及何时继续利用最优臂的决策交给随机性来决定。
- en: Formally, an ε-Greedy algorithm is parameterized by a number, ε, between zero
    and one, denoting the exploration probability of the algorithm. After the first
    exploration rounds, the algorithm will choose to pull the arm with the greatest
    running reward average with probability 1 - ε. Otherwise, it will uniformly choose
    one out of all the available arms (with probability ε). Unlike Explore-then-commit,
    where we know for sure the algorithm will be forced to explore for the first few
    rounds, an ε-Greedy algorithm might explore arms with suboptimal reward averages
    during later rounds too. However, when exploration happens, this is entirely due
    to chance, and the choice of the parameter, ε, controls how often these exploration
    rounds are expected to happen.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，ε-贪婪算法由一个数字ε（介于0和1之间）参数化，表示算法的探索概率。在第一次探索轮之后，算法将以1 - ε的概率选择拉取奖励平均值最大的臂。否则，它将以ε的概率均匀地选择一个可用的臂。与探索后决策不同，在后者中我们可以确定算法在前几轮会被强制探索，ε-贪婪算法可能在后续轮次中也会探索奖励平均值不佳的臂。然而，当探索发生时，这完全是由随机性决定的，参数ε的选择控制了这些探索轮次发生的频率。
- en: For example, a common choice for ε is 0.01\. In a typical bandit problem, an
    ε-Greedy algorithm will pull each arm once at the start of the process and begin
    choosing the arm with the best reward history. However, at each step, with probability
    0.01 (one percent), the algorithm might choose to explore this, in which case
    it will randomly choose one of all the arms without any bias. ε, like *T* in the
    Explore-then-commit algorithm, is used to control how much an MAB algorithm should
    explore. A high value of ε will cause the algorithm to explore more often, although,
    again, when it does explore, this is completely random.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ε的常见选择是0.01。在典型的强盗问题中，ε-贪婪算法将在过程开始时每个臂都拉一次，然后开始选择具有最佳奖励历史的臂。然而，在每一步中，以0.01（1%）的概率，算法可能会选择进行探索，在这种情况下，它将随机选择一个臂而不带任何偏见。ε就像*Explore-then-commit*算法中的*T*一样，用于控制MAB算法应该进行多少探索。较高的ε值会导致算法更频繁地进行探索，尽管同样地，当它进行探索时，这完全是随机的。
- en: 'The intuition behind ε-Greedy is clear: we still want to preserve the greedy
    nature of the Greedy algorithm, but to avoid incorrect committing to a suboptimal
    arm due to nonrepresentative reward samples, we also want exploration to happen
    every now and then throughout the entire process. Hopefully, ε-Greedy will kill
    two birds with one stone, being able to greedily exploit the temporarily good
    arms while leaving the possibility that other seemingly suboptimal arms are better
    open.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ε-贪婪算法的直觉很明确：我们仍然希望保留贪婪算法的贪婪特性，但为了避免由于不具代表性的奖励样本而错误地选择一个次优臂，我们还希望在整个过程中不时进行探索。希望ε-贪婪能够一举两得，在贪婪地利用暂时表现较好的臂的同时，也留给其他看似次优的臂更好的机会。
- en: 'Implementation-wise, the `decide()` method of the algorithm should have an
    additional conditional where we check whether the algorithm should explore:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现角度来看，算法的`decide()`方法应该增加一个条件判断，检查算法是否应该进行探索：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And with that, let's move on and complete this chapter's first exercise, where
    we will implement the ε-Greedy algorithm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在我们继续并完成本章的第一个练习，我们将在其中实现ε-贪婪算法。
- en: Exercise 8.01 Implementing the ε-Greedy Algorithm
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.01 实现ε-贪婪算法
- en: 'Similar to what we did to implement the Greedy algorithm, in this exercise,
    we will learn how to implement the ε-Greedy algorithm. This exercise will consist
    of three main sections: implementing the logic of ε-Greedy, testing it in a sample
    bandit problem, and finally putting it through multiple experiments to benchmark
    its performance.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于实现贪婪算法时的做法，在本练习中，我们将学习如何实现ε-贪婪算法。这个练习将分为三个主要部分：实现ε-贪婪的逻辑，测试其在示例强盗问题中的表现，最后通过多次实验来评估其性能。
- en: 'We will follow these steps to achieve this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤来实现：
- en: 'Create a new Jupyter Notebook and import NumPy, Matplotlib, and the `Bandit`
    class from the `utils.py` file included in the code repository for this chapter:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Jupyter Notebook，并导入NumPy、Matplotlib以及本章代码库中`utils.py`文件中的`Bandit`类：
- en: '[PRE19]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we are now fixing the random seed number of NumPy to ensure the reproducibility
    of our code.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们现在将NumPy的随机种子数固定，以确保代码的可重复性。
- en: 'Now, let''s begin implementing the logic of the ε-Greedy algorithm. First,
    its initialization method should take in two parameters: the number of arms for
    the bandit problem it is to solve and ε, the exploration probability:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，开始实现ε-贪婪算法的逻辑。首先，它的初始化方法应该接受两个参数：要解决的强盗问题的臂数和ε，探索概率：
- en: '[PRE20]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Similar to what we had with Greedy, here, we are also keeping track of the reward
    history, which is stored in the `reward_history` attribute of the class object.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与贪婪算法类似，在这里，我们也在跟踪奖励历史，这些历史记录存储在类对象的`reward_history`属性中。
- en: In the same code cell, implement the `decide()` method for the `eGreedy` class.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个代码单元格中，实现`eGreedy`类的`decide()`方法。
- en: 'This method should be mostly similar to its counterpart in the `Greedy` class.
    However, before computing the reward averages of the arms, it should draw a random
    number between 0 and 1 and check to see if it is less than its parameter, ε. If
    this is the case, it should randomly return the index of one of the arms:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法应该与`Greedy`类中的对应方法大致相似。然而，在计算各臂的奖励平均值之前，它应该生成一个介于0和1之间的随机数，并检查它是否小于其参数ε。如果是这种情况，它应该随机返回一个臂的索引：
- en: '[PRE21]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the same code cell, implement the `update()` method for the `eGreedy` class,
    which should be identical to the corresponding method in the `Greedy` class:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一代码单元格中，为`eGreedy`类实现`update()`方法，该方法应与`Greedy`类中的相应方法相同：
- en: '[PRE22]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Again, this method only needs to append the most recent reward from an arm to
    the reward history of that arm.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次说明，这种方法只需要将最近一次的奖励添加到该臂的奖励历史中。
- en: And that is the complete implementation of our ε-Greedy algorithm.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是我们ε-Greedy算法的完整实现。
- en: In the next code cell, create a single experiment with the bandit problem with
    three Bernoulli arms with respective probabilities of `0.9`, `0.8`, and `0.7`
    and run it with an instance of the `eGreedy` class (with `ε = 0.01`, which is
    the default value that does not need to be specified) using the `automate()` method.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个代码单元格中，创建一个具有三个伯努利臂的赌博机问题实验，这些臂的相应概率分别为`0.9`、`0.8`和`0.7`，并使用`eGreedy`类的实例（`ε
    = 0.01`，即默认值，不需要显式指定）通过`automate()`方法运行该实验。
- en: 'Make sure to specify the `visualize_regret=True` parameter to plot out the
    cumulative regret of the algorithm throughout the process:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保指定`visualize_regret=True`参数，以便绘制算法在整个过程中累积后悔的图表：
- en: '[PRE23]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This should produce the following graph:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会产生以下图表：
- en: '![Figure 8.7: Sample cumulative regret of the ε-Greedy algorithm'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.7：ε-Greedy 算法的样本累积后悔'
- en: '](img/B16182_08_07.jpg)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_07.jpg)'
- en: 'Figure 8.7: Sample cumulative regret of the ε-Greedy algorithm'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.7：ε-Greedy 算法的样本累积后悔
- en: Compared to the corresponding plot we saw with Greedy, our cumulative regret
    here has more variation, sometimes growing to `4` and sometimes dropping to `-2`.
    This is an effect of the increase in exploration of the algorithm.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与我们在 Greedy 算法中看到的相应图表相比，这里的累积后悔变化更大，有时会增长到`4`，有时会降到`-2`。这正是算法探索增加的效果。
- en: 'In the next code cell, we print out the `history` variable and see how it compares
    to the Greedy algorithm:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个代码单元格中，我们打印出`history`变量，看看它与 Greedy 算法的历史相比如何：
- en: '[PRE24]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will produce the following output:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE25]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, we can see that after the first few rounds, most of the choices made by
    the algorithm were all arm 0\. But from time to time, arm 1 or arm 2 would be
    chosen, presumably from the random exploration probability.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，在前几轮之后，算法做出的选择大多数都是臂 0。但偶尔也会选择臂 1 或臂 2，这大概是由于随机探索概率的原因。
- en: 'In the next code cell, we will conduct the same experiment, but this time,
    we will set `ε = 0.1`:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个代码单元格中，我们将进行相同的实验，不过这次我们将设置`ε = 0.1`：
- en: '[PRE26]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will produce the following graph:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![Figure 8.8: Sample cumulative regret with increased exploration probability'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.8：增加探索概率后的样本累积后悔'
- en: '](img/B16182_08_08.jpg)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_08.jpg)'
- en: 'Figure 8.8: Sample cumulative regret with increased exploration probability'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.8：增加探索概率后的样本累积后悔
- en: Here, our cumulative regret is a lot higher than what we got with ε = 0.01 in
    *step 5*. This is presumably due to the increased exploration probability, which
    is too high.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们的累积后悔比在*步骤 5* 中设置`ε = 0.01`时要高得多。这大概是因为增加的探索概率过高所导致的。
- en: 'To analyze this experiment further, we can print out the action history once
    more:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进一步分析这个实验，我们可以再次打印出动作历史：
- en: '[PRE27]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will produce the following output:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE28]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Comparing this with the same history of the previous algorithm, we can see that
    this algorithm did indeed explore significantly more during the late rounds. All
    of this indicates to us that `ε = 0.1` might not be an appropriate exploration
    probability.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将此与之前算法相同历史的数据进行比较，可以看到该算法确实在后期的轮次中进行了更多的探索。所有这些都表明，`ε = 0.1` 可能不是一个合适的探索概率。
- en: 'As the last component of our analysis of the ε-Greedy algorithm, let''s utilize
    the repeated-experiment option. This time, we will choose `ε = 0.03`, like so:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为我们对ε-Greedy算法分析的最后一个部分，让我们利用重复实验选项。这次，我们将选择`ε = 0.03`，如下所示：
- en: '[PRE29]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following graph will be produced, which visualizes the distribution of
    cumulative regret resulting from these repeated experiments:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来的图表将展示来自这些重复实验的累积后悔分布：
- en: '![Figure 8.9: Distribution of cumulative regret by the ε-Greedy algorithm'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.9：ε-Greedy 算法的累积后悔分布'
- en: '](img/B16182_08_09.jpg)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_09.jpg)'
- en: 'Figure 8.9: Distribution of cumulative regret by the ε-Greedy algorithm'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.9：ε-Greedy 算法的累积后悔分布
- en: This distribution is quite similar to what we obtained with the Greedy algorithm.
    Next, we will compare the two algorithms further.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个分布与我们在贪婪算法中得到的结果非常相似。接下来，我们将进一步比较这两种算法。
- en: 'Calculate the mean and max of these cumulative regret values with the following
    code:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码计算这些累积遗憾值的均值和最大值：
- en: '[PRE30]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output will be as follows:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE31]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Comparing this with what we had with the Greedy algorithm (`8.66` and `62`),
    this result indicates that the ε-Greedy algorithm might be inferior in this specific
    bandit problem. However, it has managed to formalize the choice between exploration
    and exploitation using its exploration rate, which was lacking in the Greedy algorithm.
    This is a valuable characteristic of a MAB algorithm, and will be the focus of
    other algorithms that we will be discussing in the rest of this chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一点与我们在贪婪算法中得到的结果（`8.66` 和 `62`）进行比较，结果表明在这个特定的强盗问题中，ε-贪婪算法可能会表现得较差。然而，它通过探索率成功地形式化了探索与利用之间的选择，而这是贪婪算法所缺乏的。这是一个多臂强盗（MAB）算法的宝贵特性，也是我们将在本章后面讨论的其他算法的重点。
- en: Note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fiE3Y5](https://packt.live/3fiE3Y5).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一特定部分的源代码，请参考 [https://packt.live/3fiE3Y5](https://packt.live/3fiE3Y5)。
- en: You can also run this example online at [https://packt.live/3cYT4fY](https://packt.live/3cYT4fY).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个例子，链接是 [https://packt.live/3cYT4fY](https://packt.live/3cYT4fY)。
- en: Before we move on to the next section, let's briefly discuss yet another so-called
    variant of the Greedy algorithm, the Softmax algorithm.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一部分之前，让我们简要讨论一下另一种所谓的贪婪算法变种——Softmax算法。
- en: The Softmax Algorithm
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax算法
- en: 'The Softmax algorithm attempts to quantify the trade-off between exploration
    and exploitation by choosing each of the available arms with a probability that
    is proportional to its average reward. Formally, the probability that arm *i*
    is chosen by the algorithm at each time step, *t*, is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax算法试图通过选择每个可用臂的概率来量化探索与利用之间的权衡，这个概率与其平均奖励成正比。形式上，算法在每个时间步骤*t*选择臂*i*的概率如下：
- en: '![Figure 8.10: Expression for the probability that the arm is chosen'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10: 表示选择该臂的概率的公式'
- en: by the algorithm at each time step
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在每个时间步骤选择该臂的概率
- en: '](img/B16182_08_10.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_10.jpg)'
- en: 'Figure 8.10: Expression for the probability that the arm is chosen by the algorithm
    at each time step'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：表示算法在每个时间步骤选择该臂的概率的公式
- en: Each term in the exponent ![1](img/B16182_08_10a.png) is the average reward
    observed from arm *i* in the first *(t - 1)* time steps. Given the way the probabilities
    are defined, the larger this average reward is, the more likely the corresponding
    arm will be chosen. In its most general form, this average term is divided by
    a tunable parameter, ![2](img/B16182_08_10b.png), which controls the exploration
    rate of the algorithm. Specifically, when ![3](img/B16182_08_10c.png) tends to
    infinity, the probability of the largest arm will approach one while the other
    probabilities approach zero, making the algorithm purely greedy (which is why
    we consider it to be a generalization of the Greedy algorithm). The smaller ![4](img/B16182_08_10d.png)
    is, the more likely it is that a temporarily sub-optimal arm is chosen. As it
    tends to 0, the algorithm uniformly explores all the available arms indefinitely.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 指数中的每一项 ![1](img/B16182_08_10a.png) 是从臂*i*在前*(t - 1)*个时间步骤中观察到的平均奖励。根据定义概率的方式，平均奖励越大，相应的臂被选择的可能性就越大。在其最一般的形式下，这个平均项被一个可调参数
    ![2](img/B16182_08_10b.png) 除以，后者控制算法的探索率。具体来说，当![3](img/B16182_08_10c.png)趋向于无穷大时，最大臂的选择概率会趋近于1，而其他臂的选择概率趋近于0，使得算法完全贪婪（这也是我们认为它是贪婪算法的一种推广的原因）。当![4](img/B16182_08_10d.png)
    趋近于0时，选择一个暂时次优的臂的可能性增大。当它趋向于0时，算法会无限期地均匀探索所有可用的臂。
- en: Similar to the problem we encounter while designing the ε-Greedy algorithm,
    it is not entirely clear how we should set the value of this parameter, ![5](img/B16182_08_10e.png),
    for each specific bandit problem, even though the performance of the algorithm
    can be highly dependent on this parameter. For that reason, the Softmax algorithm
    is not as popular as the algorithms we will be discussing in this chapter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在设计ε-贪婪算法时遇到的问题，如何为每个特定的强盗问题设置该参数![5](img/B16182_08_10e.png)的值并不完全明确，尽管算法的性能在很大程度上依赖于该参数。因此，Softmax算法不像我们将在本章讨论的其他算法那样流行。
- en: 'And with that, we conclude our discussion of the Greedy algorithm, our first
    approach to solving the MAB problem, and three of its variations: Explore-then-commit,
    ε-Greedy, and Softmax. Overall, these algorithms focus on exploiting the arm with
    the greatest reward mean while sometimes deviating from that to explore other,
    seemingly suboptimal, arms.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们结束了对贪心算法的讨论，这是我们解决MAB问题的第一种方法，以及它的三种变体：先探索后承诺、ε-贪心和Softmax。总体来说，这些算法专注于利用具有最大奖励均值的臂，同时有时偏离这个臂去探索其他看似次优的臂。
- en: In the next section, we will move on to another common MAB algorithm called
    **Upper Confidence Bound** (**UCB**), the intuition of which is slightly different
    from what we have seen so far.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍另一种常见的MAB算法——**上置信界限**（**UCB**），其直觉与我们到目前为止所看到的稍有不同。
- en: The UCB algorithm
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UCB算法
- en: The term *upper confidence bound* denotes the fact that instead of considering
    the average of past rewards returned from each arm like Greedy, the algorithm
    computes an upper bound for its estimates of the expected reward for each arm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*上置信界限*这个术语表示，算法不是像贪心算法那样考虑每个臂返回的过去奖励的平均值，而是计算每个臂预期奖励的估计值的上界。'
- en: This concept of a confidence bound is quite common in probability and statistics,
    where the distribution of a quantity that we care about (in this case, the reward
    from each arm) cannot be represented well using simply the average of past observations.
    Instead, a confidence bound is a numerical range that aims to estimate and narrow
    down where most of the values in the distribution in question will lie. For example,
    this idea is widely used in Bayesian analyses and Bayesian optimization.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 置信界限这个概念在概率论和统计学中是非常常见的，其中我们关心的量（在这个例子中是每个臂的奖励）的分布不能仅通过过去观测值的平均值来良好表示。相反，置信界限是一个数值范围，旨在估计并缩小在该分布中大多数值所在的范围。例如，这一概念在贝叶斯分析和贝叶斯优化中被广泛应用。
- en: In the following section, we will discuss how UCB establishes its use of a confidence
    bound.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论UCB如何建立其使用置信界限的方法。
- en: Optimism in the Face of Uncertainty
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定性面前的乐观主义
- en: Consider the middle of the process of a bandit with only two arms. We have already
    pulled the first arm 100 times and observed an average reward of `0.61`; for the
    second arm, we have only seen five samples, three of which were `1`, so its average
    reward is `0.6`. Should we commit to exploring the first arm for the rest of the
    remaining rounds and ignore the second?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个只有两个臂的赌博机过程的中间部分。我们已经拉动过第一个臂100次，并观察到平均奖励为`0.61`；对于第二个臂，我们仅见过五个样本，其中三个样本的奖励为`1`，所以它的平均奖励是`0.6`。我们是否应该承诺在剩余的回合中探索第一个臂并忽视第二个臂？
- en: 'Many would say no; we should at least explore the second arm more to get a
    better estimation of its expected reward. The motivation for this observation
    is that since we only have very few samples of the reward from the second arm,
    we should not be *confident* that the mean reward of the second arm is actually
    lower than that of the first. How, then, should we formalize our intuition? The
    UCB algorithm, or specifically, its most common variant – the UCB1 algorithm –
    states that instead of the mean reward, we will use the following sum of the average
    reward and the confidence bound:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人会说不；我们至少应该更多地探索第二个臂，以便更好地估计其期望奖励。这一观察的动机是，由于我们仅有很少的第二个臂的奖励样本，我们不应该*确信*第二个臂的平均奖励实际上低于第一个臂。那么，我们应该如何将我们的直觉形式化呢？UCB算法，或者说其最常见的变种——UCB1算法——指出，我们将不再使用平均奖励，而是使用以下平均奖励和置信界限之和：
- en: '![Figure 8.11: Expression for the UCB algorithm'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.11：UCB算法的表达式'
- en: '](img/B16182_08_11.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_11.jpg)'
- en: 'Figure 8.11: Expression for the UCB algorithm'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：UCB算法的表达式
- en: 'Here, ![6](img/B16182_08_11a.png) denotes the time step, or the round number,
    that we are currently in while interacting with a bandit, and ![7](img/B16182_08_11b.png)
    denotes the number of times we have pulled arm ![8](img/B16182_08_11c.png) up
    to round ![9](img/B16182_08_11e.png). The rest of UCB works in the same way as
    the Greedy algorithm: at each step, we choose to pull the arm that maximizes the
    preceding sum, observe the returned reward, add it to our reward, and repeat the
    process.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![6](img/B16182_08_11a.png)表示当前我们与赌博机互动时的时间步长，或回合数，![7](img/B16182_08_11b.png)表示我们已经拉动过的臂数，直到回合![9](img/B16182_08_11e.png)。UCB的其余部分与贪心算法的运作方式相同：在每一步中，我们选择拉动能够最大化前述总和的臂，观察返回的奖励，将其加到我们的奖励中，然后重复这个过程。
- en: 'To implement this logic, we can use the `decide()` method, which contains the
    following code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一逻辑，我们可以使用`decide()`方法，方法中包含如下代码：
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, `self.t` should be equal to the current step time. As we can see, the
    method returns the arm that maximizes the element in `conf_bounds`, which is the
    list storing the optimistic estimation of each arm.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`self.t`应当等于当前的步骤时间。正如我们所见，该方法返回的是使`conf_bounds`中元素最大化的臂，这个列表存储了每只臂的乐观估算值。
- en: 'You might be wondering why using the preceding quantity can capture the idea
    behind the confidence bound that we''d like to apply to our estimations of the
    expected reward. Remember the example of a two-arm bandit we sketched out earlier,
    where we would like to have a formalization that encourages exploration of the
    rarely explored arm (the second one, in our example). As you can see, at any given
    round, this quantity is a decreasing function of ![10](img/B16182_08_11d.png).
    In other words, the quantity gets smaller when ![11](img/B16182_08_11f.png) is
    large and grows larger when the opposite is true. So, this quantity is maximized
    by the arm that has the lower number of pulls – the arm that is explored the least.
    In our example, the estimation of the first arm is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么使用前述的量能捕捉我们想要应用于期望奖励估算的置信区间的概念。请记住我们之前所举的两臂赌博机的例子，我们希望有一个形式化的过程，能够鼓励探索那些很少被探索的臂（在我们的例子中是第二只臂）。正如你所见，在任何给定回合，这个量是![10](img/B16182_08_11d.png)的递减函数。换句话说，当![11](img/B16182_08_11f.png)很大时，这个量会变小；而当情况相反时，它会变大。因此，这个量由那些拉动次数较少的臂最大化——也就是那些探索较少的臂。在我们的例子中，第一只臂的估算如下：
- en: '![Figure 8.12: Estimation of the first arm'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.12：第一只臂的估算'
- en: '](img/B16182_08_12.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_12.jpg)'
- en: 'Figure 8.12: Estimation of the first arm'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12：第一只臂的估算
- en: 'The estimation of the second arm is as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 第二只臂的估算如下：
- en: '![Figure 8.13: Estimation of the second arm'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.13：第二只臂的估算'
- en: '](img/B16182_08_13.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_13.jpg)'
- en: 'Figure 8.13: Estimation of the second arm'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13：第二只臂的估算
- en: Using UCB, we choose to pull the second arm next, which is what we argued was
    the correct choice. By adding what is called an exploration term to the mean reward,
    we are, in a way, estimating the largest possible value of the expected mean,
    not just the expected mean itself. This intuition is best summed up with the term
    *optimism in the face of uncertainty*, and it is the quintessential characteristic
    of the UCB algorithm.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用UCB算法时，我们选择拉取第二只臂，这也是我们认为正确的选择。通过在平均奖励中加入所谓的探索项，我们从某种意义上来说，是在估算期望均值的最大可能值，而不仅仅是期望均值本身。这一直觉可以用“*在不确定性面前保持乐观*”这一术语来总结，它是UCB算法的核心特征。
- en: Other Properties of UCB
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UCB的其他特性
- en: UCB is not unjustifiably optimistic. When an arm is significantly under-explored,
    the exploration term will make the sum larger, making it more likely to be chosen
    by UCB, but it is never guaranteed that the arm will surely be chosen. Specifically,
    when the mean reward of an arm is so low that a large value of the term cannot
    compensate for it, UCB will choose to exploit the good arms anyway.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: UCB并非毫无根据地乐观。当一只臂显著未被充分探索时，探索项会使得该量变大，从而增加被UCB选择的可能性，但并不能保证这只臂一定会被选择。具体来说，当某只臂的平均奖励非常低，以至于探索项无法弥补时，UCB依然会选择利用那些表现良好的臂。
- en: We should also discuss its variation in cost-centric MAB problems, known as
    the **Lower Confidence Bound** (**LCB**). With respect to a reward-centric problem,
    we are adding the exploration term to the mean reward to compute an optimistic
    estimation of the true mean. When the MAB problem is the minimization of costs
    returned by the arms, our optimistic estimation becomes the mean cost *subtracted*
    by the exploration term, and the arm that minimizes this quantity will be chosen
    by UCB, or in this case, LCB.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应当讨论它在以成本为中心的MAB问题中的变化，这就是**下置信区间**（**LCB**）。对于奖励为中心的问题，我们将探索项加入到平均奖励中，以计算对真实均值的乐观估算。当MAB问题是最小化臂返回的成本时，我们的乐观估算变成了平均成本*减去*探索项，UCB将选择最小化这一量的臂，或者在这种情况下，选择LCB。
- en: In particular, we are saying that if an arm is under-explored, its true mean
    cost might be lower than what we have observed so far, so we subtract the average
    cost from the exploration term to estimate the lowest possible cost of an arm.
    Aside from this, the implementation of this variation of UCB remains the same.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们在这里说的是，如果某个臂的探索次数较少，它的真实平均成本可能比我们目前观察到的要低，因此我们从探索项中减去平均成本，以估算某个臂的最低可能成本。除此之外，这种
    UCB 变体的实现保持不变。
- en: That is enough theory about UCB. To conclude our discussions of this algorithm,
    we will implement it for the Bernoulli three-arm bandit problem that we have been
    using in the next exercise.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是关于 UCB 的全部理论内容。为了总结我们对该算法的讨论，我们将在下一个练习中实现它，以解决我们之前使用的伯努利三臂赌博机问题。
- en: Exercise 8.02 Implementing the UCB Algorithm
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.02 实现 UCB 算法
- en: 'In this exercise, we will be implementing the UCB algorithm. This exercise
    will walk us through the familiar workflow that we have been using to analyze
    the performance of an MAB algorithm: implement it as a Python class, put it through
    a single experiment and observe its behavior, and finally repeat the experiment
    many times to consider the resulting distribution of its regret.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将实现 UCB 算法。本练习将引导我们通过熟悉的工作流程来分析 MAB 算法的表现：将其实现为一个 Python 类，进行一次实验并观察其行为，最后多次重复实验，考虑由此产生的后悔分布。
- en: 'We will follow these steps to do so:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤进行：
- en: 'Create a new Jupyter Notebook and import `NumPy`, `Matplotlib`, and the `Bandit`
    class from the `utils.py` file included in the code repository for this chapter:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 Jupyter Notebook，导入 `NumPy`、`Matplotlib`，以及从代码库中包含的 `utils.py` 文件中的 `Bandit`
    类：
- en: '[PRE33]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Declare a Python class named `UCB` with the following initialization method:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明一个名为 `UCB` 的 Python 类，并定义以下初始化方法：
- en: '[PRE34]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Different from Greedy and its variants, our implementation of `UCB` needs to
    keep track of an additional piece of information, the current round number, in
    its attribute, `t`. This information is used when calculating the exploration
    term of the upper confidence bound.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 Greedy 及其变体不同，我们对 `UCB` 的实现需要在其属性 `t` 中跟踪一个额外的信息——当前轮次号。这个信息在计算上置信界限的探索项时使用。
- en: 'Implement the `decide()` method of the class, as follows:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现类的 `decide()` 方法，如下所示：
- en: '[PRE35]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code is self-explanatory: after pulling each arm at least once,
    we compute the confidence bounds as the sum of the empirical mean reward and the
    exploration term. Finally, we return the arm with the largest sum, randomly tie-breaking
    if necessary.'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码不言自明：在每个臂至少拉一次之后，我们计算置信界限，作为经验均值奖励和探索项的总和。最后，我们返回具有最大总和的臂，必要时随机打破平局。
- en: 'In the same code cell, implement the `update()` method of the class, like so:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个代码单元中，像这样实现类的 `update()` 方法：
- en: '[PRE36]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We are already familiar with most of the logic here from the previous algorithms.
    Notice here that with each call to `update()`, we also need to increment the attribute,
    `t`.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经对大部分逻辑比较熟悉，来自之前的算法。注意，在每次调用 `update()` 时，我们还需要递增属性 `t`。
- en: 'Declare the Bernoulli three-arm bandit problem that we have been considering
    and run it on an instance of the UCB algorithm we just implemented:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明我们一直在考虑的伯努利三臂赌博机问题，并在我们刚刚实现的 UCB 算法实例上运行它：
- en: '[PRE37]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This code will produce the following graph:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码将生成以下图表：
- en: '![Figure 8.14: Sample cumulative regret from UCB'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.14：UCB 算法的样本累计后悔'
- en: '](img/B16182_08_14.jpg)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_14.jpg)'
- en: 'Figure 8.14: Sample cumulative regret from UCB'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.14：UCB 算法的样本累计后悔
- en: Here, we can see that this cumulative regret is significantly worse than what
    we saw with Greedy, which was, at most, 2\. We hypothesize that the difference
    is a direct result of the optimistic nature of the algorithm.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，这个累计后悔显著比我们在 Greedy 算法中看到的要糟糕，后者最多为 2。我们假设这种差异直接源自算法的乐观性质。
- en: 'To understand this behavior better, we will inspect the pulling history of
    the algorithm:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地理解这种行为，我们将检查算法的拉臂历史：
- en: '[PRE38]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This produces the following output:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE39]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, we can observe that instead of exploiting the true optimal arm (arm 0),
    UCB frequently chose to deviate. This is a direct effect of its tendency to optimistically
    explore seemingly suboptimal arms.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以观察到，UCB 经常选择偏离真正的最优臂（臂 0）。这是由于它倾向于乐观地探索看似次优的臂的直接影响。
- en: 'At face value, we might conclude that for this bandit problem, UCB is, in fact,
    not superior than the Greedy algorithm, but to truly confirm whether that is true
    or not, we need to inspect how the algorithm does across multiple experiments.
    Use the `repeat()` method from the bandit API to confirm this:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表面上，我们可能会得出结论：对于这个赌博机问题，UCB算法实际上并不优于贪婪算法。但要真正确认这一点，我们需要检查该算法在多个实验中的表现。使用来自赌博机API的`repeat()`方法来确认这一点：
- en: '[PRE40]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This code snippet will generate the following plot:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码将生成以下图表：
- en: '![Figure 8.15: Distribution of regret of UCB'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.15：UCB算法的遗憾分布'
- en: '](img/B16182_08_15.jpg)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_15.jpg)'
- en: 'Figure 8.15: Distribution of regret of UCB'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.15：UCB算法的遗憾分布
- en: To our surprise, the regret values in this distribution are significantly lower
    than those resulting from the Greedy algorithm.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令我们惊讶的是，这个分布中的遗憾值明显低于由贪婪算法得出的结果。
- en: 'In addition to visualizing the distribution, we also need to consider the average
    and max regret across all experiments:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了可视化分布外，我们还需要考虑所有实验的平均遗憾和最大遗憾：
- en: '[PRE41]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output will be as follows:'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE42]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As you can see, the values are significantly lower than the corresponding statistics
    we saw in Greedy, which were `8.66` and `62`. Here, we can say that we have evidence
    supporting the claim that UCB is better than Greedy in terms of minimizing the
    cumulative regret of a bandit problem.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数值明显低于我们在贪婪算法中看到的对应统计数据，后者为`8.66`和`62`。在这里，我们可以说有证据支持“UCB算法在最小化赌博机问题的累计遗憾方面优于贪婪算法”这一说法。
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhxSmX](https://packt.live/3fhxSmX).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3fhxSmX](https://packt.live/3fhxSmX)。
- en: You can also run this example online at [https://packt.live/2XXuJmK](https://packt.live/2XXuJmK).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在网上运行这个示例，地址是：[https://packt.live/2XXuJmK](https://packt.live/2XXuJmK)。
- en: This example also illustrates the importance of repeating experiments when analyzing
    the performance of a MAB algorithm. As we saw earlier, using just a single experiment,
    we could have arrived at the wrong conclusion that UCB is inferior to the Greedy
    algorithm in the specific bandit problem we are considering. However, across many
    repeated experiments, we can see that the opposite is true.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例也说明了在分析MAB算法性能时重复实验的重要性。正如我们之前所见，仅使用一次实验，我们可能得出错误的结论，认为UCB算法在我们考虑的特定赌博机问题中劣于贪婪算法。然而，通过多次重复实验，我们可以看到事实恰恰相反。
- en: 'Throughout this exercise, we have implemented UCB, as well as learned about
    the need for comprehensive analysis with multiple experiments while working with
    MAB algorithms. This also marks the end of the topic surrounding the UCB algorithm.
    In the next section, we will begin talking about the last MAB algorithm in this
    chapter: Thompson Sampling.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，我们实现了UCB算法，并学习了在使用多臂老虎机算法（MAB）时进行全面分析的必要性。这也标志着UCB算法话题的结束。在接下来的章节中，我们将开始讨论本章的最后一种MAB算法：汤普森采样。
- en: Thompson Sampling
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汤普森采样
- en: 'The algorithms we have seen so far make up a set of diverse insights: Greedy
    and its variants mostly focus on exploitation and might need to be explicitly
    forced to employ exploration; UCB, on the other hand, tends to be optimistic about
    the true expected reward of under-explored arms and therefore naturally, but also
    justifiably, focuses on exploration.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所看到的算法组成了一套多元化的见解：贪婪算法及其变体主要关注利用，可能需要明确地强制执行探索；而UCB则倾向于对尚未充分探索的臂的真实期望回报持乐观态度，因此自然地，但也正当合理地，专注于探索。
- en: 'Thompson Sampling also uses a completely different intuition. However, before
    we can understand the idea behind the algorithm, we need to discuss one of its
    principal building blocks: the concept of Bayesian probability.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 汤普森采样也采用了完全不同的直觉。然而，在我们理解算法背后的思想之前，需要讨论其主要构建模块之一：贝叶斯概率的概念。
- en: Introduction to Bayesian Probability
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯概率简介
- en: 'Generally speaking, the workflow of using Bayesian probability to describe
    a quantity consists of the following elements:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，使用贝叶斯概率描述某个量的工作流程包括以下元素：
- en: A prior probability representing whatever prior knowledge or belief we have
    about the quantity.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个先验概率，表示我们对某个量的先验知识或信念。
- en: A likelihood probability that denotes, as the name of the term suggests, how
    likely the data that we have observed so far is.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个似然概率，表示正如术语的名称所示，当前为止我们所观察到的数据的可能性。
- en: And finally, a posterior probability, which is the combination of the preceding
    two elements.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，后验概率是前面两个元素的组合。
- en: 'One fundamental component of Bayesian probability is Bayes'' theorem:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯概率的一个基本组成部分是贝叶斯定理：
- en: '![Figure 8.16: Bayes'' theorem'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.16：贝叶斯定理'
- en: '](img/B16182_08_16.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_16.jpg)'
- en: 'Figure 8.16: Bayes'' theorem'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16：贝叶斯定理
- en: Here, *P(X)* denotes the probability of a given event, *X*, while *P(X | Y)*
    is the probability of a given event, *X*, provided that event *Y* has already
    happened. The latter is an example of conditional probabilities, which is a common
    object in machine learning, especially when different events/quantities are conditionally
    dependent on each other.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*P(X)*表示给定事件*X*的概率，而*P(X | Y)*是给定事件*Y*已经发生的情况下，事件*X*发生的概率。后者是条件概率的一个例子，条件概率是机器学习中常见的对象，尤其是当不同事件/量彼此条件依赖时。
- en: 'This specific formula outlines the general idea of Bayesian probability that
    we have here: say we are given a prior probability for an event, *A*, and we also
    know how likely event *B* happens *given* event *A*. Here, the posterior probability
    of the same event, *A*, given event *B*, is proportional to the product of the
    two aforementioned probabilities. Event A is typically what we care about, while
    event B is the data that we have observed. To put this into perspective, let''s
    consider the application of this formula in the context of a Bernoulli distribution.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式具体阐明了我们这里的贝叶斯概率的基本思想：假设我们给定了一个事件*A*的先验概率，并且我们也知道在事件*A*发生的情况下，事件*B*发生的概率。这里，给定事件*B*发生后，事件*A*的后验概率与上述两种概率的乘积成正比。事件A通常是我们关心的事件，而事件B则是我们已经观察到的数据。为了更好理解，让我们将这个公式应用于伯努利分布的背景下。
- en: We'd like to estimate the unknown parameter, *p*, that characterizes a Bernoulli
    distribution, from which we have observed five samples. Due to how a Bernoulli
    distribution is defined, the probability that the sum of these five samples is
    equal to *x*, an integer between 0 and 5, is ![12](img/B16182_08_16a.png) (don't
    worry if you aren't familiar with this expression).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想估计未知参数*p*，该参数描述了一个伯努利分布，从中我们已经观察到了五个样本。由于伯努利分布的定义，这五个样本的和等于*x*，即一个介于0到5之间的整数的概率为![12](img/B16182_08_16a.png)（如果你不熟悉这个表达式也不用担心）。
- en: But what if the samples are what we can observe, and we are unsure what the
    actual value of *p* is? How can we "flip" the direction of the preceding probabilistic
    quantity so that we can draw some conclusions about the value of *p* from the
    samples? This is where Bayes' theorem comes into play. In the Bernoulli example,
    from the likelihood of the sum of the observed samples given any value of *p*,
    we can calculate the probability that *p* is indeed that value, given the observations
    that we have.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果样本是我们能观察到的，而我们不确定*p*的实际值是什么，该怎么办呢？我们如何“翻转”前面提到的概率量，以便从样本中得出关于*p*值的结论？这时贝叶斯定理就派上用场了。在伯努利例子中，给定任何*p*值的观察样本的似然性，我们可以计算*p*确实是该值的概率，前提是我们有了观察数据。
- en: This is directly connected to the MAB problem. We, of course, always start out
    not knowing what the actual value, *p*, that parameterizes the reward distribution
    of a given arm is, but we can observe the reward samples drawn from it by pulling
    that arm. So, from a number of samples, we can calculate what the probability
    that *p* is equal to, say, 0.5 is, and whether that probability is larger than
    the probability that *p* is equal to 0.6.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这与MAB问题直接相关。当然，我们总是从不知道实际值*p*开始，它是给定臂的奖励分布的参数，但我们可以通过拉动该臂来观察从中获得的奖励样本。因此，从若干样本中，我们可以计算*p*等于0.5的概率，并判断该概率是否大于*p*等于0.6的概率。
- en: A question remains about how to choose the prior distribution for *p*. In our
    case, when we start out without any prior information about *p*, we might say
    that *p* is equally likely to be any number between 0 and 1\. So, we model *p*
    using a uniform distribution between 0 and 1\. The Beta distribution is a generalization
    of the uniform distribution where its parameters are α = 1 and β = 1, so let's
    say *p*, for now, follows Beta(1, 1).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一个问题是如何选择*p*的先验分布。在我们的例子中，当我们没有关于*p*的任何先验信息时，我们可以说*p*在0到1之间是等概率的。因此，我们使用一个0到1之间的均匀分布来对*p*进行建模。Beta分布是均匀分布的一种广义形式，其参数为α
    = 1和β = 1，因此暂时假设*p*服从Beta(1, 1)分布。
- en: Bayes' theorem allows us to *update* this Beta distribution to another Beta
    distribution with different parameters after seeing some observations. Following
    our running example, say, after five separate observations from this Bernoulli
    distribution that we are modeling, we have three instances of 1 and two instances
    of 0\. According to the Bayesian updating rule (the math of which is out of scope
    for this book), a Beta distribution with α and β parameters will be updated to
    α + 3 and β + 2.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理允许我们在看到一些观察后，*更新*这个Beta分布，得到一个具有不同参数的Beta分布。以我们正在进行的示例为例，假设在对这个伯努利分布进行五次独立观察后，我们得到三次1和两次0。根据贝叶斯更新规则（具体的数学内容超出了本书的范围），一个具有α和β参数的Beta分布将更新为α
    + 3和β + 2。
- en: Note
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In general, out of *n* observations, `x` of which are `1` and the others are
    `0`, a `Beta(α, β)` distribution will be updated to `Beta(α + x, β + n - x)`.
    Roughly speaking, in an update, *α* should be incremented by the number of samples
    observed, while *β* should be incremented by the number of zero samples.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在*n*次观察中，其中`x`次是`1`，其余的是`0`，一个`Beta(α, β)`分布将更新为`Beta(α + x, β + n - x)`。粗略来说，在一次更新中，*α*应该根据观察到的样本数递增，而*β*应该根据零样本的数量递增。
- en: From this newly updated distribution, which reflects that data that we can observe,
    the new estimation of *p*, which is the mean of the distribution, can be computed
    as α / (α + β). We said that we typically start out by modeling *p* using a uniform,
    or Beta(1, 1), distribution; the expectation of *p*, in this case, is 1 / (1 +
    1) = 0.5\. As we see more and more samples from the Bernoulli distribution with
    the true value of *p*, we will update this Beta distribution that we are using
    to better model *p* to reflect which values of *p* are now likely, given those
    samples.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个新的更新分布中，它反映了我们可以观察到的数据，新的*p*估计值，即分布的均值，可以计算为α / (α + β)。我们说过，通常我们从一个均匀分布，或者Beta(1,
    1)，来建模*p*；在这种情况下，*p*的期望值是1 / (1 + 1) = 0.5。当我们看到越来越多来自伯努利分布的样本，且真实的*p*值被确认后，我们将更新我们使用的Beta分布，从而更好地建模*p*，使其反映出根据这些样本，目前最可能的*p*值。
- en: 'Let''s consider a visual illustration to tie all of this together. Consider
    a Bernoulli distribution with *p = 0.9*, which we consider unknown to us. We can,
    again, only draw samples from this distribution and we''d like to use the Bayesian
    update rule described previously to model our belief about *p*. Say that at each
    timestep out of 1,000 timesteps, we can draw one sample from the distribution.
    Our observations are as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个可视化的例子来把这一切联系起来。考虑一个伯努利分布，其中*p = 0.9*，我们假设这个值对我们是未知的。我们同样只能从这个分布中抽样，并且希望使用前面描述的贝叶斯更新规则来建模我们对*p*的信念。假设在每个时刻中，我们从这个分布中抽取一个样本，总共进行1,000次时刻。我们的观察结果如下：
- en: At timestep 0, we don't have any observations yet.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第0时刻，我们还没有任何观察。
- en: At timestep 5, we have all observations being ones, and none being zero.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第5时刻，我们的所有观察都是1，且没有零观察。
- en: At timestep 10, we have 9 positive observations and 1 zero.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第10时刻，我们有9个正向观察和1个零观察。
- en: At timestep 20, we have 18 positive observations and 2 zeros.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第20时刻，我们有18个正向观察和2个零观察。
- en: At timestep 100, we have 91 positive observations and 9 zeros.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第100时刻，我们有91个正向观察和9个零观察。
- en: At timestep 1,000, we have 892 positive observations and 108 zeros.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第1,000时刻，我们有892个正向观察和108个零观察。
- en: First of all, we can see that the fraction of positive observations is roughly
    equal to the true value of *p = 0.9*, which is unknown to us. Additionally, we
    don't have any prior knowledge on this value of *p*, so we choose to model it
    using Beta(1, 1). This corresponds to the horizontal probability density function
    that we have in the upper-left panel of the following plot.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到正向观察的比例大致等于未知的真实值*p = 0.9*。此外，我们对*p*的值没有任何先验知识，因此我们选择使用Beta(1, 1)来建模它。这对应于下图左上面板中的水平概率密度函数。
- en: For the rest of the panels, we use the Bayesian update rule to compute a Beta
    distribution with new parameters to fit the data we observe better. The blue lines
    are the probability density function of *p*, indicating how likely it is that
    *p* is equal to one specific value between 0 and 1, given the observations we
    have.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其余的面板，我们使用贝叶斯更新规则来计算一个新的Beta分布，以便更好地拟合我们观察到的数据。蓝色的线表示*p*的概率密度函数，显示了根据我们拥有的观察数据，*p*等于某个介于0和1之间特定值的可能性。
- en: 'At timestep 5, all of our observations are one, so our belief gets update to
    reflect that the probability that *p* is some value close to 1 is very large.
    This is indicated by an increase in probability mass on the right-hand side of
    the plot. At timestep 10, one zero observation occurs, so the probability that
    *p* is exactly 1 decreases, giving more mass to the values close to but below
    1\. In latter timesteps, the curve grows tighter and tighter, indicating that
    the model is becoming more and more confident about what values *p* can take on.
    Finally, at timestep 1,000, the function peaks around the point 0.9 and nowhere
    else, indicating that it is extremely confident *p is roughly 0.9*:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5时间步，我们的所有观察值都是1，因此我们的信念会更新，反映出*p*接近1的概率非常大。这通过图表右侧概率质量的增加得以体现。到第10时间步时，出现了一个零值观察，因此*p*恰好为1的概率下降，将更多的概率质量分配给接近但小于1的值。在后续的时间步中，曲线越来越紧密，表明模型对*p*可能取的值越来越有信心。最终，在第1000时间步时，函数在0.9附近达到峰值，并且没有其他地方的峰值，表明它非常有信心*p大约为0.9*：
- en: '![Figure 8.17: A visual illustration of the Bayesian updating process'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.17：贝叶斯更新过程的视觉说明'
- en: '](img/B16182_08_17.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_08_17.jpg)'
- en: 'Figure 8.17: A visual illustration of the Bayesian updating process'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17：贝叶斯更新过程的视觉说明
- en: In our example, Beta distributions are used to model the unknown parameter of
    a Bernoulli distribution; it is important that Beta distributions are used because
    when Bayes' theorem is applied, the prior probability associated with a Beta distribution,
    combined with the likelihood probability of a Bernoulli distribution, simplifies
    the math significantly, allowing the posterior to become a different Beta distribution
    with newly updated parameters. If another distribution aside from Beta were to
    be used, the formula would not be simplified in such a way. The Beta distribution
    is therefore called the *conjugate prior* of the Bernoulli distribution. In Bayesian
    probability, when we'd like to model the unknown parameters of a given distribution,
    the conjugate prior of that distribution should be used so that the math will
    work out.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，Beta分布用于对伯努利分布中的未知参数进行建模；使用Beta分布是非常重要的，因为当应用贝叶斯定理时，Beta分布的先验概率与伯努利分布的似然概率结合起来，显著简化了数学计算，使后验分布成为一个不同的Beta分布，并更新了参数。如果使用Beta以外的其他分布，公式将不会以这种方式简化。因此，Beta分布被称为伯努利分布的*共轭先验*。在贝叶斯概率中，当我们希望对给定分布的未知参数进行建模时，应使用该分布的共轭先验，这样数学推导才会顺利进行。
- en: If this process is still confusing to you, don't worry, as most of the theory
    behind Bayesian updating and conjugate priors has already been worked out for
    common probability distributions. For our purposes, we simply need to remember
    the update rule for the Bernoulli/Beta distribution that we just discussed.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个过程仍然让你感到困惑，不用担心，因为贝叶斯更新和共轭先验背后的大部分理论已经为常见概率分布得到了很好的推导。对于我们的目的，我们只需要记住我们刚才讨论过的伯努利/Beta分布的更新规则。
- en: Note
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For those of you who are interested, please feel free to consult the following
    material from MIT, which further introduces conjugate priors of various probability
    distributions: [https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading15a.pdf](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading15a.pdf).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 对于感兴趣的读者，欢迎查阅以下来自麻省理工学院的材料，进一步介绍各种概率分布的共轭先验：[https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading15a.pdf](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading15a.pdf)。
- en: 'So far, we have learned how to model the unknown parameter, *p*, of a Bernoulli
    distribution in a Bayesian fashion when given data that we can observe. In the
    next section, we will finally connect this topic back to our original point of
    discussion: the Thompson Sampling algorithm.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了如何在给定可观察数据的情况下，以贝叶斯方式对伯努利分布中的未知参数*p*进行建模。在下一节中，我们将最终将这个话题与我们最初的讨论点——汤普森采样算法——连接起来。
- en: The Thompson Sampling Algorithm
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汤普森采样算法
- en: Consider the Bayesian technique of modeling *p* that we just learned about in
    the context of an MAB problem with Bernoulli reward distributions. We now have
    a way to quantify, probabilistically, our belief about the value of *p*, given
    the reward samples we have observed from the corresponding arm. From here, we
    can simply employ a greedy strategy again and choose the arm with the greatest
    expectation of *p*, which is, again, computed as *α / (α + β)*, where *α* and
    *β* are the running parameters of the current Beta distribution modeling *p*.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们刚刚在Bernoulli奖励分布的MAB问题背景下学习的贝叶斯技术来建模*p*。我们现在有了一种方法，可以通过概率的方式量化我们对*p*值的信念，前提是我们已经观察到了来自相应臂的奖励样本。从这里，我们可以再次简单地采用贪婪策略，选择具有最大期望值的臂，即再次计算为*α
    / (α + β)*，其中*α*和*β*是当前Beta分布的运行参数，用于建模*p*。
- en: Instead, to implement Thompson Sampling, we draw a sample from each of the Beta
    distributions that model the *p* parameter of each of the Bernoulli distributions
    and select the maximal one. In other words, each arm in the bandit problem has
    a Bernoulli reward distribution whose parameter, *p*, is being modeled by some
    Beta distribution. We sample from each of these Beta distributions and pick the
    arm with the highest-valued sample.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，为了实现Thompson采样，我们从每个Beta分布中抽取一个样本，这些Beta分布建模了每个Bernoulli分布的*p*参数，然后选择最大的样本。换句话说，带宽问题中的每个臂都有一个Bernoulli奖励分布，其参数*p*由某个Beta分布建模。我们从这些Beta分布中抽样，并选择样本值最高的臂。
- en: 'Let''s say that, in the class object syntax that we have been using to implement
    MAB algorithms, we store the running values of alpha and beta used by a Beta distribution
    in the `temp_beliefs` attribute to model the parameter, *p*, of each arm. The
    logic of Thompson Sampling can be applied as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在我们用来实现MAB算法的类对象语法中，我们将用于Beta分布的α和β的运行值存储在`temp_beliefs`属性中，以建模每个臂的*p*参数。Thompson采样的逻辑可以如下应用：
- en: '[PRE43]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Different from Greedy or UCB, to estimate the true value of *p* for each arm,
    we draw a random sample from the corresponding Beta distribution whose parameters
    have been updated by the Bayesian updating rule throughout the process (as can
    be seen in the `draws` variable). To choose an arm to pull, we simply identify
    the arm that has the best sample.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 与贪婪算法或UCB不同，为了估计每个臂的真实值*p*，我们从相应的Beta分布中随机抽取一个样本，该分布的参数通过贝叶斯更新规则在整个过程中不断更新（如`draws`变量所示）。为了选择一个臂进行拉动，我们只需识别出具有最佳样本的臂。
- en: 'Two immediate questions come to mind: first, why is this sampling process a
    good way to estimate the reward expectation of each arm, and second, how does
    the technique address the trade-off between exploration and exploitation?'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个直接的问题：首先，为什么这个采样过程是估计每个臂奖励期望的好方法；其次，这个技术是如何解决探索与开发之间的权衡问题的？
- en: When we sample from each of the Beta distributions, the more likely *p* is equal
    to a given value, the more likely that value will be chosen as our sample – this
    is simply the nature of a probability distribution. So, in a way, a sample from
    a distribution is an approximation of the quantity that the distribution models.
    This is why samples from the Beta distributions can be justifiably used as estimations
    of the true value of *p* of each of the Bernoulli distributions.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从每个Beta分布中抽样时，*p*值越可能等于某个给定值，那么这个值作为我们的样本被选中的可能性就越大——这就是概率分布的本质。所以，从某种意义上讲，分布的样本是对该分布所建模的量的近似。这就是为什么从Beta分布中抽取的样本可以合理地用作每个Bernoulli分布中*p*真实值的估计。
- en: With that said, when the current distribution representing our belief about
    parameter *p* of a given Bernoulli is flat and does not have a sharp peak (as
    opposed to the one in the final panel of the preceding visualization), it indicates
    that we still have a lot of uncertainty about what value *p* might be, which is
    why many numbers are given more probability mass than in a distribution with a
    single sharp peak. When a distribution is relatively flat, the samples drawn from
    it are likely to be dispersed across the range of the distribution, as opposed
    to surrounding one single region, again indicating our uncertainty about the true
    value. All of this is to say that even though samples can be used as approximations
    of a given quantity, the accuracy of those approximations depends on how flat
    the modeling distribution is (and therefore, ultimately, how certain our belief
    is about the true value).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，当当前表示我们对给定Bernoulli参数*p*的信念的分布是平坦的，且没有尖峰时（与前面可视化的最后一面图不同），这表明我们对*p*的具体值仍然有很多不确定性，这就是为什么在这种分布中，许多数值被赋予比单一尖峰分布更多的概率质量。当分布相对平坦时，从中抽取的样本很可能会分散在分布的范围内，而不是集中在某个单一区域，这再次表明我们对真实值的认识存在不确定性。所有这些都意味着，尽管样本可以作为给定量的近似值，但这些近似的准确性取决于建模分布的平坦程度（因此，最终取决于我们对真实值的信念有多确定）。
- en: This fact directly helps us address the exploration-exploitation dilemma. When
    samples for *p*'s are drawn from distributions with single sharp peaks, they are
    more likely to be very close to the true values of the corresponding *p*'s, so
    choosing the arm with the highest sample is equivalent to choosing the arm with
    the highest *p* (or expected reward). When a distribution is still flat, the values
    of the samples drawn from it are likely to be volatile and might, therefore, take
    on large values. If, somehow, an arm is chosen because of this reason, this means
    that we are not certain enough about the value of *p* for this arm, and it's therefore
    worth exploring.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事实直接帮助我们解决了探索-开发困境。当从具有单一尖峰的分布中抽取*p*的样本时，它们更可能非常接近对应*p*的真实值，因此选择样本值最高的臂相当于选择具有最高*p*（或期望奖励）的臂。当分布仍然平坦时，从中抽取的样本值可能会波动，因此可能会取较大的值。如果因这个原因选择了某个臂，这意味着我们对该臂的*p*值还不够确定，因此值得进行探索。
- en: 'Thompson Sampling, by sampling from the modeling distributions, offers an elegant
    method of balancing exploitation and exploration: if we are certain with our beliefs
    about each arm, picking the best sample is likely to be equivalent to picking
    the actual optimal arm; if we are not certain about an arm enough that its corresponding
    sample has the best value, exploring it will be beneficial.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Thompson采样通过从建模分布中抽样，提供了一种平衡开发与探索的优雅方法：如果我们对每个臂的信念非常确定，选择最佳样本可能就等同于选择实际的最优臂；如果我们对某个臂的信念还不够确定，以至于其对应样本没有最佳值，进行探索将是有益的。
- en: As we will see in the upcoming exercise, the actual implementation of Thompson
    Sampling is quite straightforward, and we won't need to include much of the theoretical
    Bayesian probability that we have discussed in the implementation.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在接下来的练习中看到的，Thompson采样的实际实现非常直接，我们在实现中不需要包含我们之前讨论的太多理论贝叶斯概率。
- en: 'Exercise 8.03: Implementing the Thompson Sampling Algorithm'
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.03：实现Thompson采样算法
- en: 'In this exercise, we will be implementing the Thompson Sampling algorithm.
    As always, we will be implementing the algorithm as a Python class and subsequently
    applying it to the Bernoulli three-arm bandit problem. Specifically, we will walk
    through the following steps:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实现Thompson采样算法。像往常一样，我们将以Python类的形式实现该算法，并随后将其应用于Bernoulli三臂赌博机问题。具体来说，我们将按以下步骤进行操作：
- en: 'Create a new Jupyter Notebook and import `NumPy`, `Matplotlib`, and the `Bandit`
    class from the `utils.py` file included in the code repository for this chapter:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Jupyter Notebook，导入`NumPy`、`Matplotlib`以及来自代码库中`utils.py`文件的`Bandit`类：
- en: '[PRE44]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Declare a Python class named `BernoulliThompsonSampling` (indicating that the
    class will be implementing the Bayesian update rule for Bernoulli/Beta distributions)
    with the following initialization method:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明一个名为`BernoulliThompsonSampling`的Python类（表示该类将实现Bernoulli/Beta分布的贝叶斯更新规则），并使用以下初始化方法：
- en: '[PRE45]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Remember that in Thompson Sampling, we maintain a running belief about *p* of
    each Bernoulli arm using a Beta distribution whose two parameters are updated
    according to the update rule. Therefore, we only need to keep track of the running
    values of these parameters; the `temp_beliefs` attribute contains this information
    for each of the arms, whose default value is (*1, 1*).
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，在汤普森采样中，我们通过Beta分布维持每个伯努利臂的*p*的运行信念，这两个参数会根据更新规则更新。因此，我们只需要追踪这些参数的运行值；`temp_beliefs`属性包含了每个臂的这些信息，默认值为(*1,
    1*)。
- en: 'Implement the `decide()` method, using the `np.random.beta` function from NumPy
    to draw a sample from a Beta distribution, like so:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`decide()`方法，使用NumPy中的`np.random.beta`函数从Beta分布中抽取样本，如下所示：
- en: '[PRE46]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Here, we can see that instead of computing the mean reward or its upper confidence
    bound, we simply draw a sample from each of the Beta distributions defined by
    the parameters stored in the `temp_beliefs` attribute.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，我们并不是计算均值奖励或其上置信边界，而是从每个由`temp_beliefs`属性存储的Beta分布中抽取样本。
- en: Finally, we pick the arm that corresponds to the maximum sample.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们选择与最大样本对应的臂。
- en: 'In the same code cell, implement the `update()` method for the class. In addition
    to appending the most recent reward to the history of the appropriate arm, we
    need to implement the logic of the update rule:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一代码单元中，为类实现`update()`方法。除了将最新的奖励附加到相应臂的历史记录中，我们还需要实现更新规则的逻辑：
- en: '[PRE47]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Remember that the first parameter, α, should be incremented once for every sample
    we observe, while β should be incremented if the sample is zero. The preceding
    code implements this logic.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，第一个参数α应该随着我们观察到的每个样本而递增，而β应该在样本为零时递增。前面的代码实现了这一逻辑。
- en: 'Next, set up the familiar Bernoulli three-arm bandit problem and apply an instance
    of the Thompson Sampling class implementation to it to plot out the cumulative
    regret in that single experiment:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，设置熟悉的伯努利三臂赌博机问题，并将汤普森采样类的实例应用于该问题，以绘制单次实验中的累积遗憾：
- en: '[PRE48]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following graph will be produced:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将生成以下图形：
- en: '![Figure 8.18: Sample cumulative regret from Thompson Sampling'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.18：汤普森采样的样本累积遗憾'
- en: '](img/B16182_08_18.jpg)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_18.jpg)'
- en: 'Figure 8.18: Sample cumulative regret from Thompson Sampling'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.18：汤普森采样的样本累积遗憾
- en: This regret plot is better than the one we obtained with UCB but worse than
    the one from Greedy. The plot will be used in conjunction with the pulling history
    in the next step for further analysis. Let's analyze the pulling history further.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个遗憾图比我们用UCB得到的要好，但比用贪心算法得到的要差。该图将在下一步中与拉取历史一起使用，用于进一步分析。我们来进一步分析拉取历史。
- en: 'Print out the pulling history:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出拉取历史：
- en: '[PRE49]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output will be as follows:'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE50]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As you can see, the algorithm was able to identify the optimal arm but deviated
    to arms 1 and 2 from time to time. However, the exploration frequency decreased
    as time went on, indicating that the algorithm was growing more and more certain
    about its beliefs (in other words, each running Beta distribution was consolidating
    around a single peak). This is the typical behavior of Thompson Sampling.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，算法能够识别出最佳臂，但偶尔会偏向臂1和臂2。然而，随着时间的推移，探索的频率减少，这表明算法对其信念越来越确定（换句话说，每次运行的Beta分布都集中在一个峰值周围）。这是汤普森采样的典型行为。
- en: 'As we have learned, just considering one single experiment might not be sufficient
    for the analysis of an algorithm. To facilitate a fuller analysis on the performance
    of Thompson Sampling, let''s set up the usual repeated experiments:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们所学，考虑单个实验可能不足以分析算法的表现。为了对汤普森采样的表现进行更全面的分析，我们来设置常规的重复实验：
- en: '[PRE51]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This will generate the following distribution of regret:'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下的遗憾分布：
- en: '![Figure 8.19: Distribution of cumulative regret from Thompson Sampling'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.19：汤普森采样的累积遗憾分布'
- en: '](img/B16182_08_19.jpg)'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_19.jpg)'
- en: 'Figure 8.19: Distribution of cumulative regret from Thompson Sampling'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.19：汤普森采样的累积遗憾分布
- en: Here, we can see that Thompson Sampling is able to minimize the cumulative regret
    across all experiments by a large margin compared to other algorithms (the maximum
    value in the distribution is only `10`).
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，汤普森采样能够将所有实验中的累积遗憾大幅度地最小化，相比其他算法（分布中的最大值仅为`10`）。
- en: 'To quantify this claim, let''s print out the mean and max regret from these
    experiments:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了量化这一说法，我们来打印出这些实验中的平均和最大后悔值：
- en: '[PRE52]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be as follows:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE53]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This is significantly better than the counterpart statistics from other algorithms:
    Greedy had `8.66` and `62` while UCB had `18.78` had `29`.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这显著优于其他算法的对比统计：Greedy 算法的值为`8.66`和`62`，而 UCB 的值为`18.78`和`29`。
- en: Note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2UCbZXw](https://packt.live/2UCbZXw).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2UCbZXw](https://packt.live/2UCbZXw)。
- en: You can also run this example online at [https://packt.live/37oQrTz](https://packt.live/37oQrTz).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/37oQrTz](https://packt.live/37oQrTz) 上在线运行这个示例。
- en: Thompson Sampling is also the last of the common MAB algorithms that will be
    discussed in this book. In the next and final section of this chapter, we will
    briefly consider a common variant of the classical MAB problem, namely the contextual
    bandit problem.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Thompson Sampling 也是本书中将要讨论的最后一个常见的 MAB 算法。在本章的下一节也是最后一节中，我们将简要讨论经典 MAB 问题的一个常见变种——即上下文
    bandit 问题。
- en: Contextual Bandits
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文 bandit
- en: 'In the classical bandit problem, the reward from pulling an arm solely depends
    on the reward distribution associated with that arm, and our goal is to identify
    the optimal arm as soon as possible and keep pulling it until the end of the process.
    A contextual bandit problem, on the other hand, includes an additional element
    to the problem: the environment, or the context. Similar to its definition in
    the context of reinforcement learning, an environment contains all of the information
    about the problem settings, the state of the world at any given time, as well
    as other agents that might be participating in the same environment as our player.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的 bandit 问题中，拉取某个 arm 所获得的奖励完全依赖于该 arm 的奖励分布，我们的目标是尽早识别出最优 arm，并在整个过程中一直拉取它。而上下文
    bandit 问题则在此基础上增加了一个额外的元素：环境或上下文。类似于在强化学习中的定义，环境包含关于问题设置的所有信息、在任何给定时间的世界状态，以及可能与我们玩家在同一环境中参与的其他代理。
- en: Context That Defines a Bandit Problem
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 bandit 问题的上下文
- en: In the traditional MAB problem, we only care about what potential reward each
    arm will return if we pull it at any time. In contextual bandits, we are provided
    with the contextual information about the environment that we are operating in,
    and depending on the setting, the reward distribution of an arm might vary. In
    other words, the choice of which arm to pull that we make at every step should
    be dependent on the state of the environment, or the context.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的 MAB 问题中，我们只关心每次拉取某个 arm 时，它会返回什么潜在奖励。而在上下文 bandit 中，我们会提供有关我们所操作环境的上下文信息，并且根据设置的不同，某个
    arm 的奖励分布可能会有所变化。换句话说，我们在每一步所做的拉取决策应该依赖于环境的状态，或者说是上下文。
- en: 'This setting complicates the model that we have been working with, as now,
    we will need to consider the quantities that we are interested in as conditional
    probabilities: given the context we are seeing, what is the probability that arm
    0 is the optimal arm, for example? In a contextual bandit problem, the context
    might have a minor role in the decision process of our algorithms, or it could
    be the main factor that drives the algorithm''s decisions.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置使得我们正在使用的模型变得复杂，因为现在我们需要考虑我们感兴趣的量作为条件概率：给定我们看到的上下文，假设 arm 0 是最优 arm 的概率是多少？在上下文
    bandit 问题中，上下文可能在我们算法的决策过程中扮演次要角色，也可能是驱动算法决策的主要因素。
- en: A real-world example is in order. We mentioned at the beginning of this chapter
    that recommender systems are a common application of the MAB problem, where, for
    each user who has just arrived at a website, the system needs to decide which
    kind of ads/recommendations would maximize the probability that the user would
    be interested in it. Every user has their own preferences and liking, and those
    factors might very well play an important role in helping the system decide whether
    they will be interested in a specific ad or not.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子是推荐系统。我们在本章开头提到，推荐系统是 MAB 问题的常见应用，在每个刚刚访问网站的用户面前，系统需要决定哪种广告/推荐能够最大化用户对此感兴趣的概率。每个用户都有自己的偏好和喜好，而这些因素可能在帮助系统决定用户是否会对某个广告感兴趣时发挥重要作用。
- en: For example, dog owners will be significantly more likely to click on dog toy
    advertisements than the average user and potentially less likely to click on cat
    food advertisements. This information about the users is a part of the context
    of the MAB problem, which is the current recommender system we are considering.
    Other factors might include their profiles, buying/viewing history, and so on.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，狗主人点击狗玩具广告的概率将明显高于普通用户，且可能点击猫粮广告的概率较低。这些关于用户的信息是MAB问题的一部分，MAB是我们当前所考虑的推荐系统。其他因素可能包括他们的个人资料、购买/浏览历史等等。
- en: 'Overall, in a contextual bandit problem, we need to consider the expectation
    of reward of each arm/decision and do so while keeping the current context that
    we are in in mind. Now, let''s start talking about the contextual bandit problem
    that we will be solving in the upcoming activity; it is also a problem that we
    have mentioned a couple of times throughout this book: queueing bandits.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，在上下文强盗问题中，我们需要考虑每个决策臂/决策的奖励期望，并在此过程中始终考虑当前的上下文。现在，让我们开始讨论即将解决的上下文强盗问题；这也是本书中多次提到的一个问题：排队强盗。
- en: Queueing Bandits
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排队强盗
- en: 'Our bandit problem has the following elements:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的强盗问题包含以下几个要素：
- en: We start out with a queue of customers, each of whom belongs to one of a predetermined
    set of customer classes. For example, let's say our queueing contains 300 customers
    in total. Among these customers, 100 customers belong to class 0, another 100
    belong to class 1, and the other 100 belong to class 2.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从一个客户队列开始，每个客户都属于预定的某个客户类别。例如，假设我们的队列总共有300个客户。在这些客户中，100个客户属于类别0，另外100个属于类别1，其余100个属于类别2。
- en: We also have a single server that is to serve all of these customers in a specific
    order. Only one customer can be served at any given time, and when a customer
    is being served, the remaining ones in the queue will have to wait until it is
    their turn to be served. Once a customer has been served, they leave the queue
    completely.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要一个单一的服务器来按照特定顺序为所有这些客户提供服务。每次只能为一个客户提供服务，当一个客户正在接受服务时，队列中的其他客户必须等待，直到轮到他们为止。一旦一个客户被服务完毕，他们就会完全离开队列。
- en: Each customer has a specific job length, that is, the amount of time that it
    will take for the server to begin and end the customer's service. The job length
    of a customer belonging to class i (i is 0, 1, or 2) is a random sample drawn
    from an exponential distribution with parameter λi, called the rate parameter.
    The larger the parameter is, the more likely it is that a sample drawn from the
    distribution is small. In other words, the expected value of a sample is inversely
    proportional to the rate parameter. (In fact, the mean of an exponential distribution
    is 1, divided by its rate parameter.)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户都有一个特定的工作时长，即服务器开始并结束客户服务所需的时间。属于类别i（i为0、1或2）的客户的工作时长是从参数为λi的指数分布中随机抽取的样本，称为速率参数。参数越大，从该分布中抽取到的小样本的概率就越大。换句话说，样本的期望值与速率参数成反比。（实际上，指数分布的均值是1除以其速率参数。）
- en: Note
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are interested in learning more about the exponential distribution,
    you can find more information here: [https://mathworld.wolfram.com/ExponentialDistribution.html](https://mathworld.wolfram.com/ExponentialDistribution.html).
    For our purposes, we only need to know that the expected value of an exponentially
    distribution random variable is inversely proportional to the rate parameter.'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于指数分布的信息，可以在这里找到更多内容：[https://mathworld.wolfram.com/ExponentialDistribution.html](https://mathworld.wolfram.com/ExponentialDistribution.html)。就我们来说，我们只需要知道，指数分布随机变量的期望值与其速率参数成反比。
- en: When a customer is being served, all the customers remaining in the queue will
    contribute to the total cumulative waiting time that we will incur at the end
    of the process. Our goal, as the queue coordinator, is to come up with a way of
    ordering these customers so that the total cumulative waiting time of all of the
    customers at the end of the process is minimized. It is known that the optimal
    ordering to minimize this total cumulative waiting time is shortest job first,
    where out of the remaining customers at any given time, the one with the shortest
    job length should be chosen.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个顾客正在被服务时，队列中剩余的所有顾客将贡献到我们在整个过程结束时所产生的总累计等待时间。作为队列协调员，我们的目标是提出一种方法来安排这些顾客的顺序，以便在整个过程结束时最小化所有顾客的总累计等待时间。已知，最小化总累计等待时间的最佳顺序是“最短作业优先”，也就是说，在任何给定时间，剩余顾客中应该选择作业时间最短的顾客。
- en: With this, we can see the parallel between this queueing problem and the classical
    MAB problem. If the true rate parameter that characterizes the job length distribution
    of customers belonging to a given class is not known, we need to find a way to
    estimate that quantity by observing the job length of a few sample customers from
    each class. The sooner we can identify and converge on processing the customers
    with the highest rate parameter, the lower our total cumulative waiting time at
    the end will be. Here, pulling an arm is equivalent to picking a customer of a
    given class to serve next, and the negative reward (or cost) that we need to minimize
    at the end is the cumulative waiting time of the whole queue.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以看到队列问题与经典的多臂赌博（MAB）问题之间的相似性。如果我们不知道表征某个类别顾客作业时间分布的真实率参数，我们需要通过观察每个类别中几个样本顾客的作业时间来估计这个参数。一旦我们能够尽早识别并集中处理具有最高率参数的顾客，我们的总累计等待时间将尽可能低。在这里，拉取一个“臂”相当于选择一个给定类别的顾客来接下来服务，而我们需要最小化的负奖励（或成本）是整个队列的累计等待时间。
- en: As a contextual bandit problem, a queueing problem also contains some extra
    context at each step that needs to be considered in the decision-making process.
    For example, we mentioned that in each experiment, we start out with a queue of
    finitely many customers (specifically, 100 customers for each of three different
    classes), and once a customer is processed, they will leave the queue forever.
    This means each of the three "arms" of our bandit problem has to be pulled exactly
    100 times, and an algorithm needs to find a way to arrange the order of these
    pulls optimally.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个上下文赌博问题，队列问题在每个步骤中也包含一些额外的上下文，需要在决策过程中加以考虑。例如，我们提到，在每次实验中，我们从一个有限数量的顾客队列开始（每个三种不同类别的顾客各100个），一旦一个顾客被处理完毕，他们将永远离开队列。这意味着我们的赌博问题的三个“臂”每个都必须被拉取100次，而算法需要找到一种方法来优化这些拉取的顺序。
- en: In the next section, the API for the queueing bandit problem we have provided
    for you will be discussed.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论为您提供的队列赌博问题的API。
- en: Working with the Queueing API
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用队列API
- en: To have the problem defined via an API, make sure to download the two following
    files from the code repository for this chapter, `utils.py`, which contains the
    API for traditional bandit problems that we have been using, as well as queueing
    bandit problems, and `data.csv`, which includes the input data that will be used
    for our queueing experiments.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过API定义该问题，请确保从本章的代码库中下载以下两个文件：`utils.py`，它包含我们一直使用的传统赌博问题和队列赌博问题的API，以及`data.csv`，它包含将用于队列实验的输入数据。
- en: 'Now, different from the API that we have been using, we need to do the following
    to interact with a queueing bandit. First, from the `utils.py` file, the `QueueBandit`
    class needs to be imported. An instance of this class is declared like so:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与我们一直在使用的API不同，我们需要执行以下操作来与队列赌博进行交互。首先，需要从`utils.py`文件中导入`QueueBandit`类。该类的实例声明如下：
- en: '[PRE54]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `filename` argument takes in the relative location of your code and the
    `data.csv` file, so that might change, depending on where your own notebook is.
    Unlike the `Bandit` class, because the `data.csv` file contains data generated
    from multiple experiments with different randomly chosen parameters, we don''t
    need to declare those specific details ourselves. In fact, what we mentioned previously
    applies to all experiments that we will be using: in each experiment, we have
    the input of a 300-customer queue belonging to three different customer classes
    with varying unknown rate parameters.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`filename` 参数接受你代码和 `data.csv` 文件的相对位置，因此可能会根据你自己笔记本的位置发生变化。与 `Bandit` 类不同，由于
    `data.csv` 文件包含来自多个实验的数据，这些实验使用不同的随机选择参数，因此我们不需要自己声明这些具体细节。事实上，我们之前提到的内容适用于我们将使用的所有实验：在每个实验中，我们都有一个由
    300 个客户组成的队列，这些客户属于三个不同的客户类别，并具有不同的未知速率参数。'
- en: This API also offers us the `repeat()` method so that we have an algorithm interact
    with the queueing problem, which similarly takes in a class implementation of
    that algorithm and any potential parameters as its two main arguments. The method
    will run the input algorithm through many different starting queues (which, again,
    were generated with different rate parameters for the three classes) and return
    the cumulative waiting time for each of those queues. The method also has an argument
    named `visualize_cumulative_times`, which, if set to `True`, will visualize the
    distribution of that cumulative waiting time in a histogram.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 API 还提供了 `repeat()` 方法，使我们能够使用某个算法与排队问题进行交互，该方法同样接受该算法的类实现和任何可能的参数作为其两个主要参数。该方法会通过许多不同的初始队列运行输入算法（这些队列再次是通过不同速率参数生成的三类客户队列），并返回每个队列的累计等待时间。该方法还有一个名为
    `visualize_cumulative_times` 的参数，如果设置为 `True`，将会以直方图形式可视化累计等待时间的分布。
- en: 'A call to this method should look as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此方法应该如下所示：
- en: '[PRE55]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Finally, the last difference that we need to keep in mind is the requirement
    for algorithm implementations. The class implementation of an algorithm should
    have an `update()` method that acts the same way as we have become familiar with
    (it should take in the index of an arm (or of a customer class) and the most recent
    corresponding cost (or job length) and update whatever appropriate information
    that the algorithm keeps track of).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要牢记的最后一个区别是对算法实现的要求。算法的类实现应该具有一个 `update()` 方法，该方法的作用与我们已经熟悉的相同（它应该接受一个臂的索引（或一个客户类别的索引）和最相关的最新成本（或工作长度），并更新该算法所跟踪的任何适当信息）。
- en: More importantly, the `decide()` method should now take in an argument that
    indicates how many customers of each class we have left in the queue at any given
    time, stored in a three-item Python list. Remember that we always start out with
    a queue consisting of 100 customers for each class, so the list at the beginning
    will be `[100, 100, 100]`. As customers are chosen by our algorithms and served,
    this list of customer numbers will be updated accordingly. This is the context
    that our algorithm needs to keep in mind while making its decisions; obviously,
    it cannot choose to serve a customer from class 1 next, for example, if there
    is no class-1 customer left in the queue. Finally, the `decide()` method should
    return the index of the class that should be chosen to serve, similar to what
    we had with the traditional MAB problem.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，`decide()` 方法现在应该接受一个参数，该参数指示在任何给定时间队列中每个类别剩余的客户数量，存储在一个包含三个元素的 Python
    列表中。记住，我们总是从一个包含每个类别 100 个客户的队列开始，所以开始时的列表将是 `[100, 100, 100]`。随着客户被我们的算法选择并服务，这个客户数量的列表将相应更新。这是我们的算法在做出决策时需要牢记的上下文；显然，如果队列中没有剩余的类别
    1 客户，算法就不能选择下一个服务类别 1 的客户。最后，`decide()` 方法应该返回应该选择服务的类别的索引，类似于传统的多臂老虎机问题。
- en: 'And that is what we need to know about this queueing bandit problem. While
    marking the end of the materials covered in this chapter, this section also prepares
    us for the upcoming activity: implementing various algorithms to solve the queueing
    bandit problem.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要了解的排队盗贼问题。在标志着本章内容结束的同时，本节也为我们即将进行的活动做好了准备：实现各种算法来解决排队盗贼问题。
- en: 'Activity 8.01: Queueing Bandits'
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 8.01：排队盗贼问题
- en: As mentioned previously, a queueing problem where the true rate parameters of
    the customer job lengths are unknown can be framed as a MAB problem. In this activity,
    we will be reimplementing the algorithms that we have learned about in this chapter
    in the context of queueing and comparing their performance. This activity will,
    therefore, reinforce the concepts that we have discussed throughout this chapter,
    while giving us the opportunity to tackle a contextual bandit problem.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，客户作业长度的真实速率参数未知的排队问题可以被框架化为一个MAB问题。在这个活动中，我们将重新实现本章中学习到的算法，并在排队问题的背景下比较它们的表现。因此，本活动将加深我们对本章所讨论概念的理解，并使我们有机会解决一个上下文强盗问题。
- en: 'With that, let''s start the activity by following these steps:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些步骤，让我们开始活动：
- en: Create a new Jupyter Notebook and in its first code cell, import `NumPy` and
    the QueueBandit class from `utils.py`. Be sure to set the random seed of NumPy
    to `0`.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Jupyter Notebook，在其第一个代码单元格中，导入`NumPy`和`utils.py`中的QueueBandit类。务必将NumPy的随机种子设置为`0`。
- en: Declare an instance of this class using the code included in the preceding text.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前文代码声明该类的实例。
- en: In a new code cell, implement the Greedy algorithm for this queueing problem
    and apply it to the bandit object using the code included in the preceding text.
    Along with the histogram of the cumulative waiting time distribution, print out
    the mean and max items among them.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新的代码单元格中，实现该排队问题的贪心算法，并使用前文提供的代码将其应用于强盗对象。除了显示累计等待时间分布的直方图外，还需要打印出其中的平均值和最大值。
- en: Again, the Greedy algorithm should choose the customer class that has the lower
    average job length out of the remaining classes at each iteration of a queue.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次，贪心算法应该选择在每次队列迭代中具有较低平均作业长度的客户类别。
- en: 'The output will be as follows:'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.20: Distribution of cumulative waiting time from Greedy'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.20：贪心算法的累计等待时间分布'
- en: '](img/B16182_08_20.jpg)'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_20.jpg)'
- en: '[PRE56]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In a new code cell, implement the Explore-then-commit algorithm for the problem.
    The class implementation of the algorithm should take in a parameter named `T`
    that specifies how may exploration rounds the algorithm should take in at the
    beginning of an experiment.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新的代码单元格中，实现该问题的Explore-then-commit算法。该算法的类实现应接受一个名为`T`的参数，指定在实验开始时算法应进行多少次探索轮次。
- en: Similar to the Greedy algorithm, apply Explore-then-commit with `T=2` to the
    bandit object. Compare the distribution of the cumulative waiting times, as well
    as the mean and max resulting from this algorithm, with what we have for Greedy.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于贪心算法，将`T=2`的Explore-then-commit算法应用于强盗对象。比较该算法产生的累计等待时间分布以及其结果中的平均值和最大值与贪心算法的结果。
- en: 'This will produce the following graph:'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 8.21: Distribution of cumulative waiting time from Explore-then-commit'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.21：Explore-then-commit的累计等待时间分布'
- en: '](img/B16182_08_21.jpg)'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_21.jpg)'
- en: '[PRE57]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In a new code cell, implement the Thompson Sampling algorithm for the problem.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新的代码单元格中，实现该问题的Thompson Sampling算法。
- en: To model an unknown rate parameter of an exponential distribution, a Gamma distribution
    should be used as the conjugate prior. A Gamma distribution is also parameterized
    by two numbers, α and β; their update rule with respect to a sample job length,
    *x*, is *α = α + 1* and *β = β + x*. At the beginning, both parameters should
    be initialized to `0`.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要对指数分布的未知速率参数进行建模，应使用伽马分布作为共轭先验。伽马分布也由两个参数α和β来参数化；它们关于样本作业长度 *x* 的更新规则是 *α =
    α + 1* 和 *β = β + x*。一开始，两个参数都应初始化为`0`。
- en: To draw a sample from a Gamma distribution, the `np.random.gamma()` function
    could be used, which takes in α and 1 / β. Similar to our logic for Greedy and
    Explore-then-commit, the class with the highest sampled rate should be chosen
    at each iteration.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要从伽马分布中抽取样本，可以使用`np.random.gamma()`函数，该函数接受α和1 / β。与我们的贪心算法和探索-再承诺算法的逻辑类似，应该在每次迭代中选择采样率最高的类别。
- en: Apply the algorithm to the bandit object and analyze its performance via the
    cumulative waiting times. Compare it to Greedy and Explore-then-commit.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将算法应用于强盗对象，并通过累计等待时间分析其性能。与贪心算法和探索-再承诺算法进行比较。
- en: 'The following plot will be produced:'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将生成以下图形：
- en: '![Figure 8.22: Distribution of cumulative waiting time from Thompson Sampling'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.22：Thompson Sampling的累计等待时间分布'
- en: '](img/B16182_08_22.jpg)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_22.jpg)'
- en: '[PRE58]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In contextual bandit problems, specialized algorithms are commonly developed.
    These algorithms are variants of common MAB algorithms, specifically designed
    to use the contextual information.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上下文老虎机问题中，通常会开发专门的算法。这些算法是常见MAB算法的变种，专门设计用于利用上下文信息。
- en: In a new code cell, implement an exploitative variant of Thompson Sampling where
    its logic is similar to Thompson Sampling at the beginning of each experiment,
    and solely exploits (like Greedy) by choosing the class with the lowest average
    job length when at least half of the customers have been served.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在一个新的代码单元中，实现汤普森采样的一个利用型变种，其逻辑在每个实验开始时类似于汤普森采样，并且在至少一半客户被服务后，完全依靠贪心策略（像贪婪算法那样），选择具有最低平均作业时长的类别。
- en: Apply the algorithm to the bandit object. Compare its performance with traditional
    Thompson Sampling, as well as the other algorithms we have implemented.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该算法应用于老虎机对象，并将其性能与传统汤普森采样以及我们已实现的其他算法进行比较。
- en: 'The plot will be as follows:'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图结果如下：
- en: '![Figure 8.23: Distribution of cumulative waiting time from modified Thompson
    Sampling'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.23：修改版汤普森采样的累计等待时间分布'
- en: '](img/B16182_08_23.jpg)'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_23.jpg)'
- en: 'Figure 8.23: Distribution of cumulative waiting time from modified Thompson
    Sampling'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23：修改版汤普森采样的累计等待时间分布
- en: 'The max and mean cumulative waiting times will be as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 最大和平均累计等待时间如下：
- en: '[PRE59]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: The solution to this activity can be found on page 734.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第734页找到。
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, the MAB problem and its motivation as a reinforcement learning
    and artificial intelligence problem were introduced. We explored a plethora of
    algorithms that are commonly used to solve the MAB problem, including the Greedy
    algorithm and its variants, UCB, and Thompson Sampling. Via these algorithms,
    we were exposed to unique insights and heuristics on how to balance exploration
    and exploitation (which is one of the most fundamental components of reinforcement
    learning) such as random exploration, optimism under uncertainty, or sampling
    from Bayesian posterior distributions.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，介绍了多臂老虎机（MAB）问题及其作为强化学习和人工智能问题的动机。我们探讨了许多常用于解决MAB问题的算法，包括贪婪算法及其变种、UCB和汤普森采样。通过这些算法，我们获得了如何平衡探索与利用（这是强化学习中最基本的组成部分之一）的独特见解和启发式方法，例如随机探索、不确定性下的乐观估计，或者从贝叶斯后验分布中采样。
- en: This knowledge was put into practice as we learned how to implement these algorithms
    from scratch in Python. During this process, we also examined the importance of
    analyzing MAB algorithms over many repeated experiments to obtain robust results.
    This procedure is integral for any analysis framework that involves randomness.
    Finally, in this chapter's activity, we applied our knowledge to a queueing bandit
    problem and learned how to modify MAB algorithms so that they fit a given contextual
    bandit.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这些知识得到了实践应用，我们学习了如何从零开始在Python中实现这些算法。在此过程中，我们还探讨了在多次重复实验中分析MAB算法的重要性，以获得稳健的结果。这个过程是任何涉及随机性的分析框架的核心。最后，在本章的活动中，我们将所学知识应用于排队老虎机问题，并学习了如何修改MAB算法，以使其适应给定的上下文老虎机。
- en: This chapter also marks the end of the topic of Markov decision problems, which
    spanned the last four chapters. From the next chapter onward, we will start looking
    at the exciting field of Deep Q Learning as a reinforcement learning framework.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 本章也标志着马尔可夫决策问题这一主题的结束，该主题涵盖了过去四章的内容。从下一章开始，我们将开始探索作为强化学习框架的深度Q学习这一激动人心的领域。
