- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Ways to Speed Up RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ é€Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•
- en: 'In ChapterÂ [8](ch012.xhtml#x1-1240008), you saw several practical tricks to
    make the deep Q-network (DQN) method more stable and converge faster. They involved
    basic DQN method modifications (like injecting noise into the network or unrolling
    the Bellman equation) to get a better policy, with less time spent on training.
    But in this chapter, we will explore another way to do this: tweaking the implementation
    details of the method to improve the speed of the training. This is a pure engineering
    approach, but itâ€™s also important since it is useful in practice.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­ï¼Œä½ çœ‹åˆ°äº†ä¸€äº›å®ç”¨çš„æŠ€å·§ï¼Œå¯ä»¥ä½¿æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰æ–¹æ³•æ›´åŠ ç¨³å®šå¹¶åŠ é€Ÿæ”¶æ•›ã€‚å®ƒä»¬æ¶‰åŠåŸºæœ¬çš„
    DQN æ–¹æ³•ä¿®æ”¹ï¼ˆä¾‹å¦‚å‘ç½‘ç»œæ³¨å…¥å™ªå£°æˆ–å±•å¼€ Bellman æ–¹ç¨‹ï¼‰æ¥è·å¾—æ›´å¥½çš„ç­–ç•¥ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒæ‰€éœ€çš„æ—¶é—´ã€‚ä½†åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦ä¸€ç§æ–¹æ³•ï¼šè°ƒæ•´æ–¹æ³•çš„å®ç°ç»†èŠ‚ï¼Œä»¥æé«˜è®­ç»ƒé€Ÿåº¦ã€‚è¿™æ˜¯ä¸€ç§çº¯ç²¹çš„å·¥ç¨‹æ–¹æ³•ï¼Œä½†å®ƒåŒæ ·é‡è¦ï¼Œå› ä¸ºåœ¨å®è·µä¸­éå¸¸æœ‰ç”¨ã€‚
- en: 'In this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: Take the Pong environment from the previous chapter and try to get it solved
    as fast as possible
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å–ç”¨ä¸Šä¸€ç« ä¸­çš„ Pong ç¯å¢ƒï¼Œå¹¶å°½é‡ä»¥æœ€å¿«é€Ÿåº¦è§£å†³å®ƒ
- en: In a step-by-step manner, get Pong solved almost 2 times faster using exactly
    the same commodity hardware
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥é€æ­¥çš„æ–¹å¼ï¼Œä½¿ç”¨å®Œå…¨ç›¸åŒçš„å•†å“ç¡¬ä»¶å°† Pong æ¸¸æˆçš„è§£å†³é€Ÿåº¦æé«˜è¿‘ 2 å€
- en: Why speed matters
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆé€Ÿåº¦å¾ˆé‡è¦
- en: First, letâ€™s talk a bit about why speed is important and why we optimize it
    at all. It might not be obvious, but enormous hardware performance improvements
    have happened in the last decade or two. Almost 20 years ago, I was involved with
    a project that focused on building a supercomputer for computational fluid dynamics
    (CFD) simulations performed by an aircraft engine design company. The system consisted
    of 64 servers, occupied three 42-inch racks, and required dedicated cooling and
    power subsystems. The hardware alone (without cooling) cost around $1M.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬æ¥è°ˆè°ˆä¸ºä»€ä¹ˆé€Ÿåº¦å¦‚æ­¤é‡è¦ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ä¼˜åŒ–å®ƒã€‚ä¹Ÿè®¸å¹¶ä¸æ˜æ˜¾ï¼Œä½†è¿‡å»äºŒåå¹´é‡Œç¡¬ä»¶æ€§èƒ½æœ‰äº†å·¨å¤§çš„æå‡ã€‚è¿‘äºŒåå¹´å‰ï¼Œæˆ‘æ›¾å‚ä¸ä¸€ä¸ªé¡¹ç›®ï¼Œä¸“æ³¨äºä¸ºä¸€ä¸ªèˆªç©ºå‘åŠ¨æœºè®¾è®¡å…¬å¸æ„å»ºä¸€ä¸ªç”¨äºè®¡ç®—æµä½“åŠ›å­¦ï¼ˆCFDï¼‰ä»¿çœŸçš„è¶…çº§è®¡ç®—æœºã€‚è¯¥ç³»ç»Ÿç”±
    64 å°æœåŠ¡å™¨ç»„æˆï¼Œå æ®äº†ä¸‰ç»„ 42 è‹±å¯¸çš„æœºæ¶ï¼Œå¹¶ä¸”éœ€è¦ä¸“é—¨çš„å†·å´å’Œç”µåŠ›å­ç³»ç»Ÿã€‚ä»…ç¡¬ä»¶éƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬å†·å´ï¼‰å°±èŠ±è´¹äº†å¤§çº¦ 100 ä¸‡ç¾å…ƒã€‚
- en: In 2005, this supercomputer was ranked fourth among ex-USSR supercomputers and
    was the fastest system installed in the industry. Its theoretical performance
    was 922 GFLOPS (almost a trillion floating-point operations per second), but in
    comparison to the GTX 1080 Ti released 12 years later, all the capabilities of
    this pile of iron look tiny.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ 2005 å¹´ï¼Œè¿™å°è¶…çº§è®¡ç®—æœºåœ¨å‰è‹è”è¶…çº§è®¡ç®—æœºä¸­æ’åç¬¬å››ï¼Œæ˜¯ä¸šå†…å®‰è£…çš„æœ€å¿«ç³»ç»Ÿã€‚å®ƒçš„ç†è®ºæ€§èƒ½ä¸º 922 GFLOPSï¼ˆå‡ ä¹æ¯ç§’ä¸€ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼‰ï¼Œä½†ä¸
    12 å¹´åå‘å¸ƒçš„ GTX 1080 Ti ç›¸æ¯”ï¼Œè¿™å †é“å—çš„æ‰€æœ‰èƒ½åŠ›çœ‹èµ·æ¥æ˜¾å¾—å¾®ä¸è¶³é“ã€‚
- en: One single GTX 1080 Ti is able to perform 11,340 GFLOPS, which is 12.3 times
    more than what supercomputers from 2005 could do. And the price was only $700
    per GPU when it was released! If we count computation power per $1, we get a price
    drop of more than 17,500 times for every GFLOP. This number is even more dramatic
    with the latest (at the time of writing) H100 GPU, which provides 134 teraflops
    (with FP32 operations).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å—å•ç‹¬çš„ GTX 1080 Ti èƒ½å¤Ÿæ‰§è¡Œ 11,340 GFLOPSï¼Œæ€§èƒ½æ˜¯ 2005 å¹´çš„è¶…çº§è®¡ç®—æœºçš„ 12.3 å€ã€‚è€Œä¸”å®ƒå‘å¸ƒæ—¶ï¼Œæ¯ä¸ª GPU
    çš„ä»·æ ¼ä»…ä¸º $700ï¼å¦‚æœæˆ‘ä»¬æŒ‰æ¯èŠ±è´¹ $1 çš„è®¡ç®—èƒ½åŠ›æ¥ç®—ï¼Œé‚£ä¹ˆæ¯ä¸ª GFLOP çš„ä»·æ ¼ä¸‹é™äº†è¶…è¿‡ 17,500 å€ã€‚è¿™ä¸ªæ•°å­—åœ¨æœ€æ–°çš„ï¼ˆæ’°å†™æ—¶ï¼‰H100
    GPU ä¸Šæ›´ä¸ºæƒŠäººï¼ŒH100 æä¾›äº† 134 teraflops çš„æ€§èƒ½ï¼ˆä½¿ç”¨ FP32 æ“ä½œï¼‰ã€‚
- en: It has been said many times that artificial intelligence (AI) progress (and
    machine learning (ML) in general) is being driven by data availability and computing
    power increases, and I believe that this is absolutely true. Imagine some computations
    that require a month to complete on one machine (a very common situation in CFD
    and other physics simulations). If we are able to increase speed by five times,
    this month of patient waiting will turn into six days. Speeding up by 100 times
    will mean that this heavy one-month computation will end up taking eight hours,
    so you could have three of them done in just one day! Itâ€™s very cool to be able
    to get 20,000 times more power for the same money nowadays. By the way, speeding
    up by 20k times will mean that our one-month problem will be done in two to three
    minutes!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šäººå¤šæ¬¡æåˆ°ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿›æ­¥ï¼ˆä»¥åŠæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„ä¸€èˆ¬è¿›å±•ï¼‰æ˜¯ç”±æ•°æ®å¯ç”¨æ€§å’Œè®¡ç®—èƒ½åŠ›çš„æå‡æ‰€æ¨åŠ¨çš„ï¼Œæˆ‘è®¤ä¸ºè¿™ç»å¯¹æ˜¯æ­£ç¡®çš„ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä¸€äº›è®¡ç®—åœ¨ä¸€å°æœºå™¨ä¸Šéœ€è¦ä¸€ä¸ªæœˆæ‰èƒ½å®Œæˆï¼ˆè¿™åœ¨è®¡ç®—æµä½“åŠ›å­¦ï¼ˆCFDï¼‰å’Œå…¶ä»–ç‰©ç†ä»¿çœŸä¸­éå¸¸å¸¸è§ï¼‰ã€‚å¦‚æœæˆ‘ä»¬èƒ½å°†é€Ÿåº¦æé«˜äº”å€ï¼Œé‚£ä¹ˆåŸæœ¬éœ€è¦è€å¿ƒç­‰å¾…ä¸€ä¸ªæœˆçš„æ—¶é—´å°†ç¼©çŸ­ä¸ºå…­å¤©ã€‚æé«˜
    100 å€çš„é€Ÿåº¦æ„å‘³ç€è¿™ä¸ªä¸€ä¸ªæœˆçš„è®¡ç®—å°†åªéœ€è¦å…«å°æ—¶å®Œæˆï¼Œé‚£ä¹ˆä½ ä¸€å¤©ä¹‹å†…å°±èƒ½å®Œæˆä¸‰æ¬¡è®¡ç®—ï¼å¦‚ä»Šï¼ŒåªèŠ±ç›¸åŒçš„é’±å°±èƒ½è·å¾— 20,000 å€çš„è®¡ç®—èƒ½åŠ›ï¼ŒçœŸæ˜¯å¤ªé…·äº†ã€‚é¡ºä¾¿æä¸€ä¸‹ï¼Œé€Ÿåº¦æé«˜
    20,000 å€æ„å‘³ç€æˆ‘ä»¬åŸæœ¬éœ€è¦ä¸€ä¸ªæœˆçš„è®¡ç®—é—®é¢˜åªéœ€è¦ä¸¤åˆ°ä¸‰åˆ†é’Ÿå°±èƒ½å®Œæˆï¼
- en: 'This has happened not only in the â€œbig ironâ€ (also known as high-performance
    computing) world; basically, it is everywhere. Modern microcontrollers have the
    performance characteristics of the desktops that we worked with 15 years ago (for
    example, you can build a pocket computer for $50, with a 32-bit microcontroller
    running at 120 MHz, that is able to run the Atari 2600 emulator: [https://hackaday.io/project/80627-badge-for-hackaday-conference-2018-in-belgrade](https://hackaday.io/project/80627-badge-for-hackaday-conference-2018-in-belgrade)).
    Iâ€™m not even talking about modern smartphones, which normally have four to eight
    cores, a graphics processing unit (GPU), and several GB of RAM.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æƒ…å†µä¸ä»…å‘ç”Ÿåœ¨â€œå·¨å‹è®¡ç®—æœºâ€ï¼ˆä¹Ÿç§°ä¸ºé«˜æ€§èƒ½è®¡ç®—ï¼‰é¢†åŸŸï¼›åŸºæœ¬ä¸Šï¼Œå®ƒæ— å¤„ä¸åœ¨ã€‚ç°ä»£å¾®æ§åˆ¶å™¨çš„æ€§èƒ½ç‰¹ç‚¹å·²ç»ä¸æˆ‘ä»¬15å¹´å‰ä½¿ç”¨çš„æ¡Œé¢è®¡ç®—æœºç›¸å½“ï¼ˆä¾‹å¦‚ï¼Œä½ å¯ä»¥èŠ±50ç¾å…ƒæ‰“é€ ä¸€å°ä¾¿æºè®¡ç®—æœºï¼Œé…å¤‡è¿è¡Œ120
    MHzçš„32ä½å¾®æ§åˆ¶å™¨ï¼Œèƒ½å¤Ÿè¿è¡ŒAtari 2600æ¨¡æ‹Ÿå™¨ï¼š[https://hackaday.io/project/80627-badge-for-hackaday-conference-2018-in-belgrade](https://hackaday.io/project/80627-badge-for-hackaday-conference-2018-in-belgrade)ï¼‰
    ã€‚æˆ‘ç”šè‡³æ²¡æœ‰æåˆ°ç°ä»£æ™ºèƒ½æ‰‹æœºï¼Œå®ƒä»¬é€šå¸¸é…æœ‰å››åˆ°å…«ä¸ªæ ¸å¿ƒã€å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ä»¥åŠæ•°GBçš„å†…å­˜ã€‚
- en: 'Of course, there are a lot of complications there. Itâ€™s not just taking the
    same code that you used a decade ago and now, magically, finding that it works
    several thousand times faster. It might be the opposite: you might not be able
    to run it at all, due to a change in libraries, operating system interfaces, and
    other factors. (Have you ever tried to read old CD-RW disks written just a decade
    ago?) Nowadays, to get the full capabilities of modern hardware, you need to parallelize
    your code, which automatically means tons of details about distributed systems,
    data locality, communications, and the internal characteristics of the hardware
    and libraries. High-level libraries try to hide all those complications from you,
    but you canâ€™t ignore all of them if you want to use these libraries efficiently.
    However, it is definitely worth it â€” one month of patient waiting could be turned
    into three minutes, remember. On the other hand, it might not be fully obvious
    why we need to speed things up in the first place. One month is not that long,
    after all; just lock the computer in a server room and go on vacation! But think
    about the process involved in preparing and making this computation work. You
    might already have noticed that even simple ML problems can be almost impossible
    to implement properly on the first attempt.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œé‚£é‡Œæœ‰å¾ˆå¤šå¤æ‚çš„å› ç´ ã€‚è¿™ä¸ä»…ä»…æ˜¯å°†åå¹´å‰ç”¨è¿‡çš„ç›¸åŒä»£ç ç°åœ¨ç¥å¥‡åœ°è®©å®ƒè¿è¡Œå¾—å¿«ä¸Šå‡ åƒå€ã€‚å¯èƒ½æ­£å¥½ç›¸åï¼šä½ ç”šè‡³å¯èƒ½å®Œå…¨æ— æ³•è¿è¡Œå®ƒï¼Œå› ä¸ºåº“ã€æ“ä½œç³»ç»Ÿæ¥å£ä»¥åŠå…¶ä»–å› ç´ çš„å˜åŒ–ã€‚ï¼ˆä½ æ˜¯å¦æ›¾å°è¯•è¯»å–åå¹´å‰å†™å…¥çš„CD-RWç£ç›˜ï¼Ÿï¼‰å¦‚ä»Šï¼Œè¦å……åˆ†å‘æŒ¥ç°ä»£ç¡¬ä»¶çš„èƒ½åŠ›ï¼Œä½ éœ€è¦å°†ä»£ç å¹¶è¡ŒåŒ–ï¼Œè¿™æ„å‘³ç€ä½ å¿…é¡»å¤„ç†å¤§é‡å…³äºåˆ†å¸ƒå¼ç³»ç»Ÿã€æ•°æ®å±€éƒ¨æ€§ã€é€šä¿¡ä»¥åŠç¡¬ä»¶å’Œåº“å†…éƒ¨ç‰¹æ€§çš„ç»†èŠ‚ã€‚é«˜çº§åº“å°½åŠ›éšè—è¿™äº›å¤æ‚æ€§ï¼Œä½†å¦‚æœä½ æƒ³é«˜æ•ˆåœ°ä½¿ç”¨è¿™äº›åº“ï¼Œå°±æ— æ³•å¿½è§†è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç»å¯¹æ˜¯å€¼å¾—çš„â€”â€”è®°ä½ï¼Œä¸€æ•´ä¸ªæœˆçš„è€å¿ƒç­‰å¾…å¯ä»¥ç¼©çŸ­ä¸ºä¸‰åˆ†é’Ÿã€‚å¦ä¸€æ–¹é¢ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¦åŠ é€Ÿæ“ä½œï¼Œå¯èƒ½å¹¶ä¸å®Œå…¨æ˜æ˜¾ã€‚æ¯•ç«Ÿï¼Œä¸€ä¸ªæœˆå¹¶ä¸ç®—å¤ªé•¿ï¼›åªéœ€å°†è®¡ç®—æœºé”åœ¨æœåŠ¡å™¨å®¤ï¼Œç„¶åå»åº¦ä¸ªå‡ï¼ä½†è¯·æƒ³ä¸€ä¸‹å‡†å¤‡å’Œä½¿è¿™ä¸ªè®¡ç®—è¿‡ç¨‹æ­£å¸¸è¿è¡Œçš„æ•´ä¸ªè¿‡ç¨‹ã€‚ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œå³ä½¿æ˜¯ç®€å•çš„æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œåˆæ¬¡å°è¯•æ—¶ä¹Ÿå‡ ä¹ä¸å¯èƒ½åšåˆ°å®Œç¾å®ç°ã€‚
- en: They require many trial runs before you find good hyperparameters and fix all
    the bugs and code ready for a clean launch. There is exactly the same process
    in physics simulations, RL research, big data processing, and programming in general.
    So, if we are able to make something run faster, itâ€™s not only beneficial for
    the single run but also enables us to iterate quickly and do more experiments
    with code, which might significantly speed up the whole process and improve the
    quality of the final result.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬éœ€è¦å¤šæ¬¡è¯•è¿è¡Œï¼Œæ‰èƒ½æ‰¾åˆ°åˆé€‚çš„è¶…å‚æ•°å¹¶ä¿®å¤æ‰€æœ‰çš„é”™è¯¯å’Œä»£ç ï¼Œå‡†å¤‡å¥½è¿›è¡Œå¹²å‡€çš„å‘å¸ƒã€‚åœ¨ç‰©ç†ä»¿çœŸã€å¼ºåŒ–å­¦ä¹ ç ”ç©¶ã€å¤§æ•°æ®å¤„ç†å’Œç¼–ç¨‹é¢†åŸŸï¼Œç¡®å®å­˜åœ¨å®Œå…¨ç›¸åŒçš„è¿‡ç¨‹ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿè®©æŸä¸ªç¨‹åºè¿è¡Œå¾—æ›´å¿«ï¼Œå®ƒä¸ä»…å¯¹å•æ¬¡è¿è¡Œæœ‰åˆ©ï¼Œè¿˜èƒ½è®©æˆ‘ä»¬å¿«é€Ÿè¿­ä»£ï¼Œè¿›è¡Œæ›´å¤šçš„å®éªŒï¼Œè¿™å¯èƒ½æ˜¾è‘—åŠ é€Ÿæ•´ä¸ªè¿‡ç¨‹ï¼Œå¹¶æé«˜æœ€ç»ˆç»“æœçš„è´¨é‡ã€‚
- en: I remember one situation from my career when we deployed a Hadoop cluster in
    our department, where we were developing a web search engine (similar to Google,
    but for Russian websites). Before the deployment, it took several weeks to conduct
    even simple experiments with data. Several terabytes of data were lying on different
    servers; you needed to run your code several times on every machine, gather and
    combine intermediate results, deal with occasional hardware failures, and do a
    lot of manual tasks not related to the problem that you were supposed to solve.
    After integrating the Hadoop platform into the data processing, the time needed
    for experiments dropped to several hours, which was completely game-changing.
    Since then, developers have been able to conduct many experiments much more easily
    and faster without bothering with unnecessary details. The number of experiments
    (and willingness to run them) has increased significantly, which has also increased
    the quality of the final product.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®°å¾—èŒä¸šç”Ÿæ¶¯ä¸­æœ‰ä¸€ä¸ªæƒ…å¢ƒï¼Œå½“æ—¶æˆ‘ä»¬åœ¨éƒ¨é—¨å†…éƒ¨éƒ¨ç½²äº†ä¸€ä¸ªHadoopé›†ç¾¤ï¼Œæˆ‘ä»¬å½“æ—¶æ­£åœ¨å¼€å‘ä¸€ä¸ªç½‘é¡µæœç´¢å¼•æ“ï¼ˆç±»ä¼¼Googleï¼Œä½†ç”¨äºä¿„ç½—æ–¯ç½‘ç«™ï¼‰ã€‚åœ¨éƒ¨ç½²ä¹‹å‰ï¼Œå³ä½¿æ˜¯è¿›è¡Œç®€å•çš„æ•°æ®å®éªŒï¼Œä¹Ÿéœ€è¦å‡ ä¸ªæ˜ŸæœŸçš„æ—¶é—´ã€‚å‡ TBçš„æ•°æ®åˆ†å¸ƒåœ¨ä¸åŒçš„æœåŠ¡å™¨ä¸Šï¼›ä½ éœ€è¦åœ¨æ¯å°æœºå™¨ä¸Šè¿è¡Œå¤šæ¬¡ä»£ç ï¼Œæ”¶é›†å’Œåˆå¹¶ä¸­é—´ç»“æœï¼Œå¤„ç†å¶å°”å‘ç”Ÿçš„ç¡¬ä»¶æ•…éšœï¼Œå¹¶å®Œæˆè®¸å¤šä¸é—®é¢˜æ— å…³çš„æ‰‹åŠ¨ä»»åŠ¡ã€‚å°†Hadoopå¹³å°é›†æˆåˆ°æ•°æ®å¤„ç†åï¼Œå®éªŒæ‰€éœ€çš„æ—¶é—´å‡å°‘åˆ°äº†å‡ ä¸ªå°æ—¶ï¼Œè¿™å®Œå…¨æ”¹å˜äº†æ¸¸æˆè§„åˆ™ã€‚ä»é‚£æ—¶èµ·ï¼Œå¼€å‘äººå‘˜èƒ½å¤Ÿæ›´è½»æ¾ã€æ›´å¿«é€Ÿåœ°è¿›è¡Œæ›´å¤šå®éªŒï¼Œè€Œä¸å¿…ä¸ºä¸å¿…è¦çš„ç»†èŠ‚çƒ¦æ¼ã€‚å®éªŒçš„æ•°é‡ï¼ˆä»¥åŠè¿›è¡Œå®éªŒçš„æ„æ„¿ï¼‰æ˜¾è‘—å¢åŠ ï¼Œè¿™ä¹Ÿæé«˜äº†æœ€ç»ˆäº§å“çš„è´¨é‡ã€‚
- en: 'Another reason in favor of optimization is the size of problems that we can
    deal with. Making some method run faster might mean two different things: we can
    get the results sooner, or we can increase the size (or some other measure of
    the problemâ€™s complexity). A complexity increase might have different meanings
    in different cases, like getting more accurate results, making fewer simplifications
    of the real world, or taking into account more data, but, almost always, this
    is a good thing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ”¯æŒä¼˜åŒ–çš„ç†ç”±æ˜¯æˆ‘ä»¬å¯ä»¥å¤„ç†çš„é—®é¢˜çš„è§„æ¨¡ã€‚è®©æŸç§æ–¹æ³•è¿è¡Œå¾—æ›´å¿«å¯èƒ½æ„å‘³ç€ä¸¤ä»¶äº‹ï¼šæˆ‘ä»¬å¯ä»¥æ›´å¿«å¾—åˆ°ç»“æœï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥å¢åŠ é—®é¢˜çš„è§„æ¨¡ï¼ˆæˆ–å…¶ä»–è¡¡é‡é—®é¢˜å¤æ‚æ€§çš„æ ‡å‡†ï¼‰ã€‚å¤æ‚æ€§å¢åŠ åœ¨ä¸åŒæƒ…å†µä¸‹å¯èƒ½æœ‰ä¸åŒçš„å«ä¹‰ï¼Œæ¯”å¦‚å¾—åˆ°æ›´å‡†ç¡®çš„ç»“æœï¼Œå‡å°‘å¯¹ç°å®ä¸–ç•Œçš„ç®€åŒ–ï¼Œæˆ–è€…è€ƒè™‘æ›´å¤šçš„æ•°æ®ï¼Œä½†å‡ ä¹æ€»æ˜¯ï¼Œè¿™æ˜¯å¥½äº‹ã€‚
- en: Returning to the main topic of the book, letâ€™s outline how RL methods might
    benefit from speed-ups. First of all, even state-of-the-art RL methods are not
    very sample efficient, which means that training needs to communicate with the
    environment many times (in the case of Atari, millions of times) before learning
    a good policy, and that might mean weeks of training. If we can speed up this
    process a bit, we can get the results faster, do more experiments, and find better
    hyperparameters. Besides this, if we have faster code, we can even increase the
    complexity of the problems that they are applied to.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°æœ¬ä¹¦çš„ä¸»é¢˜ï¼Œè®©æˆ‘ä»¬æ¦‚è¿°ä¸€ä¸‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¦‚ä½•é€šè¿‡åŠ é€Ÿæ¥å—ç›Šã€‚é¦–å…ˆï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¹Ÿä¸æ˜¯éå¸¸é«˜æ•ˆï¼Œè¿™æ„å‘³ç€è®­ç»ƒéœ€è¦å¤šæ¬¡ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼ˆåœ¨é›…è¾¾åˆ©çš„æƒ…å†µä¸‹æ˜¯æ•°ç™¾ä¸‡æ¬¡ï¼‰ï¼Œæ‰èƒ½å­¦åˆ°ä¸€ä¸ªå¥½çš„ç­–ç•¥ï¼Œè¿™å¯èƒ½éœ€è¦å‡ å‘¨çš„è®­ç»ƒã€‚å¦‚æœæˆ‘ä»¬èƒ½åŠ é€Ÿè¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ›´å¿«åœ°å¾—åˆ°ç»“æœï¼Œè¿›è¡Œæ›´å¤šçš„å®éªŒï¼Œå¹¶æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¦‚æœæˆ‘ä»¬çš„ä»£ç æ›´å¿«ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥å¢åŠ åº”ç”¨è¿™äº›æ–¹æ³•æ—¶é—®é¢˜çš„å¤æ‚æ€§ã€‚
- en: In modern RL, Atari games are considered solved; even so-called â€œhard-exploration
    games,â€ like Montezumaâ€™s Revenge, can be trained to superhuman accuracy. Therefore,
    new frontiers in research require more complex problems, with richer observation
    and action spaces, which inevitably require more training time and more hardware.
    Such research has already been started (and has increased the complexity of problems
    a bit too much, from my point of view) by DeepMind and OpenAI, which have switched
    from Atari to much more challenging problems like protein folding (AlphaFold system)
    and Large Language Models (LLMs). Those problems require thousands of GPUs working
    in parallel.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°ä»£å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œé›…è¾¾åˆ©æ¸¸æˆè¢«è®¤ä¸ºå·²ç»è§£å†³ï¼›å³ä½¿æ˜¯æ‰€è°“çš„â€œå›°éš¾æ¢ç´¢æ¸¸æˆâ€ï¼Œå¦‚ã€Šè’™ç‰¹ç¥–ç›çš„å¤ä»‡ã€‹ï¼Œä¹Ÿå¯ä»¥è¢«è®­ç»ƒåˆ°è¶…äººç±»çš„å‡†ç¡®åº¦ã€‚å› æ­¤ï¼Œæ–°çš„ç ”ç©¶å‰æ²¿éœ€è¦æ›´å¤æ‚çš„é—®é¢˜ï¼Œå…·æœ‰æ›´ä¸°å¯Œçš„è§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´ï¼Œè¿™å¿…ç„¶éœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´å’Œç¡¬ä»¶ã€‚è¿™æ ·çš„ç ”ç©¶å·²ç»ç”±DeepMindå’ŒOpenAIå¼€å§‹ï¼ˆä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¹Ÿå¢åŠ äº†é—®é¢˜çš„å¤æ‚æ€§ï¼Œå¯èƒ½æœ‰ç‚¹è¿‡å¤´äº†ï¼‰ï¼Œä»–ä»¬ä»é›…è¾¾åˆ©è½¬å‘äº†æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå¦‚è›‹ç™½è´¨æŠ˜å ï¼ˆAlphaFoldç³»ç»Ÿï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¿™äº›é—®é¢˜éœ€è¦æˆåƒä¸Šä¸‡çš„GPUå¹¶è¡Œå·¥ä½œã€‚
- en: 'I want to end this introduction with a small warning: all performance optimizations
    make sense only when the core method is working properly (which is not always
    obvious in cases of RL and ML in general). As an instructor of an online course
    about performance optimizations said, â€œItâ€™s much better to have a slow and correct
    program than a fast but incorrect one.â€'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³ä»¥ä¸€ä¸ªå°å°çš„è­¦å‘Šç»“æŸè¿™æ®µä»‹ç»ï¼šæ‰€æœ‰çš„æ€§èƒ½ä¼˜åŒ–åªæœ‰åœ¨æ ¸å¿ƒæ–¹æ³•æ­£å¸¸å·¥ä½œçš„æƒ…å†µä¸‹æ‰æœ‰æ„ä¹‰ï¼ˆè¿™åœ¨å¼ºåŒ–å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„æƒ…å†µä¸‹å¹¶ä¸æ€»æ˜¯æ˜¾è€Œæ˜“è§ï¼‰ã€‚æ­£å¦‚ä¸€ä½åœ¨çº¿è¯¾ç¨‹çš„è®²å¸ˆæ‰€è¯´ï¼šâ€œæœ‰ä¸€ä¸ªæ…¢è€Œæ­£ç¡®çš„ç¨‹åºï¼Œæ¯”ä¸€ä¸ªå¿«ä½†ä¸æ­£ç¡®çš„ç¨‹åºè¦å¥½å¾—å¤šã€‚â€
- en: Baseline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºå‡†çº¿
- en: 'In this chapter, we will take the Atari Pong environment that you are already
    familiar with and try to speed up its convergence. As a baseline, we will take
    the same simple DQN that we used in ChapterÂ [8](ch012.xhtml#x1-1240008), and the
    hyperparameters will also be the same. To compare the effect of our changes, we
    will use two characteristics:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä½ å·²ç»ç†Ÿæ‚‰çš„Atari Pongç¯å¢ƒï¼Œå¹¶å°è¯•åŠ é€Ÿå…¶æ”¶æ•›é€Ÿåº¦ã€‚ä½œä¸ºåŸºå‡†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç¬¬[8ç« ](ch012.xhtml#x1-1240008)ä¸­ä½¿ç”¨çš„ç›¸åŒç®€å•DQNï¼Œè¶…å‚æ•°ä¹Ÿå°†ä¿æŒä¸€è‡´ã€‚ä¸ºäº†æ¯”è¾ƒæˆ‘ä»¬æ”¹åŠ¨çš„æ•ˆæœï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªç‰¹å¾ï¼š
- en: The number of frames that we consume from the environment every second (FPS).
    This indicates how fast we can communicate with the environment during the training.
    It is very common in RL papers to indicate the number of frames that the agent
    observed during the training; normal numbers are 25Mâ€“50M frames. So, if our FPS=200,
    it will take ![--50â‹…106--- 200â‹…60â‹…60â‹…24](img/eq39.png) â‰ˆ 2.89 days. In such calculations,
    you need to take into account that RL papers commonly report raw environment frames.
    But if frame skip is used (and it almost always is), the count of frames needs
    to be divided by this factor, which is commonly equal to 4\. In our measurements,
    we calculate FPS in terms of agent communications with the environment, so the
    â€œraw environment FPSâ€ will be four times larger.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ç§’é’Ÿæˆ‘ä»¬ä»ç¯å¢ƒä¸­æ¶ˆè€—çš„å¸§æ•°ï¼ˆFPSï¼‰ã€‚è¿™è¡¨ç¤ºæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ç¯å¢ƒçš„äº¤äº’é€Ÿåº¦ã€‚åœ¨å¼ºåŒ–å­¦ä¹ çš„æ–‡çŒ®ä¸­ï¼Œé€šå¸¸ä¼šæ ‡æ˜æ™ºèƒ½ä½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„å¸§æ•°ï¼Œæ­£å¸¸çš„æ•°å­—èŒƒå›´æ˜¯2500ä¸‡åˆ°5000ä¸‡å¸§ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬çš„FPS=200ï¼Œé‚£ä¹ˆéœ€è¦çš„æ—¶é—´ä¸º![--50â‹…106---
    200â‹…60â‹…60â‹…24](img/eq39.png) â‰ˆ 2.89å¤©ã€‚åœ¨è¿™ç§è®¡ç®—ä¸­ï¼Œéœ€è¦è€ƒè™‘åˆ°å¼ºåŒ–å­¦ä¹ æ–‡çŒ®é€šå¸¸æŠ¥å‘Šçš„æ˜¯åŸå§‹ç¯å¢ƒå¸§æ•°ã€‚ä½†å¦‚æœä½¿ç”¨äº†å¸§è·³è·ƒï¼ˆå‡ ä¹æ€»æ˜¯ä½¿ç”¨ï¼‰ï¼Œåˆ™å¸§æ•°éœ€è¦é™¤ä»¥è¿™ä¸ªå› å­ï¼Œé€šå¸¸æ˜¯4ã€‚åœ¨æˆ‘ä»¬çš„æµ‹é‡ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ˜¯æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’å¸§æ•°ï¼Œå› æ­¤â€œåŸå§‹ç¯å¢ƒFPSâ€å°†æ˜¯å…¶å››å€ã€‚
- en: The wall clock time before the game is solved. We stop training when the smoothed
    reward for the last 100 episodes reaches 18 (the maximum score in Pong is 21.)
    This boundary could be increased, but normally 18 is a good indication that the
    agent has almost mastered the game and polishing the policy to perfection is just
    a matter of the training time. We check the wall clock time because FPS alone
    is not the best indicator of training speed-up.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¸¸æˆè§£å†³ä¹‹å‰çš„å¢™é’Ÿæ—¶é—´ã€‚å½“æœ€å100ä¸ªå›åˆçš„å¹³æ»‘å¥–åŠ±è¾¾åˆ°18æ—¶ï¼Œæˆ‘ä»¬åœæ­¢è®­ç»ƒï¼ˆPongæ¸¸æˆçš„æœ€é«˜åˆ†æ˜¯21ï¼‰ã€‚è¿™ä¸ªè¾¹ç•Œå€¼å¯ä»¥æé«˜ï¼Œä½†é€šå¸¸18å·²ç»æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŒ‡æ ‡ï¼Œè¡¨æ˜æ™ºèƒ½ä½“å‡ ä¹æŒæ¡äº†æ¸¸æˆï¼Œè¿›ä¸€æ­¥ç²¾ç‚¼ç­–ç•¥ä»…ä»…æ˜¯è®­ç»ƒæ—¶é—´çš„é—®é¢˜ã€‚æˆ‘ä»¬æ£€æŸ¥å¢™é’Ÿæ—¶é—´ï¼Œå› ä¸ºå•çº¯çš„FPSå¹¶ä¸æ˜¯è®­ç»ƒåŠ é€Ÿçš„æœ€ä½³æŒ‡æ ‡ã€‚
- en: Due to our manipulations performed with the code, we can get a very high FPS,
    but convergence might suffer. This value alone also canâ€™t be used as a reliable
    characteristic of our improvements, as the training process is stochastic. Even
    by specifying random seeds (we need to set seeds explicitly for PyTorch, Gym,
    and NumPy), parallelization (which will be used in subsequent steps) adds randomness
    to the process, which is almost impossible to avoid. So, the best we can do is
    run the benchmark several times and average the results. But one single runâ€™s
    outcome canâ€™t be used to make any decisions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬åœ¨ä»£ç ä¸­è¿›è¡Œçš„æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—éå¸¸é«˜çš„FPSï¼Œä½†æ”¶æ•›å¯èƒ½ä¼šå—åˆ°å½±å“ã€‚ä»…å‡­è¿™ä¸ªæ•°å€¼ä¹Ÿä¸èƒ½ä½œä¸ºæˆ‘ä»¬æ”¹è¿›æ•ˆæœçš„å¯é æŒ‡æ ‡ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹æ˜¯éšæœºçš„ã€‚å³ä½¿æŒ‡å®šäº†éšæœºç§å­ï¼ˆæˆ‘ä»¬éœ€è¦æ˜ç¡®è®¾ç½®PyTorchã€Gymå’ŒNumPyçš„ç§å­ï¼‰ï¼Œå¹¶è¡ŒåŒ–ï¼ˆåœ¨åç»­æ­¥éª¤ä¸­ä¼šä½¿ç”¨ï¼‰ä¹Ÿä¼šä¸ºè¿‡ç¨‹å¢åŠ éšæœºæ€§ï¼Œå‡ ä¹æ— æ³•é¿å…ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬èƒ½åšçš„æœ€å¥½çš„æ˜¯å¤šæ¬¡è¿è¡ŒåŸºå‡†æµ‹è¯•å¹¶å–å¹³å‡å€¼ã€‚ä½†å•æ¬¡è¿è¡Œçš„ç»“æœä¸èƒ½ä½œä¸ºå†³ç­–çš„ä¾æ®ã€‚
- en: Because of the randomness mentioned above, all the charts in this chapter were
    obtained from averaging 5 runs of the same experiment. All the benchmarks use
    the same machine with an Intel i5-7600K CPU, a GTX 1080 Ti GPU with CUDA 12.3,
    and NVIDIA drivers version 545.29.06.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä¸Šè¿°æåˆ°çš„éšæœºæ€§ï¼Œæœ¬ç« ä¸­çš„æ‰€æœ‰å›¾è¡¨éƒ½æ˜¯é€šè¿‡å¯¹åŒä¸€å®éªŒçš„5æ¬¡è¿è¡Œç»“æœè¿›è¡Œå¹³å‡å¾—åˆ°çš„ã€‚æ‰€æœ‰åŸºå‡†æµ‹è¯•éƒ½ä½¿ç”¨ç›¸åŒçš„æœºå™¨ï¼Œé…ç½®ä¸ºIntel i5-7600K
    CPUã€GTX 1080 Ti GPUï¼ŒCUDAç‰ˆæœ¬12.3ï¼Œä»¥åŠNVIDIAé©±åŠ¨ç‰ˆæœ¬545.29.06ã€‚
- en: 'Our first benchmark will be our baseline version, which is in Chapter09/01_baseline.py.
    I will not provide the source code here, as it has already been given in the previous
    chapter and is the same here. During the training, the code writes into TensorBoard
    several metrics:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªåŸºå‡†å°†æ˜¯åŸºçº¿ç‰ˆæœ¬ï¼Œä½äº Chapter09/01_baseline.pyã€‚æˆ‘è¿™é‡Œä¸æä¾›æºä»£ç ï¼Œå› ä¸ºå®ƒå·²ç»åœ¨å‰ä¸€ç« ä¸­ç»™å‡ºï¼Œå¹¶ä¸”åœ¨æ­¤ä¸å‰é¢ç›¸åŒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»£ç ä¼šå‘
    TensorBoard å†™å…¥å‡ ä¸ªæŒ‡æ ‡ï¼š
- en: 'reward: The raw undiscounted reward from the episode; the x axis is the episode
    number.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward: æ¥è‡ªå‰§é›†çš„åŸå§‹æœªæŠ˜æ‰£å¥–åŠ±ï¼›x è½´æ˜¯å‰§é›†çš„ç¼–å·ã€‚'
- en: 'avg_reward: The same as reward but smoothed by running the average with Î± =
    0.98.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'avg_reward: ä¸å¥–åŠ±ç›¸åŒï¼Œä½†é€šè¿‡ä½¿ç”¨ Î± = 0.98 çš„æ»‘åŠ¨å¹³å‡è¿›è¡Œå¹³æ»‘å¤„ç†ã€‚'
- en: 'steps: The number of steps that the episode lasted. Normally, in the beginning,
    the agent loses very quickly, so every episode is around 1,000 steps. Then, it
    learns how to act better, so the number of steps increases to 3,000â€“4,000 with
    the reward increase; but, in the end, when the agent masters the game, the number
    of steps drops back to 2,000 steps, as the policy is polished to win as quickly
    as possible (due to the discount factor Î³). In fact, this drop in episode length
    might be an indication of overfitting to the environment, which is a huge problem
    in RL. However, dealing with this issue is beyond the scope of our experiments.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'steps: å‰§é›†æŒç»­çš„æ­¥æ•°ã€‚é€šå¸¸ï¼Œåœ¨å¼€å§‹æ—¶ï¼Œä»£ç†ä¼šè¿…é€Ÿå¤±è´¥ï¼Œæ‰€ä»¥æ¯ä¸ªå‰§é›†å¤§çº¦æœ‰ 1,000 æ­¥ã€‚ç„¶åï¼Œä»£ç†å­¦ä¼šäº†æ›´å¥½çš„è¡Œä¸ºï¼Œæ‰€ä»¥æ­¥æ•°å¢åŠ åˆ° 3,000â€“4,000ï¼Œå¹¶ä¸”å¥–åŠ±ä¹Ÿå¢åŠ ï¼›ä½†æ˜¯ï¼Œæœ€ç»ˆï¼Œå½“ä»£ç†æŒæ¡äº†æ¸¸æˆæ—¶ï¼Œæ­¥æ•°ä¼šé™å›åˆ°
    2,000 æ­¥ï¼Œå› ä¸ºç­–ç•¥è¢«ä¼˜åŒ–åˆ°å°½å¯èƒ½å¿«åœ°è·èƒœï¼ˆç”±äºæŠ˜æ‰£å› å­ Î³ï¼‰ã€‚äº‹å®ä¸Šï¼Œè¿™ç§å‰§é›†é•¿åº¦çš„ä¸‹é™å¯èƒ½è¡¨æ˜è¿‡æ‹Ÿåˆäº†ç¯å¢ƒï¼Œè¿™åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ˜¯ä¸€ä¸ªå·¨å¤§çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤„ç†è¿™ä¸ªé—®é¢˜è¶…å‡ºäº†æˆ‘ä»¬å®éªŒçš„èŒƒå›´ã€‚'
- en: 'loss: The loss during the training, sampled every 100 iterations. It should
    be around 2â‹…10^(âˆ’3)â€¦1â‹…10^(âˆ’2), with occasional increases when the agent discovers
    new behavior, leading to a different reward from that learned by the Q-value.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'loss: è®­ç»ƒä¸­çš„æŸå¤±ï¼Œæ¯ 100 æ¬¡è¿­ä»£é‡‡æ ·ä¸€æ¬¡ã€‚å®ƒåº”è¯¥åœ¨ 2â‹…10^(âˆ’3)â€¦1â‹…10^(âˆ’2) ä¹‹é—´ï¼Œå¶å°”ä¼šæœ‰å¢åŠ ï¼Œå½“ä»£ç†å‘ç°æ–°çš„è¡Œä¸ºæ—¶ï¼Œå¯¼è‡´å¥–åŠ±ä¸
    Q å€¼å­¦ä¹ çš„å¥–åŠ±ä¸åŒã€‚'
- en: 'avg_loss: A smoothed version of the loss.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'avg_loss: æŸå¤±çš„å¹³æ»‘ç‰ˆæœ¬ã€‚'
- en: 'epsilon: The current value of ğœ– â€” probability of taking the random action.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'epsilon: å½“å‰ ğœ– çš„å€¼â€”â€”é‡‡å–éšæœºè¡ŒåŠ¨çš„æ¦‚ç‡ã€‚'
- en: 'avg_fps: The speed of agent communication with the environment (observations
    per second), smoothed with a running average.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'avg_fps: ä»£ç†ä¸ç¯å¢ƒé€šä¿¡çš„é€Ÿåº¦ï¼ˆæ¯ç§’è§‚å¯Ÿæ•°ï¼‰ï¼Œé€šè¿‡æ»‘åŠ¨å¹³å‡è¿›è¡Œå¹³æ»‘å¤„ç†ã€‚'
- en: 'In FigureÂ [9.1](#x1-162006r1) and FigureÂ [9.2](#x1-162007r2), the charts are
    averaged from 5 baseline runs. As before, each chart is drawn with two x axes:
    the bottom one is the wall clock time in hours, and the top is the step number
    (episode in FigureÂ [9.1](#x1-162006r1) and training iteration in FigureÂ [9.2](#x1-162007r2)):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾Â [9.1](#x1-162006r1) å’Œå›¾Â [9.2](#x1-162007r2) ä¸­ï¼Œå›¾è¡¨æ˜¯ä» 5 æ¬¡åŸºçº¿è¿è¡Œä¸­å¹³å‡å¾—å‡ºçš„ã€‚å¦‚ä¹‹å‰æ‰€ç¤ºï¼Œæ¯ä¸ªå›¾è¡¨éƒ½ç»˜åˆ¶äº†ä¸¤ä¸ª
    x è½´ï¼šåº•éƒ¨æ˜¯å°æ—¶ä¸ºå•ä½çš„å¢™é’Ÿæ—¶é—´ï¼Œä¸Šé¢æ˜¯æ­¥æ•°ï¼ˆå›¾Â [9.1](#x1-162006r1) ä¸­ä¸ºå‰§é›†æ•°ï¼Œå›¾Â [9.2](#x1-162007r2) ä¸­ä¸ºè®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼‰ï¼š
- en: '![PIC](img/B21150_09_01.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_09_01.png)'
- en: 'FigureÂ 9.1: Reward and episode length in baseline version'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 9.1ï¼šåŸºçº¿ç‰ˆæœ¬ä¸­çš„å¥–åŠ±å’Œå‰§é›†é•¿åº¦
- en: '![PIC](img/B21150_09_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_09_02.png)'
- en: 'FigureÂ 9.2: Loss and FPS during the training of baseline version'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 9.2ï¼šåŸºçº¿ç‰ˆæœ¬è®­ç»ƒä¸­çš„æŸå¤±å’Œ FPS
- en: The computation graph in PyTorch
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch ä¸­çš„è®¡ç®—å›¾
- en: 'Our first examples wonâ€™t be around speeding up the baseline, but will show
    one common, and not always obvious, situation that can cost you performance. In
    ChapterÂ [3](ch007.xhtml#x1-530003), we discussed the way PyTorch calculates gradients:
    it builds the graph of all operations that you perform on tensors, and when you
    call the backward() method of the final loss, all gradients in the model parameters
    are automatically calculated.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä¾‹å­ä¸ä¼šé›†ä¸­åœ¨åŠ é€ŸåŸºçº¿ä¸Šï¼Œè€Œæ˜¯å±•ç¤ºä¸€ç§å¸¸è§çš„ã€å¹¶ä¸æ€»æ˜¯æ˜¾è€Œæ˜“è§çš„æƒ…å†µï¼Œè¿™ç§æƒ…å†µå¯èƒ½ä¼šå½±å“æ€§èƒ½ã€‚åœ¨ç¬¬Â [3](ch007.xhtml#x1-530003)
    ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº† PyTorch å¦‚ä½•è®¡ç®—æ¢¯åº¦ï¼šå®ƒä¼šæ„å»ºä½ å¯¹å¼ é‡æ‰§è¡Œçš„æ‰€æœ‰æ“ä½œçš„å›¾ï¼Œå½“ä½ è°ƒç”¨æœ€ç»ˆæŸå¤±çš„ backward() æ–¹æ³•æ—¶ï¼Œæ‰€æœ‰æ¨¡å‹å‚æ•°ä¸­çš„æ¢¯åº¦ä¼šè¢«è‡ªåŠ¨è®¡ç®—å‡ºæ¥ã€‚
- en: 'This works well, but RL code is normally much more complex than traditional
    supervised learning training, so the RL model that we are currently training is
    also being applied to get the actions that the agent needs to perform in the environment.
    The target network discussed in ChapterÂ [6](#) makes it even more tricky. So,
    in DQN, a neural network (NN) is normally used in three different situations:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†å¼ºåŒ–å­¦ä¹ ä»£ç é€šå¸¸æ¯”ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ è®­ç»ƒè¦å¤æ‚å¾—å¤šï¼Œå› æ­¤æˆ‘ä»¬å½“å‰è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä¹Ÿåœ¨ç”¨äºè·å–ä»£ç†åœ¨ç¯å¢ƒä¸­éœ€è¦æ‰§è¡Œçš„åŠ¨ä½œã€‚ç¬¬Â [6](#)
    ç« ä¸­è®¨è®ºçš„ç›®æ ‡ç½‘ç»œä½¿å¾—è¿™ä¸€è¿‡ç¨‹æ›´åŠ å¤æ‚ã€‚å› æ­¤ï¼Œåœ¨ DQN ä¸­ï¼Œç¥ç»ç½‘ç»œï¼ˆNNï¼‰é€šå¸¸åœ¨ä¸‰ç§ä¸åŒçš„æƒ…å†µä¸­ä½¿ç”¨ï¼š
- en: When we want to calculate Q-values predicted by the network to get the loss
    in respect to reference Q-values approximated by the Bellman equation
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å¸Œæœ›è®¡ç®—ç”±ç½‘ç»œé¢„æµ‹çš„Qå€¼ï¼Œä»¥æ ¹æ®Bellmanæ–¹ç¨‹å¾—åˆ°ç›¸å¯¹äºå‚è€ƒQå€¼çš„æŸå¤±æ—¶
- en: When we apply the target network to get Q-values for the next state to calculate
    a Bellman approximation
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬åº”ç”¨ç›®æ ‡ç½‘ç»œæ¥è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„Qå€¼ï¼Œä»¥è®¡ç®—Bellmanè¿‘ä¼¼æ—¶
- en: When the agent wants to make a decision about the action to perform
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“ä»£ç†æƒ³è¦å†³å®šæ‰§è¡Œçš„åŠ¨ä½œæ—¶
- en: In our training, we need gradients calculated only for the first situation.
    In ChapterÂ [6](#), we avoided gradients by explicitly calling detach() on the
    tensor returned by the target network. This detach is very important, as it prevents
    gradients from flowing into our model â€œfrom the unexpected directionâ€ and, without
    this, the DQN might not converge at all. In the third situation, gradients were
    stopped by converting the network result into a NumPy array.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹è®¡ç®—æ¢¯åº¦ã€‚åœ¨ç¬¬[6](#)ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ˜¾å¼è°ƒç”¨detach()æ¥é¿å…è®¡ç®—æ¢¯åº¦ï¼Œè¿™ä¸ªdetachéå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒé˜²æ­¢äº†æ¢¯åº¦â€œä»æ„å¤–çš„æ–¹å‘â€æµå…¥æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰å®ƒï¼ŒDQNå¯èƒ½æ ¹æœ¬æ— æ³•æ”¶æ•›ã€‚åœ¨ç¬¬ä¸‰ç§æƒ…å†µä¸‹ï¼Œæ¢¯åº¦é€šè¿‡å°†ç½‘ç»œç»“æœè½¬æ¢ä¸ºNumPyæ•°ç»„æ¥åœæ­¢ã€‚
- en: 'Our code in ChapterÂ [6](#), worked, but we missed one subtle detail: the computation
    graph that is created for all three situations. This is not a major problem, but
    creating the graph still uses some resources (in terms of both speed and memory),
    which are wasted because PyTorch creates this computation graph even if we donâ€™t
    call backward() on some graph. To prevent this, one very nice option exists: the
    decorator torch.no_grad().'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ç¬¬[6](#)ç« ä¸­çš„ä»£ç æ˜¯æœ‰æ•ˆçš„ï¼Œä½†æˆ‘ä»¬é”™è¿‡äº†ä¸€ä¸ªç»†èŠ‚ï¼šä¸‰ç§æƒ…å†µä¸‹åˆ›å»ºçš„è®¡ç®—å›¾ã€‚è¿™ä¸ªé—®é¢˜ä¸å¤§ï¼Œä½†åˆ›å»ºè®¡ç®—å›¾ä»ç„¶ä¼šä½¿ç”¨ä¸€äº›èµ„æºï¼ˆæ— è®ºæ˜¯é€Ÿåº¦è¿˜æ˜¯å†…å­˜ï¼‰ï¼Œè€Œè¿™äº›èµ„æºä¼šæµªè´¹ï¼Œå› ä¸ºå³ä½¿æˆ‘ä»¬æ²¡æœ‰å¯¹æŸä¸ªå›¾è°ƒç”¨backward()ï¼ŒPyTorchä¹Ÿä¼šåˆ›å»ºè¿™ä¸ªè®¡ç®—å›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ‰ä¸€ä¸ªéå¸¸å¥½çš„é€‰é¡¹ï¼šè£…é¥°å™¨torch.no_grad()ã€‚
- en: 'Decorators in Python is a very wide topic. They give the developer a lot of
    power (when properly used), but are well beyond the scope of this book. Here,
    Iâ€™ll just give an example where we define two functions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä¸€ä¸ªéå¸¸å¹¿æ³›çš„è¯é¢˜ã€‚å®ƒä»¬ä¸ºå¼€å‘è€…æä¾›äº†å¾ˆå¤šåŠŸèƒ½ï¼ˆå¦‚æœä½¿ç”¨å¾—å½“ï¼‰ï¼Œä½†è¶…å‡ºäº†æœ¬ä¹¦çš„è®¨è®ºèŒƒå›´ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»…ç»™å‡ºä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªå‡½æ•°ï¼š
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Both these functions are doing the same thing, doubling its argument, but the
    first function is declared with torch.no_grad() and the second is just a normal
    function. This decorator temporarily disables gradient computation for all tensors
    passed to the function. As you can see, although the tensor, t, requires grad,
    the result from fun_a (the decorated function) doesnâ€™t have gradients:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªå‡½æ•°åšçš„æ˜¯ç›¸åŒçš„äº‹æƒ…ï¼Œéƒ½æ˜¯å°†å‚æ•°ç¿»å€ï¼Œä½†ç¬¬ä¸€ä¸ªå‡½æ•°ä½¿ç”¨äº†torch.no_grad()è£…é¥°å™¨ï¼Œç¬¬äºŒä¸ªåˆ™æ˜¯æ™®é€šå‡½æ•°ã€‚è¿™ä¸ªè£…é¥°å™¨æš‚æ—¶ç¦ç”¨ä¼ é€’ç»™å‡½æ•°çš„æ‰€æœ‰å¼ é‡çš„æ¢¯åº¦è®¡ç®—ã€‚å¦‚ä½ æ‰€è§ï¼Œå°½ç®¡å¼ é‡téœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä½†ä»fun_aï¼ˆè¢«è£…é¥°çš„å‡½æ•°ï¼‰è¿”å›çš„ç»“æœå¹¶æ²¡æœ‰æ¢¯åº¦ï¼š
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But this effect is bounded inside the decorated function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯è¿™ä¸ªæ•ˆæœä»…é™äºè£…é¥°å™¨å‡½æ•°å†…éƒ¨ï¼š
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The function torch.no_grad() also could be used as a context manager (another
    powerful Python concept that I recommend you learn about) to stop gradients in
    some chunk of code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°torch.no_grad()ä¹Ÿå¯ä»¥ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨ï¼ˆè¿™æ˜¯å¦ä¸€ä¸ªå¼ºå¤§çš„Pythonæ¦‚å¿µï¼Œæˆ‘å»ºè®®ä½ å­¦ä¹ å®ƒï¼‰ï¼Œç”¨äºåœæ­¢æŸæ®µä»£ç ä¸­çš„æ¢¯åº¦è®¡ç®—ï¼š
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This functionality provides you with a very convenient way to indicate parts
    of your code that should be excluded from the gradient machinery completely. This
    has already been done in ptan.agent.DQNAgent (and other agents provided by PTAN)
    and in the common.calc_loss_dqn function. But if you are writing a custom agent
    or implementing your own code, it might be very easy to forget about this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåŠŸèƒ½ä¸ºä½ æä¾›äº†ä¸€ç§éå¸¸æ–¹ä¾¿çš„æ–¹å¼ï¼Œèƒ½å¤ŸæŒ‡ç¤ºä½ ä»£ç ä¸­åº”è¯¥å®Œå…¨æ’é™¤æ¢¯åº¦è®¡ç®—çš„éƒ¨åˆ†ã€‚è¿™åœ¨ptan.agent.DQNAgentï¼ˆä»¥åŠPTANæä¾›çš„å…¶ä»–ä»£ç†ï¼‰å’Œcommon.calc_loss_dqnå‡½æ•°ä¸­å·²ç»å®Œæˆã€‚ä½†æ˜¯å¦‚æœä½ æ­£åœ¨ç¼–å†™è‡ªå®šä¹‰ä»£ç†æˆ–å®ç°è‡ªå·±çš„ä»£ç ï¼Œå¾ˆå®¹æ˜“å¿˜è®°è¿™ä¸€ç‚¹ã€‚
- en: 'To benchmark the effect of unnecessary graph calculation, Iâ€™ve provided the
    modified baseline code in Chapter09/00_slow_grads.py, which is exactly the same,
    but the agent and loss calculations are copied without torch.no_grad(). The following
    charts show the effect of this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼°ä¸å¿…è¦çš„å›¾è®¡ç®—çš„æ•ˆæœï¼Œæˆ‘åœ¨Chapter09/00_slow_grads.pyä¸­æä¾›äº†ä¿®æ”¹åçš„åŸºå‡†ä»£ç ï¼Œå®ƒä¸åŸä»£ç å®Œå…¨ç›¸åŒï¼Œä½†ä»£ç†å’ŒæŸå¤±è®¡ç®—éƒ¨åˆ†æ²¡æœ‰ä½¿ç”¨torch.no_grad()ã€‚ä»¥ä¸‹å›¾è¡¨å±•ç¤ºäº†è¿™ä¸€æ•ˆæœï¼š
- en: '![PIC](img/B21150_09_03.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_09_03.png)'
- en: 'FigureÂ 9.3: A comparison of reward and FPS between the baseline and version
    without torch.no_grad()'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.3ï¼šåŸºå‡†ç‰ˆæœ¬ä¸æ²¡æœ‰torch.no_grad()ç‰ˆæœ¬ä¹‹é—´çš„å¥–åŠ±å’ŒFPSæ¯”è¾ƒ
- en: As you can see, the speed penalty is not that large (around 10 FPS), but that
    might become different in the case of a larger network with a more complicated
    structure. Iâ€™ve seen a 50% performance boost in more complex recurrent NNs obtained
    after adding torch.no_grad().
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œé€Ÿåº¦æŸå¤±å¹¶ä¸å¤§ï¼ˆå¤§çº¦10FPSï¼‰ï¼Œä½†åœ¨ç½‘ç»œæ›´å¤§ä¸”ç»“æ„æ›´å¤æ‚çš„æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚æˆ‘æ›¾çœ‹åˆ°åœ¨æ›´å¤æ‚çš„é€’å½’ç¥ç»ç½‘ç»œä¸­ï¼ŒåŠ å…¥torch.no_grad()åï¼Œæ€§èƒ½æå‡äº†50%ã€‚
- en: Several environments
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šä¸ªç¯å¢ƒ
- en: 'The first idea that we usually apply to speed up deep learning training is
    larger batch size. Itâ€™s also applicable to the domain of deep RL, but you need
    to be careful here. In the normal supervised learning case, the simple rule â€œa
    large batch is betterâ€ is usually true: you just increase your batch as your GPU
    memory allows, and a larger batch normally means more samples will be processed
    in a unit of time thanks to enormous GPU parallelism.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šå¸¸ç”¨æ¥åŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒçš„ç¬¬ä¸€ä¸ªæ€è·¯æ˜¯å¢åŠ æ‰¹é‡å¤§å°ã€‚è¿™åŒæ ·é€‚ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œä½†ä½ éœ€è¦å°å¿ƒã€‚åœ¨æ™®é€šçš„ç›‘ç£å­¦ä¹ ä¸­ï¼Œç®€å•çš„è§„åˆ™â€œè¾ƒå¤§çš„æ‰¹é‡æ›´å¥½â€é€šå¸¸æ˜¯æˆç«‹çš„ï¼šåªè¦ä½ çš„GPUå†…å­˜å…è®¸ï¼Œå°±å¢åŠ æ‰¹é‡ï¼Œè¾ƒå¤§çš„æ‰¹é‡é€šå¸¸æ„å‘³ç€åœ¨å•ä½æ—¶é—´å†…å¤„ç†æ›´å¤šæ ·æœ¬ï¼Œè¿™å¾—ç›Šäºå¼ºå¤§çš„GPUå¹¶è¡Œè®¡ç®—ã€‚
- en: 'The RL case is slightly different. During the training, two things happen simultaneously:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ çš„æƒ…å†µç¨æœ‰ä¸åŒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸¤ä¸ªäº‹æƒ…æ˜¯åŒæ—¶å‘ç”Ÿçš„ï¼š
- en: Your network is trained to get better predictions on the current data
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ çš„ç½‘ç»œç»è¿‡è®­ç»ƒï¼Œå¯ä»¥åœ¨å½“å‰æ•°æ®ä¸Šè·å¾—æ›´å¥½çš„é¢„æµ‹
- en: Your agent explores the environment
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ çš„ä»£ç†æ¢ç´¢ç¯å¢ƒ
- en: As the agent explores the environment and learns about the outcome of its actions,
    the training data changes. In a shooter example, your agent can run randomly for
    a time while being shot by monsters and have only a miserable â€œdeath is everywhereâ€
    experience in the training buffer. But after a while, the agent will discover
    that it has a weapon it can use. This new experience can dramatically change the
    data that we are using for training. RL convergence usually lies on a fragile
    balance between training and exploration. If we just increase a batch size without
    tweaking other options, we can easily overfit to the current data (for our shooter
    example, your agent can start thinking that â€œdying youngâ€ is the only option to
    minimize suffering and may never discover the gun it has).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ä»£ç†æ¢ç´¢ç¯å¢ƒå¹¶å­¦ä¹ å…¶è¡Œä¸ºçš„ç»“æœï¼Œè®­ç»ƒæ•°æ®ä¼šå‘ç”Ÿå˜åŒ–ã€‚åœ¨å°„å‡»æ¸¸æˆçš„ä¾‹å­ä¸­ï¼Œä»£ç†å¯èƒ½ä¼šéšæœºè¿è¡Œä¸€æ®µæ—¶é—´ï¼Œè¢«æ€ªç‰©å‡»ä¸­å¹¶åœ¨è®­ç»ƒç¼“å†²åŒºä¸­åªè·å¾—â€œæ­»äº¡æ— å¤„ä¸åœ¨â€çš„ç—›è‹¦ç»éªŒã€‚ä½†è¿‡äº†ä¸€æ®µæ—¶é—´ï¼Œä»£ç†ä¼šå‘ç°å®ƒæœ‰ä¸€æŠŠå¯ä»¥ä½¿ç”¨çš„æ­¦å™¨ã€‚è¿™ç§æ–°çš„ç»éªŒå¯èƒ½ä¼šæå¤§åœ°æ”¹å˜æˆ‘ä»¬ç”¨äºè®­ç»ƒçš„æ•°æ®ã€‚å¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›é€šå¸¸ä¾èµ–äºè®­ç»ƒå’Œæ¢ç´¢ä¹‹é—´çš„å¾®å¦™å¹³è¡¡ã€‚å¦‚æœæˆ‘ä»¬åªæ˜¯å¢åŠ æ‰¹é‡å¤§å°è€Œæ²¡æœ‰è°ƒæ•´å…¶ä»–é€‰é¡¹ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“åœ¨å½“å‰æ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼ˆåœ¨å°„å‡»æ¸¸æˆçš„ä¾‹å­ä¸­ï¼Œä»£ç†å¯èƒ½å¼€å§‹è®¤ä¸ºâ€œæ—©æ­»â€æ˜¯æœ€å°åŒ–ç—›è‹¦çš„å”¯ä¸€é€‰æ‹©ï¼Œç”šè‡³æ°¸è¿œä¸ä¼šå‘ç°å®ƒæ‹¥æœ‰çš„æªï¼‰ã€‚
- en: 'So, in the example in Chapter09/02_n_envs.py, our agent uses several copies
    of the same environment to gather the training data. On every training iteration,
    we populate our replay buffer with samples from all those environments and then
    sample a proportionally larger batch size. This also allows us to speed up inference
    time a bit, as we can make a decision about the actions to execute for all N environments
    in one forward pass of the NN. In terms of implementation, the preceding logic
    requires just a couple of changes in the code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨Chapter09/02_n_envs.pyä¸­çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬çš„ä»£ç†ä½¿ç”¨å¤šä¸ªç›¸åŒç¯å¢ƒçš„å‰¯æœ¬æ¥æ”¶é›†è®­ç»ƒæ•°æ®ã€‚åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ç¯å¢ƒä¸­çš„æ ·æœ¬å¡«å……åˆ°é‡æ”¾ç¼“å†²åŒºï¼Œç„¶åæŒ‰æ¯”ä¾‹å¢å¤§æ‰¹é‡å¤§å°ã€‚è¿™ä¹Ÿä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç¨å¾®åŠ é€Ÿæ¨ç†æ—¶é—´ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ç¥ç»ç½‘ç»œçš„ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ï¼Œå¯¹æ‰€æœ‰Nä¸ªç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œå†³ç­–ã€‚åœ¨å®ç°æ–¹é¢ï¼Œå‰é¢çš„é€»è¾‘åªéœ€è¦å¯¹ä»£ç è¿›è¡Œå‡ å¤„ä¿®æ”¹ï¼š
- en: As PTAN supports several environments out of the box, what we need to do is
    just pass N Gym environments to the ExperienceSource instance
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºPTANåŸç”Ÿæ”¯æŒå¤šä¸ªç¯å¢ƒï¼Œæˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯å°†Nä¸ªGymç¯å¢ƒä¼ é€’ç»™ExperienceSourceå®ä¾‹
- en: The agent code (in our case, DQNAgent) is already optimized for the batched
    application of the NN
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†ä»£ç ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯DQNAgentï¼‰å·²ç»ä¸ºç¥ç»ç½‘ç»œçš„æ‰¹å¤„ç†åº”ç”¨è¿›è¡Œäº†ä¼˜åŒ–
- en: 'Several pieces of code were changed to address this. The function that generates
    batches now performs multiple steps (equal to the total number of environments)
    for every training iteration:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¿®æ”¹äº†å‡ æ®µä»£ç ã€‚ç”Ÿæˆæ‰¹æ¬¡çš„å‡½æ•°ç°åœ¨åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­æ‰§è¡Œå¤šä¸ªæ­¥éª¤ï¼ˆç­‰äºç¯å¢ƒæ€»æ•°ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The experience source accepts the array of environments instead of a single
    environment:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç»éªŒæºæ¥å—å¤šä¸ªç¯å¢ƒçš„æ•°ç»„ï¼Œè€Œä¸æ˜¯å•ä¸€ç¯å¢ƒï¼š
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Other changes are just minor tweaks of constants to adjust the FPS tracker
    and compensated speed of epsilon decay (ratio of random steps). As the number
    of environments is the new hyperparameter that needs to be tuned, I ran several
    experiments with N from 2â€¦6\. The following charts show the averaged dynamics:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–å˜åŒ–ä»…æ˜¯å¯¹å¸¸é‡çš„å°è°ƒæ•´ï¼Œç”¨äºè°ƒæ•´FPSè¿½è¸ªå™¨å’Œè¡¥å¿Îµè¡°å‡çš„é€Ÿåº¦ï¼ˆéšæœºæ­¥éª¤çš„æ¯”ä¾‹ï¼‰ã€‚ç”±äºç¯å¢ƒæ•°é‡æ˜¯éœ€è¦è°ƒä¼˜çš„æ–°è¶…å‚æ•°ï¼Œæˆ‘è¿›è¡Œäº†å‡ ä¸ªå®éªŒï¼ŒNçš„èŒƒå›´æ˜¯ä»2åˆ°6ã€‚ä»¥ä¸‹å›¾è¡¨å±•ç¤ºäº†å¹³å‡çš„åŠ¨æ€ï¼š
- en: '![PIC](img/B21150_09_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_09_04.png)'
- en: 'FigureÂ 9.4: Reward and FPS in the baseline, two, and three environments'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.4ï¼šåŸºå‡†ã€ä¸¤ä¸ªå’Œä¸‰ä¸ªç¯å¢ƒä¸­çš„å¥–åŠ±ä¸FPS
- en: '![PIC](img/B21150_09_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_09_05.png)'
- en: 'FigureÂ 9.5: Reward and FPS for n = 3â€¦6'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.5ï¼šn = 3â€¦6æ—¶çš„å¥–åŠ±ä¸FPS
- en: As you can see from the charts, adding an extra environment provided a 47% gain
    in FPS (from 227 FPS to 335 FPS) and sped up the convergence about 10% (from 52
    minutes to 48 minutes). The same effect came from adding the third environment
    (398 FPS, and 36 minutes), but adding more environments had a negative effect
    on convergence speed, despite a further increase in FPS. So, it looks like N =
    3 is more or less the optimal value for our hyperparameter, but, of course, you
    are free to tweak and experiment. It also illustrates why weâ€™re monitoring not
    just raw speed in FPS but also how quickly the agent is able to solve the game.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ ä»å›¾è¡¨ä¸­çœ‹åˆ°çš„ï¼Œå¢åŠ ä¸€ä¸ªé¢å¤–çš„ç¯å¢ƒæä¾›äº†47%çš„FPSå¢ç›Šï¼ˆä»227 FPSåˆ°335 FPSï¼‰ï¼Œå¹¶åŠ é€Ÿäº†çº¦10%çš„æ”¶æ•›é€Ÿåº¦ï¼ˆä»52åˆ†é’Ÿåˆ°48åˆ†é’Ÿï¼‰ã€‚åŒæ ·çš„æ•ˆæœä¹Ÿæ¥è‡ªäºå¢åŠ ç¬¬ä¸‰ä¸ªç¯å¢ƒï¼ˆ398
    FPSï¼Œ36åˆ†é’Ÿï¼‰ï¼Œä½†æ˜¯å°½ç®¡FPSè¿›ä¸€æ­¥å¢åŠ ï¼Œå¢åŠ æ›´å¤šç¯å¢ƒå¯¹æ”¶æ•›é€Ÿåº¦äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚å› æ­¤ï¼Œçœ‹èµ·æ¥N = 3å·®ä¸å¤šæ˜¯æˆ‘ä»¬çš„è¶…å‚æ•°çš„æœ€ä¼˜å€¼ï¼Œä½†å½“ç„¶ï¼Œä½ å¯ä»¥è‡ªç”±è°ƒæ•´å’Œå®éªŒã€‚è¿™ä¹Ÿè¯´æ˜äº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ä»…ç›‘æ§FPSçš„åŸå§‹é€Ÿåº¦ï¼Œè¿˜è¦è§‚å¯Ÿæ™ºèƒ½ä½“è§£å†³æ¸¸æˆçš„é€Ÿåº¦ã€‚
- en: Playing and training in separate processes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ä¸åŒè¿›ç¨‹ä¸­è¿›è¡Œæ¸¸æˆå’Œè®­ç»ƒ
- en: 'At a high level, our training contains a repetition of the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œæˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹åŒ…å«ä»¥ä¸‹æ­¥éª¤çš„é‡å¤ï¼š
- en: Ask the current network to choose actions and execute them in our array of environments.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯·æ±‚å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œå¹¶åœ¨æˆ‘ä»¬çš„ç¯å¢ƒé˜µåˆ—ä¸­æ‰§è¡Œè¿™äº›åŠ¨ä½œã€‚
- en: Put observations into the replay buffer.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è§‚å¯Ÿå€¼æ”¾å…¥é‡æ”¾ç¼“å†²åŒºã€‚
- en: Randomly sample the training batch from the replay buffer.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»é‡æ”¾ç¼“å†²åŒºéšæœºæŠ½å–è®­ç»ƒæ‰¹æ¬¡ã€‚
- en: Train on this batch.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€æ‰¹æ¬¡ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: The purpose of the first two steps is to populate the replay buffer with samples
    from the environment (which are observation, action, reward, and next observation).
    The last two steps are for training our network.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸¤æ­¥çš„ç›®çš„æ˜¯å°†ç¯å¢ƒä¸­çš„æ ·æœ¬ï¼ˆåŒ…æ‹¬è§‚å¯Ÿã€åŠ¨ä½œã€å¥–åŠ±å’Œä¸‹ä¸€ä¸ªè§‚å¯Ÿï¼‰å¡«å……åˆ°é‡æ”¾ç¼“å†²åŒºä¸­ã€‚æœ€åä¸¤æ­¥ç”¨äºè®­ç»ƒæˆ‘ä»¬çš„ç½‘ç»œã€‚
- en: The following is an illustration of the preceding steps that will make potential
    parallelism slightly more obvious. On the left, the training flow is shown. The
    training steps use environments, the replay buffer, and our NN. The solid lines
    show data and code flow. Dotted lines represent usage of the NN for training and
    inference.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å‰é¢æ­¥éª¤çš„ç¤ºæ„å›¾ï¼Œæ—¨åœ¨ä½¿æ½œåœ¨çš„å¹¶è¡Œæ€§æ›´ä¸ºæ˜æ˜¾ã€‚åœ¨å·¦ä¾§æ˜¾ç¤ºäº†è®­ç»ƒæµç¨‹ã€‚è®­ç»ƒæ­¥éª¤ä½¿ç”¨äº†ç¯å¢ƒã€é‡æ”¾ç¼“å†²åŒºå’Œæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ã€‚å®çº¿è¡¨ç¤ºæ•°æ®å’Œä»£ç æµåŠ¨ã€‚è™šçº¿ä»£è¡¨ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„ä½¿ç”¨ã€‚
- en: '![PIC](img/B21150_09_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_09_06.png)'
- en: 'FigureÂ 9.6: A sequential diagram of the training process'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.6ï¼šè®­ç»ƒè¿‡ç¨‹çš„é¡ºåºå›¾
- en: 'As you can see, the top two steps communicate with the bottom only via the
    replay buffer and NN. This makes it possible to separate those two parts in different
    parallel processes. The following figure is a diagram of the scheme:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€è§ï¼Œå‰ä¸¤ä¸ªæ­¥éª¤ä¸ä¸‹éƒ¨ä»…é€šè¿‡é‡æ”¾ç¼“å†²åŒºå’Œç¥ç»ç½‘ç»œé€šä¿¡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸¤éƒ¨åˆ†åœ¨ä¸åŒçš„å¹¶è¡Œè¿›ç¨‹ä¸­åˆ†å¼€ã€‚ä»¥ä¸‹å›¾æ˜¯è¯¥æ–¹æ¡ˆçš„ç¤ºæ„å›¾ï¼š
- en: '![PIC](img/B21150_09_07.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_09_07.png)'
- en: 'FigureÂ 9.7: The parallel version of the training and play steps'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.7ï¼šè®­ç»ƒå’Œæ¸¸æˆæ­¥éª¤çš„å¹¶è¡Œç‰ˆæœ¬
- en: In the case of our Pong environment, it might look like an unnecessary complication
    of the code, but this separation might be extremely useful in some cases. Imagine
    that you have a very slow and heavy environment, so every step takes seconds of
    computations. Thatâ€™s not a contrived example; for instance, past NeurIPS competitions,
    such as Learning to Run, AI for Prosthetics Challenge, and Learn to Move ( [https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around](https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around)),
    have very slow neuromuscular simulators, so you have to separate experience gathering
    from the training process. In such cases, you can have many concurrent environments
    that deliver the experience to the central training process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„Pongç¯å¢ƒä¸­ï¼Œè¿™çœ‹èµ·æ¥åƒæ˜¯å¯¹ä»£ç çš„ä¸€ä¸ªä¸å¿…è¦çš„å¤æ‚åŒ–ï¼Œä½†è¿™ç§åˆ†ç¦»åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½éå¸¸æœ‰ç”¨ã€‚æƒ³è±¡ä¸€ä¸‹ä½ æœ‰ä¸€ä¸ªéå¸¸æ…¢ä¸”å¤æ‚çš„ç¯å¢ƒï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦æ•°ç§’çš„è®¡ç®—ã€‚è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªäººä¸ºçš„ä¾‹å­ï¼›ä¾‹å¦‚ï¼Œè¿‡å»çš„NeurIPSç«èµ›ï¼Œå¦‚Learning
    to Runã€AI for Prosthetics Challengeå’ŒLearn to Moveï¼ˆ[https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around](https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around)ï¼‰ä¸­ä½¿ç”¨äº†éå¸¸ç¼“æ…¢çš„ç¥ç»è‚Œè‚‰æ¨¡æ‹Ÿå™¨ï¼Œå› æ­¤ä½ å¿…é¡»å°†ç»éªŒæ”¶é›†ä¸è®­ç»ƒè¿‡ç¨‹åˆ†å¼€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥æœ‰è®¸å¤šå¹¶è¡Œçš„ç¯å¢ƒï¼Œå°†ç»éªŒä¼ é€’ç»™ä¸­å¤®è®­ç»ƒè¿‡ç¨‹ã€‚
- en: To turn our serial code into parallel code, some modifications are needed. In
    the file Chapter09/03_parallel.py, you can find the full source of the example.
    In the following, Iâ€™ll focus only on major differences.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†æˆ‘ä»¬çš„ä¸²è¡Œä»£ç è½¬å˜ä¸ºå¹¶è¡Œä»£ç ï¼Œéœ€è¦åšä¸€äº›ä¿®æ”¹ã€‚åœ¨æ–‡ä»¶Chapter09/03_parallel.pyä¸­ï¼Œä½ å¯ä»¥æ‰¾åˆ°è¿™ä¸ªç¤ºä¾‹çš„å®Œæ•´æºç ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°†åªå…³æ³¨ä¸»è¦çš„åŒºåˆ«ã€‚
- en: 'First, we use the torch.multiprocessing module as a drop-in replacement for
    the standard Python multiprocessing module:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨torch.multiprocessingæ¨¡å—æ¥æ›¿ä»£æ ‡å‡†çš„Python multiprocessingæ¨¡å—ï¼š
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The version from the standard library provides several primitives to work with
    code executed in separated processes, such as mp.Queue (distributed queue), mp.Process
    (child process), and others. PyTorch provides a wrapper around the standard multiprocessing
    library, which allows torch tensors to be shared between processes without copying
    them. This is implemented using shared memory in the case of CPU tensors, or CUDA
    references for tensors on a GPU. This sharing mechanism removes the major bottleneck
    when communication is performed within a single computer. Of course, in the case
    of truly distributed communications, you need to serialize data yourself.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†åº“ä¸­çš„ç‰ˆæœ¬æä¾›äº†å‡ ä¸ªç”¨äºå¤„ç†åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­æ‰§è¡Œçš„ä»£ç çš„åŸè¯­ï¼Œä¾‹å¦‚mp.Queueï¼ˆåˆ†å¸ƒå¼é˜Ÿåˆ—ï¼‰ã€mp.Processï¼ˆå­è¿›ç¨‹ï¼‰ç­‰ã€‚PyTorchæä¾›äº†ä¸€ä¸ªå¯¹æ ‡å‡†multiprocessingåº“çš„å°è£…ï¼Œå®ƒå…è®¸åœ¨è¿›ç¨‹é—´å…±äº«torchå¼ é‡ï¼Œè€Œæ— éœ€å¤åˆ¶å®ƒä»¬ã€‚è¿™æ˜¯é€šè¿‡å…±äº«å†…å­˜å®ç°çš„ï¼Œé’ˆå¯¹CPUå¼ é‡ä½¿ç”¨å…±äº«å†…å­˜ï¼Œæˆ–è€…é’ˆå¯¹GPUä¸Šçš„å¼ é‡ä½¿ç”¨CUDAå¼•ç”¨ã€‚è¿™ç§å…±äº«æœºåˆ¶æ¶ˆé™¤äº†åœ¨å•å°è®¡ç®—æœºå†…éƒ¨è¿›è¡Œé€šä¿¡æ—¶çš„ä¸»è¦ç“¶é¢ˆã€‚å½“ç„¶ï¼Œåœ¨çœŸæ­£åˆ†å¸ƒå¼çš„é€šä¿¡ä¸­ï¼Œä½ éœ€è¦è‡ªè¡Œåºåˆ—åŒ–æ•°æ®ã€‚
- en: The function play_func implements our â€œplay processâ€ and will be running in
    a separate child process started by the main process. Its responsibility is to
    get experience from the environment and push it into the shared queue. In addition,
    it wraps information about the end of the episode into a dataclass and pushes
    it into the same queue to keep the training process informed about the episode
    reward and the number of steps.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°play_funcå®ç°äº†æˆ‘ä»¬çš„â€œæ’­æ”¾è¿›ç¨‹â€ï¼Œå¹¶å°†åœ¨ç”±ä¸»è¿›ç¨‹å¯åŠ¨çš„å•ç‹¬å­è¿›ç¨‹ä¸­è¿è¡Œã€‚å®ƒçš„èŒè´£æ˜¯ä»ç¯å¢ƒä¸­è·å–ç»éªŒå¹¶å°†å…¶æ¨é€åˆ°å…±äº«é˜Ÿåˆ—ä¸­ã€‚æ­¤å¤–ï¼Œå®ƒå°†å…³äºå›åˆç»“æŸçš„ä¿¡æ¯å°è£…æˆä¸€ä¸ªæ•°æ®ç±»ï¼Œå¹¶å°†å…¶æ¨é€åˆ°åŒä¸€ä¸ªé˜Ÿåˆ—ä¸­ï¼Œä»¥ä¾¿è®­ç»ƒè¿‡ç¨‹å¯ä»¥è·å¾—å…³äºå›åˆå¥–åŠ±å’Œæ­¥æ•°çš„ä¿¡æ¯ã€‚
- en: 'The function batch_generator is replaced by the class BatchGenerator:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°batch_generatorè¢«ç±»BatchGeneratoræ‰€æ›¿ä»£ï¼š
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This class provides an iterator over batches and additionally mimics the ExperienceSource
    interface with the method pop_reward_steps(). The logic of this class is simple:
    it consumes the queue (populated by the â€œplay processâ€), and if the EpisodeEnded
    object was received, it remembers information about epsilon and the count of steps
    the game took; otherwise, the object is a piece of experience that needs to be
    added into the replay buffer. From the queue, we consume all objects available
    at the moment, and then the training batch is sampled from the buffer and yielded.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»æä¾›äº†ä¸€ä¸ªæ‰¹æ¬¡çš„è¿­ä»£å™¨ï¼Œå¹¶ä¸”é€šè¿‡æ–¹æ³•pop_reward_steps()é¢å¤–æ¨¡æ‹Ÿäº†ExperienceSourceæ¥å£ã€‚è¿™ä¸ªç±»çš„é€»è¾‘å¾ˆç®€å•ï¼šå®ƒæ¶ˆè´¹é˜Ÿåˆ—ï¼ˆç”±â€œæ’­æ”¾è¿›ç¨‹â€å¡«å……ï¼‰ï¼Œå¦‚æœæ¥æ”¶åˆ°EpisodeEndedå¯¹è±¡ï¼Œå®ƒä¼šè®°ä½å…³äºepsilonçš„ä¿¡æ¯å’Œæ¸¸æˆæ‰€ç»å†çš„æ­¥éª¤æ•°ï¼›å¦åˆ™ï¼Œè¯¥å¯¹è±¡å°±æ˜¯ä¸€æ¡éœ€è¦æ·»åŠ åˆ°é‡æ”¾ç¼“å†²åŒºçš„ç»éªŒã€‚ä»é˜Ÿåˆ—ä¸­ï¼Œæˆ‘ä»¬æ¶ˆè´¹å½“å‰å¯ç”¨çš„æ‰€æœ‰å¯¹è±¡ï¼Œç„¶åä»ç¼“å†²åŒºä¸­æŠ½æ ·è®­ç»ƒæ‰¹æ¬¡å¹¶è¿”å›ã€‚
- en: 'In the beginning of the training process, we need to tell torch.multiprocessing
    which start method to use:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹çš„å¼€å§‹ï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰torch.multiprocessingä½¿ç”¨å“ªç§å¯åŠ¨æ–¹æ³•ï¼š
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are several of them, but spawn is the most flexible.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬æœ‰å‡ ä¸ªï¼Œä½† spawn æ˜¯æœ€çµæ´»çš„ã€‚
- en: 'Then, the queue for communication is created, and we start our play_func as
    a separate process. As arguments, we pass the NN, hyperparameters, and queue to
    be used for experience:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œåˆ›å»ºç”¨äºé€šä¿¡çš„é˜Ÿåˆ—ï¼Œå¹¶å°† play_func ä½œä¸ºå•ç‹¬çš„è¿›ç¨‹å¯åŠ¨ã€‚æˆ‘ä»¬ä¼ é€’çš„å‚æ•°åŒ…æ‹¬ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ã€è¶…å‚æ•°ä»¥åŠç”¨äºç»éªŒçš„é˜Ÿåˆ—ï¼š
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The rest of the code is almost the same, with the exception that we use a BatchGenerator
    instance as the data source for Ignite and for EndOfEpisodeHandler (which requires
    the method pop_rewards_steps()). The following charts were obtained from my benchmarks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä½™çš„ä»£ç å‡ ä¹ç›¸åŒï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯æˆ‘ä»¬ä½¿ç”¨ BatchGenerator å®ä¾‹ä½œä¸º Ignite å’Œ EndOfEpisodeHandler çš„æ•°æ®æºï¼ˆåè€…éœ€è¦ä½¿ç”¨æ–¹æ³•
    pop_rewards_steps()ï¼‰ã€‚ä»¥ä¸‹å›¾è¡¨æ˜¯ä»æˆ‘çš„åŸºå‡†æµ‹è¯•ä¸­è·å¾—çš„ï¼š
- en: '![PIC](img/B21150_09_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_09_08.png)'
- en: 'FigureÂ 9.8: Reward and FPS in the baseline and parallel version'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.8ï¼šåŸºçº¿ç‰ˆæœ¬å’Œå¹¶è¡Œç‰ˆæœ¬ä¸­çš„å¥–åŠ±å’Œ FPS
- en: 'As you can see, in terms of FPS, we got an increase of 27%: 290 FPS in the
    parallel version versus 228 in the baseline. The average time to solve the environment
    decreased by 41%.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œåœ¨ FPS æ–¹é¢ï¼Œæˆ‘ä»¬è·å¾—äº† 27% çš„æå‡ï¼šå¹¶è¡Œç‰ˆæœ¬çš„ FPS ä¸º 290ï¼Œè€ŒåŸºçº¿ä¸º 228ã€‚è§£å†³ç¯å¢ƒçš„å¹³å‡æ—¶é—´å‡å°‘äº† 41%ã€‚
- en: In terms of FPS increase, the parallel version looks worse than the best result
    from the previous section (with 3 game environments, we got almost 400 FPS), but
    the convergence speed is better.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å°± FPS å¢åŠ è€Œè¨€ï¼Œå°½ç®¡å¹¶è¡Œç‰ˆæœ¬æ¯”å‰ä¸€èŠ‚ä¸­çš„æœ€ä½³ç»“æœï¼ˆä½¿ç”¨ 3 ä¸ªæ¸¸æˆç¯å¢ƒæ—¶ï¼Œæˆ‘ä»¬è·å¾—äº†æ¥è¿‘ 400 FPSï¼‰çœ‹èµ·æ¥è¦å·®ï¼Œä½†æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚
- en: Tweaking wrappers
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è°ƒæ•´å°è£…å™¨
- en: 'The final step in our sequence of experiments will be tweaking wrappers applied
    to the environment. This is very easy to overlook, as wrappers are normally written
    once or just borrowed from other code, applied to the environment, and left to
    sit there. But you should be aware of their importance in terms of the speed and
    convergence of your method. For example, the normal DeepMind-style stack of wrappers
    applied to an Atari game looks like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®éªŒåºåˆ—ä¸­çš„æœ€åä¸€æ­¥æ˜¯è°ƒæ•´åº”ç”¨äºç¯å¢ƒçš„å°è£…å™¨ã€‚è¿™ä¸ªæ­¥éª¤å¾ˆå®¹æ˜“è¢«å¿½è§†ï¼Œå› ä¸ºå°è£…å™¨é€šå¸¸æ˜¯å†™ä¸€æ¬¡æˆ–ä»å…¶ä»–ä»£ç å€Ÿç”¨ååº”ç”¨åˆ°ç¯å¢ƒä¸­çš„ï¼Œå¹¶ä¸”è¢«ç•™åœ¨é‚£é‡Œã€‚ä½†ä½ åº”è¯¥æ„è¯†åˆ°å®ƒä»¬åœ¨æ–¹æ³•çš„é€Ÿåº¦å’Œæ”¶æ•›æ€§æ–¹é¢çš„é‡è¦æ€§ã€‚ä¾‹å¦‚ï¼ŒDeepMind
    é£æ ¼çš„æ­£å¸¸ Atari æ¸¸æˆå°è£…å™¨å †æ ˆå¦‚ä¸‹ï¼š
- en: 'NoopResetEnv: Applies a random amount of NOOP operations to the game reset.
    In some Atari games, this is needed to remove weird initial observations.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NoopResetEnvï¼šå¯¹æ¸¸æˆé‡ç½®åº”ç”¨éšæœºæ•°é‡çš„ NOOP æ“ä½œã€‚åœ¨æŸäº› Atari æ¸¸æˆä¸­ï¼Œéœ€è¦è¿™ä¸ªæ“ä½œæ¥å»é™¤å¥‡æ€ªçš„åˆå§‹è§‚å¯Ÿå€¼ã€‚
- en: 'MaxAndSkipEnv: Applies max to N observations (four by default) and returns
    this as an observation for the step. This solves the â€œflickeringâ€ problem in some
    Atari games, when the game draws different portions of the screen on even and
    odd frames (a normal practice among Atari developers to overcome the platformâ€™s
    limitations and increase the complexity of the gameâ€™s sprites).'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MaxAndSkipEnvï¼šå¯¹ N æ¬¡è§‚å¯Ÿåº”ç”¨æœ€å¤§å€¼ï¼ˆé»˜è®¤å››æ¬¡ï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸ºè¯¥æ­¥çš„è§‚å¯Ÿè¿”å›ã€‚è¿™æ ·è§£å†³äº†æŸäº› Atari æ¸¸æˆä¸­çš„â€œé—ªçƒâ€é—®é¢˜ï¼Œå› ä¸ºè¿™äº›æ¸¸æˆåœ¨å¶æ•°å¸§å’Œå¥‡æ•°å¸§ä¸Šç»˜åˆ¶å±å¹•çš„ä¸åŒéƒ¨åˆ†ï¼ˆè¿™æ˜¯
    Atari å¼€å‘è€…å¸¸ç”¨çš„åšæ³•ï¼Œä»¥å…‹æœå¹³å°çš„é™åˆ¶å¹¶å¢åŠ æ¸¸æˆç²¾çµçš„å¤æ‚åº¦ï¼‰ã€‚
- en: 'EpisodicLifeEnv: In some games, this detects a lost life and turns this situation
    into the end of the episode. This significantly increases convergence, as our
    episodes become shorter (one single life versus several given by the game logic).
    This is relevant only for some games supported by the Atari 2600 Learning Environment.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: EpisodicLifeEnvï¼šåœ¨æŸäº›æ¸¸æˆä¸­ï¼Œè¿™ä¼šæ£€æµ‹ä¸¢å¤±çš„ç”Ÿå‘½å¹¶å°†å…¶è½¬åŒ–ä¸ºè¯¥é›†çš„ç»“æŸã€‚è¿™æ˜¾è‘—æé«˜äº†æ”¶æ•›æ€§ï¼Œå› ä¸ºæˆ‘ä»¬çš„å›åˆå˜å¾—æ›´çŸ­ï¼ˆä¸€æ¬¡ç”Ÿå‘½è€Œéæ¸¸æˆé€»è¾‘ç»™å‡ºçš„å¤šæ¬¡ç”Ÿå‘½ï¼‰ã€‚è¿™ä»…å¯¹
    Atari 2600 å­¦ä¹ ç¯å¢ƒæ”¯æŒçš„æŸäº›æ¸¸æˆç›¸å…³ã€‚
- en: 'FireResetEnv: Executes a FIRE action on game reset. Some games require this
    to start the gameplay. Without this, our environment becomes a partially observable
    Markov decision process (POMDP), which makes it impossible to converge.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: FireResetEnvï¼šåœ¨æ¸¸æˆé‡ç½®æ—¶æ‰§è¡Œ FIRE æ“ä½œã€‚æœ‰äº›æ¸¸æˆéœ€è¦è¿™ä¸ªæ“ä½œæ‰èƒ½å¼€å§‹æ¸¸æˆã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬çš„ç¯å¢ƒå°±å˜æˆäº†éƒ¨åˆ†å¯è§‚å¯Ÿçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œè¿™ä½¿å¾—æ— æ³•æ”¶æ•›ã€‚
- en: 'WarpFrame: Also known as ProcessFrame84, this converts an image to grayscale
    and resizes it to 84 Ã— 84.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WarpFrameï¼šä¹Ÿç§°ä¸º ProcessFrame84ï¼Œ å°†å›¾åƒè½¬æ¢ä¸ºç°åº¦å¹¶å°†å…¶è°ƒæ•´ä¸º 84 Ã— 84 å¤§å°ã€‚
- en: 'ClipRewardEnv: Clips the reward to a âˆ’1â€¦1 range, which unifies wide variability
    in scoring among different Atari games. For example, Pong might have a âˆ’21â€¦21
    score range, but the score in the River Raid game could be 0â€¦âˆ.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ClipRewardEnvï¼šå°†å¥–åŠ±è£å‰ªåˆ° âˆ’1â€¦1 èŒƒå›´ï¼Œè¿™ç»Ÿä¸€äº†ä¸åŒ Atari æ¸¸æˆä¸­åˆ†æ•°çš„å¹¿æ³›å˜åŒ–ã€‚ä¾‹å¦‚ï¼ŒPong çš„åˆ†æ•°èŒƒå›´å¯èƒ½æ˜¯ âˆ’21â€¦21ï¼Œè€Œ
    River Raid æ¸¸æˆçš„åˆ†æ•°å¯èƒ½æ˜¯ 0â€¦âˆã€‚
- en: 'FrameStack: Stacks N sequential observations into the stack (the default is
    four). As we already discussed in ChapterÂ [6](#), in some games, this is required
    to fulfill the Markov property. For example, in Pong, from one single frame, it
    is impossible to get the direction the ball is moving in.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: FrameStackï¼šå°†Nä¸ªè¿ç»­çš„è§‚å¯Ÿå †å åˆ°æ ˆä¸­ï¼ˆé»˜è®¤æ˜¯å››ä¸ªï¼‰ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[6](#)ç« ä¸­è®¨è®ºçš„é‚£æ ·ï¼Œåœ¨æŸäº›æ¸¸æˆä¸­ï¼Œè¿™æ˜¯å®ç°é©¬å°”å¯å¤«æ€§è´¨æ‰€å¿…éœ€çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨Pongæ¸¸æˆä¸­ï¼Œä»å•ä¸€çš„ä¸€å¸§å›¾åƒä¸­æ— æ³•å¾—çŸ¥çƒçš„è¿åŠ¨æ–¹å‘ã€‚
- en: 'The code of those wrappers was heavily optimized by many people and several
    versions exist. My personal favorite is the Stable Baselines3, which is a fork
    from the OpenAI Baselines project. You can find it here: [https://stable-baselines3.readthedocs.io/.](https://stable-baselines3.readthedocs.io/.)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŒ…è£…å™¨çš„ä»£ç ç»è¿‡äº†è®¸å¤šäººçš„ç²¾å¿ƒä¼˜åŒ–ï¼Œå¹¶ä¸”å­˜åœ¨å¤šä¸ªç‰ˆæœ¬ã€‚ä¸ªäººæœ€å–œæ¬¢çš„æ˜¯ Stable Baselines3ï¼Œå®ƒæ˜¯ OpenAI Baselines é¡¹ç›®çš„ä¸€ä¸ªåˆ†æ”¯ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å®ƒï¼š[https://stable-baselines3.readthedocs.io/.](https://stable-baselines3.readthedocs.io/.)
- en: But you shouldnâ€™t take this code as the final source of truth, as your concrete
    environment might have different requirements and specifics. For example, if you
    are interested in speeding up one specific game from the Atari suite, NoopResetEnv
    and MaxAndSkipEnv (more precisely, the max pooling operation from MaxAndSkipEnv)
    might not be needed. Another thing that could be tweaked is the number of frames
    in the FrameStack wrapper. The normal practice is to use four, but you need to
    understand that this number was used by DeepMind and other researchers to train
    on the full Atari 2600 game suite, which currently includes more than 50 games.
    For your specific case, a history of two frames might be enough to give you a
    performance boost, as less data will need to be processed by the NN.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ ä¸åº”è¯¥å°†è¿™æ®µä»£ç è§†ä¸ºæœ€ç»ˆçš„çœŸç†æºï¼Œå› ä¸ºä½ çš„å…·ä½“ç¯å¢ƒå¯èƒ½æœ‰ä¸åŒçš„éœ€æ±‚å’Œç»†èŠ‚ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰å…´è¶£åŠ é€Ÿ Atari å¥—ä»¶ä¸­çš„æŸä¸ªç‰¹å®šæ¸¸æˆï¼ŒNoopResetEnv
    å’Œ MaxAndSkipEnvï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œæ˜¯ MaxAndSkipEnv ä¸­çš„æœ€å¤§æ± åŒ–æ“ä½œï¼‰å¯èƒ½å¹¶ä¸éœ€è¦ã€‚å¦ä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„åœ°æ–¹æ˜¯ FrameStack åŒ…è£…å™¨ä¸­çš„å¸§æ•°ã€‚é€šå¸¸åšæ³•æ˜¯ä½¿ç”¨å››å¸§ï¼Œä½†ä½ éœ€è¦ç†è§£ï¼Œè¿™ä¸ªæ•°å­—æ˜¯
    DeepMind å’Œå…¶ä»–ç ”ç©¶äººå‘˜åœ¨å¯¹å®Œæ•´çš„ Atari 2600 æ¸¸æˆå¥—ä»¶è¿›è¡Œè®­ç»ƒæ—¶ä½¿ç”¨çš„ï¼Œè¯¥å¥—ä»¶ç›®å‰åŒ…å«è¶…è¿‡ 50 ä¸ªæ¸¸æˆã€‚å¯¹äºä½ çš„ç‰¹å®šæƒ…å†µï¼Œä½¿ç”¨ä¸¤å¸§çš„å†å²å¯èƒ½è¶³ä»¥æä¾›æ€§èƒ½æå‡ï¼Œå› ä¸ºç¥ç»ç½‘ç»œéœ€è¦å¤„ç†çš„æ•°æ®ä¼šæ›´å°‘ã€‚
- en: Finally, the image resize could be the bottleneck of wrappers, so you might
    want to optimize libraries used by wrappers, for example, rebuilding them or replacing
    them with faster versions. Prior to 2020, replacing the OpenCV2 library with the
    pillow-simd library gave a boost of about 50 frames per second. Nowadays, OpenCV2
    uses an optimized rescaling operation, so such replacement has no effect. But
    still, you might experiment with different scaling methods and different libraries.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå›¾åƒè°ƒæ•´å¤§å°å¯èƒ½æ˜¯åŒ…è£…å™¨çš„ç“¶é¢ˆï¼Œå› æ­¤ä½ å¯èƒ½éœ€è¦ä¼˜åŒ–åŒ…è£…å™¨ä½¿ç”¨çš„åº“ï¼Œä¾‹å¦‚é‡æ–°æ„å»ºå®ƒä»¬æˆ–æ›¿æ¢ä¸ºæ›´å¿«é€Ÿçš„ç‰ˆæœ¬ã€‚2020å¹´ä¹‹å‰ï¼Œå°† OpenCV2 åº“æ›¿æ¢ä¸º
    pillow-simd åº“èƒ½æé«˜å¤§çº¦ 50 å¸§æ¯ç§’çš„é€Ÿåº¦ã€‚å¦‚ä»Šï¼ŒOpenCV2 ä½¿ç”¨äº†ä¼˜åŒ–çš„é‡æ–°ç¼©æ”¾æ“ä½œï¼Œå› æ­¤è¿™ç§æ›¿æ¢å·²ä¸å†æœ‰æ•ˆã€‚ä½†ä½ ä»ç„¶å¯ä»¥å°è¯•ä¸åŒçš„ç¼©æ”¾æ–¹æ³•å’Œä¸åŒçš„åº“ã€‚
- en: 'Here, weâ€™ll apply the following changes to the Pong wrappers:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å¯¹ Pong çš„åŒ…è£…å™¨åº”ç”¨ä»¥ä¸‹æ›´æ”¹ï¼š
- en: Disable NoopResetEnv
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¦ç”¨ NoopResetEnv
- en: Replace MaxAndSkipEnv with a simplified version, which just skips four frames
    without max pooling
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨ç®€åŒ–ç‰ˆæœ¬æ›¿æ¢ MaxAndSkipEnvï¼Œåªè·³è¿‡å››å¸§è€Œä¸è¿›è¡Œæœ€å¤§æ± åŒ–ã€‚
- en: Keep only two frames in FrameStack
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªä¿ç•™ä¸¤ä¸ªå¸§åœ¨ FrameStack ä¸­
- en: 'To check the combined effect of our tweaks, weâ€™ll add the above changes to
    the modifications done in the previous two sections: several environments and
    parallel execution of playing and training.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ£€æŸ¥æˆ‘ä»¬è°ƒæ•´çš„ç»¼åˆæ•ˆæœï¼Œæˆ‘ä»¬å°†æŠŠä¸Šé¢çš„æ›´æ”¹æ·»åŠ åˆ°å‰ä¸¤èŠ‚æ‰€åšçš„ä¿®æ”¹ä¸­ï¼šå¤šä¸ªç¯å¢ƒå’Œå¹¶è¡Œæ‰§è¡Œæ¸¸æˆä¸è®­ç»ƒã€‚
- en: 'As the changes are not complex, letâ€™s just quickly discuss them without the
    actual code (the full code can be found in the files Chapter09/04_wrappers_n_env.py,
    Chapter09/04_wrappers_parallel.py, and Chapter09/lib/atari_wrappers.py):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™äº›æ›´æ”¹å¹¶ä¸å¤æ‚ï¼Œæˆ‘ä»¬å°±ä¸å±•ç¤ºå…·ä½“ä»£ç äº†ï¼ˆå®Œæ•´ä»£ç å¯ä»¥åœ¨æ–‡ä»¶ Chapter09/04_wrappers_n_env.pyã€Chapter09/04_wrappers_parallel.py
    å’Œ Chapter09/lib/atari_wrappers.py ä¸­æ‰¾åˆ°ï¼‰ï¼š
- en: Library atari_wrappers.py is quite simple â€” it contains the copy of the wrap_dqn
    function from PTAN and the AtariWrapper class from Stable Baselines3.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº“ atari_wrappers.py ç›¸å½“ç®€å•â€”â€”å®ƒåŒ…å«äº† PTAN ä¸­ wrap_dqn å‡½æ•°çš„å‰¯æœ¬å’Œ Stable Baselines3 ä¸­çš„ AtariWrapper
    ç±»ã€‚
- en: In AtariWrapper, the class MaxAndSkipEnv was replaced with a simplified version
    without max pooling between frames.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ AtariWrapper ä¸­ï¼ŒMaxAndSkipEnv ç±»è¢«ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬æ›¿ä»£ï¼Œå»é™¤äº†å¸§é—´çš„æœ€å¤§æ± åŒ–æ“ä½œã€‚
- en: Two modules, 04_wrappers_n_env.py and 04_wrappers_prallel.py, are just copies
    of 02_n_env.py and 03_parallel.py weâ€™ve already seen, with tweaked environment
    creation.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªæ¨¡å—ï¼Œ04_wrappers_n_env.py å’Œ 04_wrappers_prallel.pyï¼Œä»…ä»…æ˜¯æˆ‘ä»¬ä¹‹å‰è§è¿‡çš„ 02_n_env.py å’Œ
    03_parallel.py çš„å‰¯æœ¬ï¼Œç¯å¢ƒåˆ›å»ºç»è¿‡è°ƒæ•´ã€‚
- en: 'Thatâ€™s it! The following are charts with reward dynamics and FPS for both versions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼ä»¥ä¸‹æ˜¯ä¸¤ç§ç‰ˆæœ¬çš„å¥–åŠ±åŠ¨æ€å’ŒFPSå›¾è¡¨ï¼š
- en: '![PIC](img/B21150_09_09.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_09_09.png)'
- en: 'FigureÂ 9.9: Reward and FPS in the baseline and â€œ3 environments and 2 framesâ€
    version'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.9ï¼šåŸºå‡†ç‰ˆå’Œâ€œ3 ä¸ªç¯å¢ƒä¸ 2 å¸§ç‰ˆæœ¬â€çš„å¥–åŠ±ä¸ FPS
- en: '![PIC](img/B21150_09_10.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_09_10.png)'
- en: 'FigureÂ 9.10: Reward and FPS in the baseline and â€œparallel and 2 framesâ€ version'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.10ï¼šåŸºå‡†ç‰ˆå’Œâ€œå¹¶è¡Œä¸ 2 å¸§ç‰ˆæœ¬â€çš„å¥–åŠ±ä¸ FPS
- en: Out of curiosity, I also tried to reduce the number of frames kept in FrameStack
    to just one frame (you can repeat the experiment with the command-line argument
    --stack 1). Surprisingly, such a version was able to solve the game, but it took
    significantly longer in terms of games needed and the training became unstable
    (about 3 out of 8 training runs didnâ€™t converge at all). This might be an indication
    that Pong with just one frame is not POMDP and the agent still can learn how to
    win the game having just one frame as observation. But the efficiency of training
    definitely suffers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºäºå¥½å¥‡ï¼Œæˆ‘è¿˜å°è¯•å°† FrameStack ä¸­ä¿æŒçš„å¸§æ•°å‡å°‘åˆ°ä»…ä¸€ä¸ªå¸§ï¼ˆä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•° --stack 1 é‡å¤å®éªŒï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™æ ·çš„ç‰ˆæœ¬ä¹Ÿèƒ½è§£å†³æ¸¸æˆï¼Œä½†æ‰€éœ€çš„æ¸¸æˆæ¬¡æ•°æ˜¾è‘—å¢åŠ ï¼Œè®­ç»ƒå˜å¾—ä¸ç¨³å®šï¼ˆå¤§çº¦
    8 æ¬¡è®­ç»ƒä¸­çš„ 3 æ¬¡å®Œå…¨æ²¡æœ‰æ”¶æ•›ï¼‰ã€‚è¿™å¯èƒ½è¡¨æ˜ï¼Œåªæœ‰ä¸€ä¸ªå¸§çš„ Pong å¹¶ä¸æ˜¯ POMDPï¼Œä»£ç†ä»ç„¶å¯ä»¥ä»…å‡­ä¸€ä¸ªå¸§ä½œä¸ºè§‚å¯Ÿï¼Œå­¦ä¹ å¦‚ä½•èµ¢å¾—æ¸¸æˆã€‚ä½†è®­ç»ƒæ•ˆç‡è‚¯å®šä¼šå—åˆ°å½±å“ã€‚
- en: Benchmark results
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºå‡†ç»“æœ
- en: 'Iâ€™ve summarized our experiments in the following table. The percentages show
    the changes versus the baseline version:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å·²å°†æˆ‘ä»¬çš„å®éªŒæ€»ç»“åœ¨ä»¥ä¸‹è¡¨æ ¼ä¸­ã€‚ç™¾åˆ†æ¯”æ˜¾ç¤ºç›¸å¯¹äºåŸºå‡†ç‰ˆçš„å˜åŒ–ï¼š
- en: '| Step | FPS | FPS Î” | Time, mins | Time Î” |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| æ­¥éª¤ | FPS | FPS Î” | æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ | æ—¶é—´ Î” |'
- en: '| Baseline | 229 |  | 52.2 |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| åŸºå‡† | 229 |  | 52.2 |  |'
- en: '| Without torch.no_grad() | 219 | -4.3% | 51.0 | -2.3% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| æ²¡æœ‰ torch.no.grad() | 219 | -4.3% | 51.0 | -2.3% |'
- en: '| 3 environments | 395 | +72.5% | 36.0 | -31.0% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 3 ä¸ªç¯å¢ƒ | 395 | +72.5% | 36.0 | -31.0% |'
- en: '| Parallel version | 290 | +26.6% | 31.2 | -40.2% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| å¹¶è¡Œç‰ˆæœ¬ | 290 | +26.6% | 31.2 | -40.2% |'
- en: '| Wrappers + 3 environments | 448 | +95.6% | 47.4 | -9.2% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| åŒ…è£…å™¨ + 3 ä¸ªç¯å¢ƒ | 448 | +95.6% | 47.4 | -9.2% |'
- en: '| Wrappers + parallel | 325 | +41.9% | 30.0 | -42.5% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| åŒ…è£…å™¨ + å¹¶è¡Œ | 325 | +41.9% | 30.0 | -42.5% |'
- en: 'TableÂ 9.1: Optimization results'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 9.1ï¼šä¼˜åŒ–ç»“æœ
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, you saw several ways to improve the performance of the RL method
    using a pure engineering approach, which was in contrast to the â€œalgorithmicâ€
    or â€œtheoreticalâ€ approach covered in ChapterÂ [8](ch012.xhtml#x1-1240008). From
    my perspective, both approaches complement each other, and a good RL practitioner
    needs to both know the latest tricks that researchers have found and be aware
    of the implementation details.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œä½ çœ‹åˆ°äº†å‡ ç§é€šè¿‡çº¯å·¥ç¨‹æ–¹æ³•æé«˜ RL æ–¹æ³•æ€§èƒ½çš„æ–¹å¼ï¼Œè¿™ä¸ç¬¬ [8](ch012.xhtml#x1-1240008) ç« ä¸­ä»‹ç»çš„â€œç®—æ³•â€æˆ–â€œç†è®ºâ€æ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚ä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸¤ç§æ–¹æ³•æ˜¯äº’è¡¥çš„ï¼Œä¸€ä¸ªä¼˜ç§€çš„
    RL ä»ä¸šè€…éœ€è¦æ—¢äº†è§£ç ”ç©¶äººå‘˜å‘ç°çš„æœ€æ–°æŠ€å·§ï¼Œä¹Ÿè¦äº†è§£å®ç°ç»†èŠ‚ã€‚
- en: In the next chapter, we will begin applying our DQN knowledge to stocks trading
    as a practical example.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å¼€å§‹å°†æˆ‘ä»¬çš„ DQN çŸ¥è¯†åº”ç”¨äºè‚¡ç¥¨äº¤æ˜“ä½œä¸ºå®é™…ç¤ºä¾‹ã€‚
