- en: '5\. Artificial Intelligence: Clustering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 人工智能：聚类
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will introduce you to the fundamentals of clustering, an unsupervised
    learning approach in contrast with the supervised learning approaches seen in
    the previous chapters. You will be implementing different types of clustering,
    including flat clustering with the k-means algorithm and hierarchical clustering
    with the mean shift algorithm and the agglomerative hierarchical model. You will
    also learn how to evaluate the performance of your clustering model using intrinsic
    and extrinsic approaches. By the end of this chapter, you will be able to analyze
    data using clustering and apply this skill to solve challenges across a variety
    of fields.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍聚类的基础知识，这是一种无监督学习方法，与前几章中看到的监督学习方法相对。你将实现不同类型的聚类，包括使用k-means算法的平面聚类，以及使用均值漂移算法和聚合层次模型的层次聚类。你还将学习如何通过内在和外在方法来评估你的聚类模型的表现。到本章结束时，你将能够使用聚类分析数据，并将这一技能应用于解决各种领域的挑战。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, you were introduced to decision trees and their applications
    in classification. You were also introduced to regression in *Chapter 2*, *An
    Introduction to Regression*. Both regression and classification are part of the
    supervised learning approach. However, in this chapter, we will be looking at
    the unsupervised learning approach; we will be dealing with datasets that don't
    have any labels (outputs). It is up to the machines to tell us what the labels
    will be based on a set of parameters that we define. In this chapter, we will
    be performing unsupervised learning by using clustering algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你已经了解了决策树及其在分类中的应用。你还在*第二章*《回归介绍》中了解了回归。回归和分类都是监督学习方法的一部分。然而，在本章中，我们将探讨无监督学习方法；我们将处理没有标签（输出）的数据集。机器需要根据我们定义的一组参数来告诉我们标签是什么。在本章中，我们将通过使用聚类算法来执行无监督学习。
- en: 'We will use clustering to analyze data to find certain patterns and create
    groups. Apart from that, clustering can be used for many purposes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用聚类分析数据，以发现特定的模式并创建群体。除此之外，聚类还可以用于许多其他目的：
- en: Market segmentation detects the best stocks in the market you should be focusing
    on.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场细分帮助识别市场中你应该关注的最佳股票。
- en: Customer segmentation detects customer cohorts using their consumption patterns
    to recommend products better.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分通过使用顾客的消费模式来识别顾客群体，以更好地推荐产品。
- en: In computer vision, image segmentation is performed using clustering. Using
    this, we can find different objects in an image.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算机视觉中，图像分割是通过聚类来执行的。通过这种方式，我们可以在图像中找到不同的物体。
- en: Clustering can be also be combined with classification to generate a compact
    representation of multiple features (inputs), which can then be fed to a classifier.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类也可以与分类相结合，生成多种特征（输入）的紧凑表示，然后可以将其输入分类器。
- en: Clustering can also filter data points by detecting outliers.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类还可以通过检测异常值来筛选数据点。
- en: Regardless of whether we are applying clustering to genetics, videos, images,
    or social networks, if we analyze data using clustering, we may find similarities
    between data points that are worth treating uniformly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是在对基因学、视频、图像还是社交网络应用聚类，如果我们使用聚类分析数据，我们可能会发现数据点之间的相似性，值得将其统一处理。
- en: For instance, consider a store manager, who is responsible for ensuring the
    profitability of their store. The products in the store are divided into different
    categories, and there are different customers who prefer different items. Each
    customer has their own preferences, but they have some similarities between them.
    You might have a customer who is interested in bio products, who tends to choose
    organic products, which are also of interest to a vegetarian customer. Even if
    they are different, they have similarities in their preferences or patterns as
    they both tend to buy organic vegetables. This can be treated as an example of
    clustering.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个店铺经理，负责确保店铺的盈利。店铺中的产品被划分为不同的类别，而有不同的顾客偏好不同的商品。每个顾客有自己的偏好，但他们之间也有一些相似之处。你可能会有一个顾客对生物产品感兴趣，倾向于选择有机产品，这也正是素食顾客感兴趣的商品。即使他们有所不同，但在偏好或模式上有相似之处，因为他们都倾向于购买有机蔬菜。这可以看作是一个聚类的例子。
- en: In *Chapter 3*, *An Introduction to Classification*, you learned about classification,
    which is a part of the supervised learning approach. In a classification problem,
    we use labels to train a model in order to be able to classify data points. With
    clustering, as we do not have labels for our features, we need to let the model
    figure out the clusters to which these features belong. This is usually based
    on the distance between each data point.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*，*分类简介*中，你学习了分类，它是监督学习方法的一部分。在分类问题中，我们使用标签来训练模型，以便能够对数据点进行分类。而在聚类中，由于我们没有特征标签，我们需要让模型自己找出这些特征属于哪个簇。这通常基于每个数据点之间的距离。
- en: In this chapter, you will learn about the k-means algorithm, which is the most
    widely used algorithm for clustering, but first, we need to define what the clustering
    problem is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习 k-means 算法，它是最广泛使用的聚类算法，但在此之前，我们需要先定义聚类问题。
- en: Defining the Clustering Problem
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义聚类问题
- en: We shall define the clustering problem so that we will be able to find similarities
    between our data points. For instance, suppose we have a dataset that consists
    of points. Clustering helps us understand this structure by describing how these
    points are distributed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义聚类问题，以便能够发现数据点之间的相似性。例如，假设我们有一个由多个数据点组成的数据集。聚类帮助我们通过描述这些数据点的分布情况来理解数据的结构。
- en: 'Let''s look at an example of data points in a two-dimensional space in *Figure
    5.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下*图 5.1*中二维空间中数据点的示例：
- en: '![Figure 5.1: Data points in a two-dimensional space'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：二维空间中的数据点'
- en: '](img/B16060_05_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_01.jpg)'
- en: 'Figure 5.1: Data points in a two-dimensional space'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：二维空间中的数据点
- en: 'Now, have a look, at *Figure 5.2*. It is evident that there are **three** clusters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看*图 5.2*。很明显，存在**三个**簇：
- en: '![Figure 5.2: Three clusters formed using the data points in a two-dimensional
    space'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：使用二维空间中的数据点形成的三个簇'
- en: '](img/B16060_05_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_02.jpg)'
- en: 'Figure 5.2: Three clusters formed using the data points in a two-dimensional
    space'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：使用二维空间中的数据点形成的三个簇
- en: The three clusters were easy to detect because the points are close to one another.
    Here, you can see that clustering determines the data points that are close to
    each other. You may have also noticed that the data points `M`1, `O`1, and `N`1
    do not belong to any cluster; these are the **outlier points**. The clustering
    algorithm you build should be prepared to treat these outlier points properly,
    without moving them into a cluster.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个簇很容易识别，因为这些点彼此接近。这里你可以看到，聚类算法能够识别彼此接近的数据点。你可能还注意到，数据点`M`1、`O`1 和 `N`1并不属于任何一个簇；这些是**离群点**。你构建的聚类算法应该能够妥善处理这些离群点，而不是将它们归入某个簇。
- en: While it is easy to recognize clusters in a two-dimensional space, we normally
    have multidimensional data points, which is where we have more than two features.
    Therefore, it is important to know which data points are close to one other. Also,
    it is important to define the distance metrics that detect whether data points
    are close to each other. One well-known distance metric is Euclidean distance,
    which we learned about in *Chapter 1*, *Introduction to Artificial Intelligence*.
    In mathematics, we often use Euclidean distance to measure the distance between
    two points. Therefore, Euclidean distance is an intuitive choice when it comes
    to clustering algorithms so that we can determine the proximity of data points
    when locating clusters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在二维空间中很容易识别簇，但我们通常处理的是多维数据点，即具有多个特征的数据。因此，了解哪些数据点彼此接近非常重要。此外，定义用于检测数据点之间接近程度的距离度量也至关重要。一个著名的距离度量是欧几里得距离，我们在*第
    1 章*，*人工智能简介*中学习过。在数学中，我们通常使用欧几里得距离来测量两个点之间的距离。因此，欧几里得距离是聚类算法中一个直观的选择，使我们能够在定位簇时判断数据点的接近度。
- en: 'However, there is one drawback to most distance metrics, including Euclidean
    distance: the more we increase the dimensions, the more uniform these distances
    will become compared to each other. When we only have a few dimensions or features,
    it is easy to see which point is the closest to another one. However, when we
    add more features, the relevant features get embedded with all the other data
    and it becomes very hard to distinguish the relevant features from the others
    as they act as noise for our model. Therefore, getting rid of these noisy features
    may greatly increase the accuracy of our clustering model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数距离度量方法（包括欧几里得距离）存在一个缺点：随着维度的增加，这些距离会变得更加均匀。只有在维度或特征较少时，才容易看到哪些点与其他点最接近。然而，当我们添加更多特征时，相关特征与所有其他数据一起嵌入，并且很难从中区分出真正的相关特征，因为它们像噪声一样影响模型。因此，去除这些噪声特征可能会大大提高我们聚类模型的准确性。
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Noise in a dataset can be irrelevant information or randomness that is unwanted.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的噪声可以是无关信息或不需要的随机性。
- en: In the next section, we will be looking at two different clustering approaches.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨两种不同的聚类方法。
- en: Clustering Approaches
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类方法
- en: 'There are two types of clustering:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类有两种类型：
- en: '**Flat**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平面聚类**'
- en: '**Hierarchical**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**'
- en: In flat clustering, we specify the number of clusters we would like the machine
    to find. One example of flat clustering is the k-means algorithm, where *k* specifies
    the number of clusters we would like the algorithm to use.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在平面聚类中，我们需要指定机器要找到的聚类数目。平面聚类的一个例子是 k-means 算法，其中 *k* 指定我们希望算法使用的聚类数目。
- en: In hierarchical clustering, however, the machine learning algorithm itself finds
    out the number of clusters that are needed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在层次聚类中，机器学习算法本身会自动确定需要的聚类数量。
- en: 'Hierarchical clustering also has two approaches:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类也有两种方法：
- en: '**Agglomerative or bottom-up hierarchical clustering** treats each point as
    a cluster to begin with. Then, the closest clusters are grouped together. The
    grouping is repeated until we reach a single cluster with every data point.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**凝聚性或自底向上的层次聚类**开始时将每个点视为一个独立的聚类。然后，最接近的聚类会合并在一起。这个过程会一直重复，直到最终得到一个包含所有数据点的单一聚类。'
- en: '**Divisive or top-down hierarchical clustering** treats data points as if they
    were all in one single cluster at the start. Then the cluster is divided into
    smaller clusters by choosing the furthest data points. The splitting is repeated
    until each data point becomes its own cluster.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分裂性或自顶向下的层次聚类**开始时将所有数据点视为一个聚类。然后，选择最远的数据点将聚类拆分成更小的聚类。这个过程会一直重复，直到每个数据点都变成自己的聚类。'
- en: '*Figure 5.3* gives you a much more accurate description of these two clustering approaches.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.3* 给出了这两种聚类方法的更准确描述。'
- en: '![Figure 5.3: Figure showing the two approaches'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：展示这两种方法的图'
- en: '](img/B16060_05_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_03.jpg)'
- en: 'Figure 5.3: Figure showing the two approaches'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：展示这两种方法的图
- en: Now that we are familiar with the different clustering approaches, let's take
    a look at the different clustering algorithms supported by scikit-learn.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了不同的聚类方法，接下来让我们看看 scikit-learn 支持的不同聚类算法。
- en: Clustering Algorithms Supported by scikit-learn
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn 支持的聚类算法
- en: 'In this chapter, we will learn about two clustering algorithms supported by
    scikit-learn:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习 scikit-learn 支持的两种聚类算法：
- en: The k-means algorithm
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 算法
- en: The mean shift algorithm
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值漂移算法
- en: '**K-means** is an example of flat clustering, where we must specify the number
    of clusters in advance. k-means is a general-purpose clustering algorithm that
    performs well if the number of clusters is not too high and the size of the clusters
    is uniform.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means** 是平面聚类的一个示例，其中我们必须事先指定聚类的数量。k-means 是一种通用的聚类算法，当聚类数量不是很高且聚类大小相对均匀时，它表现良好。'
- en: '**Mean shift** is an example of hierarchical clustering, where the clustering
    algorithm determines the number of clusters. Mean shift is used when we do not
    know the number of clusters in advance. In contrast with k-means, mean shift supports
    use cases where there may be many clusters present, even if the size of the clusters
    greatly varies.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**均值漂移**是层次聚类的一种示例，在这种算法中，聚类算法会自动确定聚类的数量。当我们事先不知道聚类数量时，可以使用均值漂移。与 k-means 相比，均值漂移支持那些聚类数量很多且大小差异较大的应用场景。'
- en: Scikit-learn contains many other algorithms, but we will be focusing on the
    k-means and mean shift algorithms in this chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 包含许多其他算法，但本章我们将重点关注 k-means 和均值漂移算法。
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For a complete description of clustering algorithms, including performance comparisons,
    visit the clustering page of scikit-learn at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有关聚类算法的完整描述，包括性能比较，请访问 scikit-learn 的聚类页面：[http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)。
- en: In the next section, we begin with the k-means algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从 k-means 算法开始。
- en: The K-Means Algorithm
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-Means 算法
- en: 'The k-means algorithm is a flat clustering algorithm, as mentioned previously.
    It works as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法是一种平面聚类算法，正如前面提到的，它的工作原理如下：
- en: Set the value of *k*.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 *k* 的值。
- en: Choose *k* data points from the dataset that are the initial centers of the
    individual clusters.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中选择 *k* 个数据点作为各个聚类的初始中心。
- en: Calculate the distance from each data point to the chosen center points and
    group each point in the cluster whose initial center is the closest to the data point.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个数据点到所选中心点的距离，并将每个点归类到初始中心点最接近的聚类中。
- en: Once all the points are in one of the *k* clusters, calculate the center point
    of each cluster. This center point does not have to be an existing data point
    in the dataset; it is simply an average.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦所有点都被分配到 *k* 个聚类中，计算每个聚类的中心点。这个中心点不必是数据集中现有的数据点，它只是一个平均值。
- en: Repeat this process of assigning each data point to the cluster whose center
    is closest to the data point. Repetition continues until the center points no
    longer move.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复这一过程，将每个数据点分配给与数据点最接近的聚类中心。重复此过程，直到中心点不再移动。
- en: 'To ensure that the k-means algorithm terminates, we need the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 k-means 算法能够终止，我们需要以下条件：
- en: A maximum threshold value at which the algorithm will then terminate
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设定一个最大阈值，算法将在达到此值时终止。
- en: A maximum number of repetitions of shifting the moving points
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动点的最大重复次数。
- en: Due to the nature of the k-means algorithm, it will have a hard time dealing
    with clusters that greatly vary in size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 k-means 算法的特性，它很难处理大小差异较大的聚类。
- en: 'The k-means algorithm has many use cases that are part of our everyday lives,
    such as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法有许多应用案例，已经成为我们日常生活的一部分，例如：
- en: '**Market segmentation**: Companies gather all sorts of data on their customers.
    Performing k-means clustering analysis on their customers will reveal customer
    segments (clusters) with defined characteristics. Customers belonging to the same
    segment can be seen as having similar patterns or preferences.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场细分**：公司收集各种关于客户的数据。对客户进行 k-means 聚类分析将揭示具有明确特征的客户群体（聚类）。属于同一细分市场的客户可以看作有着相似的模式或偏好。'
- en: '**Tagging of content**: Any content (videos, books, documents, movies, or photos)
    can be assigned tags in order to group together similar content or themes. These
    tags are the result of clustering.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容标签化**：任何内容（视频、书籍、文档、电影或照片）都可以分配标签，以便将相似的内容或主题进行分组。这些标签是聚类的结果。'
- en: '**Detection of fraud and criminal activities**: Fraudsters often leave clues
    in the form of unusual behaviors compared to other customers. For instance, in
    the car insurance industry, a normal customer will make a claim for a damaged
    car arising from an incident, whereas fraudsters will make claims for deliberate
    damage. Clustering can help detect whether the damage has arisen from a real accident
    or from a fake accident.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈和犯罪活动检测**：欺诈者通常会留下与其他客户行为不同的异常线索。例如，在车险行业，正常客户会因事故索赔损坏的汽车，而欺诈者则会索赔故意损坏的车辆。聚类可以帮助检测损坏是否来自真实事故或伪造的事故。'
- en: In the next exercise, we will be implementing the k-means algorithm in scikit-learn.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将实现 scikit-learn 中的 k-means 算法。
- en: 'Exercise 5.01: Implementing K-Means in scikit-learn'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.01：在 scikit-learn 中实现 K-Means。
- en: In this exercise, we will be plotting a dataset in a two-dimensional plane and
    performing clustering on it using the k-means algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在二维平面上绘制一个数据集，并使用 k-means 算法对其进行聚类。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件。
- en: 'Now create an artificial dataset as a NumPy array to demonstrate the k-means
    algorithm. The data points are shown in the following code snippet:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建一个作为NumPy数组的人工数据集，以演示k-means算法。数据点如下代码片段所示：
- en: '[PRE0]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, plot these data points in the two-dimensional plane using `matplotlib.pyplot`,
    as shown in the following code snippet:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`matplotlib.pyplot`在二维平面上绘制这些数据点，如下代码片段所示：
- en: '[PRE1]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The expected output is this:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出是这样的：
- en: '![Figure 5.4: Graph showing the data points on a two-dimensional plane using'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.4：使用matplotlib.pyplot在二维平面上显示数据点的图表'
- en: matplotlib.pyplot
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: matplotlib.pyplot
- en: '](img/B16060_05_04.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_05_04.jpg)'
- en: 'Figure 5.4: Graph showing the data points on a two-dimensional plane using
    matplotlib.pyplot'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.4：使用matplotlib.pyplot在二维平面上显示数据点的图表
- en: Note
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'We used the `transpose` array method to get the values of the first feature
    and the second feature. We could also use proper array indexing to access these
    columns: `dataPoints[:,0]`, which is equivalent to `dataPoints.transpose()[0]`.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了`transpose`数组方法来获取第一个特征和第二个特征的值。我们也可以使用适当的数组索引来访问这些列：`dataPoints[:,0]`，这等同于`dataPoints.transpose()[0]`。
- en: Now that we have the data points, it is time to execute the k-means algorithm
    on them.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经有了数据点，是时候对其执行k-means算法了。
- en: 'Define `k` as `3` in the k-means algorithm. We expect a cluster in the bottom-left,
    top-left, and bottom-right corners of the graph. Add `random_state = 8` in order
    to reproduce the same results:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在k-means算法中，将`k`定义为`3`。我们期望图表的左下角、左上角和右下角各有一个聚类。添加`random_state = 8`以便重现相同的结果：
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code snippet, we have used the `KMeans` module from `sklearn.cluster`.
    As always with `sklearn`, we need to define a model with the parameter and then
    fit the model on the dataset.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`sklearn.cluster`中的`KMeans`模块。像往常一样，在使用`sklearn`时，我们需要先定义一个模型并设置参数，然后将模型应用到数据集上。
- en: 'The expected output is this:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出是这样的：
- en: '[PRE3]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output shows all the parameters for our k-means models, but the important
    ones are:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出显示了我们k-means模型的所有参数，但重要的参数是：
- en: '`max_iter`: Represents the maximum number of times the k-means algorithm will
    iterate through.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`max_iter`：表示k-means算法的最大迭代次数。'
- en: '`n_clusters`: Represents the number of clusters to be formed by the k-means
    algorithm.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`n_clusters`：表示由k-means算法形成的聚类数量。'
- en: '`n_init`: Represents the number of times the k-means algorithm will initialize
    a random point.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`n_init`：表示k-means算法初始化随机点的次数。'
- en: '`tol`: Represents the threshold for checking whether the k-means algorithm
    can terminate.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tol`：表示检查k-means算法是否可以终止的阈值。'
- en: 'Once the clustering is done, access the center point of each cluster as shown
    in the following code snippet:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类完成后，访问每个聚类的中心点，如下代码片段所示：
- en: '[PRE4]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of `centers` will be as follows:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`centers`的输出将如下所示：'
- en: '[PRE5]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This output is showing the coordinates of the center of our three clusters.
    If you look back at *Figure 5.4*, you will see that the center points of the clusters
    appear to be in the bottom-left, (`1.3, 1.5`), the top-left (`3.1, 9.6`), and
    the bottom-right (`7.265, 0.75`) corners of the graph. The *x* coordinate of the
    top-left cluster is `3.1`, most likely because it contains our outlier data point
    at `[10, 10]`.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该输出显示了我们三个聚类中心的坐标。如果你回顾*图5.4*，你会看到聚类的中心点似乎位于图表的左下角（`1.3, 1.5`）、左上角（`3.1, 9.6`）和右下角（`7.265,
    0.75`）。左上角聚类的*x*坐标为`3.1`，很可能是因为它包含了我们位于`[10, 10]`的异常数据点。
- en: 'Next, plot the clusters with different colors and their center points. To find
    out which data point belongs to which cluster, we must query the `labels` property
    of the k-means classifier, as shown in the following code snippet:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，用不同的颜色绘制聚类及其中心点。要找出哪个数据点属于哪个聚类，我们必须查询k-means分类器的`labels`属性，如下代码片段所示：
- en: '[PRE6]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of `labels` will be as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`labels`的输出将如下所示：'
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output array shows which data point belongs to which cluster. This is all
    we need to plot the data.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出数组显示了每个数据点属于哪个聚类。这就是我们绘制数据所需要的所有信息。
- en: 'Now, plot the data as shown in the following code snippet:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下代码片段绘制数据：
- en: '[PRE8]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code snippets, we used the `matplotlib` library to plot the
    data points with the center of each coordinate. Each cluster has its marker (`x`,
    `+`, and `-`), and its center is represented by a filled circle.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`matplotlib`库将数据点与每个坐标的中心一起绘制。每个聚类都有自己的标记（`x`、`+`和`-`），其中心由一个实心圆表示。
- en: 'The expected output is this:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出是这样的：
- en: '![Figure 5.5: Graph showing the center points of the three clusters'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.5：显示三个簇的中心点图'
- en: '](img/B16060_05_05.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_05_05.jpg)'
- en: 'Figure 5.5: Graph showing the center points of the three clusters'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.5：显示三个簇的中心点图
- en: Having a look at *Figure 5.5*, you can see that the center points are inside
    their clusters, which are represented by the `x`, `+`, and `-` marks.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看*图 5.5*，你可以看到中心点位于它们的簇内，这些簇由`x`、`+`和`-`标记表示。
- en: 'Now, reuse the same code and choose only two clusters instead of three:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，重新使用相同的代码，只选择两个簇而不是三个：
- en: '[PRE9]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The expected output is this:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望输出为：
- en: '![Figure 5.6: Graph showing the data points of the two clusters'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.6：显示两个簇的数据点图'
- en: '](img/B16060_05_06.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_05_06.jpg)'
- en: 'Figure 5.6: Graph showing the data points of the two clusters'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.6：显示两个簇的数据点图
- en: This time, we only have `x` and `+` points, and we can clearly see a bottom
    cluster and a top cluster. Interestingly, the top cluster in the second try contains
    the same points as the top cluster in the first try. The bottom cluster of the
    second try consists of the data points joining the bottom-left and the bottom-right
    clusters of the first try.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一次，我们只有`x`和`+`两种点，并且可以清楚地看到一个底部簇和一个顶部簇。有趣的是，第二次尝试中的顶部簇包含了与第一次尝试中的顶部簇相同的点。第二次尝试中的底部簇由第一次尝试中连接左下角和右下角簇的数据点组成。
- en: 'Finally, use the k-means model for prediction as shown in the following code
    snippet. The output will be an array containing the cluster numbers belonging
    to each data point:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用k-means模型进行预测，如以下代码片段所示。输出将是一个包含每个数据点所属簇编号的数组：
- en: '[PRE10]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of `predictions` is as follows:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`predictions`的输出如下：'
- en: '[PRE11]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This means that our first point belongs to the first cluster (at the bottom)
    and the second point belongs to the second cluster (at the top).
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着我们的第一个点属于第一个簇（位于底部），第二个点属于第二个簇（位于顶部）。
- en: Note
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2CpvMDo](https://packt.live/2CpvMDo).
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2CpvMDo](https://packt.live/2CpvMDo)。
- en: You can also run this example online at [https://packt.live/2Nnv7F2](https://packt.live/2Nnv7F2).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2Nnv7F2](https://packt.live/2Nnv7F2)上在线运行此示例。你必须执行整个Notebook才能获得期望的结果。
- en: By completing this exercise, you were able to use a simple k-means clustering
    model on sample data points.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本练习后，你能够在样本数据点上使用一个简单的k-means聚类模型。
- en: The Parameterization of the K-Means Algorithm in scikit-learn
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn中的K-Means算法参数化
- en: Like the classification and regression models in *Chapter 2*, *An Introduction
    to Regression*, *Chapter 3*, *An Introduction to Classification*, and *Chapter
    4*, *An Introduction to Decision Trees*, the k-means algorithm can also be parameterized.
    The complete list of parameters can be found at [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与*第2章*《回归介绍》、*第3章*《分类介绍》和*第4章*《决策树介绍》中的分类和回归模型类似，k-means算法也可以进行参数化。完整的参数列表可以在[http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)找到。
- en: 'Some examples are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一些示例如下：
- en: '`n_clusters`: The number of clusters into which the data points are separated.
    The default value is `8`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_clusters`：将数据点分为的簇的数量。默认值为`8`。'
- en: '`max_iter`: The maximum number of iterations.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_iter`：最大迭代次数。'
- en: '`tol`: The threshold for checking whether we can terminate the k-means algorithm.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tol`：检查是否可以终止k-means算法的阈值。'
- en: 'We also used two attributes to retrieve the cluster center points and the clusters themselves:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了两个属性来检索簇中心点和簇本身：
- en: '`cluster_centers_`: This returns the coordinates of the cluster center points.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster_centers_`：返回簇中心点的坐标。'
- en: '`labels_`: This returns an array of integers representing the number of clusters
    the data point belongs to. Numbering starts from zero.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels_`：返回一个整数数组，表示数据点所属的簇编号。编号从零开始。'
- en: 'Exercise 5.02: Retrieving the Center Points and the Labels'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.02：检索中心点和标签
- en: In this exercise, you will be able to understand the usage of `cluster_centers_`
    and `labels_`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你将能够理解`cluster_centers_`和`labels_`的用法。
- en: 'The following steps will help you complete the exercise:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: 'Next, create the same 12 data points from *Exercise 5.01*, *Implementing K-Means
    in scikit-learn*, but here, perform k-means clustering with four clusters, as
    shown in the following code snippet:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建与*练习5.01*、*在scikit-learn中实现K-Means*相同的12个数据点，但在这里执行四个簇的k-means聚类，如下所示的代码片段：
- en: '[PRE12]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of `centers` is as follows:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`centers`的输出如下：'
- en: '[PRE13]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output of the `cluster_centers_` property shows the *x* and *y* coordinates
    of the center points.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`cluster_centers_`属性的输出显示了中心点的*x*和*y*坐标。'
- en: From the output, we can see the `4` centers, which are bottom right (`7.6, 0.75`),
    top left (`1.3, 9.5`), bottom left (`1.3, 1.5`), and top right (`10, 10`). We
    can also note that the fourth cluster (the top-right cluster) is only made of
    a single data point. This data point can be assumed to be an **outlier**.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出结果中，我们可以看到`4`个中心，分别是右下角（`7.6, 0.75`）、左上角（`1.3, 9.5`）、左下角（`1.3, 1.5`）和右上角（`10,
    10`）。我们还可以注意到，第四个簇（右上角簇）只有一个数据点。这个数据点可以被认为是一个**异常值**。
- en: 'Now, apply `labels_ property` on the cluster:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在簇上应用`labels_属性`：
- en: '[PRE14]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of `labels` is as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`labels`的输出如下：'
- en: '[PRE15]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `labels_` property is an array of length `12`, showing the cluster of each
    of the `12` data points it belongs to. The first cluster is associated with the
    number 0, the second is associated with 1, the third is associated with 2, and
    so on (remember that Python indexes always start from 0 and not 1).
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`labels_`属性是一个长度为`12`的数组，显示了每个`12`个数据点所属的簇。第一个簇与数字0相关，第二个与1相关，第三个与2相关，以此类推（请记住，Python的索引从0开始，而不是从1开始）。'
- en: Note
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3dmHsDX](https://packt.live/3dmHsDX).
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参考[https://packt.live/3dmHsDX](https://packt.live/3dmHsDX)。
- en: You can also run this example online at [https://packt.live/2B0ebld](https://packt.live/2B0ebld).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/2B0ebld](https://packt.live/2B0ebld)上在线运行此示例。您必须执行整个Notebook以获得期望的结果。
- en: By completing this exercise, you were able to retrieve the coordinates of a
    cluster's center. You were also able to see which label (cluster) each data point
    has been assigned to.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成此练习，您可以获取簇的中心坐标。同时，您也可以看到每个数据点被分配到哪个标签（簇）。
- en: K-Means Clustering of Sales Data
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 销售数据的K-Means聚类
- en: In the upcoming activity, we will be looking at sales data, and we will perform
    k-means clustering on that sales data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的活动中，我们将查看销售数据，并对这些销售数据执行k-means聚类。
- en: 'Activity 5.01: Clustering Sales Data Using K-Means'
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.01：使用K-Means聚类销售数据
- en: In this activity, you will work on the Sales Transaction Dataset Weekly dataset,
    which contains the weekly sales data of 800 products over 1 year. Our dataset
    won't contain any information regarding the product except sales.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，您将处理销售交易数据集（Weekly dataset），该数据集包含了800种产品在1年内的每周销售数据。我们的数据集不会包含任何有关产品的信息，除了销售数据。
- en: Your goal will be to identify products with similar sales trends using the k-means
    clustering algorithm. You will have to experiment with the number of clusters
    in order to find the optimal number of clusters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是使用k-means聚类算法识别具有相似销售趋势的产品。你需要尝试不同的簇数，以找到最佳簇数。
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset can be found at [https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在[https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly)找到。
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/3hVH42v](https://packt.live/3hVH42v).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件也可以在我们的GitHub存储库中找到：[https://packt.live/3hVH42v](https://packt.live/3hVH42v)。
- en: 'Citation: *Tan, S., & San Lau, J. (2014). Time series clustering: A superior
    alternative for market basket analysis. In Proceedings of the First International
    Conference on Advanced Data and Information Engineering (DaEng-2013) (pp. 241–248)*.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：*Tan, S., & San Lau, J. (2014). 时间序列聚类：市场篮子分析的优越替代方法。在《第一届国际先进数据与信息工程会议论文集（DaEng-2013）》（第241-248页）中*。
- en: 'The following steps will help you complete this activity:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助您完成此活动：
- en: Open a new Jupyter Notebook file.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: Load the dataset as a DataFrame and inspect the data.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集加载为 DataFrame，并检查数据。
- en: Create a new DataFrame without the unnecessary columns using the `drop` function
    from pandas (that is, the first `55` columns of the dataset) and use the `inplace`
    parameter, which is a part of pandas.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 的`drop`函数创建一个没有不必要列的新 DataFrame（即数据集中的前`55`列），并使用 pandas 的`inplace`参数。
- en: Create a k-means clustering model with `8` clusters and with `random_state =
    8`.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含`8`个集群且`random_state = 8`的 k-means 聚类模型。
- en: Retrieve the labels from the first clustering model.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个聚类模型中提取标签。
- en: From the first DataFrame, `df`, keep only the `W` columns and the labels as
    a new column.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个 DataFrame，`df`，只保留`W`列和标签作为新列。
- en: Perform the required aggregation using the `groupby` function from pandas in
    order to obtain the yearly average sale of each cluster.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 中的`groupby`函数进行所需的聚合，以便获取每个集群的年度平均销售额。
- en: 'The expected output is this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '![Figure 5.7: Expected output on the Sales Transaction Data using k-means'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：使用 k-means 算法处理销售交易数据的预期输出'
- en: '](img/B16060_05_07.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_07.jpg)'
- en: 'Figure 5.7: Expected output on the Sales Transaction Data using k-means'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：使用 k-means 算法处理销售交易数据的预期输出
- en: Note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity is available on page 363.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案见第363页。
- en: Now that you have seen the k-means algorithm in detail, we will move on to another
    type of clustering algorithm, the mean shift algorithm.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经详细了解了 k-means 算法，我们将继续介绍另一种聚类算法，即均值漂移算法。
- en: The Mean Shift Algorithm
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均值漂移算法
- en: Mean shift is a hierarchical clustering algorithm that assigns data points to
    a cluster by calculating a cluster's center and moving it towards the mode at
    each iteration. The mode is the area with the most data points. At the first iteration,
    a random point will be chosen as the cluster's center and then the algorithm will
    calculate the mean of all nearby data points within a certain radius. The mean
    will be the new cluster's center. The second iteration will then begin with the
    calculation of the mean of all nearby data points and setting it as the new cluster's
    center. At each iteration, the cluster's center will move closer to where most
    of the data points are. The algorithm will stop when it is not possible for a
    new cluster's center to contain more data points. When the algorithm stops, each
    data point will be assigned to a cluster.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移是一种层次聚类算法，通过计算集群中心并在每次迭代时将其移动到模式位置来分配数据点到某个集群。模式是数据点最多的区域。在第一次迭代中，随机选择一个点作为集群中心，然后算法将计算所有在某个半径范围内的邻近数据点的均值。该均值将成为新的集群中心。第二次迭代将从计算所有邻近数据点的均值并将其设为新集群中心开始。每次迭代时，集群中心将向数据点最多的地方移动。当新集群中心无法包含更多数据点时，算法将停止。当算法停止时，每个数据点将被分配到一个集群。
- en: The mean shift algorithm will also determine the number of clusters needed,
    in contrast with the k-means algorithm. This is advantageous as we rarely know
    how many clusters we are looking for.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与 k-means 算法不同，均值漂移算法还会确定所需的集群数量。这一点非常有优势，因为我们通常并不知道需要多少个集群。
- en: This algorithm also has many use cases. For instance, the Xbox Kinect device
    detects human body parts using the mean shift algorithm. Each main body part (head,
    arms, legs, hands, and so on) is a cluster of data points assigned by the mean
    shift algorithm.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法也有许多应用场景。例如，Xbox Kinect 设备使用均值漂移算法来检测人体部位。每个主要部位（头部、手臂、腿部、手等）都是由均值漂移算法分配的数据点集群。
- en: In the next exercise, we will be implementing the mean shift algorithm.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将实现均值漂移算法。
- en: 'Exercise 5.03: Implementing the Mean Shift Algorithm'
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.03：实现均值漂移算法
- en: In this exercise, we will implement clustering by using the mean shift algorithm.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用均值漂移算法来实现聚类。
- en: We will use the `scipy.spatial` library in order to compute the Euclidean distance,
    seen in *Chapter 1*, *Introduction to Artificial Intelligence*. This library simplifies
    the calculation of distances (such as Euclidean or Manhattan) between a list of
    coordinates. More details about this library can be found at [https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`scipy.spatial`库来计算欧几里得距离，见于*第1章*，*人工智能导论*。这个库简化了计算坐标列表之间的距离（如欧几里得距离或曼哈顿距离）。更多关于此库的细节可以在[https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance)找到。
- en: 'The following steps will help you complete the exercise:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: 'Let''s use the data points from *Exercise 5.01*, *Implementing K-Means in scikit-learn*:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用*练习 5.01*中的数据点，*在scikit-learn中实现K均值算法*：
- en: '[PRE16]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our task now is to find point P (x, y), for which the number of data points
    within radius R from point P is maximized. The points are distributed as follows:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在的任务是找到点P(x, y)，使得从点P出发，半径R范围内的数据点数量最大化。点的分布如下：
- en: '![Figure 5.8: Graph showing the data points from the data_points array'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.8：显示数据点的图表，来自data_points数组'
- en: '](img/B16060_05_08.jpg)'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_05_08.jpg)'
- en: 'Figure 5.8: Graph showing the data points from the data_points array'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.8：显示数据点的图表，来自data_points数组
- en: 'Equate point `P1` to the first data point, `[1, 1]` of our list:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将点`P1`等同于我们列表中的第一个数据点`[1, 1]`：
- en: '[PRE17]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Find the points that are within a distance of `r = 2` from this point. We will
    use the `scipy` library, which simplifies mathematical calculations, including
    spatial distance:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到位于此点`r = 2`距离范围内的点。我们将使用`scipy`库，它简化了数学计算，包括空间距离：
- en: '[PRE18]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code snippet, we used the Euclidean distance to find all the
    points that fall within the `r` radius of point `P1`.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了欧几里得距离来找到所有位于点`P1`的半径`r`范围内的点。
- en: 'The output of `points1` will be as follows:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points1`的输出将如下所示：'
- en: '[PRE19]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: From the output, we can see that we found three points that fall within the
    radius of `P1`. They are the three points at the bottom left of the graph we saw
    earlier, in *Figure 5.8* of this chapter.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到我们找到了三个位于`P1`半径内的点。它们是我们之前在图表左下方看到的三个点，位于本章的*图 5.8*中。
- en: 'Now, calculate the mean of the data points to obtain the new coordinates of
    `P2`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，计算数据点的均值，以获得`P2`的新坐标：
- en: '[PRE20]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code snippet, we have calculated the mean of the array containing
    the three data points in order to obtain the new coordinates of `P2`.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们计算了包含三个数据点的数组的均值，以便获得`P2`的新坐标。
- en: 'The output of `P2` will be as follows:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`P2`的输出将如下所示：'
- en: '[PRE21]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that the new `P2` has been calculated, retrieve the points within the given
    radius again, as shown in the following code snippet:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，`P2`已被计算出来，再次检索给定半径内的点，如下所示的代码片段：
- en: '[PRE22]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of `points` will be as follows:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points`的输出将如下所示：'
- en: '[PRE23]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These are the same three points that we found in *Step 4*, so we can stop here.
    Three points have been found around the mean of `[1.3333333333333333, 1.5]`. The
    points around this center within a radius of `2` form a cluster.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些是我们在*步骤 4*中找到的相同的三个点，因此我们可以在此停止。我们已经找到了围绕均值`[1.3333333333333333, 1.5]`的三个点。以该中心为圆心，半径为`2`的点形成了一个簇。
- en: 'Since data points `[1, 1.5]` and `[2, 2]` are already in a cluster with `[1,1]`,
    we can directly continue with the fourth point in our list, `[8, 1]`:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据点`[1, 1.5]`和`[2, 2]`已经与`[1,1]`形成一个簇，我们可以直接继续使用我们列表中的第四个点`[8, 1]`：
- en: '[PRE24]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code snippet, we used the same code as *Step 4* but with a
    new `P3`.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了与*步骤 4*相同的代码，但采用了新的`P3`。
- en: 'The output of `points3` will be as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points3`的输出将如下所示：'
- en: '[PRE25]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This time, we found four points inside the radius `r` of `P4`.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，我们找到了四个位于`P4`半径`r`内的点。
- en: 'Now, calculate the mean, as shown in the following code snippet:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下代码片段计算均值：
- en: '[PRE26]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding code snippet, we calculated the mean of the array containing
    the four data points in order to obtain the new coordinates of `P4`, as in *Step
    5*.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们计算了包含四个数据点的数组的均值，以便获得`P4`的新坐标，如*步骤 5*所示。
- en: 'The output of `P4` will be as follows:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`P4`的输出将如下所示：'
- en: '[PRE27]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This mean will not change because in the next iteration, we will find the same
    data points.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个均值不会改变，因为在下一次迭代中，我们会找到相同的数据点。
- en: 'Notice that we got lucky with the selection of point `[8, 1]`. If we started
    with `P = [8, 0]` or `P = [8.5, 1]`, we would only find three points instead of
    four. Let''s try with `P5 = [8, 0]`:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们在选择点`[8, 1]`时比较幸运。如果我们从`P = [8, 0]`或`P = [8.5, 1]`开始，我们只能找到三个点，而不是四个。让我们尝试使用`P5
    = [8, 0]`：
- en: '[PRE28]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the preceding code snippet, we used the same code as in *Step 4* but with
    a new `P5`.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了与*步骤 4*相同的代码，但采用了新的`P5`。
- en: 'The output of `points4` will be as follows:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points4`的输出将如下所示：'
- en: '[PRE29]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This time, we found three points inside the radius `r` of `P5`.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，我们找到了三个位于`P5`的半径`r`内的点。
- en: 'Now, rerun the distance calculation with the shifted mean as shown in *Step
    5*:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用*步骤 5*中所示的偏移均值重新运行距离计算：
- en: '[PRE30]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code snippet, we calculated the mean of the array containing
    the three data points in order to obtain the new coordinates of `P6`.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们计算了包含三个数据点的数组的均值，以获得新的 `P6` 坐标。
- en: 'The output of `P6` will be as follows:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`P6` 的输出将如下所示：'
- en: '[PRE31]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now do the same again but with `P7 = [8.5, 1]`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在再做一次，但使用 `P7 = [8.5, 1]`：
- en: '[PRE32]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code snippet, we used the same code as in *Step 4* but with
    a new `P7`.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了与 *步骤 4* 中相同的代码，但用了一个新的 `P7`。
- en: 'The output of `points5` will be as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points5` 的输出将如下所示：'
- en: '[PRE33]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This time, we found the same three points again inside the radius `r` of `P`.
    This means that starting from `[8,1]`, we got a larger cluster than starting from
    `[8, 0]` or `[8.5, 1]`. Therefore, we must take the center point that contains
    the maximum number of data points.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一次，我们又在 `P` 的半径 `r` 内找到了相同的三个点。这意味着，从 `[8,1]` 开始，我们得到了一个比从 `[8, 0]` 或 `[8.5,
    1]` 开始更大的聚类。因此，我们必须选择包含最多数据点的中心点。
- en: 'Now, let''s see what would happen if we started the discovery from the fourth
    data point, that is, `[6, 1]`:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看如果从第四个数据点 `[6, 1]` 开始会发生什么：
- en: '[PRE34]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding code snippet, we used the same code as in *Step 4* but with
    a new `P8`.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了与 *步骤 4* 中相同的代码，但用了一个新的 `P8`。
- en: 'The output of `points6` will be as follows:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points6` 的输出将如下所示：'
- en: '[PRE35]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This time, we found only two data points inside the radius `r` of `P8`. We successfully
    found the data point `[8, 1]`.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，我们只在 `P8` 的半径 `r` 内找到了两个数据点。我们成功找到了数据点 `[8, 1]`。
- en: 'Now, shift the mean from `[6, 1]` to the calculated new mean:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将均值从 `[6, 1]` 移动到计算得到的新均值：
- en: '[PRE36]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding code snippet, we calculated the mean of the array containing
    the three data points in order to obtain the new coordinates of `P9`, as in *Step
    5*.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们计算了包含三个数据点的数组的均值，以获得新的 `P9` 坐标，正如 *步骤 5* 中所示。
- en: 'The output of `P9` will be as follows:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`P9` 的输出将如下所示：'
- en: '[PRE37]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Check whether you have obtained more points with this new `P9`:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查是否通过这个新的 `P9` 得到了更多的点：
- en: '[PRE38]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding code snippet, we used the same code as in *Step 4* but with
    a new `P9`.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了与 *步骤 4* 中相同的代码，但用了一个新的 `P9`。
- en: 'The output of `points7` will be as follows:'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`points7` 的输出将如下所示：'
- en: '[PRE39]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We successfully found all four points. Therefore, we have successfully defined
    a cluster of size `4`. The mean will be the same as before: `[7.625, 0.75]`.'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们成功找到了所有四个点。因此，我们已经成功定义了一个大小为 `4` 的聚类。均值将与之前相同：`[7.625, 0.75]`。
- en: Note
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3drUZtE](https://packt.live/3drUZtE).
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问这一特定部分的源代码，请参考 [https://packt.live/3drUZtE](https://packt.live/3drUZtE)。
- en: You can also run this example online at [https://packt.live/2YoSu78](https://packt.live/2YoSu78).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以在 [https://packt.live/2YoSu78](https://packt.live/2YoSu78) 在线运行这个示例。你必须执行整个
    Notebook 才能得到期望的结果。
- en: This was a simple clustering example that applied the mean shift algorithm.
    We only illustrated what the algorithm considers when finding clusters.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的聚类示例，应用了均值迁移算法。我们只是展示了算法在寻找聚类时会考虑什么。
- en: However, there is still one question, and that is what will the value of the
    radius be?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍然有一个问题，那就是半径的值应该是多少？
- en: Note that if the radius of `2` was not set, we could simply start either with
    a huge radius that includes all data points and then reduce the radius, or we
    could start with a tiny radius, making sure that each data point is in its cluster,
    and then increase the radius until we get the desired result.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果没有设置 `2` 的半径，我们可以简单地从一个包含所有数据点的大半径开始，然后逐步减小半径，或者从一个非常小的半径开始，确保每个数据点都在其所属的聚类中，然后逐步增加半径，直到得到期望的结果。
- en: In the next section, we will be looking at the mean shift algorithm but using
    scikit-learn.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍均值迁移算法，但这次使用 scikit-learn。
- en: The Mean Shift Algorithm in scikit-learn
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn 中的均值迁移算法
- en: 'Let''s use the same data points we used with the k-means algorithm:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用与 k-means 算法相同的数据点：
- en: '[PRE40]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The syntax of the mean shift clustering algorithm is like the syntax for the
    k-means clustering algorithm:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 均值迁移聚类算法的语法与 k-means 聚类算法的语法类似：
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once the clustering is done, we can access the center point of each cluster:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦聚类完成，我们就可以访问每个聚类的中心点：
- en: '[PRE42]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The expected output is this:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的输出是这样的：
- en: '[PRE43]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The mean shift model found five clusters with the centers shown in the preceding code.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 均值迁移模型找到了五个聚类，中心如前面的代码所示。
- en: 'Like k-means, we can also get the labels:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 和k-means一样，我们也可以获得标签：
- en: '[PRE44]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The expected output is this:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '[PRE45]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output array shows which data point belongs to which cluster. This is all
    we need to plot the data:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数组显示哪个数据点属于哪个簇。这是我们绘制数据所需要的全部内容：
- en: '[PRE46]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In the preceding code snippet, we made a plot of the data points and the centers
    of the five clusters. Each data point belonging to the same cluster will have
    the same marker. The cluster centers are marked as a dot.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们绘制了数据点和五个簇的中心。属于同一簇的每个数据点将使用相同的标记。簇中心则用点标记。
- en: 'The expected output is this:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '![Figure 5.9: Graph showing the data points of the five clusters'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.9：显示五个簇的数据点的图形'
- en: '](img/B16060_05_09.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_09.jpg)'
- en: 'Figure 5.9: Graph showing the data points of the five clusters'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：显示五个簇的数据点的图形
- en: We can see that three clusters contain more than a single dot (the top left,
    the bottom left, and the bottom right). The two single data points that are also
    their own cluster can be seen as outliers, as mentioned previously, as they are
    too far from the other clusters to be part of any of them.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，三个簇包含多个点（左上、左下和右下）。那两个作为单独簇的数据点可以被视为异常值，正如之前提到的，它们离其他簇太远，因此不属于任何簇。
- en: Now that we have learned about the mean shift algorithm, we can have look at
    hierarchical clustering, and more specifically at agglomerative hierarchical clustering
    (the *bottom-up* approach).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了均值漂移算法，可以来看看层次聚类，尤其是聚合层次聚类（*自下而上*的方法）。
- en: Hierarchical Clustering
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: 'Hierarchical clustering algorithms fall into two categories:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法分为两类：
- en: Agglomerative (or bottom-up) hierarchical clustering
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合（或自下而上）层次聚类
- en: Divisive (or top-down) hierarchical clustering
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割（或自上而下）层次聚类
- en: We will only talk about agglomerative hierarchical clustering in this chapter,
    as it is the most widely used and most efficient of the two approaches.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将只讨论聚合层次聚类，因为它是两种方法中最广泛使用且最有效的。
- en: Agglomerative hierarchical clustering treats each data point as a single cluster
    in the beginning and then successively merges (or agglomerates) the closest clusters
    together in pairs. In order to find the closest data clusters, agglomerative hierarchical
    clustering uses a heuristic such as the Euclidean or Manhattan distance to define
    the distance between data points. A linkage function will also be required to
    aggregate the distance between data points in clusters in order to define a unique
    value of the closeness of clusters.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合层次聚类（Agglomerative Hierarchical Clustering）最开始将每个数据点视为一个单独的簇，然后依次将最接近的簇按对进行合并（或聚合）。为了找到最接近的数据簇，聚合层次聚类使用欧几里得距离或曼哈顿距离等启发式方法来定义数据点之间的距离。还需要一个聚类函数来聚合簇内数据点之间的距离，从而定义簇之间的接近度的唯一值。
- en: Examples of linkage functions include single linkage (simple distance), average
    linkage (average distance), maximum linkage (maximum distance), and Ward linkage
    (square difference). The pairs of clusters with the smallest value of linkage
    will be grouped together. The grouping is repeated until we reach a single cluster
    containing every data point. In the end, this algorithm terminates when there
    is only a single cluster left.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类函数的示例包括单一聚类（简单距离）、平均聚类（平均距离）、最大聚类（最大距离）和沃德聚类（平方差）。具有最小聚类值的簇对将被归为一组。此过程将重复进行，直到只剩下一个包含所有数据点的簇。最终，当只剩下一个簇时，该算法终止。
- en: In order to visually represent the hierarchy of clusters, a dendrogram can be
    used. A dendrogram is a tree where the leaves at the bottom represent data points.
    Each intersection between two leaves is the grouping of these two leaves. The
    root (top) represents a unique cluster that contains all the data points. Have
    a look at *Figure 5.10*, which represents a dendrogram.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地表示簇的层次结构，可以使用树状图（dendrogram）。树状图是一棵树，底部的叶子表示数据点。两个叶子之间的每个交点表示这两个叶子的分组。根（顶部）表示一个包含所有数据点的唯一簇。请看*图5.10*，它展示了一个树状图。
- en: '![Figure 5.10: Example of a dendrogram'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10：树状图示例'
- en: '](img/B16060_05_10.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_10.jpg)'
- en: 'Figure 5.10: Example of a dendrogram'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：树状图示例
- en: Agglomerative Hierarchical Clustering in scikit-learn
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn中的聚合层次聚类
- en: 'Have a look at the following example, where we use the same data points as
    we used with the k-means algorithm:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下示例，我们使用了与k-means算法相同的数据点：
- en: '[PRE47]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In order to plot a dendrogram, we need to first import the `scipy` library:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制树状图，我们需要先导入`scipy`库：
- en: '[PRE48]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then we can plot a dendrogram using SciPy with the `ward` linkage function,
    as it is the most commonly used linkage function:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用SciPy中的`ward`连接函数绘制一个树状图，因为它是最常用的连接函数：
- en: '[PRE49]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output of the dendrogram will be as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图的输出结果如下所示：
- en: '![Figure 5.11: Dendrogram based on random data points'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：基于随机数据点的树状图'
- en: '](img/B16060_05_11.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_11.jpg)'
- en: 'Figure 5.11: Dendrogram based on random data points'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：基于随机数据点的树状图
- en: With the dendrogram, we can generally guess what will be a good number of clusters
    by simply drawing a horizontal line as shown in *Figure 5.12*, in the area with
    the highest vertical distance, and counting the number of intersections. In this
    case, it should be two clusters, but we will go to the next biggest area as two
    is too small a number.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 使用树状图时，我们通常可以通过简单地在*图 5.12*中所示的最大垂直距离区域画一条水平线，并计算交点数量，来大致猜测一个合适的聚类数。在这个例子中，应该是两个聚类，但我们将选择下一个较大的区域，因为两个聚类的数量太小了。
- en: '![Figure 5.12: Division on clusters in the dendrogram'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12：树状图中的聚类划分'
- en: '](img/B16060_05_12.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_12.jpg)'
- en: 'Figure 5.12: Division on clusters in the dendrogram'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：树状图中的聚类划分
- en: The *y* axis represents the measure of closeness, and the *x* axis represents
    the index of each data point. So our first three data points (`0,1,2`) are parts
    of the same cluster, then another cluster is made of the next four points (`3,4,5,6`),
    data point `10` is a cluster on its own, and the remaining data points (`7,8,9,11`)
    form the last cluster.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*轴表示聚类的接近度，而*x*轴表示每个数据点的索引。因此，我们的前三个数据点（`0,1,2`）属于同一个聚类，接下来的四个数据点（`3,4,5,6`）组成另一个聚类，数据点`10`独立成一个聚类，剩余的（`7,8,9,11`）则组成最后一个聚类。'
- en: 'The syntax of the agglomerative hierarchical clustering algorithm is similar
    to the k-means clustering algorithm except that we need to specify the number
    type of `affinity` (here, we choose the Euclidean distance) and the linkage (here,
    we choose the `ward` linkage):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法的语法与k-means聚类算法类似，不同之处在于我们需要指定`affinity`的类型（在这里我们选择欧几里得距离）和连接方式（在这里我们选择`ward`连接）：
- en: '[PRE50]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE51]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Similar to k-means, we can also get the labels as shown in the following code
    snippet:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于k-means，我们也可以通过以下代码片段获得标签：
- en: '[PRE52]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The expected output is this:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出为：
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output array shows which data point belongs to which cluster. This is all
    we need to plot the data:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数组显示了每个数据点所属的聚类。这就是我们绘制数据所需的全部信息：
- en: '[PRE54]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In the preceding code snippet, we made a plot of the data points and the four
    clusters' centers. Each data point belonging to the same cluster will have the
    same marker.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们绘制了数据点和四个聚类中心的图。每个属于同一聚类的数据点将具有相同的标记。
- en: 'The expected output is this:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出为：
- en: '![Figure 5.13: Graph showing the data points of the four clusters'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13：显示四个聚类数据点的图'
- en: '](img/B16060_05_13.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_13.jpg)'
- en: 'Figure 5.13: Graph showing the data points of the four clusters'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：显示四个聚类数据点的图
- en: We can see that, in contrast with the result from the mean shift method, agglomerative
    clustering was able to properly group the data point at (`6,1`) with the bottom-right
    cluster instead of having his own cluster. In situations like this one, where
    we have a very small amount of data, agglomerative hierarchical clustering and
    mean shift will work better than k-means. However, they have very expensive computational
    time requirements, which will make them struggle on very large datasets. However,
    k-means is very fast and is a better choice for very large datasets.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与均值漂移方法的结果相比，层次聚类能够正确地将位于（`6,1`）的数据点与右下角的聚类组合在一起，而不是单独形成一个聚类。在像这样的情况下，当数据量非常少时，层次聚类和均值漂移比k-means效果更好。然而，它们的计算时间需求非常高，这使得它们在非常大的数据集上表现不佳。不过，k-means非常快速，是处理大数据集的更好选择。
- en: Now that we have learned about a few different clustering algorithms, we need
    to start evaluating these models and comparing them in order to choose the best
    model for clustering.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些不同的聚类算法，接下来需要开始评估这些模型并进行比较，以选择最适合聚类的最佳模型。
- en: Clustering Performance Evaluation
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类性能评估
- en: 'Unlike supervised learning, where we always have the labels to evaluate our
    predictions with, unsupervised learning is a bit more complex as we do not usually
    have labels. In order to evaluate a clustering model, two approaches can be taken
    depending on whether the label data is available or not:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，监督学习中我们始终可以使用标签来评估预测结果，而无监督学习则稍显复杂，因为我们通常没有标签。为了评估聚类模型，可以根据是否有标签数据采取两种方法：
- en: The first approach is the extrinsic method, which requires the existence of
    label data. This means that in absence of label data, human intervention is required
    in order to label the data or at least a subset of it.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法是外在方法，它需要标签数据的存在。这意味着，如果没有标签数据，必须有人为干预来标记数据，或者至少是其中的一部分。
- en: The other approach is the intrinsic approach. In general, the extrinsic approach
    tries to assign a score to clustering, given the label data, whereas the intrinsic
    approach evaluates clustering by examining how well the clusters are separated
    and how compact they are.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是内在方法。一般来说，外在方法试图根据标签数据为聚类分配一个分数，而内在方法则通过检查聚类的分离度和紧凑度来评估聚类。
- en: Note
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We will skip the mathematical explanations as they are quite complicated.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将跳过数学解释，因为它们相当复杂。
- en: 'You can find more mathematical details on the sklearn website at this URL:
    [https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation
    )'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在sklearn网站上找到更多数学细节，网址为：[https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation
    )
- en: 'We will begin with the extrinsic approach (as it is the most widely used method)
    and define the following scores using sklearn on our k-means example:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从外在方法开始（因为它是最广泛使用的方法），并使用sklearn在我们的k均值示例中定义以下分数：
- en: The adjusted Rand index
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的兰德指数
- en: The adjusted mutual information
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的互信息
- en: The homogeneity
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同质性
- en: The completeness
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整性
- en: The V-Measure
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V-度量
- en: The Fowlkes-Mallows score
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows分数
- en: The contingency matrix
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互信息矩阵
- en: 'Let''s have a look at an example in which we first need to import the `metrics`
    module from `sklearn.cluster`:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例，在这个示例中，我们首先需要从`sklearn.cluster`导入`metrics`模块：
- en: '[PRE55]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We will be reusing the code from our k-means example in *Exercise 5.01*, *Implementing
    K-Means in scikit-learn:*
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用在*练习5.01*中使用的代码，*在scikit-learn中实现K均值算法：*
- en: '[PRE56]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output of our predicted labels using `k_means_model.labels_` was:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`k_means_model.labels_`预测标签的输出是：
- en: '[PRE57]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Finally, define the true labels of this dataset, as shown in the following
    code snippet:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，定义该数据集的真实标签，如下代码片段所示：
- en: '[PRE58]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The Adjusted Rand Index
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整后的兰德指数
- en: The adjusted Rand index is a function that measures the similarity between the
    cluster predictions and the labels while ignoring permutations. The adjusted Rand
    index works quite well when the labels are large equal-sized clusters.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的兰德指数是一个度量聚类预测与标签之间相似性的函数，同时忽略排列顺序。当标签是大而相等的聚类时，调整后的兰德指数表现得很好。
- en: The adjusted Rand index has a range between **[-1.1]**, where negative values
    are not desirable. A negative score means that our model is performing worse than
    if we were to randomly assign labels. If we were to randomly assign them, our
    score would be close to 0\. However, the closer we are to 1, the better our clustering
    model is at predicting the right label.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的兰德指数的范围是**[-1.1]**，其中负值是不理想的。负分数意味着我们的模型表现比随机分配标签还要差。如果我们随机分配标签，分数接近0。但是，越接近1，说明我们的聚类模型在预测正确标签方面表现越好。
- en: 'With `sklearn`, we can easily compute the adjusted Rand index by using this
    code:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`，我们可以通过以下代码轻松计算调整后的兰德指数：
- en: '[PRE59]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The expected output is this:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的输出是这样的：
- en: '[PRE60]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In this case, the adjusted Rand index indicates that our k-means model is not
    far from our true labels.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，调整后的兰德指数表明我们的k均值模型与真实标签之间的差距不大。
- en: The Adjusted Mutual Information
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整后的互信息
- en: The adjusted mutual information is a function that measures the entropy between
    the cluster predictions and the labels while ignoring permutations.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的互信息是一个度量聚类预测与标签之间的熵的函数，同时忽略排列顺序。
- en: The adjusted mutual information has no defined range, but negative values are
    considered bad. The closer we are to 1, the better our clustering model is at
    predicting the right label.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 调整互信息没有固定的范围，但负值被认为是差的。我们越接近1，聚类模型预测正确标签的效果就越好。
- en: 'With `sklearn`, we can easily compute it by using this code:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`，我们可以通过以下代码轻松计算它：
- en: '[PRE61]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The expected output is this:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE62]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In this case, the adjusted mutual information indicates that our k-means model
    is quite good and not far from our true labels.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，调整互信息表明我们的k-means模型相当优秀，与真实标签的差距不大。
- en: The V-Measure, Homogeneity, and Completeness
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V-Measure、同质性和完整性
- en: The V-Measure is defined as the harmonic mean of homogeneity and completeness.
    The harmonic mean is a type of average (other types are the arithmetic mean and
    the geometric mean) using reciprocals (a reciprocal is the inverse of a number.
    For example the reciprocal of 2 is ![formula](img/B16060_05_13a.png), and the
    reciprocal of 3 is ![37](img/B16060_05_13b.png)).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: V-Measure被定义为同质性和完整性的调和均值。调和均值是一种平均值（其他类型的平均值有算术平均和几何平均），它使用倒数（倒数是一个数的倒数。例如，2的倒数是![公式](img/B16060_05_13a.png)，3的倒数是![37](img/B16060_05_13b.png)）。
- en: 'The formula of the harmonic mean is as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 调和均值的公式如下：
- en: '![Figure 5.14: The harmonic mean formula'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14：调和均值公式](img/B16060_05_13a.png)'
- en: '](img/B16060_05_14.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_14.jpg)'
- en: 'Figure 5.14: The harmonic mean formula'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：调和均值公式
- en: '![formula 3](img/B16060_05_14a.png) is the number of values and ![formula 4](img/B16060_05_14b.png)
    is the value of each point.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '![公式 3](img/B16060_05_14a.png)是值的数量，![公式 4](img/B16060_05_14b.png)是每个点的值。'
- en: In order to calculate the V-Measure, we first need to define homogeneity and completeness.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算V-Measure，我们首先需要定义同质性和完整性。
- en: Perfect homogeneity refers to a situation where each cluster has data points
    belonging to the same label. The homogeneity score will reflect how well each
    of our clusters is grouping data from the same label.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的同质性指的是每个聚类中的数据点都属于相同的标签。该同质性得分将反映我们的每个聚类在将数据从相同标签分组时的效果。
- en: Perfect completeness refers to the situation where all data points belonging
    to the same label are clustered into the same cluster. The homogeneity score will
    reflect how well, for each of our labels, its data points are all grouped inside
    the same cluster.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的完整性指的是所有属于相同标签的数据点被聚集到同一个聚类中的情况。同质性得分将反映每个标签的所有数据点是否都被很好地分组到同一个聚类中。
- en: 'Hence, the formula of V-Measure is as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，V-Measure的公式如下：
- en: '![Figure 5.15: The V-Measure formula'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15：V-Measure公式](img/B16060_05_15.jpg)'
- en: '](img/B16060_05_15.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_15.jpg)'
- en: 'Figure 5.15: The V-Measure formula'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：V-Measure公式
- en: '![formula 5](img/B16060_05_15a.png) has a default value of **1**, but it can
    be changed to further emphasize either homogeneity or completeness.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '![公式 5](img/B16060_05_15a.png)的默认值为**1**，但可以根据需要进行调整，以进一步强调同质性或完整性。'
- en: These three scores have a range between [**0,1**], with **0** being the worst
    possible score and **1** being the perfect score.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个得分的范围是[**0,1**]，其中**0**表示最差得分，**1**表示完美得分。
- en: 'With `sklearn`, we can easily compute these three scores by using this code:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`，我们可以通过以下代码轻松计算这三个得分：
- en: '[PRE63]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output of `homogeneity_score` is as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`homogeneity_score`的输出如下：'
- en: '[PRE64]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In this case, the homogeneity score indicates that our k-means model has clusters
    containing different labels.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，同质性得分表明我们的k-means模型中的聚类包含了不同的标签。
- en: 'The output of `completeness_score` is as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '`completeness_score`的输出如下：'
- en: '[PRE65]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In this case, the completeness score indicates that our k-means model has successfully
    put every data point of each label inside the same cluster.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，完整性得分表明我们的k-means模型成功地将每个标签的所有数据点放入同一个聚类中。
- en: 'The output of `v_measure_score` is as follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '`v_measure_score`的输出如下：'
- en: '[PRE66]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In this case, the V-Measure indicates that our k-means model, while not being
    perfect, has a good score in general.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，V-Measure表明我们的k-means模型虽然不完美，但总体得分良好。
- en: The Fowlkes-Mallows Score
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows得分
- en: The Fowlkes-Mallows score is a metric measuring the similarity within a label
    cluster and the prediction of the cluster, and this is defined as the geometric
    mean of the precision and recall (you learned about this in *Chapter 4*, *An Introduction
    to Decision Trees*).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows得分是衡量标签聚类内相似性和聚类预测的一个指标，它定义为精确度和召回率的几何均值（你在*第4章*，*决策树入门*中学到过这个概念）。
- en: 'The formula of the Fowlkes-Mallows score is as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows 分数的公式如下：
- en: '![Figure 5.16: The Fowlkes-Mallows formula'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.16：Fowlkes-Mallows 公式'
- en: '](img/B16060_05_16.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_16.jpg)'
- en: 'Figure 5.16: The Fowlkes-Mallows formula'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16：Fowlkes-Mallows 公式
- en: 'Let''s break this down:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来分解一下：
- en: 'True positive (or *TP*): Are all the observations where the predictions are
    in the same cluster as the label cluster'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性（*TP*）：是所有预测与标签簇相同的观测值。
- en: 'False positive (or *FP*): Are all the observations where the predictions are
    in the same cluster but not the same as the label cluster'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性（*FP*）：是所有预测在同一簇中，但与标签簇不同的观测值。
- en: 'False negative (or *FN*): Are all the observations where the predictions are
    not in the same cluster but are in the same label cluster'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性（*FN*）：是所有预测不在同一簇中，但位于同一标签簇中的观测值。
- en: The Fowlkes-Mallows score has a range between [**0, 1**], with **0** being the
    worst possible score and **1** being the perfect score.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows 分数的范围是 [**0, 1**]，其中 **0** 是最差的分数，**1** 是完美的分数。
- en: 'With `sklearn`, we can easily compute it by using this code:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `sklearn`，我们可以通过以下代码轻松计算它：
- en: '[PRE67]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The expected output is this:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的输出是：
- en: '[PRE68]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: In this case, the Fowlkes-Mallows score indicates that our k-means model is
    quite good and not far from our true labels.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Fowlkes-Mallows 分数表明我们的 k-means 模型相当不错，并且与真实标签相差不远。
- en: The Contingency Matrix
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类矩阵
- en: The contingency matrix is not a score, but it reports the intersection cardinality
    for every true/predicted cluster pair and the required label data. It is very
    similar to the *Confusion Matrix* seen in *Chapter 4*, *An Introduction to Decision
    Trees*. The matrix must be the same for the label and cluster name, so we need
    to be careful to give our cluster the same name as our label, which was not the
    case with the previously seen scores.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类矩阵不是一个分数，而是报告每个真实/预测簇对的交集基数以及所需的标签数据。它非常类似于在《第四章，决策树简介》中看到的 *混淆矩阵*。该矩阵必须与标签和簇名称一致，因此我们需要小心地将簇命名为与标签相同，而在之前看到的分数中并未做到这一点。
- en: 'We will modify our labels from this:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把标签从以下内容修改为：
- en: '[PRE69]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'To this:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 到此：
- en: '[PRE70]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then, with `sklearn`, we can easily compute the contingency matrix by using
    this code:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 `sklearn`，我们可以通过以下代码轻松计算聚类矩阵：
- en: '[PRE71]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output of `contingency_matrix` is as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '`contingency_matrix` 的输出如下：'
- en: '[PRE72]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The first row of the `contingency_matrix` output indicates that there are `4`
    data points whose true cluster is the first cluster (`0`). The second row indicates
    that there are also four data points whose true cluster is the second cluster
    (`1`); however, an extra `1` was incorrectly predicted in this cluster, but it
    belongs to the fourth cluster (`3`). The third row indicates that there are three
    data points whose true cluster is the third cluster (`2`).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '`contingency_matrix` 输出的第一行表示有 `4` 个数据点的真实簇为第一簇（`0`）。第二行表示有 `4` 个数据点的真实簇为第二簇（`1`）；然而，在这个簇中错误地预测了一个额外的
    `1`，但它实际上属于第四簇（`3`）。第三行表示有 `3` 个数据点的真实簇为第三簇（`2`）。'
- en: 'We will now look at the intrinsic approach, which is required when we do not
    have the label. We will define the following scores using sklearn on our k-means
    example:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下内在方法，当我们没有标签时，这种方法是必需的。我们将使用 sklearn 在我们的 k-means 示例中定义以下分数：
- en: The Silhouette Coefficient
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silhouette Coefficient（轮廓系数）
- en: The Calinski-Harabasz index
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calinski-Harabasz 指数
- en: The Davies-Bouldin index
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davies-Bouldin 指数
- en: The Silhouette Coefficient
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Silhouette Coefficient（轮廓系数）
- en: The Silhouette Coefficient is an example of an intrinsic evaluation. It measures
    the similarity between a data point and its cluster when compared to other clusters.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: Silhouette Coefficient（轮廓系数）是一个内在评价的例子。它衡量数据点与其簇之间的相似度，相对于其他簇的相似度。
- en: 'It comprises two scores:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括两个分数：
- en: '`a`: The average distance between a data point and all other data points in
    the same cluster.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`a`：数据点与同一簇内所有其他数据点之间的平均距离。'
- en: '`b`: The average distance between a data point and all the data points in the
    nearest cluster.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b`：数据点与最近簇内所有数据点之间的平均距离。'
- en: 'The Silhouette Coefficient formula is:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数公式为：
- en: '![Figure 5.17: The Silhouette Coefficient formula'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.17：轮廓系数公式'
- en: '](img/B16060_05_17.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_05_17.jpg)'
- en: 'Figure 5.17: The Silhouette Coefficient formula'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17：轮廓系数公式
- en: The Silhouette Coefficient has a range between [**-1,1**], with **-1** meaning
    an incorrect clustering. A score close to zero indicates that our clusters are
    overlapping. A score close to **1** indicates that all the data points are assigned
    to the appropriate clusters.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数的范围为[**-1,1**]，其中**-1**表示错误的聚类。接近零的分数表示聚类存在重叠。接近**1**的分数表示所有数据点都分配到了合适的聚类中。
- en: 'Then, with `sklearn`, we can easily compute the silhouette coefficient by using
    this code:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`sklearn`，我们可以通过以下代码轻松计算轮廓系数：
- en: '[PRE73]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output of `silhouette_score` is as follows:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '`silhouette_score`的输出如下：'
- en: '[PRE74]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In this case, the Silhouette Coefficient indicates that our k-means model has
    some overlapping clusters, and some improvements can be made by separating some
    of the data points from one of the clusters.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，轮廓系数表明我们的k-means模型存在一些重叠的聚类，并且可以通过将某些数据点从其中一个聚类中分离出来来进行改进。
- en: The Calinski-Harabasz Index
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Calinski-Harabasz指数
- en: The Calinski-Harabasz index measures how the data points inside each cluster
    are spread. It is defined as the ratio of the variance between clusters and the
    variance inside each cluster. The Calinski-Harabasz index doesn't have a range
    and starts from **0**. The higher the score is, the denser our clusters are. A
    dense cluster is an indication of a well-defined cluster.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: Calinski-Harabasz指数衡量每个聚类内部数据点的分布情况。它被定义为聚类之间方差与每个聚类内部方差的比值。Calinski-Harabasz指数没有范围，起始值为**0**。分数越高，聚类越密集。密集的聚类表明聚类定义得较好。
- en: 'With `sklearn`, we can easily compute it by using this code:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`，我们可以通过以下代码轻松计算它：
- en: '[PRE75]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output of `calinski_harabasz_score` is as follows:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '`calinski_harabasz_score`的输出如下：'
- en: '[PRE76]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: In this case, the Calinski-Harabasz index indicates that our k-means model clusters
    are quite spread out and suggests that we might have overlapping clusters.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Calinski-Harabasz指数表明我们的k-means模型聚类比较分散，建议我们可能有重叠的聚类。
- en: The Davies-Bouldin Index
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数
- en: The Davies-Bouldin index measures the average similarity between clusters. The
    similarity is a ratio of the distance between a cluster and its closest cluster
    and the average distance between each data point of a cluster and it's cluster's
    center. The Davies-Bouldin index doesn't have a range and starts from **0**. The
    closer the score is to **0** the better; it means the clusters are well separated,
    which is an indication of a good cluster.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数衡量聚类之间的平均相似性。相似性是聚类与其最近聚类之间的距离与聚类内每个数据点与聚类中心之间的平均距离之比。Davies-Bouldin指数没有范围，起始值为**0**。分数越接近**0**越好，这意味着聚类之间分离得很好，是聚类效果良好的指示。
- en: 'With `sklearn`, we can easily compute the Davis-Bouldin index by using this
    code:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`，我们可以通过以下代码轻松计算Davies-Bouldin指数：
- en: '[PRE77]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The output of `davies_bouldin_score` is as follows:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '`davies_bouldin_score`的输出如下：'
- en: '[PRE78]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In this case, the Calinski-Harabasz score indicates that our k-means model has
    some overlapping clusters and an improvement could be made by better separating
    some of the data points in one of the clusters.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Calinski-Harabasz评分表明我们的k-means模型存在一些重叠的聚类，通过更好地分离某些数据点，可以对其中一个聚类进行改进。
- en: 'Activity 5.02: Clustering Red Wine Data Using the Mean Shift Algorithm and
    Agglomerative Hierarchical Clustering'
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.02：使用均值漂移算法和聚合层次聚类对红酒数据进行聚类
- en: In this activity, you will work on the Wine Quality dataset and, more specifically,
    on red wine data. This dataset contains data on the quality of 1,599 red wines
    and the results of their chemical tests.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，您将使用红酒质量数据集，具体来说是红葡萄酒数据。该数据集包含1,599个红酒的质量数据以及它们的化学测试结果。
- en: Your goal will be to build two clustering models (using the mean shift algorithm
    and agglomerative hierarchical clustering) in order to identify whether wines
    of similar quality also have similar physicochemical properties. You will also
    have to evaluate and compare the two clustering models using extrinsic and intrinsic
    approaches.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 您的目标是构建两个聚类模型（使用均值漂移算法和聚合层次聚类），以识别相似质量的葡萄酒是否具有相似的物理化学性质。您还需要使用外部和内部方法评估并比较这两个聚类模型。
- en: Note
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be found at the following URL: [https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过以下网址找到：[https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)。
- en: The dataset file can be found on our GitHub repository at [https://packt.live/2YYsxuu](https://packt.live/2YYsxuu).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件可以在我们的GitHub仓库中找到，链接：[https://packt.live/2YYsxuu](https://packt.live/2YYsxuu)。
- en: 'Citation: *P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling
    wine preferences by data mining from physicochemical properties. In Decision Support
    Systems, Elsevier, 47(4):547-553, 2009*.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：*P. Cortez, A. Cerdeira, F. Almeida, T. Matos 和 J. Reis. 通过数据挖掘物理化学性质建模葡萄酒偏好。在《决策支持系统》期刊，Elsevier，47(4)：547-553，2009*。
- en: 'The following steps will help you complete the activity:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该活动：
- en: Open a new Jupyter Notebook file.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: Load the dataset as a DataFrame with `sep = ";"` and inspect the data.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集加载为DataFrame，使用`sep = ";"`，并检查数据。
- en: Create a mean shift clustering model, then retrieve the model's predicted labels
    and the number of clusters created.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个均值迁移聚类模型，然后提取该模型的预测标签及其创建的聚类数。
- en: Create an agglomerative hierarchical clustering model after creating a dendrogram
    and selecting the optimal number of clusters.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建树状图并选择最佳聚类数后，创建一个聚合层次聚类模型。
- en: Retrieve the labels from the first clustering model.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个聚类模型中提取标签。
- en: 'Compute the following extrinsic approach scores for both models:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个模型计算以下外部方法得分：
- en: The adjusted Rand index
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调整后的兰德指数
- en: The adjusted mutual information
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调整后的互信息
- en: The V-Measure
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: V度量
- en: The Fowlkes-Mallows score
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows分数
- en: 'Compute the following intrinsic approach scores for both models:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个模型计算以下内部方法得分：
- en: The Silhouette Coefficient
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: The Calinski-Harabasz index
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Calinski-Harabasz指数
- en: The Davies-Bouldin index
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数
- en: 'The expected output is this:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 期望输出为：
- en: 'The values of each score for the mean shift clustering model will be as follows:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 均值迁移聚类模型的每个得分值如下：
- en: 'The adjusted Rand index: `0.0006771608724007207`'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的兰德指数：`0.0006771608724007207`
- en: 'The adjusted mutual information: `0.004837187596124968`'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的互信息：`0.004837187596124968`
- en: 'The V-Measure: `0.021907254751144124`'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V度量：`0.021907254751144124`
- en: 'The Fowlkes-Mallows score: `0.5721233634622408`'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows分数：`0.5721233634622408`
- en: 'The Silhouette Coefficient: `0.32769323700400077`'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮廓系数：`0.32769323700400077`
- en: 'The Calinski-Harabasz index: `44.62091774102674`'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calinski-Harabasz指数：`44.62091774102674`
- en: 'The Davies-Bouldin index: `0.8106334674570222`'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数：`0.8106334674570222`
- en: 'The values of each score for the agglomerative hierarchical clustering will
    be as follows:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合层次聚类的每个得分值如下：
- en: 'The adjusted Rand index: `0.05358047852603172`'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的兰德指数：`0.05358047852603172`
- en: 'The adjusted mutual information: `0.05993098663692826`'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的互信息：`0.05993098663692826`
- en: 'The V-Measure: `0.07549735446050691`'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V度量：`0.07549735446050691`
- en: 'The Fowlkes-Mallows score: `0.3300681478007641`'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows分数：`0.3300681478007641`
- en: 'The Silhouette Coefficient: `0.1591882574407987`'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮廓系数：`0.1591882574407987`
- en: 'The Calinski-Harabasz index: `223.5171774491095`'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calinski-Harabasz指数：`223.5171774491095`
- en: 'The Davies-Bouldin index: `1.4975443816135114`'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数：`1.4975443816135114`
- en: Note
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity is available on page 368.
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第368页找到。
- en: By completing this activity, you performed mean shift and agglomerative hierarchical
    clustering on multiple columns for many products. You also learned how to evaluate
    a clustering model with an extrinsic and intrinsic approach. Finally, you used
    the results of your models and their evaluation to find an answer to a real-world
    problem.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，你已对多个产品的多个列进行了均值迁移和聚合层次聚类。你还学习了如何使用外部和内部方法评估聚类模型。最后，你利用模型的结果及其评估，找到了现实世界问题的答案。
- en: Summary
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned the basics of how clustering works. Clustering is
    a form of unsupervised learning where the features are given, but not the labels.
    It is the goal of the clustering algorithms to find the labels based on the similarity
    of the data points.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了聚类的基础知识。聚类是一种无监督学习形式，其中给定了特征，但没有标签。聚类算法的目标是根据数据点的相似性找到标签。
- en: We also learned that there are two types of clustering, flat and hierarchical,
    with the first type requiring the number of clusters to find, whereas the second
    type finds the optimal number of clusters itself.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到聚类有两种类型，平面聚类和层次聚类，其中第一种类型需要指定聚类的数量，而第二种类型能够自动找到最佳的聚类数量。
- en: The k-means algorithm is an example of flat clustering, whereas mean shift and
    agglomerative hierarchical clustering are examples of a hierarchical clustering algorithm.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法是平面聚类的一个例子，而均值漂移和凝聚层次聚类则是层次聚类算法的例子。
- en: We also learned about the numerous scores to evaluate the performance of a clustering
    model, with the labels in the extrinsic approach or without the labels in the
    intrinsic approach.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了评估聚类模型性能的众多评分标准，包括有标签的外部方法和无标签的内部方法。
- en: In *Chapter 6*, *Neural Networks and Deep Learning*, you will be introduced
    to a field that has become popular in this decade due to the explosion of computation
    power and cheap, scalable online server capacity. This field is the science of
    neural networks and deep learning.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第六章*，*神经网络与深度学习*中，你将接触到一个在过去十年因计算能力的爆炸性增长以及廉价、可扩展的在线服务器容量而变得流行的领域。这个领域是神经网络与深度学习的科学。
