- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Deep Q-Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦Qç½‘ç»œ
- en: 'In ChapterÂ [5](ch009.xhtml#x1-820005), you became familiar with the Bellman
    equation and the practical method of its application called value iteration. This
    approach allowed us to significantly improve our speed and convergence in the
    FrozenLake environment, which is promising, but can we go further? In this chapter,
    we will apply the same approach to problems of much greater complexity: arcade
    games from the Atari 2600 platform, which are the de facto benchmark of the reinforcement
    learning (RL) research community.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[5ç« ](ch009.xhtml#x1-820005)ä¸­ï¼Œä½ å·²ç»ç†Ÿæ‚‰äº†è´å°”æ›¼æ–¹ç¨‹ä»¥åŠå…¶åº”ç”¨çš„å®é™…æ–¹æ³•â€”â€”ä»·å€¼è¿­ä»£ã€‚è¿™ä¸ªæ–¹æ³•è®©æˆ‘ä»¬åœ¨FrozenLakeç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†é€Ÿåº¦å’Œæ”¶æ•›æ€§ï¼Œè¿™å¾ˆæœ‰å‰æ™¯ï¼Œä½†æˆ‘ä»¬èƒ½è¿›ä¸€æ­¥æå‡å—ï¼Ÿåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠç›¸åŒçš„æ–¹æ³•åº”ç”¨åˆ°å¤æ‚åº¦æ›´é«˜çš„é—®é¢˜ä¸Šï¼šæ¥è‡ªAtari
    2600å¹³å°çš„è¡—æœºæ¸¸æˆï¼Œè¿™äº›æ¸¸æˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç ”ç©¶ç¤¾åŒºçš„äº‹å®æ ‡å‡†ã€‚
- en: 'To deal with this new and more challenging goal, in this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åº”å¯¹è¿™ä¸€æ–°çš„ã€æ›´å…·æŒ‘æˆ˜æ€§çš„ç›®æ ‡ï¼Œæœ¬ç« å°†ï¼š
- en: Talk about problems with the value iteration method and consider its variation,
    called Q-learning.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºä»·å€¼è¿­ä»£æ–¹æ³•çš„é—®é¢˜ï¼Œå¹¶è€ƒè™‘å®ƒçš„å˜ä½“â€”â€”Qå­¦ä¹ ã€‚
- en: Apply Q-learning to so-called grid world environments, which is called tabular
    Q-learning.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†Qå­¦ä¹ åº”ç”¨äºæ‰€è°“çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºè¡¨æ ¼Qå­¦ä¹ ã€‚
- en: Discuss Q-learning in conjunction with neural networks (NNs). This combination
    has the name deep Q-network (DQN).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºQå­¦ä¹ ä¸ç¥ç»ç½‘ç»œï¼ˆNNsï¼‰çš„ç»“åˆã€‚è¿™ç§ç»“åˆè¢«ç§°ä¸ºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ã€‚
- en: At the end of the chapter, we will reimplement a DQN algorithm from the famous
    paper Playing Atari with deep reinforcement learning [[Mni13](#)], which was published
    in 2013 and started a new era in RL development. Although it is too early to discuss
    the practical applicability of these basic methods, this will become clearer to
    you as you progress with the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„ç»“å°¾ï¼Œæˆ‘ä»¬å°†é‡æ–°å®ç°è‘—åè®ºæ–‡ã€ŠPlaying Atari with Deep Reinforcement Learningã€‹[[Mni13](#)]ä¸­çš„DQNç®—æ³•ï¼Œè¯¥è®ºæ–‡äº2013å¹´å‘å¸ƒï¼Œå¹¶å¼€å¯äº†å¼ºåŒ–å­¦ä¹ å‘å±•çš„æ–°çºªå…ƒã€‚è™½ç„¶è®¨è®ºè¿™äº›åŸºæœ¬æ–¹æ³•çš„å®é™…åº”ç”¨è¿˜ä¸ºæ—¶è¿‡æ—©ï¼Œä½†éšç€ä½ æ·±å…¥é˜…è¯»æœ¬ä¹¦ï¼Œä½ ä¼šæ›´æ¸…æ¥šåœ°çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: Real-life value iteration
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç°å®ç”Ÿæ´»ä¸­çš„ä»·å€¼è¿­ä»£
- en: The improvements that we got in the FrozenLake environment by switching from
    the cross-entropy method to the value iteration method are quite encouraging,
    so itâ€™s tempting to apply the value iteration method to more challenging problems.
    However, it is important to look at the assumptions and limitations that our value
    iteration method has. But letâ€™s start with a quick recap of the method. On every
    step, the value iteration method does a loop on all states, and for every state,
    it performs an update of its value with a Bellman approximation. The variation
    of the same method for Q-values (values for actions) is almost the same, but we
    approximate and store values for every state and action. So whatâ€™s wrong with
    this process?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†äº¤å‰ç†µæ–¹æ³•è½¬æ¢ä¸ºä»·å€¼è¿­ä»£æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨FrozenLakeç¯å¢ƒä¸­å–å¾—çš„æ”¹è¿›éå¸¸ä»¤äººé¼“èˆï¼Œå› æ­¤å°†ä»·å€¼è¿­ä»£æ–¹æ³•åº”ç”¨äºæ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜éå¸¸æœ‰å¸å¼•åŠ›ã€‚ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦æŸ¥çœ‹æˆ‘ä»¬ä»·å€¼è¿­ä»£æ–¹æ³•çš„å‡è®¾å’Œå±€é™æ€§ã€‚ä½†è®©æˆ‘ä»¬å…ˆå¿«é€Ÿå›é¡¾ä¸€ä¸‹è¿™ä¸ªæ–¹æ³•ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä»·å€¼è¿­ä»£æ–¹æ³•ä¼šéå†æ‰€æœ‰çŠ¶æ€ï¼Œå¹¶å¯¹æ¯ä¸ªçŠ¶æ€ä½¿ç”¨è´å°”æ›¼è¿‘ä¼¼è¿›è¡Œä»·å€¼æ›´æ–°ã€‚å¯¹Qå€¼ï¼ˆåŠ¨ä½œçš„ä»·å€¼ï¼‰è¿›è¡Œçš„ç›¸åŒæ–¹æ³•å˜ä½“å‡ ä¹ç›¸åŒï¼Œä½†æˆ‘ä»¬è¦ä¸ºæ¯ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œè¿‘ä¼¼å¹¶å­˜å‚¨ä»·å€¼ã€‚é‚£ä¹ˆï¼Œè¿™ä¸ªè¿‡ç¨‹åˆ°åº•æœ‰ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ
- en: The first obvious problem is the count of environment states and our ability
    to iterate over them. In value iteration, we assume that we know all states in
    our environment in advance, can iterate over them, and can store their value approximations.
    Itâ€™s easy to do for the simple grid world environment of FrozenLake, but what
    about other tasks?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ˜æ˜¾çš„é—®é¢˜æ˜¯ç¯å¢ƒçŠ¶æ€çš„æ•°é‡ä»¥åŠæˆ‘ä»¬éå†å®ƒä»¬çš„èƒ½åŠ›ã€‚åœ¨ä»·å€¼è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬äº‹å…ˆçŸ¥é“ç¯å¢ƒä¸­çš„æ‰€æœ‰çŠ¶æ€ï¼Œèƒ½å¤Ÿéå†å®ƒä»¬ï¼Œå¹¶ä¸”å¯ä»¥å­˜å‚¨å®ƒä»¬çš„ä»·å€¼è¿‘ä¼¼å€¼ã€‚å¯¹äºFrozenLakeçš„ç®€å•ç½‘æ ¼ä¸–ç•Œç¯å¢ƒï¼Œè¿™å¾ˆå®¹æ˜“åšåˆ°ï¼Œä½†å¯¹äºå…¶ä»–ä»»åŠ¡å‘¢ï¼Ÿ
- en: To understand this, letâ€™s first look at how scalable the value iteration approach
    is, or, in other words, how many states we can easily iterate over in every loop.
    Even a moderate-sized computer can keep several billion float values in memory
    (8.5 billion in 32 GB of RAM), so the memory required for value tables doesnâ€™t
    look like a huge constraint. Iteration over billions of states and actions will
    be more central processing unit (CPU)-demanding but is not an insurmountable problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œé¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹ä»·å€¼è¿­ä»£æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œæ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åœ¨æ¯æ¬¡å¾ªç¯ä¸­èƒ½å¤Ÿè½»æ¾éå†å¤šå°‘ä¸ªçŠ¶æ€ã€‚å³ä½¿æ˜¯ä¸­ç­‰å¤§å°çš„è®¡ç®—æœºï¼Œä¹Ÿå¯ä»¥åœ¨å†…å­˜ä¸­å­˜å‚¨æ•°åäº¿ä¸ªæµ®ç‚¹å€¼ï¼ˆåœ¨32GBçš„å†…å­˜ä¸­æ˜¯85äº¿ï¼‰ï¼Œå› æ­¤ï¼Œä»·å€¼è¡¨æ‰€éœ€çš„å†…å­˜ä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªå·¨å¤§çš„é™åˆ¶ã€‚éå†æ•°åäº¿ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œä¼šæ›´åŠ æ¶ˆè€—ä¸­å¤®å¤„ç†å•å…ƒï¼ˆCPUï¼‰ï¼Œä½†è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæ— æ³•å…‹æœçš„é—®é¢˜ã€‚
- en: Nowadays, we have multicore systems that are mostly idle, so by using parallelism,
    we can iterate over billions of values in a reasonable amount of time. The real
    problem is the number of samples required to get good approximations for state
    transition dynamics. Imagine that you have some environment with, say, a billion
    states (which corresponds approximately to a FrozenLake of size 31600 Ã— 31600).
    To calculate even a rough approximation for every state of this environment, we
    would need hundreds of billions of transitions evenly distributed over our states,
    which is not practical.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†å¤šæ ¸ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå¤§å¤šæ˜¯é—²ç½®çš„ï¼Œæ‰€ä»¥é€šè¿‡ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…éå†æ•°åäº¿ä¸ªå€¼ã€‚çœŸæ­£çš„é—®é¢˜åœ¨äºï¼Œè·å–çŠ¶æ€è½¬æ¢åŠ¨æ€çš„è‰¯å¥½è¿‘ä¼¼æ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚å‡è®¾ä½ æœ‰ä¸€ä¸ªç¯å¢ƒï¼Œå‡è®¾æœ‰åäº¿ä¸ªçŠ¶æ€ï¼ˆè¿™å¤§çº¦å¯¹åº”ä¸€ä¸ª
    31600 Ã— 31600 å¤§å°çš„ FrozenLakeï¼‰ã€‚è¦è®¡ç®—è¿™ä¸ªç¯å¢ƒä¸­æ¯ä¸ªçŠ¶æ€çš„ç²—ç•¥è¿‘ä¼¼ï¼Œæˆ‘ä»¬éœ€è¦æ•°ç™¾äº¿ä¸ªçŠ¶æ€ä¹‹é—´å‡åŒ€åˆ†å¸ƒçš„è½¬æ¢ï¼Œè¿™åœ¨å®è·µä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚
- en: To give you an example of an environment with an even larger number of potential
    states, letâ€™s consider the Atari 2600 game console again. This was very popular
    in the 1980s, and many arcade-style games were available for it. The Atari console
    is archaic by todayâ€™s gaming standards, but its games provide an excellent set
    of RL problems that humans can master fairly quickly, yet are still challenging
    for computers. Not surprisingly, this platform (using an emulator, of course)
    is a very popular benchmark within RL research, as I mentioned.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ›´å¤§æ½œåœ¨çŠ¶æ€æ•°é‡çš„ç¯å¢ƒç¤ºä¾‹ï¼Œæˆ‘ä»¬å†æ¥çœ‹çœ‹ Atari 2600 æ¸¸æˆä¸»æœºã€‚è¿™æ¬¾ä¸»æœºåœ¨1980å¹´ä»£éå¸¸æµè¡Œï¼Œå¹¶ä¸”æœ‰è®¸å¤šè¡—æœºé£æ ¼çš„æ¸¸æˆå¯ä¾›é€‰æ‹©ã€‚è™½ç„¶æŒ‰ç…§ä»Šå¤©çš„æ¸¸æˆæ ‡å‡†ï¼ŒAtari
    ä¸»æœºæ˜¾å¾—è¿‡æ—¶ï¼Œä½†å®ƒçš„æ¸¸æˆæä¾›äº†ä¸€ç»„å¾ˆå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œäººç±»å¯ä»¥ç›¸å¯¹å¿«é€Ÿåœ°æŒæ¡è¿™äº›é—®é¢˜ï¼Œä½†å¯¹è®¡ç®—æœºæ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸è¶³ä¸ºå¥‡çš„æ˜¯ï¼Œæ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œè¿™ä¸ªå¹³å°ï¼ˆå½“ç„¶æ˜¯ä½¿ç”¨æ¨¡æ‹Ÿå™¨ï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸­æ˜¯ä¸€ä¸ªéå¸¸å—æ¬¢è¿çš„åŸºå‡†ã€‚
- en: Letâ€™s calculate the state space for the Atari platform. The resolution of the
    screen is 210 Ã— 160 pixels, and every pixel has one of 128 colors. So every frame
    of the screen has 210 â‹… 160 = 33600 pixels and the total number of different screens
    possible is 128^(33600), which is slightly more than 10^(70802). If we decide
    to just enumerate all possible states of the Atari once, it will take billions
    of billions of years even for the fastest supercomputer. Also, 99(.9)% of this
    job will be a waste of time, as most of the combinations will never be shown during
    even long gameplay, so we will never have samples of those states. However, the
    value iteration method wants to iterate over them just in case.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹ Atari å¹³å°çš„çŠ¶æ€ç©ºé—´ã€‚å±å¹•çš„åˆ†è¾¨ç‡æ˜¯ 210 Ã— 160 åƒç´ ï¼Œæ¯ä¸ªåƒç´ æœ‰ 128 ç§é¢œè‰²ã€‚å› æ­¤ï¼Œæ¯ä¸€å¸§å±å¹•æœ‰ 210 â‹… 160
    = 33600 ä¸ªåƒç´ ï¼Œè€Œæ‰€æœ‰å¯èƒ½çš„ä¸åŒå±å¹•æ€»æ•°æ˜¯ 128^(33600)ï¼Œçº¦ç­‰äº 10^(70802)ã€‚å¦‚æœæˆ‘ä»¬å†³å®šåªåˆ—ä¸¾ä¸€æ¬¡ Atari æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼Œå³ä½¿æ˜¯æœ€å¿«çš„è¶…çº§è®¡ç®—æœºä¹Ÿéœ€è¦æ•°åäº¿å¹´ã€‚è€Œä¸”ï¼Œè¿™é¡¹å·¥ä½œä¸­99(.9)%çš„æ—¶é—´éƒ½å°†æ˜¯æµªè´¹ï¼Œå› ä¸ºå¤§å¤šæ•°ç»„åˆåœ¨é•¿æ—¶é—´çš„æ¸¸æˆè¿‡ç¨‹ä¸­ä»æœªå‡ºç°è¿‡ï¼Œæˆ‘ä»¬ä¹Ÿä¸ä¼šæœ‰è¿™äº›çŠ¶æ€çš„æ ·æœ¬ã€‚ç„¶è€Œï¼Œå€¼è¿­ä»£æ–¹æ³•ä»ç„¶æƒ³éå†è¿™äº›çŠ¶æ€ï¼Œä»¥é˜²ä¸‡ä¸€ã€‚
- en: The second main problem with the value iteration approach is that it limits
    us to discrete action spaces. Indeed, both Q(s,a) and V (s) approximations assume
    that our actions are a mutually exclusive discrete set, which is not true for
    continuous control problems where actions can represent continuous variables,
    such as the angle of a steering wheel, the force on an actuator, or the temperature
    of a heater. This issue is much more challenging than the first, and we will talk
    about it in the last part of the book, in chapters dedicated to continuous action
    space problems. For now, letâ€™s assume that we have a discrete count of actions
    and that this count is not very large (i.e., orders of 10s). How should we handle
    the state space size issue?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£æ–¹æ³•çš„ç¬¬äºŒä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼Œå®ƒå°†æˆ‘ä»¬é™åˆ¶åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸­ã€‚å®é™…ä¸Šï¼ŒQ(s,a) å’Œ V(s) çš„è¿‘ä¼¼éƒ½å‡è®¾æˆ‘ä»¬çš„åŠ¨ä½œæ˜¯ä¸€ä¸ªç›¸äº’æ’æ–¥çš„ç¦»æ•£é›†åˆï¼Œä½†å¯¹äºè¿ç»­æ§åˆ¶é—®é¢˜æ¥è¯´ï¼ŒåŠ¨ä½œå¯ä»¥è¡¨ç¤ºè¿ç»­çš„å˜é‡ï¼Œä¾‹å¦‚æ–¹å‘ç›˜çš„è§’åº¦ã€æ‰§è¡Œå™¨ä¸Šçš„åŠ›ï¼Œæˆ–è€…åŠ çƒ­å™¨çš„æ¸©åº¦ï¼Œè¿™åœ¨æ­¤ç±»é—®é¢˜ä¸­å¹¶ä¸æˆç«‹ã€‚è¿™ä¸ªé—®é¢˜æ¯”ç¬¬ä¸€ä¸ªé—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬å°†åœ¨ä¹¦çš„æœ€åéƒ¨åˆ†ï¼Œä¸“é—¨è®¨è®ºè¿ç»­åŠ¨ä½œç©ºé—´é—®é¢˜çš„ç« èŠ‚ä¸­è¿›è¡Œè®²è§£ã€‚ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¦»æ•£çš„åŠ¨ä½œè®¡æ•°ï¼Œå¹¶ä¸”è¿™ä¸ªè®¡æ•°ä¸æ˜¯å¾ˆå¤§ï¼ˆå³æ•°é‡çº§ä¸º10çš„æ•°é‡ï¼‰ã€‚æˆ‘ä»¬åº”è¯¥å¦‚ä½•å¤„ç†çŠ¶æ€ç©ºé—´å¤§å°çš„é—®é¢˜å‘¢ï¼Ÿ
- en: Tabular Q-learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¡¨æ ¼ Q å­¦ä¹ 
- en: The key question to focus on when trying to handle the state space issue is,
    do we really need to iterate over every state in the state space? We have an environment
    that can be used as a source of real-life samples of states. If some state in
    the state space is not shown to us by the environment, why should we care about
    its value? We can only use states obtained from the environment to update the
    values of states, which can save us a lot of work.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†çŠ¶æ€ç©ºé—´é—®é¢˜æ—¶éœ€è¦å…³æ³¨çš„å…³é”®é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬æ˜¯å¦çœŸçš„éœ€è¦éå†çŠ¶æ€ç©ºé—´ä¸­çš„æ¯ä¸ªçŠ¶æ€ï¼Ÿæˆ‘ä»¬æœ‰ä¸€ä¸ªç¯å¢ƒï¼Œå¯ä»¥ç”¨ä½œç°å®ç”Ÿæ´»ä¸­çŠ¶æ€æ ·æœ¬çš„æ¥æºã€‚å¦‚æœæŸä¸ªçŠ¶æ€æ²¡æœ‰è¢«ç¯å¢ƒå±•ç¤ºå‡ºæ¥ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦å…³å¿ƒå®ƒçš„ä»·å€¼å‘¢ï¼Ÿæˆ‘ä»¬åªèƒ½ä½¿ç”¨ä»ç¯å¢ƒä¸­è·å¾—çš„çŠ¶æ€æ¥æ›´æ–°çŠ¶æ€å€¼ï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå¤§é‡çš„å·¥ä½œã€‚
- en: 'This modification of the value iteration method is known as Q-learning, as
    mentioned earlier, and for cases with explicit state-to-value mappings, it entails
    the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¿™ç§å€¼è¿­ä»£æ–¹æ³•çš„ä¿®æ”¹è¢«ç§°ä¸º Q å­¦ä¹ ï¼Œå¯¹äºå…·æœ‰æ˜ç¡®çŠ¶æ€åˆ°å€¼æ˜ å°„çš„æƒ…å†µï¼Œå®ƒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
- en: Start with an empty table, mapping states to values of actions.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªç©ºè¡¨å¼€å§‹ï¼Œå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œçš„å€¼ã€‚
- en: By interacting with the environment, obtain the tuple s, a, r, sâ€² (state, action,
    reward, and the new state). In this step, you need to decide which action to take,
    and there is no single proper way to make this decision. We discussed this problem
    as exploration versus exploitation in ChapterÂ [1](ch005.xhtml#x1-190001) and will
    talk a lot about it in this chapter.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œè·å–å…ƒç»„ s, a, r, sâ€²ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œæ–°çŠ¶æ€ï¼‰ã€‚åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œä½ éœ€è¦å†³å®šé‡‡å–å“ªä¸ªåŠ¨ä½œï¼Œè€Œä¸”æ²¡æœ‰å•ä¸€çš„æ­£ç¡®æ–¹æ³•æ¥åšå‡ºè¿™ä¸ªå†³ç­–ã€‚æˆ‘ä»¬åœ¨ç¬¬
    [1](ch005.xhtml#x1-190001) ç« ä¸­è®¨è®ºè¿‡è¿™ä¸ªé—®é¢˜ï¼Œæ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¹¶å°†åœ¨æœ¬ç« ä¸­æ·±å…¥è®¨è®ºã€‚
- en: 'Update the Q(s,a) value using the Bellman approximation:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Bellman è¿‘ä¼¼æ›´æ–° Q(s,a) çš„å€¼ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq18.png)'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq18.png)'
- en: Repeat from step 2.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ã€‚
- en: As in value iteration, the end condition could be some threshold of the update,
    or we could perform test episodes to estimate the expected reward from the policy.
    Another thing to note here is how to update the Q-values. As we take samples from
    the environment, itâ€™s generally a bad idea to just assign new values on top of
    existing values, as training can become unstable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å€¼è¿­ä»£ä¸€æ ·ï¼Œç»“æŸæ¡ä»¶å¯ä»¥æ˜¯æŸä¸ªæ›´æ–°çš„é˜ˆå€¼ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥æ‰§è¡Œæµ‹è¯•å›åˆæ¥ä¼°ç®—ç­–ç•¥çš„æœŸæœ›å¥–åŠ±ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„å¦ä¸€ç‚¹æ˜¯å¦‚ä½•æ›´æ–° Q å€¼ã€‚å½“æˆ‘ä»¬ä»ç¯å¢ƒä¸­é‡‡æ ·æ—¶ï¼Œç›´æ¥åœ¨ç°æœ‰å€¼ä¸Šåˆ†é…æ–°å€¼é€šå¸¸æ˜¯ä¸€ä¸ªåä¸»æ„ï¼Œå› ä¸ºè®­ç»ƒå¯èƒ½ä¼šå˜å¾—ä¸ç¨³å®šã€‚
- en: 'What is usually done in practice is updating the Q(s,a) with approximations
    using a â€œblendingâ€ technique, which is just averaging between old and new values
    of Q using learning rate Î± with a value from 0 to 1:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å®è·µä¸­é€šå¸¸é‡‡ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ç§â€œæ··åˆâ€æŠ€æœ¯æ›´æ–° Q(s,a)ï¼Œå³é€šè¿‡å­¦ä¹ ç‡ Î± å¯¹ Q çš„æ—§å€¼å’Œæ–°å€¼è¿›è¡Œå¹³å‡ï¼ŒÎ± çš„å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq19.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq19.png)'
- en: 'This allows values of Q to converge smoothly, even if our environment is noisy.
    The final version of the algorithm is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾— Q å€¼èƒ½å¤Ÿå¹³ç¨³æ”¶æ•›ï¼Œå³ä½¿æˆ‘ä»¬çš„ç¯å¢ƒæ˜¯å˜ˆæ‚çš„ã€‚æœ€ç»ˆç‰ˆæœ¬çš„ç®—æ³•å¦‚ä¸‹ï¼š
- en: Start with an empty table for Q(s,a).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªç©ºè¡¨å¼€å§‹ï¼Œè¡¨ç¤º Q(s,a)ã€‚
- en: Obtain (s, a, r, sâ€²) from the environment.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç¯å¢ƒä¸­è·å– (s, a, r, sâ€²)ã€‚
- en: 'Make a Bellman update:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›è¡Œ Bellman æ›´æ–°ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq19.png)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq19.png)'
- en: Check convergence conditions. If not met, repeat from step 2.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ”¶æ•›æ¡ä»¶ã€‚å¦‚æœæ¡ä»¶æœªæ»¡è¶³ï¼Œä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ã€‚
- en: 'As mentioned earlier, this method is called tabular Q-learning, as we keep
    a table of states with their Q-values. Letâ€™s try it on our FrozenLake environment.
    The whole example code is in Chapter06/01_frozenlake_q_learning.py:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºè¡¨æ ¼ Q å­¦ä¹ ï¼Œå› ä¸ºæˆ‘ä»¬ç»´æŠ¤ä¸€ä¸ªåŒ…å«çŠ¶æ€åŠå…¶ Q å€¼çš„è¡¨æ ¼ã€‚è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ FrozenLake ç¯å¢ƒä¸­å°è¯•ä¸€ä¸‹ã€‚å®Œæ•´çš„ç¤ºä¾‹ä»£ç åœ¨
    Chapter06/01_frozenlake_q_learning.py ä¸­ï¼š
- en: 'First, we import packages and define constants and used types:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥åŒ…å¹¶å®šä¹‰å¸¸é‡å’Œä½¿ç”¨çš„ç±»å‹ï¼š
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The new thing here is the value of Î±, which will be used as the learning rate
    in the value update. The initialization of our Agent class is simpler now, as
    we donâ€™t need to track the history of rewards and transition counters, just our
    value table. This will make our memory footprint smaller, which is not a big issue
    for FrozenLake but can be critical for larger environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æ–°å†…å®¹æ˜¯ Î± çš„å€¼ï¼Œå®ƒå°†ç”¨ä½œå€¼æ›´æ–°ä¸­çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬ç°åœ¨çš„ Agent ç±»åˆå§‹åŒ–æ›´åŠ ç®€åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦è·Ÿè¸ªå¥–åŠ±å’Œè¿‡æ¸¡è®¡æ•°çš„å†å²è®°å½•ï¼Œåªéœ€è¦æˆ‘ä»¬çš„å€¼è¡¨ã€‚è¿™å°†ä½¿æˆ‘ä»¬çš„å†…å­˜å ç”¨æ›´å°ï¼Œè™½ç„¶å¯¹äº
    FrozenLake æ¥è¯´è¿™ä¸æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ï¼Œä½†å¯¹äºæ›´å¤§çš„ç¯å¢ƒå¯èƒ½è‡³å…³é‡è¦ã€‚
- en: 'The method sample_env is used to obtain the next transition from the environment:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹æ³• sample_env ç”¨äºä»ç¯å¢ƒä¸­è·å–ä¸‹ä¸€ä¸ªè¿‡æ¸¡ï¼š
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We sample a random action from the action space and return the tuple of the
    old state, the action taken, the reward obtained, and the new state. The tuple
    will be used in the training loop later.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶è¿”å›åŒ…å«æ—§çŠ¶æ€ã€æ‰€é‡‡å–çš„åŠ¨ä½œã€è·å¾—çš„å¥–åŠ±å’Œæ–°çŠ¶æ€çš„å…ƒç»„ã€‚è¯¥å…ƒç»„å°†åœ¨åç»­çš„è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ã€‚
- en: 'The next method receives the state of the environment:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªæ–¹æ³•æ¥æ”¶ç¯å¢ƒçš„çŠ¶æ€ï¼š
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This method finds the best action to take from the given state of the environment
    by taking the action with the largest value that we have in the table. If we donâ€™t
    have the value associated with the state and action pair, then we take it as zero.
    This method will be used two times: first, in the test method that plays one episode
    using our current values table (to evaluate our policyâ€™s quality), and second,
    in the method that performs the value update to get the value of the next state.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•é€šè¿‡é€‰æ‹©åœ¨è¡¨æ ¼ä¸­å…·æœ‰æœ€å¤§å€¼çš„åŠ¨ä½œæ¥ä»ç»™å®šçš„ç¯å¢ƒçŠ¶æ€ä¸­æ‰¾åˆ°æœ€ä½³åŠ¨ä½œã€‚å¦‚æœæˆ‘ä»¬æ²¡æœ‰ä¸çŠ¶æ€å’ŒåŠ¨ä½œå¯¹ç›¸å…³çš„å€¼ï¼Œåˆ™å°†å…¶è§†ä¸ºé›¶ã€‚è¯¥æ–¹æ³•å°†è¢«ä½¿ç”¨ä¸¤æ¬¡ï¼šç¬¬ä¸€æ¬¡ï¼Œåœ¨æµ‹è¯•æ–¹æ³•ä¸­ï¼Œå®ƒä½¿ç”¨æˆ‘ä»¬å½“å‰çš„å€¼è¡¨è¿›è¡Œä¸€æ¬¡å›åˆï¼ˆä»¥è¯„ä¼°æˆ‘ä»¬çš„ç­–ç•¥è´¨é‡ï¼‰ï¼›ç¬¬äºŒæ¬¡ï¼Œåœ¨æ‰§è¡Œå€¼æ›´æ–°çš„æ–¹æ³•ä¸­ï¼Œç”¨äºè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ã€‚
- en: 'Next, we update our values table using one step from the environment:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¯å¢ƒä¸­çš„ä¸€æ­¥æ“ä½œæ¥æ›´æ–°æˆ‘ä»¬çš„å€¼è¡¨ï¼š
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we first calculate the Bellman approximation for our state, s, and action,
    a, by summing the immediate reward with the discounted value of the next state.
    Then, we obtain the previous value of the state and action pair and blend these
    values together using the learning rate. The result is the new approximation for
    the value of state s and action a, which is stored in our table.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å°†å³æ—¶å¥–åŠ±ä¸ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æŠ˜æ‰£å€¼ç›¸åŠ æ¥è®¡ç®—æˆ‘ä»¬å½“å‰çŠ¶æ€ s å’ŒåŠ¨ä½œ a çš„è´å°”æ›¼è¿‘ä¼¼ã€‚ç„¶åï¼Œæˆ‘ä»¬è·å¾—çŠ¶æ€å’ŒåŠ¨ä½œå¯¹çš„å…ˆå‰å€¼ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ ç‡å°†è¿™äº›å€¼æ··åˆåœ¨ä¸€èµ·ã€‚ç»“æœæ˜¯çŠ¶æ€
    s å’ŒåŠ¨ä½œ a çš„æ–°è¿‘ä¼¼å€¼ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨æˆ‘ä»¬çš„è¡¨æ ¼ä¸­ã€‚
- en: 'The last method in our Agent class plays one full episode using the provided
    test environment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ Agent ç±»ä¸­çš„æœ€åä¸€ä¸ªæ–¹æ³•ä½¿ç”¨æä¾›çš„æµ‹è¯•ç¯å¢ƒè¿›è¡Œä¸€æ¬¡å®Œæ•´çš„å›åˆï¼š
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The action on every step is taken using our current value table of Q-values.
    This method is used to evaluate our current policy to check the progress of learning.
    Note that this method doesnâ€™t alter our value table; it only uses it to find the
    best action to take.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸€æ­¥çš„åŠ¨ä½œéƒ½æ˜¯ä½¿ç”¨æˆ‘ä»¬å½“å‰çš„ Q å€¼è¡¨æ¥é€‰æ‹©çš„ã€‚è¯¥æ–¹æ³•ç”¨äºè¯„ä¼°æˆ‘ä»¬å½“å‰çš„ç­–ç•¥ï¼Œä»¥æ£€æŸ¥å­¦ä¹ çš„è¿›å±•ã€‚è¯·æ³¨æ„ï¼Œæ­¤æ–¹æ³•ä¸ä¼šæ›´æ”¹æˆ‘ä»¬çš„å€¼è¡¨ï¼Œå®ƒä»…ä»…ä½¿ç”¨å€¼è¡¨æ¥æ‰¾åˆ°æœ€ä½³åŠ¨ä½œã€‚
- en: 'The rest of the example is the training loop, which is very similar to examples
    from ChapterÂ [5](ch009.xhtml#x1-820005): we create a test environment, agent,
    and summary writer, and then, in the loop, we do one step in the environment and
    perform a value update using the obtained data. Next, we test our current policy
    by playing several test episodes. If a good reward is obtained, then we stop training:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹çš„å…¶ä½™éƒ¨åˆ†æ˜¯è®­ç»ƒå¾ªç¯ï¼Œç±»ä¼¼äºç¬¬ [5](ch009.xhtml#x1-820005) ç« ä¸­çš„ç¤ºä¾‹ï¼šæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç¯å¢ƒã€ä»£ç†å’Œæ‘˜è¦å†™å…¥å™¨ï¼Œç„¶ååœ¨å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬åœ¨ç¯å¢ƒä¸­è¿›è¡Œä¸€æ­¥æ“ä½œï¼Œå¹¶ä½¿ç”¨è·å¾—çš„æ•°æ®æ‰§è¡Œå€¼æ›´æ–°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡è¿›è¡Œå¤šä¸ªæµ‹è¯•å›åˆæ¥æµ‹è¯•æˆ‘ä»¬å½“å‰çš„ç­–ç•¥ã€‚å¦‚æœè·å¾—äº†è‰¯å¥½çš„å¥–åŠ±ï¼Œåˆ™åœæ­¢è®­ç»ƒï¼š
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result of the example is shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You may have noticed that this version used more iterations (but your experiment
    might have a different count of steps) to solve the problem compared to the value
    iteration method from the previous chapter. The reason for that is that we are
    no longer using the experience obtained during testing. In the example Chapter05/02_frozenlake_q_iteration.py,
    periodical tests caused an update of Q-table statistics. Here, we donâ€™t touch
    Q-values during the test, which causes more iterations before the environment
    gets solved.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œä¸ä¸Šä¸€ç« çš„å€¼è¿­ä»£æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ä¸ªç‰ˆæœ¬ä½¿ç”¨äº†æ›´å¤šçš„è¿­ä»£ï¼ˆä½†ä½ çš„å®éªŒå¯èƒ½æœ‰ä¸åŒçš„æ­¥éª¤æ•°ï¼‰æ¥è§£å†³é—®é¢˜ã€‚åŸå› åœ¨äºæˆ‘ä»¬ä¸å†ä½¿ç”¨åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­è·å¾—çš„ç»éªŒã€‚åœ¨ç¤ºä¾‹
    Chapter05/02_frozenlake_q_iteration.py ä¸­ï¼Œå‘¨æœŸæ€§æµ‹è¯•å¯¼è‡´ Q è¡¨ç»Ÿè®¡çš„æ›´æ–°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä¸è§¦åŠ Q å€¼ï¼Œè¿™å¯¼è‡´åœ¨ç¯å¢ƒè§£å†³ä¹‹å‰éœ€è¦æ›´å¤šçš„è¿­ä»£ã€‚
- en: 'Overall, the total number of samples required from the environment is almost
    the same. The reward chart in TensorBoard also shows good training dynamics, which
    is very similar to the value iteration method (the reward plot for value iteration
    is shown in FigureÂ [5.9](ch009.xhtml#x1-87114r9)):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œä»ç¯å¢ƒä¸­æ‰€éœ€çš„æ ·æœ¬æ€»æ•°å‡ ä¹ç›¸åŒã€‚TensorBoard ä¸­çš„å¥–åŠ±å›¾ä¹Ÿæ˜¾ç¤ºäº†è‰¯å¥½çš„è®­ç»ƒåŠ¨æ€ï¼Œè¿™ä¸å€¼è¿­ä»£æ–¹æ³•éå¸¸ç›¸ä¼¼ï¼ˆå€¼è¿­ä»£çš„å¥–åŠ±å›¾å¦‚å›¾ [5.9](ch009.xhtml#x1-87114r9)
    æ‰€ç¤ºï¼‰ï¼š
- en: '![PIC](img/B22150_06_01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_06_01.png)'
- en: 'FigureÂ 6.1: Reward dynamics of FrozenLake'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6.1ï¼šFrozenLake çš„å¥–åŠ±åŠ¨æ€
- en: In the next section, we will extend the Q-learning method with NNsâ€™ preprocessing
    environment states. This will greatly extend the flexibility and applicability
    of the method we discussed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ‰©å±•Q-learningæ–¹æ³•ï¼Œç»“åˆç¥ç»ç½‘ç»œï¼ˆNNsï¼‰å¯¹ç¯å¢ƒçŠ¶æ€çš„é¢„å¤„ç†ã€‚è¿™å°†æå¤§åœ°æ‰©å±•æˆ‘ä»¬è®¨è®ºè¿‡çš„æ–¹æ³•çš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚
- en: Deep Q-learning
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦Qå­¦ä¹ 
- en: The Q-learning method that we have just covered solves the issue of iteration
    over the full set of states, but it can still struggle with situations when the
    count of the observable set of states is very large. For example, Atari games
    can have a large variety of different screens, so if we decide to use raw pixels
    as individual states, we will quickly realize that we have too many states to
    track and approximate values for.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšä»‹ç»çš„Q-learningæ–¹æ³•è§£å†³äº†éå†æ‰€æœ‰çŠ¶æ€é›†çš„é—®é¢˜ï¼Œä½†å½“å¯è§‚å¯ŸçŠ¶æ€é›†çš„æ•°é‡éå¸¸å¤§æ—¶ï¼Œå®ƒä»ç„¶å¯èƒ½é‡åˆ°å›°éš¾ã€‚ä¾‹å¦‚ï¼ŒAtariæ¸¸æˆå¯èƒ½æœ‰è®¸å¤šä¸åŒçš„å±å¹•ï¼Œå¦‚æœæˆ‘ä»¬å†³å®šå°†åŸå§‹åƒç´ ä½œä¸ºå•ç‹¬çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å¾ˆå¿«å°±ä¼šæ„è¯†åˆ°æˆ‘ä»¬æœ‰å¤ªå¤šçš„çŠ¶æ€éœ€è¦è¿½è¸ªå’Œä¼°ç®—å€¼ã€‚
- en: In some environments, the count of different observable states could be almost
    infinite. For example, in CartPole, the environment gives us a state that is four
    floating point numbers. The number of value combinations is finite (theyâ€™re represented
    as bits), but this number is extremely large. With just bit values, it is around
    2^(4â‹…32) â‰ˆ 3.4 â‹… 10^(38). In reality, it is less, as state values of the environment
    are bounded, so not all bit combinations of 4 float32 values are possible, but
    the resulting state space is still too large. We could create some bins to discretize
    those values, but this often creates more problems than it solves; we would need
    to decide what ranges of parameters are important to distinguish as different
    states and what ranges could be clustered together. As weâ€™re trying to implement
    RL methods in a general way (without looking inside the environmentâ€™s internals),
    this is not a very promising direction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›ç¯å¢ƒä¸­ï¼Œä¸åŒçš„å¯è§‚å¯ŸçŠ¶æ€çš„æ•°é‡å‡ ä¹æ˜¯æ— é™çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨CartPoleä¸­ï¼Œç¯å¢ƒç»™æˆ‘ä»¬æä¾›çš„çŠ¶æ€æ˜¯å››ä¸ªæµ®åŠ¨ç‚¹æ•°ã€‚æ•°å€¼ç»„åˆçš„æ•°é‡æ˜¯æœ‰é™çš„ï¼ˆå®ƒä»¬ä»¥æ¯”ç‰¹è¡¨ç¤ºï¼‰ï¼Œä½†è¿™ä¸ªæ•°å­—æå…¶åºå¤§ã€‚ä»…ç”¨æ¯”ç‰¹å€¼è¡¨ç¤ºæ—¶ï¼Œçº¦ä¸º2^(4â‹…32)
    â‰ˆ 3.4 â‹… 10^(38)ã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªå€¼ä¼šå°ä¸€äº›ï¼Œå› ä¸ºç¯å¢ƒçŠ¶æ€çš„å€¼æ˜¯æœ‰é™åˆ¶çš„ï¼Œå¹¶éæ‰€æœ‰çš„4ä¸ªfloat32å€¼çš„æ¯”ç‰¹ç»„åˆéƒ½æ˜¯å¯èƒ½çš„ï¼Œä½†ç»“æœçš„çŠ¶æ€ç©ºé—´ä¾ç„¶å¤ªå¤§ã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€äº›ç®±å­æ¥ç¦»æ•£åŒ–è¿™äº›å€¼ï¼Œä½†è¿™é€šå¸¸ä¼šå¸¦æ¥æ¯”è§£å†³æ›´å¤šé—®é¢˜ï¼›æˆ‘ä»¬éœ€è¦å†³å®šå“ªäº›å‚æ•°èŒƒå›´é‡è¦ï¼Œéœ€è¦åŒºåˆ†æˆä¸åŒçš„çŠ¶æ€ï¼Œè€Œå“ªäº›èŒƒå›´å¯ä»¥å½’ç±»åœ¨ä¸€èµ·ã€‚ç”±äºæˆ‘ä»¬å°è¯•ä»¥ä¸€èˆ¬çš„æ–¹å¼å®ç°RLæ–¹æ³•ï¼ˆè€Œä¸æ·±å…¥äº†è§£ç¯å¢ƒçš„å†…éƒ¨ç»“æ„ï¼‰ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„æ–¹å‘ã€‚
- en: In the case of Atari, one single pixel change doesnâ€™t make much difference,
    so we might want to treat similar images as one state. However, we still need
    to distinguish some of the states.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Atariæ¸¸æˆçš„æƒ…å†µä¸‹ï¼Œå•ä¸€åƒç´ çš„å˜åŒ–å¹¶ä¸ä¼šé€ æˆå¤ªå¤§å·®å¼‚ï¼Œå› æ­¤æˆ‘ä»¬å¯èƒ½å¸Œæœ›å°†ç›¸ä¼¼çš„å›¾åƒè§†ä¸ºä¸€ä¸ªçŠ¶æ€ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦åŒºåˆ†ä¸€äº›çŠ¶æ€ã€‚
- en: The following image shows two different situations in a game of Pong. Weâ€™re
    playing against the artificial intelligence (AI) opponent by controlling a paddle
    (our paddle is on the right, whereas our opponentâ€™s is on the left). The objective
    of the game is to get the bouncing ball past our opponentâ€™s paddle, while preventing
    the ball from getting past our paddle. We can consider the two situations to be
    completely different. In the situation shown on the right, the ball is close to
    the opponent, so we can relax and watch. However, the situation on the left is
    more demanding; assuming that the ball is moving from left to right, the ball
    is moving toward our side, so we need to move our paddle quickly to avoid losing
    a point. The situations in FigureÂ [6.2](#x1-93003r2) are just two from the 10^(70802)
    possible situations, but we want our agent to act on them differently.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†Pongæ¸¸æˆä¸­çš„ä¸¤ç§ä¸åŒæƒ…å†µã€‚æˆ‘ä»¬æ­£åœ¨ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¯¹æ‰‹å¯¹æˆ˜ï¼Œé€šè¿‡æ§åˆ¶ä¸€ä¸ªæŒ¡æ¿ï¼ˆæˆ‘ä»¬çš„æŒ¡æ¿åœ¨å³ä¾§ï¼Œå¯¹æ‰‹çš„æŒ¡æ¿åœ¨å·¦ä¾§ï¼‰ã€‚æ¸¸æˆçš„ç›®æ ‡æ˜¯å°†å¼¹è·³çƒé€è¿‡å¯¹æ‰‹çš„æŒ¡æ¿ï¼ŒåŒæ—¶é˜²æ­¢çƒä»æˆ‘ä»¬çš„æŒ¡æ¿æ—è¾¹é£è¿‡ã€‚æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™ä¸¤ç§æƒ…å†µæ˜¯å®Œå…¨ä¸åŒçš„ã€‚åœ¨å³ä¾§å±•ç¤ºçš„æƒ…å†µä¸­ï¼Œçƒé è¿‘å¯¹æ‰‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ”¾æ¾å¹¶è§‚å¯Ÿã€‚ç„¶è€Œï¼Œå·¦ä¾§çš„æƒ…å†µè¦æ±‚æ›´é«˜ï¼›å‡è®¾çƒä»å·¦å‘å³ç§»åŠ¨ï¼Œçƒæ­£æœæˆ‘ä»¬çš„æŒ¡æ¿ç§»åŠ¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è¿…é€Ÿç§»åŠ¨æˆ‘ä»¬çš„æŒ¡æ¿ï¼Œä»¥é¿å…å¤±åˆ†ã€‚å›¾[6.2](#x1-93003r2)ä¸­çš„ä¸¤ç§æƒ…å†µåªæ˜¯10^(70802)ç§å¯èƒ½æƒ…å†µä¸­çš„ä¸¤ç§ï¼Œä½†æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¯¹è¿™äº›æƒ…å†µåšå‡ºä¸åŒçš„ååº”ã€‚
- en: '![PIC](img/file32.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file32.png)'
- en: 'FigureÂ 6.2: The ambiguity of observations in Pong. In the left image, the ball
    is moving to the right, toward our paddle, and on the right, its direction is
    opposite'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6.2ï¼šPongä¸­è§‚å¯Ÿçš„æ¨¡ç³Šæ€§ã€‚åœ¨å·¦ä¾§çš„å›¾åƒä¸­ï¼Œçƒæ­£å‘å³ç§»åŠ¨ï¼Œæœç€æˆ‘ä»¬çš„æŒ¡æ¿ï¼Œè€Œåœ¨å³ä¾§ï¼Œå®ƒçš„æ–¹å‘ç›¸åã€‚
- en: 'As a solution to this problem, we can use a nonlinear representation that maps
    both the state and action onto a value. In machine learning, this is called a
    â€œregression problem.â€ The concrete way to represent and train such a representation
    can vary, but, as you may have already guessed from this sectionâ€™s title, using
    a deep NN is one of the most popular options, especially when dealing with observations
    represented as screen images. With this in mind, letâ€™s make modifications to the
    Q-learning algorithm:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè¯¥é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªéçº¿æ€§è¡¨ç¤ºï¼Œå°†çŠ¶æ€å’ŒåŠ¨ä½œæ˜ å°„åˆ°ä¸€ä¸ªå€¼ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œè¿™ç§°ä¸ºâ€œå›å½’é—®é¢˜â€ã€‚è¡¨ç¤ºå’Œè®­ç»ƒè¿™ç§è¡¨ç¤ºçš„å…·ä½“æ–¹æ³•å¯ä»¥æœ‰æ‰€ä¸åŒï¼Œä½†æ­£å¦‚ä½ ä»æœ¬èŠ‚æ ‡é¢˜ä¸­å·²ç»çŒœåˆ°çš„é‚£æ ·ï¼Œä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ˜¯æœ€æµè¡Œçš„é€‰æ‹©ä¹‹ä¸€ï¼Œå°¤å…¶æ˜¯å½“å¤„ç†ä»¥å±å¹•å›¾åƒè¡¨ç¤ºçš„è§‚å¯Ÿæ—¶ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯¹Q-learningç®—æ³•è¿›è¡Œä¿®æ”¹ï¼š
- en: Initialize Q(s,a) with some initial approximation.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸€äº›åˆå§‹è¿‘ä¼¼å€¼åˆå§‹åŒ–Q(s,a)ã€‚
- en: By interacting with the environment, obtain the tuple (s, a, r, sâ€²).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œè·å¾—å…ƒç»„(s, a, r, sâ€²)ã€‚
- en: 'Calculate the loss:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq20.png)![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq21.png)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq20.png)![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq21.png)'
- en: Update Q(s,a) using the stochastic gradient descent (SGD) algorithm, by minimizing
    the loss with respect to the model parameters.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç®—æ³•æ›´æ–°Q(s,a)ï¼Œé€šè¿‡æœ€å°åŒ–å…³äºæ¨¡å‹å‚æ•°çš„æŸå¤±æ¥è¿›è¡Œæ›´æ–°ã€‚
- en: Repeat from step 2 until converged.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ­¥éª¤2å¼€å§‹é‡å¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚
- en: This algorithm looks simple, but, unfortunately, it wonâ€™t work very well. Letâ€™s
    discuss some of the aspects that could go wrong and the potential ways we could
    approach these scenarios.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®—æ³•çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒçš„æ•ˆæœå¹¶ä¸å¥½ã€‚è®©æˆ‘ä»¬è®¨è®ºä¸€äº›å¯èƒ½å‡ºé”™çš„æ–¹é¢ï¼Œä»¥åŠæˆ‘ä»¬å¯ä»¥å¦‚ä½•å¤„ç†è¿™äº›æƒ…å†µã€‚
- en: Interaction with the environment
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ç¯å¢ƒäº¤äº’
- en: First of all, we need to interact with the environment somehow to receive data
    to train on. In simple environments, such as FrozenLake, we can act randomly,
    but is this the best strategy to use? Imagine the game of Pong. Whatâ€™s the probability
    of winning a single point by randomly moving the paddle? Itâ€™s not zero, but itâ€™s
    extremely small, which just means that we will need to wait for a very long time
    for such a rare situation. As an alternative, we can use our Q-function approximation
    as a source of behavior (as we did before in the value iteration method, when
    we remembered our experience during testing).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»¥æŸç§æ–¹å¼ä¸ç¯å¢ƒäº¤äº’ï¼Œä»¥æ¥æ”¶æ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨ç®€å•çš„ç¯å¢ƒä¸­ï¼Œæ¯”å¦‚FrozenLakeï¼Œæˆ‘ä»¬å¯ä»¥éšæœºè¡ŒåŠ¨ï¼Œä½†è¿™çœŸçš„æ˜¯æœ€å¥½çš„ç­–ç•¥å—ï¼Ÿæƒ³è±¡ä¸€ä¸‹Pongæ¸¸æˆã€‚é€šè¿‡éšæœºç§»åŠ¨æŒ¡æ¿ï¼Œè·å¾—ä¸€ä¸ªå•ç‹¬å¾—åˆ†çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿè¿™ä¸ªæ¦‚ç‡ä¸æ˜¯é›¶ï¼Œä½†å®ƒæå…¶å°ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ç­‰å¾…å¾ˆé•¿æ—¶é—´ï¼Œæ‰èƒ½é‡åˆ°è¿™ç§ç½•è§çš„æƒ…å†µã€‚ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Qå‡½æ•°çš„è¿‘ä¼¼å€¼ä½œä¸ºè¡Œä¸ºçš„æ¥æºï¼ˆå°±åƒæˆ‘ä»¬åœ¨ä»·å€¼è¿­ä»£æ–¹æ³•ä¸­åšçš„é‚£æ ·ï¼Œå½“æ—¶æˆ‘ä»¬åœ¨æµ‹è¯•æœŸé—´è®°ä½äº†è‡ªå·±çš„ç»éªŒï¼‰ã€‚
- en: If our representation of Q is good, then the experience that we get from the
    environment will show the agent relevant data to train on. However, weâ€™re in trouble
    when our approximation is not perfect (at the beginning of the training, for example).
    In such a case, our agent can be stuck with bad actions for some states without
    ever trying to behave differently. This is the exploration versus exploitation
    dilemma mentioned briefly in ChapterÂ [1](ch005.xhtml#x1-190001), which we will
    discuss in detail now. On the one hand, our agent needs to explore the environment
    to build a complete picture of transitions and action outcomes. On the other hand,
    we should use interaction with the environment efficiently; we shouldnâ€™t waste
    time by randomly trying actions that we have already tried and learned outcomes
    for.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„Qè¡¨ç¤ºæ˜¯å¥½çš„ï¼Œé‚£ä¹ˆä»ç¯å¢ƒä¸­è·å¾—çš„ç»éªŒå°†å‘ä»£ç†æä¾›ç›¸å…³çš„æ•°æ®ç”¨äºè®­ç»ƒã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬çš„è¿‘ä¼¼ä¸å®Œç¾æ—¶ï¼ˆä¾‹å¦‚åœ¨è®­ç»ƒçš„åˆæœŸï¼‰ï¼Œæˆ‘ä»¬å°±ä¼šé‡åˆ°é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä»£ç†å¯èƒ½ä¼šåœ¨æŸäº›çŠ¶æ€ä¸‹ä¸€ç›´é‡‡å–é”™è¯¯çš„è¡Œä¸ºï¼Œè€Œä»æœªå°è¯•è¿‡ä¸åŒçš„è¡Œä¸ºã€‚è¿™å°±æ˜¯åœ¨ç¬¬[1](ch005.xhtml#x1-190001)ç« ä¸­ç®€è¦æåˆ°çš„æ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒï¼Œæˆ‘ä»¬å°†åœ¨æ­¤è¯¦ç»†è®¨è®ºã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„ä»£ç†éœ€è¦æ¢ç´¢ç¯å¢ƒï¼Œä»¥å»ºç«‹å®Œæ•´çš„è½¬ç§»å’ŒåŠ¨ä½œç»“æœçš„å›¾æ™¯ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬åº”å½“é«˜æ•ˆåœ°åˆ©ç”¨ä¸ç¯å¢ƒçš„äº¤äº’ï¼›æˆ‘ä»¬ä¸åº”æµªè´¹æ—¶é—´éšæœºå°è¯•æˆ‘ä»¬å·²ç»å°è¯•è¿‡å¹¶ä¸”å·²çŸ¥ç»“æœçš„åŠ¨ä½œã€‚
- en: As you can see, random behavior is better at the beginning of the training when
    our Q approximation is bad, as it gives us more uniformly distributed information
    about the environment states. As our training progresses, random behavior becomes
    inefficient, and we want to fall back to our Q approximation to decide how to
    act.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œåœ¨è®­ç»ƒåˆæœŸï¼Œå½“æˆ‘ä»¬çš„Qè¿‘ä¼¼å€¼è¾ƒå·®æ—¶ï¼Œéšæœºè¡Œä¸ºåè€Œæ›´å¥½ï¼Œå› ä¸ºå®ƒèƒ½æä¾›æ›´å¤šå‡åŒ€åˆ†å¸ƒçš„ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ã€‚éšç€è®­ç»ƒçš„è¿›å±•ï¼Œéšæœºè¡Œä¸ºå˜å¾—ä½æ•ˆï¼Œæˆ‘ä»¬å¸Œæœ›å›å½’åˆ°Qè¿‘ä¼¼å€¼ä¸Šï¼Œä»¥å†³å®šå¦‚ä½•è¡ŒåŠ¨ã€‚
- en: A method that performs such a mix of two extreme behaviors is known as an epsilon-greedy
    method, which just means switching between random and Q policy using the probability
    hyperparameter ğœ–. By varying ğœ–, we can select the ratio of random actions. The
    usual practice is to start with ğœ– = 1.0 (100% random actions) and slowly decrease
    it to some small value, such as 5% or 2% random actions. Using an epsilon-greedy
    method helps us to both explore the environment in the beginning and stick to
    good policy at the end of the training. There are other solutions to the exploration
    versus exploitation problem, and we will discuss some of them in the third part
    of the book. This problem is one of the fundamental open questions in RL and an
    active area of research that is not even close to being resolved completely.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰§è¡Œè¿™ç§ä¸¤ç§æç«¯è¡Œä¸ºæ··åˆçš„æ–¹æ³•è¢«ç§°ä¸ºepsilon-è´ªå©ªæ–¹æ³•ï¼Œæ„æ€æ˜¯ä½¿ç”¨æ¦‚ç‡è¶…å‚æ•°ğœ–åœ¨éšæœºè¡Œä¸ºå’ŒQç­–ç•¥ä¹‹é—´åˆ‡æ¢ã€‚é€šè¿‡æ”¹å˜ğœ–çš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©éšæœºè¡Œä¸ºçš„æ¯”ä¾‹ã€‚é€šå¸¸çš„åšæ³•æ˜¯ä»ğœ–
    = 1.0ï¼ˆ100%çš„éšæœºè¡Œä¸ºï¼‰å¼€å§‹ï¼Œå¹¶é€æ¸å°†å…¶å‡å°‘åˆ°ä¸€ä¸ªè¾ƒå°çš„å€¼ï¼Œå¦‚5%æˆ–2%çš„éšæœºè¡Œä¸ºã€‚ä½¿ç”¨epsilon-è´ªå©ªæ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬åœ¨è®­ç»ƒåˆæœŸæ¢ç´¢ç¯å¢ƒï¼Œå¹¶åœ¨è®­ç»ƒç»“æŸæ—¶åšæŒå¥½çš„ç­–ç•¥ã€‚è¿˜æœ‰å…¶ä»–è§£å†³æ¢ç´¢ä¸åˆ©ç”¨é—®é¢˜çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ä¹¦çš„ç¬¬ä¸‰éƒ¨åˆ†è®¨è®ºå…¶ä¸­çš„ä¸€äº›ã€‚è¿™ä¸ªé—®é¢˜æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ä¸€ä¸ªåŸºç¡€æ€§æœªè§£é—®é¢˜ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªä»åœ¨ç§¯æç ”ç©¶çš„é¢†åŸŸï¼Œç¦»å®Œå…¨è§£å†³è¿˜è¿œã€‚
- en: SGD optimization
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SGDä¼˜åŒ–
- en: The core of our Q-learning procedure is borrowed from supervised learning. Indeed,
    we are trying to approximate a complex, nonlinear function, Q(s,a), with an NN.
    To do this, we must calculate targets for this function using the Bellman equation
    and then pretend that we have a supervised learning problem at hand. Thatâ€™s okay,
    but one of the fundamental requirements for SGD optimization is that the training
    data is independent and identically distributed (frequently abbreviated as iid),
    which means that our training data is randomly sampled from the underlying dataset
    weâ€™re trying to learn on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„Qå­¦ä¹ è¿‡ç¨‹çš„æ ¸å¿ƒå€Ÿé‰´äº†ç›‘ç£å­¦ä¹ ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬æ­£è¯•å›¾ç”¨ç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ¥é€¼è¿‘ä¸€ä¸ªå¤æ‚çš„éçº¿æ€§å‡½æ•°Q(s,a)ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—è¯¥å‡½æ•°çš„ç›®æ ‡å€¼ï¼Œç„¶åå‡è£…æˆ‘ä»¬é¢ä¸´çš„æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ã€‚è¿™æ˜¯å¯ä»¥çš„ï¼Œä½†SGDä¼˜åŒ–çš„ä¸€ä¸ªåŸºæœ¬è¦æ±‚æ˜¯è®­ç»ƒæ•°æ®æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼ˆé€šå¸¸ç¼©å†™ä¸ºiidï¼‰ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®æ˜¯ä»æˆ‘ä»¬è¯•å›¾å­¦ä¹ çš„åº•å±‚æ•°æ®é›†ä¸­éšæœºæŠ½æ ·çš„ã€‚
- en: 'In our case, data that we are going to use for the SGD update doesnâ€™t fulfill
    these criteria:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç”¨äºSGDæ›´æ–°çš„æ•°æ®ä¸ç¬¦åˆè¿™äº›æ ‡å‡†ï¼š
- en: Our samples are not independent. Even if we accumulate a large batch of data
    samples, they will all be very close to each other, as they will belong to the
    same episode.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ ·æœ¬ä¸æ˜¯ç‹¬ç«‹çš„ã€‚å³ä½¿æˆ‘ä»¬ç´¯ç§¯äº†å¤§é‡çš„æ•°æ®æ ·æœ¬ï¼Œå®ƒä»¬ä¹‹é—´ä¹Ÿä¼šéå¸¸æ¥è¿‘ï¼Œå› ä¸ºå®ƒä»¬éƒ½å±äºåŒä¸€ä¸ªå›åˆã€‚
- en: 'Distribution of our training data wonâ€™t be identical to samples provided by
    the optimal policy that we want to learn. Data that we have will be a result of
    some other policy (our current policy, a random one, or both in the case of epsilon-greedy),
    but we donâ€™t want to learn how to play randomly: we want an optimal policy with
    the best reward.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå°†ä¸å®Œå…¨ç­‰åŒäºæˆ‘ä»¬æƒ³è¦å­¦ä¹ çš„æœ€ä¼˜ç­–ç•¥æä¾›çš„æ ·æœ¬ã€‚æˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®å°†æ˜¯å…¶ä»–ç­–ç•¥ï¼ˆæˆ‘ä»¬çš„å½“å‰ç­–ç•¥ã€éšæœºç­–ç•¥ï¼Œæˆ–åœ¨epsilon-è´ªå©ªæƒ…å†µä¸‹çš„ä¸¤è€…ç»“åˆï¼‰çš„ç»“æœï¼Œä½†æˆ‘ä»¬å¹¶ä¸æƒ³å­¦ä¹ å¦‚ä½•éšæœºåœ°è¡ŒåŠ¨ï¼šæˆ‘ä»¬å¸Œæœ›è·å¾—ä¸€ä¸ªå…·æœ‰æœ€ä½³å¥–åŠ±çš„æœ€ä¼˜ç­–ç•¥ã€‚
- en: To deal with this nuisance, we usually need to use a large buffer of our past
    experience and sample training data from it, instead of using our latest experience.
    This technique is called a replay buffer. The simplest implementation is a buffer
    of a fixed size, with new data added to the end of the buffer so that it pushes
    the oldest experience out of it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åº”å¯¹è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä½¿ç”¨ä¸€ä¸ªåŒ…å«æˆ‘ä»¬è¿‡å»ç»éªŒçš„å¤§ç¼“å†²åŒºï¼Œå¹¶ä»ä¸­æŠ½å–è®­ç»ƒæ•°æ®ï¼Œè€Œä¸æ˜¯ä»…ä½¿ç”¨æˆ‘ä»¬æœ€æ–°çš„ç»éªŒã€‚è¿™ç§æŠ€æœ¯å«åšå›æ”¾ç¼“å†²åŒºã€‚æœ€ç®€å•çš„å®ç°æ–¹å¼æ˜¯ä¸€ä¸ªå›ºå®šå¤§å°çš„ç¼“å†²åŒºï¼Œæ–°æ•°æ®è¢«æ·»åŠ åˆ°ç¼“å†²åŒºçš„æœ«å°¾ï¼Œä»è€Œå°†æœ€æ—§çš„ç»éªŒæ¨å‡ºç¼“å†²åŒºä¹‹å¤–ã€‚
- en: The replay buffer allows us to train on more-or-less independent data, but the
    data will still be fresh enough to train on samples generated by our recent policy.
    In ChapterÂ [8](ch012.xhtml#x1-1240008), we will check another kind of replay buffer,
    prioritized, which provides a more sophisticated sampling approach.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å›æ”¾ç¼“å†²åŒºå…è®¸æˆ‘ä»¬åœ¨æˆ–å¤šæˆ–å°‘ç‹¬ç«‹çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è¿™äº›æ•°æ®ä»ç„¶è¶³å¤Ÿæ–°é²œï¼Œå¯ä»¥ç”¨äºè®­ç»ƒæˆ‘ä»¬æœ€è¿‘çš„ç­–ç•¥ç”Ÿæˆçš„æ ·æœ¬ã€‚åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å¦ä¸€ç§å›æ”¾ç¼“å†²åŒºï¼Œä¼˜å…ˆçº§å›æ”¾ï¼Œå®ƒæä¾›äº†ä¸€ç§æ›´å¤æ‚çš„é‡‡æ ·æ–¹æ³•ã€‚
- en: Correlation between steps
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ä¹‹é—´çš„ç›¸å…³æ€§
- en: Another practical issue with the default training procedure is also related
    to the lack of iid data, but in a slightly different manner. The Bellman equation
    provides us with the value of Q(s,a) via Q(sâ€²,aâ€²) (this process is called bootstrapping,
    when we use the formula recursively). However, both the states s and sâ€² have only
    one step between them. This makes them very similar, and itâ€™s very hard for NNs
    to distinguish between them. When we perform an update of our NNsâ€™ parameters
    to make Q(s,a) closer to the desired result, we can indirectly alter the value
    produced for Q(sâ€²,aâ€²) and other states nearby. This can make our training very
    unstable, like chasing our own tail; when we update Q for state s, then on subsequent
    states, we will discover that Q(sâ€²,aâ€²) becomes worse but attempts to update it
    can spoil our Q(s,a) approximation even more, and so on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤è®­ç»ƒè¿‡ç¨‹çš„å¦ä¸€ä¸ªå®é™…é—®é¢˜ä¹Ÿä¸ç¼ºä¹ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰æ•°æ®æœ‰å…³ï¼Œä½†æ–¹å¼ç¨æœ‰ä¸åŒã€‚è´å°”æ›¼æ–¹ç¨‹é€šè¿‡Q(sâ€²,aâ€²)ä¸ºæˆ‘ä»¬æä¾›Q(s,a)çš„å€¼ï¼ˆè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºè‡ªä¸¾ï¼Œå½“æˆ‘ä»¬é€’å½’ä½¿ç”¨è¯¥å…¬å¼æ—¶ï¼‰ã€‚ç„¶è€Œï¼ŒçŠ¶æ€så’Œsâ€²ä¹‹é—´åªæœ‰ä¸€æ­¥ä¹‹é¥ã€‚è¿™ä½¿å¾—å®ƒä»¬éå¸¸ç›¸ä¼¼ï¼Œç¥ç»ç½‘ç»œå¾ˆéš¾åŒºåˆ†å®ƒä»¬ã€‚å½“æˆ‘ä»¬æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œä½¿Q(s,a)æ›´æ¥è¿‘é¢„æœŸç»“æœæ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé—´æ¥åœ°æ”¹å˜Q(sâ€²,aâ€²)å’Œé™„è¿‘å…¶ä»–çŠ¶æ€çš„å€¼ã€‚è¿™å¯èƒ½å¯¼è‡´æˆ‘ä»¬çš„è®­ç»ƒéå¸¸ä¸ç¨³å®šï¼Œå°±åƒåœ¨è¿½é€è‡ªå·±çš„å°¾å·´ï¼›å½“æˆ‘ä»¬æ›´æ–°çŠ¶æ€sçš„Qå€¼æ—¶ï¼Œåœ¨éšåçš„çŠ¶æ€ä¸­ï¼Œæˆ‘ä»¬ä¼šå‘ç°Q(sâ€²,aâ€²)å˜å¾—æ›´ç³Ÿï¼Œä½†å°è¯•æ›´æ–°å®ƒå¯èƒ½ä¼šè¿›ä¸€æ­¥ç ´åQ(s,a)çš„è¿‘ä¼¼ï¼Œä¾æ­¤ç±»æ¨ã€‚
- en: To make training more stable, there is a trick, called target network, by which
    we keep a copy of our network and use it for the Q(sâ€²,aâ€²) value in the Bellman
    equation. This network is synchronized with our main network only periodically,
    for example, once in N steps (where N is usually quite a large hyperparameter,
    such as 1k or 10k training iterations).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è®­ç»ƒæ›´åŠ ç¨³å®šï¼Œæœ‰ä¸€ä¸ªæŠ€å·§å«åšç›®æ ‡ç½‘ç»œï¼Œæˆ‘ä»¬ä¿ç•™ä¸€ä»½ç½‘ç»œçš„å‰¯æœ¬ï¼Œå¹¶ç”¨å®ƒæ¥è®¡ç®—è´å°”æ›¼æ–¹ç¨‹ä¸­çš„Q(sâ€²,aâ€²)å€¼ã€‚è¿™ä¸ªç½‘ç»œä¸æˆ‘ä»¬çš„ä¸»ç½‘ç»œåªä¼šå®šæœŸåŒæ­¥ï¼Œä¾‹å¦‚ï¼Œæ¯éš”Næ­¥åŒæ­¥ä¸€æ¬¡ï¼ˆå…¶ä¸­Né€šå¸¸æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„è¶…å‚æ•°ï¼Œå¦‚1kæˆ–10kè®­ç»ƒè¿­ä»£ï¼‰ã€‚
- en: The Markov property
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«æ€§è´¨
- en: 'Our RL methods use Markov decision process (MDP) formalism as their basis,
    which assumes that the environment obeys the Markov property: observations from
    the environment are all that we need to act optimally. In other words, our observations
    allow us to distinguish states from one another.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å½¢å¼ä¸»ä¹‰ä¸ºåŸºç¡€ï¼Œå‡è®¾ç¯å¢ƒéµå¾ªé©¬å°”å¯å¤«æ€§è´¨ï¼šæ¥è‡ªç¯å¢ƒçš„è§‚å¯Ÿæ˜¯æˆ‘ä»¬é‡‡å–æœ€ä¼˜è¡ŒåŠ¨æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬çš„è§‚å¯Ÿå…è®¸æˆ‘ä»¬åŒºåˆ†ä¸åŒçš„çŠ¶æ€ã€‚
- en: As you saw from the preceding Pong screenshot in FigureÂ [6.2](#x1-93003r2),
    one single image from the Atari game is not enough to capture all the important
    information (using only one image, we have no idea about the speed and direction
    of objects, like the ball and our opponentâ€™s paddle). This obviously violates
    the Markov property and moves our single-frame Pong environment into the area
    of partially observable MDPs (POMDPs). A POMDP is basically an MDP without the
    Markov property, and it is very important in practice. For example, for most card
    games in which you donâ€™t see your opponentsâ€™ cards, game observations are POMDPs
    because the current observation (i.e., your cards and the cards on the table)
    could correspond to different cards in your opponentsâ€™ hands.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ ä»å‰é¢çš„Pongæ¸¸æˆæˆªå›¾ï¼ˆå›¾[6.2](#x1-93003r2)ï¼‰æ‰€è§ï¼Œä¸€å¼ æ¥è‡ªAtariæ¸¸æˆçš„å•ä¸€å›¾åƒä¸è¶³ä»¥æ•è·æ‰€æœ‰é‡è¦ä¿¡æ¯ï¼ˆä»…ä½¿ç”¨ä¸€å¼ å›¾åƒï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“ç‰©ä½“çš„é€Ÿåº¦å’Œæ–¹å‘ï¼Œæ¯”å¦‚çƒå’Œæˆ‘ä»¬å¯¹æ‰‹çš„æŒ¡æ¿ï¼‰ã€‚è¿™æ˜¾ç„¶è¿åäº†é©¬å°”å¯å¤«æ€§è´¨ï¼Œå¹¶å°†æˆ‘ä»¬çš„å•å¸§Pongç¯å¢ƒç§»å…¥éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰çš„èŒƒç•´ã€‚POMDPåŸºæœ¬ä¸Šæ˜¯æ²¡æœ‰é©¬å°”å¯å¤«æ€§è´¨çš„MDPï¼Œå®ƒåœ¨å®é™…åº”ç”¨ä¸­éå¸¸é‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å¤šæ•°æ‰‘å…‹ç‰Œæ¸¸æˆä¸­ï¼Œä½ æ— æ³•çœ‹åˆ°å¯¹æ‰‹çš„ç‰Œï¼Œè¿™äº›æ¸¸æˆè§‚å¯Ÿå°±æ˜¯POMDPï¼Œå› ä¸ºå½“å‰çš„è§‚å¯Ÿï¼ˆå³ä½ æ‰‹ä¸­çš„ç‰Œå’Œæ¡Œé¢ä¸Šçš„ç‰Œï¼‰å¯èƒ½å¯¹åº”å¯¹æ‰‹æ‰‹ä¸­çš„ä¸åŒç‰Œã€‚
- en: We wonâ€™t discuss POMDPs in detail in this book, but we will use a small technique
    to push our environment back into the MDP domain. The solution is maintaining
    several observations from the past and using them as a state. In the case of Atari
    games, we usually stack k subsequent frames together and use them as the observation
    at every state. This allows our agent to deduct the dynamics of the current state,
    for instance, to get the speed of the ball and its direction. The usual â€œclassicalâ€
    number of k for Atari is four. Of course, itâ€™s just a hack, as there can be longer
    dependencies in the environment, but for most of the games, it works well.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦ä¸­æˆ‘ä»¬ä¸ä¼šè¯¦ç»†è®¨è®ºéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰ï¼Œä½†æˆ‘ä»¬ä¼šä½¿ç”¨ä¸€ä¸ªå°æŠ€å·§å°†æˆ‘ä»¬çš„ç¯å¢ƒæ¨å›åˆ°MDPé¢†åŸŸã€‚è§£å†³æ–¹æ¡ˆæ˜¯ç»´æŠ¤è¿‡å»çš„å¤šä¸ªè§‚å¯Ÿï¼Œå¹¶å°†å®ƒä»¬ç”¨ä½œçŠ¶æ€ã€‚åœ¨Atariæ¸¸æˆçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸å°†kä¸ªè¿ç»­çš„å¸§å †å åœ¨ä¸€èµ·ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºæ¯ä¸ªçŠ¶æ€çš„è§‚å¯Ÿã€‚è¿™è®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿæ¨æ–­å‡ºå½“å‰çŠ¶æ€çš„åŠ¨æ€ï¼Œä¾‹å¦‚ï¼Œè·å–çƒçš„é€Ÿåº¦å’Œæ–¹å‘ã€‚å¯¹äºAtariæ¸¸æˆï¼Œé€šå¸¸çš„â€œç»å…¸â€kå€¼æ˜¯å››ã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯ä¸€ä¸ªæŠ€å·§ï¼Œå› ä¸ºç¯å¢ƒä¸­å¯èƒ½å­˜åœ¨æ›´é•¿çš„ä¾èµ–å…³ç³»ï¼Œä½†å¯¹äºå¤§å¤šæ•°æ¸¸æˆæ¥è¯´ï¼Œå®ƒè¡¨ç°å¾—å¾ˆå¥½ã€‚
- en: The final form of DQN training
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQNè®­ç»ƒçš„æœ€ç»ˆå½¢å¼
- en: There are many more tips and tricks that researchers have discovered to make
    DQN training more stable and efficient, and we will cover the best of them in
    ChapterÂ [8](ch012.xhtml#x1-1240008). However, epsilon-greedy, the replay buffer,
    and the target network form a basis that has allowed the company DeepMind to successfully
    train a DQN on a set of 49 Atari games, demonstrating the efficiency of this approach
    when applied to complicated environments.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶äººå‘˜å·²ç»å‘ç°äº†è®¸å¤šæŠ€å·§ï¼Œä½¿å¾— DQN è®­ç»ƒæ›´åŠ ç¨³å®šå’Œé«˜æ•ˆï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä»‹ç»å…¶ä¸­æœ€å¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œepsilon-greedy
    ç­–ç•¥ã€å›æ”¾ç¼“å†²åŒºå’Œç›®æ ‡ç½‘ç»œæ„æˆäº†åŸºç¡€ï¼Œä½¿å¾— DeepMind å…¬å¸èƒ½å¤ŸæˆåŠŸåœ°åœ¨ 49 æ¬¾ Atari æ¸¸æˆä¸Šè®­ç»ƒ DQNï¼Œå±•ç¤ºäº†è¿™ç§æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ•ˆç‡ã€‚
- en: The original paper Playing Atari with deep reinforcement learning [[Mni13](#)]
    (without a target network) was published at the end of 2013 and used seven games
    for testing. Later, at the beginning of 2015, a revised version of the article
    with the title Human-level control through deep reinforcement learning [[Mni+15](#)],
    already with 49 different games, was published in Nature.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹è®ºæ–‡ã€ŠPlaying Atari with deep reinforcement learningã€‹[[Mni13](#)]ï¼ˆæ²¡æœ‰ç›®æ ‡ç½‘ç»œï¼‰å‘å¸ƒäº
    2013 å¹´åº•ï¼Œæµ‹è¯•ä½¿ç”¨äº†ä¸ƒæ¬¾æ¸¸æˆã€‚åæ¥ï¼Œåœ¨ 2015 å¹´åˆï¼Œæ–‡ç« ç»è¿‡ä¿®è®¢ï¼Œæ ‡é¢˜ä¸ºã€ŠHuman-level control through deep reinforcement
    learningã€‹[[Mni+15](#)]ï¼Œæ­¤æ—¶å·²ä½¿ç”¨äº† 49 æ¬¾ä¸åŒçš„æ¸¸æˆï¼Œå¹¶å‘è¡¨äºã€Šè‡ªç„¶ã€‹æ‚å¿—ã€‚
- en: 'The algorithm for DQN from the preceding papers has the following steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå‰è¿°è®ºæ–‡çš„ DQN ç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼š
- en: Initialize parameters for Q(s,a) and QÌ‚(s,a) with random weights, ğœ– â† 1.0, and
    empty the replay buffer.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºæƒé‡åˆå§‹åŒ– Q(s,a) å’Œ QÌ‚(s,a) çš„å‚æ•°ï¼Œğœ– â† 1.0ï¼Œå¹¶æ¸…ç©ºå›æ”¾ç¼“å†²åŒºã€‚
- en: With probability ğœ–, select a random action a; otherwise, a = arg max[a]Q(s,a).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡ ğœ– é€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œ aï¼›å¦åˆ™ï¼Œa = arg max[a]Q(s,a)ã€‚
- en: Execute action a in an emulator and observe the reward, r, and the next state,
    sâ€².
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡æ‹Ÿå™¨ä¸­æ‰§è¡ŒåŠ¨ä½œ aï¼Œå¹¶è§‚å¯Ÿå¥–åŠ± r å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ sâ€²ã€‚
- en: Store the transition (s, a, r, sâ€²) in the replay buffer.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è¿‡æ¸¡ (s, a, r, sâ€²) å­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒºä¸­ã€‚
- en: Sample a random mini-batch of transitions from the replay buffer.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»å›æ”¾ç¼“å†²åŒºä¸­éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡çš„è¿‡æ¸¡ã€‚
- en: 'For every transition in the buffer, calculate the target:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºç¼“å†²åŒºä¸­çš„æ¯ä¸ªè¿‡æ¸¡ï¼Œè®¡ç®—ç›®æ ‡ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq22.png) ![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq23.png)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq22.png) ![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq23.png)'
- en: 'Calculate the loss: â„’ = (Q(s,a) âˆ’y)Â².'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±ï¼šâ„’ = (Q(s,a) âˆ’y)Â²ã€‚
- en: Update Q(s,a) using the SGD algorithm by minimizing the loss in respect to the
    model parameters.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡æœ€å°åŒ–æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°ï¼Œä½¿ç”¨ SGD ç®—æ³•æ›´æ–° Q(s,a)ã€‚
- en: Every N steps, copy weights from Q to QÌ‚.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯éš” N æ­¥ï¼Œä» Q å¤åˆ¶æƒé‡åˆ° QÌ‚ã€‚
- en: Repeat from step 2 until converged.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚
- en: Letâ€™s implement this algorithm now and try to beat some of the Atari games!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å®ç°è¿™ä¸ªç®—æ³•ï¼Œå¹¶å°è¯•å‡»è´¥ä¸€äº› Atari æ¸¸æˆï¼
- en: DQN on Pong
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN åœ¨ Pong æ¸¸æˆä¸­çš„åº”ç”¨
- en: Before we jump into the code, some introduction is needed. Our examples are
    becoming increasingly challenging and complex, which is not surprising, as the
    complexity of the problems that we are trying to tackle is also growing. The examples
    are as simple and concise as possible, but some of the code may be difficult to
    understand at first.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹ä»£ç ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œä¸€äº›ä»‹ç»ã€‚æˆ‘ä»¬çš„ç¤ºä¾‹å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¤æ‚æ€§ï¼Œè¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜çš„å¤æ‚æ€§ä¹Ÿåœ¨å¢åŠ ã€‚å°½ç®¡ä¾‹å­å°½å¯èƒ½ç®€å•ç®€æ´ï¼Œä½†æœ‰äº›ä»£ç åˆçœ‹å¯èƒ½éš¾ä»¥ç†è§£ã€‚
- en: Another thing to note is performance. Our previous examples for FrozenLake,
    or CartPole, were not demanding from a resource perspective, as observations were
    small, NN parameters were tiny, and shaving off extra milliseconds in the training
    loop wasnâ€™t important. However, from now on, thatâ€™s not the case. One single observation
    from the Atari environment is 100k values, which have to be preprocessed, rescaled,
    and stored in the replay buffer. One extra copy of this data array can cost you
    training speed, which will not be seconds and minutes anymore but, instead, hours
    on even the fastest graphics processing unit (GPU) available.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªéœ€è¦æ³¨æ„çš„äº‹é¡¹æ˜¯æ€§èƒ½ã€‚æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ï¼ˆä¾‹å¦‚ FrozenLake æˆ– CartPoleï¼‰ä»èµ„æºè§’åº¦æ¥çœ‹å¹¶ä¸è‹›åˆ»ï¼Œå› ä¸ºè§‚å¯Ÿå€¼è¾ƒå°ï¼Œç¥ç»ç½‘ç»œå‚æ•°ä¹Ÿå¾ˆå°ï¼Œè®­ç»ƒå¾ªç¯ä¸­çš„é¢å¤–æ¯«ç§’å¹¶ä¸é‡è¦ã€‚ç„¶è€Œï¼Œä»ç°åœ¨å¼€å§‹ï¼Œæƒ…å†µå°±ä¸åŒäº†ã€‚æ¥è‡ª
    Atari ç¯å¢ƒçš„æ¯ä¸ªè§‚å¯Ÿå€¼æœ‰ 10 ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œè¿™äº›æ•°æ®éœ€è¦é¢„å¤„ç†ã€é‡æ–°ç¼©æ”¾å¹¶å­˜å‚¨åœ¨å›æ”¾ç¼“å†²åŒºä¸­ã€‚å¤šä¸€ä»½æ•°æ®å‰¯æœ¬å¯èƒ½ä¼šå½±å“è®­ç»ƒé€Ÿåº¦ï¼Œè¿™ä¸å†æ˜¯ç§’å’Œåˆ†é’Ÿçš„é—®é¢˜ï¼Œè€Œæ˜¯å³ä½¿æ˜¯æœ€å¿«çš„å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ä¹Ÿå¯èƒ½éœ€è¦æ•°å°æ—¶ã€‚
- en: The NN training loop could also be a bottleneck. Of course, RL models are not
    as huge monsters as state-of-the-art large language models (LLMs), but even the
    DQN model from 2015 has more than 1.5M parameters, which has to be adjusted millions
    of times. So, to cut a long story short, performance matters, especially when
    you are experimenting with hyperparameters and need to wait not for a single model
    to train but dozens of them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œï¼ˆNNï¼‰è®­ç»ƒå¾ªç¯ä¹Ÿå¯èƒ½æˆä¸ºç“¶é¢ˆã€‚å½“ç„¶ï¼Œå¼ºåŒ–å­¦ä¹ æ¨¡å‹å¹¶ä¸åƒæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‚£æ ·åºå¤§ï¼Œä½†å³ä¾¿æ˜¯2015å¹´çš„DQNæ¨¡å‹ä¹Ÿæœ‰è¶…è¿‡150ä¸‡ä¸ªå‚æ•°ï¼Œéœ€è¦è°ƒæ•´æ•°ç™¾ä¸‡æ¬¡ã€‚å› æ­¤ï¼Œç®€è€Œè¨€ä¹‹ï¼Œæ€§èƒ½éå¸¸é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ä½ è¿›è¡Œè¶…å‚æ•°å®éªŒæ—¶ï¼Œä¸ä»…éœ€è¦ç­‰å¾…ä¸€ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€Œæ˜¯å‡ åä¸ªæ¨¡å‹ã€‚
- en: PyTorch is quite expressive, so more-or-less efficient processing code could
    look much less cryptic than optimized TensorFlow graphs, but there is still a
    significant opportunity to do things slowly and make mistakes. For example, a
    naÃ¯ve version of DQN loss computation, which loops over every batch sample, is
    about two times slower than a parallel version. However, a single extra copy of
    the data batch could make the speed of the same code 13 times slower, which is
    quite significant.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchç›¸å½“å…·æœ‰è¡¨ç°åŠ›ï¼Œå› æ­¤æ•ˆç‡è¾ƒé«˜çš„å¤„ç†ä»£ç çœ‹èµ·æ¥é€šå¸¸ä¸å¦‚ä¼˜åŒ–è¿‡çš„TensorFlowå›¾é‚£ä¹ˆæ™¦æ¶©ï¼Œä½†ä»ç„¶å­˜åœ¨å¾ˆå¤§æœºä¼šåšå¾—å¾ˆæ…¢å¹¶çŠ¯é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•ç‰ˆçš„DQNæŸå¤±è®¡ç®—ï¼Œå®ƒå¯¹æ¯ä¸ªæ‰¹æ¬¡æ ·æœ¬è¿›è¡Œå¾ªç¯å¤„ç†ï¼Œæ¯”å¹¶è¡Œç‰ˆæœ¬æ…¢å¤§çº¦ä¸¤å€ã€‚ç„¶è€Œï¼Œä»…ä»…æ˜¯å¯¹æ•°æ®æ‰¹æ¬¡åšä¸€ä¸ªé¢å¤–çš„å‰¯æœ¬ï¼Œå°±ä¼šä½¿å¾—ç›¸åŒä»£ç çš„é€Ÿåº¦å˜æ…¢13å€ï¼Œè¿™éå¸¸æ˜¾è‘—ã€‚
- en: 'This example has been split into three modules due to its length, logical structure,
    and reusability. The modules are as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¶é•¿åº¦ã€é€»è¾‘ç»“æ„å’Œå¯é‡ç”¨æ€§ï¼Œè¯¥ç¤ºä¾‹è¢«æ‹†åˆ†ä¸ºä¸‰ä¸ªæ¨¡å—ã€‚æ¨¡å—å¦‚ä¸‹ï¼š
- en: 'Chapter06/lib/wrappers.py: These are Atari environment wrappers, mostly taken
    from the Stable Baselines3 (SB3) project: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapter06/lib/wrappers.pyï¼šè¿™äº›æ˜¯Atariç¯å¢ƒçš„åŒ…è£…å™¨ï¼Œä¸»è¦æ¥è‡ªStable Baselines3ï¼ˆSB3ï¼‰é¡¹ç›®ï¼š[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)ã€‚
- en: 'Chapter06/lib/dqn_model.py: This is the DQN NN layer, with the same architecture
    as the DeepMind DQN from the Nature paper.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapter06/lib/dqn_model.pyï¼šè¿™æ˜¯DQNç¥ç»ç½‘ç»œå±‚ï¼Œå…¶æ¶æ„ä¸DeepMindåœ¨ã€ŠNatureã€‹è®ºæ–‡ä¸­çš„DQNç›¸åŒã€‚
- en: 'Chapter06/02_dqn_pong.py: This is the main module, with the training loop,
    loss function calculation, and experience replay buffer.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapter06/02_dqn_pong.pyï¼šè¿™æ˜¯ä¸»è¦æ¨¡å—ï¼ŒåŒ…å«è®­ç»ƒå¾ªç¯ã€æŸå¤±å‡½æ•°è®¡ç®—å’Œç»éªŒå›æ”¾ç¼“å†²åŒºã€‚
- en: Wrappers
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŒ…è£…å™¨
- en: Tackling Atari games with RL is quite demanding from a resource perspective.
    To make things faster, several transformations are applied to the Atari platform
    interaction, which are described in DeepMindâ€™s paper. Some of these transformations
    influence only performance, but some address Atari platform features that make
    learning long and unstable. Transformations are implemented as Gym wrappers of
    various kinds. The full list is quite lengthy and there are several implementations
    of the same wrappers in various sources. My personal favorite is the SB3 repository,
    which is an evolution of OpenAI Baselines code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§£å†³Atariæ¸¸æˆåœ¨èµ„æºæ–¹é¢æ˜¯ç›¸å½“æœ‰æŒ‘æˆ˜çš„ã€‚ä¸ºäº†åŠ å¿«é€Ÿåº¦ï¼Œé’ˆå¯¹Atariå¹³å°çš„äº¤äº’åº”ç”¨äº†å‡ ç§è½¬æ¢ï¼Œè¿™äº›è½¬æ¢åœ¨DeepMindçš„è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚éƒ¨åˆ†è½¬æ¢ä»…å½±å“æ€§èƒ½ï¼Œè€Œæœ‰äº›åˆ™æ˜¯è§£å†³Atariå¹³å°çš„ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§ä½¿å¾—å­¦ä¹ è¿‡ç¨‹æ—¢æ¼«é•¿åˆä¸ç¨³å®šã€‚è½¬æ¢é€šè¿‡ä¸åŒç§ç±»çš„GymåŒ…è£…å™¨æ¥å®ç°ã€‚å®Œæ•´çš„åˆ—è¡¨ç›¸å½“é•¿ï¼Œå¹¶ä¸”åŒä¸€ä¸ªåŒ…è£…å™¨æœ‰å¤šä¸ªå®ç°ç‰ˆæœ¬æ¥è‡ªä¸åŒæ¥æºã€‚æˆ‘çš„ä¸ªäººåå¥½æ˜¯SB3ä»“åº“ï¼Œå®ƒæ˜¯OpenAI
    Baselinesä»£ç çš„æ¼”å˜ç‰ˆæœ¬ã€‚
- en: 'SB3 includes lots of RL methods implemented using PyTorch and is supposed to
    be a unifying benchmark to compare various methods. At the moment, weâ€™re not interested
    in those methodsâ€™ implementation (weâ€™re going to reimplement most of them ourselves),
    but some wrappers are very useful. The repository is avaliable at [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)
    and wrappers are documented at [https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml](https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml).
    The list of the most popular Atari transformations used by RL researchers includes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SB3åŒ…å«å¤§é‡ä½¿ç”¨PyTorchå®ç°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œæ¯”è¾ƒå„ç§æ–¹æ³•ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•çš„å®ç°ä¸æ„Ÿå…´è¶£ï¼ˆæˆ‘ä»¬æ‰“ç®—è‡ªå·±é‡æ–°å®ç°å¤§å¤šæ•°æ–¹æ³•ï¼‰ï¼Œä½†ä¸€äº›åŒ…è£…å™¨éå¸¸æœ‰ç”¨ã€‚è¯¥ä»“åº“å¯ä»¥åœ¨[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)æ‰¾åˆ°ï¼ŒåŒ…è£…å™¨çš„æ–‡æ¡£å¯ä»¥åœ¨[https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml](https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml)æŸ¥çœ‹ã€‚å¼ºåŒ–å­¦ä¹ ç ”ç©¶äººå‘˜å¸¸ç”¨çš„Atariè½¬æ¢åˆ—è¡¨åŒ…æ‹¬ï¼š
- en: 'Converting individual lives in the game into separate episodes: In general,
    an episode contains all the steps from the beginning of the game until the â€œGame
    overâ€ screen, which can last for thousands of game steps (observations and actions).
    Usually, in arcade games, the player is given several lives, which provide several
    attempts in the game. This transformation splits a full episode into individual
    small episodes for every life that a player has. Internally, this is implemented
    as checking an emulatorâ€™s information about remaining lives. Not all games support
    this feature (although Pong does), but for the supported environments, it usually
    helps to speed up convergence, as our episodes become shorter. This logic is implemented
    in the EpisodicLifeEnv wrapper in SB3 code.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ¸¸æˆä¸­çš„æ¯ä¸ªç”Ÿå‘½è½¬åŒ–ä¸ºå•ç‹¬çš„å›åˆï¼šä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ªå›åˆåŒ…å«ä»æ¸¸æˆå¼€å§‹åˆ°â€œæ¸¸æˆç»“æŸâ€ç”»é¢æ‰€æœ‰æ­¥éª¤ï¼Œè¿™å¯èƒ½ä¼šæŒç»­æ•°åƒä¸ªæ¸¸æˆæ­¥éª¤ï¼ˆè§‚å¯Ÿå’ŒåŠ¨ä½œï¼‰ã€‚é€šå¸¸ï¼Œåœ¨è¡—æœºæ¸¸æˆä¸­ï¼Œç©å®¶ä¼šè·å¾—å‡ æ¡å‘½ï¼Œè¿™æä¾›äº†å‡ æ¬¡æ¸¸æˆå°è¯•ã€‚è¿™ç§è½¬æ¢å°†ä¸€ä¸ªå®Œæ•´å›åˆæ‹†åˆ†ä¸ºæ¯æ¡å‘½å¯¹åº”çš„å•ç‹¬å°å›åˆã€‚åœ¨å†…éƒ¨ï¼Œè¿™æ˜¯é€šè¿‡æ£€æŸ¥æ¨¡æ‹Ÿå™¨å…³äºå‰©ä½™ç”Ÿå‘½çš„ä¿¡æ¯æ¥å®ç°çš„ã€‚å¹¶éæ‰€æœ‰æ¸¸æˆéƒ½æ”¯æŒæ­¤åŠŸèƒ½ï¼ˆå°½ç®¡ä¹’ä¹“çƒæ¸¸æˆæ”¯æŒï¼‰ï¼Œä½†å¯¹äºæ”¯æŒçš„ç¯å¢ƒï¼Œè¿™é€šå¸¸æœ‰åŠ©äºåŠ é€Ÿæ”¶æ•›ï¼Œå› ä¸ºæˆ‘ä»¬çš„å›åˆå˜å¾—æ›´çŸ­ã€‚æ­¤é€»è¾‘åœ¨SB3ä»£ç ä¸­çš„EpisodicLifeEnvåŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚
- en: 'At the beginning of the game, performing a random amount (up to 30) of empty
    actions (also called â€œno-opâ€): This skips intro screens in some Atari games, which
    are not relevant for the gameplay. It is implemented in the NoopResetEnv wrapper.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ¸¸æˆå¼€å§‹æ—¶æ‰§è¡Œä¸€ä¸ªéšæœºæ•°é‡ï¼ˆæœ€å¤š30æ¬¡ï¼‰çš„ç©ºæ“ä½œï¼ˆä¹Ÿç§°ä¸ºâ€œæ— æ“ä½œâ€ï¼‰ï¼šè¿™è·³è¿‡äº†ä¸€äº›é›…è¾¾åˆ©æ¸¸æˆä¸­çš„ä»‹ç»ç”»é¢ï¼Œè¿™äº›ç”»é¢ä¸æ¸¸æˆç©æ³•æ— å…³ã€‚å®ƒåœ¨NoopResetEnvåŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚
- en: 'Making an action decision every K steps, where K is usually 3 or 4: On intermediate
    frames, the chosen action is simply repeated. This allows training to speed up
    significantly, as processing every frame with an NN is quite a demanding operation,
    but the difference between consequent frames is usually minor. This is implemented
    in the MaxAndSkipEnv wrapper, which also includes the next transformation in the
    list (the maximum between two frames).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯Kæ­¥åšä¸€æ¬¡åŠ¨ä½œå†³ç­–ï¼Œå…¶ä¸­Ké€šå¸¸æ˜¯3æˆ–4ï¼šåœ¨ä¸­é—´å¸§ä¸Šï¼Œæ‰€é€‰çš„åŠ¨ä½œä¼šè¢«ç®€å•åœ°é‡å¤ã€‚è¿™ä½¿å¾—è®­ç»ƒèƒ½å¤Ÿæ˜¾è‘—åŠ é€Ÿï¼Œå› ä¸ºä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†æ¯ä¸€å¸§æ˜¯ä¸€ä¸ªéå¸¸è´¹æ—¶çš„æ“ä½œï¼Œä½†è¿ç»­å¸§ä¹‹é—´çš„å·®å¼‚é€šå¸¸è¾ƒå°ã€‚è¿™åœ¨MaxAndSkipEnvåŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ï¼Œè¯¥ç±»ä¹ŸåŒ…å«åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ä¸ªè½¬æ¢ï¼ˆä¸¤å¸§ä¹‹é—´çš„æœ€å¤§å€¼ï¼‰ã€‚
- en: 'Taking the maximum of every pixel in the last two frames and using it as an
    observation: Some Atari games have a flickering effect, which is due to the platformâ€™s
    limitation. (Atari has a limited number of sprites that can be shown on a single
    frame.) For the human eye, such quick changes are not visible, but they can confuse
    NNs.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å–æ¯ä¸ªåƒç´ åœ¨æœ€åä¸¤å¸§ä¸­çš„æœ€å¤§å€¼å¹¶ä½œä¸ºè§‚å¯Ÿå€¼ï¼šä¸€äº›é›…è¾¾åˆ©æ¸¸æˆå­˜åœ¨é—ªçƒæ•ˆæœï¼Œè¿™æ˜¯ç”±äºå¹³å°çš„é™åˆ¶ã€‚ï¼ˆé›…è¾¾åˆ©æ¯å¸§ä¸Šå¯ä»¥æ˜¾ç¤ºçš„ç²¾çµæ•°é‡æ˜¯æœ‰é™çš„ã€‚ï¼‰å¯¹äºäººçœ¼æ¥è¯´ï¼Œè¿™ç§å¿«é€Ÿå˜åŒ–æ˜¯ä¸å¯è§çš„ï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šå¹²æ‰°ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ã€‚
- en: 'Pressing FIRE at the beginning of the game: Some games (including Pong and
    Breakout) require a user to press the FIRE button to start the game. Without this,
    the environment becomes a POMDP, as from observation, an agent cannot tell whether
    FIRE was already pressed. This is implemented in the FireResetEnv wrapper class.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ¸¸æˆå¼€å§‹æ—¶æŒ‰ä¸‹FIREé”®ï¼šæŸäº›æ¸¸æˆï¼ˆåŒ…æ‹¬ä¹’ä¹“çƒå’Œæ‰“ç –å—ï¼‰éœ€è¦ç”¨æˆ·æŒ‰ä¸‹FIREæŒ‰é’®æ‰èƒ½å¼€å§‹æ¸¸æˆã€‚å¦‚æœæ²¡æœ‰æŒ‰ä¸‹è¯¥æŒ‰é’®ï¼Œç¯å¢ƒå°†å˜ä¸ºéƒ¨åˆ†å¯è§‚æµ‹çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œå› ä¸ºä»è§‚å¯Ÿä¸­ï¼Œä»£ç†æ— æ³•åˆ¤æ–­æ˜¯å¦å·²ç»æŒ‰ä¸‹äº†FIREé”®ã€‚è¿™åœ¨FireResetEnvåŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚
- en: 'Scaling every frame down from 210 Ã— 160, with three color frames, to a single-color
    84 Ã— 84 image: Different approaches are possible. For example, the DeepMind paper
    describes this transformation as taking the Y-color channel from the YCbCr color
    space and then rescaling the full image to an 84 Ã— 84 resolution. Some other researchers
    do grayscale transformation, cropping non-relevant parts of an image and then
    scaling down. In the SB3 repository, the latter approach is used. This is implemented
    in the WarpFrame wrapper class.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ¯å¸§ä»210 Ã— 160çš„ä¸‰è‰²å›¾åƒç¼©æ”¾ä¸ºå•è‰²çš„84 Ã— 84å›¾åƒï¼šæœ‰ä¸åŒçš„æ–¹æ³•å¯ä»¥å®ç°ã€‚ä¾‹å¦‚ï¼ŒDeepMindçš„è®ºæ–‡å°†æ­¤è½¬æ¢æè¿°ä¸ºä»YCbCrè‰²å½©ç©ºé—´ä¸­æå–Yè‰²é€šé“ï¼Œç„¶åå°†æ•´ä¸ªå›¾åƒé‡æ–°ç¼©æ”¾ä¸º84
    Ã— 84çš„åˆ†è¾¨ç‡ã€‚å…¶ä»–ä¸€äº›ç ”ç©¶äººå‘˜è¿›è¡Œç°åº¦è½¬æ¢ï¼Œè£å‰ªæ‰å›¾åƒä¸­ä¸ç›¸å…³çš„éƒ¨åˆ†ç„¶åè¿›è¡Œç¼©æ”¾ã€‚åœ¨SB3çš„ä»£ç åº“ä¸­ï¼Œä½¿ç”¨äº†åä¸€ç§æ–¹æ³•ã€‚è¿™åœ¨WarpFrameåŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚
- en: 'Stacking several (usually four) subsequent frames together to give the network
    information about the dynamics of the gameâ€™s objects: This approach was already
    discussed as a quick solution to the lack of game dynamics in a single game frame.
    There is no wrapper in the SB3 project, I implemented my version in wrappers.BufferWrapper.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å¤šä¸ªï¼ˆé€šå¸¸æ˜¯å››ä¸ªï¼‰è¿ç»­çš„å¸§å †å åœ¨ä¸€èµ·ï¼Œä»¥å‘ç½‘ç»œæä¾›æ¸¸æˆä¸­ç‰©ä½“åŠ¨æ€çš„ä¿¡æ¯ï¼šè¿™ç§æ–¹æ³•å·²ç»ä½œä¸ºè§£å†³å•ä¸€æ¸¸æˆå¸§ç¼ºä¹æ¸¸æˆåŠ¨æ€çš„å¿«é€Ÿæ–¹æ¡ˆè¿›è¡Œäº†è®¨è®ºã€‚åœ¨SB3é¡¹ç›®ä¸­æ²¡æœ‰ç°æˆçš„åŒ…è£…ç±»ï¼Œæˆ‘åœ¨wrappers.BufferWrapperä¸­å®ç°äº†æˆ‘çš„ç‰ˆæœ¬ã€‚
- en: 'Clipping the reward to -1, 0, and 1 values: The obtained score can vary wildly
    among the games. For example, in Pong, you get a score of 1 for every ball you
    pass behind the opponentâ€™s paddle. However, in some games, like KungFuMaster,
    you get a reward of 100 for every enemy killed. This spread in reward values makes
    our loss have completely different scales between the games, which makes it harder
    to find common hyperparameters for a set of games. To fix this, the reward just
    gets clipped to the range âˆ’1â€¦1\. This is implemented in the ClipRewardEnv wrapper.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å¥–åŠ±è£å‰ªåˆ° -1ã€0 å’Œ 1 çš„å€¼ï¼šè·å¾—çš„åˆ†æ•°åœ¨ä¸åŒæ¸¸æˆä¹‹é—´å¯èƒ½å·®å¼‚å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œåœ¨ Pong æ¸¸æˆä¸­ï¼Œæ¯å½“ä½ å°†çƒæ‰“è¿‡å¯¹æ–¹çš„æŒ¡æ¿æ—¶ï¼Œä½ ä¼šè·å¾— 1 åˆ†ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æ¸¸æˆä¸­ï¼Œå¦‚
    KungFuMasterï¼Œæ¯æ€æ­»ä¸€ä¸ªæ•Œäººä½ ä¼šè·å¾— 100 åˆ†ã€‚å¥–åŠ±å€¼çš„è¿™ç§å·®å¼‚ä½¿å¾—æˆ‘ä»¬åœ¨ä¸åŒæ¸¸æˆä¹‹é—´çš„æŸå¤±å‡½æ•°å°ºåº¦å®Œå…¨ä¸åŒï¼Œè¿™ä½¿å¾—æ‰¾åˆ°é€‚ç”¨äºä¸€ç»„æ¸¸æˆçš„é€šç”¨è¶…å‚æ•°å˜å¾—æ›´åŠ å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¥–åŠ±è¢«è£å‰ªåˆ°
    âˆ’1 åˆ° 1 çš„èŒƒå›´å†…ã€‚è¿™åœ¨ ClipRewardEnv å°è£…å™¨ä¸­å®ç°ã€‚
- en: 'Rearrange observation dimensions to meet the PyTorch convolution layer: As
    weâ€™re going to use convolution, our tensors have to be rearranged the way PyTorch
    expects them. The Atari environment returns the observation in a (height, width,
    color) format, but the PyTorch convolution layer wants the channel layer to come
    first. This is implemented in wrappers.ImageToPyTorch.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡æ’è§‚å¯Ÿç»´åº¦ä»¥æ»¡è¶³ PyTorch å·ç§¯å±‚çš„è¦æ±‚ï¼šç”±äºæˆ‘ä»¬å°†ä½¿ç”¨å·ç§¯ï¼Œå¼ é‡éœ€è¦æŒ‰ PyTorch æœŸæœ›çš„æ–¹å¼è¿›è¡Œé‡æ’ã€‚Atari ç¯å¢ƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé¢œè‰²ï¼‰çš„æ ¼å¼è¿”å›è§‚å¯Ÿæ•°æ®ï¼Œä½†
    PyTorch å·ç§¯å±‚è¦æ±‚é€šé“ç»´åº¦æ’åœ¨æœ€å‰é¢ã€‚è¿™åœ¨ wrappers.ImageToPyTorch ä¸­å¾—ä»¥å®ç°ã€‚
- en: Most of those wrappers are implemented in the stable-baseline3 library, which
    provides the AtariWrapper class that applies wrappers in the required order, according
    to the constructorâ€™s parameters. It also detects the underlying environment properties
    and enables FireResetEnv if needed. Not all of the wrappers are required for the
    Pong game, but you should be aware of existing wrappers, just in case you decide
    to experiment with other games. Sometimes, when the DQN does not converge, the
    problem is not in the code but in the wrongly wrapped environment. I once spent
    several days debugging convergence issues, which were caused by missing the FIRE
    button press at the beginning of a game!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°è¿™äº›å°è£…å™¨éƒ½åœ¨ stable-baseline3 åº“ä¸­å®ç°ï¼Œåº“ä¸­æä¾›äº† AtariWrapper ç±»ï¼Œå®ƒæ ¹æ®æ„é€ å‡½æ•°çš„å‚æ•°æŒ‰éœ€è¦çš„é¡ºåºåº”ç”¨å°è£…å™¨ã€‚å®ƒè¿˜ä¼šæ£€æµ‹åº•å±‚ç¯å¢ƒçš„å±æ€§ï¼Œå¹¶åœ¨éœ€è¦æ—¶å¯ç”¨
    FireResetEnvã€‚å¹¶éæ‰€æœ‰å°è£…å™¨éƒ½éœ€è¦åœ¨ Pong æ¸¸æˆä¸­ä½¿ç”¨ï¼Œä½†ä½ åº”è¯¥äº†è§£ç°æœ‰çš„å°è£…å™¨ï¼Œä»¥é˜²ä½ å†³å®šå°è¯•å…¶ä»–æ¸¸æˆã€‚æœ‰æ—¶ï¼Œå½“ DQN ä¸æ”¶æ•›æ—¶ï¼Œé—®é¢˜å¹¶ä¸åœ¨ä»£ç ä¸­ï¼Œè€Œæ˜¯ç¯å¢ƒå°è£…é”™è¯¯ã€‚æˆ‘æ›¾ç»èŠ±äº†å‡ å¤©æ—¶é—´è°ƒè¯•æ”¶æ•›é—®é¢˜ï¼Œç»“æœæ˜¯å› ä¸ºåœ¨æ¸¸æˆå¼€å§‹æ—¶æ²¡æœ‰æŒ‰ä¸‹
    FIRE æŒ‰é’®ï¼
- en: 'Letâ€™s take a look at the implementation of individual wrappers. We will start
    with classes provided by stable-baseline3:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹å„ä¸ªå°è£…å™¨çš„å®ç°ã€‚æˆ‘ä»¬å°†ä» stable-baseline3 æä¾›çš„ç±»å¼€å§‹ï¼š
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding wrapper presses the FIRE button in environments that require that
    for the game to start. In addition to pressing FIRE, this wrapper checks for several
    corner cases that are present in some games.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å°è£…å™¨åœ¨éœ€è¦æŒ‰ä¸‹ FIRE æŒ‰é’®æ‰èƒ½å¼€å§‹æ¸¸æˆçš„ç¯å¢ƒä¸­æŒ‰ä¸‹è¯¥æŒ‰é’®ã€‚é™¤äº†æŒ‰ä¸‹ FIRE æŒ‰é’®å¤–ï¼Œè¿™ä¸ªå°è£…å™¨è¿˜ä¼šæ£€æŸ¥ä¸€äº›åœ¨æŸäº›æ¸¸æˆä¸­å­˜åœ¨çš„è¾¹ç¼˜æƒ…å†µã€‚
- en: 'This wrapper combines the repetition of actions during K frames and pixels
    from two consecutive frames:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå°è£…å™¨ç»“åˆäº†åœ¨ K å¸§å†…é‡å¤æ‰§è¡Œçš„åŠ¨ä½œå’Œæ¥è‡ªä¸¤å¸§ä¹‹é—´çš„åƒç´ ä¿¡æ¯ï¼š
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The goal of the following wrapper is to convert input observations from the
    emulator, which has a resolution of 210 Ã— 160 pixels with RGB color channels,
    to a grayscale 84 Ã— 84 image. It does this using CV2 libraryâ€™s function cvtColor,
    which does a colorimetric grayscale conversion (which is closer to human color
    perception than a simple averaging of color channels), and then the image is resized:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å°è£…å™¨çš„ç›®æ ‡æ˜¯å°†æ¥è‡ªæ¨¡æ‹Ÿå™¨çš„è¾“å…¥è§‚å¯Ÿè½¬æ¢ä¸ºä¸€ä¸ªåˆ†è¾¨ç‡ä¸º 210 Ã— 160 åƒç´ å¹¶å…·æœ‰ RGB é¢œè‰²é€šé“çš„å›¾åƒï¼Œè½¬æ¢ä¸ºä¸€ä¸ªç°åº¦ 84 Ã— 84 çš„å›¾åƒã€‚å®ƒé€šè¿‡ä½¿ç”¨
    CV2 åº“ä¸­çš„ cvtColor å‡½æ•°æ¥å®ç°è¯¥æ“ä½œï¼ŒcvTColor å‡½æ•°æ‰§è¡Œçš„æ˜¯è‰²åº¦ç°åº¦è½¬æ¢ï¼ˆè¿™ç§è½¬æ¢æ¯”ç®€å•çš„é¢œè‰²é€šé“å¹³å‡æ›´æ¥è¿‘äººç±»çš„é¢œè‰²æ„ŸçŸ¥ï¼‰ï¼Œç„¶åå›¾åƒä¼šè¢«ç¼©æ”¾ï¼š
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So far, weâ€™ve used wrappers from stable-baseline3 (I skipped EpisodicLifeEnv
    wrapper, as it is a bit complicated and not very relevant); you can find the code
    of other available wrappers in the repo stable_baselines3/common/atari_wrappers.py.
    Now, letâ€™s check out two wrappers from lib/wrappers.py:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨äº† stable-baseline3 ä¸­çš„å°è£…å™¨ï¼ˆæˆ‘è·³è¿‡äº† EpisodicLifeEnv å°è£…å™¨ï¼Œå› ä¸ºå®ƒæœ‰ç‚¹å¤æ‚ä¸”ä¸æ­¤ä¸å¤ªç›¸å…³ï¼‰ï¼›ä½ å¯ä»¥åœ¨ä»“åº“
    stable_baselines3/common/atari_wrappers.py ä¸­æ‰¾åˆ°å…¶ä»–å¯ç”¨å°è£…å™¨çš„ä»£ç ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ¥è‡ª lib/wrappers.py
    ä¸­çš„ä¸¤ä¸ªå°è£…å™¨ï¼š
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The BufferWrapper class creates a stack (implemented with the deque class) of
    subsequent frames along the first dimension and returns them as an observation.
    The purpose is to give the network an idea about the dynamics of the objects,
    such as the speed and direction of the ball in Pong or how enemies are moving.
    This is very important information, which it is not possible to obtain from a
    single image.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: BufferWrapperç±»åˆ›å»ºäº†ä¸€ä¸ªå †æ ˆï¼ˆä½¿ç”¨dequeç±»å®ç°ï¼‰ï¼Œæ²¿ç€ç¬¬ä¸€ç»´åº¦å †å éšåçš„å¸§ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºè§‚å¯Ÿå€¼è¿”å›ã€‚ç›®çš„æ˜¯è®©ç½‘ç»œäº†è§£ç‰©ä½“çš„åŠ¨æ€ä¿¡æ¯ï¼Œæ¯”å¦‚ä¹’ä¹“çƒçš„é€Ÿåº¦å’Œæ–¹å‘ï¼Œæˆ–è€…æ•Œäººæ˜¯å¦‚ä½•ç§»åŠ¨çš„ã€‚è¿™äº›ä¿¡æ¯éå¸¸é‡è¦ï¼Œæ˜¯ä»å•ä¸€å›¾åƒä¸­æ— æ³•è·å¾—çš„ã€‚
- en: One very important but not very obvious detail about this wrapper is that the
    observation method returns the copy of our buffered observations. This is very
    important, as weâ€™re going to keep our observations in the replay buffer, so the
    copy is needed to avoid buffer modification on the future environmentâ€™s steps.
    In principle, we can avoid making a copy (and reduce our memory footprint four
    times) by keeping the episodesâ€™ observations and indices in them, but it will
    require much more sophisticated data structure management. What is important currently
    is that this wrapper has to be the last in the chain of the wrappers applied to
    the environment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™ä¸ªåŒ…è£…å™¨æœ‰ä¸€ä¸ªéå¸¸é‡è¦ä½†ä¸å¤ªæ˜¾çœ¼çš„ç»†èŠ‚ï¼Œé‚£å°±æ˜¯è§‚å¯Ÿæ–¹æ³•è¿”å›çš„æ˜¯æˆ‘ä»¬ç¼“å†²åŒºä¸­è§‚å¯Ÿå€¼çš„å‰¯æœ¬ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬è¦å°†è§‚å¯Ÿå€¼ä¿å­˜åœ¨é‡æ”¾ç¼“å†²åŒºä¸­ï¼Œå› æ­¤éœ€è¦å‰¯æœ¬ä»¥é¿å…æœªæ¥ç¯å¢ƒæ­¥éª¤ä¸­å¯¹ç¼“å†²åŒºçš„ä¿®æ”¹ã€‚ä»åŸåˆ™ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å…¶ä¸­ä¿å­˜å›åˆçš„è§‚å¯Ÿå€¼å’Œå®ƒä»¬çš„ç´¢å¼•æ¥é¿å…åˆ¶ä½œå‰¯æœ¬ï¼ˆå¹¶å°†å†…å­˜å ç”¨å‡å°‘å››å€ï¼‰ï¼Œä½†è¿™éœ€è¦æ›´åŠ å¤æ‚çš„æ•°æ®ç»“æ„ç®¡ç†ã€‚ç›®å‰éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªåŒ…è£…å™¨å¿…é¡»æ˜¯åº”ç”¨äºç¯å¢ƒçš„åŒ…è£…å™¨é“¾ä¸­çš„æœ€åä¸€ä¸ªã€‚
- en: 'The last wrapper is ImageToPyTorch, and it changes the shape of the observation
    from height, width, channel (HWC) to the channel, height, width (CHW) format required
    by PyTorch:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åçš„åŒ…è£…å™¨æ˜¯ImageToPyTorchï¼Œå®ƒå°†è§‚å¯Ÿå€¼çš„å½¢çŠ¶ä»é«˜åº¦ã€å®½åº¦ã€é€šé“ï¼ˆHWCï¼‰æ ¼å¼è½¬æ¢ä¸ºPyTorchæ‰€éœ€çš„é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼ˆCHWï¼‰æ ¼å¼ï¼š
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The input shape of the tensor has a color channel as the last dimension, but
    PyTorchâ€™s convolution layers assume the color channel to be the first dimension.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ é‡çš„è¾“å…¥å½¢çŠ¶çš„æœ€åä¸€ç»´æ˜¯é¢œè‰²é€šé“ï¼Œä½†PyTorchçš„å·ç§¯å±‚å‡è®¾é¢œè‰²é€šé“æ˜¯ç¬¬ä¸€ç»´ã€‚
- en: 'At the end of the file is a simple function that creates an environment with
    a name and applies all the required wrappers to it:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡ä»¶çš„æœ€åæ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå®ƒåˆ›å»ºä¸€ä¸ªå¸¦æœ‰åç§°çš„ç¯å¢ƒï¼Œå¹¶å°†æ‰€æœ‰éœ€è¦çš„åŒ…è£…å™¨åº”ç”¨äºå®ƒï¼š
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, weâ€™re using the AtariWrapper class from stable-baseline3 and
    disabling some unnecessary wrappers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ¥è‡ªstable-baseline3çš„AtariWrapperç±»ï¼Œå¹¶ç¦ç”¨äº†ä¸€äº›ä¸å¿…è¦çš„åŒ…è£…å™¨ã€‚
- en: Thatâ€™s it for wrappers; letâ€™s look at our model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯åŒ…è£…å™¨çš„å†…å®¹ï¼›æ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: The DQN model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQNæ¨¡å‹
- en: The model published in Nature has three convolution layers followed by two fully
    connected layers. All layers are separated by rectified linear unit (ReLU) nonlinearities.
    The output of the model is Q-values for every action available in the environment,
    without nonlinearity applied (as Q-values can have any value). The approach of
    having all Q-values calculated with one pass through the network helps us to increase
    speed significantly in comparison to treating Q(s,a) literally, feeding observations
    and actions to the network to obtain the value of the action.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å‘è¡¨åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ä¸Šçš„æ¨¡å‹æœ‰ä¸‰ä¸ªå·ç§¯å±‚ï¼Œåé¢è·Ÿç€ä¸¤ä¸ªå…¨è¿æ¥å±‚ã€‚æ‰€æœ‰çš„å±‚éƒ½ç”±ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰éçº¿æ€§å‡½æ•°åˆ†éš”ã€‚è¯¥æ¨¡å‹çš„è¾“å‡ºæ˜¯ç¯å¢ƒä¸­æ¯ä¸ªå¯ç”¨åŠ¨ä½œçš„Qå€¼ï¼Œä¸”æ²¡æœ‰åº”ç”¨éçº¿æ€§ï¼ˆå› ä¸ºQå€¼å¯ä»¥æ˜¯ä»»æ„å€¼ï¼‰ã€‚é€šè¿‡è®©æ‰€æœ‰Qå€¼é€šè¿‡ç½‘ç»œä¸€æ¬¡è®¡ç®—å‡ºæ¥ï¼Œè¿™ç§æ–¹æ³•ç›¸æ¯”å°†Q(s,a)ç›´æ¥å¤„ç†å¹¶å°†è§‚å¯Ÿå’ŒåŠ¨ä½œè¾“å…¥ç½‘ç»œä»¥è·å–åŠ¨ä½œå€¼çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†é€Ÿåº¦ã€‚
- en: 'The code of the model is in Chapter06/lib/dqn_model.py:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„ä»£ç åœ¨Chapter06/lib/dqn_model.pyä¸­ï¼š
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To be able to write our network in a generic way, it was implemented in two
    parts: convolution and linear. The convolution part processes the imput image,
    which is a 4 Ã— 84 Ã— 84 tensor. The output from the last convolution filter is
    flattened into a one-dimensional vector and fed into two Linear layers.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†èƒ½å¤Ÿä»¥é€šç”¨çš„æ–¹å¼ç¼–å†™æˆ‘ä»¬çš„ç½‘ç»œï¼Œå®ƒè¢«å®ç°ä¸ºä¸¤éƒ¨åˆ†ï¼šå·ç§¯éƒ¨åˆ†å’Œçº¿æ€§éƒ¨åˆ†ã€‚å·ç§¯éƒ¨åˆ†å¤„ç†è¾“å…¥å›¾åƒï¼Œå®ƒæ˜¯ä¸€ä¸ª4 Ã— 84 Ã— 84çš„å¼ é‡ã€‚æœ€åä¸€ä¸ªå·ç§¯æ»¤æ³¢å™¨çš„è¾“å‡ºè¢«å±•å¹³ä¸ºä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œå¹¶è¾“å…¥åˆ°ä¸¤ä¸ªçº¿æ€§å±‚ä¸­ã€‚
- en: Another small problem is that we donâ€™t know the exact number of values in the
    output from the convolution layer produced with the input of the given shape.
    However, we need to pass this number to the first fully connected layer constructor.
    One possible solution would be to hard-code this number, which is a function of
    the input shape and the last convolution layer configuration (for 84 Ã— 84 input,
    the output from the convolution layer will have 3,136 values); however, itâ€™s not
    the best way, as our code will become less robust to input shape change. The better
    solution is to obtain the required dimension in runtime, by applying the convolution
    part to a fake input tensor. The dimension of the result would be equal to the
    number of parameters returned by this application. It would be fast, as this call
    would be done once on model creation, and it would also allow us to have generic
    code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå°é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬ä¸çŸ¥é“ç”±ç»™å®šå½¢çŠ¶è¾“å…¥äº§ç”Ÿçš„å·ç§¯å±‚è¾“å‡ºä¸­ç¡®åˆ‡çš„å€¼çš„æ•°é‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ•°å­—ä¼ é€’ç»™ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æ„é€ å‡½æ•°ã€‚ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¡¬ç¼–ç è¿™ä¸ªæ•°å­—ï¼Œå®ƒæ˜¯è¾“å…¥å½¢çŠ¶å’Œæœ€åä¸€ä¸ªå·ç§¯å±‚é…ç½®çš„å‡½æ•°ï¼ˆå¯¹äº
    84 Ã— 84 çš„è¾“å…¥ï¼Œå·ç§¯å±‚çš„è¾“å‡ºå°†æœ‰ 3,136 ä¸ªå€¼ï¼‰ï¼›ç„¶è€Œï¼Œè¿™ä¸æ˜¯æœ€å¥½çš„æ–¹å¼ï¼Œå› ä¸ºæˆ‘ä»¬çš„ä»£ç ä¼šå˜å¾—ä¸å¤ªå¥å£®ï¼Œæ— æ³•åº”å¯¹è¾“å…¥å½¢çŠ¶çš„å˜åŒ–ã€‚æ›´å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡åº”ç”¨å·ç§¯éƒ¨åˆ†åˆ°ä¸€ä¸ªå‡è¾“å…¥å¼ é‡ï¼Œåœ¨è¿è¡Œæ—¶è·å–æ‰€éœ€çš„ç»´åº¦ã€‚ç»“æœçš„ç»´åº¦å°†ç­‰äºè¯¥åº”ç”¨è¿”å›çš„å‚æ•°æ•°é‡ã€‚è¿™æ ·åšéå¸¸å¿«é€Ÿï¼Œå› ä¸ºè¿™ä¸ªè°ƒç”¨åªä¼šåœ¨æ¨¡å‹åˆ›å»ºæ—¶æ‰§è¡Œä¸€æ¬¡ï¼Œè€Œä¸”å®ƒè¿˜å…è®¸æˆ‘ä»¬æ‹¥æœ‰é€šç”¨çš„ä»£ç ã€‚
- en: 'The final piece of the model is the forward() function, which accepts the 4D
    input tensor. The first dimension is the batch size and the second is the color
    channel, which is our stack of subsequent frames; the third and fourth are image
    dimensions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„æœ€åä¸€éƒ¨åˆ†æ˜¯ `forward()` å‡½æ•°ï¼Œå®ƒæ¥å—4Dè¾“å…¥å¼ é‡ã€‚ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ‰¹é‡å¤§å°ï¼Œç¬¬äºŒä¸ªç»´åº¦æ˜¯é¢œè‰²é€šé“ï¼Œå®ƒæ˜¯æˆ‘ä»¬åç»­å¸§çš„å †å ï¼›ç¬¬ä¸‰å’Œç¬¬å››ä¸ªç»´åº¦æ˜¯å›¾åƒå°ºå¯¸ï¼š
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, before applying our network, we perform the scaling and type conversion
    of the input data. This requires a bit of explanation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œåœ¨åº”ç”¨æˆ‘ä»¬çš„ç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬å¯¹è¾“å…¥æ•°æ®è¿›è¡Œäº†ç¼©æ”¾å’Œç±»å‹è½¬æ¢ã€‚è¿™éœ€è¦ä¸€äº›è§£é‡Šã€‚
- en: 'Every pixel in the Atari image is represented as an unsigned byte with a value
    from 0 to 255\. This is convenient in two aspects: memory efficiency and GPU bandwidth.
    From the memory standpoint, we should keep environment observations as small as
    possible because our replay buffer will keep thousands of observations, and we
    want to keep it small. On the other hand, during the training, we need to transfer
    those observations into GPU memory to calculate the gradients and update the network
    parameters. The bandwidth between the main memory and GPU is a limited resource,
    so it also makes sense to keep observations as small as possible.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Atari å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸ºä¸€ä¸ªæ— ç¬¦å·å­—èŠ‚ï¼Œå€¼çš„èŒƒå›´ä» 0 åˆ° 255ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼šå†…å­˜æ•ˆç‡å’Œ GPU å¸¦å®½ã€‚ä»å†…å­˜çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åº”è¯¥å°½é‡ä¿æŒç¯å¢ƒè§‚å¯Ÿæ•°æ®çš„å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬çš„å›æ”¾ç¼“å†²åŒºä¼šä¿å­˜æˆåƒä¸Šä¸‡çš„è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒä¿æŒå°½å¯èƒ½å°ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›è§‚å¯Ÿæ•°æ®è½¬ç§»åˆ°
    GPU å†…å­˜ä¸­ï¼Œä»¥è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚ä¸»å†…å­˜å’Œ GPU ä¹‹é—´çš„å¸¦å®½æ˜¯æœ‰é™èµ„æºï¼Œå› æ­¤ä¿æŒè§‚å¯Ÿæ•°æ®å°½å¯èƒ½å°ä¹Ÿæ˜¯æœ‰é“ç†çš„ã€‚
- en: Thatâ€™s why we keep observations as a numpy array with dtype=uint8, and the input
    tensor to the network is ByteTensor. But the Conv2D layer expects the float tensor
    as an input, so by dividing the input tensor by 255.0, we scale to the 0â€¦1 range
    and do type conversion. This is fast, as the input byte tensor is already inside
    the GPU memory. After that, we apply both parts of our network to the resulting
    scaled tensor.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†è§‚å¯Ÿç»“æœä¿æŒä¸º `dtype=uint8` çš„ numpy æ•°ç»„ï¼Œå¹¶ä¸”ç½‘ç»œçš„è¾“å…¥å¼ é‡æ˜¯ ByteTensorã€‚ä½†æ˜¯ Conv2D å±‚æœŸæœ›è¾“å…¥çš„æ˜¯æµ®åŠ¨ç±»å‹å¼ é‡ï¼Œå› æ­¤é€šè¿‡å°†è¾“å…¥å¼ é‡é™¤ä»¥
    255.0ï¼Œæˆ‘ä»¬å°†å…¶ç¼©æ”¾åˆ° 0â€¦1 èŒƒå›´ï¼Œå¹¶è¿›è¡Œç±»å‹è½¬æ¢ã€‚è¿™æ˜¯å¿«é€Ÿçš„ï¼Œå› ä¸ºè¾“å…¥å­—èŠ‚å¼ é‡å·²ç»åœ¨ GPU å†…å­˜ä¸­ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†ç½‘ç»œçš„ä¸¤ä¸ªéƒ¨åˆ†åº”ç”¨äºç»“æœçš„ç¼©æ”¾å¼ é‡ã€‚
- en: Training
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: The third module contains the experience replay buffer, the agent, the loss
    function calculation, and the training loop itself. Before going into the code,
    something needs to be said about the training hyperparameters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªæ¨¡å—åŒ…å«ç»éªŒå›æ”¾ç¼“å†²åŒºã€æ™ºèƒ½ä½“ã€æŸå¤±å‡½æ•°è®¡ç®—å’Œè®­ç»ƒå¾ªç¯æœ¬èº«ã€‚åœ¨è¿›å…¥ä»£ç ä¹‹å‰ï¼Œéœ€è¦å…ˆè°ˆä¸€ä¸‹è®­ç»ƒçš„è¶…å‚æ•°ã€‚
- en: 'DeepMindâ€™s Nature paper contained a table with all the details about the hyperparameters
    used to train its model on all 49 Atari games used for evaluation. DeepMind kept
    all those parameters the same for all games (but trained individual models for
    every game), and it was the teamâ€™s intention to show that the method is robust
    enough to solve lots of games with varying complexity, action space, reward structure,
    and other details using one single model architecture and hyperparameters. However,
    our goal here is much more modest: we want to solve just the Pong game.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind çš„ Nature è®ºæ–‡ä¸­åŒ…å«äº†ä¸€ä¸ªè¡¨æ ¼ï¼Œåˆ—å‡ºäº†ç”¨äºè®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ‰€æœ‰ 49 ä¸ª Atari æ¸¸æˆçš„è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ã€‚DeepMind
    å¯¹æ‰€æœ‰æ¸¸æˆä¿æŒäº†ç›¸åŒçš„å‚æ•°è®¾ç½®ï¼ˆä½†æ¯ä¸ªæ¸¸æˆè®­ç»ƒäº†å•ç‹¬çš„æ¨¡å‹ï¼‰ï¼Œä»–ä»¬çš„ç›®çš„æ˜¯å±•ç¤ºè¯¥æ–¹æ³•è¶³å¤Ÿç¨³å¥ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å’Œè¶…å‚æ•°è§£å†³å¤šç§å¤æ‚åº¦ã€åŠ¨ä½œç©ºé—´ã€å¥–åŠ±ç»“æ„åŠå…¶ä»–ç»†èŠ‚å„å¼‚çš„æ¸¸æˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç›®æ ‡è¦è°¦é€Šå¾—å¤šï¼šæˆ‘ä»¬åªå¸Œæœ›è§£å†³
    Pong æ¸¸æˆã€‚
- en: Pong is quite simple and straightforward in comparison to other games in the
    Atari test set, so the hyperparameters in the paper are overkill for our task.
    For example, to get the best result on all 49 games, DeepMind used a million-observations
    replay buffer, which requires approximately 20 GB of RAM to store it and lots
    of samples from the environment to populate it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Pong ç›¸æ¯”äº Atari æµ‹è¯•é›†ä¸­å…¶ä»–æ¸¸æˆæ¥è¯´ç›¸å½“ç®€å•ç›´æ¥ï¼Œå› æ­¤æ–‡ä¸­æåˆ°çš„è¶…å‚æ•°å¯¹äºæˆ‘ä»¬çš„ä»»åŠ¡æ¥è¯´æ˜¯è¿‡å¤šçš„ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†åœ¨æ‰€æœ‰ 49 ä¸ªæ¸¸æˆä¸­è·å¾—æœ€ä½³ç»“æœï¼ŒDeepMind
    ä½¿ç”¨äº†ç™¾ä¸‡æ¬¡è§‚æµ‹çš„é‡æ”¾ç¼“å†²åŒºï¼Œè¿™éœ€è¦å¤§çº¦ 20 GB çš„å†…å­˜æ¥å­˜å‚¨ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„ç¯å¢ƒæ ·æœ¬æ¥å¡«å……å®ƒã€‚
- en: 'The epsilon decay schedule that was used is also not the best for a single
    Pong game. In the training, DeepMind linearly decayed epsilon from 1.0 to 0.1
    during the first million frames obtained from the environment. However, my own
    experiments have shown that for Pong, itâ€™s enough to decay epsilon over the first
    150k frames and then keep it stable. The replay buffer also can be much smaller:
    10k transitions will be enough.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä½¿ç”¨çš„ epsilon è¡°å‡è®¡åˆ’å¯¹äºå•ä¸€çš„ Pong æ¸¸æˆä¹Ÿä¸æ˜¯æœ€ä¼˜çš„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDeepMind å°† epsilon ä» 1.0 çº¿æ€§è¡°å‡åˆ° 0.1ï¼Œè¡°å‡è¿‡ç¨‹æŒç»­äº†ä»ç¯å¢ƒä¸­è·å¾—çš„å‰ç™¾ä¸‡å¸§ã€‚ç„¶è€Œï¼Œæˆ‘è‡ªå·±çš„å®éªŒè¡¨æ˜ï¼Œå¯¹äº
    Pong æ¸¸æˆï¼Œè¡°å‡ epsilon åªéœ€è¦åœ¨å‰ 15 ä¸‡å¸§å†…å®Œæˆï¼Œç„¶åä¿æŒç¨³å®šå³å¯ã€‚é‡æ”¾ç¼“å†²åŒºä¹Ÿå¯ä»¥æ›´å°ï¼š10k æ¬¡è½¬æ¢å°±è¶³å¤Ÿäº†ã€‚
- en: In the following example, Iâ€™ve used my parameters. These differ from the parameters
    in the paper but will allow us to solve Pong about 10 times faster. On a GeForce
    GTX 1080 Ti, the following version converges to a mean score of 19.0 in about
    50 minutes, but with DeepMindâ€™s hyperparameters, it will require at least a day.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æˆ‘çš„å‚æ•°ã€‚è™½ç„¶è¿™äº›å‚æ•°ä¸è®ºæ–‡ä¸­çš„å‚æ•°ä¸åŒï¼Œä½†å®ƒä»¬èƒ½è®©æˆ‘ä»¬å¤§çº¦ä»¥ 10 å€çš„é€Ÿåº¦è§£å†³ Pong æ¸¸æˆã€‚åœ¨ GeForce GTX 1080
    Ti ä¸Šï¼Œä»¥ä¸‹ç‰ˆæœ¬å¤§çº¦ 50 åˆ†é’Ÿå°±èƒ½æ”¶æ•›åˆ° 19.0 çš„å¹³å‡åˆ†æ•°ï¼Œä½†ä½¿ç”¨ DeepMind çš„è¶…å‚æ•°è‡³å°‘éœ€è¦ä¸€å¤©æ—¶é—´ã€‚
- en: This speedup, of course, involves fine-tuning for one particular environment
    and can break convergence on other games. You are free to play with the options
    and other games from the Atari set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ é€Ÿå½“ç„¶æ˜¯é’ˆå¯¹ç‰¹å®šç¯å¢ƒçš„å¾®è°ƒï¼Œå¹¶å¯èƒ½å¯¼è‡´åœ¨å…¶ä»–æ¸¸æˆä¸­æ— æ³•æ”¶æ•›ã€‚ä½ å¯ä»¥è‡ªç”±åœ°è°ƒæ•´é€‰é¡¹å’Œå°è¯• Atari é›†åˆä¸­çš„å…¶ä»–æ¸¸æˆã€‚
- en: 'First, we import the required modules:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ï¼š
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then define the hyperparameters:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®šä¹‰è¶…å‚æ•°ï¼š
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These two values set the default environment to train on and the reward boundary
    for the last 100 episodes to stop training. If you want, you can redefine the
    environment name using the command-line --env argument:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªå€¼è®¾ç½®äº†é»˜è®¤çš„è®­ç»ƒç¯å¢ƒä»¥åŠåœæ­¢è®­ç»ƒçš„å¥–åŠ±è¾¹ç•Œï¼ˆæœ€å 100 å›åˆï¼‰ã€‚å¦‚æœéœ€è¦ï¼Œä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œ --env å‚æ•°é‡æ–°å®šä¹‰ç¯å¢ƒåç§°ï¼š
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding parameters define the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å‚æ•°å®šä¹‰äº†ä»¥ä¸‹å†…å®¹ï¼š
- en: Our Î³ value used for the Bellman approximation (GAMMA)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨äºè´å°”æ›¼è¿‘ä¼¼çš„ Î³ å€¼ï¼ˆGAMMAï¼‰
- en: The batch size sampled from the replay buffer (BATCH_SIZE)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»é‡æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·çš„æ‰¹æ¬¡å¤§å°ï¼ˆBATCH_SIZEï¼‰
- en: The maximum capacity of the buffer (REPLAY_SIZE)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºçš„æœ€å¤§å®¹é‡ï¼ˆREPLAY_SIZEï¼‰
- en: The count of frames we wait for before starting training to populate the replay
    buffer (REPLAY_START_SIZE)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å¼€å§‹è®­ç»ƒå‰ç­‰å¾…çš„å¸§æ•°ï¼Œç”¨äºå¡«å……é‡æ”¾ç¼“å†²åŒºï¼ˆREPLAY_START_SIZEï¼‰
- en: The learning rate used in the Adam optimizer, which is used in this example
    (LEARNING_RATE)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨çš„ Adam ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ï¼ˆLEARNING_RATEï¼‰
- en: How frequently we sync model weights from the training model to the target model,
    which is used to get the value of the next state in the Bellman approximation
    (SYNC_TARGET_FRAMES)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹çš„æƒé‡åŒæ­¥åˆ°ç›®æ ‡æ¨¡å‹çš„é¢‘ç‡ï¼Œç›®æ ‡æ¨¡å‹ç”¨äºåœ¨è´å°”æ›¼è¿‘ä¼¼ä¸­è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼ˆSYNC_TARGET_FRAMESï¼‰
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The last batch of hyperparameters is related to the epsilon decay schedule.
    To achieve proper exploration, we start with ğœ– = 1.0 at the early stages of training,
    which causes all actions to be selected randomly. Then, during the first 150,000
    frames, ğœ– is linearly decayed to 0.01, which corresponds to the random action
    taken in 1% of steps. A similar scheme was used in the original DeepMind paper,
    but the duration of decay was almost 10 times longer (so ğœ– = 0.01 was reached
    after a million frames).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ‰¹è¶…å‚æ•°ä¸ epsilon è¡°å‡è°ƒåº¦æœ‰å…³ã€‚ä¸ºäº†å®ç°é€‚å½“çš„æ¢ç´¢ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µä» ğœ– = 1.0 å¼€å§‹ï¼Œè¿™ä¼šå¯¼è‡´æ‰€æœ‰åŠ¨ä½œéƒ½è¢«éšæœºé€‰æ‹©ã€‚ç„¶åï¼Œåœ¨å‰
    150,000 å¸§ä¸­ï¼Œğœ– ä¼šçº¿æ€§è¡°å‡åˆ° 0.01ï¼Œè¿™å¯¹åº”äº 1% çš„æ­¥éª¤ä¸­é‡‡å–éšæœºåŠ¨ä½œã€‚åŸå§‹ DeepMind è®ºæ–‡ä¸­ä¹Ÿä½¿ç”¨äº†ç±»ä¼¼çš„æ–¹æ¡ˆï¼Œä½†è¡°å‡çš„æŒç»­æ—¶é—´å‡ ä¹æ˜¯åŸæ¥çš„
    10 å€ï¼ˆå› æ­¤ ğœ– = 0.01 æ˜¯åœ¨ä¸€ç™¾ä¸‡å¸§åè¾¾åˆ°çš„ï¼‰ã€‚
- en: 'Here, we define our type aliases and the dataclass Experience, used to keep
    entries in the experience replay buffer. It contains the current state, the action
    taken, the reward obtained, the termination or truncation flag, and the new state:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†ç±»å‹åˆ«åå’Œæ•°æ®ç±» Experienceï¼Œç”¨äºä¿å­˜ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­çš„æ¡ç›®ã€‚å®ƒåŒ…å«å½“å‰çŠ¶æ€ã€é‡‡å–çš„åŠ¨ä½œã€è·å¾—çš„å¥–åŠ±ã€ç»ˆæ­¢æˆ–æˆªæ–­æ ‡å¿—ä»¥åŠæ–°çš„çŠ¶æ€ï¼š
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next chunk of code defines our experience replay buffer, the purpose of
    which is to keep the transitions obtained from the environment:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ®µä»£ç å®šä¹‰äº†æˆ‘ä»¬çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œç›®çš„æ˜¯ä¿å­˜ä»ç¯å¢ƒä¸­è·å¾—çš„è½¬ç§»ï¼š
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Each time we do a step in the environment, we push the transition into the buffer,
    keeping only a fixed number of steps (in our case, 10k transitions). For training,
    we randomly sample the batch of transitions from the replay buffer, which allows
    us to break the correlation between subsequent steps in the environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥æ—¶ï¼Œæˆ‘ä»¬å°†è½¬ç§»æ¨å…¥ç¼“å†²åŒºï¼Œåªä¿ç•™å›ºå®šæ•°é‡çš„æ­¥æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯ 10k æ¬¡è½¬ç§»ï¼‰ã€‚åœ¨è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä»å›æ”¾ç¼“å†²åŒºéšæœºæŠ½å–ä¸€æ‰¹è½¬ç§»ï¼Œè¿™æ ·å¯ä»¥æ‰“ç ´ç¯å¢ƒä¸­åç»­æ­¥éª¤ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
- en: 'Most of the experience replay buffer code is quite straightforward: it basically
    exploits the capability of the deque class to maintain the given number of entries
    in the buffer. In the sample() method, we create a list of random indices and
    return a list of Experience items to be repackaged and converted into tensors.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§éƒ¨åˆ†ç»éªŒå›æ”¾ç¼“å†²åŒºçš„ä»£ç éƒ½éå¸¸ç›´æ¥ï¼šå®ƒåŸºæœ¬ä¸Šåˆ©ç”¨äº† deque ç±»æ¥ä¿æŒç¼“å†²åŒºä¸­çš„æŒ‡å®šæ•°é‡çš„æ¡ç›®ã€‚åœ¨ sample() æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªéšæœºç´¢å¼•çš„åˆ—è¡¨ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«ç»éªŒæ¡ç›®çš„åˆ—è¡¨ï¼Œä»¥ä¾¿é‡æ–°åŒ…è£…å¹¶è½¬æ¢ä¸ºå¼ é‡ã€‚
- en: 'The next class we need to have is an Agent, which interacts with the environment
    and saves the result of the interaction in the experience replay buffer that you
    have just seen:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦çš„ç±»æ˜¯ Agentï¼Œå®ƒä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶å°†äº¤äº’çš„ç»“æœä¿å­˜åˆ°ä½ åˆšæ‰çœ‹åˆ°çš„ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­ï¼š
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: During the agentâ€™s initialization, we need to store references to the environment
    and experience replay buffer, tracking the current observation and the total reward
    accumulated so far.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ™ºèƒ½ä½“åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å­˜å‚¨å¯¹ç¯å¢ƒå’Œç»éªŒå›æ”¾ç¼“å†²åŒºçš„å¼•ç”¨ï¼Œè·Ÿè¸ªå½“å‰çš„è§‚å¯Ÿå€¼å’Œè¿„ä»Šä¸ºæ­¢ç´¯è®¡çš„æ€»å¥–åŠ±ã€‚
- en: 'The main method of the agent is to perform a step in the environment and store
    its result in the buffer. To do this, we need to select the action first:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æ™ºèƒ½ä½“çš„ä¸»è¦æ–¹æ³•æ˜¯åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥å¹¶å°†å…¶ç»“æœå­˜å‚¨åœ¨ç¼“å†²åŒºä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å…ˆé€‰æ‹©åŠ¨ä½œï¼š
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With the probability epsilon (passed as an argument), we take the random action;
    otherwise, we use the model to obtain the Q-values for all possible actions and
    choose the best. In this method, we use the PyTorch no_grad() decorator to disable
    gradient tracking during the whole method, as we donâ€™t need them anyway.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡ epsilonï¼ˆä½œä¸ºå‚æ•°ä¼ é€’ï¼‰ï¼Œæˆ‘ä»¬é‡‡å–éšæœºåŠ¨ä½œï¼›å¦åˆ™ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹æ¥è·å¾—æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„ Q å€¼ï¼Œå¹¶é€‰æ‹©æœ€ä¼˜çš„åŠ¨ä½œã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch
    çš„ no_grad() è£…é¥°å™¨åœ¨æ•´ä¸ªæ–¹æ³•ä¸­ç¦ç”¨æ¢¯åº¦è¿½è¸ªï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬ä¸éœ€è¦å®ƒä»¬ã€‚
- en: 'As the action has been chosen, we pass it to the environment to get the next
    observation and reward, store the data in the experience buffer, and then handle
    the end-of-episode situation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å½“åŠ¨ä½œè¢«é€‰ä¸­åï¼Œæˆ‘ä»¬å°†å…¶ä¼ é€’ç»™ç¯å¢ƒä»¥è·å–ä¸‹ä¸€ä¸ªè§‚å¯Ÿå€¼å’Œå¥–åŠ±ï¼Œå°†æ•°æ®å­˜å‚¨åœ¨ç»éªŒç¼“å†²åŒºä¸­ï¼Œç„¶åå¤„ç†å›åˆç»“æŸçš„æƒ…å†µï¼š
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The result of the function is the total accumulated reward if we have reached
    the end of the episode with this step, or None otherwise.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„ç»“æœæ˜¯æ€»çš„ç´¯è®¡å¥–åŠ±ï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡è¿™ä¸€æ­¥å·²ç»åˆ°è¾¾äº†å›åˆçš„ç»“æŸï¼Œåˆ™è¿”å›å¥–åŠ±ï¼Œå¦åˆ™è¿”å› Noneã€‚
- en: 'The function batch_to_tensors takes the batch of Experience objects and returns
    a tuple with states, actions, rewards, done flags, and new states repacked as
    PyTorch tensors of the corresponding types:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° batch_to_tensors æ¥å—ä¸€æ‰¹ Experience å¯¹è±¡ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€å®Œæˆæ ‡å¿—å’Œæ–°çŠ¶æ€çš„å…ƒç»„ï¼Œè¿™äº›æ•°æ®ä¼šè¢«é‡æ–°æ‰“åŒ…ä¸ºå¯¹åº”ç±»å‹çš„
    PyTorch å¼ é‡ï¼š
- en: '[PRE24]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When we work with states, we try to avoid memory copy (by using np.asarray()
    function), which is important, as Atari observations are large (4 frames with
    84 Ã— 84 bytes each), and we have a batch of 32 such objects. Without this optimization,
    performance drops about 20 times.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å¤„ç†çŠ¶æ€æ—¶ï¼Œæˆ‘ä»¬å°½é‡é¿å…å†…å­˜å¤åˆ¶ï¼ˆé€šè¿‡ä½¿ç”¨np.asarray()å‡½æ•°ï¼‰ï¼Œè¿™æ˜¯å¾ˆé‡è¦çš„ï¼Œå› ä¸ºAtariçš„è§‚æµ‹æ•°æ®é‡å¤§ï¼ˆæ¯å¸§æœ‰84 Ã— 84å­—èŠ‚ï¼Œå…±å››å¸§ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰32ä¸ªè¿™æ ·çš„å¯¹è±¡ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªä¼˜åŒ–ï¼Œæ€§èƒ½ä¼šä¸‹é™å¤§çº¦20å€ã€‚
- en: 'Now, it is time for the last function in the training module, which calculates
    the loss for the sampled batch. This function is written in a form to maximally
    exploit GPU parallelism by processing all batch samples with vector operations,
    which makes it harder to understand when compared with a naÃ¯ve loop over the batch.
    Yet this optimization pays off: the parallel version is more than two times faster
    than an explicit loop.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯è®­ç»ƒæ¨¡å—ä¸­æœ€åä¸€ä¸ªå‡½æ•°çš„æ—¶é—´äº†ï¼Œè¿™ä¸ªå‡½æ•°è®¡ç®—é‡‡æ ·æ‰¹æ¬¡çš„æŸå¤±ã€‚è¿™ä¸ªå‡½æ•°çš„å†™æ³•æ—¨åœ¨æœ€å¤§åŒ–åˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—ï¼Œé€šè¿‡å‘é‡åŒ–æ“ä½œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡æ ·æœ¬ï¼Œè¿™ä½¿å¾—å®ƒæ¯”ä¸€ä¸ªç®€å•çš„æ‰¹æ¬¡å¾ªç¯æ›´éš¾ç†è§£ã€‚ç„¶è€Œï¼Œè¿™ä¸ªä¼˜åŒ–æ˜¯å€¼å¾—çš„ï¼šå¹¶è¡Œç‰ˆæœ¬æ¯”æ˜¾å¼çš„å¾ªç¯å¿«äº†ä¸¤å€å¤šã€‚
- en: 'As a reminder, here is the loss expression we need to calculate:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼Œè¿™æ˜¯æˆ‘ä»¬éœ€è¦è®¡ç®—çš„æŸå¤±è¡¨è¾¾å¼ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq24.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq24.png)'
- en: 'We use the preceding equation for steps that arenâ€™t at the end of the episode
    and the following for the final steps:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨å‰é¢çš„æ–¹ç¨‹å¤„ç†éå›åˆç»“æŸçš„æ­¥éª¤ï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹å¤„ç†æœ€åçš„æ­¥éª¤ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq25.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq25.png)'
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the arguments, we pass our batch, the network that we are training, and the
    target network, which is periodically synced with the trained one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›å‚æ•°ä¸­ï¼Œæˆ‘ä»¬ä¼ å…¥äº†æˆ‘ä»¬çš„æ‰¹æ¬¡ã€æ­£åœ¨è®­ç»ƒçš„ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œï¼Œç›®æ ‡ç½‘ç»œä¼šå®šæœŸä¸è®­ç»ƒå¥½çš„ç½‘ç»œåŒæ­¥ã€‚
- en: The first model (passed as the parameter net) is used to calculate gradients;
    the second model in the tgt_net argument is used to calculate values for the next
    states, and this calculation shouldnâ€™t affect gradients. To achieve this, we use
    the detach() function of the PyTorch tensor to prevent gradients from flowing
    into the target networkâ€™s graph. This function was described in ChapterÂ [3](ch007.xhtml#x1-530003).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆä½œä¸ºnetå‚æ•°ä¼ å…¥ï¼‰ç”¨äºè®¡ç®—æ¢¯åº¦ï¼›ç¬¬äºŒä¸ªæ¨¡å‹ï¼ˆåœ¨tgt_netå‚æ•°ä¸­ï¼‰ç”¨äºè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼Œè¿™ä¸€è®¡ç®—ä¸åº”å½±å“æ¢¯åº¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨PyTorchå¼ é‡çš„detach()å‡½æ•°æ¥é˜²æ­¢æ¢¯åº¦æµå…¥ç›®æ ‡ç½‘ç»œçš„å›¾ä¸­ã€‚è¿™ä¸ªå‡½æ•°åœ¨ç¬¬[3](ch007.xhtml#x1-530003)ç« ä¸­æœ‰æè¿°ã€‚
- en: At the beginning of the function, we call the function batch_to_tensors to repack
    the batch into individual tensor variables.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡½æ•°å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨batch_to_tensorså‡½æ•°å°†æ‰¹æ¬¡é‡æ–°æ‰“åŒ…æˆå•ç‹¬çš„å¼ é‡å˜é‡ã€‚
- en: 'The next line is a bit tricky:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€è¡Œæœ‰ç‚¹å¤æ‚ï¼š
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Letâ€™s discuss it in detail. Here, we pass observations to the first model and
    extract the specific Q-values for the taken actions, using the gather() tensor
    operation. The first argument to the gather() call is a dimension index that we
    want to perform gathering on (in our case, it is equal to 1, which corresponds
    to actions).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯¦ç»†è®¨è®ºä¸€ä¸‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§‚æµ‹æ•°æ®ä¼ å…¥ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨gather()å¼ é‡æ“ä½œæå–å·²æ‰§è¡ŒåŠ¨ä½œçš„ç‰¹å®šQå€¼ã€‚gather()è°ƒç”¨çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æˆ‘ä»¬å¸Œæœ›è¿›è¡Œèšåˆçš„ç»´åº¦ç´¢å¼•ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒç­‰äº1ï¼Œè¡¨ç¤ºåŠ¨ä½œç»´åº¦ï¼‰ã€‚
- en: The second argument is a tensor of indices of elements to be chosen. Extra unsqueeze()
    and squeeze() calls are required to compute the index argument for the gather()
    function and to get rid of the extra dimensions that we created, respectively.
    (The index should have the same number of dimensions as the data we are processing.)
    In FigureÂ [6.3](#x1-102122r3), you can see an illustration of what gather() does
    on the example case, with a batch of six entries and four actions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå…ƒç´ ç´¢å¼•çš„å¼ é‡ï¼Œç”¨æ¥é€‰æ‹©éœ€è¦çš„å…ƒç´ ã€‚ä¸ºäº†è®¡ç®—gather()å‡½æ•°çš„ç´¢å¼•å‚æ•°å¹¶å»é™¤æˆ‘ä»¬åˆ›å»ºçš„å¤šä½™ç»´åº¦ï¼Œåˆ†åˆ«éœ€è¦é¢å¤–çš„unsqueeze()å’Œsqueeze()è°ƒç”¨ã€‚ï¼ˆç´¢å¼•åº”ä¸æˆ‘ä»¬å¤„ç†çš„æ•°æ®å…·æœ‰ç›¸åŒçš„ç»´åº¦æ•°ã€‚ï¼‰åœ¨å›¾[6.3](#x1-102122r3)ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°gather()åœ¨ç¤ºä¾‹ä¸­çš„ä½œç”¨ï¼Œç¤ºä¾‹ä¸­æœ‰å…­ä¸ªæ¡ç›®çš„æ‰¹æ¬¡å’Œå››ä¸ªåŠ¨ä½œã€‚
- en: '![PIC](img/file33.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file33.png)'
- en: 'FigureÂ 6.3: Transformation of tensors during a DQN loss calculation'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6.3ï¼šDQNæŸå¤±è®¡ç®—ä¸­çš„å¼ é‡è½¬æ¢
- en: Keep in mind that the result of gather() applied to tensors is a differentiable
    operation that will keep all gradients with respect to the final loss value.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œgather()åº”ç”¨äºå¼ é‡çš„ç»“æœæ˜¯ä¸€ä¸ªå¯å¾®åˆ†æ“ä½œï¼Œå®ƒä¼šä¿æŒä¸æœ€ç»ˆæŸå¤±å€¼ç›¸å…³çš„æ‰€æœ‰æ¢¯åº¦ã€‚
- en: 'Next, we disable the gradientsâ€™ calculations (which ends up in a small speedup),
    apply the target network to our next state observations, and calculate the maximum
    Q-value along the same action dimension, 1:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆè¿™ä¼šå¸¦æ¥ä¸€äº›é€Ÿåº¦æå‡ï¼‰ï¼Œå°†ç›®æ ‡ç½‘ç»œåº”ç”¨åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€çš„è§‚æµ‹ä¸­ï¼Œå¹¶æ²¿ç€ç›¸åŒè¡ŒåŠ¨ç»´åº¦ï¼ˆ1ï¼‰è®¡ç®—æœ€å¤§Qå€¼ï¼š
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The function max() returns both maximum values and indices of those values (so
    it calculates both max and argmax), which is very convenient. However, in this
    case, we are interested only in values, so we take the first entry of the result
    (max values).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° max() è¿”å›æœ€å¤§å€¼åŠå…¶ç´¢å¼•ï¼ˆå› æ­¤å®ƒåŒæ—¶è®¡ç®— max å’Œ argmaxï¼‰ï¼Œè¿™éå¸¸æ–¹ä¾¿ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªå¯¹å€¼æ„Ÿå…´è¶£ï¼Œå› æ­¤æˆ‘ä»¬å–ç»“æœä¸­çš„ç¬¬ä¸€ä¸ªæ¡ç›®ï¼ˆæœ€å¤§å€¼ï¼‰ã€‚
- en: 'The following is the next line:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸‹ä¸€è¡Œï¼š
- en: '[PRE28]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we make one simple but very important transformation: if the transition
    in the batch is from the last step in the episode, then our value of the action
    doesnâ€™t have a discounted reward for the next state, as there is no next state
    from which to gather the reward. This may look minor, but it is very important
    in practice; without this, training will not converge (I personally have wasted
    several hours debugging this case).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿›è¡Œä¸€ä¸ªç®€å•ä½†éå¸¸é‡è¦çš„è½¬æ¢ï¼šå¦‚æœæ‰¹æ¬¡ä¸­çš„è¿‡æ¸¡æ¥è‡ªå›åˆçš„æœ€åä¸€æ­¥ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„åŠ¨ä½œå€¼å°±æ²¡æœ‰ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æŠ˜æ‰£å¥–åŠ±ï¼Œå› ä¸ºæ²¡æœ‰ä¸‹ä¸€ä¸ªçŠ¶æ€å¯ä»¥è·å–å¥–åŠ±ã€‚è¿™çœ‹èµ·æ¥å¯èƒ½æ˜¯å°äº‹ï¼Œä½†åœ¨å®é™…ä¸­éå¸¸é‡è¦ï¼›æ²¡æœ‰è¿™ä¸€ç‚¹ï¼Œè®­ç»ƒå°†æ— æ³•æ”¶æ•›ï¼ˆæˆ‘ä¸ªäººèŠ±äº†å‡ ä¸ªå°æ—¶è°ƒè¯•è¿™ä¸ªé—®é¢˜ï¼‰ã€‚
- en: 'In the next line, we detach the value from its computation graph to prevent
    gradients from flowing into the NN used to calculate the Q approximation for the
    next states:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€è¡Œä¸­ï¼Œæˆ‘ä»¬å°†å€¼ä»å…¶è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä»¥é˜²æ­¢æ¢¯åº¦æµå…¥ç”¨äºè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ Q è¿‘ä¼¼å€¼çš„ç¥ç»ç½‘ç»œï¼š
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This is important, as without this, our backpropagation of the loss will start
    to affect both predictions for the current state and the next state. However,
    we donâ€™t want to touch predictions for the next state, as they are used in the
    Bellman equation to calculate the reference Q-values. To block gradients from
    flowing into this branch of the graph, we use the detach() method of the tensor,
    which returns the tensor without connection to its calculation history.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœæ²¡æœ‰è¿™ä¸ªï¼Œæˆ‘ä»¬çš„æŸå¤±åå‘ä¼ æ’­å°†å¼€å§‹å½±å“å½“å‰çŠ¶æ€å’Œä¸‹ä¸€ä¸ªçŠ¶æ€çš„é¢„æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸æƒ³è§¦åŠä¸‹ä¸€ä¸ªçŠ¶æ€çš„é¢„æµ‹ï¼Œå› ä¸ºå®ƒä»¬åœ¨è´å°”æ›¼æ–¹ç¨‹ä¸­ç”¨äºè®¡ç®—å‚è€ƒ
    Q å€¼ã€‚ä¸ºäº†é˜»æ­¢æ¢¯åº¦æµå…¥å›¾çš„è¿™ä¸ªåˆ†æ”¯ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ é‡çš„ detach() æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªæ²¡æœ‰è¿æ¥åˆ°è®¡ç®—å†å²çš„å¼ é‡ã€‚
- en: 'Finally, we calculate the Bellman approximation value and the mean squared
    error loss:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è´å°”æ›¼è¿‘ä¼¼å€¼å’Œå‡æ–¹è¯¯å·®æŸå¤±ï¼š
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To get the full picture of the loss function calculation code, letâ€™s look at
    this function in full:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…¨é¢äº†è§£æŸå¤±å‡½æ•°è®¡ç®—ä»£ç ï¼Œè®©æˆ‘ä»¬å®Œæ•´æŸ¥çœ‹è¿™ä¸ªå‡½æ•°ï¼š
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This ends our loss function calculation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»“æŸäº†æˆ‘ä»¬çš„æŸå¤±å‡½æ•°è®¡ç®—ã€‚
- en: 'The rest of the code is our training loop. To begin with, we create a parser
    of command-line arguments:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä¸‹çš„ä»£ç æ˜¯æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ï¼š
- en: '[PRE32]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Our script allows us to specify a device for computation and train on environments
    that are different from the default.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è„šæœ¬å…è®¸æˆ‘ä»¬æŒ‡å®šä¸€ä¸ªç”¨äºè®¡ç®—çš„è®¾å¤‡ï¼Œå¹¶åœ¨ä¸é»˜è®¤ç¯å¢ƒä¸åŒçš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚
- en: 'Here, we create our environment:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„ç¯å¢ƒï¼š
- en: '[PRE33]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Our environment has all the required wrappers applied, the NN that we are going
    to train, and our target network with the same architecture. At first, they will
    be initialized with different random weights, but it doesnâ€™t matter much, as we
    will sync them every 1k frames, which roughly corresponds to one episode of Pong.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç¯å¢ƒå·²ç»åº”ç”¨äº†æ‰€æœ‰å¿…éœ€çš„åŒ…è£…å™¨ï¼Œæˆ‘ä»¬å°†è®­ç»ƒçš„ç¥ç»ç½‘ç»œå’Œå…·æœ‰ç›¸åŒæ¶æ„çš„ç›®æ ‡ç½‘ç»œã€‚æœ€åˆï¼Œå®ƒä»¬ä¼šç”¨ä¸åŒçš„éšæœºæƒé‡åˆå§‹åŒ–ï¼Œä½†è¿™å¹¶ä¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šæ¯ 1k
    å¸§åŒæ­¥ä¸€æ¬¡å®ƒä»¬ï¼Œè¿™å¤§è‡´å¯¹åº”ä¸€ä¸ª Pong å›åˆã€‚
- en: 'Then, we create our experience replay buffer of the required size and pass
    it to the agent:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºæ‰€éœ€å¤§å°çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ™ºèƒ½ä½“ï¼š
- en: '[PRE34]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Epsilon is initially initialized to 1.0 but will be decreased every iteration.
    Here are the last things we do before the training loop:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon åˆå§‹å€¼ä¸º 1.0ï¼Œä½†ä¼šåœ¨æ¯æ¬¡è¿­ä»£æ—¶å‡å°ã€‚ä»¥ä¸‹æ˜¯è®­ç»ƒå¾ªç¯å¼€å§‹å‰æˆ‘ä»¬æ‰€åšçš„æœ€åå‡ ä»¶äº‹ï¼š
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We create an optimizer, a buffer for full episode rewards, a counter of frames
    and several variables to track our speed, and the best mean reward reached. Every
    time our mean reward beats the record, we will save the model in the file.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªä¼˜åŒ–å™¨ã€ä¸€ä¸ªç”¨äºå­˜å‚¨å®Œæ•´å›åˆå¥–åŠ±çš„ç¼“å†²åŒºã€ä¸€ä¸ªå¸§è®¡æ•°å™¨å’Œå‡ ä¸ªå˜é‡æ¥è·Ÿè¸ªæˆ‘ä»¬çš„é€Ÿåº¦ï¼Œä»¥åŠè¾¾åˆ°çš„æœ€ä½³å¹³å‡å¥–åŠ±ã€‚æ¯å½“æˆ‘ä»¬çš„å¹³å‡å¥–åŠ±çªç ´è®°å½•æ—¶ï¼Œæˆ‘ä»¬ä¼šå°†æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚
- en: 'At the beginning of the training loop, we count the number of iterations completed
    and decrease epsilon according to our schedule:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬ä¼šè®¡ç®—å®Œæˆçš„è¿­ä»£æ¬¡æ•°ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„è®¡åˆ’é™ä½ epsilon å€¼ï¼š
- en: '[PRE36]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Epsilon will drop linearly during the given number of frames (EPSILON_DECAY_LAST_FRAME=150k)
    and then be kept on the same level as EPSILON_FINAL=0.01.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon ä¼šåœ¨ç»™å®šçš„å¸§æ•°ï¼ˆEPSILON_DECAY_LAST_FRAME=150kï¼‰å†…çº¿æ€§ä¸‹é™ï¼Œç„¶åä¿æŒåœ¨ç›¸åŒçš„æ°´å¹³ï¼Œå³ EPSILON_FINAL=0.01ã€‚
- en: 'In this block of code, we ask our agent to make a single step in the environment
    (using our current network and value for epsilon):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬è®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥æ“ä½œï¼ˆä½¿ç”¨å½“å‰çš„ç½‘ç»œå’Œ epsilon çš„å€¼ï¼‰ï¼š
- en: '[PRE37]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This function returns a float value only if this step is the final step in
    the episode. In that case, we report our progress. Specifically, we calculate
    and show, both in the console and in TensorBoard, these values:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°ä»…åœ¨æ­¤æ­¥éª¤ä¸ºå›åˆçš„æœ€åä¸€æ­¥æ—¶è¿”å›æµ®åŠ¨å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŠ¥å‘Šæˆ‘ä»¬çš„è¿›åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¡ç®—å¹¶æ˜¾ç¤ºä»¥ä¸‹å€¼ï¼Œåœ¨æ§åˆ¶å°å’Œ TensorBoard ä¸­å±•ç¤ºï¼š
- en: Speed as a count of frames processed per second
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€Ÿåº¦ï¼ˆæ¯ç§’å¤„ç†çš„å¸§æ•°ï¼‰
- en: Count of episodes played
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·²è¿›è¡Œçš„å›åˆæ•°
- en: Mean reward for the last 100 episodes
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‡å» 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±
- en: Current value for epsilon
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“å‰ epsilon çš„å€¼
- en: 'Every time our mean reward for the last 100 episodes reaches a maximum, we
    report this and save the model parameters:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“è¿‡å» 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±è¾¾åˆ°æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬ä¼šæŠ¥å‘Šè¿™ä¸€æƒ…å†µå¹¶ä¿å­˜æ¨¡å‹å‚æ•°ï¼š
- en: '[PRE38]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If our mean reward exceeds the specified boundary, then we stop training. For
    Pong, the boundary is 19.0, which means winning more than 19 from 21 total games.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„å¹³å‡å¥–åŠ±è¶…è¿‡äº†æŒ‡å®šçš„è¾¹ç•Œï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚å¯¹äº Pongï¼Œè¾¹ç•Œæ˜¯ 19.0ï¼Œæ„å‘³ç€ä» 21 åœºæ¯”èµ›ä¸­èµ¢å¾—è¶…è¿‡ 19 åœºã€‚
- en: 'Here, we check whether our buffer is large enough for training:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ£€æŸ¥æˆ‘ä»¬çš„ç¼“å†²åŒºæ˜¯å¦è¶³å¤Ÿå¤§ï¼Œèƒ½å¤Ÿè¿›è¡Œè®­ç»ƒï¼š
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: First, we should wait for enough data to be accumulated, which in our case is
    10k transitions. The next condition syncs parameters from our main network to
    the target network every SYNC_TARGET_FRAMES, which is 1k by default.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥ç­‰å¾…è¶³å¤Ÿçš„æ•°æ®ç§¯ç´¯ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯ 10k æ¬¡è¿‡æ¸¡ã€‚ä¸‹ä¸€ä¸ªæ¡ä»¶æ˜¯åœ¨æ¯ä¸ª SYNC_TARGET_FRAMESï¼ˆé»˜è®¤æ˜¯ 1kï¼‰å‘¨æœŸåï¼Œä»ä¸»ç½‘ç»œåŒæ­¥å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œã€‚
- en: 'The last piece of the training loop is very simple but requires the most time
    to execute:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯çš„æœ€åä¸€éƒ¨åˆ†éå¸¸ç®€å•ï¼Œä½†éœ€è¦èŠ±è´¹æœ€å¤šçš„æ—¶é—´æ¥æ‰§è¡Œï¼š
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, we zero gradients, sample data batches from the experience replay buffer,
    calculate loss, and perform the optimization step to minimize the loss.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¢¯åº¦å½’é›¶ï¼Œä»ç»éªŒé‡æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·æ•°æ®æ‰¹æ¬¡ï¼Œè®¡ç®—æŸå¤±ï¼Œå¹¶æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ä»¥æœ€å°åŒ–æŸå¤±ã€‚
- en: Running and performance
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿è¡Œä¸æ€§èƒ½
- en: This example is demanding on resources. On Pong, it requires about 400k frames
    to reach a mean reward of 17 (which means winning more than 80% of games). A similar
    number of frames will be required to get from 17 to 19, as our learning progress
    will saturate, and it will be hard for the model to â€œpolish the policyâ€ and further
    improve the score. So, on average, a million game frames are needed to train it
    fully. On the GTX 1080Ti, I have a speed of about 250 frames per second, which
    is about an hour of training. On a CPU (i5-7600k), the speed is much slower, about
    40 frames per second, which will take about seven hours. Remember that this is
    for Pong, which is relatively easy to solve. Other games might require hundreds
    of millions of frames and a 100 times larger experience replay buffer.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­å¯¹èµ„æºè¦æ±‚è¾ƒé«˜ã€‚åœ¨ Pong ä¸Šï¼Œå®ƒéœ€è¦å¤§çº¦ 400k å¸§æ‰èƒ½è¾¾åˆ°å¹³å‡å¥–åŠ± 17ï¼ˆè¿™æ„å‘³ç€èµ¢å¾—è¶…è¿‡ 80% çš„æ¯”èµ›ï¼‰ã€‚ä¸ºäº†ä» 17 æå‡åˆ° 19ï¼Œç±»ä¼¼æ•°é‡çš„å¸§ä¹Ÿä¼šè¢«æ¶ˆè€—ï¼Œå› ä¸ºæˆ‘ä»¬çš„å­¦ä¹ è¿›åº¦å°†é¥±å’Œï¼Œæ¨¡å‹å¾ˆéš¾â€œæ¶¦è‰²ç­–ç•¥â€å¹¶è¿›ä¸€æ­¥æé«˜åˆ†æ•°ã€‚å› æ­¤ï¼Œå¹³å‡æ¥è¯´ï¼Œéœ€è¦ä¸€ç™¾ä¸‡ä¸ªæ¸¸æˆå¸§æ¥å®Œå…¨è®­ç»ƒå®ƒã€‚åœ¨
    GTX 1080Ti ä¸Šï¼Œæˆ‘çš„é€Ÿåº¦å¤§çº¦æ˜¯ 250 å¸§æ¯ç§’ï¼Œå¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶çš„è®­ç»ƒã€‚åœ¨ CPUï¼ˆi5-7600kï¼‰ä¸Šï¼Œé€Ÿåº¦è¦æ…¢å¾—å¤šï¼Œå¤§çº¦æ˜¯ 40 å¸§æ¯ç§’ï¼Œè®­ç»ƒå°†éœ€è¦å¤§çº¦ä¸ƒå°æ—¶ã€‚è®°ä½ï¼Œè¿™ä¸ªæ˜¯åœ¨
    Pong ä¸Šï¼ŒPong ç›¸å¯¹å®¹æ˜“è§£å†³ã€‚å…¶ä»–æ¸¸æˆå¯èƒ½éœ€è¦æ•°äº¿å¸§å’Œä¸€ä¸ªå¤§ 100 å€çš„ç»éªŒé‡æ”¾ç¼“å†²åŒºã€‚
- en: 'In ChapterÂ [8](ch012.xhtml#x1-1240008), we will look at various approaches
    found by researchers since 2015 that can help to increase both training speed
    and data efficiency. ChapterÂ [9](ch013.xhtml#x1-1600009) will be devoted to engineering
    tricks to speed up RL methodsâ€™ performance. Nevertheless, for Atari, you will
    need resources and patience. The following figure shows a chart with reward dynamics
    during the training:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è‡ª 2015 å¹´ä»¥æ¥ç ”ç©¶äººå‘˜å‘ç°çš„å„ç§æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©æé«˜è®­ç»ƒé€Ÿåº¦å’Œæ•°æ®æ•ˆç‡ã€‚ç¬¬[9](ch013.xhtml#x1-1600009)ç« å°†ä¸“æ³¨äºåŠ é€Ÿ
    RL æ–¹æ³•æ€§èƒ½çš„å·¥ç¨‹æŠ€å·§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œé’ˆå¯¹ Atariï¼Œä½ ä»ç„¶éœ€è¦èµ„æºå’Œè€å¿ƒã€‚ä»¥ä¸‹å›¾è¡¨å±•ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­å¥–åŠ±åŠ¨æ€çš„å˜åŒ–ï¼š
- en: '![PIC](img/B22150_06_04.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_06_04.png)'
- en: 'FigureÂ 6.4: Dynamics of average reward calculated over the last 100 episodes'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6.4ï¼šè®¡ç®—è¿‡å» 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±åŠ¨æ€
- en: 'Now, letâ€™s look at the console output from our training process (only the beginning
    of the output is shown):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ§åˆ¶å°è¾“å‡ºï¼ˆè¿™é‡Œåªå±•ç¤ºè¾“å‡ºçš„å¼€å§‹éƒ¨åˆ†ï¼‰ï¼š
- en: '[PRE41]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: During the first 10k steps, our speed is very high, as we donâ€™t do any training,
    which is the most expensive operation in our code. After 10k, we start sampling
    the training batches and the performance drops to more representative numbers.
    During the training, the performance also decreases slightly, just because of
    the epsilon decrease. When ğœ– is high, the actions are chosen randomly. As ğœ– approaches
    zero, we need to perform inference to get Q-values for action selection, which
    also costs time.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰10kæ­¥ä¸­ï¼Œæˆ‘ä»¬çš„é€Ÿåº¦éå¸¸å¿«ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è¿›è¡Œä»»ä½•è®­ç»ƒï¼Œè€Œè®­ç»ƒæ˜¯æˆ‘ä»¬ä»£ç ä¸­æœ€æ˜‚è´µçš„æ“ä½œã€‚10kæ­¥åï¼Œæˆ‘ä»¬å¼€å§‹é‡‡æ ·è®­ç»ƒæ‰¹æ¬¡ï¼Œæ€§èƒ½ä¸‹é™åˆ°æ›´å…·ä»£è¡¨æ€§çš„æ•°å­—ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ€§èƒ½ä¹Ÿä¼šç•¥æœ‰ä¸‹é™ï¼Œè¿™åªæ˜¯å› ä¸ºğœ–çš„å‡å°ã€‚å½“ğœ–è¾ƒé«˜æ—¶ï¼ŒåŠ¨ä½œæ˜¯éšæœºé€‰æ‹©çš„ã€‚å½“ğœ–æ¥è¿‘é›¶æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œæ¨ç†æ¥è·å¾—Qå€¼ä»¥è¿›è¡ŒåŠ¨ä½œé€‰æ‹©ï¼Œè¿™ä¹Ÿä¼šæ¶ˆè€—æ—¶é—´ã€‚
- en: 'Several dozens of games later, our DQN should start to figure out how to win
    1 or 2 games out of 21, and an average reward begins to grow (this normally happens
    around ğœ– = 0.5):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ååœºæ¸¸æˆä¹‹åï¼Œæˆ‘ä»¬çš„DQNåº”è¯¥å¼€å§‹å¼„æ˜ç™½å¦‚ä½•åœ¨21åœºæ¯”èµ›ä¸­èµ¢å¾—1åˆ°2åœºï¼Œå¹¶ä¸”å¹³å‡å¥–åŠ±å¼€å§‹å¢é•¿ï¼ˆé€šå¸¸åœ¨ğœ– = 0.5æ—¶å‘ç”Ÿï¼‰ï¼š
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, after many more games, our DQN can finally dominate and beat the (not
    very sophisticated) built-in Pong AI opponent:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç»è¿‡æ›´å¤šçš„æ¯”èµ›ï¼Œæˆ‘ä»¬çš„DQNç»ˆäºèƒ½å¤Ÿä¸»å®°å¹¶å‡»è´¥ï¼ˆä¸å¤ªå¤æ‚çš„ï¼‰å†…ç½®Pong AIå¯¹æ‰‹ï¼š
- en: '[PRE43]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Due to randomness in the training process, your actual dynamics might differ
    from what is displayed here. In some rare cases (1 run from 10 according to my
    experiments), the training does not converge at all, which looks like a constant
    stream of rewards âˆ’21 for a long time. This is not an uncommon situation in deep
    learning (due to the randomness of the training) and might occur even more often
    in RL (due to the added randomness of environment communication). If your training
    doesnâ€™t show any positive dynamics for the first 100kâ€“200k iterations, you should
    restart it.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨éšæœºæ€§ï¼Œä½ çš„å®é™…åŠ¨æ€å¯èƒ½ä¸è¿™é‡Œå±•ç¤ºçš„ä¸åŒã€‚åœ¨ä¸€äº›ç½•è§çš„æƒ…å†µä¸‹ï¼ˆæ ¹æ®æˆ‘çš„å®éªŒï¼Œå¤§çº¦10æ¬¡è¿è¡Œä¸­æœ‰1æ¬¡ï¼‰ï¼Œè®­ç»ƒæ ¹æœ¬æ— æ³•æ”¶æ•›ï¼Œè¡¨ç°ä¸ºé•¿æ—¶é—´çš„å¥–åŠ±å§‹ç»ˆä¸ºâˆ’21ã€‚è¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­å¹¶ä¸ç½•è§ï¼ˆç”±äºè®­ç»ƒçš„éšæœºæ€§ï¼‰ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¯èƒ½ä¼šæ›´å¸¸è§ï¼ˆç”±äºç¯å¢ƒäº¤äº’çš„é¢å¤–éšæœºæ€§ï¼‰ã€‚å¦‚æœä½ çš„è®­ç»ƒåœ¨å‰100kåˆ°200kæ¬¡è¿­ä»£ä¸­æ²¡æœ‰ä»»ä½•æ­£å‘åŠ¨æ€ï¼Œä½ åº”è¯¥é‡æ–°å¼€å§‹è®­ç»ƒã€‚
- en: Your model in action
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ çš„æ¨¡å‹åœ¨å®è·µä¸­çš„è¡¨ç°
- en: The training process is just one part of the picture. Our final goal is not
    only to train the model; we also want our model to play the game with a good outcome.
    During the training, every time we update the maximum of the mean reward for the
    last 100 games, we save the model into the file PongNoFrameskip-v4-best_<score>.dat.
    In the Chapter06/03_dqn_play.py file, we have a program that can load this model
    file and play one episode, displaying the modelâ€™s dynamics.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹åªæ˜¯æ•´ä¸ªè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡ä¸ä»…ä»…æ˜¯è®­ç»ƒæ¨¡å‹ï¼›æˆ‘ä»¬è¿˜å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹èƒ½ä»¥è‰¯å¥½çš„ç»“æœæ¥ç©æ¸¸æˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡æ›´æ–°è¿‡å»100åœºæ¸¸æˆçš„å¹³å‡å¥–åŠ±æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå°†æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶
    PongNoFrameskip-v4-best_<score>.dat ä¸­ã€‚åœ¨ Chapter06/03_dqn_play.py æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç¨‹åºå¯ä»¥åŠ è½½è¿™ä¸ªæ¨¡å‹æ–‡ä»¶å¹¶è¿›è¡Œä¸€æ¬¡æ¸¸æˆï¼Œå±•ç¤ºæ¨¡å‹çš„åŠ¨æ€ã€‚
- en: The code is very simple, but it can be like magic seeing how several matrices,
    with just a million parameters, can play Pong with superhuman accuracy by observing
    only the pixels.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç éå¸¸ç®€å•ï¼Œä½†çœ‹åˆ°å‡ ä¸ªçŸ©é˜µï¼ˆä»…æœ‰ç™¾ä¸‡ä¸ªå‚æ•°ï¼‰é€šè¿‡è§‚å¯Ÿåƒç´ ï¼Œèƒ½å¤Ÿä»¥è¶…äººç²¾åº¦ç©Pongæ¸¸æˆï¼Œç®€ç›´åƒé­”æ³•ä¸€æ ·ã€‚
- en: 'First, we import the familiar PyTorch and Gym modules:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥ç†Ÿæ‚‰çš„PyTorchå’ŒGymæ¨¡å—ï¼š
- en: '[PRE44]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The script accepts the filename of the saved model and allows the specification
    of the Gym environment (of course, the model and environment have to match):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬æ¥å—å·²ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶åï¼Œå¹¶å…è®¸æŒ‡å®šGymç¯å¢ƒï¼ˆå½“ç„¶ï¼Œæ¨¡å‹å’Œç¯å¢ƒå¿…é¡»åŒ¹é…ï¼‰ï¼š
- en: '[PRE45]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Additionally, you have to pass option -r with the name of a nonexistent directory,
    which will be used to save a video of your game.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½ è¿˜éœ€è¦ä¼ é€’é€‰é¡¹ -rï¼Œå¹¶æŒ‡å®šä¸€ä¸ªä¸å­˜åœ¨çš„ç›®å½•åï¼Œç³»ç»Ÿå°†æŠŠä½ æ¸¸æˆçš„å½•åƒä¿å­˜åœ¨è¯¥ç›®å½•ä¸‹ã€‚
- en: 'The following code is also not very complicated:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ä¹Ÿä¸å¤ªå¤æ‚ï¼š
- en: '[PRE46]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We create the environment, wrap it in the RecordVideo wrapper, create the model,
    and then we load weights from the file passed in the arguments. The argument map_location,
    passed to the torch.load() function, is needed to map the loaded tensor location
    from the GPU to the CPU. By default, torch tries to load tensors on the same device
    where they were saved, but if you copy the model from the machine you used for
    training (with a GPU) to a laptop without a GPU, the locations need to be remapped.
    Our example doesnâ€™t use the GPU at all, as inference is fast enough without acceleration.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºç¯å¢ƒï¼Œå°†å…¶åŒ…è£…åœ¨ RecordVideo å°è£…å™¨ä¸­ï¼Œåˆ›å»ºæ¨¡å‹ï¼Œç„¶åä»ä¼ å…¥çš„æ–‡ä»¶ä¸­åŠ è½½æƒé‡ã€‚ä¼ é€’ç»™ torch.load() å‡½æ•°çš„ map_location
    å‚æ•°ç”¨äºå°†åŠ è½½çš„å¼ é‡ä½ç½®ä» GPU æ˜ å°„åˆ° CPUã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œtorch å°è¯•åŠ è½½ä¸ä¿å­˜æ—¶ç›¸åŒè®¾å¤‡ä¸Šçš„å¼ é‡ï¼Œä½†å¦‚æœä½ å°†æ¨¡å‹ä»ç”¨äºè®­ç»ƒçš„æœºå™¨ï¼ˆæœ‰ GPUï¼‰å¤åˆ¶åˆ°æ²¡æœ‰
    GPU çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šï¼Œå°±éœ€è¦é‡æ–°æ˜ å°„ä½ç½®ã€‚æˆ‘ä»¬çš„ç¤ºä¾‹å®Œå…¨ä¸ä½¿ç”¨ GPUï¼Œå› ä¸ºæ¨ç†é€Ÿåº¦è¶³å¤Ÿå¿«ï¼Œä¸éœ€è¦åŠ é€Ÿã€‚
- en: 'This is almost an exact copy of the Agent classâ€™ method play_step() from the
    training code, without the epsilon-greedy action selection:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å‡ ä¹ä¸è®­ç»ƒä»£ç ä¸­çš„ Agent ç±»çš„ play_step() æ–¹æ³•å®Œå…¨ç›¸åŒçš„ä»£ç ï¼Œä¸åŒ…å« epsilon-greedy åŠ¨ä½œé€‰æ‹©ï¼š
- en: '[PRE47]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We just pass our observation to the agent and select the action with the maximum
    value.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªéœ€å°†è§‚å¯Ÿç»“æœä¼ é€’ç»™æ™ºèƒ½ä½“ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€å¤§å€¼çš„åŠ¨ä½œã€‚
- en: 'The rest of the code is also simple:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä½™çš„ä»£ç ä¹Ÿå¾ˆç®€å•ï¼š
- en: '[PRE48]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We pass the action to the environment, count the total reward, and stop our
    loop when the episode ends. After the episode, we show the total reward and the
    number of times that the agent executed the action.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åŠ¨ä½œä¼ é€’ç»™ç¯å¢ƒï¼Œè®¡ç®—æ€»å¥–åŠ±ï¼Œå¹¶åœ¨å›åˆç»“æŸæ—¶åœæ­¢å¾ªç¯ã€‚å›åˆç»“æŸåï¼Œæˆ‘ä»¬æ˜¾ç¤ºæ€»å¥–åŠ±å’Œæ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œçš„æ¬¡æ•°ã€‚
- en: 'In this YouTube playlist, you can find recordings of the gameplay at different
    stages of the training: [https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu](https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ª YouTube æ’­æ”¾åˆ—è¡¨ä¸­ï¼Œä½ å¯ä»¥æ‰¾åˆ°åœ¨è®­ç»ƒä¸åŒé˜¶æ®µçš„æ¸¸æˆå½•åƒï¼š[https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu](https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu)ã€‚
- en: Things to try
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°è¯•çš„äº‹é¡¹
- en: 'If you are curious and want to experiment with this chapterâ€™s material on your
    own, then here is a shortlist of directions to explore. Be warned though: they
    can take lots of time and may cause you some moments of frustration during your
    experiments. However, these experiments are a very efficient way to really master
    the material from a practical point of view:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ„Ÿåˆ°å¥½å¥‡å¹¶æƒ³è‡ªå·±å°è¯•æœ¬ç« çš„å†…å®¹ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å€¼å¾—æ¢ç´¢çš„æ–¹å‘ã€‚éœ€è¦è­¦å‘Šçš„æ˜¯ï¼šè¿™äº›å®éªŒå¯èƒ½ä¼šè€—è´¹å¤§é‡æ—¶é—´ï¼Œå¹¶ä¸”åœ¨å®éªŒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°æŒ«æŠ˜ã€‚ç„¶è€Œï¼Œä»å®è·µè§’åº¦çœ‹ï¼Œè¿™äº›å®éªŒæ˜¯æŒæ¡ææ–™çš„éå¸¸æœ‰æ•ˆçš„æ–¹å¼ï¼š
- en: Try to take some other games from the Atari set, such as Breakout, Atlantis,
    or River Raid (my childhood favorite). This could require the tuning of hyperparameters.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•ä» Atari å¥—ä»¶ä¸­é€‰æ‹©å…¶ä»–æ¸¸æˆï¼Œä¾‹å¦‚ã€ŠBreakoutã€‹ã€ã€ŠAtlantisã€‹æˆ–ã€ŠRiver Raidã€‹ï¼ˆæˆ‘ç«¥å¹´çš„æœ€çˆ±ï¼‰ã€‚è¿™å¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°ã€‚
- en: As an alternative to FrozenLake, there is another tabular environment, Taxi,
    which emulates a taxi driver who needs to pick up passengers and take them to
    a destination.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½œä¸º FrozenLake çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¿˜æœ‰å¦ä¸€ä¸ªè¡¨æ ¼ç¯å¢ƒï¼ŒTaxiï¼Œå®ƒæ¨¡æ‹Ÿä¸€ä¸ªå‡ºç§Ÿè½¦å¸æœºéœ€è¦æ¥é€ä¹˜å®¢å¹¶å°†å…¶é€åˆ°ç›®çš„åœ°çš„åœºæ™¯ã€‚
- en: Play with Pong hyperparameters. Is it possible to train faster? OpenAI claims
    that it can solve Pong in 30 minutes using the asynchronous advantage actor-critic
    method (which is a subject of Part 3 of this book). Maybe itâ€™s possible with a
    DQN.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•è°ƒæ•´ Pong çš„è¶…å‚æ•°ã€‚æ˜¯å¦æœ‰å¯èƒ½è®­ç»ƒå¾—æ›´å¿«ï¼ŸOpenAI å£°ç§°ä½¿ç”¨å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼ˆæœ¬ä¹¦ç¬¬ 3 éƒ¨åˆ†çš„ä¸»é¢˜ï¼‰å¯ä»¥åœ¨ 30 åˆ†é’Ÿå†…è§£å†³ Pong
    é—®é¢˜ã€‚ä¹Ÿè®¸ç”¨ DQN ä¹Ÿèƒ½åšåˆ°ã€‚
- en: Can you make the DQN training code faster? The OpenAI Baselines project has
    shown 350 FPS using TensorFlow on GTX 1080 Ti. So, it looks like itâ€™s possible
    to optimize the PyTorch code. We will discuss this topic in ChapterÂ [9](ch013.xhtml#x1-1600009),
    but meanwhile, you can do your own experiments.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ èƒ½è®© DQN è®­ç»ƒä»£ç æ›´å¿«å—ï¼ŸOpenAI Baselines é¡¹ç›®å·²ç»åœ¨ GTX 1080 Ti ä¸Šä½¿ç”¨ TensorFlow å®ç°äº† 350 FPSã€‚å› æ­¤ï¼Œä¼˜åŒ–
    PyTorch ä»£ç æ˜¯å¯èƒ½çš„ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬[9](ch013.xhtml#x1-1600009)ç« è®¨è®ºè¿™ä¸ªè¯é¢˜ï¼Œä½†åŒæ—¶ä½ ä¹Ÿå¯ä»¥è¿›è¡Œè‡ªå·±çš„å®éªŒã€‚
- en: In the video recording, you might notice that models with a mean score around
    zero play quite well. In fact, I had the impression that those models play better
    than models with mean scores of 10â€“19\. This might be the case due to overfitting
    to the particular game situations. Could you try to fix this? Maybe it would be
    possible to use a generative adversarial network-style approach to make one model
    play with another?
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è§†é¢‘å½•åˆ¶ä¸­ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œå¹³å‡åˆ†æ¥è¿‘é›¶çš„æ¨¡å‹è¡¨ç°å¾—ç›¸å½“ä¸é”™ã€‚å®é™…ä¸Šï¼Œæˆ‘æœ‰ä¸€ç§å°è±¡ï¼Œå¹³å‡åˆ†åœ¨ 10-19 ä¹‹é—´çš„æ¨¡å‹è¡¨ç°å¾—ä¸å¦‚è¿™äº›æ¨¡å‹ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹å¯¹ç‰¹å®šçš„æ¸¸æˆæƒ…å†µè¿‡æ‹Ÿåˆã€‚ä½ èƒ½å°è¯•ä¿®å¤è¿™ä¸ªé—®é¢˜å—ï¼Ÿä¹Ÿè®¸å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œé£æ ¼çš„æ–¹æ³•ï¼Œè®©ä¸€ä¸ªæ¨¡å‹ä¸å¦ä¸€ä¸ªæ¨¡å‹å¯¹æˆ˜ï¼Ÿ
- en: Can you get the Ultimate Pong Dominator model with a mean score of 21? It shouldnâ€™t
    be very hard â€“ the learning rate decay is the obvious method to try.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ èƒ½é€šè¿‡ä¸€ä¸ªå¹³å‡å¾—åˆ†ä¸º21çš„æœ€ç»ˆPongä¸»å®°è€…æ¨¡å‹å—ï¼Ÿè¿™åº”è¯¥ä¸éš¾â€”â€”å­¦ä¹ ç‡è¡°å‡æ˜¯ä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„å°è¯•æ–¹æ³•ã€‚
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, we covered a lot of new and complex material. You became familiar
    with the limitations of value iteration in complex environments with large observation
    spaces, and we discussed how to overcome them with Q-learning. We checked the
    Q-learning algorithm on the FrozenLake environment and discussed the approximation
    of Q-values with NNs, as well as the extra complications that arise from this
    approximation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬æ¶‰åŠäº†è®¸å¤šæ–°çš„å¤æ‚å†…å®¹ã€‚ä½ å·²ç»äº†è§£äº†åœ¨å…·æœ‰å¤§è§‚æµ‹ç©ºé—´çš„å¤æ‚ç¯å¢ƒä¸­ï¼Œå€¼è¿­ä»£çš„å±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†å¦‚ä½•é€šè¿‡Qå­¦ä¹ æ¥å…‹æœè¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬åœ¨FrozenLakeç¯å¢ƒä¸­éªŒè¯äº†Qå­¦ä¹ ç®—æ³•ï¼Œå¹¶è®¨è®ºäº†ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆNNsï¼‰å¯¹Qå€¼è¿›è¡Œé€¼è¿‘ï¼Œä»¥åŠç”±æ­¤é€¼è¿‘å¸¦æ¥çš„é¢å¤–å¤æ‚æ€§ã€‚
- en: We covered several tricks with DQNs to improve their training stability and
    convergence, such as an experience replay buffer, target networks, and frame stacking.
    Finally, we combined those extensions into one single implementation of DQN that
    solves the Pong environment from the Atari games suite.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»‹ç»äº†å¤šä¸ªé’ˆå¯¹æ·±åº¦Qç½‘ç»œï¼ˆDQNsï¼‰çš„æŠ€å·§ï¼Œä»¥æé«˜å®ƒä»¬çš„è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œä¾‹å¦‚ç»éªŒå›æ”¾ç¼“å†²åŒºã€ç›®æ ‡ç½‘ç»œå’Œå¸§å †å ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿™äº›æ‰©å±•æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„DQNå®ç°ä¸­ï¼ŒæˆåŠŸè§£å†³äº†Atariæ¸¸æˆå¥—ä»¶ä¸­çš„Pongç¯å¢ƒã€‚
- en: In the next chapter, we will take a quick look at higher-level RL libraries,
    and after that, we will take a look at a set of tricks that researchers have found
    since 2015 to improve DQN convergence and quality, which (combined) can produce
    state-of-the-art results on most of the 54 (newly added) Atari games. This set
    was published in 2017, and we will analyze and reimplement all of the tricks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦äº†è§£ä¸€äº›æ›´é«˜çº§çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº“ï¼Œä¹‹åï¼Œæˆ‘ä»¬å°†å›é¡¾ä¸€ç»„è‡ª2015å¹´ä»¥æ¥ç ”ç©¶äººå‘˜å‘ç°çš„æŠ€å·§ï¼Œç”¨ä»¥æ”¹å–„DQNçš„æ”¶æ•›æ€§å’Œè´¨é‡ï¼Œè¿™äº›æŠ€å·§ï¼ˆç»¼åˆèµ·æ¥ï¼‰å¯ä»¥åœ¨å¤§å¤šæ•°54æ¬¾ï¼ˆæ–°æ·»åŠ çš„ï¼‰Atariæ¸¸æˆä¸Šå®ç°æœ€å…ˆè¿›çš„æˆæœã€‚è¿™äº›æŠ€å·§åœ¨2017å¹´å‘å¸ƒï¼Œæˆ‘ä»¬å°†åˆ†æå¹¶é‡æ–°å®ç°æ‰€æœ‰è¿™äº›æŠ€å·§ã€‚
