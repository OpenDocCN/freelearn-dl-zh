- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Deep Q-Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: 'In Chapter [5](ch009.xhtml#x1-820005), you became familiar with the Bellman
    equation and the practical method of its application called value iteration. This
    approach allowed us to significantly improve our speed and convergence in the
    FrozenLake environment, which is promising, but can we go further? In this chapter,
    we will apply the same approach to problems of much greater complexity: arcade
    games from the Atari 2600 platform, which are the de facto benchmark of the reinforcement
    learning (RL) research community.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5章](ch009.xhtml#x1-820005)中，你已经熟悉了贝尔曼方程以及其应用的实际方法——价值迭代。这个方法让我们在FrozenLake环境中显著提高了速度和收敛性，这很有前景，但我们能进一步提升吗？在本章中，我们将把相同的方法应用到复杂度更高的问题上：来自Atari
    2600平台的街机游戏，这些游戏是强化学习（RL）研究社区的事实标准。
- en: 'To deal with this new and more challenging goal, in this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一新的、更具挑战性的目标，本章将：
- en: Talk about problems with the value iteration method and consider its variation,
    called Q-learning.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论价值迭代方法的问题，并考虑它的变体——Q学习。
- en: Apply Q-learning to so-called grid world environments, which is called tabular
    Q-learning.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Q学习应用于所谓的网格世界环境，这种方法被称为表格Q学习。
- en: Discuss Q-learning in conjunction with neural networks (NNs). This combination
    has the name deep Q-network (DQN).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论Q学习与神经网络（NNs）的结合。这种结合被称为深度Q网络（DQN）。
- en: At the end of the chapter, we will reimplement a DQN algorithm from the famous
    paper Playing Atari with deep reinforcement learning [[Mni13](#)], which was published
    in 2013 and started a new era in RL development. Although it is too early to discuss
    the practical applicability of these basic methods, this will become clearer to
    you as you progress with the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的结尾，我们将重新实现著名论文《Playing Atari with Deep Reinforcement Learning》[[Mni13](#)]中的DQN算法，该论文于2013年发布，并开启了强化学习发展的新纪元。虽然讨论这些基本方法的实际应用还为时过早，但随着你深入阅读本书，你会更清楚地看到这一点。
- en: Real-life value iteration
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实生活中的价值迭代
- en: The improvements that we got in the FrozenLake environment by switching from
    the cross-entropy method to the value iteration method are quite encouraging,
    so it’s tempting to apply the value iteration method to more challenging problems.
    However, it is important to look at the assumptions and limitations that our value
    iteration method has. But let’s start with a quick recap of the method. On every
    step, the value iteration method does a loop on all states, and for every state,
    it performs an update of its value with a Bellman approximation. The variation
    of the same method for Q-values (values for actions) is almost the same, but we
    approximate and store values for every state and action. So what’s wrong with
    this process?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将交叉熵方法转换为价值迭代方法，我们在FrozenLake环境中取得的改进非常令人鼓舞，因此将价值迭代方法应用于更具挑战性的问题非常有吸引力。然而，重要的是要查看我们价值迭代方法的假设和局限性。但让我们先快速回顾一下这个方法。在每一步中，价值迭代方法会遍历所有状态，并对每个状态使用贝尔曼近似进行价值更新。对Q值（动作的价值）进行的相同方法变体几乎相同，但我们要为每个状态和动作近似并存储价值。那么，这个过程到底有什么问题呢？
- en: The first obvious problem is the count of environment states and our ability
    to iterate over them. In value iteration, we assume that we know all states in
    our environment in advance, can iterate over them, and can store their value approximations.
    It’s easy to do for the simple grid world environment of FrozenLake, but what
    about other tasks?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个明显的问题是环境状态的数量以及我们遍历它们的能力。在价值迭代中，我们假设我们事先知道环境中的所有状态，能够遍历它们，并且可以存储它们的价值近似值。对于FrozenLake的简单网格世界环境，这很容易做到，但对于其他任务呢？
- en: To understand this, let’s first look at how scalable the value iteration approach
    is, or, in other words, how many states we can easily iterate over in every loop.
    Even a moderate-sized computer can keep several billion float values in memory
    (8.5 billion in 32 GB of RAM), so the memory required for value tables doesn’t
    look like a huge constraint. Iteration over billions of states and actions will
    be more central processing unit (CPU)-demanding but is not an insurmountable problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，首先让我们看看价值迭代方法的可扩展性，换句话说，我们在每次循环中能够轻松遍历多少个状态。即使是中等大小的计算机，也可以在内存中存储数十亿个浮点值（在32GB的内存中是85亿），因此，价值表所需的内存似乎不是一个巨大的限制。遍历数十亿个状态和动作会更加消耗中央处理单元（CPU），但这并不是一个无法克服的问题。
- en: Nowadays, we have multicore systems that are mostly idle, so by using parallelism,
    we can iterate over billions of values in a reasonable amount of time. The real
    problem is the number of samples required to get good approximations for state
    transition dynamics. Imagine that you have some environment with, say, a billion
    states (which corresponds approximately to a FrozenLake of size 31600 × 31600).
    To calculate even a rough approximation for every state of this environment, we
    would need hundreds of billions of transitions evenly distributed over our states,
    which is not practical.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了多核系统，这些系统大多是闲置的，所以通过使用并行处理，我们可以在合理的时间内遍历数十亿个值。真正的问题在于，获取状态转换动态的良好近似所需的样本数量。假设你有一个环境，假设有十亿个状态（这大约对应一个
    31600 × 31600 大小的 FrozenLake）。要计算这个环境中每个状态的粗略近似，我们需要数百亿个状态之间均匀分布的转换，这在实践中是不可行的。
- en: To give you an example of an environment with an even larger number of potential
    states, let’s consider the Atari 2600 game console again. This was very popular
    in the 1980s, and many arcade-style games were available for it. The Atari console
    is archaic by today’s gaming standards, but its games provide an excellent set
    of RL problems that humans can master fairly quickly, yet are still challenging
    for computers. Not surprisingly, this platform (using an emulator, of course)
    is a very popular benchmark within RL research, as I mentioned.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个更大潜在状态数量的环境示例，我们再来看看 Atari 2600 游戏主机。这款主机在1980年代非常流行，并且有许多街机风格的游戏可供选择。虽然按照今天的游戏标准，Atari
    主机显得过时，但它的游戏提供了一组很好的强化学习（RL）问题，人类可以相对快速地掌握这些问题，但对计算机来说仍然具有挑战性。不足为奇的是，正如我之前提到的，这个平台（当然是使用模拟器）在强化学习研究中是一个非常受欢迎的基准。
- en: Let’s calculate the state space for the Atari platform. The resolution of the
    screen is 210 × 160 pixels, and every pixel has one of 128 colors. So every frame
    of the screen has 210 ⋅ 160 = 33600 pixels and the total number of different screens
    possible is 128^(33600), which is slightly more than 10^(70802). If we decide
    to just enumerate all possible states of the Atari once, it will take billions
    of billions of years even for the fastest supercomputer. Also, 99(.9)% of this
    job will be a waste of time, as most of the combinations will never be shown during
    even long gameplay, so we will never have samples of those states. However, the
    value iteration method wants to iterate over them just in case.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来计算一下 Atari 平台的状态空间。屏幕的分辨率是 210 × 160 像素，每个像素有 128 种颜色。因此，每一帧屏幕有 210 ⋅ 160
    = 33600 个像素，而所有可能的不同屏幕总数是 128^(33600)，约等于 10^(70802)。如果我们决定只列举一次 Atari 所有可能的状态，即使是最快的超级计算机也需要数十亿年。而且，这项工作中99(.9)%的时间都将是浪费，因为大多数组合在长时间的游戏过程中从未出现过，我们也不会有这些状态的样本。然而，值迭代方法仍然想遍历这些状态，以防万一。
- en: The second main problem with the value iteration approach is that it limits
    us to discrete action spaces. Indeed, both Q(s,a) and V (s) approximations assume
    that our actions are a mutually exclusive discrete set, which is not true for
    continuous control problems where actions can represent continuous variables,
    such as the angle of a steering wheel, the force on an actuator, or the temperature
    of a heater. This issue is much more challenging than the first, and we will talk
    about it in the last part of the book, in chapters dedicated to continuous action
    space problems. For now, let’s assume that we have a discrete count of actions
    and that this count is not very large (i.e., orders of 10s). How should we handle
    the state space size issue?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代方法的第二个主要问题是，它将我们限制在离散动作空间中。实际上，Q(s,a) 和 V(s) 的近似都假设我们的动作是一个相互排斥的离散集合，但对于连续控制问题来说，动作可以表示连续的变量，例如方向盘的角度、执行器上的力，或者加热器的温度，这在此类问题中并不成立。这个问题比第一个问题更具挑战性，我们将在书的最后部分，专门讨论连续动作空间问题的章节中进行讲解。现在，假设我们有一个离散的动作计数，并且这个计数不是很大（即数量级为10的数量）。我们应该如何处理状态空间大小的问题呢？
- en: Tabular Q-learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格 Q 学习
- en: The key question to focus on when trying to handle the state space issue is,
    do we really need to iterate over every state in the state space? We have an environment
    that can be used as a source of real-life samples of states. If some state in
    the state space is not shown to us by the environment, why should we care about
    its value? We can only use states obtained from the environment to update the
    values of states, which can save us a lot of work.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 处理状态空间问题时需要关注的关键问题是，我们是否真的需要遍历状态空间中的每个状态？我们有一个环境，可以用作现实生活中状态样本的来源。如果某个状态没有被环境展示出来，为什么我们还需要关心它的价值呢？我们只能使用从环境中获得的状态来更新状态值，这样可以节省大量的工作。
- en: 'This modification of the value iteration method is known as Q-learning, as
    mentioned earlier, and for cases with explicit state-to-value mappings, it entails
    the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种值迭代方法的修改被称为 Q 学习，对于具有明确状态到值映射的情况，它包括以下步骤：
- en: Start with an empty table, mapping states to values of actions.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个空表开始，将状态映射到动作的值。
- en: By interacting with the environment, obtain the tuple s, a, r, s′ (state, action,
    reward, and the new state). In this step, you need to decide which action to take,
    and there is no single proper way to make this decision. We discussed this problem
    as exploration versus exploitation in Chapter [1](ch005.xhtml#x1-190001) and will
    talk a lot about it in this chapter.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过与环境交互，获取元组 s, a, r, s′（状态、动作、奖励和新状态）。在此步骤中，你需要决定采取哪个动作，而且没有单一的正确方法来做出这个决策。我们在第
    [1](ch005.xhtml#x1-190001) 章中讨论过这个问题，探索与利用，并将在本章中深入讨论。
- en: 'Update the Q(s,a) value using the Bellman approximation:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Bellman 近似更新 Q(s,a) 的值：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq18.png)'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq18.png)'
- en: Repeat from step 2.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第 2 步开始重复。
- en: As in value iteration, the end condition could be some threshold of the update,
    or we could perform test episodes to estimate the expected reward from the policy.
    Another thing to note here is how to update the Q-values. As we take samples from
    the environment, it’s generally a bad idea to just assign new values on top of
    existing values, as training can become unstable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与值迭代一样，结束条件可以是某个更新的阈值，或者我们可以执行测试回合来估算策略的期望奖励。这里需要注意的另一点是如何更新 Q 值。当我们从环境中采样时，直接在现有值上分配新值通常是一个坏主意，因为训练可能会变得不稳定。
- en: 'What is usually done in practice is updating the Q(s,a) with approximations
    using a “blending” technique, which is just averaging between old and new values
    of Q using learning rate α with a value from 0 to 1:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中通常采用的方法是使用一种“混合”技术更新 Q(s,a)，即通过学习率 α 对 Q 的旧值和新值进行平均，α 的值在 0 到 1 之间：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq19.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq19.png)'
- en: 'This allows values of Q to converge smoothly, even if our environment is noisy.
    The final version of the algorithm is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 Q 值能够平稳收敛，即使我们的环境是嘈杂的。最终版本的算法如下：
- en: Start with an empty table for Q(s,a).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个空表开始，表示 Q(s,a)。
- en: Obtain (s, a, r, s′) from the environment.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中获取 (s, a, r, s′)。
- en: 'Make a Bellman update:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行 Bellman 更新：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq19.png)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq19.png)'
- en: Check convergence conditions. If not met, repeat from step 2.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查收敛条件。如果条件未满足，从第 2 步开始重复。
- en: 'As mentioned earlier, this method is called tabular Q-learning, as we keep
    a table of states with their Q-values. Let’s try it on our FrozenLake environment.
    The whole example code is in Chapter06/01_frozenlake_q_learning.py:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种方法被称为表格 Q 学习，因为我们维护一个包含状态及其 Q 值的表格。让我们在我们的 FrozenLake 环境中尝试一下。完整的示例代码在
    Chapter06/01_frozenlake_q_learning.py 中：
- en: 'First, we import packages and define constants and used types:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入包并定义常量和使用的类型：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The new thing here is the value of α, which will be used as the learning rate
    in the value update. The initialization of our Agent class is simpler now, as
    we don’t need to track the history of rewards and transition counters, just our
    value table. This will make our memory footprint smaller, which is not a big issue
    for FrozenLake but can be critical for larger environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的新内容是 α 的值，它将用作值更新中的学习率。我们现在的 Agent 类初始化更加简化，因为我们不再需要跟踪奖励和过渡计数的历史记录，只需要我们的值表。这将使我们的内存占用更小，虽然对于
    FrozenLake 来说这不是一个大问题，但对于更大的环境可能至关重要。
- en: 'The method sample_env is used to obtain the next transition from the environment:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 sample_env 用于从环境中获取下一个过渡：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We sample a random action from the action space and return the tuple of the
    old state, the action taken, the reward obtained, and the new state. The tuple
    will be used in the training loop later.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从动作空间中随机采样一个动作，并返回包含旧状态、所采取的动作、获得的奖励和新状态的元组。该元组将在后续的训练循环中使用。
- en: 'The next method receives the state of the environment:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个方法接收环境的状态：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This method finds the best action to take from the given state of the environment
    by taking the action with the largest value that we have in the table. If we don’t
    have the value associated with the state and action pair, then we take it as zero.
    This method will be used two times: first, in the test method that plays one episode
    using our current values table (to evaluate our policy’s quality), and second,
    in the method that performs the value update to get the value of the next state.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过选择在表格中具有最大值的动作来从给定的环境状态中找到最佳动作。如果我们没有与状态和动作对相关的值，则将其视为零。该方法将被使用两次：第一次，在测试方法中，它使用我们当前的值表进行一次回合（以评估我们的策略质量）；第二次，在执行值更新的方法中，用于获取下一个状态的值。
- en: 'Next, we update our values table using one step from the environment:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用环境中的一步操作来更新我们的值表：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we first calculate the Bellman approximation for our state, s, and action,
    a, by summing the immediate reward with the discounted value of the next state.
    Then, we obtain the previous value of the state and action pair and blend these
    values together using the learning rate. The result is the new approximation for
    the value of state s and action a, which is stored in our table.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先通过将即时奖励与下一个状态的折扣值相加来计算我们当前状态 s 和动作 a 的贝尔曼近似。然后，我们获得状态和动作对的先前值，并使用学习率将这些值混合在一起。结果是状态
    s 和动作 a 的新近似值，并将其存储在我们的表格中。
- en: 'The last method in our Agent class plays one full episode using the provided
    test environment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Agent 类中的最后一个方法使用提供的测试环境进行一次完整的回合：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The action on every step is taken using our current value table of Q-values.
    This method is used to evaluate our current policy to check the progress of learning.
    Note that this method doesn’t alter our value table; it only uses it to find the
    best action to take.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步的动作都是使用我们当前的 Q 值表来选择的。该方法用于评估我们当前的策略，以检查学习的进展。请注意，此方法不会更改我们的值表，它仅仅使用值表来找到最佳动作。
- en: 'The rest of the example is the training loop, which is very similar to examples
    from Chapter [5](ch009.xhtml#x1-820005): we create a test environment, agent,
    and summary writer, and then, in the loop, we do one step in the environment and
    perform a value update using the obtained data. Next, we test our current policy
    by playing several test episodes. If a good reward is obtained, then we stop training:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的其余部分是训练循环，类似于第 [5](ch009.xhtml#x1-820005) 章中的示例：我们创建一个测试环境、代理和摘要写入器，然后在循环中，我们在环境中进行一步操作，并使用获得的数据执行值更新。接下来，我们通过进行多个测试回合来测试我们当前的策略。如果获得了良好的奖励，则停止训练：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result of the example is shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的结果如下所示：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You may have noticed that this version used more iterations (but your experiment
    might have a different count of steps) to solve the problem compared to the value
    iteration method from the previous chapter. The reason for that is that we are
    no longer using the experience obtained during testing. In the example Chapter05/02_frozenlake_q_iteration.py,
    periodical tests caused an update of Q-table statistics. Here, we don’t touch
    Q-values during the test, which causes more iterations before the environment
    gets solved.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，与上一章的值迭代方法相比，这个版本使用了更多的迭代（但你的实验可能有不同的步骤数）来解决问题。原因在于我们不再使用在测试过程中获得的经验。在示例
    Chapter05/02_frozenlake_q_iteration.py 中，周期性测试导致 Q 表统计的更新。在这里，我们在测试过程中不触及 Q 值，这导致在环境解决之前需要更多的迭代。
- en: 'Overall, the total number of samples required from the environment is almost
    the same. The reward chart in TensorBoard also shows good training dynamics, which
    is very similar to the value iteration method (the reward plot for value iteration
    is shown in Figure [5.9](ch009.xhtml#x1-87114r9)):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，从环境中所需的样本总数几乎相同。TensorBoard 中的奖励图也显示了良好的训练动态，这与值迭代方法非常相似（值迭代的奖励图如图 [5.9](ch009.xhtml#x1-87114r9)
    所示）：
- en: '![PIC](img/B22150_06_01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_06_01.png)'
- en: 'Figure 6.1: Reward dynamics of FrozenLake'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：FrozenLake 的奖励动态
- en: In the next section, we will extend the Q-learning method with NNs’ preprocessing
    environment states. This will greatly extend the flexibility and applicability
    of the method we discussed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将扩展Q-learning方法，结合神经网络（NNs）对环境状态的预处理。这将极大地扩展我们讨论过的方法的灵活性和适用性。
- en: Deep Q-learning
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: The Q-learning method that we have just covered solves the issue of iteration
    over the full set of states, but it can still struggle with situations when the
    count of the observable set of states is very large. For example, Atari games
    can have a large variety of different screens, so if we decide to use raw pixels
    as individual states, we will quickly realize that we have too many states to
    track and approximate values for.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚介绍的Q-learning方法解决了遍历所有状态集的问题，但当可观察状态集的数量非常大时，它仍然可能遇到困难。例如，Atari游戏可能有许多不同的屏幕，如果我们决定将原始像素作为单独的状态，我们很快就会意识到我们有太多的状态需要追踪和估算值。
- en: In some environments, the count of different observable states could be almost
    infinite. For example, in CartPole, the environment gives us a state that is four
    floating point numbers. The number of value combinations is finite (they’re represented
    as bits), but this number is extremely large. With just bit values, it is around
    2^(4⋅32) ≈ 3.4 ⋅ 10^(38). In reality, it is less, as state values of the environment
    are bounded, so not all bit combinations of 4 float32 values are possible, but
    the resulting state space is still too large. We could create some bins to discretize
    those values, but this often creates more problems than it solves; we would need
    to decide what ranges of parameters are important to distinguish as different
    states and what ranges could be clustered together. As we’re trying to implement
    RL methods in a general way (without looking inside the environment’s internals),
    this is not a very promising direction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些环境中，不同的可观察状态的数量几乎是无限的。例如，在CartPole中，环境给我们提供的状态是四个浮动点数。数值组合的数量是有限的（它们以比特表示），但这个数字极其庞大。仅用比特值表示时，约为2^(4⋅32)
    ≈ 3.4 ⋅ 10^(38)。实际上，这个值会小一些，因为环境状态的值是有限制的，并非所有的4个float32值的比特组合都是可能的，但结果的状态空间依然太大。我们可以创建一些箱子来离散化这些值，但这通常会带来比解决更多问题；我们需要决定哪些参数范围重要，需要区分成不同的状态，而哪些范围可以归类在一起。由于我们尝试以一般的方式实现RL方法（而不深入了解环境的内部结构），这不是一个很有前景的方向。
- en: In the case of Atari, one single pixel change doesn’t make much difference,
    so we might want to treat similar images as one state. However, we still need
    to distinguish some of the states.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atari游戏的情况下，单一像素的变化并不会造成太大差异，因此我们可能希望将相似的图像视为一个状态。然而，我们仍然需要区分一些状态。
- en: The following image shows two different situations in a game of Pong. We’re
    playing against the artificial intelligence (AI) opponent by controlling a paddle
    (our paddle is on the right, whereas our opponent’s is on the left). The objective
    of the game is to get the bouncing ball past our opponent’s paddle, while preventing
    the ball from getting past our paddle. We can consider the two situations to be
    completely different. In the situation shown on the right, the ball is close to
    the opponent, so we can relax and watch. However, the situation on the left is
    more demanding; assuming that the ball is moving from left to right, the ball
    is moving toward our side, so we need to move our paddle quickly to avoid losing
    a point. The situations in Figure [6.2](#x1-93003r2) are just two from the 10^(70802)
    possible situations, but we want our agent to act on them differently.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了Pong游戏中的两种不同情况。我们正在与人工智能（AI）对手对战，通过控制一个挡板（我们的挡板在右侧，对手的挡板在左侧）。游戏的目标是将弹跳球送过对手的挡板，同时防止球从我们的挡板旁边飞过。我们可以认为这两种情况是完全不同的。在右侧展示的情况中，球靠近对手，因此我们可以放松并观察。然而，左侧的情况要求更高；假设球从左向右移动，球正朝我们的挡板移动，因此我们需要迅速移动我们的挡板，以避免失分。图[6.2](#x1-93003r2)中的两种情况只是10^(70802)种可能情况中的两种，但我们希望我们的智能体能对这些情况做出不同的反应。
- en: '![PIC](img/file32.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file32.png)'
- en: 'Figure 6.2: The ambiguity of observations in Pong. In the left image, the ball
    is moving to the right, toward our paddle, and on the right, its direction is
    opposite'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：Pong中观察的模糊性。在左侧的图像中，球正向右移动，朝着我们的挡板，而在右侧，它的方向相反。
- en: 'As a solution to this problem, we can use a nonlinear representation that maps
    both the state and action onto a value. In machine learning, this is called a
    “regression problem.” The concrete way to represent and train such a representation
    can vary, but, as you may have already guessed from this section’s title, using
    a deep NN is one of the most popular options, especially when dealing with observations
    represented as screen images. With this in mind, let’s make modifications to the
    Q-learning algorithm:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为该问题的解决方案，我们可以使用一个非线性表示，将状态和动作映射到一个值。在机器学习中，这称为“回归问题”。表示和训练这种表示的具体方法可以有所不同，但正如你从本节标题中已经猜到的那样，使用深度神经网络（NN）是最流行的选择之一，尤其是当处理以屏幕图像表示的观察时。考虑到这一点，我们对Q-learning算法进行修改：
- en: Initialize Q(s,a) with some initial approximation.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一些初始近似值初始化Q(s,a)。
- en: By interacting with the environment, obtain the tuple (s, a, r, s′).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过与环境交互，获得元组(s, a, r, s′)。
- en: 'Calculate the loss:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq20.png)![π (a |s) = P[At = a|St = s]
    ](img/eq21.png)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq20.png)![π (a |s) = P[At = a|St = s]
    ](img/eq21.png)'
- en: Update Q(s,a) using the stochastic gradient descent (SGD) algorithm, by minimizing
    the loss with respect to the model parameters.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用随机梯度下降（SGD）算法更新Q(s,a)，通过最小化关于模型参数的损失来进行更新。
- en: Repeat from step 2 until converged.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤2开始重复，直到收敛。
- en: This algorithm looks simple, but, unfortunately, it won’t work very well. Let’s
    discuss some of the aspects that could go wrong and the potential ways we could
    approach these scenarios.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法看起来很简单，但不幸的是，它的效果并不好。让我们讨论一些可能出错的方面，以及我们可以如何处理这些情况。
- en: Interaction with the environment
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与环境交互
- en: First of all, we need to interact with the environment somehow to receive data
    to train on. In simple environments, such as FrozenLake, we can act randomly,
    but is this the best strategy to use? Imagine the game of Pong. What’s the probability
    of winning a single point by randomly moving the paddle? It’s not zero, but it’s
    extremely small, which just means that we will need to wait for a very long time
    for such a rare situation. As an alternative, we can use our Q-function approximation
    as a source of behavior (as we did before in the value iteration method, when
    we remembered our experience during testing).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要以某种方式与环境交互，以接收数据进行训练。在简单的环境中，比如FrozenLake，我们可以随机行动，但这真的是最好的策略吗？想象一下Pong游戏。通过随机移动挡板，获得一个单独得分的概率是多少？这个概率不是零，但它极其小，这意味着我们需要等待很长时间，才能遇到这种罕见的情况。作为替代方案，我们可以使用Q函数的近似值作为行为的来源（就像我们在价值迭代方法中做的那样，当时我们在测试期间记住了自己的经验）。
- en: If our representation of Q is good, then the experience that we get from the
    environment will show the agent relevant data to train on. However, we’re in trouble
    when our approximation is not perfect (at the beginning of the training, for example).
    In such a case, our agent can be stuck with bad actions for some states without
    ever trying to behave differently. This is the exploration versus exploitation
    dilemma mentioned briefly in Chapter [1](ch005.xhtml#x1-190001), which we will
    discuss in detail now. On the one hand, our agent needs to explore the environment
    to build a complete picture of transitions and action outcomes. On the other hand,
    we should use interaction with the environment efficiently; we shouldn’t waste
    time by randomly trying actions that we have already tried and learned outcomes
    for.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的Q表示是好的，那么从环境中获得的经验将向代理提供相关的数据用于训练。然而，当我们的近似不完美时（例如在训练的初期），我们就会遇到问题。在这种情况下，我们的代理可能会在某些状态下一直采取错误的行为，而从未尝试过不同的行为。这就是在第[1](ch005.xhtml#x1-190001)章中简要提到的探索与利用困境，我们将在此详细讨论。一方面，我们的代理需要探索环境，以建立完整的转移和动作结果的图景。另一方面，我们应当高效地利用与环境的交互；我们不应浪费时间随机尝试我们已经尝试过并且已知结果的动作。
- en: As you can see, random behavior is better at the beginning of the training when
    our Q approximation is bad, as it gives us more uniformly distributed information
    about the environment states. As our training progresses, random behavior becomes
    inefficient, and we want to fall back to our Q approximation to decide how to
    act.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在训练初期，当我们的Q近似值较差时，随机行为反而更好，因为它能提供更多均匀分布的环境状态信息。随着训练的进展，随机行为变得低效，我们希望回归到Q近似值上，以决定如何行动。
- en: A method that performs such a mix of two extreme behaviors is known as an epsilon-greedy
    method, which just means switching between random and Q policy using the probability
    hyperparameter 𝜖. By varying 𝜖, we can select the ratio of random actions. The
    usual practice is to start with 𝜖 = 1.0 (100% random actions) and slowly decrease
    it to some small value, such as 5% or 2% random actions. Using an epsilon-greedy
    method helps us to both explore the environment in the beginning and stick to
    good policy at the end of the training. There are other solutions to the exploration
    versus exploitation problem, and we will discuss some of them in the third part
    of the book. This problem is one of the fundamental open questions in RL and an
    active area of research that is not even close to being resolved completely.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这种两种极端行为混合的方法被称为epsilon-贪婪方法，意思是使用概率超参数𝜖在随机行为和Q策略之间切换。通过改变𝜖的值，我们可以选择随机行为的比例。通常的做法是从𝜖
    = 1.0（100%的随机行为）开始，并逐渐将其减少到一个较小的值，如5%或2%的随机行为。使用epsilon-贪婪方法可以帮助我们在训练初期探索环境，并在训练结束时坚持好的策略。还有其他解决探索与利用问题的方法，我们将在本书的第三部分讨论其中的一些。这个问题是强化学习（RL）中的一个基础性未解问题，也是一个仍在积极研究的领域，离完全解决还远。
- en: SGD optimization
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SGD优化
- en: The core of our Q-learning procedure is borrowed from supervised learning. Indeed,
    we are trying to approximate a complex, nonlinear function, Q(s,a), with an NN.
    To do this, we must calculate targets for this function using the Bellman equation
    and then pretend that we have a supervised learning problem at hand. That’s okay,
    but one of the fundamental requirements for SGD optimization is that the training
    data is independent and identically distributed (frequently abbreviated as iid),
    which means that our training data is randomly sampled from the underlying dataset
    we’re trying to learn on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Q学习过程的核心借鉴了监督学习。事实上，我们正试图用神经网络（NN）来逼近一个复杂的非线性函数Q(s,a)。为此，我们必须使用贝尔曼方程计算该函数的目标值，然后假装我们面临的是一个监督学习问题。这是可以的，但SGD优化的一个基本要求是训练数据是独立同分布的（通常缩写为iid），这意味着我们的训练数据是从我们试图学习的底层数据集中随机抽样的。
- en: 'In our case, data that we are going to use for the SGD update doesn’t fulfill
    these criteria:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将用于SGD更新的数据不符合这些标准：
- en: Our samples are not independent. Even if we accumulate a large batch of data
    samples, they will all be very close to each other, as they will belong to the
    same episode.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的样本不是独立的。即使我们累积了大量的数据样本，它们之间也会非常接近，因为它们都属于同一个回合。
- en: 'Distribution of our training data won’t be identical to samples provided by
    the optimal policy that we want to learn. Data that we have will be a result of
    some other policy (our current policy, a random one, or both in the case of epsilon-greedy),
    but we don’t want to learn how to play randomly: we want an optimal policy with
    the best reward.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的训练数据的分布将不完全等同于我们想要学习的最优策略提供的样本。我们拥有的数据将是其他策略（我们的当前策略、随机策略，或在epsilon-贪婪情况下的两者结合）的结果，但我们并不想学习如何随机地行动：我们希望获得一个具有最佳奖励的最优策略。
- en: To deal with this nuisance, we usually need to use a large buffer of our past
    experience and sample training data from it, instead of using our latest experience.
    This technique is called a replay buffer. The simplest implementation is a buffer
    of a fixed size, with new data added to the end of the buffer so that it pushes
    the oldest experience out of it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一难题，我们通常需要使用一个包含我们过去经验的大缓冲区，并从中抽取训练数据，而不是仅使用我们最新的经验。这种技术叫做回放缓冲区。最简单的实现方式是一个固定大小的缓冲区，新数据被添加到缓冲区的末尾，从而将最旧的经验推出缓冲区之外。
- en: The replay buffer allows us to train on more-or-less independent data, but the
    data will still be fresh enough to train on samples generated by our recent policy.
    In Chapter [8](ch012.xhtml#x1-1240008), we will check another kind of replay buffer,
    prioritized, which provides a more sophisticated sampling approach.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 回放缓冲区允许我们在或多或少独立的数据上进行训练，但这些数据仍然足够新鲜，可以用于训练我们最近的策略生成的样本。在第[8](ch012.xhtml#x1-1240008)章中，我们将检查另一种回放缓冲区，优先级回放，它提供了一种更复杂的采样方法。
- en: Correlation between steps
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤之间的相关性
- en: Another practical issue with the default training procedure is also related
    to the lack of iid data, but in a slightly different manner. The Bellman equation
    provides us with the value of Q(s,a) via Q(s′,a′) (this process is called bootstrapping,
    when we use the formula recursively). However, both the states s and s′ have only
    one step between them. This makes them very similar, and it’s very hard for NNs
    to distinguish between them. When we perform an update of our NNs’ parameters
    to make Q(s,a) closer to the desired result, we can indirectly alter the value
    produced for Q(s′,a′) and other states nearby. This can make our training very
    unstable, like chasing our own tail; when we update Q for state s, then on subsequent
    states, we will discover that Q(s′,a′) becomes worse but attempts to update it
    can spoil our Q(s,a) approximation even more, and so on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 默认训练过程的另一个实际问题也与缺乏独立同分布（iid）数据有关，但方式稍有不同。贝尔曼方程通过Q(s′,a′)为我们提供Q(s,a)的值（这个过程称为自举，当我们递归使用该公式时）。然而，状态s和s′之间只有一步之遥。这使得它们非常相似，神经网络很难区分它们。当我们更新神经网络的参数，使Q(s,a)更接近预期结果时，我们可能会间接地改变Q(s′,a′)和附近其他状态的值。这可能导致我们的训练非常不稳定，就像在追逐自己的尾巴；当我们更新状态s的Q值时，在随后的状态中，我们会发现Q(s′,a′)变得更糟，但尝试更新它可能会进一步破坏Q(s,a)的近似，依此类推。
- en: To make training more stable, there is a trick, called target network, by which
    we keep a copy of our network and use it for the Q(s′,a′) value in the Bellman
    equation. This network is synchronized with our main network only periodically,
    for example, once in N steps (where N is usually quite a large hyperparameter,
    such as 1k or 10k training iterations).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使训练更加稳定，有一个技巧叫做目标网络，我们保留一份网络的副本，并用它来计算贝尔曼方程中的Q(s′,a′)值。这个网络与我们的主网络只会定期同步，例如，每隔N步同步一次（其中N通常是一个较大的超参数，如1k或10k训练迭代）。
- en: The Markov property
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫性质
- en: 'Our RL methods use Markov decision process (MDP) formalism as their basis,
    which assumes that the environment obeys the Markov property: observations from
    the environment are all that we need to act optimally. In other words, our observations
    allow us to distinguish states from one another.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的强化学习方法以马尔可夫决策过程（MDP）形式主义为基础，假设环境遵循马尔可夫性质：来自环境的观察是我们采取最优行动所需的全部信息。换句话说，我们的观察允许我们区分不同的状态。
- en: As you saw from the preceding Pong screenshot in Figure [6.2](#x1-93003r2),
    one single image from the Atari game is not enough to capture all the important
    information (using only one image, we have no idea about the speed and direction
    of objects, like the ball and our opponent’s paddle). This obviously violates
    the Markov property and moves our single-frame Pong environment into the area
    of partially observable MDPs (POMDPs). A POMDP is basically an MDP without the
    Markov property, and it is very important in practice. For example, for most card
    games in which you don’t see your opponents’ cards, game observations are POMDPs
    because the current observation (i.e., your cards and the cards on the table)
    could correspond to different cards in your opponents’ hands.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从前面的Pong游戏截图（图[6.2](#x1-93003r2)）所见，一张来自Atari游戏的单一图像不足以捕获所有重要信息（仅使用一张图像，我们无法知道物体的速度和方向，比如球和我们对手的挡板）。这显然违反了马尔可夫性质，并将我们的单帧Pong环境移入部分可观察马尔可夫决策过程（POMDPs）的范畴。POMDP基本上是没有马尔可夫性质的MDP，它在实际应用中非常重要。例如，在大多数扑克牌游戏中，你无法看到对手的牌，这些游戏观察就是POMDP，因为当前的观察（即你手中的牌和桌面上的牌）可能对应对手手中的不同牌。
- en: We won’t discuss POMDPs in detail in this book, but we will use a small technique
    to push our environment back into the MDP domain. The solution is maintaining
    several observations from the past and using them as a state. In the case of Atari
    games, we usually stack k subsequent frames together and use them as the observation
    at every state. This allows our agent to deduct the dynamics of the current state,
    for instance, to get the speed of the ball and its direction. The usual “classical”
    number of k for Atari is four. Of course, it’s just a hack, as there can be longer
    dependencies in the environment, but for most of the games, it works well.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们不会详细讨论部分可观察马尔可夫决策过程（POMDPs），但我们会使用一个小技巧将我们的环境推回到MDP领域。解决方案是维护过去的多个观察，并将它们用作状态。在Atari游戏的情况下，我们通常将k个连续的帧堆叠在一起，并将它们作为每个状态的观察。这让我们的智能体能够推断出当前状态的动态，例如，获取球的速度和方向。对于Atari游戏，通常的“经典”k值是四。当然，这只是一个技巧，因为环境中可能存在更长的依赖关系，但对于大多数游戏来说，它表现得很好。
- en: The final form of DQN training
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN训练的最终形式
- en: There are many more tips and tricks that researchers have discovered to make
    DQN training more stable and efficient, and we will cover the best of them in
    Chapter [8](ch012.xhtml#x1-1240008). However, epsilon-greedy, the replay buffer,
    and the target network form a basis that has allowed the company DeepMind to successfully
    train a DQN on a set of 49 Atari games, demonstrating the efficiency of this approach
    when applied to complicated environments.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经发现了许多技巧，使得 DQN 训练更加稳定和高效，我们将在第[8](ch012.xhtml#x1-1240008)章介绍其中最好的方法。然而，epsilon-greedy
    策略、回放缓冲区和目标网络构成了基础，使得 DeepMind 公司能够成功地在 49 款 Atari 游戏上训练 DQN，展示了这种方法在复杂环境中的效率。
- en: The original paper Playing Atari with deep reinforcement learning [[Mni13](#)]
    (without a target network) was published at the end of 2013 and used seven games
    for testing. Later, at the beginning of 2015, a revised version of the article
    with the title Human-level control through deep reinforcement learning [[Mni+15](#)],
    already with 49 different games, was published in Nature.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文《Playing Atari with deep reinforcement learning》[[Mni13](#)]（没有目标网络）发布于
    2013 年底，测试使用了七款游戏。后来，在 2015 年初，文章经过修订，标题为《Human-level control through deep reinforcement
    learning》[[Mni+15](#)]，此时已使用了 49 款不同的游戏，并发表于《自然》杂志。
- en: 'The algorithm for DQN from the preceding papers has the following steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 来自前述论文的 DQN 算法步骤如下：
- en: Initialize parameters for Q(s,a) and Q̂(s,a) with random weights, 𝜖 ← 1.0, and
    empty the replay buffer.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机权重初始化 Q(s,a) 和 Q̂(s,a) 的参数，𝜖 ← 1.0，并清空回放缓冲区。
- en: With probability 𝜖, select a random action a; otherwise, a = arg max[a]Q(s,a).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以概率 𝜖 选择一个随机动作 a；否则，a = arg max[a]Q(s,a)。
- en: Execute action a in an emulator and observe the reward, r, and the next state,
    s′.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模拟器中执行动作 a，并观察奖励 r 和下一个状态 s′。
- en: Store the transition (s, a, r, s′) in the replay buffer.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将过渡 (s, a, r, s′) 存储到回放缓冲区中。
- en: Sample a random mini-batch of transitions from the replay buffer.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从回放缓冲区中随机抽取一个小批量的过渡。
- en: 'For every transition in the buffer, calculate the target:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于缓冲区中的每个过渡，计算目标：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq22.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq23.png)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq22.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq23.png)'
- en: 'Calculate the loss: ℒ = (Q(s,a) −y)².'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失：ℒ = (Q(s,a) −y)²。
- en: Update Q(s,a) using the SGD algorithm by minimizing the loss in respect to the
    model parameters.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过最小化损失相对于模型参数，使用 SGD 算法更新 Q(s,a)。
- en: Every N steps, copy weights from Q to Q̂.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每隔 N 步，从 Q 复制权重到 Q̂。
- en: Repeat from step 2 until converged.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第 2 步开始重复，直到收敛。
- en: Let’s implement this algorithm now and try to beat some of the Atari games!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现这个算法，并尝试击败一些 Atari 游戏！
- en: DQN on Pong
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 在 Pong 游戏中的应用
- en: Before we jump into the code, some introduction is needed. Our examples are
    becoming increasingly challenging and complex, which is not surprising, as the
    complexity of the problems that we are trying to tackle is also growing. The examples
    are as simple and concise as possible, but some of the code may be difficult to
    understand at first.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始代码之前，需要进行一些介绍。我们的示例变得越来越具有挑战性和复杂性，这并不奇怪，因为我们要解决的问题的复杂性也在增加。尽管例子尽可能简单简洁，但有些代码初看可能难以理解。
- en: Another thing to note is performance. Our previous examples for FrozenLake,
    or CartPole, were not demanding from a resource perspective, as observations were
    small, NN parameters were tiny, and shaving off extra milliseconds in the training
    loop wasn’t important. However, from now on, that’s not the case. One single observation
    from the Atari environment is 100k values, which have to be preprocessed, rescaled,
    and stored in the replay buffer. One extra copy of this data array can cost you
    training speed, which will not be seconds and minutes anymore but, instead, hours
    on even the fastest graphics processing unit (GPU) available.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的事项是性能。我们之前的例子（例如 FrozenLake 或 CartPole）从资源角度来看并不苛刻，因为观察值较小，神经网络参数也很小，训练循环中的额外毫秒并不重要。然而，从现在开始，情况就不同了。来自
    Atari 环境的每个观察值有 10 万个数据点，这些数据需要预处理、重新缩放并存储在回放缓冲区中。多一份数据副本可能会影响训练速度，这不再是秒和分钟的问题，而是即使是最快的图形处理单元（GPU）也可能需要数小时。
- en: The NN training loop could also be a bottleneck. Of course, RL models are not
    as huge monsters as state-of-the-art large language models (LLMs), but even the
    DQN model from 2015 has more than 1.5M parameters, which has to be adjusted millions
    of times. So, to cut a long story short, performance matters, especially when
    you are experimenting with hyperparameters and need to wait not for a single model
    to train but dozens of them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NN）训练循环也可能成为瓶颈。当然，强化学习模型并不像最先进的大型语言模型（LLM）那样庞大，但即便是2015年的DQN模型也有超过150万个参数，需要调整数百万次。因此，简而言之，性能非常重要，尤其是在你进行超参数实验时，不仅需要等待一个模型训练完成，而是几十个模型。
- en: PyTorch is quite expressive, so more-or-less efficient processing code could
    look much less cryptic than optimized TensorFlow graphs, but there is still a
    significant opportunity to do things slowly and make mistakes. For example, a
    naïve version of DQN loss computation, which loops over every batch sample, is
    about two times slower than a parallel version. However, a single extra copy of
    the data batch could make the speed of the same code 13 times slower, which is
    quite significant.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch相当具有表现力，因此效率较高的处理代码看起来通常不如优化过的TensorFlow图那么晦涩，但仍然存在很大机会做得很慢并犯错误。例如，一个简单版的DQN损失计算，它对每个批次样本进行循环处理，比并行版本慢大约两倍。然而，仅仅是对数据批次做一个额外的副本，就会使得相同代码的速度变慢13倍，这非常显著。
- en: 'This example has been split into three modules due to its length, logical structure,
    and reusability. The modules are as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其长度、逻辑结构和可重用性，该示例被拆分为三个模块。模块如下：
- en: 'Chapter06/lib/wrappers.py: These are Atari environment wrappers, mostly taken
    from the Stable Baselines3 (SB3) project: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapter06/lib/wrappers.py：这些是Atari环境的包装器，主要来自Stable Baselines3（SB3）项目：[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)。
- en: 'Chapter06/lib/dqn_model.py: This is the DQN NN layer, with the same architecture
    as the DeepMind DQN from the Nature paper.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapter06/lib/dqn_model.py：这是DQN神经网络层，其架构与DeepMind在《Nature》论文中的DQN相同。
- en: 'Chapter06/02_dqn_pong.py: This is the main module, with the training loop,
    loss function calculation, and experience replay buffer.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapter06/02_dqn_pong.py：这是主要模块，包含训练循环、损失函数计算和经验回放缓冲区。
- en: Wrappers
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装器
- en: Tackling Atari games with RL is quite demanding from a resource perspective.
    To make things faster, several transformations are applied to the Atari platform
    interaction, which are described in DeepMind’s paper. Some of these transformations
    influence only performance, but some address Atari platform features that make
    learning long and unstable. Transformations are implemented as Gym wrappers of
    various kinds. The full list is quite lengthy and there are several implementations
    of the same wrappers in various sources. My personal favorite is the SB3 repository,
    which is an evolution of OpenAI Baselines code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强化学习（RL）解决Atari游戏在资源方面是相当有挑战的。为了加快速度，针对Atari平台的交互应用了几种转换，这些转换在DeepMind的论文中有详细描述。部分转换仅影响性能，而有些则是解决Atari平台的特性，这些特性使得学习过程既漫长又不稳定。转换通过不同种类的Gym包装器来实现。完整的列表相当长，并且同一个包装器有多个实现版本来自不同来源。我的个人偏好是SB3仓库，它是OpenAI
    Baselines代码的演变版本。
- en: 'SB3 includes lots of RL methods implemented using PyTorch and is supposed to
    be a unifying benchmark to compare various methods. At the moment, we’re not interested
    in those methods’ implementation (we’re going to reimplement most of them ourselves),
    but some wrappers are very useful. The repository is avaliable at [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)
    and wrappers are documented at [https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml](https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml).
    The list of the most popular Atari transformations used by RL researchers includes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SB3包含大量使用PyTorch实现的强化学习方法，旨在作为一个统一的基准，比较各种方法。目前，我们对这些方法的实现不感兴趣（我们打算自己重新实现大多数方法），但一些包装器非常有用。该仓库可以在[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)找到，包装器的文档可以在[https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml](https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml)查看。强化学习研究人员常用的Atari转换列表包括：
- en: 'Converting individual lives in the game into separate episodes: In general,
    an episode contains all the steps from the beginning of the game until the “Game
    over” screen, which can last for thousands of game steps (observations and actions).
    Usually, in arcade games, the player is given several lives, which provide several
    attempts in the game. This transformation splits a full episode into individual
    small episodes for every life that a player has. Internally, this is implemented
    as checking an emulator’s information about remaining lives. Not all games support
    this feature (although Pong does), but for the supported environments, it usually
    helps to speed up convergence, as our episodes become shorter. This logic is implemented
    in the EpisodicLifeEnv wrapper in SB3 code.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将游戏中的每个生命转化为单独的回合：一般来说，一个回合包含从游戏开始到“游戏结束”画面所有步骤，这可能会持续数千个游戏步骤（观察和动作）。通常，在街机游戏中，玩家会获得几条命，这提供了几次游戏尝试。这种转换将一个完整回合拆分为每条命对应的单独小回合。在内部，这是通过检查模拟器关于剩余生命的信息来实现的。并非所有游戏都支持此功能（尽管乒乓球游戏支持），但对于支持的环境，这通常有助于加速收敛，因为我们的回合变得更短。此逻辑在SB3代码中的EpisodicLifeEnv包装类中得到了实现。
- en: 'At the beginning of the game, performing a random amount (up to 30) of empty
    actions (also called “no-op”): This skips intro screens in some Atari games, which
    are not relevant for the gameplay. It is implemented in the NoopResetEnv wrapper.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在游戏开始时执行一个随机数量（最多30次）的空操作（也称为“无操作”）：这跳过了一些雅达利游戏中的介绍画面，这些画面与游戏玩法无关。它在NoopResetEnv包装类中得到了实现。
- en: 'Making an action decision every K steps, where K is usually 3 or 4: On intermediate
    frames, the chosen action is simply repeated. This allows training to speed up
    significantly, as processing every frame with an NN is quite a demanding operation,
    but the difference between consequent frames is usually minor. This is implemented
    in the MaxAndSkipEnv wrapper, which also includes the next transformation in the
    list (the maximum between two frames).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每K步做一次动作决策，其中K通常是3或4：在中间帧上，所选的动作会被简单地重复。这使得训练能够显著加速，因为使用神经网络处理每一帧是一个非常费时的操作，但连续帧之间的差异通常较小。这在MaxAndSkipEnv包装类中得到了实现，该类也包含列表中的下一个转换（两帧之间的最大值）。
- en: 'Taking the maximum of every pixel in the last two frames and using it as an
    observation: Some Atari games have a flickering effect, which is due to the platform’s
    limitation. (Atari has a limited number of sprites that can be shown on a single
    frame.) For the human eye, such quick changes are not visible, but they can confuse
    NNs.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取每个像素在最后两帧中的最大值并作为观察值：一些雅达利游戏存在闪烁效果，这是由于平台的限制。（雅达利每帧上可以显示的精灵数量是有限的。）对于人眼来说，这种快速变化是不可见的，但它们可能会干扰神经网络（NN）。
- en: 'Pressing FIRE at the beginning of the game: Some games (including Pong and
    Breakout) require a user to press the FIRE button to start the game. Without this,
    the environment becomes a POMDP, as from observation, an agent cannot tell whether
    FIRE was already pressed. This is implemented in the FireResetEnv wrapper class.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在游戏开始时按下FIRE键：某些游戏（包括乒乓球和打砖块）需要用户按下FIRE按钮才能开始游戏。如果没有按下该按钮，环境将变为部分可观测的马尔可夫决策过程（POMDP），因为从观察中，代理无法判断是否已经按下了FIRE键。这在FireResetEnv包装类中得到了实现。
- en: 'Scaling every frame down from 210 × 160, with three color frames, to a single-color
    84 × 84 image: Different approaches are possible. For example, the DeepMind paper
    describes this transformation as taking the Y-color channel from the YCbCr color
    space and then rescaling the full image to an 84 × 84 resolution. Some other researchers
    do grayscale transformation, cropping non-relevant parts of an image and then
    scaling down. In the SB3 repository, the latter approach is used. This is implemented
    in the WarpFrame wrapper class.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每帧从210 × 160的三色图像缩放为单色的84 × 84图像：有不同的方法可以实现。例如，DeepMind的论文将此转换描述为从YCbCr色彩空间中提取Y色通道，然后将整个图像重新缩放为84
    × 84的分辨率。其他一些研究人员进行灰度转换，裁剪掉图像中不相关的部分然后进行缩放。在SB3的代码库中，使用了后一种方法。这在WarpFrame包装类中得到了实现。
- en: 'Stacking several (usually four) subsequent frames together to give the network
    information about the dynamics of the game’s objects: This approach was already
    discussed as a quick solution to the lack of game dynamics in a single game frame.
    There is no wrapper in the SB3 project, I implemented my version in wrappers.BufferWrapper.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个（通常是四个）连续的帧堆叠在一起，以向网络提供游戏中物体动态的信息：这种方法已经作为解决单一游戏帧缺乏游戏动态的快速方案进行了讨论。在SB3项目中没有现成的包装类，我在wrappers.BufferWrapper中实现了我的版本。
- en: 'Clipping the reward to -1, 0, and 1 values: The obtained score can vary wildly
    among the games. For example, in Pong, you get a score of 1 for every ball you
    pass behind the opponent’s paddle. However, in some games, like KungFuMaster,
    you get a reward of 100 for every enemy killed. This spread in reward values makes
    our loss have completely different scales between the games, which makes it harder
    to find common hyperparameters for a set of games. To fix this, the reward just
    gets clipped to the range −1…1\. This is implemented in the ClipRewardEnv wrapper.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将奖励裁剪到 -1、0 和 1 的值：获得的分数在不同游戏之间可能差异很大。例如，在 Pong 游戏中，每当你将球打过对方的挡板时，你会获得 1 分。然而，在某些游戏中，如
    KungFuMaster，每杀死一个敌人你会获得 100 分。奖励值的这种差异使得我们在不同游戏之间的损失函数尺度完全不同，这使得找到适用于一组游戏的通用超参数变得更加困难。为了解决这个问题，奖励被裁剪到
    −1 到 1 的范围内。这在 ClipRewardEnv 封装器中实现。
- en: 'Rearrange observation dimensions to meet the PyTorch convolution layer: As
    we’re going to use convolution, our tensors have to be rearranged the way PyTorch
    expects them. The Atari environment returns the observation in a (height, width,
    color) format, but the PyTorch convolution layer wants the channel layer to come
    first. This is implemented in wrappers.ImageToPyTorch.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重排观察维度以满足 PyTorch 卷积层的要求：由于我们将使用卷积，张量需要按 PyTorch 期望的方式进行重排。Atari 环境以（高度，宽度，颜色）的格式返回观察数据，但
    PyTorch 卷积层要求通道维度排在最前面。这在 wrappers.ImageToPyTorch 中得以实现。
- en: Most of those wrappers are implemented in the stable-baseline3 library, which
    provides the AtariWrapper class that applies wrappers in the required order, according
    to the constructor’s parameters. It also detects the underlying environment properties
    and enables FireResetEnv if needed. Not all of the wrappers are required for the
    Pong game, but you should be aware of existing wrappers, just in case you decide
    to experiment with other games. Sometimes, when the DQN does not converge, the
    problem is not in the code but in the wrongly wrapped environment. I once spent
    several days debugging convergence issues, which were caused by missing the FIRE
    button press at the beginning of a game!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些封装器都在 stable-baseline3 库中实现，库中提供了 AtariWrapper 类，它根据构造函数的参数按需要的顺序应用封装器。它还会检测底层环境的属性，并在需要时启用
    FireResetEnv。并非所有封装器都需要在 Pong 游戏中使用，但你应该了解现有的封装器，以防你决定尝试其他游戏。有时，当 DQN 不收敛时，问题并不在代码中，而是环境封装错误。我曾经花了几天时间调试收敛问题，结果是因为在游戏开始时没有按下
    FIRE 按钮！
- en: 'Let’s take a look at the implementation of individual wrappers. We will start
    with classes provided by stable-baseline3:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看各个封装器的实现。我们将从 stable-baseline3 提供的类开始：
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding wrapper presses the FIRE button in environments that require that
    for the game to start. In addition to pressing FIRE, this wrapper checks for several
    corner cases that are present in some games.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述封装器在需要按下 FIRE 按钮才能开始游戏的环境中按下该按钮。除了按下 FIRE 按钮外，这个封装器还会检查一些在某些游戏中存在的边缘情况。
- en: 'This wrapper combines the repetition of actions during K frames and pixels
    from two consecutive frames:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个封装器结合了在 K 帧内重复执行的动作和来自两帧之间的像素信息：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The goal of the following wrapper is to convert input observations from the
    emulator, which has a resolution of 210 × 160 pixels with RGB color channels,
    to a grayscale 84 × 84 image. It does this using CV2 library’s function cvtColor,
    which does a colorimetric grayscale conversion (which is closer to human color
    perception than a simple averaging of color channels), and then the image is resized:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下封装器的目标是将来自模拟器的输入观察转换为一个分辨率为 210 × 160 像素并具有 RGB 颜色通道的图像，转换为一个灰度 84 × 84 的图像。它通过使用
    CV2 库中的 cvtColor 函数来实现该操作，cvTColor 函数执行的是色度灰度转换（这种转换比简单的颜色通道平均更接近人类的颜色感知），然后图像会被缩放：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So far, we’ve used wrappers from stable-baseline3 (I skipped EpisodicLifeEnv
    wrapper, as it is a bit complicated and not very relevant); you can find the code
    of other available wrappers in the repo stable_baselines3/common/atari_wrappers.py.
    Now, let’s check out two wrappers from lib/wrappers.py:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了 stable-baseline3 中的封装器（我跳过了 EpisodicLifeEnv 封装器，因为它有点复杂且与此不太相关）；你可以在仓库
    stable_baselines3/common/atari_wrappers.py 中找到其他可用封装器的代码。现在，让我们来看一下来自 lib/wrappers.py
    中的两个封装器：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The BufferWrapper class creates a stack (implemented with the deque class) of
    subsequent frames along the first dimension and returns them as an observation.
    The purpose is to give the network an idea about the dynamics of the objects,
    such as the speed and direction of the ball in Pong or how enemies are moving.
    This is very important information, which it is not possible to obtain from a
    single image.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: BufferWrapper类创建了一个堆栈（使用deque类实现），沿着第一维度堆叠随后的帧，并将它们作为观察值返回。目的是让网络了解物体的动态信息，比如乒乓球的速度和方向，或者敌人是如何移动的。这些信息非常重要，是从单一图像中无法获得的。
- en: One very important but not very obvious detail about this wrapper is that the
    observation method returns the copy of our buffered observations. This is very
    important, as we’re going to keep our observations in the replay buffer, so the
    copy is needed to avoid buffer modification on the future environment’s steps.
    In principle, we can avoid making a copy (and reduce our memory footprint four
    times) by keeping the episodes’ observations and indices in them, but it will
    require much more sophisticated data structure management. What is important currently
    is that this wrapper has to be the last in the chain of the wrappers applied to
    the environment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个包装器有一个非常重要但不太显眼的细节，那就是观察方法返回的是我们缓冲区中观察值的副本。这一点非常重要，因为我们要将观察值保存在重放缓冲区中，因此需要副本以避免未来环境步骤中对缓冲区的修改。从原则上讲，我们可以通过在其中保存回合的观察值和它们的索引来避免制作副本（并将内存占用减少四倍），但这需要更加复杂的数据结构管理。目前需要注意的是，这个包装器必须是应用于环境的包装器链中的最后一个。
- en: 'The last wrapper is ImageToPyTorch, and it changes the shape of the observation
    from height, width, channel (HWC) to the channel, height, width (CHW) format required
    by PyTorch:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的包装器是ImageToPyTorch，它将观察值的形状从高度、宽度、通道（HWC）格式转换为PyTorch所需的通道、高度、宽度（CHW）格式：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The input shape of the tensor has a color channel as the last dimension, but
    PyTorch’s convolution layers assume the color channel to be the first dimension.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的输入形状的最后一维是颜色通道，但PyTorch的卷积层假设颜色通道是第一维。
- en: 'At the end of the file is a simple function that creates an environment with
    a name and applies all the required wrappers to it:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的最后是一个简单的函数，它创建一个带有名称的环境，并将所有需要的包装器应用于它：
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, we’re using the AtariWrapper class from stable-baseline3 and
    disabling some unnecessary wrappers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们正在使用来自stable-baseline3的AtariWrapper类，并禁用了一些不必要的包装器。
- en: That’s it for wrappers; let’s look at our model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是包装器的内容；接下来我们来看看我们的模型。
- en: The DQN model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN模型
- en: The model published in Nature has three convolution layers followed by two fully
    connected layers. All layers are separated by rectified linear unit (ReLU) nonlinearities.
    The output of the model is Q-values for every action available in the environment,
    without nonlinearity applied (as Q-values can have any value). The approach of
    having all Q-values calculated with one pass through the network helps us to increase
    speed significantly in comparison to treating Q(s,a) literally, feeding observations
    and actions to the network to obtain the value of the action.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 发表在《自然》杂志上的模型有三个卷积层，后面跟着两个全连接层。所有的层都由修正线性单元（ReLU）非线性函数分隔。该模型的输出是环境中每个可用动作的Q值，且没有应用非线性（因为Q值可以是任意值）。通过让所有Q值通过网络一次计算出来，这种方法相比将Q(s,a)直接处理并将观察和动作输入网络以获取动作值的方法，显著提高了速度。
- en: 'The code of the model is in Chapter06/lib/dqn_model.py:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的代码在Chapter06/lib/dqn_model.py中：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To be able to write our network in a generic way, it was implemented in two
    parts: convolution and linear. The convolution part processes the imput image,
    which is a 4 × 84 × 84 tensor. The output from the last convolution filter is
    flattened into a one-dimensional vector and fed into two Linear layers.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够以通用的方式编写我们的网络，它被实现为两部分：卷积部分和线性部分。卷积部分处理输入图像，它是一个4 × 84 × 84的张量。最后一个卷积滤波器的输出被展平为一个一维向量，并输入到两个线性层中。
- en: Another small problem is that we don’t know the exact number of values in the
    output from the convolution layer produced with the input of the given shape.
    However, we need to pass this number to the first fully connected layer constructor.
    One possible solution would be to hard-code this number, which is a function of
    the input shape and the last convolution layer configuration (for 84 × 84 input,
    the output from the convolution layer will have 3,136 values); however, it’s not
    the best way, as our code will become less robust to input shape change. The better
    solution is to obtain the required dimension in runtime, by applying the convolution
    part to a fake input tensor. The dimension of the result would be equal to the
    number of parameters returned by this application. It would be fast, as this call
    would be done once on model creation, and it would also allow us to have generic
    code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个小问题是，我们不知道由给定形状输入产生的卷积层输出中确切的值的数量。然而，我们需要将这个数字传递给第一个全连接层的构造函数。一个可能的解决方案是硬编码这个数字，它是输入形状和最后一个卷积层配置的函数（对于
    84 × 84 的输入，卷积层的输出将有 3,136 个值）；然而，这不是最好的方式，因为我们的代码会变得不太健壮，无法应对输入形状的变化。更好的解决方案是通过应用卷积部分到一个假输入张量，在运行时获取所需的维度。结果的维度将等于该应用返回的参数数量。这样做非常快速，因为这个调用只会在模型创建时执行一次，而且它还允许我们拥有通用的代码。
- en: 'The final piece of the model is the forward() function, which accepts the 4D
    input tensor. The first dimension is the batch size and the second is the color
    channel, which is our stack of subsequent frames; the third and fourth are image
    dimensions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最后一部分是 `forward()` 函数，它接受4D输入张量。第一个维度是批量大小，第二个维度是颜色通道，它是我们后续帧的堆叠；第三和第四个维度是图像尺寸：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, before applying our network, we perform the scaling and type conversion
    of the input data. This requires a bit of explanation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在应用我们的网络之前，我们对输入数据进行了缩放和类型转换。这需要一些解释。
- en: 'Every pixel in the Atari image is represented as an unsigned byte with a value
    from 0 to 255\. This is convenient in two aspects: memory efficiency and GPU bandwidth.
    From the memory standpoint, we should keep environment observations as small as
    possible because our replay buffer will keep thousands of observations, and we
    want to keep it small. On the other hand, during the training, we need to transfer
    those observations into GPU memory to calculate the gradients and update the network
    parameters. The bandwidth between the main memory and GPU is a limited resource,
    so it also makes sense to keep observations as small as possible.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 图像中的每个像素表示为一个无符号字节，值的范围从 0 到 255。这样做有两个好处：内存效率和 GPU 带宽。从内存的角度来看，我们应该尽量保持环境观察数据的大小，因为我们的回放缓冲区会保存成千上万的观察结果，我们希望它保持尽可能小。另一方面，在训练过程中，我们需要将这些观察数据转移到
    GPU 内存中，以计算梯度并更新网络参数。主内存和 GPU 之间的带宽是有限资源，因此保持观察数据尽可能小也是有道理的。
- en: That’s why we keep observations as a numpy array with dtype=uint8, and the input
    tensor to the network is ByteTensor. But the Conv2D layer expects the float tensor
    as an input, so by dividing the input tensor by 255.0, we scale to the 0…1 range
    and do type conversion. This is fast, as the input byte tensor is already inside
    the GPU memory. After that, we apply both parts of our network to the resulting
    scaled tensor.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们将观察结果保持为 `dtype=uint8` 的 numpy 数组，并且网络的输入张量是 ByteTensor。但是 Conv2D 层期望输入的是浮动类型张量，因此通过将输入张量除以
    255.0，我们将其缩放到 0…1 范围，并进行类型转换。这是快速的，因为输入字节张量已经在 GPU 内存中。之后，我们将网络的两个部分应用于结果的缩放张量。
- en: Training
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: The third module contains the experience replay buffer, the agent, the loss
    function calculation, and the training loop itself. Before going into the code,
    something needs to be said about the training hyperparameters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个模块包含经验回放缓冲区、智能体、损失函数计算和训练循环本身。在进入代码之前，需要先谈一下训练的超参数。
- en: 'DeepMind’s Nature paper contained a table with all the details about the hyperparameters
    used to train its model on all 49 Atari games used for evaluation. DeepMind kept
    all those parameters the same for all games (but trained individual models for
    every game), and it was the team’s intention to show that the method is robust
    enough to solve lots of games with varying complexity, action space, reward structure,
    and other details using one single model architecture and hyperparameters. However,
    our goal here is much more modest: we want to solve just the Pong game.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind 的 Nature 论文中包含了一个表格，列出了用于训练模型并评估所有 49 个 Atari 游戏的超参数的详细信息。DeepMind
    对所有游戏保持了相同的参数设置（但每个游戏训练了单独的模型），他们的目的是展示该方法足够稳健，可以使用一个统一的模型架构和超参数解决多种复杂度、动作空间、奖励结构及其他细节各异的游戏。然而，我们的目标要谦逊得多：我们只希望解决
    Pong 游戏。
- en: Pong is quite simple and straightforward in comparison to other games in the
    Atari test set, so the hyperparameters in the paper are overkill for our task.
    For example, to get the best result on all 49 games, DeepMind used a million-observations
    replay buffer, which requires approximately 20 GB of RAM to store it and lots
    of samples from the environment to populate it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Pong 相比于 Atari 测试集中其他游戏来说相当简单直接，因此文中提到的超参数对于我们的任务来说是过多的。例如，为了在所有 49 个游戏中获得最佳结果，DeepMind
    使用了百万次观测的重放缓冲区，这需要大约 20 GB 的内存来存储，并且需要大量的环境样本来填充它。
- en: 'The epsilon decay schedule that was used is also not the best for a single
    Pong game. In the training, DeepMind linearly decayed epsilon from 1.0 to 0.1
    during the first million frames obtained from the environment. However, my own
    experiments have shown that for Pong, it’s enough to decay epsilon over the first
    150k frames and then keep it stable. The replay buffer also can be much smaller:
    10k transitions will be enough.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所使用的 epsilon 衰减计划对于单一的 Pong 游戏也不是最优的。在训练过程中，DeepMind 将 epsilon 从 1.0 线性衰减到 0.1，衰减过程持续了从环境中获得的前百万帧。然而，我自己的实验表明，对于
    Pong 游戏，衰减 epsilon 只需要在前 15 万帧内完成，然后保持稳定即可。重放缓冲区也可以更小：10k 次转换就足够了。
- en: In the following example, I’ve used my parameters. These differ from the parameters
    in the paper but will allow us to solve Pong about 10 times faster. On a GeForce
    GTX 1080 Ti, the following version converges to a mean score of 19.0 in about
    50 minutes, but with DeepMind’s hyperparameters, it will require at least a day.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我使用了我的参数。虽然这些参数与论文中的参数不同，但它们能让我们大约以 10 倍的速度解决 Pong 游戏。在 GeForce GTX 1080
    Ti 上，以下版本大约 50 分钟就能收敛到 19.0 的平均分数，但使用 DeepMind 的超参数至少需要一天时间。
- en: This speedup, of course, involves fine-tuning for one particular environment
    and can break convergence on other games. You are free to play with the options
    and other games from the Atari set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加速当然是针对特定环境的微调，并可能导致在其他游戏中无法收敛。你可以自由地调整选项和尝试 Atari 集合中的其他游戏。
- en: 'First, we import the required modules:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的模块：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then define the hyperparameters:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义超参数：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These two values set the default environment to train on and the reward boundary
    for the last 100 episodes to stop training. If you want, you can redefine the
    environment name using the command-line --env argument:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个值设置了默认的训练环境以及停止训练的奖励边界（最后 100 回合）。如果需要，你可以通过命令行 --env 参数重新定义环境名称：
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding parameters define the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述参数定义了以下内容：
- en: Our γ value used for the Bellman approximation (GAMMA)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用于贝尔曼近似的 γ 值（GAMMA）
- en: The batch size sampled from the replay buffer (BATCH_SIZE)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从重放缓冲区中采样的批次大小（BATCH_SIZE）
- en: The maximum capacity of the buffer (REPLAY_SIZE)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓冲区的最大容量（REPLAY_SIZE）
- en: The count of frames we wait for before starting training to populate the replay
    buffer (REPLAY_START_SIZE)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在开始训练前等待的帧数，用于填充重放缓冲区（REPLAY_START_SIZE）
- en: The learning rate used in the Adam optimizer, which is used in this example
    (LEARNING_RATE)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个示例中使用的 Adam 优化器的学习率（LEARNING_RATE）
- en: How frequently we sync model weights from the training model to the target model,
    which is used to get the value of the next state in the Bellman approximation
    (SYNC_TARGET_FRAMES)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将训练模型的权重同步到目标模型的频率，目标模型用于在贝尔曼近似中获取下一个状态的值（SYNC_TARGET_FRAMES）
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The last batch of hyperparameters is related to the epsilon decay schedule.
    To achieve proper exploration, we start with 𝜖 = 1.0 at the early stages of training,
    which causes all actions to be selected randomly. Then, during the first 150,000
    frames, 𝜖 is linearly decayed to 0.01, which corresponds to the random action
    taken in 1% of steps. A similar scheme was used in the original DeepMind paper,
    but the duration of decay was almost 10 times longer (so 𝜖 = 0.01 was reached
    after a million frames).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一批超参数与 epsilon 衰减调度有关。为了实现适当的探索，我们在训练的早期阶段从 𝜖 = 1.0 开始，这会导致所有动作都被随机选择。然后，在前
    150,000 帧中，𝜖 会线性衰减到 0.01，这对应于 1% 的步骤中采取随机动作。原始 DeepMind 论文中也使用了类似的方案，但衰减的持续时间几乎是原来的
    10 倍（因此 𝜖 = 0.01 是在一百万帧后达到的）。
- en: 'Here, we define our type aliases and the dataclass Experience, used to keep
    entries in the experience replay buffer. It contains the current state, the action
    taken, the reward obtained, the termination or truncation flag, and the new state:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了类型别名和数据类 Experience，用于保存经验回放缓冲区中的条目。它包含当前状态、采取的动作、获得的奖励、终止或截断标志以及新的状态：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next chunk of code defines our experience replay buffer, the purpose of
    which is to keep the transitions obtained from the environment:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码定义了我们的经验回放缓冲区，目的是保存从环境中获得的转移：
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Each time we do a step in the environment, we push the transition into the buffer,
    keeping only a fixed number of steps (in our case, 10k transitions). For training,
    we randomly sample the batch of transitions from the replay buffer, which allows
    us to break the correlation between subsequent steps in the environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在环境中执行一步时，我们将转移推入缓冲区，只保留固定数量的步数（在我们的情况下是 10k 次转移）。在训练中，我们从回放缓冲区随机抽取一批转移，这样可以打破环境中后续步骤之间的相关性。
- en: 'Most of the experience replay buffer code is quite straightforward: it basically
    exploits the capability of the deque class to maintain the given number of entries
    in the buffer. In the sample() method, we create a list of random indices and
    return a list of Experience items to be repackaged and converted into tensors.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分经验回放缓冲区的代码都非常直接：它基本上利用了 deque 类来保持缓冲区中的指定数量的条目。在 sample() 方法中，我们创建一个随机索引的列表，并返回一个包含经验条目的列表，以便重新包装并转换为张量。
- en: 'The next class we need to have is an Agent, which interacts with the environment
    and saves the result of the interaction in the experience replay buffer that you
    have just seen:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要的类是 Agent，它与环境进行交互，并将交互的结果保存到你刚才看到的经验回放缓冲区中：
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: During the agent’s initialization, we need to store references to the environment
    and experience replay buffer, tracking the current observation and the total reward
    accumulated so far.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体初始化时，我们需要存储对环境和经验回放缓冲区的引用，跟踪当前的观察值和迄今为止累计的总奖励。
- en: 'The main method of the agent is to perform a step in the environment and store
    its result in the buffer. To do this, we need to select the action first:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的主要方法是在环境中执行一步并将其结果存储在缓冲区中。为此，我们需要先选择动作：
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With the probability epsilon (passed as an argument), we take the random action;
    otherwise, we use the model to obtain the Q-values for all possible actions and
    choose the best. In this method, we use the PyTorch no_grad() decorator to disable
    gradient tracking during the whole method, as we don’t need them anyway.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以概率 epsilon（作为参数传递），我们采取随机动作；否则，我们使用模型来获得所有可能动作的 Q 值，并选择最优的动作。在此方法中，我们使用 PyTorch
    的 no_grad() 装饰器在整个方法中禁用梯度追踪，因为我们根本不需要它们。
- en: 'As the action has been chosen, we pass it to the environment to get the next
    observation and reward, store the data in the experience buffer, and then handle
    the end-of-episode situation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当动作被选中后，我们将其传递给环境以获取下一个观察值和奖励，将数据存储在经验缓冲区中，然后处理回合结束的情况：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The result of the function is the total accumulated reward if we have reached
    the end of the episode with this step, or None otherwise.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的结果是总的累计奖励，如果我们通过这一步已经到达了回合的结束，则返回奖励，否则返回 None。
- en: 'The function batch_to_tensors takes the batch of Experience objects and returns
    a tuple with states, actions, rewards, done flags, and new states repacked as
    PyTorch tensors of the corresponding types:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 batch_to_tensors 接受一批 Experience 对象，并返回一个包含状态、动作、奖励、完成标志和新状态的元组，这些数据会被重新打包为对应类型的
    PyTorch 张量：
- en: '[PRE24]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When we work with states, we try to avoid memory copy (by using np.asarray()
    function), which is important, as Atari observations are large (4 frames with
    84 × 84 bytes each), and we have a batch of 32 such objects. Without this optimization,
    performance drops about 20 times.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理状态时，我们尽量避免内存复制（通过使用np.asarray()函数），这是很重要的，因为Atari的观测数据量大（每帧有84 × 84字节，共四帧），并且我们有32个这样的对象。如果没有这个优化，性能会下降大约20倍。
- en: 'Now, it is time for the last function in the training module, which calculates
    the loss for the sampled batch. This function is written in a form to maximally
    exploit GPU parallelism by processing all batch samples with vector operations,
    which makes it harder to understand when compared with a naïve loop over the batch.
    Yet this optimization pays off: the parallel version is more than two times faster
    than an explicit loop.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是训练模块中最后一个函数的时间了，这个函数计算采样批次的损失。这个函数的写法旨在最大化利用GPU的并行计算，通过向量化操作处理所有批次样本，这使得它比一个简单的批次循环更难理解。然而，这个优化是值得的：并行版本比显式的循环快了两倍多。
- en: 'As a reminder, here is the loss expression we need to calculate:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这是我们需要计算的损失表达式：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq24.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq24.png)'
- en: 'We use the preceding equation for steps that aren’t at the end of the episode
    and the following for the final steps:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用前面的方程处理非回合结束的步骤，使用以下方程处理最后的步骤：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq25.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq25.png)'
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the arguments, we pass our batch, the network that we are training, and the
    target network, which is periodically synced with the trained one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些参数中，我们传入了我们的批次、正在训练的网络和目标网络，目标网络会定期与训练好的网络同步。
- en: The first model (passed as the parameter net) is used to calculate gradients;
    the second model in the tgt_net argument is used to calculate values for the next
    states, and this calculation shouldn’t affect gradients. To achieve this, we use
    the detach() function of the PyTorch tensor to prevent gradients from flowing
    into the target network’s graph. This function was described in Chapter [3](ch007.xhtml#x1-530003).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型（作为net参数传入）用于计算梯度；第二个模型（在tgt_net参数中）用于计算下一个状态的值，这一计算不应影响梯度。为了实现这一点，我们使用PyTorch张量的detach()函数来防止梯度流入目标网络的图中。这个函数在第[3](ch007.xhtml#x1-530003)章中有描述。
- en: At the beginning of the function, we call the function batch_to_tensors to repack
    the batch into individual tensor variables.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数开始时，我们调用batch_to_tensors函数将批次重新打包成单独的张量变量。
- en: 'The next line is a bit tricky:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行有点复杂：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let’s discuss it in detail. Here, we pass observations to the first model and
    extract the specific Q-values for the taken actions, using the gather() tensor
    operation. The first argument to the gather() call is a dimension index that we
    want to perform gathering on (in our case, it is equal to 1, which corresponds
    to actions).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论一下。在这里，我们将观测数据传入第一个模型，并使用gather()张量操作提取已执行动作的特定Q值。gather()调用的第一个参数是我们希望进行聚合的维度索引（在我们的例子中，它等于1，表示动作维度）。
- en: The second argument is a tensor of indices of elements to be chosen. Extra unsqueeze()
    and squeeze() calls are required to compute the index argument for the gather()
    function and to get rid of the extra dimensions that we created, respectively.
    (The index should have the same number of dimensions as the data we are processing.)
    In Figure [6.3](#x1-102122r3), you can see an illustration of what gather() does
    on the example case, with a batch of six entries and four actions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是一个元素索引的张量，用来选择需要的元素。为了计算gather()函数的索引参数并去除我们创建的多余维度，分别需要额外的unsqueeze()和squeeze()调用。（索引应与我们处理的数据具有相同的维度数。）在图[6.3](#x1-102122r3)中，您可以看到gather()在示例中的作用，示例中有六个条目的批次和四个动作。
- en: '![PIC](img/file33.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file33.png)'
- en: 'Figure 6.3: Transformation of tensors during a DQN loss calculation'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：DQN损失计算中的张量转换
- en: Keep in mind that the result of gather() applied to tensors is a differentiable
    operation that will keep all gradients with respect to the final loss value.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，gather()应用于张量的结果是一个可微分操作，它会保持与最终损失值相关的所有梯度。
- en: 'Next, we disable the gradients’ calculations (which ends up in a small speedup),
    apply the target network to our next state observations, and calculate the maximum
    Q-value along the same action dimension, 1:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们禁用梯度计算（这会带来一些速度提升），将目标网络应用到下一个状态的观测中，并沿着相同行动维度（1）计算最大Q值：
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The function max() returns both maximum values and indices of those values (so
    it calculates both max and argmax), which is very convenient. However, in this
    case, we are interested only in values, so we take the first entry of the result
    (max values).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 max() 返回最大值及其索引（因此它同时计算 max 和 argmax），这非常方便。然而，在这种情况下，我们只对值感兴趣，因此我们取结果中的第一个条目（最大值）。
- en: 'The following is the next line:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是下一行：
- en: '[PRE28]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we make one simple but very important transformation: if the transition
    in the batch is from the last step in the episode, then our value of the action
    doesn’t have a discounted reward for the next state, as there is no next state
    from which to gather the reward. This may look minor, but it is very important
    in practice; without this, training will not converge (I personally have wasted
    several hours debugging this case).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们进行一个简单但非常重要的转换：如果批次中的过渡来自回合的最后一步，那么我们的动作值就没有下一个状态的折扣奖励，因为没有下一个状态可以获取奖励。这看起来可能是小事，但在实际中非常重要；没有这一点，训练将无法收敛（我个人花了几个小时调试这个问题）。
- en: 'In the next line, we detach the value from its computation graph to prevent
    gradients from flowing into the NN used to calculate the Q approximation for the
    next states:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行中，我们将值从其计算图中分离出来，以防止梯度流入用于计算下一个状态的 Q 近似值的神经网络：
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This is important, as without this, our backpropagation of the loss will start
    to affect both predictions for the current state and the next state. However,
    we don’t want to touch predictions for the next state, as they are used in the
    Bellman equation to calculate the reference Q-values. To block gradients from
    flowing into this branch of the graph, we use the detach() method of the tensor,
    which returns the tensor without connection to its calculation history.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为如果没有这个，我们的损失反向传播将开始影响当前状态和下一个状态的预测。然而，我们不想触及下一个状态的预测，因为它们在贝尔曼方程中用于计算参考
    Q 值。为了阻止梯度流入图的这个分支，我们使用张量的 detach() 方法，该方法返回一个没有连接到计算历史的张量。
- en: 'Finally, we calculate the Bellman approximation value and the mean squared
    error loss:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算贝尔曼近似值和均方误差损失：
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To get the full picture of the loss function calculation code, let’s look at
    this function in full:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面了解损失函数计算代码，让我们完整查看这个函数：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This ends our loss function calculation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们的损失函数计算。
- en: 'The rest of the code is our training loop. To begin with, we create a parser
    of command-line arguments:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码是我们的训练循环。首先，我们创建一个命令行参数解析器：
- en: '[PRE32]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Our script allows us to specify a device for computation and train on environments
    that are different from the default.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的脚本允许我们指定一个用于计算的设备，并在与默认环境不同的环境中进行训练。
- en: 'Here, we create our environment:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建我们的环境：
- en: '[PRE33]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Our environment has all the required wrappers applied, the NN that we are going
    to train, and our target network with the same architecture. At first, they will
    be initialized with different random weights, but it doesn’t matter much, as we
    will sync them every 1k frames, which roughly corresponds to one episode of Pong.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的环境已经应用了所有必需的包装器，我们将训练的神经网络和具有相同架构的目标网络。最初，它们会用不同的随机权重初始化，但这并不重要，因为我们会每 1k
    帧同步一次它们，这大致对应一个 Pong 回合。
- en: 'Then, we create our experience replay buffer of the required size and pass
    it to the agent:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建所需大小的经验回放缓冲区，并将其传递给智能体：
- en: '[PRE34]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Epsilon is initially initialized to 1.0 but will be decreased every iteration.
    Here are the last things we do before the training loop:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon 初始值为 1.0，但会在每次迭代时减小。以下是训练循环开始前我们所做的最后几件事：
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We create an optimizer, a buffer for full episode rewards, a counter of frames
    and several variables to track our speed, and the best mean reward reached. Every
    time our mean reward beats the record, we will save the model in the file.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个优化器、一个用于存储完整回合奖励的缓冲区、一个帧计数器和几个变量来跟踪我们的速度，以及达到的最佳平均奖励。每当我们的平均奖励突破记录时，我们会将模型保存到文件中。
- en: 'At the beginning of the training loop, we count the number of iterations completed
    and decrease epsilon according to our schedule:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环开始时，我们会计算完成的迭代次数，并根据我们的计划降低 epsilon 值：
- en: '[PRE36]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Epsilon will drop linearly during the given number of frames (EPSILON_DECAY_LAST_FRAME=150k)
    and then be kept on the same level as EPSILON_FINAL=0.01.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon 会在给定的帧数（EPSILON_DECAY_LAST_FRAME=150k）内线性下降，然后保持在相同的水平，即 EPSILON_FINAL=0.01。
- en: 'In this block of code, we ask our agent to make a single step in the environment
    (using our current network and value for epsilon):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们让我们的智能体在环境中执行一步操作（使用当前的网络和 epsilon 的值）：
- en: '[PRE37]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This function returns a float value only if this step is the final step in
    the episode. In that case, we report our progress. Specifically, we calculate
    and show, both in the console and in TensorBoard, these values:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数仅在此步骤为回合的最后一步时返回浮动值。在这种情况下，我们报告我们的进度。具体来说，我们计算并显示以下值，在控制台和 TensorBoard 中展示：
- en: Speed as a count of frames processed per second
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度（每秒处理的帧数）
- en: Count of episodes played
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已进行的回合数
- en: Mean reward for the last 100 episodes
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去 100 个回合的平均奖励
- en: Current value for epsilon
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前 epsilon 的值
- en: 'Every time our mean reward for the last 100 episodes reaches a maximum, we
    report this and save the model parameters:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 每当过去 100 个回合的平均奖励达到最大值时，我们会报告这一情况并保存模型参数：
- en: '[PRE38]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If our mean reward exceeds the specified boundary, then we stop training. For
    Pong, the boundary is 19.0, which means winning more than 19 from 21 total games.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的平均奖励超过了指定的边界，则停止训练。对于 Pong，边界是 19.0，意味着从 21 场比赛中赢得超过 19 场。
- en: 'Here, we check whether our buffer is large enough for training:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们检查我们的缓冲区是否足够大，能够进行训练：
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: First, we should wait for enough data to be accumulated, which in our case is
    10k transitions. The next condition syncs parameters from our main network to
    the target network every SYNC_TARGET_FRAMES, which is 1k by default.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该等待足够的数据积累，在我们的案例中是 10k 次过渡。下一个条件是在每个 SYNC_TARGET_FRAMES（默认是 1k）周期后，从主网络同步参数到目标网络。
- en: 'The last piece of the training loop is very simple but requires the most time
    to execute:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环的最后一部分非常简单，但需要花费最多的时间来执行：
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, we zero gradients, sample data batches from the experience replay buffer,
    calculate loss, and perform the optimization step to minimize the loss.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将梯度归零，从经验重放缓冲区中采样数据批次，计算损失，并执行优化步骤以最小化损失。
- en: Running and performance
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行与性能
- en: This example is demanding on resources. On Pong, it requires about 400k frames
    to reach a mean reward of 17 (which means winning more than 80% of games). A similar
    number of frames will be required to get from 17 to 19, as our learning progress
    will saturate, and it will be hard for the model to “polish the policy” and further
    improve the score. So, on average, a million game frames are needed to train it
    fully. On the GTX 1080Ti, I have a speed of about 250 frames per second, which
    is about an hour of training. On a CPU (i5-7600k), the speed is much slower, about
    40 frames per second, which will take about seven hours. Remember that this is
    for Pong, which is relatively easy to solve. Other games might require hundreds
    of millions of frames and a 100 times larger experience replay buffer.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子对资源要求较高。在 Pong 上，它需要大约 400k 帧才能达到平均奖励 17（这意味着赢得超过 80% 的比赛）。为了从 17 提升到 19，类似数量的帧也会被消耗，因为我们的学习进度将饱和，模型很难“润色策略”并进一步提高分数。因此，平均来说，需要一百万个游戏帧来完全训练它。在
    GTX 1080Ti 上，我的速度大约是 250 帧每秒，大约需要一个小时的训练。在 CPU（i5-7600k）上，速度要慢得多，大约是 40 帧每秒，训练将需要大约七小时。记住，这个是在
    Pong 上，Pong 相对容易解决。其他游戏可能需要数亿帧和一个大 100 倍的经验重放缓冲区。
- en: 'In Chapter [8](ch012.xhtml#x1-1240008), we will look at various approaches
    found by researchers since 2015 that can help to increase both training speed
    and data efficiency. Chapter [9](ch013.xhtml#x1-1600009) will be devoted to engineering
    tricks to speed up RL methods’ performance. Nevertheless, for Atari, you will
    need resources and patience. The following figure shows a chart with reward dynamics
    during the training:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[8](ch012.xhtml#x1-1240008)章中，我们将探讨自 2015 年以来研究人员发现的各种方法，这些方法可以帮助提高训练速度和数据效率。第[9](ch013.xhtml#x1-1600009)章将专注于加速
    RL 方法性能的工程技巧。尽管如此，针对 Atari，你仍然需要资源和耐心。以下图表展示了训练过程中奖励动态的变化：
- en: '![PIC](img/B22150_06_04.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_06_04.png)'
- en: 'Figure 6.4: Dynamics of average reward calculated over the last 100 episodes'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：计算过去 100 个回合的平均奖励动态
- en: 'Now, let’s look at the console output from our training process (only the beginning
    of the output is shown):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下训练过程中的控制台输出（这里只展示输出的开始部分）：
- en: '[PRE41]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: During the first 10k steps, our speed is very high, as we don’t do any training,
    which is the most expensive operation in our code. After 10k, we start sampling
    the training batches and the performance drops to more representative numbers.
    During the training, the performance also decreases slightly, just because of
    the epsilon decrease. When 𝜖 is high, the actions are chosen randomly. As 𝜖 approaches
    zero, we need to perform inference to get Q-values for action selection, which
    also costs time.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在前10k步中，我们的速度非常快，因为我们没有进行任何训练，而训练是我们代码中最昂贵的操作。10k步后，我们开始采样训练批次，性能下降到更具代表性的数字。在训练过程中，性能也会略有下降，这只是因为𝜖的减小。当𝜖较高时，动作是随机选择的。当𝜖接近零时，我们需要执行推理来获得Q值以进行动作选择，这也会消耗时间。
- en: 'Several dozens of games later, our DQN should start to figure out how to win
    1 or 2 games out of 21, and an average reward begins to grow (this normally happens
    around 𝜖 = 0.5):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 几十场游戏之后，我们的DQN应该开始弄明白如何在21场比赛中赢得1到2场，并且平均奖励开始增长（通常在𝜖 = 0.5时发生）：
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, after many more games, our DQN can finally dominate and beat the (not
    very sophisticated) built-in Pong AI opponent:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经过更多的比赛，我们的DQN终于能够主宰并击败（不太复杂的）内置Pong AI对手：
- en: '[PRE43]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Due to randomness in the training process, your actual dynamics might differ
    from what is displayed here. In some rare cases (1 run from 10 according to my
    experiments), the training does not converge at all, which looks like a constant
    stream of rewards −21 for a long time. This is not an uncommon situation in deep
    learning (due to the randomness of the training) and might occur even more often
    in RL (due to the added randomness of environment communication). If your training
    doesn’t show any positive dynamics for the first 100k–200k iterations, you should
    restart it.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练过程中存在随机性，你的实际动态可能与这里展示的不同。在一些罕见的情况下（根据我的实验，大约10次运行中有1次），训练根本无法收敛，表现为长时间的奖励始终为−21。这在深度学习中并不罕见（由于训练的随机性），在强化学习中可能会更常见（由于环境交互的额外随机性）。如果你的训练在前100k到200k次迭代中没有任何正向动态，你应该重新开始训练。
- en: Your model in action
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的模型在实践中的表现
- en: The training process is just one part of the picture. Our final goal is not
    only to train the model; we also want our model to play the game with a good outcome.
    During the training, every time we update the maximum of the mean reward for the
    last 100 games, we save the model into the file PongNoFrameskip-v4-best_<score>.dat.
    In the Chapter06/03_dqn_play.py file, we have a program that can load this model
    file and play one episode, displaying the model’s dynamics.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程只是整个过程的一部分。我们的最终目标不仅仅是训练模型；我们还希望我们的模型能以良好的结果来玩游戏。在训练过程中，每次更新过去100场游戏的平均奖励最大值时，我们都会将模型保存到文件
    PongNoFrameskip-v4-best_<score>.dat 中。在 Chapter06/03_dqn_play.py 文件中，我们有一个程序可以加载这个模型文件并进行一次游戏，展示模型的动态。
- en: The code is very simple, but it can be like magic seeing how several matrices,
    with just a million parameters, can play Pong with superhuman accuracy by observing
    only the pixels.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单，但看到几个矩阵（仅有百万个参数）通过观察像素，能够以超人精度玩Pong游戏，简直像魔法一样。
- en: 'First, we import the familiar PyTorch and Gym modules:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入熟悉的PyTorch和Gym模块：
- en: '[PRE44]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The script accepts the filename of the saved model and allows the specification
    of the Gym environment (of course, the model and environment have to match):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本接受已保存模型的文件名，并允许指定Gym环境（当然，模型和环境必须匹配）：
- en: '[PRE45]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Additionally, you have to pass option -r with the name of a nonexistent directory,
    which will be used to save a video of your game.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要传递选项 -r，并指定一个不存在的目录名，系统将把你游戏的录像保存在该目录下。
- en: 'The following code is also not very complicated:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码也不太复杂：
- en: '[PRE46]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We create the environment, wrap it in the RecordVideo wrapper, create the model,
    and then we load weights from the file passed in the arguments. The argument map_location,
    passed to the torch.load() function, is needed to map the loaded tensor location
    from the GPU to the CPU. By default, torch tries to load tensors on the same device
    where they were saved, but if you copy the model from the machine you used for
    training (with a GPU) to a laptop without a GPU, the locations need to be remapped.
    Our example doesn’t use the GPU at all, as inference is fast enough without acceleration.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建环境，将其包装在 RecordVideo 封装器中，创建模型，然后从传入的文件中加载权重。传递给 torch.load() 函数的 map_location
    参数用于将加载的张量位置从 GPU 映射到 CPU。默认情况下，torch 尝试加载与保存时相同设备上的张量，但如果你将模型从用于训练的机器（有 GPU）复制到没有
    GPU 的笔记本电脑上，就需要重新映射位置。我们的示例完全不使用 GPU，因为推理速度足够快，不需要加速。
- en: 'This is almost an exact copy of the Agent class’ method play_step() from the
    training code, without the epsilon-greedy action selection:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这是几乎与训练代码中的 Agent 类的 play_step() 方法完全相同的代码，不包含 epsilon-greedy 动作选择：
- en: '[PRE47]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We just pass our observation to the agent and select the action with the maximum
    value.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将观察结果传递给智能体，并选择具有最大值的动作。
- en: 'The rest of the code is also simple:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码也很简单：
- en: '[PRE48]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We pass the action to the environment, count the total reward, and stop our
    loop when the episode ends. After the episode, we show the total reward and the
    number of times that the agent executed the action.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将动作传递给环境，计算总奖励，并在回合结束时停止循环。回合结束后，我们显示总奖励和智能体执行动作的次数。
- en: 'In this YouTube playlist, you can find recordings of the gameplay at different
    stages of the training: [https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu](https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 YouTube 播放列表中，你可以找到在训练不同阶段的游戏录像：[https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu](https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu)。
- en: Things to try
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试的事项
- en: 'If you are curious and want to experiment with this chapter’s material on your
    own, then here is a shortlist of directions to explore. Be warned though: they
    can take lots of time and may cause you some moments of frustration during your
    experiments. However, these experiments are a very efficient way to really master
    the material from a practical point of view:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感到好奇并想自己尝试本章的内容，以下是一些值得探索的方向。需要警告的是：这些实验可能会耗费大量时间，并且在实验过程中可能会让你感到挫折。然而，从实践角度看，这些实验是掌握材料的非常有效的方式：
- en: Try to take some other games from the Atari set, such as Breakout, Atlantis,
    or River Raid (my childhood favorite). This could require the tuning of hyperparameters.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试从 Atari 套件中选择其他游戏，例如《Breakout》、《Atlantis》或《River Raid》（我童年的最爱）。这可能需要调整超参数。
- en: As an alternative to FrozenLake, there is another tabular environment, Taxi,
    which emulates a taxi driver who needs to pick up passengers and take them to
    a destination.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为 FrozenLake 的替代方案，还有另一个表格环境，Taxi，它模拟一个出租车司机需要接送乘客并将其送到目的地的场景。
- en: Play with Pong hyperparameters. Is it possible to train faster? OpenAI claims
    that it can solve Pong in 30 minutes using the asynchronous advantage actor-critic
    method (which is a subject of Part 3 of this book). Maybe it’s possible with a
    DQN.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试调整 Pong 的超参数。是否有可能训练得更快？OpenAI 声称使用异步优势演员-评论家方法（本书第 3 部分的主题）可以在 30 分钟内解决 Pong
    问题。也许用 DQN 也能做到。
- en: Can you make the DQN training code faster? The OpenAI Baselines project has
    shown 350 FPS using TensorFlow on GTX 1080 Ti. So, it looks like it’s possible
    to optimize the PyTorch code. We will discuss this topic in Chapter [9](ch013.xhtml#x1-1600009),
    but meanwhile, you can do your own experiments.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能让 DQN 训练代码更快吗？OpenAI Baselines 项目已经在 GTX 1080 Ti 上使用 TensorFlow 实现了 350 FPS。因此，优化
    PyTorch 代码是可能的。我们将在第[9](ch013.xhtml#x1-1600009)章讨论这个话题，但同时你也可以进行自己的实验。
- en: In the video recording, you might notice that models with a mean score around
    zero play quite well. In fact, I had the impression that those models play better
    than models with mean scores of 10–19\. This might be the case due to overfitting
    to the particular game situations. Could you try to fix this? Maybe it would be
    possible to use a generative adversarial network-style approach to make one model
    play with another?
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在视频录制中，你可能会注意到，平均分接近零的模型表现得相当不错。实际上，我有一种印象，平均分在 10-19 之间的模型表现得不如这些模型。这可能是因为模型对特定的游戏情况过拟合。你能尝试修复这个问题吗？也许可以使用生成对抗网络风格的方法，让一个模型与另一个模型对战？
- en: Can you get the Ultimate Pong Dominator model with a mean score of 21? It shouldn’t
    be very hard – the learning rate decay is the obvious method to try.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能通过一个平均得分为21的最终Pong主宰者模型吗？这应该不难——学习率衰减是一个显而易见的尝试方法。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a lot of new and complex material. You became familiar
    with the limitations of value iteration in complex environments with large observation
    spaces, and we discussed how to overcome them with Q-learning. We checked the
    Q-learning algorithm on the FrozenLake environment and discussed the approximation
    of Q-values with NNs, as well as the extra complications that arise from this
    approximation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们涉及了许多新的复杂内容。你已经了解了在具有大观测空间的复杂环境中，值迭代的局限性，并讨论了如何通过Q学习来克服这些局限性。我们在FrozenLake环境中验证了Q学习算法，并讨论了使用神经网络（NNs）对Q值进行逼近，以及由此逼近带来的额外复杂性。
- en: We covered several tricks with DQNs to improve their training stability and
    convergence, such as an experience replay buffer, target networks, and frame stacking.
    Finally, we combined those extensions into one single implementation of DQN that
    solves the Pong environment from the Atari games suite.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了多个针对深度Q网络（DQNs）的技巧，以提高它们的训练稳定性和收敛性，例如经验回放缓冲区、目标网络和帧堆叠。最后，我们将这些扩展整合到一个单一的DQN实现中，成功解决了Atari游戏套件中的Pong环境。
- en: In the next chapter, we will take a quick look at higher-level RL libraries,
    and after that, we will take a look at a set of tricks that researchers have found
    since 2015 to improve DQN convergence and quality, which (combined) can produce
    state-of-the-art results on most of the 54 (newly added) Atari games. This set
    was published in 2017, and we will analyze and reimplement all of the tricks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将简要了解一些更高级的强化学习（RL）库，之后，我们将回顾一组自2015年以来研究人员发现的技巧，用以改善DQN的收敛性和质量，这些技巧（综合起来）可以在大多数54款（新添加的）Atari游戏上实现最先进的成果。这些技巧在2017年发布，我们将分析并重新实现所有这些技巧。
