- en: Applying Transfer Learning to Network Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将迁移学习应用于网络模型
- en: In this chapter, we will talk about transfer learning methods, which are essential
    to reuse a model that was previously developed. We will see how we can apply transfer
    learning to the model created in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml),
    *Building Deep Neural Networks for Binary Classification*, as well as a pre-trained
    model from the DL4J Model Zoo API. We can use the DL4J transfer learning API to
    modify the network architecture, hold specific layer parameters while training,
    and fine-tune model configurations. Transfer learning enables improved performance
    and can develop skillful models. We pass learned parameters learned from another
    model to the current training session. If you have already set up the DL4J workspace
    for previous chapters, then you don't have to add any new dependencies in `pom.xml`;
    otherwise, you need to add the basic Deeplearning4j Maven dependency in `pom.xml`, as
    specified in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building
    Deep Neural Networks for Binary Classification*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论迁移学习方法，它们对于重用先前开发的模型至关重要。我们将展示如何将迁移学习应用于在[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)，*构建二分类深度神经网络*中创建的模型，以及来自
    DL4J 模型库 API 的预训练模型。我们可以使用 DL4J 迁移学习 API 来修改网络架构，在训练过程中保持特定层的参数，并微调模型配置。迁移学习能够提高性能，并且可以开发出更高效的模型。我们将从另一个模型中传递已学习的参数到当前的训练会话。如果你已经为前几章设置好了
    DL4J 工作区，那么就不需要在`pom.xml`中添加新的依赖项；否则，你需要根据[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)，*构建二分类深度神经网络*中的说明，在`pom.xml`中添加基本的
    Deeplearning4j Maven 依赖项。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Modifying an existing customer retention model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改现有的客户保持模型
- en: Fine-tuning the learning configurations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调学习配置
- en: Implementing frozen layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现冻结层
- en: Importing and loading Keras models and layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入和加载 Keras 模型及层
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter's source code can be located here: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在此找到：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java)。
- en: After cloning the GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/11_Applying_Transfer_Learning_to_network_models/sourceCode`
    directory, then import the `cookbookapp` project as a Maven project by importing `pom.xml`*.*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在克隆 GitHub 仓库后，导航到`Java-Deep-Learning-Cookbook/11_Applying_Transfer_Learning_to_network_models/sourceCode`目录，然后通过导入`pom.xml`*将`cookbookapp`项目作为
    Maven 项目导入*。
- en: You need to have the pre-trained model from [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building
    Deep Neural Networks for Binary Classification*, to run the transfer learning
    example. The model file should be saved in your local system once the [Chapter
    3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building Deep Neural Networks
    for Binary Classification* source code is executed. You need to load the model
    here while executing the source code in this chapter. Also, for the `SaveFeaturizedDataExample` example,
    you need to update the train/test directories where the application will be saving
    featurized datasets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要拥有[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)，*构建二分类深度神经网络*中的预训练模型，才能运行迁移学习示例。模型文件应该在执行[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)，*构建二分类深度神经网络*源代码后保存到本地系统中。在执行本章源代码时，你需要在此加载模型。此外，对于`SaveFeaturizedDataExample`示例，你还需要更新训练/测试目录，以便应用程序能够保存特征化数据集。
- en: Modifying an existing customer retention model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改现有的客户保持模型
- en: We created a customer churn model in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml),
    *Building Deep Neural Networks for Binary Classification*, that is capable of
    predicting whether a customer will leave an organization based on specified data.
    We might want to train the existing model on newly available data. Transfer learning occurs
    when an existing model is exposed to fresh training on a similar model. We used
    the `ModelSerializer` class to save the model after training the neural network.
    We used a feed-forward network architecture to build a customer retention model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml)中创建了一个客户流失模型，*构建二分类深度神经网络*，它能够根据指定数据预测客户是否会离开组织。我们可能希望在新的数据上训练现有模型。迁移学习发生在一个现有模型暴露于类似模型的全新训练时。我们使用`ModelSerializer`类在训练神经网络后保存模型。我们使用前馈网络架构来构建客户保持模型。
- en: In this recipe, we will import an existing customer retention model and further
    optimize it using the DL4J transfer learning API.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将导入一个现有的客户保持模型，并使用DL4J迁移学习API进一步优化它。
- en: How to do it...
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Call the `load()` method to import the model from the saved location:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`load()`方法从保存的位置导入模型：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add the required `pom` dependency to use the `deeplearning4j-zoo` module:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加所需的`pom`依赖项以使用`deeplearning4j-zoo`模块：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Add the fine-tuning configuration for `MultiLayerNetwork` using the `TransferLearning`
    API:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TransferLearning` API为`MultiLayerNetwork`添加微调配置：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Add the fine-tuning configuration for `ComputationGraph` using the `TransferLearning` API:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TransferLearning` API为`ComputationGraph`添加微调配置：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Configure the training session using `TransferLearningHelper`. `TransferLearningHelper`
    can be created in two ways:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TransferLearningHelper`配置训练会话。`TransferLearningHelper`可以通过两种方式创建：
- en: 'Pass in the model object that was created using the transfer learning builder
    (step 2) with the frozen layers mentioned:'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传入使用迁移学习构建器（步骤2）创建的模型对象，并附加冻结层：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create it directly from the imported model by specifying the frozen layers
    explicitly:'
  id: totrans-26
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过显式指定冻结层，从导入的模型中直接创建：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Featurize the train/test data using the `featurize()` method:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`featurize()`方法对训练/测试数据进行特征化：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create train/test iterators by using `ExistingMiniBatchDataSetIterator`:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ExistingMiniBatchDataSetIterator`创建训练/测试迭代器：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Start the training instance on top of the featurized data by calling `fitFeaturized()`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fitFeaturized()`在特征化数据上启动训练实例：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Evaluate the model by calling `evaluate()` for unfrozen layers:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`evaluate()`评估未冻结层的模型：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In step 1, the value of `saveUpdater` is going to be `true` if we plan to train
    the model at a later point. We have also discussed pre-trained models provided
    by DL4J''s model zoo API. Once we add the dependency for `deeplearning4j-zoo`,
    as mentioned in step 1, we can load pre-trained models such as VGG16, as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，如果我们计划稍后训练模型，`saveUpdater`的值将设置为`true`。我们还讨论了DL4J模型库API提供的预训练模型。一旦我们按照步骤1中提到的添加了`deeplearning4j-zoo`依赖项，就可以加载如VGG16等预训练模型，方法如下：
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: DL4J has support for many more pre-trained models under its transfer learning
    API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J支持更多在其迁移学习API下的预训练模型。
- en: 'Fine-tuning a configuration is the process of taking a model that was trained
    to perform a task and training it to perform another similar task. Fine-tuning
    configurations is specific to transfer learning. In steps 3 and 4, we added a
    fine-tuning configuration specific to the type of neural network. The following
    are possible changes that can be made using the DL4J transfer learning API:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 微调配置是将一个训练过的模型调整为执行另一个类似任务的过程。微调配置是迁移学习特有的。在步骤3和4中，我们为特定类型的神经网络添加了微调配置。以下是使用DL4J迁移学习API可以进行的可能修改：
- en: Update the weight initialization scheme, gradient update strategy, and the optimization
    algorithm (fine-tuning)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新权重初始化方案、梯度更新策略和优化算法（微调）
- en: Modify specific layers without altering other layers
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改特定层而不改变其他层
- en: Attach new layers to the model
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向模型中添加新层
- en: All these modifications can be applied using the transfer learning API. The
    DL4J transfer learning API comes with a builder class to support these modifications. We
    will add a fine-tuning configuration by calling the `fineTuneConfiguration()` builder
    method.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些修改都可以通过迁移学习API应用。DL4J迁移学习API提供了一个构建器类来支持这些修改。我们将通过调用`fineTuneConfiguration()`构建方法来添加微调配置。
- en: As we saw earlier, in step 4 we use `GraphBuilder` for transfer learning with
    computation graphs. Refer to our GitHub repository for concrete examples. Note
    that the transfer learning API returns an instance of the model from the imported
    model after applying all the modifications that were specified. The regular `Builder` class
    will build an instance of `MultiLayerNetwork` while `GraphBuilder` will build
    an instance of `ComputationGraph`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，在第4步中，我们使用`GraphBuilder`进行基于计算图的迁移学习。请参考我们的GitHub仓库以获取具体示例。请注意，迁移学习API会在应用所有指定的修改后，从导入的模型返回一个模型实例。常规的`Builder`类将构建一个`MultiLayerNetwork`实例，而`GraphBuilder`则会构建一个`ComputationGraph`实例。
- en: 'We may also be interested in making changes only in certain layers rather than
    making global changes across layers. The main motive is to apply further optimization
    to certain layers that are identified for further optimization. That also begs
    another question: How do we know the model details of a stored model? In order
    to specify layers that are to be kept unchanged, the transfer learning API requires
    layer attributes such as the layer name/layer number.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可能只对某些层进行更改，而不是在所有层之间进行全局更改。主要动机是对那些已识别的层进行进一步优化。这也引出了另一个问题：我们如何知道存储模型的详细信息？为了指定需要保持不变的层，迁移学习API要求提供层的属性，如层名/层号。
- en: 'We can get these using the `getLayerWiseConfigurations()` method, as shown
    here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`getLayerWiseConfigurations()`方法来获取这些信息，如下所示：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we execute the preceding, you should see the network configuration mentioned
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述操作后，你应该看到如下所示的网络配置：
- en: '![](img/439ce467-8a8e-4ad1-9005-46b639006f7b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/439ce467-8a8e-4ad1-9005-46b639006f7b.png)'
- en: Gist URL for complete network configuration JSON is at [https://gist.github.com/rahul-raj/ee71f64706fa47b6518020071711070b](https://gist.github.com/rahul-raj/ee71f64706fa47b6518020071711070b).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完整网络配置的Gist URL：[https://gist.github.com/rahul-raj/ee71f64706fa47b6518020071711070b](https://gist.github.com/rahul-raj/ee71f64706fa47b6518020071711070b)
- en: Neural network configurations such as the learning rate, the weights used in
    neurons, optimization algorithms used, layer-specific configurations, and so on
    can be verified from the displayed JSON content.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的配置，如学习率、神经元使用的权重、使用的优化算法、每层特定的配置等，可以从显示的JSON内容中验证。
- en: 'The following are some possible configurations from the DL4J transfer learning
    API to support model modifications. We need layer details (name/ID) in order to
    invoke these methods:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DL4J迁移学习API支持模型修改的一些可能配置。我们需要层的详细信息（名称/ID）来调用这些方法：
- en: '`setFeatureExtractor()`: To freeze the changes on specific layers'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setFeatureExtractor()`: 用于冻结特定层的变化'
- en: '`addLayer()`: To add one or more layers to the model'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`addLayer()`: 用于向模型中添加一个或多个层'
- en: '`nInReplace()/nOutReplace()`: Modifies the architecture of the specified layer
    by changing the `nIn` or `nOut` of the specified layer'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nInReplace()/nOutReplace()`: 通过修改指定层的`nIn`或`nOut`来改变指定层的架构'
- en: '`removeLayersFromOutput()`: Removes the last `n` layers from the model (from
    the point where an output layer must be added back)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`removeLayersFromOutput()`: 从模型中删除最后`n`个层（从需要添加回输出层的点开始）'
- en: Note that the last layer in the imported transfer learning model is a *dense*
    layer. because the DL4J transfer learning API doesn't enforce training configuration
    on imported model. So, we need to add an output layer to the model using the `addLayer()`
    method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，导入的迁移学习模型的最后一层是一个*全连接*层，因为DL4J的迁移学习API不会强制对导入的模型进行训练配置。所以，我们需要使用`addLayer()`方法向模型添加输出层。
- en: '`setInputPreProcessor()`: Adds the specified preprocessor to the specified
    layer'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setInputPreProcessor()`: 将指定的预处理器添加到指定的层'
- en: In step 5, we saw another way to apply transfer learning in DL4J, by using `TransferLearningHelper`.
    We discussed two ways in which it can be implemented. When you create `TransferLearningHelper` from
    the transfer learning builder, you need to specify `FineTuneConfiguration` as
    well. Values configured in `FineTuneConfiguration` will override for all non-frozen
    layers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们看到了在DL4J中应用迁移学习的另一种方式，使用`TransferLearningHelper`。我们讨论了它可以实现的两种方式。当你从迁移学习构建器创建`TransferLearningHelper`时，你还需要指定`FineTuneConfiguration`。在`FineTuneConfiguration`中配置的值将覆盖所有非冻结层的配置。
- en: There's a reason why `TransferLearningHelper` stands out from the regular way of
    handling transfer learning. Transfer learning models usually have frozen layers
    with constant values across training sessions. The purpose of frozen layers depends
    on the observation being made in the existing model performance. We have also
    mentioned the `setFeatureExtractor()` method, which is used to freeze specific
    layers. Layers can be skipped using this method. However, the model instance still
    holds the entire frozen and unfrozen part. So, we still use the entire model (including
    both the frozen and unfrozen parts) for computations during training.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransferLearningHelper` 与传统迁移学习处理方法的不同之处是有原因的。迁移学习模型通常具有冻结层，这些冻结层在整个训练过程中保持常数值。冻结层的作用取决于对现有模型性能的观察。我们也提到了
    `setFeatureExtractor()` 方法，用于冻结特定的层。使用这个方法可以跳过某些层。然而，模型实例仍然保留整个冻结和非冻结部分。因此，我们在训练期间仍然使用整个模型（包括冻结和非冻结部分）进行计算。'
- en: Using `TransferLearningHelper`, we can reduce the overall training time by creating
    a model instance of just the unfrozen part. The frozen dataset (with all the frozen
    parameters) is saved to disk and we use the model instance that refers to the
    unfrozen part for the training. If all we have to train is just one epoch, then
    `setFeatureExtractor()` and the transfer learning helper API will have almost
    the same performance. Let's say we have 100 layers with 99 frozen layers and we
    are doing *N* epochs of training. If we use `setFeatureExtractor()`, then we will
    end up doing a forward pass for those 99 layers *N* times, which essentially takes
    additional time and memory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `TransferLearningHelper`，我们可以通过仅创建非冻结部分的模型实例来减少整体训练时间。冻结的数据集（包括所有冻结参数）将保存到磁盘，我们使用指向非冻结部分的模型实例进行训练。如果我们只需训练一个
    epoch，那么 `setFeatureExtractor()` 和迁移学习助手 API 的性能几乎相同。假设我们有 100 层，其中 99 层是冻结的，并且我们进行
    *N* 次训练。如果我们使用 `setFeatureExtractor()`，那么我们将为这 99 层做 *N* 次前向传播，这本质上会增加额外的时间和内存消耗。
- en: In order to save training time, we create the model instance after saving the
    activation results of the frozen layers using the transfer learning helper API.
    This process is also known as featurization. The motive is to skip computations
    for frozen layers and train on unfrozen layers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省训练时间，我们在使用迁移学习助手 API 保存冻结层的激活结果后创建模型实例。这个过程也被称为特征化。目的是跳过冻结层的计算，并只训练非冻结层。
- en: As a prerequisite, frozen layers need to be defined using the transfer learning
    builder or explicitly mentioned in the transfer learning helper.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作为先决条件，需要使用迁移学习构建器定义冻结层，或者在迁移学习助手中明确提到这些冻结层。
- en: '`TransferLearningHelper` was created in step 3, as shown here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransferLearningHelper` 是在步骤 3 中创建的，如下所示：'
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding case, we explicitly specified freezing all of the layers up
    to `layer2` in the layer structure.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们明确指定了冻结层的结构，直到 `layer2`。
- en: 'In step 6, we discussed saving the dataset after featurization. After featurization,
    we save the data to disk. We will need to fetch this featurized data to train
    on top of it. Training/evaluation will be easier if we separate it and then save
    it to disk. The dataset can be saved to disk using the `save()` method, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 6 中，我们讨论了在特征化后保存数据集。特征化后，我们将数据保存到磁盘。我们将需要获取这些特征化数据以便在其上进行训练。如果将数据集分开并保存到磁盘，训练和评估会变得更加容易。数据集可以使用
    `save()` 方法保存到磁盘，如下所示：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`saveTodisk()` is the customary way to save a dataset for training or testing.
    The implementation is straightforward as it''s all about creating two different
    directories (train/test) and deciding on the range of files that can be used for
    train/test. We''ll leave that implementation to you. You can refer to our example
    in the GitHub repository (`SaveFeaturizedDataExample.java`): [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/11_Applying%20Transfer%20Learning%20to%20network%20models/sourceCode/cookbookapp/src/main/java/SaveFeaturizedDataExample.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java/SaveFeaturizedDataExample.java)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`saveTodisk()`是保存数据集用于训练或测试的常用方法。实现过程很简单，只需要创建两个不同的目录（train/test），并决定可以用于训练/测试的文件范围。具体实现留给你去做。你可以参考我们的GitHub仓库中的示例（`SaveFeaturizedDataExample.java`）：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/11_Applying%20Transfer%20Learning%20to%20network%20models/sourceCode/cookbookapp/src/main/java/SaveFeaturizedDataExample.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java/SaveFeaturizedDataExample.java)'
- en: 'In steps 7/8, we discussed training our neural network on top of featurized
    data. Our customer retention model follows `MultiLayerNetwork` architecture. This
    training instance will alter the network configuration for the unfrozen layers.
    Hence, we need to evaluate the unfrozen layers. In step 5, we evaluated just the
    model on the featurized test data as shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7/8步中，我们讨论了在特征化数据上训练我们的神经网络。我们的客户保持模型遵循`MultiLayerNetwork`架构。此训练实例将改变未冻结层的网络配置。因此，我们需要评估未冻结层。在第5步中，我们仅对特征化的测试数据进行了模型评估，如下所示：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If your network has the `ComputationGraph` structure, then you can use the `unfrozenGraph()`
    method instead of `unfrozenMLN()` to achieve the same result.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的网络具有`ComputationGraph`结构，则可以使用`unfrozenGraph()`方法来代替`unfrozenMLN()`，以获得相同的结果。
- en: There's more...
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Here are some important pre-trained models offered by the DL4J Model Zoo API:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DL4J模型库API提供的一些重要的预训练模型：
- en: '**VGG16**: VGG-16 referred to in this paper: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VGG16**：文中提到的VGG-16：[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)。'
- en: 'This is a very deep convolutional neural network targeting large-scale image
    recognition tasks. We can use transfer learning to train the model further. All
    we have to do is import VGG16 from the model zoo:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常深的卷积神经网络，旨在解决大规模图像识别任务。我们可以使用迁移学习进一步训练该模型。我们所要做的就是从模型库导入VGG16：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that the underlying architecture of the VGG16 model in the DL4J Model Zoo
    API is `ComputationGraph`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DL4J模型库API中VGG16模型的底层架构是`ComputationGraph`。
- en: '**TinyYOLO**: TinyYOLO is referred to in this paper: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TinyYOLO**：文中提到的TinyYOLO：[https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf)。'
- en: 'This is a real-time object detection model for fast and accurate image classification.
    We can apply transfer learning to this model as well after importing from it the
    model zoo, as shown here:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实时物体检测模型，用于快速且准确的图像分类。我们同样可以在从模型库导入该模型后应用迁移学习，示例如下：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the underlying architecture of the TinyYOLO model in the DL4J model
    zoo API is `ComputationGraph`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DL4J模型库API中TinyYOLO模型的底层架构是`ComputationGraph`。
- en: '**Darknet19**: Darknet19 is referred to in this paper: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Darknet19**：文中提到的Darknet19：[https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf)。'
- en: 'This is also known as YOLOV2, a faster object detection model for real-time
    object detection. We can apply transfer learning to this model after importing
    it from the model zoo, as shown here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为YOLOV2，它是一个用于实时物体检测的更快的物体检测模型。我们可以在从模型库导入该模型后，应用迁移学习，示例如下：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Fine-tuning the learning configurations
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调学习配置
- en: While performing transfer learning, we might want to update the strategy for
    how weights are initialized, which gradients are updated, which activation functions
    are to be used, and so on. For that purpose, we fine-tune the configuration. In
    this recipe, we will fine-tune the configuration for transfer learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行迁移学习时，我们可能希望更新权重初始化的策略、哪些梯度需要更新、哪些激活函数需要使用等等。为此，我们会对配置进行微调。在本节中，我们将微调迁移学习的配置。
- en: How to do it...
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Use `FineTuneConfiguration()` to manage modifications in the model configuration:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FineTuneConfiguration()`管理模型配置中的修改：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Call `fineTuneConfiguration()` to fine-tune the model configuration:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`fineTuneConfiguration()`来微调模型配置：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We saw a sample fine-tuning implementation in step 1\. Fine-tuning configurations
    are intended for default/global changes that are applicable across layers. So,
    if we want to remove specific layers from being considered for fine-tuning configuration,
    then we need to make those layers frozen. Unless we do that, all the current values
    for the specified modification type (gradients, activation, and so on) will be
    overridden in the new model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中我们看到了一个示例的微调实现。微调配置是针对适用于各层的默认/全局更改。因此，如果我们想要从微调配置中排除某些特定层，那么我们需要将这些层冻结。除非我们这么做，否则所有指定修改类型（如梯度、激活等）的当前值将在新模型中被覆盖。
- en: All the fine-tuning configurations mentioned above will be applied to all unfrozen
    layers, including output layers. So, you might get errors due to the addition
    of the `activation()` and `dropOut()` methods. Dropouts are relevant to hidden
    layers and we may have a different value range for output activation as well.
    A quick fix would be to remove these unless really needed. Otherwise, remove output
    layers from the model using the transfer learning helper API, apply fine-tuning,
    and then add the output layer back with a specific activation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有的微调配置将应用于所有未冻结的层，包括输出层。因此，你可能会遇到由于添加`activation()`和`dropOut()`方法而产生的错误。Dropout与隐藏层相关，输出激活可能有不同的值范围。一个快速的解决方法是，除非确实需要，否则删除这些方法。否则，使用迁移学习助手API从模型中删除输出层，应用微调，然后用特定的激活函数重新添加输出层。
- en: In step 2, if our original `MultiLayerNetwork` model has convolutional layers,
    then it is possible to make modifications in the convolution mode as well. As
    you might have guessed, this is applicable if you perform transfer learning for
    the image classification model from [Chapter 4](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml),
    *Building Convolutional Neural Networks*. Also, if your convolutional neural network
    is supposed to run in CUDA-enabled GPU mode, then you can also mention the cuDNN
    algo mode with your transfer learning API. We can specify an algorithmic approach
    (`PREFER_FASTEST`, `NO_WORKSPACE`, or `USER_SPECIFIED`) for cuDNN. It will impact
    the performance and memory usage of cuDNN. Use the `cudnnAlgoMode()` method with
    the `PREFER_FASTEST` mode to achieve performance improvements.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，如果我们的原始`MultiLayerNetwork`模型包含卷积层，那么也可以在卷积模式上进行修改。如你所料，这适用于从[第4章](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml)进行迁移学习的图像分类模型，*构建卷积神经网络*。此外，如果你的卷积神经网络需要在支持CUDA的GPU模式下运行，那么也可以在迁移学习API中提到cuDNN算法模式。我们可以为cuDNN指定一个算法模式（`PREFER_FASTEST`、`NO_WORKSPACE`
    或 `USER_SPECIFIED`）。这将影响cuDNN的性能和内存使用。使用`cudnnAlgoMode()`方法并设置`PREFER_FASTEST`模式可以提升性能。
- en: Implementing frozen layers
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现冻结层
- en: 'We might want to keep the training instance limited to certain layers, which
    means some layers can be kept frozen for the training instance, so we can focus
    on optimizing other layers while frozen layers are kept unchanged. We saw two
    ways of implementing frozen layers earlier: using the regular transfer learning
    builder and using the transfer learning helper. In this recipe, we will implement
    frozen layers for transfer layers.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望将训练实例限制为某些特定的层，这意味着某些层可以保持冻结，以便我们能够集中优化其他层，同时冻结层保持不变。之前我们看过两种实现冻结层的方法：使用常规的迁移学习构建器和使用迁移学习助手。在本例中，我们将为迁移层实现冻结层。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Define frozen layers by calling `setFeatureExtractor()`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`setFeatureExtractor()`定义冻结层：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Call `fit()` to start the training instance:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`fit()`来启动训练实例：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we used `MultiLayerNetwork` for demonstration purposes. For `MultiLayerNetwork`,
    `featurizeExtractionLayer` refers to the layer number (integer). For `ComputationGraph`, `featurizeExtractionLayer` refers
    to the layer name (`String`). By shifting frozen layer management to the transfer
    learning builder, it can be grouped along with all the other transfer learning
    functions, such as fine-tuning. This gives better modularization. However, the
    transfer learning helper has its own advantages, as we discussed in the previous
    recipe.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们使用了`MultiLayerNetwork`进行演示。对于`MultiLayerNetwork`，`featurizeExtractionLayer`指的是层号（整数）。对于`ComputationGraph`，`featurizeExtractionLayer`指的是层名称（`String`）。通过将冻结层管理移交给迁移学习构建器，它可以与其他所有迁移学习功能（例如微调）一起进行分组，从而实现更好的模块化。然而，迁移学习助手有其自身的优势，正如我们在前面的食谱中讨论的那样。
- en: Importing and loading Keras models and layers
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入和加载Keras模型及层
- en: There can be times when you want to import a model that is not available in
    the DL4J Model Zoo API. You might have created your own model in Keras/TensorFlow,
    or you might be using a pre-trained model from Keras/TensorFlow. Either way, we
    can still load models from Keras/TensorFlow using the DL4J model import API.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你可能希望导入一个在DL4J模型库API中不可用的模型。你可能已经在Keras/TensorFlow中创建了自己的模型，或者你可能在使用Keras/TensorFlow的预训练模型。无论哪种情况，我们仍然可以使用DL4J模型导入API从Keras/TensorFlow加载模型。
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This recipe assumes that you already have the Keras model (pre-trained/not
    pre-trained) set up and ready to be imported to DL4J. We will skip the details
    about how to save Keras models to disk as it is beyond the scope of this book.
    Usually, Keras models are stored in `.h5` format, but that isn''t a restriction
    as the model-import API can import from other formats as well. As a prerequisite,
    we need to add the following Maven dependency in `pom.xml`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设你已经设置好了Keras模型（无论是预训练还是未预训练），并准备将其导入到DL4J。我们将跳过关于如何将Keras模型保存到磁盘的细节，因为它超出了本书的范围。通常，Keras模型以`.h5`格式存储，但这并不是限制，因为模型导入API也可以导入其他格式。作为前提条件，我们需要在`pom.xml`中添加以下Maven依赖：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Use `KerasModelImport` to load an external `MultiLayerNetwork` model:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KerasModelImport`加载外部`MultiLayerNetwork`模型：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Use `KerasModelImport` to load an external `ComputationGraph` model:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KerasModelImport`加载外部`ComputationGraph`模型：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Use `KerasModelBuilder` to import an external model:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KerasModelBuilder`导入外部模型：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, we used `KerasModelImport` to load the external Keras model from
    disk. If the model was saved separately by calling `model.to_json()` and `model.save_weights()` 
    (in Keras), then we need to use the following variant:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们使用`KerasModelImport`从磁盘加载外部Keras模型。如果模型是通过调用`model.to_json()`和`model.save_weights()`（在Keras中）单独保存的，那么我们需要使用以下变体：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Note the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下事项：
- en: '`importKerasSequentialModelAndWeights()`: Imports and creates `MultiLayerNetwork`
    from the Keras model'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`importKerasSequentialModelAndWeights()`：从Keras模型导入并创建`MultiLayerNetwork`'
- en: '`importKerasModelAndWeights()`: Imports and creates `ComputationGraph` from
    the Keras model'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`importKerasModelAndWeights()`：从Keras模型导入并创建`ComputationGraph`'
- en: 'Consider the following implementation for the `importKerasModelAndWeights()`
    method to perform step 2:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下`importKerasModelAndWeights()`方法实现来执行步骤 2：
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The third attribute, `enforceTrainConfig`, is a Boolean type, which indicates
    whether to enforce a training configuration or not. Again, if the model was saved separately
    using the `model.to_json()` and `model.save_weights()` Keras calls, then we need
    to use the following variant:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个属性，`enforceTrainConfig`，是一个布尔类型，表示是否强制使用训练配置。如果模型是通过调用`model.to_json()`和`model.save_weights()`（在Keras中）单独保存的，那么我们需要使用以下变体：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In step 3, we discussed how to load `ComputationGraph` from the external model
    using `KerasModelBuilder`. One of the builder methods is `inputShape()`. It assigns
    input shape to the imported Keras model. DL4J requires the input shape to be specified.
    However, you don't have to deal with these if you go for the first two methods,
    discussed earlier, for the Keras model import. Those methods (`importKerasModelAndWeights()` and `importKerasSequentialModelAndWeights()`)
    internally make use of `KerasModelBuilder` to import models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们讨论了如何使用`KerasModelBuilder`从外部模型加载`ComputationGraph`。其中一个构建器方法是`inputShape()`。它为导入的Keras模型指定输入形状。DL4J要求指定输入形状。然而，如果你选择前面讨论的前两种方法来导入Keras模型，你就不需要处理这些问题。那些方法（`importKerasModelAndWeights()`和`importKerasSequentialModelAndWeights()`）在内部使用`KerasModelBuilder`来导入模型。
