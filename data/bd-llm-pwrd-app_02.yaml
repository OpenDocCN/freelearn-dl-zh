- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: LLMs for AI-Powered Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs for AI-Powered Applications
- en: 'In *Chapter 1*, *Introduction to Large Language Models*, we introduced **large
    language models** (**LLM**s) as powerful foundation models with generative capabilities
    as well as powerful common-sense reasoning. Now, the next question is: what should
    I do with those models?'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*大型语言模型简介*中，我们介绍了**大型语言模型**（LLM）作为具有生成能力和强大常识推理能力的强大基础模型。现在，下一个问题是：我应该用这些模型做什么？
- en: In this chapter, we are going to see how LLMs are revolutionizing the world
    of software development, leading to a new era of AI-powered applications. By the
    end of this chapter, you will have a clearer picture of how LLMs can be embedded
    in different application scenarios, thanks to the new AI orchestrator frameworks
    that are populating the market of AI development.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到LLM如何改变软件开发的世界，引领一个由AI驱动的应用程序的新时代。到本章结束时，你将更清楚地了解如何利用新的AI编排器框架将LLM嵌入到不同的应用程序场景中，这些框架正在充斥着AI开发市场。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: How LLMs are changing software development
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: How LLMs are changing software development
- en: The copilot system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协作者系统
- en: Introducing AI orchestrators to embed LLMs into applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将AI编排器引入以嵌入LLM到应用程序中
- en: How LLMs are changing software development
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: How LLMs are changing software development
- en: 'LLMs have proven to have extraordinary capabilities: from natural language
    understanding tasks (summarization, named entity recognition, and classification)
    to text generation, from common-sense reasoning to brainstorming skills. However,
    they are not just incredible by themselves. As discussed in *Chapter 1*, LLMs
    and, generally speaking, **large foundation models** (**LFMs**), are revolutionizing
    software development by serving as platforms for building powerful applications.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLM已被证明具有非凡的能力：从自然语言理解任务（摘要、命名实体识别和分类）到文本生成，从常识推理到头脑风暴技能。然而，它们并不仅仅如此出色。正如*第一章*中讨论的那样，LLM以及一般而言的**大型基础模型**（LFM），正通过作为构建强大应用程序的平台而彻底改变软件开发。
- en: In fact, instead of starting from scratch, today developers can make API calls
    to a hosted version of an LLM, with the option of customizing it for their specific
    needs, as we saw in the previous chapter. This shift allows teams to incorporate
    the power of AI more easily and efficiently into their applications, similar to
    the transition from single-purpose computing to time-sharing in the past.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，开发者今天不必从头开始，他们可以直接调用一个托管版本的LLM（大型语言模型），并根据他们的特定需求进行定制，正如我们在上一章所看到的。这种转变使得团队能够更轻松、更高效地将AI的力量融入他们的应用程序中，类似于过去从专用计算向分时系统的转变。
- en: 'But what does it mean, concretely, to incorporate LLMs within applications?
    There are two main aspects to consider when incorporating LLMs within applications:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但在应用程序中具体如何整合LLM呢？在将LLM整合到应用程序时，有两个主要方面需要考虑：
- en: '**The technical aspect**, which covers the *how*. Integrating LLMs into applications
    involves embedding them through REST API calls and managing them with AI orchestrators.
    This means setting up architectural components that allow seamless communication
    with the LLMs via API calls. Additionally, using AI orchestrators helps to efficiently
    manage and coordinate the LLMs’ functionality within the application, as we will
    discuss later in this chapter.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术方面**，涉及*如何*。将LLM集成到应用程序中涉及通过REST API调用嵌入它们，并使用AI编排器进行管理。这意味着设置允许通过API调用无缝与LLM通信的架构组件。此外，使用AI编排器有助于在应用程序中高效地管理和协调LLM的功能，正如我们将在本章后面讨论的那样。'
- en: '**The conceptual aspect**, which covers the *what*. LLMs bring a plethora of
    new capabilities that can be harnessed within applications. These capabilities
    will be explored in detail later in this book. One way to view LLMs’ impact is
    by considering them as a new category of software, often referred to as *copilot*.
    This categorization highlights the significant assistance and collaboration provided
    by LLMs in enhancing application functionalities.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念方面**，涉及*什么*。LLM带来了大量可以在应用程序中利用的新能力。这些能力将在本书的后续章节中详细探讨。一种看待LLM影响的方式是将它们视为一种新的软件类别，通常被称为*协作者*。这种分类突出了LLM在增强应用程序功能方面提供的重大帮助和协作。'
- en: We will delve into the technical aspect later on in this chapter, while the
    next section will cover a brand-new category of software – the copilot system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面深入探讨技术方面，而下一节将涵盖一个全新的软件类别——协作者系统。
- en: The copilot system
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 飞行员系统
- en: The copilot system is a new category of software that serves as an expert helper
    to users trying to accomplish complex tasks. This concept was coined by Microsoft
    and has already been introduced into its applications, such as M365 Copilot and
    the new Bing, now powered by GPT-4\. With the same framework that is used by these
    products, developers can now build their own copilots to embed within their applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 飞行员系统是一种新的软件类别，作为专家助手服务于试图完成复杂任务的用户。这个概念由微软提出，并已应用于其应用程序中，如M365 Copilot和新的Bing，现在由GPT-4驱动。使用这些产品相同的框架，开发者现在可以构建自己的飞行员系统，将其嵌入到他们的应用程序中。
- en: But what exactly is a copilot?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但飞行员系统究竟是什么？
- en: As the name suggests, copilots are meant to be AI assistants that work side
    by side with users and support them in various activities, from information retrieval
    to blog writing and posting, from brainstorming ideas to code review and generation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，飞行员系统旨在成为AI助手，与用户并肩工作，并在各种活动中支持他们，从信息检索到博客写作和发布，从头脑风暴想法到代码审查和生成。
- en: 'The following are some unique features of copilots:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是飞行员系统的一些独特特性：
- en: '**A copilot is powered by LLMs**, or, more generally, LFMs, meaning that these
    are the reasoning engines that make the copilot “intelligent.” This reasoning
    engine is one of its components, but not the only one. A copilot also relies on
    other technologies, such as apps, data sources, and user interfaces, to provide
    a useful and engaging experience for users. The following illustration shows how
    this works:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**飞行员系统由LLM驱动**，或者更普遍地说，由LFM（大型语言模型）驱动，这意味着这些是使飞行员“智能”的推理引擎。这个推理引擎是其组件之一，但并非唯一。飞行员还依赖于其他技术，如应用程序、数据源和用户界面，为用户提供有用且吸引人的体验。以下插图显示了这是如何工作的：'
- en: '![A cartoon of a person  Description automatically generated](img/B21714_02_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![一个人的卡通  自动生成的描述](img/B21714_02_01.png)'
- en: 'Figure 2.1: A copilot is powered by an LLM'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：飞行员系统由一个大型语言模型（LLM）驱动
- en: '**A copilot is designed to have a conversational user interface,** allowing
    users to interact with it using natural language. This reduces or even eliminates
    the knowledge gap between complex systems that need domain-specific taxonomy (for
    example, querying tabular data needs the knowledge of programming languages such
    as T-SQL) and users. Let’s look at an example of such a conversation:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**飞行员系统被设计成具有会话式用户界面**，允许用户使用自然语言与之交互。这减少了或甚至消除了需要特定领域分类法（例如，查询表格数据需要了解编程语言如T-SQL）的复杂系统与用户之间的知识差距。让我们看看这样一个对话的例子：'
- en: '![](img/B21714_02_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_02.png)'
- en: 'Figure 2.2: An example of a conversational UI to reduce the gap between the
    user and the database'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：一个用于减少用户与数据库之间差距的会话式用户界面示例
- en: '**A copilot has a scope.** This means that it is **grounded** to domain-specific
    data so that it is entitled to answer only within the perimeter of the application
    or domain.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**飞行员系统具有一个范围**。这意味着它被**定位**在特定领域的数据上，因此它只能在该应用或领域的范围内回答问题。'
- en: '**Definition**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Grounding is the process of using LLMs with information that is use case specific,
    relevant, and not available as part of the LLM’s trained knowledge. It is crucial
    for ensuring the quality, accuracy, and relevance of the output. For example,
    let’s say you want an LLM-powered application that assists you during your research
    on up-to-date papers (not included in the training dataset of your LLM). You also
    want your app to only respond if the answer is included in those papers. To do
    so, you will need to ground your LLM to the set of papers, so that your application
    will only respond within this perimeter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 定位是使用与特定用例相关、有用且不是LLM训练知识一部分的信息来使用LLM的过程。这对于确保输出的质量、准确性和相关性至关重要。例如，假设你想要一个由LLM驱动的应用程序，在研究最新的论文（不包括在你LLM的训练数据集中）时帮助你。你还希望你的应用程序只有在答案包含在这些论文中时才做出回应。为此，你需要将你的LLM定位到这些论文的集合上，这样你的应用程序就只能在这个范围内做出回应。
- en: Grounding is achieved through an architectural framework called retrieval-augmented
    generation (RAG), a technique that enhances the output of LLMs by incorporating
    information from an external, authoritative knowledge base before generating a
    response. This process helps to ensure that the generated content is relevant,
    accurate, and up to date.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 定位是通过一个名为检索增强生成（RAG）的架构框架实现的，这是一种在生成响应之前，通过结合外部权威知识库的信息来增强LLM输出的技术。这个过程有助于确保生成的内容是相关、准确且最新的。
- en: 'What is the difference between a copilot and a RAG? RAG can be seen as one
    of the architectural patterns that feature a copilot. Whenever we want our copilot
    to be grounded to domain-specific data, we use a RAG framework. Note that RAG
    is not the only architectural pattern that can feature a copilot: there are further
    frameworks such as function calling or multi-agents that we will explore throughout
    the book.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是一个副驾驶和RAG之间的区别？RAG可以被看作是一种具有副驾驶功能的架构模式。每当我们需要我们的副驾驶与特定领域的数据进行关联时，我们就使用RAG框架。请注意，RAG并不是唯一可以具有副驾驶功能的架构模式：书中还将探讨其他框架，如函数调用或多智能体。
- en: For example, let’s say we developed a copilot within our company that allows
    employees to chat with their enterprise knowledge base. As fun as it can be, we
    cannot provide users with a copilot they can use to plan their summer trip (it
    would be like providing users with a ChatGPT-like tool at our own hosting cost!);
    on the contrary, we want the copilot to be grounded only to our enterprise knowledge
    base so that it can respond only if the answer is pertinent to the domain-specific
    context.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在公司内部开发了一个副驾驶，允许员工与企业知识库进行聊天。虽然这很有趣，但我们不能为用户提供一个可以用来规划夏季旅行的副驾驶（这就像是在我们自己的托管成本下为用户提供一个类似ChatGPT的工具！）；相反，我们希望副驾驶仅关联到我们的企业知识库，以便它只能对特定领域的上下文相关的答案做出响应。
- en: 'The following figure shows an example of grounding a copilot system:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了副驾驶系统归一化的一个示例：
- en: '![](img/B21714_02_03.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_03.png)'
- en: 'Figure 2.3: Example of grounding a copilot'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：副驾驶归一化的示例
- en: '**The copilot’s capabilities can be extended by skills**, which can be code
    or calls to other models. In fact, the LLM (our reasoning engine) might have two
    kinds of limitations:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过技能扩展副驾驶的能力**，这些技能可以是代码或对其他模型的调用。实际上，LLM（我们的推理引擎）可能有两种类型的限制：'
- en: '**Limited parametric knowledge.** This is due to the knowledge base cutoff
    date, which is a physiological feature of LLMs. In fact, their training dataset
    will always be “outdated,” not in line with the current trends. This can be overcome
    by adding non-parametric knowledge with grounding, as previously seen.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的参数知识**。这是由于知识库截止日期，这是LLM（大型语言模型）的一个生理特征。事实上，它们的训练数据集总是会“过时”，不符合当前趋势。这可以通过添加具有归一化的非参数知识来克服，正如之前所看到的。'
- en: '**Lack of executive power.** This means that LLMs by themselves are not empowered
    to carry out actions. Let’s consider, for example, the well-known ChatGPT: if
    we ask it to generate a LinkedIn post about productivity tips, we will then need
    to copy and paste it onto our LinkedIn profile as ChatGPT is not able to do so
    by itself. That is the reason why we need plug-ins. Plug-ins are LLMs’ connectors
    toward the external world that serve not only as input sources to extend LLMs’
    non-parametric knowledge (for example, to allow a web search) but also as output
    sources so that the copilot can actually execute actions. For example, with a
    LinkedIn plug-in, our copilot powered by an LLM will be able not only to generate
    the post but also to post it online.![A cartoon of a person pointing at a plug-in  Description
    automatically generated](img/B21714_02_04.png)'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏执行权力**。这意味着LLM本身没有执行动作的权限。让我们以众所周知的ChatGPT为例：如果我们要求它生成一篇关于生产力技巧的LinkedIn帖子，那么我们就需要将其复制粘贴到我们的LinkedIn个人资料上，因为ChatGPT本身无法这样做。这就是为什么我们需要插件的原因。插件是LLM连接外部世界的中介，不仅作为扩展LLM非参数知识的输入源（例如，允许网络搜索），而且还作为输出源，以便副驾驶实际上可以执行动作。例如，通过LinkedIn插件，我们由LLM驱动的副驾驶不仅能够生成帖子，还可以将其发布到网上。![一个指向插件的卡通人物
    描述自动生成](img/B21714_02_04.png)'
- en: 'Figure 2.4: Example of Wikipedia and LinkedIn plug-ins'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.4：Wikipedia和LinkedIn插件的示例
- en: Note that the user’s prompt in natural language is not the only input the model
    processes. In fact, it is a crucial component of the backend logic of our LLM-powered
    applications and the set of instructions we provide to the model. This *metaprompt*
    or system message is the object of a new discipline called **prompt engineering**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用户的自然语言提示并不是模型处理的唯一输入。实际上，它是我们LLM（大型语言模型）驱动应用程序的后端逻辑以及我们提供给模型的一组指令的关键组成部分。这种*元提示*或系统消息是新学科**提示工程**的研究对象。
- en: '**Definition**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Prompt engineering is the process of designing and optimizing prompts to LLMs
    for a wide variety of applications and research topics. Prompts are short pieces
    of text that are used to guide the LLM’s output. Prompt engineering skills help
    to better understand the capabilities and limitations of LLMs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是设计并优化用于各种应用和研究主题的LLMs提示的过程。提示是用于引导LLM输出的简短文本片段。提示工程技能有助于更好地理解LLMs的能力和局限性。
- en: Prompt engineering involves selecting the right words, phrases, symbols, and
    formats that elicit the desired response from the LLM. Prompt engineering also
    involves using other controls, such as parameters, examples, or data sources,
    to influence the LLM’s behavior. For example, if we want our LLM-powered application
    to generate responses for a 5-year-old child, we can specify this in a system
    message similar to “Act as a teacher who explains complex concepts to 5-year-old
    children.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程涉及选择能够从LLM中获得期望响应的正确单词、短语、符号和格式。提示工程还涉及使用其他控制手段，如参数、示例或数据源，来影响LLM的行为。例如，如果我们想让我们的LLM驱动应用为5岁的孩子生成响应，我们可以在类似“扮演一个向5岁孩子解释复杂概念的教师”的系统消息中指定这一点。
- en: In fact, Andrej Karpathy, the previous Director of AI at Tesla, who returned
    to OpenAI in February 2023, tweeted that “English is the hottest new programming
    language.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，安德烈·卡帕西（Andrej Karpathy），特斯拉前AI总监，于2023年2月重返OpenAI，他在推特上表示：“英语是最新最热门的编程语言。”
- en: We will dive deeper into the concept of prompt engineering in *Chapter 4*, *Prompt
    Engineering*. In the next section, we are going to focus on the emerging AI orchestrators.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第4章“提示工程”中更深入地探讨提示工程的概念。在下一节中，我们将重点关注新兴的AI编排器。
- en: Introducing AI orchestrators to embed LLMs into applications
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍AI编排器以将LLMs嵌入应用中
- en: 'Earlier in this chapter, we saw that there are two main aspects to consider
    when incorporating LLMs within applications: a technical aspect and a conceptual
    aspect. While we can explain the conceptual aspect with the brand-new category
    of software called Copilot, in this section, we are going to further explore how
    to technically embed and orchestrate LLMs within our applications.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们了解到在将大型语言模型（LLMs）整合到应用中时，需要考虑两个主要方面：技术方面和概念方面。虽然我们可以用名为Copilot的全新软件类别来解释概念方面，但在本节中，我们将进一步探讨如何在我们的应用中技术性地嵌入和编排LLMs。
- en: The main components of AI orchestrators
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI编排器的主要组件
- en: 'From one side, the paradigm shift of foundation models implies a great simplification
    in the domain of AI-powered applications: after producing models, now the trend
    is consuming models. On the other side, many roadblocks might arise in developing
    this new kind of AI, since there are LLM-related components that are brand new
    and have never been managed before within an application life cycle. For example,
    there might be malicious actors that could try to change the LLM instructions
    (the system message mentioned earlier) so that the application does not follow
    the correct instructions. This is an example of a new set of security threats
    that are typical to LLM-powered applications and need to be addressed with powerful
    counterattacks or preventive techniques.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从一方面来看，基础模型范式转变意味着在AI驱动应用领域的大大简化：在生成模型之后，现在的趋势是消费模型。另一方面，在开发这种新型AI时可能会遇到许多障碍，因为LLM相关的组件是全新的，以前从未在应用生命周期中管理过。例如，可能会有恶意行为者试图更改LLM指令（前面提到的系统消息），以便应用不遵循正确的指令。这是一个典型的LLM驱动应用的新安全威胁示例，需要用强大的反击或预防技术来解决。
- en: 'The following is an illustration of the main components of such applications:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了此类应用的主要组件：
- en: '![A diagram of a computer program  Description automatically generated](img/B21714_02_05.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序图示 自动生成描述](img/B21714_02_05.png)'
- en: 'Figure 2.5: High-level architecture of LLM-powered applications'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：LLM驱动应用的高级架构
- en: 'Let’s inspect each of these components in detail:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细检查这些组件的每个部分：
- en: '**Models**: The model is simply the type of LLM we decide to embed in our application.
    There are two main categories of models:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：模型就是我们决定嵌入到我们的应用中的LLM类型。模型主要有两大类：'
- en: '**Proprietary LLMs:** Models that are owned by specific companies or organizations.
    Examples include GPT-3 and GPT-4, developed by OpenAI, or Bard, developed by Google.
    As their source code and architecture are not available, those models cannot be
    re-trained from scratch on custom data, yet they can be fine-tuned if needed.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专有 LLM**：由特定公司或组织拥有的模型。例如，由 OpenAI 开发的 GPT-3 和 GPT-4，或由 Google 开发的 Bard。由于它们的源代码和架构不可用，这些模型不能从零开始使用自定义数据进行重新训练，但在需要时可以进行微调。'
- en: '**Open-source:** Models with code and architecture freely available and distributed,
    hence they can also be trained from scratch on custom data. Examples include Falcon
    LLM, developed by Abu Dhabi’s **Technology Innovation Institute** (**TII**), or
    LLaMA, developed by Meta.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源模型**：带有代码和架构的模型可以自由获取和分发，因此也可以从零开始使用自定义数据进行训练。例如，由阿布扎比的**技术创新研究所**（**TII**）开发的
    Falcon LLM，或由 Meta 开发的 LLaMA。'
- en: We will dive deeper into the main set of LLMs available today in *Chapter 3*,
    *Choosing an LLM for Your Application*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第 3 章 *选择适合您应用的 LLM* 中更深入地探讨目前可用的主要 LLM 集合。
- en: '**Memory**: LLM applications commonly use a conversational interface, which
    requires the ability to refer back to earlier information within the conversation.
    This is achieved through a “memory” system that allows the application to store
    and retrieve past interactions. Note that past interactions could also constitute
    additional non-parametric knowledge to be added to the model. To achieve that,
    it is important to store all the past conversations – properly embedded – into
    VectorDB, which is at the core of the application’s data.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：LLM 应用通常使用对话界面，这需要能够在对话中回溯到之前的信息。这是通过一个“记忆”系统实现的，该系统允许应用程序存储和检索过去的交互。请注意，过去的交互也可能构成要添加到模型中的额外非参数知识。为了实现这一点，将所有过去的对话——适当嵌入——存储到
    VectorDB 中非常重要，这是应用程序数据的核心。'
- en: '**Definition**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: VectorDB is a type of database that stores and retrieves information based on
    vectorized embeddings, the numerical representations that capture the meaning
    and context of text. By using VectorDB, you can perform semantic search and retrieval
    based on the similarity of meanings rather than keywords. VectorDB can also help
    LLMs generate more relevant and coherent text by providing contextual understanding
    and enriching generation results. Some examples of VectorDBs are Chroma, Elasticsearch,
    Milvus, Pinecone, Qdrant, Weaviate, and **Facebook AI Similarity Search** (**FAISS**).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: VectorDB 是一种基于向量嵌入存储和检索信息的数据库，向量嵌入是捕获文本意义和上下文的数值表示。通过使用 VectorDB，您可以根据意义的相似性而不是关键词执行语义搜索和检索。VectorDB
    还可以通过提供上下文理解和丰富生成结果来帮助 LLM 生成更相关和连贯的文本。VectorDB 的例子包括 Chroma、Elasticsearch、Milvus、Pinecone、Qdrant、Weaviate
    和 **Facebook AI 相似性搜索**（**FAISS**）。
- en: FAISS, developed by Facebook (now Meta) in 2017, was one of the pioneering vector
    databases. It was designed for efficient similarity search and clustering of dense
    vectors and is particularly useful for multimedia documents and dense embeddings.
    It was initially an internal research project at Facebook. Its primary goal was
    to better utilize GPUs for identifying similarities related to user preferences.
    Over time, it evolved into the fastest available library for similarity search
    and can handle billion-scale datasets. FAISS has opened up possibilities for recommendation
    engines and AI-based assistant systems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: FAISS，由 Facebook（现在 Meta）于 2017 年开发，是早期向量数据库之一。它旨在进行密集向量的高效相似性搜索和聚类，特别适用于多媒体文档和密集嵌入。最初是
    Facebook 的内部研究项目。其主要目标是更好地利用 GPU 来识别与用户偏好相关的相似性。随着时间的推移，它发展成为可用的最快相似性搜索库，可以处理数十亿规模的数据集。FAISS
    为推荐引擎和基于 AI 的助手系统开辟了可能性。
- en: '**Plug-ins:** They can be seen as additional modules or components that can
    be integrated into the LLM to extend its functionality or adapt it to specific
    tasks and applications. These plug-ins act as add-ons, enhancing the capabilities
    of the LLM beyond its core language generation or comprehension abilities.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：它们可以被视为可以集成到 LLM 中以扩展其功能或适应特定任务和应用的附加模块或组件。这些插件作为附加组件，增强了 LLM 在其核心语言生成或理解能力之外的额外能力。'
- en: The idea behind plug-ins is to make LLMs more versatile and adaptable, allowing
    developers and users to customize the behavior of the language model for their
    specific needs. Plug-ins can be created to perform various tasks, and they can
    be seamlessly incorporated into the LLM’s architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 插件背后的理念是使LLM更加灵活和适应性强，允许开发者和用户根据他们的特定需求自定义语言模型的行为。可以创建插件来执行各种任务，并且它们可以无缝地集成到LLM的架构中。
- en: '**Prompts**: This is probably the most interesting and pivotal component of
    an LLM-powered application. We’ve already quoted, in the previous section, Andrej
    Karpathy’s affirmation that “English is the hottest new programming language,”
    and you will understand why in the upcoming chapters. Prompts can defined at two
    different levels:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示词**：这可能是LLM驱动应用中最有趣和关键的部分。我们在上一节中已经引用了安德烈·卡帕西（Andrej Karpathy）的断言，“英语是最新最热门的编程语言”，你将在接下来的章节中了解到原因。提示词可以在两个不同的层面上定义：'
- en: '**“Frontend,” or what the user sees**: A “prompt” refers to the input to the
    model. It is the way the user interacts with the application, asking things in
    natural language.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“前端”，或用户所见**：一个“提示词”指的是模型输入。这是用户与应用程序交互的方式，以自然语言提问。'
- en: '**“Backend,” or what the user does not see**: Natural language is not only
    the way to interact, as a user, with the frontend; it is also the way we “program”
    the backend. In fact, on top of the user’s prompt, there are many natural language
    instructions, or meta-promts, that we give to the model so that it can properly
    address the user’s query. Meta-prompts are meant to instruct the model to act
    as it is meant to. For example, if we want to limit our application to answer
    only questions related to the documentation we provided in VectorDB, we will specify
    the following in our meta-prompts to the model: “*Answer only if the question
    is related to the provided documentation.*”'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“后端”，或用户所见不到的部分**：自然语言不仅是用户与前端交互的方式；它也是我们“编程”后端的方式。事实上，在用户的提示词之上，有许多自然语言指令或元提示词，我们提供给模型以便它能够正确地处理用户的查询。元提示词的目的是指导模型按照预期的方式行动。例如，如果我们想限制我们的应用程序只回答与我们在VectorDB中提供的文档相关的问题，我们将在我们的元提示词中指定以下内容：“*只有当问题与提供的相关文档相关时才回答。*”'
- en: Finally, we get to the core of the high-level architecture shown in *Figure
    2.5*, that is, the **AI orchestrator**. With the AI orchestrator, we refer to
    lightweight libraries that make it easier to embed and orchestrate LLMs within
    applications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了*图2.5*所示的高级架构的核心，即**AI编排器**。有了AI编排器，我们指的是轻量级库，它使得在应用程序中嵌入和编排LLM变得更加容易。
- en: 'As LLMs went viral by the end of 2022, many libraries started arising in the
    market. In the next sections, we are going to focus on three of them: LangChain,
    Semantic Kernel, and Haystack.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到2022年底，随着LLM的病毒式传播，市场上出现了许多库。在接下来的几节中，我们将重点关注其中的三个：LangChain、Semantic Kernel和Haystack。
- en: LangChain
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain
- en: LangChain was launched as an open-source project by Harrison Chase in October
    2022\. It can be used both in Python and JS/TS. It is a framework for developing
    applications powered by language models, making them data-aware (with grounding)
    and agentic – which means they are able to interact with external environments.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain是由哈里森·查斯（Harrison Chase）于2022年10月作为一个开源项目推出的。它既可以用Python，也可以用JS/TS使用。它是一个用于开发由语言模型驱动的应用程序的框架，使它们具有数据感知性（具有基础）和代理性——这意味着它们能够与外部环境交互。
- en: 'Let’s take a look at the key components of LangChain:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看LangChain的关键组件：
- en: '![](img/B21714_02_06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_02_06.png)'
- en: 'Figure 2.6: LangChain’s components'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：LangChain的组件
- en: 'Overall, LangChain has the following core modules:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LangChain具有以下核心模块：
- en: '**Models**: These are the LLMs or LFMs that will be the engine of the application.
    LangChain supports proprietary models, such as those available in OpenAI and Azure
    OpenAI, and open-source models consumable from the **Hugging Face Hub**.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：这些是将成为应用程序引擎的LLM或LFM。LangChain支持专有模型，如OpenAI和Azure OpenAI中可用的模型，以及从**Hugging
    Face Hub**可消费的开源模型。'
- en: '**Definition**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Hugging Face is a company and a community that builds and shares state-of-the-art
    models and tools for natural language processing and other machine learning domains.
    It developed the Hugging Face Hub, a platform where people can create, discover,
    and collaborate on machine learning models and LLMs, datasets, and demos. The
    Hugging Face Hub hosts over 120k models, 20k datasets, and 50k demos in various
    domains and tasks, such as audio, vision, and language.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是一家公司和社区，它构建和分享自然语言处理和其他机器学习领域的最先进模型和工具。它开发了Hugging Face Hub，这是一个平台，人们可以在这里创建、发现和协作机器学习模型、LLMs、数据集和演示。Hugging
    Face Hub托管了超过120k个模型、20k个数据集和50k个不同领域和任务（如音频、视觉和语言）的演示。
- en: Alongside models, LangChain also offers many prompt-related components that
    make it easier to manage the prompt flow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型之外，LangChain还提供了许多与提示相关的组件，使得管理提示流程变得更加容易。
- en: '**Data connectors**: These refer to the building blocks needed to retrieve
    the additional external knowledge (for example, in RAG-based scenarios) we want
    to provide the model with. Examples of data connectors are document loaders or
    text embedding models.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据连接器**: 这些是指构建所需以检索我们希望提供给模型的额外外部知识（例如，在基于RAG的场景中）的构建块。数据连接器的例子包括文档加载器或文本嵌入模型。'
- en: '**Memory**: This allows the application to keep references to the user’s interactions,
    in both the short and long term. It is typically based on vectorized embeddings
    stored in VectorDB.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**: 这允许应用程序在短期和长期内保留对用户交互的引用。它通常基于存储在VectorDB中的向量嵌入。'
- en: '**Chains**: These are predetermined sequences of actions and calls to LLMs
    that make it easier to build complex applications that require chaining LLMs with
    each other or with other components. An example of a chain might be: take the
    user query, chunk it into smaller pieces, embed those chunks, search for similar
    embeddings in VectorDB, use the top three most similar chunks in VectorDB as context
    to provide the answer, and generate the answer.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链式操作**: 这些是预定义的动作序列和对LLMs的调用，使得构建需要将LLMs相互连接或与其他组件连接的复杂应用变得更加容易。一个链式操作的例子可能是：接收用户查询，将其分割成更小的部分，嵌入这些部分，在VectorDB中搜索相似的嵌入，使用VectorDB中前三个最相似的片段作为上下文来提供答案，并生成答案。'
- en: '**Agents**: Agents are entities that drive decision-making within LLM-powered
    applications. They have access to a suite of tools and can decide which tool to
    call based on the user input and the context. Agents are dynamic and adaptive,
    meaning that they can change or adjust their actions based on the situation or
    the goal.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**: 代理是在LLM驱动的应用程序中驱动决策的实体。它们可以访问一系列工具，并根据用户输入和上下文决定调用哪个工具。代理是动态和自适应的，这意味着它们可以根据情况或目标改变或调整其行为。'
- en: 'LangChain offers the following benefits:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain提供了以下好处：
- en: LangChain provides modular abstractions for the components we previously mentioned
    that are necessary to work with language models, such as prompts, memory, and
    plug-ins.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain为与语言模型一起工作所需的组件提供了模块化抽象，例如提示、内存和插件。
- en: Alongside those components, LangChain also offers pre-built **chains**, which
    are structured concatenations of components. Those chains can be pre-built for
    specific use cases or be customized.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了这些组件之外，LangChain还提供了预构建的**链式操作**，这些是组件的结构化连接。这些链式操作可以是针对特定用例预构建的，也可以是定制的。
- en: In *Part 2* of this book, we will go through a series of hands-on applications,
    all LangChain based. So, starting from *Chapter 5*, *Embedding LLMs within Your
    Applications*, we will focus much deeper on LangChain components and overall frameworks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*第2部分*中，我们将通过一系列基于LangChain的动手应用，从*第5章*开始，专注于将LLMs嵌入您的应用程序，我们将更深入地探讨LangChain组件和整体框架。
- en: Haystack
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Haystack
- en: Haystack is a Python-based framework developed by Deepset, a startup founded
    in 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. Deepset provides
    developers with the tools to build **natural language processing** (**NLP**)-based
    applications, and with the introduction of Haystack, they are taking them to the
    next level.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack是由Deepset开发的基于Python的框架，Deepset是一家成立于2018年的柏林初创公司，由Milos Rusic、Malte
    Pietsch和Timo Möller共同创立。Deepset为开发者提供了构建基于自然语言处理（NLP）的应用程序的工具，随着Haystack的引入，他们将这些工具提升到了新的水平。
- en: 'The following illustration shows the core components of Haystack:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图显示了Haystack的核心组件：
- en: '![](img/B21714_02_07.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_02_07.png)'
- en: 'Figure 2.7: Haystack’s components'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：Haystack的组件
- en: 'Let’s look at these components in detail:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些组件：
- en: '**Nodes**: These are components that perform a specific task or function, such
    as a retriever, a reader, a generator, a summarizer, etc. Nodes can be LLMs or
    other utilities that interact with LLMs or other resources. Among LLMs, Haystack
    supports proprietary models, such as those available in OpenAI and Azure OpenAI,
    and open-source models consumable from the Hugging Face Hub.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：这些是执行特定任务或功能的组件，例如检索器、阅读器、生成器、摘要器等。节点可以是LLM或其他与LLM或其他资源交互的实用工具。在LLM中，Haystack支持专有模型，例如OpenAI和Azure
    OpenAI中可用的模型，以及可以从Hugging Face Hub获取的开源模型。'
- en: '**Pipelines**: These are sequences of calls to nodes that perform natural language
    tasks or interact with other resources. Pipelines can be querying pipelines or
    indexing pipelines, depending on whether they perform searches on a set of documents
    or prepare documents for search. Pipelines are predetermined and hardcoded, meaning
    that they do not change or adapt based on the user input or the context.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：这些是调用执行自然语言任务或与其他资源交互的节点的序列。管道可以是查询管道或索引管道，具体取决于它们是在一组文档上执行搜索还是为搜索准备文档。管道是预先确定和硬编码的，这意味着它们不会根据用户输入或上下文改变或适应。'
- en: '**Agent**: This is an entity that uses LLMs to generate accurate responses
    to complex queries. An agent has access to a set of tools, which can be pipelines
    or nodes, and it can decide which tool to call based on the user input and the
    context. An agent is dynamic and adaptive, meaning that it can change or adjust
    its actions based on the situation or the goal.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：这是一个使用LLM生成对复杂查询的准确响应的实体。代理可以访问一组工具，这些工具可以是管道或节点，并且它可以根据用户输入和上下文来决定调用哪个工具。代理是动态和自适应的，这意味着它可以根据情况或目标改变或调整其行为。'
- en: '**Tools**: There are functions that an agent can call to perform natural language
    tasks or interact with other resources. Tools can be pipelines or nodes that are
    available to the agent and they can be grouped into toolkits, which are sets of
    tools that can accomplish specific objectives.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：这些是代理可以调用来执行自然语言任务或与其他资源交互的功能。工具可以是代理可用的管道或节点，它们可以被分组到工具包中，这些工具包是一组可以完成特定目标的工具。'
- en: '**DocumentStores**: These are backends that store and retrieve documents for
    searches. DocumentStores can be based on different technologies, also including
    VectorDB (such as FAISS, Milvus, or Elasticsearch).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档存储**：这些是用于搜索存储和检索文档的后端。文档存储可以基于不同的技术，也包括VectorDB（例如FAISS、Milvus或Elasticsearch）。'
- en: 'Some of the benefits offered by Haystack are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack提供的部分好处包括：
- en: '**Ease of use**: Haystack is user-friendly and straightforward. It’s often
    chosen for lighter tasks and rapid prototypes.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：Haystack易于使用且直观。它通常用于轻量级任务和快速原型。'
- en: '**Documentation quality**: Haystack’s documentation is considered high-quality,
    aiding developers in building search systems, question-answering, summarization,
    and conversational AI.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档质量**：Haystack的文档被认为是高质量的，有助于开发者构建搜索系统、问答、摘要和对话式AI。'
- en: '**End-to-end framework**: Haystack covers the entire LLM project life cycle,
    from data preprocessing to deployment. It’s ideal for large-scale search systems
    and information retrieval.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端框架**：Haystack涵盖了整个LLM项目生命周期，从数据预处理到部署。它非常适合大规模搜索系统和信息检索。'
- en: Another nice thing about Haystack is that you can deploy it as a REST API and
    it can be consumed directly.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haystack的另一个优点是你可以将其作为REST API部署，并且可以直接消费。
- en: Semantic Kernel
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义内核
- en: Semantic Kernel is the third open-source SDK we are going to explore in this
    chapter. It was developed by Microsoft, originally in C# and now also available
    in Python.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核是我们将在本章中探索的第三个开源SDK。它最初由微软开发，最初是用C#编写的，现在也支持Python。
- en: This framework takes its name from the concept of a “kernel,” which, generally
    speaking, refers to the core or essence of a system. In the context of this framework,
    a kernel is meant to act as the engine that addresses a user’s input by chaining
    and concatenating a series of components into pipelines, encouraging **function
    composition.**
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架的名字来源于“内核”的概念，通常指系统的核心或本质。在这个框架的上下文中，内核是指通过将一系列组件链接和连接成管道来处理用户输入的引擎，鼓励**函数组合**。
- en: '**Definition**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In mathematics, function composition is a way to combine two functions to create
    a new function. The idea is to use the output of one function as the input to
    another function, forming a chain of functions. The composition of two functions
    *f* and *g* is denoted as (*f ![](img/circle.png) g*), where the function *g*
    is applied first, followed by the function *f* ![](img/arrow.png)(*f ![](img/circle.png)
    g*)(*x*) = *f*(*g*(*x*)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，函数组合是将两个函数组合起来创建一个新函数的方法。其思想是使用一个函数的输出作为另一个函数的输入，形成一个函数链。两个函数 *f* 和 *g*
    的组合表示为 (*f ![](img/circle.png) g*)，其中函数 *g* 首先应用，然后是函数 *f* ![](img/arrow.png)(*f
    ![](img/circle.png) g*)(*x*) = *f*(*g*(*x*))。
- en: Function composition in computer science is a powerful concept that allows for
    the creation of more sophisticated and reusable code by combining smaller functions
    into larger ones. It enhances modularity and code organization, making programs
    easier to read and maintain.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学中的函数组合是一个强大的概念，它通过将较小的函数组合成较大的函数来创建更复杂和可重用的代码。它增强了模块化和代码组织，使程序更容易阅读和维护。
- en: 'The following is an illustration of the anatomy of Semantic Kernel:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对语义内核解剖结构的说明：
- en: '![](img/B21714_02_08.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_08.png)'
- en: 'Figure 2.8: Anatomy of Semantic Kernel'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：语义内核的解剖结构
- en: 'Semantic Kernel has the following main components:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核有以下主要组件：
- en: '**Models**: These are the LLMs or LFMs that will be the engine of the application.
    Semantic Kernel supports proprietary models, such as those available in OpenAI
    and Azure OpenAI, and open-source models consumable from the Hugging Face Hub.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：这些是将成为应用程序引擎的LLM或LFM。语义内核支持专有模型，例如OpenAI和Azure OpenAI中可用的模型，以及可以从Hugging
    Face Hub消费的开源模型。'
- en: '**Memory**: It allows the application to keep references to the user’s interactions,
    both in the short and long term. Within the framework of Semantic Kernel, memories
    can be accessed in three ways:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：它允许应用程序在短期和长期内保留对用户交互的引用。在语义内核的框架内，记忆可以通过以下三种方式访问：'
- en: '**Key**-**value pairs**: This consists of saving environment variables that
    store simple information, such as names or dates.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键值对**：这包括保存存储简单信息的环境变量，例如名称或日期。'
- en: '**Local storage**: This consists of saving information to a file that can be
    retrieved by its filename, such as a CSV or JSON file.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地存储**：这包括将信息保存到可以按其文件名检索的文件中，例如CSV或JSON文件。'
- en: '**Semantic memory search**: This is similar to LangChain’s and Haystack’s memory,
    as it uses embeddings to represent and search for text information based on its
    meaning.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义记忆搜索**：这与LangChain和Haystack的记忆类似，因为它使用嵌入来表示和基于其意义搜索文本信息。'
- en: '**Functions**: Functions can be seen as skills that mix LLM prompts and code,
    with the goal of making users’ asks interpretable and actionable. There are two
    types of functions:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数**：函数可以被视为混合LLM提示和代码的技能，目的是使用户的请求可解释和可操作。有两种类型的函数：'
- en: '**Semantic functions**: These are a type of templated prompt, which is a natural
    language query that specifies the input and output format for the LLM, also incorporating
    prompt configuration, which sets the parameters for the LLM.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义函数**：这是一种模板化提示，它是一个自然语言查询，指定了LLM的输入和输出格式，同时也结合了提示配置，该配置设置了LLM的参数。'
- en: '**Native functions**: These refer to the native computer code that can route
    the intent captured by the semantic function and perform the related task.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地函数**：这些指的是可以路由语义函数捕获的意图并执行相关任务的本地计算机代码。'
- en: To make an example, a semantic function could ask the LLM to write a short paragraph
    about AI, while a native function could actually post it on social media like
    LinkedIn.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，一个语义函数可以要求LLM写一段关于AI的短段落，而一个本地函数实际上可以在社交媒体如领英上发布。
- en: '**Plug-ins:** These are connectors toward external sources or systems that
    are meant to provide additional information or the ability to perform autonomous
    actions. Semantic Kernel offers out-of-the-box plug-ins, such as the Microsoft
    Graph connector kit, but you can build a custom plug-in by leveraging functions
    (both native and semantic, or a mix of the two).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：这些是连接到外部来源或系统的连接器，旨在提供额外信息或执行自主操作的能力。语义内核提供了一些开箱即用的插件，例如Microsoft Graph连接器套件，但你也可以通过利用函数（本地和语义，或两者的组合）来构建自定义插件。'
- en: '**Planner**: As LLMs can be seen as reasoning engines, they can also be leveraged
    to auto-create chains or pipelines to address new users’ needs. This goal is achieved
    with a planner, which is a function that takes as input a user’s task and produces
    the set of actions, plug-ins, and functions needed to achieve the goal.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划器**：由于LLM可以被视为推理引擎，它们也可以被利用来自动创建链或管道以满足新用户的需求。这个目标是通过规划器实现的，这是一个接受用户任务作为输入并生成实现目标所需的一组动作、插件和函数的功能。'
- en: 'Some benefits of Semantic Kernel are:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核的一些优点包括：
- en: '**Lightweight and C# support**: Semantic Kernel is more lightweight and includes
    C# support. It’s a great choice for C# developers or those using the .NET framework.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻量级和C#支持**：语义内核更加轻量级，并包含C#支持。对于C#开发者或使用.NET框架的用户来说，这是一个极佳的选择。'
- en: '**Wide range of use cases**: Semantic Kernel is versatile, supporting various
    LLM-related tasks.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的应用场景**：语义内核功能多样，支持各种与LLM相关的任务。'
- en: '**Industry-led**: Semantic Kernel was developed by Microsoft, and it is the
    framework the company used to build its own copilots. Hence, it is mainly driven
    by industry needs and asks, making it a solid tool for enterprise-scale applications.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业引领**：语义内核由微软开发，是公司构建其自己的协同飞行员所使用的框架。因此，它主要是由行业需求和问题驱动的，使其成为企业级应用的稳固工具。'
- en: How to choose a framework
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何选择框架
- en: 'Overall, the three frameworks offer, more or less, similar core components,
    sometimes called by a different taxonomy, yet covering all the blocks illustrated
    within the concept of the copilot system. So, a natural question might be: “Which
    one should I use to build my LLM-powered application?” Well, there is no right
    or wrong answer! All three are extremely valid. However, there are some features
    that might be more relevant for specific use cases or developers’ preferences.
    The following are some criteria you might want to consider:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这三个框架提供，或多或少，相似的核心组件，有时被称为不同的分类法，但涵盖了协同飞行员系统概念中所示的所有块。因此，一个自然的问题可能是：“我应该使用哪一个来构建我的LLM驱动应用程序？”
    好吧，没有正确或错误答案！三者都非常有效。然而，有些功能可能对特定用例或开发者的偏好更为相关。以下是一些您可能想要考虑的标准：
- en: '**The programming language you are comfortable with or prefer to use:** Different
    frameworks may support different programming languages or have different levels
    of compatibility or integration with them. For example, Semantic Kernel supports
    C#, Python, and Java, while LangChain and Haystack are mainly based on Python
    (even though LangChain also introduced JS/TS support). You may want to choose
    a framework that matches your existing skills or preferences, or that allows you
    to use the language that is most suitable for your application domain or environment.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您熟悉或偏好的编程语言**：不同的框架可能支持不同的编程语言，或者与它们的兼容性或集成程度不同。例如，语义内核支持C#、Python和Java，而LangChain和Haystack主要基于Python（尽管LangChain也引入了JS/TS支持）。您可能希望选择一个与您现有技能或偏好相匹配的框架，或者允许您使用最适合您的应用程序领域或环境的语言。'
- en: '**The type and complexity of the natural language tasks you want to perform
    or support:** Different frameworks may have different capabilities or features
    for handling various natural language tasks, such as summarization, generation,
    translation, reasoning, etc. For example, LangChain and Haystack provide utilities
    and components for orchestrating and executing natural language tasks, while Semantic
    Kernel allows you to use natural language semantic functions to invoke LLMs and
    services. You may want to choose a framework that offers the functionality and
    flexibility you need or want for your application goals or scenarios.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您想要执行或支持的自然语言任务类型和复杂性**：不同的框架可能具有不同的处理各种自然语言任务的能力或功能，例如摘要、生成、翻译、推理等。例如，LangChain和Haystack提供了编排和执行自然语言任务的实用程序和组件，而语义内核允许您使用自然语言语义函数调用LLM和服务。您可能希望选择一个提供您为应用程序目标或场景所需的功能和灵活性的框架。'
- en: '**The level of customization and control you need or want over the LLMs and
    their parameters or options**: Different frameworks may have different ways of
    accessing, configuring, and fine-tuning the LLMs and their parameters or options,
    such as model selection, prompt design, inference speed, output format, etc. For
    example, Semantic Kernel provides connectors that make it easy to add memories
    and models to your AI app, while LangChain and Haystack allow you to plug in different
    components for the document store, retriever, reader, generator, summarizer, and
    evaluator. You may want to choose a framework that gives you the level of customization
    and control you need or want over the LLMs and their parameters or options.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您需要或希望对LLM及其参数或选项进行定制和控制的程度**：不同的框架可能有不同的方式来访问、配置和微调LLM及其参数或选项，例如模型选择、提示设计、推理速度、输出格式等。例如，Semantic
    Kernel提供了连接器，使您能够轻松地将记忆和模型添加到您的AI应用中，而LangChain和Haystack允许您为文档存储、检索器、阅读器、生成器、摘要器和评估器插入不同的组件。您可能希望选择一个框架，该框架能够提供您对LLM及其参数或选项所需或希望达到的定制和控制程度。'
- en: '**The availability and quality of the documentation, tutorials, examples, and
    community support for the framework:** Different frameworks may have different
    levels of documentation, tutorials, examples, and community support that can help
    you learn, use, and troubleshoot the framework. For example, Semantic Kernel has
    a website with documentation, tutorials, examples, and a Discord community; LangChain
    has a GitHub repository with documentation, examples, and issues; Haystack has
    a website with documentation, tutorials, demos, blog posts, and a Slack community.
    You may want to choose a framework that has the availability and quality of documentation,
    tutorials, examples, and community support that can help you get started and solve
    problems with the framework.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架的文档、教程、示例和社区支持的可获得性和质量**：不同的框架可能有不同水平的文档、教程、示例和社区支持，这些可以帮助您学习、使用和调试框架。例如，Semantic
    Kernel有一个包含文档、教程、示例和Discord社区的网站；LangChain有一个包含文档、示例和问题的GitHub存储库；Haystack有一个包含文档、教程、演示、博客文章和Slack社区的网站。您可能希望选择一个具有可获取性和高质量的文档、教程、示例和社区支持的框架，以帮助您开始使用并解决框架中的问题。'
- en: 'Let’s briefly summarize the differences between these orchestrators:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要总结一下这些编排器之间的差异：
- en: '| **Feature** | **LangChain** | **Haystack** | **Semantic Kernel** |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | **LangChain** | **Haystack** | **Semantic Kernel** |'
- en: '| **LLM support** | Proprietary and open-source | Proprietary and open source
    | Proprietary and open source |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **LLM支持** | 商业和开源 | 商业和开源 | 商业和开源 |'
- en: '| **Supported languages** | Python and JS/TS | Python | C#, Java, and Python
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **支持的语言** | Python和JS/TS | Python | C#、Java和Python |'
- en: '| **Process orchestration** | Chains | Pipelines of nodes | Pipelines of functions
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **流程编排** | 链 | 节点管道 | 函数管道 |'
- en: '| **Deployment** | No REST API | REST API | No REST API |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **部署** | 无REST API | REST API | 无REST API |'
- en: '| **Feature** | **LangChain** | **Haystack** | **Semantic Kernel** |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | **LangChain** | **Haystack** | **Semantic Kernel** |'
- en: 'Table 2.1: Comparisons among the three AI orchestrators'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：三个AI编排器之间的比较
- en: Overall, all three frameworks offer a wide range of tools and integrations to
    build your LLM-powered applications, and a wise approach could be to use the one
    that is most in line with your current skills or the company’s overall approach.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，所有三个框架都提供了一系列的工具和集成，用于构建您的LLM驱动应用程序，一个明智的方法可能是使用与您当前技能或公司整体方法最一致的框架。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we delved into the new way of developing applications that
    LLMs have been paving, as we introduced the concept of the copilot and discussed
    the emergence of new AI orchestrators. Among those, we focused on three projects
    – LangChain, Haystack, and Semantic Kernel – and we examined their features, main
    components, and some criteria to decide which one to pick.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了LLM（大型语言模型）正在铺就的新应用开发方式，介绍了协作者的概念，并讨论了新AI编排器的出现。在这些中，我们重点关注了三个项目——LangChain、Haystack和Semantic
    Kernel——并检查了它们的功能、主要组件以及一些选择哪个项目的标准。
- en: Once we have decided on the AI orchestrator, another pivotal step is to decide
    which LLM(s) we want to embed into our applications. In *Chapter 3*, *Choosing
    an LLM for Your Application*, we are going to see the most prominent LLMs on the
    market today – both proprietary and open-source – and understand some decision
    criteria to pick the proper models with respect to the application use cases.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了人工智能编排器，另一个关键步骤就是决定我们想要嵌入到我们的应用程序中的LLM（大型语言模型）。在*第3章*，*为您的应用程序选择一个LLM*，我们将了解市场上最突出的LLM——既有专有也有开源的——并了解一些决策标准，以便根据应用程序用例选择合适的模型。
- en: References
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'LangChain repository: [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain仓库：[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)
- en: 'Semantic Kernel documentation: [https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义内核文档：[https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages)
- en: 'Copilot stack: [https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b](https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Copilot堆栈：[https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b](https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b)
- en: 'The Copilot system: [https://www.youtube.com/watch?v=E5g20qmeKpg](https://www.youtube.com/watch?v=E5g20qmeKpg)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Copilot系统：[https://www.youtube.com/watch?v=E5g20qmeKpg](https://www.youtube.com/watch?v=E5g20qmeKpg)
- en: Join our community on Discord
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code214329708533108046.png)'
