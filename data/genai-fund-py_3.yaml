- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Tracing the Foundations of Natural Language Processing and the Impact of the
    Transformer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪自然语言处理的基础及其对Transformer的影响
- en: The transformer architecture is a key advancement that underpins most modern
    generative language models. Since its introduction in 2017, it has become a fundamental
    part of **natural language processing** (**NLP**), enabling models such as **Generative
    Pre-trained Transformer 4** (**GPT-4**) and Claude to advance text generation
    capabilities significantly. A deep understanding of the transformer architecture
    is crucial for grasping the mechanics of modern **large language** **models**
    (**LLMs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构是支撑大多数现代生成语言模型的关键进步。自2017年引入以来，它已成为自然语言处理（NLP）的基础部分，使得如**生成预训练Transformer
    4**（GPT-4）和Claude等模型在文本生成能力上取得了显著进步。对Transformer架构的深入理解对于掌握现代**大型语言模型**（LLMs）的机制至关重要。
- en: In the previous chapter, we explored generative modeling techniques, including
    **generative adversarial networks** (**GANs**), diffusion models, and **autoregressive**
    (**AR**) transformers. We discussed how Transformers can be leveraged to generate
    images from text. However, transformers are more than just one generative approach
    among many; they form the basis for nearly all state-of-the-art generative language
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了生成建模技术，包括**生成对抗网络**（GANs）、扩散模型和**自回归**（AR）Transformer。我们讨论了如何利用Transformer从文本生成图像。然而，Transformer不仅仅是在众多生成方法中的一种；它是几乎所有最先进生成语言模型的基础。
- en: In this chapter, we’ll cover the evolution of NLP that ultimately led to the
    advent of the transformer architecture. We cannot cover all the critical steps
    forward, but we will attempt to cover major milestones, starting with early linguistic
    analysis techniques and statistical language modeling, followed by advancements
    in **recurrent neural networks** (**RNNs**) and **convolutional neural networks**
    (**CNNs**) that highlight the potential of **deep learning** (**DL**) for NLP.
    Our main objective will be to introduce the transformer—its basis in DL, its self-attention
    architecture, and its rapid evolution, which has led to LLMs and this phenomenon
    we call **generative** **AI** (**GenAI**).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍导致Transformer架构出现的NLP演变。我们无法涵盖所有关键的前进步骤，但我们将尝试涵盖主要里程碑，从早期的语言分析技术和统计语言建模开始，接着是**循环神经网络**（RNNs）和**卷积神经网络**（CNNs）的进步，这些进步突出了深度学习（DL）在NLP中的潜力。我们的主要目标是介绍Transformer——其在深度学习中的基础、其自注意力架构以及其快速演变，这些演变导致了LLMs以及我们称之为**生成****人工智能**（GenAI）的现象。
- en: Understanding the origins and mechanics of the transformer architecture is important
    for recognizing its groundbreaking impact. The principles and modeling capabilities
    introduced by transformers are carried forward by all modern language models built
    upon this framework. We will build our intuition for Transformers through historical
    context and hands-on implementation, as this foundational understanding is key
    to understanding the future of GenAI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Transformer架构的起源和机制对于认识其开创性影响至关重要。Transformer引入的原则和建模能力被所有基于此框架的现代语言模型所继承。我们将通过历史背景和动手实践来建立对Transformer的直觉，因为这种基础理解对于理解GenAI的未来至关重要。
- en: Early approaches in NLP
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP的早期方法
- en: Before the widespread use of **neural networks** (**NNs**) in language processing,
    NLP was largely grounded in methods that counted words. Two particularly notable
    techniques were **count vectors** and **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**). In essence, count vectors tallied up how often each word appeared
    in a document. Building on this, Dadgar et al. applied the TF-IDF algorithm (historically
    used for information retrieval) to text classification in 2016\. This method assigned
    weights to words based on their significance in one document relative to their
    occurrence across a collection of documents. These count-based methods were successful
    for tasks such as searching and categorizing. However, they presented a key limitation
    in that they could not capture the semantic relationships between words, meaning
    they could not interpret the nuanced meanings of words in context. This challenge
    paved the way for exploring NNs, offering a deeper and more nuanced way to understand
    and represent text.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在**神经网络**（**NNs**）在语言处理中得到广泛应用之前，自然语言处理（NLP）主要基于计数单词的方法。其中两种特别显著的技术是**计数向量**和**词频-逆文档频率**（**TF-IDF**）。本质上，计数向量统计了每个单词在文档中出现的频率。在此基础上，Dadgar等人于2016年将TF-IDF算法（历史上用于信息检索）应用于文本分类。这种方法根据单词在单个文档中的重要性相对于整个文档集合中出现的频率来分配权重。这些基于计数的方
    法在搜索和分类等任务中取得了成功。然而，它们存在一个关键限制，即无法捕捉单词之间的语义关系，这意味着它们无法解释单词在上下文中的细微含义。这一挑战为探索神经网络铺平了道路，提供了一种更深入、更细微的方式来理解和表示文本。
- en: Advent of neural language models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经语言模型的兴起
- en: In 2003, Yoshua Bengio’s team at the University of Montreal introduced the **Neural
    Network Language Model** (**NNLM**), a novel approach to language technology.
    The NNLM was designed to predict the next word in a sequence based on prior words
    using a particular type of **neural network** (**NN**). The design prominently
    featured hidden layers that learned word embeddings, which are compact vector
    representations capturing the core semantic meanings of words. This aspect was
    absent in count-based approaches. However, the NNLM was still limited in its ability
    to interpret longer sequences and handle large vocabularies. Despite these limitations,
    the NNLM sparked widespread exploration of NNs in language modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2003年，蒙特利尔大学的Yoshua Bengio团队引入了**神经网络语言模型**（**NNLM**），这是一种新颖的语言技术方法。NNLM旨在根据先前单词预测序列中的下一个单词，使用特定类型的**神经网络**（**NN**）。其设计突出显示了学习词嵌入的隐藏层，这些嵌入是紧凑的向量表示，能够捕捉单词的核心语义含义。这一点在基于计数的方
    法中是缺失的。然而，NNLM在解释较长序列和处理大型词汇方面的能力仍然有限。尽管存在这些限制，NNLM仍然激发了在语言建模中广泛探索神经网络的热情。
- en: The introduction of the NNLM highlighted the potential of NNs in language processing,
    particularly using word embeddings. Yet, its limitations with long sequences and
    large vocabulary signaled the need for further research.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络语言模型（NNLM）的引入突显了神经网络在语言处理中的潜力，尤其是在使用词嵌入方面。然而，它在处理长序列和大型词汇方面的局限性表明了进一步研究的必要性。
- en: Distributed representations
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式表示
- en: 'Following the inception of the NNLM, NLP research was propelled toward crafting
    high-quality word vector representations. These representations could be initially
    learned from extensive sets of unlabeled text data and later applied to downstream
    models for various tasks. The period saw the emergence of two prominent methods:
    **Word2Vec** (introduced by Mikolov et al., 2013) and **Global Vectors** (**GloVe**,
    introduced by Pennington et al., 2014). These methods applied **distributed representation**
    to craft high-quality word vector representations. Distributed representation
    portrays items such as words not as unique identifiers but as sets of continuous
    values or vectors. In these vectors, each value corresponds to a specific feature
    or characteristic of the item. Unlike traditional representations, where each
    item has a unique symbol, distributed representations allow these items to share
    features with others, enabling a more intelligent capture of underlying patterns
    in the data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络语言模型（NNLM）的诞生，自然语言处理（NLP）研究被推动着向构建高质量的词向量表示方向发展。这些表示最初可以从大量的未标记文本数据中学习，后来可以应用于各种任务的下游模型。这一时期见证了两种突出方法的兴起：**Word2Vec**（由
    Mikolov 等人于 2013 年引入）和 **全局向量**（**GloVe**，由 Pennington 等人于 2014 年引入）。这些方法将 **分布式表示**
    应用于构建高质量的词向量表示。分布式表示将诸如词语之类的项目不是作为唯一的标识符，而是作为连续值或向量的集合。在这些向量中，每个值对应于项目的一个特定特征或特性。与传统的表示不同，其中每个项目都有一个唯一的符号，分布式表示允许这些项目与其他项目共享特征，从而能够更智能地捕捉数据中的潜在模式。
- en: 'Let us elucidate this concept a bit further. Suppose we represent words based
    on two features: **Formality** and **Positivity**. We might have vectors such
    as the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步阐明这个概念。假设我们根据两个特征来表示词语：**Formality** 和 **Positivity**。我们可能会有以下向量：
- en: '`Formal: [1, 0]`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`Formal: [1, 0]`'
- en: '`Happy: [0, 1]`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`Happy: [0, 1]`'
- en: '`Cheerful: [0, 1]`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cheerful: [0, 1]`'
- en: In this example, each element in the vector corresponds to one of these features.
    In the vector for `Formal`, the `1` element under *Formality* indicates that the
    word is formal, while the `0` element under *Positivity* indicates neutrality
    in terms of positivity. Similarly, for `Happy` and `Cheerful`, the 1 element under
    *Positivity* indicates that these words have a positive connotation. This way,
    distributed representation captures the essence of words through vectors, allowing
    for shared features among different words to understand underlying patterns in
    data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，向量中的每个元素对应于这些特征中的一个。在 `Formal` 的向量中，位于 *Formality* 下的 `1` 元素表明该词是正式的，而位于
    *Positivity* 下的 `0` 元素表明在积极方面是中性的。同样，对于 `Happy` 和 `Cheerful`，位于 *Positivity* 下的
    `1` 元素表明这些词具有积极的含义。这样，分布式表示通过向量捕捉了词的精髓，允许不同词之间共享特征，从而理解数据中的潜在模式。
- en: Word2Vec employs a relatively straightforward approach where NNs are used to
    predict the surrounding words for each target word in a dataset. Through this
    process, the NN ascertains values or “weights” for each target word. These weights
    form a vector for each word in a **continuous vector space**—a mathematical space
    wherein each point represents a possible value a vector can take. In the context
    of NLP, each dimension of this space corresponds to a feature, and the position
    of a word in this space captures its semantic or linguistic relationships to other
    words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 采用了一种相对直接的方法，其中神经网络被用来预测数据集中每个目标词周围的词语。通过这个过程，神经网络为每个目标词确定了值或“权重”。这些权重形成了一个向量，代表每个词在
    **连续向量空间** 中的一个向量——这是一个数学空间，其中每个点代表向量可能取的某个可能值。在自然语言处理（NLP）的背景下，这个空间中的每个维度对应一个特征，而一个词在这个空间中的位置捕捉了它与其他词的语义或语言关系。
- en: These vectors form a **feature-based representation**—a type of representation
    where each dimension represents a different feature that contributes to the word’s
    meaning. Unlike a symbolic representation, where each word is represented as a
    unique symbol, a feature-based representation captures the semantic essence of
    words in terms of shared features.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量形成了一个 **基于特征的表示**——一种表示类型，其中每个维度代表对词的意义有贡献的不同特征。与将每个词表示为唯一符号的符号表示不同，基于特征的表示从共享特征的角度捕捉词的语义本质。
- en: On the other hand, GloVe adopts a different approach. It analyzes the **global
    co-occurrence statistics**—a count of how often words appear together in a large
    text corpus. GloVe learns vector representations that capture the relationships
    between words by analyzing these counts across the entire corpus. This method
    also results in a distributed representation of words in a continuous vector space,
    capturing **semantic similarity**—a measure of the degree to which two words are
    similar in meaning. In a continuous vector space, we can think about semantic
    similarity as the simple geometric proximity of vectors representing words.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GloVe采用了一种不同的方法。它分析**全局共现统计**——一个计数，表示单词在一个大型文本语料库中一起出现的频率。GloVe通过在整个语料库中分析这些计数来学习向量表示，这些表示捕捉了单词之间的关系。这种方法还导致单词在连续向量空间中的分布式表示，捕捉**语义相似性**——衡量两个单词在意义上相似程度的度量。在连续向量空间中，我们可以将语义相似性视为表示单词的向量之间的简单几何邻近性。
- en: 'To further illustrate, suppose we have a tiny corpus of text containing the
    following sentences:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明，假设我们有一个包含以下句子的微小文本语料库：
- en: “Coffee is hot.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: “咖啡很热。”
- en: “Ice cream is cold.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “冰淇淋很冷。”
- en: 'From this corpus, GloVe would notice that “*coffee*” co-occurs with “*hot*”
    and “*ice* *cream*” co-occurs with “*cold*.” Through its optimization process,
    it would aim to create vectors for these words in a way that reflects these relationships.
    In this oversimplified example, GloVe might produce a vector such as this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个语料库中，GloVe会注意到“*咖啡*”与“*热*”共现，“*冰淇淋*”与“*冷*”共现。通过其优化过程，它会试图以反映这些关系的方式为这些词创建向量。在这个过于简化的例子中，GloVe可能会产生如下向量：
- en: '`Coffee: [1, 0]`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`咖啡: [1, 0]`'
- en: '`Hot: [0.9, 0]`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`热: [0.9, 0]`'
- en: '`Ice Cream: [0, 1]`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`冰淇淋: [0, 1]`'
- en: '`Cold: [0, 0.9]`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`冷: [0, 0.9]`'
- en: Here, the closeness of the vectors for “*coffee*” and “*hot*” (and, similarly,
    “*ice* *cream*” and “*cold*”) in this space reflects the co-occurrence relationships
    observed in the corpus. The vector difference between “*coffee*” and “*hot*” might
    resemble the vector difference between “*ice* *cream*” and “*cold*,” capturing
    the contrasting temperature relationships in a geometric way within the vector
    space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个空间中，“*咖啡*”和“*热*”的向量之间的接近程度（以及类似地，“*冰淇淋*”和“*冷*”）反映了语料库中观察到的共现关系。在向量空间中，“*咖啡*”和“*热*”之间的向量差异可能与“*冰淇淋*”和“*冷*”之间的向量差异相似，以几何方式捕捉向量空间内对比温度关系。
- en: Both Word2Vec and GloVe excel at encapsulating relevant semantic information
    about words to represent an efficient **encoding**—a compact way of representing
    information that captures the essential features necessary for a task while reducing
    the dimensionality and complexity of the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec和GloVe都擅长封装关于单词的相关语义信息，以表示一种有效的**编码**——一种紧凑地表示信息的方式，同时捕捉完成任务所需的基本特征，同时减少数据的维度和复杂性。
- en: These methodologies in creating meaningful vector representations served as
    a step toward the adoption of **transfer learning** in NLP. The vectors provide
    a shared semantic foundation that facilitates the transfer of learned relationships
    across varying tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些创建有意义的向量表示的方法为在自然语言处理（NLP）中采用**迁移学习**迈出了第一步。这些向量提供了一个共享的语义基础，促进了在不同任务之间学习关系的迁移。
- en: Transfer Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习
- en: GloVe and other methods of deriving distributed representations paved the way
    for transfer learning in NLP. By creating rich vector representations of words
    that encapsulate semantic relationships, these methods provided a foundational
    understanding of text. The vectors serve as a shared base of knowledge that can
    be applied to different tasks. When a model, initially trained on one task, is
    utilized for another, the pre-learned vector representations aid in preserving
    the semantic understanding, thereby reducing the data or training needed for the
    new task. This practice of transferring acquired knowledge has become fundamental
    for efficiently addressing a range of NLP tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe和其他推导分布式表示的方法为自然语言处理中的迁移学习铺平了道路。通过创建包含语义关系的丰富向量表示，这些方法提供了对文本的基础理解。这些向量作为共享的知识基础，可以应用于不同的任务。当模型最初在一个任务上训练后，用于另一个任务时，预先学习的向量表示有助于保留语义理解，从而减少新任务所需的数据或训练。这种转移已获得的知识的做法已成为高效解决一系列自然语言处理任务的基础。
- en: 'Consider a model trained to understand sentiments (positive or negative) in
    movie reviews. Through training, this model has learned distributed representations
    of words, capturing sentiment-related nuances. Now, suppose there is a new task:
    understanding sentiments in product reviews. Instead of training a new model from
    the beginning, transfer learning allows us to use the distributed representations
    from the movie review task to initiate training for the product review task. This
    could lead to quicker training and better performance, especially with limited
    data for the product review task.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个训练用于理解电影评论中情感（正面或负面）的模型。通过训练，这个模型已经学会了词语的分布式表示，捕捉了与情感相关的细微差别。现在，假设有一个新的任务：理解产品评论中的情感。而不是从头开始训练一个新的模型，迁移学习允许我们使用电影评论任务中的分布式表示来启动产品评论任务的训练。这可能导致更快的训练和更好的性能，尤其是在产品评论任务数据有限的情况下。
- en: The effectiveness of transfer learning, bolstered by distributed representations
    from methods such as GloVe, highlighted the potential of leveraging pre-existing
    knowledge for new tasks. It was a precursor to the integration of NNs in NLP,
    highlighting the benefits of utilizing learned representations across tasks. The
    advent of NNs in NLP brought about models capable of learning even richer representations,
    further amplifying the impact and scope of transfer learning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的有效性，得益于GloVe等方法的分布式表示，凸显了利用现有知识进行新任务的潜力。它是神经网络在NLP中集成的先导，突出了在任务间利用学习到的表示的优势。神经网络在NLP中的兴起带来了能够学习更丰富表示的模型，进一步放大了迁移学习的影响和范围。
- en: Advent of NNs in NLP
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP 中神经网络（NNs）的兴起
- en: The advent of NNs in NLP marked a monumental shift in the field’s capability
    to understand and process language. Building upon the groundwork laid by methodologies
    such as Word2Vec, GloVe, and the practice of transfer learning, NNs introduced
    a higher level of abstraction and learning capacity. Unlike previous methods that
    often relied on hand-crafted features, NNs could automatically learn intricate
    patterns and relationships from data. This ability to learn from data propelled
    NLP into a new era where models could achieve unprecedented levels of performance
    across a myriad of language-related tasks. The emergence of architectures such
    as CNNs and RNNs, followed by the revolutionary transformer architecture, showcased
    the remarkable versatility and efficacy of NNs in tackling complex NLP challenges.
    This transition not only accelerated the pace of innovation but also expanded
    the horizon of what could be achieved in understanding human language computationally.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在NLP中的兴起标志着该领域在理解和处理语言能力方面的重大转变。在Word2Vec、GloVe等方法的基石上，以及迁移学习实践的基础上，神经网络引入了更高层次的抽象和学习能力。与以往依赖手工特征的方法不同，神经网络能够自动从数据中学习复杂的模式和关系。这种从数据中学习的能力推动了NLP进入一个新时代，模型能够在众多与语言相关的任务上实现前所未有的性能水平。卷积神经网络（CNNs）和循环神经网络（RNNs）的兴起，以及革命性的transformer架构的出现，展示了神经网络在解决复杂NLP挑战方面的非凡适应性和有效性。这一转变不仅加速了创新的步伐，也扩展了在计算上理解人类语言所能实现的范围。
- en: Language modeling with RNNs
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用RNN进行语言建模
- en: Despite how well these distributed word vectors excelled at encoding local semantic
    relationships, modeling long-range dependencies would require a more sophisticated
    network architecture. This led to the use of RNNs. RNNs (originally introduced
    by Elman in 1990) are a type of NN architecture that processes data sequences
    by iterating through each element of the sequence while maintaining a dynamic
    internal state that captures information about the previous elements. Unlike traditional
    **feedforward networks** (**FNNs**) that processed each input independently, RNNs
    introduced iterations that allowed information to be passed from one step in the
    sequence to the next, enabling them to capture temporal dependencies in data.
    The iterative processing and dynamic updating in NNs enable them to learn and
    represent relationships within the text. These networks can capture contextual
    connections and interdependencies across sentences or even entire documents.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些分布式词向量在编码局部语义关系方面表现出色，但建模长距离依赖需要更复杂的网络架构。这导致了RNNs的使用。RNNs（最初由Elman在1990年提出）是一种NN架构，通过迭代序列中的每个元素并保持一个动态内部状态来处理数据序列，该状态捕获了先前元素的信息。与处理每个输入独立的传统**前馈网络**（FNNs）不同，RNNs引入了迭代，允许信息从一个序列步骤传递到下一个步骤，使它们能够捕捉数据中的时间依赖性。NNs中的迭代处理和动态更新使它们能够学习和表示文本中的关系。这些网络可以捕捉句子或甚至整个文档中的上下文联系和相互依赖性。
- en: However, standard RNNs had technical limitations when dealing with long sequences.
    This led to the development of **long short-term memory** (**LSTM**) networks.
    LSTMs were first introduced by Hochreiter and Schmidhuber in 1997\. They were
    a special class of RNNs designed to address the **vanishing gradient** problem,
    which is the challenge where the network cannot learn from earlier parts of a
    sequence as the sequence gets longer. LSTMs applied a unique gating architecture
    to control the flow of information within the network, enabling them to maintain
    and access information over long sequences without suffering from the vanishing
    gradient problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，标准RNNs在处理长序列时存在技术限制。这导致了**长短期记忆**（LSTM）网络的发展。LSTMs最早由Hochreiter和Schmidhuber在1997年提出。它们是一类特殊的RNNs，旨在解决**梯度消失**问题，即随着序列变长，网络无法从序列的早期部分学习。LSTMs应用了一种独特的门控架构来控制网络内部的信息流动，使它们能够在不遭受梯度消失问题的前提下，在长序列中保持和访问信息。
- en: 'The name “**long short-term memory**” refers to the network’s ability to keep
    track of information over both short and long sequences of data:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: “**长短期记忆**”这个名字指的是网络在处理短序列和长序列数据时跟踪信息的能力：
- en: '**Short-term**: LSTMs can remember recent information, which is useful for
    understanding the current context. For example, in language modeling, knowing
    the last few words can be crucial for predicting the next word. Consider a phrase
    such as, “The cat, which already ate a lot, was not hungry.” As the LSTM processes
    the text, when it reaches the word “not,” the recent information that the cat
    “ate a lot” is crucial to predict the next word, “hungry,” accurately.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**短期**：LSTMs能够记住最近的信息，这对于理解当前上下文很有用。例如，在语言建模中，知道最后几个单词对于预测下一个单词至关重要。考虑一个短语，例如，“The
    cat, which already ate a lot, was not hungry.” 当LSTM处理文本时，当它到达单词“not”时，最近的信息即猫“eat
    a lot”对于准确预测下一个单词“hungry”至关重要。'
- en: '**Long-term**: Unlike standard RNNs, LSTMs are also capable of retaining information
    from many steps back in the sequence, which is particularly useful for long-range
    dependencies, where a piece of information early in a sentence could be important
    for understanding a word much later in the sequence. In the same phrase, the information
    that “The cat” is the subject of the sentence is introduced early on. This information
    is crucial later to understand who “was not hungry” as it processes the later
    part of the sentence.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期**：与标准RNNs不同，LSTMs还能够保留序列中很多步骤之前的信息，这对于处理长距离依赖特别有用，因为在句子中较早的信息可能对理解序列中较晚的部分的单词很重要。在同一个短语中，关于“The
    cat”是句子主语的信息是在一开始就引入的。这些信息在稍后理解“was not hungry”时至关重要，因为它处理句子的后半部分。'
- en: The **M** or **memory** in LSTMs is maintained through a unique architecture
    that employs three gating mechanisms—input, output, and forget gates. These gates
    control the flow of information within the network, deciding what information
    should be kept, discarded, or used at each step in the sequence, enabling LSTMs
    to maintain and access information over long sequences. Effectively, these gates
    and the network state allowed LTSMs to carry the “memory” across time steps, ensuring
    that valuable information was retained throughout the processing of the sequence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM中的**M**或**记忆**通过一种独特的架构来维持，该架构采用三个门控机制——输入门、输出门和遗忘门。这些门控机制控制网络中的信息流，决定在每个序列步骤中应该保留、丢弃或使用哪些信息，使LSTMs能够在长序列中维持和访问信息。实际上，这些门控机制和网络状态允许LTSMs在时间步长之间携带“记忆”，确保在整个序列处理过程中保留了有价值的信息。
- en: Ultimately, LSTMs obtained state-of-the-art results on many language modeling
    and text classification benchmarks. They became the dominant NN architecture for
    NLP tasks due to their ability to capture short- and long-range contextual relationships.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，LSTMs在许多语言建模和文本分类基准测试中获得了最先进的结果。由于它们能够捕捉短距离和长距离的上下文关系，LSTMs成为了NLP任务的占主导地位的NN架构。
- en: The success of LSTMs demonstrated the potential of neural architectures in capturing
    the complex relationships inherent in language, significantly advancing the field
    of NLP. However, the continuous pursuit of more efficient and effective models
    led the community toward exploring other NN architectures.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的成功展示了神经网络架构在捕捉语言中固有的复杂关系方面的潜力，极大地推动了NLP领域的发展。然而，对更高效和有效模型的持续追求使社区转向探索其他NN架构。
- en: Rise of CNNs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN的兴起
- en: Around 2014, the NLP domain witnessed a rise in the popularity of CNNs for tackling
    NLP tasks, a notable shift led by Yoon Kim. CNNs (originally brought forward by
    LeCun et al. for image recognition) operate based on convolutional layers that
    scan the input by moving a filter (or kernel) across the input data, at each position
    calculating the dot product of the filter’s weights and the input data. In NLP,
    these layers work over local n-gram windows (consecutive sequences of *n* words)
    to identify patterns or features, such as specific sequences of words or characters
    in the text. Employing convolutional layers over local n-gram windows, CNNs scan
    and analyze the data to detect initial patterns or features. Following this, pooling
    layers are used to reduce the dimensionality of the data, which helps in both
    reducing computational complexity and focusing on the most salient features identified
    by the convolutional layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2014年，NLP领域见证了CNN在处理NLP任务时受欢迎程度的上升，这一显著的转变是由Yoon Kim引领的。CNN（最初由LeCun等人提出用于图像识别）基于卷积层，通过移动过滤器（或核）在输入数据上扫描，在每个位置计算过滤器权重与输入数据的点积。在NLP中，这些层在局部n-gram窗口（连续的*n*个单词序列）上工作，以识别模式或特征，如文本中的特定单词或字符序列。通过在局部n-gram窗口上使用卷积层，CNN扫描和分析数据以检测初始模式或特征。随后，池化层被用来降低数据的维度，这有助于减少计算复杂度并关注卷积层识别的最显著特征。
- en: Combining convolutional and pooling layers, CNNs can extract hierarchical features.
    These features represent information at different levels of abstraction by combining
    simpler, lower-level features to form more complex, higher-level features. In
    NLP, this process might start with detecting basic patterns such as common word
    pairs or phrases in the initial layers, progressing to recognizing more abstract
    concepts such as semantic relationships in the higher layers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合卷积和池化层，CNN可以提取层次特征。这些特征通过结合更简单、低级的特征来形成更复杂、高级的特征，从而代表不同抽象层次的信息。在NLP中，这个过程可能从检测初始层中的基本模式开始，例如常见的词对或短语，然后发展到识别更高层次的概念，如语义关系。
- en: 'For comparison, we again consider a scenario where a CNN is employed to analyze
    and categorize customer reviews into positive, negative, or neutral sentiments:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们再次考虑一个场景，其中使用CNN来分析和分类客户评论为正面、负面或中性情感：
- en: '**Lower-level features (initial layers)**: The CNN might identify basic patterns
    such as common word pairs or phrases in the initial layers. For instance, it might
    recognize phrases such as “great service,” “terrible experience,” or “not happy.”'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低级特征（初始层）**：CNN可能在初始层识别基本模式，例如常见的词对或短语。例如，它可能识别到短语如“优质服务”、“糟糕的经历”或“不高兴”。'
- en: '**Intermediate-level features (middle layers)**: As data progresses through
    the network, middle layers might start recognizing more complex patterns, such
    as negations (“not good”) or contrasts (“good but expensive”).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中间层特征（中间层）**：随着数据在网络中传递，中间层可能会开始识别更复杂的模式，例如否定（“不好”）或对比（“好但贵”）。'
- en: '**Higher-level features (later layers)**: The CNN could identify abstract concepts
    such as overall sentiment in the later layers. For instance, it might deduce a
    positive sentiment from phrases such as “excellent service” or “loved the ambiance”
    and a negative sentiment from phrases such as “worst experience” or “terrible
    food.”'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级特征（后续层）**：CNNs（卷积神经网络）可以在后续层识别抽象概念，例如整体情感。例如，它可能从“优秀的服务”或“喜欢氛围”之类的短语中推断出积极情感，而从“最糟糕的经历”或“糟糕的食物”之类的短语中推断出消极情感。'
- en: In this way, CNNs inherently learn higher-level abstract representations of
    text. Although they lack the sequential processing characteristic of RNNs, they
    offer a computational advantage due to their inherent **parallelism** or ability
    to process multiple parts of the data simultaneously. Unlike RNNs, which process
    sequences iteratively and require the previous step to be completed before proceeding
    to the next, CNNs can process various parts of the input data in parallel, significantly
    speeding up training times.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，CNNs（卷积神经网络）本质上学习了文本的高级抽象表示。尽管它们缺乏RNNs（循环神经网络）的序列处理特性，但它们由于固有的**并行性**或同时处理数据多个部分的能力，提供了计算上的优势。与RNNs不同，RNNs是迭代处理序列，并且需要在进行下一步之前完成前一步，而CNNs可以并行处理输入数据的各个部分，从而显著加快训练时间。
- en: CNNs, while efficient, have a limitation in their convolution operation, which
    only processes local data from smaller or nearby regions, thereby missing relationships
    across more significant portions of the entire input data, referred to as global
    information. This gave rise to attention-augmented convolutional networks that
    integrate self-attention with convolutions to address this limitation. Self-attention,
    initially used in sequence and generative modeling, was adapted for visual tasks
    such as image classification, enabling the network to process and capture relationships
    across the entire input data. However, attention augmentation, which combines
    convolutions and self-attention, yielded the best results. This method retained
    the computational efficiency of CNNs and captured global information, marking
    an advancement in image classification and object detection tasks. We will discuss
    self-attention in detail later as it became a critical component of the transformer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CNNs（卷积神经网络）效率高，但它们的卷积操作存在局限性，该操作仅处理来自较小或附近区域的局部数据，从而错过了整个输入数据中更大部分的关系，这被称为全局信息。这导致了结合自注意力与卷积的注意力增强卷积网络的出现，以解决这一局限性。自注意力最初用于序列和生成建模，后来被用于视觉任务，如图像分类，使网络能够处理和捕获整个输入数据中的关系。然而，结合卷积和自注意力的注意力增强产生了最佳结果。这种方法保留了CNNs的计算效率并捕获了全局信息，标志着图像分类和目标检测任务中的进步。我们将在后面详细讨论自注意力，因为它成为了transformer的关键组成部分。
- en: The ability of CNNs to process multiple parts of data simultaneously marked
    a significant advancement in computational efficiency, paving the way for further
    innovations in NN architectures for NLP. As the field progressed, a pivotal shift
    occurred with the advent of attention-augmented NNs, introducing a new paradigm
    in how models handle sequential data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs（卷积神经网络）能够同时处理数据的多个部分，这标志着计算效率的显著提升，为NLP（自然语言处理）中NN（神经网络）架构的进一步创新铺平了道路。随着该领域的发展，随着注意力增强型NNs的出现，发生了一个关键的转变，引入了模型处理序列数据的新范式。
- en: The emergence of the Transformer in advanced language models
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级语言模型中Transformer的出现
- en: In 2017, inspired by the capabilities of CNNs and the innovative application
    of attention mechanisms, Vaswani et al. introduced the transformer architecture
    in the seminal paper *Attention is All You Need*. The original transformer applied
    several novel methods, particularly emphasizing the instrumental impact of attention.
    It employed a **self-attention mechanism**, allowing each element in the input
    sequence to focus on distinct parts of the sequence, capturing dependencies regardless
    of their positions in a structured manner. The term “self” in “self-attention”
    refers to how the attention mechanism is applied to the input sequence itself,
    meaning each element in the sequence is compared to every other element to determine
    its attention scores.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，受CNN的能力和注意力机制的创新应用启发，Vaswani等人发表了开创性论文《Attention is All You Need》，在其中引入了变压器架构。原始的变压器应用了多种新颖的方法，特别强调了注意力的重要性。它采用了**自注意力机制**，允许输入序列中的每个元素专注于序列的不同部分，以结构化的方式捕捉依赖关系，无论它们在序列中的位置如何。在“自注意力”中的“自”指的是注意力机制是如何应用于输入序列本身的，意味着序列中的每个元素都会与其他每个元素进行比较，以确定其注意力分数。
- en: 'To truly appreciate how the transformer architecture works, we can describe
    how the components in its architecture play a role in handling a particular task.
    Suppose we need our transformer to translate the English sentence “*Hello, how
    are you?*” into French: “*Bonjour, comment ça va?*” Let us walk through this step
    by step to examine and elucidate how the transformer might accomplish this task.
    For now, we will describe each step in detail and later implement the full architecture
    using Python.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正理解变压器架构的工作原理，我们可以描述其架构中的组件如何处理特定任务。假设我们需要我们的变压器将英语句子“*Hello, how are you?*”翻译成法语：“*Bonjour,
    comment ça va?*”。让我们一步一步地走过这个过程，检查和阐明变压器可能如何完成这项任务。目前，我们将详细描述每个步骤，稍后我们将使用Python实现完整的架构。
- en: Components of the transformer architecture
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器架构的组件
- en: Before diving into how the transformer model fulfills our translation task,
    we need to understand the steps involved. The complete architecture is quite dense,
    so we will break it down into small, logical, and digestible components.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨如何使变压器模型完成我们的翻译任务之前，我们需要了解其中涉及的步骤。完整的架构相当密集，因此我们将将其分解为小、逻辑性强且易于消化的组件。
- en: 'First, we discuss the two components central to the architectural design of
    the transformer model: the encoder and decoder stacks. We will also explain how
    data flows within these layer stacks, including the concept of tokens, and how
    relationships between tokens are captured and refined using critical techniques
    such as self-attention and FFNs.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们讨论变压器模型架构设计的两个核心组件：编码器栈和解码器栈。我们还将解释数据如何在这些层栈中流动，包括标记的概念，以及如何使用诸如自注意力机制和前馈神经网络（FFNs）等关键技术来捕捉和细化标记之间的关系。
- en: Then, we transition into the training process of the transformer model. Here,
    we review fundamental concepts such as batches, masking, the training loop, data
    preparation, optimizer selection, and strategies to improve performance. We will
    explain how the transformer optimizes performance using a loss function, which
    is crucial in shaping how the model learns to translate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们过渡到变压器模型的训练过程。在这里，我们回顾了批处理、掩码、训练循环、数据准备、优化器选择以及提高性能的策略等基本概念。我们将解释变压器如何使用损失函数来优化性能，这对于塑造模型学习翻译的方式至关重要。
- en: Following the training process, we discuss model inference, which is how our
    trained model generates translations. This section points out the order in which
    individual model components operate during translation and emphasizes the importance
    of each step.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程之后，我们讨论模型推理，即我们的训练模型如何生成翻译。本节指出在翻译过程中各个模型组件的操作顺序，并强调每个步骤的重要性。
- en: As discussed, central to the transformer are two vital components, often called
    the encoder stack and the decoder stack.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所讨论的，变压器核心的两个关键组件通常被称为编码器栈和解码器栈。
- en: Encoder and decoder stacks
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器栈和解码器栈
- en: In the context of the transformer model, **stacks** reference a hierarchical
    arrangement of **layers**. Each layer in this context is, in fact, an NN layer
    like the layers we come across in classical DL models. While a layer is a level
    in the model where specific computational operations occur, a stack refers to
    multiple such layers arranged consecutively.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer模型的背景下，**堆叠**指的是**层**的层次排列。在这个上下文中，每一层实际上是一个类似于我们在经典深度学习模型中遇到的NN层。虽然层是模型中发生特定计算操作的级别，但堆叠则指的是多个这样的层按顺序排列。
- en: Encoder stack
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编码器堆叠
- en: 'Consider our example sentence “*Hello, how are you?*”. We first convert it
    into tokens. Each token typically represents a word. In the case of our example
    sentence, tokenization would break it down into separate tokens, resulting in
    the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们的示例句子“*Hello, how are you?*”。我们首先将其转换为标记。每个标记通常代表一个单词。在我们的示例句子中，标记化会将它分解成单独的标记，结果如下：
- en: '`[“Hello”, “,”, “how”, “are”, “``you”, “?”]`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`[“Hello”，“，”，“how”，“are”，“``you”，“？”]`'
- en: Here, each word or punctuation represents a distinct token. These tokens are
    then transformed into numerical representations, also known as **embeddings**.
    These embedding vectors capture the semantic meaning and context of the words,
    enabling the model to understand and process the input data effectively. The embeddings
    aid in capturing complex relationships and contexts from the original English
    input sentence through this series of transformations across layers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个单词或标点符号代表一个不同的标记。然后，这些标记被转换成数值表示，也称为**嵌入**。这些嵌入向量捕捉了单词的语义意义和上下文，使模型能够有效地理解和处理输入数据。通过这一系列层间的转换，嵌入有助于捕捉原始英语输入句子中的复杂关系和上下文。
- en: This stack comprises multiple layers, where each layer applies self-attention
    and FFN computations on its input data (which we will describe in detail shortly).
    The embeddings iteratively capture complex relationships and context from the
    original English input sentence through this series of transformations across
    layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个堆叠由多个层组成，其中每一层对其输入数据（我们将在稍后详细描述）应用自注意力和FFN计算。嵌入通过这一系列层间的转换迭代地捕捉原始英语输入句子中的复杂关系和上下文。
- en: Decoder stack
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解码器堆叠
- en: Once the encoder completes its task, the output vectors—or the embeddings of
    the input sentence that hold its contextual information—are passed on to the decoder.
    Within the decoder stack, multiple layers work sequentially to generate a French
    translation from the embeddings.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦编码器完成了其任务，输出向量——即包含输入句子上下信息的输入句子的嵌入——就会被传递到解码器。在解码器堆叠中，多个层按顺序工作，从嵌入中生成法语翻译。
- en: The process begins by converting the first embedding into the French phrase
    “*Bonjour*.” The subsequent layer uses the following embedding and context from
    the previously generated words to predict the next word in the French sentence.
    This process is repeated through all the layers in the stack, each using input
    embeddings and generated words to define and refine the translation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程首先将第一个嵌入转换为法语短语“*Bonjour*”。随后的层使用以下嵌入和之前生成的单词的上下文来预测法语句子中的下一个单词。这个过程通过堆叠中的所有层重复进行，每一层都使用输入嵌入和生成的单词来定义和细化翻译。
- en: The decoder stack progressively builds (or decodes) the translated sentence
    through this iterative process, arriving at “*Bonjour, comment* *ça va?*”.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器堆叠通过这种迭代过程逐步构建（或解码）翻译句子，最终到达“*Bonjour, comment* *ça va?*”。
- en: With an overall understanding of the encoder-decoder structure, our next step
    is unraveling the intricate operations within each stack. However, before delving
    into the self-attention mechanism and FFNs, there is one vital component we need
    to understand — positional encoding. Positional encoding is paramount to the transformer’s
    performance because it gives the transformer model a sense of the order of words,
    something subsequent operations in the stack lack.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在对编码器-解码器结构有一个整体理解之后，我们的下一步是揭示每个堆叠中复杂的操作。然而，在深入探讨自注意力机制和FFN之前，有一个至关重要的组件我们需要理解——位置编码。位置编码对于Transformer的性能至关重要，因为它为Transformer模型提供了对单词顺序的感觉，这是堆叠中后续操作所缺乏的。
- en: Positional encoding
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: Every word in a sentence holds two types of information — its meaning and its
    role in the larger context of the sentence. The contextual role often stems from
    a word’s position in the arrangement of words. A sentence such as “*Hello, how
    are you?*” makes sense because the words are in a specific order. Change that
    to “*Are you, how hello?*” and the meaning becomes unclear.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的每个单词都包含两种类型的信息——其含义和在句子更大上下文中的角色。上下文角色通常源于单词在单词排列中的位置。例如，“*你好，你好吗？*”这样的句子是有意义的，因为单词的顺序是特定的。如果将其改为“*你是，如何你好？*”，则意义变得不清楚。
- en: Consequently, Vaswani et al. introduced **positional encoding** to ensure that
    the transformer encodes each word with additional data about its position in the
    sentence. Positional encodings are computed using a blend of sine and cosine functions
    across different frequencies, which generate a unique set of values for each position
    in a sequence. These values are then added to the original embeddings of the tokens,
    providing a way for the model to capture the order of words. These enriched embeddings
    are then ready to be processed by the self-attention mechanism in the subsequent
    layers of the transformer model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Vaswani等人引入了**位置编码**以确保transformer能够将每个单词编码为包含其句子中位置额外数据的附加信息。位置编码是通过在不同频率上使用正弦和余弦函数的混合来计算的，这为序列中的每个位置生成一组独特的值。然后，这些值被添加到标记的原始嵌入中，为模型提供了一种捕捉单词顺序的方法。这些增强的嵌入随后准备好在transformer模型的后续层中由自注意力机制进行处理。
- en: Self-attention mechanism
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力机制
- en: As each token of our input sentence “*Hello, how are you?*” passes through each
    layer of the encoder stack, it undergoes a transformation via the self-attention
    mechanism.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的输入句子“*你好，你好吗？*”的每个标记通过编码堆栈的每一层时，它都会通过自注意力机制进行转换。
- en: As the name suggests, the self-attention mechanism allows each token (word)
    to attend to (or focus on) other vital tokens to understand the full context within
    the sentence. Before encoding a particular word, this attention mechanism interprets
    the relationship between each word and the others in the sequence. It then assigns
    distinct attention scores to different words based on their relevance to the current
    word being processed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，自注意力机制允许每个标记（单词）关注（或聚焦于）其他重要标记，以理解句子中的完整上下文。在编码特定单词之前，这种注意力机制解释了序列中每个单词与其他单词之间的关系。然后，根据它们与当前正在处理的单词的相关性，为不同的单词分配不同的注意力分数。
- en: 'Consider again our input sentence “*Hello, how are you?*”. When the self-attention
    mechanism is processing the last word, “*you*,” it does not just focus on “*you*.”
    Instead, it takes into consideration the entire sentence: it looks at “*Hello*,”
    glances over “*how*,” reflects on “*are*,” and, of course, focuses on “*you*.”'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们的输入句子“*你好，你好吗？*”。当自注意力机制处理最后一个单词“*你*”时，它不仅仅关注“*你*”。相反，它考虑整个句子：它查看“*你好*”，瞥了一眼“*如何*”，反思了“*是*”，当然，聚焦于“*你*”。
- en: 'In doing so, it assigns various levels of attention to each word. You can visualize
    attention (*Figure 3**.1*) as lines connecting “*you*” to every other word. The
    line to “*Hello*” might be thick, indicating a lot of attention, representing
    the influence of “*Hello*” on the encoding of “*you*.” The line connecting “*you*”
    and “*how*” might be thinner, suggesting less attention given to “*how*.” The
    lines to “*are*” and “*you*” would have other thicknesses based on how they help
    in providing context to “*you*”:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样做的时候，它为每个单词分配了不同级别的注意力。你可以将注意力（*图3**.1*）想象为连接“*你*”到每个其他单词的线条。连接到“*你好*”的线条可能很粗，表示很多注意力，代表着“*你好*”对“*你*”编码的影响。连接“*你*”和“*如何*”的线条可能较细，表明对“*如何*”的关注较少。连接到“*是*”和“*你*”的线条会有其他厚度，这取决于它们在为“*你*”提供上下文方面的帮助：
- en: '![Figure 3.1: Self-attention mechanism](img/B21773_03_01.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1：自注意力机制](img/B21773_03_01.jpg)'
- en: 'Figure 3.1: Self-attention mechanism'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：自注意力机制
- en: This way, when encoding “*you*,” a weighted mix of the entire sentence is considered,
    not just the single word. And these weights defining the mix are what we refer
    to as attention scores.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，当编码“*你*”时，会考虑整个句子的加权混合，而不仅仅是单个单词。而定义这种混合的权重就是我们所说的注意力分数。
- en: 'The self-attention mechanism is implemented through a few steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制通过以下步骤实现：
- en: Initially, each input word is represented as a vector, which we obtain from
    the word embedding.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始时，每个输入单词被表示为一个向量，这是我们通过词嵌入获得的。
- en: These vectors are then mapped to new vectors called query, key, and value vectors
    through learned transformations.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些向量随后通过学习变换映射到新的向量，称为查询、键和值向量。
- en: An attention score for each word is then computed by taking the dot product
    of the query vector of the word with the key vector of every other word, followed
    by a SoftMax operation (which we will describe later).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过将单词的查询向量与每个其他单词的键向量进行点积，并随后进行SoftMax操作（我们将在后面描述），来计算每个单词的注意力分数。
- en: These scores indicate how much focus to place on other parts of the input sentence
    for each word as it is encoded.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些分数表示在编码每个单词时，对输入句子其他部分的关注程度。
- en: Finally, a weighted sum of the value vectors is computed based on these scores
    to give us our final output vectors, or the self-attention outputs.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，根据这些分数计算值向量的加权总和，以给出我们的最终输出向量，或自注意力输出。
- en: It is important to note that this computation is done for each word in the sentence.
    This ensures a comprehensive understanding of the context in the sentence, considering
    multiple parts of the sentence at once. This concept set the transformer apart
    from nearly every model that came before it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这种计算是对句子中的每个单词进行的。这确保了对句子上下文的全面理解，同时考虑句子中的多个部分。这个概念使Transformer与几乎所有之前的模型都区分开来。
- en: Instead of running the self-attention mechanism once (or “single-head” attention),
    the transformer replicates the self-attention mechanism multiple times in parallel.
    Each replica or head operates on the same input but has its own independent set
    of learned parameters to compute the attention scores. This allows each head to
    learn different contextual relationships between words. This parallel process
    is known as **multi-head** **attention** (**MHA**).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与只运行一次自注意力机制（或“单头注意力”）不同，Transformer在并行中多次复制自注意力机制。每个副本或头都操作相同的输入，但有自己的独立学习参数集来计算注意力分数。这使得每个头可以学习单词之间的不同上下文关系。这种并行过程被称为**多头注意力**（**MHA**）。
- en: Imagine our sentence “*Hello, how are you?*” again. One head might concentrate
    on how “*Hello*” relates to “*you*,” whereas another head might focus more on
    how “*how*” relates to “*you*.” Each head has its own set of query, key, and value
    weights, further enabling them to specialize and learn different things. The outputs
    of these multiple heads are then concatenated and transformed to produce final
    values passed onto the next layer in the stack.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 再次想象我们的句子“*你好，你好吗？*”。一个头可能专注于“*你好*”与“*你*”之间的关系，而另一个头可能更多地关注“*你好吗*”与“*你*”之间的关系。每个头都有自己的查询、键和值权重集，这进一步使它们能够专门化并学习不同的事物。然后，将这些多个头的输出连接起来并转换，以产生传递到堆叠中下一层的最终值。
- en: This multi-head approach allows the model to capture a wider range of information
    from the same input words. It is like having several perspectives on the same
    sentence, each providing unique insights.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多头方法允许模型从相同的输入单词中捕捉更广泛的信息。这就像对同一个句子有多个视角，每个视角都提供独特的见解。
- en: So far, for our input sentence “*Hello, how are you?*”, we have converted each
    word into token representations, which are then contextualized using the MHA mechanism.
    Through parallel self-attention, our transformer can consider the full range of
    interactions between each word and every other word in the sentence. We now have
    a set of diverse and context-enriched word representations, each containing a
    textured understanding of a word’s role in the sentence. However, this contextual
    understanding contained within the attention mechanism is just one component of
    the information processing in our transformer model. Next comes another layer
    of interpretation through position-wise FFNs. The FFN will add further nuances
    to these representations, making them more informative and valuable for our translation
    task.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，对于我们的输入句子“*你好，你好吗？*”，我们已经将每个单词转换成标记表示，然后使用MHA机制进行上下文化。通过并行自注意力，我们的Transformer可以考虑到句子中每个单词与句子中其他每个单词之间的全部交互范围。我们现在有一组多样化和上下文丰富的单词表示，每个表示都包含对单词在句子中角色纹理化的理解。然而，这种包含在注意力机制中的上下文理解只是我们Transformer模型中信息处理的一个组成部分。接下来，通过位置感知前馈网络（FFN）的另一个层次进行解释。FFN将为这些表示添加更多的细微差别，使它们对我们的翻译任务更有信息和价值。
- en: 'In the next section, we discuss a vital aspect of the transformer’s training
    sequence: masking. Specifically, the transformer applies causal (or look-ahead)
    masking during the decoder self-attention to ensure that each output token prediction
    depends only on previously generated tokens, not future unknown tokens.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们讨论了变压器训练序列的一个关键方面：掩码。具体来说，变压器在解码器的自我注意力过程中应用因果（或前瞻）掩码，以确保每个输出标记预测只依赖于先前生成的标记，而不是未来的未知标记。
- en: Masking
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掩码
- en: The transformer applies two types of masking during training. The first is a
    preprocessing step to ensure input sentences are of the same length, which enables
    efficient batch computation. The second is look-ahead (or causal) masking, which
    allows the model to selectively ignore future tokens in a sequence. This type
    of masking occurs in the self-attention mechanism in the decoder and prevents
    the model from peeking ahead at future tokens in the sequence. For example, when
    translating the word “*Hello*” to French, look-ahead masking ensures that the
    model does not have access to the subsequent words “*how*,” “*are*,” or “*you*.”
    This way, the model learns to generate translations based on the current and preceding
    words, adhering to a natural progression in translation tasks, mimicking that
    of human translation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器在训练过程中应用两种类型的掩码。第一种是预处理步骤，以确保输入句子长度相同，从而实现高效的批量计算。第二种是前瞻（或因果）掩码，允许模型选择性地忽略序列中的未来标记。这种掩码发生在解码器的自我注意力机制中，防止模型在序列中向前窥视未来的标记。例如，当将单词
    “*Hello*” 翻译成法语时，前瞻掩码确保模型无法访问随后的单词 “*how*”、“*are*” 或 “*you*”。这样，模型学会根据当前和前面的单词生成翻译，遵循翻译任务的自然进程，模仿人类的翻译方式。
- en: 'With a clearer understanding of how data is prepared and masked for training,
    we now transition to another significant aspect of the training process: hyperparameters.
    Unlike parameters learned from the data, hyperparameters are configurations set
    before training to control the model optimization process and guide the learning
    journey. The following section will explore various hyperparameters and their
    roles during training.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对数据准备和掩码化的更清晰理解，我们现在转向训练过程中的另一个重要方面：超参数。与从数据中学习到的参数不同，超参数是在训练之前设置的配置，用于控制模型优化过程并指导学习过程。下一节将探讨各种超参数及其在训练过程中的作用。
- en: SoftMax
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SoftMax
- en: 'To understand the role of the FFN, we can describe its two primary components—linear
    transformations and an activation function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 FFN 的作用，我们可以描述其两个主要组件——线性变换和激活函数：
- en: '**Linear transformations** are essentially matrix multiplications. Think of
    them as tools that reshape or tweak the input data. In the FFN, these transformations
    occur twice, where two different weights (or matrices) are used.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性变换** 实质上是矩阵乘法。可以将它们视为重塑或调整输入数据的工具。在 FFN 中，这些变换发生两次，使用两种不同的权重（或矩阵）。'
- en: A **rectified linear unit** (**ReLU**) function is applied between these two
    transformations. The role of the ReLU function is to introduce non-linearity in
    the model. Simply put, the ReLU function allows the model to capture patterns
    within the input data that are not strictly proportional, i.e., non-linear, which
    is typical of **natural language** (**NL**) data.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这两个变换之间应用 **ReLU**（修正线性单元）函数。ReLU 函数的作用是在模型中引入非线性。简单来说，ReLU 函数允许模型捕捉输入数据中的非严格成比例的图案，即非线性，这是
    **自然语言**（NL）数据的特点。
- en: The FFN is called **position-wise** because it treats each word in the sentence
    separately (position by position), regardless of the sequence. This contrasts
    with the self-attention mechanism, which considers the entire sequence at once.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: FFN 被称为 **位置敏感**，因为它将句子中的每个单词单独处理（逐个位置），而不考虑序列。这与同时考虑整个序列的自我注意力机制形成对比。
- en: 'So, let us attempt to visualize the process: Imagine our word “*Hello*” arriving
    here after going through the self-attention mechanism. It carries with it information
    about its own identity mixed with contextual references to “*how*,” “*are*,” and
    “*you*.” This integrated information resides within a vector that characterizes
    “*Hello*.”'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们尝试可视化这个过程：想象我们的单词 “*Hello*” 在经过自我注意力机制后到达这里。它携带有关其自身身份的信息，以及关于 “*how*”、“*are*”
    和 “*you*” 的上下文参考。这种综合信息存在于一个描述 “*Hello*” 的向量中。
- en: When “*Hello*” enters the FFN, picture it as a tunnel with two gates. At the
    first gate (or linear layer), “*Hello*” is transformed by a matrix multiplication
    operation, changing its representation. Afterward, it encounters the ReLU function—which
    makes the representation non-linear.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当“*Hello*”进入FFN时，可以将其想象为一个有两个门的隧道。在第一个门（或线性层）处，“*Hello*”通过矩阵乘法操作被转换，改变了其表示。之后，它遇到了ReLU函数——这使得表示变得非线性。
- en: After this, “*Hello*” passes through a second gate (another linear layer), emerging
    on the other side transformed yet again. The core identity of “*Hello*” remains
    but is now imbued with even more context, carefully calibrated and adjusted by
    the FFN.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，“*Hello*”通过第二个门（另一个线性层），在另一侧再次被转换。虽然“*Hello*”的核心身份保持不变，但现在它被赋予了更多的上下文，这是通过FFN精心校准和调整的。
- en: Once the input passes through the gates, there is one additional step. The transformed
    vector still must be converted into a form that can be interpreted as a prediction
    for our final translation task.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入通过门，还有一个额外的步骤。转换后的向量仍然必须被转换成可以解释为我们最终翻译任务预测的形式。
- en: This brings us to using the SoftMax function, the final transformation within
    the transformer’s decoder. After the vectors pass through the FFN, they are further
    processed through a final linear layer. The result is then fed into a SoftMax
    function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了使用SoftMax函数，这是transformer解码器中的最终转换。在向量通过FFN之后，它们会进一步通过一个最终的线性层进行处理。然后，结果被输入到SoftMax函数中。
- en: 'SoftMax serves as a mechanism for converting the output of our model into a
    form that can be interpreted as probabilities. In essence, the SoftMax function
    will take the output from our final linear layer (which could be any set of real
    numbers) and transform it into a distribution of probabilities, representing the
    likelihood of each word being the next word in our output sequence. For example,
    if our target vocabulary includes “*Bonjour*,” “*Hola*,” “*Hello*,” and “*Hallo*,”
    the SoftMax function will assign each of these words a probability, and the word
    with the highest probability will be chosen as the output translation for the
    word “*Hello*.” We can illustrate with this oversimplified representation of the
    output probabilities:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SoftMax充当将我们模型输出转换为可解释为概率的机制。本质上，SoftMax函数将来自我们最终线性层的输出（可能是一组实数）转换成一个概率分布，表示每个单词成为我们输出序列中下一个单词的可能性。例如，如果我们的目标词汇包括“*Bonjour*”，“*Hola*”，“*Hello*”和“*Hallo*”，SoftMax函数将为这些单词中的每一个分配一个概率，并且具有最高概率的单词将被选择作为“*Hello*”的输出翻译。我们可以用这个过于简化的输出概率表示来展示：
- en: '`[ Bonjour: 0.4, Hola: 0.3, Hello: 0.2, Hallo:` `0.1 ]`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`[ Bonjour: 0.4, Hola: 0.3, Hello: 0.2, Hallo: 0.1 ]`'
- en: '*Figure 3**.2* shows a more complete (albeit oversimplified) view of the flow
    of information through the architecture.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3**.2* 展示了通过架构的信息流的一个更完整的（尽管过于简化的）视图。'
- en: '![Figure 3.2: A simplified illustration of the transformer](img/B21773_03_02.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2：transformer的简化示意图](img/B21773_03_02.jpg)'
- en: 'Figure 3.2: A simplified illustration of the transformer'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：transformer的简化示意图
- en: Now that we’ve introduced the architectural components of the transformer, we
    are poised to understand how its components work together.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了transformer的架构组件，我们准备了解其组件是如何协同工作的。
- en: Sequence-to-sequence learning
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列到序列学习
- en: The components of a transformer come together to learn from data using a mechanism
    known as **sequence-to-sequence** (**Seq2Seq**) learning, a subset of **supervised
    learning** (**SL**). Recall that SL is a technique that uses labeled data to train
    models to predict outcomes accurately. In Seq2Seq learning, we provide the transformer
    with training data that comprises examples of input and corresponding correct
    output, which, in this case, are correct translations. Seq2Seq learning is particularly
    well suited for tasks such as machine translation where both the input and output
    are sequences of words.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的组件通过称为**序列到序列**（**Seq2Seq**）学习（**SL**）的机制一起学习数据，这是**监督学习**（**SL**）的一个子集。回想一下，SL是一种使用标记数据来训练模型以准确预测结果的技术。在Seq2Seq学习中，我们向transformer提供包含输入和相应正确输出的训练数据，在这种情况下，是正确的翻译。Seq2Seq学习特别适合像机器翻译这样的任务，其中输入和输出都是单词序列。
- en: The very first step in the learning process is to convert each word in the phrase
    into tokens, which are then transformed into numerical embeddings. These embeddings
    carry the semantic essence of each word. Positional encodings are computed and
    added to these embeddings to imbue them with positional awareness.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程中的第一步是将短语中的每个词转换为标记，然后将其转换为数值嵌入。这些嵌入携带每个词的语义本质。计算位置编码并将其添加到这些嵌入中，以赋予它们位置意识。
- en: As these enriched embeddings traverse through the encoder stack, within each
    layer, the self-attention mechanism refines the embeddings by aggregating contextual
    information from the entire phrase. Following self-attention, each word’s embedding
    undergoes further transformation in the position-wise FFNs, adjusting the embeddings
    to capture even more complex relationships.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些丰富的嵌入通过编码器堆栈时，在每个层中，自注意力机制通过聚合整个短语中的上下文信息来细化嵌入。在自注意力之后，每个词的嵌入在位置感知FFNs中进行进一步转换，调整嵌入以捕获更复杂的关系。
- en: Upon exiting the encoder, the embeddings now hold a rich mixture of semantic
    and contextual information. They are passed onto the decoder stack, which aims
    to translate the phrase into another language (that is, the target sequence).
    As with the encoder, each layer in the decoder also employs self-attention and
    position-wise FFNs, but with an additional layer of cross-attention that interacts
    with the encoder’s outputs. This interaction helps align the input and output
    phrases, a crucial aspect of translation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当编码器退出时，嵌入现在包含丰富的语义和上下文信息混合。它们被传递到解码器堆栈，其目的是将短语翻译成另一种语言（即目标序列）。与编码器一样，解码器中的每一层也使用自注意力和位置感知前馈网络（FFNs），但增加了一个与编码器输出交互的交叉注意力层。这种交互有助于对齐输入和输出短语，这是翻译的一个关键方面。
- en: As the embeddings move through the decoder layers, they are progressively refined
    to represent the translated phrase that the model will predict. The final layer
    of the decoder processes the embeddings through a linear transformation and SoftMax
    function to produce a probability distribution over the target vocabulary. This
    distribution defines the model’s predicted likelihood for each potential next
    token at each step. The decoder then samples from this distribution to select
    the token with the highest predicted probability as its next output. By iteratively
    sampling the most likely next tokens according to the predicted distributions,
    the decoder can autoregressively generate the full translated output sequence
    token by token.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 随着嵌入通过解码器层移动，它们逐渐被细化以表示模型将预测的翻译短语。解码器的最后一层通过线性变换和SoftMax函数处理嵌入，以在目标词汇上产生一个概率分布。这个分布定义了模型在每个步骤上对每个潜在下一个标记的预测可能性。然后，解码器从这个分布中采样，选择具有最高预测概率的标记作为其下一个输出。通过根据预测分布迭代采样最可能的下一个标记，解码器可以自回归地逐个生成完整的翻译输出序列标记。
- en: However, for the transformer to reliably sample from the predicted next-token
    distributions to generate high-quality translations, it must progressively learn
    by iterating over thousands of examples of input-output pairs. In the next section,
    we explore model training in further detail.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了使transformer能够从预测的下一个标记分布中可靠地采样以生成高质量的翻译，它必须通过迭代成千上万的输入-输出对来逐步学习。在下一节中，我们将更详细地探讨模型训练。
- en: Model training
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练
- en: As discussed, the primary goal of the training phase is to refine the model’s
    parameters to facilitate accurate translation from one language to another. But
    what does the refinement of parameters entail, and why is it pivotal?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所讨论的，训练阶段的主要目标是细化模型的参数，以促进从一种语言到另一种语言的准确翻译。但是，参数的细化意味着什么，为什么它是关键的？
- en: Parameters are internal variables that the model utilizes to generate translations.
    Initially, these parameters are assigned random values, which are adjusted with
    each training iteration. Again, the model is provided with training data that
    comprises thousands of examples of input data and corresponding correct output,
    which, in this case, is the correct translation. It then compares its predicted
    output tokens to the correct (or actual) target sequences using an error (or loss)
    function.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是模型用于生成翻译的内部变量。最初，这些参数被赋予随机值，并在每次训练迭代中进行调整。再次，模型被提供包含成千上万输入数据示例及其对应正确输出的训练数据，在这种情况下，是正确的翻译。然后，它使用一个错误（或损失）函数将其预测的输出标记与正确的（或实际）目标序列进行比较。
- en: Based on the loss, the model updates its parameters, gradually improving its
    ability to choose the correct item in the sequence at each step of decoding. This
    slowly refines the probability distributions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据损失，模型更新其参数，逐渐提高其在解码的每一步中选择正确项目的能力。这逐渐细化了概率分布。
- en: Over thousands of training iterations, the model learns associations between
    source and target languages. Eventually, it acquires enough knowledge to decode
    coherent, human-like translations from unseen inputs by relying on patterns discovered
    during training. Therefore, training drives the model’s ability to produce accurate
    target sequences from the predicted vocabulary distributions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在数千次训练迭代中，模型学习源语言和目标语言之间的关联。最终，它获得了足够的知识，可以通过在训练期间发现的模式来解码从未见过的输入中的一致、类似人类的翻译。因此，训练推动了模型从预测词汇分布中产生准确目标序列的能力。
- en: After training on sufficient translation pairs, the transformer reaches reliable
    translation performance. The trained model can then take in new input sequences
    and output translated sequences by generalizing to that new data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了足够的翻译对之后，transformer达到了可靠的翻译性能。训练好的模型可以接受新的输入序列，并通过对新数据的泛化来输出翻译序列。
- en: For instance, with our example sentence “*Hello, how are you?*” and its French
    translation “*Bonjour, comment ça va?*”, the English sentence serves as the input,
    and the French sentence serves as the target output. The training data comprises
    many translated pairs. Each time the model processes a batch of data, it generates
    predictions for the translation, compares them to the actual target translations,
    and then adjusts its parameters to reduce the discrepancy (or minimize the loss)
    between the predicted and actual translations. This is repeated with numerous
    batches of data until the model’s translations are sufficiently accurate.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以我们的示例句子“*Hello, how are you?*”及其法语翻译“*Bonjour, comment ça va?*”为例，英语句子作为输入，法语句子作为目标输出。训练数据由许多翻译对组成。每次模型处理一批数据时，它都会为翻译生成预测，将它们与实际的目标翻译进行比较，然后调整其参数以减少预测翻译和实际翻译之间的差异（或最小化损失）。这个过程会重复使用大量的数据批次，直到模型的翻译足够准确。
- en: Hyperparameters
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: Again, unlike parameters, which the model learns from the training data, hyperparameters
    are preset configurations that govern the training process and the structure of
    the model. They are a crucial part of setting up a successful training run.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，与模型从训练数据中学习的参数不同，超参数是预先设置的配置，它控制着训练过程和模型的架构。它们是设置成功训练运行的关键部分。
- en: 'Some key hyperparameters in the context of transformer models include the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformer模型背景下，一些关键的超参数包括以下内容：
- en: '**Learning rate**: This value determines the step size at which the optimizer
    updates the model parameters. A higher learning rate could speed up the training
    but may overshoot the optimal solution. A lower learning rate may result in a
    more precise convergence to the optimal solution, albeit at the cost of longer
    training time. We will discuss optimizers in detail in the next section.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：这个值决定了优化器更新模型参数的步长。较高的学习率可能会加快训练速度，但可能导致超过最佳解。较低的学习率可能会导致更精确地收敛到最佳解，尽管代价是更长的训练时间。我们将在下一节中详细讨论优化器。'
- en: '**Batch size**: The number of data examples processed in a single batch affects
    the computational accuracy and the memory requirements during training.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批大小**：单个批次中处理的数据示例数量会影响训练期间的计算精度和内存需求。'
- en: '**Model dimensions**: The model’s size (for example, the number of layers in
    the encoder and decoder, the dimensionality of the embeddings, and so on) is a
    crucial hyperparameter that affects the model’s capacity to learn and generalize.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型维度**：模型的大小（例如，编码器和解码器的层数、嵌入的维度等）是一个关键的超参数，它影响着模型的学习能力和泛化能力。'
- en: '**Optimizer settings**: Choosing an optimizer and its settings, such as the
    initial learning rate, beta values in the Adam optimizer, and so on, are also
    considered hyperparameters. Again, we will explore optimizers further in the next
    section.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器设置**：选择优化器及其设置，例如Adam优化器的初始学习率、beta值等，也被认为是超参数。我们将在下一节中进一步探讨优化器。'
- en: '**Regularization terms**: Regularization terms such as dropout rate are hyperparameters
    that help prevent overfitting by adding some form of randomness or constraint
    to the training process.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化项**：如dropout率这样的正则化项是超参数，通过在训练过程中添加某种形式的随机性或约束来帮助防止过拟合。'
- en: Selecting the proper values for hyperparameters is crucial for the training
    process as it significantly impacts the model’s performance and efficiency. It
    often involves hyperparameter tuning, which involves experimentation and refining
    to find values for hyperparameters that yield reliable performance for a given
    task. Hyperparameter tuning can be somewhat of an art and a science. We will touch
    on this more in later chapters.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的超参数值对于训练过程至关重要，因为它会显著影响模型的表现和效率。这通常涉及到超参数调整，它包括实验和细化，以找到给定任务中产生可靠性能的超参数值。超参数调整可以说是一种艺术和科学。我们将在后面的章节中进一步探讨这一点。
- en: With a high-level grasp of hyperparameters, we will move on to the choice of
    optimizer, which is pivotal in controlling how efficiently the model learns from
    the training data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在对超参数有了一定的了解之后，我们将继续探讨优化器的选择，这对于控制模型如何有效地从训练数据中学习至关重要。
- en: Choice of optimizer
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器的选择
- en: The optimizer is a fundamental component of the training process and is responsible
    for updating the model’s parameters to minimize error. Different optimizers have
    different strategies for navigating the parameter space to find a set of parameter
    values that yield low loss (or less error). The choice of optimizer can significantly
    impact the speed and quality of the training process.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是训练过程中的一个基本组成部分，负责更新模型的参数以最小化错误。不同的优化器有不同的策略来导航参数空间，找到一组参数值，以产生低损失（或更少的错误）。优化器的选择可以显著影响训练过程的速度和质量。
- en: In the context of transformer models, the Adam optimizer is often the optimizer
    of choice due to its efficiency and empirical success in training deep networks.
    Adam adapts learning rates during training. For simplicity, we will not explore
    all the possible optimizers but instead describe their purpose.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器模型的背景下，Adam优化器由于其效率和在训练深度网络中的经验成功，通常是被选用的优化器。Adam在训练过程中会调整学习率。为了简化，我们不会探索所有可能的优化器，而是描述它们的目的。
- en: The optimizer’s primary task is to fine-tune the model’s parameters to reduce
    translation errors, progressively guiding the model toward the desired level of
    performance. However, an over-zealous optimization could lead the model to memorize
    the training data, failing to generalize well to unseen data. To mitigate this,
    we employ regularization techniques.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的主要任务是微调模型的参数，以减少翻译错误，逐步引导模型达到期望的性能水平。然而，过于激进的优化可能会导致模型记住训练数据，无法很好地泛化到未见过的数据。为了减轻这一点，我们采用正则化技术。
- en: In the next section, we will explore regularization—a technique that works with
    optimization to ensure that while the model learns to minimize translation errors,
    it also remains adaptable to new, unseen data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨正则化——这是一种与优化相结合的技术，确保模型在学会最小化翻译错误的同时，也能适应新的、未见过的数据。
- en: Regularization
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization techniques are employed to deter the model from memorizing the
    training data (a phenomenon known as overfitting) and to promote better performance
    on new, unseen data. Overfitting arises when the model, to minimize the error,
    learns the training data to such an extent that it captures useless patterns (or
    noise) along with the actual patterns. This over-precision in learning the training
    data leads to a decline in performance when the model is exposed to new data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化技术被用来防止模型记住训练数据（这种现象称为过拟合），并促进在新的、未见过的数据上的更好表现。过拟合发生在模型为了最小化错误，学习到训练数据到这种程度，以至于它捕获了无用的模式（或噪声）以及实际的模式。这种在学习训练数据上的过度精确导致模型在接触到新数据时性能下降。
- en: Let us revisit our simple scenario where we train a model to translate English
    greetings to French greetings using a dataset that includes the word “*Hello*”
    and its translation “*Bonjour*.” If the model is overfitting, it may memorize
    the exact phrases from the training data without understanding the broader translation
    pattern.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们的简单场景，其中我们训练一个模型将英语问候语翻译成法语问候语，使用的数据集包括单词“*Hello*”及其翻译“*Bonjour*”。如果模型过拟合，它可能会记住训练数据中的确切短语，而没有理解更广泛的翻译模式。
- en: In an overfit scenario, suppose the model learns to translate “*Hello*” to “*Bonjour*”
    with a probability of 1.0 because that is what it encountered most often in the
    training data. When presented with new, unseen data, it may encounter variations
    it has not seen before, such as “*Hi*,” which should also translate to “*Bonjour*.”
    However, due to overfitting, the model might fail to generalize from “*Hello*”
    to “*Hi*” as it is overly focused on the exact mappings it saw during training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在过拟合的情况下，假设模型以1.0的概率学习将“*Hello*”翻译为“*Bonjour*”，因为在训练数据中它遇到的最频繁。当遇到新的、未见过的数据时，它可能会遇到之前未见过的变化，例如“*Hi*”，这也应该翻译为“*Bonjour*”。然而，由于过拟合，模型可能无法从“*Hello*”泛化到“*Hi*”，因为它过于关注训练期间看到的精确映射。
- en: Several regularization techniques can mitigate the overfitting problem. These
    techniques apply certain constraints on the model’s parameters during training,
    encouraging the model to learn a more generalized representation of the data rather
    than memorizing the training dataset.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 几种正则化技术可以减轻过拟合问题。这些技术在对模型参数进行训练时施加某些约束，鼓励模型学习数据的更一般化表示，而不是记住训练数据集。
- en: 'Here are some standard regularization techniques used in the context of transformer
    models:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些在transformer模型背景下使用的标准正则化技术：
- en: '**Dropout**: In the context of NN-based models such as the transformer, the
    term “neurons” refers to individual elements within the model that work together
    to learn from the data and make predictions. Each neuron learns specific aspects
    or features from the data, enabling the model to understand and translate text.
    During training, dropout randomly deactivates or “drops out” a fraction of these
    neurons, temporarily removing them from the network. This random deactivation
    encourages the model to spread its learning across many neurons rather than relying
    too heavily on a few. By doing so, dropout helps the model to better generalize
    its learning to unseen data rather than merely memorizing the training data (that
    is, overfitting).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：在基于NN的模型（如transformer）的背景下，术语“神经元”指的是模型内部协同工作以从数据中学习和进行预测的各个独立元素。每个神经元从数据中学习特定的方面或特征，使模型能够理解和翻译文本。在训练过程中，dropout随机地使一部分这些神经元失活或“丢弃”，暂时从网络中移除。这种随机失活鼓励模型将学习分散到许多神经元上，而不是过度依赖少数几个神经元。通过这样做，dropout有助于模型更好地将学习泛化到未见过的数据上，而不是仅仅记住训练数据（即过拟合）。'
- en: '**Layer normalization**: Layer normalization is a technique that normalizes
    the activations of neurons in a layer for each training example rather than across
    a batch of examples. This normalization helps stabilize the training process and
    acts as a form of regularization, preventing overfitting.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层归一化**：层归一化是一种技术，它对每个训练示例中的层的神经元激活进行归一化，而不是对示例批次进行归一化。这种归一化有助于稳定训练过程，并作为一种正则化形式，防止过拟合。'
- en: '**L1 or L2 regularization**: L1 regularization, also known as Lasso, adds a
    penalty equal to the absolute magnitude of coefficients, promoting parameter sparsity.
    L2 regularization, or Ridge, adds a penalty based on the square of the coefficients,
    discouraging large values to prevent overfitting. Although these techniques help
    in controlling model complexity and enhancing generalization, they were not part
    of the transformer’s initial design.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1或L2正则化**：L1正则化，也称为Lasso，添加一个等于系数绝对值的惩罚，促进参数稀疏性。L2正则化，或称为Ridge，添加一个基于系数平方的惩罚，以防止过拟合而避免大值。尽管这些技术有助于控制模型复杂性和增强泛化，但它们并非transformer初始设计的一部分。'
- en: By employing these regularization techniques, the model is guided toward learning
    more generalized patterns in the data, which improves its ability to perform well
    on unseen data, thus making the model more reliable and robust in translating
    new text inputs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这些正则化技术，模型被引导学习数据中的更一般化模式，这提高了它在未见过的数据上表现良好的能力，从而使模型在翻译新的文本输入时更加可靠和稳健。
- en: Throughout the training process, we have mentioned the loss function and discussed
    how the optimizer leverages it to adjust the model’s parameters, aiming to minimize
    prediction error. The loss function quantifies the model’s performance. We discussed
    how regularization penalizes the loss function to prevent overfitting, encouraging
    the model to learn simpler, more generalizable patterns. In the next section,
    we look closer at the nuanced role of the loss function itself.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个训练过程中，我们提到了损失函数，并讨论了优化器如何利用它来调整模型的参数，目的是最小化预测误差。损失函数量化了模型的表现。我们讨论了正则化如何惩罚损失函数以防止过拟合，鼓励模型学习更简单、更可泛化的模式。在下一节中，我们将更深入地探讨损失函数本身的微妙作用。
- en: Loss function
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function is vital in training the transformer model, quantifying the
    differences between the model’s predictions and the actual data. In language translation,
    this error is measured between generated and actual translations in the training
    dataset. A common choice for this task is cross-entropy loss, which measures the
    difference between the model’s predicted probability distribution across the target
    vocabulary and the actual distribution, where the truth has a probability of 1
    for the correct word and 0 for the rest.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在训练转换器模型中至关重要，它量化了模型预测与实际数据之间的差异。在语言翻译中，这种错误是在训练数据集中生成的翻译与实际翻译之间的误差。这个任务的一个常见选择是交叉熵损失，它衡量了模型在目标词汇表上的预测概率分布与实际分布之间的差异，其中正确单词的概率为1，其余单词的概率为0。
- en: 'The transformer often employs a variant known as label-smoothed cross-entropy
    loss. Label smoothing adjusts the target probability distribution during training,
    slightly lowering the probability for the correct class and increasing the probability
    for all other classes, which helps prevent the model from becoming too confident
    in its predictions. For instance, with a target vocabulary comprising “*Bonjour*,”
    “*Hola*,” “*Hello*,” and “*Hallo*,” and assuming “*Bonjour*” is the correct translation,
    a standard cross-entropy loss would aim for the probability distribution of `Bonjour:
    1.0`, `Hola: 0.0`, `Hello: 0.0`, `Hallo: 0.0`. However, the label-smoothed cross-entropy
    loss would slightly adjust these probabilities, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '转换器通常采用一种称为标签平滑交叉熵损失的变体。标签平滑在训练过程中调整目标概率分布，略微降低正确类的概率，并增加其他所有类的概率，这有助于防止模型对其预测过于自信。例如，假设目标词汇包括“*Bonjour*”、“*Hola*”、“*Hello*”和“*Hallo*”，并且“*Bonjour*”是正确的翻译，标准交叉熵损失的目标是概率分布为`Bonjour:
    1.0`，`Hola: 0.0`，`Hello: 0.0`，`Hallo: 0.0`。然而，标签平滑交叉熵损失会略微调整这些概率，如下所示：'
- en: '`[ “Bonjour”: 0.925, “Hola”: 0.025, “Hello”: 0.025, “Hallo”:` `0.025 ]`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`[ “Bonjour”: 0.925, “Hola”: 0.025, “Hello”: 0.025, “Hallo”:` `0.025 ]`'
- en: The smoothing reduces the model’s confidence and promotes better generalization
    to unseen data. With a clearer understanding of the loss function’s role, we can
    move on to the inference phase, where the trained model generates translations
    for new, unseen data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑降低了模型的信心，并促进了更好的泛化到未见过的数据。在更清楚地理解损失函数的作用后，我们可以继续到推理阶段，在那里训练好的模型为新的、未见过的数据生成翻译。
- en: Inference
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理
- en: Having traversed the training landscape, our trained model is now adept with
    optimized parameters to tackle the translation task. In the inference stage, these
    learned parameters are employed to translate new, unseen text. We will continue
    with our example phrase “*Hello, how are you?*” to elucidate this process.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 经过遍历训练景观，我们的训练模型现在已经熟练掌握优化参数，可以应对翻译任务。在推理阶段，这些学习到的参数被用来翻译新的、未见过的文本。我们将继续使用示例短语“*Hello,
    how are you?*”来阐明这一过程。
- en: 'The inference stage is the practical application of the trained model on new
    data. The trained parameters, refined after numerous iterations during training,
    are now used to translate text from one language to another. The inference steps
    can be described as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 推理阶段是训练模型在新的数据上的实际应用。经过训练过程中的多次迭代后，训练好的参数现在被用来将一种语言的文本翻译成另一种语言。推理步骤可以描述如下：
- en: '**Input preparation**: Initially, our phrase “Hello, how are you?” is tokenized
    and encoded into a format that the model can process, akin to the preparation
    steps in the training phase.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入准备**：最初，我们的短语“Hello, how are you?”被标记化并编码成模型可以处理的形式，类似于训练阶段中的准备步骤。'
- en: '**Passing through the model**: The encoded input is then propagated through
    the model. As it navigates through the encoder and decoder stacks, the trained
    parameters guide the transformation of the input data, inching closer to accurate
    translations at each step.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过模型传递**：编码后的输入随后通过模型传递。在导航通过编码器和解码器堆栈时，训练好的参数指导输入数据的转换，每一步都使翻译更准确。'
- en: '**Output generation**: At the culmination of the decoder stack, the model generates
    a probability distribution across the target vocabulary for each word in the input
    text. For the word “*Hello*,” a probability distribution is formed over the target
    vocabulary, which, in our case, comprises French words. The word with the highest
    probability is selected as the translation. This process is replicated for each
    word in the phrase, rendering the translated output “*Bonjour, comment* *ça va?*”.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出生成**：在解码器堆栈的最终阶段，模型为输入文本中的每个单词在目标词汇表上生成一个概率分布。对于单词“*Hello*”，在目标词汇表（在我们的例子中是法语单词）上形成一个概率分布，选择概率最高的单词作为翻译。这个过程为短语中的每个单词重复进行，生成翻译输出“*Bonjour,
    comment* *ça va?*”。'
- en: 'Now that we understand how the model produces the final output, we can implement
    a transformer model step by step to solidify the concepts we have discussed. However,
    before we dive into the code, we can briefly give a synopsis of the end-to-end
    architecture flow:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了模型如何产生最终输出，我们可以逐步实现一个transformer模型，以巩固我们讨论的概念。然而，在我们深入代码之前，我们可以简要概述端到端架构流程：
- en: '**Input tokenization**: The initial English phrase “*Hello, how are you?*”
    is tokenized into smaller units such as “*Hello*,” “*,*,” “*how*,” and so on.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入分词**：初始英语短语“*Hello, how are you?*”被分词成更小的单元，如“*Hello*”，“*,*”，“*how*”等。'
- en: '**Embeddings**: These tokens are then mapped to continuous vector representations
    through an embedding layer.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入**：这些标记随后通过嵌入层映射到连续的向量表示。'
- en: '**Positional encoding**: To preserve the order of the sequence, positional
    encodings are added to the embeddings.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**位置编码**：为了保持序列的顺序，将位置编码添加到嵌入中。'
- en: '**Encoder self-attention**: The embedded input sequence navigates through the
    encoder’s sequence of self-attention layers. Here, each word gauges the relevance
    of every other word to comprehend the full context.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器自注意力机制**：嵌入的输入序列在编码器的自注意力层序列中导航。在这里，每个单词评估每个其他单词的相关性，以理解完整的上下文。'
- en: '**FFN**: The representations are subsequently refined by position-wise FFNs
    within each encoder layer.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**FFN**：随后，每个编码器层内的位置wise FFN对表示进行细化。'
- en: '**Encoder output**: The encoder renders contextual representations capturing
    the essence of the input sequence.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器输出**：编码器生成捕捉输入序列本质的上下文表示。'
- en: '**Decoder attention**: Incrementally, the decoder crafts the output sequence,
    employing self-attention solely on preceding words to maintain the sequence order.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解码器注意力机制**：逐步地，解码器构建输出序列，仅对前面的单词使用自注意力以保持序列顺序。'
- en: '**Encoder-decoder attention**: The decoder evaluates the encoder’s output,
    centering on pertinent input context while generating each word in the output
    sequence.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器-解码器注意力机制**：解码器评估编码器的输出，在生成输出序列中的每个单词时，集中于相关的输入上下文。'
- en: '**Output layers**: The decoder feeds its output to the linear and SoftMax layers
    to produce “*Bonjour, comment* *ça va?*'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出层**：解码器将其输出馈送到线性层和SoftMax层，以产生“*Bonjour, comment* *ça va?*”。'
- en: At the end of this chapter, we will adapt a best-in-class implementation of
    the original transformer (Huang et al., 2022) into a minimal example that could
    later be trained on various downstream tasks. This will serve as a theoretical
    exercise to further solidify our understanding. In practice, we would rely on
    pre-trained or foundation models, which we will learn to implement in later chapters.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将将原始transformer（黄等人，2022）的最佳实现改编成一个最小示例，该示例可以后来在各种下游任务上进行训练。这将作为一个理论练习，以进一步巩固我们的理解。在实践中，我们将依赖预训练或基础模型，我们将在后面的章节中学习如何实现。
- en: However, before we begin our practice project, we can trace its impact on the
    current landscape of GenAI. We follow the trajectory of early applications of
    the architecture (for example, **Bidirectional Encoded Representations from Transformers**
    (**BERT**)) through to the first GPT.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始我们的实践项目之前，我们可以追溯其对当前GenAI领域的影响。我们跟随架构早期应用的轨迹（例如，**来自Transformers的双向编码表示**（**BERT**））到第一个GPT。
- en: Evolving language models – the AR Transformer and its role in GenAI
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的发展——AR Transformer 及其在生成人工智能中的作用
- en: 'In [*Chapter 2*](B21773_02.xhtml#_idTextAnchor045), we reviewed some of the
    generative paradigms that apply a transformer-based approach. Here, we trace the
    evolution of Transformers more closely, outlining some of the most impactful transformer-based
    language models from the initial transformer in 2017 to more recent state-of-the-art
    models that demonstrate the scalability, versatility, and societal considerations
    involved in this fast-moving domain of AI (as illustrated in *Figure 3**.3*):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 2 章*](B21773_02.xhtml#_idTextAnchor045)中，我们回顾了一些应用基于 Transformer 方法的生成范式。在此，我们更详细地追踪了
    Transformer 的发展，概述了一些从 2017 年的初始 Transformer 到更近期的最先进模型的最具影响力的基于 Transformer 的语言模型，这些模型展示了在人工智能快速发展的领域中涉及的扩展性、多功能性和社会考虑（如图
    *3.3* 所示）：
- en: '![Figure 3.3: From the original transformer to GPT-4](img/B21773_03_03.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3：从原始 Transformer 到 GPT-4](img/B21773_03_03.jpg)'
- en: 'Figure 3.3: From the original transformer to GPT-4'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：从原始 Transformer 到 GPT-4
- en: '**2017 – Transformer**: The transformer model, introduced by Vaswani et al.,
    was a paradigm shift in NLP, featuring self-attention layers that could process
    entire sequences of data in parallel. This architecture enabled the model to evaluate
    the importance of each word in a sentence relative to all other words, thereby
    enhancing the model’s ability to capture the context.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2017 – Transformer**：Vaswani 等人提出的 Transformer 模型在 NLP 中实现了范式转变，其特征是能够并行处理整个数据序列的自注意力层。这种架构使得模型能够评估句子中每个词相对于所有其他词的重要性，从而增强了模型捕捉上下文的能力。'
- en: '**2018 – BERT**: Google’s BERT model innovated on the transformer architecture
    by utilizing a bidirectional context in its encoder layers during pre-training.
    It was one of the first models to understand the context of a word based on its
    entire sentence, both left and right, significantly improving performance on a
    wide range of NLP tasks, especially those requiring a deep understanding of context.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2018 – BERT**：Google 的 BERT 模型通过在其编码器层中使用双向上下文在预训练期间进行了创新。它是第一个基于整个句子（包括左右两侧）理解词的上下文的模型之一，显著提高了在广泛
    NLP 任务上的性能，特别是那些需要深入理解上下文的任务。'
- en: '**2018 – GPT-1**: OpenAI’s GPT-1 model was a milestone in NLP, adopting a generative
    pre-trained approach with a transformer’s decoder-only model. It was pre-trained
    on a diverse corpus of text data and fine-tuned for various tasks, using a unidirectional
    approach that generated text sequentially from left to right, which was particularly
    suited for generative text applications.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2018 – GPT-1**：OpenAI 的 GPT-1 模型是自然语言处理（NLP）的一个里程碑，采用了仅使用 Transformer 解码器模型的生成预训练方法。它在多样化的文本数据语料库上进行了预训练，并针对各种任务进行了微调，使用了一种从左到右顺序生成文本的单向方法，这对于生成文本应用特别适合。'
- en: '**2019 – GPT-2**: GPT-2 built upon the foundation laid by GPT-1, maintaining
    its decoder-only architecture but significantly expanding its scale in terms of
    dataset and model size. This allowed GPT-2 to generate text that was more coherent
    and contextually relevant across a broader range of topics, demonstrating the
    power of scaling up transformer models.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2019 – GPT-2**：GPT-2 在 GPT-1 奠定的基础上进行了扩展，保持了其仅使用解码器架构，但在数据集和模型规模上显著扩大。这使得
    GPT-2 能够在更广泛的主题上生成更连贯和上下文相关的文本，展示了扩大 Transformer 模型的力量。'
- en: '**2020 – GPT-3**: OpenAI’s GPT-3 pushed the boundaries of scale in transformer
    models to 175 billion parameters, enabling a wide range of tasks to be performed
    with minimal input, often with **zero-shot learning** (**ZSL**) or **few-shot
    learning** (**FSL**). This showed that Transformers could generalize across tasks
    and data types, often without the need for extensive task-specific data or fine-tuning.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2020 – GPT-3**：OpenAI 的 GPT-3 将 Transformer 模型的规模边界推进到 1750 亿个参数，使得在最小输入的情况下执行各种任务成为可能，通常使用**零样本学习**（**ZSL**）或**少样本学习**（**FSL**）。这表明
    Transformer 可以跨任务和数据类型进行泛化，通常无需大量特定任务数据或微调。'
- en: '**2021 – InstructGPT**: An optimized variant of GPT-3, InstructGPT was fine-tuned
    specifically to follow user instructions and generate aligned responses, incorporating
    feedback loops that emphasized safety and relevance in its outputs. This represented
    a focus on creating AI models that could more accurately interpret and respond
    to human prompts.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2021 – InstructGPT**：InstructGPT 是 GPT-3 的优化变体，专门针对遵循用户指令和生成一致响应进行微调，其输出中包含强调安全性和相关性的反馈循环。这代表了创建能够更准确地解释和响应人类提示的
    AI 模型的关注点。'
- en: '**2023 – GPT-4**: GPT-4 was an evolution of OpenAI’s transformer models into
    the multimodal space, capable of understanding and generating content based on
    both text and images. This model aimed to produce safer and more contextually
    nuanced responses, showcasing a significant advancement in the model’s ability
    to handle complex tasks and generate creative content.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2023 – GPT-4**: GPT-4是OpenAI的transformer模型在多模态空间中的进化，能够理解和生成基于文本和图像的内容。此模型旨在产生更安全、更具有情境细微差别的响应，展示了模型在处理复杂任务和生成创意内容能力上的显著进步。'
- en: '**2023 – LLaMA 2**: Meta AI’s LLaMA 2 was part of a suite of models that focused
    on efficiency and accessibility, allowing for high-performance language modeling
    while being more resource-efficient. This model was aimed at facilitating a broader
    range of research and application development within the AI community.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2023 – LLaMA 2**: Meta AI的LLaMA 2是一系列关注效率和可访问性的模型的组成部分，允许在更高效资源使用的同时实现高性能的语言建模。此模型旨在促进AI社区更广泛的研究和应用开发。'
- en: '**2023 – Claude 2**: Anthropic’s Claude 2 was an advancement over Claude 1,
    increasing its token context window and improving its reasoning and memory capabilities.
    It aimed to align more closely with human values, offering responsible and nuanced
    generative capabilities for open-domain question-answering and other conversational
    AI applications, marking progress in ethical AI development.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2023 – Claude 2**: Anthropic的Claude 2在Claude 1的基础上取得了进步，增加了其标记上下文窗口，并提高了其推理和记忆能力。它旨在更接近人类价值观，为开放域问答和其他对话式AI应用提供负责任和细微的生成能力，标志着在道德AI开发方面的进步。'
- en: The timeline presented highlights the remarkable progress in transformer-based
    language models over the past several years. What originated as an architecture
    that introduced the concept of self-attention has rapidly evolved into models
    with billions of parameters that can generate coherent text, answer questions,
    and perform a variety of intellectual tasks at high levels of performance. The
    increase in scale and accessibility of models such as GPT-4 has opened new possibilities
    for AI applications. At the same time, recent models have illustrated a focus
    on safety and ethics and providing more nuanced, helpful responses to users.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 展示的时间线突出了过去几年基于transformer的语言模型的显著进步。最初作为一种引入自注意力概念的架构，它迅速演变成具有数十亿参数的模型，能够生成连贯的文本、回答问题，并在高性能水平上执行各种智力任务。GPT-4等模型规模和可访问性的增加为AI应用开辟了新的可能性。同时，最近的研究表明，模型更加关注安全和伦理，并为用户提供更细微、更有帮助的响应。
- en: In the next section, we accomplish a rite of passage for practitioners with
    an interest in the NL field. We implement the key components of the original transformer
    architecture using Python to more fully understand the mechanics that started
    it all.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们通过使用Python实现原始transformer架构的关键组件，为对自然语言处理领域感兴趣的从业者完成了一个仪式。这有助于更全面地理解启动这一切的机制。
- en: Implementing the original Transformer
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现原始Transformer
- en: The following code demonstrates how to implement a minimal transformer model
    for a Seq2Seq translation task, mainly translating English text to French. The
    code is structured into multiple sections, handling various aspects from data
    loading to model training and translation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码演示了如何实现一个用于Seq2Seq翻译任务的简化transformer模型，主要翻译英语文本到法语。代码结构分为多个部分，从数据加载到模型训练和翻译，处理了各种方面。
- en: Data loading and preparation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载和准备
- en: 'Initially, the code loads a dataset and prepares it for training. The data
    is loaded from a CSV file, which is then split into English and French text. The
    text is limited to 100 characters for demonstration purposes to reduce training
    time. The CSV file includes a few thousand example data points and can be found
    in the book’s GitHub repository ([https://github.com/PacktPublishing/Python-Generative-AI](https://github.com/PacktPublishing/Python-Generative-AI))
    along with the complete code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，代码加载了一个数据集并为其训练做准备。数据从CSV文件中加载，然后分为英语和法语文本。为了演示目的，文本限制为100个字符以减少训练时间。CSV文件包含几千个示例数据点，可以在本书的GitHub仓库（[https://github.com/PacktPublishing/Python-Generative-AI](https://github.com/PacktPublishing/Python-Generative-AI)）中找到，包括完整的代码：
- en: '[PRE0]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tokenization
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: 'Next, a tokenizer is trained on the text data. The tokenizer is essential for
    converting text data into numerical data that can be fed into the model:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在文本数据上训练了一个分词器。分词器对于将文本数据转换为模型可以输入的数值数据至关重要：
- en: '[PRE1]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data tensorization
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据张量化
- en: 'The text data is then tensorized, which involves converting the text data into
    tensor format. This step is crucial for preparing the data for training with `PyTorch`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据随后被张量化，这涉及到将文本数据转换为张量格式。这一步对于使用`PyTorch`进行训练数据准备至关重要：
- en: '[PRE2]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Dataset creation
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集创建
- en: 'A custom dataset class is created to handle the data. This class is essential
    for loading the data in batches during training:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个自定义数据集类来处理数据。这个类对于在训练期间分批加载数据至关重要：
- en: '[PRE3]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Embeddings layer
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入层
- en: 'The embeddings layer maps each token to a continuous vector space. This layer
    is crucial for the model to understand and process the text data:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将每个标记映射到连续的向量空间。这一层对于模型理解和处理文本数据至关重要：
- en: '[PRE4]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Positional encoding
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'The positional encoding layer adds positional information to the embeddings,
    which helps the model understand the order of tokens in the sequence:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码层将位置信息添加到嵌入中，这有助于模型理解序列中标记的顺序：
- en: '[PRE5]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Multi-head self-attention
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力
- en: 'The **multi-head self-attention** (**MHSA**) layer is a crucial part of the
    transformer architecture that allows the model to focus on different parts of
    the input sequence when producing an output sequence:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**多头自注意力**（**MHSA**）层是Transformer架构的关键部分，它允许模型在生成输出序列时关注输入序列的不同部分：'
- en: '[PRE6]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: FFN
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FFN
- en: 'The FFN is a simple **fully connected NN** (**FCNN**) that operates independently
    on each position:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: FFN是一个简单的**全连接神经网络**（**FCNN**），它独立地对每个位置进行操作：
- en: '[PRE7]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Encoder layer
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器层
- en: 'The encoder layer consists of an MHSA mechanism and a simple FFNN. This structure
    is repeated in a stack to form the complete encoder:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器层由一个MHSA机制和一个简单的FFNN组成。这种结构通过堆叠重复，形成完整的编码器：
- en: '[PRE8]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Encoder
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The encoder is a stack of identical layers with an MHSA mechanism and an FFN:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器是一个具有MHSA机制和FFN的相同层的堆栈：
- en: '[PRE9]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Decoder layer
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器层
- en: 'Similarly, the decoder layer consists of two MHA mechanisms—one self-attention
    and one cross-attention—followed by an FFN:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，解码器层由两个MHA机制组成——一个自注意力和一个交叉注意力——随后是一个FFN：
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Decoder
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: 'The decoder is also a stack of identical layers. Each layer contains two MHA
    mechanisms and an FFN:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器也是一个相同层的堆栈。每个层包含两个MHA机制和一个FFN：
- en: '[PRE11]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This stacking layer pattern continues to build the transformer architecture.
    Each block has a specific role in processing the input data and generating output
    translations.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这种堆叠层模式继续构建Transformer架构。每个块在处理输入数据和生成输出翻译方面都有特定的作用。
- en: Complete transformer
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整的Transformer
- en: 'The transformer model encapsulates the previously defined encoder and decoder
    structures. This is the primary class that will be used for training and translation
    tasks:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型封装了之前定义的编码器和解码器结构。这是用于训练和翻译任务的主要类：
- en: '[PRE12]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Training function
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练函数
- en: 'The `train` function iterates through the epochs and batches, calculates the
    loss, and updates the model parameters:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`函数遍历epoch和批次，计算损失，并更新模型参数：'
- en: '[PRE13]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Translation function
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 翻译函数
- en: 'The `translate` function uses the trained model to translate a source text
    into the target language. It generates a translation token by token and stops
    when an **end-of-sequence** (**EOS**) token is generated or when the maximum target
    length is reached:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`translate`函数使用训练好的模型将源文本翻译成目标语言。它逐个生成翻译标记，并在生成**序列结束**（**EOS**）标记或达到最大目标长度时停止：'
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Main execution
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主执行
- en: 'In the main block of the script, hyperparameters are defined, the tokenizer
    and model are instantiated, and training and translation processes are initiated:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本的主体块中，定义了超参数，实例化了标记化器和模型，并启动了训练和翻译过程：
- en: '[PRE15]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This script orchestrates a machine translation task from loading data to training
    a transformer model and eventually translating text from English to French. Initially,
    it loads a dataset, processes the text, and establishes tokenizers to convert
    text to numerical data. Following this, it defines the architecture of a transformer
    model in `PyTorch`, detailing each component from the embeddings’ self-attention
    mechanisms to the encoder and decoder stacks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本从加载数据到训练Transformer模型，最终从英语翻译到法语进行机器翻译任务。最初，它加载一个数据集，处理文本，并建立标记化器以将文本转换为数值数据。随后，它定义了`PyTorch`中Transformer模型的架构，详细说明了从嵌入的自注意力机制到编码器和解码器堆栈的每个组件。
- en: The script further organizes the data into batches, sets up a training loop,
    and defines a translation function. Training the model on the provided English
    and French sentences teaches it to map sequences from one language to another.
    Finally, it translates a sample sentence from English to French to demonstrate
    the model’s capabilities.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本进一步将数据组织成批次，设置训练循环，并定义翻译函数。在提供的英语和法语句子上训练模型，教会它将一个语言的序列映射到另一个语言。最后，它将一个示例句子从英语翻译成法语，以展示模型的能力。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The advent of the transformer significantly propelled the field of NLP forward,
    serving as the foundation for today’s cutting-edge generative language models.
    This chapter delineated the progression of NLP that paved the way for this pivotal
    innovation. Initial statistical techniques such as count vectors and TF-IDF were
    adept at extracting rudimentary word patterns, yet they fell short in grasping
    semantic nuances.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的出现极大地推动了NLP领域的发展，成为今天尖端生成语言模型的基础。本章概述了NLP的发展历程，为这一关键创新铺平了道路。最初的统计技术，如计数向量和TF-IDF，擅长提取基本的词模式，但它们在把握语义细微差别方面有所不足。
- en: Incorporating neural language models marked a stride toward more profound representations
    through word embeddings. Nevertheless, recurrent networks encountered hurdles
    in handling longer sequences. This inspired the emergence of CNNs, which introduced
    computational efficacy via parallelism, albeit at the expense of global contextual
    awareness.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经语言模型纳入其中，标志着通过词嵌入向更深层次表示迈进的一大步。然而，循环网络在处理较长的序列时遇到了障碍。这激发了CNN的出现，它通过并行性引入了计算效率，尽管是以牺牲全局上下文意识为代价。
- en: The inception of attention mechanisms emerged as a cornerstone. In 2017, Vaswani
    et al. augmented these advancements, unveiling the transformer architecture. The
    hallmark self-attention mechanism of the transformer facilitates contextual modeling
    across extensive sequences in a parallelized manner. The layered encoder-decoder
    structure meticulously refines representations to discern relationships indispensable
    for endeavors such as translation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的引入成为了一个基石。2017年，Vaswani等人进一步扩展了这些进展，揭幕了Transformer架构。Transformer的标志性自注意力机制以并行化的方式促进了跨广泛序列的上下文建模。层叠的编码器-解码器结构精心细化表示，以辨别翻译等努力不可或缺的关系。
- en: The transformer, with its parallelizable and scalable self-attention design,
    set new benchmarks in performance. Its core tenets are the architectural bedrock
    for contemporary high-achieving generative language models such as GPT.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer凭借其可并行化和可扩展的自注意力设计，在性能上设定了新的基准。其核心原则是当代高性能生成语言模型（如GPT）的建筑基石。
- en: In the next chapter, we will discuss how to apply pre-trained generative models
    from prototype to production.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何将预训练的生成模型从原型应用到生产中。
- en: References
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本参考部分作为本书中引用的资源的存储库；您可以探索这些资源，以进一步加深对主题的理解和知识：
- en: Bahdanau, D., Cho, K., and Bengio, Y. (2014). *Neural machine translation by
    jointly learning to align and translate*. arXiv preprint arXiv:1409.0473.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau, D., Cho, K., and Bengio, Y. (2014). *Neural machine translation by
    jointly learning to align and translate*. arXiv preprint arXiv:1409.0473.
- en: Bengio, Y., Ducharme, R., and Vincent, P. (2003). *A neural probabilistic language
    model. The Journal of Machine Learning Research*, 3, 1137-1155.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio, Y., Ducharme, R., and Vincent, P. (2003). *A neural probabilistic language
    model. The Journal of Machine Learning Research*, 3, 1137-1155.
- en: Dadgar, S. M. H., Araghi, M. S., and Farahani, M. M. (2016). *Improving text
    classification performance based on TFIDF and LSI index*. 2016 IEEE International
    Conference on Engineering & Technology (ICETECH).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dadgar, S. M. H., Araghi, M. S., and Farahani, M. M. (2016). *Improving text
    classification performance based on TFIDF and LSI index*. 2016 IEEE International
    Conference on Engineering & Technology (ICETECH).
- en: Elman, J. L. (1990). *Finding structure in time. Cognitive science*, 14(2),
    179-211.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elman, J. L. (1990). *Finding structure in time. Cognitive science*, 14(2),
    179-211.
- en: Hochreiter, S., and Schmidhuber, J. (1997). *Long short-term memory. Neural
    computation*, 9(8), 1735-1780.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S., and Schmidhuber, J. (1997). *Long short-term memory. Neural
    computation*, 9(8), 1735-1780.
- en: Kim, Y. (2014). *Convolutional neural networks for sentence classification*.
    arXiv preprint arXiv:1408.5882.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim, Y. (2014). *Convolutional neural networks for sentence classification*.
    arXiv preprint arXiv:1408.5882.
- en: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). *Distributed
    representations of words and phrases and their compositionality. Advances in neural
    information processing* *systems*, 26.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). *词和短语的分布式表示及其组合性。神经信息处理系统进展*，26.
- en: 'Pennington, J., Socher, R., and Manning, C. (2014). *GloVe: Global vectors
    for word representation. Proceedings of the 2014 conference on empirical methods
    in natural language processing (**EMNLP)*, 1532-1543.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington, J., Socher, R., and Manning, C. (2014). *GloVe：全局词表示。2014年自然语言处理实证方法会议（**EMNLP**）论文集，1532-1543。
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    ... and Polosukhin, I. (2017). *Attention is all you need. Advances in neural
    information processing* *systems*, 30.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    ... and Polosukhin, I. (2017). *注意即一切。神经信息处理系统进展*，30.
