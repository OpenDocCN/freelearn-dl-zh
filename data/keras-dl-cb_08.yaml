- en: Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'An autoencoder is a type of neural network that is trained to attempt to copy
    its input to its output. It has a hidden layer (let''s call it *h*) that describes
    a code used to represent the input. The network may be viewed as consisting of
    two parts:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种神经网络，它的训练目的是尝试将输入复制到输出。它有一个隐藏层（我们称之为*h*），该层描述了一种用于表示输入的编码。可以将网络视为由两个部分组成：
- en: '**Encoder function**: *h = f (x)*'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器函数**：*h = f (x)*'
- en: '**Decoder that produces a reconstruction**: *r = g(h)*'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成重建的解码器**：*r = g(h)*'
- en: 'The following figure shows a basic autoencoder with input *n* and a hidden
    layer with neurons *m*:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个基本的自编码器，输入为 *n*，隐藏层的神经元为 *m*：
- en: '![](img/af478e87-a284-4b89-95bd-027cbdca7a02.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af478e87-a284-4b89-95bd-027cbdca7a02.png)'
- en: Basic representation of an autoencoder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的基本表示
- en: Autoencoders are designed to be unable to learn to copy perfectly. They are
    restricted in ways that allow them to copy only approximately, and to copy only
    input that resembles the training data. As the model is forced to prioritize which
    aspects of the input should be copied, it often learns useful properties of the
    data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器设计为不能完美地学习复制。它们受限于只能近似地复制，并且只能复制与训练数据相似的输入。由于模型被迫优先选择哪些输入方面应该被复制，它通常会学习到数据的有用属性。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Autoencoder algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器算法
- en: Under-complete autoencoders
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠完备自编码器
- en: Basic autoencoders
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本自编码器
- en: Additive Gaussian Noise autoencoders
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加性高斯噪声自编码器
- en: Sparse autoencoders
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Autoencoder algorithms
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器算法
- en: In the following notation, `x` is the input, `y` is the encoded data, `z` is
    the decoded data, `σ` is a nonlinear activation function (sigmoid or hyperbolic
    tangent, usually), and `f(x;θ)` means a function of `x` parameterized by `θ`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下符号中，`x` 是输入，`y` 是编码数据，`z` 是解码数据，`σ` 是非线性激活函数（通常是 Sigmoid 或双曲正切），`f(x;θ)`
    表示由 `θ` 参数化的 `x` 的函数。
- en: 'The model can be summarized in the following way:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以总结如下：
- en: 'The input data is mapped to the hidden layer (encoding). The mapping is usually
    an affine (allowing for or preserving parallel relationships.) transformation
    followed by a non-linearity:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据被映射到隐藏层（编码）。该映射通常是一个仿射变换（允许或保持平行关系），后跟非线性处理：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The hidden layer is mapped to the output layer, which is also called **decoding**.
    The mapping is an affine transformation (affine transformation is a linear mapping
    method that preserves points, straight lines, and planes) optionally followed
    by a non linearity. The following equation explains this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层被映射到输出层，这也称为**解码**。该映射是一个仿射变换（仿射变换是一种保持点、直线和平面的线性映射方法），后面可选择非线性化处理。以下方程式说明了这一点：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In order to reduce the size of the model, tied weights can be used, which means
    that the decoder weights matrix is constrained and can be the transpose of the
    encoder weights matrix, *θ'=θ^T*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减小模型的大小，可以使用绑定权重，这意味着解码器权重矩阵受到约束，可以是编码器权重矩阵的转置，*θ'=θ^T*。
- en: The hidden layer can have a lower or higher dimensionality than that of the
    input/output layers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的维度可以小于或大于输入/输出层的维度。
- en: In the case of lower dimensionality, the decoder reconstructs the original input
    from a lower-dimensional representation of it (also called **under-complete representation**).
    For the overall algorithm to work, the encoder should learn to provide a low-dimensional
    representation that captures the essence of the data (that is, the main factors
    of variations in the distribution). It is forced to find a good way to summarize
    the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维情况下，解码器从低维表示中重建原始输入（也称为**欠完备表示**）。为了使整个算法有效，编码器应学习提供一个低维表示，该表示捕捉数据的本质（即分布中变化的主要因素）。它被迫找到一种有效的方式来总结数据。
- en: 'Reference: [http://blackecho.github.io/blog/machine-learning/2016/02/29/denoising-autoencoder-tensorflow.html](http://blackecho.github.io/blog/machine-learning/2016/02/29/denoising-autoencoder-tensorflow.html).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：[http://blackecho.github.io/blog/machine-learning/2016/02/29/denoising-autoencoder-tensorflow.html](http://blackecho.github.io/blog/machine-learning/2016/02/29/denoising-autoencoder-tensorflow.html).
- en: Under-complete autoencoders
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠完备自编码器
- en: One of the ways to obtain useful features from the autoencoder is done by constraining
    *h* to have a smaller dimension than input `x`. An autoencoder with a code dimension
    less than the input dimension is called under-complete.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从自编码器中获取有用特征的一种方式是通过将 *h* 的维度约束为比输入 `x` 更小。一个编码维度小于输入维度的自编码器被称为欠完备。
- en: Learning a representation that is under-complete forces the autoencoder to capture
    the most salient features of the training data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 学习一个欠完备的表示迫使自编码器捕捉训练数据中最重要的特征。
- en: The learning process is described as minimizing a loss function, `L(x, g(f(x)))`,
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程描述为最小化损失函数 `L(x, g(f(x)))`，
- en: where `L` is a loss function penalizing `g(f (x))` for being dissimilar from
    `x`, such as the mean squared error.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `L` 是一个损失函数，惩罚 `g(f (x))` 与 `x` 的不相似性，例如均方误差。
- en: Dataset
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We are planning to use the MNIST dataset in the `idx3` format as input to train
    our autoencoders. We will be testing the autoencoder on the first 100 images.
    Let''s first plot the original images:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划使用 `idx3` 格式的 MNIST 数据集作为输入来训练我们的自编码器。我们将测试自编码器在前 100 张图像上的表现。让我们首先绘制原始图像：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding is the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下图所示：
- en: '![](img/3ea4bce8-c710-41bb-9eed-3ba8deac2dde.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ea4bce8-c710-41bb-9eed-3ba8deac2dde.png)'
- en: Plot of original MNIST images
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 MNIST 图像的绘图
- en: Basic autoencoders
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本自编码器
- en: 'Let''s look at a basic example of an autoencoder that also happens to be a
    basic autoencoder. First, we will create an `AutoEncoder` class and initialize
    it with the following parameters passed to `__init__()`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个基本的自编码器示例，它恰好是一个基础自编码器。首先，我们将创建一个 `AutoEncoder` 类，并使用以下参数初始化它：
- en: '`num_input`: Number of input samples'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_input`: 输入样本的数量'
- en: '`num_hidden`: Number of neurons in the hidden layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden`: 隐藏层神经元的数量'
- en: '`transfer_function=tf.nn.softplus`: Transfer function'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transfer_function=tf.nn.softplus`: 激活函数'
- en: '`optimizer = tf.train.AdamOptimizer()`: Optimizer'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer = tf.train.AdamOptimizer()`: 优化器'
- en: 'You can either pass a custom `transfer_function` and `optimizer` or use the
    default one specified. In our example, we are using softplus as the default `transfer_function`
    (also called **activation function**): `f(x)=ln(1+e^x)`.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以传递一个自定义的 `transfer_function` 和 `optimizer`，或者使用指定的默认值。在我们的示例中，我们使用 softplus
    作为默认的 `transfer_function`（也叫做 **激活函数**）：`f(x)=ln(1+e^x)`。
- en: Autoencoder initialization
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器初始化
- en: 'First, we initialize the class variables and weights:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化类变量和权重：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, the `_initialize_weigths()` function is a local function that initializes
    values for the `weights` dictionary:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`_initialize_weigths()` 函数是一个局部函数，用于初始化 `weights` 字典的值：
- en: '`w1` is a 2D tensor with shape `num_input X num_hidden`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w1` 是一个形状为 `num_input X num_hidden` 的二维张量'
- en: '`b1` is a 1D tensor with shape `num_hidden`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1` 是一个形状为 `num_hidden` 的一维张量'
- en: '`w2` is a 2D tensor with shape `num_hidden X num_input`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w2` 是一个形状为 `num_hidden X num_input` 的二维张量'
- en: '`b2` is a 2D tensor with shape `num_input`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2` 是一个形状为 `num_input` 的二维张量'
- en: 'The following code shows how weights are initialized as a dictionary of TensorFlow
    variables for two hidden layers:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何将权重初始化为两个隐藏层的 TensorFlow 变量字典：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we define `x_var`, `hidden_layer`, and `reconstruction layer`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义 `x_var`、`hidden_layer` 和 `reconstruction layer`：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/65f7a30e-20da-4026-b85e-8536e42987f3.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65f7a30e-20da-4026-b85e-8536e42987f3.jpg)'
- en: Cost function
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数
- en: Instantiate the global variables initializer and pass it to TensorFlow session.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化全局变量初始化器并将其传递给 TensorFlow 会话。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: AutoEncoder class
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoEncoder 类
- en: 'Th following code shows `AutoEncoder` class. This class with be instantiated
    for samples in next few sections to create autoencoders:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了 `AutoEncoder` 类。该类将在接下来的几个部分实例化，以创建自编码器：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Basic autoencoders with MNIST data
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本自编码器与 MNIST 数据
- en: 'Let''s use the autoencoder with MNIST data: `mnist = input_data.read_data_sets(''MNIST_data'',
    one_hot = True)`.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 MNIST 数据集中的自编码器：`mnist = input_data.read_data_sets('MNIST_data', one_hot
    = True)`。
- en: 'Use StandardScalar from Scikit Learn''s `sklearn.``preprocessing` module to
    extract `testmnist.test.images` and training images `mnist.train.images`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Scikit Learn 的 `sklearn.``preprocessing` 模块中的 StandardScalar 来提取 `testmnist.test.images`
    和训练图像 `mnist.train.images`：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preprocessing module provides a utility class, `StandardScaler`, which implements
    the Transformer API. This computes and transforms the mean and standard deviation
    of a training set. It reapplies the same transformation to the testing set. By
    default, Scalar centers the mean and makes the variance one.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理模块提供了一个工具类`StandardScaler`，它实现了Transformer API。它计算并转换训练集的均值和标准差，并将相同的转换应用到测试集。默认情况下，Scaler会将均值居中，并使方差为1。
- en: It is possible to disable either centering or scaling by passing `with_mean=False`
    or `with_std=False` to the constructor of StandardScaler.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将`with_mean=False`或`with_std=False`传递给StandardScaler的构造函数来禁用中心化或标准化。
- en: 'Next, we define an instance of the AutoEncoder class listed earlier:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义之前列出的AutoEncoder类的实例：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice that the autoencoder includes the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，自编码器包括以下内容：
- en: Number of input neurons is `784`
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入神经元的数量是`784`
- en: Number of neurons in the hidden layer is `200`
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的神经元数量是`200`
- en: Activation function is `tf.nn.softplus`
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数是`tf.nn.softplus`
- en: Optimizer is `tf.train.AdamOptimizer`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器是`tf.train.AdamOptimizer`
- en: 'Next, we iterate over the training data and display the cost function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将遍历训练数据并显示成本函数：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Print the total cost:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 打印总成本：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the epochs is listed as follows; as expected, the cost converges
    with each iteration:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练周期的输出如下所示；如预期，成本随着每次迭代而收敛：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Basic autoencoder plot of weights
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本自编码器权重图示
- en: 'Once the training is done show the plot of weights using the Matplotlib library
    using code listing:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，使用Matplotlib库绘制权重图，代码清单如下：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/8f561398-b0db-4d6f-b0eb-622b94e61396.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f561398-b0db-4d6f-b0eb-622b94e61396.png)'
- en: Basic autoencoder weights plot
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基本自编码器权重图
- en: In the next section, we will look at how images are recreated using the weights
    shown in the preceding image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究如何使用前面图像中显示的权重重建图像。
- en: Basic autoencoder recreated images plot
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本自编码器重建图像图示
- en: 'Having recreated the images, let''s plot them to get a feel of how they look.
    First, we will reconstruct the images using the `autoencoder` instance created
    earlier:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在重建了图像后，让我们绘制它们，以感受它们的外观。首先，我们将使用之前创建的`autoencoder`实例重建图像：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s look at the created images from the neural network:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下神经网络创建的图像：
- en: '![](img/fcaccfd9-080c-4775-a79e-3f0756e2c92b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcaccfd9-080c-4775-a79e-3f0756e2c92b.png)'
- en: Basic autoencoder plot of output images
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基本自编码器的输出图像图示
- en: Basic autoencoder full code listing
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本自编码器完整代码清单
- en: 'The full code listing can be found here or can also be downloaded from GitHub--[https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch07/basic_autoencoder_example.py](https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch07/basic_autoencoder_example.py):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码清单可以在这里找到，也可以从GitHub下载--[https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch07/basic_autoencoder_example.py](https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch07/basic_autoencoder_example.py)：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Basic autoencoder summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本自编码器总结
- en: 'The autoencoder created a basic approximation of MNSIT images using 200 neuron
    hidden layers. The following diagram shows nine images and how they were transformed
    into approximations using a basic autoencoder:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器使用200个神经元的隐藏层创建了MNSIT图像的基本近似。下图展示了九张图像以及它们如何通过基本自编码器转换为近似：
- en: '![](img/f98d20e7-7c2e-4a37-8e18-5fc0bc23ab87.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f98d20e7-7c2e-4a37-8e18-5fc0bc23ab87.png)'
- en: Basic autoencoder input and output representation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基本自编码器输入和输出表示
- en: In the next section, we will look at a more advanced autoencoder, an **Additive
    Gaussian Noise AutoEncoder**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究一种更先进的自编码器，**加性高斯噪声自编码器**。
- en: Additive Gaussian Noise autoencoder
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性高斯噪声自编码器
- en: What are Denoising autoencoders? They are very similar to the basic model we
    saw in previous sections, the difference is that, the input is corrupted before
    being passed to the network. By matching the original version (not the corrupted
    one) with the reconstruction at training time, this autoencoder gets trained to
    reconstruct the original input image from the corrupted image.  The ability to
    reconstruct original image from corrupted image makes autoencoder very smart.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是去噪自编码器？它们与我们在前面章节中看到的基本模型非常相似，不同之处在于，输入在传递到网络之前会被破坏。通过在训练时将原始版本（而不是破坏后的版本）与重建版本进行匹配，这个自编码器被训练用来从破损图像中重建原始输入图像。能够从破损图像重建原始图像使自编码器变得非常智能。
- en: 'An additive noise autoencoder uses the following equation to add corruption
    to incoming data:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 加性噪声自编码器使用以下公式将噪声添加到输入数据中：
- en: '*x[corr] = x + scale*random_normal(n)*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*x[corr] = x + scale*random_normal(n)*'
- en: 'The following is the detail describe about the preceding equation:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于前面方程的详细描述：
- en: '*x* is the original image'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是原始图像'
- en: '*scale* is the multiplier for a random normal number generated from *n*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*scale* 是乘以从 *n* 中生成的随机正态数的因子'
- en: '*n* is the number of training samples'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是训练样本的数量'
- en: '*x[corr]* is the corrupted output'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[corr]* 是损坏后的输出'
- en: Autoencoder class
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器类
- en: 'We initialize the autoencoder defined in `class AdditiveGaussianNoiseAutoEncoder`
    by passing following parameters:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过传入以下参数来初始化在 `class AdditiveGaussianNoiseAutoEncoder` 中定义的自编码器：
- en: '`num_input`: Number of input samples'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_input`: 输入样本数量'
- en: '`num_hidden`: Number of neurons in the hidden layer'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden`: 隐藏层中的神经元数量'
- en: '`transfer_function=tf.nn.sigmoid`: Transfer function'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transfer_function=tf.nn.sigmoid`: 转换函数'
- en: '`optimizer = tf.train.AdamOptimizer()`: Optimizer'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer = tf.train.AdamOptimizer()`: 优化器'
- en: '`scale=0.1`: Scale for corruption of the image'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale=0.1`: 图像损坏的比例'
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Assign the passed parameters to the instance variables:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将传入的参数分配给实例变量：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Initialize the hidden layer `hidden_layer` and the reconstruction layer `reconstruction`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化隐藏层 `hidden_layer` 和重建层 `reconstruction`：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the cost function and the optimizer:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 定义成本函数和优化器：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The cost function remains the same as the basic autoencoder
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数与基本自编码器相同
- en: '![](img/594dd7ad-8b38-4948-9df7-edea73778020.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/594dd7ad-8b38-4948-9df7-edea73778020.jpg)'
- en: Cost function of additive Gaussian autoencoder
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 加性高斯自编码器的成本函数
- en: 'Finally, we initialize global variables, create a TensorFlow session, and run
    it to execute the `init` graph:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们初始化全局变量，创建 TensorFlow 会话，并运行它来执行 `init` 图：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the next section, we will look at how this autoencoder will be used to encode
    MNIST data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究如何使用该自编码器对 MNIST 数据进行编码。
- en: Additive Gaussian Autoencoder with the MNIST dataset
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性高斯自编码器与 MNIST 数据集
- en: 'First, we load the train and test datasets, `X_train` and `X_test`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载训练和测试数据集 `X_train` 和 `X_test`：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the variables for the number of samples, `n_samples`, `training_epoch`,
    and `batch_size` for each iteration of the training and `display_step`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次训练迭代的样本数量 `n_samples`、训练纪元 `training_epoch` 和批次大小 `batch_size` 以及显示步数 `display_step`：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Instantiate the autoencoder and the optimizer. The autoencoder has 200 hidden
    units and uses sigmoid as the `transfer_function`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化自编码器和优化器。自编码器有 200 个隐藏单元，并使用 sigmoid 作为 `transfer_function`：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Training the model
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Once the neural network layers have been defined we train the model by calling
    method
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦神经网络层被定义，我们通过调用方法来训练模型：
- en: '`autoencoder.partial_fit(batch_xs)` for each batch of data:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`autoencoder.partial_fit(batch_xs)` 每次处理一批数据：'
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The cost of each epoch is printed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每个纪元的成本已打印：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The total cost of training is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的总成本如下：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Plotting the weights
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制权重
- en: 'Let''s plot the weights visually and plot them using Matplotlib:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 Matplotlib 可视化地绘制权重：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/02034b23-6d96-47a1-bd02-38459aaecc86.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02034b23-6d96-47a1-bd02-38459aaecc86.png)'
- en: Weights of the neurons in hidden layers for the Additive Gaussian Auto Encoder
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 加性高斯自编码器中隐藏层神经元的权重
- en: Plotting the reconstructed images
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制重建的图像
- en: 'The last step is to print the reconstructed images to give us visual proof
    of how the encoder is able to reconstruct the images based on the weights:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是打印重建的图像，给我们提供视觉证据，说明编码器如何根据权重重建图像：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](img/29d01b60-b0b1-4d30-b2f5-cdd4801808d6.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29d01b60-b0b1-4d30-b2f5-cdd4801808d6.png)'
- en: Reconstructed images using the Additive Gaussian Auto Encoder
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加性高斯自编码器重建的图像
- en: Additive Gaussian autoencoder full code listing
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性高斯自编码器完整代码列表
- en: 'The following is the code of the Additive Gaussian autoencoder:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加性高斯自编码器的代码：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Comparing basic encoder costs with the Additive Gaussian Noise autoencoder
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将基本编码器的成本与加性高斯噪声自编码器进行比较
- en: 'The following graph shows the cost of two algorithms for each epoch. It can
    be inferred that the basic autoencoder is much more expensive compared to the
    Additive Gaussian Noise autoencoder:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了两个算法在每个纪元中的成本。可以推断，基本自编码器比加性高斯噪声自编码器更昂贵：
- en: '![](img/5ce1b147-0a74-4a1d-a216-df06b6355f71.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ce1b147-0a74-4a1d-a216-df06b6355f71.png)'
- en: 'Cost comparison: basic versus Additive Gaussian Noise autoencoder'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 成本比较：基本自编码器与加性高斯噪声自编码器
- en: Additive Gaussian Noise autoencoder summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性高斯噪声自编码器总结
- en: You learned how to create an autoencoder with Gaussian noise, which helps in
    improving the accuracy of the model drastically when compared to the basic autoencoder.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习了如何创建带有高斯噪声的自编码器，与基础自编码器相比，这大大提高了模型的准确性。
- en: Sparse autoencoder
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: In this section, we will look at how adding sparsity to the cost function helps
    in reducing the cost of training. Most of the code remains the same, but the primary
    changes are in the way the cost function is calculated.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将研究如何通过向代价函数添加稀疏性来帮助降低训练代价。大多数代码保持不变，但主要的变化在于计算代价函数的方式。
- en: KL divergence
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KL散度
- en: Let's first try to understand KL divergence, which is used to add sparsity to
    the cost function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试理解KL散度，它用于向代价函数添加稀疏性。
- en: We can think of a neuron as active (or *firing*) if a neuron's output value
    is close to one, and *inactive* if its output value is close to zero. We would
    like to constrain the neurons to be inactive most of the time. This discussion
    assumes a sigmoid activation function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个神经元视为处于激活状态（或*发放*）当其输出值接近1时，处于*非激活*状态当其输出值接近零时。我们希望大多数时间神经元保持非激活状态。这个讨论假设使用sigmoid激活函数。
- en: Recall that *a^((2))[j]* denotes the activation of the hidden unit *j* in the
    autoencoder. This notation does not state explicitly what the input *x *was that
    led to this activation. We will write *a^((2))[j](x)* to denote the activation
    of the hidden unit when the network is given a specific input *x*. Further, let
    ![](img/10147870-75b0-464d-b1cf-2edc0807e4d2.jpg) be the average activation of
    the hidden unit *j* (averaged over the training set). We would like to (approximately)
    enforce the constraint ![](img/9971f609-c670-4c63-a7d8-381212c5620c.png), where
    [![](img/82da78e8-7a12-4a56-8037-b83b98326850.png)] is a sparsity parameter, typically
    a small value close to zero (say, = *0.05*). Our aim is that the average activation
    of each hidden neuron *j* be close to *0.05* (as an example). To satisfy the preceding
    constraint, the hidden unit's activation must mostly be close to zero.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，*a^((2))[j]*表示自编码器中隐藏单元*j*的激活值。这个符号没有明确说明导致此激活的输入*x*是什么。我们将写作 *a^((2))[j](x)*，表示当网络给定特定输入*x*时，隐藏单元的激活值。此外，令
    ![](img/10147870-75b0-464d-b1cf-2edc0807e4d2.jpg) 为隐藏单元*j*的平均激活值（在训练集上平均）。我们希望（大致）强制执行约束
    ![](img/9971f609-c670-4c63-a7d8-381212c5620c.png)，其中 [![](img/82da78e8-7a12-4a56-8037-b83b98326850.png)]
    是一个稀疏性参数，通常是一个接近零的小值（例如，= *0.05*）。我们的目标是让每个隐藏神经元*j*的平均激活值接近*0.05*（作为一个例子）。为了满足前述约束，隐藏单元的激活必须大部分时间接近零。
- en: 'To achieve this, an extra penalty term is added to the optimization objective
    that penalizes it, deviating significantly from[![](img/82da78e8-7a12-4a56-8037-b83b98326850.png)]:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们在优化目标中加入了一个额外的惩罚项，该惩罚项会惩罚与[![](img/82da78e8-7a12-4a56-8037-b83b98326850.png)]的偏差过大的情况：
- en: '![](img/6088760a-c222-4657-b2d1-7fcf511893dd.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6088760a-c222-4657-b2d1-7fcf511893dd.jpg)'
- en: 'Let''s take a look at how KL divergence varies as a function of the average
    activation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看KL散度如何随平均激活值的变化而变化：
- en: '![](img/16a1258c-7aef-41ae-95dd-00965d51e402.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16a1258c-7aef-41ae-95dd-00965d51e402.png)'
- en: Average activation versus KL divergence plot
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 平均激活值与KL散度图
- en: KL divergence in TensorFlow
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的KL散度
- en: 'In our implementation of a sparse encoder, we defined KL divergence in a `kl_divergence`
    function in the `SparseEncoder` class, which is nothing but an implementation
    of the preceding formula:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现的稀疏编码器中，我们在`SparseEncoder`类中的`kl_divergence`函数里定义了KL散度，这只是前述公式的实现：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Cost of a sparse autoencoder based on KL Divergence
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于KL散度的稀疏自编码器代价
- en: 'The cost function is redefined with two new parameters, `sparse_reg` and `kl_divergence`,
    when compared to the previous encoders discussed in this chapter:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章讨论的先前编码器相比，代价函数在重新定义时引入了两个新参数，`sparse_reg`和`kl_divergence`：
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Complete code listing of the sparse autoencoder
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器的完整代码列表
- en: 'For reference, we have given the code listing for `SparseAutoEncoder` here,
    with the `kl_divergence` and `cost` discussed earlier:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，下面给出了`SparseAutoEncoder`的代码列表，其中包括之前讨论过的`kl_divergence`和`cost`：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the next section we will look at Sparse autoencoder applied to a specific
    dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究稀疏自编码器在特定数据集上的应用。
- en: Sparse autoencoder on MNIST data
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MNIST数据上的稀疏自编码器
- en: 'Let''s run this encoder on the same dataset that we used in the other examples
    and compare the results:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在与其他示例中使用相同的数据集上运行这个编码器，并比较结果：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As can be seen, the cost is lower than other encoders, hence KL divergence and
    sparsity definitely help.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，成本低于其他编码器，因此 KL 散度和稀疏性确实起到了帮助作用。
- en: Comparing the Sparse encoder with the Additive Gaussian Noise encoder
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较稀疏编码器和加性高斯噪声编码器
- en: 'The following graph shows how the costs compare for the Additive Gaussian Noise
    autoencoder and the Sparse autoencoder:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了加性高斯噪声自编码器和稀疏自编码器的成本比较：
- en: '![](img/7bafe3e7-94b6-4a06-96e7-8422a114d7ab.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bafe3e7-94b6-4a06-96e7-8422a114d7ab.png)'
- en: Cost comparison of two autoencoders for five epochs on the MNIST dataset
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MNIST 数据集上运行五个周期的两个自编码器的成本比较
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned three different types of autoencoders: basic,
    Additive Gaussian Noise, and Sparse. We understood the use cases where they can
    be useful. We ran them against the MNIST dataset and also compared the cost of
    the three autoencoders. We also plotted the weights as well as the approximate
    output.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了三种不同类型的自编码器：基本自编码器、加性高斯噪声自编码器和稀疏自编码器。我们理解了它们的应用场景，并在 MNIST 数据集上运行了它们，同时比较了这三种自编码器的成本。我们还绘制了权重以及近似输出。
