- en: '*Chapter 7:* Implementing NLP Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章：* 实现NLP应用'
- en: In [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent
    Neural Networks for Demand Prediction*, we introduced **Recurrent Neural Networks**
    (**RNNs**) as a family of neural networks that are especially powerful to analyze
    sequential data. As a case study, we trained a **Long Short-Term Memory** (**LSTM**)-based
    RNN to predict the next value in the time series of consumed electrical energy.
    However, RNNs are not just suitable for strictly numeric time series, as they
    have also been applied successfully to other types of time series.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181)《需求预测的循环神经网络》中，我们介绍了**循环神经网络**（**RNN**）作为一种特别擅长分析顺序数据的神经网络家族。作为案例研究，我们训练了一个基于**长短期记忆**（**LSTM**）的RNN来预测消耗的电能时间序列中的下一个值。然而，RNN不仅仅适用于严格的数字时间序列，它们也成功地应用于其他类型的时间序列。
- en: Another field where RNNs are state of the art is **Natural Language Processing**
    (**NLP**). Indeed, RNNs have been applied successfully to text classification,
    language models, and neural machine translation. In all of these tasks, the time
    series is a sequence of words or characters, rather than numbers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的另一个先进应用领域是**自然语言处理**（**NLP**）。实际上，RNN已成功应用于文本分类、语言模型和神经机器翻译等任务。在所有这些任务中，时间序列是由单词或字符组成的序列，而不是数字。
- en: 'In this chapter, we will run a short review of some classic NLP case studies
    and their RNN-based solutions: a sentiment analysis application, a solution for
    free text generation, and a similar solution for the generation of name candidates
    for new products.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将简要回顾一些经典的NLP案例研究及其基于RNN的解决方案：情感分析应用、自由文本生成解决方案，以及为新产品生成名称候选的类似解决方案。
- en: We will start with an overview of text encoding techniques to prepare the sequence
    of words/characters to feed our neural network. The first case study, then, classifies
    text based on its sentiment. The last two case studies generate new text as sequences
    of new words, and new words as sequences of new characters, respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述文本编码技术，以准备将单词/字符序列输入我们的神经网络。第一个案例研究将文本根据其情感进行分类。接下来的两个案例研究分别生成新的文本序列和新的单词序列，新的单词则由新字符序列构成。
- en: 'In this chapter we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Exploring Text Encoding Techniques for Neural Networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索神经网络的文本编码技术
- en: Finding the Tone of your Customers' Voice – Sentiment Analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到客户声音的语气——情感分析
- en: Generating Free Text with RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN生成自由文本
- en: Generating Product Names with RNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN生成产品名称
- en: Exploring Text Encoding Techniques for Neural Networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索神经网络的文本编码技术
- en: In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Neural Network*, you learned that feedforward networks
    – and all other neural networks as well – are trained on numbers and don't understand
    nominal values. In this chapter, we want to feed words and characters into neural
    networks. Therefore, we need to introduce some techniques to encode sequences
    of words or characters – that is, sequences of nominal values – into sequences
    of numbers or numerical vectors. In addition, in NLP applications with RNNs, it
    is mandatory that the order of words or characters in the sequence is retained
    throughout the text encoding procedure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建与训练前馈神经网络》中，你学习了前馈网络——以及所有其他神经网络——是通过数字进行训练的，并不理解名义值。在本章中，我们希望将单词和字符输入神经网络。因此，我们需要引入一些技术，将单词或字符序列——也就是名义值的序列——编码为数字序列或数值向量。此外，在使用RNN的NLP应用中，必须确保在整个文本编码过程中保持单词或字符序列的顺序。
- en: Let's have a look at some **text encoding** techniques before we dive into the
    NLP case studies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入NLP案例研究之前，让我们先看一下几种**文本编码**技术。
- en: Index Encoding
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引编码
- en: In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Neural Network*, you learned about **index encoding**
    for nominal values. The idea was to represent each nominal class with an integer
    value, also called an index.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建与训练前馈神经网络》中，你学习了针对名义值的**索引编码**。其思想是用整数值表示每个名义类，这个整数值也叫做索引。
- en: 'We can use this same idea for text encoding. Here, instead of encoding each
    class with a different index, we encode each word or each character with a different
    index. First, a dictionary must be created to map all words/characters in the
    text collection to an index; afterward, through this mapping, each word/character
    is transformed into its corresponding index and, therefore, each sequence of words/characters
    into the sequence of corresponding indexes. In the end, each text is represented
    as a sequence of indexes, where each index encodes a word or a character. The
    following figure gives you an example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的思路进行文本编码。在这里，我们不是用不同的索引来编码每个类别，而是用不同的索引来编码每个单词或每个字符。首先，必须创建一个字典，将文本集合中的所有单词/字符映射到一个索引；然后，通过这个映射，每个单词/字符被转换成其对应的索引，从而将每个单词/字符序列转换成相应索引的序列。最终，每个文本被表示为一个索引序列，其中每个索引编码了一个单词或一个字符。下面的图给出了一个例子：
- en: '![Figure 7.1 – An example of text encoding via indexes at the word level](img/B16391_07_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 基于索引的单词级文本编码示例](img/B16391_07_001.jpg)'
- en: Figure 7.1 – An example of text encoding via indexes at the word level
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 基于索引的单词级文本编码示例
- en: Notice that index 1, for the word *the*, and index 13, for the word *brown*,
    are repeated twice in the sequence, as the words appear twice in the example sentence,
    *the quick brown fox jumped over the brown dog*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在序列中，单词*the*的索引1和单词*brown*的索引13在序列中被重复了两次，因为这两个单词在示例句子*the quick brown fox
    jumped over the brown dog*中各出现了两次。
- en: Later in this chapter, in the *Finding the Tone of Your Customers' Voice – Sentiment
    Analysis* section, we'll use index encoding on words to represent text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，在*找到客户声音的语气 – 情感分析*一节中，我们将使用基于索引的编码来表示文本中的单词。
- en: In the *Free Text Generation with RNNs* section, on the other hand, we'll use
    one-hot vectors as text encoding on characters. Let's explore what one-hot vector
    encoding is.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在*使用RNN进行自由文本生成*一节中，我们将使用独热向量作为字符级文本编码。让我们来探索什么是独热向量编码。
- en: One-Hot Vector Encoding
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热向量编码
- en: The sequence of indexes has the disadvantage that it introduces an artificial
    distance between words/characters. For example, if *apple* is encoded as 1, *shoe*
    as 2, and *pear* as 3, *apple* and *pear* are further away from each other (distance
    = 2) than *shoe* and *pear* (distance = 1), which semantically might not make
    sense. In this way, as words don't have an ordered structure, we would introduce
    an artificial distance/similarity between words that might not exist in reality.
    We also encountered this problem in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Neural Network*, and we solved it by introducing
    the concept of one-hot vectors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 索引序列的缺点在于，它在单词/字符之间引入了人为的距离。例如，如果*apple*被编码为1，*shoe*为2，*pear*为3，那么*apple*和*pear*之间的距离是2，而*shoe*和*pear*之间的距离是1，这在语义上可能并不合理。通过这种方式，由于单词没有顺序结构，我们会人为地在单词之间引入一些可能在实际中并不存在的距离/相似性。我们在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建和训练前馈神经网络》中也遇到了这个问题，并通过引入独热向量的概念来解决它。
- en: The idea of `1` to encode a specific word/character, or otherwise to `0`. This
    means that each word/character is represented as a one-hot vector and therefore,
    each text is a sequence of one-hot vectors. The following figure shows an example
    of one-hot vector encoding for the sentence *the quick brown fox jumped over the
    brown dog*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`1`来编码特定的单词/字符，否则使用`0`。这意味着每个单词/字符都被表示为一个独热向量，因此，每个文本是一个独热向量的序列。下图展示了句子*the
    quick brown fox jumped over the brown dog*的独热向量编码示例。
- en: 'Notice, in *Figure 7.2*, that the one-hot vectors for the words *the* and *brown*
    repeat twice in the sequence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图 7.2*中，单词*the*和*brown*的独热向量在序列中被重复了两次：
- en: '![Figure 7.2 – An example of text encoding via one-hot vectors at the word
    level](img/B16391_07_002.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 基于独热向量的单词级文本编码示例](img/B16391_07_002.jpg)'
- en: Figure 7.2 – An example of text encoding via one-hot vectors at the word level
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 基于独热向量的单词级文本编码示例
- en: Tip
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Remember that the **Keras Learner** node can convert index-based encodings into
    one-hot vectors. Thus, to train a neural network on one-hot-vectors, it is sufficient
    to feed it with an index-based encoding of the text document.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，**Keras Learner**节点可以将基于索引的编码转换为独热向量。因此，要训练一个神经网络来处理独热向量，只需将文本文档的基于索引的编码输入即可。
- en: A commonly used text encoding – similar to one-hot vectors but that doesn't
    retain the word order – are `1`) or absence (`0`) of the words. One vector represents
    one text document and contains multiple 1s. Notice that this encoding does not
    retain the word order because all of the text is encoded within the same vector
    structure regardless of the word order.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的文本编码——类似于独热向量，但不保留单词顺序——是`1）`或缺失（`0`）的单词。一个向量表示一个文本文档，包含多个1。请注意，这种编码不保留单词顺序，因为所有文本都被编码在同一个向量结构中，而不管单词的顺序如何。
- en: Working with words, the dimension of one-hot vectors is equal to the dictionary
    size – that is, to the number of words available in the document corpus. If the
    document corpus is large, the dictionary size quickly becomes the number of words
    in the whole language. Therefore, one-hot vector encoding on a word level can
    lead to very large and sparse representations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理单词时，独热编码向量的维度等于词典大小——也就是说，等于文档语料库中可用单词的数量。如果文档语料库很大，词典大小会迅速变成整个语言中的单词数量。因此，在单词级别上的独热编码可能会导致非常大且稀疏的表示。
- en: Working with characters, the dictionary size is the size of the character set,
    which, even including punctuation and special signs, is much smaller than in the
    previous case. Thus, one-hot vector encoding fits well for character encoding
    but might lead to dimensionality explosion on word encoding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理字符时，词典大小是字符集的大小，即使包括标点符号和特殊符号，这也比前一种情况小得多。因此，独热向量编码适合字符编码，但在单词编码时可能导致维度爆炸。
- en: To encode a document at the word level, a much more appropriate method is embeddings.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要在单词级别对文档进行编码，一种更合适的方法是使用嵌入技术。
- en: Embeddings for Word Encoding
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词编码的嵌入技术
- en: The goal of word embeddings is to map words into a geometric space. This is
    done by associating a numeric vector to every word in a dictionary in a way that
    words with similar meanings have similar vectors and the distance between any
    two vectors captures part of the semantic relationship between the two associated
    words. The geometric space formed by these vectors is called the *embedding space*.
    For word encoding, the embedding space has a lower dimension (only a few tens
    or hundreds) than the vector space for one-hot vector encodings (in the order
    of many thousands).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的目标是将单词映射到几何空间中。通过为词典中的每个单词关联一个数值向量来实现这一点，使得具有相似含义的单词具有相似的向量，并且任何两个向量之间的距离捕捉了这两个相关单词之间的某些语义关系。由这些向量构成的几何空间被称为*嵌入空间*。对于单词编码，嵌入空间的维度比独热编码的向量空间要低（通常只有几十或几百），而独热编码的向量空间维度则通常达到几千。
- en: To learn the projection of each word into the continuous vector space, a dedicated
    neural network layer is used, which is called the embedding layer. This layer
    learns to associate a vector representation with each word. The best-known word
    embedding techniques are **Word2vec** and **GloVe**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习每个单词在连续向量空间中的投影，使用了一个专门的神经网络层，这个层被称为嵌入层。这个层学习将每个单词与一个向量表示关联起来。最著名的词嵌入技术是**Word2vec**和**GloVe**。
- en: 'There are two ways that words embeddings can be used (J. Brownlee, *How to
    Use Word Embedding Layers for Deep Learning with Keras*, Machine Learning Mastery
    Blog, 2017, [https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以有两种使用方式（J. Brownlee，*如何在Keras中使用词嵌入层进行深度学习*，机器学习精通博客，2017年，[https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)）：
- en: Adopting a ready-to-go layer previously trained on some external text corpus
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用一个已经在某些外部文本语料库上训练好的现成层
- en: Training a new embedding layer as part of your neural network
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新的嵌入层作为神经网络的一部分进行训练
- en: If trained jointly with a neural network, the input to an embedding layer is
    an index-based encoded sequence. The number of output units in the embedding layer
    defines the dimension of the embedding space. The weights of the embedding layer,
    which are used to calculate the embedding representation of each index, and therefore
    of each word, are learned during the training of the network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与神经网络联合训练，嵌入层的输入是一个基于索引的编码序列。嵌入层中的输出单元数量定义了嵌入空间的维度。嵌入层的权重用于计算每个索引的嵌入表示，因此也是每个单词的嵌入表示，这些权重在网络训练过程中学习得到。
- en: Now that we are familiar with different text encoding techniques, let's move
    on to our first NLP use case.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了不同的文本编码技术，接下来让我们进入第一个自然语言处理应用案例。
- en: Finding the Tone of Your Customers' Voice – Sentiment Analysis
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找客户声音的语气 – 情感分析
- en: A common use case for NLP is **sentiment analysis**. Here, the goal is to identify
    the underlying emotion in some text, whether positive or negative, and all the
    nuances in between. Sentiment analysis is implemented in many fields, such as
    to analyze incoming messages, emails, reviews, recorded conversations, and other
    similar texts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的一个常见用例是 **情感分析**。在这里，目标是识别文本中的潜在情感，无论是正面、负面，还是介于两者之间的各种细微差别。情感分析已被应用于许多领域，如分析来电信息、电子邮件、评论、录音对话及其他类似文本。
- en: Generally, sentiment analysis belongs to a bigger group of NLP applications
    known as text classification. In the case of sentiment analysis, the goal is to
    predict the sentiment class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，情感分析属于更大范围的自然语言处理（NLP）应用组，被称为文本分类。在情感分析的情况下，目标是预测情感类别。
- en: Another common example of text classification is language detection. Here, the
    goal is to recognize the text language. In both cases, if we use an RNN for the
    task, we need to adopt a *many-to-one architecture*. A many-to-one neural architecture
    accepts a sequence of inputs at different times, ![](img/Formula_B16391_07_001.png),
    and uses the final state of the output unit to predict the one single class –
    that is, sentiment or language.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的文本分类示例是语言检测。在这种情况下，目标是识别文本的语言。在这两种情况下，如果我们使用 RNN 来处理任务，我们需要采用 *多对一架构*。多对一神经网络架构接受不同时间的输入序列，![](img/Formula_B16391_07_001.png)，并利用输出单元的最终状态来预测单一类别——即情感或语言。
- en: '*Figure 7.3* shows an example of a many-to-one architecture:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.3* 显示了一个多对一架构的示例：'
- en: '![Figure 7.3  –  An example of a many-to-one neural architecture: a sequence
    of many inputs at different times and only the final status of the output](img/B16391_07_003.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 一个多对一神经网络架构的示例：一系列在不同时间的输入，仅使用输出的最终状态](img/B16391_07_003.jpg)'
- en: 'Figure 7.3 – An example of a many-to-one neural architecture: a sequence of
    many inputs at different times and only the final status of the output'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 一个多对一神经网络架构的示例：一系列在不同时间的输入，仅使用输出的最终状态
- en: In our first use case in this chapter, we want to analyze the sentiment of movie
    reviews. The goal is to train an RNN at a word level, with an embedding layer
    and an LSTM layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个用例中，我们想要分析电影评论的情感。目标是训练一个基于词级别的 RNN，并结合嵌入层和 LSTM 层。
- en: 'For this example, we will use the IMDb dataset, which contains two columns:
    the text of the movie reviews and the sentiment. The sentiment is encoded as `1`
    for positive reviews and as `0` for negative reviews.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 IMDb 数据集，该数据集包含两列：电影评论的文本和情感标签。情感标签用 `1` 表示正面评论，用 `0` 表示负面评论。
- en: '*Figure 7.4* shows you a small subset with some positive and some negative
    movie reviews:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.4* 显示了一个小子集，其中包含一些正面和一些负面的电影评论：'
- en: '![Figure 7.4 – Extract of the IMDb dataset, showing positive- and negative-labeled
    reviews](img/B16391_07_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – IMDb 数据集的摘录，显示了标记为正面和负面的评论](img/B16391_07_004.jpg)'
- en: Figure 7.4 – Extract of the IMDb dataset, showing positive- and negative-labeled
    reviews
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – IMDb 数据集的摘录，显示了标记为正面和负面的评论
- en: Let's start with reading and encoding the texts of the movie reviews.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从读取和编码电影评论文本开始。
- en: Preprocessing Movie Reviews
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电影评论的预处理
- en: The embedding layer expects index-based encoded input sequences. That is, each
    review must be encoded as a sequence of indexes, where each index (an integer
    value) represents a word in the dictionary.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层期望基于索引的编码输入序列。也就是说，每条评论必须编码为一个索引序列，其中每个索引（整数值）代表字典中的一个词。
- en: As the number of words available in the IMDb document corpus is very high, we
    decided to reduce them during the text preprocessing phase, by removing stop words
    and reducing all words to their stems. In addition, only the ![](img/Formula_B16391_07_002.png)
    most frequent terms in the training set are encoded with a dedicated index, while
    all others receive just the default index.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 IMDb 文档语料库中的可用词汇量非常大，我们决定在文本预处理阶段对其进行简化，方法是去除停用词并将所有词语还原为词干。此外，仅将训练集中的最频繁术语用专用索引进行编码，而其他所有术语则使用默认索引。
- en: In theory, RNNs can handle sequences of variable length. In practice, though,
    the sequence length for all input samples in one training batch must be the same.
    As the number of words per review might differ, we define a fixed sequence length
    and we zero-pad too-short sequences – that is, we add 0s to complete the sequence
    – and we truncate too-long sequences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，RNN 可以处理可变长度的序列。然而，在实践中，一个训练批次中的所有输入样本的序列长度必须相同。由于每个评论中的词汇数量可能不同，我们定义了固定的序列长度，并对过短的序列进行零填充
    —— 即，我们添加 0 来补充序列 —— 并截断过长的序列。
- en: All these preprocessing steps are applied to the training set and the test set,
    with one difference. In the preprocessing of the training set, the dictionary
    with the ![](img/Formula_B16391_06_021.png) most frequent terms is created. This
    dictionary is then only applied during the preprocessing of the test set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些预处理步骤都应用于训练集和测试集，唯一的区别在于：在训练集的预处理中，会创建一个包含最频繁词汇的字典。这个字典仅在测试集的预处理过程中使用。
- en: 'In summary, we perform the following preprocessing steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们执行以下预处理步骤：
- en: Read and partition the dataset into training and test sets.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取并将数据集划分为训练集和测试集。
- en: Tokenize, clean, and stem the movie reviews in the training set and the test
    set.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和测试集中的电影评论进行分词、清理和词干提取。
- en: Create a dictionary of all the terms. The ![](img/Formula_B16391_03_029.png)
    most frequent terms in the training set are represented by dedicated indexes and
    all other terms by a default index.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建所有术语的字典。训练集中最频繁的术语由专用索引表示，所有其他术语由默认索引表示。
- en: Map the words in the training and test set to the corresponding dictionary indexes.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集和测试集中的词汇映射到相应的字典索引。
- en: Truncate too-long word sequences in the training set and test set.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截断训练集和测试集中过长的词序列。
- en: Zero-pad too-short sequences in the training set and test set.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和测试集中的过短序列进行零填充。
- en: 'The workflow in *Figure 7.5* performs all these steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.5* 中的工作流执行了所有这些步骤：'
- en: '![Figure 7.5 – Preprocessing workflow snippet for the sentiment analysis case
    study](img/B16391_07_005.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 情感分析案例研究的预处理工作流片段](img/B16391_07_005.jpg)'
- en: Figure 7.5 – Preprocessing workflow snippet for the sentiment analysis case
    study
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 情感分析案例研究的预处理工作流片段
- en: The first metanode, **Read and partition data**, reads the table with the movie
    reviews and sentiment information and partitions the dataset into a training set
    and a test set. The **Preprocessing training set** metanode performs the different
    preprocessing steps on the training set and creates and applies the dictionary,
    which is available at the second output port. The last metanode, **Preprocess
    test set**, applies the created dictionary to the test set and performs the different
    preprocessing steps on the test set.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个元节点**读取并划分数据**读取包含电影评论和情感信息的表格，并将数据集划分为训练集和测试集。**预处理训练集**元节点对训练集执行不同的预处理步骤，创建并应用字典，该字典会通过第二个输出端口提供。最后一个元节点**预处理测试集**将创建的字典应用于测试集，并对测试集执行不同的预处理步骤。
- en: Let's see how all these steps are implemented in KNIME Analytics Platform.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些步骤是如何在 KNIME Analytics Platform 中实现的。
- en: Reading and Partitioning the Dataset
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取和划分数据集
- en: The first part, reading and partitioning the dataset, is performed by the **Read
    and partition data** metanode.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，读取和划分数据集，由**读取并划分数据**元节点执行。
- en: '*Figure 7.6* shows you the workflow snippet inside the metanode:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.6* 显示了元节点内部的工作流片段：'
- en: '![Figure 7.6 – Workflow snippet inside the Read and partition metanode](img/B16391_07_006.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 读取并划分元节点内部的工作流片段](img/B16391_07_006.jpg)'
- en: Figure 7.6 – Workflow snippet inside the Read and partition metanode
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 读取并划分元节点内部的工作流片段
- en: The **Table Reader** node reads the table with the sentiment information as
    an integer value and the movie reviews as a string value. Next, the sentiment
    information is transformed into a string with the **Number To String** node. This
    step is necessary to allow stratified sampling in the **Partitioning** node. In
    the last step, the data type of the column sentiment is transformed back into
    an integer using the **String To Number** node so that it can be used as the target
    column during training by the Keras Learner node.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格读取器**节点读取包含情感信息的表格作为整数值，以及作为字符串值的电影评论。接下来，使用**数字转字符串**节点将情感信息转换为字符串。这一步骤是必要的，以便在**分区**节点中进行分层抽样。在最后一步，使用**字符串转数字**节点将情感列的数据类型转换回整数，以便在训练过程中作为目标列由
    Keras Learner 节点使用。'
- en: Now that we have a training set and a test set, let's continue with the preprocessing
    of the training set.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练集和测试集，让我们继续进行训练集的预处理。
- en: Preprocessing the Training Set and Dictionary Creation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练集预处理和字典创建
- en: The preprocessing of the training set and the creation of the dictionary is
    performed in the **Preprocess training set** metanode.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集的预处理和字典的创建在**预处理训练集**元节点中进行。
- en: '*Figure 7.7* shows you the inside of the metanode:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.7* 显示了元节点内部的内容：'
- en: '![Figure 7.7 – Workflow snippet inside the Preprocess training set metanode](img/B16391_07_007.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 预处理训练集元节点中的工作流片段](img/B16391_07_007.jpg)'
- en: Figure 7.7 – Workflow snippet inside the Preprocess training set metanode
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 预处理训练集元节点中的工作流片段
- en: For the preprocessing of the movie reviews, the **KNIME Text Processing** extension
    is used.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电影评论的预处理，使用了**KNIME 文本处理**扩展。
- en: Tip
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The KNIME Text Processing extension includes nodes to read and write documents
    from and to a variety of text formats; to transform words; to clean up sentences
    of spurious characters and meaningless words; to transform a text into a numeric
    table; to calculate all required text statistics; and finally, to explore topics
    and sentiment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 文本处理扩展包含读取和写入多种文本格式文档的节点；转换单词；清理句子中的无关字符和无意义单词；将文本转换为数字表格；计算所有所需的文本统计信息；最后，探索主题和情感。
- en: 'The KNIME Text Processing extension relies on a new data type: **Document object**.
    Raw text becomes a document when additional metadata, such as title, author(s),
    source, and class, are added to it. Text in a document is tokenized following
    one of the many available language-specific tokenization algorithms. **Document
    tokenization** produces a hierarchical structure of the text items: sections,
    paragraphs, sentences, and words. Words are often referred to as tokens or terms.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 文本处理扩展依赖于一种新的数据类型：**文档对象**。原始文本通过添加附加的元数据，如标题、作者、来源和类别，变成了文档。文档中的文本按照多种可用的语言特定分词算法进行分词。**文档分词**生成了文本项的层次结构：章节、段落、句子和单词。单词通常被称为令牌或术语。
- en: To make use of the preprocessing nodes of the KNIME Text Processing extension,
    we need to transform the movie reviews into documents, via the **Strings To Document**
    node. This node collects values from different columns and turns them into a document
    object, after tokenizing the main text.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 KNIME 文本处理扩展的预处理节点，我们需要通过**Strings To Document**节点将电影评论转换为文档。该节点从不同的列收集值，并在对主文本进行分词后将其转换为文档对象。
- en: '*Figure 7.8* shows you the configuration window of the **Strings To Document**
    node:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.8* 显示了 **Strings To Document** 节点的配置窗口：'
- en: '![Figure 7.8 – Configuration window of the Strings To Document node](img/B16391_07_008.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – Strings To Document 节点的配置窗口](img/B16391_07_008.jpg)'
- en: Figure 7.8 – Configuration window of the Strings To Document node
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – Strings To Document 节点的配置窗口
- en: 'The node gives you the opportunity to define the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点为您提供了定义以下内容的机会：
- en: The document text via the **Full text** option.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**全文**选项获取文档文本。
- en: The document title, as a **Column**, **Row ID**, or **Empty string** value.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档标题，作为**列**、**行 ID**或**空字符串**值。
- en: The document source, document category, document authors, and document publication
    date as a fixed string or a column value. If column values are used, remember
    to enable the corresponding flag. Often, the **Document category** field is used
    to store the task class.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为固定字符串或列值的文档来源、文档类别、文档作者和文档发布日期。如果使用列值，请记得启用相应的标志。通常，**文档类别**字段用于存储任务类别。
- en: The document type, as **Transaction**, **Proceeding**, **Book**, or just **UNKNOWN**.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档类型，如**事务**、**会议记录**、**书籍**，或只是**未知**。
- en: The name of the output document column.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出文档列的名称。
- en: The maximum number of parallel processes to execute the word tokenizer.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行词法分析器的最大并行进程数。
- en: The word tokenizer algorithm.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词法分析器算法。
- en: 'Next, the document objects are cleaned through a sequence of text preprocessing
    nodes, contained in the **Text Preprocessing** component of the workflow in *Figure
    7.7*. The inside of the **Text Preprocessing** component is shown in *Figure 7.9*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，文档对象通过一系列文本预处理节点进行清理，这些节点包含在工作流的**文本预处理**组件中，见*图 7.7*。**文本预处理**组件的内部结构如*图
    7.9*所示：
- en: '![Figure 7.9 – Workflow snippet showing the inside of the Preprocessing component
    ](img/B16391_07_009.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 显示预处理组件内部的工作流片段](img/B16391_07_009.jpg)'
- en: Figure 7.9 – Workflow snippet showing the inside of the Preprocessing component
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 显示预处理组件内部的工作流片段
- en: The workflow snippet starts with the **Punctuation Erasure** node, to strip
    all punctuation from the input documents.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流片段从**标点符号删除**节点开始，用于去除输入文档中的所有标点符号。
- en: The **Number Filter** node filters out all numbers, expressed as digits, including
    decimal separators (**,** or **.**) and possible leading signs (**+** or **-**).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**数字过滤器**节点过滤掉所有数字，包含数字形式的数字、十进制分隔符（**,** 或 **.**）和可能的前导符号（**+** 或 **-**）。'
- en: The **N Chars Filter** node filters out all terms with less than ![](img/Formula_B16391_07_005.png)
    – in our case, ![](img/Formula_B16391_07_006.png) – characters, as specified in
    the configuration window of the node.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**字符数过滤器**节点过滤掉所有少于 ![](img/Formula_B16391_07_005.png) – 在我们的例子中，少于 ![](img/Formula_B16391_07_006.png)
    – 字符的术语，正如在节点的配置窗口中指定的那样。'
- en: Filler words, such as *so*, *thus*, and so on, are called **stop words**. They
    carry little information and can be removed with the **Stop Word Filter** node.
    This node filters out all terms that are contained in the selected stop word list.
    A custom stop word list can be passed to the node via the second input port, or
    a default built-in stop word list can be adopted. A number of built-in stop word
    lists are available for various languages.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 填充词，如*so*、*thus* 等，被称为**停用词**。它们承载的信息较少，可以通过**停用词过滤器**节点去除。该节点过滤掉所有在选定停用词列表中的术语。可以通过第二输入端口传递自定义停用词列表，或采用默认的内置停用词列表。为不同语言提供了多个内置停用词列表。
- en: The **Case Converter** node converts all terms into upper or lowercase. In this
    case study, they are converted into lowercase.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**大小写转换器**节点将所有术语转换为大写或小写。在这个案例中，它们被转换为小写。'
- en: Lastly, the **Snowball Stemmer** node reduces words to their stem, removing
    the grammar inflection, using the Snowball stemming library ([http://snowball.tartarus.org/](http://snowball.tartarus.org/)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**Snowball 词干提取器**节点将单词简化为词干，去除语法屈折，使用 Snowball 词干提取库 ([http://snowball.tartarus.org/](http://snowball.tartarus.org/))。
- en: Important note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The goal of stemming is to reduce inflectional forms and derivationally related
    forms to a common base form. For example, *look*, *looking*, *looks*, and *looked*
    are all replaced by their stem, *look*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取的目标是将屈折形式和派生相关形式归约为共同的基础形式。例如，*look*、*looking*、*looks* 和 *looked* 都被替换为它们的词干
    *look*。
- en: Now that we have cleaned up the text of the movie reviews of the training set,
    we can create the dictionary.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理了训练集中的电影评论文本，可以创建词典了。
- en: Creating the Dictionary Based on the Training Set
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于训练集创建词典
- en: 'The dictionary must assign two indexes to each word:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 词典必须为每个单词分配两个索引：
- en: '**Index**: A progressive integer index to each of the ![](img/Formula_B16391_07_007.png)
    most frequent terms in the training set and the same default index to all other
    terms.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：为训练集中最常见的术语分配递增的整数索引，并为所有其他术语分配相同的默认索引。  '
- en: '**Counter**: A progressive eight-digit index to each of the words. This eight-digit
    index is just a temporary index that will help us deal with truncation.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数器**：为每个单词分配一个递增的八位数索引。这个八位数索引只是一个临时索引，帮助我们处理截断问题。'
- en: '*Figure 7.10* shows you a subset of the dictionary we want to create:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.10* 显示了我们想要创建的词典的子集：'
- en: '![Figure 7.10 – A small subset of the dictionary, where each word is represented
    by a progressive integer index and another progressive eight-digit integer index](img/B16391_07_010.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 词典的小子集，每个单词通过递增的整数索引和另一个递增的八位数整数索引来表示](img/B16391_07_010.jpg)'
- en: Figure 7.10 – A small subset of the dictionary, where each word is represented
    by a progressive integer index and another progressive eight-digit integer index
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 字典的小子集，其中每个单词由递增的整数索引和另一个递增的八位数字索引表示
- en: 'Both indexes are created in the **Create Dictionary** component and *Figure
    7.11* shows you the workflow snippet inside the component:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 两个索引都在**创建字典**组件中创建，*图 7.11*向您展示了组件内部的工作流片段：
- en: '![Figure 7.11 – Workflow snippet contained in the Create Dictionary component](img/B16391_07_011.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 创建字典组件中的工作流片段](img/B16391_07_011.jpg)'
- en: Figure 7.11 – Workflow snippet contained in the Create Dictionary component
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 创建字典组件中的工作流片段
- en: 'The **Create Dictionary** component has a configuration window, which you can
    see in *Figure 7.12*. The input option in the configuration window is inherited
    from the **Integer Configuration** node and requests the dictionary size as the
    number of the ![](img/Formula_B16391_03_173.png) most frequent words in the document
    collection. The default is ![](img/Formula_B16391_07_009.png):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建字典**组件有一个配置窗口，您可以在*图 7.12*中看到。配置窗口中的输入选项继承自**整数配置**节点，并要求字典大小为文档集合中出现最频繁的！[](img/Formula_B16391_03_173.png)个单词。默认值为！[](img/Formula_B16391_07_009.png)：'
- en: '![Figure 7.12 – Configuration window of the Create Dictionary component](img/B16391_07_012.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 创建字典组件的配置窗口](img/B16391_07_012.jpg)'
- en: Figure 7.12 – Configuration window of the Create Dictionary component
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 创建字典组件的配置窗口
- en: 'The workflow inside the component first creates a global set of unique terms
    over all the documents by using the **Unique Term Extractor** node:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 组件内的工作流首先通过使用**独特术语提取器**节点在所有文档中创建一个独特术语的全局集合：
- en: '![Figure 7.13 – Configuration window of the Unique Term Extractor node](img/B16391_07_013.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 独特术语提取器节点的配置窗口](img/B16391_07_013.jpg)'
- en: Figure 7.13 – Configuration window of the Unique Term Extractor node
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 独特术语提取器节点的配置窗口
- en: This node allows us to create an index column and a frequency column, as shown
    in the preceding screenshot. The index column contains a progressive integer number
    starting from `1`, where `1` is assigned to the most frequent term.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点允许我们创建索引列和频率列，如上面的截图所示。索引列包含从`1`开始的递增整数，其中`1`分配给最频繁的术语。
- en: 'The node optionally provides the possibility to filter the top *k* most frequent
    terms. For that, three frequency measures are available: the **term frequency**,
    the **document frequency**, and the **inverse document frequency**. For now, we
    want to select all terms and we will work on the dictionary size later.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点可选择性地提供过滤最*常见的 k 个术语*的功能。为此，有三种频率度量可用：**词频**、**文档频率**和**逆文档频率**。现在，我们希望选择所有术语，字典大小稍后再处理。
- en: Important note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Term frequency** (**TF**): The number of occurrences of a term in all documents'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频**（**TF**）：术语在所有文档中出现的次数'
- en: '**Document frequency** (**DF**): The number of documents in which a term occurs'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档频率**（**DF**）：术语出现的文档数量'
- en: '**Inverse document frequency** (**IDF**): Logarithm of the number of documents
    divided by DF'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆文档频率**（**IDF**）：文档总数与文档频率（DF）之比的对数'
- en: The eight-digit index is created via the `1` as the step size. This minimum
    value guarantees the eight-digit format.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 八位数字索引是通过`1`作为步长创建的。这个最小值确保了八位数字格式。
- en: The **Index** and **Counter** columns are then converted from integers into
    strings with the **Number To String** node.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**索引**和**计数器**列将通过**数字转字符串**节点从整数转换为字符串。
- en: 'Next comes the reduction of the dictionary size. The top ![](img/Formula_B16391_03_173.png)
    most frequent terms keep the progressive index assigned by the **Unique Term Extractor**
    node, while all other terms get a default index of ![](img/Formula_B16391_07_011.png).
    Remember that ![](img/Formula_B16391_03_252.png) can be changed via the component''s
    configuration window. For this example, ![](img/Formula_B16391_07_013.png) was
    set to 20,000\. In the lower part of the component sub-workflow, the **Row Splitter**
    node splits the input data table into two sub-tables: the top ![](img/Formula_B16391_07_007.png)
    rows (top output port) and the rest of the rows (lower output port).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是字典大小的缩减。最上方的 ![](img/Formula_B16391_03_173.png) 最常见术语保留由**唯一术语提取器**节点分配的逐步索引，而所有其他术语都获得一个默认索引
    ![](img/Formula_B16391_07_011.png)。请记住，![](img/Formula_B16391_03_252.png) 可以通过组件的配置窗口进行更改。在这个例子中，![](img/Formula_B16391_07_013.png)
    设置为 20,000。在组件子工作流的下部分，**行分割器**节点将输入数据表拆分为两个子表：最上方的 ![](img/Formula_B16391_07_007.png)
    行（顶部输出端口）和其余行（底部输出端口）。
- en: The **Constant Value Column** node then replaces all index values with the default
    index value ![](img/Formula_B16391_07_015.png) in the lower sub-table. Lastly,
    the two sub-tables are concatenated back together.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**常量值列**节点将下方子表中的所有索引值替换为默认索引值 ![](img/Formula_B16391_07_015.png)。最后，两个子表被重新连接在一起。
- en: Now that the dictionary is ready, we can continue with the truncation of the
    movie reviews.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在字典已经准备好，我们可以继续进行电影评论的截断。
- en: Truncating Too-Long Documents
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 截断过长的文档
- en: 'We have stated that we will work with fixed-size documents – that is, with
    a maximum number of words for each document. If a document has more words than
    allowed, it will be truncated. If it has fewer words than allowed, it will be
    zero-padded. Let''s see how the **truncation** procedure works – that is, how
    we remove the last words from a too-long document. This all happens in the **Truncation**
    component. *Figure 7.14* shows you the workflow snippet inside the component:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说明我们将处理固定大小的文档——即每个文档的最大单词数。如果文档的单词数超过允许的数量，它将被截断。如果文档的单词数少于允许的数量，它将被零填充。现在让我们看看**截断**过程是如何工作的——即如何从过长的文档中删除最后的单词。这一切都发生在**截断**组件中。*图
    7.14*展示了组件内部的工作流片段：
- en: '![Figure 7.14 – Workflow snippet inside the Truncation component](img/B16391_07_014.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 截断组件中的工作流片段](img/B16391_07_014.jpg)'
- en: Figure 7.14 – Workflow snippet inside the Truncation component
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 截断组件中的工作流片段
- en: First, we set the maximum number, ![](img/Formula_B16391_07_016.png), of terms
    allowed in a document. Again, this is a parameter that can be changed through
    the component's configuration window, shaped via the **Integer Configuration**
    node. We set the maximum number of terms in a document – that is, the maximum
    document size – as ![](img/Formula_B16391_07_017.png) terms. If a document is
    too long, we should just keep the first ![](img/Formula_B16391_07_018.png) terms
    and throw away the rest.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置文档中允许的最大单词数 ![](img/Formula_B16391_07_016.png)。同样，这个参数可以通过组件的配置窗口进行更改，形状由**整数配置**节点定义。我们设置文档中的最大单词数——即最大文档大小——为
    ![](img/Formula_B16391_07_017.png) 个单词。如果文档太长，我们应该只保留前 ![](img/Formula_B16391_07_018.png)
    个单词，并丢弃其余部分。
- en: It is not easy to count the number of words in a text. Since words have variable
    lengths, we should detect the spaces separating the words within a loop and then
    count the words. Loops, however, often slow down execution. So, an alternative
    trick is to use the eight-digit representation of the words inside the text.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 计算文本中的单词数并不容易。由于单词的长度可变，我们应该在循环中检测分隔单词的空格，然后计算单词数。然而，循环往往会减慢执行速度。因此，一个替代的技巧是使用文本中单词的八位数字表示。
- en: Within the text, each word is substituted by its eight-digit code via the **Dictionary
    Replacer** node. The **Dictionary Replacer** node matches terms in the input documents
    at the top input port with dictionary terms at the lower input port and then replaces
    them with the corresponding value in the dictionary table.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中，每个单词通过**字典替换器**节点用其八位数字代码进行替换。**字典替换器**节点将输入文档顶部输入端口中的术语与下方输入端口中的字典术语进行匹配，然后用字典表中对应的值进行替换。
- en: 'The **Dictionary Replacer** node has two input ports:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**字典替换器**节点有两个输入端口：'
- en: The upper input port for the documents containing the terms to be replaced
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上输入端口用于包含要替换术语的文档
- en: The lower input port with the dictionary table for the matching and replacement
    operation
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下输入端口与用于匹配和替换操作的字典表
- en: Important note
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The dictionary table must consist of at least two string columns. One string
    column contains the terms to replace (keys) and the other string column contains
    the replacement strings (values). In the configuration window, we can set both
    columns from the data table at the lower input port.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字典表必须至少包含两列字符串。一列字符串列包含要替换的术语（键），另一列字符串列包含替换的字符串（值）。在配置窗口中，我们可以从数据表的下部输入端口设置这两列。
- en: At this point, we have text with terms of fixed length (`8 digits + 1 <space>`)
    and not words of variable length. So, limiting a text to ![](img/Formula_B16391_07_019.png)
    words is the same as limiting a text to ![](img/Formula_B16391_07_020.png) characters,
    if ![](img/Formula_B16391_07_021.png), to 720 characters. This operation is much
    easier to carry out without loops or complex node structures, but just with a
    **String Manipulation** node. However, the **String Manipulation** node works
    on string objects and not on documents. To use it, we need to move temporarily
    back to text as strings.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们得到的是具有固定长度（`8 位数字 + 1 <space>`）的文本，而不是可变长度的单词。因此，将文本限制为 ![](img/Formula_B16391_07_019.png)
    个单词就等于将文本限制为 ![](img/Formula_B16391_07_020.png) 个字符，如果 ![](img/Formula_B16391_07_021.png)，则为
    720 个字符。这个操作可以更容易地进行，无需使用循环或复杂的节点结构，只需使用 **字符串操作** 节点即可。然而，**字符串操作** 节点只对字符串对象进行操作，而不是对文档进行操作。为了使用它，我们需要暂时将文本返回为字符串。
- en: The text is extracted from the document as a simple string with the **Document
    Data Extractor** node. This node extracts information, such as, for example, the
    text and title, from a document cell.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 文本通过 **文档数据提取器** 节点从文档中提取为简单的字符串。该节点提取文档单元格中的信息，例如文本和标题。
- en: The **Math Formula (Variable)** node takes the flow variable for the maximum
    document size and calculates the maximum number of characters allowed in a document.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学公式（变量）** 节点获取流变量，以确定文档的最大大小，并计算文档中允许的最大字符数。'
- en: The `0`) until the maximum number of characters allowed, using the `substr()`
    function. This effectively keeps only the top ![](img/Formula_B16391_03_031.png)
    terms and removes all others.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `substr()` 函数，直到达到允许的最大字符数，`0`)。这实际上仅保留前 ![](img/Formula_B16391_03_031.png)
    个单词并移除其他所有单词。
- en: Lastly, the text is transformed back into a document, called **Truncated Document**,
    and all superfluous columns are removed in the **Column Filter** node.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，文本被转换回一个称为 **截断文档** 的文档，所有多余的列将在 **列过滤器** 节点中被移除。
- en: At this point, the eight-digit indexes have exhausted their task and can be
    substituted with the progressive integer index for the encoding. This is done
    in the **Dictionary Replacer** node, once again.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，八位数字索引已完成其任务，可以被逐渐递增的整数索引替代，用于编码。这在 **字典替换器** 节点中再次完成。
- en: With that, we have truncated too-long documents to the maximum number of terms
    allowed. Next, we need to zero-pad too-short documents.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就将过长的文档截断至允许的最大单词数。接下来，我们需要对过短的文档进行零填充。
- en: Zero-Padding Too-Short Documents
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零填充过短的文档
- en: When sequences are too short with respect to a set number of values, **zero-padding**
    is often applied. Zero-padding means that 0s are added to the sequence until the
    set number of values is reached. In our case, if a document has fewer words than
    the set number, we fill the remaining empty spaces with 0s. This happens in the
    **Zero Pad** component.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当序列相对于设置的值数量过短时，通常会应用 **零填充**。零填充意味着向序列中添加 0，直到达到设定的值数量。在我们的例子中，如果文档的单词数少于设定数量，我们将用
    0 填充剩余的空位。这发生在 **Zero Pad** 组件中。
- en: '*Figure 7.15* shows you the workflow snippet inside the Zero Pad component:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.15* 展示了 Zero Pad 组件中的工作流片段：'
- en: '![Figure 7.15 – Workflow snippet inside the Zero Pad component](img/B16391_07_015.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – Zero Pad 组件中的工作流片段](img/B16391_07_015.jpg)'
- en: Figure 7.15 – Workflow snippet inside the Zero Pad component
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – Zero Pad 组件中的工作流片段
- en: Zero-padding is again performed at the string level, and not at the document
    level. After the text has been extracted as a string from the input document using
    the `<space>` and creates one new column for each index.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Zero-padding 再次是在字符串级别进行，而不是在文档级别进行。文本通过 `<space>` 从输入文档中提取为字符串后，并为每个索引创建一个新的列。
- en: Remember that all truncated text now has a maximum length of ![](img/Formula_B16391_03_255.png)
    indexes from the previous step. So, from those texts, the number of newly generated
    columns is surely ![](img/Formula_B16391_03_255.png). For all other texts with
    shorter-term sequences, the **Cell Splitter** node will fill the empty columns
    with missing values. It is enough to turn these missing values into 0s and the
    zero-padding procedure is complete. This replacement of missing values with 0s
    is performed by the **Missing Value** node.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，所有被截断的文本现在的最大长度是来自前一步骤的 ![](img/Formula_B16391_03_255.png) 索引。因此，从这些文本中，新生成的列数肯定是
    ![](img/Formula_B16391_03_255.png)。对于所有其他文本（具有较短术语序列的文本），**单元拆分器**节点将用缺失值填充空列。只需要将这些缺失值替换为
    0，零填充过程就完成了。这种用 0 替换缺失值的操作是由**缺失值**节点完成的。
- en: Lastly, all superfluous columns are removed within the **Column Filter** node.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有多余的列在**列过滤器**节点中被移除。
- en: Now that all term sequences – that is, all text – have the same length, collection
    cells are created with the **Create Collection Cell** node to feed the Keras Learner
    node.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有的术语序列——即所有文本——都有相同的长度，使用**创建集合单元**节点来创建集合单元，以便将其输入到 Keras 学习节点中。
- en: Next, we need to perform the same preprocessing on the test and apply the created
    dictionary.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对测试集进行相同的预处理，并应用已创建的字典。
- en: Preprocessing the Test Set
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试集预处理
- en: 'The preprocessing of the test set is performed in the **Preprocess test set**
    metanode. This metanode has two input ports: the upper port for the dictionary
    created in the **Preprocess training set** metanode and the lower port for the
    test set.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的预处理是在**预处理测试集**元节点中执行的。此元节点有两个输入端口：上端口用于从**预处理训练集**元节点创建的字典，下端口用于测试集。
- en: '*Figure 7.16* shows you the workflow snippet inside the Preprocess test set
    metanode:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.16* 展示了预处理测试集元节点内的工作流片段：'
- en: '![Figure 7.16 – Workflow snippet inside the Preprocess test set metanode](img/B16391_07_016.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 预处理测试集元节点内的工作流片段](img/B16391_07_016.jpg)'
- en: Figure 7.16 – Workflow snippet inside the Preprocess test set metanode
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 预处理测试集元节点内的工作流片段
- en: The lower part of the workflow is similar to the workflow snippet inside the
    **Preprocess training set** metanode, only the part including the creation of
    the dictionary is different. Here, the dictionary for the test set is based on
    the dictionary from the training set. All terms available in the training set
    dictionary receive the corresponding index encoding; all remaining terms receive
    the default index.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流的下半部分与**预处理训练集**元节点内的工作流片段类似，只不过包括字典创建的部分不同。在这里，测试集的字典是基于训练集的字典创建的。所有在训练集字典中可用的术语都将获得相应的索引编码；所有其余的术语将获得默认索引。
- en: Therefore, first a list of all terms in the test set is created using the **Unique
    Term Extractor** node. Next, this list is joined with the list of terms in the
    training set dictionary using a right outer join. A right outer join allows us
    to keep all the rows from the lower input port – that is, all terms in the test
    set – and to add the indexes from the training dictionary, if available. For all
    terms that are not in the training dictionary, the joiner node creates missing
    values in the index columns. These missing values are then replaced with the default
    index value using the **Missing Value** node.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先使用**唯一术语提取器**节点创建测试集中的所有术语的列表。接下来，使用右外连接将该列表与训练集字典中的术语列表连接。右外连接允许我们保留来自下输入端口的所有行——即测试集中的所有术语——并添加训练字典中的索引（如果有的话）。对于所有不在训练字典中的术语，连接节点会在索引列中创建缺失值。这些缺失值随后会通过**缺失值**节点被替换为默认的索引值。
- en: All other steps, such as truncation and zero-padding, are performed in the same
    way as in the preprocessing of the training set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他步骤，如截断和零填充，都与训练集的预处理方式相同。
- en: We have finished the preprocessing phase and we can now continue with the definition
    of the network architecture and its training.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了预处理阶段，现在可以继续定义网络架构并进行训练。
- en: Defining and Training the Network Architecture
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义和训练网络架构
- en: In this section, we will define and train the network architecture for this
    sentiment classification task.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义并训练用于情感分类任务的网络架构。
- en: Network Architecture
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络架构
- en: 'We want to use an LSTM-based RNN, where we train the embedding as well. The
    embedding is trained by an embedding layer. Therefore, we create a neural network
    with four layers:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用基于 LSTM 的 RNN，其中我们也训练嵌入。嵌入由嵌入层进行训练。因此，我们创建一个具有四层的神经网络：
- en: An **input layer** to define the input size
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **输入层** 用于定义输入大小
- en: An **embedding layer** to produce an embedding representation of the term space
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **嵌入层** 用于生成术语空间的嵌入表示
- en: An **LSTM layer** to exploit the sequential property of the text
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **LSTM 层** 用于利用文本的序列特性
- en: A **dense layer** with one unit with the sigmoid activation function, as we
    have a binary classification problem at hand
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **全连接层**，具有一个单元，并使用 sigmoid 激活函数，因为我们面临的是一个二分类问题
- en: The embedding layer expects a sequence of index-based encoded terms as input.
    Therefore, the input layer must accept sequences of ![](img/Formula_B16391_03_031.png)
    integer indexes (in our case, ![](img/Formula_B16391_07_026.png)). This means
    `Shape = 80` and `data type = Int 32` in the configuration window of the **Keras
    Input Layer** node.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层期望以基于索引的编码术语序列作为输入。因此，输入层必须接受 ![](img/Formula_B16391_03_031.png) 整数索引序列（在我们的例子中，![](img/Formula_B16391_07_026.png)）。这意味着在
    **Keras 输入层** 节点的配置窗口中，`Shape = 80` 和 `data type = Int 32`。
- en: 'Next, the **Keras Embedding Layer** node must learn to embed the integer indexes
    into an appropriate high-dimensional vector space. *Figure 7.17* shows its configuration
    window. The input tensor is directly recovered from the output of the previous
    input layer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**Keras 嵌入层**节点必须学习将整数索引嵌入到合适的高维向量空间中。*图 7.17* 显示了它的配置窗口。输入张量直接从前一个输入层的输出中恢复：
- en: '![Figure 7.17 – Configuration window of the Keras Embedding Layer node](img/B16391_07_017.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – Keras 嵌入层节点的配置窗口](img/B16391_07_017.jpg)'
- en: Figure 7.17 – Configuration window of the Keras Embedding Layer node
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – Keras 嵌入层节点的配置窗口
- en: There are two important configuration settings for the `128`. The output tensor
    of the `[sequence length` ![](img/Formula_B16391_03_255.png)`, embedding dimension]`.
    In our case, this is `[80, 128]`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `128`，有两个重要的配置设置。输出张量为 `[序列长度` ![](img/Formula_B16391_03_255.png)`, 嵌入维度]`。在我们的例子中，这就是
    `[80, 128]`。
- en: Next, the `128` units, which means `Units = 128`, `Activation = Tanh`, `Recurrent
    activation = Hard sigmoid`, `Dropout = 0.2`, `Recurrent dropout = 0.2`, and return
    sequences, return state, go backward, and unroll all `unchecked`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`128` 单元，表示 `Units = 128`，`Activation = Tanh`，`Recurrent activation = Hard
    sigmoid`，`Dropout = 0.2`，`Recurrent dropout = 0.2`，并且返回序列，返回状态，反向传播，和展开所有 `unchecked`。
- en: Lastly, a **Keras Dense Layer** node with one unit with the sigmoid activation
    function is used to predict the final binary sentiment classification.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用一个 **Keras 全连接层** 节点，具有一个单元和 sigmoid 激活函数，用于预测最终的二元情感分类。
- en: Now that we have our preprocessed data and the neural architecture, we can start
    training the network.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了预处理的数据和神经网络架构，可以开始训练网络。
- en: Training the Recurrent Network with Embeddings
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用嵌入训练递归网络
- en: The network is trained, as usual, with the **Keras Network Learner** node.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 网络按常规方式训练，通过 **Keras 网络学习器** 节点。
- en: In the first tab, **Input Data**, the **From Collection of Number (integer)**
    conversion is selected, as our input is a collection cell of integer values (the
    indexes), encoding our movie reviews. Next, the collection cell is selected as
    input.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个标签页，**输入数据**，选择了 **从数字集合（整数）** 转换，因为我们的输入是一个整数值集合单元（索引），它对我们的电影评论进行编码。接下来，选择集合单元作为输入。
- en: In the second tab, **Target Data**, the **From Number (integer)** conversion
    type and the column with the sentiment class are selected. In the lower part,
    the binary cross-entropy is selected as the loss function since it is a binary
    classification task.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个标签页，**目标数据**，选择了 **从数字（整数）** 转换类型和带有情感类别的列。在下方选择了二元交叉熵作为损失函数，因为这是一个二分类任务。
- en: In the third tab, `Epochs = 30`, `Training batch size = 100`, shuffle training
    data before each epoch is activated, and `Optimizer = Adam` (with the default
    settings).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个标签页中，`Epochs = 30`，`训练批次大小 = 100`，在每个 epoch 之前对训练数据进行洗牌，`优化器 = Adam`（使用默认设置）。
- en: Now that the network is trained, we can apply it to the test set and evaluate
    how good its performance is at predicting the sentiment behind a review text.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在网络已经训练完成，我们可以将其应用到测试集并评估其在预测评论文本情感方面的表现。
- en: Executing and Evaluating the Network on the Test Set
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上执行并评估网络
- en: To execute the network on the test set, the **Keras Network Executor** node
    is used.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要在测试集上执行该网络，使用**Keras网络执行器**节点。
- en: In the configuration window, we again select **From Collection of Number (integer)**
    as the conversion type and the collection cell as input.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置窗口中，我们再次选择**从数字集合（整数）**作为转换类型，并将集合单元格作为输入。
- en: As output, we are interested in the output of the last dense layer, as this
    gives us the probability for sentiment being equal to `1` (positive). Therefore,
    we click on the **add output** button, select the sigmoid layer, and make sure
    that the **To Number (double)** conversion is used.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出，我们关注最后一个密集层的输出，因为它给出了情感为`1`（正面）的概率。因此，我们点击**添加输出**按钮，选择sigmoid层，并确保使用**转换为数字（双精度）**。
- en: The `1`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`1`。'
- en: 'Next, the **Rule Engine** node translates this probability into a class prediction
    with the following expression:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**规则引擎**节点将此概率转换为类预测，使用以下表达式：
- en: '[PRE0]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `$dense_1/Sigmoid:0_0$` is the name of the output column from the network.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`$dense_1/Sigmoid:0_0$`是网络输出列的名称。
- en: The expression transforms all values above `0.5` into 1s, and into 0s otherwise.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该表达式将所有大于`0.5`的值转换为1，其它则转换为0。
- en: Important note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Remember that the different instruction lines in a **Rule Engine** node are
    executed sequentially. Execution stops when the antecedent in one line is verified.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，**规则引擎**节点中的不同指令行是顺序执行的。当某一行的前提被验证时，执行停止。
- en: 'Lastly, the **Scorer** node evaluates the performance of the model and the
    **Keras Network Writer** node saves the trained network for deployment. *Figure
    7.18* shows the network performance, in the view of the **Scorer** node, achieving
    a respectable 83% of correct sentiment classification on the movie reviews:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**评分器**节点评估模型的性能，**Keras网络写入器**节点将训练好的网络保存以供部署。*图7.18*展示了网络性能，从**评分器**节点的角度来看，在电影评论上的情感分类正确率达到了83%：
- en: '![Figure 7.18 – Performance of the LSTM and embedding-based network on sentiment
    classification](img/B16391_07_018.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图7.18 – LSTM和基于嵌入的网络在情感分类上的表现](img/B16391_07_018.jpg)'
- en: Figure 7.18 – Performance of the LSTM and embedding-based network on sentiment
    classification
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – LSTM和基于嵌入的网络在情感分类上的表现
- en: 'With this, we have finished our first NLP case study. *Figure 7.19* displays
    the complete workflow used to implement the example. You can download the workflow
    from the KNIME Hub at https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们完成了第一个NLP案例研究。*图7.19*展示了实现该示例所使用的完整工作流程。你可以从KNIME Hub下载该工作流程，网址为 https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/：
- en: '![Figure 7.19 – Complete workflow to prepare the text and build, train, and
    evaluate the neural network for sentiment analysis](img/B16391_07_019.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图7.19 – 准备文本并构建、训练和评估情感分析神经网络的完整工作流程](img/B16391_07_019.jpg)'
- en: Figure 7.19 – Complete workflow to prepare the text and build, train, and evaluate
    the neural network for sentiment analysis
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 准备文本并构建、训练和评估情感分析神经网络的完整工作流程
- en: For now, we offer no deployment workflow. In [*Chapter 10*](B16391_10_Final_VK_ePUB.xhtml#_idTextAnchor367),
    *Deploying a Deep Learning Network*, we will come back to this trained network
    to build a deployment workflow.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们没有提供部署工作流程。在[*第10章*](B16391_10_Final_VK_ePUB.xhtml#_idTextAnchor367)中，*部署深度学习网络*，我们将回到这个已训练的网络，构建一个部署工作流程。
- en: 'Let''s now move on to the next NLP application: free text generation with RNNs.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进入下一个NLP应用：使用RNN生成自由文本。
- en: Generating Free Text with RNNs
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN生成自由文本
- en: 'Now that we have seen how RNNs can be used for text classification, we can
    move on to the next case study. Here, we want to train an RNN to generate new
    free text in a certain style, be it Shakespearean English, a rap song, or mimicking
    a Brothers Grimm fairy tale. We will focus on the last application: training a
    network to generate free text in the style of Brothers Grimm fairy tales. However,
    the network and the process can be easily adjusted to produce a new rap song or
    a text in old Shakespearean English.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到RNN可以用于文本分类，我们可以进入下一个案例研究。在这里，我们想要训练一个RNN来生成具有特定风格的新自由文本，无论是莎士比亚式英语、rap歌曲，还是模仿格林兄弟的童话故事。我们将专注于最后一个应用：训练一个网络以生成格林兄弟童话风格的自由文本。然而，该网络和过程可以轻松调整，以生成新的rap歌曲或莎士比亚风格的旧英语文本。
- en: So, how can we train an RNN to generate new text?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何训练一个RNN来生成新的文本呢？
- en: The Dataset
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: First of all, you need a text corpus to train the network to generate new text.
    Any text corpus is good. However, keep in mind that the text you use for training
    will define the style of the text automatically generated. If you train the network
    on Shakespearean theater, you will get new text in old Shakespearean English;
    if you train the network on rap songs, you will get urban-style text, maybe even
    with rhyme; if you train the network on fairy tales, you will get text in the
    fairy tale style.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要一个文本语料库来训练网络生成新的文本。任何文本语料库都可以。然而，请记住，你用于训练的文本将自动决定生成文本的风格。如果你在莎士比亚的戏剧上训练网络，你将得到用古老的莎士比亚英语生成的新文本；如果你在说唱歌曲上训练网络，你将得到城市风格的文本，甚至可能带有韵律；如果你在童话故事上训练网络，你将得到童话风格的文本。
- en: Thus, for a network to generate new fairy tales, it must be trained on existing
    fairy tales. We downloaded the Brothers Grimm corpus from the Gutenberg project,
    from [https://www.gutenberg.org/ebooks/2591](https://www.gutenberg.org/ebooks/2591).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了让网络生成新的童话，它必须在现有的童话故事上进行训练。我们从古腾堡计划下载了格林兄弟语料库，链接地址为[https://www.gutenberg.org/ebooks/2591](https://www.gutenberg.org/ebooks/2591)。
- en: Predicting Words or Characters?
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测单词还是字符？
- en: The second decision to make is whether to train the network at the word or character
    level. Both options have their pros and cons.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个需要做出的决定是是否在单词级别或字符级别上训练网络。两种选择各有利弊。
- en: Training a network at the word level sounds more logical since languages are
    structured by words and not by characters. Input sequences (sequences of words)
    are short but the dictionary size (all words in the domain) is large. On the other
    hand, training the network at a character level relies on much smaller and more
    manageable dictionaries, but might lead to very long input sequences. According
    to Wikipedia, the English language, for example, has around 170,000 different
    words and only 26 different letters. Even if we distinguish between uppercase
    and lowercase, and we add numbers, punctuation signs, and special characters,
    we have a dictionary with less than 100 characters.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在单词级别训练网络听起来更合逻辑，因为语言是由单词而非字符构成的。输入序列（单词序列）较短，但字典的大小（领域中的所有单词）非常大。另一方面，在字符级别训练网络依赖于更小且更易管理的字典，但可能导致非常长的输入序列。根据维基百科的说法，英语语言大约有170,000个不同的单词，而只有26个不同的字母。即使我们区分大小写，并添加数字、标点符号和特殊字符，字典也只有不到100个字符。
- en: We want to train a network to generate text in the Brothers Grimm style. In
    order to do that, we train the network with a few Brothers Grimm tales, which
    already implies a very large number of words in the dictionary. So, to avoid the
    problem of a huge dictionary and the consequent possibly unmanageable network
    size, we opt to train our fairy tale generator at the character level.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望训练一个网络，以生成格林兄弟风格的文本。为了实现这一目标，我们使用了少量的格林兄弟故事来训练网络，这已经意味着字典中的单词数量非常庞大。因此，为了避免庞大字典和随之而来的网络规模管理问题，我们选择在字符级别上训练我们的童话生成器。
- en: Training at the character level means that the network must learn to predict
    the next character after the past ![](img/Formula_B16391_03_029.png) characters
    have passed through the input. The training set, then, must consist of many samples
    of sequences of ![](img/Formula_B16391_05_005.png) characters together with the
    next character to predict (the target value).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在字符级别上训练意味着网络必须学习在过去的！[](img/Formula_B16391_03_029.png)字符经过输入后，预测下一个字符。那么，训练集必须由许多包含！[](img/Formula_B16391_05_005.png)字符序列及其对应的下一个预测字符（目标值）的样本组成。
- en: During deployment, a start sequence of ![](img/Formula_B16391_05_005.png) characters
    must trigger the network to generate the new text. Indeed, this first sequence
    predicts the next character; then in the next step, the ![](img/Formula_B16391_07_031.png)
    most recent initial characters and the predicted character will make the new input
    sequence to predict the next character, and so on.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署过程中，一段启动序列的！[](img/Formula_B16391_05_005.png)字符必须触发网络生成新的文本。实际上，这个初始序列预测了下一个字符；然后在下一步中，最近的初始字符和预测字符将组成新的输入序列来预测下一个字符，以此类推。
- en: In the next section, we will explain how to clean, transform, and encode the
    text data from the Grimms' fairy tales to feed the network.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释如何清理、转换和编码格林童话的文本数据，以供网络使用。
- en: Preprocessing and Encoding
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理和编码
- en: We populate the training set using the sliding window approach – that is, with
    partially overlapping sequences. To make this clearer, let's include the sentence
    `Once upon a time` in the training set using a window length of ![](img/Formula_B16391_07_032.png)
    and a sliding step of `1`. The five characters `Once<space>` should predict `u`;
    then we slide the window one step to the right, and `nce<space>u` should predict
    `p`. Again, we slide the window one character to the right and `ce<space>up` should
    predict `o`, and so on.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用滑动窗口方法填充训练集——即，使用部分重叠的序列。为了更清楚地说明这一点，让我们使用窗口长度为![](img/Formula_B16391_07_032.png)并滑动步长为`1`的窗口，将句子`从前有一个`包含在训练集中。五个字符`Once<space>`应该预测`u`；然后，我们将窗口向右滑动一步，`nce<space>u`应该预测`p`。再次向右滑动窗口一个字符，`ce<space>up`应该预测`o`，依此类推。
- en: 'On the left of *Figure 7.20*, you can see the created input sequences and the
    target values:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.20*的左侧，你可以看到创建的输入序列和目标值：
- en: '![Figure 7.20 – Example of overlapping sequences used for training](img/B16391_07_020.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.20 – 用于训练的重叠序列示例](img/B16391_07_020.jpg)'
- en: Figure 7.20 – Example of overlapping sequences used for training
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 用于训练的重叠序列示例
- en: Next, we need to encode the character sequences. In order to avoid introducing
    an artificial distance among characters, we opted for one-hot vector encoding.
    We will perform the one-hot encoding in two steps. First, we perform an index-based
    encoding; then we convert it into one-hot encoding in the **Keras Network Learner**
    node via the **From Collection of Number (integer)** conversion option to **One-Hot
    Tensor**. The resulting overlapping index-encoded sequences for the training set
    are shown on the right of *Figure 7.20*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对字符序列进行编码。为了避免引入字符之间的人工距离，我们选择了独热向量编码。我们将通过两个步骤进行独热编码。首先，我们执行基于索引的编码；然后，通过**Keras
    网络学习器**节点，使用**从数字集合（整数）**转换选项为**独热张量**，将其转换为独热编码。训练集的重叠索引编码序列显示在*图 7.20*的右侧。
- en: 'The workflow snippet in the next figure reads and transforms the fairy tales
    into overlapping index-based encoded character sequences and their associated
    target character. Both the input sequence and target character are stored in a
    collection-type column:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图中的工作流片段读取并转换童话故事，生成重叠的基于索引编码的字符序列及其相关目标字符。输入序列和目标字符都存储在集合类型的列中：
- en: '![Figure 7.21 – Preprocessing workflow snippet reading and transforming text
    from'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.21 – 预处理工作流片段，读取并转换文本](img/B16391_07_021.jpg)'
- en: Brothers Grimm fairy tales](img/B16391_07_021.jpg)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 《格林童话》](img/B16391_07_021.jpg)
- en: Figure 7.21 – Preprocessing workflow snippet reading and transforming text from
    Brothers Grimm fairy tales
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 – 预处理工作流片段，读取并转换《格林童话》中的文本
- en: 'The workflow performs the following steps:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流执行以下步骤：
- en: Reads all the fairy tales from the corpus and extracts five fairy tales for
    training and `Snow white and Rose red` as the seed for deployment
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语料库中读取所有童话故事，提取五个童话故事进行训练，并以`白雪公主与玫瑰红`作为部署的种子
- en: Reshapes the text, placing one character per row in a single column
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新调整文本格式，将每个字符放置在单独一行的单列中
- en: Creates and applies the index-based dictionary, consisting, in this case, of
    the character set, including punctuation and special signs
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并应用基于索引的字典，在本例中，字典由字符集组成，包括标点符号和特殊符号
- en: Using the **Lag Column** node, creates the overlapping sequences and then re-sorts
    them from the oldest to the newest character in the sequence
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**滞后列**节点创建重叠序列，然后按从最旧到最新的顺序对其进行重新排序
- en: Encapsulates the input sequence and target character into collection-type columns
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入序列和目标字符封装成集合类型的列
- en: Let's have a look at these steps in detail.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解这些步骤。
- en: Reading and Extracting Fairy Tales
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取和提取童话故事
- en: The workflow snippet, in the **Read and Extract Fairy Tales** metanode, first
    reads the fairy tales using a **File Reader** node. The table has one column,
    where the content of each row corresponds to one line of a fairy tale.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在**读取和提取童话故事**的元节点中，工作流片段首先通过**文件读取器**节点读取童话故事。该表只有一列，其中每一行的内容对应童话故事中的一行。
- en: Then, a **Row Filter** node removes the unnecessary meta-information at the
    top and the bottom of the file, such as the author, title, table of contents,
    and license agreement. We will not use any of this meta-information during training
    or deployment.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**行过滤器**节点去除文件顶部和底部的不必要的元信息，如作者、标题、目录和许可协议。在训练或部署过程中，我们不会使用这些元信息。
- en: The `Snow white and Rose red` and at the top output port, all the other fairy
    tales. We'll save `Snow white and Rose red` for deployment.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`白雪公主与玫瑰红`和顶部输出端口的其他所有童话故事。我们将保存`白雪公主与玫瑰红`以供部署。'
- en: Next, a **Row Filter** node is used to extract the first five fairy tales, which
    are used for training.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用**行过滤器**节点提取前五个童话故事，这些将用于训练。
- en: The next step is the reshaping of the text into a sequence of characters with
    one single column.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将文本重塑为单一列的字符序列。
- en: Reshaping the Text
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重塑文本
- en: 'Before we can create the overlapping sequences of characters to feed the network,
    we need to transform all the fairy tales text into a long sequence (column) of
    single characters: one character in each row. This step is called **reshaping**
    and it is implemented in the **Reshape Text** metanode. *Figure 7.22* shows its
    contents:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够创建重叠的字符序列以输入网络之前，我们需要将所有童话故事文本转化为一个由单个字符组成的长序列（列）：每行一个字符。这个步骤叫做**重塑**，它是在**重塑文本**元节点中实现的。*图
    7.22* 显示了它的内容：
- en: '![Figure 7.22 – Workflow snippet inside the Reshape Text metanode](img/B16391_07_022.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.22 – 重塑文本元节点中的工作流片段](img/B16391_07_022.jpg)'
- en: Figure 7.22 – Workflow snippet inside the Reshape Text metanode
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22 – 重塑文本元节点中的工作流片段
- en: 'It starts with two `<space>` character, by using the `regexReplace()` function.
    `regexReplace()` takes advantage of regular expressions, such as `"[^\\s]"` to
    match any character in the input string and `"$0 "` for the matched character
    plus `<space>`. The final syntax for the `regexReplace()` function, used within
    the `$Col0$`, is then the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 它从两个`<space>`字符开始，使用`regexReplace()`函数。`regexReplace()`利用正则表达式，例如`"[^\\s]"`来匹配输入字符串中的任何字符，`"$0
    "`为匹配的字符加上`<space>`。在`$Col0$`中使用的`regexReplace()`函数的最终语法如下：
- en: '[PRE1]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, the `<space>` character, producing many columns with one character per
    cell.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`<space>`字符用于生成多个列，每个单元格一个字符。
- en: Notice that the last character in the paragraph (the newline) has not received
    the `<space>` character afterward. To solve this problem, a constant column with
    a `<space>` character is added using the **Constant Value Column** node.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，段落中的最后一个字符（换行符）后面没有添加`<space>`字符。为了解决这个问题，使用**常量值列**节点添加了一个包含`<space>`字符的常量列。
- en: 'The **Unpivoting** node reshapes the data table from many columns into one
    column only with a sequence of single characters. Let''s spend a bit of time on
    the **Unpivoting** node and its unsuspected tricks for reshaping data tables.
    The **Unpivoting** node performs a disaggregation of the input data table. *Figure
    7.23* shows you an example. It distinguishes between value columns and retaining
    columns. The selected value columns are then rotated to become rows and attached
    to the corresponding values in the retaining columns. Since the rotation of the
    value columns might result in more than one row, a duplication of the rows with
    the retaining column values might be necessary:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**未旋转**节点将数据表从多列重塑为仅包含单字符序列的一列。让我们花些时间来了解**未旋转**节点及其意想不到的技巧，用于重塑数据表。**未旋转**节点执行输入数据表的去聚合操作。*图
    7.23*为你展示了一个例子。它区分了值列和保持列。选中的值列随后会被旋转成行，并与保持列中的相应值相附加。由于值列的旋转可能会导致多行，因此可能需要重复保持列值的行：'
- en: '![Figure 7.23 – Example for the unpivoting operation, where Product 1 and Product
    2 are the selected Value columns, and ID and City are the selected retaining columns](img/B16391_07_023.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.23 – 展示了未旋转操作的示例，其中产品1和产品2是选择的值列，ID和城市是选择的保持列](img/B16391_07_023.jpg)'
- en: Figure 7.23 – Example for the unpivoting operation, where Product 1 and Product
    2 are the selected Value columns, and ID and City are the selected retaining columns
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.23 – 展示了未旋转操作的示例，其中产品1和产品2是选择的值列，ID和城市是选择的保持列
- en: For the reshaping of the text, we set all columns as value columns and none
    as retaining columns. The result is the representation of the fairy tale as a
    long sequence of characters within one column.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本进行重塑时，我们将所有列设置为值列，而没有设置任何列为保持列。最终结果是将童话故事表示为一列长字符序列。
- en: 'At last, some cleaning up: all rows with missing values are removed with the
    **Row Filter** node.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后进行一些清理：使用**行过滤器**节点删除所有缺失值的行。
- en: Creating and Applying the Dictionary
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建并应用字典
- en: We now need to create the dictionary and the index-based mapping for the index-based
    encoding. Since we work at the character level, the dictionary here is nothing
    more than the character set – that is, the list of unique characters in the fairy
    tales corpus. To get this list, we remove all duplicate characters from the reshaped
    text using the **Remove Duplicate Filter** node.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要创建字典和基于索引的编码映射。由于我们在字符级别进行操作，因此这里的字典实际上就是字符集——也就是童话语料库中的唯一字符列表。为了获得这个列表，我们使用**删除重复过滤器**节点从重塑后的文本中删除所有重复字符。
- en: Tip
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The **Remove Duplicate Filter** node is a powerful node when it comes to detecting
    and handling duplicate records in the dataset.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**删除重复过滤器**节点在检测和处理数据集中的重复记录时非常强大。'
- en: Next, we assign an index to each row – that is, to each unique character – with
    the `0` for `1` for **Scale Unit**.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为每一行分配一个索引——即为每个唯一字符分配一个索引——使用`0`表示`1`表示**缩放单元**。
- en: Now that we have the dictionary ready, we apply it with the **Cell Replacer**
    node, already introduced in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Neural Network*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好字典，接下来我们通过**单元替换器**节点应用它，之前在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)中已经介绍过，*构建和训练前馈神经网络*。
- en: Creating and Resorting the Overlapping Sequences
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建并重新排序重叠的序列
- en: To create the overlapping sequences of characters, we use the `100`, `1`, and
    incomplete rows at the beginning and end of the output table are skipped.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建重叠的字符序列，我们使用`100`、`1`，并且在输出表格的开始和结束处跳过不完整的行。
- en: According to the way the `col-100`) is in the farthest column to the right;
    the current character to predict (`col`) is in the farthest column to the left.
    Basically, the time of the sequence is sorted backward with respect to what the
    network is expecting.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`col-100`的方式，它位于最右边的列；当前需要预测的字符（`col`）则位于最左边的列。基本上，序列的时间顺序是相对于网络期望的内容进行倒序排序的。
- en: 'The following figure shows you an example:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个示例：
- en: '![Figure 7.24 – Resulting output of the Lag Column node, where the time is
    sorted in ascending order from right to left](img/B16391_07_024.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.24 – 滞后列节点的结果输出，时间按从右到左的升序排序](img/B16391_07_024.jpg)'
- en: Figure 7.24 – Resulting output of the Lag Column node, where the time is sorted
    in ascending order from right to left
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24 – 滞后列节点的结果输出，时间按从右到左的升序排序
- en: We need to reorder the columns to follow an ascending order from left to right,
    in order to have the oldest character on the left and the most recent character
    on the right. This re-sorting is performed by the **Resort Columns** metanode.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重新排序列，使其按从左到右的升序排列，以便最旧的字符位于左侧，最新的字符位于右侧。这一重新排序操作由**重新排序列**元节点执行。
- en: '*Figure 7.25* shows you the inside of the metanode:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.25*展示了元节点内部的内容：'
- en: '![Figure 7.25 – Workflow snippet contained in the Resort Columns metanode](img/B16391_07_025.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.25 – 包含在“重新排序列”元节点中的工作流片段](img/B16391_07_025.jpg)'
- en: Figure 7.25 – Workflow snippet contained in the Resort Columns metanode
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25 – 包含在“重新排序列”元节点中的工作流片段
- en: Here, the **Reference Column Resorter** node changes the order of the data columns
    in the table at the top input port according to the order established in the data
    table at the lower input port. The reference data table at the lower input port
    must contain a string-type column with the column headers from the first input
    table in a particular order. The columns in the first data table are then sorted
    according to the row order of the column names in the second data table.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**参考列排序器**节点根据下输入端口数据表中所建立的顺序更改数据表的列顺序。下输入端口中的参考数据表必须包含一个字符串类型的列，列标题的顺序应与第一个输入表中的列标题顺序一致。然后，第一数据表中的列将根据第二数据表中列名的行顺序进行排序。
- en: To create the table with sorted column headers, we extract the column headers
    with the **Extract Column Header** node. The **Extract Column Header** node separates
    the column headers from the table content and outputs the column headers at the
    top output port and the content at the lower output port.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建包含排序列标题的表格，我们使用**提取列标题**节点提取列标题。**提取列标题**节点将列标题从表格内容中分离，并在上输出端口输出列标题，在下输出端口输出内容。
- en: Then, the row of column headers is transposed into a column with the **Transpose**
    node.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过**转置**节点将列头的行转置为一列。
- en: Finally, we assign an increasing integer number to each column header via the
    **Counter Generation** node and we sort them by counter value in descending order
    using the **Sorter** node.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过**计数器生成**节点为每个列头分配一个递增的整数编号，并通过**排序器**节点按计数器值降序排序它们。
- en: Now that we have the column headers from the first table sorted correctly in
    time, we can input it at the lower port of the **Reference Column Resorter** node.
    The result is a data table where each row is a sequence of ![](img/Formula_B16391_07_034.png)
    characters, time is sorted from left to right, and subsequent rows contain overlapping
    character sequences. At this point, we can create the collection cells for the
    input and target data of the network.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将来自第一个表的列头按时间顺序正确排序，可以将其输入到**参考列重新排序器**节点的下端口。结果是一个数据表，其中每一行都是一个由 ![](img/Formula_B16391_07_034.png)
    字符组成的序列，时间从左到右排序，后续的行包含重叠的字符序列。在这一点上，我们可以为网络的输入和目标数据创建集合单元。
- en: Important note
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Even though the target data consists of only one single value, we still need
    to transform it into a collection cell so that the index can be transformed into
    a one-hot vector by the **Keras Network Learner** node.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 即使目标数据只有一个单一的值，我们仍然需要将其转换为集合单元，这样索引就可以通过**Keras 网络学习器**节点转换为一个独热向量。
- en: 'Let''s move on to the next step: defining and training the network architecture.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入下一步：定义和训练网络架构。
- en: Defining and Training the Network Architecture
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义和训练网络架构。
- en: Let's now design and train an appropriate neural network architecture to deal
    with time series, character encoding, and overfitting, and to predict the next
    character in the sequence.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来设计并训练一个适合处理时间序列、字符编码和过拟合的神经网络架构，以预测序列中的下一个字符。
- en: Defining the Network Architecture
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义网络架构。
- en: 'For this case study, we decided to use a neural network with four layers:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们决定使用一个包含四层的神经网络：
- en: A **Keras input layer**, to define the input shape
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras 输入层**，用于定义输入形状。
- en: A **Keras LSTM layer**, to deal with time series
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras LSTM层**，用于处理时间序列。
- en: A **Keras dropout layer**, to prevent overfitting
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras Dropout层**，用于防止过拟合。
- en: A **Keras dense layer**, to output the probability of the next character
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras 全连接层**，用于输出下一个字符的概率。
- en: As usual, we define the input shape of the neural network using a `?, 65`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们使用`?, 65`来定义神经网络的输入形状。
- en: As we don't need the intermediate hidden states, we leave most of the settings
    as default in the `512`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不需要中间的隐藏状态，我们将大部分设置保持为`512`的默认值。
- en: Free text generation can be seen as a multi-class classification application,
    where the characters are the classes. Therefore, the **Keras Dense Layer** node
    at the output of the network is set to have 65 units (one for each character in
    the character set) with the softmax activation function, to score the probability
    of each character to be the next character.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 自由文本生成可以视为一个多类分类应用，其中字符是类别。因此，网络输出的**Keras 全连接层**节点被设置为65个单元（每个字符集中的一个字符），并使用softmax激活函数来评分每个字符作为下一个字符的概率。
- en: Let's proceed with training this network on the encoded overlapping sequences.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在编码的重叠序列上训练这个网络。
- en: Training the Network
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络。
- en: Again, to train the network, we use the by-now-familiar **Keras Network Learner**
    node.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了训练网络，我们使用已经熟悉的**Keras 网络学习器**节点。
- en: In the first configuration tab, **Input Data**, we select **From Collection
    of Number (integer) to One-Hot-Tensor** to handle encoding conversion and the
    collection column with the character sequence as input.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个配置标签中，**输入数据**，我们选择**从数字集合（整数）到独热张量**来处理编码转换，并将字符序列的集合列作为输入。
- en: In the second configuration tab, **Target Data**, we select **From Collection
    of Number (integer) to One-Hot-Tensor** again on the collection column containing
    the target value. As this is a multi-class classification problem, we set the
    loss function to **Categorical Cross Entropy**.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个配置标签中，**目标数据**，我们再次在包含目标值的集合列上选择**从数字集合（整数）到独热张量**。由于这是一个多类分类问题，我们将损失函数设置为**分类交叉熵**。
- en: In the third configuration tab, `50 epochs`, training batch size `256`, shuffling
    option `on`, and optimizer as `Adam` with default settings for the learning rate.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个配置标签中，`50 epochs`，训练批次大小为`256`，打乱选项为`on`，优化器选择`Adam`，学习率使用默认设置。
- en: The network is finally saved in **Keras format** with the **Keras Network Writer**
    node. In addition, the network is converted into a TensorFlow network with the
    **Keras to TensorFlow Network Converter** node and saved with the **TensorFlow
    Network Writer** node. The TensorFlow network is used in deployment to avoid a
    time-consuming Python startup, required by the Keras network.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，网络使用**Keras 格式**通过**Keras 网络写入器**节点保存。此外，网络通过**Keras 到 TensorFlow 网络转换器**节点转换为
    TensorFlow 网络，并通过**TensorFlow 网络写入器**节点保存。TensorFlow 网络在部署时使用，以避免 Keras 网络所需的耗时
    Python 启动。
- en: '*Figure 7.26* shows the full workflow implementing all the described steps
    to train a neural network to generate fairy tales. This workflow and the used
    dataset are available on KNIME Hub at https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.26* 显示了完整的工作流，实施了所有描述的步骤来训练神经网络生成童话故事。此工作流和所使用的数据集可在 KNIME Hub 上找到，网址为
    https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/：'
- en: '![Figure 7.26 – Workflow to train a neural network to generate fairy tales](img/B16391_07_026.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.26 – 训练神经网络生成童话故事的工作流](img/B16391_07_026.jpg)'
- en: Figure 7.26 – Workflow to train a neural network to generate fairy tales
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.26 – 训练神经网络生成童话故事的工作流
- en: Now that we have trained and saved the network, let's move on to deployment
    to generate a new fairy tale's text.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练并保存了网络，接下来我们将进行部署，生成新的童话故事文本。
- en: Building a Deployment Workflow
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建部署工作流
- en: 'To trigger the generation of new text during deployment, we start with an input
    sequence of the same length as each of the training sequences (![](img/Formula_B16391_07_036.png)).
    We feed the network with that sequence to predict the next character; then, we
    delete the oldest character in the sequence, add the predicted one, and apply
    the network again to our new input sequence, and so on. This is exactly the same
    procedure that we used in the case study for demand prediction. So, we will implement
    it here again with a recursive loop (*Figure 7.27*):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在部署期间触发新文本的生成，我们从与每个训练序列相同长度的输入序列开始（![](img/Formula_B16391_07_036.png)）。我们将该序列输入网络，以预测下一个字符；然后，我们删除序列中的最旧字符，添加预测的字符，再次将网络应用于新的输入序列，依此类推。这与我们在需求预测案例研究中使用的程序完全相同。因此，我们将在这里再次实现它，使用递归循环（*图
    7.27*）：
- en: '![Figure 7.27 – Deployment workflow to generate new free text](img/B16391_07_027.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.27 – 部署工作流生成新的自由文本](img/B16391_07_027.jpg)'
- en: Figure 7.27 – Deployment workflow to generate new free text
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27 – 部署工作流生成新的自由文本
- en: 'The trigger sequence was taken from the **Snow white and Rose red** fairy tale.
    The text for the trigger sequence was preprocessed, sequenced, and encoded as
    in the workflow used to train the network. This is done in the **Read and Pre-Process**
    metanode, shown in *Figure 7.28*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 触发序列取自**白雪公主与玫瑰红**的童话故事。触发序列的文本经过预处理、分序列和编码，方式与训练网络时使用的工作流相同。这是在**读取和预处理**元节点中完成的，如*图
    7.28*所示：
- en: '![Figure 7.28 – Workflow content in the Read and Pre-Process metanode to read
    and preprocess the trigger sequence](img/B16391_07_028.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.28 – 读取和预处理元节点中的工作流内容，用于读取和预处理触发序列](img/B16391_07_028.jpg)'
- en: Figure 7.28 – Workflow content in the Read and Pre-Process metanode to read
    and preprocess the trigger sequence
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.28 – 读取和预处理元节点中的工作流内容，用于读取和预处理触发序列
- en: The workflow reads the **Snow white and Rose red** fairy tale as well as the
    dictionary from the files created in the training workflow. Then, the same preprocessing
    steps as in the training workflow are applied.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作流读取**白雪公主与玫瑰红**的童话故事以及训练工作流中创建的字典文件。然后，应用与训练工作流相同的预处理步骤。
- en: After that, we read the trained TensorFlow network and apply it to the trigger
    sequence with the **TensorFlow Network Executor** node.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们读取训练好的 TensorFlow 网络，并将其应用于触发序列，使用**TensorFlow 网络执行器**节点。
- en: 'The output of the network is the probability of each character to be the next.
    We can pick the predicted character following two possible strategies:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出是每个字符成为下一个字符的概率。我们可以采用两种可能的策略来选择预测的字符：
- en: The character with the highest probability is assigned to be the next character,
    known as the greedy strategy.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率最高的字符被分配为下一个字符，这就是贪心策略。
- en: The next character is picked randomly according to the probability distribution.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个字符是根据概率分布随机选择的。
- en: We have implemented both strategies in the **Extract Index** metanode in two
    different deployment workflows.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 **提取索引** 元节点中实现了这两种策略，并在两个不同的部署工作流中使用。
- en: '*Figure 7.29* shows the content of the **Extract Index** metanode when implementing
    the first strategy:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.29* 显示了实现第一种策略时 **提取索引** 元节点的内容：'
- en: '![Figure 7.29 – Workflow snippet to extract the character with the highest
    probability](img/B16391_07_029.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.29 – 提取具有最高概率的字符的工作流片段](img/B16391_07_029.jpg)'
- en: Figure 7.29 – Workflow snippet to extract the character with the highest probability
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29 – 提取具有最高概率的字符的工作流片段
- en: This metanode takes as input the output probabilities from the executed network
    and extracts the character with the highest probability. The key node here is
    the **Many to One** node, which extracts the cell with the highest score (probability)
    from the network output.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元节点以已执行网络的输出概率为输入，提取具有最高概率的字符。这里的关键节点是 **多对一** 节点，它从网络输出中提取得分（概率）最高的单元格。
- en: '*Figure 7.30* shows the content of the **Extract Index** metanode when implementing
    the second strategy:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.30* 显示了实现第二种策略时 **提取索引** 元节点的内容：'
- en: '![Figure 7.30 – Workflow snippet to pick the next character based on a probability
    distribution](img/B16391_07_030.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.30 – 基于概率分布选择下一个字符的工作流片段](img/B16391_07_030.jpg)'
- en: Figure 7.30 – Workflow snippet to pick the next character based on a probability
    distribution
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.30 – 基于概率分布选择下一个字符的工作流片段
- en: This workflow snippet expects as input the probability distribution for the
    characters and picks one according to it. The key node here is the **Random Label
    Assigner (Data)** node, which assigns a value based on the input probability distribution.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流片段期望输入字符的概率分布，并根据它选择一个字符。这里的关键节点是 **随机标签分配器（数据）** 节点，它根据输入的概率分布分配一个值。
- en: 'The **Random Label Assigner (Data)** node assigns one index to each data row
    at the lower input port based on the probability distribution at the upper input
    port. The data table at the upper input port must have two columns: one column
    with the class values – in our case, the index-encoded characters in string format
    – and one column with the corresponding probabilities. Therefore, the first part
    of the workflow snippet in *Figure 7.30* prepares the data table for the top input
    port of the **Random Label Assigner (Data)** node, from the network output, using
    the **Transpose** node, the **Counter Generation** node, and the **Number To String**
    node, while the **Table Creator** node creates a new table with only one row using
    the **Table Creator** node. This means the **Random Label Assigner (Data)** node
    then picks one index, based on the probability distribution defined by the table
    at the first input port.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机标签分配器（数据）** 节点根据上游输入端口的概率分布为下游输入端口的每一行数据分配一个索引。上游输入端口的数据表必须包含两列：一列是类别值
    —— 在我们的例子中，是以字符串格式编码的字符索引 —— 另一列是相应的概率。因此，*图 7.30* 中的工作流片段第一部分利用 **转置** 节点、**计数器生成**
    节点和 **数字转字符串** 节点，准备好数据表供 **随机标签分配器（数据）** 节点的顶部输入端口使用，而 **表创建器** 节点则创建一个只有一行的新表。也就是说，**随机标签分配器（数据）**
    节点根据第一输入端口的表中定义的概率分布，选择一个索引。'
- en: Tip
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The idea of the recursive loop and its implementation are explained in detail
    in [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent Neural
    Networks for Demand Prediction*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 递归循环的概念及其实现详细解释请参见 [*第 6 章*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181)，*需求预测的递归神经网络*。
- en: 'You can download the deployment workflow, implementing both options, from the
    KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 KNIME Hub 下载实现这两种选项的部署工作流：https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.
- en: The New Fairy Tale
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的童话故事
- en: At last, I am sure you want to see the kind of free text that the network was
    able to produce. The following is an example of free generated text, using the
    first strategy.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我相信你一定想看看网络能够生成的自由文本。以下是使用第一种策略生成的自由文本示例。
- en: The trigger sequence of 100 characters (not italics) comes from the first sentence
    of the fairy tale, *Snow white and Rose red*. The remaining text has been automatically
    generated by the network.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 触发序列中的 100 个字符（非斜体）来自童话故事的第一句话，*白雪公主与玫瑰红*。其余的文本由网络自动生成。
- en: '*SNOW-WHITE AND ROSE-RED There was once a poor widow who lived in a lonely
    cottage*. In front of the cas, and a hunbred of wine behind the door of the; and
    he said the ansmer: ''What want yeurnKyow yours went for bridd, like is good any,
    or cries, and we will say I only gave the witeved to the brood of the country
    to go away with it.'' But when the father said, ''The cat soon crick.'' The youth,
    the old …'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '*雪白与玫红 从前有一位贫穷的寡妇，她住在一个孤独的小屋里*。小屋前有一只猫，门后有一百瓶酒；他回答道：“你想要的是什么？你说的那东西像是好事，还是哭了？我们会说我只给了寡妇那群人的大地去带走它。”但当父亲说：“猫很快就会死。”年青人，老人……'
- en: The network has successfully learned the structure of the English language.
    Although the text is not perfect, you can see sensible character combinations,
    full words, some correct usage of quotation marks, and other similarly interesting
    features that the network has assimilated from the training text.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 网络成功地学会了英语语言的结构。尽管文本不完美，但你可以看到合理的字符组合、完整的单词、一些正确使用的引号以及其他类似的有趣特征，这些都是网络从训练文本中吸收的。
- en: Generating Product Names with RNNs
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN生成产品名称
- en: This last NLP case study is similar to the previous one. There, we wanted the
    network to create new free text based on a start sequence; here, we want the network
    to create new free words based on a start token. There, we wanted the network
    to create new sequences of words; here, we want the network to create new sequences
    of characters. Indeed, the goal of this product name generation case study is
    to create new names – that is, new words. While there'll be some differences,
    the approaches will be similar.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的自然语言处理案例研究与前一个类似。前者我们希望网络根据一个起始序列生成新的自由文本；而在这里，我们希望网络根据一个起始符号生成新的自由单词。在前者中，我们希望网络生成新的单词序列；而在这里，我们希望网络生成新的字符序列。实际上，这个产品名称生成案例研究的目标是创造新的名称——也就是新的单词。尽管会有一些差异，但方法是相似的。
- en: In this section, we will explore the details of this new approach.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨这种新方法的细节。
- en: The Problem of Product Name Generation
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 产品名称生成问题
- en: Normally, we don't associate artificial intelligence with creativity, as it
    is usually used to predict the outcome based on previously seen examples. The
    challenge for this case study is to use artificial intelligence to create something
    new, which is thought to be in the domain of creative minds.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不会将人工智能与创意联系在一起，因为它通常用于根据之前见过的例子预测结果。本案例的挑战是使用人工智能创造出一些新的东西，而这通常被认为是创意人才的领域。
- en: 'Let''s take a classic creative marketing example: product naming. Before a
    new product can be launched to the market, it actually needs a name. To find the
    name, the most creative minds of the company come together to generate a number
    of proposals for product names, taking different requirements into account. For
    example, the product name should sound familiar to the customers and yet be new
    and fresh too. Of all those candidates, ultimately only one will survive and be
    adopted as the name for the new product. Not an easy task!'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个经典的创意营销例子：产品命名。在新产品推出市场之前，它实际上需要一个名字。为了找到这个名字，公司的创意团队会集思广益，生成多个产品名称提案，并考虑不同的需求。例如，产品名称应该让顾客感到熟悉，同时又要新颖、富有创意。在这些候选名称中，最终只有一个能够脱颖而出，成为新产品的名称。这可不是一项容易的任务！
- en: 'Now, let''s take one of the most creative industries: fashion. A company specializing
    in outdoor wear has a new line of clothes ready for the market. The task is to
    generate a sufficiently large number of name candidates for the new line of clothing.
    Names of mountains were proposed, as many other outdoor fashion labels have. Names
    of mountains evoke the feeling of nature and sound familiar to potential customers.
    However, new names must also be copyright free and original enough to stand out
    in the market.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一个最具创意的行业：时尚。一家专门生产户外服饰的公司准备推出一系列新款服装。任务是为这一新款服装系列生成足够多的名称候选。许多户外时尚品牌提出过山脉的名字，这些名字能唤起自然的感觉，而且顾客也会觉得很熟悉。然而，新名称还必须没有版权问题，并且要足够原创，能够在市场中脱颖而出。
- en: Why not use fictitious mountain names then? Since they are fictitious, they
    are copyright free and differ from competitor names; however, since they are similar
    to existing mountain names, they also sound familiar enough to potential customers.
    Could an artificial intelligence model help generate new fictitious mountain names
    that still sound realistic enough and are evocative of adventure? What kind of
    network architecture could we use for such a task?
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 那为什么不使用虚构的山脉名称呢？由于它们是虚构的，因此不受版权保护，并且与竞争者名称不同；然而，由于它们类似于现有的山脉名称，它们也足够熟悉，能吸引潜在客户。人工智能模型是否可以帮助生成既具有现实感又能唤起冒险精神的新虚构山脉名称？我们可以为这样的任务使用什么样的网络架构？
- en: As we want to be able to form new words that are somehow reminiscent of mountain
    names, the network must be trained on the names of already-existing mountains.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望能够形成某种程度上类似于山脉名称的新词，因此网络必须基于已有的山脉名称进行训练。
- en: To form the training set, we use a list of 33,012 names of US mountains, as
    extracted from Wikipedia through a Wikidata query.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形成训练集，我们使用从维基百科通过Wikidata查询提取的33,012个美国山脉名称列表。
- en: '*Figure 7.31* shows you a subset of the training data:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.31* 显示了训练数据的一个子集：'
- en: '![Figure 7.31 – Subset of US mountain names in the training set](img/B16391_07_031.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.31 – 训练集中美国山脉名称的子集](img/B16391_07_031.jpg)'
- en: Figure 7.31 – Subset of US mountain names in the training set
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31 – 训练集中美国山脉名称的子集
- en: 'Now that we have some training data, we can think about the network architecture.
    This time, we want to train a **many-to-many** LSTM-based RNN (see *Figure 7.32*).
    This means that during training, we have a sequence as input and a sequence as
    output. During deployment, the RNN, based on some initialized hidden states and
    the start token, must predict the first character of the new name candidate; then
    at the next step, based on the predicted character and on the updated hidden states,
    it must predict the next character – and so on until an end token is predicted
    and the process of generating the new candidate name is concluded:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些训练数据，可以开始考虑网络架构。这次，我们想训练一个**多对多**的基于LSTM的RNN（见*图 7.32*）。这意味着在训练过程中，我们有一个输入序列和一个输出序列。在部署时，RNN基于一些初始化的隐藏状态和开始标记，必须预测新名称候选的第一个字符；然后在下一步，根据预测的字符和更新后的隐藏状态，它必须预测下一个字符——依此类推，直到预测出结束标记并完成生成新候选名称的过程：
- en: '![Figure 7.32 – Simplified, unrolled visualization of the many-to-many RNN
    architecture for the product name generation case study](img/B16391_07_032.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.32 – 用于产品名称生成案例研究的多对多RNN架构的简化展开可视化](img/B16391_07_032.jpg)'
- en: Figure 7.32 – Simplified, unrolled visualization of the many-to-many RNN architecture
    for the product name generation case study
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.32 – 用于产品名称生成案例研究的多对多RNN架构的简化展开可视化
- en: 'To train the LSTM unit for this task, we need two sequences: an input sequence,
    made of a start token plus the mountain name, and a target sequence, made of the
    mountain name plus an end token. Notice that, at each training iteration, we feed
    the correct character into the network from the training set and not its prediction.
    This is called the **teacher forcing** training approach.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练LSTM单元执行此任务，我们需要两个序列：一个输入序列，由开始标记和山脉名称组成，一个目标序列，由山脉名称和结束标记组成。请注意，在每次训练迭代中，我们将正确的字符输入到网络中，而不是它的预测。这称为**教师强制**训练方法。
- en: Let's focus first on preprocessing and encoding input and target sequences.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先专注于预处理和编码输入及目标序列。
- en: Preprocessing and Encoding Mountain Names
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理和编码山脉名称
- en: The goal of the preprocessing is to create and encode input and target sequences,
    including the start and end tokens. As in the previous case study, we want to
    use one-hot encoding. Therefore, we create an index-based encoding, and we use
    the `1` as the start token index and `0` as the end token index.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的目标是创建并编码输入和目标序列，包括开始标记和结束标记。与前一个案例研究一样，我们希望使用独热编码。因此，我们创建一个基于索引的编码，并使用`1`作为开始标记的索引，`0`作为结束标记的索引。
- en: 'In the last case study, you learned that during training, the lengths of the
    sequences in one batch have to be the same. Therefore, we take the number of characters
    of the longest mountain name (58) plus 1 as the sequence length. Since this is
    the length of the longest mountain name, there is no need for truncation, but
    all shorter sequences will be zero-padded by adding multiple end tokens:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个案例研究中，你学到了在训练过程中，一批次中的序列长度必须相同。因此，我们将最长山脉名称的字符数（58）加 1 作为序列长度。由于这是最长山脉名称的长度，无需截断，但所有较短的序列将通过添加多个结束标记来进行零填充：
- en: '![Figure 7.33 – Workflow to read, encode, and create the input and target sequences
    for mountain name generation](img/B16391_07_033.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.33 – 用于读取、编码和创建输入及目标序列的山脉名称生成工作流](img/B16391_07_033.jpg)'
- en: Figure 7.33 – Workflow to read, encode, and create the input and target sequences
    for mountain name generation
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.33 – 用于读取、编码和创建输入及目标序列的山脉名称生成工作流
- en: 'The workflow snippet in the preceding figure creates the input and target sequences
    by doing the following:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图中的工作流片段通过以下方式创建输入和目标序列：
- en: Reading the mountain names and removing duplicates by using the **Table Reader**
    node and the **Duplicate Row Filter** node
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**表格读取器**节点和**重复行过滤器**节点读取山脉名称并去除重复项
- en: Replacing each `<space>` with a tilde character and afterward, each character
    with the character itself and `<space>`, using two **String Manipulation** nodes
    (this step is described in detail in the preprocessing of the previous case study,
    *Free text generation with RNNs*)
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用两个**字符串操作**节点，将每个`<space>`替换为波浪号字符，然后将每个字符与字符本身和`<space>`一起替换（该步骤在前一个案例研究的预处理部分中有详细描述，*基于
    RNN 的自由文本生成*）
- en: Creating and applying a dictionary (we will have a close look at this step in
    the next sub-section)
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建和应用字典（我们将在下一小节中仔细查看此步骤）
- en: Character splitting based on `<space>` and replacing all missing values with
    end tokens, to zero pad too-short sequences
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于`<space>`进行字符拆分，并将所有缺失值替换为结束标记，以零填充过短的序列
- en: Creating input and target sequences as collection type cells
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和目标序列作为集合类型单元
- en: Most of the steps are similar to the preprocessing steps in the case study of
    free text generation with RNNs. We will only take a closer look at *step 3* and
    *step 5*. Let's start with *step 3*.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数步骤类似于自由文本生成的预处理步骤，我们只会更仔细地查看*步骤 3*和*步骤 5*。我们从*步骤 3*开始。
- en: Creating and Applying a Dictionary
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建和应用字典
- en: 'Creating and applying the dictionary is implemented in the **Create and apply
    dictionary** metanode. *Figure 7.34* shows its contents. The input to this metanode
    is mountain names with spaced characters:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和应用字典是在**创建和应用字典**元节点中实现的。*图 7.34* 展示了它的内容。该元节点的输入是具有空格字符的山脉名称：
- en: '![Figure 7.34 – Workflow snippet inside the Create and apply dictionary metanode](img/B16391_07_034.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.34 – 创建和应用字典元节点中的工作流片段](img/B16391_07_034.jpg)'
- en: Figure 7.34 – Workflow snippet inside the Create and apply dictionary metanode
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.34 – 创建和应用字典元节点中的工作流片段
- en: In this metanode, we again use nodes from the KNIME Text Processing extension.
    The `2`, as we want to use indexes `0` and `1` for the end and start tokens. To
    use it as a dictionary in the next step, the created numerical indexes are transformed
    into strings by the **Number To String** node. Finally, the dictionary is applied
    (the **Dictionary Replacer** node), to transform characters into indexes in the
    original mountain names, and the text is extracted from the document (the **Document
    Data Extractor** node).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在此元节点中，我们再次使用 KNIME 文本处理扩展中的节点。`2` 是因为我们希望使用索引 `0` 和 `1` 作为开始和结束标记。为了在下一步中将其用作字典，创建的数字索引通过**数字转字符串**节点转换为字符串。最后，字典被应用（**字典替换器**节点），以将字符转换为原始山脉名称中的索引，文本则通过**文档数据提取器**节点从文档中提取。
- en: Tip
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The KNIME Text Processing extension and some of their nodes, such as **Strings
    To Document**, **Unique Term Extractor**, **Dictionary Replacer**, and **Document
    Data Extractor**, were introduced more in detail in the first case study of this
    chapter, *Finding the Tone of Your Customers' Voice – Sentiment Analysis*.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 文本处理扩展及其一些节点，如**字符串转文档**、**唯一术语提取器**、**字典替换器**和**文档数据提取器**，在本章的第一个案例研究中有更详细的介绍，*寻找客户声音的语调——情感分析*。
- en: 'In the separate, lower branch of the workflow snippet, we finalize the dictionary
    for the deployment by adding one more row for the end token, using the `0` as
    the default value for the integer cells and an empty string for the string cells.
    This adds one new row to our dictionary table, with `0` in the index column and
    empty strings in the character columns. We need this additional row in the deployment
    workflow to remove the end token(s):'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在流程图的独立下分支中，我们通过添加一行用于结束标记，使用 `0` 作为整数单元的默认值，空字符串作为字符串单元的默认值，来为部署完成词典。这会为我们的词典表格增加一行，在索引列中填充
    `0`，在字符列中填充空字符串。我们需要这个附加的行在部署工作流程中去除结束标记：
- en: '![Figure 7.35 – Configuration window of the Add Empty Rows node](img/B16391_07_035.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.35 – 添加空行节点的配置窗口](img/B16391_07_035.jpg)'
- en: Figure 7.35 – Configuration window of the Add Empty Rows node
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.35 – 添加空行节点的配置窗口
- en: Let's move on to the last step of the preprocessing.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行预处理的最后一步。
- en: Creating the Input and Target Sequences
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建输入和目标序列
- en: After the **Missing Value** node in the workflow in *Figure 7.35*, we have the
    zero-padded, encoded sequences. What is missing, though, is the start token at
    the beginning of the input sequence and the end token at the end of the target
    sequence, to make sure that the input and target sequence have the same length.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在**缺失值**节点处理之后，流程图中*图 7.35*展示了零填充的编码序列。然而，缺少的是输入序列开头的开始标记和目标序列末尾的结束标记，以确保输入序列和目标序列的长度一致。
- en: The additional values are added with `1` is used for the start token in the
    input sequence and the value `0` for the end token in the target sequence. In
    the case of the input sequence, the new column with the start token must be at
    the beginning. This is taken care of by the **Column Resorter** node. Now, the
    sequences can be aggregated and transformed into collection cells, using the **Create
    Collection Column** node.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 附加的值是通过 `1` 来表示输入序列中的开始标记，而目标序列中的结束标记则由值 `0` 表示。对于输入序列，新的开始标记列必须放在序列的开头。这一过程由**列重排序器**节点处理。现在，可以将序列聚合并转换为集合单元，使用**创建集合列**节点。
- en: Let's now design and train the appropriate network architecture.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来设计并训练适当的网络架构。
- en: Defining and Training the Network Architecture
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义与训练网络架构
- en: The process of designing and training the network is similar to the process
    used in the previous NLP case studies.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 设计和训练网络的过程与之前 NLP 案例研究中使用的过程相似。
- en: Designing the Network
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计网络
- en: 'In this case, we want to use a network with five layers:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们希望使用一个五层的网络：
- en: A **Keras input layer** to define the input shape
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras 输入层**，用于定义输入形状
- en: A **Keras LSTM layer** for the sequence analysis
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras LSTM 层**用于序列分析
- en: A **Keras dropout layer** for regularization
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras dropout 层**用于正则化
- en: A **Keras dense layers** with linear activation
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras 全连接层**，使用线性激活
- en: A **Keras softmax layer** to transform the output into a probability distribution
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Keras softmax 层**，用于将输出转化为概率分布
- en: The number of unique characters in the training set – that is, the character
    set size – is `95`. Since we allow sequences of variable length, the shape of
    the input layer is `?, 95`. The `?` stands for a variable sequence length.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集中的唯一字符数量，也就是字符集的大小为 `95`。由于我们允许变长的序列，输入层的形状为 `?, 95`。其中 `?` 代表变量序列长度。
- en: Next, we have the `256` units for this layer and we have left all other settings
    unchanged.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为该层设置了 `256` 单元，并且其他所有设置保持不变。
- en: In this case study, we want to add even more randomization to the character
    pick at the output layer, to increment the network creativity. This is done by
    introducing the ![](img/Formula_B16391_07_037.png) **temperature** parameter in
    the softmax function of the trained output layer.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们希望在输出层增加更多的随机化，以提升网络的创造力。这是通过在训练好的输出层的 softmax 函数中引入 ![](img/Formula_B16391_07_037.png)
    **温度**参数来实现的。
- en: 'Remember, the softmax function is defined as follows:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，softmax 函数的定义如下：
- en: '![](img/Formula_B16391_07_038.png) with ![](img/Formula_B16391_07_039.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_07_038.png) 与 ![](img/Formula_B16391_07_039.png)'
- en: 'If we now introduce the additional ![](img/Formula_B16391_07_040.png) **temperature**
    parameter, the formula for the activation function changes to the following:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在引入额外的 ![](img/Formula_B16391_07_040.png) **温度**参数，则激活函数的公式将改变为如下所示：
- en: '![](img/Formula_B16391_07_041.png) with ![](img/Formula_B16391_07_042.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_07_041.png) 与 ![](img/Formula_B16391_07_042.png)'
- en: This means we divide the linear part by ![](img/Formula_B16391_07_043.png) before
    applying the softmax function.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在应用 softmax 函数之前将线性部分除以 ![](img/Formula_B16391_07_043.png)。
- en: 'To be able to insert the temperature parameter after training, we split the
    output layer into two layers: one **Keras Dense Layer** node with a linear activation
    function for the linear part and one **Keras Softmax Layer** node to apply the
    activation function.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够在训练后插入温度参数，我们将输出层拆分为两个层：一个**Keras Dense Layer**节点，用于线性部分的线性激活函数，另一个**Keras
    Softmax Layer**节点用于应用激活函数。
- en: Important note
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Temperature is a parameter that can be added after training to control the confidence
    of the network output. ![](img/Formula_B16391_07_044.png) makes the network more
    confident but also more conservative. This often leads to generating the same
    results at every run.![](img/Formula_B16391_07_045.png) implements softer probability
    distributions over the different outputs. This leads to more diversity but, at
    the same time, also to more mistakes, such as in this case, character combinations
    that are impossible in English.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 温度是一个可以在训练后添加的参数，用于控制网络输出的置信度。![](img/Formula_B16391_07_044.png)使网络更有信心，但也更保守，这通常导致每次运行时生成相同的结果。![](img/Formula_B16391_07_045.png)则对不同的输出实现更柔和的概率分布，从而带来更多的多样性，但同时也会带来更多的错误，比如在这个例子中，生成了英语中不可能的字符组合。
- en: Training and Postprocessing the Network
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练与后处理网络
- en: The network is trained using the **Keras Network Learner** node. For the input
    data and the target data, the **From Collection of Number (integer)** conversion
    to **One-Hot Tensor** is selected. The different characters are again like different
    classes in a multi-class classification problem; therefore, the **Categorical
    Cross Entropy** loss function is adopted for training.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用**Keras Network Learner**节点进行训练。对于输入数据和目标数据，选择**从数字集合（整数）**转换为**One-Hot
    Tensor**。不同的字符再次类似于多类别分类问题中的不同类别，因此，采用**Categorical Cross Entropy**损失函数进行训练。
- en: In the third tab, `30` epochs, with a batch size of `128` data rows, shuffling
    the data before each epoch, and using `Adam` as the optimizer algorithm with the
    default settings. So far, this is all the same as in the previous NLP case studies.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个选项卡中，使用`30`个训练周期，`128`行数据的批量大小，在每个周期之前对数据进行洗牌，并使用`Adam`作为优化算法，采用默认设置。到目前为止，这与之前的
    NLP 案例研究中的设置完全相同。
- en: 'After training the network, the temperature, ![](img/Formula_B16391_07_040.png),
    is added by using the **DL Python Editor** node with the following lines of Python
    code:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络后，通过使用**DL Python Editor**节点和以下 Python 代码行来添加温度，![](img/Formula_B16391_07_040.png)：
- en: '[PRE2]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Remember that the hidden states of the previous LSTM unit are always used as
    input in the next LSTM unit. Therefore, three inputs are defined in the code:
    two for the two hidden states and one for the last predicted character encoded
    as a one-hot vector.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，前一个 LSTM 单元的隐藏状态始终作为输入传递给下一个 LSTM 单元。因此，在代码中定义了三个输入：两个用于两个隐藏状态，一个用于最后预测的字符，编码为
    one-hot 向量。
- en: 'Finally, the network is transformed into a TensorFlow network object and saved
    for deployment. The final training workflow is shown in *Figure 7.36*:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，网络被转化为一个 TensorFlow 网络对象并保存以供部署。最终的训练工作流如*图 7.36*所示：
- en: '![Figure 7.36 – Training workflow for the product name generation case study](img/B16391_07_036.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.36 – 产品名称生成案例的训练工作流](img/B16391_07_036.jpg)'
- en: Figure 7.36 – Training workflow for the product name generation case study
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.36 – 产品名称生成案例的训练工作流
- en: 'The workflow is available on the KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流可以在 KNIME Hub 上找到：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/)。
- en: Let's continue with the deployment workflow.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续部署工作流。
- en: Building a Deployment Workflow
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建部署工作流
- en: The deployment workflow again uses the recursive loop approach, similar to the
    deployment workflow of the NLP and the demand prediction case studies. This time,
    though, there is one big difference.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 部署工作流再次使用递归循环方法，类似于 NLP 和需求预测案例中的部署工作流。不同的是，这次有一个重大区别。
- en: In the last two case studies, the hidden state vectors were re-initialized at
    each iteration, as we always had ![](img/Formula_B16391_03_031.png) previous characters
    or ![](img/Formula_B16391_03_255.png) previous values as input. In this case study,
    we pass back, from the loop end node to the loop start node, not only the predicted
    index but also the two hidden state tensors from the LSTM layer.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两个案例研究中，隐藏状态向量在每次迭代时都会重新初始化，因为我们总是有![](img/Formula_B16391_03_031.png)前一个字符或![](img/Formula_B16391_03_255.png)前一个值作为输入。在本案例研究中，我们从循环结束节点返回到循环起始节点时，不仅传递预测的索引，还包括来自LSTM层的两个隐藏状态张量。
- en: 'In *Figure 7.37*, you can see the deployment workflow, which is also available
    on the KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.
    Let''s look at the setting differences in detail:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.37*中，您可以看到部署工作流，它也可以在KNIME Hub上找到：https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/。我们来详细看看设置的差异：
- en: '![Figure 7.37 – Deployment workflow to create multiple possible product names](img/B16391_07_037.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![图7.37 – 部署工作流以创建多个可能的产品名称](img/B16391_07_037.jpg)'
- en: Figure 7.37 – Deployment workflow to create multiple possible product names
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.37 – 部署工作流以创建多个可能的产品名称
- en: The first component, `1`. The other two columns contain the initial hidden states
    – that is, collection cells with 256 zeros in both columns.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个组件，`1`。另外两列包含初始的隐藏状态——也就是，两个列中各有256个零的集合单元。
- en: 'The **TensorFlow Network Executor** node executes the network one first time,
    producing as output the probability distribution over the indexes. In the configuration
    window of **TensorFlow Network Executor**, we have selected as input the columns
    with the first hidden state, the second hidden state, and the input collection.
    In addition, we set three output columns: one output column for the probability
    distribution, one output column for the first hidden state, and one output column
    for the second hidden state. We then pick the next index-encoded character according
    to the output probability distribution using the **Random Label Assigner (Data)**
    node in the **First Char** metanode. All these output values, predicted indexes,
    and hidden states make their way to the loop start node to predict the second
    index-encoded character.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 网络执行器**节点首次执行网络，输出索引的概率分布。在**TensorFlow 网络执行器**的配置窗口中，我们选择了作为输入的列：第一个隐藏状态、第二个隐藏状态和输入集合。此外，我们设置了三个输出列：一个用于概率分布的输出列，一个用于第一个隐藏状态的输出列，和一个用于第二个隐藏状态的输出列。然后，我们根据输出的概率分布，使用**随机标签分配器（数据）**节点在**第一个字符**元节点中选择下一个索引编码字符。所有这些输出值、预测的索引和隐藏状态都传送到循环起始节点，用于预测第二个索引编码字符。'
- en: Then, we start the recursive loop to generate one character after the next.
    At each iteration, we apply the network to the last predicted index and hidden
    states. We then pick the next character, again with the **Random Number Assigner
    (Data)** node, and we feed the last predicted value and the new hidden states
    into the lower input port of the **Recursive Loop End** node so that they can
    reach back to the loop start node.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们开始递归循环，每次生成一个字符。在每次迭代中，我们将网络应用于最后预测的索引和隐藏状态。然后，我们通过**随机数分配器（数据）**节点选取下一个字符，并将最后预测的值和新的隐藏状态输入到**递归循环结束**节点的下输入端口，以便它们能返回到循环起始节点。
- en: In the **Extract Mountain Names** component, we finally apply the dictionary
    – created in the training workflow – and we remove all the mountain names that
    appeared already in the training set.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在**提取山脉名称**组件中，我们最终应用了在训练工作流中创建的字典，并移除了在训练集中已经出现过的山脉名称。
- en: 'In *Figure 7.38*, you can see some of the generated mountain names. Indeed,
    they are new, copyright-free, evocative of mountains, and nature-feeling, and
    can be generated automatically in a number ![](img/Formula_B16391_03_036.png)
    as high as desired:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.38*中，您可以看到一些生成的山脉名称。事实上，它们是全新的、无版权的、富有山脉和自然感觉的名称，并且可以按照所需的数量自动生成！[](img/Formula_B16391_03_036.png)。
- en: '![Figure 7.38 – Mountain names generated by the deployment workflow](img/B16391_07_038.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![图7.38 – 通过部署工作流生成的山脉名称](img/B16391_07_038.jpg)'
- en: Figure 7.38 – Mountain names generated by the deployment workflow
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.38 – 通过部署工作流生成的山脉名称
- en: One of them will eventually be chosen as the new product name.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个将最终被选为新的产品名称。
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have reached the end of this relatively long chapter. Here, we have described
    three NLP case studies, each one solved by training an LSTM-based RNN applied
    to a time series prediction kind of problem.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经结束了这一相对较长的章节。在这里，我们描述了三个NLP案例研究，每个案例都是通过训练一个基于LSTM的RNN，应用于时间序列预测问题来解决的。
- en: The first case study analyzed movie review texts to extract the sentiment hidden
    in it. We dealt there with a simplified problem, considering a binary classification
    (positive versus negative) rather than considering too many nuances of possible
    user sentiment.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个案例研究分析了电影评论文本，以提取其中隐藏的情感。我们在这里处理了一个简化的问题，采用二分类（正面与负面）来代替考虑过多的用户情感细节。
- en: The second case study was language modeling. Training an RNN on a given text
    corpus with a given style produced a network capable of generating free text in
    that given style. Depending on the text corpus on which the network is trained,
    it can produce fairy tales, Shakespearean dialogue, or even rap songs. We showed
    an example that generates text in fairy tale style. The same workflows can be
    easily extended with more success to generate rap songs (R. Silipo, *AI generated
    rap songs*, CustomerThink, 2019, [https://customerthink.com/ai-generated-rap-songs/](https://customerthink.com/ai-generated-rap-songs/))
    or Shakespearean dialogue (R. Silipo, *Can AI write like Shakespeare?*, Towards
    data Science, 2019, [https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee](https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee)).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个案例研究是语言建模。我们在给定文本语料库和风格的基础上训练了一个RNN，产生了一个能够按照特定风格生成自由文本的网络。根据网络训练时使用的文本语料库，它可以生成童话故事、莎士比亚的对话，甚至是说唱歌曲。我们展示了一个生成童话风格文本的例子。相同的工作流可以轻松地扩展并成功地生成说唱歌曲（R.
    Silipo，*AI生成说唱歌曲*，CustomerThink，2019，[https://customerthink.com/ai-generated-rap-songs/](https://customerthink.com/ai-generated-rap-songs/)）或莎士比亚的对话（R.
    Silipo，*AI能像莎士比亚那样写作吗？*，Towards data Science，2019，[https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee](https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee)）。
- en: The last case study involved the generation of candidates for new product names
    that must be innovative and copyright-free, stands out in the market, and be evocative
    of nature. So, we trained an RNN to generate fictitious mountain names to be used
    as name candidates for a new outdoor clothing line.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的案例研究涉及生成新的产品名称候选，这些名称必须具有创新性、无版权并且在市场上突出，同时能够唤起自然的联想。因此，我们训练了一个RNN来生成虚构的山脉名称，作为新户外服装系列的名称候选。
- en: 'In the next chapter, we will describe one more NLP example: neural machine
    translation.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将描述另一个NLP示例：神经机器翻译。
- en: Questions and Exercises
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与练习
- en: What is a word embedding?
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是词嵌入？
- en: a) An encoding functionality that can be trained within the neural network
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 可以在神经网络内训练的编码功能
- en: b) A text cleaning procedure
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 文本清理程序
- en: c) A training algorithm for an RNN
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) RNN的训练算法
- en: d) A postprocessing technique to choose the most likely character
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 一种后处理技术，用于选择最可能的字符
- en: Which statement regarding sentiment analysis is true?
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种关于情感分析的说法是正确的？
- en: a) Sentiment analysis can only be solved with RNNs.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 情感分析只能通过RNN解决。
- en: b) Sentiment analysis is the same as emotion detection.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 情感分析与情绪检测相同。
- en: c) Sentiment analysis identifies the underlying sentiment in a text.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 情感分析识别文本中的潜在情感。
- en: d) Sentiment analysis is an image processing task.
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 情感分析是一个图像处理任务。
- en: What does a many-to-many architecture mean?
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多对多架构是什么意思？
- en: a) An architecture with an input sequence and an output sequence
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 具有输入序列和输出序列的架构
- en: b) An architecture with an input sequence and a vector as output
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 具有输入序列和向量作为输出的架构
- en: c) An architecture with many hidden units and many outputs
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 具有多个隐藏单元和多个输出的架构
- en: d) An architecture with one input feature and an output sequence
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 具有一个输入特征和一个输出序列的架构
- en: Why do I need a trigger sequence for free text generation?
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我需要触发序列来生成自由文本？
- en: a) To calculate the probabilities
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 计算概率
- en: b) To compare the prediction with the target
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 将预测与目标进行比较
- en: c) To initialize the hidden states before predicting the next character
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在预测下一个字符之前初始化隐藏状态
- en: d) To save the network in TensorFlow format
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 以TensorFlow格式保存网络
