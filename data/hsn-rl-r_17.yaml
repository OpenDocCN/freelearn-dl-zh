- en: Whats Next?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: In this chapter, we will summarize what has been covered so far in this book
    and what the next steps are from this point on. You will learn how to apply the
    skills you have gained to other projects, including real-life challenges in building
    and deploying **reinforcement learning** (**RL**) models and other common technologies
    that data scientists often use. By the end of this chapter, you will have a better
    understanding of real-life challenges in building and deploying RL models and
    additional resources and technologies that you can learn about to sharpen your
    RL skills.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将总结本书至今为止的内容，并说明接下来的步骤。你将学习如何将你所掌握的技能应用于其他项目，包括在构建和部署**强化学习**（**RL**）模型以及数据科学家常用的其他技术方面的现实挑战。通过本章的学习，你将更好地理解在构建和部署RL模型时面临的实际挑战，并获得进一步学习RL技能的资源和技术。
- en: By the end of this chapter, we will have explored a quick summary of RL concepts
    and the main RL applications in real life. Therefore, we will discover the next
    steps for RL and the real-world challenges in terms of constructing and implementing
    machine learning models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，我们将快速总结RL概念及其在现实生活中的主要应用。因此，我们将探索RL的下一步发展以及构建和实现机器学习模型时面临的现实世界挑战。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: RL summarized
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习总结
- en: Exploring RL projects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索强化学习项目
- en: Next steps for RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL的下一步
- en: RL summarized
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习总结
- en: '**Machine learning** (**ML**)-based algorithms can be classified based on the
    training paradigm they use. In supervised learning, there is a teacher who tells
    the system what the correct output is. This is not always possible. Often, we
    only have qualitative information that''s sometimes binary, right/wrong, or success/failure.
    This information is called **reinforcement signals**. However, the problem is
    that the system does not provide any information on how to update the behavior
    of the agent. A cost function or gradient is not available. In the case of RL,
    we want to create intelligent agents who can learn from their experience.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于**机器学习**（**ML**）的算法可以根据它们使用的训练范式进行分类。在监督学习中，有一个教师告诉系统正确的输出是什么。但这并不总是可能的。通常，我们只有定性的、二元的对错或成功/失败的信息。这些信息被称为**强化信号**。然而，问题在于系统没有提供如何更新代理行为的信息，也没有成本函数或梯度。在强化学习（RL）的情况下，我们希望创造能够从经验中学习的智能体。
- en: Initially, RL was viewed as **supervised learning**, but subsequently it was
    considered to be the third paradigm of machine learning algorithms. It is applied
    in different contexts in which supervised learning is inefficient, for example,
    when we have problems interacting with the environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，强化学习（RL）被视为**监督学习**，但随后它被认为是机器学习算法的第三种范式。它应用于监督学习效率低的不同场景，例如，当我们面临与环境交互的问题时。
- en: 'The following flow shows the steps to follow to correctly apply an RL algorithm:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程展示了正确应用RL算法的步骤：
- en: Preparation of the agent.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备代理。
- en: Observation of the environment.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察环境。
- en: Selection of the optimal strategy.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最优策略。
- en: Execution of actions.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作。
- en: Calculation of the corresponding reward (or penalty).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相应的奖励（或惩罚）。
- en: Development of updating strategies (if necessary).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发更新策略（如有必要）。
- en: Repeat steps 2-5 repeatedly until the agent learns the optimal strategies.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-5，直到代理学习到最优策略。
- en: RL algorithms work by trial and error using a feedback loop of rewards and punishments.
    When we insert a dataset into the algorithm, it treats the environment like a
    game and, every time it performs an action, it is told whether it has won or lost.
    In this way, it builds an image of winning and losing moves.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RL算法通过反复试验和奖励惩罚的反馈循环工作。当我们将数据集输入算法时，它将环境视为一种游戏，每次执行一个动作时，它会被告知是否赢得了游戏。通过这种方式，它建立了对胜负的认知。
- en: As a consequence of undesirable behavior, punishment is applied, which reduces
    the probability of a repetition of the error. In the case of the correct behavior
    being displayed, a reward is applied that identifies a correct policy. Having
    defined the objective to be reached, the algorithm tries to maximize the rewards
    that are received for the execution of the action. The object that must achieve
    the goal is called the agent. The object that the agent must interact with is
    called the environment, which corresponds to everything that is external to the
    agent.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不良行为，会施加惩罚，从而降低错误重复的概率。在展示正确行为的情况下，会施加奖励，从而确认一个正确的策略。在确定了要达成的目标后，算法会尽力最大化因执行某一行动所获得的奖励。必须达成目标的对象被称为代理。代理必须与之互动的对象被称为环境，它对应于所有外部于代理的事物。
- en: The agent is a software object that performs actions automatically and invisibly.
    The agent has a goal-oriented behavior but acts in an uncertain environment that's
    not initially known or is only partially known. An agent learns by interacting
    with the environment. The decision it makes can be developed while learning about
    the environment through measurements that are made by the agent itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代理是一个自动执行操作并且不显现的软件对象。代理具有目标导向的行为，但它在一个不确定的环境中执行操作，这个环境最初并不完全已知，或者只有部分已知。代理通过与环境的互动来学习。它所做的决策可以通过代理本身进行的测量，逐步发展，在学习环境的过程中形成。
- en: The interaction is characterized by repeated and various attempts that continue
    until success or until the agent stops trying. The agent-environment interaction
    is continuous; the agent chooses an action to be taken and, in response, the environment
    changes state, presenting a new situation to be addressed. In the case of RL,
    the environment provides the agent with a reward. It is essential that the source
    of the reward is the environment to avoid the formation of a personal reinforcement
    mechanism within the agent that would compromise learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种互动的特点是反复且多样的尝试，直到成功或代理停止尝试为止。代理-环境互动是连续的；代理选择要执行的行动，随后环境的状态发生变化，呈现一个新的情况需要应对。在强化学习中，环境向代理提供奖励。奖励的来源必须是环境，以避免在代理内形成个人强化机制，这样会妨碍学习的进行。
- en: The value of the reward is proportional to the influence the action has on achieving
    the goal; therefore, it is positive or high in the case of a correct action or
    negative or low for a wrong action.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的价值与行动对实现目标的影响成正比；因此，在正确的行动下，奖励是正的或较高的，而在错误的行动下，奖励则是负的或较低的。
- en: In the next section, we will discover the most famous projects that use RL.
    We will see that the largest companies in the world have invested large resources
    to exploit the potential offered by these algorithms.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用强化学习的最著名项目。我们将看到世界上最大的公司已经投入大量资源，以挖掘这些算法所提供的潜力。
- en: Exploring RL projects
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索强化学习项目
- en: RL is a programming paradigm that processes algorithms that are capable of learning
    and adapting to changes in the environment. At the base of this programming technique,
    there is the interaction with the environment, where the agent receives stimuli
    from the outside according to the choices of the algorithm. A correct choice will
    provide a reward, while an incorrect choice will provide a penalty. The best possible
    result is achieved by maximizing the rewards that are obtained by the system.
    For example, the computer learns to beat an opponent in a game by performing a
    certain task, with the goal of maximizing the reward. This means that the system
    learns from the mistakes it made previously, improving on performance based on
    the results that were achieved in previous explorations. The applications of RL
    in everyday life are already numerous, some of which have entered everyday use
    for some time. For example, RL is the basis for the development of self-driving
    cars – in fact, they learn to recognize the surrounding environment with data
    that's been collected by sensors. As a result, they adapt their behavior according
    to the obstacles they have to overcome. Another example is given by programs for
    profiling web users. They benefit from automatic learning, that is, from learning
    from the behavior and preferences of users browsing websites. Further examples
    include e-commerce platforms, such as Amazon, or entertainment and access to content,
    such as Netflix or Spotify. In the upcoming sections, we will look at some practical
    examples of the application of RL-based technologies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种编程范式，用于处理能够学习和适应环境变化的算法。在这种编程技术的基础上，存在与环境的交互，代理根据算法的选择从外界接收刺激。正确的选择将提供奖励，而错误的选择将带来惩罚。通过最大化系统获得的奖励，能够实现最优结果。例如，计算机通过执行某个任务学习在游戏中击败对手，目标是最大化奖励。这意味着系统从之前的错误中学习，基于之前探索中获得的结果来提高性能。强化学习在日常生活中的应用已经非常广泛，其中一些已经进入日常使用。例如，强化学习是自动驾驶汽车发展的基础——事实上，它们通过传感器收集的数据学习识别周围的环境。因此，它们根据需要克服的障碍来调整自己的行为。另一个例子是用于分析网页用户的程序。它们受益于自动学习，也就是从用户浏览网站时的行为和偏好中学习。其他应用包括电商平台，如亚马逊，或者娱乐和内容访问，如Netflix或Spotify。在接下来的部分中，我们将探讨一些基于强化学习技术应用的实际例子。
- en: Facebook ReAgent
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Facebook ReAgent
- en: 'Recommendation systems are applied in different sectors, but their purpose
    is unique: to help people make choices based on different aspects. According to
    the person, these aspects can be, for example, their own chronology, the purchases
    they''ve already made, the positive votes they''ve already given, or the preferences
    of similar people.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统应用于不同的领域，但其目标是独特的：帮助人们根据不同的方面做出选择。根据个人的不同，这些方面可以是例如他们的时间轴、他们已经购买的物品、他们已经给予的正面评价，或者类似人的偏好。
- en: 'A recommendation system is a useful tool for producing object recommendations
    based on the usage history of the person using them. There are already several
    applications of these tools that still constitute an important area of research
    and range from the recommendation of consumer goods in the case of Amazon, to
    the recommendation of films in the case of MovieLens, to the recommendation of
    news or research articles. Primarily, a recommendation system performs three operations:
    it gets people''s preferences from the input data, calculates recommendations,
    and presents them to people.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一种有用的工具，通过分析使用者的历史记录来生成物品推荐。目前，已经有多个这些工具的应用，它们仍然是一个重要的研究领域，涵盖了从亚马逊的消费品推荐，到MovieLens的电影推荐，再到新闻或研究文章的推荐。通常，推荐系统执行三个操作：从输入数据中获取人们的偏好，计算推荐结果，并将其呈现给用户。
- en: Therefore, it's a system that guides the user in making decisions. Such a system
    can be treated as a Markovian process, which means it can be modeled with RL.
    For example, we can use contextual bandits that are effective at selecting actions
    based on contextual information. In the recommendation of news articles, this
    involves selecting articles for users based on the contextual information of users
    and articles, such as the historical activities of user's descriptive information
    and content categories.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它是一个引导用户做决策的系统。这样的系统可以被视为一个马尔可夫过程，这意味着它可以通过强化学习来建模。例如，我们可以使用上下文赌博机（contextual
    bandits），它在根据上下文信息选择动作时非常有效。在新闻文章推荐中，这涉及到根据用户和文章的上下文信息来为用户选择文章，比如用户的历史活动、描述性信息和内容类别。
- en: Facebook ReAgent is a platform based on RL that uses this paradigm to optimize
    products and services that are used by billions of people ([https://github.com/facebookresearch/ReAgent](https://github.com/facebookresearch/ReAgent)).
    Horizon is the first platform based on open source RL for production. The researchers
    at Facebook have developed this platform so that it extends the remarkable results
    they've achieved in research in the use of decision algorithms to the productive
    sector.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook ReAgent 是一个基于强化学习的 платформ，利用这一范式来优化数十亿人使用的产品和服务 ([https://github.com/facebookresearch/ReAgent](https://github.com/facebookresearch/ReAgent))。Horizon
    是第一个基于开源强化学习的生产平台。Facebook 的研究人员开发了这个平台，旨在将他们在研究中取得的卓越成果，尤其是决策算法的应用，扩展到生产领域。
- en: Unity ML-Agents
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity ML-Agents
- en: The Machine Learning Agents Toolkit is an open source plugin offered for free
    on GitHub with the aim of providing developers with the possibility of training
    virtual entities in ad hoc environments using the Unity engine ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习代理工具包是一个开源插件，免费提供于 GitHub，旨在为开发者提供在 Unity 引擎中使用临时环境训练虚拟实体的可能性 ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents))。
- en: For agent training, RL, imitation learning, neuroevolution, and other machine
    learning methods can be used through an easy to use Python API. Furthermore, TensorFlow-based
    implementations of algorithms are available to easily train intelligent agents
    for 2D, 3D, and VR/AR games. The ML-Agents toolkit can be used to develop applications
    in a very productive way – in fact, progress can be evaluated in the rich environments
    of Unity.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代理训练，可以通过一个易于使用的 Python API 使用强化学习、模仿学习、神经进化以及其他机器学习方法。此外，基于 TensorFlow 的算法实现也可用于轻松训练适用于
    2D、3D 和 VR/AR 游戏的智能代理。ML-Agents 工具包可以高效地开发应用程序——实际上，进展可以在 Unity 丰富的环境中得到评估。
- en: Google DeepMind
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google DeepMind
- en: DeepMind is a British artificial intelligence company that was founded in 2010
    as DeepMind Technologies but was later acquired by Google in 2014\. DeepMind is
    Google's already legendary artificial intelligence wing for creating AlphaGo,
    the AI that beat the best human Go player in the world.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind 是一家英国的人工智能公司，成立于 2010 年，最初名为 DeepMind Technologies，后来在 2014 年被 Google
    收购。DeepMind 是 Google 已经传奇的人工智能部门，创造了 AlphaGo，这个 AI 打败了世界上最强的围棋选手。
- en: Therefore, the choices that DeepMind makes are not based on external indications
    or instructions written in the lines of the source code of a program. The Google
    supercomputer can prefer an option among multiple possibilities that's based on
    the experience acquired and the information coming from the external environment.
    All of this is made possible by the **Differential Neural Computer** (**DNC**),
    a computer system that replicates the functionality of human memory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DeepMind 所做的选择并不是基于外部指示或写在程序源代码行中的指令。Google 的超级计算机可以从多个可能性中选择一个选项，这个选项基于所获得的经验和来自外部环境的信息。所有这些都得益于
    **差分神经计算机** (**DNC**)，一种模仿人类记忆功能的计算机系统。
- en: Managing large amounts of data using IBM Watson
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 IBM Watson 管理大量数据
- en: Watson is a project of the IBM group that's focused on creating an artificial
    intelligence system that's capable of handling large amounts of unstructured data
    to transform this resource into structured information and then decisions. Watson
    was born as a platform that users can access to give life to their ideas. The
    complexity and heterogeneity of data, many of which are unstructured, constitute
    the most difficult obstacle to overcome. Despite the improvements of classical
    analytical methods, the construction of useful information starting from disaggregated
    data remains only partially realized. Data is generated seamlessly, producing
    2.5 billion GB each day. Therefore, it is essential to consider the data as a
    new natural resource that competitive strength can derive from.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Watson是IBM集团的一个项目，旨在创建一个能够处理大量非结构化数据的人工智能系统，将这些资源转化为结构化信息，进而做出决策。Watson最初作为一个平台诞生，用户可以通过它将自己的创意付诸实践。数据的复杂性和异质性，尤其是许多非结构化数据，是需要克服的最大障碍。尽管经典分析方法有所改进，从分散的数据中构建有用信息仍然只是部分实现。数据以无缝方式生成，每天产生25亿GB的数据。因此，必须将数据视为一种新的自然资源，从中可以获取竞争优势。
- en: So far, we have analyzed numerous algorithms and learned how to use RL in different
    areas. But what are the future challenges facing researchers all over the world?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经分析了众多算法，并学习了如何在不同领域应用强化学习。那么，全球研究人员面临的未来挑战是什么？
- en: Next steps for RL
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的下一步
- en: Modern technology has made us accustomed to seeing machines that perform tasks
    instead of humans. In the automation of production processes, quick and precise
    calculations and the execution of instructions with a minimum margin of error
    make the machines competitive, if not even more so than human beings. In short,
    more and more often, we're developing algorithms that can help the software learn
    from experience. This is called machine learning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现代技术使我们习惯于看到机器代替人类执行任务。在生产过程的自动化中，快速且精准的计算以及执行指令时的最小误差使得机器具有竞争力，甚至可能比人类更具竞争力。简而言之，越来越多的算法正在被开发出来，帮助软件从经验中学习。这就是机器学习。
- en: Based on these algorithms, predefined rules to be learned with the machine are
    no longer necessary, but they make decisions using models and instructions through
    which we can learn the right rules to solve the problem in question. As we saw
    in the previous section, these technologies are already widely used in real life.
    Examples include the fight against spam and the identification of credit card
    frauds, voice recognition and manual writing, economic and financial forecasts,
    automatic classification of images, and so on.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些算法，不再需要预定义的规则来与机器一起学习，但它们通过模型和指令做出决策，通过这些我们可以学习解决当前问题的正确规则。正如我们在上一节中看到的，这些技术已经在现实生活中得到了广泛应用。例子包括反垃圾邮件斗争、信用卡欺诈识别、语音识别与手写识别、经济与金融预测、自动分类图像等等。
- en: What will this technology offer us in the future? Let's see what the future
    challenges are when it comes to using RL algorithms.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术未来将为我们提供什么？让我们来看一看使用强化学习算法时未来的挑战是什么。
- en: Understanding Inverse RL
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解逆向强化学习
- en: In RL algorithms, the agent receives a reinforcement signal as soon as they
    perform an action. There are several areas where it is difficult to estimate a
    reinforcement function. **Inverse RL** (**IRL**) allows you to reconstruct a reinforcement
    function that defines the behavior of an agent based on a set of demonstrations.
    When learning about reverse reinforcement, the reward function derives from the
    observed behavior. Generally, in RL, we use rewards to learn the behavior of a
    system. In IRL, this function is reversed; in fact, the agent observes the behavior
    of the system to understand what the goal is trying to achieve. To do this, it
    is necessary to have the state of the environment available to learn the optimal
    policy for each reinforcement function that's considered. Furthermore, it is necessary
    to define a set of features to be combined in order to identify a single reinforcement
    function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）算法中，智能体在执行某个动作后会立即收到强化信号。有几个领域中很难估计强化函数。**逆强化学习**（**IRL**）允许你通过一组示范重建一个定义智能体行为的强化函数。在学习逆强化的过程中，奖励函数来自于观察到的行为。通常，在强化学习中，我们使用奖励来学习系统的行为；而在逆强化学习中，这个过程是反向的；实际上，智能体观察系统的行为，进而理解目标正在尝试实现什么。为了做到这一点，必须拥有环境的状态，以便学习每个强化函数的最优策略。此外，还需要定义一组特征，进行组合，以识别单一的强化函数。
- en: 'In an IRL, the problem starts with the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在逆强化学习（IRL）中，问题从以下内容开始：
- en: Measurements of an agent's behavior over time
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对智能体行为随时间的测量
- en: Measurements of sensory input for that agent
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对该智能体的感官输入进行测量
- en: A model of the physical environment
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理环境的模型
- en: Based on this data, we can determine the reward function that the agent is optimizing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些数据，我们可以确定智能体正在优化的奖励函数。
- en: In terms of policy, solving an IRL problem means coding the expert's behavior
    in a reference policy and then identifying a reward function so that this policy
    is an optimal policy. This approach, whose goal is to identify the reward function,
    can provide better results than one that merely builds a policy so that the agent
    imitates the expert. This is because the reward function does not simply describe
    the behavior of the agent, but rather encodes the motivations in more depth.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 就策略而言，解决逆强化学习问题意味着将专家的行为编码为参考策略，然后识别一个奖励函数，以使该策略成为最优策略。这种方法的目标是识别奖励函数，它可以比仅仅构建一个策略使智能体模仿专家的方式提供更好的结果。这是因为奖励函数不仅描述了智能体的行为，还更深入地编码了动机。
- en: This representation is more concise than a policy-based representation and allows
    the agent's behavior to be generalized, even to regions of the state space that
    haven't been explored by the expert. By knowing about the reward function, it
    is also possible to react to changes in the model by regenerating a new optimal
    policy starting from the reward function itself, instead of having to relearn
    it completely. Finally, there are contexts in which the reward function itself
    is the objective of the research.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示比基于策略的表示更加简洁，并且允许智能体的行为被泛化，甚至扩展到专家尚未探索的状态空间区域。通过了解奖励函数，还可以通过重新生成一个新的最优策略，从奖励函数本身开始来应对模型的变化，而不必完全重新学习。最后，有一些情境，其中奖励函数本身就是研究的目标。
- en: Another challenge that concerns policy algorithms is deep deterministic policy
    gradients. Let's see how this can be addressed.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与策略算法相关的挑战是深度确定性策略梯度。我们来看看如何解决这个问题。
- en: Deep deterministic policy gradients
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: '**Deep deterministic policy gradients** (**DDPG**) is part of the family of
    policy gradient algorithms that uses a stochastic behavioral policy in the exploration
    phase. The DDPG algorithm evaluates a deterministic target policy that''s much
    easier to learn. In these policy algorithms, iteration is characterized by the
    following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度确定性策略梯度**（**DDPG**）是策略梯度算法家族的一部分，它在探索阶段使用随机行为策略。DDPG算法评估的是一个确定性的目标策略，这个策略更容易学习。在这些策略算法中，迭代的特点如下：'
- en: Evaluation of the policy
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略评估
- en: Following the policy gradient to maximize performance
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循策略梯度以最大化性能
- en: 'The DDPG represents an out-of-policy algorithm that uses a deterministic target
    policy. Under these conditions, the use of the gradient theorem of deterministic
    politics is allowed. DDPG is also an example of an actor-critic algorithm. In
    this sense, it mainly uses two neural networks: one for the actor and one for
    the critic. Both networks calculate the action forecasts for the current state
    and generate a **time difference** (**TD**) error signal each time. The actor''s
    network input uses the current state, and the output is represented by an action
    that''s been chosen by a continuous action space. The output of the critic is
    simply the estimated Q value of the current state and of the action given by the
    actor. The deterministic policy gradient theorem provides the updating rule for
    actor net weights. The critical network is updated by the gradients that have
    been obtained from the error signal TD.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG代表了一种基于策略外算法，它使用确定性目标策略。在这些条件下，允许使用确定性政策的梯度定理。DDPG也是一种演员-评论员算法的例子。从这个意义上说，它主要使用两个人工神经网络：一个用于演员，一个用于评论员。两个网络都计算当前状态的行动预测，并每次生成一个**时差**
    (**TD**) 错误信号。演员网络的输入使用当前状态，输出是由连续动作空间选择的动作。评论员的输出只是当前状态和由演员提供的动作的估计Q值。确定性策略梯度定理提供了演员网络权重更新规则。评论员网络则通过从TD错误信号获得的梯度进行更新。
- en: So far, we have seen how the agent interacts with the environment, but what
    happens when it interacts with humans?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到智能体如何与环境互动，但当它与人类互动时会发生什么呢？
- en: RL from human preferences
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自人类偏好的强化学习
- en: In many cases, it isn't possible to define a well-specified reward function.
    Many real-life problems are characterized by complex goals that have been poorly
    defined or difficult to specify. For example, suppose you want to use RL to train
    a robot to choose the best path to reach a target. The reward function is not
    easily defined. It will have to depend on the data coming from the robot sensors.
    In case there was the possibility of successfully communicating our real goals
    to our agents, we would have achieved a great result in solving these problems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，无法定义一个明确的奖励函数。许多现实问题的特点是目标复杂且定义不清或难以指定。例如，假设你想用强化学习训练一个机器人选择最佳路径以到达目标。奖励函数并不容易定义，它必须依赖于来自机器人传感器的数据。如果我们能够成功地将我们的真实目标传达给智能体，我们在解决这些问题时将会取得巨大的成果。
- en: If we had examples of the desired task, we could extract a reward function using
    reverse RL. Later, we could use learning imitation to clone proven behavior. Unfortunately,
    these approaches are not directly applicable to many of the real problems. Something
    that could help solve this problem could come from the feedback from a human defining
    the activity. In this case, we find ourselves in the paradigm of RL, although
    the use of human feedback directly as a reward function is prohibitive for RL
    systems that require hundreds or thousands of hours of experience.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有期望任务的示例，我们可以使用逆向强化学习提取奖励函数。之后，我们可以通过模仿学习来克隆已经验证的行为。不幸的是，这些方法并不直接适用于许多现实问题。解决这个问题的一个可能途径是来自人类反馈，定义活动的过程。在这种情况下，我们进入了强化学习的范式，尽管将人类反馈直接作为奖励函数使用对那些需要数百或数千小时经验的强化学习系统来说是不可行的。
- en: In deep RL based on human preferences, the agent learns a reward function from
    human feedback and thus optimizes this reward function. This represents a solution
    to the problems of sequential decision-making without a well-specified reward
    function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于人类偏好的深度强化学习中，智能体从人类反馈中学习奖励函数，从而优化这个奖励函数。这为没有明确奖励函数的序列决策问题提供了解决方案。
- en: 'The algorithm has the following characteristics:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法具有以下特点：
- en: It allows us to solve tasks where we can only recognize the desired behavior,
    but not necessarily prove it.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使我们能够解决那些我们只能识别出期望行为，但不一定能够证明的任务。
- en: It allows agents to be instructed by non-expert users.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许智能体接受非专家用户的指导。
- en: It is a large-scale problem.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个大规模问题。
- en: It is economical with user feedback.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它节省了用户反馈。
- en: This algorithm adapts a reward function to the preferences of the human being
    and at the same time forms a policy to optimize the current expected reward function.
    There are cases in which exploration of the environment is complex, so we must
    adopt a different approach. Let's see what can be done.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将奖励函数调整为符合人类偏好的同时，形成一个策略来优化当前期望的奖励函数。有些情况下，环境的探索非常复杂，因此我们必须采用不同的方法。让我们看看可以做些什么。
- en: Hindsight experience replay
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后视经验回放
- en: 'One of the characteristics of humans is to learn from their mistakes and adapt
    to them to avoid making the same mistake. This is the secret of RL algorithms.
    Problems in the implementation of these algorithms are encountered when scattered
    prizes are encountered. Let''s analyze the following scenario: an agent must manage
    a robot arm to open a box and place an object inside it. The reward for this task
    is simple to define; on the contrary, the learning problem is difficult to implement.
    The agent must explore a long sequence of correct actions to identify an environment
    configuration that returns the scattered reward: in this case, the position of
    the object inside the box. Finding this reward signal is a complicated exploration
    problem that can hardly be addressed through random exploration.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的一个特点是从错误中学习并适应，以避免重复犯同样的错误。这正是强化学习（RL）算法的秘密。当遇到分散奖励时，实施这些算法时会遇到问题。我们来分析以下场景：一个智能体必须控制一个机器人臂打开一个盒子并将物体放入其中。这个任务的奖励很容易定义；相反，学习问题却很难实现。智能体必须探索一系列正确的动作，以识别返回分散奖励的环境配置：在这种情况下，物体在盒子内的位置。找到这个奖励信号是一个复杂的探索问题，几乎无法通过随机探索解决。
- en: To solve this type of problem, a new technology called Hindsight Experience
    Replay can be adopted. This technique adopts efficient learning based on scarce
    and binary prizes by omitting the complicated reward techniques that are used
    in other methods. It can be combined with an arbitrary off-policy RL algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这类问题，可以采用一种名为**后视经验回放**（Hindsight Experience Replay）的新技术。该技术通过省略其他方法中使用的复杂奖励技术，采用基于稀缺和二元奖励的高效学习。它可以与任何非策略强化学习算法结合使用。
- en: This approach is suitable for handling objects with a robotic arm and for pushing,
    sliding, and pick-and-place tasks. Only binary prizes are used, which indicate
    whether the activity has been completed. Recent studies show that Hindsight Experience
    Replay is crucial in making training possible in these difficult environments.
    Other studies show that policies that have been trained on a physical simulation
    can be implemented on a physical robot and successfully complete the task at hand.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法适用于处理机器人臂操作物体，以及推、滑、拾取和放置任务。只使用二元奖励，指示活动是否已完成。最近的研究表明，后视经验回放在这些困难环境中使训练成为可能。其他研究表明，已在物理仿真中训练的策略可以在物理机器人上实施，并成功完成任务。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we summarized the essential elements of algorithms based on
    RL. Then, we explored some practical examples of the application of RL technologies.
    Finally, we discussed some future challenges in using the RL algorithm. We explored
    IRL, DDPG, RL from human preferences, and Hindsight Experience Replay.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们总结了基于强化学习算法的基本要素。然后，我们探索了强化学习技术的一些实际应用示例。最后，我们讨论了使用强化学习算法的未来挑战。我们探讨了逆向强化学习（IRL）、深度确定性策略梯度（DDPG）、基于人类偏好的强化学习和后视经验回放。
- en: This is the last chapter in this book, and at this point we have explored different
    scenarios with the use of algorithms based on RL. First of all, I must congratulate
    you on reaching the end of this book; you have completed a difficult challenge.
    I also recommend that you immediately apply the skills you've acquired to real-world
    problems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章，到此为止，我们已经探索了基于强化学习算法的不同场景。首先，我必须祝贺你已经完成了本书的学习；你已完成了一项艰难的挑战。我还建议你将所学技能立即应用于实际问题。
