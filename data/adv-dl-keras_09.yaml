- en: Chapter 9. Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章 深度强化学习
- en: '**Reinforcement Learning** (**RL**) is a framework that is used by an agent
    for decision-making. The agent is not necessarily a software entity such as in
    video games. Instead, it could be embodied in hardware such as a robot or an autonomous
    car. An embodied agent is probably the best way to fully appreciate and utilize
    reinforcement learning since a physical entity interacts with the real-world and
    receives responses.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** (**RL**) 是一个框架，由代理用于决策。代理不一定是软件实体，比如在视频游戏中。相反，它可以体现在硬件中，如机器人或自动驾驶汽车。体现代理可能是充分理解和利用强化学习的最佳方式，因为物理实体与现实世界互动并接收响应。'
- en: The agent is situated within an **environment**. The environment has a **state**
    that can be partially or fully observable. The agent has a set of **actions**
    that it can use to interact with its environment. The result of an action transitions
    the environment to a new state. A corresponding scalar **reward** is received
    after executing an action. The goal of the agent is to maximize the accumulated
    future reward by learning a **policy** that will decide which action to take given
    a state.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 代理位于一个**环境**中。环境具有部分或完全可观察的**状态**。代理有一组**行动**，可以用来与其环境交互。行动的结果会将环境转换到一个新的状态。执行行动后将收到相应的标量**奖励**。代理的目标是通过学习一个**策略**，在给定状态下决定采取哪个行动来最大化累积的未来奖励。
- en: Reinforcement learning has a strong similarity to human psychology. Humans learn
    by experiencing the world. Wrong actions result in a certain form of penalty and
    should be avoided in the future, whilst actions which are right are rewarded and
    should be encouraged. This strong similarity to human psychology has convinced
    many researchers to believe that reinforcement learning can lead us towards **Artificial
    Intelligence** (**AI**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与人类心理学有很强的相似性。人类通过经验世界来学习。错误的行动会导致一定形式的惩罚，并应在未来避免，而正确的行动则受到奖励并应该鼓励。这种与人类心理学的强烈相似性已经使许多研究人员相信，强化学习可以引领我们走向**人工智能**
    (**AI**)。
- en: Reinforcement learning has been around for decades. However, beyond simple world
    models, RL has struggled to scale. This is where **Deep Learning** (**DL**), came into
    play. It solved this scalability problem which opened up the era of **Deep Reinforcement
    Learning** (**DRL**), which is what we are going to focus on in this chapter.
    One of the notable examples in DRL is the work of DeepMind on agents that were
    able to surpass the best human performance on different video games. In this chapter,
    we discuss both RL and DRL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习已经存在几十年了。然而，超越简单世界模型，RL 在扩展方面一直存在困难。这就是**深度学习** (**DL**) 发挥作用的地方。它解决了这个可扩展性问题，开启了**深度强化学习**
    (**DRL**) 的时代，这也是本章重点讨论的内容。DRL 中的一个显著例子是 DeepMind 在能够超越不同视频游戏中最佳人类表现的代理工作。在本章中，我们讨论了
    RL 和 DRL 两者。
- en: 'In summary, the goal of this chapter is to present:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本章的目标是提供：
- en: The principles of RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL 的原理
- en: The Reinforcement Learning technique, Q-Learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习技术 Q 学习
- en: Advanced topics including **Deep Q-Network** (**DQN**), and **Double Q-Learning**
    (**DDQN**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括**深度 Q 网络** (**DQN**) 和 **双 Q 学习** (**DDQN**) 在内的高级主题
- en: Instructions on how to implement RL on Python and DRL within Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 上实施 RL 和在 Keras 中实现 DRL 的指南
- en: Principles of reinforcement learning (RL)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习（RL）的原理
- en: '*Figure 9.1.1* shows the perception-action-learning loop that is used to describe
    RL. The environment is a soda can sitting on the floor. The agent is a mobile
    robot whose goal is to pick up the soda can. It observes the environment around
    it and tracks the location of the soda can through an onboard camera. The observation
    is summarized in a form of state which the robot will use to decide which action
    to take. The actions it takes may pertain to low-level control such as the rotation
    angle/speed of each wheel, rotation angle/speed of each joint of the arm, and
    whether the gripper is open or close.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.1.1* 显示了用于描述 RL 的感知-行动-学习循环。环境是一个放在地板上的苏打罐。代理是一个移动机器人，其目标是拾取苏打罐。它观察周围的环境，并通过机载摄像头跟踪苏打罐的位置。观察结果以一种状态的形式总结，机器人将使用该状态来决定采取哪些行动。它采取的行动可能涉及低级控制，如每个轮子的旋转角度/速度，每个机械臂关节的旋转角度/速度，以及夹持器是否打开或关闭。'
- en: 'Alternatively, the actions may be high-level control moves such as moving the
    robot forward/backward, steering with a certain angle, and grab/release. Any action
    that moves the gripper away from the soda receives a negative reward. Any action
    that closes the gap between the gripper location and the soda receives a positive
    reward. When the robot arm successfully picks up the soda can, it receives a big
    positive reward. The goal of RL is to learn the optimal policy that helps the
    robot to decide which action to take given a state to maximize the accumulated
    discounted reward:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，动作可能是高层次的控制动作，例如让机器人前进/后退，按照某个角度转向，抓取/释放。任何使抓手远离可乐的动作都会得到负奖励。任何缩小抓手位置与可乐之间距离的动作都会得到正奖励。当机器人臂成功拾起可乐罐时，会得到一个较大的正奖励。RL的目标是学习最优策略，帮助机器人根据状态决定采取何种动作，以最大化累积的折扣奖励：
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习原理（RL）](img/B08956_09_01.jpg)'
- en: 'Figure 9.1.1: The perception-action-learning loop in reinforcement learning'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1.1：强化学习中的感知-行动-学习循环
- en: 'Formally, the RL problem can be described as a **Markov Decision Process**
    (**MDP**). For simplicity, we''ll assume a *deterministic* environment where a
    certain action in a given state will consistently result in a known next state
    and reward. In a later section of this chapter, we''ll look at how to consider
    stochasticity. At timestep *t*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，RL问题可以描述为**马尔可夫决策过程**（**MDP**）。为简化起见，我们假设是一个*确定性*环境，其中在给定状态下的某个动作将始终导致已知的下一个状态和奖励。在本章稍后的部分，我们将探讨如何考虑随机性。在时间步*t*时：
- en: The environment is in a state *s*[t] from the state space ![Principles of reinforcement
    learning (RL)](img/B08956_09_001.jpg) which may be discrete or continuous. The
    starting state is *s*[0] while the terminal state is *s*[t].
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境处于状态*s*[t]，来自状态空间![强化学习原理（RL）](img/B08956_09_001.jpg)，状态可能是离散的或连续的。起始状态是*s*[0]，而终止状态是*s*[t]。
- en: The agent takes action *a*[t] from the action space ![Principles of reinforcement
    learning (RL)](img/B08956_09_002.jpg) by obeying the policy, ![Principles of reinforcement
    learning (RL)](img/B08956_09_003.jpg). ![Principles of reinforcement learning
    (RL)](img/B08956_09_004.jpg) may be discrete or continuous.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理根据策略从动作空间中采取动作*a*[t]，![强化学习原理（RL）](img/B08956_09_002.jpg)。![强化学习原理（RL）](img/B08956_09_003.jpg)可能是离散的或连续的。
- en: The environment transitions to a new state *s*[t+1] using the state transition
    dynamics ![Principles of reinforcement learning (RL)](img/B08956_09_005.jpg).
    The next state is only dependent on the current state and action. ![Principles
    of reinforcement learning (RL)](img/B08956_09_006.jpg) is not known to the agent.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境使用状态转移动态![强化学习原理（RL）](img/B08956_09_005.jpg)转移到新的状态*s*[t+1]。下一个状态只依赖于当前的状态和动作。![强化学习原理（RL）](img/B08956_09_006.jpg)对代理不可知。
- en: The agent receives a scalar reward using a reward function, r*[t+1]* = *R*(*s*[t],*a*[t])
    with ![Principles of reinforcement learning (RL)](img/B08956_09_007.jpg). The
    reward is only dependent on the current state and action. *R* is not known to
    the agent.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理使用奖励函数r*[t+1]* = *R*(*s*[t],*a*[t])接收一个标量奖励，其中![强化学习原理（RL）](img/B08956_09_007.jpg)。奖励仅依赖于当前的状态和动作。*R*对代理不可知。
- en: Future rewards are discounted by ![Principles of reinforcement learning (RL)](img/B08956_09_008.jpg)
    where ![Principles of reinforcement learning (RL)](img/B08956_09_009.jpg) and
    *k* is the future timestep.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来的奖励由![强化学习原理（RL）](img/B08956_09_008.jpg)折扣，其中![强化学习原理（RL）](img/B08956_09_009.jpg)，*k*是未来的时间步。
- en: '*Horizon*, *H*, is the number of timesteps, *T*, needed to complete one episode
    from *s*[0] to *s*[t].'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Horizon*，*H*，是完成一次从*s*[0]到*s*[t]的回合所需的时间步数*T*。'
- en: The environment may be fully or partially observable. The latter is also known
    as a **partially observable MDP** or **POMDP**. Most of the time, it's unrealistic
    to fully observe the environment. To improve the observability, past observations
    are also taken into consideration with the current observation. The state comprises
    the sufficient observations about the environment for the policy to decide on
    which action to take. In *Figure 9.1.1*, this could be the 3D position of the
    soda can with respect to the robot gripper as estimated by the robot camera.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 环境可以是完全可观察的，也可以是部分可观察的。后者通常被称为**部分可观察的MDP**或**POMDP**。大多数情况下，完全观察环境是不现实的。为了提高可观察性，除了当前观察之外，过去的观察也会被纳入考虑。状态包括关于环境的足够观察信息，供策略决定采取何种行动。在*图
    9.1.1*中，这可以是机器人抓手与苏打罐相对的3D位置，由机器人摄像头估算得出。
- en: 'Every time the environment transitions to a new state, the agent receives a
    scalar reward, r*[t+1]*. In *Figure 9.1.1*, the reward could be +1 whenever the
    robot gets closer to the soda can, -1 whenever it gets farther, and +100 when
    it closes the gripper and successfully picks up the soda can. The goal of the
    agent is to learn an optimal policy ![Principles of reinforcement learning (RL)](img/B08956_09_010.jpg)
    that maximizes the *return* from all states:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每次环境过渡到新状态时，智能体会收到一个标量奖励，r*[t+1]*。在*图 9.1.1*中，每当机器人靠近苏打罐时，奖励为+1；每当远离时，奖励为-1；当机器人关闭抓手并成功拿起苏打罐时，奖励为+100。智能体的目标是学习最优策略![强化学习原理](img/B08956_09_010.jpg)，以最大化所有状态的*回报*：
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_011.jpg) (Equation
    9.1.1)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习原理](img/B08956_09_011.jpg)（方程 9.1.1）'
- en: The return is defined as the discounted cumulative reward, ![Principles of reinforcement
    learning (RL)](img/B08956_09_012.jpg). It can be observed from *Equation 9.1.1*
    that future rewards have lower weights when compared to the immediate rewards
    since generally ![Principles of reinforcement learning (RL)](img/B08956_09_013.jpg)
    where ![Principles of reinforcement learning (RL)](img/B08956_09_014.jpg). At the
    extremes, when ![Principles of reinforcement learning (RL)](img/B08956_09_015.jpg),
    only the immediate reward matters. When ![Principles of reinforcement learning
    (RL)](img/B08956_09_016.jpg) future rewards have the same weight as the immediate
    reward.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回报被定义为折扣累积奖励，![强化学习原理](img/B08956_09_012.jpg)。从*方程 9.1.1*可以观察到，未来的奖励相较于即时奖励权重较低，因为通常情况下![强化学习原理](img/B08956_09_013.jpg)，其中![强化学习原理](img/B08956_09_014.jpg)。在极端情况下，当![强化学习原理](img/B08956_09_015.jpg)，只有即时奖励才重要；当![强化学习原理](img/B08956_09_016.jpg)时，未来的奖励和即时奖励有相同的权重。
- en: 'Return can be interpreted as a measure of the *value* of a given state by following
    an arbitrary policy, ![Principles of reinforcement learning (RL)](img/B08956_09_017.jpg):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回报可以被解释为通过跟随某一任意策略，*某一状态*的*价值*，![强化学习原理](img/B08956_09_017.jpg)：
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_018.jpg) (Equation
    9.1.2)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习原理](img/B08956_09_018.jpg)（方程 9.1.2）'
- en: 'To put the RL problem in another way, the goal of the agent is to learn the
    optimal policy that maximizes ![Principles of reinforcement learning (RL)](img/B08956_09_019.jpg)
    for all states *s*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 换个角度来看强化学习（RL）问题，智能体的目标是学习最优策略，以最大化所有状态*s*的![强化学习原理](img/B08956_09_019.jpg)：
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_020.jpg) (Equation
    9.1.3)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习原理](img/B08956_09_020.jpg)（方程 9.1.3）'
- en: The value function of the optimal policy is simply *V**. In *Figure 9.1.1*,
    the optimal policy is the one that generates the shortest sequence of actions
    that brings the robot closer and closer to the soda can until it has been fetched.
    The closer the state is to the goal state, the higher its value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略的价值函数简单地表示为*V**。在*图 9.1.1*中，最优策略是产生最短行动序列的策略，这个序列使得机器人越来越接近苏打罐，直到将其抓取。状态距离目标状态越近，其价值越高。
- en: 'The sequence of events leading to the goal (or terminal state) can be modeled
    as the *trajectory* or *rollout* of the policy:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 导致目标（或终止状态）事件序列可以被建模为策略的*轨迹*或*展开*：
- en: '*Trajectory* = (s0a0r1s1,s1a1r2s2,...,s*T*-1a*T*-1*r* *T* *s*[t]) (Equation
    9.1.4)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*轨迹* = (s0a0r1s1,s1a1r2s2,...,s*T*-1a*T*-1*r* *T* *s*[t]) （方程 9.1.4）'
- en: If the MDP is *episodic* when the agent reaches the terminal state, *s*[T'],
    the state is reset to *s*[0]. If *T* is finite, we have a finite *horizon*. Otherwise,
    the horizon is infinite. In *Figure 9.1.1*, if the MDP is episodic, after collecting
    the soda can, the robot may look for another soda can to pick up and the RL problem
    repeats.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果MDP是*有终止的*，当代理到达终止状态*s*[T']时，状态会被重置为*s*[0]。如果*T*是有限的，我们就有一个有限的*视野*。否则，视野是无限的。在*图9.1.1*中，如果MDP是有终止的，那么在收集完可乐罐后，机器人可能会寻找另一个可乐罐来捡起来，RL问题就会重复。
- en: The Q value
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q值
- en: 'An important question is that if the RL problem is to find ![The Q value](img/B08956_09_022.jpg),
    how does the agent learn by interacting with the environment? *Equation * *9.1.3*
    does not explicitly indicate the action to try and the succeeding state to compute
    the return. In RL, we find that it''s easier to learn ![The Q value](img/B08956_09_023.jpg)
    by using the *Q* value:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的问题是，如果RL问题是找到![Q值](img/B08956_09_022.jpg)，那么代理通过与环境交互是如何学习的？*方程* *9.1.3*并没有明确指出要尝试的动作和计算回报的下一个状态。在RL中，我们发现通过使用*Q*值来学习![Q值](img/B08956_09_023.jpg)会更容易：
- en: '![The Q value](img/B08956_09_024.jpg) (Equation 9.2.1)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Q值](img/B08956_09_024.jpg)（方程9.2.1）'
- en: 'Where:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![The Q value](img/B08956_09_025.jpg) (Equation 9.2.2)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Q值](img/B08956_09_025.jpg)（方程9.2.2）'
- en: In other words, instead of finding the policy that maximizes the value for all
    states, *Equation 9.2.1* looks for the action that maximizes the quality (*Q*)
    value for all states. After finding the *Q* value function, *V** and hence ![The
    Q value](img/B08956_09_026.jpg) are determined by *Equation 9.2.2* and *9.1.3*
    respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*方程9.2.1*不是寻找最大化所有状态的值的策略，而是寻找最大化所有状态的质量（*Q*）值的动作。在找到*Q*值函数之后，*V*值和因此得出的![Q值](img/B08956_09_026.jpg)分别由*方程9.2.2*和*9.1.3*确定。
- en: 'If for every action, the reward and the next state can be observed, we can
    formulate the following iterative or trial and error algorithm to learn the *Q*
    value:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于每个动作，都能观察到奖励和下一个状态，我们可以制定以下迭代或试错算法来学习*Q*值：
- en: '![The Q value](img/B08956_09_027.jpg) (Equation 9.2.3)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Q值](img/B08956_09_027.jpg)（方程9.2.3）'
- en: For notational simplicity, both *s* *'* and *a* *'* are the next state and action
    respectively. *Equation 9.2.3* is known as the **Bellman Equation** which is the
    core of the Q-Learning algorithm. Q-Learning attempts to approximate the first-order
    expansion of return or value (*Equation 9.1.2*) as a function of both current
    state and action.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，*s* *'* 和 *a* *'* 分别表示下一个状态和动作。*方程9.2.3*被称为**贝尔曼方程**，它是Q学习算法的核心。Q学习尝试将回报或价值的一级展开（*方程9.1.2*）近似为当前状态和动作的函数。
- en: From zero knowledge of the dynamics of the environment, the agent tries an action
    *a*, observes what happens in the form of reward, *r*, and next state, *s* *'*.
    ![The Q value](img/B08956_09_028.jpg) chooses the next logical action that will
    give the maximum *Q* value for the next state. With all terms in *Equation* *9.2.3*
    known, the *Q* value for that current state-action pair is updated. Doing the
    update iteratively will eventually learn the *Q* value function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从对环境动态的零知识开始，代理尝试一个动作*a*，以奖励*r*和下一个状态*s* *'的形式观察发生了什么。![Q值](img/B08956_09_028.jpg)选择下一个合乎逻辑的动作，从而为下一个状态提供最大的*Q*值。所有在*方程*
    *9.2.3*中已知的项，当前状态-动作对的*Q*值就被更新。通过迭代地进行更新，最终会学习到*Q*值函数。
- en: Q-Learning is an *off-policy* RL algorithm. It learns to improve the policy
    by not directly sampling experiences from that policy. In other words, the *Q*
    values are learned independently of the underlying policy being used by the agent.
    When the *Q* value function has converged, only then is the optimal policy determined
    using *Equation* *9.2.1*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种*脱离策略*的RL算法。它通过不直接从该策略中采样经验来学习改进策略。换句话说，*Q*值是独立于代理使用的底层策略进行学习的。当*Q*值函数收敛时，只有通过*方程*
    *9.2.1*才能确定最优策略。
- en: Before giving an example on how to use Q-Learning, we should note that the agent
    must continually explore its environment while gradually taking advantage of what
    it has learned so far. This is one of the issues in RL – finding the right balance
    between *Exploration* and *Exploitation*. Generally, during the start of learning,
    the action is random (exploration). As the learning progresses, the agent takes
    advantage of the *Q* value (exploitation). For example, at the start, 90% of the
    action is random and 10% from *Q* value function, and by the end of each episode,
    this is gradually decreased. Eventually, the action is 10% random and 90% from
    *Q* value function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在给出 Q-Learning 的使用示例之前，我们需要注意，智能体必须不断探索其环境，同时逐渐利用到目前为止所学到的内容。这是强化学习中的一个问题——如何在
    *探索* 和 *利用* 之间找到正确的平衡。通常，在学习初期，动作是随机的（探索）。随着学习的进行，智能体利用 *Q* 值（利用）。例如，刚开始时，90%
    的动作是随机的，10% 是基于 *Q* 值函数的，而在每一轮结束时，这个比例逐渐减小。最终，动作是 10% 随机的，90% 基于 *Q* 值函数。
- en: Q-Learning example
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-Learning 示例
- en: 'To illustrate the Q-Learning algorithm, we need to consider a simple deterministic
    environment, as shown in the following figure. The environment has six states.
    The rewards for allowed transitions are shown. The reward is non-zero in two cases.
    Transition to the **Goal** (**G**) state has +100 reward while moving into **Hole**
    (**H**) state has -100 reward. These two states are terminal states and constitute
    the end of one episode from the **Start** state:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 Q-Learning 算法，我们需要考虑一个简单的确定性环境，如下图所示。环境中有六个状态。允许转移的奖励如图所示。只有在两种情况下，奖励才为非零。转移到
    **目标** (**G**) 状态时奖励 +100，而进入 **洞** (**H**) 状态时奖励为 -100。这两个状态是终止状态，构成从 **起始**
    状态结束一个回合的条件：
- en: '![Q-Learning example](img/B08956_09_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_02.jpg)'
- en: 'Figure 9.3.1: Rewards in a simple deterministic world'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.1：简单确定性世界中的奖励
- en: 'To formalize the identity of each state, we need to use a (*row*, *column*)
    identifier as shown in the following figure. Since the agent has not learned anything
    yet about its environment, the Q-Table also shown in the following figure has
    zero initial values. In this example, the discount factor, ![Q-Learning example](img/B08956_09_029.jpg).
    Recall that in the estimate of current *Q* value, the discount factor determines
    the weight of future *Q* values as a function of the number of steps, ![Q-Learning
    example](img/B08956_09_030.jpg). In *Equation* *9.2.3*, we only consider the immediate
    future *Q* value, *k* = 1:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正式化每个状态的标识，我们需要使用（*行*，*列*）标识符，如下图所示。由于智能体尚未了解其环境，因此下图中显示的 Q-表的初始值为零。在此示例中，折扣因子，![Q-Learning
    示例](img/B08956_09_029.jpg)。回顾一下，在当前 *Q* 值的估算中，折扣因子决定了未来 *Q* 值的权重，权重是根据步数的函数，![Q-Learning
    示例](img/B08956_09_030.jpg)。在 *方程式* *9.2.3* 中，我们只考虑了即时的未来 *Q* 值，*k* = 1：
- en: '![Q-Learning example](img/B08956_09_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_03.jpg)'
- en: 'Figure 9.3.2: States in the simple deterministic environment and the agent''s
    initial Q-Table'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.2：简单确定性环境中的状态和智能体的初始 Q-表
- en: Initially, the agent assumes a policy that selects a random action 90% of the
    time and exploits the Q-Table 10% of the time. Suppose the first action is randomly
    chosen and indicates a move in the right direction. *Figure 9.3.3* illustrates
    the computation of the new *Q* value of state (0, 0) for a move to the right action.
    The next state is (0, 1). The reward is 0, and the maximum of all the next state's
    *Q* values is zero. Therefore, the *Q* value of state (0, 0) for a move to the
    right action remains 0.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，智能体假设采取的策略是 90% 的时间选择随机动作，10% 的时间利用 Q-表。假设第一次动作是随机选择的，并且表示朝正确方向移动。*图 9.3.3*
    展示了向右移动动作中，状态 (0, 0) 的新 *Q* 值的计算。下一个状态是 (0, 1)。奖励为 0，下一个状态的所有 *Q* 值的最大值是零。因此，状态
    (0, 0) 在向右移动动作中的 *Q* 值保持为 0。
- en: 'To easily track the initial state and next state, we use different shades of
    gray on both the environment and the Q-Table–lighter gray for initial state and
    darker gray for the next state. In choosing the next action for the next state,
    the candidate actions are in the thicker border:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于追踪初始状态和下一个状态，我们在环境和 Q-表中使用不同的灰度阴影——初始状态使用较浅的灰色，下一状态使用较深的灰色。在为下一状态选择动作时，候选动作的边框较粗：
- en: '![Q-Learning example](img/B08956_09_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_04.jpg)'
- en: 'Figure 9.3.3: Assuming the action taken by the agent is a move to the right,
    the update on Q value of state (0, 0) is shown'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.3：假设智能体采取的动作是向右移动，显示了状态 (0, 0) 的 Q 值更新
- en: '![Q-Learning example](img/B08956_09_05.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_05.jpg)'
- en: 'Figure 9.3.4: Assuming the action chosen by the agent is move down, the update
    on Q value of state (0, 1) is shown'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.4：假设智能体选择的动作是向下移动，状态 (0, 1) 的 Q 值更新如图所示。
- en: '![Q-Learning example](img/B08956_09_06.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_06.jpg)'
- en: 'Figure 9.3.5: Assuming the action chosen by the agent is a move to the right,
    the update on Q value of state (1, 1) is shown'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.5：假设智能体选择的动作是向右移动，状态 (1, 1) 的 Q 值更新如图所示。
- en: Let's suppose that the next randomly chosen action is move down. *Figure 9.3.4*
    shows no change in the *Q* value of state (0, 1) for the move down action. In
    *Figure 9.3.5*, the agent's third random action is a move to the right. It encountered
    the **H** and received a -100 reward. This time, the update is non-zero. The new
    *Q* value for the state (1, 1) is -100 for the move to the right direction. One
    episode has just finished, and the agent returns to the **Start** state.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设下一步随机选择的动作是向下移动。*图 9.3.4* 显示向下移动时，状态 (0, 1) 的 *Q* 值没有变化。在 *图 9.3.5* 中，智能体的第三个随机动作是向右移动。它遇到了
    **H** 状态并收到了 -100 的惩罚。这次更新非零。状态 (1, 1) 向右移动的 *Q* 值为 -100。一次试验刚刚结束，智能体返回到 **Start**
    状态。
- en: '![Q-Learning example](img/B08956_09_07.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_07.jpg)'
- en: 'Figure 9.3.6: Assuming the actions chosen by the agent are two successive moves
    to the right, the update on Q value of state (0, 1) is shown'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.6：假设智能体选择的动作是连续两步向右移动，状态 (0, 1) 的 Q 值更新如图所示。
- en: Let's suppose the agent is still in the exploration mode as shown in *Figure
    9.3.6*. The first step it took for the second episode is a move to the right.
    As expected, the update is 0\. However, the second random action it chose is also
    move to the right. The agent reached the **G** state and received a big +100 reward.
    The *Q* value for the state (0, 1) move to the right becomes 100\. The second
    episode is done, and the agent goes back to the **Start** state.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设智能体仍处于探索模式，如 *图 9.3.6* 所示。它在第二次试验的第一步选择了向右移动。正如预期的那样，更新值为 0。然而，它选择的第二个随机动作仍然是向右移动。智能体达到了
    **G** 状态并获得了 +100 的奖励。状态 (0, 1) 向右移动的 *Q* 值变为 100。第二次试验完成，智能体返回到 **Start** 状态。
- en: '![Q-Learning example](img/B08956_09_08.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_08.jpg)'
- en: 'Figure 9.3.7: Assuming the action chosen by the agent is a move to the right,
    the update on Q value of state (0, 0) is shown'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.7：假设智能体选择的动作是向右移动，状态 (0, 0) 的 Q 值更新如图所示。
- en: '![Q-Learning example](img/B08956_09_09.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 示例](img/B08956_09_09.jpg)'
- en: 'Figure 9.3.8: In this instance, the agent''s policy decided to exploit the
    Q-Table to determine the action at states (0, 0) and (0, 1). The Q-Table suggests
    to move to the right for both states.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.8：在此情况下，智能体的策略决定利用 Q 表来确定状态 (0, 0) 和 (0, 1) 的动作。Q 表建议两个状态都向右移动。
- en: At the beginning of the third episode, the random action taken by the agent
    is a move to the right. The *Q* value of state (0, 0) is now updated with a non-zero
    value because the next state's possible actions have 100 as the maximum *Q* value.
    *Figure 9.3.7* shows the computation involved. The *Q* value of the next state
    (0, 1) ripples back to the earlier state (0, 0). It is like giving credit to the
    earlier states that helped in finding the **G** state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三次试验开始时，智能体随机选择的动作是向右移动。状态 (0, 0) 的 *Q* 值现在被更新为非零值，因为下一状态的可能动作的最大 *Q* 值为 100。*图
    9.3.7* 展示了相关的计算过程。下一状态 (0, 1) 的 *Q* 值回传到之前的状态 (0, 0)，这就像是对帮助找到 **G** 状态的早期状态进行的奖励。
- en: The progress in Q-Table has been substantial. In fact, in the next episode,
    if for some reason the policy decided to exploit the Q-Table instead of randomly
    exploring the environment, the first action is to move to the right according
    to the computation in *Figure 9.3.8*. In the first row of the Q-Table, the action
    that results in maximum *Q* value is a move to the right. For the next state (0,
    1), the second row of Q-Table suggests that the next action is still to move to
    the right. The agent has successfully reached the goal. The policy guided the
    agent on the right set of actions to achieve its goal.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q 表中的进展是显著的。事实上，在下一次试验中，如果出于某种原因，策略决定利用 Q 表而不是随机探索环境，那么根据 *图 9.3.8* 中的计算，第一步将是向右移动。在
    Q 表的第一行中，导致最大 *Q* 值的动作是向右移动。对于下一状态 (0, 1)，Q 表的第二行建议下一个动作仍然是向右移动。智能体成功地达到了目标。策略引导智能体采取正确的动作集来实现其目标。
- en: If the Q-Learning algorithm continues to run indefinitely, the Q-Table will
    converge. The assumptions for convergence are the RL problem must be deterministic
    MDP with bounded rewards and all states are visited infinitely often.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Q-Learning 算法继续无限运行，Q-表格将会收敛。收敛的假设条件是 RL 问题必须是确定性 MDP，奖励有界，并且所有状态都被无限次访问。
- en: Q-Learning in Python
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 中的 Q-Learning
- en: The environment and the Q-Learning discussed in the previous section can be
    implemented in Python. Since the policy is just a simple table, there is, at this
    point in time no need for Keras. *Listing* *9.3.1* shows `q-learning-9.3.1.py`,
    the implementation of the simple deterministic world (environment, agent, action,
    and Q-Table algorithms) using the `QWorld` class. For conciseness, the functions
    dealing with the user interface are not shown.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 环境和上一节讨论的 Q-Learning 可以在 Python 中实现。由于策略只是一个简单的表格，因此目前不需要 Keras。*列表* *9.3.1*
    显示了 `q-learning-9.3.1.py`，这是使用 `QWorld` 类实现简单确定性世界（环境、智能体、动作和 Q-表格算法）的代码。为了简洁起见，处理用户界面的函数未显示。
- en: In this example, the environment dynamics is represented by `self.transition_table`.
    At every action, `self.transition_table` determines the next state. The reward
    for executing an action is stored in `self.reward_table`. The two tables are consulted
    every time an action is executed by the `step()` function. The Q-Learning algorithm
    is implemented by `update_q_table()` function. Every time the agent needs to decide
    which action to take, it calls the `act()` function. The action may be randomly
    drawn or decided by the policy using the Q-Table. The percent chance that the
    action chosen is random is stored in the `self.epsilon` variable which is updated
    by `update_epsilon()` function using a fixed `epsilon_decay`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，环境动态通过 `self.transition_table` 表示。在每次执行动作时，`self.transition_table` 决定下一个状态。执行动作的奖励存储在
    `self.reward_table` 中。每次执行动作时，这两个表格都会被查询。Q-Learning 算法通过 `update_q_table()` 函数实现。每当智能体需要决定执行哪一个动作时，它会调用
    `act()` 函数。动作可以是随机选择的，也可以是通过 Q-表格的策略决定的。选择随机动作的概率存储在 `self.epsilon` 变量中，并通过 `update_epsilon()`
    函数使用固定的 `epsilon_decay` 进行更新。
- en: 'Before executing the code in *Listing* *9.3.1*, we need to run:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 *列表* *9.3.1* 中的代码之前，我们需要运行：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To install `termcolor` package. This package helps in visualizing text outputs
    on the Terminal.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `termcolor` 包。该包有助于在终端中可视化文本输出。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The complete code can be found on GitHub at: [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在 GitHub 上找到：[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)。
- en: 'Listing 9.3.1, `q-learning-9.3.1.py`. A simple deterministic MDP with six states:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3.1，`q-learning-9.3.1.py`。一个简单的确定性 MDP，包含六个状态：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 9.3.2, `q-learning-9.3.1.py`. The main Q-Learning loop. The agent''s
    Q-Table is updated every state, action, reward, and next state iteration:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3.2，`q-learning-9.3.1.py`。主要的 Q-Learning 循环。每次状态、动作、奖励和下一个状态的迭代都会更新智能体的
    Q-表格：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The perception-action-learning loop is illustrated in *Listing* *9.3.2*. At
    every episode, the environment resets to the *Start* state. The action to execute
    is chosen and applied to the environment. The reward and next state are observed
    and used to update the Q-Table. The episode is completed (`done = True`) upon
    reaching the *Goal* or *Hole* state. For this example, the Q-Learning runs for
    100 episodes or 10 wins, whichever comes first. Due to the decrease in the value
    of the `self.epsilon` variable at every episode, the agent starts to favor exploitation
    of Q-Table to determine the action to perform given a state. To see the Q-Learning
    simulation we simply need to run:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 感知-动作-学习循环在 *列表* *9.3.2* 中有所说明。在每一轮中，环境重置为 *开始* 状态。选择并应用要执行的动作到环境中。观察到奖励和下一个状态，并用于更新
    Q-表格。当达到 *目标* 或 *洞* 状态时，回合结束（`done = True`）。对于此示例，Q-Learning 执行 100 轮或 10 次胜利，以先达到的为准。由于
    `self.epsilon` 变量在每一轮中减少，智能体开始倾向于利用 Q-表格来确定给定状态下的执行动作。为了查看 Q-Learning 模拟，我们只需要运行：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Q-Learning in Python](img/B08956_09_10.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Python 中的 Q-Learning](img/B08956_09_10.jpg)'
- en: 'Figure 9.3.9: A screenshot showing the Q-Table after 2000 wins of the agent'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.9：显示智能体经过 2000 次胜利后的 Q-表格截图
- en: 'The preceding figure shows the screenshot if `maxwins = 2000` (2000*x* *Goal*
    state is reached) and `delay = 0` (to see the final Q-Table only) by running:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了在运行时 `maxwins = 2000`（达到 2000*x* *目标* 状态）和 `delay = 0`（仅查看最终 Q-表格）的截图：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Q-Table has converged and shows the logical action that the agent can take
    given a state. For example, in the first row or state (0, 0), the policy advises
    move to the right. The same for the state (0, 1) on the second row. The second
    action reaches the *Goal* state. The `scores` variable dump shows that the minimum
    number of steps taken decreases as the agent gets correct actions from the policy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Table 已经收敛，并显示了在给定状态下代理可以采取的逻辑行动。例如，在第一行或状态 (0, 0) 中，策略建议向右移动。第二行的状态 (0, 1)
    也是如此。第二个动作到达 *目标* 状态。`scores` 变量转储显示，随着代理从策略中获得正确的动作，采取的最小步骤数逐渐减少。
- en: 'From *Figure 9.3.9*, we can compute the value of each state from *Equation
    9.2.2*, ![Q-Learning in Python](img/B08956_09_031.jpg). For example, for state
    (0, 0), *V**(*s*) = max(81.0,72.9,90.0,81.0) = 90.0\. Following figure shows the
    value for each state:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 9.3.9* 中，我们可以通过 *方程式 9.2.2* 来计算每个状态的值，![Q-Learning 在 Python 中](img/B08956_09_031.jpg)。例如，对于状态
    (0, 0)，*V**(*s*) = max(81.0,72.9,90.0,81.0) = 90.0。以下图显示了每个状态的值：
- en: '![Q-Learning in Python](img/B08956_09_11.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning 在 Python 中](img/B08956_09_11.jpg)'
- en: 'Figure 9.3.10: The value for each state from Figure 9.3.9 and Equation9.2.2'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3.10：来自图 9.3.9 和方程式 9.2.2 的每个状态的值
- en: Nondeterministic environment
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非确定性环境
- en: 'In the event that the environment is nondeterministic, both the reward and
    action are probabilistic. The new system is a stochastic MDP. To reflect the nondeterministic
    reward the new value function is:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境是非确定性的，那么奖励和动作都是概率性的。新系统是一个随机的 MDP。为了反映非确定性奖励，新的值函数为：
- en: '![Nondeterministic environment](img/B08956_09_032.jpg) (Equation 9.4.1)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![非确定性环境](img/B08956_09_032.jpg)（方程式 9.4.1）'
- en: 'The Bellman equation is modified as:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程被修改为：
- en: '![Nondeterministic environment](img/B08956_09_033.jpg) (Equation 9.4.2)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![非确定性环境](img/B08956_09_033.jpg)（方程式 9.4.2）'
- en: Temporal-difference learning
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差学习
- en: 'Q-Learning is a special case of a more generalized **Temporal-Difference Learning**
    or **TD-Learning** ![Temporal-difference learning](img/B08956_09_034.jpg). More
    specifically, it''s a special case of one-step TD-Learning *TD*(0):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Learning 是一种更广泛的 **时间差学习** 或 **TD-Learning** 的特例 ![时间差学习](img/B08956_09_034.jpg)。更具体来说，它是一步
    TD-Learning *TD*(0) 的特例：
- en: '![Temporal-difference learning](img/B08956_09_035.jpg) (Equation 9.5.1)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![时间差学习](img/B08956_09_035.jpg)（方程式 9.5.1）'
- en: In the equation ![Temporal-difference learning](img/B08956_09_036.jpg) is the
    learning rate. We should note that when ![Temporal-difference learning](img/B08956_09_037.jpg),
    *Equation* *9.5.1* is similar to the Bellman equation. For simplicity, we'll refer
    to *Equation* *9.5.1* as Q-Learning or generalized Q-Learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程式中 ![时间差学习](img/B08956_09_036.jpg) 是学习率。我们应该注意，当 ![时间差学习](img/B08956_09_037.jpg)
    时，*方程式* *9.5.1* 类似于贝尔曼方程。为简便起见，我们将 *方程式* *9.5.1* 称为 Q-Learning 或广义 Q-Learning。
- en: 'Previously, we referred to Q-Learning as an off-policy RL algorithm since it
    learns the Q value function without directly using the policy that it is trying
    to optimize. An example of an *on-policy* one-step TD-learning algorithm is SARSA
    which similar to *Equation* *9.5.1*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们将 Q-Learning 称为一种离策略 RL 算法，因为它在不直接使用正在优化的策略的情况下学习 Q 值函数。一种 *在策略* 的一步 TD-Learning
    算法的示例是 SARSA，类似于 *方程式* *9.5.1*：
- en: '![Temporal-difference learning](img/B08956_09_038.jpg) (Equation 9.5.2)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![时间差学习](img/B08956_09_038.jpg)（方程式 9.5.2）'
- en: The main difference is the use of the policy that is being optimized to determine
    *a**'*. The terms *s*, *a*, *r*, *s**'* and *a**'* (thus the name SARSA) must
    be known to update the *Q* value function at every iteration. Both Q-Learning
    and SARSA use existing estimates in the *Q* value iteration, a process known as
    **bootstrapping**. In bootstrapping, we update the current *Q* value estimate
    from the reward and the subsequent *Q* value estimate(s).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于使用正在优化的策略来确定 *a**'*。术语 *s*、*a*、*r*、*s**'* 和 *a**'*（因此称为 SARSA）必须已知，以便在每次迭代时更新
    *Q* 值函数。Q-Learning 和 SARSA 都在 *Q* 值迭代中使用现有的估计，这一过程称为 **自举法**。在自举法中，我们通过奖励和随后的
    *Q* 值估计来更新当前的 *Q* 值估计。
- en: Q-Learning on OpenAI gym
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI gym 上的 Q-Learning
- en: Before presenting another example, there appears to be a need for a suitable
    RL simulation environment. Otherwise, we can only run RL simulations on very simple
    problems like in the previous example. Fortunately, OpenAI created **Gym**, [https://gym.openai.com](https://gym.openai.com).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示另一个示例之前，似乎需要一个合适的 RL 模拟环境。否则，我们只能在非常简单的问题上运行 RL 模拟，就像前面的示例一样。幸运的是，OpenAI
    创建了 **Gym**，[https://gym.openai.com](https://gym.openai.com)。
- en: 'The gym is a toolkit for developing and comparing RL algorithms. It works with
    most deep learning libraries, including Keras. The gym can be installed by running
    the following command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Gym 是一个开发和比较强化学习算法的工具包。它可以与大多数深度学习库（包括 Keras）一起使用。可以通过运行以下命令来安装 Gym：
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The gym has several environments where an RL algorithm can be tested against
    such as toy text, classic control, algorithmic, Atari, and 2D/3D robots. For example,
    `FrozenLake-v0` (*Figure 9.5.1*) is a toy text environment similar to the simple
    deterministic world used in the Q-Learning in Python example. `FrozenLake-v0`
    has 12 states. The state marked **S** is the starting state, **F** is the frozen
    part of the lake which is safe, **H** is the Hole state that should be avoided,
    and **G** is the Goal state where the frisbee is. The reward is +1 for transitioning
    to the Goal state. For all other states, the reward is zero.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Gym 提供了多个可以用来测试强化学习算法的环境，比如玩具文本、经典控制、算法、Atari 和 2D/3D 机器人等。例如，`FrozenLake-v0`
    (*图 9.5.1*) 是一个玩具文本环境，类似于 Python 示例中的简单确定性世界。`FrozenLake-v0` 有 12 个状态。标记为 **S**
    的是起始状态，**F** 是冰冻的湖面，安全的区域，**H** 是应该避免的洞穴状态，**G** 是目标状态，飞盘的位置。转移到目标状态的奖励为 +1，其他所有状态的奖励为
    0。
- en: 'In `FrozenLake-v0`, there are also four available actions (Left, Down, Right,
    Up) known as action space. However, unlike the simple deterministic world earlier,
    the actual movement direction is only partially dependent on the chosen action.
    There are two variations of the `FrozenLake-v0` environment, slippery and non-slippery.
    As expected, the slippery mode is more challenging:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `FrozenLake-v0` 中，还有四个可用的动作（左、下、右、上），称为动作空间。然而，与之前的简单确定性世界不同，实际的移动方向仅部分依赖于所选的动作。`FrozenLake-v0`
    环境有两种变化：滑动和非滑动。如预期，滑动模式更具挑战性。
- en: '![Q-Learning on OpenAI gym](img/B08956_09_12.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI gym 中的 Q 学习](img/B08956_09_12.jpg)'
- en: 'Figure 9.5.1: Frozen lake environment in OpenAI Gym'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5.1：OpenAI Gym 中的冰冻湖环境
- en: An action applied on `FrozenLake-v0` returns the observation (equivalent to
    the next state), reward, done (whether the episode is finished), and a dictionary
    of debugging information. The observable attributes of the environment, known
    as observation space, are captured by the returned observation object.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `FrozenLake-v0` 上应用一个动作会返回观察（相当于下一个状态）、奖励、完成（回合是否结束）以及调试信息的字典。环境的可观察属性，称为观察空间，通过返回的观察对象来捕捉。
- en: The generalized Q-Learning can be applied to the `FrozenLake-v0` environment.
    *Table 9.5.1* shows the improvement in performance of both slippery and non-slippery environments.
    A method of measuring the performance of the policy is the percent of episodes
    executed that resulted in reaching the Goal state. The higher is the percentage,
    the better. From the baseline of pure exploration (random action) of about 1.5%,
    the policy can achieve ~76% Goal state for non-slippery and ~71% for the slippery
    environment. As expected, it is harder to control the slippery environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 广义 Q 学习可以应用于 `FrozenLake-v0` 环境。*表 9.5.1* 显示了滑动和非滑动环境中性能的改进。衡量策略性能的一种方法是执行的回合中，达到目标状态的比例。这个比例越高，效果越好。从纯探索（随机动作）约
    1.5% 的基线开始，策略可以在非滑动环境中达到 ~76% 的目标状态，而在滑动环境中则为 ~71%。如预期，控制滑动环境更具挑战性。
- en: The code can still be implemented in Python and NumPy since it only requires
    a Q-Table. *Listing* *9.5.1* shows the implementation of the `QAgent` class while
    listing *9.5.2* demonstrates the agent's perception-action-learning loop. Apart
    from using `FrozenLake-v0` environment from OpenAI Gym, the most important change
    is the implementation of the generalized Q-Learning as defined by *Equation* *9.5.1*
    in the `update_q_table()` function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只需要一个 Q 表，代码仍然可以在 Python 和 NumPy 中实现。*列表* *9.5.1* 展示了 `QAgent` 类的实现，而 *列表*
    *9.5.2* 展示了智能体的感知-行动-学习循环。除了使用 OpenAI Gym 中的 `FrozenLake-v0` 环境外，最重要的变化是实现了由 *方程式*
    *9.5.1* 所定义的广义 Q 学习，该实现位于 `update_q_table()` 函数中。
- en: The `qagent` object can operate in either slippery or non-slippery mode. The
    agent is trained for 40,000 iterations. After training, the agent can exploit
    the Q-Table to choose the action to execute given any policy as shown in the test
    mode of *Table 9.5.1*. There is a huge performance boost in using the learned
    policy as demonstrated in *Table 9.5.1*. With the use of the gym, a lot of the
    code in constructing the environment is gone.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`qagent` 对象可以在滑动或非滑动模式下操作。该代理经过 40,000 次迭代训练。训练后，代理可以利用 Q-表来选择执行任何策略下的动作，如
    *表 9.5.1* 的测试模式所示。使用学习到的策略后，性能大幅提升，正如 *表 9.5.1* 中所示。通过使用 gym，构建环境的许多代码都被简化了。'
- en: 'This will help us to focus on building a working RL algorithm. To run the code
    in slow motion or delay of 1 sec per action:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助我们专注于构建一个有效的强化学习（RL）算法。要使代码以慢动作或每个动作延迟 1 秒运行：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '| Mode | Run | Approx % Goal |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 运行 | 目标近似百分比 |'
- en: '| --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Train non-slippery |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 训练非滑动 |'
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '| 26.0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 26.0 |'
- en: '| Test non-slippery |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 测试非滑动 |'
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '| 76.0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 76.0 |'
- en: '| Pure random action non-slippery |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 纯随机非滑动动作 |'
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '| 1.5 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1.5 |'
- en: '| Train slippery |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 训练滑动 |'
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '| 26 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 26 |'
- en: '| Test slippery |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 测试滑动 |'
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '| 71.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 71.0 |'
- en: '| Pure random slippery |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 纯随机滑动 |'
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '| 1.5 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 1.5 |'
- en: 'Table 9.5.1: Baseline and performance of generalized Q-Learning on the FrozenLake-v0
    environment with learning rate = 0.5'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表 9.5.1：在 `FrozenLake-v0` 环境上使用学习率 = 0.5 的广义 Q-Learning 的基准和性能
- en: 'Listing 9.5.1, `q-frozenlake-9.5.1.py` shows the implementation of Q-Learning
    on `FrozenLake-v0` environment:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5.1，`q-frozenlake-9.5.1.py` 显示了在 `FrozenLake-v0` 环境中实现 Q-Learning：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Listing 9.5.2, `q-frozenlake-9.5.1.py`. The main Q-Learning loop for the `FrozenLake-v0`
    environment:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5.2，`q-frozenlake-9.5.1.py`。`FrozenLake-v0` 环境的主要 Q-Learning 循环：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Deep Q-Network (DQN)
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q-网络 (DQN)
- en: Using the Q-Table to implement Q-Learning is fine in small discrete environments.
    However, when the environment has numerous states or continuous as in most cases,
    a Q-Table is not feasible or practical. For example, if we are observing a state
    made of four continuous variables, the size of the table is infinite. Even if
    we attempt to discretize the four variables into 1000 values each, the total number
    of rows in the table is a staggering 1000⁴ = 1*e*^(12). Even after training, the
    table is sparse - most of the cells in this table are zero.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在小型离散环境中，使用 Q-表来实现 Q-Learning 是可行的。然而，当环境有大量状态或像大多数情况那样是连续的时，Q-表就不再可行或实际。例如，如果我们观察的是由四个连续变量组成的状态，那么表的大小是无限的。即使我们尝试将这四个变量每个离散化为
    1000 个值，表中行的总数将是令人吃惊的 1000⁴ = 1*e*^(12)。即使在训练后，表仍然是稀疏的——表中的大多数单元格都是零。
- en: 'A solution to this problem is called DQN [2] which uses a deep neural network
    to approximate the Q-Table. As shown in *Figure 9.6.1*. There are two approaches
    to build the Q-network:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法被称为 DQN [2]，它使用深度神经网络来逼近 Q-表。如 *图 9.6.1* 所示。构建 Q-网络有两种方法：
- en: The input is the state-action pair, and the prediction is the *Q* value
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入是状态-动作对，预测是 *Q* 值
- en: The input is the state, and the prediction is the *Q* value for each action
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入是状态，预测是每个动作的*Q*值
- en: The first option is not optimal since the network will be called a number of
    times equal to the number of actions. The second is the preferred method. The
    Q-Network is called only once.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项不是最优的，因为网络将根据动作的数量被调用若干次。第二个是首选方法。Q-网络仅被调用一次。
- en: 'The most desirable action is simply the action with the biggest *Q* value:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最理想的动作是拥有最大 *Q* 值的动作：
- en: '![Deep Q-Network (DQN)](img/B08956_09_13.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![深度 Q-网络 (DQN)](img/B08956_09_13.jpg)'
- en: 'Figure 9.6.1: A Deep Q-Network'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6.1：深度 Q-网络
- en: 'The data required to train the Q-Network come from the agent''s experiences:
    ![Deep Q-Network (DQN)](img/B08956_09_039.jpg). Each training sample is a unit
    of experience ![Deep Q-Network (DQN)](img/B08956_09_040.jpg). At a given state
    at timestep *t*, *s* = *s*[t], the action, *a* = a[t], is determined using the
    Q-Learning algorithm similar to the previous section:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 Q-网络所需的数据来自代理的经验：![深度 Q-网络 (DQN)](img/B08956_09_039.jpg)。每个训练样本是一个经验单元！[深度
    Q-网络 (DQN)](img/B08956_09_040.jpg)。在给定的状态下，在时间步 *t*，*s* = *s*[t]，动作 *a* = a[t]
    是使用 Q-Learning 算法确定的，类似于前一节所述：
- en: '![Deep Q-Network (DQN)](img/B08956_09_041.jpg) (Equation 9.6.1)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![深度 Q-网络 (DQN)](img/B08956_09_041.jpg)（方程 9.6.1）'
- en: 'For notational simplicity, we omit the subscript and the use of the bold letter.
    We need to note that *Q*(*s*,*a*) is the Q-Network. Strictly speaking, it is *Q*(*a*|*s*)
    since the action is moved to the prediction as shown on the right of *Figure 9.6.1*.
    The action with the highest *Q* value is the action that is applied on the environment
    to get the reward, *r* = *r* [t+1], the next state, *s* *''* = *s*[t+1] and a
    Boolean `done` indicating if the next state is terminal. From *Equation* *9.5.1*
    on generalized Q-Learning, an MSE loss function can be determined by applying
    the chosen action:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，我们省略了下标和粗体字母的使用。需要注意的是，*Q*(*s*,*a*) 是 Q 网络。严格来说，它是 *Q*(*a*|*s*)，因为动作被移到预测中，如
    *图 9.6.1* 右侧所示。具有最高 *Q* 值的动作是应用于环境中的动作，以获得奖励 *r* = *r* [t+1]，下一个状态 *s* *'* = *s*[t+1]，以及一个布尔值
    `done`，表示下一个状态是否为终止状态。从 *方程* *9.5.1* 中的广义 Q 学习，可以通过应用所选动作确定 MSE 损失函数：
- en: '![Deep Q-Network (DQN)](img/B08956_09_042.jpg) (Equation 9.6.2)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![深度 Q 网络 (DQN)](img/B08956_09_042.jpg) （方程 9.6.2）'
- en: Where all terms are familiar from the previous discussion on Q-Learning and
    *Q*(*a*|*s*) → *Q*(*s*,*a*). The term ![Deep Q-Network (DQN)](img/B08956_09_043.jpg).
    In other words, using the Q-Network, predict the *Q* value of each action given
    next state and get the maximum among them. Note that at the terminal state *s'*,
    ![Deep Q-Network (DQN)](img/B08956_09_044.jpg).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中所有项都来自前面的 Q 学习讨论，*Q*(*a*|*s*) → *Q*(*s*,*a*)。项 ![深度 Q 网络 (DQN)](img/B08956_09_043.jpg)。换句话说，使用
    Q 网络预测给定下一个状态下每个动作的 *Q* 值，并从中选择最大值。注意，在终止状态 *s'*，![深度 Q 网络 (DQN)](img/B08956_09_044.jpg)。
- en: '**Algorithm 9.6.1, DQN algorithm:**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法 9.6.1，DQN 算法：**'
- en: '*Require*: Initialize replay memory *D* to capacity *N*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*要求*：将回放记忆 *D* 初始化至容量 *N*'
- en: '*Require*: Initialize action-value function *Q* with random weights ![Deep
    Q-Network (DQN)](img/B08956_09_045.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*要求*：用随机权重初始化动作值函数 *Q* ![深度 Q 网络 (DQN)](img/B08956_09_045.jpg)'
- en: '*Require*: Initialize target action-value function *Q*[*target*] with weights
    ![Deep Q-Network (DQN)](img/B08956_09_046.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*要求*：初始化目标动作值函数 *Q*[*target*]，并赋予权重 ![深度 Q 网络 (DQN)](img/B08956_09_046.jpg)'
- en: '*Require*: Exploration rate, ![Deep Q-Network (DQN)](img/B08956_09_047.jpg)
    and discount factor, ![Deep Q-Network (DQN)](img/B08956_09_048.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*要求*：探索率，![深度 Q 网络 (DQN)](img/B08956_09_047.jpg) 和折扣因子，![深度 Q 网络 (DQN)](img/B08956_09_048.jpg)'
- en: '`for` *episode* = 1, …,*M* `do:`'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`for` *episode* = 1, …,*M* `do:`'
- en: Given initial state *s*
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定初始状态 *s*
- en: '`for` *step* = 1,…, *T* `do`:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`for` *step* = 1,…, *T* `do`:'
- en: Choose action ![Deep Q-Network (DQN)](img/B08956_09_049.jpg)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作 ![深度 Q 网络 (DQN)](img/B08956_09_049.jpg)
- en: Execute action *a*, observe reward *r* and next state *s'*
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a*，观察奖励 *r* 和下一个状态 *s'*
- en: Store transition (*s*, *a*, *r*, *s**'*) in *D*
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转移 (*s*, *a*, *r*, *s**'*) 存储在 *D* 中
- en: Update the state, *s* = *s**'*
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态，*s* = *s**'*
- en: //experience replay
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: //经验回放
- en: Sample a mini batch of episode experiences (*s*[*j*], *a*[*j*], *r*[*j+1*],
    *s*[*j+1*]) from *D*
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *D* 中采样一个小批量的经验（*s*[*j*], *a*[*j*], *r*[*j+1*], *s*[*j+1*]）
- en: '![Deep Q-Network (DQN)](img/B08956_09_050.jpg)'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![深度 Q 网络 (DQN)](img/B08956_09_050.jpg)'
- en: Perform gradient descent step on ![Deep Q-Network (DQN)](img/B08956_09_051.jpg)with
    respect to parameters ![Deep Q-Network (DQN)](img/B08956_09_052.jpg)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 ![深度 Q 网络 (DQN)](img/B08956_09_051.jpg) 执行梯度下降步骤，更新参数 ![深度 Q 网络 (DQN)](img/B08956_09_052.jpg)
- en: // periodic update of the target network
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: // 定期更新目标网络
- en: Every *C* steps *Q*[*target*] = *Q*, that is set ![Deep Q-Network (DQN)](img/B08956_09_053.jpg)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每 *C* 步 *Q*[*target*] = *Q*，即设置为 ![深度 Q 网络 (DQN)](img/B08956_09_053.jpg)
- en: End
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束
- en: 'However, it turns out that training the Q-Network is unstable. There are two
    problems causing the instability:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事实证明，训练 Q 网络是不稳定的。导致不稳定的原因有两个问题：
- en: A high correlation between samples
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本之间存在高度相关性
- en: A non-stationary target
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非平稳目标
- en: A high correlation is due to the sequential nature of sampling experiences.
    DQN addressed this issue by creating a buffer of experiences. The training data
    are randomly sampled from this buffer. This process is known as **experience replay**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 高相关性是由于采样经验的顺序性。DQN 通过创建经验缓冲区解决了这个问题。训练数据从该缓冲区中随机采样。此过程称为 **经验回放**。
- en: The issue of the non-stationary target is due to the target network *Q*(*s*
    *'*,*a* *'*) that is modified after every mini batch of training. A small change
    in the target network can create a significant change in the policy, the data
    distribution, and the correlation between the current *Q* value and target *Q*
    value. This is resolved by freezing the weights of the target network for *C*
    training steps. In other words, two identical Q-Networks are created. The target
    Q-Network parameters are copied from the Q-Network under training every *C* training
    steps.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 非平稳目标问题来源于每次训练的小批量后，目标网络 *Q*(*s* *'*,*a* *'*) 的更新。目标网络的微小变化可能会对策略、数据分布以及当前 *Q*
    值与目标 *Q* 值之间的关联产生显著影响。通过在 *C* 次训练步骤内冻结目标网络的权重来解决这个问题。换句话说，创建了两个相同的 Q-网络。每 *C*
    次训练步骤，目标 Q-网络的参数都会从正在训练的 Q-网络复制过来。
- en: The DQN algorithm is summarized in *Algorithm* *9.6.1*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 算法在 *算法* *9.6.1* 中做了总结。
- en: DQN on Keras
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 在 Keras 上
- en: 'To illustrate DQN, the `CartPole-v0` environment of the OpenAI Gym is used.
    `CartPole-v0` is a pole balancing problem. The goal is to keep the pole from falling
    over. The environment is 2D. The action space is made of two discrete actions
    (left and right movements). However, the state space is continuous and is made
    of four variables:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 DQN，使用了 OpenAI Gym 的 `CartPole-v0` 环境。`CartPole-v0` 是一个杆子平衡问题，目标是保持杆子不倒。该环境是二维的。动作空间由两个离散动作（左移和右移）组成。然而，状态空间是连续的，包含四个变量：
- en: Linear position
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性位置
- en: Linear velocity
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性速度
- en: Angle of rotation
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旋转角度
- en: Angular velocity
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 角速度
- en: The `CartPole-v0` is shown in *Figure 9.6.1*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`CartPole-v0` 如 *图 9.6.1* 所示。'
- en: 'Initially, the pole is upright. A reward of +1 is provided for every timestep
    that the pole remains upright. The episode ends when the pole exceeds 15 degrees
    from the vertical or 2.4 units from the center. The `CartPole-v0` problem is considered
    solved if the average reward is 195.0 in 100 consecutive trials:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，杆子是竖直的。每个保持杆子竖直的时间步都会获得 +1 的奖励。当杆子偏离竖直超过 15 度或偏离中心超过 2.4 单位时，回合结束。如果在 100
    次连续试验中，平均奖励为 195.0，则认为 `CartPole-v0` 问题已解决：
- en: '![DQN on Keras](img/B08956_09_14.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![DQN on Keras](img/B08956_09_14.jpg)'
- en: 'Figure 9.6.1: The CartPole-v0 environment'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6.1：CartPole-v0 环境
- en: '*Listing* *9.6.1* shows us the DQN implementation for `CartPole-v0`. The `DQNAgent`
    class represents the agent using DQN. Two Q-Networks are created:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing* *9.6.1* 展示了 `CartPole-v0` 的 DQN 实现。`DQNAgent` 类表示使用 DQN 的代理。创建了两个
    Q-网络：'
- en: Q-Network or *Q* in *Algorithm* *9.6.1*
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*算法* *9.6.1* 中的 Q-网络或 *Q*'
- en: Target Q-Network or *Q*[target] in *Algorithm* *9.6.1*
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标 Q-网络或 *Q*[target] 在 *算法* *9.6.1* 中
- en: Both networks are MLP with three hidden layers of 256 units each. The Q-Network
    is trained during experience replay, `replay()`. At a regular interval of *C*
    = 10 training steps, the Q-Network parameters are copied to the Target Q-Network
    by `update_weights()`. This implements line *13*, *Q*[target] = *Q*, in algorithm
    *9.6.1*. After every episode, the ratio of exploration-exploitation is decreased
    by `update_epsilon()` to take advantage of the learned policy.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 两个网络都是具有三层隐藏层，每层 256 单元的 MLP。Q-网络在经验回放期间训练，使用 `replay()` 方法。在每 *C* = 10 次训练步骤的常规间隔中，Q-网络的参数通过
    `update_weights()` 复制到目标 Q-网络中。这实现了 *算法 9.6.1* 中的 *13* 行，*Q*[target] = *Q*。每个回合结束后，探索-利用比例通过
    `update_epsilon()` 被降低，以利用已学习的策略。
- en: To implement line *10* in *Algorithm* *9.6.1* during experience replay, `replay()`,
    for each experience unit, (*s*[j], *a*[j], *r*[j+1], *s*[j+1]), the *Q* value
    for the action *a*[j] is set to *Q*[*max*]. All other actions have their *Q* values
    unchanged.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在经验回放期间实现 *算法* *9.6.1* 中的 *10* 行，`replay()`，对于每个经验单元 (*s*[j], *a*[j], *r*[j+1],
    *s*[j+1])，动作 *a*[j] 的 *Q* 值被设置为 *Q*[*max*]。所有其他动作的 *Q* 值保持不变。
- en: 'This is implemented by the following lines:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码实现了这一点：
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Only the action *a*[j] has a non-zero loss equal to ![DQN on Keras](img/B08956_09_054.jpg)
    as shown by line *11* of *Algorithm 9.6.1*. Note that the experience replay is
    called by the perception-action-learning loop in *Listing* *9.6.2* after the end
    of each episode assuming that there is sufficient data in the buffer (that is,
    buffer size, is greater or equal to batch size). During the experience replay,
    one batch of experience units is randomly sampled and used to train the Q-Network.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 只有动作 *a*[j] 的损失值不为零，且等于 ![DQN on Keras](img/B08956_09_054.jpg)，如 *算法 9.6.1*
    中 *11* 行所示。请注意，经验回放是在 *Listing* *9.6.2* 中的感知-动作-学习循环调用的，在每个回合结束后，假设缓冲区中有足够的数据（即缓冲区大小大于或等于批量大小）。在经验回放期间，随机采样一批经验单元并用于训练
    Q-网络。
- en: Similar to the Q-Table, `act()` implements the ![DQN on Keras](img/B08956_09_055.jpg)-greedy
    policy, *Equation* *9.6.1*. Experiences are stored by `remember()` in the replay
    buffer. The computation of *Q* is done by the `get_target_q_value()` function.
    On the average of 10 runs, `CartPole-v0` is solved by DQN within 822 episodes.
    We need to take note that the results may vary every time the training runs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Q 表，`act()` 实现了 ![DQN on Keras](img/B08956_09_055.jpg)-贪婪策略，*方程式* *9.6.1*。经验由
    `remember()` 存储在回放缓冲区中。*Q* 的计算由 `get_target_q_value()` 函数完成。在 10 次运行的平均值中，DQN
    在 822 次训练中解决了 `CartPole-v0`。需要注意的是，结果在每次训练运行时可能会有所不同。
- en: 'Listing 9.6.1, `dqn-cartpole-9.6.1.py` shows us the DQN implementation within
    Keras:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.6.1，`dqn-cartpole-9.6.1.py` 向我们展示了 Keras 中的 DQN 实现：
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Listing 9.6.2, `dqn-cartpole-9.6.1.py`. Training loop of DQN implementation
    in Keras:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.6.2，`dqn-cartpole-9.6.1.py`。Keras 中 DQN 实现的训练循环：
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Double Q-Learning (DDQN)
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重 Q 学习（DDQN）
- en: In DQN, the target Q-Network selects and evaluates every action resulting in
    an overestimation of *Q* value. To resolve this issue, DDQN [3] proposes to use
    the Q-Network to choose the action and use the target Q-Network to evaluate the
    action.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DQN 中，目标 Q 网络选择并评估每个动作，导致 *Q* 值的过度估计。为了解决这个问题，DDQN [3] 提议使用 Q 网络来选择动作，并使用目标
    Q 网络来评估该动作。
- en: 'In DQN as summarized by *Algorithm 9.6.1*, the estimate of the *Q* value in
    line *10* is:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *算法 9.6.1* 总结的 DQN 中，第 *10* 行中的 *Q* 值估算为：
- en: '![Double Q-Learning (DDQN)](img/B08956_09_056.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![Double Q-Learning (DDQN)](img/B08956_09_056.jpg)'
- en: '*Q*[target] chooses and evaluates the action *a* [j+1].'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*[target] 选择并评估动作 *a* [j+1]。'
- en: 'DDQN proposes to change line *10* to:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN 提议将第 *10* 行更改为：
- en: '![Double Q-Learning (DDQN)](img/B08956_09_057.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![Double Q-Learning (DDQN)](img/B08956_09_057.jpg)'
- en: The term ![Double Q-Learning (DDQN)](img/B08956_09_058.jpg) lets *Q* to choose
    the action. Then this action is evaluated by *Q*[target].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 ![Double Q-Learning (DDQN)](img/B08956_09_058.jpg) 让 *Q* 来选择动作。然后这个动作由 *Q*[target]
    进行评估。
- en: 'In Listing 9.6.1, both DQN and DDQN are implemented. Specifically, for DDQN,
    the modification on the *Q* value computation performed by `get_target_q_value()`
    function is highlighted:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 9.6.1 中，DQN 和 DDQN 都得到了实现。具体来说，对于 DDQN，`get_target_q_value()` 函数在计算 *Q*
    值时所做的修改被突出显示：
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For comparison, on the average of 10 runs, the `CartPole-v0` is solved by DDQN
    within 971 episodes. To use DDQN, run:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行对比，在 10 次运行的平均值中，`CartPole-v0` 通过 DDQN 在 971 次训练中解决。要使用 DDQN，运行：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Conclusion
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we've been introduced to DRL. A powerful technique believed
    by many researchers as the most promising lead towards artificial intelligence.
    Together, we've gone over the principles of RL. RL is able to solve many toy problems,
    but the Q-Table is unable to scale to more complex real-world problems. The solution
    is to learn the Q-Table using a deep neural network. However, training deep neural
    networks on RL is highly unstable due to sample correlation and non-stationarity
    of the target Q-Network.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了深度强化学习（DRL）。许多研究人员认为这是通向人工智能最有前途的技术。我们一起回顾了强化学习（RL）的原理。强化学习能够解决许多简单问题，但
    Q 表无法扩展到更复杂的实际问题。解决方案是使用深度神经网络来学习 Q 表。然而，由于样本相关性和目标 Q 网络的非平稳性，在 RL 上训练深度神经网络非常不稳定。
- en: DQN proposed a solution to these problems using experience replay and separating
    the target network from the Q-Network under training. DDQN suggested further improvement
    of the algorithm by separating the action selection from action evaluation to
    minimize the overestimation of *Q* value. There are other improvements proposed
    for the DQN. Prioritized experience replay [6] argues that that experience buffer
    should not be sampled uniformly. Instead, experiences that are more important
    based on TD errors should be sampled more frequently to accomplish more efficient
    training. [7] proposes a dueling network architecture to estimate the state value
    function and the advantage function. Both functions are used to estimate the *Q*
    value for faster learning.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 提出了使用经验回放和将目标网络与正在训练的 Q 网络分开的方法来解决这些问题。DDQN 建议通过将动作选择与动作评估分开，进一步改进算法，从而减少
    *Q* 值的过度估计。DQN 还有其他改进建议。优先经验回放 [6] 认为，经验缓冲区不应均匀采样。相反，基于 TD 误差的重要经验应该被更加频繁地采样，以实现更高效的训练。[7]
    提出了对抗性网络架构，用于估计状态值函数和优势函数。两者均用于估算 *Q* 值，从而加速学习。
- en: The approach presented in this chapter is value iteration/fitting. The policy
    is learned indirectly by finding an optimal value function. In the next chapter,
    the approach will be to learn the optimal policy directly by using a family of
    algorithms called policy gradient methods. Learning the policy has many advantages.
    In particular, policy gradient methods can deal with both discrete and continuous
    action spaces.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节介绍的方法是值迭代/拟合。通过找到最优值函数来间接学习策略。下一章将直接学习最优策略，使用一类被称为策略梯度方法的算法。学习策略具有许多优势。特别是，策略梯度方法可以处理离散和连续的动作空间。
- en: References
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sutton and Barto. *Reinforcement Learning: An Introduction*, 2017 ([http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)).'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto. *强化学习：一种介绍*, 2017 ([http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)).
- en: 'Volodymyr Mnih and others, *Human-level control through deep reinforcement
    learning*. Nature 518.7540, 2015: 529 ([http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf))'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Volodymyr Mnih 和其他人, *通过深度强化学习实现人类水平控制*. Nature 518.7540, 2015: 529 ([http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf))'
- en: Hado Van Hasselt, Arthur Guez, and David Silver *Deep Reinforcement Learning
    with Double Q-Learning*. AAAI. Vol. 16, 2016 ([http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)).
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hado Van Hasselt, Arthur Guez, 和 David Silver *双Q学习的深度强化学习*. AAAI. Vol. 16,
    2016 ([http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)).
- en: Kai Arulkumaran and others *A Brief Survey of Deep Reinforcement Learning*.
    arXiv preprint arXiv:1708.05866, 2017 ([https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kai Arulkumaran 和其他人 *深度强化学习简要调查*. arXiv 预印本 arXiv:1708.05866, 2017 ([https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)).
- en: David Silver *Lecture Notes on Reinforcement Learning*, ([http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)).
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: David Silver *强化学习讲义*, ([http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)).
- en: Tom Schaul and others. *Prioritized experience replay*. arXiv preprint arXiv:1511.05952,
    2015 ([https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)).
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tom Schaul 和其他人. *优先经验重放*. arXiv 预印本 arXiv:1511.05952, 2015 ([https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)).
- en: Ziyu Wang and others. *Dueling Network Architectures for Deep Reinforcement
    Learning*. arXiv preprint arXiv:1511.06581, 2015 ([https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)).
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ziyu Wang 和其他人. *深度强化学习的对抗网络架构*. arXiv 预印本 arXiv:1511.06581, 2015 ([https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)).
