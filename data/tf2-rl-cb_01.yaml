- en: '*Chapter 1*: Developing Building Blocks for Deep Reinforcement Learning Using
    Tensorflow 2.x'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 1 章*：使用 Tensorflow 2.x 开发深度强化学习的构建模块'
- en: This chapter provides a practical and concrete description of the fundamentals
    of **Deep Reinforcement Learning** (**Deep RL**) filled with recipes for implementing
    the building blocks using the latest major version of **TensorFlow 2.x**. It includes
    recipes for getting started with RL environments, **OpenAI Gym**, developing neural
    network-based agents, and evolutionary neural agents for addressing applications
    with both discrete and continuous value spaces for Deep RL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了 **深度强化学习**（**Deep RL**）基础的实际和具体描述，充满了使用最新主要版本 **TensorFlow 2.x** 实现构建模块的食谱。它包括启动
    RL 环境、**OpenAI Gym**、开发基于神经网络的智能体以及用于解决具有离散和连续值空间的深度 RL 应用的进化神经智能体的食谱。
- en: 'The following recipes are discussed in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了以下食谱：
- en: Building an environment and reward mechanism for training RL agents
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建训练 RL 智能体的环境和奖励机制
- en: Implementing neural network-based RL policies for discrete action spaces and
    decision-making problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为离散动作空间和决策问题实现基于神经网络的 RL 策略
- en: Implementing neural network-based RL policies for continuous action spaces and
    continuous-control problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为连续动作空间和连续控制问题实现基于神经网络的 RL 策略
- en: Working with OpenAI Gym for RL training environments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OpenAI Gym 进行 RL 训练环境的工作
- en: Building a neural agent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建神经智能体
- en: Building a neural evolutionary agent
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建神经进化智能体
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code in the book has been extensively tested on Ubuntu 18.04 and Ubuntu
    20.04 and should work with later versions of Ubuntu as long as Python 3.6+ is
    available. With Python 3.6 installed along with the necessary Python packages
    as listed before the start of each of the recipes, the code should run fine on
    Windows and macOS X too. It is advised to create and use a Python virtual environment
    named `tf2rl-cookbook` to install the packages and run the code in this book.
    Miniconda or Anaconda installation for Python virtual environment management is
    recommended.The complete code for each recipe in this chapter will be available
    here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码已经在 Ubuntu 18.04 和 Ubuntu 20.04 上进行了广泛测试，只要 Python 3.6+ 可用，它应该也能在更高版本的
    Ubuntu 上正常运行。安装 Python 3.6 以及书中每个食谱开始前列出的必要 Python 包后，代码也可以在 Windows 和 macOS X
    上顺利运行。建议创建并使用名为 `tf2rl-cookbook` 的 Python 虚拟环境来安装包并运行本书中的代码。建议使用 Miniconda 或 Anaconda
    安装 Python 虚拟环境管理工具。本章中每个食谱的完整代码可以在此获取：[https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook)。
- en: Building an environment and reward mechanism for training RL agents
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建训练 RL 智能体的环境和奖励机制
- en: 'This recipe will walk you through the steps to build a **Gridworld** learning
    environment to train RL agents. Gridworld is a simple environment where the world
    is represented as a grid. Each location on the grid can be referred to as a cell.
    The goal of an agent in this environment is to find its way to the goal state
    in a grid like the one shown here:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将引导你完成构建 **Gridworld** 学习环境的步骤，以训练 RL 智能体。Gridworld 是一个简单的环境，其中世界被表示为一个网格。网格中的每个位置可以称为一个单元格。智能体在这个环境中的目标是找到一条通往目标状态的路径，类似于这里展示的网格：
- en: '![Figure 1.1 – A screenshot of the Gridworld environment ](img/B15074_01_001.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Gridworld 环境的截图](img/B15074_01_001.jpg)'
- en: Figure 1.1 – A screenshot of the Gridworld environment
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Gridworld 环境的截图
- en: The agent's location is represented by the blue cell in the grid, while the
    goal and a mine/bomb/obstacle's location is represented in the grid using green
    and red cells, respectively. The agent (blue cell) needs to find its way through
    the grid to reach the goal (green cell) without running over the mine/bomb (red
    cell).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的位置由网格中的蓝色单元格表示，目标和地雷/炸弹/障碍物的位置分别通过绿色和红色单元格在网格中表示。智能体（蓝色单元格）需要找到一条穿越网格到达目标（绿色单元格）的路径，同时避免踩到地雷/炸弹（红色单元格）。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/Conda virtual environment and `pip install numpy gym`. If the following
    import statements run without issues, you are ready to get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，你首先需要激活 `tf2rl-cookbook` Python/Conda 虚拟环境并运行 `pip install numpy gym`。如果以下导入语句没有问题，那么你可以开始了！
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we can begin.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始了。
- en: How to do it…
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: To train RL agents, we need a learning environment that is akin to the datasets
    used in supervised learning. The learning environment is a simulator that provides
    the observation for the RL agent, supports a set of actions that the RL agent
    can perform by executing the actions, and returns the resultant/new observation
    as a result of the agent taking the action.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练强化学习（RL）智能体，我们需要一个学习环境，类似于监督学习中使用的数据集。学习环境是一个模拟器，它为RL智能体提供观察，支持RL智能体通过执行动作来执行一组动作，并返回执行动作后获得的结果/新观察。
- en: 'Perform the following steps to implement a Gridworld learning environment that
    represents a simple 2D map with colored cells representing the location of the
    agent, goal, mine/bomb/obstacle, wall, and empty space on a grid:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来实现一个Gridworld学习环境，该环境表示一个简单的二维地图，具有不同颜色的单元格，表示智能体、目标、地雷/炸弹/障碍物、墙壁和空白空间在网格中的位置：
- en: 'We''ll start by first defining the mapping between different cell states and
    their color codes to be used in the Gridworld environment:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义不同单元格状态与其颜色代码之间的映射关系，以便在Gridworld环境中使用：
- en: '[PRE1]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, generate a color map using RGB intensity values:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用RGB强度值生成颜色映射：
- en: '[PRE2]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s now define the action mapping:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义动作映射关系：
- en: '[PRE3]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s then create a `GridworldEnv` class with an `__init__` function to define
    necessary class variables, including the observation and action space:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`GridworldEnv`类，并定义一个`__init__`函数来定义必要的类变量，包括观察空间和动作空间：
- en: '[PRE4]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will implement `__init__()` in the following steps.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在接下来的步骤中实现`__init__()`方法。
- en: 'In this step, let''s define the layout of the Gridworld environment using the
    grid cell state mapping:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，让我们使用网格单元格状态映射来定义Gridworld环境的布局：
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding layout, `0` corresponds to the empty cells, `1` corresponds
    to walls, `2` corresponds to the agent's starting location, `3` corresponds to
    the location of the mine/bomb/obstacle, and `4` corresponds to the goal location
    based on the mapping we defined in step 1.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在之前的布局中，`0`表示空白单元格，`1`表示墙壁，`2`表示智能体的起始位置，`3`表示地雷/炸弹/障碍物的位置，`4`表示目标位置，这些是根据我们在第一步中定义的映射关系。
- en: 'Now, we are ready to define the observation space for the Gridworld RL environment:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备定义Gridworld RL环境的观察空间：
- en: '[PRE6]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s define the action space and the mapping between the actions and the
    movement of the agent in the grid:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义动作空间，以及动作与智能体在网格中移动之间的映射关系：
- en: '[PRE7]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s now wrap up the `__init__` function by initializing the agent''s start
    and goal states using the `get_state()` method (which we will implement in the
    next step):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们通过使用`get_state()`方法初始化智能体的起始状态和目标状态来完成`__init__`函数（我们将在下一步实现该方法）：
- en: '[PRE8]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we need to implement the `get_state()` method that returns the start and
    goal state for the Gridworld environment:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要实现`get_state()`方法，该方法返回Gridworld环境的起始状态和目标状态：
- en: '[PRE9]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we will be implementing the `step(action)` method to execute
    the action and retrieve the next state/observation, the associated reward, and
    whether the episode ended:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将实现`step(action)`方法来执行动作并获取下一个状态/观察、相关奖励以及是否结束回合：
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let''s specify the rewards and finally, return `grid_state`, `reward`,
    `done`, and `info`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，指定奖励，最后返回`grid_state`、`reward`、`done`和`info`：
- en: '[PRE11]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Up next is the `reset()` method, which resets the Gridworld environment when
    an episode completes (or if a request to reset the environment is made):'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是`reset()`方法，它会在一个回合完成时（或者在请求重置环境时）重置Gridworld环境：
- en: '[PRE12]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To visualize the state of the Gridworld environment in a human-friendly manner,
    let's implement a render function that will convert the `grid_layout` that we
    defined in step 5 to an image and display it. With that, the Gridworld environment
    implementation will be complete!
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了以更易于人类理解的方式可视化Gridworld环境的状态，让我们实现一个渲染函数，将我们在第五步中定义的`grid_layout`转换为图像并显示它。至此，Gridworld环境的实现将完成！
- en: '[PRE13]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To test whether the environment is working as expected, let''s add a `__main__`
    function that gets executed if the environment script is run directly:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试环境是否按预期工作，让我们添加一个`__main__`函数，该函数将在直接运行环境脚本时执行：
- en: '[PRE14]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'All set! The Gridworld environment is ready and we can quickly test it by running
    the script (`python envs/gridworld.py`). An output such as the following will
    be displayed:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切就绪！Gridworld环境已经准备好，我们可以通过运行脚本（`python envs/gridworld.py`）来快速测试它。将显示类似如下的输出：
- en: '[PRE15]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following rendering of the Gridworld environment will also be displayed:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还将显示以下Gridworld环境的渲染图：
- en: '![Figure 1.2 – The Gridworld ](img/B15074_01_002.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – Gridworld](img/B15074_01_002.jpg)'
- en: Figure 1.2 – The Gridworld
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – Gridworld
- en: Let's now see how it works!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看它是如何工作的！
- en: How it works…
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The `grid_layout` defined in step 5 in the *How to do it…* section represents
    the state of the learning environment. The Gridworld environment defines the observation
    space, action spaces, and the rewarding mechanism to implement a `env.render()`
    method converts the environment's internal grid representation to an image and
    displays it for visual understanding.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步中*如何实现…*部分定义的`grid_layout`表示学习环境的状态。Gridworld环境定义了观察空间、动作空间和奖励机制，以实现`env.render()`方法将环境的内部网格表示转换为图像并显示，以便进行视觉理解。
- en: Implementing neural network-based RL policies for discrete action spaces and
    decision-making problems
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为离散动作空间和决策问题实现基于神经网络的RL策略
- en: Many environments (both simulated and real) for RL requires the RL agent to
    choose an action from a list of actions or, in other words, take discrete actions.
    While simple linear functions can be used to represent policies for such agents,
    they are often not scalable to complex problems. A non-linear function approximator
    such as a (deep) neural network can approximate arbitrary functions, even those
    required to solve complex problems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 许多强化学习（RL）环境（无论是模拟的还是实际的）要求RL智能体从一系列动作中选择一个动作，换句话说，执行离散动作。虽然简单的线性函数可以用来表示此类智能体的策略，但它们通常无法扩展到复杂问题。像（深度）神经网络这样的非线性函数逼近器可以逼近任意函数，甚至是解决复杂问题所需的函数。
- en: The neural network-based policy network is a crucial building block for advanced
    RL and **Deep RL** and will be applicable to general, discrete decision-making
    problems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的策略网络是高级RL和**深度RL**的关键构建块，并将适用于一般的离散决策问题。
- en: By the end of this recipe, you will have an agent with a neural network-based
    policy implemented in **TensorFlow 2.x** that can take actions in the **Gridworld**
    environment and (with little or no modifications) in any discrete-action space
    environment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，你将实现一个基于神经网络的策略智能体，该智能体使用**TensorFlow 2.x**并能够在**Gridworld**环境中执行动作，并且（经过很少或不需要修改）可以在任何离散动作空间环境中运行。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Activate the `tf2rl-cookbook` Python virtual environment and run the following
    to install and import the packages:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 激活`tf2rl-cookbook` Python虚拟环境，并运行以下命令来安装和导入相关包：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let's get started.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: How to do it…
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will look at policy distribution types that can be used by agents in environments
    with discrete action spaces:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看智能体在离散动作空间环境中可以使用的策略分布类型：
- en: 'Let''s begin by creating a binary policy distribution in TensorFlow 2.x using
    the `tensorflow_probability` library:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先在TensorFlow 2.x中使用`tensorflow_probability`库创建一个二元策略分布：
- en: '[PRE17]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code should print something like the following:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码应输出如下内容：
- en: '[PRE18]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Important note
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The values of the action that you get will differ from what is shown here because
    they will be sampled from the Bernoulli distribution, which is not a deterministic
    process.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你得到的动作值将与这里显示的不同，因为它们是从伯努利分布中采样的，而这不是一个确定性过程。
- en: 'Let''s quickly visualize the binary policy distribution:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们快速可视化二元策略分布：
- en: '[PRE19]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code will generate a distribution plot as shown here:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将生成一个如图所示的分布图：
- en: '![Figure 1.3 – A distribution plot of the binary policy ](img/B15074_01_003.jpg)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图1.3 – 二元策略的分布图](img/B15074_01_003.jpg)'
- en: Figure 1.3 – A distribution plot of the binary policy
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.3 – 二元策略的分布图
- en: 'In this step, we will be implementing a discrete policy distribution. A categorical
    distribution over a single discrete variable with *k* finite categories is referred
    to as a **multinoulli** distribution. The generalization of the multinoulli distribution
    to multiple trials is the multinomial distribution that we will be using to represent
    discrete policy distributions:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一阶段，我们将实现一个离散的策略分布。一个单一离散变量的类别分布，具有*k*个有限类别，被称为**多项分布**。多项分布的推广是多次试验的多项分布，我们将使用它来表示离散策略分布：
- en: '[PRE20]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code should print something along the lines of the following:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码应输出类似以下内容：
- en: '[PRE21]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we visualize the discrete probability distribution:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将可视化离散的概率分布：
- en: '[PRE22]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code will generate a distribution plot, like the one shown here
    for `discrete_policy`:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将生成一个分布图，类似于这里显示的`discrete_policy`：
- en: '![Figure 1.4 – A distribution plot of the discrete policy ](img/B15074_01_004.jpg)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图1.4 – 离散策略的分布图](img/B15074_01_004.jpg)'
- en: Figure 1.4 – A distribution plot of the discrete policy
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.4 – 离散策略的分布图
- en: 'Then, calculate the entropy of a discrete policy:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，计算离散策略的熵：
- en: '[PRE23]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Also, implement a discrete policy class:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，实现一个离散策略类：
- en: '[PRE24]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we implement a helper method to evaluate the agent in a given environment:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们实现一个辅助方法，用于在给定环境中评估智能体：
- en: '[PRE25]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s now implement a neural network Brain class using TensorFlow 2.x:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用TensorFlow 2.x实现一个神经网络大脑类：
- en: '[PRE26]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s now implement a simple agent class that uses a `DiscretePolicy` object
    to act in discrete environments:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个简单的智能体类，使用`DiscretePolicy`对象在离散环境中进行操作：
- en: '[PRE27]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s now test the agent in `GridworldEnv`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在`GridworldEnv`中测试智能体：
- en: '[PRE28]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This shows how to implement the policy. We will see how this works in the following
    section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何实现策略。我们将在接下来的部分看到这一点是如何运作的。
- en: How it works…
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: One of the central components of an RL agent is the policy function that maps
    between observations and actions. Formally, a policy is a distribution over actions
    that prescribes the probabilities of choosing an action given an observation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RL智能体的核心组件之一是策略函数，它将观察与动作之间进行映射。形式上，策略是一个动作的分布，它规定了给定观察时选择某个动作的概率。
- en: 'In environments where the agent can take at most two different actions, for
    example, in a binary action space, we can represent the policy using a **Bernoulli
    distribution**, where the probability of taking action 0 is given by ![](img/Formula_01_001.png),
    and the probability of taking action 1 is given by ![](img/Formula_01_002.png),
    which gives rise to the following probability distribution:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体最多只能采取两个不同动作的环境中，例如，在二元动作空间中，我们可以使用**伯努利分布**来表示策略，其中采取动作0的概率由![](img/Formula_01_001.png)给出，采取动作1的概率由![](img/Formula_01_002.png)给出，从而产生以下概率分布：
- en: '![](img/Formula_01_003.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_003.jpg)'
- en: A discrete probability distribution can be used to represent an RL agent's policy
    when the agent can take one of *k* possible actions in an environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 离散概率分布可以用来表示RL智能体的策略，当智能体可以在环境中采取*k*个可能的行动时。
- en: In a general sense, such distributions can be used to describe the possible
    results of a random variable that can take one of *k* possible categories and
    is therefore also called a **categorical distribution**. This is a generalization
    of the Bernoulli distribution to k-way events and is therefore a multinoulli distribution.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般意义上讲，这种分布可以用来描述当随机变量可以取*k*个可能类别之一时的可能结果，因此也被称为**类别分布**。这是伯努利分布对k种事件的推广，因此是一个多项伯努利分布。
- en: Implementing neural network-based RL policies for continuous action spaces and
    continuous-control problems
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现基于神经网络的RL策略，适用于连续动作空间和连续控制问题
- en: Reinforcement learning has been used to achieve the state of the art in many
    control problems, not only in games as varied as Atari, Go, Chess, Shogi, and
    StarCraft, but also in real-world deployments, such as HVAC control systems.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习已被用于许多控制问题中，取得了最先进的成果，不仅在像Atari、围棋、国际象棋、将棋和星际争霸等各种游戏中，而且在现实世界的部署中，如暖通空调控制系统（HVAC）。
- en: In environments where the action space is continuous, meaning that the actions
    are real-valued, a real-valued, continuous policy distribution is necessary. A
    continuous probability distribution can be used to represent an RL agent's policy
    when the action space of the environment contains real numbers. In a general sense,
    such distributions can be used to describe the possible results of a random variable
    when the random variable can take any (real) value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作空间是连续的环境中，意味着动作是实值的，需要使用实值的连续策略分布。当环境的动作空间包含实数时，可以使用连续概率分布来表示RL智能体的策略。从一般意义上讲，这种分布可以用来描述当随机变量可以取任何（实）值时，随机变量的可能结果。
- en: 'Once the recipe is complete, you will have a complete script to control a car
    in two dimensions to drive up a hill using the `MountainCarContinuous` environment
    with a continuous action space. A screenshot from the `MountainCarContinuous`
    environment is shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配方完成，你将拥有一个完整的脚本，控制一个在二维空间中驱车上坡的汽车，使用`MountainCarContinuous`环境和连续动作空间。`MountainCarContinuous`环境的截图如下：
- en: '![Figure 1.5 – A screenshot of the MountainCarContinuous environment ](img/B15074_01_005.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – MountainCarContinuous环境的截图](img/B15074_01_005.jpg)'
- en: Figure 1.5 – A screenshot of the MountainCarContinuous environment
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – MountainCarContinuous环境的截图
- en: Getting ready
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Activate the `tf2rl-cookbook` Conda Python environment and run the following
    command to install and import the necessary Python packages for this recipe:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 激活`tf2rl-cookbook` Conda Python环境，并运行以下命令来安装和导入本食谱所需的Python包：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's get started.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: How to do it…
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'We will begin by creating continuous policy distributions using `tensorflow_probability`
    library and build upon the necessary action sampling methods to generate action
    for a given continuous space of an RL environment:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过`tensorflow_probability`库创建连续策略分布，并在此基础上构建必要的动作采样方法，以便为RL环境中给定的连续空间生成动作：
- en: 'We create a continuous policy distribution in TensorFlow 2.x using the `tensorflow_probability`
    library. We will use a Gaussian/normal distribution to create a policy distribution
    over continuous values:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`tensorflow_probability`库在TensorFlow 2.x中创建一个连续的策略分布。我们将使用高斯/正态分布来创建一个在连续值上的策略分布：
- en: '[PRE30]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we visualize a continuous policy distribution:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可视化一个连续的策略分布：
- en: '[PRE31]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code will generate a distribution plot of the continuous policy,
    like the plot shown here:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将生成一个连续策略的分布图，类似于这里所示的图：
- en: '![Figure 1.6 – A distribution plot of the continuous policy ](img/B15074_01_006.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 1.6 – 连续策略的分布图](img/B15074_01_006.jpg)'
- en: Figure 1.6 – A distribution plot of the continuous policy
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.6 – 连续策略的分布图
- en: 'Let''s now implement a continuous policy distribution using a Gaussian/normal
    distribution:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用高斯/正态分布实现一个连续策略分布：
- en: '[PRE32]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code should print something similar to what is shown in the following
    code block:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码应当打印出类似于以下代码块中的内容：
- en: '[PRE33]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Important note
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The values of the action that you get will differ from what is shown here because
    they will be sampled from the Gaussian distribution, which is not a deterministic
    process.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你得到的动作值将与这里显示的不同，因为它们是从高斯分布中采样的，而不是一个确定性的过程。
- en: 'Let''s now move one step further and implement a multi-dimensional continuous
    policy. A **multivariate Gaussian distribution** can be used to represent multi-dimensional
    continuous policies. Such polices are useful for agents when acting in environments
    with action spaces that are multi-dimensional, as well as continuous and real-valued:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们更进一步，实施一个多维连续策略。**多元高斯分布**可以用来表示多维连续策略。此类策略对于在具有多维、连续且实数值动作空间的环境中行动的智能体非常有用：
- en: '[PRE34]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code should print something similar to what follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码应当打印出类似以下内容：
- en: '[PRE35]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Before moving on, let''s visualize the multi-dimensional continuous policy:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，让我们可视化多维连续策略：
- en: '[PRE36]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code will generate a joint distribution plot similar to the plot
    shown here:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将生成一个联合分布图，类似于这里所示的图：
- en: '![Figure 1.7 – Joint distribution plot of a multi-dimensional continuous policy
    ](img/B15074_01_007.jpg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 1.7 – 多维连续策略的联合分布图](img/B15074_01_007.jpg)'
- en: Figure 1.7 – Joint distribution plot of a multi-dimensional continuous policy
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.7 – 多维连续策略的联合分布图
- en: 'Now, we are ready to implement the continuous policy class:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备实现连续策略类：
- en: '[PRE37]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As a next step, let''s implement a multi-dimensional continuous policy class:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，让我们实现一个多维连续策略类：
- en: '[PRE38]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s now implement a function to evaluate an agent in an environment with
    a continuous action space to assess episodic performance:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个函数，在具有连续动作空间的环境中评估智能体，以评估每一回合的表现：
- en: '[PRE39]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We are now ready to test the agent in a continuous action environment:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备在一个连续动作环境中测试智能体：
- en: '[PRE40]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s implement a simple agent class that utilizes the `ContinuousPolicy`
    object to act in continuous action space environments:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现一个简单的智能体类，利用`ContinuousPolicy`对象在连续动作空间环境中进行操作：
- en: '[PRE41]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As a final step, we will test the performance of the agent in a continuous
    action space environment:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步，我们将在一个连续动作空间环境中测试智能体的性能：
- en: '[PRE42]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding script will call the `MountainCarContinuous` environment, render
    it to the screen, and show how the agent is performing in this continuous action
    space environment:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的脚本将调用`MountainCarContinuous`环境，渲染到屏幕上，并展示智能体在这个连续动作空间环境中的表现：
- en: '![Figure 1.8 – A screenshot of the agent in the MountainCarContinuous-v0 environment
    ](img/B15074_01_008.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – MountainCarContinuous-v0环境中智能体的截图](img/B15074_01_008.jpg)'
- en: Figure 1.8 – A screenshot of the agent in the MountainCarContinuous-v0 environment
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 – MountainCarContinuous-v0环境中代理的截图
- en: Next, let's explore how it works.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索它是如何工作的。
- en: How it works…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We implemented a continuous-valued policy for RL agents using a **Gaussian
    distribution**. Gaussian distribution, which is also known as **normal distribution**,
    is the most widely used distribution for real numbers. It is represented using
    two parameters, µ and σ. We generated continuous-valued actions from such a policy
    by sampling from the distribution, based on the probability density that is given
    by the following equation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为RL代理实现了一个使用**高斯分布**的连续值策略。高斯分布，也称为**正态分布**，是最广泛使用的实数分布。它通过两个参数表示：µ和σ。我们通过从分布中抽样，基于由以下公式给出的概率密度，生成这种策略的连续值动作：
- en: '![](img/Formula_01_004.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_004.jpg)'
- en: The **multivariate normal distribution** extends the normal distribution to
    multiple variables. We used this distribution to generate multi-dimensional continuous
    policies.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**多元正态分布**将正态分布扩展到多个变量。我们使用这种分布来生成多维连续策略。'
- en: Working with OpenAI Gym for RL training environments
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAI Gym进行RL训练环境的工作
- en: This recipe provides a quick run-through for getting up and running with OpenAI
    Gym environments. The Gym environment and the interface provide a platform for
    training RL agents and is the most widely used and accepted RL environment interface.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本篇提供了一个快速指南，帮助你快速启动并运行OpenAI Gym环境。Gym环境和接口为训练RL代理提供了一个平台，是目前最广泛使用和接受的RL环境接口。
- en: Getting ready
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be needing the full installation of OpenAI Gym to be able to use the
    available environments. Please follow the Gym installation steps listed at [https://github.com/openai/gym#id5](https://github.com/openai/gym#id5).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要完整安装OpenAI Gym，以便能够使用可用的环境。请按照[https://github.com/openai/gym#id5](https://github.com/openai/gym#id5)中列出的Gym安装步骤进行操作。
- en: 'As a minimum, you should execute the following command:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，你应该执行以下命令：
- en: '[PRE43]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How to do it…
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: Let's start by picking an environment and exploring the Gym interface. You may
    already be familiar with the basic function calls to create a Gym environment
    from the previous recipes.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从选择一个环境并探索Gym接口开始。你可能已经熟悉了前面食谱中用于创建Gym环境的基本函数调用。
- en: 'Your steps should be formatted like so:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你的步骤应该按以下格式排列：
- en: 'Let''s first explore the list of environments in Gym:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先探索Gym中的环境列表：
- en: '[PRE44]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This script will print the names of all the environments available through your
    Gym installation, sorted alphabetically. You can run this script using the following
    command to see the names of the environments that are installed and available
    in your system. You should see a long list of environments listed. The first few
    are shown in the following screenshot for your reference:![Figure 1.9 – List of
    environments available using the openai-gym package ](img/B15074_01_009.jpg)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个脚本将打印出通过你的Gym安装可用的所有环境的名称，按字母顺序排序。你可以使用以下命令运行这个脚本，查看已安装并可用的环境名称。你应该能看到列出的一长串环境。前几个环境在以下截图中提供，供你参考：![图1.9
    – 使用openai-gym包可用的环境列表](img/B15074_01_009.jpg)
- en: Figure 1.9 – List of environments available using the openai-gym package
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.9 – 使用openai-gym包可用的环境列表
- en: Let's now see how we can run one of the Gym environments.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何运行其中一个Gym环境。
- en: 'The following script will let you explore any of the available Gym environments:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下脚本将让你探索任何可用的Gym环境：
- en: '[PRE45]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You can save the preceding script to `run_gym_env.py` and run the script like
    this:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以将前面的脚本保存为`run_gym_env.py`，然后像这样运行脚本：
- en: '[PRE46]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Figure 1.10 – Sample output of run_gym_env.py with Alien-v4 1000 as the arguments
    ](img/B15074_01_010.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 使用Alien-v4 1000作为参数的run_gym_env.py样本输出](img/B15074_01_010.jpg)'
- en: Figure 1.10 – Sample output of run_gym_env.py with Alien-v4 1000 as the arguments
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 使用Alien-v4 1000作为参数的run_gym_env.py样本输出
- en: Tip
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You can change `Alien-v4` to any of the available Gym environments listed in
    the previous step.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将`Alien-v4`更改为上一步骤中列出的任何可用Gym环境。
- en: How it works…
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'A summary of how the Gym environments work is presented in the following table:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了Gym环境的工作原理：
- en: '![Table 1.1 – Summary of the Gym environment interface ](img/Table_1.1.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![表1.1 – Gym环境接口概述](img/Table_1.1.jpg)'
- en: Table 1.1 – Summary of the Gym environment interface
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 – Gym环境接口概述
- en: See also
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can find more information on OpenAI Gym here: [http://gym.openai.com/](http://gym.openai.com/).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到有关OpenAI Gym的更多信息：[http://gym.openai.com/](http://gym.openai.com/)。
- en: Building a neural agent
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个神经代理
- en: 'This recipe will guide you through the steps to build a complete agent and
    the agent-environment interaction loop, which is the main building block for any
    RL application. When you complete the recipe, you will have an executable script
    where a simple agent tries to act in a Gridworld environment. A glimpse of what
    the agent you build will likely be doing is shown in the following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将指导您完成构建完整代理和代理-环境交互循环的步骤，这是任何强化学习应用程序的主要构建块。完成教程后，您将拥有一个可执行脚本，其中一个简单代理尝试在Gridworld环境中执行动作。您建立的代理可能会执行如下截图所示的操作：
- en: '![Figure 1.11 – Screenshot of output from the neural_agent.py script ](img/B15074_01_011.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.11 – neural_agent.py脚本输出的截图](img/B15074_01_011.jpg)'
- en: Figure 1.11 – Screenshot of output from the neural_agent.py script
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – neural_agent.py脚本输出的截图
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s get started by activating the `tf2rl-cookbook` Conda Python environment
    and running the following code to install and import the necessary Python modules:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始通过激活`tf2rl-cookbook`的Conda Python环境，并运行以下代码来安装和导入必要的Python模块：
- en: '[PRE47]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: import tensorflow as tf
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: import tensorflow as tf
- en: '[PRE48]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How to do it…
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will start by implementing a Brain class powered by a neural network implemented
    using TensorFlow 2.x:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实现由TensorFlow 2.x驱动的Brain类，其基于神经网络实现：
- en: 'Let''s first initialize a neural brain model using TensorFlow 2.x and the Keras
    functional API:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先使用TensorFlow 2.x和Keras函数API初始化一个神经脑模型：
- en: '[PRE49]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we implement the Brain class''s `call(…)` method:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现`Brain`类的`call(…)`方法：
- en: '[PRE50]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now we need to implement the Brain class''s `process()` method to conveniently
    perform predictions on a batch of inputs/observations:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要实现Brain类的`process()`方法，方便地对一批输入/观测进行预测：
- en: '[PRE51]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s now implement the init function of the agent class:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们实现代理类的init函数：
- en: '[PRE52]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now let''s define a simple policy function for the agent:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们为代理定义一个简单的策略函数：
- en: '[PRE53]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'After that, let''s implement a convenient `get_action` method for the agent:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们为代理实现一个方便的`get_action`方法：
- en: '[PRE54]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s now create a placeholder function for `learn()` that will be implemented
    as part of RL algorithm implementation in future recipes:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建一个`learn()`的占位函数，将作为未来教程中RL算法实现的一部分：
- en: '[PRE55]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This completes our basic agent implementation with the necessary ingredients!
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这完成了我们基本代理的实现及所需的组成部分！
- en: 'Let''s now evaluate the agent in a given environment for one episode:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们在给定环境中为代理评估一个episode：
- en: '[PRE56]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, let''s implement the main function:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们实现主函数：
- en: '[PRE57]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Execute the script as follows:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按以下方式执行脚本：
- en: '[PRE58]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You should see the Gridworld environment GUI pop up. This will show you what
    the agent is doing in the environment, and it will look like the following screenshot:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该看到Gridworld环境GUI弹出。这将显示代理在环境中的操作，看起来像以下截图所示：
- en: '![Figure 1.12 – A screenshot of the neural agent acting in the Gridworld environment
    ](img/B15074_01_012.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – 神经代理在Gridworld环境中执行操作的截图](img/B15074_01_012.jpg)'
- en: Figure 1.12 – A screenshot of the neural agent acting in the Gridworld environment
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – 神经代理在Gridworld环境中执行操作的截图
- en: This provides a simple, yet complete, recipe to build an agent and the agent-environment
    interaction loop. All that is left is to add the RL algorithm of your choice to
    the `learn()` method and the agent will start acting intelligently!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一个简单但完整的配方，用于构建一个代理和代理-环境交互循环。剩下的就是向`learn()`方法添加您选择的RL算法，代理将开始智能地行动！
- en: How it works…
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: This recipe puts together the necessary ingredients to build a complete agent-environment
    system. The `Brain` class implements the neural network that serves as the processing
    unit of the agent, and the agent class utilizes the `Brain` class and a simple
    policy that chooses an action based on the output of the brain after processing
    the observations received from the environment.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程汇集了构建完整代理-环境系统的必要组成部分。`Brain`类实现了作为代理处理单元的神经网络，代理类利用`Brain`类和一个简单的策略，在处理从环境接收到的观测后基于脑的输出选择动作。
- en: We implemented the `Brain` class as a subclass of the `keras.Model` class, which
    allows us to define a custom neural network-based model for the agent's brain.
    The `__init__` method initializes the `Brain` model and defines the necessary
    layers using the `Brain` model, we are creating two `__init__` method, the `call(…)`
    method is also a mandatory method that needs to be implemented by child classes
    inheriting from the `keras.Model` class. The `call(…)` method first converts the
    inputs to a TensorFlow 2.x tensor and then flattens the inputs to be of the shape
    `1 x total_number_of_elements` in the input tensor. For example, if the input
    data has a shape of 8 x 8 (8 rows and 8 columns), the data is first converted
    to a tensor and the shape is flattened to 1 x 8 * 8 = 1 x 64\. The flattened inputs
    are then processed by the dense1 layer, which contains 32 neurons and a ReLU activation
    function. Finally, the logits layer processes the output from the previous layer
    and produces n number of outputs corresponding to the action dimension (n).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`Brain`类实现为`keras.Model`类的子类，这使我们能够为智能体的大脑定义一个自定义的基于神经网络的模型。`__init__`方法初始化`Brain`模型并使用`Brain`模型定义所需的层，我们创建了两个`__init__`方法，`call(…)`方法也是一个必须由继承自`keras.Model`类的子类实现的方法。`call(…)`方法首先将输入转换为TensorFlow
    2.x张量，然后将输入展平为形状为`1 x total_number_of_elements`的输入张量。例如，如果输入数据的形状是8 x 8（8行8列），数据首先被转换为张量，形状展平为1
    x 8 * 8 = 1 x 64。然后，展平后的输入通过包含32个神经元和ReLU激活函数的dense1层进行处理。最后，logits层处理来自前一层的输出，并产生n个输出，分别对应动作维度（n）。
- en: The `predict_on_batch(…)` method performs predictions on the batch of inputs
    given as the argument. This function (unlike the `predict()` function of **Keras**)
    assumes that the inputs (observations) provided as the argument are exactly one
    batch of inputs and thus feeds the batch to the network without any further splitting
    of the input data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict_on_batch(…)`方法对作为参数提供的一批输入进行预测。与**Keras**的`predict()`函数不同，这个函数假设提供的输入（观测值）正好是一批输入，因此将这批输入直接传递给网络，而不再进一步拆分输入数据。'
- en: 'We then implemented the `Agent` class and, in the agent initialization function,
    we created an object instance of the Brain class by defining the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实现了`Agent`类，并在智能体初始化函数中通过定义以下内容创建了一个`Brain`类的对象实例：
- en: '[PRE59]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Here, `input_shape` is the shape of the input that is expected to be processed
    by the brain, and `action_dim` is the shape of the output expected from the brain.
    The agent's policy is defined to be a custom `DiscretePolicy` from the previous
    recipe to initialize the agent's policy as well.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`input_shape`是大脑预计处理的输入形状，`action_dim`是大脑预计输出的形状。智能体的策略被定义为来自前述配方的自定义`DiscretePolicy`，以初始化智能体的策略。
- en: The agent's policy function, `policy_mlp`, flattens the input observations and
    sends it for processing by the agent's brain to receive the `action_logits`, which
    are the unnormalized probabilities for the actions. The final action to be taken
    is obtained by using TensorFlow 2.x's `categorical` method from the random module,
    which samples a valid action from the given `action_logits` (unnormalized probabilities).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的策略函数`policy_mlp`会展平输入观测值，并将其发送到智能体的大脑进行处理，得到`action_logits`，即动作的未归一化概率。最终的动作是通过使用TensorFlow
    2.x的`categorical`方法从随机模块中获取的，该方法从给定的`action_logits`（未归一化概率）中采样一个有效的动作。
- en: Important note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If all of the observations supplied to the `predict_on_batch` function cannot
    be accommodated in the given amount of GPU memory or RAM (CPU), the operation
    can cause a GPU **Out Of Memory** (**OOM**) error.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供给`predict_on_batch`函数的所有观测值无法在给定的GPU内存或RAM（CPU）中容纳，则此操作可能导致GPU **内存溢出**（**OOM**）错误。
- en: The main function that gets launched – if the `neural_agent.py` script is run
    directly – creates an instance of the Gridworld-v0 environment, initializes an
    agent using the action and observation space of this environment, and starts evaluating
    the agent for 10 episodes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果直接运行`neural_agent.py`脚本，启动的主函数会创建一个Gridworld-v0环境实例，使用该环境的动作空间和观测空间初始化一个智能体，并开始对智能体进行10轮评估。
- en: Building a neural evolutionary agent
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个神经进化智能体
- en: Evolutionary methods are based on black-box optimization and are also known
    as gradient-free methods since no gradient computation is involved. This recipe
    will walk you through the steps for implementing a simple, approximate cross-entropy-based
    neural evolutionary agent using **TensorFlow 2.x**.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 进化方法基于黑盒优化，也被称为无梯度方法，因为它不涉及梯度计算。本食谱将指导你实现一个简单的基于交叉熵的神经进化代理，使用 **TensorFlow 2.x**。
- en: Getting ready
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Activate the `tf2rl-cookbook` Python environment and import the following packages
    necessary to run this recipe:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 激活 `tf2rl-cookbook` Python 环境并导入以下运行此食谱所需的包：
- en: '[PRE60]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: With the packages installed, we are ready to begin.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了所需的包后，我们准备开始了。
- en: How to do it…
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'Let''s put together all that we have learned in this chapter to build a neural
    agent that improves its policy to navigate the Gridworld environment using an
    evolutionary process:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将本章所学的所有内容结合起来，构建一个神经代理，通过进化过程改进其策略，以在 Gridworld 环境中导航：
- en: 'Let''s start by importing the basic neural agent and the Brain class from `neural_agent.py`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从 `neural_agent.py` 导入基本的神经代理和 Brain 类：
- en: '[PRE61]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, let''s implement a method to roll out the agent in a given environment
    for one episode and return `obs_batch`, `actions_batch`, and `episode_reward`:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个方法，在给定的环境中滚动代理进行一轮，并返回 `obs_batch`、`actions_batch` 和 `episode_reward`：
- en: '[PRE62]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s now test the trajectory rollout method:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们测试轨迹滚动方法：
- en: '[PRE63]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, it''s time for us to verify that the experience data generated using the
    rollouts is coherent:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候验证使用滚动操作生成的经验数据是否一致了：
- en: '[PRE64]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Let''s now roll out multiple complete trajectories to collect experience data:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们滚动多个完整的轨迹以收集经验数据：
- en: '[PRE65]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We can then visualize the reward distribution from a sample of experience data.
    Let''s also plot a red vertical line at the 50th percentile of the episode reward
    values in the collected experience data:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以从经验数据样本中可视化奖励分布。我们还将在收集的经验数据中的期望奖励值的第 50 百分位处绘制一条红色竖线：
- en: '[PRE66]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Running this code will generate a plot like the one shown in the following
    diagram:'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码将生成一个如下面图所示的图表：
- en: '![Figure 1.13 – Histogram plot of the episode reward values ](img/B15074_01_013.jpg)'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 1.13 – 期望奖励值的直方图](img/B15074_01_013.jpg)'
- en: Figure 1.13 – Histogram plot of the episode reward values
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.13 – 期望奖励值的直方图
- en: 'Let''s now create a container for storing trajectories:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建一个用于存储轨迹的容器：
- en: '[PRE67]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now it''s time to choose elite experiences for the evolution process:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候为进化过程选择精英经验了：
- en: '[PRE68]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s now test the elite experience gathering routine:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们测试精英经验收集例程：
- en: '[PRE69]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s now look at implementing a helper method to convert discrete action
    indices to one-hot encoded vectors or probability distribution over actions:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们看一下如何实现一个帮助方法，将离散的动作索引转换为 one-hot 编码向量或动作概率分布：
- en: '[PRE70]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'It''s now time to test the action distribution generation function:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是测试动作分布生成函数的时候了：
- en: '[PRE71]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now, let''s create and compile the neural network brain with TensorFlow 2.x
    using the Keras functional API:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Keras 函数式 API 在 TensorFlow 2.x 中创建并编译神经网络大脑：
- en: '[PRE72]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You can now test the brain training loop as follows:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在可以按如下方式测试大脑训练循环：
- en: '[PRE73]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'This should produce the following output:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE74]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Note
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: The numbers may vary.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数值可能会有所不同。
- en: 'The next big step is to implement an agent class that can be initialized with
    a brain to act in an environment:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一大步是实现一个代理类，可以通过大脑初始化并在环境中行动：
- en: '[PRE75]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Next, we will implement a helper function to evaluate the agent in a given
    environment:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个帮助函数，用于评估给定环境中的代理：
- en: '[PRE76]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let''s now test the agent evaluation loop:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们测试代理评估循环：
- en: '[PRE77]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'As a next step, let''s define the parameters for the training loop:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，让我们定义训练循环的参数：
- en: '[PRE78]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Let''s now create the `environment`, `brain`, and `agent` objects:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建 `environment`、`brain` 和 `agent` 对象：
- en: '[PRE79]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'This will generate a plot like the one shown in the following diagram:'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成一个如下面图所示的图表：
- en: Important note
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The episode rewards will vary and the plots may look different.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望奖励将有所不同，图表可能看起来不同。
- en: '![Figure 1.14 – Plot of the mean reward (solid, red) and reward threshold for
    elites (dotted, green) ](img/B15074_01_014.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.14 – 平均奖励（实线，红色）和精英奖励阈值（虚线，绿色）](img/B15074_01_014.jpg)'
- en: Figure 1.14 – Plot of the mean reward (solid, red) and reward threshold for
    elites (dotted, green)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 – 平均奖励（实线，红色）和精英奖励阈值（虚线，绿色）的图
- en: The solid line in the plot is the mean reward obtained by the neural evolutionary
    agent, and the dotted line shows the reward threshold used for determining the
    elites.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的实线表示神经进化代理获得的平均回报，虚线则表示用于确定精英的回报阈值。
- en: How it works…
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: On every iteration, the evolutionary process rolls out or collects a bunch of
    trajectories to build up the experience data using the current set of neural weights
    in the agent's brain. An elite selection process is then employed that picks the
    top *k* percentile (elitism criterion) trajectories/experiences based on the episode
    reward obtained in that trajectory. This shortlisted experience data is then used
    to update the agent's brain model. The process repeats for a preset number of
    iterations allowing the agent's brain model to improve and collect more rewards.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，进化过程会展开或收集一系列轨迹，通过使用当前神经网络权重来构建经验数据，这些权重位于代理的大脑中。然后，会采用精英选择过程，基于该轨迹中获得的回报来挑选出前*
    k *百分位（精英标准）的轨迹/经验。接着，这些筛选出的经验数据将用于更新代理的大脑模型。这个过程会重复预设的迭代次数，从而使代理的大脑模型不断改进，并收集更多的回报。
- en: See also
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'For more information, I suggest reading *The CMA Evolution Strategy: A Tutorial*:
    [https://arxiv.org/pdf/1604.00772.pdf](https://arxiv.org/pdf/1604.00772.pdf).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，我建议阅读 *CMA进化策略：教程*：[https://arxiv.org/pdf/1604.00772.pdf](https://arxiv.org/pdf/1604.00772.pdf)。
