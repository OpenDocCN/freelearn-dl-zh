- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: An Introduction to AutoML
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoML简介
- en: The goal of AutoML is to enable domain experts who are unfamiliar with machine
    learning technologies to use ML techniques easily.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的目标是使那些不熟悉机器学习技术的领域专家能够轻松使用机器学习技术。
- en: In this chapter, we will go through a practical exercise using Google Cloud
    Platform and do quite a bit of hands-on work after briefly discussing the fundamentals.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过一个实际练习使用Google Cloud平台，并在简要讨论基础知识后进行大量动手操作。
- en: 'We will cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖：
- en: Automatic data preparation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动数据准备
- en: Automatic feature engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动特征工程
- en: Automatic model generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动模型生成
- en: AutoKeras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoKeras
- en: Google Cloud AutoML with its multiple solutions for table, vision, text, translation,
    and video processing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud AutoML提供了多种解决方案，包括表格、视觉、文本、翻译和视频处理。
- en: Let’s begin with an introduction to AutoML.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从AutoML的介绍开始。
- en: What is AutoML?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是AutoML？
- en: During the previous chapters, we introduced several models used in modern machine
    learning and deep learning. For instance, we have seen architectures such as dense
    networks, CNNs, RNNs, autoencoders, and GANs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们介绍了几种现代机器学习和深度学习中使用的模型。例如，我们见过密集网络、卷积神经网络（CNN）、递归神经网络（RNN）、自编码器和生成对抗网络（GAN）等架构。
- en: Two observations are in order. First, these architectures are manually designed
    by deep learning experts and are not necessarily easy to explain to non-experts.
    Second, the composition of these architectures themselves was a manual process,
    which involved a lot of human intuition and trial and error.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有两点需要说明。首先，这些架构是由深度学习专家手工设计的，并不一定容易向非专家解释。其次，这些架构的组合本身是一个手工过程，涉及大量的人工直觉和试错。
- en: Today, one primary goal of artificial intelligence research is to achieve **Artificial
    General Intelligence** (**AGI**) – the intelligence of a machine that can understand
    and automatically learn any type of work or activity that a human being can do.
    It should be noted that many researchers do not believe that AGI is achievable
    because there is not only one form of intelligence but many forms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，人工智能研究的一个主要目标是实现**人工通用智能**（**AGI**）——一种能够理解并自动学习任何人类能做的工作或活动的机器智能。需要注意的是，许多研究人员认为AGI是不可实现的，因为智能并不只有一种形式，而是有多种形式。
- en: Personally, I tend to agree with this view. See [https://twitter.com/ylecun/status/1526672565233758213](https://twitter.com/ylecun/status/1526672565233758213)
    for Yann LeCun’s position on this subject. However, the reality was very different
    before AutoML research and industrial applications started. Indeed, before AutoML,
    designing deep learning architectures was very similar to crafting – the activity
    or hobby of making decorative articles by hand.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，我倾向于认同这一观点。请查看[https://twitter.com/ylecun/status/1526672565233758213](https://twitter.com/ylecun/status/1526672565233758213)了解Yann
    LeCun在这一主题上的立场。然而，AutoML研究和工业应用开始之前，现实情况是非常不同的。事实上，在AutoML出现之前，设计深度学习架构与手工制作装饰品非常相似——这是一种通过手工制作装饰物品的活动或爱好。
- en: Take, for instance, the task of recognizing breast cancer from X-rays. After
    reading the previous chapters, you will probably think that a deep learning pipeline
    created by composing several CNNs may be an appropriate tool for this purpose.
    That is probably a good intuition to start with. The problem is that it is not
    easy to explain to the users of your model why a *particular* composition of CNN
    works well within the breast cancer detection domain. Ideally, you want to provide
    easily accessible deep learning tools to the domain experts (in this case, medical
    professionals) without such a tool requiring a strong machine learning background.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以通过X光识别乳腺癌的任务为例。在阅读了前几章之后，你可能会认为，通过组合多个卷积神经网络（CNN）创建的深度学习管道可能是一个合适的工具。这个直觉可能是一个很好的起点。问题在于，向你的模型使用者解释为什么某个*特定*的CNN组合在乳腺癌检测领域有效并不容易。理想情况下，你希望为领域专家（在这个例子中是医学专业人士）提供易于访问的深度学习工具，而这些工具不需要强大的机器学习背景。
- en: The other problem is that it is not easy to understand whether or not there
    are variants (for example, different compositions) of the original manually crafted
    model that can achieve better results. Ideally, you want to provide deep learning
    tools for exploring the space of variants (for example, different compositions)
    in a more principled and automatic way.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，不容易理解是否存在原始手工设计的模型的变种（例如不同的组合），这些变种可能会取得更好的结果。理想情况下，你希望为探索变种空间（例如不同组合）提供深度学习工具，并以一种更有原则且自动化的方式进行。
- en: 'So, the central idea of AutoML is to reduce the steep learning curve and the
    huge costs of handcrafting machine learning solutions by making the whole end-to-end
    machine learning pipeline more automated. To this end, we assume that the AutoML
    pipeline consists of three macro-steps: data preparation, feature engineering,
    and automatic model generation, as shown in *Figure 13.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，AutoML的核心理念是通过使整个端到端的机器学习流程更加自动化，从而减少陡峭的学习曲线和手工构建机器学习解决方案的巨大成本。为此，我们假设AutoML管道包括三个宏观步骤：数据准备、特征工程和自动化模型生成，如*图13.1*所示：
- en: '![Text, letter  Description automatically generated with medium confidence](img/B18331_13_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Text, letter  Description automatically generated with medium confidence](img/B18331_13_01.png)'
- en: 'Figure 13.1: Three steps of an AutoML pipeline'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：AutoML管道的三个步骤
- en: Throughout the initial part of this chapter, we are going to discuss these three
    steps in detail. Then, we will focus on Google Cloud AutoML.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的初步部分，我们将详细讨论这三个步骤。接着，我们将重点关注Google Cloud AutoML。
- en: Achieving AutoML
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现AutoML
- en: How can AutoML achieve the goal of end-to-end automatization? Well, you have
    probably already guessed that a natural choice is to use machine learning – that’s
    very cool. AutoML uses ML for automating ML pipelines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML如何实现端到端自动化的目标？嗯，你可能已经猜到了，自然的选择是使用机器学习——这非常酷。AutoML使用机器学习来自动化机器学习管道。
- en: What are the benefits? Automating the creation and tuning of machine learning
    end to end offers simpler solutions, reduces the time to produce them, and ultimately
    might produce architectures that could potentially outperform models that were
    crafted by hand.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其好处是什么？自动化创建和调整机器学习端到端的过程提供了更简单的解决方案，减少了生产时间，并最终可能产生比手工构建的模型更优秀的架构。
- en: Is this a closed research area? Quite the opposite. At the beginning of 2022,
    AutoML is a very open research field, which is not surprising, as the initial
    paper drawing attention to AutoML was published at the end of 2016.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个封闭的研究领域吗？恰恰相反。到2022年初，AutoML是一个非常开放的研究领域，这并不令人惊讶，因为最初引起人们对AutoML关注的论文是在2016年底发表的。
- en: Automatic data preparation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化数据准备
- en: 'The first stage of a typical machine learning pipeline deals with data preparation
    (recall the pipeline in *Figure 13.1*). There are two main aspects that should
    be taken into account: data cleansing and data synthesis:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 典型机器学习管道的第一阶段是数据准备（回想一下*图13.1*中的管道）。在这一步中，有两个主要方面需要考虑：数据清洗和数据合成：
- en: '**Data cleansing** is about improving the quality of data by checking for wrong
    data types, missing values, and errors, and by applying data normalization, bucketization,
    scaling, and encoding. A robust AutoML pipeline should automate all of these mundane
    but extremely important steps as much as possible.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据清洗**是通过检查错误的数据类型、缺失值和错误，以及应用数据归一化、分桶、缩放和编码等方法来提高数据质量。一个健壮的AutoML管道应该尽可能自动化所有这些枯燥但至关重要的步骤。'
- en: '**Data synthesis** is about generating synthetic data via augmentation for
    training, evaluation, and validation. Normally, this step is domain-specific.
    For instance, we have seen how to generate synthetic CIFAR10-like images (*Chapter
    4*) by using cropping, rotation, resizing, and flipping operations. One can also
    think about generating additional images or video via GANs (see *Chapter 9*) and
    using the augmented synthetic dataset for training. A different approach should
    be taken for text, where it is possible to train RNNs (*Chapter 5*) to generate
    synthetic text or to adopt more NLP techniques such as BERT, Seq2Seq, or Transformers
    (see *Chapter 6*) to annotate or translate text across languages and then translate
    it back to the original one – another domain-specific form of augmentation.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据合成**是指通过数据增强生成合成数据，用于训练、评估和验证。通常，这一步是领域特定的。例如，我们已经看到如何通过裁剪、旋转、调整大小和翻转操作生成类似CIFAR10的合成图像（*第4章*）。也可以考虑通过GAN生成额外的图像或视频（参见*第9章*），并使用增强的合成数据集进行训练。对于文本，应该采取不同的方法，可以训练RNN（*第5章*）生成合成文本，或者采用更多的NLP技术，如BERT、Seq2Seq或Transformers（参见*第6章*）对文本进行标注或跨语言翻译，然后再翻译回原始语言——这是另一种领域特定的增强形式。'
- en: A different approach is to generate synthetic environments where machine learning
    can occur. This became very popular in reinforcement learning and gaming, especially
    with toolkits such as OpenAI Gym, which aims to provide an easy-to-set-up simulation
    environment with a variety of different (gaming) scenarios.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是生成可以进行机器学习的合成环境。这种方法在强化学习和游戏中非常流行，特别是在像 OpenAI Gym 这样的工具包中，旨在提供易于设置的模拟环境，并包含各种不同的（游戏）场景。
- en: Put simply, we can say that synthetic data generation is another option that
    should be provided by AutoML engines. Frequently, the tools used are very domain-specific
    and what works for image or video would not necessarily work in other domains
    such as text. Therefore, we need a (quite) large set of tools for performing synthetic
    data generation across domains.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们可以说合成数据生成是 AutoML 引擎应提供的另一种选择。通常，使用的工具非常具有领域特定性，适用于图像或视频的工具不一定适用于其他领域，如文本。因此，我们需要一套（相当）庞大的工具集，用于跨领域执行合成数据生成。
- en: Automatic feature engineering
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动特征工程
- en: 'Feature engineering is the second step of a typical machine learning pipeline
    (see *Figure 13.1*). It consists of three major steps: feature selection, feature
    construction, and feature mapping. Let’s look at each of them in turn:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是典型机器学习管道中的第二步（参见*图 13.1*）。它包括三个主要步骤：特征选择、特征构建和特征映射。让我们依次来看每个步骤：
- en: '**Feature selection** aims at selecting a subset of *meaningful* features by
    discarding those that are making little contribution to the learning task. In
    this context, “meaningful” truly depends on the application and the domain of
    your specific problem.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择**旨在通过丢弃对学习任务贡献较小的特征来选择一个有意义的特征子集。在这个过程中，“有意义”的定义确实取决于应用和你特定问题的领域。'
- en: '**Feature construction** has the goal of building new derived features, starting
    from the basic ones. Frequently, this technique is used to allow better generalization
    and to have a richer representation of the data.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征构建**的目标是从基本特征出发，构建新的派生特征。通常，这一技术用于实现更好的泛化能力，并对数据进行更丰富的表示。'
- en: '**Feature mapping** aims at altering the original feature space by means of
    a mapping function. This can be implemented in multiple ways; for instance, it
    can use autoencoders (see *Chapter 8*), PCA (see *Chapter 7*), or clustering (see
    *Chapter 7*).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征映射**旨在通过映射函数改变原始特征空间。这可以通过多种方式实现；例如，它可以使用自编码器（参见*第 8 章*）、PCA（参见*第 7 章*）或聚类（参见*第
    7 章*）。'
- en: In short, feature engineering is an art based on intuition, trial and error,
    and a lot of human experience. Modern AutoML engines aim to make the entire process
    more automated, requiring less human intervention.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，特征工程是一门基于直觉、试验和错误，以及大量人类经验的艺术。现代的 AutoML 引擎旨在使整个过程更加自动化，从而减少人工干预。
- en: Automatic model generation
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动模型生成
- en: Model generation and hyperparameter tuning is the typical third macro-step of
    a machine learning pipeline (see *Figure 13.1*).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成和超参数调优是机器学习管道中的典型第三个宏观步骤（参见*图 13.1*）。
- en: '**Model generation** consists of creating a suitable model for solving specific
    tasks. For instance, you will probably use CNNs for visual recognition, and you
    will use RNNs for either time series analysis or for sequences. Of course, many
    variants are possible, each of which is manually crafted through a process of
    trial and error and works for very specific domains.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型生成**包括为解决特定任务创建合适的模型。例如，你可能会使用 CNN 进行视觉识别，使用 RNN 进行时间序列分析或处理序列问题。当然，也有很多变种，每一种都通过试验和错误的过程手动构建，且适用于非常特定的领域。'
- en: '**Hyperparameter tuning** happens once the model is manually crafted. This
    process is generally very computationally expensive and can significantly change
    the quality of the results in a positive way. That’s because tuning the hyperparameters
    can help to optimize our model further.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调优**发生在模型手动构建之后。这个过程通常计算开销非常大，且可以显著改善结果质量。因为调优超参数有助于进一步优化我们的模型。'
- en: Automatic model generation is the ultimate goal of any AutoML pipeline. How
    can this be achieved? One approach consists of generating the model by combining
    a set of primitive operations including convolution, pooling, concatenation, skip
    connections, recurrent neural networks, autoencoders, and pretty much all the
    deep learning models we have encountered throughout this book. These operations
    constitute a (typically very large) search space to be explored, and the goal
    is to make this exploration as efficient as possible. In AutoML jargon, the exploration
    is called **NAS**, or **Neural Architecture Search**. The seminal paper on AutoML
    [1] was produced in November 2016\. The key idea (see *Figure 13.2*) is to use
    reinforcement learning (RL, see *Chapter 11*). An RNN acts as the controller,
    and it generates the model descriptions of candidate neural networks. RL is used
    to maximize the expected accuracy of the generated architectures on a validation
    set.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自动模型生成是任何 AutoML 流水线的最终目标。如何实现这一目标？一种方法是通过组合一组基本操作来生成模型，包括卷积、池化、拼接、跳跃连接、递归神经网络、自编码器以及我们在本书中遇到的几乎所有深度学习模型。这些操作构成了一个（通常非常大的）搜索空间，需要进行探索，目标是使这一探索尽可能高效。在
    AutoML 行话中，这种探索被称为 **NAS**，即 **神经架构搜索**。关于 AutoML 的开创性论文[1]发布于2016年11月。其关键思想（见*图
    13.2*）是使用强化学习（RL，见*第11章*）。RNN 充当控制器，生成候选神经网络的模型描述。强化学习被用来最大化生成架构在验证集上的预期准确性。
- en: On the CIFAR-10 dataset, this method, starting from scratch, designed a novel
    network architecture that rivals the best human-invented architecture in terms
    of test set accuracy. The CIFAR-10 model achieves a test error rate of 3.65, which
    is 0.09 percent better and 1.05x faster than the previous state-of-the-art model
    that used a similar architectural scheme. On the Penn Treebank dataset, the model
    can compose a novel recurrent cell that outperforms the widely used LSTM cell
    (see *Chapter 9*) and other state-of-the-art baselines. The cell achieves a test
    set perplexity of 62.4 on the Penn Treebank, which is 3.6 better than the previous
    state-of-the-art model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CIFAR-10 数据集上，这种方法从头开始设计了一个新颖的网络架构，其测试集准确性可与最优秀的人工发明架构相媲美。CIFAR-10 模型的测试误差率为
    3.65，比之前的最先进模型提高了 0.09 个百分点，且速度比其快了 1.05 倍。对于 Penn Treebank 数据集，该模型能够构建一个新型递归单元，其表现优于广泛使用的
    LSTM 单元（见*第9章*）及其他最先进的基线。该单元在 Penn Treebank 上的测试集困惑度为 62.4，比之前的最先进模型低了 3.6。
- en: The key outcome of the paper is shown in *Figure 13.2*. A controller network
    based on RNNs produces a sample architecture A with probability p. This candidate
    architecture A is trained by a child network to get a candidate accuracy R. Then
    a gradient of p is computed and scaled by R to update the controller. This reinforcement
    learning operation is computed in a cycle a number of times. The process of generating
    an architecture stops if the number of layers exceeds a certain value.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的关键结果如*图 13.2*所示。基于 RNN 的控制网络以概率 p 生成样本架构 A。该候选架构 A 通过子网络训练以得到候选准确性 R。然后计算
    p 的梯度，并由 R 进行缩放以更新控制器。这个强化学习操作在多个循环中计算。生成架构的过程如果层数超过一定值就会停止。
- en: 'The details of how an RL-based policy gradient method is used by the controller
    RNN to generate better architectures are in [1]. Here we emphasize the fact that
    NAS uses a meta-modeling algorithm based on Q-learning with an ϵ-greedy exploration
    strategy and with experience replay (see *Chapter 11*) to explore the model search
    space:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器 RNN 如何使用基于强化学习的策略梯度方法来生成更好的架构的细节，请参考[1]。在这里，我们强调 NAS 使用基于 Q-learning 的元建模算法，并采用
    ϵ-greedy 探索策略以及经验回放（见*第11章*）来探索模型搜索空间：
- en: '![Diagram  Description automatically generated](img/B18331_13_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_13_02.png)'
- en: 'Figure 13.2: NAS with recurrent neural networks'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：使用递归神经网络的神经架构搜索（NAS）
- en: 'Since the original paper in late 2016, a Cambrian explosion of model generation
    techniques has been observed. Initially, the goal was to generate the entire model
    in one single step. Later, a *cell-based* approach was proposed where the generation
    is divided into two macro-steps: first, a cell structure is automatically built,
    and then a predefined number of discovered cells are stacked together to generate
    an entire end-to-end architecture [2]. This **Efficient Neural Architecture Search**
    (**ENAS**) delivers strong empirical performance using significantly fewer GPU
    hours compared with all existing automatic model design approaches, and notably,
    is 1,000x less computationally expensive than standard neural architecture search
    (in 2018). Here, the primary ENAS goal is to reduce the search space via hierarchical
    composition. Variants of the cell-based approach have been proposed including
    pure hierarchical methods where higher-level cells are generated by incorporating
    lower-level cells iteratively.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 自2016年底的原始论文以来，模型生成技术经历了“寒武纪大爆发”。最初，目标是在一次性步骤中生成整个模型。后来，提出了一种*基于细胞*的方法，将生成过程分为两个宏观步骤：首先自动构建一个细胞结构，然后将预定义数量的已发现细胞堆叠在一起生成整个端到端架构[2]。这种**高效神经架构搜索**（**ENAS**）在使用显著更少的GPU小时的情况下，表现出强大的经验性能，相较于所有现有的自动模型设计方法，特别是在2018年时，其计算开销比标准神经架构搜索低了1,000倍。这里，ENAS的主要目标是通过层次化组合来减少搜索空间。已经提出了基于细胞的方法的变种，包括纯层次化方法，其中更高层次的细胞通过逐步结合较低层次的细胞生成。
- en: A completely different approach to NAS is to use transfer learning (see *Chapter
    5*) to transfer the learning of an existing neural network into a new neural network
    in order to speed up the design [3]. In other words, we want to use transfer learning
    in AutoML.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: NAS的另一种完全不同的方法是使用迁移学习（见*第5章*），将现有神经网络的学习迁移到新的神经网络，以加速设计过程[3]。换句话说，我们希望在AutoML中使用迁移学习。
- en: Another approach is based on **Genetic Programming** (**GP**) and **Evolutionary
    Algorithms** (**EAs**), where the basic operations constituting the model search
    space are encoded into a suitable representation, and then this encoding is gradually
    mutated to progressively better models in a way that resembles the genetic evolution
    of living beings [4].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法基于**遗传编程**（**GP**）和**进化算法**（**EAs**），其中构成模型搜索空间的基本操作被编码为合适的表示，然后这种编码通过逐步变异的方式，逐步演化成更好的模型，这种方式类似于生物体的基因进化[4]。
- en: '**Hyperparameter tuning** consists of finding the optimal combination of hyperparameters
    both related to learning optimization (batch size, learning rate, and so on) and
    model-specific ones (kernel size; number of feature maps and so on for CNNs; or
    number of neurons for dense or autoencoder networks, and so on). Again, the search
    space can be extremely large. There are three approaches generally used: Bayesian
    optimization, grid search, and random search.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调优**包括找到与学习优化（批量大小、学习率等）和模型特定（卷积神经网络的卷积核大小、特征图数量等；或密集网络或自编码器网络的神经元数量等）相关的超参数的最优组合。同样，搜索空间可能非常大。通常使用三种方法：贝叶斯优化、网格搜索和随机搜索。'
- en: '**Bayesian optimization** builds a probability model of the objective function
    and uses it to select the most promising hyperparameters to evaluate in the true
    objective function.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化**构建了一个目标函数的概率模型，并利用该模型选择最有前景的超参数来在真实的目标函数中进行评估。'
- en: '**Grid search** divides the search space into a discrete grid of values and
    tests all the possible combinations in the grid. For instance, if there are three
    hyperparameters and a grid with only two candidate values for each of them, then
    a total of 2 x 3 = 6 combinations must be checked. There are also hierarchical
    variants of grid search, which progressively refine the grid for regions of the
    search space and provide better results. The key idea is to use a coarse grid
    first, and after finding a better grid region, implement a finer grid search on
    that region.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格搜索**将搜索空间划分为离散的值网格，并测试网格中的所有可能组合。例如，如果有三个超参数，每个超参数有两个候选值的网格，那么需要检查总共2 x
    3 = 6种组合。网格搜索还有分层变种，逐步细化搜索空间中的网格区域，并提供更好的结果。其核心思想是首先使用粗网格，在找到更好的网格区域后，在该区域进行更细致的网格搜索。'
- en: '**Random search** performs a random sampling of the parameter search space,
    and this simple approach has been proven to work very well in many situations
    [5].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机搜索**对参数搜索空间进行随机抽样，这种简单的方法在许多情况下已经证明非常有效[5]。'
- en: Now that we have briefly discussed the fundamentals, we will do quite a bit
    of hands-on work on Google Cloud. Let’s start.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经简要讨论了基础知识，接下来我们将在 Google Cloud 上进行大量的动手操作。让我们开始吧。
- en: AutoKeras
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoKeras
- en: 'AutoKeras [6] provides functions to automatically search for architecture and
    hyperparameters of deep learning models. The framework uses Bayesian optimization
    for efficient neural architecture search. You can install the alpha version by
    using `pip`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AutoKeras [6] 提供了自动搜索深度学习模型架构和超参数的功能。该框架使用贝叶斯优化进行高效的神经网络架构搜索。您可以通过 `pip` 安装
    Alpha 版本：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The architecture is explained in *Figure 13.3* [6]:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 架构在*图 13.3* [6]中有详细说明：
- en: '![Chart  Description automatically generated with medium confidence](img/B18331_13_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成，信心中等](img/B18331_13_03.png)'
- en: 'Figure 13.3: AutoKeras system overview'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：AutoKeras 系统概述
- en: 'The architecture follows these steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 架构遵循以下步骤：
- en: The user calls the API.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户调用 API。
- en: The searcher generates neural architectures on the CPU.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索器在 CPU 上生成神经网络架构。
- en: Real neural networks with parameters are built on RAM from the neural architectures.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于神经架构在 RAM 上构建的带参数的真实神经网络。
- en: The neural network is copied to the GPU for training.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络被复制到 GPU 上进行训练。
- en: The trained neural networks are saved on storage devices.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后的神经网络被保存在存储设备中。
- en: The searcher is updated based on the training results.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索器基于训练结果进行更新。
- en: Steps 2 to 6 will repeat until a time limit is reached.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2 到 6 将重复，直到达到时间限制。
- en: Google Cloud AutoML and Vertex AI
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud AutoML 和 Vertex AI
- en: Google Cloud AutoML ([https://cloud.google.com/automl/](https://cloud.google.com/automl/))
    is a full suite of products for image, video, and text processing. AutoML can
    be used to train high-quality custom machine learning models with minimal effort
    and machine learning expertise.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud AutoML ([https://cloud.google.com/automl/](https://cloud.google.com/automl/))
    是一整套用于图像、视频和文本处理的产品。AutoML 可以用最少的努力和机器学习专业知识训练高质量的自定义机器学习模型。
- en: Vertex AI brings together the Google Cloud services for building ML under one,
    unified UI and API. In Vertex AI, you can now easily train, compare, test, and
    deploy models. Then you can serve a model with sophisticated ways to monitor and
    run experiments (see [https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 将构建机器学习的 Google Cloud 服务整合到一个统一的用户界面和 API 中。在 Vertex AI 中，您现在可以轻松地训练、比较、测试和部署模型。然后，您可以使用先进的方式监控和运行实验（请参见
    [https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)）。
- en: 'As of 2022, the suite consists of the following components, which do not require
    you to know how the deep learning networks are shaped internally:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2022 年，套件由以下组件组成，这些组件无需您了解深度学习网络的内部结构：
- en: '**Vertex AI**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vertex AI**'
- en: Unified platform to help you build, deploy, and scale more AI models
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一平台，帮助您构建、部署和扩展更多的 AI 模型
- en: '**Structured data**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**结构化数据**'
- en: 'AutoML Tables: Automatically build and deploy state-of-the-art machine learning
    models on structured data'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML Tables：自动构建和部署最先进的机器学习模型以处理结构化数据
- en: '**Sight**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sight**'
- en: 'AutoML Image: Derive insights from object detection and image classification,
    in the cloud or at the edge'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML Image：在云端或边缘设备上从物体检测和图像分类中提取见解
- en: 'AutoML Video: Enable powerful content discovery and engaging video experiences'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML Video：实现强大的内容发现和互动视频体验
- en: '**Language**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言**'
- en: 'AutoML Text: Reveal the structure and meaning of text through machine learning'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML Text：通过机器学习揭示文本的结构和意义
- en: 'AutoML Translation: Dynamically detect and translate between languages'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML Translation：动态检测并在不同语言之间进行翻译
- en: 'In the remainder of this chapter, we will review three AutoML solutions: AutoML
    Tables, AutoML Text, and AutoML Video.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将回顾三种 AutoML 解决方案：AutoML Tables、AutoML Text 和 AutoML Video。
- en: Using the Google Cloud AutoML Tables solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Google Cloud AutoML Tables 解决方案
- en: Let’s see an example of using Google Cloud AutoML Tables. We’ll aim to import
    some tabular data and train a classifier on that data; we’ll use some marketing
    data from a bank. Note that this and the following examples might be charged by
    Google according to different usage criteria (please check online for the latest
    cost estimation – see [https://cloud.google.com/products/calculator/](https://cloud.google.com/products/calculator/)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用 Google Cloud AutoML Tables 的示例。我们的目标是导入一些表格数据并在这些数据上训练一个分类器；我们将使用某银行的营销数据。请注意，这个和随后的示例可能会根据不同的使用标准由
    Google 收费（请查看在线的最新费用估算 – 见[https://cloud.google.com/products/calculator/](https://cloud.google.com/products/calculator/)）。
- en: 'The first step required is to enable the Vertex AI API:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的第一步是启用Vertex AI API：
- en: '![Graphical user interface, text, application, website  Description automatically
    generated](img/B18331_13_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，网站 描述自动生成](img/B18331_13_04.png)'
- en: 'Figure 13.4: Enable the Vertex AI API'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：启用Vertex AI API
- en: 'We can then select the **TABULAR** dataset from the console (see *Figure 13.5*).
    The name of the dataset is `bank-marketing.csv`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以从控制台选择**TABULAR**数据集（见*图13.5*）。数据集的名称是`bank-marketing.csv`：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_13_05.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B18331_13_05.png)'
- en: 'Figure 13.5: Selecting TABULAR datasets'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：选择TABULAR数据集
- en: 'On the next screen, we indicate that we want to load the data from CSV:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个屏幕上，我们指明要从CSV加载数据：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_06.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B18331_13_06.png)'
- en: 'Figure 13.6: AutoML Tables – loading data from a CSV file'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：AutoML表格——从CSV文件加载数据
- en: 'Next, we can train a new model, as shown in *Figure 13.7*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以训练一个新模型，如*图13.7*所示：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B18331_13_07.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，聊天或文本消息 描述自动生成](img/B18331_13_07.png)'
- en: 'Figure 13.7: Training a new model'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：训练新模型
- en: 'Several options for training are offered for **Classification** and **Regression**:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了多个训练选项，适用于**分类**和**回归**：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_08.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B18331_13_08.png)'
- en: 'Figure 13.8: Options offered for Classification and Regression'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：分类和回归的选项
- en: Let’s select the target as the **Deposit** column. The dataset is described
    at [https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing).
    The data is related to direct marketing campaigns (phone calls) of a Portuguese
    banking institution. The classification goal is to predict if the client will
    subscribe to a term deposit.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择**Deposit**列作为目标。数据集描述在[https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing)中。数据与葡萄牙一家银行的直接营销活动（电话营销）有关。分类目标是预测客户是否会订阅定期存款。
- en: 'Since the selected column is categorical data, AutoML Tables will build a classification
    model. This will predict the target from the classes in the selected column. The
    classification is binary: *1* represents a negative outcome, meaning that a deposit
    is not made at the bank; *2* represents a positive outcome, meaning that a deposit
    is made at the bank, as shown in *Figure 13.9*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于选定的列是分类数据，AutoML表格将构建一个分类模型。它将根据所选列中的类别预测目标。分类是二元的：*1*表示负面结果，意味着没有在银行进行存款；*2*表示正面结果，意味着在银行进行了存款，如*图13.9*所示：
- en: '![Text, application  Description automatically generated](img/B18331_13_09.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![文本，应用程序 描述自动生成](img/B18331_13_09.png)'
- en: 'Figure 13.9: Training a new model with Target column set to Deposit'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：将目标列设置为Deposit时训练新模型
- en: 'We can then inspect the dataset (see *Figure 13.10*), which gives us the opportunity
    to inspect the dataset with several features, such as *names*, *type*, *missing
    values*, *distinct values*, *invalid values, correlation with the target*, *mean*,
    and *standard deviation*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以检查数据集（见*图13.10*），这使我们有机会检查数据集的多个特征，如*名称*、*类型*、*缺失值*、*不同值*、*无效值*、*与目标的相关性*、*均值*和*标准差*：
- en: '![Table  Description automatically generated](img/B18331_13_10.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_13_10.png)'
- en: 'Figure 13.10: AutoML Tables – inspecting the dataset'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：AutoML表格——检查数据集
- en: 'It is now time to train the model by using the **Train** tab. First let’s give
    a budget for training, as shown in *Figure 13.11*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是通过**训练**标签来训练模型的时间了。首先让我们为训练设定一个预算，如*图13.11*所示：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_11.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B18331_13_11.png)'
- en: 'Figure 13.11: Setting up the budget for training'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：设置训练预算
- en: 'In this example, we accept **3** hours as our training budget. During this
    time, you can go and take a coffee whilst AutoML works on your behalf (see *Figure
    13.12*). The training budget is a number between 1 and 72 for the maximum number
    of node hours to spend training your model. If your model stops improving before
    then, AutoML Tables will stop training and you’ll only be charged the money corresponding
    to the actual node budget used:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们接受**3**小时作为我们的训练预算。在此期间，您可以去喝杯咖啡，而 AutoML 会为您工作（见*图 13.12*）。训练预算是一个介于
    1 到 72 之间的数字，表示最大节点小时数，用于训练您的模型。如果您的模型在此之前停止改进，AutoML 表格将停止训练，您只会被收取与实际使用的节点预算相对应的费用：
- en: '![Text  Description automatically generated](img/B18331_13_12.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Text  Description automatically generated](img/B18331_13_12.png)'
- en: 'Figure 13.12: AutoML Tables training process'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：AutoML 表格训练过程
- en: 'While training, we can check the progress, as shown in *Figure 13.13*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们可以检查进度，如*图 13.13*所示：
- en: '![Table  Description automatically generated](img/B18331_13_13.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B18331_13_13.png)'
- en: 'Figure 13.13: Checking the training progress'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：检查训练进度
- en: 'After less than one hour, Google AutoML should send an email to our inbox:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到一小时的时间内，Google AutoML 应该会向我们的收件箱发送一封电子邮件：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_14.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_14.png)'
- en: 'Figure 13.14: AutoML Tables: training is concluded, and an email is sent to
    my account'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14：AutoML 表格：训练结束，并向我的账户发送了电子邮件
- en: 'Clicking on the suggested URL, it is possible to see the results of our training.
    The AutoML-generated model reached an accuracy of 94% (see *Figure 13.15*). Remember
    that accuracy is the fraction of classification predictions produced by the model
    that were correct on a test, set which is held automatically. The log-loss (for
    example, the cross-entropy between the model predictions and the label values)
    is also provided. In the case of log-loss, a lower value indicates a higher-quality
    model:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 点击建议的 URL，您可以查看我们训练的结果。AutoML 生成的模型达到了 94% 的准确率（见*图 13.15*）。记住，准确率是模型在自动保留的测试集上正确分类预测的比例。还提供了对数损失（例如，模型预测与标签值之间的交叉熵）。对于对数损失，较低的值表示更高质量的模型：
- en: '![Chart, line chart  Description automatically generated](img/B18331_13_15.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18331_13_15.png)'
- en: 'Figure 13.15: AutoML Tables – analyzing the results of our training'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.15：AutoML 表格 – 分析我们的训练结果
- en: 'In addition, the **Area Under the Receiver Operating Characteristic Curve**
    (**AUC ROC**) is represented. This ranges from zero to one, and a higher value
    indicates a higher-quality model. This statistic summarizes an AUC ROC curve,
    which is a graph showing the performance of a classification model at all classification
    thresholds. The **True Positive Rate** (**TPR**) (also known as “recall”) is:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**接收者操作特征曲线下面积**（**AUC ROC**）也被表示出来。该值范围从零到一，值越高表示模型质量越高。该统计数据总结了 AUC ROC
    曲线，这是一张显示分类模型在所有分类阈值下性能的图表。**真正率**（**TPR**）（也称为“召回率”）为：
- en: '![](img/B18331_13_001.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_13_001.png)'
- en: 'where *TP* is the number of true positives and *FN* is the number of false
    negatives. The **False Positive Rate** (**FPR**) is:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*TP* 是真正例的数量，*FN* 是假负例的数量。**假正率**（**FPR**）为：
- en: '![](img/B18331_13_002.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_13_002.png)'
- en: where *FP* is the number of false positives and *TN* is the number of true negatives.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*FP* 是假正例的数量，*TN* 是真负例的数量。
- en: 'A ROC curve plots TPR vs. FPR at different classification thresholds. In *Figure
    13.16* you will see the **Area Under the Curve** (**AUC**) for one threshold of
    a ROC curve:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线绘制了不同分类阈值下的真正率（TPR）与假正率（FPR）。在*图 13.16*中，您将看到 ROC 曲线一个阈值下的**曲线下面积**（**AUC**）：
- en: '![Chart, line chart  Description automatically generated](img/B18331_13_16.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18331_13_16.png)'
- en: 'Figure 13.16: AutoML Tables – deep dive on the results of our training'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.16：AutoML 表格 – 深入分析我们的训练结果
- en: 'It is possible to deep dive into the evaluation and access the confusion matrix
    (see *Figure 13.17*):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可以深入评估并访问混淆矩阵（见*图 13.17*）：
- en: '![A picture containing chart  Description automatically generated](img/B18331_13_17.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing chart  Description automatically generated](img/B18331_13_17.png)'
- en: 'Figure 13.17: AutoML Tables – additional deep dive on the results of our training'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.17：AutoML 表格 – 进一步深入分析我们的训练结果
- en: Note that manually crafted models available in [https://www.kaggle.com/uciml/adult-census-income/kernels](https://www.kaggle.com/uciml/adult-census-income/kernels)
    get to an accuracy of ˜86-90%. Therefore, our model generated with AutoML is definitively
    a very good result!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，手工制作的模型在 [https://www.kaggle.com/uciml/adult-census-income/kernels](https://www.kaggle.com/uciml/adult-census-income/kernels)
    上的准确率为 ˜86-90%。因此，我们通过 AutoML 生成的模型无疑是一个非常好的结果！
- en: 'We can also have a look at the importance of each feature in isolation, as
    shown in *Figure 13.18*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看每个特征在孤立情况下的重要性，如 *图 13.18* 所示：
- en: '![Chart, bar chart  Description automatically generated](img/B18331_13_18.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图表，条形图 描述自动生成](img/B18331_13_18.png)'
- en: 'Figure 13.18: Specific importance of each feature considered in isolation'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.18：孤立考虑每个特征的具体重要性
- en: 'If we are happy with our results, we can then deploy the model in production
    via **DEPLOY & TEST** (see *Figure 13.19*). We can decide to create a Docker container
    deployable at the edge or we can simply use an endpoint. Let’s go for this option
    and just use the default setting for each available choice:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对结果满意，可以通过 **DEPLOY & TEST** 将模型部署到生产环境中（参见 *图 13.19*）。我们可以选择创建一个可在边缘部署的
    Docker 容器，或者直接使用端点。我们选择后者，并对每个可用选项使用默认设置：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18331_13_19.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，Teams 描述自动生成](img/B18331_13_19.png)'
- en: 'Figure 13.19: AutoML Tables – deploying in production'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.19：AutoML 表格 – 在生产中部署
- en: 'Then it is possible to make online predictions of income by using a REST API
    (see [https://en.wikipedia.org/wiki/Representational_state_transfer](https://en.wikipedia.org/wiki/Representational_state_transfer)),
    using this command for the example we’re looking at in this chapter, as shown
    in *Figure 13.20*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 REST API 可以进行在线收入预测（参见 [https://en.wikipedia.org/wiki/Representational_state_transfer](https://en.wikipedia.org/wiki/Representational_state_transfer)），使用本章所示示例的命令，如
    *图 13.20* 所示：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_13_20.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 描述自动生成](img/B18331_13_20.png)'
- en: 'Figure 13.20: AutoML Tables – querying the deployed model in production'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.20：AutoML 表格 – 查询已部署的生产模型
- en: 'Put simply, we can say that Google Cloud ML is very focused on simplicity of
    use and efficiency for AutoML. Let’s summarize the main steps required (see *Figure
    13.21*):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们可以说 Google Cloud ML 非常注重 AutoML 的易用性和效率。让我们总结一下所需的主要步骤（参见 *图 13.21*）：
- en: The dataset is imported.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集已导入。
- en: Your dataset schema and labels are defined.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集的架构和标签已定义。
- en: The input features are automatically recognized.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入特征会被自动识别。
- en: AutoML performs magic by automatically doing feature engineering, creating a
    model, and tuning the hyperparameters.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AutoML 通过自动执行特征工程、创建模型并调整超参数来实现“魔法”。
- en: The automatically built model can then be evaluated.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动生成的模型可以进行评估。
- en: The model is then deployed in production.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型接着会在生产环境中部署。
- en: Of course, it is possible to repeat the steps 2-6 by changing the schema and
    the definition of the labels.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可以通过更改架构和标签定义来重复步骤 2-6。
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_13_21.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B18331_13_21.png)'
- en: 'Figure 13.21: AutoML Tables – the main steps required'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.21：AutoML 表格 – 所需的主要步骤
- en: 'In this section, we have seen an example of AutoML focused on ease of use and
    efficiency. The progress made is shown in Faes et al. [7], quoting the paper:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了一个关注易用性和效率的 AutoML 示例。Faes 等人[7]展示了取得的进展，引用了该论文：
- en: ”We show, to our knowledge, a first of its kind automated design and implementation
    of deep learning models for health-care application by non-AI experts, namely
    physicians. Although comparable performance to expert-tuned medical image classification
    algorithms was obtained in internal validations of binary and multiple classification
    tasks, more complex challenges, such as multilabel classification, and external
    validation of these models was insufficient. We believe that AI might advance
    medical care by improving efficiency of triage to subspecialists and the personalisation
    of medicine through tailored prediction models. The automated approach to prediction
    model design improves access to this technology, thus facilitating engagement
    by the medical community and providing a medium through which clinicians can enhance
    their understanding of the advantages and potential pitfalls of AI integration.”
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “据我们所知，这是首次由非人工智能专家（即医生）自动设计和实现深度学习模型用于医疗应用。尽管在二分类和多分类任务的内部验证中取得了与专家调优的医学图像分类算法相当的表现，但在更复杂的挑战（如多标签分类）和这些模型的外部验证方面尚显不足。我们相信，人工智能有可能通过提高分诊效率和个性化医疗（通过量身定制的预测模型）来推动医疗保健的发展。自动化的预测模型设计方法提高了技术的可访问性，从而促进了医疗社区的参与，并为临床医生提供了一个平台，帮助他们更好地理解人工智能集成的优势和潜在风险。”
- en: In this case, Cloud AutoML Tables has been used. So, let’s look at another example.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用了Cloud AutoML Tables。接下来，我们来看另一个例子。
- en: Using the Google Cloud AutoML Text solution
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Cloud AutoML文本解决方案
- en: 'In this section, we are going to build a classifier using AutoML. Let’s create
    a dataset for text from the Vertex AI console. We want to focus on the task of
    single-label classification:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用AutoML构建一个分类器。让我们从Vertex AI控制台创建一个文本数据集。我们要集中在单标签分类任务上：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_13_22.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 自动生成的描述](img/B18331_13_22.png)'
- en: 'Figure 13.22: AutoML Text classification – creating a dataset'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.22：AutoML文本分类—创建数据集
- en: We are going to use a dataset already available online (the happy moments dataset
    is stored in `cloud-ml-data/NL-classification/happiness.csv`), load it into a
    dataset named **happiness**, and perform single-label classification (as shown
    in *Figure 13.23*). This can take several minutes or more.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个已经在线可用的数据集（快乐时光数据集存储在`cloud-ml-data/NL-classification/happiness.csv`），将其加载到名为**happiness**的数据集中，并进行单标签分类（如*图13.23*所示）。这可能需要几分钟或更长时间。
- en: 'We will be emailed once processing completes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完成后，我们将通过电子邮件收到通知：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_23.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 自动生成的描述](img/B18331_13_23.png)'
- en: 'Figure 13.23: AutoML Text classification – creating the dataset'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.23：AutoML文本分类—创建数据集
- en: 'Once the dataset is loaded, you should be able to see that each text fragment
    is annotated with one category out of seven, as shown in *Figure 13.24*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集加载完成，您应该能够看到每个文本片段都被标注为七个类别中的一个，如*图13.24*所示：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_13_24.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 自动生成的描述](img/B18331_13_24.png)'
- en: 'Figure 13.24: AutoML Text classification – a sample of categories'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.24：AutoML文本分类—类别示例
- en: 'It is now time to start training the model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始训练模型了：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B18331_13_25.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，聊天或文本消息 自动生成的描述](img/B18331_13_25.png)'
- en: 'Figure 13.25: AutoML Text classification – start training'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.25：AutoML文本分类—开始训练
- en: 'By the end, the model is built, and it achieves a good precision of 90.2% and
    recall of 86.7%:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型建立完成，达到了90.2%的精度和86.7%的召回率：
- en: '![Table  Description automatically generated](img/B18331_13_26.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![表格 自动生成的描述](img/B18331_13_26.png)'
- en: 'Figure 13.26: AutoML Text classification – precision and recall'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.26：AutoML文本分类—精度与召回率
- en: 'We can also have a look at the precision-recall curve and precision-recall
    by threshold (see *Figure 13.27*). These curves can be used to calibrate the classifier,
    calibrating on the threshold (based on the prediction probabilities that are greater
    than the threshold):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看精度-召回率曲线以及基于阈值的精度-召回率（见*图13.27*）。这些曲线可以用于对分类器进行校准，根据阈值（基于预测概率大于阈值的值）进行校准：
- en: '![Chart, line chart  Description automatically generated](img/B18331_13_27.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 描述自动生成](img/B18331_13_27.png)'
- en: 'Figure 13.27: Precision-recall and Precision-recall by threshold'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.27：精度-召回率与阈值下的精度-召回率
- en: 'The confusion matrix is shown in *Figure 13.28*:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如*图 13.28*所示：
- en: '![Application, table  Description automatically generated](img/B18331_13_28.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![应用程序，表格 描述自动生成](img/B18331_13_28.png)'
- en: 'Figure 13.28: Confusion matrix for the text classification problem'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.28：文本分类问题的混淆矩阵
- en: Using the Google Cloud AutoML Video solution
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Google Cloud AutoML 视频解决方案
- en: 'In this solution, we are going to automatically build a new model for video
    classification. The intent is to be able to sort different video segments into
    various categories (or classes) based on their content. The first step is to create
    the dataset, as shown in *Figure 13.29*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个解决方案中，我们将自动构建一个用于视频分类的新模型。目的是能够根据视频的内容将不同的视频片段分类到各种类别（或类）中。第一步是创建数据集，如*图
    13.29*所示：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_13_29.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 描述自动生成](img/B18331_13_29.png)'
- en: 'Figure 13.29: AutoML Video intelligence – a classification problem'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.29：AutoML 视频智能 – 一个分类问题
- en: 'We are going to use a collection of about 5,000 videos available in a demo
    already stored in a GCP bucket on `automl-video-demo-data/hmdb_split1_5classes_all.csv`,
    as shown in *Figure 13.30*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用大约 5,000 个视频的集合，这些视频已存储在 GCP 存储桶中的一个示例数据集中，路径为`automl-video-demo-data/hmdb_split1_5classes_all.csv`，如*图
    13.30*所示：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_13_30.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 描述自动生成](img/B18331_13_30.png)'
- en: Figure 13.30\. Importing the demo dataset
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.30\. 导入示例数据集
- en: 'As usual, importing will take a while and we will be notified when it is done
    with an email. Once the videos are imported, we can preview them with their associated
    categories:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，导入过程需要一些时间，当完成时我们会收到一封电子邮件通知。一旦视频导入完成，我们可以预览它们及其关联的类别：
- en: '![Table  Description automatically generated](img/B18331_13_31.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_13_31.png)'
- en: 'Figure 13.31: AutoML Video intelligence – imported video preview'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.31：AutoML 视频智能 – 导入的视频预览
- en: 'We can now start to build a model. There are a number of options including
    training with AutoML, using AutoML at the edge for models to be exported at the
    edge, and custom models built on TensorFlow. Let’s use the default, as shown in
    *Figure 13.32*:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始构建模型了。有多种选项，包括使用 AutoML 进行训练，使用边缘设备的 AutoML 模型进行导出，或基于 TensorFlow 构建自定义模型。让我们使用默认选项，如*图
    13.32*所示：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_13_32.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B18331_13_32.png)'
- en: 'Figure 13.32: AutoML Video intelligence – warning to get more videos'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.32：AutoML 视频智能 – 提示获取更多视频
- en: 'In this case, we decide to run an experiment training with a few labels and
    divide the dataset into 20% training and 80% testing:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们决定进行一个实验训练，使用一些标签，并将数据集分为 20% 的训练集和 80% 的测试集：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_13_33.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B18331_13_33.png)'
- en: 'Figure 13.33: Test and Training dataset split'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.33：测试集和训练集划分
- en: 'Once the model is trained, you can access the results from the console (*Figure
    13.34*). In this case, we achieved a precision of 99.5% and a recall of 99.5%
    even though we were using only 20% of the labels for training in our experiment.
    We wanted to keep the training short and still achieve awesome results. You can
    play with the model, for instance, increasing the number of labeled videos available,
    to see how the performance will change:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，您可以从控制台访问结果（*图 13.34*）。在本例中，尽管我们只使用了 20% 的标签进行实验训练，但我们实现了 99.5% 的精度和
    99.5% 的召回率。我们希望将训练时间保持较短，同时仍能取得优异的结果。您可以尝试调整模型，例如增加可用的标记视频数量，看看性能如何变化：
- en: '![Graphical user interface, table  Description automatically generated](img/B18331_13_34.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，表格 描述自动生成](img/B18331_13_34.png)'
- en: 'Figure 13.34: AutoML Video intelligence – evaluating the results'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.34：AutoML 视频智能 – 评估结果
- en: 'Let’s have a detailed look at the results. For instance, we can analyze the
    precision/recall graph for different levels of threshold:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看一下结果。例如，我们可以分析不同阈值水平下的精度/召回率图：
- en: '![Chart, line chart  Description automatically generated](img/B18331_13_35.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 描述自动生成](img/B18331_13_35.png)'
- en: 'Figure 13.35: AutoML Video intelligence – precision and recall'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.35：AutoML视频智能 – 精度和召回率
- en: 'The confusion matrix shows examples of the wrong classification of shots:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵展示了错误分类的镜头示例：
- en: '![Application  Description automatically generated with low confidence](img/B18331_13_36.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![应用程序 描述自动生成，置信度较低](img/B18331_13_36.png)'
- en: 'Figure 13.36: AutoML Video intelligence – confusion matrix'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.36：AutoML视频智能 – 混淆矩阵
- en: Cost
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Training on GCP has different costs depending on the type of AutoML adopted;
    for example, training all the solutions presented in this chapter and serving
    models for testing had a cost of less than $10 in 2022\. This is, however, not
    including the initial six hours of free discount that were available for the account
    (around $150 were available at the time of writing). Depending on your organizational
    needs, this is likely to work out significantly less than the cost of buying expensive
    on-premises hardware.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP上的训练成本因采用的AutoML类型而异；例如，2022年在本章中介绍的所有解决方案的训练和模型服务测试的成本低于10美元。不过，这还未包括帐户初始时提供的六小时免费折扣（在撰写本文时，约有150美元可用）。根据您的组织需求，这可能显著低于购买昂贵本地硬件的成本。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The goal of AutoML is to enable domain experts who are not familiar with machine
    learning technologies to use ML techniques easily. The primary goal is to reduce
    the steep learning curve and the huge costs of handcrafting machine learning solutions
    by making the whole end-to-end machine learning pipeline (data preparation, feature
    engineering, and automatic model generation) more automated.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的目标是使不熟悉机器学习技术的领域专家能够轻松使用机器学习技术。其主要目标是通过使整个端到端的机器学习流程（数据准备、特征工程和自动模型生成）更加自动化，从而减少陡峭的学习曲线和手工制作机器学习解决方案的巨大成本。
- en: After reviewing the state-of-the-art solution available at the end of 2022,
    we discussed how to use Google Cloud AutoML both for text, videos, and images,
    achieving results comparable to the ones achieved with handcrafted models. AutoML
    is probably the fastest-growing research topic and interested readers can find
    the latest results at [https://www.automl.org/](https://www.automl.org/).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了2022年底的最先进解决方案后，我们讨论了如何使用Google Cloud AutoML来处理文本、视频和图像，并取得了与手工制作模型相当的效果。AutoML可能是目前发展最快的研究课题，感兴趣的读者可以在[https://www.automl.org/](https://www.automl.org/)找到最新的成果。
- en: The next chapter discusses the math behind deep learning, a rather advanced
    topic that is recommended if you are interested in understanding what is going
    on “under the hood” when you play with neural networks.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论深度学习背后的数学，这是一个相对先进的话题，如果你对理解在玩神经网络时“幕后发生的事情”感兴趣，推荐阅读。
- en: References
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Zoph, B., Le, Q. V. (2016). *Neural Architecture Search with Reinforcement Learning.*
    [http://arxiv.org/abs/1611.01578](http://arxiv.org/abs/1611.01578)
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zoph, B., Le, Q. V. (2016). *使用强化学习的神经架构搜索*。[http://arxiv.org/abs/1611.01578](http://arxiv.org/abs/1611.01578)
- en: Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., Dean, J. (2018). *Efficient Neural
    Architecture Search via Parameter Sharing*. [https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., Dean, J. (2018). *通过参数共享的高效神经架构搜索*。[https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268)
- en: 'Borsos, Z., Khorlin, A., Gesmundo, A. (2019). *Transfer NAS: Knowledge Transfer
    between Search Spaces with Transformer Agents*. [https://arxiv.org/abs/1906.08102](https://arxiv.org/abs/1906.08102)'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Borsos, Z., Khorlin, A., Gesmundo, A. (2019). *Transfer NAS: 使用Transformer代理在搜索空间之间进行知识转移*。[https://arxiv.org/abs/1906.08102](https://arxiv.org/abs/1906.08102)'
- en: 'Lu, Z., Whalen, I., Boddeti V., Dhebar, Y., Deb, K., Goodman, E., and Banzhaf,
    W. (2018). *NSGA-Net: Neural Architecture Search using Multi-Objective Genetic
    Algorithm*. [https://arxiv.org/abs/1810.03522](https://arxiv.org/abs/1810.03522)'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lu, Z., Whalen, I., Boddeti V., Dhebar, Y., Deb, K., Goodman, E., and Banzhaf,
    W. (2018). *NSGA-Net: 使用多目标遗传算法的神经架构搜索*。[https://arxiv.org/abs/1810.03522](https://arxiv.org/abs/1810.03522)'
- en: Bergstra, J., Bengio, Y. (2012). *Random search for hyper-parameter optimization*.
    [http://www.jmlr.org/papers/v13/bergstra12a.xhtml](http://www.jmlr.org/papers/v13/bergstra12a.xhtml)
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bergstra, J., Bengio, Y. (2012). *超参数优化的随机搜索*。[http://www.jmlr.org/papers/v13/bergstra12a.xhtml](http://www.jmlr.org/papers/v13/bergstra12a.xhtml)
- en: 'Jin, H., Song, Q., and Hu, X. (2019). *Auto-Keras: An Efficient Neural Architecture
    Search System*. [https://arxiv.org/abs/1806.10282](https://arxiv.org/abs/1806.10282)'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jin, H., Song, Q., 和 Hu, X.（2019）。 *Auto-Keras：一个高效的神经架构搜索系统*。 [https://arxiv.org/abs/1806.10282](https://arxiv.org/abs/1806.10282)
- en: 'Faes, L., et al. (2019). *Automated deep learning design for medical image
    classification by health-care professionals with no coding experience: a feasibility
    study*. The Lancet Digital Health Volume 1, Issue 5, September 2019\. Pages e232-e242\.
    [https://www.sciencedirect.com/science/article/pii/S2589750019301086](https://www.sciencedirect.com/science/article/pii/S2589750019301086%20)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Faes, L., 等人（2019）。 *无编程经验的医疗专业人员自动化深度学习设计，用于医学图像分类：可行性研究*。《柳叶刀数字健康》 第1卷，第5期，2019年9月。第e232-e242页。
    [https://www.sciencedirect.com/science/article/pii/S2589750019301086](https://www.sciencedirect.com/science/article/pii/S2589750019301086%20)
- en: Join our book’s Discord space
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起交流，并与超过 2000 名成员共同学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
