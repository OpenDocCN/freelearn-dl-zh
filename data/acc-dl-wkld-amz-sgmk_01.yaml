- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introducing Deep Learning with Amazon SageMaker
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍与 Amazon SageMaker 的深度学习
- en: '**Deep learning** (**DL**) is a fairly new but actively developing area of
    **machine learning** (**ML**). Over the past 15 years, DL has moved from research
    labs to our homes (such as smart homes and smart speakers) and cars (that is,
    self-driving capabilities), phones (for example, photo enhancement software),
    and applications you use every day (such as recommendation systems in your favorite
    video platform).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）是一个相对较新的但正在积极发展的**机器学习**（**ML**）领域。在过去的 15 年里，深度学习从研究实验室走向了我们的家庭（如智能家居和智能音响）、汽车（即自动驾驶功能）、手机（例如，照片增强软件）以及你每天使用的应用程序（如你最喜欢的视频平台中的推荐系统）。'
- en: DL models are achieving and, at times, exceeding human accuracy on tasks such
    as computer vision (object detection and segmentation, image classification tasks,
    and image generation) and language tasks (translation, entity extraction, and
    text sentiment analysis). Beyond these areas, DL is also actively applied to complex
    domains such as healthcare, information security, robotics, and automation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在计算机视觉（如物体检测与分割、图像分类任务和图像生成）以及语言任务（如翻译、实体提取和文本情感分析）等任务中，已经达到了甚至超越了人类的准确性。除这些领域外，深度学习还被广泛应用于复杂领域，如医疗保健、信息安全、机器人技术和自动化。
- en: We should expect that DL applications in these domains will only grow over time.
    With current results and future promises also come challenges when implementing
    DL models. But before talking about the challenges, let’s quickly refresh ourselves
    on what DL is.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该预期这些领域中的深度学习应用将随着时间的推移不断增长。随着当前成果和未来的承诺，也伴随而来的是实现深度学习模型时的挑战。但在讨论这些挑战之前，我们先来快速回顾一下深度学习是什么。
- en: 'In this chapter, we will do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将完成以下内容：
- en: We’ll get a quick refresher on DL and its challenges
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将快速回顾一下深度学习及其挑战
- en: We’ll provide an overview of Amazon SageMaker and its value proposition for
    DL projects
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将概述 Amazon SageMaker 以及它在深度学习项目中的价值主张
- en: We’ll provide an overview of the foundational SageMaker components – that is,
    managed training and hosting stacks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将概述 SageMaker 的基础组件——即托管训练和托管栈
- en: We’ll provide an overview of other key AWS services
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将概述其他关键的 AWS 服务
- en: 'These will be covered in the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下主题将涵盖：
- en: Exploring DL with Amazon SageMaker
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 探索深度学习
- en: Choosing Amazon SageMaker for DL workloads
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择 Amazon SageMaker 处理深度学习工作负载
- en: Exploring SageMaker’s managed training stack
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 SageMaker 的托管训练栈
- en: Using SageMaker’s managed hosting stack
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker 的托管托管栈
- en: Integration with AWS services
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 AWS 服务的集成
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'There are several hands-on code samples in this chapter. To follow along with
    them, you will need the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中有多个动手编码示例。要跟随这些示例，您需要以下内容：
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 AWS 账户和具有管理 Amazon SageMaker 资源权限的 IAM 用户。
- en: A computer or cloud host with Python3 and SageMaker SDK installed ([https://pypi.org/project/sagemaker/](https://pypi.org/project/sagemaker/)).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了 Python3 和 SageMaker SDK 的计算机或云主机（[https://pypi.org/project/sagemaker/](https://pypi.org/project/sagemaker/)）。
- en: Have the AWS CLI installed (see [https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.xhtml))
    and configured to use IAM user (see the instructions at [https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml)).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装 AWS CLI（请参见 [https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.xhtml)）并配置为使用
    IAM 用户（请参见 [https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml)
    的说明）。
- en: To use certain SageMaker instance types for training purposes, you will likely
    need to request a service limit increase in your AWS account. For more information,
    go to [https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用某些 SageMaker 实例类型进行训练，您可能需要请求在 AWS 账户中的服务限制增加。有关更多信息，请访问 [https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml)。
- en: All of the code in this chapter can be downloaded from [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码可以从 [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker)
    下载。
- en: Exploring DL with Amazon SageMaker
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 探索深度学习
- en: 'DL is a subset of the ML field, which uses a specific type of architecture:
    layers of learnable parameters connected to each other. In this architecture,
    each layer is “learning” a representation from a training dataset. Each new training
    data sample slightly tweaks the learnable parameters across all the layers of
    the model to minimize the loss function. The number of stacked layers constitutes
    the “depth” of the model. At inference time (that is, when we use our model to
    infer output from the input signal), each layer receives input from the previous
    layer’s output, calculates its representation based on the input, and sends it
    to the next layer.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习领域的一个子集，它使用一种特定的架构：各层之间相互连接的可学习参数。在这种架构中，每一层都在从训练数据集中“学习”一种表示。每一组新的训练数据样本会稍微调整模型各层中的可学习参数，以最小化损失函数。堆叠的层数构成了模型的“深度”。在推理时（即，当我们使用模型从输入信号推理输出时），每一层接收来自前一层输出的输入，根据输入计算其表示，并将其传递给下一层。
- en: '![Figure 1.1 – Fully connected DL network ](img/B17519_01_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 全连接深度学习网络](img/B17519_01_001.jpg)'
- en: Figure 1.1 – Fully connected DL network
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 全连接深度学习网络
- en: Simple DL models can consist of just a few fully connected layers, while **state-of-the-art**
    (**SOTA**) models have hundreds of layers with millions and billions of learnable
    parameters. And the model size continues to grow. For example, let’s take a look
    at the evolution of the GPT family of models for various NLP tasks. The GPT-1
    model was released in 2018 and had 110 million parameters; the GPT-2 model released
    in 2019 had 1,500 million parameters; the latest version, GPT-3, was released
    in 2020 and has 175 billion parameters!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的深度学习模型可能仅由几个全连接层组成，而**最先进的**（**SOTA**）模型则有数百个层次，包含数百万甚至数十亿个可学习的参数。而且，模型的规模还在不断增长。例如，我们来看看
    GPT 系列模型在各种自然语言处理任务中的演变。GPT-1 模型于 2018 年发布，拥有 1.1 亿个参数；2019 年发布的 GPT-2 模型拥有 15
    亿个参数；最新版本 GPT-3 于 2020 年发布，拥有 1750 亿个参数！
- en: 'As the number of parameters grows, DL practitioners deal with several engineering
    problems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着参数数量的增加，深度学习从业者面临着若干工程问题：
- en: How do we fit models into instance memory at training time? If it’s not possible,
    then how do we split the model between the memory of multiple GPU devices and/or
    compute nodes?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在训练时将模型放入实例内存？如果不可能，那么如何将模型拆分到多个 GPU 设备和/或计算节点的内存中？
- en: How can we organize communication between multiple nodes at training time so
    that the overall model can aggregate learnings from individual nodes?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在训练时组织多个节点之间的通信，以便整体模型能够汇总各个节点的学习成果？
- en: Layer internals are also becoming more complex and require more computing power.
    DL models also typically require vast amounts of data in specific formats.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 层内部结构也变得越来越复杂，需要更多的计算能力。深度学习模型通常还需要大量特定格式的数据。
- en: 'So, to be able to successfully train and use SOTA DL models, ML engineers need
    to solve the following tasks (beyond implementing the SOTA model, of course):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了能够成功地训练和使用最先进的深度学习模型，机器学习工程师需要解决以下任务（当然，除了实现最先进的模型）：
- en: Gain access to a large number of specialized compute resources during training
    and inference
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练和推理过程中，获得大量专用计算资源
- en: Set up and maintain a software stack (for example, GPU libraries, DL frameworks,
    and acceleration libraries)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置并维护软件堆栈（例如，GPU 库、深度学习框架和加速库）
- en: Implement, manage, and optimize distributed training jobs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现、管理和优化分布式训练任务
- en: Implement, deploy, and monitor inference pipelines
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现、部署和监控推理管道
- en: Organize time- and cost-efficient experiments when tuning model performance
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调优模型性能时，组织高效的时间和成本实验
- en: Pre-process, label, and access large datasets (GBs and TBs of data)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理、标记和访问大数据集（数 GB 和 TB 的数据）
- en: As you can see, these tasks are not necessarily related to solving any specific
    business problem. However, you need to get all these components right to ensure
    that a particular business case can be solved using DL models with the highest
    possible accuracy in time and within expected budgets.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这些任务不一定与解决特定的业务问题相关。然而，你需要将这些组件都配置正确，以确保能够使用深度学习模型在时间和预算的范围内，以最高的准确度解决某一特定的业务问题。
- en: Using SageMaker
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker
- en: Amazon SageMaker is an AWS service that promises to simplify the lives of ML
    practitioners by removing “undifferentiated heavy lifting” (such as the tasks
    mentioned previously) and lets you focus on actually solving business problems
    instead. It integrates various functional capabilities to build, train, and deploy
    ML models. SageMaker was first introduced in late 2017 and has since expanded
    greatly, adding more than 200 features in 2020 alone according to AWS’s *Re:invent
    2020* keynotes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker是AWS的一项服务，旨在通过消除“无差异的繁重工作”（例如前面提到的任务）来简化机器学习从业者的工作，让你能专注于实际解决业务问题。它集成了各种功能，帮助构建、训练和部署机器学习模型。SageMaker首次推出是在2017年末，之后大幅扩展，仅在2020年，根据AWS的*Re:invent
    2020*大会的主题演讲，便新增了200多项功能。
- en: 'Amazon SageMaker caters to a broad audience and set of use cases, including
    the following (this is a non-exclusive list):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker面向广泛的受众和使用场景，包括以下内容（这只是一个非独占的列表）：
- en: Developers without much ML background
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有太多机器学习背景的开发者
- en: Enterprise data science teams
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业数据科学团队
- en: Leading research organizations who are looking to run cutting-edge research
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领先的研究机构希望开展前沿研究
- en: SageMaker is a managed service as it abstracts the management of underlying
    compute resources and software components via APIs. AWS customers use these APIs
    to create training jobs, manage model artifacts, and deploy models for inference
    on Amazon SageMaker. AWS is responsible for the high availability of SageMaker
    resources and provides respective **Service-Level Agreements** (**SLAs**). Amazon
    SageMaker has a “pay as you go” model, with customers paying only for the resources
    they consume.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker是一个托管服务，通过API抽象了底层计算资源和软件组件的管理。AWS客户使用这些API创建训练任务，管理模型工件，并将模型部署到Amazon
    SageMaker进行推理。AWS负责SageMaker资源的高可用性，并提供相应的**服务级别协议**（**SLAs**）。Amazon SageMaker采用“按需付费”模式，客户仅为实际使用的资源付费。
- en: In this book, we will explore SageMaker capabilities that are relevant for DL
    models and workloads, and we will build end-to-end solutions for popular DL use
    cases, such as **Natural Language Processing** (**NLP**) and **Computer Vision**
    (**CV**).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将探索与深度学习模型和工作负载相关的SageMaker功能，并为流行的深度学习用例（如**自然语言处理**（**NLP**）和**计算机视觉**（**CV**））构建端到端解决方案。
- en: 'We will focus on the following Amazon SageMaker capabilities:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于以下Amazon SageMaker功能：
- en: 'Model development phase:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型开发阶段：
- en: Data preparation using SageMaker GroundTruth
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker GroundTruth进行数据准备
- en: Data pre- and post-processing using SageMaker Processing
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Processing进行数据的预处理和后处理
- en: Integration with data storage solutions – Amazon S3, Amazon EFS, and Amazon
    FSx for Lustre
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据存储解决方案的集成 – Amazon S3、Amazon EFS 和 Amazon FSx for Lustre
- en: Developing models using SageMaker Studio IDE and Notebooks
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Studio IDE和Notebooks开发模型
- en: 'Training phase:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练阶段：
- en: Managed training instances and software stack
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管的训练实例和软件堆栈
- en: DL containers for TensorFlow and Pytorch frameworks
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持TensorFlow和Pytorch框架的深度学习容器
- en: Implementing distributed training using SageMaker’s DataParallel and ModelParallel
    libraries
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker的DataParallel和ModelParallel库实现分布式训练
- en: Monitoring and debugging training with SageMaker Debugger
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger进行训练监控和调试
- en: 'Inference:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理：
- en: Managed hosting platform for batch and real-time inference
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管的批处理和实时推理平台
- en: Model monitoring at inference time
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理时对模型进行监控
- en: Compute instances for DL serving
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为深度学习服务计算实例
- en: As we progress through the book, we will also learn how to use the SageMaker
    API and SDKs to programmatically manage our resources. Additionally, we will discuss
    optimization strategies and SageMaker capabilities to reduce cost and time-to-market.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的进程中，我们还将学习如何使用SageMaker API和SDK来编程管理我们的资源。此外，我们还将讨论优化策略和SageMaker功能，以降低成本并缩短上市时间。
- en: This book focuses on DL capabilities, so we will set several SageMaker features
    and capabilities aside. You can explore Amazon SageMaker further by reading the
    SageMaker documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.xhtml))
    and practical blog posts ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/sagemaker/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/sagemaker/)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专注于深度学习功能，因此我们将暂时搁置一些SageMaker的功能和特点。你可以通过阅读SageMaker文档（[https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.xhtml)）和实践博客（[https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/sagemaker/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/sagemaker/)）进一步探索Amazon
    SageMaker。
- en: In the next section, we’ll find out exactly how SageMaker can help us with DL
    workloads.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将了解SageMaker如何帮助我们处理深度学习工作负载。
- en: Choosing Amazon SageMaker for DL workloads
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择Amazon SageMaker进行深度学习工作负载
- en: As discussed earlier, DL workloads present several engineering challenges due
    to their need to access high quantities of specialized resources (primarily GPU
    devices and high-throughput storage solutions). However, managing a software stack
    can also present a challenge as new versions of ML and DL frameworks are released
    frequently. Due to high associated costs, it’s also imperative to organize your
    training and inference efficiently to avoid waste.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习工作负载由于需要访问大量专门资源（主要是GPU设备和高吞吐量存储解决方案），因此面临多种工程挑战。然而，管理软件栈也是一个挑战，因为机器学习和深度学习框架会频繁发布新版本。由于相关成本较高，因此必须高效地组织训练和推理工作，以避免浪费。
- en: Let’s review how SageMaker can address these challenges.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来回顾一下SageMaker如何应对这些挑战。
- en: Managed compute and storage infrastructure
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 托管的计算和存储基础设施
- en: SageMaker provides a fully managed compute infrastructure for your training
    and inference workloads. SageMaker Training and Inference clusters can scale to
    tens and up to hundreds of individual instances within minutes. This can be particularly
    useful in scenarios where you need to access a large compute cluster with short
    notice and for a limited period (for example, you need to train a complex DL model
    on a large dataset once every couple of months). As with other AWS services, SageMaker
    resources provide advanced auto-scaling features for inference endpoints so that
    customers can match demand without resource overprovisioning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker为你的训练和推理工作负载提供了完全托管的计算基础设施。SageMaker训练和推理集群能够在几分钟内扩展至几十个甚至上百个独立实例。这在需要快速访问大型计算集群且只需要短时间使用的场景中尤其有用（例如，你需要每隔几个月就训练一次复杂的深度学习模型，并且数据集很大）。与其他AWS服务一样，SageMaker资源为推理端点提供了先进的自动扩展功能，以便客户根据需求调整资源，而无需过度配置资源。
- en: You can also choose from a growing number of available compute instances based
    on the requirements of specific DL models and types of workload. For instance,
    in many scenarios, you would need to use a GPU-based instance to train your DL
    model, while it may be possible to use cheaper CPU instances at inference time
    without this having an impact on end user performance. SageMaker gives you the
    flexibility to choose the most optimal instances for particular DL models and
    tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以根据特定深度学习模型和工作负载类型的需求，从越来越多的计算实例中进行选择。例如，在许多场景下，你可能需要使用基于GPU的实例来训练你的深度学习模型，而在推理时，可能可以使用较便宜的CPU实例，而不会影响最终用户性能。SageMaker使你能够根据特定的深度学习模型和任务选择最优实例。
- en: In case of failure, AWS will automatically replace faulty instances without
    any customer intervention.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在发生故障时，AWS会自动替换故障实例，无需客户干预。
- en: These SageMaker features greatly benefit customers as SageMaker simplifies capacity
    planning and operational management of your ML infrastructure.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些SageMaker功能极大地惠及客户，因为SageMaker简化了机器学习基础设施的容量规划和运营管理。
- en: Managed DL software stacks
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理的深度学习软件栈
- en: To build, train, and deploy DL models, you need to use various frameworks and
    software components to perform specialized computations and communication between
    devices and nodes in distributed clusters. Creating and maintaining software stacks
    across various development environments can be labor-intensive.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建、训练和部署深度学习模型，你需要使用各种框架和软件组件来执行专门的计算以及在分布式集群中进行设备和节点之间的通信。在不同开发环境中创建和维护软件栈可能会非常繁琐。
- en: To address these needs, as part of the SageMaker ecosystem, AWS provides multiple
    pre-built open source Docker containers for popular DL frameworks such as PyTorch,
    TensorFlow, MXNet, and others. These containers are built and tested by AWS and
    optimized for specific tasks (for instance, different containers for training
    and inference) and compute platforms (CPU or GPU-based containers, different versions
    of CUDA toolkits, and others).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些需求，作为 SageMaker 生态系统的一部分，AWS 提供了多个预构建的开源 Docker 容器，支持流行的深度学习框架，如 PyTorch、TensorFlow、MXNet
    等。这些容器由 AWS 构建和测试，并针对特定任务（例如，训练与推理的不同容器）和计算平台（基于 CPU 或 GPU 的容器，不同版本的 CUDA 工具包等）进行了优化。
- en: Since Docker containers provide interoperability and encapsulation, developers
    can utilize pre-built SageMaker containers to build and debug their workloads
    locally before deploying them to a cloud cluster to shorten the development cycle.
    You also have the flexibility to extend or modify SageMaker containers based on
    specific requirements.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Docker 容器提供了互操作性和封装性，开发人员可以利用预构建的 SageMaker 容器，在将工作负载部署到云集群之前，先在本地构建和调试它们，从而缩短开发周期。你还可以根据特定需求扩展或修改
    SageMaker 容器。
- en: Advanced operational capabilities
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级操作功能
- en: 'While Amazon SageMaker utilizes several popular open source solutions for DL,
    it also provides several unique capabilities to address certain challenges when
    operationalizing your ML workloads, such as the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Amazon SageMaker 利用多个流行的开源深度学习解决方案，但它还提供了一些独特的功能来解决在实现 ML 工作负载时遇到的特定挑战，例如：
- en: SageMaker Debugger
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Debugger
- en: SageMaker Model Monitor
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker 模型监控
- en: SageMaker’s DataParallel/ModelParallel distributed training libraries
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker 的 DataParallel/ModelParallel 分布式训练库
- en: Let’s move on next to look at integration with other AWS services.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们继续查看与其他 AWS 服务的集成。
- en: Integration with other AWS services
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与其他 AWS 服务的集成
- en: Amazon SageMaker is well integrated with other AWS services so that developers
    can build scalable, performant, and secure workloads.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 与其他 AWS 服务紧密集成，帮助开发人员构建可扩展、高效且安全的工作负载。
- en: In the next section, we’ll see how Amazon SageMaker’s managed training stack
    can be leveraged to run DL models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将看看如何利用 Amazon SageMaker 的托管训练堆栈来运行深度学习模型。
- en: Exploring SageMaker’s managed training stack
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 SageMaker 的托管训练堆栈
- en: 'Amazon SageMaker provides a set of capabilities and integration points with
    other AWS services to configure, run, and monitor ML training jobs. With SageMaker
    managed training, developers can do the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供一套功能和与其他 AWS 服务的集成点，用于配置、运行和监控机器学习训练任务。通过 SageMaker 托管训练，开发人员可以执行以下操作：
- en: Choose from a variety of built-in algorithms and containers, as well as BYO
    models
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各种内置算法和容器中选择，或者使用自带的模型（BYO 模型）
- en: Choose from a wide range of compute instances, depending on the model requirements
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据模型需求，从广泛的计算实例中选择
- en: Debug and profile their training process in near-real-time using SageMaker Debugger
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Debugger 调试和分析其训练过程，接近实时
- en: Run bias detection and model explainability jobs
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行偏差检测和模型可解释性任务
- en: Run incremental training jobs, resume from checkpoints, and use spot instances
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行增量训练任务，从检查点恢复并使用 Spot 实例
- en: Spot instances
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Spot 实例
- en: Amazon EC2 Spot Instances provides customers with access to unused compute capacity
    at a lower price point (up to 90%). Spot instances will be released when someone
    else claims them, which results in workload interruption.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EC2 Spot 实例为客户提供了以较低价格（最高可达 90%）访问未使用的计算资源的机会。当其他人抢占这些实例时，Spot 实例会被释放，导致工作负载中断。
- en: Run model tuning jobs to search for optimal combinations of model hyperparameters
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行模型调优任务，搜索最优的模型超参数组合
- en: Organize training jobs in a searchable catalog of experiments
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可搜索的实验目录中组织训练任务
- en: 'Amazon SageMaker provides the following capabilities out of the box:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供以下开箱即用的功能：
- en: Provisioning, bootstrapping, and tearing down training nodes
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置、引导和销毁训练节点
- en: Capturing training job logs and metrics
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获训练任务的日志和指标
- en: 'In this section, we will go over all the stages of a SageMaker training job
    and the components involved. Please refer to the following diagram for a visual
    narrative that provides a step-by-step guide on creating, managing, and monitoring
    your first SageMaker training job. We will address the advanced features of SageMaker’s
    managed training stack in *Part 2* of this book:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步介绍 SageMaker 训练任务的所有阶段及其相关组件。请参考下图，图中提供了创建、管理和监控您第一个 SageMaker 训练任务的逐步指南。我们将在本书的
    *第二部分* 讨论 SageMaker 管理训练堆栈的高级功能：
- en: '![Figure 1.2 – Amazon SageMaker training stack ](img/B17519_01_002.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – Amazon SageMaker 训练架构](img/B17519_01_002.jpg)'
- en: Figure 1.2 – Amazon SageMaker training stack
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – Amazon SageMaker 训练架构
- en: Let’s walk through each step in turn.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解每个步骤。
- en: We will also provide code snippets to illustrate how to perform a SageMaker
    training job configuration using the SageMaker Python SDK ([https://sagemaker.readthedocs.io/en/stable/](https://sagemaker.readthedocs.io/en/stable/)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将提供代码示例，说明如何使用 SageMaker Python SDK 配置 SageMaker 训练任务（[https://sagemaker.readthedocs.io/en/stable/](https://sagemaker.readthedocs.io/en/stable/)）。
- en: Step 1 – configuring and creating a training job
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 配置并创建训练任务
- en: You can instantiate a SageMaker training job via an API call.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过 API 调用实例化 SageMaker 训练任务。
- en: SageMaker defines several mandatory configuration parameters that need to be
    supplied by you. They are listed as follows.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 定义了几个必须提供的配置参数。它们如下所示。
- en: Choosing an algorithm to train
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择训练算法
- en: 'Amazon SageMaker supports several types of ML algorithms:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 支持多种类型的机器学习算法：
- en: '**Built-in algorithms** are available out of the box for all SageMaker users.
    At the time of writing, 18 built-in algorithms cover a variety of use cases, including
    DL algorithms for computer vision and NLP tasks. The user is only responsible
    for providing algorithm hyperparameters.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内置算法**是所有 SageMaker 用户开箱即用的。截止到本文写作时，18 个内置算法涵盖了各种使用场景，包括计算机视觉和自然语言处理任务的深度学习算法。用户只需要负责提供算法的超参数。'
- en: '**Custom algorithms** are developed by the user. AWS is not responsible for
    training logic in this case. The training script will be executed inside a Docker
    container. Developers can choose to use AWS-authored Docker images with pre-installed
    software dependencies or can use BYO Docker images.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义算法**由用户开发。在这种情况下，AWS 不负责训练逻辑。训练脚本将在 Docker 容器内执行。开发人员可以选择使用 AWS 提供的预装软件依赖的
    Docker 镜像，也可以使用自带 Docker 镜像。'
- en: '**Marketplace algorithms** are developed by third-party vendors and available
    via the AWS Marketplace. Similar to built-in algorithms, they typically offer
    a fully managed experience where the user is responsible for providing algorithm
    hyperparameters. Unlike built-in algorithms, which are free to use, the user usually
    pays a fee for using marketplace algorithms.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场算法**由第三方供应商开发，并通过 AWS Marketplace 提供。与内置算法类似，它们通常提供完全托管的体验，用户负责提供算法的超参数。不同于内置算法是免费的，使用市场算法时用户通常需要支付费用。'
- en: Defining an IAM role
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义 IAM 角色
- en: Amazon SageMaker relies on the Amazon IAM service and, specifically, IAM roles
    to define which AWS resources and services can be accessed from the training job.
    That’s why, whenever scheduling a SageMaker training job, you need to provide
    an IAM role, which will be then assigned to training nodes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 依赖于 Amazon IAM 服务，特别是 IAM 角色，用于定义哪些 AWS 资源和服务可以从训练任务中访问。这就是为什么在每次调度
    SageMaker 训练任务时，都需要提供一个 IAM 角色，之后该角色会被分配给训练节点。
- en: Defining a training cluster
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义训练集群
- en: Another set of required parameters defines the hardware configuration of the
    training cluster, which includes several compute instances, types of instances,
    and instance storage.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组必需的参数定义了训练集群的硬件配置，包括多个计算实例、实例类型以及实例存储。
- en: It’s recommended that you carefully choose your instance type based on specific
    requirements. At a minimum, ML engineers need to understand which compute device
    is used at training time. For example, in most cases for DL models, you will likely
    need to use a GPU-based instance, while many classical ML algorithms (such as
    linear regression or Random Forest) are CPU-bound.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 建议根据具体需求仔细选择实例类型。至少，ML 工程师需要了解训练时使用的是哪种计算设备。例如，在大多数深度学习（DL）模型的情况下，通常需要使用基于 GPU
    的实例，而许多经典的机器学习算法（如线性回归或随机森林）则是 CPU 绑定型的。
- en: ML engineers also need to consider how many instances to provision. When provisioning
    multiple nodes, you need to make sure that your algorithm and training script
    support distributed training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程师还需要考虑应该配置多少实例。在配置多个节点时，您需要确保您的算法和训练脚本支持分布式训练。
- en: Built-in algorithms usually provide recommended instance types and counts in
    their public documentation. They also define whether distributed training is supported
    or not. In the latter case, you should configure a single-node training cluster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 内置算法通常会在其公共文档中提供推荐的实例类型和数量。它们还会定义是否支持分布式训练。如果不支持分布式训练，您应该配置单节点训练集群。
- en: Defining training data
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义训练数据
- en: 'Amazon SageMaker supports several storage solutions for training data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 支持几种用于训练数据的存储解决方案：
- en: '**Amazon S3**: This is a low-cost, highly durable, and highly available object
    storage. This is considered a default choice for storing training datasets. Amazon
    S3 supports two types of input mode (also defined in training job configuration)
    for training datasets:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3**：这是一个低成本、高耐久性且高可用性的对象存储。它被视为存储训练数据集的默认选择。Amazon S3 支持两种输入模式（也在训练作业配置中定义）用于训练数据集：'
- en: '**File**: Amazon SageMaker copies the training dataset from the S3 location
    to a local directory'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件**：Amazon SageMaker 将训练数据集从 S3 位置复制到本地目录'
- en: '**Pipe**: Amazon SageMaker streams data directly from S3 to the container via
    a Unix-named pipe'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：Amazon SageMaker 通过 Unix 命名管道直接从 S3 流式传输数据到容器'
- en: '**FastFile**: A new file streaming capability provided by Amazon SageMaker.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FastFile**：Amazon SageMaker 提供的一项新的文件流能力。'
- en: '**Amazon EFS**: This is an elastic filesystem service. If it’s used to persist
    training data, Amazon SageMaker will automatically mount the training instance
    to a shared filesystem.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon EFS**：这是一个弹性文件系统服务。如果用于持久化训练数据，Amazon SageMaker 会自动将训练实例挂载到共享文件系统。'
- en: '**Amazon FSx for Luster**: This is a high-performant shared filesystem optimized
    for the lowest latency and highest throughput.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon FSx for Lustre**：这是一个高性能的共享文件系统，优化了最低延迟和最高吞吐量。'
- en: Before training can begin, you need to make sure that data is persisted in one
    of these solutions, and then provide the location of the datasets.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始之前，您需要确保数据已经存储在这些解决方案中的某个位置，然后提供数据集的位置。
- en: Please note that you can provide the locations of several datasets (for example,
    training, test, and evaluation sets) in your training job.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以在训练作业中提供多个数据集的位置（例如，训练集、测试集和评估集）。
- en: Picking your algorithm hyperparameters
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择您的算法超参数
- en: While it’s not strictly mandatory, in most cases, you will need to define certain
    hyperparameters of the algorithm. Examples of such hyperparameters include the
    batch size, number of training epochs, and learning rate.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是严格要求的，但在大多数情况下，您需要定义算法的某些超参数。此类超参数的示例包括批量大小、训练轮数和学习率。
- en: At training time, these hyperparameters will be passed to the training script
    as command-line arguments. In the case of custom algorithms, developers are responsible
    for parsing and setting hyperparameters in the training script.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，这些超参数将作为命令行参数传递给训练脚本。在自定义算法的情况下，开发人员需要负责在训练脚本中解析并设置超参数。
- en: Defining the training metrics
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义训练度量指标
- en: Metrics are another optional but important parameter. SageMaker provides out-of-the-box
    integration with Amazon CloudWatch to stream training logs and metrics. In the
    case of logs, SageMaker will automatically stream `stdout` and `stderr` from the
    training container to CloudWatch.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量指标**是另一个可选但重要的参数。SageMaker 提供了与 Amazon CloudWatch 的开箱即用集成，可以实时传输训练日志和度量指标。在日志的情况下，SageMaker
    会自动将 `stdout` 和 `stderr` 从训练容器传输到 CloudWatch。'
- en: stdout and stderr
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: stdout 和 stderr
- en: '`stdout` and `stderr` are standard data streams in Linux and Unix-like OSs.
    Every time you run a Linux command, these data streams are established automatically.
    Normal command output is sent to `stdout`; any error messages are sent to `stderr`.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`stdout` 和 `stderr` 是 Linux 和类 Unix 操作系统中的标准数据流。每次运行 Linux 命令时，这些数据流都会自动建立。正常的命令输出会发送到
    `stdout`；任何错误消息则会发送到 `stderr`。'
- en: In the case of metrics, the user needs to define the regex pattern for each
    metric first. At training time, the SageMaker utility running on the training
    instance will monitor `stdout` and `stderr` for the regex pattern match, then
    extract the value of the metric and submit the metric name and value to CloudWatch.
    As a result, developers can monitor training processes in CloudWatch in near-real
    time.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指标，用户需要首先定义每个指标的正则表达式模式。在训练时，运行在训练实例上的SageMaker实用程序将监控`stdout`和`stderr`中的正则表达式模式匹配，然后提取该指标的值并将指标名称和值提交到CloudWatch。因此，开发人员可以在CloudWatch中近实时地监控训练过程。
- en: Some common examples of training metrics include loss value and accuracy measures.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的训练指标示例包括损失值和准确性度量。
- en: Configuring a SageMaker training job for image classification
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置SageMaker训练任务以进行图像分类
- en: 'In the following Python code sample, we will demonstrate how to configure a
    simple training job for a built-in **image classification** algorithm ([https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.xhtml)):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的Python代码示例中，我们将演示如何配置一个简单的训练任务，使用内置的**图像分类**算法（[https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.xhtml)）：
- en: 'Begin with your initial imports:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从您的初始导入开始：
- en: '[PRE0]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `get_execution_role()` method allows you to get the current IAM role. This
    role will be used to call the SageMaker APIs, whereas `sagemaker.Session()` stores
    the context of the interaction with SageMaker and other AWS services such as S3:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`get_execution_role()`方法允许您获取当前的IAM角色。此角色将用于调用SageMaker API，而`sagemaker.Session()`则存储与SageMaker及其他AWS服务（如S3）的交互上下文：'
- en: '[PRE1]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The`.image_uris.retrieve()` method allows you to identify the correct container
    with the built-in image classification algorithm. Note that if you choose to use
    custom containers, you will need to specify a URI for your specific training container:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.image_uris.retrieve()`方法允许您识别与内置图像分类算法相关的正确容器。请注意，如果选择使用自定义容器，您需要为您的特定训练容器指定一个URI：'
- en: '[PRE2]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the number of instances in the training cluster. Since image classification
    supports distributed training, we can allocate more than one instance to the training
    cluster to speed up training:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练集群中的实例数量。由于图像分类支持分布式训练，我们可以为训练集群分配多个实例以加速训练：
- en: '[PRE3]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The image classification algorithm requires GPU-based instances, so we will
    choose to use a SageMaker P2 instance type:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像分类算法需要基于GPU的实例，因此我们将选择使用SageMaker P2实例类型：
- en: '[PRE4]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we must define the location of training and validation datasets. Note
    that the image classification algorithm supports several data formats. In this
    case, we choose to use the JPG file format, which also requires `.lst` files to
    list all the available images:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须定义训练和验证数据集的位置。请注意，图像分类算法支持多种数据格式。在本例中，我们选择使用JPG文件格式，这还需要`.lst`文件列出所有可用的图像：
- en: '[PRE5]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Configure the training hyperparameters:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置训练的超参数：
- en: '[PRE6]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Configure the `Estimator` object, which encapsulates training job configuration:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`Estimator`对象，它封装了训练任务的配置：
- en: '[PRE7]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `fit()` method submits the training job to the SageMaker API. If there
    are no issues, you should observe a new training job instance in your AWS Console.
    You can do so by going to **Amazon SageMaker** | **Training** | **Training Jobs**:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit()`方法将训练任务提交给SageMaker API。如果没有问题，您应该会在AWS控制台中看到一个新的训练任务实例。您可以通过进入**Amazon
    SageMaker** | **Training** | **Training Jobs**来查看：'
- en: '[PRE8]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we’ll provision the training cluster.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置训练集群。
- en: Step 2 – provisioning the SageMaker training cluster
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 配置SageMaker训练集群
- en: 'Once you submit a request for the training job, SageMaker automatically does
    the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 提交训练任务请求后，SageMaker会自动执行以下操作：
- en: Allocates the requested number of training nodes
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配请求的训练节点数量
- en: Allocates the Amazon EBS volumes and mounts them on the training nodes
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配Amazon EBS卷并将其挂载到训练节点上
- en: Assigns an IAM role to each node
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个节点分配IAM角色
- en: Bootstraps various utilities (such as Docker, SageMaker toolkit libraries, and
    so on)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动各种实用工具（如Docker、SageMaker工具包库等）
- en: Defines the training configuration (hyperparameters, input data configuration,
    and so on) as an environment variable
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练配置（超参数、输入数据配置等）定义为环境变量
- en: Next up is the training data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练数据。
- en: Step 3 – SageMaker accesses the training data
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – SageMaker访问训练数据
- en: 'When your training cluster is ready, SageMaker establishes access to training
    data for compute instances. The exact mechanism to access training data depends
    on your storage solution:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的训练集群准备就绪时，SageMaker为计算实例建立访问训练数据的权限。访问训练数据的确切机制取决于您的存储解决方案：
- en: If data is stored in S3 and the input mode is **File**, then data will be downloaded
    to instance EBS volumes. Note that depending on the dataset’s size, it may take
    minutes to download the data.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据存储在S3中并且输入模式为**File**，则数据将下载到实例的EBS卷上。请注意，根据数据集的大小，下载数据可能需要几分钟。
- en: If the data is stored in S3 and the input mode is **Pipe**, then the data will
    be streamed from S3 at training time as needed.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据存储在S3中并且输入模式为**Pipe**，则数据将根据需要在训练时从S3流式传输。
- en: If the data is stored in S3 and the input mode is **FastFile**, then the training
    program will access the files as if they are stored on training nodes. However,
    under the hood, the files will be streamed from S3.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据存储在S3中，并且输入模式为**FastFile**，则训练程序将像文件存储在训练节点上一样访问这些文件。但在底层，文件将从S3流式传输。
- en: If the data is stored in EFS or FSx for Luster, then the training nodes will
    be mounted on the filesystem.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据存储在EFS或FSx for Luster中，则训练节点将挂载在文件系统上。
- en: The training continues with deploying the container.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 训练继续部署容器。
- en: Step 4 – SageMaker deploys the training container
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 - SageMaker部署训练容器
- en: SageMaker automatically pulls the training images from the ECR repository. Note
    that built-in algorithms abstract the underlying training images so that users
    don’t have to define the container image, just the algorithm to be used.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker会自动从ECR仓库中拉取训练映像。请注意，内置算法会抽象出底层的训练映像，使用户无需定义容器映像，只需定义要使用的算法。
- en: Step 5 – SageMaker starts and monitors the training job
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 - SageMaker启动并监控训练作业
- en: 'To start the training job, SageMaker issues the following command on all training
    nodes:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动训练作业，SageMaker在所有训练节点上执行以下命令：
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If the training cluster has instances with GPU devices, then `nvidia-docker`
    will be used.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练集群具有带GPU设备的实例，则将使用`nvidia-docker`。
- en: 'Once the training script has started, SageMaker does the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本启动后，SageMaker会执行以下操作：
- en: Captures `stdout`/`stderr` and sends it to CloudWatch logs.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获`stdout`/`stderr`并将其发送到CloudWatch日志。
- en: Runs a regex pattern match for metrics and sends the metric values to CloudWatch.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行正则表达式模式匹配以获取度量指标并将度量值发送到CloudWatch。
- en: Monitors for a `SIGTERM` signal from the SageMaker API (for example, if the
    user decides to stop the training job earlier).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从SageMaker API监听`SIGTERM`信号（例如，如果用户决定提前停止训练作业）。
- en: Monitors if an early stopping condition occurs and issues `SIGTERM` when this
    happens.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控是否发生早停止条件并在此情况下发出`SIGTERM`。
- en: Monitors for the exit code of the training script. In the case of a non-zero
    exit code, SageMaker will mark the training job as “failed.”
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控训练脚本的退出代码。在非零退出代码的情况下，SageMaker将标记训练作业为“失败”。
- en: Step 6 – SageMaker persists the training artifacts
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步 - SageMaker将持久化训练产物
- en: 'Regardless of whether the training job fails or succeeds, SageMaker stores
    artifacts in the following locations:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 无论训练作业成功还是失败，SageMaker都会将产物存储在以下位置：
- en: The `/opt/ml/output` directory, which can be used to persist any training artifacts
    after the job is completed.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/opt/ml/output`目录，可以用来在作业完成后持久化任何训练产物。'
- en: The `/opt/ml/model` directory, the content of which will be compressed into
    `.tar` format and stored in the SageMaker model registry.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/opt/ml/model`目录，其内容将被压缩成`.tar`格式并存储在SageMaker模型注册表中。'
- en: Once you have your first model trained to solve a particular business problem,
    the next step is to use your model (in ML parlance, **perform inference**). In
    the next few sections, we will learn what capabilities SageMaker provides to run
    ML inference workloads for various use cases.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的第一个模型训练完毕以解决特定的业务问题，下一步就是使用您的模型（在ML术语中，**进行推断**）。在接下来的几节中，我们将了解SageMaker为运行各种用例的ML推断工作负载提供了哪些能力。
- en: Using SageMaker’s managed hosting stack
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker的托管主机堆栈
- en: 'Amazon SageMaker supports several types of managed hosting infrastructure:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker支持多种类型的托管主机基础设施：
- en: A persistent synchronous HTTPS endpoint for real-time inference
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个持久的同步HTTPS端点，用于实时推断
- en: An asynchronous endpoint for near-real-time inference
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于准实时推断的异步端点
- en: A transient Batch Transform job that performs inference across the entire dataset
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个瞬时的批量转换作业，用于整个数据集的推断
- en: In the next section, we will discuss use cases regarding when to use what type
    of hosting infrastructure, and we’ll review real-time inference endpoints in detail.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论在何时使用哪种类型的托管基础设施的用例，并将详细回顾实时推理端点。
- en: Real-time inference endpoints
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时推理端点。
- en: 'Real-time endpoints are built for use cases where you need to get inference
    results as soon as possible. SageMaker’s real-time endpoint is an HTTPS endpoint:
    model inputs are provided by the client via a POST request payload, and inference
    results are returned in the response body. The communication is synchronous.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 实时端点专为需要尽快获得推理结果的用例设计。SageMaker 的实时端点是一个 HTTPS 端点：模型输入由客户端通过 POST 请求负载提供，推理结果在响应体中返回。通信是同步的。
- en: 'There are many scenarios when real-time endpoints are applicable, such as the
    following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多场景适合使用实时端点，例如：
- en: To provide movie recommendations when the user opens a streaming application,
    based on the user’s watch history, individual ratings, and what’s trending now
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户的观看历史、个人评分和当前流行趋势，在用户打开流媒体应用程序时提供电影推荐。
- en: To detect objects in a real-time video stream
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测实时视频流中的物体。
- en: To generate a suggested next word as the user inputs text
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在用户输入文本时生成建议的下一个单词。
- en: 'SageMaker real-time endpoints provide customers with a range of capabilities
    to design and manage their inference workloads:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 实时端点为客户提供一系列功能，用于设计和管理其推理工作负载：
- en: Create a fully managed compute infrastructure with horizontal scaling (meaning
    that a single endpoint can use multiple compute instances to serve high traffic
    load without performance degradation)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个完全托管的计算基础设施，并实现横向扩展（意味着单个端点可以使用多个计算实例来应对高流量负载，而不会导致性能下降）。
- en: There is a wide spectrum of EC2 compute instance types to choose from based
    on model requirements including AWS’ custom chip Inferentia and SageMaker Elastic
    Inference
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多种 EC2 计算实例类型可供选择，具体取决于模型的需求，包括 AWS 的自定义芯片 Inferentia 和 SageMaker 弹性推理。
- en: Pre-built inference containers for popular DL frameworks
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对流行的深度学习框架提供预构建的推理容器。
- en: Multi-model and multi-container endpoints
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模型和多容器端点。
- en: Model production variants for A/B testing
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于 A/B 测试的模型生产变体。
- en: Multi-model inference pipelines
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模型推理管道。
- en: Model monitoring for performance, accuracy, and bias
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能、准确性和偏差的模型监控。
- en: SageMaker Neo and SageMaker Edge Manager to optimize and manage inference at
    edge devices
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Neo 和 SageMaker Edge Manager 在边缘设备上优化和管理推理。
- en: 'Since this is a managed capability, Amazon SageMaker is responsible for the
    following aspects of managing users’ real-time endpoints:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个托管能力，Amazon SageMaker 负责管理用户实时端点的以下方面：
- en: Provisioning and scaling the underlying compute infrastructure based on customer-defined
    scaling policies
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据客户定义的扩展策略提供和扩展基础计算基础设施。
- en: Traffic shaping between model versions and containers in cases where multiple
    models are deployed on a single SageMaker endpoint.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个 SageMaker 端点上部署多个模型时，模型版本和容器之间的流量塑形。
- en: Streaming logs and metrics at the level of the compute instance and model.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算实例和模型级别流式传输日志和度量指标。
- en: Creating and using your SageMaker endpoint
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和使用您的 SageMaker 端点。
- en: 'Let’s walk through the process of configuring, provisioning, and using your
    first SageMaker real-time endpoint. This will help to build your understanding
    of its internal workings and the available configuration options. The following
    diagram provides a visual guide:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起走过配置、提供和使用第一个 SageMaker 实时端点的过程。这将帮助您理解其内部工作原理和可用的配置选项。以下图表提供了一个视觉指南：
- en: '![Figure 1.3 – SageMaker inference endpoint deployment and usage ](img/B17519_01_003.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – SageMaker 推理端点的部署与使用](img/B17519_01_003.jpg)'
- en: Figure 1.3 – SageMaker inference endpoint deployment and usage
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – SageMaker 推理端点的部署与使用。
- en: Step 1 – initiating endpoint creation
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步 – 启动端点创建。
- en: 'There are several ways to initiate SageMaker endpoint creation: the SageMaker
    Python SDK, the boto3 SDK, the AWS CLI, or via a CloudFormation template. As part
    of the request, there are several parameters that you need to provide, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以启动 SageMaker 端点创建：SageMaker Python SDK、boto3 SDK、AWS CLI 或通过 CloudFormation
    模板。在请求过程中，您需要提供以下几个参数：
- en: '**Model definition** in SageMaker Model Registry, which will be used at inference
    time. The model definition includes references to serialized model artifacts in
    S3 and a reference to the inference container (or several containers in the case
    of a multi-container endpoint) in Amazon ECR.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Model Registry中的**模型定义**，将在推理时使用。模型定义包括对S3中序列化模型工件的引用，以及对Amazon ECR中推理容器（或多容器端点的情况下的多个容器）的引用。
- en: '**Endpoint configuration**, which defines the number and type of compute instances,
    and (optional) a combination of several models (in the case of a multi-model endpoint)
    or several model production variants (in the case of A/B testing).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端点配置**，定义计算实例的数量和类型，以及（可选）多个模型的组合（在多模型端点的情况下）或多个模型生产变体的组合（在A/B测试的情况下）。'
- en: Step 2 – configuring the SageMaker endpoint for image classification
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步 – 配置SageMaker端点用于图像分类
- en: 'The following Python code sample shows how to create and deploy an endpoint
    using the previously trained image classification model:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码示例展示了如何使用先前训练的图像分类模型创建和部署端点：
- en: 'Begin with your initial imports, IAM role, and SageMaker session instantiation:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始导入、IAM角色和SageMaker会话实例化开始：
- en: '[PRE10]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Retrieve the inference container URI for the image classification algorithm:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取图像分类算法的推理容器URI：
- en: '[PRE11]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define where the model artifacts (such as trained weights) are stored in S3:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义存储在S3中的模型工件（如训练权重）的位置：
- en: '[PRE12]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a SageMaker `Model` object that encapsulates the model configuration:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个SageMaker `Model`对象，封装模型配置：
- en: '[PRE13]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the endpoint configuration parameters:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义端点配置参数：
- en: '[PRE14]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `.predict()` method submits a request to SageMaker to create an endpoint
    with a specific model deployed:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.predict()`方法向SageMaker提交请求，创建部署了特定模型的端点：'
- en: '[PRE15]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With that done, SageMaker gets to work.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，SageMaker开始工作。
- en: Step 3 – SageMaker provisions the endpoint
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步 – SageMaker 配置端点
- en: 'Once your provision request is submitted, SageMaker performs the following
    actions:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 提交配置请求后，SageMaker执行以下操作：
- en: It will allocate several instances according to the endpoint configuration
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据端点配置分配多个实例
- en: It will deploy an inference container
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将部署推理容器
- en: It will download the model artifacts
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将下载模型工件
- en: It takes several minutes to complete endpoint provisioning from start to finish.
    The provisioning time depends on the number of parameters, such as instance type,
    size of the inference container, and the size of the model artifacts that need
    to be uploaded to the inference instance(s).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 从开始到完成端点配置需要几分钟时间。配置时间取决于多个参数，如实例类型、推理容器大小以及需要上传到推理实例的模型工件大小。
- en: Please note that SageMaker doesn’t expose inference instances directly. Instead,
    it uses a fronting load balancer, which then distributes the traffic between provisioned
    instances. As SageMaker is a managed service, you will never interact with inference
    instances directly, only via the SageMaker API.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SageMaker不直接暴露推理实例。相反，它使用前端负载均衡器，然后在配置的实例之间分发流量。作为托管服务，您永远不会直接与推理实例交互，只能通过SageMaker
    API进行操作。
- en: Step 4 – SageMaker starts the model server
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四步 – SageMaker 启动模型服务器
- en: 'Once the endpoint has been fully provisioned, SageMaker starts the inference
    container by running the following command, which executes the `ENTRYPOINT` command
    in the container:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦端点完全配置完成，SageMaker通过运行以下命令启动推理容器，执行容器中的`ENTRYPOINT`命令：
- en: '[PRE16]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This script does the following:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本执行以下操作：
- en: Starts the model server, which exposes the HTTP endpoint
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动模型服务器，暴露HTTP端点
- en: Makes the model server load the model artifacts in memory
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使模型服务器将模型工件加载到内存中
- en: At inference time, it makes the model server execute the inference script, which
    defines how to preprocess data
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理时，使模型服务器执行推理脚本，定义如何预处理数据
- en: In the case of SageMaker-managed Docker images, the model server and startup
    logic are already implemented by AWS. If you choose to BYO serving container,
    then this needs to be implemented separately.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker托管的Docker镜像情况下，模型服务器和启动逻辑已由AWS实现。如果选择自己带服务的容器（BYO serving container），则需要单独实现。
- en: SageMaker captures the `stdout`/`stderr` streams and automatically streams them
    to CloudWatch logs. It also streams instances metrics such as the number of invocations
    total and per instance, invocation errors, and latency measures.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 捕获 `stdout`/`stderr` 流并自动将其流式传输到 CloudWatch 日志。它还会流式传输实例指标，如总调用次数和每个实例的调用次数、调用错误和延迟指标。
- en: Step 5 – the SageMaker endpoint serves traffic
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 5 – SageMaker 端点处理流量
- en: Once the model server is up and running, end users can send a `POST` request
    to the SageMaker endpoint. The endpoint authorizes the request based on the authorization
    headers (these headers are automatically generated when using the SageMaker Python
    SDK or the AWS CLI based on the IAM profile). If authorization is successful,
    then the payload is sent to the inference instance.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型服务器启动并运行，最终用户可以向 SageMaker 端点发送 `POST` 请求。该端点会根据授权头进行请求授权（当使用 SageMaker
    Python SDK 或 AWS CLI 时，这些头部会根据 IAM 配置文件自动生成）。如果授权成功，则将负载发送到推理实例。
- en: The running model server handles the request by executing the inference script
    and returns the response payload, which is then delivered to the end user.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 运行中的模型服务器通过执行推理脚本来处理请求，并返回响应负载，然后将其传递给最终用户。
- en: Step 6 – SageMaker scales the inference endpoint in or out
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 6 – SageMaker 扩展推理端点的规模
- en: You may choose to define an auto-scaling policy to scale endpoint instances
    in and out. In this case, SageMaker will add or remove compute nodes behind the
    endpoint to match demand more efficiently. Please note that SageMaker only supports
    horizontal scaling, such as adding or removing compute nodes, and not changing
    instance type.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择定义自动扩展策略，以便在端点实例之间进行扩展或缩减。在这种情况下，SageMaker 会根据需求更高效地添加或移除端点后面的计算节点。请注意，SageMaker
    仅支持水平扩展，例如添加或移除计算节点，而不支持更改实例类型。
- en: 'SageMaker supports several types of scaling events:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 支持多种类型的扩展事件：
- en: Manual, where the user updates the endpoint configuration via an API call
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动方式，用户通过 API 调用更新端点配置
- en: A target tracking policy, where SageMaker scales in or out based on the value
    of the user-defined metric (for example, the number of invocations or resource
    utilization)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种目标追踪策略，SageMaker 根据用户定义的指标的值（例如调用次数或资源利用率）来扩展或缩减
- en: A step scaling policy, which provides the user with more granular control over
    how to adjust the number of instances based on how much threshold is breached
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种步骤扩展策略，它提供更细粒度的控制，允许用户根据阈值的突破程度调整实例的数量
- en: A scheduled scaling policy, which allows you to scale the SageMaker endpoint
    based on a particular schedule (for example, scale in during the weekend, where
    there’s low traffic, and scale out during the weekday, where there’s peak traffic)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种计划扩展策略，允许您根据特定的计划扩展 SageMaker 端点（例如，在流量较低的周末进行缩减，在流量较高的工作日进行扩展）
- en: Advanced model deployment patterns
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级模型部署模式
- en: We just reviewed the anatomy of a simple, single-model real-time endpoint. However,
    in many real-life scenarios where there are tens or hundreds of models that need
    to be available at any given point in time, this approach will lead to large numbers
    of underutilized or unevenly utilized inference nodes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚回顾了一个简单的单模型实时端点的结构。然而，在许多现实场景中，需要在任何给定时间点都能提供数十个或数百个模型，这种方法将导致大量的推理节点未被充分利用或不均匀使用。
- en: This is a generally undesirable situation as it will lead to high compute costs
    without any end user value. To address this problem, Amazon SageMaker provides
    a few advanced deployment options that allow you to combine several models within
    the same real-time endpoint and, hence, utilize resources more efficiently.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况通常是不可取的，因为它会导致高昂的计算成本而没有为最终用户带来任何价值。为了解决这个问题，Amazon SageMaker 提供了一些高级部署选项，允许您将多个模型合并到同一个实时端点中，从而更高效地利用资源。
- en: Multi-container endpoint
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多容器端点
- en: When deploying a multi-container endpoint, you may specify up to 15 different
    containers within the same endpoint. Each inference container has model artifacts
    and its own runtime environment. This allows you to deploy models built in various
    frameworks and runtime environments within a single SageMaker endpoint.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 部署多容器端点时，您可以在同一个端点中指定最多 15 个不同的容器。每个推理容器都有自己的模型工件和运行时环境。这使得您能够在单个 SageMaker
    端点内部署使用不同框架和运行时环境构建的模型。
- en: At creation time, you define a unique container hostname. Each container can
    then be invoked independently. During endpoint invocation, you are required to
    provide this container hostname as one of the request headers. SageMaker will
    automatically route the inference request to a correct container based on this
    header.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建时，你需要定义一个唯一的容器主机名。然后，每个容器可以独立调用。在终端调用期间，你需要将该容器主机名作为请求头之一提供。SageMaker 将根据该请求头自动将推理请求路由到正确的容器。
- en: This feature comes in handy when there are several models with relatively low
    traffic and different runtime environments (for example, Pytorch and TensorFlow).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当有多个模型且流量较低，且这些模型有不同的运行时环境（例如，Pytorch 和 TensorFlow）时，这个功能非常实用。
- en: Inference pipeline
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理管道
- en: 'Like a multi-container endpoint, the inference pipeline allows you to combine
    different models and container runtimes within a single SageMaker endpoint. However,
    the containers are called sequentially. This feature is geared toward scenarios
    where an inference request requires pre and/or post-processing with different
    runtime requirements; for example:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于多容器终端，推理管道允许你在单个 SageMaker 终端中组合不同的模型和容器运行时环境。然而，这些容器是按顺序调用的。该功能适用于需要使用不同运行时要求进行前处理和/或后处理的推理请求场景；例如：
- en: The pre-processing phase is done using the scikit-learn library
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前处理阶段使用 scikit-learn 库完成
- en: Inference is done using a DL framework
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理使用深度学习框架完成
- en: Post-processing is done using a custom runtime environment, such as Java or
    C++
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理使用自定义运行时环境完成，例如 Java 或 C++
- en: By encapsulating different phases of the inference pipeline in separate containers,
    changes in one container won’t impact adversely other containers, such as updating
    the dependency version. Since containers within inference pipelines are located
    on the same compute node, this guarantees low latency during request handoff between
    containers.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将推理管道的不同阶段封装在单独的容器中，一个容器的变化不会对其他容器产生不利影响，例如更新依赖版本。由于推理管道中的容器位于同一计算节点上，因此可以保证容器之间请求交接时的低延迟。
- en: Multi-model endpoint
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模型终端
- en: Multi-model endpoints allow you to deploy hundreds of models within a single
    endpoint. Unlike multi-container endpoints and inference pipelines, a multi-model
    endpoint has a single runtime environment. SageMaker automatically loads model
    artifacts into memory and handles inference requests. When a model is no longer
    needed, SageMaker unloads it from memory to free up resources. This leads to some
    additional latency when invoking the model for the first time after a while. Model
    artifacts are stored on Amazon S3 and loaded by SageMaker automatically.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型终端允许你在单个终端中部署数百个模型。与多容器终端和推理管道不同，多模型终端只有一个运行时环境。SageMaker 会自动将模型工件加载到内存中，并处理推理请求。当模型不再需要时，SageMaker
    会将其从内存中卸载，以释放资源。这会导致在一段时间后首次调用模型时出现一些额外的延迟。模型工件存储在 Amazon S3 中，并由 SageMaker 自动加载。
- en: At the core of the multi-model endpoint is the AWS-developed open source Multi-Model
    Server, which provides model management capabilities (loading, unloading, and
    resource allocation) and an HTTP frontend to receive inference requests, execute
    inference code for a given model, and return the resulting payload.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型终端的核心是 AWS 开发的开源 Multi-Model Server，它提供模型管理功能（加载、卸载和资源分配），并通过 HTTP 前端接收推理请求，执行给定模型的推理代码，并返回结果负载。
- en: Multi-model endpoints are optimal when there’s a large number of homogeneous
    models and end users can tolerate warmup latency.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型终端在有大量同质模型且最终用户可以容忍预热延迟时最为适用。
- en: SageMaker asynchronous endpoints
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 异步终端
- en: 'So far, we have discussed SageMaker real-time endpoints, which work synchronously:
    users invoke the endpoint by sending a POST request, wait for the endpoint to
    run inference code, and then return the inference results in the response payload.
    The inference code is expected to complete within 60 seconds; otherwise, the SageMaker
    endpoint will return a timeout response.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了 SageMaker 实时终端，它们是同步工作的：用户通过发送 POST 请求调用终端，等待终端运行推理代码，然后返回推理结果到响应负载中。推理代码预计会在
    60 秒内完成；否则，SageMaker 终端将返回超时响应。
- en: 'In certain scenarios, however, this synchronous communication pattern can be
    problematic:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些场景中，这种同步通信模式可能会带来问题：
- en: Large models can take a considerable time to perform inference
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型模型可能需要相当长的时间来进行推理
- en: Large payload sizes (for instance, high-resolution imagery)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型负载数据（例如，高分辨率图像）
- en: For such scenarios, SageMaker provides Asynchronous Endpoints, which allow you
    to queue inference requests and process them asynchronously, avoiding potential
    timeouts. Asynchronous endpoints also allow for a considerably larger payload
    size of up to 1 GB, whereas SageMaker real-time endpoints have a limit of 5 MB.
    Asynchronous endpoints can be scaled to 0 instances when the inference queue is
    empty to provide additional cost savings. This is specifically useful for scenarios
    with sporadic inference traffic patterns.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此类场景，SageMaker 提供了异步端点，允许您排队推理请求并异步处理，避免了潜在的超时问题。异步端点还允许处理更大的负载，最大可达 1 GB，而
    SageMaker 实时端点的限制为 5 MB。异步端点可以在推理队列为空时缩减到 0 个实例，以提供额外的成本节省。这对于具有零星推理流量模式的场景特别有用。
- en: 'The main tradeoff of asynchronous endpoints is that the inference results are
    delivered in *near* real-time and may not be suited for scenarios where consistent
    latency is expected:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 异步端点的主要权衡是推理结果在*接近*实时的情况下交付，可能不适合那些对一致延迟有要求的场景：
- en: '![Figure 1.4 – SageMaker asynchronous endpoint ](img/B17519_01_004.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – SageMaker 异步端点 ](img/B17519_01_004.jpg)'
- en: Figure 1.4 – SageMaker asynchronous endpoint
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – SageMaker 异步端点
- en: SageMaker Batch Transform
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 批量转换
- en: SageMaker Batch Transform allows you to get predictions for a batch of inference
    inputs. This can be useful for scenarios where there is a recurrent business process
    and there are no strict latency requirements. An example is a nightly job that
    calculates risks for load applications.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 批量转换允许您对一批推理输入进行预测。这对于那些有重复业务流程且对延迟没有严格要求的场景非常有用。例如，每晚执行的任务用于计算负载应用程序的风险。
- en: 'SageMaker Batch Transform is beneficial for the following use cases:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 批量转换对以下用例非常有利：
- en: Customers only pay for resources that are consumed during job execution
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户仅为任务执行期间消耗的资源付费
- en: Batch Transform jobs can scale to GBs and tens of compute nodes
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量转换任务可以扩展到 GB 级和数十个计算节点
- en: 'When scheduling a Batch Transform job, you define the cluster configuration
    (type and number of compute nodes), model artifacts, inference container, the
    input S3 location for the inference dataset, and the output S3 location for the
    generated predictions. Please note that customers can use the same container for
    SageMaker real-time endpoints and the Batch Transform job. This allows developers
    to use the same models/containers for online predictions (as real-time endpoints)
    and offline (as Batch Transform jobs):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度批量转换任务时，您需要定义集群配置（计算节点的类型和数量）、模型工件、推理容器、推理数据集的输入 S3 位置以及生成的预测结果的输出 S3 位置。请注意，客户可以使用相同的容器用于
    SageMaker 实时端点和批量转换任务。这使得开发人员可以使用相同的模型/容器进行在线预测（作为实时端点）和离线预测（作为批量转换任务）：
- en: '![Figure 1.5 – SageMaker Batch Transform job ](img/B17519_01_005.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – SageMaker 批量转换任务 ](img/B17519_01_005.jpg)'
- en: Figure 1.5 – SageMaker Batch Transform job
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – SageMaker 批量转换任务
- en: With that, you understand how to train a simple DL model using a SageMaker training
    job and then create a real-time endpoint to perform inference. Before we proceed
    further, we need to learn about several foundational AWS services that are used
    by Amazon SageMaker that you will see throughout this book.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此方法，您可以了解如何使用 SageMaker 训练任务训练一个简单的深度学习模型，并创建一个实时端点进行推理。在继续之前，我们需要了解一些亚马逊
    SageMaker 使用的基础 AWS 服务，这些服务将贯穿本书。
- en: Integration with AWS services
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 AWS 服务的集成
- en: Amazon SageMaker relies on several AWS services, such as storage and key management.
    In this section, we will review some key integrations with other AWS services
    and use cases when they can be useful.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 依赖于多个 AWS 服务，如存储和密钥管理。在本节中，我们将回顾与其他 AWS 服务的关键集成，以及它们在何种场景下可能有用。
- en: Data storage services
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储服务
- en: Data storage services are key to building any ML workload. AWS provides several
    storage solutions to address a wide range of real-life use cases.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储服务是构建任何机器学习工作负载的关键。AWS 提供了多种存储解决方案，以应对各种实际应用场景。
- en: '**Amazon S3** is a form of serverless object storage and one of AWS’s foundational
    services. SageMaker utilizes S3 for a wide range of use cases, such as the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon S3** 是一种无服务器对象存储服务，是 AWS 的基础服务之一。SageMaker 利用 S3 处理各种用例，例如：'
- en: To store training datasets
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储训练数据集
- en: To store model artifacts and training output
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储模型工件和训练输出
- en: To store inference inputs and outputs for Asynchronous Endpoints and Batch Transform
    jobs
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '存储异步端点和批量转换作业的推理输入和输出  '
- en: Amazon S3 is a highly durable, scalable, and cost-efficient storage solution.
    When accessing data stored on S3, developers may choose to either download the
    full dataset from the S3 location to SageMaker compute nodes or stream the data.
    Downloading a large dataset from S3 to SageMaker compute nodes to add to the training
    job’s startup time.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 'Amazon S3 是一个高耐久性、可扩展且成本高效的存储解决方案。在访问存储在 S3 上的数据时，开发者可以选择将完整数据集从 S3 位置下载到 SageMaker
    计算节点，或者通过流式传输数据。将大型数据集从 S3 下载到 SageMaker 计算节点会增加训练作业的启动时间。  '
- en: Amazon EFS
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EFS
- en: '**Amazon Elastic File System** (**EFS**) is an elastic filesystem service.
    Amazon SageMaker supports storing training datasets in EFS locations. At training
    time, SageMaker nodes mount to the EFS location and directly access the training
    datasets. In this case, no data movement is required for nodes to access data,
    which typically results in reduced startup times for training jobs. EFS also allows
    multiple nodes to persist and seamlessly share data (since this is a shared system).
    This can be beneficial when the cache or system state needs to be shared across
    training nodes.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Elastic File System**（**EFS**）是一个弹性文件系统服务。Amazon SageMaker 支持将训练数据集存储在
    EFS 位置。在训练时，SageMaker 节点会挂载到 EFS 位置并直接访问训练数据集。在这种情况下，节点访问数据时不需要数据迁移，通常可以减少训练作业的启动时间。EFS
    还允许多个节点持久化并无缝共享数据（因为这是一个共享系统）。当缓存或系统状态需要在训练节点之间共享时，这非常有用。  '
- en: Amazon FSx for Lustre
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Amazon FSx for Lustre  '
- en: '**Amazon FSx for Lustre** is a shared filesystem service designed specifically
    for low-latency, high-performance scenarios. Amazon FSx automatically copies data
    from the S3 source and makes it available for SageMaker compute nodes. Amazon
    FSx has similar benefits to EFS – that is, a reduced startup time for training
    jobs and a shared filesystem.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon FSx for Lustre** 是专门为低延迟、高性能场景设计的共享文件系统服务。Amazon FSx 会自动从 S3 源复制数据，并使其在
    SageMaker 计算节点上可用。Amazon FSx 具有与 EFS 相似的优势——即减少训练作业的启动时间，并提供共享文件系统。  '
- en: Orchestration services
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '编排服务  '
- en: Orchestration services allow you to integrate SageMaker-based workloads with
    the rest of the IT ecosystem.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '编排服务允许你将基于 SageMaker 的工作负载与其他 IT 生态系统进行集成。  '
- en: AWS Step Functions
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'AWS Step Functions  '
- en: AWS Step Functions is a serverless workflow service that allows you to orchestrate
    business processes and interactions with other AWS services. With Step Functions,
    it’s easy to combine individual steps into reusable and deployable workflows.
    It supports visual design and branching and conditional logic.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 'AWS Step Functions 是一个无服务器工作流服务，允许你编排业务流程和与其他 AWS 服务的交互。通过 Step Functions，你可以轻松地将单独的步骤组合成可重用和可部署的工作流。它支持可视化设计、分支和条件逻辑。  '
- en: Step Functions provides native integration with SageMaker resources. Step Functions
    can be useful in scenarios where you need to orchestrate complex ML pipelines
    using multiple services in addition to SageMaker. AWS provides developers with
    the AWS Step Functions Data Science Python SDK to develop, test, and execute such
    pipelines.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 'Step Functions 提供与 SageMaker 资源的原生集成。在需要通过多个服务协调复杂的机器学习流程时，Step Functions 非常有用。AWS
    为开发者提供了 AWS Step Functions Data Science Python SDK 来开发、测试和执行此类流程。  '
- en: Amazon API Gateway
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Amazon API Gateway  '
- en: 'Amazon API Gateway is a fully managed API management service that’s used to
    develop, monitor, and manage APIs. API Gateway supports several features that
    can be helpful when developing highly-scalable and secure ML inference APIs:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 'Amazon API Gateway 是一个完全托管的 API 管理服务，用于开发、监控和管理 API。API Gateway 支持多个功能，在开发高度可扩展和安全的机器学习推理
    API 时非常有用：  '
- en: Authentication and authorization mechanisms
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '认证和授权机制  '
- en: Request caching, rate limiting, and throttling
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '请求缓存、速率限制和流量控制  '
- en: Firewall features
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '防火墙功能  '
- en: Request headers and payload transformation
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '请求头和负载转换  '
- en: API Gateway allows you to insulate SageMaker real-time endpoints from external
    traffic and provide an additional layer of security. It also allows you to provide
    end users with a unified API without exposing the specifics of the SageMaker runtime
    API.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 'API Gateway 允许你将 SageMaker 实时端点与外部流量隔离，并提供额外的安全层。它还允许你为终端用户提供统一的 API，而无需暴露
    SageMaker 运行时 API 的具体细节。  '
- en: Security services
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '安全服务  '
- en: Robust security controls are a must-have for any ML workload, especially when
    dealing with private and sensitive data. While this book does not focus on security,
    it’s important to understand the basic aspects of permissions and data encryption
    on AWS.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的安全控制对于任何机器学习工作负载都是必不可少的，特别是在处理私密和敏感数据时。虽然本书并不专注于安全性，但了解 AWS 上权限和数据加密的基本概念非常重要。
- en: AWS IAM
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS IAM
- en: '**AWS Identity and Access Management** (**IAM**) allows customers to manage
    access to AWS services and resources. In the case of SageMaker, IAM has a twofold
    function:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS 身份与访问管理** (**IAM**) 允许客户管理对 AWS 服务和资源的访问。在 SageMaker 的情况下，IAM 具有双重功能：'
- en: IAM roles and policies define which AWS resources SageMaker jobs can access
    and manage – for example, whether the training job with the assumed IAM role can
    access the given dataset on S3 or not.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAM 角色和策略定义了 SageMaker 任务可以访问和管理的 AWS 资源——例如，使用假定的 IAM 角色的训练任务是否可以访问 S3 上给定的数据集。
- en: IAM roles and policies define which principal (user or service) can access and
    manage SageMaker resources. For instance, it defines whether the given user can
    schedule a SageMaker training job with a specific cluster configuration.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAM 角色和策略定义了哪些主体（用户或服务）可以访问和管理 SageMaker 资源。例如，它定义了给定用户是否可以使用特定集群配置安排 SageMaker
    训练任务。
- en: Reviewing IAM is outside the scope of this book, but you need to be aware of
    it. Setting up IAM permissions and roles are prerequisites when working with SageMaker.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 审查 IAM 超出了本书的范围，但您需要了解它。在使用 SageMaker 时，设置 IAM 权限和角色是必要的前提。
- en: Amazon VPC
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon VPC
- en: '**Amazon Virtual Private Cloud** (**VPC**) is a service that allows you to
    run your cloud workloads in logically isolated private networks. This network-level
    isolation provides an additional level of security and control over who can access
    your workload. SageMaker allows you to run training and inference workloads inside
    a dedicated VPC so that you can control egress and ingress traffic to and from
    your SageMaker resources.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon 虚拟私有云** (**VPC**) 是一项服务，允许您在逻辑上隔离的私有网络中运行云工作负载。这种网络级别的隔离为谁可以访问您的工作负载提供了额外的安全性和控制。SageMaker
    允许您在专用的 VPC 内运行训练和推理工作负载，这样您就可以控制从 SageMaker 资源进出的流量。'
- en: AWS KMS
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS KMS
- en: '**AWS Key Management Service** (**KMS**) is used to encrypt underlying data.
    It also manages access to cryptographic keys when encrypting and decrypting data.
    In the context of SageMaker, KMS is primarily used to encrypt training data, model
    artifacts, and underlying disks in SageMaker clusters. KMS is integrated with
    all available storage solutions, such as S3, EFS, and EBS (underlying disk volumes).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS 密钥管理服务** (**KMS**) 用于加密底层数据。它还管理在加密和解密数据时对加密密钥的访问。在 SageMaker 中，KMS 主要用于加密训练数据、模型工件以及
    SageMaker 集群中的底层磁盘。KMS 与所有可用的存储解决方案（如 S3、EFS 和 EBS（底层磁盘卷））集成。'
- en: Monitoring services
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控服务
- en: AWS has dedicated services to monitor the management, auditing, and execution
    of other AWS resources.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了专门的服务，用于监控其他 AWS 资源的管理、审计和执行。
- en: Amazon CloudWatch
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon CloudWatch
- en: '**CloudWatch** provides monitoring and observability capabilities. In the context
    of SageMaker, it’s used primarily for two purposes:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**CloudWatch** 提供监控和可观察性功能。在 SageMaker 的上下文中，它主要用于两个目的：'
- en: To store and manage logs coming from SageMaker resources such as endpoints or
    training jobs. By default, SageMaker ships `stdout`/`stderr` logs to CloudWatch.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储和管理来自 SageMaker 资源（如端点或训练任务）的日志。默认情况下，SageMaker 会将 `stdout`/`stderr` 日志发送到
    CloudWatch。
- en: To store time series metrics. SageMaker provides several metrics out of the
    box (for example, for real-time endpoints, it streams latency and invocation metrics).
    However, developers can implement custom metrics.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储时间序列指标。SageMaker 默认提供了若干指标（例如，对于实时端点，它会流式传输延迟和调用指标）。不过，开发人员也可以实现自定义指标。
- en: Amazon CloudTrail
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon CloudTrail
- en: '**CloudTrail** captures all activities (such as API calls) related to managing
    any AWS resources, including SageMaker resources. Typically, CloudTrail is used
    for governance and auditing purposes, but it can be used to build event-driven
    workflows. For instance, developers can use it to monitor resource creation or
    update requests and programmatically react to specific events.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**CloudTrail** 捕获与管理任何 AWS 资源（包括 SageMaker 资源）相关的所有活动（如 API 调用）。通常，CloudTrail
    用于治理和审计目的，但它也可以用来构建事件驱动的工作流。例如，开发人员可以使用它来监控资源创建或更新请求，并通过编程方式响应特定事件。'
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We started this chapter by providing a general overview of the DL domain and
    its challenges, as well as the Amazon SageMaker service and its value proposition
    for DL workloads. Then, we reviewed the core SageMaker capabilities: managed training
    and managed hosting. We examined the life cycle of a SageMaker training job and
    real-time inference endpoint. Code snippets demonstrated how to configure and
    provision SageMaker resources programmatically using its Python SDK. We also looked
    at other relevant AWS services as we will be using them a lot in the rest of this
    book. This will help us as we now have a good grounding in their uses and capabilities.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们提供了深度学习领域及其挑战的概述，以及 Amazon SageMaker 服务及其对深度学习工作负载的价值主张。然后，我们回顾了 SageMaker
    的核心功能：托管训练和托管推理。我们研究了 SageMaker 训练作业和实时推理端点的生命周期。代码片段展示了如何使用 Python SDK 编程方式配置和提供
    SageMaker 资源。我们还介绍了其他相关的 AWS 服务，因为在本书的其余部分中，我们将频繁使用它们。这将帮助我们更好地理解它们的使用和功能。
- en: 'In the next chapter, we will dive deeper into the foundational building blocks
    of any SageMaker workload: runtime environments (specifically, supported DL frameworks)
    and containers. SageMaker provides several popular pre-configured runtime environments
    and containers, but it also allows you to fully customize them via its “BYO container”
    feature. We will learn when to choose one of these options and how to use them.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更深入地探讨任何 SageMaker 工作负载的基础构建块：运行时环境（特别是支持的深度学习框架）和容器。SageMaker 提供了多个流行的预配置运行时环境和容器，但它也允许通过其“BYO
    容器”功能来完全自定义这些环境。我们将学习何时选择这些选项之一以及如何使用它们。
