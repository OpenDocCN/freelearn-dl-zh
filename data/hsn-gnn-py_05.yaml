- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Including Node Features with Vanilla Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用香草神经网络在节点特征中包含节点特征
- en: 'So far, the only type of information we’ve considered is the graph topology.
    However, graph datasets tend to be richer than a mere set of connections: nodes
    and edges can also have features to represent scores, colors, words, and so on.
    Including this additional information in our input data is essential to produce
    the best embeddings possible. In fact, this is something natural in machine learning:
    node and edge features have the same structure as a tabular (non-graph) dataset.
    This means that traditional techniques can be applied to this data, such as neural
    networks.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑的唯一类型的信息是图拓扑结构。然而，图形数据集往往比单纯的连接更丰富：节点和边缘也可以具有特征，用于表示分数、颜色、单词等等。将这些附加信息包含在我们的输入数据中对于产生最佳嵌入至关重要。事实上，这在机器学习中是自然而然的：节点和边缘特征具有与表格（非图形）数据集相同的结构。这意味着可以将传统技术应用于这些数据，例如神经网络。
- en: 'In this chapter, we will introduce two new graph datasets: `Cora` and `Facebook
    Page-Page`. We will see how **Vanilla Neural Networks** perform on node features
    only by considering them as tabular datasets. We will then experiment to include
    topological information in our neural networks. This will give us our first GNN
    architecture: a simple model that considers both node features and edges. Finally,
    we’ll compare the performance of the two architectures and obtain one of the most
    important results of this book.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍两个新的图形数据集：`Cora` 和 `Facebook Page-Page`。我们将看到**香草神经网络**如何仅将其视为表格数据集来处理节点特征。然后，我们将尝试在神经网络中包含拓扑信息。这将给我们带来我们第一个GNN架构：一个简单的模型，同时考虑节点特征和边缘。最后，我们将比较这两种架构的性能，并得到本书中最重要的结果之一。
- en: By the end of this chapter, you will master the implementation of vanilla neural
    networks and vanilla GNNs in PyTorch. You will be able to embed topological features
    into the node representations, which is the basis of every GNN architecture. This
    will allow you to greatly improve the performance of your models by transforming
    tabular datasets into graph problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将掌握在PyTorch中实现香草神经网络和香草GNN的方法。您将能够将拓扑特征嵌入节点表示中，这是每个GNN架构的基础。这将允许您通过将表格数据集转换为图问题来大大提高模型的性能。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing graph datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍图形数据集
- en: Classifying nodes with vanilla neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用香草神经网络对节点进行分类
- en: Classifying nodes with vanilla graph neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用香草图神经网络对节点进行分类
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到：[https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05)。
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* of this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*前言*中可以找到运行代码所需的安装步骤。
- en: Introducing graph datasets
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍图形数据集
- en: 'The graph datasets we’re going to use in this chapter are richer than Zachary’s
    Karate Club: they have more nodes, more edges, and include node features. In this
    section, we will introduce them to give us a good understanding of these graphs
    and how to process them with PyTorch Geometric. Here are the two datasets we will
    use:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将使用的图形数据集比Zachary's Karate Club更丰富：它们具有更多的节点、更多的边缘，并包含节点特征。在本节中，我们将介绍它们，以便更好地理解这些图形及如何使用PyTorch
    Geometric处理它们。以下是我们将使用的两个数据集：
- en: The `Cora` dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cora` 数据集'
- en: The `Facebook` `Page-Page` dataset
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Facebook` `Page-Page` 数据集'
- en: 'Let’s start with the smaller one: the popular `Cora` dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从较小的一个开始：流行的`Cora`数据集。
- en: The Cora dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cora 数据集
- en: Introduced by Sen et al. in 2008 [1], `Cora` (no license) is the most popular
    dataset for node classification in the scientific literature. It represents a
    network of 2,708 publications, where each connection is a reference. Each publication
    is described as a binary vector of 1,433 unique words, where `0` and `1` indicate
    the absence or presence of the corresponding word, respectively. This representation
    is also called a binary **bag of words** in natural language processing. Our goal
    is to classify each node into one of seven categories.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Sen 等人于 2008 年提出 [1]，`Cora`（无许可）是科学文献中最流行的节点分类数据集。它代表了一个包含 2,708 篇出版物的网络，其中每个连接代表一个参考文献。每篇出版物用一个包含
    1,433 个独特单词的二进制向量表示，其中 `0` 和 `1` 分别表示对应单词的缺失或存在。这种表示方法也被称为自然语言处理中的二进制 **词袋**。我们的目标是将每个节点分类到七个类别中的一个。
- en: 'Regardless of the type of data, visualization is always an important step to
    getting a good grasp of the problem we face. However, graphs can quickly become
    too big to visualize using Python libraries such as `networkx`. This is why dedicated
    tools have been developed specifically for graph data visualization. In this book,
    we utilize two of the most popular ones: **yEd Live** ([https://www.yworks.com/yed-live/](https://www.yworks.com/yed-live/))
    and **Gephi** ([https://gephi.org/](https://gephi.org/)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据类型如何，可视化始终是理解我们面临问题的一个重要步骤。然而，图形很快就会变得太大，无法使用像 `networkx` 这样的 Python 库进行可视化。这就是为什么专门的工具被开发出来，专门用于图数据的可视化。在本书中，我们使用了两款最流行的工具：**yEd
    Live** ([https://www.yworks.com/yed-live/](https://www.yworks.com/yed-live/))
    和 **Gephi** ([https://gephi.org/](https://gephi.org/))。
- en: The following figure is a plot of the `Cora` dataset made with yEd Live. You
    can see nodes corresponding to papers in orange and connections between them in
    green. Some papers are so interconnected that they form clusters. These clusters
    should be easier to classify than poorly connected nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是使用 yEd Live 绘制的 `Cora` 数据集。你可以看到，节点以橙色表示对应的论文，绿色表示它们之间的连接。一些论文的互联性如此强，以至于它们形成了聚类。这些聚类应该比连接较差的节点更容易分类。
- en: '![Figure 5.1 – The Cora dataset visualized with yEd Live](img/B19153_05_001.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 使用 yEd Live 可视化的 Cora 数据集](img/B19153_05_001.jpg)'
- en: Figure 5.1 – The Cora dataset visualized with yEd Live
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 使用 yEd Live 可视化的 Cora 数据集
- en: 'Let’s import it and analyze its main characteristics with PyTorch Geometric.
    This library has a dedicated class to download the dataset and return a relevant
    data structure. We assume here that PyTorch Geometric has already been installed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入它并使用 PyTorch Geometric 分析它的主要特征。这个库有一个专门的类来下载数据集并返回相关的数据结构。我们假设这里已经安装了 PyTorch
    Geometric：
- en: 'We import the `Planetoid` class from PyTorch Geometric:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 PyTorch Geometric 导入 `Planetoid` 类：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We download it using this class:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这个类下载它：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Cora` only has one graph we can store in a dedicated `data` variable:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Cora` 只有一个图，我们可以将其存储在一个专门的 `data` 变量中：'
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s print information about the dataset in general:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出有关数据集的一般信息：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following output:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: '[PRE4]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also get detailed information thanks to dedicated functions from PyTorch
    Geometric:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以通过 PyTorch Geometric 的专门函数获得详细信息：
- en: '[PRE5]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the result of the previous block:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是前一个代码块的结果：
- en: '[PRE6]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first output confirms the information about the number of nodes, features,
    and classes. The second one gives more insights into the graph itself: edges are
    undirected, every node has neighbors, and the graph doesn’t have any self-loop.
    We could test other properties using PyTorch Geometric’s utils functions, but
    we wouldn’t learn anything new in this example.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出确认了关于节点数量、特征和类别的信息。第二个输出则提供了更多关于图形本身的见解：边是无向的，每个节点都有邻居，而且图中没有自环。我们可以使用
    PyTorch Geometric 的 utils 函数测试其他属性，但在这个例子中不会学到任何新东西。
- en: 'Now that we know more about `Cora`, let’s see one that is more representative
    of the size of real-world social networks: the `Facebook` `Page-Page` dataset.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了更多关于 `Cora` 的信息，让我们来看一个更具代表性的，体现现实世界社交网络规模的数据集：`Facebook` `Page-Page`
    数据集。
- en: The Facebook Page-Page dataset
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Facebook Page-Page 数据集
- en: 'This dataset was introduced by Rozemberczki et al. in 2019 [2]. It was created
    using the Facebook Graph API in November 2017\. In this dataset, each of the 22,470
    nodes represents an official Facebook page. Pages are connected when there are
    mutual likes between them. Node features (128-dim vectors) are created from textual
    descriptions written by the owners of these pages. Our goal is to classify each
    node into one of four categories: politicians, companies, television shows, and
    governmental organizations.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由 Rozemberczki 等人在 2019 年提出[2]。它是通过 Facebook Graph API 于 2017 年 11 月创建的。在这个数据集中，22,470
    个节点中的每一个代表一个官方 Facebook 页面。当两个页面之间有互相点赞时，它们会被连接。节点特征（128 维向量）是通过这些页面的所有者编写的文本描述创建的。我们的目标是将每个节点分类到四个类别之一：政治人物、公司、电视节目和政府组织。
- en: 'The `Facebook Page-Page` dataset is similar to the previous one: it’s a social
    network with a node classification task. However, there are three major differences
    with `Cora`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Facebook 页面-页面`数据集与前一个数据集相似：它是一个具有节点分类任务的社交网络。然而，它与`Cora`有三大不同之处：'
- en: The number of nodes is much higher (2,708 versus 22,470)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的数量要高得多（2,708 与 22,470）
- en: The dimensionality of the node features decreased dramatically (from 1,433 to
    128)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点特征的维度大幅度降低（从 1,433 降到 128）
- en: The goal is to classify each node into four categories instead of seven (which
    is easier since there are fewer options)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是将每个节点分类为四个类别，而不是七个类别（这更容易，因为选项更少）。
- en: 'The following figure is a visualization of the dataset using Gephi. First,
    nodes with few connections have been filtered out to improve performance. The
    size of the remaining nodes depends on their number of connections, and their
    color indicates the category they belong to. Finally, two layouts have been applied:
    Fruchterman-Reingold and ForceAtlas2.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是使用 Gephi 可视化的数据集。首先，少量连接的节点已被过滤掉，以提高性能。剩余节点的大小取决于它们的连接数量，颜色表示它们所属的类别。最后，应用了两种布局：Fruchterman-Reingold
    和 ForceAtlas2。
- en: '![Figure 5.2 – The Facebook Page-Page dataset visualized with Gephi](img/B19153_05_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 使用 Gephi 可视化的 Facebook 页面-页面数据集](img/B19153_05_002.jpg)'
- en: Figure 5.2 – The Facebook Page-Page dataset visualized with Gephi
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 使用 Gephi 可视化的 Facebook 页面-页面数据集
- en: 'We can import the `Facebook Page-Page` dataset the same way we did it for `Cora`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以与`Cora`相同的方式导入`Facebook 页面-页面`数据集：
- en: 'We import the `FacebookPagePage` class from PyTorch Geometric:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 PyTorch Geometric 导入`FacebookPagePage`类：
- en: '[PRE7]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We download it using this class:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这个类下载它：
- en: '[PRE8]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We store the graph in a dedicated `data` variable:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图存储在一个专用的`data`变量中：
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s print information about the dataset in general:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印一下关于数据集的一般信息：
- en: '[PRE10]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the following output:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会给我们以下输出：
- en: '[PRE11]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The same dedicated functions can be applied here:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样的专用函数可以在这里应用：
- en: '[PRE12]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is the result of the previous block:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上一部分的结果：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Unlike `Cora`, `Facebook Page-Page` doesn’t have training, evaluation, and
    test masks by default. We can arbitrarily create masks with the `range()` function:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与`Cora`不同，`Facebook 页面-页面`数据集默认没有训练、评估和测试掩码。我们可以使用`range()`函数随意创建掩码：
- en: '[PRE14]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Alternatively, PyTorch Geometric offers a transform function to calculate random
    masks when the dataset is loaded:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，PyTorch Geometric 提供了一个转换函数，用来在加载数据集时计算随机掩码：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first output confirms the number of nodes and classes we saw in the description
    of the dataset. The second output tells us that this graph has `self` loops: some
    pages are connected to themselves. This is surprising but, in practice, it will
    not matter, as we’re going to see soon.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出确认了我们在数据集描述中看到的节点和类别的数量。第二个输出告诉我们，这个图包含了`self`环路：一些页面与自己连接。这虽然让人惊讶，但实际上它不会影响结果，因为我们很快就会看到。
- en: These are the two graph datasets we will use in the next section, to compare
    the performance of a Vanilla Neural Network to the performance of our first GNN.
    Let’s implement them step by step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将在下一部分中使用的两个图数据集，用来比较普通神经网络与我们第一个图神经网络（GNN）的性能。让我们一步步实现它们。
- en: Classifying nodes with vanilla neural networks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用普通神经网络进行节点分类
- en: 'Compared to Zachary’s Karate Club, these two datasets include a new type of
    information: node features. They provide additional information about the nodes
    in a graph, such as a user’s age, gender, or interests in a social network. In
    a vanilla neural network (also called **multilayer perceptron**), these embeddings
    are directly used in the model to perform downstream tasks such as node classification.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Zachary 的 Karate Club 数据集相比，这两个数据集包含了一种新的信息类型：节点特征。它们提供了关于图中节点的额外信息，例如社交网络中用户的年龄、性别或兴趣。在一个普通的神经网络（也称为**多层感知机**）中，这些嵌入将直接用于模型中执行下游任务，如节点分类。
- en: In this section, we will consider node features as a regular tabular dataset.
    We will train a simple neural network on this dataset to classify our nodes. Note
    that this architecture does not take into account the topology of the network.
    We will try to fix this issue in the next section and compare our results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把节点特征视为常规的表格数据集。我们将对该数据集训练一个简单的神经网络来对节点进行分类。请注意，这种架构并未考虑网络的拓扑结构。我们将在下一节中尝试解决这个问题并比较结果。
- en: 'The tabular dataset of node features can be easily accessed through the `data`
    object we created. First, I would like to convert this object into a regular pandas
    DataFrame by merging `data.x` (containing the node features) and `data.y` (containing
    the class label of each node among seven classes). In the following, we will use
    the `Cora` dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 节点特征的表格数据集可以通过我们创建的`data`对象轻松访问。首先，我想将这个对象转换为常规的 pandas DataFrame，通过合并`data.x`（包含节点特征）和`data.y`（包含每个节点的类标签，标签来自七个类别）。在接下来的内容中，我们将使用`Cora`数据集：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives us the following dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们带来以下数据集：
- en: '|  | **0** | **1** | **…** | **1432** | **label** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | **0** | **1** | **…** | **1432** | **标签** |'
- en: '| **0** | 0 | 0 | … | 0 | 3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 0 | 0 | … | 0 | 3 |'
- en: '| **1** | 0 | 0 | … | 0 | 4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 0 | … | 0 | 4 |'
- en: '| **…** | … | … | … | … | … |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **…** | … | … | … | … | … |'
- en: '| **2707** | 0 | 0 | … | 0 | 3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **2707** | 0 | 0 | … | 0 | 3 |'
- en: Figure 5.3 – Tabular representation of the Cora dataset (without topological
    information)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – Cora 数据集的表格表示（不包含拓扑信息）
- en: If you’re familiar with machine learning, you probably recognize a typical dataset
    with data and labels. We can develop a simple `data.x` with the labels provided
    by `data.y`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉机器学习，你可能会认识到一个典型的数据集，它包含数据和标签。我们可以开发一个简单的`data.x`，并用`data.y`提供的标签。
- en: 'Let’s create our own MLP class with four methods:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们自己的 MLP 类，并包含四个方法：
- en: '`__init__()` to initialize an instance'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`__init__()`来初始化一个实例
- en: '`forward()` to perform the forward pass'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`forward()`来执行前向传播
- en: '`fit()` to train the model'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`fit()`来训练模型
- en: '`test()` to evaluate it'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`test()`来评估它
- en: 'Before we can train our model, we must define the main metric. There are several
    metrics for multiclass classification problems: accuracy, F1 score, **Area Under
    the Receiver Operating Characteristic Curve** (**ROC AUC**) score, and so on.
    For this work, let’s implement a simple accuracy, which is defined as the fraction
    of correct predictions. It is not the best metric for multiclass classification,
    but it is simpler to understand. Feel free to replace it with your metric of choice:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，我们必须定义主要的评估指标。多分类问题有几种常见的评估指标：准确率、F1 分数、**接收者操作特征曲线下的面积**（**ROC AUC**）得分等。对于这项工作，我们实现一个简单的准确率，它定义为正确预测的比例。虽然这不是多分类问题中最好的评估指标，但它更容易理解。你可以自由地将其替换为你选择的指标：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can start the actual implementation. We don’t need PyTorch Geometric
    to implement the MLP in this section. Everything can be done in regular PyTorch
    with the following steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始实际的实现。我们在本节中不需要使用 PyTorch Geometric 来实现 MLP。所有操作都可以通过常规的 PyTorch 按照以下步骤完成：
- en: 'We import the required classes from PyTorch:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 PyTorch 导入所需的类：
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We create a new class called `MLP`, which will inherit all the methods and
    properties from `torch.nn.Module`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个新的类`MLP`，它将继承`torch.nn.Module`的所有方法和属性：
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `__init__()` method has three arguments (`dim_in`, `dim_h`, and `dim_out`)
    for the number of neurons in the input, hidden, and output layers, respectively.
    We also define two linear layers:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`__init__()`方法有三个参数（`dim_in`、`dim_h` 和 `dim_out`），分别表示输入层、隐藏层和输出层中的神经元数量。我们还定义了两个线性层：'
- en: '[PRE20]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `forward()` method performs the forward pass. The input is fed to the first
    linear layer with a **Rectified Linear Unit** (**ReLU**) activation function,
    and the result is passed to the second linear layer. We return the log softmax
    of this final result for classification:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forward()` 方法执行前向传递。输入传递给第一个线性层，并使用 **整流线性单元** (**ReLU**) 激活函数，然后结果传递给第二个线性层。我们返回这个最终结果的对数
    softmax，用于分类：'
- en: '[PRE21]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `fit()` method is in charge of the training loop. First, we initialize
    a loss function and an optimizer that will be used during the training process:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit()` 方法负责训练循环。首先，我们初始化一个损失函数和一个优化器，这些将在训练过程中使用：'
- en: '[PRE22]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A regular PyTorch training loop is then implemented. We use our `accuracy()`
    function on top of the loss function:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后实现一个常规的 PyTorch 训练循环。我们在损失函数的基础上使用 `accuracy()` 函数：
- en: '[PRE23]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the same loop, we plot the loss and accuracy for training and evaluation
    data every 20 epochs:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个循环中，我们每 20 个 epoch 绘制一次训练数据和评估数据的损失和准确度：
- en: '[PRE24]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `test()` method evaluates the model on the test set and returns the accuracy
    score:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test()` 方法在测试集上评估模型并返回准确度分数：'
- en: '[PRE25]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that our class is complete, we can create, train, and test an instance of
    MLP.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的类已完成，我们可以创建、训练并测试一个 MLP 实例。
- en: 'We have two datasets, so we need a model dedicated to `Cora` and another one
    for `Facebook Page-Page`. First, let’s train an MLP on `Cora`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个数据集，所以我们需要一个专门针对 `Cora` 的模型，以及另一个针对 `Facebook Page-Page` 的模型。首先，让我们在 `Cora`
    上训练一个 MLP：
- en: 'We create an MLP model and print it to check that our layers are correct:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个 MLP 模型并打印它，检查我们的层是否正确：
- en: '[PRE26]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This gives us the following output:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE27]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Good, we get the right number of features. Let’s train this model for 100 epochs:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好的，我们得到了正确数量的特征。让我们训练这个模型 100 个 epoch：
- en: '[PRE28]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here are the metrics that are printed in the training loop:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是训练循环中打印的指标：
- en: '[PRE29]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we can evaluate its performance in terms of accuracy with the following
    lines:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过以下代码评估它的准确度表现：
- en: '[PRE30]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We obtain the following accuracy score on test data:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在测试数据上获得了以下准确度分数：
- en: '[PRE31]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We repeat the same process for the `Facebook Page-Page` dataset, and here is
    the output we obtain:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对 `Facebook Page-Page` 数据集重复相同的过程，以下是我们获得的输出：
- en: '[PRE32]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Even though these datasets are similar in some aspects, we can see that the
    accuracy scores we obtain are vastly different. This will make an interesting
    comparison when we combine node features and network topology in the same model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些数据集在某些方面相似，但我们可以看到我们获得的准确度分数差异很大。当我们将节点特征和网络拓扑结合在同一个模型中时，这将形成一个有趣的比较。
- en: Classifying nodes with vanilla graph neural networks
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基础图神经网络对节点进行分类
- en: Instead of directly introducing well-known GNN architectures, let’s try to build
    our own model to understand the thought process behind GNNs. First, we need to
    go back to the definition of a simple linear layer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与其直接引入著名的 GNN 架构，不如尝试构建我们自己的模型，以理解 GNN 背后的思维过程。首先，我们需要回顾简单线性层的定义。
- en: A basic neural network layer corresponds to a linear transformation ![](img/Formula_B19153_05_001.png),
    where ![](img/Formula_B19153_05_002.png) is the input vector of node ![](img/Formula_B19153_05_005.png)
    and ![](img/Formula_B19153_05_003.png) is the weight matrix. In PyTorch, this
    equation can be implemented with the `torch.mm()` function, or with the `nn.Linear`
    class that adds other parameters such as biases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的神经网络层对应于线性变换 ![](img/Formula_B19153_05_001.png)，其中 ![](img/Formula_B19153_05_002.png)
    是节点 ![](img/Formula_B19153_05_005.png) 的输入向量，![](img/Formula_B19153_05_003.png)
    是权重矩阵。在 PyTorch 中，这个方程可以通过 `torch.mm()` 函数实现，或者使用 `nn.Linear` 类，它会添加其他参数，比如偏置项。
- en: 'With our graph datasets, the input vectors are node features. It means that
    nodes are completely separate from each other. This is not enough to capture a
    good understanding of the graph: like a pixel in an image, the context of a node
    is essential to understand it. If you look at a group of pixels instead of a single
    one, you can recognize edges, patterns, and so on. Likewise, to understand a node,
    you need to look at its neighborhood.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的图数据集，输入向量是节点特征。这意味着节点彼此完全独立。这还不足以充分理解图：就像图像中的一个像素，节点的上下文对于理解它至关重要。如果你看一组像素而不是单个像素，你可以识别出边缘、图案等等。同样，为了理解一个节点，你需要观察它的邻域。
- en: 'Let’s call ![](img/Formula_B19153_05_004.png) the set of neighbors of node
    ![](img/Formula_B19153_05_0051.png). Our **graph linear layer** can be written
    as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称 ![](img/Formula_B19153_05_004.png) 为节点 ![](img/Formula_B19153_05_0051.png)
    的邻居集。我们的 **图线性层** 可以如下书写：
- en: '![](img/Formula_B19153_05_006.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_05_006.jpg)'
- en: You can imagine several variants of this equation. For instance, we could have
    a weight matrix ![](img/Formula_B19153_05_007.png) dedicated to the central node,
    and another one ![](img/Formula_B19153_05_008.png) for the neighbors. Note that
    we cannot have a weight matrix per neighbor, as this number can change from node
    to node.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象这个方程的几种变体。例如，我们可以为中央节点设置一个权重矩阵 ![](img/Formula_B19153_05_007.png)，为邻居节点设置另一个权重矩阵
    ![](img/Formula_B19153_05_008.png)。需要注意的是，我们不能为每个邻居节点设置一个权重矩阵，因为邻居的数量会根据节点不同而变化。
- en: We’re talking about neural networks, so we can’t apply the previous equation
    to each node. Instead, we perform matrix multiplications that are much more efficient.
    For instance, the equation of the linear layer can be rewritten as ![](img/Formula_B19153_05_009.png),
    where ![](img/Formula_B19153_05_010.png) is the input matrix.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的是神经网络，所以我们不能对每个节点应用之前的方程。相反，我们执行矩阵乘法，这种方式更高效。例如，线性层的方程可以重写为 ![](img/Formula_B19153_05_009.png)，其中
    ![](img/Formula_B19153_05_010.png) 是输入矩阵。
- en: 'In our case, the adjacency matrix ![](img/Formula_B19153_05_011.png) contains
    the connections between every node in the graph. Multiplying the input matrix
    by this adjacency matrix will directly sum up the neighboring node features. We
    can add `self` loops to the adjacency matrix so that the central node is also
    considered in this operation. We call this updated adjacency matrix ![](img/Formula_B19153_05_012.png).
    Our graph linear layer can be rewritten as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，邻接矩阵 ![](img/Formula_B19153_05_011.png) 包含了图中每个节点之间的连接。将输入矩阵与这个邻接矩阵相乘，将直接累加邻居节点的特征。我们可以将
    `self` 循环添加到邻接矩阵中，这样就可以在这个操作中考虑中央节点。我们称这个更新后的邻接矩阵为 ![](img/Formula_B19153_05_012.png)。我们的图线性层可以重新编写如下：
- en: '![](img/Formula_B19153_05_013.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_05_013.jpg)'
- en: 'Let’s test this layer by implementing it in PyTorch Geometric. We’ll then be
    able to use it as a regular layer to build a GNN:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在 PyTorch Geometric 中实现这个层来测试它。然后我们就可以像普通层一样使用它来构建一个 GNN：
- en: 'First, we create a new class, which is a subclass of `torch.nn.Module`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个新的类，它是 `torch.nn.Module` 的子类：
- en: '[PRE33]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This class takes two parameters, `dim_in` and `dim_out`, for the number of
    features of the input and the output, respectively. We add a basic linear transformation
    without bias:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个类接收两个参数，`dim_in` 和 `dim_out`，分别代表输入和输出的特征数量。我们添加一个没有偏置的基本线性变换：
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We perform two operations – the linear transformation, and then the multiplication
    with the adjacency matrix ![](img/Formula_B19153_05_014.png):'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们执行两个操作——线性变换，然后与邻接矩阵 ![](img/Formula_B19153_05_014.png) 相乘：
- en: '[PRE35]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Before we can create our vanilla GNN, we need to convert the edge index from
    our dataset (`data.edge_index`) in coordinate format to a dense adjacency matrix.
    We also need to include `self` loops; otherwise, the central nodes won’t be taken
    into account in their own embeddings.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的普通 GNN 之前，我们需要将数据集中的边索引（`data.edge_index`）从坐标格式转换为稠密的邻接矩阵。我们还需要包括 `self`
    循环；否则，中央节点的嵌入不会被考虑。
- en: 'This is easily implemented with the `to_den_adj()` and `torch.eye()` functions:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个可以通过 `to_den_adj()` 和 `torch.eye()` 函数轻松实现：
- en: '[PRE36]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is what the adjacency matrix looks like:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是邻接矩阵的样子：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Unfortunately, we only see zeros in this tensor because it’s a sparse matrix.
    A more detailed print would show a few connections between nodes (represented
    by ones). Now that we have our dedicated layer and the adjacency matrix, the implementation
    of the vanilla GNN is very similar to that of the MLP.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们在这个张量中只看到零，因为它是一个稀疏矩阵。更详细的打印会显示一些节点之间的连接（用 1 表示）。现在，我们已经有了专用层和邻接矩阵，实现普通的
    GNN 与实现 MLP 非常相似。
- en: 'We create a new class with two vanilla graph linear layers:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个新的类，包含两个常规的图线性层：
- en: '[PRE38]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We perform the same operations with our new layers, which take the adjacency
    matrix we previously calculated as an additional input:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用新层执行相同的操作，这些层将之前计算的邻接矩阵作为额外输入：
- en: '[PRE39]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `fit()` and `test()` methods work in the exact same way:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit()` 和 `test()` 方法的工作方式完全相同：'
- en: '[PRE40]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can create, train, and evaluate our model with the following lines:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下几行代码来创建、训练和评估我们的模型：
- en: '[PRE41]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This gives us the following output:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE42]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We replicate the same training process with the `Facebook Page-Page` dataset.
    In order to obtain comparable results, the same experiment is repeated 100 times
    for each model on each dataset. The following table summarizes the results:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Facebook Page-Page`数据集复制了相同的训练过程。为了获得可比的结果，我们对每个模型和每个数据集重复了100次相同的实验。以下表格总结了结果：
- en: '|  | **MLP** | **GNN** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | **MLP** | **GNN** |'
- en: '| **Cora** | 53.47%(±1.81%) | 74.98%(±1.50%) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| **Cora** | 53.47%(±1.81%) | 74.98%(±1.50%) |'
- en: '| **Facebook** | 75.21%(±0.40%) | 84.85%(±1.68%) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **Facebook** | 75.21%(±0.40%) | 84.85%(±1.68%) |'
- en: Figure 5.4 – Summary of accuracy scores with a standard deviation
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 带标准偏差的准确度得分总结
- en: As we can see, the MLP has poor accuracy on `Cora`. It performs better on the
    `Facebook Page-Page` dataset but is still surpassed by our vanilla GNN in both
    cases. These results show the importance of including topological information
    in node features. Instead of a tabular dataset, our GNN considers the entire neighborhood
    of each node, which leads to a 10-20% boost in terms of accuracy in these examples.
    This architecture is still crude, but it gives us a guideline to refine it and
    build even better models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，MLP在`Cora`数据集上的准确率较低。它在`Facebook Page-Page`数据集上的表现较好，但仍然在这两个数据集中被我们的原始GNN超越。这些结果展示了在节点特征中包含拓扑信息的重要性。与表格数据集不同，我们的GNN考虑了每个节点的整个邻域，这在这些示例中带来了10-20%的准确率提升。这个架构仍然很粗糙，但它为我们改进和构建更好的模型提供了指导。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about the missing link between vanilla neural networks
    and GNNs. We built our own GNN architecture using our intuition and a bit of linear
    algebra. We explored two popular graph datasets from the scientific literature
    to compare our two architectures. Finally, we implemented them in PyTorch and
    evaluated their performance. The result is clear: even our intuitive version of
    a GNN completely outperforms the MLP on both datasets.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了原始神经网络和GNN之间缺失的联系。我们基于直觉和一些线性代数知识构建了自己的GNN架构。我们探索了来自科学文献的两个流行图数据集，以比较我们的两种架构。最后，我们在PyTorch中实现了它们，并评估了它们的性能。结果很明显：即使是我们直观版本的GNN，在这两个数据集上的表现也完全超越了MLP。
- en: 'In [*Chapter 6*](B19153_06.xhtml#_idTextAnchor074), *Normalizing Embeddings
    with Graph Convolutional Networks*, we refine our vanilla GNN architecture to
    correctly normalize its inputs. This graph convolutional network model is an incredibly
    efficient baseline we’ll keep using in the rest of the book. We will compare its
    results on our two previous datasets and introduce a new interesting task: node
    regression.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B19153_06.xhtml#_idTextAnchor074)《使用图卷积网络对嵌入进行归一化》中，我们对原始的GNN架构进行了改进，以正确归一化其输入。这个图卷积网络模型是一个极其高效的基准模型，我们将在本书的后续部分继续使用它。我们将对其在之前两个数据集上的结果进行比较，并引入一个新的有趣任务：节点回归。
- en: Further reading
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[1] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad,
    “Collective Classification in Network Data”, AIMag, vol. 29, no. 3, p. 93, Sep.
    2008\. Available: [https://ojs.aaai.org//index.php/aimagazine/article/view/2157](https://ojs.aaai.org//index.php/aimagazine/article/view/2157)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, 和 T. Eliassi-Rad,
    “网络数据中的集体分类”，AIMag，第29卷，第3期，第93页，2008年9月。可用链接：[https://ojs.aaai.org//index.php/aimagazine/article/view/2157](https://ojs.aaai.org//index.php/aimagazine/article/view/2157)'
- en: '[2] B. Rozemberczki, C. Allen, and R. Sarkar, Multi-Scale Attributed Node Embedding.
    arXiv, 2019\. doi: 10.48550/ARXIV.1909.13021\. Available: [https://arxiv.org/abs/1909.13021](https://arxiv.org/abs/1909.13021)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Rozemberczki, C. Allen, 和 R. Sarkar, 《多尺度属性化节点嵌入》。arXiv, 2019。doi: 10.48550/ARXIV.1909.13021。可用链接：[https://arxiv.org/abs/1909.13021](https://arxiv.org/abs/1909.13021)'
