- en: Introducing Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入循环神经网络
- en: This chapter will introduce you to the theoretical side of the **recurrent neural
    network** (**RNN**) model. Gaining knowledge about what lies behind this powerful
    architecture will give you a head start on mastering the practical examples that
    are provided later in the book. Since you may often find yourself in a situation
    where a critical decision for your application is needed, it is essential to be
    aware of the building parts of this model. This will help you act appropriately
    for the situation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍**循环神经网络**（**RNN**）模型的理论部分。了解这种强大架构背后的原理将帮助你更好地掌握本书后续提供的实际示例。由于你可能会经常遇到需要为应用做出关键决策的情况，因此了解这个模型的构成部分是至关重要的。这将帮助你在面对不同情况时做出合适的反应。
- en: 'The prerequisite knowledge for this chapter includes basic linear algebra (matrix
    operations). A basic knowledge in deep learning and neural networks is also a
    plus. If you are new to that field, I would recommend first watching the great
    series of videos made by Andrew Ng ([https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0));
    they will help you make your first steps so you are prepared to expand your knowledge.
    After reading the chapter, you will be able to answer questions such as the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前提知识包括基础的线性代数（矩阵运算）。对深度学习和神经网络有一定了解也是一个加分项。如果你是该领域的新手，我建议你首先观看Andrew Ng制作的优秀视频系列（[https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)）；这些视频将帮助你迈出第一步，为你扩展知识做好准备。阅读本章后，你将能够回答如下问题：
- en: What is an RNN?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是RNN？
- en: Why is an RNN better than other solutions?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么RNN比其他解决方案更好？
- en: How do you train an RNN?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练RNN？
- en: What are some problems with the RNN model?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN模型有哪些问题？
- en: What is an RNN?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是RNN？
- en: An RNN is one powerful model from the deep learning family that has shown incredible
    results in the last five years. It aims to make predictions on sequential data
    by utilizing a powerful memory-based architecture.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是深度学习家族中一个强大的模型，在过去五年中取得了令人惊讶的成果。它通过利用强大的基于记忆的架构，旨在对顺序数据进行预测。
- en: 'But how is it different from a standard neural network? A normal (also called
    **feedforward**) neural network acts like a mapping function, where a single input
    is associated with a single output. In this architecture, no two inputs share
    knowledge and the each moves in only one direction—starting from the input nodes,
    passing through hidden nodes, and finishing at the output nodes. Here is an illustration
    of the aforementioned model:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但它与标准神经网络有何不同？普通（也叫**前馈**）神经网络像一个映射函数，其中一个输入对应一个输出。在这种架构中，没有两个输入共享知识，每个输入只沿着一个方向移动——从输入节点开始，经过隐藏节点，最终到达输出节点。下面是上述模型的示意图：
- en: '![](img/0960058e-e745-455b-ae03-1896a3a317d3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0960058e-e745-455b-ae03-1896a3a317d3.png)'
- en: 'On the contrary, a recurrent (also called feedback) neural network uses an
    additional memory state. When an input A[1] (word **I**) is added, the network
    produces an output B[1] (word **love**) and stores information about the input
    A[1] in the memory state. When the next input A[2] (word **love**) is added, the
    network produces the associated output B[2] (word **to**) with the help of the
    memory state. Then, the memory state is updated using information from the new
    input A[2]. This operation is repeated for each input:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，循环神经网络（也叫反馈神经网络）使用额外的记忆状态。当输入A[1]（单词**I**）被加入时，网络会产生输出B[1]（单词**love**）并将输入A[1]的信息存储在记忆状态中。当下一个输入A[2]（单词**love**）加入时，网络会借助记忆状态生成关联的输出B[2]（单词**to**）。然后，记忆状态会使用来自新输入A[2]的信息进行更新。这个操作会对每个输入重复进行：
- en: '![](img/2afa42f1-d542-46e4-9c28-3317f227173a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2afa42f1-d542-46e4-9c28-3317f227173a.png)'
- en: You can see how with this method our predictions depend not only on the current
    input, but also on previous data. This is the reason why RNNs are the state-of-the-art
    model for dealing with sequences. Let's illustrate this with some examples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在这种方法中，我们的预测不仅依赖于当前输入，还依赖于之前的数据。这就是为什么RNN是处理序列问题的最先进模型。让我们通过一些例子来说明这一点。
- en: A typical use case for the feedforward architecture is image recognition. We
    can see its application in agriculture for analyzing plants, in healthcare for
    diagnosing diseases, and in driverless cars for detecting pedestrians. Since no
    output in any of these examples requires specific information from a previous
    input, the feedforward network is a great fit for such problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的前馈网络应用案例是图像识别。我们可以看到它在农业中的应用，例如分析植物，在医疗保健中用于疾病诊断，以及在无人驾驶汽车中用于检测行人。由于这些例子中的任何输出都不需要来自先前输入的特定信息，前馈网络非常适合这种类型的问题。
- en: 'There is also another set of problems, which are based on sequential data.
    In these cases, predicting the next element in the sequence depends on all the
    previous elements. The following is a list of several examples:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一类问题，基于序列数据。在这些情况下，预测序列中的下一个元素依赖于所有先前的元素。以下是一些示例：
- en: Translating text to speech
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本转语音
- en: Predicting the next word in a sentence
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测句子中的下一个词
- en: Converting audio to text
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将音频转换为文本
- en: Language translation
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Captioning videos
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频字幕生成
- en: RNNs were first introduced in the 1980s with the invention of the Hopfield network.
    Later, in 1997, Hochreiter and Schmidhuber proposed an advanced RNN model called
    **long short-term memory** (**LSTM**). It aims to solve some major issues with
    the simplest recurrent neural network model, which we will reveal later in the
    chapter. A more recent improvement to the RNN family was presented in 2014 by
    Chung et al. This new architecture, called Gated Recurrent Unit, solves the same
    problem as LSTM but in a simpler manner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 最早是在 1980 年代通过霍普菲尔德网络的发明而提出的。后来，在 1997 年，Hochreiter 和 Schmidhuber 提出了一个先进的
    RNN 模型，叫做 **长短时记忆**（**LSTM**）。它旨在解决一些最简单的递归神经网络模型存在的主要问题，这些问题将在本章稍后揭示。2014 年，Chung
    等人提出了 RNN 系列的另一个改进。这种新架构叫做门控递归单元（GRU），以更简单的方式解决了与 LSTM 相同的问题。
- en: In the next chapters of this book, we will go over the aforementioned models
    and see how they work and why researchers and large companies are using them on
    a daily basis to solve fundamental problems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章中，我们将介绍上述模型，了解它们的工作原理，并探讨为什么研究人员和大公司每天都在使用它们来解决基本问题。
- en: Comparing recurrent neural networks with similar models
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较递归神经网络与类似模型
- en: In recent years, RNNs, similarly to any neural network model, have become widely
    popular due to the easier access to large amounts of structured data and increases
    in computational power. But researchers have been solving sequence-based problems
    for decades with the help of other methods, such as the Hidden Markov Model. We
    will briefly compare this technique to an RNNs and outline the benefits of both
    approaches.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，类似于任何神经网络模型，RNN 因为更容易访问大量结构化数据和计算能力的提升而变得广泛流行。但研究人员通过其他方法（如隐马尔可夫模型）已经解决序列问题几十年了。我们将简要地将这种技术与
    RNN 进行比较，并概述两种方法的优点。
- en: The **Hidden Markov Model** (**HMM**) is a probabilistic sequence model that
    aims to assign a label (class) to each element in a sequence. HMM computes the
    probability for each possible sequence and picks the most likely one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐马尔可夫模型**（**HMM**）是一种概率序列模型，旨在为序列中的每个元素分配一个标签（类别）。HMM 计算每个可能序列的概率，并选择最可能的一个。'
- en: Both the HMM and RNN are powerful models that yield phenomenal results but,
    depending on the use case and resources available, RNN can be much more effective.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: HMM 和 RNN 都是强大的模型，能够产生惊人的结果，但根据使用场景和可用资源，RNN 可以更为有效。
- en: Hidden Markov model
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: 'The following are the pros and cons of a Hidden Markov Model when solving sequence-related
    tasks:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是隐马尔可夫模型在解决序列相关任务时的优缺点：
- en: '**Pros:** Less complex to implement, works faster and as efficiently as RNNs
    on problems of medium difficulty.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点：** 实现起来较为简单，且在中等难度问题上能像 RNN 一样更快速、高效地工作。'
- en: '**Cons:** HMM becomes exponentially expensive with the desire to increase accuracy.
    For example, predicting the next word in a sentence may depend on a word from
    far behind. HMM needs to perform some costly operations to obtain this information.
    That is the reason why this model is not ideal for complex tasks that require
    large amounts of data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点：** 随着精度要求的提高，HMM 会变得指数级昂贵。例如，预测句子中的下一个词可能依赖于一个很久之前的词。HMM 需要执行一些昂贵的操作来获取这些信息。这也是该模型不适合处理需要大量数据的复杂任务的原因。'
- en: These costly operations include calculating the probability for each possible
    element with respect to all the previous elements in the sequence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些昂贵的操作包括计算相对于序列中所有先前元素的每个可能元素的概率。
- en: Recurrent neural network
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'The following are the pros and cons of a recurrent neural network when solving
    sequence-related tasks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是递归神经网络在解决与序列相关任务时的优缺点：
- en: '**Pros**: Performs significantly better and is less expensive when working
    on complex tasks with large amounts of data.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：在处理复杂任务和大量数据时，表现显著更好，且成本较低。'
- en: '**Cons**: Complex to build the right architecture suitable for a specific problem.
    Does not yield better results if the prepared data is relatively small.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：构建适合特定问题的正确架构较为复杂。如果准备的数据相对较小，结果不会更好。'
- en: As a result of our observations, we can state that RNNs are slowly replacing
    HMMs in the majority of real-life applications. One ought to be aware of both
    models, but with the right architecture and data, RNNs often end up being the
    better choice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的观察，可以得出结论，RNN正在逐渐取代大多数实际应用中的HMM。我们应该了解这两种模型，但在正确的架构和数据下，RNN往往是更好的选择。
- en: 'Nevertheless, if you are interested in learning more about hidden Markov models,
    I strongly recommend going through some video series ([https://www.youtube.com/watch?v=TPRoLreU9lA](https://www.youtube.com/watch?v=TPRoLreU9lA)) and
    papers of example applications, such as *Introduction to Hidden Markov Models* by
    Degirmenci (Harvard University) ([https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf](https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf))
    or *Issues and Limitations of HMM in Speech Processing: A Survey* ([https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf](https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf)).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你有兴趣深入了解隐马尔可夫模型，我强烈建议你观看一些视频系列（[https://www.youtube.com/watch?v=TPRoLreU9lA](https://www.youtube.com/watch?v=TPRoLreU9lA)）和一些应用实例的论文，如Degirmenci（哈佛大学）撰写的《隐马尔可夫模型简介》（[https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf](https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf)）或《隐马尔可夫模型在语音处理中的问题与局限性：综述》（[https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf](https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf)）。
- en: Understanding how recurrent neural networks work
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解递归神经网络的工作原理
- en: With the use of a memory state, the RNN architecture perfectly addresses every
    sequence-based problem. In this section of the chapter, we will go over a full
    explanation of how this works. You will obtain knowledge about the general characteristics
    of a neural network as well as what makes RNNs special. This section emphasizes on
    the theoretical side (including mathematical equations), but I can assure you
    that once you grasp the fundamentals, any practical example will go smoothly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用记忆状态，RNN架构完美地解决了每一个基于序列的问题。在这一章节中，我们将全面解释其工作原理。你将了解神经网络的基本特征，以及RNN的独特之处。本节重点讲解理论部分（包括数学公式），但我可以保证，一旦你掌握了基础，任何实际案例都将顺利进行。
- en: To make the explanations understandable, let's discuss the task of generating
    text and, in particular, producing a new chapter based on one of my favorite book
    series, *The Hunger Games*, by Suzanne Collins.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让解释更易于理解，我们来讨论一下生成文本的任务，特别是基于我最喜欢的书籍系列之一《饥饿游戏》（*The Hunger Games*）由Suzanne
    Collins编写的，创作一个新章节。
- en: Basic neural network overview
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的神经网络概述
- en: 'At the highest level, a neural network, which solves supervised problems, works
    as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，解决监督问题的神经网络的工作方式如下：
- en: Obtain training data (such as images for image recognition or sentences for
    generating text)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练数据（例如图像用于图像识别或句子用于生成文本）
- en: Encode the data (neural networks work with numbers so a numeric representation
    of the data is required)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码数据（神经网络处理的是数字，因此需要数据的数字表示）
- en: Build the architecture of your neural network model
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建神经网络模型的架构
- en: Train the model until you are satisfied with the results
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型直到你对结果满意为止
- en: Evaluate your model by making a fresh new prediction
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过做出一个全新的预测来评估你的模型
- en: Let's see how these steps are applied for an RNN.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些步骤如何应用于RNN。
- en: Obtaining data
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: For the problem of generating a new book chapter based on the book series *The
    Hunger Games*, you can extract the text from all books in *The Hunger Games* series
    (*The Hunger Games*, *Mockingjay*,and *Catching Fire*) by copying and pasting
    it. To do that, you need to find the books, content online.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于《饥饿游戏》系列书籍生成新章节的问题，你可以通过复制和粘贴的方式提取《饥饿游戏》系列所有书籍中的文本（《饥饿游戏》，《嘲笑鸟》和《燃烧的旗帜》）。为此，你需要在网上找到这些书籍和内容。
- en: Encoding the data
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码数据
- en: We use *word embeddings* ([https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/))
    for this purpose. Word embedding is a collective name of all techniques where
    words or phrases from a vocabulary are mapped to vectors of real numbers. Some
    methods include *one-hot encoding*, *word2vec*, and *GloVe*. You will learn more
    about them in the forthcoming chapters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 *词嵌入*（[https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)）来实现这一目标。词嵌入是将词汇表中的单词或短语映射到实数向量的一种集体称谓。一些方法包括
    *独热编码*、*word2vec* 和 *GloVe*。你将在接下来的章节中了解更多关于它们的信息。
- en: Building the architecture
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建架构
- en: 'Each neural network consists of three sets of layers—input, hidden, and output.
    There is always one input and one output layer. If the neural network is deep,
    it has multiple hidden layers:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经网络由三组层组成——输入层、隐藏层和输出层。总是有一个输入层和一个输出层。如果神经网络较深，则会有多个隐藏层：
- en: '![](img/3e6fdf95-417b-4f9e-b287-d43d59d98fcb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e6fdf95-417b-4f9e-b287-d43d59d98fcb.png)'
- en: 'The difference between an RNN and the standard feedforward network comes in
    the cyclical hidden states. As seen in the following diagram, recurrent neural
    networks use cyclical hidden states. This way, data propagates from one time step
    to another, making each one of these steps dependent on the previous:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 和标准前馈网络的区别在于其循环隐藏状态。正如下图所示，循环神经网络使用循环隐藏状态。这样，数据从一个时间步传播到另一个时间步，使得每一个时间步都依赖于前一个时间步：
- en: '![](img/5ab51cdd-7c1a-4589-8cd3-e42648ca7991.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ab51cdd-7c1a-4589-8cd3-e42648ca7991.png)'
- en: 'A common practice is to unfold the preceding diagram for better and more fluent
    understanding. After rotating the illustration vertically and adding some notations
    and labels, based on the example we picked earlier (generating a new chapter based
    on *The Hunger Game*s books), we end up with the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的做法是展开上述图示，以便更好、更流畅地理解。通过将插图垂直旋转并添加一些符号和标签，基于我们之前选择的示例（基于《饥饿游戏》书籍生成新章节），我们最终得到了以下图示：
- en: '![](img/1f692adf-92d4-45dc-88a9-d6481ba195b8.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f692adf-92d4-45dc-88a9-d6481ba195b8.png)'
- en: 'This is an unfolded RNN with one hidden layer. The identically looking sets
    of (input + hidden RNN unit + output) are actually the different time steps (or
    cycles) in the RNN. For example, the combination of ![](img/2c7fd095-312a-4ed7-bbcc-ee8f3bb1e87a.png) +
    RNN + ![](img/ba8c3aef-052e-45c6-aded-7f0fdf5900a7.png) illustrates what is happening
    at time step  ![](img/aa6cc93f-ec7f-43b5-9cb7-857d56873e7a.png). At each time
    step, these operations perform as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个展开的 RNN，具有一个隐藏层。看似相同的（输入 + 隐藏 RNN 单元 + 输出）集合实际上是 RNN 中的不同时间步（或循环）。例如，组合
    ![](img/2c7fd095-312a-4ed7-bbcc-ee8f3bb1e87a.png) + RNN + ![](img/ba8c3aef-052e-45c6-aded-7f0fdf5900a7.png)
    展示了在时间步 ![](img/aa6cc93f-ec7f-43b5-9cb7-857d56873e7a.png) 时发生的情况。在每个时间步，这些操作执行如下：
- en: The network encodes the word at the current time step (for example, *t-1*) using
    any of the word embedding techniques and produces a vector ![](img/03170e2f-c7b6-46e2-a41c-73b750d49306.png) (The
    produced vector can be ![](img/e38865d2-edc6-4d1e-b2e4-a04c750037fd.png) or ![](img/7baad917-f493-4b4f-86f4-5a73c6bc2803.png) depending
    on the specific time step)
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络使用任何词嵌入技术对当前时间步（例如，*t-1*）的词进行编码，并生成一个向量 ![](img/03170e2f-c7b6-46e2-a41c-73b750d49306.png)（生成的向量可以是
    ![](img/e38865d2-edc6-4d1e-b2e4-a04c750037fd.png) 或 ![](img/7baad917-f493-4b4f-86f4-5a73c6bc2803.png)，具体取决于时间步的不同）
- en: Then, ![](img/d1ffcf32-69ed-49ad-8513-ac64521a3811.png), the encoded version
    of the input word **I** at time step *t-1*, is plugged into the RNN cell (located
    in the hidden layer). After several equations (not displayed here but happening
    inside the RNN cell), the cell produces an output ![](img/3a8d2142-2b9f-49cc-ac38-c7d2a8418570.png) and
    a memory state ![](img/0e571081-ba4c-46fc-894e-082186548e71.png). The memory state
    is the result of the input ![](img/6ebaf1d4-ff95-459d-8d31-1e23e76381e9.png) and
    the previous value of that memory state ![](img/5cc65152-194e-4699-add4-6b897aa1ae88.png).
    For the initial time step, one can assume that ![](img/c87969e4-4c5c-471c-bcc1-670173b3560e.png) is
    a zero vector
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，![](img/d1ffcf32-69ed-49ad-8513-ac64521a3811.png)，输入单词**I**在时间步* t-1 *的编码版本，被插入到RNN单元（位于隐藏层）。经过若干方程（此处未显示，但在RNN单元内部发生），单元生成输出 ![](img/3a8d2142-2b9f-49cc-ac38-c7d2a8418570.png)和一个记忆状态 ![](img/0e571081-ba4c-46fc-894e-082186548e71.png)。记忆状态是输入 ![](img/6ebaf1d4-ff95-459d-8d31-1e23e76381e9.png)和该记忆状态的前一个值 ![](img/5cc65152-194e-4699-add4-6b897aa1ae88.png)的结果。对于初始时间步，可以假设 ![](img/c87969e4-4c5c-471c-bcc1-670173b3560e.png)是一个零向量。
- en: Producing the actual word (volunteer) at time step *t-1* happens after decoding
    the output ![](img/3a8d2142-2b9f-49cc-ac38-c7d2a8418570.png) using a *text corpus* specified
    at the beginning of the training
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步* t-1 *时生成实际的单词（志愿者）发生在解码输出 ![](img/3a8d2142-2b9f-49cc-ac38-c7d2a8418570.png)时，使用的是训练开始时指定的*文本语料库*。
- en: Finally, the network moves multiple time steps forward until reaching the final
    step where it predicts the word
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，网络继续向前推进多个时间步，直到达到最终步骤，在那里它预测出单词。
- en: You can see how each one of {…, ![](img/5017ca77-a53e-4b7e-9266-f1da40891353.png), ![](img/ca7494e8-58cb-442f-91f4-e9ad07bc8180.png), ![](img/d376e907-27bb-42b1-a831-7fb348d249e6.png),
    …} holds information about all the previous inputs. This makes RNNs very special
    and really good at predicting the next unit in a sequence. Let's now see what
    mathematical equations sit behind the preceding operations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每一个{…, ![](img/5017ca77-a53e-4b7e-9266-f1da40891353.png), ![](img/ca7494e8-58cb-442f-91f4-e9ad07bc8180.png), ![](img/d376e907-27bb-42b1-a831-7fb348d249e6.png),
    …}都包含有关所有先前输入的信息。这使得RNN（循环神经网络）非常特殊，且在预测序列中的下一个单元时表现得尤为出色。现在让我们来看一下支撑这些操作的数学方程。
- en: Text corpus—an array of all words in the example vocabulary.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 文本语料库——示例词汇表中所有单词的数组。
- en: Training the model
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'All the magic in this model lies behind the RNN cells. In our simple example,
    each cell presents the same equations, just with a different set of variables.
    A detailed version of a single cell looks like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的所有“魔法”都在于RNN单元。在我们简单的例子中，每个单元呈现相同的方程，只是变量集不同。一个单元的详细版本如下所示：
- en: '![](img/cc1142dc-a296-4282-929c-c837bb8156ef.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc1142dc-a296-4282-929c-c837bb8156ef.png)'
- en: 'First, let''s explain the new terms that appear in the preceding diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们解释前面图表中出现的新术语：
- en: '**Weights** (![](img/2ab90e1a-84a7-4944-8ea9-ac7dd4926e98.png), ![](img/1f8e671b-caa3-4eb3-9834-073e1f82f11e.png),
    ![](img/5e662750-3c2c-46ce-b196-e610129a9f75.png)): A weight is a matrix (or a
    number) that represents the strength of the value it is applied to. For example, ![](img/ed73bf73-06a9-471c-ba23-8ecff48cc905.png)determines
    how much of the input ![](img/6a3d0d98-3604-4252-b2f9-80c042e561aa.png)should
    be considered in the following equations.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**（![](img/2ab90e1a-84a7-4944-8ea9-ac7dd4926e98.png), ![](img/1f8e671b-caa3-4eb3-9834-073e1f82f11e.png),
    ![](img/5e662750-3c2c-46ce-b196-e610129a9f75.png)）：权重是一个矩阵（或数字），表示它所应用的值的强度。例如，![](img/ed73bf73-06a9-471c-ba23-8ecff48cc905.png)决定了输入 ![](img/6a3d0d98-3604-4252-b2f9-80c042e561aa.png)在后续方程中应该被考虑的程度。'
- en: If  ![](img/5548d652-c336-497d-92ca-f608725bd586.png)consists of high values,
    then ![](img/aa76c78b-9a21-4286-ab8f-65d4db13ea2a.png)should have significant
    influence on the end result. The weight values are often initialized randomly
    or with a distribution (such as normal/Gaussian distribution). It is important
    to be noted that  ![](img/0bd1b875-eaa0-4056-83bd-614aa9e82f23.png),  ![](img/cb748064-2362-413f-91dd-f5f1dbf0f713.png), 
    and ![](img/ef1c08c8-a815-4898-a69a-5b83fb4ff560.png) are the same for each step.
    Using the backpropagation algorithm, they are being modified with the aim of 
    producing accurate predictions
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 ![](img/5548d652-c336-497d-92ca-f608725bd586.png)包含较高的值，那么 ![](img/aa76c78b-9a21-4286-ab8f-65d4db13ea2a.png)应该对最终结果有显著的影响。权重值通常是随机初始化的，或者使用某种分布（例如正态/高斯分布）。需要注意的是， ![](img/0bd1b875-eaa0-4056-83bd-614aa9e82f23.png)， ![](img/cb748064-2362-413f-91dd-f5f1dbf0f713.png)，以及 ![](img/ef1c08c8-a815-4898-a69a-5b83fb4ff560.png) 在每个步骤中都是相同的。通过反向传播算法，它们会被修改，目的是产生准确的预测。
- en: '**Biases** (![](img/cf5c1a7d-e905-40b6-b72a-296f678a6497.png), ![](img/5f181ca2-832e-44a0-a169-6125059fe5ac.png)):
    An offset vector (different for each layer), which adds a change to the value
    of the output ![](img/b5e6a945-1bf6-4006-a131-89b958f982df.png)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏置** (![](img/cf5c1a7d-e905-40b6-b72a-296f678a6497.png), ![](img/5f181ca2-832e-44a0-a169-6125059fe5ac.png)):
    一个偏移向量（每一层不同），它将一个变化加到输出值上！[](img/b5e6a945-1bf6-4006-a131-89b958f982df.png)'
- en: '**Activation function** (**tanh**): This determines the final value of the
    current memory state ![](img/498e4f45-afe2-4a6d-920b-246b0c8ba1af.png) and the
    output ![](img/2ea2f288-c311-445e-bbc9-46e4131d20f8.png). Basically, the activation
    functions map the resultant values of several equations similar to the following
    ones into a desired range: (`-1, 1`) if we are using the **tanh** function, (`0,
    1`) if we are using sigmoid function, and (`0, +infinity`) if we are using ReLu
    ([https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks](https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks))'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数** (**tanh**): 这个函数决定当前记忆状态 ![](img/498e4f45-afe2-4a6d-920b-246b0c8ba1af.png)
    和输出 ![](img/2ea2f288-c311-445e-bbc9-46e4131d20f8.png) 的最终值。基本上，激活函数将类似以下的多个方程式的结果值映射到期望的范围：如果使用
    **tanh** 函数，则范围为 (`-1, 1`)；如果使用 sigmoid 函数，则范围为 (`0, 1`)；如果使用 ReLu，则范围为 (`0, +infinity`)
    ([https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks](https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks))'
- en: 'Now, let''s go over the process of computing the variables. To calculate ![](img/bc7b3fd5-4fb7-4d84-94f5-a69d8a1c8fc7.png)
    and ![](img/2ea2f288-c311-445e-bbc9-46e4131d20f8.png), we can do the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下计算变量的过程。为了计算 ![](img/bc7b3fd5-4fb7-4d84-94f5-a69d8a1c8fc7.png) 和 ![](img/2ea2f288-c311-445e-bbc9-46e4131d20f8.png)，我们可以做如下操作：
- en: '![](img/7b42d2cb-3a54-4673-b594-9d26434eb57e.png)![](img/ff12e2a1-875f-403f-97d0-b1055df7b1e2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b42d2cb-3a54-4673-b594-9d26434eb57e.png)![](img/ff12e2a1-875f-403f-97d0-b1055df7b1e2.png)'
- en: As you can see, the memory state ![](img/2f13a990-cb41-4a06-9cc0-76179861ef76.png)
    is a result of the previous value ![](img/a1a73217-c1a7-4e18-b3ef-c21d6c69f20a.png)
    and the input ![](img/7c0302c2-2f04-4f75-9594-ec6f3b7c1b0e.png). Using this formula
    helps in retaining information about all the previous states.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，记忆状态 ![](img/2f13a990-cb41-4a06-9cc0-76179861ef76.png) 是前一个值 ![](img/a1a73217-c1a7-4e18-b3ef-c21d6c69f20a.png)
    和输入 ![](img/7c0302c2-2f04-4f75-9594-ec6f3b7c1b0e.png) 的结果。使用此公式有助于保留关于所有先前状态的信息。
- en: The input ![](img/398dabbd-5043-437e-bfee-1082a8fb5968.png) is a one-hot representation
    of the word *volunteer*. Recall from before that one-hot encoding is a type of
    word embedding. If the text corpus consists of 20,000 unique words and volunteer is
    the 19th word, then ![](img/7d341d05-cdab-47a1-acb7-5434a25cdd8c.png) is a 20,000-dimensional
    vector where all elements are 0 except the one at the 19^(th) position, which
    has a value of 1, which suggests that we only taking into account this particular
    word.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 ![](img/398dabbd-5043-437e-bfee-1082a8fb5968.png) 是单词 *volunteer* 的独热编码表示。回想一下，独热编码是一种词嵌入方式。如果文本语料库包含
    20,000 个独特的单词，且 "volunteer" 是第 19 个单词，那么 ![](img/7d341d05-cdab-47a1-acb7-5434a25cdd8c.png)
    是一个 20,000 维的向量，所有元素为 0，除了第 19 个位置，其值为 1，表示我们只考虑这个特定的单词。
- en: 'The sum between ![](img/c16273cd-dad0-49f0-913d-e5235ff8d9cd.png), ![](img/7c299c35-24cf-4226-a201-2840b5309a7a.png), and ![](img/b0fb3e81-ef6f-4853-b4d5-d63c819ff4b6.png)
    is passed to the *tanh* activation function, which squashes the result between
    `-1` and `1` using the following formula:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 ![](img/c16273cd-dad0-49f0-913d-e5235ff8d9cd.png)、 ![](img/7c299c35-24cf-4226-a201-2840b5309a7a.png)
    和 ![](img/b0fb3e81-ef6f-4853-b4d5-d63c819ff4b6.png) 相加，结果被传递到 *tanh* 激活函数，该函数使用以下公式将结果压缩到
    `-1` 和 `1` 之间：
- en: '![](img/6e3eb640-9563-4b39-b371-cef15c75b12b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e3eb640-9563-4b39-b371-cef15c75b12b.png)'
- en: In this, `e = 2.71828` (Euler's number) and *z* is any real number.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，`e = 2.71828`（欧拉数），*z* 是任何实数。
- en: 'The output ![](img/2a4b4321-8736-45c7-82f6-d05d8acab57d.png) at time step t
    is calculated using ![](img/18f550fa-70c9-4d40-bb6e-26eb0fa20a22.png) and the
    `softmax` function. This function can be categorized as an activation with the
    exception that its primary usage is at the output layer when a probability distribution
    is needed. For example, predicting the correct outcome in a classification problem
    can be achieved by picking the highest probable value from a vector where all
    the elements sum up to `1`. Softmax produces this vector, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步 t 时的输出 ![](img/2a4b4321-8736-45c7-82f6-d05d8acab57d.png) 是通过 ![](img/18f550fa-70c9-4d40-bb6e-26eb0fa20a22.png)
    和 `softmax` 函数计算得出的。这个函数可以归类为激活函数，但与其他激活函数不同的是，它主要用于输出层，尤其是在需要概率分布时。例如，在分类问题中，预测正确结果可以通过从一个所有元素总和为`1`的向量中选取概率最高的值来实现。Softmax
    就是生成这种向量的函数，具体如下：
- en: '![](img/c8e8ea49-faae-45b9-8d4b-c16bf01d50b5.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8e8ea49-faae-45b9-8d4b-c16bf01d50b5.png)'
- en: In this, `e = 2.71828` (Euler's number) and z is a K-dimensional vector. The
    formula calculates probability for the value at the `i`^(th) position in the vector
    z.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，`e = 2.71828`（欧拉数），z 是一个 K 维向量。该公式计算了向量 z 中`i`^(th)位置值的概率。
- en: After applying the `softmax` function, ![](img/5779c435-a0bb-465d-8950-49978065336d.png)
    becomes a vector of the same dimension as ![](img/aad81213-464e-4c00-b983-da58fb2b719a.png)
    (the corpus size `20,000`) with all its elements having a total sum of `1`. With
    that in mind, finding the predicted word from the text corpus becomes straightforward.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 应用`softmax`函数后，![](img/5779c435-a0bb-465d-8950-49978065336d.png)变成了与![](img/aad81213-464e-4c00-b983-da58fb2b719a.png)（语料库大小`20,000`）相同维度的向量，且所有元素的总和为`1`。有了这一点，从文本语料库中找到预测的单词变得非常简单。
- en: Evaluating the model
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Once an assumption for the next word in the sequence is made, we need to assess
    how good this prediction is. To do that, we need to compare the predicted word ![](img/13a825fd-b8f2-4656-8bf5-b6745ce0c43d.png)
    with the actual word from the training data (let''s call it  ![](img/baadadd4-3f8c-497c-984a-cb2e098c697d.png)).
    This operation can be accomplished using a loss (cost) function. These types of
    functions aim to find the error between predicted and actual values. Our choice
    will be the cross-entropy loss function, which looks like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦对序列中的下一个单词做出假设，我们需要评估这个预测有多准确。为了做到这一点，我们需要将预测的单词 ![](img/13a825fd-b8f2-4656-8bf5-b6745ce0c43d.png)
    与训练数据中的实际单词（我们称之为 ![](img/baadadd4-3f8c-497c-984a-cb2e098c697d.png)）进行比较。这个操作可以通过损失（代价）函数来完成。这些函数旨在找到预测值与实际值之间的误差。我们选择的函数是交叉熵损失函数，公式如下：
- en: '![](img/a6ebb6e0-25eb-4d9a-9706-e72df4bb8ddd.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6ebb6e0-25eb-4d9a-9706-e72df4bb8ddd.png)'
- en: Since we are not going to give a detailed explanation of this formula, you can
    treat it as a black box. If you are curious about how it works, I recommend reading
    the article *Improving the way neural networks work* by Michael Nielson ([http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function)).
    A useful thing to know is that the cross-entropy function performs really well
    on classification problems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不会对这个公式进行详细解释，你可以把它当作一个黑箱。如果你对它是如何工作的感兴趣，建议阅读 Michael Nielson 写的文章《改进神经网络的工作方式》([http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function))。有用的信息是，交叉熵函数在分类问题中表现得非常好。
- en: After computing the error, we came to one of the most complex and, at the same
    time, powerful techniques in deep learning, called backpropagation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完误差后，我们进入了深度学习中最复杂且最强大的技术之一——反向传播。
- en: In simple terms, we can state that the backpropagation algorithm traverses backward
    through all (or several) time steps while updating the weights and biases of the
    network. After repeating this procedure, and a certain amount of training steps,
    the network learns the correct parameters and will be able to yield better predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们可以这样表述：反向传播算法会在更新网络的权重和偏置时，逆向遍历所有（或多个）时间步。经过多次重复这一过程，并进行一定数量的训练步骤后，网络能够学习到正确的参数，并能够产生更好的预测结果。
- en: To clear out any confusion, training and time steps are completely different
    terms. In one time step, we get a single element from the sequence and predict
    the next one. A training step is composed of multiple time steps where the number
    of time steps depends on how large the sequence for this training step is. In
    addition, time steps are only used in RNNs, but training ones are a general neural
    network concept.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清任何混淆，训练和时间步骤是完全不同的术语。在一个时间步骤中，我们从序列中获得一个元素并预测下一个元素。一个训练步骤由多个时间步骤组成，时间步骤的数量取决于该训练步骤的序列长度。此外，时间步骤仅在RNN中使用，而训练步骤是一个通用的神经网络概念。
- en: After each training step, we can see that the value from the loss function decreases.
    Once it crosses a certain threshold, we can state that the network has successfully
    learned to predict new words in the text.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练步骤后，我们可以看到来自损失函数的值减小。一旦它超过某个阈值，我们可以说网络已经成功学会了预测文本中的新词。
- en: 'The last step is to generate the new chapter. This can happen by choosing a
    random word as a start (such as: games) and then predicting the next words using
    the preceding formulas with the pre-trained weights and biases. Finally, we should
    end up with somewhat meaningful text.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是生成新的章节。这可以通过选择一个随机词作为开始（例如：games），然后使用先前的公式和预训练的权重与偏置预测下一个词。最终，我们应该得到一些有意义的文本。
- en: Key problems with the standard recurrent neural network model
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准递归神经网络模型的关键问题
- en: Hopefully, now you have a good understanding of how a recurrent neural network
    works. Unfortunately, this simple model fails to make good predictions on longer
    and complex sequences. The reason behind this lies in the so-called vanishing/exploding
    gradient problem that prevents the network from learning efficiently.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在你已经对递归神经网络的工作原理有了很好的理解。不幸的是，这个简单的模型在处理更长和更复杂的序列时无法做出好的预测。其背后的原因在于所谓的梯度消失/爆炸问题，这使得网络无法高效地学习。
- en: As you already know, the training process updates the weights and biases using
    the backpropagation algorithm. Let's dive one step further into the mathematical
    explanations. In order to know how much to adjust the parameters (weights and
    biases), the network computes the derivative of the loss function (at each time
    step) with respect to the current value of these parameters. When this operation
    is done for multiple time steps with the same set of parameters, the value of
    the derivative can become too large or too small. Since we use it to update the
    parameters, a large value can result in undefined weights and biases and a small
    value can result in no significant update, and thus no *learning*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，训练过程通过反向传播算法更新权重和偏置。让我们进一步深入数学解释。为了知道该调整多少参数（权重和偏置），网络计算损失函数相对于当前参数值的导数（在每个时间步骤）。当对多个时间步骤使用相同的参数集进行此操作时，导数的值可能会变得过大或过小。由于我们用它来更新参数，过大的值可能导致权重和偏置未定义，过小的值则可能导致没有显著更新，从而没有*学习*。
- en: Derivative is a way to show the rate of change; that is, the amount by which
    a function is changing at one given point. In our case, this is the rate of change
    of the loss function with respect to the given weights and biases.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 导数是表示变化率的一种方式；也就是说，它表示函数在某个特定点的变化量。在我们的例子中，这是损失函数相对于给定权重和偏置的变化率。
- en: This issue was first addressed by Bengio et al. in 1994, which led to an introduction
    of the LSTM network with the aim of solving the vanishing/exploding gradient problem.
    Later in the book, we will reveal how LSTM does this in an excellent fashion.
    Another model, which also overcomes this challenge, is the gated recurrent unit. In
    [Chapter 3](05d1d826-3bc6-48e4-ad62-a53c7a609059.xhtml), *Generating Your Own
    Book Chapter,* you will see how this is being done.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题最早由Bengio等人在1994年提出，这导致了LSTM网络的引入，旨在解决梯度消失/爆炸问题。稍后在本书中，我们将揭示LSTM是如何以出色的方式解决这一问题的。另一个同样克服这一挑战的模型是门控递归单元。在[第3章](05d1d826-3bc6-48e4-ad62-a53c7a609059.xhtml)，*生成你的章节*，你将看到如何做到这一点。
- en: For more information on the vanishing/exploding gradient problem, it would be
    useful to go over Lecture 8 from the course *Natural Language Processing with
    Deep Learning* by Stanford University ([https://www.youtube.com/watch?v=Keqep_PKrY8](https://www.youtube.com/watch?v=Keqep_PKrY8))
    and the paper *On the difficulty of training recurrent neural networks* ([http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于梯度消失/爆炸问题的信息，建议回顾斯坦福大学的《深度学习自然语言处理》课程第8讲([https://www.youtube.com/watch?v=Keqep_PKrY8](https://www.youtube.com/watch?v=Keqep_PKrY8))和论文《训练递归神经网络的困难》([http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf))。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we introduce the recurrent neural network model using theoretical
    explanations together with a particular example. The aim is to grasp the fundamentals
    of this powerful system so you can understand the programming exercises better.
    Overall, the chapter included the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过理论解释和一个具体示例介绍了递归神经网络模型。目的是掌握这个强大系统的基础知识，以便更好地理解编程练习。总体来说，本章包括了以下内容：
- en: A brief introduction to RNNs
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN简要介绍
- en: The difference between RNNs and other popular models
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN与其他流行模型的区别
- en: Illustrating the use of RNNs through an example
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一个示例说明RNN的使用
- en: The main problems with a standard RNN
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准RNN的主要问题
- en: In the next chapter, we will go over our first practical exercise using recurrent
    neural networks. You will get to know the popular TensorFlow library, which makes
    it easy to build machine learning models. The next section will give you a nice
    first hands-on experience and prepare you for solving more difficult problems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过第一个实际的递归神经网络练习进行讲解。你将了解流行的TensorFlow库，这使得构建机器学习模型变得容易。接下来的部分将为你提供一个很好的第一次实践经验，并为解决更复杂的问题做好准备。
- en: External links
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部链接
- en: Andrew Ng's deep learning course: [https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew Ng的深度学习课程：[https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
- en: Hidden Markov model: [https://www.youtube.com/watch?v=TPRoLreU9lA](https://www.youtube.com/watch?v=TPRoLreU9lA)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型：[https://www.youtube.com/watch?v=TPRoLreU9lA](https://www.youtube.com/watch?v=TPRoLreU9lA)
- en: '*Introduction to Hidden Markov Model*s by Degirmenci: [https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf](https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐马尔可夫模型简介* 由Degirmenci撰写：[https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf](https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf)'
- en: '*Issues and Limitations of HMM in Speech Processing: A Survey: *[https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf](https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音处理中的HMM问题与局限性：一项调查：* [https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf](https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf)'
- en: Words embeddings: [https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/ ](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)and [https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词向量：[https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)
    和 [https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)
- en: '*Understanding activation functions: *[https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks](https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解激活函数：* [https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks](https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks)'
- en: '*Improving the way neural networks work* by Michael Nielson: [http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*改善神经网络工作方式* 由Michael Nielson撰写：[http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function)'
- en: Lecture 8 from the course, *Natural Language Processing with Deep Learning* by
    Stanford University: [https://www.youtube.com/watch?v=Keqep_PKrY8](https://www.youtube.com/watch?v=Keqep_PKrY8)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福大学课程《*深度学习自然语言处理*》的第8讲：[https://www.youtube.com/watch?v=Keqep_PKrY8](https://www.youtube.com/watch?v=Keqep_PKrY8)
- en: O*n the difficulty of training recurrent neural networks*: [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练递归神经网络的难度*：[http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)'
