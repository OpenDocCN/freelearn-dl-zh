- en: Machine Learning and Deep Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和深度学习模型
- en: In almost all of the applications that we have been discussing up to now, the
    implicit assumption has been that you are creating a new machine learning NLP
    pipeline. Now, that may not always be the case. If you are already working on
    an established platform, fastText may also be a good addition to make the pipeline
    better.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到目前为止讨论的几乎所有应用中，都隐含着一个假设，那就是你正在创建一个新的机器学习 NLP 流水线。现在，情况不一定总是如此。如果你已经在一个成熟的平台上工作，fastText
    可能也是一个很好的补充，可以使流水线更强大。
- en: This chapter will give you some of the methods and recipes for implementing
    fastText using popular frameworks such as scikit-learn, Keras, TensorFlow, and
    PyTorch. We will look at how we can augment the power of word embeddings in fastText,
    using other deep neural architectures such as **convolutional neural networks**
    (**CNN**) or attention networks to solve various NLP problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为你提供一些使用流行框架（如 scikit-learn、Keras、TensorFlow 和 PyTorch）实现 fastText 的方法和示例。我们将讨论如何利用其他深度神经网络架构，如**卷积神经网络**（**CNN**）或注意力网络，来增强
    fastText 词嵌入的能力，解决各种 NLP 问题。
- en: 'The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Scikit-learn and fastText
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn 和 fastText
- en: Embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入层
- en: Keras
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras
- en: Embeddings layer in Keras
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 中的嵌入层
- en: Convolutional neural network architectures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络架构
- en: TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: PyTorch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Torchtext
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torchtext
- en: Scikit-learn and fastText
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-learn 和 fastText
- en: In this section, we will be talking about how to integrate fastText into your
    statistical models. The most common and popular library for statistical machine
    learning is scikit-learn, so we will focus on that.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何将 fastText 集成到统计模型中。最常用和流行的统计机器学习库是 scikit-learn，因此我们将重点关注它。
- en: 'scikit-learn is one of the most popular machine learning tools and the reason
    is that the API is very simple and uniform. The flow is like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 是最流行的机器学习工具之一，原因是其 API 非常简单且统一。流程如下：
- en: You basically convert your data into matrix format.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你基本上是将数据转换为矩阵格式。
- en: Then, you create an instance of the predictor class.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你创建预测类的实例。
- en: Using the instance, you run the `fit` method on the data.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用实例，你在数据上运行`fit`方法。
- en: Once the model is created, you can run `predict` on it.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，你可以在其上运行 `predict`。
- en: This means that you can create a custom classifier by defining the fit and predict
    methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你可以通过定义 `fit` 和 `predict` 方法来创建一个自定义分类器。
- en: Custom classifiers for fastText
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fastText 的自定义分类器
- en: 'Since we are interested in combining fastText word vectors with the linear
    classifiers, you cannot pass the whole vectors and would need a way to define
    a single vector. In this case, let''s go with the mean:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有兴趣将 fastText 词向量与线性分类器结合使用，你不能直接传递整个向量，而是需要一种方法来定义一个单一的向量。在这种情况下，我们选择使用均值：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, you will need to pass the token dictionary to the model, which can be
    built from the fastText library:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要将令牌字典传递给模型，该字典可以通过 fastText 库构建：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Bringing the whole thing together
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将整个过程整合在一起
- en: 'You can use scikit-learn''s `Pipeline` to combine the whole pipeline, demonstrated
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 scikit-learn 的 `Pipeline` 来将整个流程组合在一起，如下所示：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The whole code is shown in the statistical machine learning notebook. For further
    ways of building a better model is if you can find better ways of reducing the
    word vectors, TF-IDF is shown in the shared notebook. Another way of reducing the
    word vectors is looking at the hashing transformer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码显示在统计机器学习笔记本中。为了进一步提高模型的效果，如果你能找到更好的方法来减少词向量，TF-IDF 已在共享笔记本中展示。另一种减少词向量的方法是使用哈希转换器。
- en: In the next sections, we will take a look at how to embed fastText vectors in
    deep learning models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探讨如何在深度学习模型中嵌入 fastText 向量。
- en: Embeddings
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入层
- en: As you have seen, when you need to work with text in machine learning, you need
    to convert the text into numerical values. The logic is the same in neural architectures
    as well. In neural networks, you implement this using the embeddings layer. All
    modern deep learning libraries provide an embeddings API for use.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，当你需要在机器学习中处理文本时，你需要将文本转换为数值。神经网络架构中也有相同的逻辑。在神经网络中，你通过实现嵌入层来做到这一点。所有现代深度学习库都提供了嵌入
    API 供使用。
- en: 'The embeddings layer is a useful and versatile layer used for various purposes:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层是一个有用且多功能的层，用于各种目的：
- en: It can be used to learn word embeddings to be used in an application later
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用于学习词嵌入，以便稍后在应用中使用
- en: It can be used with a larger model where the embeddings are also tuned as part
    of the model
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与更大的模型一起使用，在该模型中，嵌入也会作为模型的一部分进行调优
- en: It can be used to load a pretrained word embedding
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用来加载预训练的词嵌入
- en: It is in the third point that will be the focus of this section. The idea is
    to utilize fastText to create superior embeddings, which can then be injected
    into your model using this embedding layer. Normally the embeddings layer is initialized
    with random weights, but in this case we will be injecting it with the word embeddings
    from our fastText model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的重点是第三点。我们的想法是利用fastText创建更优的词嵌入，然后通过这个嵌入层将其注入到你的模型中。通常，嵌入层会使用随机权重进行初始化，但在这种情况下，我们将通过fastText模型的词嵌入来初始化它。
- en: Keras
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: Keras is a widely popular high-level neural network API. It supports TensorFlow,
    CNTK, and Theano as the backend. Due to the user-friendly API of Keras, many people
    use it in lieu of the base libraries.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个广泛流行的高级神经网络API。它支持TensorFlow、CNTK和Theano作为后端。由于Keras具有用户友好的API，许多人使用它代替基础库。
- en: Embedding layer in Keras
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的嵌入层
- en: 'The embedding layer will be the first hidden layer of the Keras network and
    you will need to specify three arguments: input dimension, output dimension, and
    input length. Since we will be using fastText to make our model better, we will
    also need to pass the weights parameter with the embedding matrix and make the
    trainable matrix to be false:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层将是Keras网络的第一个隐藏层，你需要指定三个参数：输入维度、输出维度和输入长度。由于我们将使用fastText来改善模型，因此还需要传递带有嵌入矩阵的权重参数，并将训练矩阵设置为不可训练：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Another thing that we need to take care of is that we need to map words to integers
    and integers to words. In Keras, you do this using the `Tokenizer` class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们需要注意的是，我们需要将词映射到整数，并将整数映射回词。在Keras中，你可以使用`Tokenizer`类来实现这一点。
- en: Let's look at this in action as part of a CNN.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在卷积神经网络（CNN）中看看这一过程的实际应用。
- en: Convolutional neural networks
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: When we talk in terms of mixing word embeddings and neural networks, convolutional
    networks are something that have yielded good results. CNNs are created by applying
    several layers of convolutions with nonlinear activation functions such as ReLu
    or *tanh* applied to the results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论将词嵌入和神经网络结合时，卷积神经网络是取得良好成果的一种方法。CNN是通过在结果上应用多个卷积层和非线性激活函数（如ReLu或*tanh*）创建的。
- en: 'Lets talk a little bit about what a convolution means. A convolution of any
    function with respect to another function is the integral that expresses the amount
    of overlap of one function as it is passed over the other function. You can think
    of this as blending one function into another. In signal theory, this is how experts
    understand convolutions: the output signal is a convolution of the input signal
    with the impulse response of the environment. The impulse response of any environment
    essentially identifies and distinguishes the environment.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微谈一下什么是卷积。一个函数与另一个函数的卷积是一个积分，表示一个函数在经过另一个函数时的重叠量。你可以把它理解为将一个函数融合到另一个函数中。在信号理论中，专家是这样理解卷积的：输出信号是输入信号与环境冲击响应的卷积。任何环境的冲击响应本质上识别并区分该环境。
- en: In a traditional feedforward network, we connect each input neuron to each output
    neuron in the next layer. In CNNs, we instead use convolutions on the input to
    compute the output. During the training phase, a CNN will automatically learn
    the values of the filters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的前馈神经网络中，我们将每个输入神经元连接到下一层中的每个输出神经元。而在CNN中，我们则使用卷积来计算输出。在训练阶段，CNN会自动学习过滤器的值。
- en: 'CNNs are generally used with word embeddings, and it is here that fastText
    comes into the picture and has the potential to bring huge gains in terms of classification
    accuracy by providing better word representations. The architecture is thus composed
    of three key sections:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常与词嵌入一起使用，在这里，fastText就发挥了作用，它能够通过提供更好的词表示来在分类精度上带来巨大的提升。因此，架构由三个关键部分组成：
- en: '![](img/00088.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.jpeg)'
- en: If you have these three pieces of the architecture already in place in your
    classification pipeline, you can identify the word embeddings part and see whether
    changing that to fasttext gives you an improvement in terms of the predictions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在分类管道中已经有了这三个架构组件，你可以识别出词嵌入部分，看看是否将其更换为fastText能改善预测效果。
- en: In this example, we will be taking a look at the previous example of Yelp reviews
    and trying to classify them using a convolution neural network. For the sake of
    brevity, we will fit a pretrained dataset that has been released, but you will
    probably be working on a domain-specific use case and hence you should be integrating
    the creation of the model, as shown in the previous chapter. As you have seen,
    you can use fastText library or the Gensim library.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将回顾之前的Yelp评论示例，并尝试使用卷积神经网络对其进行分类。为了简便起见，我们将使用一个已经发布的预训练数据集，但你可能会处理一个特定领域的用例，因此你应该按照前一章中的示范，整合模型的创建。如你所见，你可以使用fastText库或Gensim库。
- en: 'From a high level, the steps are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，步骤如下：
- en: Text samples in the dataset are converted into sequences of word indices. A
    *word index* would simply be an integer ID for the word. We will only consider
    the top 20,000 most common words in the dataset, and we will truncate the sequences
    to a maximum of 1,000 words. This is done for ease of computation. You can play
    around with this approach and find the approach that brings the greatest generalization.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中的文本样本会被转换为单词索引序列。*单词索引* 仅仅是一个整数ID，表示某个单词。我们将只考虑数据集中最常见的20,000个单词，并将序列截断至最多1,000个单词。这么做是为了计算的方便。你可以尝试这种方法，并找到最能带来通用性的做法。
- en: Prepare an embedding matrix, which will contain at index `i` the embedding vector
    for the word `i` in the word index.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个嵌入矩阵，它将在索引`i`处包含单词索引中单词`i`的嵌入向量。
- en: The embedding matrix is then loaded to the Keras embedding layer, and will set
    the layer to be frozen so that it is not updated while training.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将嵌入矩阵加载到Keras嵌入层，并将该层设置为冻结状态，以确保在训练过程中不会被更新。
- en: The layers that come after it will be the convolution networks. At the end,
    there will be a softmax layer to converge the output to our five categories.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 紧接其后的层将是卷积网络。最后将有一个softmax层，将输出收敛到我们的五个类别。
- en: 'In this case, we can use the pandas library to create the list of input text
    and the list of the output labels:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以使用pandas库来创建输入文本的列表和输出标签的列表：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we will need to format our text samples and labels into tensors that can
    be fed into the neural network. This is the place where we will be using the `Tokenizer`
    class. We will also need to pad the sequences as we need matrices of equal lengths:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将文本样本和标签格式化为可以输入神经网络的张量。在这一部分，我们将使用`Tokenizer`类。我们还需要对序列进行填充，以保证所有矩阵的长度相等：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we will be using our fastText embeddings. In this case, we are using pretrained
    embeddings, but you can train your own embeddings on the fly during the training
    process. You have the choice of loading from the `.vec` file, but since this is
    fastText, we will load from the BIN file. The advantage of using the BIN file
    is that the out of vocabulary case will be avoided to a large extent. We will
    use the fastText model and generate an embedding matrix:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用我们的fastText嵌入。在这个案例中，我们使用的是预训练嵌入，但你也可以在训练过程中动态地训练自己的嵌入。你可以选择从`.vec`文件加载，但由于这是fastText，我们将从BIN文件加载。使用BIN文件的优势在于，它可以在很大程度上避免词汇表外的情况。我们将使用fastText模型并生成一个嵌入矩阵：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We load this to the embedding layer. It is important to note that the trainable
    parameter should be set to `False` to prevent the weights from being updated,
    as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其加载到嵌入层。需要注意的是，`trainable`参数应该设置为`False`，以防止在训练过程中更新权重，如下所示：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can build a 1D ConvNet to apply to our Yelp classification problem:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建一个1D卷积网络来应用于我们的Yelp分类问题：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The summary of this model is shown as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的摘要如下所示：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, you can try out some other hyperparameters and try to improve the accuracy
    from here.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以尝试一些其他的超参数，看看能否提高准确度。
- en: In this section, you saw how to use the fastText word embeddings as part of
    a larger CNN Keras classifier. Using a similar approach, you can use fastText
    embeddings with whichever neural architectures benefit from word embeddings in
    Keras.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解了如何将fastText词嵌入作为更大CNN Keras分类器的一部分。使用类似的方法，你可以在Keras中使用fastText嵌入与任何受益于词嵌入的神经网络架构。
- en: TensorFlow
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow is a computation library developed by Google. It is quite popular
    now and is used by many companies to create their neural network models. After
    what you have seen in Keras, the logic behind augmenting TensorFlow models using
    fastText is the same.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是由Google开发的一个计算库。它现在非常流行，许多公司使用它来创建他们的神经网络模型。在你在Keras中看到的内容之后，使用fastText增强TensorFlow模型的逻辑是相同的。
- en: Word embeddings in TensorFlow
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的词嵌入
- en: 'To create word embeddings in TensorFlow, you will need to create an embeddings
    matrix where all the tokens in your list of documents have unique IDs, and so
    each document is a vector of these IDs. Now, let''s say you have an embedding
    in a NumPy array called `word_embedding`, with `vocab_size` rows and `embedding_dim`
    columns, and you want to create a tensor `W`. Taking a specific example, the sentence
    "I have a cat." can be split up into ["I", "have", "a", "cat", "."], and the tensor
    of the corresponding `word_ids` will be of shape 5\. To map these word IDs into
    vectors, create the word embedding variable and use the `tf.nn.embedding_lookup`
    function:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要在TensorFlow中创建词嵌入，你需要创建一个嵌入矩阵，其中所有文档列表中的标记都有唯一的ID，因此每个文档都是这些ID的向量。现在，假设你有一个NumPy数组叫做`word_embedding`，它有`vocab_size`行和`embedding_dim`列，并且你想要创建一个张量`W`。以一个具体的例子，“I
    have a cat.”可以分解为["I", "have", "a", "cat", "." ]，对应的`word_ids`张量的形状将是5。为了将这些单词ID映射到向量中，创建词嵌入变量并使用`tf.nn.embedding_lookup`函数：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After this, the `embedded_word_ids` tensor will have `shape [5, embedding_size]`
    in our example and contain the embeddings (dense vectors) for each of the five
    words.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，`embedded_word_ids`张量将在我们的示例中具有`shape [5, embedding_size]`，并包含五个单词的嵌入（密集向量）。
- en: 'To be able to use a word embedding with pretrained vectors, create `W` as a
    `tf.Variable` and initialize it from the NumPy array via a `tf.placeholder()`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用带有预训练向量的词嵌入，创建`W`作为`tf.Variable`，并通过`tf.placeholder()`从NumPy数组初始化它：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can then pass the actual embeddings in the TensorFlow session:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以在TensorFlow会话中传递实际的嵌入：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will avoid storing a copy of the embeddings in the graph, but it does require
    enough memory to keep two copies of the matrix at once (one for the NumPy array
    and one for the `tf.Variable`). You would want to have word embeddings constant
    during the training process, so as discussed earlier, the trainable parameter
    for the embeddings needs to be `False`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以避免在图中存储嵌入的副本，但它确实需要足够的内存来同时保留矩阵的两个副本（一个用于NumPy数组，一个用于`tf.Variable`）。你希望在训练过程中保持词嵌入不变，因此如前所述，词嵌入的可训练参数需要设置为`False`。
- en: RNN architectures
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN架构
- en: 'NLP has always been considered to be an excellent use case for LSTMs and RNN-type
    neural architectures. LSTMs and RNNs use sequential processing. NLP has always
    been considered one of the biggest use cases because the meaning of any sentence
    is context-based. The meaning of a word can be considered to have meaning based
    on all the words that came before it:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）一直被认为是LSTM和RNN类型神经网络架构的一个绝佳应用案例。LSTM和RNN使用顺序处理。NLP一直被认为是最大应用案例之一，因为任何句子的意义都是基于上下文的。一个单词的意义可以被认为是基于它之前的所有单词来确定的：
- en: '![](img/00089.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpeg)'
- en: Now, when you are running an LSTM network, you need to convert the words into
    an embedding layer. Generally in such cases, a random initializer is used. But,
    you probably should be able to increase the performance of the model using a fastText
    model. Let's take a look at how a fastText model is used in such cases.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你运行LSTM网络时，你需要将单词转换成嵌入层。通常在这种情况下，会使用随机初始化器。但你可能应该能够通过使用fastText模型来提高模型的性能。让我们来看看在这种情况下如何使用fastText模型。
- en: 'In this example, the crawl vectors that are released by Facebook are loaded
    into memory. In your use case, you should probably train a fastText model on your
    text corpus and load that model. We are creating the embedding using the VEC file
    here, but you can chose to load from a `.bin` file as well, as shown in the Keras
    example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，由Facebook发布的爬取向量被加载到内存中。在你的使用案例中，你可能需要在你的文本语料库上训练一个fastText模型并加载该模型。我们在这里使用VEC文件来创建嵌入，但你也可以选择从`.bin`文件加载，如Keras示例中所示：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Call the block of text that is your target of interest, and then run the normal
    cleaning step. Similar to the example in Keras, you will next need a mechanism
    that maps the tokens to a unique integer and a way to get the token back from
    the integer. So, we will need to create a dictionary and a reverse dictionary
    with the words:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 调用你感兴趣的文本块，然后执行正常的清理步骤。类似于 Keras 中的示例，接下来你将需要一个机制，将标记映射到唯一的整数，并能通过整数获取回标记。因此，我们需要创建一个字典和一个反向字典来存储单词：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create the embedding array from the dictionary that we created using the
    fastText model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 fastText 模型创建的字典来创建嵌入数组：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, we set up the RNN model. We will be reading three words at a time and
    hence our `x` is a matrix with an undetermined number of rows and three columns
    wide. Another input of note is the `embedding_placeholder` has one row per word
    and has a width of 300 dimensions, corresponding to the number of dimensions for
    the input word vector.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置 RNN 模型。我们将每次读取三个单词，因此我们的 `x` 是一个行数不确定、宽度为三列的矩阵。另一个需要注意的输入是 `embedding_placeholder`，它每个单词有一行，宽度为
    300 维，对应输入词向量的维度数。
- en: 'Then, the TensorFlow `tf.nn.embedding_lookup()` function can be used to look
    up each of our inputs from `x` in matrix `W`, resulting in the 3D tensor `embedded_chars`.
    This can then be fed into the RNN:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，TensorFlow 的 `tf.nn.embedding_lookup()` 函数可以用于从矩阵 `W` 中查找每个来自 `x` 的输入，从而生成
    3D 张量 `embedded_chars`。接着，可以将其输入到 RNN 中：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have the RNN, we need to figure out how to train it and what kind
    of cost function can be used. FastText internally uses the softmax function. The
    softmax function may not be suitable as a cost function in this case because,
    by definition, softmax normalizes the vectors before comparing. Thus, the actual
    vector may grow or shrink in an arbitrary manner. There may be a good case for
    having the final vectors with the same magnitude as the vectors in the training
    set, and thus with the same magnitudes as the pretrained vectors. In this example,
    the focus is on L2 loss:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了 RNN，接下来需要弄清楚如何训练它以及可以使用什么样的代价函数。FastText 内部使用 softmax 函数。softmax 函数在这里可能不适合作为代价函数，因为按照定义，softmax
    会在比较之前对向量进行归一化。因此，实际的向量可能会以任意的方式变大或变小。也许有必要让最终的向量与训练集中的向量具有相同的大小，从而与预训练向量的大小相同。在这个例子中，重点是
    L2 损失：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: PyTorch
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch
- en: Following the same logic as the previous two libraries, you can use the `torch.nn.EmbeddingBag`
    class to inject the pretrained embeddings. There is a small drawback though. Keras
    and TensorFlow make the assumption that your tensors are actually implemented
    as NumPy arrays, while in the case of PyTorch, that's not the case. PyTorch implements
    the torch tensor. Generally, this is not an issue, but this means that you will
    need to write your own text conversion and tokenizing pipelines. To circumvent
    all this rewriting and reinvention of the wheel, you can use the torchtext library.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按照与前两个库相同的逻辑，你可以使用 `torch.nn.EmbeddingBag` 类来注入预训练的嵌入。虽然有一个小缺点。Keras 和 TensorFlow
    假设你的张量实际上是作为 NumPy 数组实现的，而在 PyTorch 中并非如此。PyTorch 实现了 torch 张量。通常这不是问题，但这意味着你需要编写自己的文本转换和分词管道。为了避免重写和重新发明轮子，你可以使用
    torchtext 库。
- en: The torchtext library
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext 库
- en: The torchtext is an excellent library that takes care of most of the preprocessing
    steps that you need to build your NLP model. Basically, think of torchtext as
    something that acts like *configuration as code* in a loose sense of the term.
    So, it makes sense to understand the torchtext data paradigm, which takes around
    three hours, instead of writing custom code, which will probably seem easier but
    would involve countless hours of confusion and debugging. And to top it off, it
    can build prebuilt models, including fastText.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: torchtext 是一个出色的库，能够处理你构建 NLP 模型所需的大部分预处理步骤。基本上，可以将 torchtext 看作是以一种松散的方式充当
    *配置即代码* 的工具。因此，理解 torchtext 数据范式是有意义的，学习大约需要三小时，而不是编写定制代码，虽然看起来可能更简单，但会涉及无数的困惑和调试。而且，torchtext
    还能构建预构建的模型，包括 fastText。
- en: Now, let's take a look at how that is done.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看是如何实现的。
- en: Data classes in torchtext
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext 中的数据类
- en: 'We will first call all the required libraries. Take note that you are calling
    data that contains the required data classes for our use:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先调用所有必要的库。请注意，你正在调用包含我们使用所需数据类的数据：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will use `spacy` for the tokenization step, for which torchtext has excellent
    support. torchtext provides excellent support for calling and loading fastText
    libraries:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `spacy` 进行分词步骤，torchtext 在这方面有很好的支持。torchtext 提供了对调用和加载 fastText 库的优异支持：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This will download the `wiki.simple.bin` model. If you provide the name `en`,
    it will download and load `wiki.en.bin`. If you load `fr`, then it will load `wiki.fr.bin`,
    and so on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载 `wiki.simple.bin` 模型。如果你提供名称 `en`，它将下载并加载 `wiki.en.bin`。如果加载 `fr`，它将加载
    `wiki.fr.bin`，以此类推。
- en: 'You will probably be loading the data from a CSV or from a text file. In that
    case, you will need to open the file, possibly in pandas, extract the relevant
    fields, and then save them in a separate file. torchtext is not able to distinguish
    between training and validation, and hence you will probably need to separate
    out those files as well:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会从 CSV 文件或文本文件中加载数据。在这种情况下，你需要打开文件，可能在 pandas 中提取相关字段，然后将其保存到单独的文件中。torchtext
    无法区分训练集和验证集，因此你可能还需要将这些文件分开：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You will now need to define the data and build the vocabulary. You can do this
    using the data module. This module has the data classes to define the pipelining
    steps and run the batching, padding, and numericalization. First, you will need
    to define the type of fields using `data.Fields`. This class defines the common
    datatypes that can be used to create the required tensor. You can also define
    some common instructions to define how the tensor should be created. Once the
    fields are created, you can call `TabularDataset` to create the dataset using
    the instructions defined in the field. The common instructions are passed as parameters:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要定义数据并构建词汇表。你可以使用数据模块来实现这一点。该模块有数据类来定义管道步骤并运行批处理、填充和数字化。首先，你需要使用 `data.Fields`
    定义字段类型。此类定义了可以用来创建所需张量的常见数据类型。你还可以定义一些常见指令来定义张量应如何创建。一旦字段创建完成，你可以调用 `TabularDataset`
    来使用字段中定义的指令创建数据集。常见的指令作为参数传递：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`sequential=True` means that the column has sequences. We probably want that
    to be the case for labels as the example is pretty much a comparison, but in cases
    where that is not the case, set this to false.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequential=True` 表示该列包含序列。对于标签，我们可能希望保持这种设置，因为示例基本上是一个比较，但在不是这种情况的情况下，可以将其设置为
    false。'
- en: We are specifying the tokenizer to be spacy in this case, but you can specify
    custom functions.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在此指定使用 spacy 作为分词器，但你可以指定自定义函数。
- en: '`fix_length` pads or trims all sequences to fixed lengths of 150 in this case.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fix_length` 将所有序列填充或裁剪到固定长度，这里是 150。'
- en: '`lower` specifies we are setting all English letters to lowercase.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower` 指定我们将所有英文字母设置为小写。'
- en: 'Once the datasets are created, you will need to create the vocabulary so that
    we can convert the tokens into integer numbers later. It is here that we are going
    to build the vocabulary from the fastText vectors that we loaded earlier:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集创建完成，你需要创建词汇表，以便稍后将标记转换为整数。在这里，我们将从之前加载的 fastText 向量构建词汇表：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Using the iterators
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迭代器
- en: 'You can now use an iterator to iterate over the dataset. In this case, we are
    using `BucketIterator`, which has the additional advantage that it batches examples
    of similar lengths together. This reduces the amount of padding needed:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用迭代器来遍历数据集。在这种情况下，我们使用 `BucketIterator`，它的额外优势是将相似长度的示例聚集在一起批处理。这减少了所需的填充量：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So, you will be able to run simple `for` loops on these iterators and they will
    provide inputs based on batches.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你将能够在这些迭代器上运行简单的 `for` 循环，并根据批次提供输入。
- en: Bringing it all together
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有步骤结合起来
- en: 'Finally, once all these steps are done, you can initialize your PyTorch model,
    and you will need to set the pretrained vectors as the weights of the model. In
    the example, an RNN model was created and the word vectors were initialized from
    the earlier field vectors. This will take care of handling the `lookup_table`
    in PyTorch:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦完成所有这些步骤，你可以初始化你的 PyTorch 模型，并需要将预训练的向量作为模型的权重。在示例中，创建了一个 RNN 模型，并且词向量是从之前的字段向量初始化的。这将处理
    PyTorch 中的 `lookup_table`：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The code shown here includes only the things that should be new to you. For
    the full code, take a look at the `pytorch torchtext rnn.ipynb` notebook in the
    repository.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的代码仅包括你可能不熟悉的部分。完整的代码可以查看仓库中的 `pytorch torchtext rnn.ipynb` 笔记本。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a look at how to integrate fastText word vectors into
    either linear machine learning models or deep learning models created in Keras,
    TensorFlow, and PyTorch. You also saw how word vectors can be easily assimilated
    into existing neural architectures that you might be using in your business application.
    If you are initializing the embeddings from random values, I would highly recommend
    that you try to initialize them using fastText values, and then see whether there
    are performance improvements in your model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何将 fastText 词向量集成到线性机器学习模型或在 Keras、TensorFlow 和 PyTorch 中创建的深度学习模型中。你还看到如何将词向量轻松地融入到你可能在业务应用中使用的现有神经网络架构中。如果你是通过随机值初始化嵌入，我强烈建议你尝试使用
    fastText 值进行初始化，然后观察模型的性能是否有所提升。
