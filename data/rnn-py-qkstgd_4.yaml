- en: Creating a Spanish-to-English Translator
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个西班牙语到英语的翻译器
- en: This chapter will push your neural network knowledge even further by introducing
    state-of-the-art concepts at the core of today's most powerful language translation
    systems. You will build a simple version of a Spanish-to-English translator, which
    accepts a sentence in Spanish and outputs its English equivalent.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过引入当今最强大语言翻译系统核心的最前沿概念，进一步推动你的神经网络知识。你将构建一个简单版本的西班牙语到英语的翻译器，该翻译器接受西班牙语句子并输出其英文等效句子。
- en: 'This chapter includes the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括以下部分：
- en: '**Understanding the translation model**: This section is entirely focused on
    the theory behind this system.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解翻译模型**：本节完全专注于该系统背后的理论。'
- en: '**What an LSTM network is**: We''ll be understanding what sits behind this
    advanced version of recurrent neural networks.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**什么是LSTM网络**：我们将了解这一先进版本的循环神经网络背后是什么。'
- en: '**Understanding sequence-to-sequence network with attention**: You will grasp
    the theory behind this powerful model, get to know what it actually does, and
    why it is so widely used for different problems.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解带注意力机制的序列到序列网络**：你将掌握这个强大模型背后的理论，了解它实际做了什么，以及为什么它在不同问题中被广泛使用。'
- en: '**Building the Spanish-to-English translator**: This section is entirely focused
    on implementing the knowledge acquired up to this point in a working program.
    It includes the following:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建西班牙语到英语的翻译器**：本节完全专注于将到目前为止获得的知识实现为一个可运行的程序。它包括以下内容：'
- en: Training the model
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Predicting English translations
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测英文翻译
- en: Evaluating the accuracy of the model
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型的准确性
- en: Understanding the translation model
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解翻译模型
- en: Machine translation is often done using so-called **s****tatistical machine
    translation**, based on statistical models. This approach works very well, but
    a key issue is that, for every pair of languages, we need to rebuild the architecture.
    Thankfully, in 2014, Cho *et al.* ([https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf))came
    out with a paper that aims to solve this, and other problems, using the increasingly
    popular recurrent neural networks. The model is called sequence-to-sequence, and
    has the ability to be trained on any pair of languages by just providing the right
    amount of data. In addition,its power lies in its ability to match sequences of
    different lengths, such as in machine translation, where a sentence in English
    may have a different size when compared to a sentence in Spanish. Let's examine
    how these tasks are achieved.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译通常采用所谓的**统计机器翻译**，基于统计模型。这种方法效果很好，但一个关键问题是，对于每一对语言，我们都需要重建架构。幸运的是，在2014年，Cho
    *等人*（[https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)）发布了一篇论文，旨在通过日益流行的循环神经网络来解决这个问题及其他问题。该模型被称为序列到序列（sequence-to-sequence），通过提供足够的数据，可以在任意语言对上进行训练。此外，它的强大之处在于能够匹配不同长度的序列，例如在机器翻译中，英文句子和西班牙语句子的长度可能不同。我们来看看这些任务是如何完成的。
- en: 'First, we will introduce the following diagram and explain what it consists
    of:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍以下图表并解释它的组成：
- en: '![](img/981ef83f-adec-4c07-9927-b41712890193.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/981ef83f-adec-4c07-9927-b41712890193.png)'
- en: 'The architecture has three major parts: the **encoder** RNN network (on the
    left side), the intermediate state (marked by the middle arrow), and the **decoder**
    RNN network (on the right side). The flow of actions for translating the sentence
    **Como te llamas?** (Spanish) into **What is your name?** (English) is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构有三个主要部分：**编码器**RNN网络（左侧）、中间状态（由中间箭头标记）和**解码器**RNN网络（右侧）。将西班牙语句子**Como te
    llamas?**（西班牙语）翻译为**What is your name?**（英语）的操作流程如下：
- en: Encode the Spanish sentence, using the encoder RNN, into the intermediate state
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编码器RNN将西班牙语句子编码为中间状态
- en: Using that state and the decoder RNN, generate the output sentence in English
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用该状态和解码器RNN，生成英文的输出句子
- en: This simple approach works with short and simple sentences, but, in practice,
    the true use of translation models lies in longer and more complicated sequences.
    That is why we are going to extend our basic approach using the powerful LSTM
    network and an attention mechanism. Let's explore these techniques in the following
    sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单方法适用于短小且简单的句子，但实际上，翻译模型的真正应用在于更长且更复杂的序列。这就是为什么我们将使用强大的LSTM网络和注意力机制来扩展我们的基本方法。接下来让我们在各节中探讨这些技术。
- en: What is an LSTM network?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是LSTM网络？
- en: '**LSTM** (**long short-term memory**) network is an advanced RNN network that
    aims to solve the vanishing gradient problem and yield excellent results on longer
    sequences. In the previous chapter, we introduced the GRU network, which is a
    simpler version of LSTM. Both include memory states that determine what information
    should be propagated further at each timestep. The LSTM cell looks as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**（**长短期记忆**）网络是一种先进的 RNN 网络，旨在解决梯度消失问题，并在长序列上取得优异的结果。在前一章中，我们介绍了 GRU
    网络，它是 LSTM 的简化版本。两者都包括记忆状态，用于决定每个时间步应该传播哪些信息。LSTM 单元如下所示：'
- en: '![](img/2f02d3d2-2e2a-4177-9787-45d5e99c23ba.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f02d3d2-2e2a-4177-9787-45d5e99c23ba.png)'
- en: 'Let''s introduce the main equations that will clarify the preceding diagram.
    They are similar to the ones for gated recurrent units (see Chapter 3, *Generating
    Your Own Book Chapter*). Here is what happens at every given timestep, *t*:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍主要的方程式，这些方程式将澄清前面的图示。它们与门控递归单元（请参见第3章，*生成你自己的书章节*）的方程式相似。以下是每个给定时间步 *t*
    发生的事情：
- en: '![](img/eeb8f477-3a8f-418c-a545-825e7c96e300.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeb8f477-3a8f-418c-a545-825e7c96e300.png)'
- en: '![](img/f4ce6068-3740-40a2-9081-bc7f0746839f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4ce6068-3740-40a2-9081-bc7f0746839f.png)'
- en: '![](img/1047d7e8-d38d-4fbb-8591-b8be9e8cb6eb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1047d7e8-d38d-4fbb-8591-b8be9e8cb6eb.png)'
- en: '![](img/c39b96fc-5442-468d-93b6-33812680f2d9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c39b96fc-5442-468d-93b6-33812680f2d9.png)'
- en: '![](img/fb3c19d6-f261-4bbe-8f5d-44ed6b423c95.png) is the **output gate**, which determines
    what exactly is important for the current prediction and what information should
    be kept around for the future. ![](img/71bed47a-6af5-4de0-b6d6-cb66d91d5f71.png) is
    called the **input gate**,and determines how much we concern ourselves about the
    current vector (cell). ![](img/47e80fed-2cd6-4481-8026-85b010ad2923.png) is the
    value for the new memory cell. ![](img/620e6cb2-41f7-47e9-a74d-8310ddbe0434.png) is
    the **forget gate**, which determines how much to forget from the current vector
    (if the forget gate is 0, we are entirely forgetting the past). All four, ![](img/5a99b695-5381-4fe9-a35b-6b8a13e62382.png), have
    the same equation insight (with its corresponding weights), but ![](img/8ca58d2c-6eb8-4453-a095-c854fd500779.png) uses
    tanh and the others use sigmoid.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/fb3c19d6-f261-4bbe-8f5d-44ed6b423c95.png) 是 **输出门**，它决定了当前预测中哪些信息是重要的，以及哪些信息应该保留以便未来使用。
    ![](img/71bed47a-6af5-4de0-b6d6-cb66d91d5f71.png) 被称为 **输入门**，它决定了我们在当前向量（单元）上应该关注多少。
    ![](img/47e80fed-2cd6-4481-8026-85b010ad2923.png) 是新记忆单元的值。 ![](img/620e6cb2-41f7-47e9-a74d-8310ddbe0434.png)
    是 **遗忘门**，它决定了应该忘记当前向量中的多少信息（如果遗忘门为0，我们就完全忘记过去）。这四个，![](img/5a99b695-5381-4fe9-a35b-6b8a13e62382.png)，有相同的方程洞察（以及相应的权重），但
    ![](img/8ca58d2c-6eb8-4453-a095-c854fd500779.png) 使用的是 tanh，而其他的使用 sigmoid。'
- en: 'Finally, we have the final memory cell ![](img/9746266a-3ec5-4578-a4ab-3bd7afd9212d.png) and
    final hidden state ![](img/6eb5c720-0596-4ce1-b080-53203eb34616.png) :'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了最终的记忆单元 ![](img/9746266a-3ec5-4578-a4ab-3bd7afd9212d.png) 和最终的隐藏状态 ![](img/6eb5c720-0596-4ce1-b080-53203eb34616.png)：
- en: '![](img/b1c7944b-3570-49b1-a2e5-2ee45c1b1044.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1c7944b-3570-49b1-a2e5-2ee45c1b1044.png)'
- en: 'The final memory cell separates the input and forget gate, and decides how
    much of the previous output ![](img/6d088790-6135-49a9-ab92-2791362fee23.png) should
    be kept and how much of the current output ![](img/30fc84a3-1fb1-4717-ab07-fb6220f19c01.png) should
    be propagated forward (in simple terms, this means: *forget the past or not, take
    the input or not*). The *dot* sign is called the Hadamard product—if `x = [1,
    2, 3]` and `y = [4, 5, 6]`, then `x dot y = [1*4, 2*5, 3*6] = [4, 10, 18]`.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的记忆单元将输入门和遗忘门分开，并决定保留多少先前的输出 ![](img/6d088790-6135-49a9-ab92-2791362fee23.png)
    以及多少当前输出 ![](img/30fc84a3-1fb1-4717-ab07-fb6220f19c01.png) 应该向前传播（简单来说，这意味着：*忘记过去还是不忘，是否接受当前输入*）。
    *点乘* 符号叫做哈达玛积——如果 `x = [1, 2, 3]` 且 `y = [4, 5, 6]`，则 `x dot y = [1*4, 2*5, 3*6]
    = [4, 10, 18]`。
- en: 'The final hidden state is defined as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的隐藏状态定义如下：
- en: '![](img/79f2f536-370c-4a5d-86a2-02a4b0e0756c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79f2f536-370c-4a5d-86a2-02a4b0e0756c.png)'
- en: It decides whether to expose the content of the cell at this particular timestep.
    Since some of the information ![](img/5b11a50f-f52d-4ec5-a98c-816ab2cbc644.png) from
    the current cell may be omitted in ![](img/a48ef98c-e704-4c47-84f9-d5fa7d16e68b.png),
    we are passing ![](img/45f013f4-0b4a-4eea-96f8-09c281f006fb.png) forward to be
    used in the next timesteps.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它决定是否暴露此特定时间步的单元内容。由于当前单元的某些信息 ![](img/5b11a50f-f52d-4ec5-a98c-816ab2cbc644.png)
    可能在 ![](img/a48ef98c-e704-4c47-84f9-d5fa7d16e68b.png) 中被省略，我们将 ![](img/45f013f4-0b4a-4eea-96f8-09c281f006fb.png)
    向前传递，以便在下一个时间步使用。
- en: This same system is repeated multiple times through the neural network. Often,
    it is the case that several LSTM cells are stacked together and use shared weights
    and biases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相同的系统在神经网络中被多次重复。通常情况下，多个LSTM单元会堆叠在一起，并使用共享的权重和偏置。
- en: Two great sources for enhancing your knowledge on LSTMs are Colah's article
    *Understanding LSTM Network* ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))and
    the Stanford University lecture on LSTM ([https://www.youtube.com/watch?v=QuELiw8tbx8](https://www.youtube.com/watch?v=QuELiw8tbx8))
    by Richard Socher.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提升你对LSTM理解的两个重要来源是Colah的文章*理解LSTM网络*（[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)）和斯坦福大学的LSTM讲座（[https://www.youtube.com/watch?v=QuELiw8tbx8](https://www.youtube.com/watch?v=QuELiw8tbx8)），讲解者是Richard
    Socher。
- en: Understanding the sequence-to-sequence network with attention
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解带有注意力机制的序列到序列网络
- en: Since you have already understood how the LSTM network works, let's take a step
    back and look at the full network architecture. As we said before, we are using
    a sequence-to-sequence model with an attention mechanism. This model consists
    of LSTM units grouped together, forming the encoder and decoder parts of the network.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经理解了LSTM网络的工作原理，让我们退后一步，看看整个网络架构。正如我们之前所说，我们使用的是带有注意力机制的序列到序列模型。这个模型由LSTM单元组成，分为编码器和解码器部分。
- en: In a simple sequence-to-sequence model, we input a sentence of a given length
    and create a vector that captures all the information in that particular sentence.
    After that, we use the vector to predict the translation. You can read more about
    how this works in a wonderful Google paper ([https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)) in
    the *External links* section at the end of this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简单的序列到序列模型中，我们输入一个给定长度的句子，并创建一个向量来捕获该句子中的所有信息。之后，我们使用该向量来预测翻译。你可以在本章末尾的*外部链接*部分阅读更多关于这一过程的精彩Google论文（[https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)）。
- en: That approach is fine, but, as in every situation, we can and must do better.
    In that case, a better approach would be to use an attention mechanism. This is
    motivated by the way a person does language translation. A person doesn't read
    the input sentence, then hide the text while they try to write down the output
    sentence. They are continuously keeping track of the original sentence while making
    the translation. This is how the attention mechanism works. At every timestep
    of the decoder, the network decides what and how many of the encoder inputs to
    use. To make that decision, special weights are attached to every encoder word.
    In practice, the attention mechanism tries to solve a fundamental problem with
    recurrent neural networks—the ability to remember long-term dependencies.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是可行的，但像在任何情况下，我们都可以并且必须做得更好。在这种情况下，一个更好的方法是使用注意力机制。这种方法受到人类翻译语言方式的启发。人类不会先阅读输入句子，然后在试图写下输出句子时把文本隐藏起来。他们在翻译的过程中持续跟踪原始句子。这就是注意力机制的工作原理。在解码器的每个时间步，网络会决定使用编码器输入中的哪些部分以及多少部分。为了做出这个决策，特定的权重被分配给每个编码器单词。实际上，注意力机制试图解决递归神经网络的一个根本问题——记住长期依赖关系的能力。
- en: 'A good illustration of the attention mechanism can be seen here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的注意力机制示例可以在这里看到：
- en: '![](img/289240d8-82af-410f-85b0-e22669837ed6.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/289240d8-82af-410f-85b0-e22669837ed6.png)'
- en: '![](img/b1d42841-3e87-4dc1-909a-d0310e15516b.png) are the inputs and ![](img/f54afd71-65ef-4d8d-9072-191e1b8cc407.png) are
    the predicted outputs. You can see the attention weights represented as ![](img/a7d1e9fe-65a0-4df6-adb4-851fa68bc28e.png), each
    one attached to its corresponding input. These weights are learned during training
    and they decide the influence of a particular input on the final output. This
    makes every output ![](img/ce6d88ba-ffb0-47cc-9dba-ac31d4c71d8d.png) dependent
    on a weighted combination of all the input states.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/b1d42841-3e87-4dc1-909a-d0310e15516b.png) 是输入，![](img/f54afd71-65ef-4d8d-9072-191e1b8cc407.png)
    是预测的输出。你可以看到表示注意力权重的![](img/a7d1e9fe-65a0-4df6-adb4-851fa68bc28e.png)，每个权重都附加在相应的输入上。这些权重在训练过程中学习，并决定特定输入对最终输出的影响。这使得每个输出![](img/ce6d88ba-ffb0-47cc-9dba-ac31d4c71d8d.png)依赖于所有输入状态的加权组合。'
- en: 'Unfortunately, the attention mechanism comes with a cost. Consider the following,
    from a WildML article ([http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，注意力机制是有代价的。请考虑以下内容，来自一篇WildML的文章（[http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)）：
- en: If we look a bit more closely at the equation for attention, we can see that
    attention comes at a cost. We need to calculate an attention value for each combination
    of input and output word. If you have a 50-word input sequence and generate a
    50-word output sequence, that would be 2500 attention values. That's not too bad,
    but if you do character-level computations and deal with sequences consisting
    of hundreds of tokens, the above attention mechanisms can become prohibitively
    expensive.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更仔细地观察注意力机制的公式，我们可以看到注意力是有代价的。我们需要计算每一对输入和输出单词的注意力值。如果你有一个50个单词的输入序列，并生成一个50个单词的输出序列，那么就会有2500个注意力值。这并不算太糟糕，但如果你进行字符级别的计算，并处理包含数百个标记的序列，上述注意力机制可能会变得代价高昂。
- en: Despite this, the attention mechanism remains a state-of-the-art model for machine
    translation that produces excellent results. The preceding statement only shows
    that there is plenty of room for improvement, so we should try to contribute to
    its development as much as possible.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，注意力机制仍然是机器翻译领域的一种最先进的模型，能产生优秀的结果。前面的陈述仅表明还有很大的改进空间，因此我们应该尽可能地为其发展做出贡献。
- en: Building the Spanish-to-English translator
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建西班牙语到英语的翻译器
- en: I hope the previous sections left you with a good understanding of the model
    we are about to build. Now, we are going to get practical and write the code behind
    our translation system. We should end up with a trained network capable of predicting
    the English version of any sentence in Spanish. Let's dive into programming.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望前面的部分已经让你对我们即将构建的模型有了清晰的理解。现在，我们将实际动手编写我们翻译系统背后的代码。最终，我们应该得到一个经过训练的网络，能够预测任何西班牙语句子的英语版本。让我们开始编程吧。
- en: Preparing the data
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'The first step, as always, is to collect the needed data and prepare it for
    training. The more complicated our systems become, the more complex it is to massage
    the data and reform it into the right shape. We are going to use Spanish-to-English
    phrases from the OpenSubtitles free data source ([http://opus.nlpl.eu/OpenSubtitles.php](http://opus.nlpl.eu/OpenSubtitles.php)).
    We will accomplish that task using the `data_utils.py` script, which you can find
    on the provided GitHub repo ([https://github.com/simonnoff/Recurrent-Neural-Networks-with-Python-Quick-Start-Guide](https://github.com/simonnoff/Recurrent-Neural-Networks-with-Python-Quick-Start-Guide)).
    There you can also find more details on which datasets to download from OpenSubtitles.
    The file calculates the following properties, which could be used further in our
    model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，像往常一样，是收集所需的数据并为训练做准备。我们的系统变得越复杂，整理数据并将其转换为正确格式的过程就越复杂。我们将使用来自OpenSubtitles免费数据源的西班牙语到英语的短语（[http://opus.nlpl.eu/OpenSubtitles.php](http://opus.nlpl.eu/OpenSubtitles.php)）。我们将使用`data_utils.py`脚本来完成这个任务，您可以在提供的GitHub仓库中找到该脚本（[https://github.com/simonnoff/Recurrent-Neural-Networks-with-Python-Quick-Start-Guide](https://github.com/simonnoff/Recurrent-Neural-Networks-with-Python-Quick-Start-Guide)）。在该仓库中，您还可以找到有关从OpenSubtitles下载哪些数据集的更多详细信息。该文件计算了以下属性，这些属性可在我们的模型中进一步使用：
- en: '`spanish_vocab`: A collection of all words in the Spanish training set, ordered
    by frequency'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spanish_vocab`：一个包含所有西班牙语训练集单词的集合，按频率排序'
- en: '`english_vocab`: A collection of all words in the English training set, ordered
    by frequency'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`english_vocab`：一个包含所有英语训练集单词的集合，按频率排序'
- en: '`spanish_idx2word`: A dictionary of key and word, where the key is the order
    of the word in `spanish_vocab`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spanish_idx2word`：一个包含键和值的字典，其中键是单词在`spanish_vocab`中的顺序'
- en: '`spanish_word2idx`: A reversed version of `spanish_idx2word`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spanish_word2idx`：`spanish_idx2word`的反向版本'
- en: '`english_idx2word`: A dictionary of key and word, where the key is the order
    of the word in `english_vocab`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`english_idx2word`：一个包含键和值的字典，其中键是单词在`english_vocab`中的顺序'
- en: '`english_word2idx`: A reversed version of the `english_idx2word`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`english_word2idx`：`english_idx2word`的反向版本'
- en: '`X`: An array of arrays of numbers. We produce that array by first reading
    the Spanish text file line by line, and storing these lines of words in separate
    arrays. Then, we encode each array of a sentence into an array of numbers where
    each word in the sentence is replaced with its index, based on `spanish_word2idx`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X`：一个包含数字数组的数组。我们通过首先逐行读取西班牙文文本文件，并将这些单词存储在单独的数组中来生成这个数组。然后，我们将每个句子的数组编码成一个数字数组，每个单词都用它在
    `spanish_word2idx` 中的索引替换。'
- en: '`Y`: An array of arrays of numbers. We produce that array by first reading
    the English text file line by line and storing these lines of words in separate
    arrays. Then, we encode each array of a sentence into an array of numbers where
    each word in the sentence is replaced with its index, based on `english_word2idx`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Y`：一个包含数字数组的数组。我们通过首先逐行读取英文文本文件，并将这些单词存储在单独的数组中来生成这个数组。然后，我们将每个句子的数组编码成一个数字数组，每个单词都用它在
    `english_word2idx` 中的索引替换。'
- en: You will see, in the following sections, how these collections are used during
    the training and testing of the model. The next step is to construct the TensorFlow
    graph.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在接下来的部分中看到这些集合在模型训练和测试期间的使用方式。下一步是构建 TensorFlow 图。
- en: Constructing the TensorFlow graph
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 TensorFlow 图
- en: 'As an initial step, we import the required Python libraries (you can see this
    in the `neural_machine_translation.py` file):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初步步骤，我们导入所需的 Python 库（你可以在 `neural_machine_translation.py` 文件中看到这一点）：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`tensorflow` and `numpy` should already be familiar to you. `matplotlib` is
    a handy python library used for visualizing data (you will see how we use it shortly).
    Then, we use the `train_test_split` function of `sklearn` to split the data into
    random train and test arrays.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorflow` 和 `numpy` 应该已经对你很熟悉了。`matplotlib` 是一个用于数据可视化的便捷 Python 库（稍后你将看到我们如何使用它）。然后，我们使用
    `sklearn` 的 `train_test_split` 函数将数据拆分为随机的训练和测试数组。'
- en: We also import `data_utils`, which is used to access the data collections mentioned
    in the previous section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还导入了 `data_utils`，它用于访问上一部分提到的数据集合。
- en: 'An important modification to do before splitting the data is making sure each
    of the arrays in *X* and *Y* is padded to indicate the start of a new sequence:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在拆分数据之前，一个重要的修改是确保 *X* 和 *Y* 中的每个数组都进行了填充，以表示新序列的开始：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we split the data as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按如下方式拆分数据：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, it is time to define the actual TensorFlow graph. We start with the variables
    that determine the input and output sequence length:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了定义实际的 TensorFlow 图的时间。我们从确定输入和输出序列长度的变量开始：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we calculate the size of each vocabulary:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算每个词汇表的大小：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `<pad>` symbol is used to align the time steps, `<go>` is used to indicate
    beginning of decoder sequence, and `<eos>` indicates empty spaces.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`<pad>` 符号用于对齐时间步，`<go>` 用于指示解码器序列的开始，`<eos>` 表示空白位置。'
- en: 'After that, we initialize our TensorFlow placeholders:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们初始化了 TensorFlow 的占位符：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`encoder_inputs`: This holds values for the Spanish training input words.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_inputs`：这个变量保存西班牙文训练输入单词的值。'
- en: '`decoder_inputs`: This holds values for the English training input words.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs`：这个变量保存英文训练输入单词的值。'
- en: '`target`: This holds the real values of the English predictions. It has the
    same length as the `decoder_inputs`, where every word is the next predicted.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`：这个变量保存英文预测的真实值。它与 `decoder_inputs` 的长度相同，每个单词是下一个预测的单词。'
- en: '`target_weights`: This is a tensor that gives weights to all predictions.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_weights`：这是一个张量，用于为所有预测值分配权重。'
- en: The final two steps of building the graph are generating the outputs and optimizing
    the weights and biases of the network.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 构建图的最后两个步骤是生成输出并优化网络的权重和偏差。
- en: 'The former uses the handy TensorFlow function, `tf.contrib.legacy_seq2seq.embedding_attention_seq2seq` ([https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq](https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq)), which
    builds a sequence-to-sequence network with attention mechanism and returns the
    generated outputs from the decoder network. The implementation is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 前者使用方便的 TensorFlow 函数 `tf.contrib.legacy_seq2seq.embedding_attention_seq2seq`（[https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq](https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq)），该函数构建一个带有注意力机制的序列到序列网络，并返回解码器网络生成的输出。实现如下：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s discuss the function''s parameters:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下函数的参数：
- en: '`encoder_inputs` and `decoder_inputs` contain values for each training pair
    of Spanish and English sentences.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_inputs` 和 `decoder_inputs` 包含每一对西班牙语和英语句子的训练数据。'
- en: '`tf.contrib.rnn.BasicLSTMCell(size)` is the RNN cell used for the sequence
    model. This is an LSTM cell with `size` (`=512`) number of hidden units.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.contrib.rnn.BasicLSTMCell(size)` 是用于序列模型的 RNN 单元。这是一个具有 `size`（`=512`）个隐藏单元的
    LSTM 单元。'
- en: '`num_encoder_symbols` and `num_decoder_symbols` are the Spanish and English
    dictionaries for the model.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_encoder_symbols` 和 `num_decoder_symbols` 是模型的西班牙语和英语词汇表。'
- en: '`embedding_size` represents the length of the embedding vector for each word.
    This vector can be obtained using the `word2vec` algorithm and helps the network
    in learning during backpropagation.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_size` 表示每个单词的嵌入向量的长度。这个向量可以通过 `word2vec` 算法获得，并帮助网络在反向传播过程中学习。'
- en: '`feed_previous` is a Boolean value that indicates whether or not to use the
    previous output at a certain timestep as the next decoder input.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feed_previous` 是一个布尔值，表示是否在某个时间步使用先前的输出作为下一个解码器输入。'
- en: '`output_projection` contains a pair of the network''s weights and biases. As
    you can see from the preceding code block, the weights have the `[english_vocab_size,
    size]` shape and the biases have the `[english_vocab_size]` shape.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_projection` 包含一对网络的权重和偏置。正如前面的代码块所示，权重的形状是 `[english_vocab_size, size]`，偏置的形状是
    `[english_vocab_size]`。'
- en: 'After computing the outputs, we need to optimize these weights and biases by
    minimizing the loss function of that model. For that purpose, we will be using
    the `tf.contrib.legacy_seq2seq.sequence_loss` TensorFlow function, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算输出之后，我们需要通过最小化该模型的损失函数来优化这些权重和偏置。为此，我们将使用 `tf.contrib.legacy_seq2seq.sequence_loss`
    TensorFlow 函数，如下所示：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We supply the predicted `outputs`, along with the actual values, `targets`,
    of the network. In addition, we provide a slight modification of the standard
    softmax loss function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供预测的 `outputs`，以及网络的实际值 `targets`。此外，我们提供了标准 softmax 损失函数的轻微修改。
- en: Finally, we define the optimizer that aims to minimize the loss function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了优化器，它的目标是最小化损失函数。
- en: 'To clarify the confusion around the `sample_loss` variable, we will give its
    definition as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清 `sample_loss` 变量的混淆，我们将给出它的定义：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This `softmax` function is used only for training. You can learn more about
    it through the TensorFlow documentation ([https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)[).](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `softmax` 函数仅用于训练。你可以通过 TensorFlow 文档了解更多关于它的内容（[https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)）。
- en: These equations result in a fully functional TensorFlow graph for our sequence-to-sequence
    model with attention mechanism. Once again, you may be amazed how little code
    is required to build a powerful neural network that yields excellent results.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程式生成了一个功能完备的 TensorFlow 图，用于我们的带有注意力机制的序列到序列模型。再一次，你可能会惊讶于构建一个强大的神经网络以获得优秀的结果竟然只需要这么少的代码。
- en: Next, we will plug the data collections into this graph and actually train the
    model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集合插入到这个图中，并实际训练模型。
- en: Training the model
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Training the neural network is accomplished using the same pattern as before:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络是通过使用与之前相同的模式来完成的：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The interesting part from the preceding implementation is the `feed_dictionary_values`
    function, which forms the placeholders from the `X_train` and `Y_train`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的实现中有一个有趣的部分是 `feed_dictionary_values` 函数，它通过 `X_train` 和 `Y_train` 来构建占位符：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's break down the above function line by line.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析上面的函数。
- en: 'It needs to return a dictionary with the values of all placeholders. Recall
    that their names are:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要返回一个字典，包含所有占位符的值。回想一下，它们的名称是：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`indices_x` is an array of 64 (`batch_size`) randomly selected indices in the
    range of `0` and `len(X_train)`.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`indices_x` 是一个大小为 64 (`batch_size`) 的数组，包含从 `0` 到 `len(X_train)` 范围内随机选择的索引。'
- en: '`indices_y` is an array of 64 (`batch_size`) randomly selected indices in the
    range of `0` and `len(Y_train)`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`indices_y` 是一个大小为 64 (`batch_size`) 的数组，包含从 `0` 到 `len(Y_train)` 范围内随机选择的索引。'
- en: The `"encoder-"` values are obtained by finding the array at each index from
    `indices_x` and collecting the values for the specific encoder.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`"encoder-"` 的值通过从 `indices_x` 中找到每个索引的数组并收集特定编码器的值来获取。'
- en: Similarly, the `"decoder-"` values are obtained by finding the array at each
    index from `indices_y` and collecting the values for the specific decoder.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`"decoder-"` 的值是通过从 `indices_y` 中查找每个索引的数组，并收集该解码器的特定值来获得的。
- en: 'Consider the following example: Let''s say our `X_train` is `[[x11, x12, ...],
    [x21, x22, ...], ...], indices_x` is `[1, 0, ...]`, then `"encoder0"` will be
    `[x21, x11, ...]` and will contain the 0-th element of all arrays from `X_train`
    that have their index stored in `indices_x`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例：假设我们的 `X_train` 是 `[[x11, x12, ...], [x21, x22, ...], ...]`，`indices_x`
    是 `[1, 0, ...]`，那么 `"encoder0"` 将是 `[x21, x11, ...]`，并且将包含 `X_train` 中所有数组的第 0
    个元素，这些数组的索引已存储在 `indices_x` 中。
- en: The value of `last_output` is an array of the `batch_size` size filled only
    with the number 3 (the associated index of the symbol `"<pad>"`).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`last_output` 的值是一个大小为 `batch_size` 的数组，数组中的值全部是 3（即 `"<pad>"` 符号的关联索引）。'
- en: 'Finally, the `"target_w-"` elements are arrays of 1''s and 0''s of the `batch_size` size. These
    arrays contain 0 at the indices of `"<pad>"` values from the decoder arrays. Let''s
    illustrate this statement with the example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`"target_w-"` 元素是大小为 `batch_size` 的 1 和 0 组成的数组。这些数组在解码器数组中 `"<pad>"` 值的索引位置上包含
    0。我们用以下例子来说明这一点：
- en: If the value of `"decoder0"` is `[10, 8, 3, ...]` where 3 is the index of `"<pad>"`
    from the `en_idx2word` array, our `"target0"` would be `[1, 1, 0, ...]`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `"decoder0"` 的值是 `[10, 8, 3, ...]`，其中 3 是 `"<pad>"` 在 `en_idx2word` 数组中的索引，那么我们的
    `"target0"` 将是 `[1, 1, 0, ...]`。
- en: The last `"target15"` is an array with only 0's.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个 `"target15"` 是一个只有 0 的数组。
- en: Having the preceding calculations in mind, we can start training our network.
    The process will take some time, since we need to iterate over 1,000 steps. Meanwhile,
    we will be storing the trained parameters at every 20 steps, so we can use them
    later for prediction.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记前面的计算，我们可以开始训练我们的网络。这个过程需要一些时间，因为我们需要迭代 1,000 步。与此同时，我们将在每 20 步时存储训练好的参数，之后可以用于预测。
- en: Predicting the translation
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测翻译
- en: 'After we have trained the model, we will use its parameters to translate some
    sentences from Spanish to English. Let''s create a new file called `predict.py`
    and write our prediction code there. The logic will work as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练好模型后，将使用其参数把一些西班牙语句子翻译成英语。让我们创建一个名为 `predict.py` 的新文件，并将预测代码写入其中。逻辑如下：
- en: Define exactly the same sequence-to-sequence model architecture as used during
    training
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义与训练时完全相同的序列到序列模型架构
- en: Use the already trained weights and biases to produce an output
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已经训练好的权重和偏置生成输出
- en: Encode a set of Spanish sentences, ready for translation
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码一组西班牙语句子，准备进行翻译
- en: Predict the final results and print the equivalent English sentences
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测最终结果并打印出相应的英语句子
- en: 'As you can see, this flow is pretty straightforward:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个流程相当直接：
- en: 'To implement it, we first import two Python libraries together with the `neural_machine_translation.py`
    file (used for training):'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们首先导入两个 Python 库以及 `neural_machine_translation.py` 文件（用于训练）。
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we define the model with the associated placeholders:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义与相关占位符配套的模型：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the outputs from the TensorFlow function, we calculate the final translations
    as follows:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 函数的输出，我们计算最终的翻译结果如下：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next step is to define the input sentences and encode them using the encoding
    dictionary:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义输入句子，并使用编码字典进行编码：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, we are also padding the input sentences so they match the length
    of the placeholder (`nmt.input_sequence_length`).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们也在对输入句子进行填充，使它们与占位符 (`nmt.input_sequence_length`) 的长度匹配。
- en: 'Finally, we will be using `spanish_sentences_encoded` with the preceding TensorFlow
    model to calculate the value of `outputs_proj` and yield our results:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用 `spanish_sentences_encoded` 和前述的 TensorFlow 模型来计算 `outputs_proj` 的值，并得到我们的结果：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we define the `decode_output` function, together with a detailed explanation
    of its functionality:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义 `decode_output` 函数，并详细解释其功能：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We prepare the feed dictionary in the same way as using the placeholders' names.
    For `encoder_inputs`, we use values from `spanish_sentences_encoded`. For `decoder_inputs`,
    we use the values saved in our model's checkpoint folder.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们以与占位符名称相同的方式准备数据字典。对于 `encoder_inputs`，我们使用来自 `spanish_sentences_encoded`
    的值。对于 `decoder_inputs`，我们使用保存在模型检查点文件夹中的值。
- en: Using the preceding data, our model calculates the `output_sentences`.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前述数据，我们的模型计算 `output_sentences`。
- en: Finally, we use the `decode_output` function to convert the predicted `output_sentences`
    matrix into an actual sentence. For that, we use the `english_idx2word` dictionary.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`decode_output`函数将预测的`output_sentences`矩阵转换为实际的句子。为此，我们使用`english_idx2word`字典。
- en: 'After running the preceding code, you should see the original sentence in Spanish,
    together with its translation in English. The correct output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码之后，你应该能看到原始的西班牙语句子以及其英文翻译。正确的输出如下：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, we will see how to evaluate our results and identify how well our translation
    model has performed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何评估我们的结果，并识别我们的翻译模型表现如何。
- en: Evaluating the final results
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估最终结果
- en: Translation models are typically evaluated using the so-called BLEU score ([https://www.youtube.com/watch?v=DejHQYAGb7Q](https://www.youtube.com/watch?v=DejHQYAGb7Q)).
    This is an automatically generated score that compares a human-generated translation
    with the prediction. It looks for the presence and absence of particular words,
    their ordering, and any distortion—that is, how much they are separated in the
    output.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译模型通常使用所谓的BLEU评分来评估（[https://www.youtube.com/watch?v=DejHQYAGb7Q](https://www.youtube.com/watch?v=DejHQYAGb7Q)）。这是一个自动生成的评分，它将人类生成的翻译与预测进行比较。它检查特定单词的出现与否、它们的排序以及任何扭曲——也就是说，它们在输出中的分离程度。
- en: A BLEU score varies between `0` and `1`, where `0` is produced if there are
    no matching words between the prediction and the human-generated sentence, and
    1 when both sentences match perfectly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU评分的范围在`0`到`1`之间，`0`表示预测与人类生成的句子没有匹配的单词，`1`则表示两个句子完全匹配。
- en: Unfortunately, this score can be sensitive about word breaks. If the word breaks
    are positioned differently, the score may be completely off.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个评分对单词断裂的位置较为敏感。如果单词断裂的位置不同，评分可能会完全不准确。
- en: A good machine translation model, such as Google's Multilingual Neural Machine
    Translation System, achieves score of around `38.0 (0.38*100)` on Spanish-to-English
    translation. This is an example of an exceptionally performing model. The result
    is pretty remarkable but, as you can see, there is a lot of room for improvement.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的机器翻译模型，例如谷歌的多语言神经机器翻译系统，在西班牙语到英语的翻译中得分大约为`38.0 (0.38*100)`。这是一个表现异常出色的模型示例。结果相当显著，但正如你所见，仍然有很大的改进空间。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter walked you through building a fairly sophisticated neural network
    model using the sequence-to-sequence model implemented with the TensorFlow library.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本章带你通过使用TensorFlow库实现的序列到序列模型构建了一个相当复杂的神经网络模型。
- en: First, you went through the theoretical part, gain an understanding of how the
    model works under the hood and why its application has resulted in remarkable
    achievements. In addition, you learned how an LSTM network works and why it is
    easily considered the best RNN model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你了解了理论部分，理解了模型的工作原理及其应用为何能取得显著的成就。此外，你学习了LSTM网络的工作原理，并且明白了为什么它被认为是最佳的RNN模型。
- en: Second, you saw how you can put the knowledge acquired here into practice using
    just several lines of code. In addition, you gain an understanding of how to prepare
    your data to fit the sequence-to-sequence model. Finally, you were able to successfully
    translate Spanish sentences into English.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，你看到了如何将这里学到的知识通过几行代码付诸实践。此外，你理解了如何准备数据以适配序列到序列模型。最后，你成功地将西班牙语句子翻译成了英语。
- en: I really hope this chapter left you more confident in your deep learning knowledge
    and armed you with new skills that you can apply to future applications.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我真心希望本章能够让你对深度学习的知识更有信心，并赋予你可以应用于未来应用程序的新技能。
- en: External links
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部链接
- en: Sequence to Sequence model (Cho et al. 2014): [https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型（Cho等人，2014）：[https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)
- en: Understanding LSTM Network: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LSTM网络：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: Stanford University lecture on LSTM: [https://www.youtube.com/watch?v=QuELiw8tbx8](https://www.youtube.com/watch?v=QuELiw8tbx8)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福大学关于LSTM的讲座：[https://www.youtube.com/watch?v=QuELiw8tbx8](https://www.youtube.com/watch?v=QuELiw8tbx8)
- en: Sequence to sequence learning using Neural Network: [https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络的序列到序列学习：[https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)
- en: WildML article on *Attention and Memory in Deep Learning and NLP*: [http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WildML 文章关于 *深度学习和自然语言处理中的注意力与记忆*： [http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)
- en: OpenSubtitles: [http://opus.nlpl.eu/OpenSubtitles.php](http://opus.nlpl.eu/OpenSubtitles.php)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenSubtitles: [http://opus.nlpl.eu/OpenSubtitles.php](http://opus.nlpl.eu/OpenSubtitles.php)'
- en: '`tf.contrib.legacy_seq2seq.embedding_attention_seq2seq`: [https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq](https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.contrib.legacy_seq2seq.embedding_attention_seq2seq`: [https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq](https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq)'
- en: '`tf.nn.sampled_softmax_loss`: [https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.nn.sampled_softmax_loss`: [https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)'
- en: 'BLEU score: [https://www.youtube.com/watch?v=DejHQYAGb7Q](https://www.youtube.com/watch?v=DejHQYAGb7Q)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BLEU 分数: [https://www.youtube.com/watch?v=DejHQYAGb7Q](https://www.youtube.com/watch?v=DejHQYAGb7Q)'
