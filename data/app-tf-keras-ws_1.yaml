- en: 1\. Introduction to Neural Networks and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 神经网络与深度学习简介
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will cover the basics of neural networks and how to set
    up a deep learning programming environment. We will also explore the common components
    and essential operations of a neural network . We will conclude this chapter with
    an exploration of a trained neural network created using TensorFlow. By the end
    of this chapter, you will be able to train a neural network.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍神经网络的基础知识，以及如何设置深度学习编程环境。我们还将探讨神经网络的常见组件和基本操作。最后，我们将通过使用TensorFlow创建的一个训练过的神经网络进行探索。通过本章学习结束时，您将能够训练一个神经网络。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: This chapter is about understanding what neural networks can do rather than
    the finer workings of deep learning. For this reason, we will not cover the mathematical
    concepts underlying deep learning algorithms but will describe the essential pieces
    that make up a deep learning system and the role of neural networks within that
    system. We will also look at examples where neural networks have been used to
    solve real-world problems using these algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在帮助理解神经网络能做什么，而不是深入探讨深度学习的具体原理。因此，我们不会涉及深度学习算法背后的数学概念，而是将描述构成深度学习系统的基本部分，以及神经网络在该系统中的作用。我们还将通过实例展示神经网络如何利用这些算法解决实际问题。
- en: At its core, this chapter challenges you to think about your problem as a mathematical
    representation of ideas. By the end of this chapter, you will be able to think
    about a problem as a collection of these representations and to recognize how
    these representations can be learned by deep learning algorithms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的核心是挑战您将问题看作是思想的数学表示。到本章结束时，您将能够将问题看作这些表示的集合，并识别深度学习算法如何学习这些表示。
- en: What are Neural Networks?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: A **neural network** is a network of neurons. In our brain, we have a network
    of billions of neurons that are interconnected with each other. The neuron is
    one of the basic elements of the nervous system. The primary function of the neuron
    is to perform actions as a response to an event and transmit messages to other
    neurons. In this case, the action is simply either activating or deactivating
    itself. Taking inspiration from the brain's design, **artificial neural networks**
    were first proposed in the 1940s by MIT professors *Warren McCullough* and *Walter
    Pitts*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**是由神经元组成的网络。在我们的大脑中，有一个由数十亿神经元相互连接的网络。神经元是神经系统的基本元素之一。神经元的主要功能是对事件做出反应并向其他神经元传递信息。在这种情况下，神经元的反应就是简单地激活或去激活自身。受到大脑设计的启发，**人工神经网络**最早由麻省理工学院的教授*沃伦·麦卡洛克*和*沃尔特·皮茨*在1940年代提出。'
- en: Note
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on neural networks, refer to *Explained: Neural networks.
    MIT News Office, April 14, 2017*, available at [http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414](http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于神经网络的信息，请参阅*《解释：神经网络》MIT新闻办公室，2017年4月14日*，可访问[http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414](http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414)。
- en: Inspired by advancements in neuroscience, they proposed to create a computer
    system that reproduced how the brain works (human or otherwise). At its core was
    the idea of a computer system that worked as an interconnected network, that is,
    a system that has many simple components. These components interpret data and
    influence each other on how to interpret that data. The same core idea r emains
    today.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 受到神经科学进展的启发，他们提出了创建一个计算机系统来再现大脑（无论是人类的大脑还是其他大脑）的工作原理。其核心思想是构建一个作为相互连接的网络工作的计算机系统，也就是说，一个由许多简单组件组成的系统。这些组件解读数据，并相互影响如何解读这些数据。这个核心思想至今仍然存在。
- en: Deep learning is largely considered the contemporary study of neural networks.
    Think of it as a contemporary name given to neural networks. The main difference
    is that the neural networks used in deep learning are typically far greater in
    size, meaning they have many more nodes and layers than earlier neural networks.
    Deep learning algorithms and applications typically require resources to achieve
    success, hence the use of the word *deep* to emphasize their size and the large
    number of interconnected components.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在很大程度上被认为是神经网络的现代研究。可以把它看作是神经网络的现代名称。主要的区别在于，深度学习中使用的神经网络通常要大得多，意味着它们比早期的神经网络包含更多的节点和层次。深度学习算法和应用通常需要大量资源才能成功，因此使用了*深度*这个词来强调其规模以及大量相互连接的组件。
- en: Successful Applications of Neural Networks
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的成功应用
- en: Neural networks have been under research in one form or another since their
    inception in the 1940s. It is only recently that deep learning systems have been
    used successfully in large-scale industry applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自1940年代神经网络诞生以来，它们一直以某种形式存在于研究中。直到最近，深度学习系统才成功地应用于大规模工业应用。
- en: Contemporary proponents of neural networks have demonstrated great success in
    speech recognition, language translation, image classification, and other fields.
    Its current prominence is backed by a significant increase in available computing
    power and the emergence of **Graphic Processing Units** (**GPUs**) and **Tensor
    Processing Units** (**TPUs**), which can perform many more simultaneous mathematical
    operations than regular CPUs, as well as much greater availability of data. Compared
    to CPUs, GPUs are designed to execute special tasks (in the "single instruction,
    multiple threads" model) where the execution can be parallelized.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现代神经网络的支持者在语音识别、语言翻译、图像分类等领域取得了巨大成功。其当前的突出地位得益于可用计算能力的大幅增加，以及**图形处理单元**（**GPU**）和**张量处理单元**（**TPU**）的出现，这些处理单元能够执行比常规CPU更多的并行数学运算，并且有更多的数据可用。与CPU相比，GPU专门设计用来执行特定任务（在“单指令，多线程”模式下），其执行可以并行化。
- en: One such success story is the power consumption of different AlphaGo algorithms.
    **AlphaGo** is an initiative by DeepMind to develop a series of algorithms to
    beat the game Go. It is considered a prime example of the power of deep learning.
    The team at DeepMind was able to do this using reinforcement learning in which
    AlphaGo becomes its own teacher.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个成功案例是不同AlphaGo算法的功耗。**AlphaGo**是DeepMind发起的一个项目，旨在开发一系列算法来击败围棋游戏。它被视为深度学习能力的典型例子。DeepMind的团队利用强化学习实现了这一目标，其中AlphaGo成为了自己的老师。
- en: 'The neural network, which initially knows nothing, plays with itself to understand
    which moves lead to victory. The algorithm used TPUs for training. TPUs are a
    type of chipset developed by Google that are specialized for use in deep learning
    programs. The article *Alpha Zero: Starting from scratch*, [https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/),
    depicts the number of GPUs and TPUs used to train different versions of the AlphaGo
    algorithm.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '这个最初一无所知的神经网络通过与自己对弈，了解哪些棋步能带来胜利。所使用的算法利用了TPU进行训练。TPU是谷歌开发的一种芯片组，专门用于深度学习程序。文章《*Alpha
    Zero: 从零开始*》，[https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/)，描述了用于训练不同版本AlphaGo算法的GPU和TPU的数量。'
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this book, we will not be using GPUs to fulfill our activities. GPUs are
    not required to work with neural networks. In several simple examples—like the
    ones provided in this book—all computations can be performed using a simple laptop's
    CPU. However, when dealing with very large datasets, GPUs can be of great help
    given that the long time taken to train a neural network would otherwise be impractical.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将不使用GPU来完成我们的任务。神经网络的工作不需要GPU。在一些简单的例子中——如本书中提供的——所有计算都可以通过一台普通笔记本的CPU来完成。然而，在处理非常大的数据集时，GPU可以提供很大帮助，因为训练一个神经网络所需的时间，否则将变得不切实际。
- en: 'Here are a few examples where neural networks have had a significant impact:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是神经网络在多个领域产生重大影响的几个例子：
- en: '**Translating text**: In 2017, Google announced the release of a new algorithm
    for its translation service called **Transformer**. The algorithm consisted of
    a recurrent neural network called **Long Short-term Memory** (**LSTM**) that was
    trained to use bilingual text. LSTM is a form of neural network that is applied
    to text data. Google showed that its algorithm had gained notable accuracy when
    compared to the industry standard, **Bilingual Evaluation Understudy** (**BLEU**),
    and was also computationally efficient. BLEU is an algorithm for evaluating the
    performance of machine-translated text. For more information on this, refer to
    the Google Research Blog, *Transformer: A Novel Neural Network Architecture for
    Language Understanding,* August 31, 2017, available at [https://research.googleblog.com/2017/08/transformer-novel-neural-network.html](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本翻译**：2017年，谷歌宣布发布了一种新的翻译服务算法，名为 **Transformer**。该算法由一种称为 **长短期记忆**（**LSTM**）的递归神经网络组成，该网络经过训练以使用双语文本。LSTM
    是一种应用于文本数据的神经网络形式。谷歌展示了该算法与行业标准 **双语评估指标**（**BLEU**）相比，具有显著的准确性，并且在计算上也非常高效。BLEU
    是一种评估机器翻译文本性能的算法。有关更多信息，请参阅谷歌研究博客，*Transformer：一种新型神经网络架构用于语言理解*，2017年8月31日，可在
    [https://research.googleblog.com/2017/08/transformer-novel-neural-network.html](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)
    查阅。'
- en: '**Self-driving vehicles**: Uber, NVIDIA, and Waymo are believed to be using
    deep learning models to control different vehicle functions related to driving.
    Each company is researching several possibilities, including training the network
    using humans, simulating vehicles driving in virtual environments, and even creating
    a small city-like environment in which vehicles can be trained based on expected
    and unexpected events.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动驾驶车辆**：据认为，Uber、NVIDIA 和 Waymo 正在使用深度学习模型来控制与驾驶相关的不同车辆功能。每家公司都在研究多种可能性，包括使用人类进行训练、在虚拟环境中模拟车辆驾驶，甚至创建一个类似小城市的环境，在其中基于预期和非预期事件对车辆进行训练。'
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: To know more about each of these achievements, refer to the following references.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解每项成就的详细信息，请参阅以下参考资料。
- en: '**Uber:** *Uber''s new AI team is looking for the shortest route to self-driving
    cars*, *Dave Gershgorn*, *Quartz*, *December 5, 2016*, available at [https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**Uber**：*Uber 的新 AI 团队正在寻找通往自动驾驶汽车的最短路线*，*Dave Gershgorn*，*Quartz*，*2016年12月5日*，可在
    [https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/)
    查阅。'
- en: '**NVIDIA**: *End-to-End Deep Learning for Self-Driving Cars*, *August 17, 2016*,
    available at [https://devblogs.nvidia.com/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**NVIDIA**：*自动驾驶汽车的端到端深度学习*，*2016年8月17日*，可在 [https://devblogs.nvidia.com/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)
    查阅。'
- en: '**Waymo**: *Inside Waymo''s Secret World for Training Self-Driving Cars. The
    Atlantic*, *Alexis C. Madrigal*, *August 23, 2017*, available at [https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Waymo**：*深入了解 Waymo 为训练自动驾驶汽车而建立的秘密世界*，《大西洋月刊》，*Alexis C. Madrigal*，*2017年8月23日*，可在
    [https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/)
    查阅。'
- en: '**Image recognition**: Facebook and Google use deep learning models to identify
    entities in images and automatically tag these entities as persons from a set
    of contacts. In both cases, the networks are trained with previously tagged images
    as well as with images from the target friend or contact. Both companies report
    that the models can suggest a friend or contact with a high level of accuracy
    in most cases.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像识别**：Facebook 和谷歌使用深度学习模型来识别图像中的实体，并自动将这些实体标记为来自联系人列表中的人物。在这两种情况下，网络都通过之前已标记的图像以及目标朋友或联系人的图像进行训练。两家公司报告称，在大多数情况下，这些模型能够高效且准确地建议朋友或联系人。'
- en: While there are many more examples in other industries, the application of deep
    learning models is still in its infancy. Many successful applications are yet
    to come, including the ones that you create.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其他行业还有许多例子，深度学习模型的应用仍处于起步阶段。许多成功的应用还未出现，包括你所创造的那些。
- en: Why Do Neural Networks Work So Well?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么神经网络效果如此好？
- en: 'Why are neural networks so powerful? Neural networks are powerful because they
    can be used to predict any given function with reasonable approximation. If we
    can represent a problem as a mathematical function and we have data that represents
    that function correctly, a deep learning model can, given enough resources, be
    able to approximate that function. This is typically called the *Universal Approximation
    Theorem*. For more information, refer to Michael Nielsen: *Neural Networks and
    Deep Learning: A visual proof that neural nets can compute any function*, available
    at [http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么神经网络如此强大？神经网络之所以强大，是因为它们可以用来预测任何给定函数的合理逼近。如果我们能将一个问题表示为一个数学函数，并且我们有正确表示该函数的数据，那么只要有足够的资源，深度学习模型就能逼近这个函数。这通常被称为*通用逼近定理*。更多信息，请参考
    Michael Nielsen 的《神经网络与深度学习：神经网络可以计算任何函数的视觉证明》，该书可在以下网址找到：[http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html)。
- en: 'We will not be exploring mathematical proofs of the universality principle
    in this book. However, two characteristics of neural networks should give you
    the right intuition on how to understand that principle: representation learning
    and function approximation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们不会探讨通用性原理的数学证明。然而，神经网络的两个特征应该能给你正确的直觉，帮助你理解这一原理：表示学习和函数逼近。
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information, refer to *A Brief Survey of Deep Reinforcement Learning*,
    *Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath*,
    *arXiv*, *September 28, 2017*, available at [https://www.arxiv-vanity.com/papers/1708.05866/](https://www.arxiv-vanity.com/papers/1708.05866/).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考 *深度强化学习简要概述*，*Kai Arulkumaran、Marc Peter Deisenroth、Miles Brundage
    和 Anil Anthony Bharath*，*arXiv*，*2017年9月28日*，可在以下网址找到：[https://www.arxiv-vanity.com/papers/1708.05866/](https://www.arxiv-vanity.com/papers/1708.05866/)。
- en: Representation Learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示学习
- en: 'The data used to train a neural network contains representations (also known
    as *features*) that explain the problem you are trying to solve. For instance,
    if we are interested in recognizing faces from images, the color values of each
    pixel from a set of images that contain faces will be used as a starting point.
    The model will then continuously learn higher-level representations by combining
    pixels together as it goes through its training process. A pictorial depiction
    is displayed here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练神经网络的数据包含表示（也称为*特征*），它们解释了你试图解决的问题。例如，如果我们有兴趣从图像中识别人脸，那么从包含人脸的一组图像中，每个像素的颜色值将作为起点。然后，模型会通过将像素组合在一起来不断学习更高层次的表示，直到完成训练过程。这里展示了一个图示：
- en: '![Figure 1.1: A series of higher-level representations based on input data'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：基于输入数据的一系列更高层次的表示'
- en: '](img/B15911_01_01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_01.jpg)'
- en: 'Figure 1.1: A series of higher-level representations based on input data'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：基于输入数据的一系列更高层次的表示
- en: Note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 1.1* is a derivate image based on an original image from Yann LeCun,
    Yoshua Bengio, and Geoffrey Hinton in *Deep Learning*, published in *Nature, 521,
    436–444 (28 May 2015) doi:10.1038/ nature14539*. You can find the paper at: [https://www.nature.com/articles/nature14539](https://www.nature.com/articles/nature14539).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.1* 是基于 Yann LeCun、Yoshua Bengio 和 Geoffrey Hinton 在《深度学习》一书中所提供的原始图像制作的派生图像，该书发表于
    *Nature, 521, 436–444 (2015年5月28日) doi:10.1038/ nature14539*。你可以在以下网址找到该论文：[https://www.nature.com/articles/nature14539](https://www.nature.com/articles/nature14539)。'
- en: 'In formal words, neural networks are computation graphs in which each step
    computes higher abstraction representations from input data. Each of these steps
    represents a progression into a different abstraction layer. Data progresses through
    each of these layers, thereby building higher-level representations. The process
    finishes with the highest representation possible: the one the model is trying
    to predict.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用正式的语言来说，神经网络是计算图，其中每一步都会根据输入数据计算出更高层次的抽象表示。每一步表示进入一个不同的抽象层次。数据通过这些层次进展，从而构建出更高层次的表示。该过程最终会完成最高层次的表示：模型试图预测的结果。
- en: Function Approximation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数逼近
- en: When neural networks learn new representations of data, they do so by combining
    weights and biases with neurons from different layers. They adjust the weights
    of these connections every time a training cycle occurs using a mathematical technique
    called **backpropagation**. The weights and biases improve at each round, up to
    the point that an optimum is achieved. This means that a neural network can measure
    how wrong it is on every training cycle, adjust the weights and biases of each
    neuron, and try again. If it determines that a certain modification produces better
    results than the previous round, it will invest in that modification until an
    optimal solution is achieved.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络学习数据的新表示时，它们通过将不同层次的神经元与权重和偏置相结合来进行学习。在每次训练周期中，它们都会使用一种叫做**反向传播**的数学方法调整这些连接的权重。每一轮中，权重和偏置都会得到改进，直到达到一个最优解。这意味着神经网络可以测量每次训练周期中它的错误程度，调整每个神经元的权重和偏置，并再次尝试。如果它确定某种修改比前一轮的结果更好，它会继续投资于这个修改，直到达到最优解。
- en: So basically, in a single cycle, three things happen. The first one is forward
    propagation where we calculate the results using weights, biases, and inputs.
    In the second step, we calculate how far the calculated value is from the expected
    value using a loss function. The final step is to update the weights and biases
    moving in the reverse direction of forward propagation, which is called backpropagation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，在一个周期中，发生了三件事。首先是前向传播，我们使用权重、偏置和输入来计算结果。第二步，我们使用损失函数计算计算值与期望值之间的差距。最后一步是更新权重和偏置，朝着与前向传播相反的方向调整，这个过程叫做反向传播。
- en: Since the weights and biases in the earlier layers do not have a direct connection
    with the later layers, we use a mathematical tool called the chain rule to calculate
    new weights for the earlier layers. Basically, the change in the earlier layer
    is equal to the multiplication of the gradients or derivatives of all the layers
    below it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于早期层次的权重和偏置与后期层次没有直接联系，我们使用一个叫做链式法则的数学工具来计算早期层次的新权重。基本上，早期层次的变化等于其下方所有层次的梯度或导数的乘积。
- en: 'In a nutshell, that procedure is the reason why neural networks can approximate
    functions. However, there are many reasons why a neural network may not be able
    to predict a function with perfection, chief among them being the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个过程是神经网络能够逼近函数的原因。然而，神经网络无法完美预测一个函数的原因有很多，其中最主要的原因包括以下几点：
- en: Many functions contain stochastic properties (that is, random properties).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多函数具有随机特性（即随机性质）。
- en: There may be overfitting to peculiarities from the training data. Overfitting
    is a situation where the model we are training doesn't generalize well to data
    it has never seen before. It just learns the training data instead of finding
    some interesting patterns.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会出现对训练数据特性的过拟合。过拟合是指我们训练的模型在面对从未见过的数据时，无法很好地泛化。它只是学习了训练数据，而不是发现一些有趣的模式。
- en: There may be a lack of training data.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能缺乏训练数据。
- en: In many practical applications, simple neural networks can approximate a function
    with reasonable precision. These sorts of applications will be our focus throughout
    this book.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际应用中，简单的神经网络可以以合理的精度逼近一个函数。这类应用将是本书的重点。
- en: Limitations of Deep Learning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的局限性
- en: Deep learning techniques are best suited to problems that can be defined with
    formal mathematical rules (such as data representations). If a problem is hard
    to define this way, it is likely that deep learning will not provide a useful
    solution. Moreover, if the data available for a given problem is either biased
    or only contains partial representations of the underlying functions that generate
    that problem, deep learning techniques will only be able to reproduce the problem
    and not learn to solve it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术最适用于那些可以用正式的数学规则（例如数据表示）来定义的问题。如果一个问题很难用这种方式定义，那么深度学习可能无法提供有效的解决方案。此外，如果给定问题的数据有偏差或仅包含生成该问题的潜在函数的部分表示，深度学习技术将只能复制该问题，而无法学会解决它。
- en: 'Remember that deep learning algorithms learn different representations of data
    to approximate a given function. If data does not represent a function appropriately,
    it is likely that the function will be incorrectly represented by the neural network.
    Consider the following analogy: you are trying to predict the national prices
    of gasoline (that is, fuel) and create a deep learning model. You use your credit
    card statement with your daily expenses on gasoline as an input data for that
    model. The model may eventually learn the patterns of your gasoline consumption,
    but it will likely misrepresent price fluctuations of gasoline caused by other
    factors only represented weekly in your data such as government policies, market
    competition, international politics, and so on. The model will ultimately yield
    incorrect results when used in production.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，深度学习算法通过学习数据的不同表示来近似给定的函数。如果数据不能恰当地表示一个函数，那么神经网络可能会错误地表示该函数。可以考虑以下类比：你正在尝试预测全国汽油价格（即燃料价格），并创建一个深度学习模型。你将信用卡账单中每日的汽油消费作为输入数据。该模型最终可能学会你的汽油消费模式，但它可能会错误地表示由于其他因素（例如政府政策、市场竞争、国际政治等）引起的汽油价格波动，这些因素在你的数据中只是每周表示一次。最终，当模型投入生产使用时，可能会得出错误的结果。
- en: To avoid this problem, make sure that the data used to train a model represents
    the problem the model is trying to address as accurately as possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，确保用于训练模型的数据尽可能准确地代表模型试图解决的问题。
- en: Inherent Bias and Ethical Considerations
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 固有偏差与伦理考量
- en: Researchers have suggested that the use of deep learning models without considering
    the inherent bias in the training data can lead not only to poorly performing
    solutions but also to ethical complications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员建议，在使用深度学习模型时，如果没有考虑训练数据中的固有偏差，不仅可能导致表现不佳的解决方案，还可能引发伦理上的问题。
- en: For instance, in late 2016, researchers from the Shanghai Jiao Tong University
    in China created a neural network that correctly classified criminals using only
    pictures of their faces. The researchers used 1,856 images of Chinese men, of
    which half had been convicted. Their model identified inmates with 89.5% accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2016年底，中国上海交通大学的研究人员创建了一个神经网络，能够仅通过面部照片正确识别犯罪分子。研究人员使用了1,856张中国男性的照片，其中一半是已定罪的。该模型以89.5%的准确率识别了这些囚犯。
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To know more about this, refer to [https://blog.keras.io/the-limitations-of-deep-learning.html](https://blog.keras.io/the-limitations-of-deep-learning.html)
    and *MIT Technology Review. Neural Network Learns to Identify Criminals by Their
    Faces*, *November 22, 2016*, available at [https://www.technologyreview.com/2016/11/22/107128/neural-network-learns-to-identify-criminals-by-their-faces/](https://www.technologyreview.com/2016/11/22/107128/neural-network-learns-to-identify-criminals-by-their-faces/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多信息，请参考 [https://blog.keras.io/the-limitations-of-deep-learning.html](https://blog.keras.io/the-limitations-of-deep-learning.html)
    和 *MIT Technology Review. 神经网络通过面部识别犯罪分子*，*2016年11月22日*，可在 [https://www.technologyreview.com/2016/11/22/107128/neural-network-learns-to-identify-criminals-by-their-faces/](https://www.technologyreview.com/2016/11/22/107128/neural-network-learns-to-identify-criminals-by-their-faces/)
    阅读。
- en: 'The paper resulted in a great furor within the scientific community and popular
    media. One key issue with the proposed solution is that it fails to properly recognize
    the bias inherent in the input data. Namely, the data used in this study came
    from two different sources: one for criminals and one for non-criminals. Some
    researchers suggest that their algorithm identifies patterns associated with the
    different data sources used in the study instead of identifying relevant patterns
    from people''s faces. While there are technical considerations that we can make
    about the reliability of the model, the key criticism is on ethical grounds: researchers
    ought to clearly recognize the inherent bias in input data used by deep learning
    algorithms and consider how its application will impact on people''s lives. *Timothy
    Revell. Concerns as face recognition tech used to ''identify'' criminals. New
    Scientist. December 1, 2016\. Available at:* [https://www.newscientist.com/article/2114900-concerns-as-face-recognition-tech-used-to-identify-criminals/](https://www.newscientist.com/article/2114900-concerns-as-face-recognition-tech-used-to-identify-crim).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文在科学界和大众媒体中引起了极大轰动。该解决方案的一个关键问题是未能正确识别输入数据中固有的偏差。即，该研究使用的数据来自两个不同的来源：一个是罪犯数据，另一个是非罪犯数据。一些研究人员建议，他们的算法识别的是与研究中使用的不同数据来源相关的模式，而不是识别来自人脸的相关模式。虽然我们可以从技术角度讨论模型的可靠性，但主要的批评是在伦理方面：研究人员应当明确认识到深度学习算法所使用的输入数据中固有的偏见，并考虑其应用会如何影响人们的生活。
    *Timothy Revell. 当面部识别技术被用来“识别”罪犯时的担忧. 新科学家. 2016年12月1日\. 访问链接：* [https://www.newscientist.com/article/2114900-concerns-as-face-recognition-tech-used-to-identify-criminals/](https://www.newscientist.com/article/2114900-concerns-as-face-recognition-tech-used-to-identify-crim)
- en: 'There are different types of biases that can occur in a dataset. Consider a
    case where you are building an automatic surveillance system that can operate
    both in the daytime and nighttime. So, if your dataset just included images from
    the daytime, then you would be introducing a sample bias in the model. This could
    be eliminated by including nighttime data and covering all the different types
    of cases possible, such as images from a sunny day, a rainy day, and so on. Another
    example to consider is where, let''s suppose, a similar kind of system is installed
    in a workplace to analyze the workers and their activities. Now, if your model
    has been fed with thousands of examples with men coding and women cooking, then
    this data clearly reflects stereotypes. A solution to this problem is the same
    as earlier: to expose the model to data that is more evenly distributed.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可能会出现不同类型的偏差。考虑一个例子，假设你正在构建一个可以在白天和夜间都能运行的自动监控系统。如果你的数据集仅包含白天的图像，那么你就会在模型中引入样本偏差。通过包含夜间数据，并覆盖所有可能的不同情况，例如晴天、雨天的图像，可以消除这种偏差。另一个例子是，假设在一个工作场所安装了类似的系统，用于分析工人及其活动。如果你的模型输入了成千上万的例子，图像中男人在编程，女人在做饭，那么这些数据显然反映了刻板印象。解决这个问题的方法与之前相同：将模型暴露于更加均衡分布的数据。
- en: Note
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To find out more about the topic of ethics in learning algorithms (including
    deep learning), refer to the work done by the AI Now Institute ([https://ainowinstitute.org/](https://ainowinstitute.org/)),
    an organization created for the understanding of the social implications of intelligent
    systems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于学习算法（包括深度学习）伦理的话题，请参考由AI Now Institute所做的研究（[https://ainowinstitute.org/](https://ainowinstitute.org/)），该组织致力于理解智能系统的社会影响。
- en: Common Components and Operations of Neural Networks
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的常见组件和操作
- en: 'Neural networks have two key components: layers and nodes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有两个关键组件：层和节点。
- en: 'Nodes are responsible for specific operations, and layers are groups of nodes
    that differentiate different stages of the system. Typically, neural networks
    are comprised of the following three layers:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 节点负责特定的操作，层是节点的组合，用于区分系统的不同阶段。典型的神经网络由以下三层组成：
- en: '**Input layer**: Where the input data is received and interpreted'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：接收并解读输入数据'
- en: '**Hidden layer**: Where computations take place, modifying the data as it passes
    through'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：进行计算并在数据传递过程中修改数据'
- en: '**Output layer**: Where the output is assembled and evaluated'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：在此处将输出结果组装并进行评估'
- en: 'The following figure displays the working of layers of neural networks:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了神经网络层的工作原理：
- en: '![Figure 1.2: An illustration of the most common layers in a neural network'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2：神经网络中最常见层的示意图'
- en: '](img/B15911_01_02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_02.jpg)'
- en: 'Figure 1.2: An illustration of the most common layers in a neural network'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：神经网络中最常见层的示意图
- en: Hidden layers are the most important layers in neural networks. They are referred
    to as *hidden* because the representations generated in them are not available
    in the data, but are learned from it instead. It is within these layers where
    the main computations take place in neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层是神经网络中最重要的层。它们被称为*隐藏层*，因为在这些层中生成的表示数据并不直接出现在数据中，而是从数据中学习到的。正是这些层内进行的主要计算构成了神经网络的核心。
- en: 'Nodes are where data is represented in the network. There are two values associated
    with nodes: biases and weights. Both values affect how data is represented by
    the nodes and passed on to other nodes. When a network *learns*, it effectively
    adjusts these values to satisfy an optimization function.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 节点是数据在网络中表示的位置。节点有两个与之相关的值：偏置和权重。这两个值影响数据在节点中的表示方式，并将数据传递给其他节点。当网络*学习*时，它实际上会调整这些值，以满足优化函数。
- en: Most of the work in neural networks happens in the hidden layers. Unfortunately,
    there isn't a clear rule for determining how many layers or nodes a network should
    have. When implementing a neural network, you will probably spend time experimenting
    with different combinations of layers and nodes. It is advisable to always start
    with a single layer and also with a number of nodes that reflect the number of
    features the input data has (that is, how many *columns* are available in a given
    dataset).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的大部分工作发生在隐藏层中。不幸的是，目前没有明确的规则来确定网络应有多少层或节点。在实现神经网络时，你可能会花费时间实验不同的层和节点组合。建议从单层开始，并且使用一个节点数量来反映输入数据的特征数量（也就是说，数据集中有多少*列*）。
- en: You can continue to add layers and nodes until a satisfactory performance is
    achieved—or whenever the network starts overfitting to the training data. Also,
    note that this depends very much on the dataset – if you were training a model
    to recognize hand-drawn digits, then a neural network with two hidden layers would
    be enough, but if your dataset was more complex, say for detecting objects like
    cars and ambulance in images, then even 10 layers would not be enough and you
    would need to have a deeper network for the objects to be recognized correctly.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续添加层和节点，直到达到满意的性能——或者当网络开始对训练数据过拟合时停止。另外，注意这很大程度上依赖于数据集——如果你训练一个模型来识别手写数字，那么两个隐藏层的神经网络就足够了，但如果你的数据集更复杂，比如检测图像中的汽车和救护车等物体，那么即使是
    10 层也可能不足以正确识别物体，你需要更深的网络。
- en: Likewise, if you were using a network with 100 hidden layers for training on
    handwritten digits, then there would be a strong possibility that you would overfit
    the model, as that much complexity would not be required by the model in the first
    place.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你使用一个有 100 个隐藏层的网络来训练手写数字，那么很有可能会导致模型过拟合，因为如此复杂的结构对模型来说并非必需。
- en: Contemporary neural network practice is generally restricted to experimentation
    with the number of nodes and layers (for example, how deep the network is), and
    the kinds of operations performed at each layer. There are many successful instances
    in which neural networks outperformed other algorithms simply by adjusting these
    parameters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现代神经网络实践通常局限于实验节点和层的数量（例如，网络的深度）以及每层执行的操作类型。在许多成功的实例中，神经网络仅通过调整这些参数就超越了其他算法。
- en: To start off with, think about data entering a neural network system via the
    input layer, and then moving through the network from node to node. The path that
    data takes will depend on how interconnected the nodes are, the weights and the
    biases of each node, the kind of operations that are performed in each layer,
    and the state of the data at the end of such operations. Neural networks often
    require many **runs** (or epochs) in order to keep tuning the weights and biases
    of nodes, meaning that data flows over the different layers of the graph multiple
    times.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最开始，想象数据通过输入层进入神经网络系统，然后在网络中从节点到节点流动。数据流动的路径取决于节点之间的连接方式、每个节点的权重和偏置、每层所执行的操作类型以及在这些操作完成后的数据状态。神经网络通常需要多次**迭代**（或周期），以不断调整节点的权重和偏置，这意味着数据会在图的不同层之间流动多次。
- en: Configuring a Deep Learning Environment
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置深度学习环境
- en: Before we finish this chapter, we want you to interact with a real neural network.
    We will start by covering the main software components used throughout this book
    and make sure that they are properly installed. We will then explore a pre-trained
    neural network and explore a few of the components and operations discussed in
    the *What are Neural Networks?* section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束之前，我们希望你能够与一个真实的神经网络进行交互。我们将首先介绍本书中使用的主要软件组件，并确保它们正确安装。接着，我们将探索一个预训练的神经网络，并探讨在*什么是神经网络？*一节中讨论的一些组件和操作。
- en: Software Components for Deep Learning
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的软件组件
- en: 'We''ll use the following software components for deep learning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下软件组件进行深度学习：
- en: Python 3
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 3
- en: We will be using Python 3 in this book. Python is a general-purpose programming
    language that is very popular with the scientific community—hence its adoption
    in deep learning. Python 2 is not supported in this book but can be used to train
    neural networks instead of Python 3\. Even if you chose to implement your solutions
    in Python 2, consider moving to Python 3 as its modern feature set is far more
    robust than that of its predecessor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将使用Python 3。Python是一种通用编程语言，在科学界非常流行——因此它在深度学习中得到了广泛采用。本书不支持Python 2，但可以用它来训练神经网络，代替Python
    3。即使你选择在Python 2中实现你的解决方案，也建议迁移到Python 3，因为其现代化的特性集远比其前身更为强大。
- en: TensorFlow
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow is a library used for performing mathematical operations in the form
    of graphs. TensorFlow was originally developed by Google, and today, it is an
    open source project with many contributors. It has been designed with neural networks
    in mind and is among the most popular choices when creating deep learning algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个用于执行图形形式数学运算的库。TensorFlow最初由Google开发，今天它已经成为一个开源项目，拥有众多贡献者。它的设计是为了神经网络而构建的，也是创建深度学习算法时最受欢迎的选择之一。
- en: TensorFlow is also well known for its production components. It comes with TensorFlow
    Serving ([https://github.com/tensorflow/serving](https://github.com/tensorflow/serving)),
    a high-performance system for serving deep learning models. Also, trained TensorFlow
    models can be consumed in other high-performance programming languages such as
    Java, Go, and C. This means that you can deploy these models on anything from
    a micro-computer (that is, a Raspberry Pi) to an Android device. As of November
    2019, TensorFlow version 2.0 is the latest version.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow也以其生产组件而著名。它包含TensorFlow Serving（[https://github.com/tensorflow/serving](https://github.com/tensorflow/serving)），这是一个用于服务深度学习模型的高性能系统。此外，训练好的TensorFlow模型可以在其他高性能编程语言中使用，如Java、Go和C。这意味着你可以将这些模型部署在从微型计算机（即树莓派）到安卓设备等各种设备上。截止2019年11月，TensorFlow
    2.0版本是最新版本。
- en: Keras
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras
- en: In order to interact efficiently with TensorFlow, we will be using Keras ([https://keras.io/](https://keras.io/)),
    a Python package with a high-level API for developing neural networks. While TensorFlow
    focuses on components that interact with each other in a computational graph,
    Keras focuses specifically on neural networks. Keras uses TensorFlow as its backend
    engine and makes developing such applications much easier.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地与TensorFlow交互，我们将使用Keras（[https://keras.io/](https://keras.io/)），这是一个具有高级API的Python包，用于开发神经网络。虽然TensorFlow专注于相互交互的计算图中的组件，Keras则专注于神经网络的开发。Keras使用TensorFlow作为其后台引擎，使得开发此类应用变得更加简便。
- en: As of November 2019, Keras is the built-in and default API of TensorFlow. It
    is available under the `tf.keras` namespace.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2019年11月，Keras是TensorFlow的内置和默认API。它位于`tf.keras`命名空间下。
- en: TensorBoard
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorBoard
- en: TensorBoard is a data visualization suite for exploring TensorFlow models and
    is natively integrated with TensorFlow. TensorBoard works by consuming the checkpoint
    and summary files created by TensorFlow as it trains a neural network. Those can
    be explored either in near real time (with a 30-second delay) or after the network
    has finished training. TensorBoard makes the process of experimenting with and
    exploring a neural network much easier—plus, it's quite exciting to follow the
    training of your network.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一个数据可视化工具套件，用于探索TensorFlow模型，并且与TensorFlow原生集成。TensorBoard通过读取TensorFlow在训练神经网络时创建的检查点和摘要文件来工作。这些文件可以在近实时（延迟30秒）或训练完成后进行探索。TensorBoard使得实验和探索神经网络的过程变得更加轻松——而且，跟踪你网络的训练过程是非常激动人心的。
- en: Jupyter Notebook, Pandas, and NumPy
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jupyter Notebook、Pandas和NumPy
- en: 'When working to create deep learning models with Python, it is common to start
    working interactively; slowly developing a model that eventually turns into more
    structured software. Three Python packages are used frequently during this process:
    Jupyter Notebooks, Pandas, and NumPy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Python 创建深度学习模型时，通常会从交互式工作开始；逐步开发一个最终变成更具结构化的软件的模型。在此过程中，经常使用三种 Python 包：Jupyter
    Notebooks、Pandas 和 NumPy：
- en: Jupyter Notebook create interactive Python sessions that use a web browser as
    their interface.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook 创建交互式 Python 会话，使用网页浏览器作为其界面。
- en: Pandas is a package for data manipulation and analysis.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas 是一个用于数据处理和分析的包。
- en: NumPy is frequently used for shaping data and performing numerical computations.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 常用于数据形状处理和执行数值计算。
- en: These packages are used occasionally throughout this book. They typically do
    not form part of a production system but are often used when exploring data and
    starting to build a model. We'll focus on the other tools in much more detail.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些软件包在本书中偶尔使用。它们通常不构成生产系统的一部分，但在探索数据和开始构建模型时经常使用。我们将更详细地关注其他工具。
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The books *Learning pandas* by Michael Heydt (June 2017, Packt Publishing),
    available at [https://www.packtpub.com/big-data-and-business-intelligence/learning-pandas-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-pandas-second-edition),
    and *Learning Jupyter* by Dan Toomey (November 2016, Packt Publishing), available
    at [https://www.packtpub.com/big-data-and-business-intelligence/learning-jupyter-5-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-jupyter-5-second-edition),
    both offer comprehensive guides on how to use these technologies. These books
    are good references for continuing to learn more.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Heydt（2017年6月，Packt Publishing）出版的《*学习 Pandas*》和 Dan Toomey（2016年11月，Packt
    Publishing）出版的《*学习 Jupyter*》是两本关于如何使用这些技术的综合指南。它们都可以在[https://www.packtpub.com/big-data-and-business-intelligence/learning-pandas-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-pandas-second-edition)和[https://www.packtpub.com/big-data-and-business-intelligence/learning-jupyter-5-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-jupyter-5-second-edition)找到。这些书籍是继续学习的好参考。
- en: 'The following table details the software requirements required for successfully
    creating the deep learning models explained in this book:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格详细列出了成功创建本书中解释的深度学习模型所需的软件要求：
- en: '![Figure 1.3: The software components necessary for creating a deep learning
    environment'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3：创建深度学习环境所需的软件组件]'
- en: '](img/B15911_01_03.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_03.jpg)'
- en: 'Figure 1.3: The software components necessary for creating a deep learning
    environment'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：创建深度学习环境所需的软件组件
- en: Anaconda is a free distribution of many useful Python packages for Windows,
    Mac or other platform. We recommend that you follow the instructions at [https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/).
    The standard Anaconda installation will install most of these components and the
    first exercise will work through how to install the others.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 是一个免费的 Python 包分发工具，适用于 Windows、Mac 或其他平台。我们建议您按照[https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/)上的说明进行操作。标准的
    Anaconda 安装将安装大多数这些组件，第一个练习将演示如何安装其他组件。
- en: 'Exercise 1.01: Verifying the Software Components'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.01：验证软件组件
- en: Before we explore a trained neural network, let's verify whether all the software
    components that we need are available. We have included a script that verifies
    whether these components work. Let's take a moment to run the script and deal
    with any eventual problems we may find. We will now be testing whether the software
    components required for this book are available in your working environment. First,
    we suggest the creation of a Python virtual environment using Python's native
    module `venv`. Virtual environments are used for managing project dependencies.
    We suggest each project you create has its own virtual environment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索一个训练好的神经网络之前，让我们先验证所需的软件组件是否齐全。我们已经包括了一个脚本，用来验证这些组件是否工作。让我们花点时间运行脚本并解决可能遇到的问题。我们现在将测试在您的工作环境中是否具备本书所需的软件组件。首先，我们建议使用
    Python 的原生模块`venv`来创建 Python 虚拟环境。虚拟环境用于管理项目依赖关系。我们建议每个项目都有自己的虚拟环境。
- en: 'A python virtual environment can be created by using the following command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下命令创建 Python 虚拟环境：
- en: '[PRE0]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The latter command will append the string `venv` at the beginning of the command
    line.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后面的命令将把字符串`venv`附加到命令行的开头。
- en: Make sure you always activate your Python virtual environment when working on
    a project. To deactivate your virtual environment, run `$ deactivate`.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保在进行项目开发时始终激活你的 Python 虚拟环境。要停用虚拟环境，请运行 `$ deactivate`。
- en: After activating your virtual environment, make sure that the right components
    are installed by executing `pip` over the `requirements.txt` file ([https://packt.live/300skHu](https://packt.live/300skHu)).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活虚拟环境后，请通过在 `requirements.txt` 文件上执行 `pip`，确保正确的组件已安装（[https://packt.live/300skHu](https://packt.live/300skHu)）。
- en: '[PRE1]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.4: A screenshot of a Terminal running pip to install dependencies
    from requirements.txt'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.4：终端截图，显示使用 pip 安装 requirements.txt 中的依赖'
- en: '](img/B15911_01_04.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_01_04.jpg)'
- en: 'Figure 1.4: A screenshot of a Terminal running pip to install dependencies
    from requirements.txt'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.4：终端截图，显示使用 pip 安装 requirements.txt 中的依赖
- en: 'This will install the libraries used in this book in that virtual environment.
    It will do nothing if they are already available. If the library is getting installed,
    a progress bar will be shown, else it will notify that ''`requirement is already
    specified`''. To check the available libraries installed, please use the following
    command:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将会在虚拟环境中安装本书中使用的库。如果这些库已经存在，它将不会做任何操作。如果库正在安装，将显示进度条，否则会提示“`requirement is
    already specified`”。要检查已安装的可用库，请使用以下命令：
- en: '[PRE2]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output will be as follows:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.5: A screenshot of a Terminal running pip to list the available
    libraries'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.5：终端截图，显示运行 pip 列出可用库'
- en: '](img/B15911_01_05.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_01_05.jpg)'
- en: 'Figure 1.5: A screenshot of a Terminal running pip to list the available libraries'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.5：终端截图，显示运行 pip 列出可用库
- en: Note
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: These libraries are essential for working with all the code activities in this
    book.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些库对于本书中的所有代码活动至关重要。
- en: 'As a final step in this exercise, execute the script `test_stack.py`. This
    can be found at: [https://packt.live/2B0JNau](https://packt.live/2B0JNau) It verifies
    that all the required packages for this book are installed and available in your
    system.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为此练习的最后一步，执行脚本 `test_stack.py`。该文件可以在以下地址找到：[https://packt.live/2B0JNau](https://packt.live/2B0JNau)。它验证本书所需的所有软件包是否已安装并且可用。
- en: 'Run the following script to check if the dependencies of Python 3, TensorFlow,
    and Keras are available. Use the following command:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下脚本检查 Python 3、TensorFlow 和 Keras 的依赖项是否可用。使用以下命令：
- en: '[PRE3]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The script returns helpful messages stating what is installed and what needs
    to be installed:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 脚本返回有用的消息，说明已安装的内容以及需要安装的内容：
- en: '![Figure 1.6: A screenshot of a Terminal displaying that not all the requirements
    are installed'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.6：终端截图，显示并非所有要求都已安装'
- en: '](img/B15911_01_06.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_01_06.jpg)'
- en: 'Figure 1.6: A screenshot of a Terminal displaying that not all the requirements
    are installed'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.6：终端截图，显示并非所有要求都已安装
- en: 'For example, in the preceding screenshot, it shows that TensorFlow 2.0 is not
    detected but Keras 2.2 or higher is detected. Hence you are shown the error message
    `Please review software requirements before proceeding to Lesson 2`. If all the
    requirements are fulfilled, then it will show Python, TensorFlow, and Keras as
    installed, as shown in the following screenshot:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在前面的截图中，它显示 TensorFlow 2.0 未被检测到，但检测到了 Keras 2.2 或更高版本。因此，你会看到错误消息 `Please
    review software requirements before proceeding to Lesson 2`。如果所有要求都已满足，则会显示已安装
    Python、TensorFlow 和 Keras，如下图所示：
- en: '![Figure 1.7: A screenshot of the Terminal displaying that all elements are
    installed'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.7：显示所有元素已安装的终端截图'
- en: '](img/B15911_01_07.jpg)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_01_07.jpg)'
- en: 'Figure 1.7: A screenshot of the Terminal displaying that all elements are installed'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.7：显示所有元素已安装的终端截图
- en: 'Run the following script command in your Terminal to find more information
    on how to configure TensorBoard:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行以下脚本命令，获取有关如何配置 TensorBoard 的更多信息：
- en: '[PRE4]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.8: An output of the --help command'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.8：--help 命令的输出'
- en: '](img/B15911_01_08.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_08.jpg)'
- en: 'Figure 1.8: An output of the --help command'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：--help 命令的输出
- en: You should see the relevant help messages that explain what each command does,
    as in *Figure 1.8*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到相关的帮助信息，解释每个命令的作用，如 *图 1.8* 所示。
- en: As you can see in the figure above, the script returns messages informing you
    that all dependencies are installed correctly.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，脚本返回消息，通知你所有依赖项已正确安装。
- en: Note
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2B0JNau](https://packt.live/2B0JNau).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参阅[https://packt.live/2B0JNau](https://packt.live/2B0JNau)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需要在本地运行。
- en: Once we have verified that Python 3, TensorFlow, Keras, TensorBoard, and the
    packages outlined in `requirements.txt` have been installed, we can continue to
    a demo on how to train a neural network and then go on to explore a trained network
    using these same tools.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确认安装了 Python 3、TensorFlow、Keras、TensorBoard 和 `requirements.txt` 中列出的所有包，就可以继续进行如何训练神经网络的演示，然后使用这些相同的工具来探索训练好的网络。
- en: Exploring a Trained Neural Network
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索训练好的神经网络
- en: In this section, we'll explore a trained neural network. We'll do this to understand
    how a neural network solves a real-world problem (predicting handwritten digits)
    and to get familiar with the TensorFlow API. When exploring this neural network,
    we will recognize many components introduced in previous sections, such as nodes
    and layers, but we will also see many that we don't recognize (such as activation
    functions); we will explore those in further sections. We will then walk through
    an exercise on how that neural network was trained and then train that same network
    ourselves.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索一个训练好的神经网络。我们这样做是为了理解神经网络如何解决实际问题（预测手写数字），并熟悉 TensorFlow API。在探索这个神经网络时，我们将识别出许多在前面章节中介绍过的组件，比如节点和层，但也会看到许多我们不认识的组件（比如激活函数）；我们将在后续章节中进一步探索这些组件。接着，我们将演示如何训练这个神经网络，并自己动手训练同样的网络。
- en: The network that we will be exploring has been trained to recognize numerical
    digits (integers) using images of handwritten digits. It uses the MNIST dataset
    ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)), a classic
    dataset frequently used for exploring pattern recognition tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的神经网络已经过训练，能够识别数字（整数），使用的是手写数字的图像。它使用了 MNIST 数据集（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)），这是一个经典的数据集，常用于探索模式识别任务。
- en: The MNIST Dataset
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST 数据集
- en: The **Modified National Institute of Standards and Technology** (**MNIST**)
    dataset contains a training set of 60,000 images and a test set of 10,000 images.
    Each image contains a single handwritten number. This dataset, which is derived
    from one created by the US Government, was originally used to test different approaches
    to the problem of recognizing handwritten text by computer systems. Being able
    to do that was important for the purpose of increasing the performance of postal
    services, taxation systems, and government services. The MNIST dataset is considered
    too naïve for contemporary methods. Different and more recent datasets are used
    in contemporary research (for example, **Canadian Institute for Advanced Research**
    (**CIFAR**). However, the MNIST dataset is still very useful for understanding
    how neural networks work because known models can achieve a high level of accuracy
    with great efficiency.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改版国家标准与技术研究院**（**MNIST**）数据集包含 60,000 张训练图像和 10,000 张测试图像。每张图像包含一个手写的数字。这个数据集源自美国政府创建的一个数据集，最初用于测试不同的计算机系统识别手写文字的方法。能够做到这一点对于提高邮政服务、税务系统和政府服务的效率非常重要。MNIST
    数据集被认为对于当代方法来说过于简单。现在的研究中使用了不同且更新的数据集（例如，**加拿大高级研究院**（**CIFAR**）数据集）。然而，MNIST
    数据集仍然非常有用，因为已知的模型能够高效地达到很高的准确度，从而帮助理解神经网络的工作原理。'
- en: Note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The CIFAR dataset is a machine learning dataset that contains images organized
    in different classes. Different than the MNIST dataset, the CIFAR dataset contains
    classes from many different areas including animals, activities, and objects.
    The CIFAR dataset is available at [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR 数据集是一个机器学习数据集，包含按不同类别组织的图像。与 MNIST 数据集不同，CIFAR 数据集包含来自多个不同领域的类别，包括动物、活动和物体。CIFAR
    数据集可以在[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)获取。
- en: However, the MNIST dataset is still very useful for understanding how neural
    networks work because known models can achieve a high level of accuracy with great
    efficiency. In the following figure, each image is a separate 20x20-pixel image
    containing a single handwritten digit. You can find the original dataset at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MNIST 数据集仍然非常有助于理解神经网络的工作原理，因为已知模型能够以极高的效率达到很高的准确率。在下图中，每张图片都是一张包含单个手写数字的
    20x20 像素图像。你可以在 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
    找到原始数据集。
- en: '![Figure 1.9: An excerpt from the training set of the MNIST dataset'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9：MNIST 数据集训练集的摘录'
- en: '](img/B15911_01_09.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_09.jpg)'
- en: 'Figure 1.9: An excerpt from the training set of the MNIST dataset'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：MNIST 数据集训练集的摘录
- en: Training a Neural Network with TensorFlow
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 训练神经网络
- en: 'Now, let''s train a neural network to recognize new digits using the MNIST
    dataset. We will be implementing a special-purpose neural network called a **Convolutional
    Neural Network**(**CNN**) to solve this problem (we will discuss those in more
    detail in later sections). Our complete network contains three hidden layers:
    two fully connected layers and a convolutional layer. The model is defined by
    the following TensorFlow snippet of Python code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 MNIST 数据集训练一个神经网络来识别新的数字。我们将实现一个名为**卷积神经网络**（**CNN**）的特殊神经网络来解决这个问题（我们将在后续部分中更详细地讨论它们）。我们的完整网络包含三个隐藏层：两个全连接层和一个卷积层。模型由以下
    TensorFlow Python 代码片段定义：
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Use the `mnist.py` file for your reference at [https://packt.live/2Cuhj9w](https://packt.live/2Cuhj9w).
    Follow along by opening the script in your code editor.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `mnist.py` 文件作为参考，地址为 [https://packt.live/2Cuhj9w](https://packt.live/2Cuhj9w)。在代码编辑器中打开该脚本并跟随练习。
- en: We execute the preceding snippet of code only once during the training of our
    network.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练神经网络时只执行一次前面的代码片段。
- en: 'We will go into a lot more detail about each one of those components using
    Keras in *Chapter 2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting
    the Price of Bitcoin*. For now, we''ll focus on understanding that the network
    is altering the values of the `Weights` and `Biases` in each layer on every run.
    These lines of Python are the culmination of dozens of years of neural network
    research.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第二章*《*使用 TensorFlow 和 Keras 进行真实世界的深度学习：预测比特币的价格*》中详细探讨每个组件的使用。现在，我们将重点理解网络在每次运行时如何改变每一层的`权重`和`偏置`值。这些
    Python 代码是数十年神经网络研究的结晶。
- en: Now let's train that network to evaluate how it performs in the MNIST dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练这个网络，评估它在 MNIST 数据集上的表现。
- en: 'Exercise 1.02: Training a Neural Network Using the MNIST Dataset'
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.02：使用 MNIST 数据集训练神经网络
- en: 'In this exercise, we will train a neural network for detecting handwritten
    digits from the MNIST dataset. Execute the following steps to set up this exercise:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将训练一个神经网络，用于检测 MNIST 数据集中的手写数字。按照以下步骤来设置这个练习：
- en: Open two Terminal instances.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开两个终端实例。
- en: Navigate to [https://packt.live/2BWNAWK](https://packt.live/2BWNAWK). Ensure
    that your Python 3 virtual environment is active and that the requirements outlined
    in `requirements.txt` are installed.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://packt.live/2BWNAWK](https://packt.live/2BWNAWK)。确保你的 Python 3 虚拟环境已激活，并且已安装
    `requirements.txt` 中列出的所有依赖项。
- en: 'In one of them, start a TensorBoard server with the following command:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在其中一个终端中，使用以下命令启动 TensorBoard 服务器：
- en: '[PRE6]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.10: The TensorBoard server'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.10：TensorBoard 服务器'
- en: '](img/B15911_01_10.jpg)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_01_10.jpg)'
- en: '[PRE7]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When you start running the script, you will see the progress bar as follows:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你开始运行脚本时，你会看到如下的进度条：
- en: '![Figure 1.11: The result of the mnist.py script'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.11：mnist.py 脚本的结果'
- en: '](img/B15911_01_11.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_01_11.jpg)'
- en: 'Figure 1.11: The result of the mnist.py script'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.11：mnist.py 脚本的结果
- en: Open your browser and navigate to the TensorBoard URL provided when you start
    the server in *step 3*, it might be `http://localhost:6006/` or similar. In the
    Terminal where you ran the `mnist.py` script, you will see a progress bar with
    the epochs of the model. When you open the browser page, you will see a couple
    of graphs, `epoch_accuracy` and `epoch_loss` graphs. Ideally, the accuracy should
    improve with each iteration and the loss should decrease with each iteration.
    You can confirm this visually with the graphs.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器并导航到启动服务器时提供的 TensorBoard URL，在*步骤 3*中可能是 `http://localhost:6006/` 或类似的网址。在你运行
    `mnist.py` 脚本的终端中，你将看到模型的训练轮次进度条。当你打开浏览器页面时，你会看到几个图表，包括 `epoch_accuracy` 和 `epoch_loss`
    图表。理想情况下，准确度应随着每次迭代而提高，而损失应随着每次迭代而减少。你可以通过图表直观地确认这一点。
- en: 'Click the `epoch_accuracy` graph, enlarge it, and let the page refresh (or
    click on the `refresh` icon). You will see the model gaining accuracy as the epochs
    go by:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `epoch_accuracy` 图表，放大它，并让页面刷新（或点击 `refresh` 图标）。你将看到随着训练轮次的增加，模型的准确度逐渐提高：
- en: '![Figure 1.12: A visualization of the accuracy and loss graphs using TensorBoard'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12：使用 TensorBoard 可视化准确度和损失图表'
- en: '](img/B15911_01_12.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_12.jpg)'
- en: 'Figure 1.12: A visualization of the accuracy and loss graphs using TensorBoard'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：使用 TensorBoard 可视化准确度和损失图表
- en: We can see that after about 5 epochs (or steps), the network surpassed 97% accuracy.
    That is, the network is getting 97% of the digits in the test set correct by this
    point.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在大约 5 个训练轮次（或步骤）后，网络的准确度超过了 97%。也就是说，到这一点为止，网络已经正确识别了测试集中的 97% 数字。
- en: Note
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Cuhj9w](https://packt.live/2Cuhj9w).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取此特定部分的源代码，请访问 [https://packt.live/2Cuhj9w](https://packt.live/2Cuhj9w)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需在本地运行。
- en: Now, let's also test how well those networks perform with unseen data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试这些网络在未见过的数据上的表现。
- en: Testing Network Performance with Unseen Data
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用未见过的数据测试网络性能
- en: 'Visit the website [http://mnist-demo.herokuapp.com/](http://mnist-demo.herokuapp.com/)
    in your browser and draw a number between 0 and 9 in the designated white box:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中访问网站 [http://mnist-demo.herokuapp.com/](http://mnist-demo.herokuapp.com/)
    并在指定的白色框中画一个 0 到 9 之间的数字：
- en: '![Figure 1.13: A web application for manually drawing digits and testing the
    accuracy of two trained networks'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：一个用于手动绘制数字并测试两个训练网络准确度的 Web 应用程序'
- en: '](img/B15911_01_13.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_13.jpg)'
- en: 'Figure 1.13: A web application for manually drawing digits and testing the
    accuracy of two trained networks'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：一个用于手动绘制数字并测试两个训练网络准确度的 Web 应用程序
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This web application we are using was created by *Shafeen Tejani* to explore
    whether a trained network can correctly predict handwritten digits that we create.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的这个 Web 应用程序是由 *Shafeen Tejani* 创建的，目的是探索训练过的网络是否能够正确预测我们手写的数字。
- en: 'Source: [https://github.com/ShafeenTejani/mnist-demo](https://github.com/ShafeenTejani/mnist-demo).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://github.com/ShafeenTejani/mnist-demo](https://github.com/ShafeenTejani/mnist-demo)。
- en: 'In the application, you can see the results of two neural networks – a `1`
    close to the right edge of the drawing area, as shown in the following figure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序中，你可以看到两个神经网络的结果 —— 如下图所示，数字 `1` 被绘制在绘图区域的右边缘附近：
- en: '![Figure 1.14: Both networks have a difficult time estimating values drawn
    on'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：两个网络在估计绘制在区域边缘的值时遇到了困难'
- en: the edges of the area
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 区域的边缘
- en: '](img/B15911_01_14.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_01_14.jpg)'
- en: 'Figure 1.14: Both networks have a difficult time estimating values drawn on
    the edges of the area'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：两个网络在估计绘制在区域边缘的值时遇到了困难
- en: In this example, we see the number 1 drawn to the right side of the drawing
    area. The probability of this number being a 1 is 0 in both networks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们看到数字 1 被画在绘图区域的右侧。在两个网络中，该数字是 1 的概率为 0。
- en: The MNIST dataset does not contain numbers on the edges of images. Hence, neither
    network assigns relevant values to the pixels located in that region. Both networks
    are much better at classifying numbers correctly if we draw them closer to the
    center of the designated area. This is due to the fact that in the training set,
    we only had images with numbers drawn in the center of the image. This shows that
    neural networks can only be as powerful as the data that is used to train them.
    If the data used for training is very different than what we are trying to predict,
    the network will most likely produce disappointing results.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集的图像边缘没有数字。因此，两个网络都没有给位于该区域的像素分配相关的值。如果我们把数字画得更靠近指定区域的中心，两个网络在正确分类数字方面会表现得更好。这是因为在训练集中，我们只有图像中间部分绘制的数字。这表明神经网络的强大程度取决于用于训练的数据。如果用于训练的数据与我们要预测的目标差异很大，网络很可能会产生令人失望的结果。
- en: 'Activity 1.01: Training a Neural Network with Different Hyperparameters'
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.01：使用不同超参数训练神经网络
- en: In this section, we will explore the neural network that we trained during our
    work on *Exercise 1.02*, *Training a Neural Network Using the MNIST Dataset*,
    where we trained our own CNN on the MNIST dataset. We have provided that same
    trained network as binary files in the directory of this book. In this activity,
    we will just cover the things that you can do using TensorBoard and we will train
    several other networks by just changing some hyperparameters.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索在*练习 1.02*，*使用 MNIST 数据集训练神经网络*中训练的神经网络，那个时候我们在 MNIST 数据集上训练了自己的卷积神经网络（CNN）。我们已经将该训练好的网络作为二进制文件放在本书的目录中。在本活动中，我们将仅介绍你可以通过
    TensorBoard 完成的操作，并通过更改一些超参数来训练其他几个网络。
- en: 'Here are the steps you need to follow:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你需要遵循的步骤：
- en: Open TensorBoard by writing the appropriate command.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输入相应命令打开 TensorBoard。
- en: Open the TensorBoard accuracy graph and play with the values of smoothening
    sliders in scalar areas.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 TensorBoard 的准确率图表，并调整平滑滑块的值来观察变化。
- en: Train another model by changing the hyperparameters.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过改变超参数来训练另一个模型。
- en: Try decreasing the learning rate and increasing the number of epochs.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试减小学习率并增加训练轮数。
- en: Now try to understand what effect this hyperparameter tuning has on the graphs
    generated on TensorBoard.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在尝试理解超参数调优对在 TensorBoard 上生成的图表有什么影响。
- en: Try increasing the learning rate and decreasing the number of epochs and repeat
    *step 5*.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试增加学习率并减少训练轮数，然后重复*步骤 5*。
- en: 'Note:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：
- en: The solution for this activity can be found on page 130.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 130 页找到。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored a TensorFlow-trained neural network using TensorBoard
    and trained our own modified version of that network with different epochs and
    learning rates. This gave you hands-on experience of how to train a highly performant
    neural network and allowed you to explore some of its limitations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用 TensorFlow 训练的神经网络进行了 TensorBoard 探索，并用不同的训练轮数和学习率训练了我们自己修改版的网络。这让你获得了训练一个高效神经网络的实战经验，并且可以探索其一些局限性。
- en: 'Do you think we can achieve similar accuracy with real Bitcoin data? We will
    attempt to predict future Bitcoin prices using a common neural network algorithm
    in *Chapter 2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting
    the Price of Bitcoin*. In *Chapter 3*, *Real-World Deep Learning with TensorFlow
    and Keras: Evaluating the Bitcoin Model*, we will evaluate and improve that model,
    and finally, in *Chapter 4*, *Productization*, we will create a program that serves
    the prediction of that system via an HTTP API.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为我们能使用真实的比特币数据达到类似的准确度吗？我们将在*第二章*，*使用 TensorFlow 和 Keras 进行实际深度学习：预测比特币价格*中尝试使用常见的神经网络算法来预测未来的比特币价格。在*第三章*，*使用
    TensorFlow 和 Keras 进行实际深度学习：评估比特币模型*中，我们将评估并改进该模型，最后在*第四章*，*产品化*中，我们将创建一个程序，通过
    HTTP API 提供该系统的预测服务。
