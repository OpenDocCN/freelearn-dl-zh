- en: 4\. Getting Started with OpenAI and TensorFlow for Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 使用OpenAI和TensorFlow进行强化学习入门
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: 'This chapter introduces you to some key technologies and concepts to get started
    with reinforcement learning. You will become familiar with and use two OpenAI
    tools: Gym and Universe. You will learn how to deal with the interfaces of these
    environments and how to create a custom environment for a specific problem. You
    will build a policy network with TensorFlow, feed it with environment states to
    retrieve corresponding actions, and save the policy network weights. You will
    also learn how to use another OpenAI resource, Baselines, and use it to train
    a reinforcement learning agent to solve a classic control problem. By the end
    of this chapter, you will be able to use all the elements we will introduce to
    build and train an agent to play a classic Atari video game, thus achieving better-than-human
    performance.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一些关键技术和概念，帮助你入门强化学习。你将熟悉并使用两个OpenAI工具：Gym和Universe。你将学习如何处理这些环境的接口，以及如何为特定问题创建自定义环境。你将构建一个策略网络，使用TensorFlow，将环境状态输入其中以获取相应的动作，并保存策略网络的权重。你还将学习如何使用另一个OpenAI资源——Baselines，并用它来训练强化学习智能体解决经典的控制问题。在本章结束时，你将能够使用我们介绍的所有元素，构建并训练一个智能体，玩经典的Atari视频游戏，从而实现超越人类的表现。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, you were introduced to TensorFlow and Keras, along
    with an overview of their key features and applications and how they work in synergy.
    You learned how to implement a deep neural network with TensorFlow, addressing
    all major topics, that is, model creation, training, validation, and testing,
    using the most advanced machine learning frameworks available. In this chapter,
    we will use this knowledge to build models that are able to solve some classical
    reinforcement learning problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你已经了解了TensorFlow和Keras，以及它们的关键特性和应用，并了解了它们如何协同工作。你学习了如何使用TensorFlow实现深度神经网络，涵盖了所有主要主题，即模型创建、训练、验证和测试，使用的是最先进的机器学习框架。在本章中，我们将利用这些知识构建能够解决一些经典强化学习问题的模型。
- en: Reinforcement learning is a branch of machine learning that comes closest to
    the idea of artificial intelligence. The goal of training an artificial system
    to learn a given task, without any prior information, and only by means of experiences
    of an environment, represents the ambitious aim of replicating human learning.
    Applying deep learning techniques to the field has recently led to a great increase
    in performance, thus allowing us to solve problems in very different domains,
    from classic control problems to video games and even robotic locomotion. This
    chapter will introduce the various resources, methods, and tools you can use to
    become familiar with the context and problems typically encountered when getting
    started in the field. In particular, we will look at **OpenAI Gym** and **OpenAI
    Universe**, two libraries that allow us to easily create environments where we
    can train Reinforcement Learning (RL) agents, and OpenAI Baselines, a tool that
    provides a clear and simple interface for state-of-the-art reinforcement learning
    algorithms. By the end of this chapter, you will be able to leverage top libraries
    and modules to easily train a state-of-the-art reinforcement learning agent to
    solve classic control problems, as well as to achieve better-than-human performance
    on classic video games.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个分支，它最接近人工智能的理念。训练一个人工系统来学习某个任务，既没有任何先验信息，也只是通过与环境的互动经验来完成，这一目标代表了复制人类学习的雄心壮志。将深度学习技术应用于该领域，最近大大提高了性能，使我们能够解决各个领域的问题，从经典的控制问题到视频游戏，甚至是机器人运动控制。本章将介绍你可以使用的各种资源、方法和工具，帮助你熟悉在该领域起步时通常遇到的背景和问题。特别是，我们将关注**OpenAI
    Gym**和**OpenAI Universe**，这两个库允许我们轻松创建环境，以便训练强化学习（RL）智能体，以及OpenAI Baselines，一个为最先进的强化学习算法提供清晰简单接口的工具。在本章结束时，你将能够利用顶尖的库和模块，轻松训练一个最先进的强化学习智能体，解决经典的控制问题，并在经典的视频游戏中实现超越人类的表现。
- en: 'Now, let''s begin our journey, starting with the very first important concept:
    how to correctly model a proper reinforcement learning environment where we can
    train an agent. For this, we will be using OpenAI Gym and Universe.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始我们的旅程，从第一个重要的概念开始：如何正确地建模一个适合强化学习的环境，以便我们可以训练一个智能体。为此，我们将使用OpenAI Gym和Universe。
- en: OpenAI Gym
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: In this section, we will study the OpenAI Gym tool. We will go through the motivations
    behind its creation and its main elements, learning how to interact with them
    to properly train a reinforcement learning algorithm to tackle state-of-the-art
    benchmark problems. Finally, we will build a custom environment with the same
    set of standardized interfaces.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习OpenAI Gym工具。我们将探讨它创建的动机及其主要元素，学习如何与它们互动，以正确训练强化学习算法，解决最前沿的基准问题。最后，我们将构建一个具有相同标准化接口的自定义环境。
- en: The role of shared standard benchmarks for machine learning algorithms is of
    paramount importance to measure performance and state-of-the-art improvements.
    While for supervised learning there have been many different examples since the
    early days of the discipline, the same is not true for the reinforcement learning
    field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 共享标准基准对于衡量机器学习算法的性能和最新进展至关重要。虽然在监督学习领域，自学以来已有许多不同的示例，但在强化学习领域却并非如此。
- en: 'With the aim of fulfilling this need, in 2016, OpenAI released OpenAI Gym ([https://gym.openai.com/](https://gym.openai.com/)).
    It was conceived to be to reinforcement learning what standardized datasets such
    as ImageNet and COCO are to supervised learning: a standard, shared context in
    which the performance of RL methods can be directly measured and compared, both
    to identify the highest-achieving ones as well as to monitor current progress.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这一需求，OpenAI于2016年发布了OpenAI Gym（[https://gym.openai.com/](https://gym.openai.com/)）。它的构思是让它成为强化学习领域的标准，就像ImageNet和COCO数据集之于监督学习一样：一个标准的共享环境，强化学习方法的性能可以在其中直接衡量和比较，以识别出最优方法并监控当前的进展。
- en: OpenAI Gym acts as an interface between the typical Markov decision process
    formulation of the reinforcement learning problem and a variety of environments,
    covering different types of problems the agent has to solve (from classic control
    to Atari video games), as well as different observations and action spaces. Gym
    is completely independent of the structure of the agent that will be interfaced
    with, as well as the machine learning framework used to build and run it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym作为强化学习问题的典型马尔科夫决策过程（MDP）模型和多种环境之间的接口，涵盖了智能体必须解决的不同类型问题（从经典控制到Atari视频游戏），以及不同的观察和动作空间。Gym完全独立于与之接口的智能体结构以及用于构建和运行智能体的机器学习框架。
- en: 'Here is the list of environment categories Gym offers, ranging from easy to
    difficult and involving many different kinds of data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Gym提供的环境类别列表，涵盖从简单到困难的任务，涉及多种不同的数据类型：
- en: '**Classic control and toy text**: Small-scale easy tasks, frequently found
    in reinforcement learning literature. These environments are the best place to
    start in order to gain confidence with Gym and to familiarize yourself with agent
    training.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典控制与玩具文本**：小规模、简单的任务，常见于强化学习文献中。这些环境是开始熟悉Gym并与智能体训练建立信心的最佳场所。'
- en: 'The following figure shows an example of classic control problem of CartPole:'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了经典控制问题的一个示例——CartPole：
- en: '![Figure 4.1: Classic control problem- CartPole'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.1：经典控制问题 - CartPole'
- en: '](img/B16182_04_01.jpg)'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_01.jpg)'
- en: 'Figure 4.1: Classic control problem- CartPole'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：经典控制问题 - CartPole
- en: 'The following figure shows an example of classic control problem of MountainCar:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了经典控制问题的一个示例——MountainCar：
- en: '![Figure 4.2: Classic control problem - Mountain Car'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：经典控制问题 - 山地车'
- en: '](img/B16182_04_02.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_02.jpg)'
- en: 'Figure 4.2: Classic control problem - Mountain Car'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：经典控制问题 - 山地车
- en: '**Algorithmic**: In these environments, the system has to learn, autonomously
    and purely from examples, to perform computations ranging from multi-digit additions
    to alphanumeric-character sequence reversal.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：在这些环境中，系统必须从示例中自学，独立地执行计算任务，从多位数加法到字母数字字符序列反转等。'
- en: 'The following figure shows screenshot representing instances of the algorithmic
    problem set:'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了代表算法问题集实例的截图：
- en: '![Figure 4.3: Algorithmic problem – copying multiple instances of the input
    sequence'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.3：算法问题 - 复制输入序列的多个实例'
- en: '](img/B16182_04_03.jpg)'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_03.jpg)'
- en: 'Figure 4.3: Algorithmic problem – copying multiple instances of the input sequence'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：算法问题 - 复制输入序列的多个实例
- en: 'The following figure shows screenshot representing instances of the algorithmic
    problem set:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了代表算法问题集实例的截图：
- en: '![Figure 4.4: Algorithmic problem - copying instance of the input sequence'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4：算法问题——复制输入序列的实例'
- en: '](img/B16182_04_04.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_04.jpg)'
- en: 'Figure 4.4: Algorithmic problem - copying instance of the input sequence'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：算法问题——复制输入序列的实例
- en: '**Atari**: Gym integrates the **Arcade Learning Environment** (**ALE**), a
    software library that provides an interface we can use to train an agent to play
    classic Atari video games. It played a major role in helping reinforcement learning
    research achieve outstanding results.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**雅达利**：Gym 集成了 **Arcade Learning Environment**（**ALE**），这是一个软件库，提供了一个接口，可以用来训练代理玩经典的雅达利视频游戏。它在强化学习研究中起到了重要作用，帮助实现了杰出的成果。'
- en: 'The following figure shows Atari video game Breakout, provided by ALE:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图展示了由 ALE 提供的雅达利视频游戏 Breakout：
- en: '![Figure 4.5: Atari video game of Breakout'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.5：雅达利视频游戏 Breakout'
- en: '](img/B16182_04_05.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_05.jpg)'
- en: 'Figure 4.5: Atari video game of Breakout'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：雅达利视频游戏 Breakout
- en: 'The following figure shows Atari video game Pong, provided by ALE:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了由 ALE 提供的雅达利视频游戏 Pong：
- en: '![Figure 4.6: Atari video game of Pong'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6：雅达利视频游戏 Pong'
- en: '](img/B16182_04_06.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_06.jpg)'
- en: 'Figure 4.6: Atari video game of Pong'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：雅达利视频游戏 Pong
- en: Note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding figures have been sourced from the official documentation for
    OpenAI Gym. Please refer to the following link for more visual examples of Atari
    games: [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图像来自 OpenAI Gym 的官方文档。请参阅以下链接以获取更多雅达利游戏的视觉示例：[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)。
- en: '**MuJoCo and Robotics**: These environments expose typical challenges that
    are encountered in the field of robot control. Some of them take advantage of
    the MuJoCo physics engine, which was designed for fast and accurate robot simulation
    and offers free licenses for trial.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MuJoCo 和机器人技术**：这些环境展示了在机器人控制领域中常见的挑战。其中一些环境利用了 MuJoCo 物理引擎，MuJoCo 专为快速准确的机器人仿真而设计，并提供免费的试用许可证。'
- en: 'The following figure shows three MuJoCo environments, all of which provide
    a meaningful overview of robotic locomotion tasks:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图展示了三个 MuJoCo 环境，它们提供了机器人运动任务的有意义概述：
- en: '![Figure 4.7: Three MuJoCo-powered environments – Ant (left), Walker (center),'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.7：三个 MuJoCo 驱动的环境——Ant（左），Walker（中），'
- en: and Humanoid (right)
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 和 Humanoid（右）
- en: '](img/B16182_04_07.jpg)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_07.jpg)'
- en: 'Figure 4.7: Three MuJoCo-powered environments – Ant (left), Walker (center),
    and Humanoid (right)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：三个 MuJoCo 驱动的环境——Ant（左），Walker（中），Humanoid（右）
- en: Note
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding images have been sourced from the official documentation for
    OpenAI Gym. Please refer to the following link for more visual examples of MuJoCo
    environments: [https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图像来自 OpenAI Gym 的官方文档。请参阅以下链接以获取更多 MuJoCo 环境的视觉示例：[https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco)。
- en: 'The following figure shows two environments contained in the "Robotics" category,
    where RL agents are trained to perform robotic manipulation tasks:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下图展示了“机器人技术”类别中的两个环境，在这些环境中，强化学习代理被训练执行机器人操作任务：
- en: '![Figure 4.8: Two robotics environments – FetchPickAndPlace (left)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8：两个机器人环境——FetchPickAndPlace（左）'
- en: and HandManipulateEgg (Right)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 和 HandManipulateEgg（右）
- en: '](img/B16182_04_08.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_08.jpg)'
- en: 'Figure 4.8: Two robotics environments – FetchPickAndPlace (left) and HandManipulateEgg
    (Right)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：两个机器人环境——FetchPickAndPlace（左）和 HandManipulateEgg（右）
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding images have been sourced from the official documentation for
    OpenAI Gym. Please refer to the following link for more visual examples of Robotics
    environments: [https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图像来自 OpenAI Gym 的官方文档。请参阅以下链接以获取更多机器人环境的视觉示例：[https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics)。
- en: '**Third-party environments**: Environments developed by third parties are also
    available with a very broad landscape of applications, complexity, and data types
    ([https://github.com/openai/gym/blob/master/docs/environments.md#third-party-environments](https://github.com/openai/gym/blob/master/docs/environments.md#third-party-environments)).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三方环境**：第三方开发的环境也可以使用，涵盖了非常广泛的应用场景、复杂度和数据类型（[https://github.com/openai/gym/blob/master/docs/environments.md#third-party-environments](https://github.com/openai/gym/blob/master/docs/environments.md#third-party-environments)）。'
- en: How to Interact with a Gym Environment
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何与 Gym 环境互动
- en: 'In order to interact with a Gym environment, it has to, first of all, be created
    and initialized. The Gym module uses the `make` method, along with the ID of the
    environment as an argument, to create and return a new instance of it. To list
    all available environments in a given Gym installation, it is sufficient to just
    run the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与Gym环境交互，首先必须创建和初始化环境。Gym模块使用`make`方法，并将环境的ID作为参数，来创建并返回其新实例。要列出给定Gym安装中所有可用的环境，只需运行以下代码：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This prints out the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is a list of so-called `EnvSpec` objects. They define specific environment-related
    parameters, such as the goal to be achieved, the reward threshold defining when
    the task is considered solved, and the maximum number of steps allowed for a single
    episode.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个所谓的`EnvSpec`对象的列表。它们定义了特定的环境相关参数，比如要实现的目标、定义任务何时完成的奖励阈值，以及单个回合允许的最大步数。
- en: One interesting thing to note is that it is possible to easily add custom environments,
    as we will see later on. Thanks to this, a user can implement a custom problem
    using standard interfaces, making it straightforward for it to be tackled by standardized,
    off-the-shelf, reinforcement learning algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的地方是，可以轻松添加自定义环境，正如我们稍后将看到的那样。得益于这一点，用户可以使用标准接口实现自定义问题，从而使标准化的现成强化学习算法可以轻松处理这些问题。
- en: 'The fundamental elements of an environment are as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的基本元素如下：
- en: '**Observation** (object): An environment-specific object representing what
    can be observed of the environment; for example, the kinematic variables (that
    is, velocities and positions) of a mechanical system, pawn positions in a chess
    game, or the pixel frames of a video game.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察** (object)：一个特定于环境的对象，表示可以观察到的环境内容；例如，机械系统的运动学变量（即速度和位置）、棋盘游戏中的棋子位置，或视频游戏中的像素帧。'
- en: '**Actions** (object): An environment-specific object representing actions the
    agent can perform in the environment; for example, joint rotations and/or joint
    torques for a robot, a legal move in a board game, or buttons being pressed in
    combination for a video game.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作** (object)：一个特定于环境的对象，表示智能体可以在环境中执行的动作；例如，机器人的关节旋转和/或关节扭矩、棋盘游戏中的合法移动，或视频游戏中按下的多个按钮组合。'
- en: '**Reward** (float): The amount of reward achieved by executing the last step
    with the prescribed action. The reward range differs between different tasks,
    but in order to solve the environment, the aim is always to increase it, since
    this is what the RL agent tries to maximize.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励** (float)：通过执行指定动作完成最后一步所获得的奖励量。不同任务的奖励范围不同，但为了完成环境任务，目标始终是增加奖励，因为这是强化学习智能体试图最大化的内容。'
- en: '**Done** (bool): This indicates whether the episode has finished. If true,
    the environment needs to be reset. Most, but not all, tasks are divided into well-defined
    episodes, where a terminated episode may represent that the robot has fallen on
    the ground, the board game has reached a final state, or the agent lost its last
    life in a video game.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完成** (bool)：这表示回合是否已结束。如果为true，环境需要重置。大多数（但不是所有）任务被划分为明确的回合，其中一个终止回合可能表示机器人已摔倒、棋盘游戏已达到最终状态，或智能体在视频游戏中失去了最后一条生命。'
- en: '**Info** (dict): This contains diagnostic information on environment internals
    and is useful for both debugging purposes and for an RL agent training, even if
    it''s not allowed for standard benchmark comparisons.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息** (dict)：这包含了环境内部的诊断信息，对于调试和强化学习（RL）智能体训练都很有用，即使在标准基准测试比较中不允许使用。'
- en: 'The fundamental methods of an environment are as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的基本方法如下：
- en: '`reset()`: Input: none, output: observation. Resets the environment, bringing
    it to the starting point. It takes no input and outputs the corresponding observation.
    It has to be called right after environment creation and every time a final state
    is reached (`done` flag equal to `True`).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reset()`：输入：无，输出：观察。重置环境，将其带回起始点。该方法没有输入并输出相应的观察。它必须在环境创建后立即调用，并且每当达到最终状态时（`done`标志为`True`）都要调用。'
- en: '`step(action)`: Input: action, output: observation – reward – done – info.
    Advances the environment by one step, applying the selected input action. Returns
    the observation of the newly reached state, which is a reward associated with
    the transition from the previous to the new state under the selected action. The
    `done` flag is used to indicate whether the new state is a terminal one or not
    (`True/False`, respectively), as well as the `Info` dict with environment internals.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step(action)`：输入：动作，输出：观察 – 奖励 – 结束 – 信息。通过应用选定的输入动作，推进环境一步。返回新状态的观察结果，该状态是从先前状态到新状态的过渡所获得的奖励。`done`标志用于指示新状态是否是终止状态（分别为`True/False`），以及包含环境内部信息的`Info`字典。'
- en: '`render()`: Input: none, output: environment rendering. Renders the environment
    and is used for visualization/presentation purposes only. It is not used during
    agent training, which only needs observations to know the environment''s state.
    For example, it presents robot movements via animation graphics or outputs a video
    game video stream.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`render()`：输入：无，输出：环境渲染。渲染环境，仅用于可视化/展示目的。在代理训练过程中不使用该功能，代理只需要观察来了解环境的状态。例如，它通过动画图形展示机器人运动，或输出视频游戏视频流。'
- en: '`close()`: Input: none, output: none. Shuts down the environment gracefully.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close()`：输入：无，输出：无。优雅地关闭环境。'
- en: These elements allow us to have complete interaction with the environment simply
    by executing it with random inputs, training an agent, and running it. It is,
    in fact, an implementation of the standard reinforcement learning contextualization,
    which is described by the agent-environment interaction. For each timestep, the
    agent executes an action. This interaction with the environment causes a transition
    from the current state to a new state, resulting in an observation of the new
    state and a reward, which are returned as results. As a preliminary step, the
    following exercise shows how to create a CartPole environment, reset it, run it
    for 1,000 steps while randomly sampling one action for each step, and finally
    close it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素使我们能够通过执行随机输入、训练代理并运行它，简单地与环境进行完全交互。事实上，这是标准强化学习情境化的实现，描述了代理与环境的交互。在每个时间步，代理执行一个动作。与环境的这种交互会导致从当前状态到新状态的过渡，产生对新状态的观察和奖励，并将这些作为结果返回。作为初步步骤，以下练习展示了如何创建一个CartPole环境，重置它，在每个步骤随机采样一个动作后运行1,000步，最后关闭它。
- en: 'Exercise 4.01: Interacting with the Gym Environment'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.01：与Gym环境交互
- en: 'In this exercise, we will familiarize ourselves with the Gym environment by
    looking at a classic control example, CartPole. Follow these steps to complete
    this exercise:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将通过查看经典控制示例CartPole来熟悉Gym环境。按照以下步骤完成此练习：
- en: 'Import the OpenAI Gym module:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入OpenAI Gym模块：
- en: '[PRE2]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Instantiate the environment and reset it:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化环境并重置它：
- en: '[PRE3]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE4]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Run the environment for `1000` steps, rendering it and resetting it if a terminal
    state is encountered. After all steps are completed, close the environment:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行环境`1000`步，在遇到终止状态时渲染并重置。如果所有步骤完成，关闭环境：
- en: '[PRE5]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It renders the environment and plays it for 1,000 steps. The following figure
    shows one frame that was extracted from step number 12 of the entire sequence:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它渲染环境并播放1,000步。下图展示了从整个序列的第12步中提取的一帧：
- en: '![Figure 4.9: One frame of the 1,000 rendered steps for the CartPole environment'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9：CartPole环境中1,000步渲染的其中一帧'
- en: '](img/B16182_04_09.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_09.jpg)'
- en: 'Figure 4.9: One frame of the 1,000 rendered steps for the CartPole environment'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：CartPole环境中1,000步渲染的其中一帧
- en: Note
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/30yFmOi](https://packt.live/30yFmOi).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参考[https://packt.live/30yFmOi](https://packt.live/30yFmOi)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，需要在本地运行。
- en: This shows that the black cart can move along its rail (the horizontal line),
    with its pole fixed on the cart with a hinge that allows it to rotate freely.
    The goal is to control the cart while pushing it left and right in order to maintain
    the pole's vertical equilibrium, as seen in the preceding figure.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明黑色小车可以沿着轨道（水平线）移动，杆子通过铰链固定在小车上，允许它自由旋转。目标是控制小车，左右推动它，以保持杆子的垂直平衡，如前图所示。
- en: Action and Observation Spaces
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作空间和观察空间
- en: In order to appropriately interact with an environment and train an agent on
    it, a fundamental initial step is to familiarize yourself with its action and
    observation spaces. For example, in the preceding exercise, the action was randomly
    sampled from the environment's action space.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与环境适当地交互并在其上训练智能体，一个基本的初步步骤是熟悉其动作空间和观察空间。例如，在前面的练习中，动作是从环境的动作空间中随机采样的。
- en: 'Every environment is characterized by `action_space` and `observation_space`,
    which are instances of the `Space` class that describe the actions and observations
    required by Gym. The following snippet prints them out for the `CartPole` environment:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每个环境都由 `action_space` 和 `observation_space` 特征定义，它们是 `Space` 类的实例，描述了 Gym 所要求的动作和观察。以下代码片段打印了
    `CartPole` 环境的相关信息：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This outputs the following two rows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下两行：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `Discrete` space represents the set of non-negative integer numbers (natural
    numbers plus 0). Its dimension defines which numbers represent valid actions.
    For example, in the `CartPole` case, it is of dimension `2` because the agent
    can only push the cart left and right, so the admissible values are 0 or 1\. The
    `Box` space can be thought of as an n-dimensional array. In the `CartPole` case,
    the system state is defined by four variables: cart position and velocity, and
    pole angle with respect to the vertical and angular velocity. So, the "box observation"
    space dimension is equal to 4, and valid observations will be an array of four
    real numbers. In the latter case, it is useful to check their upper and lower
    bounds. This can be done as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discrete` 空间表示非负整数集合（自然数加上 0）。它的维度定义了哪些数字代表有效的动作。例如，在 `CartPole` 案例中，它的维度是
    `2`，因为智能体只能将小车向左或向右推，因此可接受的值为 0 或 1。`Box` 空间可以看作是一个 n 维数组。在 `CartPole` 案例中，系统状态由四个变量定义：小车位置和速度，以及杆子相对于竖直方向的角度和角速度。因此，“盒子观察”空间的维度为
    4，有效的观察将是一个包含四个实数的数组。在后面的情况下，检查其上下限是很有用的。可以通过以下方式进行：'
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This prints out the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它会打印出如下内容：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With these new elements, it is possible to write a more complete snippet to
    interact with the environment, using all the previously presented interfaces.
    The following code shows a complete loop, executing `20` episodes, each for `100`
    steps, rendering the environment, retrieving observations, and printing them out
    while taking random actions and resetting once it reaches a terminal state:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些新元素，就可以编写一个更完整的代码片段来与环境交互，利用所有之前介绍的接口。以下代码展示了一个完整的循环，执行 `20` 轮，每轮 `100`
    步，渲染环境，获取观察结果，并在执行随机动作时打印它们，一旦到达终止状态则重置：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code runs the environment for `20` episodes of `100` steps each,
    also rendering the environment, as we saw in *Exercise 4.01*, *Interacting with
    the Gym Environment*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码运行了 `20` 轮，每轮 `100` 步，同时呈现环境，正如我们在 *练习 4.01*，*与 Gym 环境交互* 中所见。
- en: Note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the preceding case, we run each episode for 100 steps instead of 1,000, as
    we did previously. There is no particular reason for doing so, but we are running
    20 different episodes, not a single one, so we opted for 100 steps to keep the
    code execution time short enough.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的案例中，我们将每轮运行 100 步，而不是之前的 1,000 步。这样做没有特别的原因，只是因为我们运行了 20 个不同的轮次，而不是一个，所以我们选择
    100 步以保持代码执行时间足够短。
- en: 'In addition to that, this code also prints out the sequence of observations,
    as returned by the environment, for each step performed. The following are a few
    lines that are received as output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，这段代码还会打印出每一步操作后环境返回的观察序列。以下是一些作为输出的行：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Looking at the previous code example, we can see how, for now, the action choice
    is completely random. It is right here that a trained agent would make a difference:
    it should choose actions based on environment observations, thus appropriately
    responding to the state it finds itself in. So, revising the previous code by
    substituting a trained agent in place of a random action choice looks as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的代码示例来看，我们可以看到目前的动作选择是完全随机的。正是在这一点上，经过训练的智能体会有所不同：它应该根据环境观察选择动作，从而恰当地响应它所处的状态。因此，通过用经过训练的智能体替换随机动作选择来修改之前的代码，如下所示：
- en: 'Import the OpenAI Gym and CartPole modules:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 OpenAI Gym 和 CartPole 模块：
- en: '[PRE12]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run `20` episodes of `100` steps each:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `20` 轮，每轮 `100` 步：
- en: '[PRE13]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Render the environment and print the observation:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境并打印观察结果：
- en: '[PRE14]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Use the agent''s knowledge to choose the action, given the current environment
    state:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用智能体的知识来选择行动，前提是给定当前环境状态：
- en: '[PRE15]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step the environment:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步进环境：
- en: '[PRE16]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If successful, break the inner loop and start a new episode:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果成功，则跳出内部循环并开始一个新的回合：
- en: '[PRE17]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With a trained agent, actions will be chosen optimally since a function of the
    state that the agent is in, is used to maximize the expected reward. This code
    would result in an output similar to the previous one.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练好的智能体下，行动将被最优选择，因为会利用当前智能体所处状态的函数来最大化期望的奖励。此代码将产生类似于之前的输出。
- en: 'But how do we proceed and train an agent from scratch? As you will learn throughout
    this book, there are many different approaches and algorithms we can use to achieve
    this quite complex task. In general, they all need the following tuple of elements:
    current state, chosen action, reward obtained by performing the chosen action,
    and new state reached by performing the chosen action.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何从零开始训练一个智能体呢？正如你在本书中将学到的那样，存在许多不同的方法和算法可以用来实现这个相当复杂的任务。通常，它们都需要以下元素的元组：当前状态、选择的动作、执行选择动作后获得的奖励，以及执行选择动作后到达的新状态。
- en: 'So, elaborating again on the previous code snippet to introduce the agent training
    step, it would look like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于前面的代码片段再次展开，加入智能体训练步骤，代码如下所示：
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The only difference in this code with respect to the previous block is the
    following line:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个代码块的唯一区别在于以下这一行：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This refers to the agent training step. The purpose of this code is to give
    us a high-level idea of all the steps involved in training an RL agent in a given
    environment.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是智能体训练步骤。此代码的目的是让我们对训练一个RL智能体在给定环境中所涉及的所有步骤有一个高层次的了解。
- en: This is the high-level idea behind the method adopted to carry out reinforcement
    learning agent training with the Gym environment. It provides access to all the
    required details through a very clean standard interface, thus giving us access
    to an extremely large set of different problems against which measuring algorithms
    and techniques can be used.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是采用的方法背后的高层次思路，用于在Gym环境中进行强化学习智能体的训练。它通过一个非常简洁的标准接口提供访问所有所需的细节，从而使我们能够访问一个极其庞大的不同问题集，利用这些问题来衡量算法和技术的效果。
- en: How to Implement a Custom Gym Environment
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现一个自定义的Gym环境
- en: All the environments that are available through Gym are perfect for learning
    purposes, but eventually, you will need to train an agent to solve a custom problem.
    One good way to achieve this is to create a custom environment, specific to the
    problem domain.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Gym提供的所有环境都非常适合用于学习，但最终你需要训练一个智能体来解决一个自定义问题。一种实现这一目标的好方法是创建一个专门针对问题领域的自定义环境。
- en: In order to do so, a class derived from `gym.Env` must be created. It will implement
    all the objects and methods described in the previous section so that it supports
    the agent-world interaction cycle that's typical of any reinforcement learning
    setting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，必须创建一个派生自`gym.Env`的类。它将实现前一节中描述的所有对象和方法，以支持典型的强化学习环境中，智能体与环境之间的交互周期。
- en: 'The following snippet represents a frame guiding a custom environment''s development:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了一个框架，指导自定义环境的开发：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the constructor, `action_space` and `observation_space` are defined. As
    mentioned previously, they will contain all possible actions the agent can take
    in the environment and all environment data observable by the agent. They are
    to be attributed to the specific problem: in particular, `action_space` will reflect
    elements the agent can control to interact with the environment, while `observation_space`
    will contain all the variables we want the agent to consider when choosing the
    action.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，定义了`action_space`和`observation_space`。如前所述，它们将包含智能体在环境中可以执行的所有可能动作，以及智能体能够观察到的所有环境数据。它们将归属于特定问题：特别地，`action_space`将反映智能体可以控制的与环境互动的元素，而`observation_space`将包含我们希望智能体在选择行动时考虑的所有变量。
- en: The `reset` method will be called to periodically reset the environment to an
    initial state, typically after the first initialization and every time after the
    end of an episode. It will return the observation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset`方法将被调用以定期将环境重置为初始状态，通常在第一次初始化后以及每次回合结束后。它将返回观测值。'
- en: The `step` method receives an action as input and executes it. This will result
    in an environment transitioning from the current state to a new state. The observation
    related to the new state is returned. This is also the method where the reward
    is calculated as a result of the state transition generated by the action. The
    new state is checked to determine whether it is a terminal one, in which case,
    the `done` flag that's returned is set to `true`. As the last step, all useful
    internals are returned in the `info` dictionary.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`step` 方法接收一个动作作为输入并执行它。这将导致环境从当前状态过渡到新状态，并返回与新状态相关的观察结果。这也是奖励计算的方法，奖励是由动作产生的状态转换的结果。新状态会被检查是否为终止状态，如果是，返回的
    `done` 标志会被设置为 `true`。最后，所有有用的内部信息会以 `info` 字典的形式返回。'
- en: Finally, the `render` method is the one in charge of rendering the environment.
    Its complexity may range from being as simple as a print statement to being as
    complicated as rendering a 3D environment using OpenGL.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`render` 方法负责渲染环境。其复杂性可能从简单的打印语句到使用 OpenGL 渲染 3D 环境的复杂操作不等。
- en: In this section, we studied the OpenAI Gym tool. We had an overview that explained
    the context and motivations behind its conception, provided details about its
    main elements, and saw how to interact with the elements to properly train a reinforcement
    learning algorithm to tackle state-of-the-art benchmark problems. Finally, we
    saw how to build a custom environment with the same set of standardized interfaces.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了 OpenAI Gym 工具。我们概述了它的背景和构思动机，详细介绍了其主要元素，并展示了如何与这些元素交互，从而正确地训练一个强化学习算法来解决最前沿的基准问题。最后，我们展示了如何构建一个具有相同标准化接口的自定义环境。
- en: OpenAI Universe – Complex Environment
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Universe – 复杂环境
- en: 'OpenAI Universe was released by OpenAI a few months after Gym. It''s a software
    platform for measuring and training artificial general intelligence on different
    applications, ranging from video games to websites. It makes an AI agent able
    to use a computer as a human does: the environment state is represented by screen
    pixels and the actions are all operations that can be performed by operating a
    virtual keyboard and mouse.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Universe 是 OpenAI 在 Gym 发布几个月后推出的。它是一个软件平台，用于在不同应用程序上衡量和训练通用人工智能，应用领域涵盖从视频游戏到网站的各种内容。它使得
    AI 代理能够像人类一样使用计算机：环境状态通过屏幕像素表示，动作是所有可以通过操作虚拟键盘和鼠标执行的操作。
- en: With Universe, it is possible to adapt any program, thus transforming the program
    into a Gym environment. It executes the program using **Virtual Network Computing**
    (**VNC**) technology, a software technology that allows the remote control of
    a computer system via graphical desktop-sharing over a network, transmitting keyboard
    and mouse events and receiving screen frames. By mimicking execution behind a
    remote desktop, it doesn't need to access program memory states, customized source
    code, or have a set of APIs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Universe，可以将任何程序适配成 Gym 环境，从而将程序转化为 Gym 环境。它使用 **虚拟网络计算**（**VNC**）技术执行程序，这是一种允许通过网络共享图形桌面来远程控制计算机系统的软件技术，传输键盘和鼠标事件并接收屏幕帧。通过模拟远程桌面背后的执行，它不需要访问程序内存状态、定制源代码或特定的
    API。
- en: 'The following snippet shows how to use Universe in a simple Python program,
    where a scripted action is always executed in every step:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在一个简单的 Python 程序中使用 Universe，其中一个脚本化的动作会在每一步中执行：
- en: 'Import the OpenAI Gym and OpenAI Universe modules:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 OpenAI Gym 和 OpenAI Universe 模块：
- en: '[PRE21]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Instantiate the OpenAI Universe environment and reset it:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 OpenAI Universe 环境并重置它：
- en: '[PRE22]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Execute a prescribed action to interact with the environment and render it:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行预定的动作与环境进行交互并渲染环境：
- en: '[PRE23]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding code successfully runs a Flash game in the browser.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码成功地在浏览器中运行了一个 Flash 游戏。
- en: The goal behind Universe is to favor the development of an AI agent that's capable
    of applying its past experience to master complex new environments, which would
    represent a fundamental step in the quest for artificial general intelligence.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Universe 的目标是促进 AI 代理的发展，使其能够将过去的经验应用到掌握复杂的新环境中，这将是实现人工通用智能的一个关键步骤。
- en: Despite the great success of AI in recent years, all developed systems can still
    be considered "Narrow AI." This is because they can only achieve better-than-human
    performance in a limited domain. Building something with a general problem-solving
    ability on a par with human common sense requires overcoming the goal of carrying
    agent experience along when shifting to a completely new task. This would allow
    an agent to avoid training from scratch, randomly going through tens of millions
    of trials.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来人工智能取得了巨大成功，但所有开发的系统仍然可以被认为是“狭义人工智能”。这是因为它们只能在有限的领域内实现超过人类的表现。构建一个具有通用问题解决能力、与人类常识相当的系统，需要克服将代理经验带到全新任务中的目标。这将使代理避免从零开始训练，随机进行数千万次试验。
- en: Now, let's take a look at the infrastructure of OpenAI Universe.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 OpenAI Universe 的基础设施。
- en: OpenAI Universe Infrastructure
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Universe 基础设施
- en: 'The following diagram effectively describes how OpenAI Universe works: it exposes
    all its environments, which will be described in detail later, through a common
    interface: by leveraging VNC technology, it makes the environment act as a server
    and the agent as a client so that the latter operates a remote desktop by observing
    the pixels of a screen (observations of the environment) and producing keyboard
    and mouse commands (actions of the agent). VNC is a well-established technology
    and is the standard for interacting with computers remotely through the network,
    as in the case of cloud computing systems or decentralized infrastructures:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示有效描述了 OpenAI Universe 的工作原理：它通过一个通用接口公开所有环境，这些环境将在后文详细描述：通过利用 VNC 技术，它使环境充当服务器，代理充当客户端，后者通过观察屏幕的像素（环境的观察）并产生键盘和鼠标命令（代理的行动）来操作远程桌面。VNC
    是一项成熟的技术，是通过网络与计算机进行远程交互的标准技术，例如云计算系统或去中心化基础设施中的情况：
- en: '![Figure 4.10: VNC server-client Universe infrastructure'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10：VNC 服务器-客户端 Universe 基础设施'
- en: '](img/B16182_04_10.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_10.jpg)'
- en: 'Figure 4.10: VNC server-client Universe infrastructure'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：VNC 服务器-客户端 Universe 基础设施
- en: 'Universe''s implementation has some notable properties, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Universe 的实现具有以下一些显著特点：
- en: '**Generality**: By adopting the VNC interface, it doesn''t require emulators
    or access to a program''s source code or memory states, thus opening a relevant
    number of opportunities in fields such as computer games, web browsing, CAD software
    usage, and much more.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用性**：通过采用 VNC 接口，它无需模拟器或访问程序的源代码或内存状态，从而在计算机游戏、网页浏览、CAD 软件使用等领域开辟了大量机会。'
- en: '**Familiarity to humans**: It can be easily used by humans to provide baselines
    for AI algorithms, which are useful to initialize agents with human demonstrations
    recorded in the form of VNC traffic. For example, a human can solve one of the
    tasks provided by OpenAI Universe by using it through VNC and recording the corresponding
    traffic. Then, it can use it to train an agent, providing good examples of policies
    to learn from.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对人类的熟悉性**：人类可以轻松地使用它为 AI 算法提供基准，这对于通过记录 VNC 流量的方式初始化代理并提供人类演示非常有用。例如，人类可以通过
    VNC 使用 OpenAI Universe 提供的任务之一并记录相应的流量。然后，它可以用来训练代理，提供良好的策略学习范例。'
- en: '**Standardization**. Leveraging VNC technology ensures portability in all major
    operating systems that have VNC software by default.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：利用 VNC 技术确保了在所有主要操作系统中都具有可移植性，这些操作系统默认安装有 VNC 软件。'
- en: '**Easiness of debugging**: It is super easy to observe the agent during training
    or evaluation by simply connecting a client for visualization to the environment''s
    VNC shared server. Saving VNC traffic also helps.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调试的简便性**：通过简单地将客户端连接到环境的 VNC 共享服务器进行可视化，可以轻松地在训练或评估过程中观察代理的状态。节省 VNC 流量也很有帮助。'
- en: Environments
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: 'In this section, we will look at the most important categories of problems
    that are already available inside Universe. Each environment is composed of a
    Docker image and hosts a VNC server. The server has the role of the interface
    and is in charge of the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将探讨 Universe 中已经提供的最重要的几类问题。每个环境由一个 Docker 镜像组成，并托管一个 VNC 服务器。该服务器作为接口，负责以下任务：
- en: Sending observations (screen pixels)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送观察信息（屏幕像素）
- en: Receiving actions (keyboard/mouse commands)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收操作（键盘/鼠标命令）
- en: Providing information for reinforcement learning tasks (reward signal, diagnosis
    elements, and so on) through a Web Socket server
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Web Socket 服务器为强化学习任务提供信息（奖励信号、诊断元素等）
- en: Now, let's take a look at each of the different categories of environments.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看不同类别的环境。
- en: Atari Games
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Atari 游戏
- en: These are the classic Atari 2600 games from the ALE. Already encountered in
    OpenAI Gym, they are also part of Universe.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 ALE 中的经典 Atari 2600 游戏。在 OpenAI Gym 中已经遇到过，它们也是 Universe 的一部分。
- en: Flash Games
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Flash 游戏
- en: The landscape of Flash games offers a large number of games with more advanced
    graphics with respect to Atari, but still with simple mechanics and goals. Universe's
    initial release contained 1,000 Flash games, 100 of which also provided reward
    as a function.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Flash 游戏的景观提供了大量相较于 Atari 更先进图形的游戏，但仍然具备简单的机制和目标。Universe 的初始版本包含了 1,000 个 Flash
    游戏，其中 100 个还提供了作为功能的奖励。
- en: 'With the Universe approach, there is a major aspect to be addressed: how the
    agent knows how well it performed, which is related to the rewards returned by
    interacting with the environment. If you don''t have access to an application''s
    internal states (that is, its RAM addresses), the only way to do so is to extract
    such information from the onscreen pixels. Many games have a score associated
    with them, and this is printed out on each frame so that it can be parsed via
    some image processing algorithm. For example, Atari Pong shows both players''
    scores in the top part of the frame, so it is possible to parse those pixels to
    retrieve it. Universe developed a high-performing image-to-text model based on
    convolutional neural networks that''s embedded into the Python controller and
    runs inside a Docker container. On the environments where it can be applied, it
    retrieves the user''s score from the frame buffer and provides this information
    through the Web Socket''s score from the frame buffer, thus providing this information
    through the Web Socket.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Universe 方法中，有一个重要的方面需要解决：代理如何知道自己表现得如何，这与与环境交互后返回的奖励相关。如果你无法访问应用程序的内部状态（即其
    RAM 地址），那么唯一的方法就是从屏幕上的像素中提取这些信息。许多游戏都有与之关联的分数，这些分数会在每一帧中打印出来，可以通过某些图像处理算法解析。例如，Atari
    Pong 在画面的顶部显示两位玩家的分数，因此可以解析这些像素来获取分数。Universe 开发了一个基于卷积神经网络的高性能图像到文本模型，该模型嵌入到
    Python 控制器中，并在 Docker 容器内运行。在可以应用的环境中，它从帧缓存中提取用户的分数，并通过 Web Socket 提供这些信息，从帧缓存中获取并通过
    Web Socket 提供分数信息。
- en: Browser Tasks
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 浏览器任务
- en: 'Universe adds a unique set of tasks based on the usage of a web browser. These
    environments put the AI agent in front of a common web browser, presenting it
    with problems that require the use of the web: reading content, navigating through
    pages and clicking buttons while observing only pixels, and using the keyboard
    and mouse. Depending on the complexity, these tasks can, conceptually, be grouped
    into two categories: Mini World of Bits and real-world browser tasks:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Universe 基于使用网页浏览器的方式，增加了一组独特的任务。这些环境将 AI 代理放置在常见的网页浏览器前，呈现需要使用网络的问题：阅读内容、浏览网页和点击按钮，同时只观察像素，使用键盘和鼠标。根据任务的复杂度，从概念上看，这些任务可以分为两类：迷你比特世界和现实世界浏览器任务：
- en: '**Mini World of Bits**:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迷你比特世界**：'
- en: 'These environments are to browser-based tasks as what the MNIST dataset is
    to image recognition: they are basic building blocks that can be found on complex
    browsing problems on which training is easier but also insightful. They are environments
    of differing difficulty levels, for example, that you click on a specific button
    or reply to a message using an email client.'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些环境就像 MNIST 数据集对图像识别的作用一样，属于浏览器任务的基本构建块：它们是可以在复杂的浏览问题中找到的基础组件，训练更容易，但也富有洞察力。它们是难度各异的环境，例如，你需要点击特定的按钮或使用电子邮件客户端回复消息。
- en: '**Real-world browser tasks**:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现实世界浏览器任务**：'
- en: With respect to the previous category, these environments require the agent
    to solve more realistic problems, usually in the form of an instruction expressed
    to the agent, which has to perform a sequence of actions on a website. An example
    could be a request for an agent to book a specific flight that would require it
    to interact with the platform in order to find the right answer.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相对于前一类别，这些环境要求代理解决更现实的问题，通常以向代理发出的指令形式呈现，代理必须在网站上执行一系列操作。例如，要求代理预订特定航班，这需要它与平台进行交互，以找到正确的答案。
- en: Running an OpenAI Universe Environment
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 OpenAI Universe 环境
- en: 'Being a large collection of tasks that can be accessed via a common interface,
    running an environment requires performing only a few steps:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个通过公共接口可以访问的大量任务集合，运行环境只需要执行几个步骤：
- en: 'Install Docker and Universe, which can be done with the following command:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Docker和Universe，可以使用以下命令完成：
- en: '[PRE24]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Start a runtime, which is a server that groups a collection of similar environments
    into a "runtime" exposing two ports: `5900` and `15900`. Port `5900` is used for
    the VNC protocol to exchange pixel information or keyboard/mouse actions, while
    `15900` is used to maintain the `WebSocket` control protocol. The following snippet
    shows how to boot a runtime from a PC console (for example, a Linux shell):'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动一个运行时，这是一个将相似环境集合在一起的服务器，暴露两个端口：`5900`和`15900`。端口`5900`用于VNC协议交换像素信息或键盘/鼠标操作，而`15900`用于维护`WebSocket`控制协议。以下代码片段展示了如何从PC控制台启动运行时（例如Linux
    shell）：
- en: '[PRE25]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With this command, the Flash game's Docker container will be downloaded. You
    can then use a VNC viewer to view and control the created remote desktop. The
    target port is `5900`. It is also possible to use the browser-based VNC client
    through the web server using port `15900` and the password `openai`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令，Flash游戏的Docker容器将被下载。然后，您可以使用VNC查看器查看并控制创建的远程桌面。目标端口是`5900`。也可以通过Web服务器使用端口`15900`和密码`openai`使用基于浏览器的VNC客户端。
- en: 'The following snippet is the very same as the one we saw previously, except
    it only adds the VNC connection step. This means that the output is also the same,
    so it is not reported here. As we saw, writing a custom agent is quite straightforward.
    Observations include a NumPy pixel array, and actions are a list of VNC events
    (mouse/keyboard interactions):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段与我们之前看到的完全相同，唯一的区别是它新增了VNC连接步骤。这意味着输出结果也相同，因此这里不再重复报告。如我们所见，编写自定义代理非常简单。观察包括一个NumPy像素数组，操作是VNC事件（鼠标/键盘交互）的列表：
- en: '[PRE26]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Exploiting the same VNC connection, the user is able to watch the agent in action
    and also send action commands using the keyboard and mouse. The VNC interface,
    managing environments as server processes, allows us to run them on remote machines,
    thus allowing us to leverage in-house computation clusters or even cloud solutions.
    For more information, refer to the OpenAI Universe website ([https://openai.com/blog/universe/](https://openai.com/blog/universe/)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用相同的VNC连接，用户可以观看代理的操作，并使用键盘和鼠标发送操作命令。VNC界面将环境管理作为服务器进程，使我们能够在远程机器上运行环境，从而可以利用内部计算集群或云解决方案。更多信息，请参阅OpenAI
    Universe网站（[https://openai.com/blog/universe/](https://openai.com/blog/universe/)）。
- en: Validating the Universe Infrastructure
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证Universe基础设施
- en: One of the intrinsic problems of Universe is the associated lag in observations
    and execution of actions that comes with the choice of architecture. In fact,
    agents must operate in real time and are accountable for fluctuating action and
    observation delays. Most environments can't be solved with current techniques,
    but the creators of Universe performed tests to guarantee that it is actually
    possible for an RL agent to learn. During these tests, the reward trends during
    training for Atari games, Flash games, and browser tasks confirm that it is actually
    possible to obtain results even in such a complex setting.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Universe的一个固有问题是架构选择带来的观察和执行操作的延迟。事实上，代理必须实时运行，并对波动的操作和观察延迟负责。大多数环境目前无法用现有技术解决，但Universe的创建者进行了测试，确保强化学习代理确实能够学习。在这些测试中，针对Atari游戏、Flash游戏和浏览器任务的奖励趋势验证了即使在如此复杂的环境下，仍然能够取得成果。
- en: Now that we've introduced the OpenAI tools for reinforcement learning, we can
    now move on and learn how to use TensorFlow in this context.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了OpenAI的强化学习工具，我们可以继续学习如何在这个上下文中使用TensorFlow。
- en: TensorFlow for Reinforcement Learning
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于强化学习的TensorFlow
- en: In this section, we will learn how to create, run, and save a policy network
    using TensorFlow. Policy networks are one of the fundamental pieces, if not the
    most important one, of reinforcement learning. As will be shown throughout this
    book, they are a very powerful implementation of containers for the knowledge
    the agent has to learn, which tells them how to choose actions based on environment
    observations.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用TensorFlow创建、运行和保存一个策略网络。策略网络是强化学习的基础之一，甚至可以说是最重要的一部分。正如本书中所展示的，它们是知识容器的强大实现，帮助代理基于环境观察选择行动。
- en: Implementing a Policy Network Using TensorFlow
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现策略网络
- en: Building a policy network is not too different from building a common deep learning
    model. Its goal is to output the "optimal" action, given the input it receives,
    that represents the environment's observation. So, it acts as a link between the
    environment state and the optimal agent behavior associated with it. Being optimal
    here means doing what maximizes the cumulative expected reward of the agent.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 构建策略网络与构建常见的深度学习模型并没有太大的不同。它的目标是根据其接收到的输入输出“最佳”动作，这代表环境的观察。因此，它充当环境状态与与之关联的最优代理行为之间的链接。在这里，最优意味着做出最大化代理累计预期奖励的行为。
- en: 'To make things as clear as possible, we will focus on a specific problem here,
    but the same approach can be adopted to solve other tasks, such as controlling
    a robotic arm or teaching locomotion to a humanoid robot. We will see how to create
    a policy network for a classic control problem that will also be at the core of
    an exercise later in this chapter. This problem is the "CartPole" problem: the
    goal is to maintain the balance of the vertical pole so that it is upright at
    all times. Here, the only way to do this is by moving the cart along either direction
    of the x-axis. The following figure shows a frame from this problem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能清晰，我们将重点放在这里的一个特定问题上，但是相同的方法可以用来解决其他任务，比如控制机器人手臂或教授人形机器人步态。我们将看到如何为经典控制问题创建策略网络，这也将是本章后续练习的核心。这个问题是“CartPole”问题：目标是保持垂直杆的平衡，使其始终保持直立。在这里，唯一的方法是沿
    x 轴的任一方向移动小车。下图显示了这个问题的一个帧：
- en: '![Figure 4.11: CartPole control problem'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11：CartPole 控制问题'
- en: '](img/B16182_04_11.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_11.jpg)'
- en: 'Figure 4.11: CartPole control problem'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：CartPole 控制问题
- en: As we mentioned previously, the policy network links the observations of the
    environment with the actions that the agent can take. So, they act as the input
    and output, respectively.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，策略网络将环境的观察与代理可以采取的动作联系起来。因此，它们分别充当输入和输出。
- en: As we saw in the previous chapter, this is the first information that you need
    in order to build a neural network. To retrieve the input and output dimensions,
    you have to instantiate the environment (in this case, this is done via OpenAI
    Gym) and print out information about the observation and action spaces.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中看到的，这是构建神经网络所需的第一个信息。要检索输入和输出维度，您必须实例化环境（在本例中，通过 OpenAI Gym 实现），并打印关于观察和动作空间的信息。
- en: Let's perform this first task by completing the following exercise.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过完成以下练习来执行这个第一个任务。
- en: 'Exercise 4.02: Building a Policy Network with TensorFlow'
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.02：使用 TensorFlow 构建策略网络
- en: 'In this exercise, we will learn how to build a policy network with TensorFlow
    for a given Gym environment. We will learn how to take its observation space and
    action space into account, which constitute the input and output of the network,
    respectively. We will then create a deep learning model that is able to generate
    actions for the agent in the environment in response to environment observations.
    This network is the piece that needs to be trained and is the final goal of every
    RL algorithm. Follow these steps to complete this exercise:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何为给定的 Gym 环境使用 TensorFlow 构建策略网络。我们将学习如何考虑其观察空间和动作空间，这构成了网络的输入和输出。然后，我们将创建一个深度学习模型，该模型能够根据环境观察生成代理的行动。这个网络是需要训练的部分，也是每个强化学习算法的最终目标。按照以下步骤完成这个练习：
- en: 'Import the required modules:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块：
- en: '[PRE27]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Instantiate the environment:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化环境：
- en: '[PRE28]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Print out the action and observation spaces:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出动作和观测空间：
- en: '[PRE29]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This prints out the following:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印如下内容：
- en: '[PRE30]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Print out the action and observation space dimensions:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出动作和观测空间的维度：
- en: '[PRE31]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output will be as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE32]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see from the preceding output, the action space is a discrete space
    of dimension `2`, meaning it can take the value `0` or `1`. The observation space
    is of the `Box` type with a dimension of `4`, meaning it consists of four real
    numbers inside the lower and upper boundaries, which, as we already saw for the
    CartPole environment, are [`±2.4`, `± inf`, `±0.20943951`, `±inf`].
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您从前面的输出中可以看到的那样，动作空间是一个离散空间，维度为 `2`，意味着它可以取值 `0` 或 `1`。观测空间是 `Box` 类型，维度为
    `4`，意味着它由四个实数组成，位于下限和上限之内，正如我们已经在 CartPole 环境中看到的那样，它们是 [`±2.4`, `± inf`, `±0.20943951`,
    `±inf`]。
- en: 'With this information, it is now possible to build a policy network that can
    be interfaced with the CartPole environment. The following code block shows one
    of many possible choices: it uses two hidden layers with `64` neurons each and
    an output layer with `2` neurons (as this is the action space''s dimension) with
    a `softmax` activation function. The model summary prints out the outline of the
    model.'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有了这些信息，现在可以构建一个可以与CartPole环境交互的策略网络。以下代码块展示了多种可能的选择之一：它使用了两个具有`64`个神经元的隐藏层和一个具有`2`个神经元的输出层（因为这是动作空间的维度），并使用`softmax`激活函数。模型总结打印出了模型的大致结构。
- en: 'Build the policy network and print its summary:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建策略网络并打印其总结：
- en: '[PRE33]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output will be as follows:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE34]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the model has been created and we also have an elaborate summary
    of it, which gives us significant information about the model, regarding the layers,
    the parameters of the network, and so on.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型已经创建，并且我们也有一个详细的模型总结，它为我们提供了关于模型的重要信息，包括网络的层次结构、参数等。
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fkxfce](https://packt.live/3fkxfce).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3fkxfce](https://packt.live/3fkxfce)。
- en: You can also run this example online at [https://packt.live/2XSXHnF](https://packt.live/2XSXHnF).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2XSXHnF](https://packt.live/2XSXHnF)上在线运行这个示例。
- en: 'Once the policy network has been built and initialized, it is possible to feed
    it. Of course, since the network hasn''t been trained, it will generate random
    outputs, but still, it can be used, for example, to run a random agent in an environment
    of choice. This is what we will implement in the following exercise: the neural
    network model will be fed with the observation provided by the environment step
    or `reset` function through the `predict` method. This outputs the action probabilities.
    The action with the highest probability is chosen and used to step through the
    environment until the episode ends.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦策略网络构建并初始化完成，就可以输入数据了。当然，由于网络尚未训练，它会生成随机的输出，但仍然可以使用，例如在选择的环境中运行一个随机智能体。这正是我们在接下来的练习中将要实现的：神经网络模型将通过`predict`方法接受环境步骤或`reset`函数提供的观察，并输出动作的概率。选择具有最高概率的动作，并用它在环境中进行步进，直到回合结束。
- en: 'Exercise 4.03: Feeding the Policy Network with Environment State Representation'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.03：用环境状态表示喂给策略网络
- en: 'In this exercise, we will be feeding information to the policy network with
    the environment state representation. This exercise is a continuation of *Exercise
    4.02*, *Building a Policy Network with TensorFlow*, so in order to carry it out,
    you need to perform all the steps of the preceding exercise and then begin this
    one right after. Follow these steps to complete this exercise:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将用环境状态表示喂给策略网络。这个练习是*练习4.02*，*使用TensorFlow构建策略网络*的延续，因此，为了完成它，你需要执行前一个练习的所有步骤，然后直接开始这个练习。按照以下步骤完成本练习：
- en: 'Reset the environment:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境：
- en: '[PRE35]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Start a loop that will run until the episode is complete. Render the environment
    and print the observations:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个循环，直到回合完成。渲染环境并打印观察结果：
- en: '[PRE36]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Feed the network with the environment observations, let it choose the appropriate
    actions, and print it:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将环境观察输入网络，让它选择合适的动作并打印：
- en: '[PRE37]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Step through the environment with the selected action. Print the received reward
    and close the environment if the terminal state has been reached:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过所选的动作步进环境。打印接收到的奖励，并在终止状态达到时关闭环境：
- en: '[PRE38]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This produces the following output (only the last few lines have been shown):'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出（仅显示最后几行）：
- en: '[PRE39]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AmwUHw](https://packt.live/2AmwUHw).
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2AmwUHw](https://packt.live/2AmwUHw)。
- en: You can also run this example online at [https://packt.live/3kvuhVQ](https://packt.live/3kvuhVQ).
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3kvuhVQ](https://packt.live/3kvuhVQ)上在线运行这个示例。
- en: By completing this exercise, we've built a policy network and used it to guide
    an agent's behavior in a Gym environment. At the moment, it behaves randomly,
    but apart from policy network training, which will be explained in the following
    chapters, every other piece of the big picture is already in place.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成这个练习，我们已经构建了一个策略网络，并用它来指导智能体在Gym环境中的行为。目前，它的行为是随机的，但除了策略网络训练（将在后续章节中解释）之外，整体框架的其他部分已经就绪。
- en: How to Save a Policy Network
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何保存一个策略网络
- en: The goal of reinforcement learning is to effectively train the network so that
    it learns how to perform the optimal action for every given environment state.
    RL theory deals with how to achieve this goal and, as we will see, different approaches
    have been successful. Supposing one of them has been applied to the previous network,
    the trained model needs to be saved so that it can be loaded every time it needs
    to run the agent on the environment.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是有效地训练网络，使其学会如何在每个给定的环境状态下执行最优动作。RL理论研究的是如何实现这一目标，正如我们将看到的那样，不同的方法已经取得了成功。如果其中一种方法已经应用于之前的网络，那么训练后的模型需要被保存，以便在每次运行智能体时加载。
- en: 'To save the policy network, we need to follow the very same steps of saving
    a common neural network, where all the weights of all the layers are dumped into
    a save file to be loaded again in the network at a later stage. The following
    code is an example of this implementation:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存策略网络，我们需要遵循保存普通神经网络的相同步骤，将所有层的权重保存到文件中，以便在以后加载到网络中。以下代码展示了这一实现的例子：
- en: '[PRE40]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This produces the following output:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE41]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In this section, we learned how to create, run, and save a policy network using
    TensorFlow. Once the inputs (environment states/observations) and outputs (actions
    the agent can perform) are clear, there is no big difference with respect to standard
    deep neural networks. The model has also been used to run the agent. When fed
    with the environment state, it produced actions for the agent to take. Being an
    untrained network, the agent behaved randomly. The only missing piece in this
    section is how to effectively train the policy network, which is the goal of reinforcement
    learning and will be covered in detail in this book and, partially, in the following
    sections.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们学习了如何使用TensorFlow创建、运行并保存一个策略网络。一旦输入（环境状态/观察）和输出（智能体可以执行的动作）明确后，它与标准的深度神经网络没有太大区别。该模型也被用于运行智能体。当输入环境状态时，它为智能体生成了需要执行的动作。由于网络尚未经过训练，智能体的行为是随机的。本节唯一缺少的部分是如何有效地训练策略网络，这也是强化学习的目标，书中将在后续章节中详细讲解。
- en: Now that we've learned how to build a policy network with TensorFlow, let's
    dive into another OpenAI resource that will allow us to easily train an RL agent.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何使用TensorFlow构建一个策略网络，接下来让我们深入探讨另一个OpenAI资源，它将帮助我们轻松训练一个强化学习（RL）智能体。
- en: OpenAI Baselines
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Baselines
- en: So far, we have studied the two different frameworks that allow us to solve
    reinforcement learning problems (OpenAI Gym and OpenAI Universe). We also studied
    how to create the "brain" of the agent, known as the policy network, with TensorFlow.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了两种不同的框架，它们使我们能够解决强化学习问题（OpenAI Gym和OpenAI Universe）。我们还研究了如何使用TensorFlow创建智能体的大脑——策略网络。
- en: The next step is to train the agent and make it learn how to act optimally,
    only through experience. Learning how to train an RL agent is the ultimate goal
    of this book. We will see how most advanced methods work and find out about all
    their internal elements and algorithms. But even before we find out all the details
    of how these approaches are implemented, it is possible to rely on some tools
    that make the task more straightforward.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是训练智能体，并让它仅通过经验学会如何采取最优行动。学习如何训练一个RL智能体是本书的终极目标。我们将看到最先进的方法是如何工作的，并了解它们所有的内部元素和算法。但即使在我们深入了解这些方法如何实现之前，也可以依靠一些工具来简化任务。
- en: OpenAI Baselines is a Python-based tool, built on TensorFlow, that provides
    a library of high-quality, state-of-the-art implementations of reinforcement learning
    algorithms. It can be used as an out-of-the-box module, but it can also be customized
    and expanded. We will be using it to solve a classic control problem and a classic
    Atari video game by training a custom policy network.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Baselines是一个基于Python的工具，构建于TensorFlow之上，提供了一套高质量、最先进的强化学习算法实现库。它可以作为开箱即用的模块使用，也可以进行定制和扩展。我们将使用它来解决一个经典控制问题和一个经典的Atari视频游戏，通过训练一个定制的策略网络。
- en: Note
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please make sure you have installed OpenAI Baselines by using the instructions
    mentioned in the preface, before moving on.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在继续之前，按照前言中提到的说明安装了OpenAI Baselines。
- en: Proximal Policy Optimization
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近端策略优化
- en: It is worth providing a high-level idea of what **Proximal Policy Optimization**
    (**PPO**) is. We will remain at the highest level when describing this state-of-the-art
    RL algorithm because, in order to deeply understand how it works, you will need
    to become familiar with the topics that will be presented in the following chapters,
    thereby preparing you to study and build other state-of-the-art RL methods by
    the end of this book.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 了解 **近端策略优化**（**PPO**）的高级概念是值得的。在描述这一最先进的强化学习算法时，我们将保持在最高的层级，因为要深入理解它的工作原理，你需要熟悉接下来几章中将介绍的主题，从而为你在本书结束时学习和构建其他最先进的强化学习方法做好准备。
- en: 'PPO is a reinforcement learning method that is part of the policy gradient
    family. Algorithms in this category aim to directly optimize the policy, instead
    of building a value function to then generate a policy. To do so, they instantiate
    a policy (in our case, in the form of a deep neural network) and build a method
    to calculate a gradient that defines where to move the policy function''s approximator
    parameters (the weights of our deep neural network, in our case) to directly improve
    the policy. The word "proximal" suggests a specific feature of these methods:
    in the policy update step, when adjusting policy parameters, the update is constrained,
    thus preventing it from moving "too far" from the starting policy. All these aspects
    will be transparent to the user, thanks to the OpenAI Baselines tool, which will
    take care of carrying out the job under the hood. You will learn about these aspects
    in the upcoming chapters.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 是一种强化学习方法，属于策略梯度家族。该类算法的目标是直接优化策略，而不是构建一个值函数来生成策略。为了做到这一点，它们会实例化一个策略（在我们的案例中，是深度神经网络的形式），并构建一种计算梯度的方法，用于定义如何调整策略函数的近似参数（在我们这里是深度神经网络的权重），以直接改进策略。词语“proximal”（近端）暗示了这些方法的一个特定特点：在策略更新步骤中，调整策略参数时，更新会受到约束，从而避免策略偏离“起始策略”太远。所有这些方面都对用户透明，感谢
    OpenAI Baselines 工具，它会在后台处理这些任务。你将在接下来的章节中了解这些方面。
- en: Note
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Please refer to the following paper to learn more about PPO: [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下论文以了解更多关于 PPO 的内容：[https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)。
- en: Command-Line Usage
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令行使用
- en: 'As stated earlier, OpenAI Baselines allows us to train state-of-the-art RL
    algorithms easily for OpenAI Gym problems. The following code snippet, for example,
    trains a PPO algorithm for 20 million steps in the Pong Gym environment:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，OpenAI Baselines 使我们能够轻松训练最先进的强化学习算法，用于 OpenAI Gym 问题。例如，以下代码片段在 Pong Gym
    环境中训练一个 PPO 算法，训练步数为 2000 万步：
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'It saves the model in the user-defined save path so that it is possible to
    reload the weights on the policy network and deploy the trained agent in the environment
    with the following command-line instruction:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 它将模型保存在用户定义的保存路径中，以便可以通过以下命令行指令重新加载策略网络的权重，并将训练好的智能体部署到环境中：
- en: '[PRE43]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You can easily train every available method on every OpenAI Gym environment
    by changing only the command-line arguments, without knowing anything about how
    they work internally.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅修改命令行参数，你可以轻松地在每个 OpenAI Gym 环境中训练所有可用方法，而不需要了解它们的内部工作原理。
- en: Methods in OpenAI Baselines
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Baselines 中的方法
- en: 'OpenAI Baselines gives us access to the following RL algorithm implementations:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Baselines 为我们提供了以下强化学习算法实现：
- en: '**A2C**: Advantage Actor-Critic'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A2C**：优势行为者-评论家'
- en: '**ACER**: Actor-Critic with Experience Replay'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ACER**：具有经验回放的行为者-评论家'
- en: '**ACKTR**: Actor-Critic using Kronecker-factored Trust Region'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ACKTR**：使用 Kronecker 因子化信赖域的行为者-评论家'
- en: '**DDPG**: Deep Deterministic Policy Gradient'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DDPG**：深度确定性策略梯度'
- en: '**DQN**: Deep Q-Network'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DQN**：深度 Q 网络'
- en: '**GAIL**: Generative Adversarial Imitation Learning'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GAIL**：生成对抗模仿学习'
- en: '**HER**: Hindsight Experience Replay'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HER**：后视经验回放'
- en: '**PPO2**: Proximal Policy Optimization'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PPO2**：近端策略优化'
- en: '**TRPO**: Trust Region Policy Optimization'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TRPO**：信赖域策略优化'
- en: For the upcoming exercise and activity, we will be using PPO.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将进行的练习和活动，我们将使用PPO。
- en: Custom Policy Network Architecture
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义策略网络架构
- en: Despite its out-of-the-box usability, OpenAI Baselines can also be customized
    and expanded. In particular, as something that will also be used in the next two
    sections of this chapter, it is possible to provide a custom definition to the
    module for the policy network architecture.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 OpenAI Baselines 具有开箱即用的可用性，但它也可以进行自定义和扩展。特别是，作为本章接下来两节内容中将使用的部分，可以为策略网络架构的模块提供自定义定义。
- en: One aspect that needs to be clear is the fact that the network will be used
    as an encoder of the environment state or observation. OpenAI Baselines will then
    take care of creating the final layer, which is in charge of linking the latent
    space (space of embeddings) to the proper output layer. The latter is chosen depending
    on the type of the action space (is it discrete or continuous? How many available
    actions are there?) of the selected environment.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个方面需要明确，那就是网络将作为环境状态或观测的编码器。OpenAI Baselines 将负责创建最终层，这一层负责将潜在空间（嵌入空间）与适当的输出层连接。后者的选择取决于所选环境的动作空间类型（是离散还是连续？可用动作有多少个？）。
- en: First of all, the user needs to import the Baselines register, which allows
    them to define a custom network and register it with a user-defined name. Then,
    they can define a custom deep learning model in the form of a function using a
    custom architecture. In this way, we are able to change the policy network architecture
    at will, testing different solutions to find the best one for a specific problem.
    A practical example will be presented in the exercise in the following section.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，用户需要导入 Baselines 注册表，这允许他们定义一个自定义网络，并使用自定义名称进行注册。然后，他们可以通过使用自定义架构定义一个自定义的深度学习模型。通过这种方式，我们能够随意更改策略网络架构，测试不同的解决方案，以找到适合特定问题的最佳方案。实践示例将在接下来的练习中呈现。
- en: Now, we are ready to train our first RL agent and solve a classic control problem.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备训练我们的第一个 RL 代理，并解决一个经典的控制问题。
- en: Training an RL Agent to Solve a Classic Control Problem
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 RL 代理解决经典控制问题
- en: In this section, we will learn how to train a reinforcement learning agent capable
    of solving a classic control problem named CartPole by building upon all the concepts
    explained previously. OpenAI Baselines will be leveraged and, following the steps
    highlighted in the previous section, we will use a custom fully connected network
    as a policy network, which is provided as input for the PPO algorithm.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何训练一个能够解决经典控制问题 CartPole 的强化学习代理，所有的学习都基于前面解释的概念。我们将利用 OpenAI Baselines，并根据前面部分的步骤，使用自定义的全连接网络作为策略网络，将其作为输入传递给
    PPO 算法。
- en: Let's have a quick recap of the CartPole control problem. It is a classic control
    problem with a continuous four-dimensional observation space and a discrete two-dimensional
    action space. The observations that are recorded are the position and velocity
    of the cart along its line of movement, as well as the angle and angular velocity
    of the pole. The actions are the left/right movement of the cart along its rail.
    The reward is +1.0 for every step that does not result in a terminal state, which
    is the case if the pole moves more than 15 degrees from the vertical or if the
    cart moves outside the rail boundary placed at +/- 2.4\. The environment is considered
    solved if it does not end before having completed 200 steps.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下 CartPole 控制问题。这是一个经典的控制问题，具有连续的四维观测空间和离散的二维动作空间。记录的观测值包括小车沿运动轨迹的位移和速度，以及杆的角度和角速度。动作是小车在轨道上的左右移动。奖励是每一步不导致终止状态时为
    +1.0，当杆的角度超过 15 度或小车移出设定的轨道边界（+/- 2.4）时，环境进入终止状态。如果在完成 200 步之前没有结束，环境就被认为是已解决的。
- en: Now, let's put all these concepts together by completing an exercise.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过完成一个练习来将这些概念结合起来。
- en: 'Exercise 4.04: Solving a CartPole Environment with the PPO Algorithm'
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.04：使用 PPO 算法解决 CartPole 环境
- en: The CartPole problem in this exercise will be solved using the PPO algorithm.
    We will use two slightly different approaches so that we will learn about both
    approaches to using OpenAI Baselines. The first approach will take advantage of
    Baselines' infrastructure but will adopt a custom path where a user-defined network
    is used as the policy network. It will be trained and run in the environment after
    being trained in a "manual" way, without relying on Baselines' automation. This
    will give you the chance to take a look at what is happening under the hood. The
    second approach will be simpler, wherein we will be directly adopting Baselines'
    pre-defined command-line interface.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习中的CartPole问题将使用PPO算法解决。我们将采用两种略有不同的方法，以便学习使用OpenAI Baselines的两种方法。第一种方法将利用Baselines的基础设施，但采用自定义路径，其中使用用户定义的网络作为策略网络。在经过“手动”训练后，它将在环境中运行，而不依赖于Baselines的自动化。这将让你有机会深入了解底层发生了什么。第二种方法会更简单，我们将直接采用Baselines预定义的命令行接口。
- en: A custom deep network will be built that will encode environment states and
    create embeddings in the latent space. The OpenAI Baselines module will then take
    care of creating the remaining layer of the policy (and value) network for linking
    the embedding space with action spaces.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 将构建一个自定义的深度网络，该网络将编码环境状态并在潜在空间中创建嵌入。然后，OpenAI Baselines模块将负责创建策略（和值）网络的剩余层，以将嵌入空间与动作空间连接起来。
- en: We will also create a specific function, which is created by customizing an
    OpenAI Baselines function, with the specific aim of building the Gym environment,
    as expected by the infrastructure. There is no particular value in it, but this
    is required in order to then leverage all Baselines modules.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一个特定的函数，该函数是通过自定义OpenAI Baselines函数创建的，目的是构建符合基础设施要求的Gym环境。虽然它本身没有特别的价值，但为了便于使用所有Baselines模块，这是必须的。
- en: Note
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In order to properly run this exercise, you will need to install OpenAI Baselines.
    Please refer to the preface for the installation instructions.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确运行此练习，您需要安装OpenAI Baselines。请参阅前言以获取安装说明。
- en: Also, in order to properly train the RL agent, many episodes are needed, so
    the training phase may take several hours to complete. A set of weights for the
    pretrained agent will be provided at the end of this exercise so that you can
    see the trained agent in action.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了正确训练RL智能体，需要多个回合，因此训练阶段可能需要几个小时才能完成。将在本次练习结束时提供预训练智能体的权重集，以便您可以查看训练好的智能体如何运行。
- en: 'Follow these steps to complete this exercise:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成此练习：
- en: 'Open a new Jupyter Notebook and import all the required modules from OpenAI
    Baselines and TensorFlow to use the PPO algorithm:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook，并导入所有所需的模块，包括OpenAI Baselines和TensorFlow，以使用PPO算法：
- en: '[PRE44]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define and register a custom multi-layer perceptron for the policy network.
    Here, some arguments have also been defined so that you can easily control network
    architecture, making the user able to specify the number of hidden layers, the
    number of neurons for the hidden layers, and their activation functions:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并注册一个自定义的多层感知器作为策略网络。在此，定义了一些参数，使得您可以轻松控制网络架构，用户可以指定隐藏层的数量、每个隐藏层的神经元数量以及它们的激活函数：
- en: '[PRE45]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create a function that will build the environment in the format required by
    OpenAI Baselines:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，构建符合OpenAI Baselines要求格式的环境：
- en: '[PRE46]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Build the `CartPole-v0` environment, choose the necessary policy network parameters,
    and train it using the specific PPO `learn` function that has been imported:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建`CartPole-v0`环境，选择必要的策略网络参数，并使用已导入的特定PPO `learn`函数进行训练：
- en: '[PRE47]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'While training, the model will produce an output similar to the following:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练过程中，模型将产生类似于以下的输出：
- en: '[PRE48]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This shows the policy network architecture, as well as the bookkeeping of some
    quantities related with the training process, where the first two are, for example,
    the mean episode length and the mean episode reward.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显示了策略网络的架构，以及一些与训练过程相关的量的记账信息，其中前两个是，例如，平均回合长度和平均回合奖励。
- en: 'Run the trained agent in the environment and print the cumulative reward:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中运行训练好的智能体并打印累计奖励：
- en: '[PRE49]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output should be similar to the following:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应类似于以下内容：
- en: '[PRE50]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Use the built-in OpenAI Baselines run script to train PPO on the `CartPole-v0`
    environment:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的OpenAI Baselines运行脚本，在`CartPole-v0`环境中训练PPO：
- en: '[PRE51]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The last few lines of the output should be similar to the following:'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的最后几行应该类似于以下内容：
- en: '[PRE52]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Use the built-in OpenAI Baselines run script to run the trained model on the
    `CartPole-v0` environment:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的 OpenAI Baselines 运行脚本在 `CartPole-v0` 环境中运行训练好的模型：
- en: '[PRE53]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The last few lines of the output should be similar to the following:'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的最后几行应类似于以下内容：
- en: '[PRE54]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Use the pretrained weights provided to see the trained agent in action:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提供的预训练权重查看训练代理的表现：
- en: '[PRE55]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output will be similar to the following:'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE56]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You can read the `.tar` file using the following command:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下命令读取 `.tar` 文件：
- en: '[PRE57]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The last few lines of the output should be similar to the following:'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的最后几行应类似于以下内容：
- en: '[PRE58]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Use the built-in OpenAI Baselines run script to train PPO on the CartPole environment:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的 OpenAI Baselines 运行脚本在 CartPole 环境上训练 PPO：
- en: '[PRE59]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output will be similar to the following:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE60]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This step will show you how a trained agent behaves so that it can solve the
    CartPole environment. It uses a set weights for the policy network that were ready
    to be used. The output will be similar to the one shown in *Step 5*, confirming
    that the environment has been solved.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一步将向你展示一个训练好的代理如何行为，从而能够解决 CartPole 环境。它使用一组为策略网络准备好的权重。输出将类似于*步骤 5*中所示，确认环境已被解决。
- en: Note
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2XS69n8](https://packt.live/2XS69n8).
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参阅 [https://packt.live/2XS69n8](https://packt.live/2XS69n8)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，需要在本地运行。
- en: In this exercise, we learned how to train a reinforcement learning agent capable
    of solving the CartPole classic control problem. We successfully used a custom
    fully connected network as a policy network. This allowed us to take a look at
    what happens behind the automation provided by OpenAI Baselines' command-line
    interface. In this hands-on exercise, we have also familiarized ourselves with
    OpenAI Baselines' out-of-the-box method, confirming that it is a straightforward
    resource that can be easily used to train a reinforcement learning agent.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们学习了如何训练一个强化学习代理，解决经典的 CartPole 控制问题。我们成功地使用了一个自定义的全连接网络作为策略网络。这使我们得以窥见
    OpenAI Baselines 命令行界面提供的自动化背后的运行机制。在这个实践练习中，我们还熟悉了 OpenAI Baselines 的现成方法，确认它是一个可以轻松使用来训练强化学习代理的简单资源。
- en: 'Activity 4.01: Training a Reinforcement Learning Agent to Play a Classic Video
    Game'
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.01：训练强化学习代理来玩经典视频游戏
- en: 'In this activity, the challenge is to adopt the same approach we used in *Exercise
    4.04*, *Solving the CartPole Environment with the PPO Algorithm*, to create a
    reinforcement learning bot that''s able to achieve better-than-human performance
    on a classic Atari video game, Pong. The game is represented in the following
    way: two paddles, one per user, can move up and down. The goal is to make the
    white ball pass the opposite paddle to score one point. The game ends when one
    of the two players reaches a score equal to `21`.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，挑战是采用我们在*练习 4.04*中使用的方法，*使用 PPO 算法解决 CartPole 环境*，创建一个能够在经典的 Atari 视频游戏
    Pong 中实现超越人类表现的强化学习机器人。游戏表示方式如下：两个挡板，每个玩家一个，可以上下移动。目标是让白色球通过对方的挡板来得分。游戏在其中一个玩家的得分达到`21`时结束。
- en: 'An approach similar to the one we saw in *Exercise 4.04*, *Solving the CartPole
    Environment with the PPO Algorithm*, has to be adopted, with a custom convolutional
    neural network, which will work as the encoder for the environment''s observation
    (the pixels frame):'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 需要采用类似于我们在*练习 4.04*中看到的方法，*使用 PPO 算法解决 CartPole 环境*，并使用自定义卷积神经网络，它将作为环境观察（像素帧）的编码器：
- en: '![Figure 4.12: One frame of the Pong game'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12：Pong 游戏的一帧画面'
- en: '](img/B16182_04_12.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_04_12.jpg)'
- en: 'Figure 4.12: One frame of the Pong game'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：Pong 游戏的一帧画面
- en: OpenAI Gym will be used to create the environment, while the OpenAI Baselines
    module will be used to train a custom policy network using the PPO algorithm.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用 OpenAI Gym 创建环境，同时使用 OpenAI Baselines 模块训练一个自定义的策略网络，利用 PPO 算法。
- en: As we saw in *Exercise 4.04*, *Solving the CartPole Environment with the PPO
    Algorithm*, both the custom approach, that is, using specific OpenAI modules,
    and the simple one, that is, using the built-in general command-line interface,
    will be implemented (in *steps 1* to *5* and *step 6*, respectively).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*练习4.04*中看到的，*使用PPO算法解决CartPole环境*，自定义方法（即使用特定的OpenAI模块）和简单方法（即使用内置的一般命令行接口）都会实现（分别在*步骤1*到*5*和*步骤6*中）。
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In order to run this exercise, you will need to install OpenAI Baselines. Please
    refer to the preface for the installation instructions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这个练习，你需要安装OpenAI Baselines。请参考前言中的安装说明。
- en: 'In order to properly train the RL agent, many episodes are needed, so the training
    phase may take several hours to complete. A set of weights you can use for a pretrained
    agent has been provided at this address: [https://packt.live/2XSY4yz](https://packt.live/2XSY4yz).
    Use them to see the trained agent in action.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确训练RL代理，需要多个回合，因此训练阶段可能需要几个小时才能完成。你可以在这个地址找到一组你可以使用的预训练代理权重：[https://packt.live/2XSY4yz](https://packt.live/2XSY4yz)。使用它们来查看训练好的代理的表现。
- en: 'The following steps will help you complete this activity:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个活动：
- en: Import all the required modules from OpenAI Baselines and TensorFlow in order
    to use the `PPO` algorithm.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从OpenAI Baselines和TensorFlow中导入所有需要的模块，以便使用`PPO`算法。
- en: Define and register a custom convolutional neural network for the policy network.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并注册一个自定义的卷积神经网络作为策略网络。
- en: Create a function to build the environment in the format required by OpenAI
    Baselines.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来构建符合OpenAI Baselines要求的环境格式。
- en: Build the `PongNoFrameskip-v4` environment, choose the required policy network
    parameters, and train it.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建`PongNoFrameskip-v4`环境，选择所需的策略网络参数并进行训练。
- en: Run the trained agent in the environment and print the cumulative reward.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中运行训练好的代理，并打印累计奖励。
- en: Use the built-in OpenAI Baselines run script to train PPO on the `PongNoFrameskip-v0`
    environment.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的OpenAI Baselines运行脚本在`PongNoFrameskip-v0`环境中训练PPO。
- en: Use the built-in OpenAI Baselines run script to run the trained model on the
    `PongNoFrameskip-v0` environment.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的OpenAI Baselines运行脚本在`PongNoFrameskip-v0`环境中运行训练好的模型。
- en: Use the pretrained weights provided to see the trained agent in action.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提供的预训练权重来查看训练好的代理的表现。
- en: At the end of this activity, the agent is expected to easily win most of the
    time.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个活动的结束时，代理应该能够大部分时间轻松获胜。
- en: 'The final score of the agent should be like the one represented in the following
    frame most of the time:'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理的最终得分大部分时间应类似于以下帧所表示的得分：
- en: '![Figure 4.13: One frame of the real-time environment, after rendering'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.13：渲染后的实时环境的一帧'
- en: '](img/B16182_04_13.jpg)'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_13.jpg)'
- en: 'Figure 4.13: One frame of the real-time environment, after rendering'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：渲染后的实时环境的一帧
- en: Note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 704.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第704页找到。
- en: Summary
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced us to the key technologies and concepts we can use to
    get started with reinforcement learning. The first two sections described two
    OpenAI Tools, OpenAI Gym and OpenAI Universe. These are collections that contain
    a large number of control problems that cover a broad spectrum of contexts, from
    classic tasks to video games, from browser usage to algorithm deduction. We learned
    how the interfaces of these environments are formalized, how to interact with
    them, and how to create a custom environment for a specific problem. Then, we
    learned how to build a policy network with TensorFlow, how to feed it with environment
    states to retrieve corresponding actions, and how to save the policy network weights.
    We also studied another OpenAI resource, Baselines. We solved problems that demonstrated
    how to train a reinforcement learning agent to solve a classic control task. Finally,
    using all the elements introduced in this chapter, we built an agent and trained
    it to play a classic Atari video game, thus achieving better-than-human performance.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向我们介绍了一些关键的技术和概念，让我们可以开始学习强化学习。前两节介绍了两个 OpenAI 工具，OpenAI Gym 和 OpenAI Universe。这些都是包含大量控制问题的集合，涵盖了广泛的背景和情境，从经典任务到视频游戏，从浏览器使用到算法推导。我们学习了这些环境的接口是如何被规范化的，如何与它们进行交互，以及如何为特定问题创建自定义环境。接着，我们学习了如何使用
    TensorFlow 构建策略网络，如何根据环境状态输入以获取相应的动作，并且如何保存策略网络的权重。我们还研究了另一个 OpenAI 资源，Baselines。我们解决了一些问题，演示了如何训练强化学习代理程序来解决经典的控制任务。最后，在本章介绍的所有元素的基础上，我们构建了一个代理程序，并训练它玩经典的
    Atari 视频游戏，从而实现了超越人类的表现。
- en: In the next chapter, we will be delving deep into dynamic programming for reinforcement
    learning.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨强化学习中的动态规划。
