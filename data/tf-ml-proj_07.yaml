- en: Credit Card Fraud Detection using Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器进行信用卡欺诈检测
- en: The digital world is growing rapidly. We are used to performing many of our
    daily tasks online, such as booking cabs, shopping on e-commerce websites, and
    even recharging our phones. For the majority of these tasks, we are used to paying
    with credit cards. However, it is a known fact that a credit card can be compromised,
    which could result in a fraudulent transaction. The Nilson report estimates that
    for every $100 spent, seven cents are stolen. It estimates the total credit card
    fraud market to be around $30 billion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数字世界正在快速发展。我们习惯于在网上完成许多日常任务，如预订出租车、在电子商务网站购物，甚至为手机充值。对于这些任务的大多数，我们习惯使用信用卡支付。然而，信用卡可能会被盗用，这可能导致欺诈交易。Nilson报告估计，每花费100美元，就有7美分被盗取。它估计信用卡欺诈市场总额约为300亿美元。
- en: Detecting whether a transaction is fraudulent or not is a very impactful data
    science problem. Every bank that issues credit cards invests in technology to
    detect fraud and take the appropriate actions immediately. There are lot of standard
    supervised learning techniques such as logistic regression, from random forest
    to classifying fraud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 检测交易是否欺诈是一个非常具有影响力的数据科学问题。每家发行信用卡的银行都在投资技术以检测欺诈并立即采取适当的行动。有很多标准的监督学习技术，如逻辑回归、随机森林等，用于欺诈分类。
- en: 'In this chapter, we will take a closer look at an unsupervised approach to
    detecting credit card fraud using auto-encoders by exploring the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更详细地了解使用自编码器检测信用卡欺诈的无监督方法，并探讨以下主题：
- en: Understanding auto-encoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解自编码器
- en: Defining and training a fraud detection model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义和训练欺诈检测模型
- en: Testing a fraud detection model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试欺诈检测模型
- en: Understanding auto-encoders
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解自编码器
- en: Auto-encoders are a type of artificial neural network whose job is to learn
    a low-dimensional representation of input data using unsupervised learning. They
    are quite popular when it comes to dimensionality reduction of input and in generative
    models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种人工神经网络，其任务是使用无监督学习学习输入数据的低维表示。当涉及到输入数据的降维和生成模型时，它们非常受欢迎。
- en: Essentially, an auto-encoder learns to compress data into a low-dimensional
    representation and then reconstructs that representation into something that matches
    the original data. This way, the low-dimensional representation ignores the noise,
    which is not helpful in reconstructing the original data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，自编码器学习将数据压缩成低维表示，然后将该表示重建为与原始数据匹配的内容。这样，低维表示忽略了噪声，这些噪声在重建原始数据时没有帮助。
- en: As mentioned previously, they are also useful in generating models, particularly
    images. For example, if we feed the representation of *dog* and *flying*, it may
    attempt to generate an image of a *flying cat*, which it has not seen before.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，它们在生成模型，特别是图像方面也很有用。例如，如果我们输入*狗*和*飞行*的表示，它可能会尝试生成一个*飞行的猫*图像，尽管它以前未见过这样的图像。
- en: Structurally, auto-encoders consist of two parts, the encoder and the decoder.
    The encoder generates the low-dimensional representation of inputs, and the decoder
    helps regenerate the input from the representation. Generally, the encoder and
    the decoder are feedforward neural networks with one or multiple hidden layers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从结构上看，自编码器由两个部分组成，编码器和解码器。编码器生成输入的低维表示，解码器帮助从表示中重新生成输入。通常，编码器和解码器是具有一个或多个隐藏层的前馈神经网络。
- en: 'The following diagram illustrates the configuration of a typical auto-encoder:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了典型自编码器的配置：
- en: '![](img/6cf79baa-d1b7-46d2-8b33-5bf772eeb6bd.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cf79baa-d1b7-46d2-8b33-5bf772eeb6bd.png)'
- en: 'To define the encoder (![](img/3edd31b1-5963-4738-b6fa-6c48d1fed1d8.png)) and
    the decoder (![](img/50571af6-621f-44c8-bb24-8f8c7aca02ce.png)), an auto-encoder
    can be mathematically represented as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义编码器（![](img/3edd31b1-5963-4738-b6fa-6c48d1fed1d8.png)）和解码器（![](img/50571af6-621f-44c8-bb24-8f8c7aca02ce.png)），自编码器可以通过以下数学公式表示：
- en: '![](img/7192ce52-f955-4587-8bdb-6bcb988d11ae.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7192ce52-f955-4587-8bdb-6bcb988d11ae.png)'
- en: As mentioned in the equation, the parameters of the encoder and the decoder
    are optimized in a way that minimizes a special kind of error, which is known
    as **reconstruction error.** Reconstruction error is the error between the reconstructed
    input and the original input.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如公式所示，编码器和解码器的参数通过一种优化方式进行调整，以最小化一种特殊类型的错误，称为**重建误差**。重建误差是重建输入与原始输入之间的误差。
- en: Building a fraud detection model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建欺诈检测模型
- en: For this project, we are going to use the credit card dataset from Kaggle ([https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)),
    Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating
    Probability with Undersampling for Unbalanced Classification. In Symposium on
    Computational Intelligence and Data Mining (CIDM), IEEE, 2015\. It consists of
    credit card transaction data from two days, from European cardholders. The dataset
    is highly imbalanced and contains approximately 284,000 pieces of transaction
    data with 492 instances of fraud (0.172% of the total).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将使用Kaggle上的信用卡数据集（[https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)），Andrea
    Dal Pozzolo，Olivier Caelen，Reid A. Johnson 和 Gianluca Bontempi的论文《Calibrating
    Probability with Undersampling for Unbalanced Classification》，发表于2015年计算智能与数据挖掘研讨会（CIDM），IEEE。该数据集包含来自欧洲持卡人的两天信用卡交易数据，数据严重不平衡，共约28.4万条交易数据，其中有492个欺诈实例（占总数的0.172%）。
- en: There are 31 numerical columns in the dataset. Two of them are time and amount.
    **Time** denotes the amount of time elapsed (in seconds) between each transaction
    and the first transaction in the dataset. **Amount** is the total amount regarding
    the transaction. For our model, we will eliminate the time column as it doesn't
    help with the accuracy of the model. The rest of the features (V1, V2 ... V28)
    are obtained from Principal Component Analysis ([https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/)) of
    original features for confidential reasons. **Class** is the target variable,
    which indicates whether the transaction was fraudulent or not.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有31个数值型列，其中两个是时间和金额。**Time**表示从每笔交易到数据集中第一次交易所经过的时间（单位：秒）。**Amount**表示交易的总金额。对于我们的模型，我们将去除时间列，因为它对模型的准确性没有帮助。其余的特征（V1,
    V2 ... V28）是通过主成分分析（[https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/)）从原始特征中提取的，出于保密原因。**Class**是目标变量，表示交易是否为欺诈交易。
- en: To pre-process the data, there is not much that needs to be done. This is mainly
    because a lot of the data is already cleaned up.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的工作并不多，主要是因为大部分数据已经清理过。
- en: Usually, in a classical machine learning model such as logistic regression,
    we feed the data points of both negative and positive classes into the algorithm.
    However, since we are using auto-encoders, we will model it differently.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在经典的机器学习模型中，如逻辑回归，我们会将负类和正类的数据点都输入到算法中。然而，由于我们使用的是自动编码器，我们将采用不同的建模方式。
- en: Essentially, our training set will consist of only non-fraudulent transaction
    data. The idea is that whenever we pass a fraudulent transaction through our trained
    model, it should detect it as an anomaly. We are framing this problem as anomaly
    detection rather than classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们的训练集将仅包含非欺诈交易数据。我们的思路是，每当我们将一笔欺诈交易输入到已训练的模型中时，模型应将其检测为异常。我们将这个问题定义为异常检测问题，而不是分类问题。
- en: The model will consist of two fully connected encoder layers with 14 and seven
    neurons, respectively. There will be two decoder layers with seven and 29 neurons,
    respectively. Additionally, we will use L1 regularization during training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将由两个全连接的编码器层组成，分别有14个和7个神经元。解码器部分将有两个层，分别有7个和29个神经元。此外，我们将在训练过程中使用L1正则化。
- en: Lastly, to define the model, we will use Keras with Tensorflow at the backend
    for training auto-encoders.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了定义模型，我们将使用Keras，并以Tensorflow作为后端来训练自动编码器。
- en: Regularization is a technique in machine learning that's used to reduce overfitting.
    Overfitting happens when the model learns a signal as well as noise in the training
    data and can't generalize well to unseen dataset. While there are many ways to
    avoid overfitting such as cross validation, sampling, and so on, regularization
    specifically adds a penalty to weights of the model so that we don't learn an
    overly complex model. L1 regularization adds an L1 norm penalty on all the weights
    of the model. This way, any weight that doesn't contribute to the accuracy is
    shrunk to zero.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是机器学习中的一种技术，用于减少过拟合。过拟合发生在模型不仅学习到训练数据中的信号，还学习到了噪音，从而无法很好地泛化到未见过的数据集。虽然有很多方法可以避免过拟合，比如交叉验证、采样等，但正则化特别是通过对模型的权重添加惩罚，防止我们学习到过于复杂的模型。L1正则化对模型的所有权重添加L1范数惩罚。这样，任何不贡献于准确度的权重都会被压缩到零。
- en: Defining and training a fraud detection model
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义并训练欺诈检测模型
- en: 'The following are the steps for defining and training the model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是定义和训练模型的步骤：
- en: 'Transform `''Amount''` by removing the mean and scaling it to the unit''s variance:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过去除均值并将其缩放到单位方差，转换`'Amount'`：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that we use the `StandardScaler` utility from scikit-learn for this purpose.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是来自scikit-learn的`StandardScaler`工具来实现此目的。
- en: 'To model our dataset, split it into train and test data, with train consisting
    of only non-fraudulent transaction and test consisting of both fraudulent and
    non-fraudulent transactions:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建我们的数据集，将其分为训练数据和测试数据，训练数据仅包含非欺诈交易，测试数据包含欺诈和非欺诈交易：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the model by using the following code:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码定义模型：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the model is defined, train the model using Keras:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型定义完成，使用Keras训练模型：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Use the following parameters to find the output of the model:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下参数查找模型的输出：
- en: EPOCHS = 100.
  id: totrans-38
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: EPOCHS = 100。
- en: BATCH_SIZE = 32.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BATCH_SIZE = 32。
- en: OPTIMIZER = 'Adam'.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OPTIMIZER = 'Adam'。
- en: LOSS = Mean squared error between reconstructed and original input.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LOSS = 重建输入与原始输入之间的均方误差。
- en: EVAL_METRIC = 'Accuracy'. This is the usual binary classification accuracy.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: EVAL_METRIC = 'Accuracy'。这是通常的二分类准确率。
- en: 'Store a `TensorBoard` file to visualize the graph or other variables on it.
    Also, store the best-performing model through the checkpoints provided by Keras. Generate
    the loss curves by epoch for the training and testing data:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储一个`TensorBoard`文件，以可视化图形或其他变量。同时，通过Keras提供的检查点存储表现最佳的模型。生成训练和测试数据的损失曲线：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following diagram illustrates the loss curves that are generated when the
    model is trained for 100 epochs:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图示展示了在训练100个epochs时生成的损失曲线：
- en: '![](img/996d8452-9c97-496b-a9cf-c5e3ff1ecbe8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/996d8452-9c97-496b-a9cf-c5e3ff1ecbe8.png)'
- en: We can observe that for the training set, the loss or reconstruction error decreases
    at the start of the training and saturates toward the end. This saturation implies
    that the model has finished learning the weights.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，对于训练集，损失或重建误差在训练初期下降，并在结束时饱和。此饱和意味着模型已经完成了权重的学习。
- en: Save the model with the lowest loss in the testing set.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 保存测试集中损失最小的模型。
- en: Testing a fraud detection model
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试欺诈检测模型
- en: 'Once the training process is complete, break down the reconstruction error
    in the testing set by fraudulent and non-fraudulent (normal) transactions. Generate
    the reconstruction error by different classes of transactions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，通过欺诈交易和非欺诈（正常）交易区分测试集中的重建误差。生成不同类别交易的重建误差：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following diagrams illustrate the reconstruction error distribution of
    fraudulent and normal transactions in the testing set:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了测试集中欺诈和正常交易的重建误差分布：
- en: '![](img/3ca17733-d635-496e-ba21-ec6d1717ded3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ca17733-d635-496e-ba21-ec6d1717ded3.png)'
- en: 'The next diagram is for fraud transactions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图示为欺诈交易：
- en: '![](img/c9facdbf-16d4-451d-a201-2015bc4ffe6f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9facdbf-16d4-451d-a201-2015bc4ffe6f.png)'
- en: As we can see, the reconstruction error for normal transactions is very close
    to zero for most of the transactions. However, the reconstruction error with fraudulent
    transactions has a wide distribution, with the majority still being close to zero.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大多数正常交易的重建误差接近零。然而，欺诈交易的重建误差分布较广，大部分仍接近零。
- en: This suggests that a threshold on the reconstruction error can serve as a classification
    threshold on normal versus fraudulent transactions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明重建误差的阈值可以作为正常交易与欺诈交易的分类阈值。
- en: 'For evaluating the model, we will use the metrics Precision and Recall which
    were defined in [Chapter 4](5de5c0e0-5fcc-4352-af84-4b813138ea90.xhtml), *Digit
    Classification Using TensorFlow Lite*, of the book. Firstly, let''s look at the
    precision and recall at various thresholds of reconstruction error:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型，我们将使用在[第4章](5de5c0e0-5fcc-4352-af84-4b813138ea90.xhtml)《*使用TensorFlow
    Lite进行数字分类*》中定义的精确度和召回率度量。首先，让我们来看一下在不同重构误差阈值下的精确度和召回率：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The reconstruction error threshold for precision and recall are shown in the
    following graph:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率的重构误差阈值如以下图所示：
- en: '![](img/7f72f802-1e2c-47bc-a9dd-8aaa6acf9b9b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f72f802-1e2c-47bc-a9dd-8aaa6acf9b9b.png)'
- en: 'The diagram represents the error threshold for recall:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表示召回率的误差阈值：
- en: '![](img/eb99eb0a-81d4-4ee5-803e-4e5566976672.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb99eb0a-81d4-4ee5-803e-4e5566976672.png)'
- en: As we can see, recall decreases when there is an increase in reconstruction
    error, and vice versa for precision. There are a few dips due to the dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当重构误差增加时，召回率下降，精确度则相反。由于数据集的原因，图中存在一些波动。
- en: There is one other thing that we need to keep in mind. As mentioned previously, there
    is always a trade-off between high precision and high recall in machine learning.
    We need to choose any one for our particular model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事我们需要记住。正如前面所提到的，在机器学习中，高精度与高召回率之间总是存在权衡。我们需要为我们的特定模型选择其中一个。
- en: Generally, businesses prefer a model with high precision or high recall. For
    fraud detection, we would like to have a model with high recall. This is essential
    as we can classify the majority of fraudulent transactions as fraud. One method
    to counter the loss of precision is to do a manual verification of transactions
    classified as fraud to determine whether they are actually fraudulent. This will
    help in ensuring a good experience for the end user.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，企业更倾向于选择高精度或高召回率的模型。对于欺诈检测，我们希望有一个高召回率的模型。这一点非常重要，因为我们可以将大多数欺诈交易分类为欺诈。应对精确度损失的一种方法是对被分类为欺诈的交易进行人工验证，确定它们是否真的属于欺诈交易。这样有助于确保最终用户的良好体验。
- en: 'Here is the code to generate a confusion matrix with `min_recall` = 80%:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成混淆矩阵的代码，`min_recall`设置为80%：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The confusion matrix that''s obtained from the preceding code is shown in the
    following diagram:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码得到的混淆矩阵如下图所示：
- en: '![](img/7bd4f84c-ac36-4477-abb8-f39711b06812.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bd4f84c-ac36-4477-abb8-f39711b06812.png)'
- en: We can observe that out of 120 fraudulent transactions, 97 of them have been
    classified correctly. However, we have also classified 1,082 normal transaction
    as being fraudulent, which will have to go through a manual verification process
    to ensure a good experience for the end user.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，在120笔欺诈交易中，97笔被正确分类。然而，我们还错误地将1,082笔正常交易分类为欺诈交易，这些交易将需要经过人工验证，以确保最终用户的良好体验。
- en: As a note of caution, we should not assume that auto-encoders are helpful in
    all binary classification tasks and can achieve better performance than state-of-the-art
    classification models. The idea behind this project was to illustrate a different
    approach of using auto-encoders to perform classification tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个警告，我们不应假设自编码器在所有二分类任务中都能有所帮助，并且能够超越最先进的分类模型。这个项目的目的是展示一种不同的使用自编码器进行分类任务的方法。
- en: Note that in this chapter, we have used the same validation and test set for
    illustrative purposes. Ideally, once we have defined the threshold on the reconstruction
    error, we should test the model on some unseen dataset to evaluate its performance
    in a better manner.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本章中，为了说明问题，我们使用了相同的验证集和测试集。理想情况下，一旦我们定义了重构误差的阈值，应该在一些未见过的数据集上测试模型，以便更好地评估其性能。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Credit card fraud are ubiquitous in nature. Every company in today's world is
    employing machine learning to combat payment fraud on their platform. In this
    chapter, we looked at the problem of classifying fraud using the credit card dataset
    from Kaggle.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 信用卡欺诈在本质上是普遍存在的。如今，每家公司都在使用机器学习来打击平台上的支付欺诈。在本章中，我们探讨了如何使用Kaggle的信用卡数据集来进行欺诈分类。
- en: 'We learned about auto-encoders as a dimensionality reduction technique. We
    understood that the auto-encoder architecture consists of two components: an encoder
    and a decoder. We model the parameters of a fully connected network using reconstruction
    loss.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了自编码器作为一种降维技术。我们理解到，自编码器架构由两个组件组成：编码器和解码器。我们使用重构损失来建模一个全连接网络的参数。
- en: Thereafter, we looked at the fraud classification problem through the lens of
    an anomaly detection problem. We trained the auto-encoder model using normal transactions.
    We then looked at the reconstruction error of the auto-encoder for both normal
    and fraudulent transactions, and observed that the reconstruction error has a
    wide distribution for fraudulent transactions. We then defined a threshold on
    reconstruction to classify the model and generated the confusion matrix.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，我们通过异常检测问题的视角来审视欺诈分类问题。我们使用正常交易训练了自动编码器模型。然后，我们查看了自动编码器在正常交易和欺诈交易中的重构误差，并观察到欺诈交易的重构误差分布较广。接着，我们在重构误差上定义了一个阈值来对模型进行分类，并生成了混淆矩阵。
- en: In the next chapter, we will explore the concept of Bayesian neural networks,
    which combines the concepts of deep learning and Bayesian learning to model uncertainty
    in the prediction of deep neural networks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨贝叶斯神经网络的概念，它将深度学习和贝叶斯学习的概念结合起来，以在深度神经网络的预测中建模不确定性。
- en: Questions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following are the questions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是问题：
- en: What is an auto-encoder?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是自动编码器？
- en: What are different components of an auto-encoder?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的不同组件是什么？
- en: What is the reconstruction loss?
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是重构损失？
- en: What is the precision and recall?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是精确度和召回率？
