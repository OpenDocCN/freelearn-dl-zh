- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Operationalizing Azure OpenAI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作化 Azure OpenAI
- en: In the preceding chapters, we’ve demonstrated how you can use **Azure OpenAI**
    (**AOAI**) while your data is secure and private. In this chapter, our focus will
    shift to operationalizing Azure OpenAI. This means we will explore how to effectively
    deploy, manage, and optimize Azure OpenAI services. We will discuss best practices
    for logging and monitoring, ensuring you can track and analyze the performance
    of your AOAI service. Additionally, we will cover the various service quotas and
    limits, helping you to understand how to manage and allocate resources efficiently.
    We will also look at quota management and how you can request increases to support
    larger workloads. Furthermore, we will explain how to provision throughput units
    to ensure your AI services can handle the required load. Finally, we will examine
    strategies for scaling Azure OpenAI services to meet growing demands.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们展示了如何在确保数据安全和隐私的前提下使用 **Azure OpenAI** (**AOAI**)。在本章中，我们的重点将转向操作化
    Azure OpenAI。这意味着我们将探索如何有效地部署、管理和优化 Azure OpenAI 服务。我们将讨论日志记录和监控的最佳实践，确保您能够跟踪和分析
    AOAI 服务的性能。此外，我们还将讨论各种服务配额和限制，帮助您理解如何高效地管理和分配资源。我们还将介绍配额管理以及如何请求增加配额以支持更大的工作负载。此外，我们将解释如何配置吞吐量单元，以确保您的
    AI 服务能够承载所需的负载。最后，我们将研究如何扩展 Azure OpenAI 服务，以满足不断增长的需求。
- en: 'In this chapter we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Azure OpenAI default Logging and Monitoring
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI 默认日志记录和监控
- en: Azure OpenAI Service Quotas and Limits
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务配额和限制
- en: Azure OpenAI Quota Management
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI 配额管理
- en: Azure OpenAI Provision Throughput Unit
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI 配置吞吐量单元
- en: Azure OpenAI Scaling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI 扩展
- en: Azure OpenAI default Logging and Monitoring
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure OpenAI 默认日志记录和监控
- en: Azure OpenAI service gathers monitoring data much like other Azure resources.
    You can set up Azure Monitor to collect data in activity logs, resource logs,
    virtual machine logs, and platform metrics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务收集监控数据的方式与其他 Azure 资源类似。您可以设置 Azure Monitor 来收集活动日志、资源日志、虚拟机日志和平台指标。
- en: Both platform metrics and the Azure Monitor activity log are automatically gathered
    and stored. To direct this data to other destinations, you can use a diagnostic
    setting. However, Azure Monitor resource logs are only collected and stored when
    you create a diagnostic setting and route the logs to one or more designated locations.
    During the configuration of a diagnostic setting, you decide which types of logs
    to collect. It’s important to note that using diagnostic settings and sending
    data to Azure Monitor Logs can incur additional costs. The following sections
    provide details on the metrics and logs that can be collected.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 平台指标和 Azure Monitor 活动日志会自动收集并存储。为了将这些数据传输到其他目的地，您可以使用诊断设置。然而，只有在创建诊断设置并将日志路由到一个或多个指定位置时，Azure
    Monitor 资源日志才会被收集和存储。在配置诊断设置时，您可以决定收集哪些类型的日志。需要注意的是，使用诊断设置并将数据发送到 Azure Monitor
    Logs 可能会产生额外费用。以下部分提供了可以收集的指标和日志的详细信息。
- en: Azure OpenAI metrics
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure OpenAI 指标
- en: You can use Azure Monitor tools in the Azure portal to examine metrics for your
    Azure OpenAI Service resources.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Azure 门户中使用 Azure Monitor 工具来检查 Azure OpenAI 服务资源的指标。
- en: Login to Azure Portal.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录 Azure 门户。
- en: Go to the **Overview** page of your Azure OpenAI resource
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入您的 Azure OpenAI 资源的 **Overview** 页面
- en: 'Choose **Metrics** from the **Monitoring** section on the left side as shown
    in *Figure 12**.1*:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图 *Figure 12.1* 所示，选择左侧 **Monitoring** 部分中的 **Metrics**：
- en: '![Figure 12.1: AOAI metrics portal view](img/B21019_12_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 12.1: AOAI 指标门户视图](img/B21019_12_1.jpg)'
- en: 'Figure 12.1: AOAI metrics portal view'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：AOAI 指标门户视图
- en: Azure OpenAI has commonality with a subset of Azure AI services. The Azure OpenAI
    Service provides several key metrics to help users monitor and optimize their
    usage. **Azure OpenAI Requests** is a fundamental metric that tracks the number
    of API calls made to the service. This helps users understand their usage patterns
    and can be crucial for managing costs and ensuring efficient use of resources.
    **Time to Response** measures the time taken to process requests, which is vital
    for assessing the performance and responsiveness of the service. High latency
    can indicate potential bottlenecks or issues that need to be addressed to maintain
    a smooth user experience.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI与Azure AI服务的一个子集具有共性。Azure OpenAI服务提供了几个关键指标，帮助用户监控和优化他们的使用情况。**Azure
    OpenAI请求**是一个基础指标，跟踪对该服务的API调用次数。这帮助用户了解他们的使用模式，对于管理成本和确保资源的高效利用至关重要。**响应时间**衡量处理请求所需的时间，这对于评估服务的性能和响应性至关重要。高延迟可能表示潜在的瓶颈或需要解决的问题，以保持顺畅的用户体验。
- en: 'Another important metric is **Prompt Token Cache Match Rate** which monitors
    the KV cache hits in PTU-M. **Key-Value** (**KV**) caching is a technique used
    in generative transformer models, including **large language models** (**LLMs**),
    to enhance the efficiency of the inference process. The main features of KV caching
    include:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的指标是**Prompt Token Cache Match Rate**，它监控PTU-M中的KV缓存命中率。**键值**（**KV**）缓存是一种在生成型变换器模型中使用的技术，包括**大型语言模型**（**LLM**），旨在提高推理过程的效率。KV缓存的主要特性包括：
- en: '**Reducing Computational Overhead**: It eliminates the need to recompute key
    and value tensors for previous tokens at each step of generation, thereby speeding
    up the process.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少计算开销**：它消除了在生成的每一步中重新计算先前标记的键值张量的需求，从而加快了过程。'
- en: '**Memory-Compute Balance**: By storing these tensors in GPU memory, KV caching
    optimizes the trade-off between memory usage and computational performance.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存与计算平衡**：通过将这些张量存储在GPU内存中，KV缓存优化了内存使用和计算性能之间的权衡。'
- en: 'To make the most of KV caching in your prompts, apply these optimization strategies:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用KV缓存，您可以采用以下优化策略：
- en: '**Position Dynamic Elements Strategically**: Place dynamic components—such
    as grounding data, date and time, or chat history—toward the end of your prompt.
    This ensures that frequently changing parts don’t disrupt caching for the static
    portions.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**战略性地定位动态元素**：将动态组件——如基础数据、日期和时间或聊天历史——放置在提示的末尾。这可以确保频繁变化的部分不会干扰静态部分的缓存。'
- en: '**Keep Static Elements Consistent**: Arrange static components like safety
    guidelines, examples, and tool or function definitions at the beginning of the
    prompt in a consistent order. This maximizes reusability and caching efficiency
    for these parts.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持静态元素一致**：将静态组件，如安全指南、示例和工具或功能定义，按一致的顺序安排在提示的开头。这可以最大化这些部分的可重用性和缓存效率。'
- en: '**Dedicate Your Deployment**: Focus your Prompt-Tuned Use (PTU) deployment
    on a limited number of use cases. This increases the uniformity of requests, enhancing
    cache hit rates and overall performance.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专注于您的部署**：将您的Prompt-Tuned Use（PTU）部署集中在有限的用例上。这增加了请求的一致性，从而提升了缓存命中率和整体性能。'
- en: '**Processed Inference Tokens**, which tracks the number of tokens processed
    in both requests and responses. This is particularly useful for understanding
    the complexity and length of interactions with the service. Monitoring token usage
    can help in optimizing prompts and managing costs effectively. Lastly, the **Provisioned-managed
    Utilization V2** metric monitors the utilization % of the PTU-M. Utilization percentage
    for a provisioned-managed deployment is calculated using the formula: (PTUs consumed
    / PTUs deployed) x 100\. When this utilization reaches or exceeds 100%, calls
    are throttled, and error code 429 is returned. Together, these metrics provide
    a comprehensive view of the service’s performance, helping users to identify areas
    for improvement and ensure optimal operation.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理过的推理标记**，它跟踪请求和响应中处理的标记数。这对于理解与服务的交互的复杂性和长度尤其有用。监控标记使用情况有助于优化提示并有效地管理成本。最后，**Provisioned-managed
    Utilization V2**指标监控PTU-M的使用百分比。预配置管理部署的使用百分比通过以下公式计算：（消耗的PTU / 部署的PTU）x 100。当此使用率达到或超过100%时，调用会被限制，并返回错误代码429。这些指标共同提供了服务性能的全面视图，帮助用户识别改进的领域，并确保最佳的操作。'
- en: 'For a list of all platform metrics collected for Azure OpenAI and similar Azure
    AI services by Azure Monitor follow the link given: [https://learn.microsoft.com/en-us/azure/ai-services/openai/monitor-openai-reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/monitor-openai-reference).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看由 Azure Monitor 收集的所有平台度量数据，涵盖 Azure OpenAI 和类似的 Azure AI 服务，请访问以下链接：[https://learn.microsoft.com/en-us/azure/ai-services/openai/monitor-openai-reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/monitor-openai-reference)。
- en: You can export all metrics using diagnostic settings in Azure Monitor. To examine
    logs and metric data using queries in Azure Monitor Log Analytics, it’s necessary
    to set up diagnostic settings for both your Azure OpenAI resource and your Log
    Analytics workspace. Now, let’s set up the diagnostic settings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 Azure Monitor 中的诊断设置导出所有度量数据。要使用 Azure Monitor Log Analytics 中的查询检查日志和度量数据，必须为你的
    Azure OpenAI 资源和日志分析工作区都设置诊断设置。现在，让我们开始设置诊断设置。
- en: Go to your Azure OpenAI resource page and choose **Diagnostic settings** under
    **Monitoring** on the left side. On the **Diagnostic settings** page, pick **Add**
    **diagnostic setting**.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到你的 Azure OpenAI 资源页面，在左侧的**监控**下选择**诊断设置**。在**诊断设置**页面，选择**添加****诊断设置**。
- en: '![Figure 12.2: Adding diagnostic settings](img/B21019_12_2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2：添加诊断设置](img/B21019_12_2.jpg)'
- en: 'Figure 12.2: Adding diagnostic settings'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：添加诊断设置
- en: 'On the **Diagnostic settings** page, do the following steps:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**诊断设置**页面，执行以下步骤：
- en: Select **Send to Log** **Analytics** workspace.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**发送到日志分析**工作区。
- en: Pick your Azure account subscription.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的 Azure 账户订阅。
- en: Pick your **Log** **Analytics** workspace.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的**日志分析**工作区。
- en: Under **Logs**, choose **allLogs**.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**日志**下，选择**所有日志**。
- en: Under **Metrics**, choose **AllMetrics**.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**度量**下，选择**所有度量**。
- en: "![Figure 12.3: Configuring diagnostic setting\uFEFF.](img/B21019_12_3.jpg)"
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3：配置诊断设置](img/B21019_12_3.jpg)'
- en: 'Figure 12.3: Configuring diagnostic setting.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：配置诊断设置。
- en: Choose a name for the **Diagnostic setting** to store the configuration.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为**诊断设置**选择一个名称，以便存储该配置。
- en: Click on **Save**.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**。
- en: Once the diagnostic settings are set up, you can use metrics and log data for
    your Azure OpenAI resource in your Log Analytics workspace.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了诊断设置，你就可以在日志分析工作区中使用 Azure OpenAI 资源的度量和日志数据。
- en: Next, we will learn how to use Kusto queries to track the logs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用 Kusto 查询来跟踪日志。
- en: Monitoring logs using Kusto queries
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Kusto 查询监控日志。
- en: To create the logs that we will monitor, we need to make API calls first. Do
    the following steps to produce the logs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们将监控的日志，首先需要发出 API 调用。请按照以下步骤生成日志。
- en: Go to chat completion inside Azure AI Foundry portal
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 Azure AI Foundry 门户中的聊天完成页面。
- en: Ask any questions in the portal
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在门户中提问任何问题。
- en: "![Figure 12.4: \uFEFFIssuing API call to generate logs\uFEFF](img/B21019_12_4.jpg)"
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4：发出 API 调用生成日志](img/B21019_12_4.jpg)'
- en: 'Figure 12.4: Issuing API call to generate logs'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：发出 API 调用生成日志
- en: When you use the **Chat completions playground** to enter any text, it produces
    metrics and log data for your Azure OpenAI resource. You can use the Kusto query
    language to query the monitoring data in the Log Analytics workspace for your
    resource.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用**聊天完成沙盒**输入任何文本时，它会为你的 Azure OpenAI 资源生成度量和日志数据。你可以使用 Kusto 查询语言在日志分析工作区查询监控数据。
- en: Next, we will use Kusto to search for logs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Kusto 搜索日志。
- en: On your Azure OpenAI resource page, choose **Logs** from the Monitoring section
    on the left side of the screen.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 Azure OpenAI 资源页面，选择屏幕左侧监控部分中的**日志**。
- en: Choose the **Log Analytics** workspace where you set up diagnostics for your
    Azure OpenAI resource.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你为 Azure OpenAI 资源设置了诊断的**日志分析**工作区。
- en: From the left pane of the **Log Analytics** workspace page, choose **Logs**.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**日志分析**工作区页面的左侧窗格中，选择**日志**。
- en: By default, the Azure portal shows a window with sample queries and suggestions.
    You can exit this window.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，Azure 门户会显示一个包含示例查询和建议的窗口。你可以退出此窗口。
- en: To run the following examples, type the Kusto query in the editor area at the
    top of the **Query** window, and then choose **Run**.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行以下示例，请在**查询**窗口顶部的编辑区域中输入 Kusto 查询，然后选择**运行**。
- en: '[PRE0]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This query shows a sample of 100 audit records. Audit logs in Azure Monitor
    capture detailed telemetry about log queries executed within the system. They
    provide information such as the time a query was run, the identity of the user
    who executed it, the tool used to run the query, the query text itself, and performance
    metrics related to the query’s execution
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该查询展示了 100 条审核记录的示例。Azure Monitor 中的审核日志捕获有关在系统内执行的日志查询的详细遥测信息。它们提供的信息包括查询运行的时间、执行查询的用户身份、用于执行查询的工具、查询文本本身，以及与查询执行相关的性能指标。
- en: "![Figure 12.5: \uFEFFAnalyzing logs using Kusto queries\uFEFF](img/B21019_12_5.jpg)"
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5: 使用 Kusto 查询分析日志](img/B21019_12_5.jpg)'
- en: 'Figure 12.5: Analyzing logs using Kusto queries'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.5: 使用 Kusto 查询分析日志'
- en: Now, you have learned how to check AOAI logs, the next section will cover the
    AOAI service limits and Quota that are important for designing the GenAI solution.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了如何检查 AOAI 日志，接下来的部分将介绍对于设计 GenAI 解决方案至关重要的 AOAI 服务限制和配额。
- en: Azure OpenAI Service quotas and limits
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务配额和限制
- en: Azure OpenAI Pay-as-you-go model is a shared tenant GPU infrastructure for inferencing.
    Therefore, AOAI service has some service limit on how you can use this resource.
    In this section we will describe the various limits and quotas for different AOAI
    model and how to prevent throttling by following some best practices.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 按需付费模式是一个共享租户 GPU 基础设施，用于推理。因此，AOAI 服务对如何使用此资源有一些服务限制。在本节中，我们将描述不同
    AOAI 模型的各种限制和配额，并介绍如何通过遵循一些最佳实践来防止节流。
- en: 'At this time of writing, each Azure subscription can access up to 30 OpenAI
    resources per region. For DALL-E models, the default quota limits are 2 concurrent
    requests for DALL-E 2 and 2 capacity units (equivalent to 6 requests per minute)
    for DALL-E 3\. Whisper, another model, has a limit of 3 requests per minute. The
    maximum number of prompt tokens per request varies by model, and more detailed
    information can be found in the Azure OpenAI Service models documentation in the
    given link: [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure%2Cglobal-standard%2Cstandard-chat-completions#gpt-4-and-gpt-4-turbo-models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure%2Cglobal-standard%2Cstandard-chat-completions#gpt-4-and-gpt-4-turbo-models).
    Fine-tuning capabilities are capped at 5 model deployments, with a total of 100
    training jobs per resource. However, only one training job can run simultaneously
    per resource, and up to 20 jobs can be queued. Each resource can contain up to
    50 files for fine-tuning, with a total size limit of 1 GB, and each training job
    must not exceed 720 hours or contain more than 2 billion tokens.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，每个 Azure 订阅可以在每个区域访问最多 30 个 OpenAI 资源。对于 DALL-E 模型，默认配额限制是 DALL-E 2 支持
    2 个并发请求，DALL-E 3 支持 2 个容量单位（相当于每分钟 6 个请求）。Whisper 模型的请求限制为每分钟 3 次。每个请求的最大提示令牌数因模型而异，更多详细信息可以在给定的链接中找到：[https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure%2Cglobal-standard%2Cstandard-chat-completions#gpt-4-and-gpt-4-turbo-models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure%2Cglobal-standard%2Cstandard-chat-completions#gpt-4-and-gpt-4-turbo-models)。微调能力被限制为每个模型最多
    5 次部署，每个资源总共有 100 个训练作业。然而，每个资源只能同时运行一个训练作业，并且最多可以排队 20 个作业。每个资源最多可以包含 50 个微调文件，总大小限制为
    1 GB，每个训练作业的时长不得超过 720 小时，且不得包含超过 20 亿个令牌。
- en: Additional constraints include a maximum upload size of 16 MB for all files
    in Azure OpenAI on your data, with a limit of 2048 inputs in an array for embeddings
    and 2048 messages for chat completions. The maximum number of functions and tools
    for chat completions is set at 128 each. Provisioned throughput units per deployment
    are capped at 100,000\. When using the API or AI Studio, each Assistant or thread
    can handle up to 10,000 files, but this is reduced to 20 files when using Azure
    AI Foundry. The file size limit for Assistants and fine-tuning is 512 MB, and
    the token limit for Assistants is 2,000,000 tokens. GPT-4o can handle up to 10
    images per request, GPT-4 turbo have a default maximum token limit of 16,384,
    which can be increased to avoid truncated responses. Lastly, API requests can
    include up to 10 custom headers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的限制包括：所有文件的最大上传大小为 16 MB，嵌入中的数组输入限制为 2048，聊天完成消息的限制为 2048。每个聊天完成的最大函数和工具数量为
    128。每个部署的预配吞吐量单元上限为 100,000。使用 API 或 AI Studio 时，每个助手或线程最多可以处理 10,000 个文件，但在使用
    Azure AI Foundry 时，此限制降至 20 个文件。助手和微调的文件大小限制为 512 MB，助手的令牌限制为 2,000,000 令牌。GPT-4o
    每次请求最多可以处理 10 张图片，GPT-4 Turbo 的默认最大令牌限制为 16,384，可以增加此限制以避免响应被截断。最后，API 请求最多可以包含
    10 个自定义头部。
- en: 'The quota limits for different Azure OpenAI models vary by region. These limits
    are determined per Azure Subscription, per region, and per model. Azure OpenAI
    offers two distinct deployment types for newer models like GPT-4 Turbo, GPT-4o,
    and GPT-4o-mini: “Standard” and “Global Standard.” The “Standard” deployment is
    region-specific, resulting in fewer **Tokens per Minute** (**TPM**) compared to
    the “Global Standard” deployment, which operates globally. Customers with an “Enterprise
    Agreement” with Microsoft receive higher quotas for both deployment types. In
    the “Global Standard” deployment, inference can occur anywhere worldwide. Customers
    requiring **General Data Protection Regulation** (**GDPR**), an EU data privacy
    law that protects personal data, grants individuals control over their information,
    and imposes strict compliance, requirements on organizations, with significant
    penalties for non-compliance. may prefer the “Standard” or “Data Zone” deployment,
    whereas those prioritizing maximum throughput might opt for the “Global Standard”
    deployment. The “Global Standard” deployment is inherently highly available, eliminating
    the need for a separate load balancing mechanism. However, if you choose the “Standard”
    deployment and require high availability, you will need to set up load balancing
    using the Azure API Management service. For the latest quota limits for Azure
    OpenAI models, visit: [https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 Azure OpenAI 模型的配额限制因地区而异。这些限制是根据每个 Azure 订阅、每个地区和每个模型来确定的。Azure OpenAI 为像
    GPT-4 Turbo、GPT-4o 和 GPT-4o-mini 这样的较新模型提供了两种不同的部署类型：“标准”部署和“全球标准”部署。“标准”部署是地区特定的，因此相较于全球运行的“全球标准”部署，其**每分钟令牌数**（**TPM**）较少。与微软签订“企业协议”的客户将获得更高的配额，无论是哪种部署类型。在“全球标准”部署中，推理可以在全球任何地方进行。需要符合**通用数据保护条例**（**GDPR**）的客户，GDPR
    是一项欧盟数据隐私法，旨在保护个人数据，赋予个人对其信息的控制权，并对组织施加严格的合规要求，对不合规者处以重大罚款，可能会倾向选择“标准”或“数据区域”部署，而那些优先考虑最大吞吐量的客户则可能选择“全球标准”部署。“全球标准”部署本身具备高可用性，消除了单独负载均衡机制的需求。然而，如果选择“标准”部署并需要高可用性，则需要使用
    Azure API 管理服务设置负载均衡。有关 Azure OpenAI 模型的最新配额限制，请访问：[https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits)。
- en: In summary, Azure OpenAI provides flexible deployment options tailored to different
    needs, ensuring that you can select the best configuration for your specific requirements,
    whether they are compliance-related or performance-driven.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Azure OpenAI 提供了灵活的部署选项，针对不同的需求量身定制，确保您可以根据具体要求选择最佳配置，无论是合规性相关的还是性能驱动的。
- en: Best Practices to prevent throttling
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免限制的最佳实践
- en: To understand the best practice to avoid throttling, we need to know how these
    rate limits are calculated in the background.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解避免限制的最佳实践，我们需要了解这些速率限制是如何在后台计算的。
- en: 'TPM rate limits are calculated from the highest number of tokens that a request
    is expected to process when the request is accepted. It’s different from the token
    count used for billing, which is determined after all processing is done. When
    Azure OpenAI gets a request, it calculates an approximate max processed-token
    count that covers the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TPM 速率限制是根据接受请求时，预计请求处理的最大令牌数来计算的。它与用于计费的令牌数不同，后者是在所有处理完成后确定的。当 Azure OpenAI
    接收到请求时，它会计算出一个近似的最大处理令牌数，涵盖以下内容：
- en: Prompt text and count
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示文本和计数
- en: The `max_tokens` parameter setting
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_tokens` 参数设置'
- en: The `best_of` parameter setting
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_of` 参数设置'
- en: The AOAI model deployment endpoint keeps track of a token count for all requests
    that is reset every minute, based on the estimated max-processed-token count for
    each request. If the token count reaches the TPM rate limit value at any point
    during that minute, then subsequent requests will get a 429-error response code
    until the counter resets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI 模型部署端点会跟踪所有请求的令牌计数，每分钟重置一次，基于每个请求的估计最大处理令牌数。如果在该分钟内的任何时刻，令牌计数达到了 TPM 速率限制值，那么后续请求将会收到
    429 错误响应代码，直到计数器重置。
- en: The **Request-Per-Minute** (**RPM**) rate limit determines the number of requests
    your organization can make to the OpenAI API within a one-minute timeframe. This
    limit helps prevent server overload and ensures equitable usage among all users.
    The specific RPM limit varies based on the endpoint and the type of account you
    possess. RPM limits assume that requests are evenly spread out over the minute.
    If this even distribution is not maintained, requests may receive a 429-error
    response even if the overall limit has not been breached within the minute.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**每分钟请求数**（**RPM**）速率限制决定了你的组织在一分钟内可以向 OpenAI API 发送的请求数量。此限制有助于防止服务器过载，并确保所有用户之间的公平使用。具体的
    RPM 限制取决于端点和你拥有的账户类型。RPM 限制假定请求均匀分布在这一分钟内。如果没有保持这种均匀分布，即使在一分钟内没有超过总体限制，请求也可能会收到
    429 错误响应。'
- en: To enforce this, the Azure OpenAI Service monitors the rate of incoming requests
    over shorter intervals, typically 1 or 10 seconds. If the request count during
    these brief intervals surpasses what is allowed by the RPM limit, subsequent requests
    will receive a 429-error code until the next interval check. For instance, if
    the service checks request rates in 1-second intervals, a deployment with a 600-RPM
    limit will be rate-limited if more than 10 requests are sent in any 1-second period
    (since 600 requests per minute translates to 10 requests per second).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制执行这一点，Azure OpenAI 服务会在更短的时间间隔内监控传入请求的速率，通常是 1 或 10 秒。如果在这些短时间内的请求数量超过 RPM
    限制所允许的数量，后续请求将会收到 429 错误代码，直到下一个时间间隔检查。例如，如果服务在 1 秒的间隔内检查请求速率，那么如果在任何 1 秒内发送的请求超过
    10 个（因为 600 每分钟请求相当于每秒 10 个请求），则该部署将受到速率限制。
- en: In summary, understanding and adhering to the RPM rate limits is crucial for
    optimizing API usage and avoiding disruptions. This mechanism ensures both system
    stability and fair access for all users.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，理解并遵守 RPM 速率限制对于优化 API 使用和避免中断至关重要。这个机制确保了系统的稳定性以及所有用户的公平访问。
- en: 'To reduce problems caused by rate limits, it’s advisable to apply the following
    methods:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少因速率限制而导致的问题，建议应用以下方法：
- en: Add retry logic to your application. This approach can help you when you face
    request rate limits, since these limits change after every 10-second interval.
    Depending on your quota, the change time could be even quicker.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的应用程序中添加重试逻辑。当你遇到请求速率限制时，这种方法可以帮助你，因为这些限制会在每个 10 秒的间隔后发生变化。根据你的配额，变化时间可能会更快。
- en: Do not make the workload change abruptly. Make the workload higher slowly.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要突然改变工作负载。应该逐步增加工作负载。
- en: Experiment with various ways of raising the load.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的方式来增加负载。
- en: To improve performance, raise the quota for your model or split the load among
    different subscriptions or regions. When you reach the quota limits of turbo or
    gpt-4-8k, think about using other options like turbo-16k or gpt-4-32k. These are
    separate quota buckets within the Azure OpenAI Service.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了提高性能，提升模型的配额或将负载分配到不同的订阅或区域。当你达到 turbo 或 gpt-4-8k 的配额限制时，可以考虑使用其他选项，如 turbo-16k
    或 gpt-4-32k。这些都是 Azure OpenAI 服务内的独立配额桶。
- en: Keep the max_tokens parameter as low as possible while making sure it meets
    your needs.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持 `max_tokens` 参数尽可能低，同时确保它能够满足你的需求。
- en: Next, we will discuss how to manage the quota from the AOAI portal.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何从 AOAI 门户管理配额。
- en: Azure OpenAI quota management
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure OpenAI 配额管理
- en: Quota lets you control how the rate limits are distributed among the deployments
    in your subscription. In this section we will show you how to manage your Azure
    OpenAI quota.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 配额让你控制如何在订阅中的各个部署之间分配速率限制。在本节中，我们将展示如何管理你的 Azure OpenAI 配额。
- en: Azure OpenAI’s quota feature allows you to allocate rate limits to your deployments,
    up to an overall limit known as your “quota.” This quota is assigned to your subscription
    on a per-region, per-model basis and is measured in **Tokens-per-Minute** (**TPM**).
    When you create Azure OpenAI service, you receive a default quota for most of
    the available models (refer to the previous section for default quotas for each
    model).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 的配额功能允许你为部署分配速率限制，最高可设置为一个称为“配额”的总限制。该配额按每个区域、每个模型分配给你的订阅，并以 **每分钟令牌数（TPM）**
    为单位进行衡量。当你创建 Azure OpenAI 服务时，你会收到一个默认配额，适用于大多数可用模型（有关每个模型的默认配额，请参阅前一节）。
- en: As you create deployments, you’ll assign TPM to each one, and the available
    quota for that model will decrease by the assigned amount. You can continue to
    create and assign TPM to deployments until you reach your quota limit. Once the
    quota is reached, you can only create new deployments of that model by reallocating
    TPM from existing deployments of the same model or by requesting and receiving
    approval for a quota increase in the desired region. For instance, a customer
    with a quota of 240,000 TPM for the GPT-35-Turbo model in the East US region can
    utilize this quota in various configurations. They could opt for a single GPT-35-Turbo
    deployment with a 240K TPM limit, or they might choose to have two separate deployments,
    each with 120K TPM. Alternatively, they can distribute their quota across multiple
    deployments in any combination, as long as the total TPM does not exceed 240K
    within the East US region.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建部署时，你将为每个部署分配 TPM，并且该模型的可用配额将减少分配的数量。你可以继续创建并为部署分配 TPM，直到达到配额限制。一旦配额达到，你只能通过从现有部署中重新分配
    TPM，或者通过请求并获得批准来增加所需区域的配额，才能创建该模型的新部署。例如，一位在美国东部地区拥有 240,000 TPM 配额的客户可以以多种配置方式使用该配额。他们可以选择创建一个
    GPT-35-Turbo 部署，限制为 240K TPM，或者选择创建两个单独的部署，每个部署为 120K TPM。或者，他们可以将配额分配到多个部署中，任意组合，只要总
    TPM 不超过 240K，且位于美国东部地区。
- en: In essence, Azure OpenAI’s quota management system helps you efficiently allocate
    and manage your API usage, ensuring that you can maximize the utility of your
    deployments while staying within your allotted limits.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，Azure OpenAI 的配额管理系统帮助你高效地分配和管理 API 使用，确保你可以在不超出配额限制的情况下最大化部署的效用。
- en: 'The inferencing requests for AOAI model deployment will have a rate limit based
    on how much TPM the deployment is assigned. The TPM assignment also determines
    the value of the **Requests-Per-Minute (RPM)** rate limit, which follows this
    ratio: 6 RPM per 1000 TPM.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI 模型部署的推理请求将根据分配给该部署的 TPM 数量进行速率限制。TPM 的分配还会决定 **每分钟请求数（RPM）** 速率限制的值，遵循以下比例：每
    1,000 TPM 对应 6 RPM。
- en: Next, we will discuss how to allocate the quota from AOAI portal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何从 AOAI 门户分配配额。
- en: Assign Quota
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分配配额
- en: You can choose how many TPM you want to allocate to your AOAI model deployment
    when you create it. TPM can be changed by 1,000 at a time and will determine the
    TPM and RPM rate limits that apply to your deployment, as explained in the previous
    section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 AOAI 模型部署时，你可以选择要分配给模型部署的 TPM 数量。TPM 可以每次增加 1,000，并将决定适用于你的部署的 TPM 和 RPM
    限制，如前面一节所述。
- en: To allocate the quote, do the following steps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要分配配额，请执行以下步骤。
- en: Login to Azure OpenAI Portal
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到 Azure OpenAI 门户
- en: Click on **Deployments** under **Shared resources**
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **部署** 下的 **共享资源**
- en: Select the existing deployment
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择现有的部署
- en: Click on **Edit deployment**
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **编辑部署**
- en: Set the desired TPM
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置所需的 TPM
- en: Click **Save** **and close.**
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **保存** **并关闭**。
- en: "![Figure 12.6: \uFEFFAssigning quota to AOAI deployment\uFEFF](img/B21019_12_6.jpg)"
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: "![图 12.6：\uFEFF为 AOAI 部署分配配额\uFEFF](img/B21019_12_6.jpg)"
- en: 'Figure 12.6: Assigning quota to AOAI deployment'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：为 AOAI 部署分配配额
- en: Based on the type of deployment you select for your model; you can navigate
    to either the “**Azure OpenAI Standard**” or “**Azure OpenAI Global-Standard**”
    tab. Once you’ve configured the quota for various deployments, you can view how
    your quota is allocated across different regional or global deployments by visiting
    the **Quotas** page under **Shared Resources**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您为模型选择的部署类型，您可以导航到“**Azure OpenAI 标准**”或“**Azure OpenAI 全球标准**”标签页。配置完不同部署的配额后，您可以通过访问
    **共享资源** 下的 **配额** 页面，查看配额在不同区域或全球部署中的分配情况。
- en: "![Figure 12.7: Overall quota assignment for AOAI deployments\uFEFF](img/B21019_12_7.jpg)"
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.7：AOAI 部署的整体配额分配](img/B21019_12_7.jpg)'
- en: 'Figure 12.7: Overall quota assignment for AOAI deployments'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7：AOAI 部署的整体配额分配
- en: 'This Quotas page has four fields:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该配额页面包含四个字段：
- en: '**Quota Name**: Each model type has a quota value for each region. The quota
    applies to all versions of that model. The quota name can be made larger in the
    UI to display the deployments that are using the quota.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配额名称**：每个模型类型在每个区域都有一个配额值。该配额适用于该模型的所有版本。可以在用户界面中将配额名称放大，以显示使用该配额的部署。'
- en: '**Deployment**: Model deployments grouped by a model class.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署**：按模型类别分组的模型部署。'
- en: '**Usage/Limit:** This shows the amount of quota consumed by deployments and
    the amount of quota allocated for this subscription and region, under the quota
    name.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用量/限制**：显示部署所消耗的配额量以及为此订阅和区域分配的配额量，配额名称下显示。'
- en: '**Request quota**: The link in this field leads to a form where requests for
    more quota for a specific AOAI model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**请求配额**：此字段中的链接指向一个表单，您可以在该表单中申请特定 AOAI 模型的更多配额。'
- en: Now, you have learned how to manage quota in APOAI resource, next section we
    will explore Provisioned Throughput Unit concept.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解如何管理 APOAI 资源中的配额，接下来的部分将探讨预配置吞吐量单元的概念。
- en: Azure OpenAI Provisioned Throughput Unit
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure OpenAI 预配置吞吐量单元
- en: The **Provisioned Throughput Unit** (**PTU**) feature lets you set the throughput
    you want for your application. It gives you more control over how you use and
    configure OpenAI’s large language models at a scale. It provides a dedicated compute
    to OpenAI models with a guaranteed throughput. You can set the total number of
    throughput units (PTU) you want and have the ability and control to distribute
    your commitment to OpenAI model you prefer. Each model needs a different amount
    of PTUs to run, for example GPT-3.5 needs less amount of PTUs compared to GPT4\.
    You can select from various commitment options. With a 1-month or 1-year commitment,
    you can secure provisioned throughput and get savings in pricing. The provisioned
    throughput model offers more control and flexibility over workload needs, ensuring
    that the system is ready when higher workloads arise.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**预配置吞吐量单元**（**PTU**）功能允许您为应用程序设置所需的吞吐量。它使您能够更好地控制在大规模下如何使用和配置 OpenAI 的大型语言模型。它为
    OpenAI 模型提供专用计算，并保证吞吐量。您可以设置所需的吞吐量单元（PTU）总数，并拥有将承诺分配给您首选的 OpenAI 模型的能力和控制权。每个模型需要不同数量的
    PTU 运行，例如，GPT-3.5 比 GPT-4 需要的 PTU 少。您可以从多种承诺选项中进行选择。通过 1 个月或 1 年的承诺，您可以确保预配置的吞吐量并获得定价优惠。预配置吞吐量模型提供了更多的控制权和灵活性，能够满足工作负载的需求，确保在更高工作负载出现时系统可以随时准备好。'
- en: 'This feature enables:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能支持：
- en: '**Consistent performance**: reliable peak latency and capacity for steady workloads.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致的性能**：可靠的峰值延迟和稳定的工作负载容量。'
- en: '**Fixed performance capacity**: A deployment sets the level of throughput.
    After deployment, the throughput is ready to use regardless of actual demand.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定性能容量**：部署设置吞吐量的水平。部署后，无论实际需求如何，吞吐量都可以随时使用。'
- en: '**Cost savings**: High throughput workloads may lower costs vs token-based
    usage.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节省成本**：高吞吐量工作负载可能比基于令牌的使用降低成本。'
- en: 'AOAI provides two kinds of PTUs:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI 提供两种类型的 PTU：
- en: '**Classic PTU**: This particular type of PTU has a high entry threshold due
    to the minimum PTU requirements for specific models, leading to substantial initial
    costs. Additionally, future cost increments are also significant. New customers
    can no longer purchase this kind of PTU. Microsoft advises existing customers
    to migrate from this PTU to PTU Managed.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典 PTU**：这种特定类型的 PTU 由于某些模型的最低 PTU 要求，具有较高的入门门槛，导致初期成本较大。此外，未来的成本增长也很显著。新客户不再能购买这种类型的
    PTU。微软建议现有客户将这种 PTU 迁移到 PTU 管理版。'
- en: '**PTU Managed**: This kind of PTU is also known as fractional PTU. It means
    that the minimum PTU needed for a certain model is low at the start. So, the initial
    cost is lower than Classic PTU. And future increments are also smaller.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PTU托管**：这种PTU也被称为分数PTU。意味着某个型号的最小PTU需求一开始较低，因此初始成本低于经典PTU，未来的增量也更小。'
- en: '**PTU Managed** (**PTU-M**) is recommended for most of the customers as it
    provides lower entry point cost with smaller increments and better monitoring
    metric compared to classic PTU.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**PTU托管**（**PTU-M**）推荐给大多数客户，因为与经典PTU相比，它提供了更低的入门成本、更小的增量和更好的监控指标。'
- en: The process for purchasing PTU-M is entirely self-service, eliminating the need
    to contact Microsoft’s account team. You can purchase PTU-M through Azure’s **Reserved
    Instance** (**RI**) purchasing mechanism. At this time of writing this book, when
    opting for PTU-M, you have the choice between a monthly RI at $260 per PTU-M or
    an annual RI at $221 per PTU-M within a specific region. Additionally, there is
    an option to purchase PTU-M on an hourly basis at a rate of $2 per PTU-M, with
    no long-term commitment required. For detailed pricing information for each PTU-M
    model, refer to Chapter 2\. You are free to discontinue the hourly PTU at any
    time. However, it’s important to note that PTU-M availability is contingent on
    the capacity available in each region for the specified model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 购买PTU-M的过程完全是自助式的，无需联系微软的账户团队。你可以通过Azure的**预留实例**（**RI**）购买机制来购买PTU-M。在撰写本书时，选择PTU-M时，你可以选择每个PTU-M每月$260的RI或每个PTU-M每年$221的RI（特定区域内）。此外，还可以选择按小时购买PTU-M，费用为每个PTU-M
    $2，无需长期承诺。如需详细的各PTU-M型号定价信息，请参见第二章。你可以随时停止按小时购买PTU-M。然而，重要的是要注意，PTU-M的可用性取决于每个区域指定型号的容量。
- en: After purchasing PTU-M via the RI mechanism, you are not restricted to a specific
    model. You have the flexibility to switch to any model, provided it meets the
    minimum PTU-M requirements for the chosen region. For detailed information on
    the minimum PTU-M and scaling requirements for each model, please refer to *Figure
    12**.8.*
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RI机制购买PTU-M后，你不受特定型号的限制。只要满足所选区域的最小PTU-M要求，你可以灵活地切换到任何型号。有关每个型号的最小PTU-M和扩展要求的详细信息，请参见*图12.8*。
- en: '![Figure 12.8: Minimum PTU-M for each model](img/B21019_12_8.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8：每个型号的最小PTU-M](img/B21019_12_8.jpg)'
- en: 'Figure 12.8: Minimum PTU-M for each model'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：每个型号的最小PTU-M
- en: For instance, if you acquire a 200-unit RI of PTU-M with a monthly commitment
    in the East US region, you could allocate 100 units to GPT-4-Turbo, 50 units to
    GPT4-o, and another 50 units to GPT4-o-mini. The allocation can be adjusted as
    needed the very next day without waiting for the renewal period. However, once
    you have made the PTU-M reservation for a specific region, you cannot increase,
    decrease, or exchange it. To modify the reservation, you would need to either
    purchase an additional reservation or cancel the existing one, which may result
    in early termination fees.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你在东美国区域购买了200个单位的PTU-M月度预留实例，可以将100个单位分配给GPT-4-Turbo，50个单位分配给GPT4-o，另外50个单位分配给GPT4-o-mini。分配可以在第二天根据需要进行调整，无需等待续订期。然而，一旦你为特定区域预定了PTU-M，就无法增加、减少或交换。要修改预定，你需要购买额外的预定或取消现有预定，这可能会导致提前终止费用。
- en: 'AOAI PTU provides three deployment options:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI PTU提供三种部署选项：
- en: '**Global Deployment**: Data processing can occur in any Azure region worldwide
    and includes built-in data plane high availability (HA).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球部署**：数据处理可以发生在全球任何Azure区域，并且包括内置的数据平面高可用性（HA）。'
- en: '**Data Zone Deployment**: Currently limited to EU and US zones, where data
    processing is confined to the selected region, either the US or the EU. This deployment
    also offers by design data plane HA'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据区域部署**：目前仅限于欧盟和美国区域，其中数据处理仅限于所选区域，即美国或欧盟。此部署还提供了设计上的数据平面高可用性（HA）。'
- en: '**Regional Deployment**: Data processing is confined to the specific Azure
    region where the AOAI service is hosted, and it does not come with built-in data
    plane **high availability** (**HA**). However, you can implement HA using **Azure
    API Management** (**APIM**), which will be covered in later sections.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域部署**：数据处理仅限于托管AOAI服务的特定Azure区域，并且没有内置的数据平面**高可用性**（**HA**）。不过，你可以使用**Azure
    API管理**（**APIM**）来实现HA，相关内容将在后续部分讲解。'
- en: In all deployment types, your data residency remains within the region where
    your AOAI service is hosted.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有部署类型中，您的数据驻留在托管您的 AOAI 服务的区域内。
- en: So far, you have explored various AOAI PTU options and their purchasing and
    deployment options. Next, we will discuss how to appropriately size the PTU before
    committing to a reservation for PTU-M.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已探索了各种 AOAI PTU 选项及其购买和部署选项。接下来，我们将讨论如何在预订 PTU-M 之前，适当确定 PTU 的尺寸。
- en: PTU-M Sizing
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PTU-M 尺寸
- en: PTU-M is a commitment-based pricing model, and it doesn’t have auto scaling
    features as of now. So, if your workload demands additional computing resources,
    you must acquire them in advance before utilization. Therefore, accurately sizing
    the PTU is essential prior to purchase. AOAI provides a PTU-M calculator within
    Azure AI Foundry to help estimate the appropriate PTU-M size for your specific
    workload.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: PTU-M 是一种基于承诺的定价模型，目前没有自动扩展功能。因此，如果您的工作负载需要额外的计算资源，您必须在使用之前预先获取这些资源。因此，准确确定
    PTU 的尺寸在购买前非常重要。AOAI 在 Azure AI Foundry 中提供了一个 PTU-M 计算器，帮助估算适合您特定工作负载的 PTU-M
    尺寸。
- en: 'To use the calculator, follow the below steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用计算器，请按照以下步骤操作：
- en: Loging to your Azure portal
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到您的 Azure 门户
- en: Pick the AOAI resource for the given region where you will be requesting the
    PTU-M
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您将请求 PTU-M 的特定区域的 AOAI 资源
- en: Open Azure AI Foundry.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Azure AI Foundry。
- en: Navigate to **Quotas** under **Management**
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**管理**下导航至**配额**
- en: Select the **Azure OpenAI** **Provisioned** tab
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**Azure OpenAI** **已配置**选项卡
- en: Choose the **Capacity calculator**
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**容量计算器**
- en: Select the model you want to use
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您想要使用的模型
- en: Choose the model version
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型版本
- en: Provide your workload name, Peaks calls per min
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供您的工作负载名称、每分钟的峰值调用次数
- en: For multimodal use cases (only applicable for GPT4-o and GPT4-o-mini), specify
    the number of tokens used in prompt calls for both text and image inputs separately.
    This typically includes the total token count for your input question and the
    context size for text. Additionally, indicate the number of tokens used in the
    response.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多模态用例（仅适用于 GPT4-o 和 GPT4-o-mini），分别指定文本和图像输入提示调用中使用的令牌数量。这通常包括您的输入问题的总令牌数和文本的上下文大小。此外，还需指明响应中使用的令牌数量。
- en: Click **Calculate** to calculate how much PTU you need for each kind of workload.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**计算**以计算您为每种工作负载所需的 PTU。
- en: "![Figure 12.9: PTU-M sizing Calculation\uFEFF](img/B21019_12_9.jpg)"
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.9：PTU-M 尺寸计算](img/B21019_12_9.jpg)'
- en: 'Figure 12.9: PTU-M sizing Calculation'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9：PTU-M 尺寸计算
- en: Once you have identified the PTU-M requirements for your application, you can
    proceed to purchase them through Azure reservations or hourly on demand PTU options.
    We will discuss this process in detail in the following section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您确定了应用程序的 PTU-M 要求，您可以通过 Azure 预留或按小时按需 PTU 选项进行购买。我们将在以下部分详细讨论此过程。
- en: PTU-M Purchase model
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PTU-M 购买模型
- en: AOAI offers two distinct purchasing models for PTU-M.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI 提供两种不同的 PTU-M 购买模型。
- en: '**On-Demand Hourly PTU**: This hourly payments for provisioned deployments
    is perfect for short-term scenarios. This includes tasks like benchmarking the
    quality and performance of new models or temporarily boosting PTU capacity for
    events such as hackathons. Under this model it charges $2/PUT/HR. For instance,
    if you deploy 300 PTUs, you’ll incur costs at the hourly rate multiplied by 300\.
    Which is 2*300 = $600/hr. If a deployment runs for part of an hour, you’ll be
    charged proportionally based on the deployment duration in minutes. if a deployment
    runs for 15 minutes within an hour, the cost would be one-quarter of the hourly
    rate. For example, with an hourly rate of $600, the charge for 15 minutes would
    be $150.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按需每小时 PTU**：这种按小时支付的预配置部署非常适用于短期场景。这包括像基准测试新模型的质量和性能，或者在黑客马拉松等活动中临时增加 PTU
    容量等任务。在此模型下，费用为 $2/PUT/HR。例如，如果您部署了 300 个 PTU，您将按每小时费率乘以 300 收费，即 2*300 = $600/小时。如果部署运行不到一个小时，您将按部署时长（以分钟计）按比例收费。例如，若每小时费率为
    $600，部署 15 分钟的费用为每小时费率的四分之一，费用为 $150。'
- en: By default, customer will have some quota (typically 100 unit) for PTU to deploy
    the model. That you can utilize to deploy the hourly PTU. Here are the steps you
    can follow to deploy the hourly PTU.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认情况下，客户会有一些配额（通常为 100 个单位）用于部署模型的 PTU，您可以利用这些配额来部署按小时计费的 PTU。以下是您可以按照的步骤来部署按小时
    PTU。
- en: Log in to your Azure portal
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到您的 Azure 门户
- en: Pick the AOAI resource for the given region where you will be requesting the
    hourly PTU-M.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您将请求按小时 PTU-M 的特定区域的 AOAI 资源。
- en: Open Azure AI Foundry.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Azure AI Foundry。
- en: Click on **Deployments** under **Shared resources**
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**共享资源**下点击**部署**。
- en: Click on **Deploy model**
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**部署模型**
- en: Choose **Deploy** **base model**
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**部署** **基础模型**
- en: Select the desired model
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择所需的模型
- en: Click on **Confirm**
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确认**
- en: Set the **Deployment Name** and **model version**
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置**部署名称**和**模型版本**
- en: Choose the **Deployment Type** as **Provisioned-managed**
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**部署类型**为**预配置管理**。
- en: Set the **Provisioned throughput** **units (PTUs).**
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置**预配置吞吐量** **单元（PTUs）**。
- en: If you have custom content filter, then choose that one or else select the **DefaultV2**
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有自定义内容过滤器，请选择该过滤器，否则选择**DefaultV2**
- en: Click on**Confirm purchasing**
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确认购买**
- en: '![Figure 12.10: Hourly PTU-M deployment](img/B21019_12_10.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图12.10：按小时部署PTU-M](img/B21019_12_10.jpg)'
- en: 'Figure 12.10: Hourly PTU-M deployment'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10：按小时部署PTU-M
- en: Acknowledge the hourly pricing for the deployment and select **Deploy**
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认部署的按小时计费价格并选择**部署**
- en: Important note
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'If your current quota for deploying a model on PTU in the specified region
    is exhausted, you’ll need to submit a request using the form available at: [https://aka.ms/oai/ptueaquotarequest](https://aka.ms/oai/ptueaquotarequest).
    Requests for less than 1000 PTU will typically be automatically approved within
    one business day. For requests exceeding 1000 PTU, you’ll need to get in touch
    with your Microsoft account representative to secure the necessary allocation.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在指定区域用于部署模型的PTU配额已用尽，你需要通过以下链接提交请求：[https://aka.ms/oai/ptueaquotarequest](https://aka.ms/oai/ptueaquotarequest)。通常，对于少于1000
    PTU的请求，将会在一个工作日内自动批准。超过1000 PTU的请求，你需要联系Microsoft账户代表以确保必要的配额。
- en: Allocating the quota alone does not ensure the availability of model capacity.
    Therefore, it is essential to have both quota and capacity to deploy the model
    in a particular region. If you possess the quota but Microsoft lacks the capacity
    to deploy the model, you will receive a notification (as shown in *Figure 12**.11*)
    indicating that the chosen region currently does not have sufficient capacity.
    The message will then prompt you to select another region that might have the
    required capacity.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 单独分配配额并不能保证模型容量的可用性。因此，部署模型到特定区域时，必须同时具备配额和容量。如果你拥有配额，但Microsoft没有足够的容量来部署模型，你将收到一条通知（如*图12.11*所示），表明所选区域当前没有足够的容量。此时，系统会提示你选择另一个可能具备所需容量的区域。
- en: '![Figure 12.11: Capacity unavailability](img/B21019_12_11.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图12.11：容量不可用](img/B21019_12_11.jpg)'
- en: 'Figure 12.11: Capacity unavailability'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：容量不可用
- en: 'To remove the hourly PTU after deployment, you can simply delete it from Azure
    AI Foundry by following these steps, as illustrated in *Figure 12**.*12:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 部署后要移除按小时计费的PTU，你只需按照以下步骤在Azure AI Foundry中删除它，步骤如*图12.12*所示：
- en: Sign in to your Azure portal.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到你的Azure门户。
- en: Choose the AOAI resource in the specific region where the hourly PTU-M is deployed.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择特定区域内部署的AOAI资源。
- en: Open Azure AI Foundry.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Azure AI Foundry。
- en: Navigate to **Deployments** under Shared resources.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在共享资源下导航到**部署**。
- en: Locate and select the hourly PTU deployment.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到并选择按小时计费的PTU部署。
- en: Press **Delete** to remove it.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按**删除**以移除它。
- en: '![Figure 12.12: Hourly PTU deployment removal](img/B21019_12_12.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图12.12：按小时部署PTU移除](img/B21019_12_12.jpg)'
- en: 'Figure 12.12: Hourly PTU deployment removal'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：按小时部署PTU移除
- en: Running PTUs continuously (24x7) can be costly. To address this, AOAI provides
    an alternative PTU purchase option called Azure Reservations, offering significant
    discounts for long-term use. We’ll explore this next.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 持续运行PTU（24x7）可能会产生高昂费用。为了解决这个问题，AOAI提供了一种名为Azure预留的替代PTU购买选项，针对长期使用提供显著折扣。接下来我们将探讨这一点。
- en: '**Azure Reservations:** This purchasing option is most economical for extended
    use. With Azure OpenAI Provisioned reservation, you receive a discount by committing
    to a set number of PTUs for either a month or a year. You can purchase the same
    from Azure reservation portal. There are few points you need to consider before
    buying AOAI PTU reservation'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure预留**：此购买选项对于长期使用最具成本效益。通过Azure OpenAI预配置预留，你可以通过承诺一定数量的PTU，获得月度或年度的折扣。你可以通过Azure预留门户购买。购买AOAI
    PTU预留之前，需要考虑以下几个要点：'
- en: 'Reservations are purchased on a regional basis and can be tailored to cover
    usage across multiple model deployments as long as model minimum PTU requirements
    meets (refer Fig: 12.8). The scope of reservations can include:'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预定是按地区购买的，可以根据多个模型部署的使用情况量身定制，只要模型的最小 PTU 要求满足（参见图 12.8）。预定的范围可以包括：
- en: Specific resource groups or subscriptions
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定的资源组或订阅
- en: A collection of subscriptions within a Management Group
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理组中的订阅集合
- en: All subscriptions under a billing account
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有账单账户下的订阅
- en: You can purchase new reservations to apply discounts to newly provisioned deployments
    within the same scope as existing ones. Additionally, the scope of current reservations
    can be adjusted at any time without any penalties
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以购买新的预定，以便对在与现有预定相同范围内新配置的部署应用折扣。此外，现有预定的范围可以随时调整而不产生任何惩罚。
- en: You can’t exchange the AOAI reservations.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能交换 AOAI 预定。
- en: You cannot add or remove units on a reservation. You can either purchase a new
    reservation or cancel the existing reservation. However, cancellations have limits.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您无法在预定中添加或删除单元。您只能购买新的预定或取消现有预定。但是，取消有一定的限制。
- en: If the provisioned deployments exceed the reserved amount, the excess is billed
    at the hourly rate. For instance, if your deployments total 400 PTUs within a
    300 PTU reservation, the additional 100 PTUs will incur hourly charges until you
    either reduce deployment sizes to 300 PTUs or acquire a new reservation for the
    extra 100.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果已配置的部署超过了预定的数量，超出的部分将按小时费率计费。例如，如果您的部署总数为 400 PTU，而预定为 300 PTU，则额外的 100 PTU
    将按小时费用计费，直到您将部署大小减少到 300 PTU 或为额外的 100 PTU 购买新的预定。
- en: Reservations ensure a lower price for the chosen term but do not secure service
    capacity or guarantee availability until a deployment created. So, it’s recommended
    for customers to set up deployments before buying a PTU reservation to avoid over
    purchase. To establish the deployment, you can initially opt for the hourly deployment
    process (as described before) to ensure capacity for the desired region. Then,
    purchase the AOAI reservation to cover the hourly PTU costs through reservation.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预定确保所选期限内的较低价格，但并不保证服务容量或在创建部署之前的可用性。因此，建议客户在购买 PTU 预定之前先设置部署，以避免过度购买。为了建立部署，您可以首先选择小时部署过程（如前所述），以确保所需地区的容量。然后，购买
    AOAI 预定，通过预定覆盖小时 PTU 成本。
- en: Let’s walk through the step-by-step process for purchasing an AOAI PTU reservation
    via the Azure Portal.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们通过 Azure 门户一步一步地了解如何购买 AOAI PTU 预定。
- en: Log in to your Azure portal
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到您的 Azure 门户
- en: Search for Reservations
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索预定
- en: Click on **Add**
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加**
- en: Search **Azure** **OpenAI Service**
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索**Azure** **OpenAI 服务**
- en: 'Select the **Scope, Billing Subscription, Region, and specific product**. You
    have three product options: a 1-year reservation with upfront payments, a 1-year
    reservation with monthly payments, or month-to-month payments. After choosing
    the appropriate product, the price per PTU will be displayed, as illustrated in
    *Figure 12**.13*.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**范围、计费订阅、地区和特定产品**。您有三个产品选项：1 年期预定（预付费）、1 年期预定（月付）或按月支付。选择合适的产品后，将显示每个 PTU
    的价格，如*图 12.13*所示。
- en: Select **Add** **to cart**
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**添加** **到购物车**
- en: Important note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: An AOAI PTU reservation allows flexibility in selecting any model that meets
    the minimum PTU requirements, but it is region-specific. Therefore, when purchasing
    an AOAI PTU reservation, it’s crucial to select the correct region.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI PTU 预定提供选择满足最小 PTU 要求的任意模型的灵活性，但它是地区特定的。因此，在购买 AOAI PTU 预定时，选择正确的地区至关重要。
- en: '![Figure 12.13: AOAI reservation purchase](img/B21019_12_13.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.13：AOAI 预定购买](img/B21019_12_13.jpg)'
- en: 'Figure 12.13: AOAI reservation purchase'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：AOAI 预定购买
- en: On the following screen, as shown in *Figure 12**.14,* specify the desired PTU
    quantity you wish to purchase for the reservation.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下屏幕中，如*图 12.14*所示，指定您希望为预定购买的 PTU 数量。
- en: '![Figure 12.14: Setting AOAI reservation PTU quantity](img/B21019_12_14.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.14：设置 AOAI 预定 PTU 数量](img/B21019_12_14.jpg)'
- en: 'Figure 12.14: Setting AOAI reservation PTU quantity'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14：设置 AOAI 预定 PTU 数量
- en: 'Click on **Next: Review +** **buy**'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步：审查 +** **购买**
- en: After purchasing the reservation, it might take up to 12 hours for the reservation
    utilization to be reported in the reservation portal.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 购买预定后，可能需要最多 12 小时才能在预定门户中报告预定的使用情况。
- en: 'After purchasing a PTU-M, if you wish to evaluate its performance and determine
    metrics like average latency, maximum TPM, or RPM, you can utilize the benchmark
    script provided by Microsoft for precise throughput calculations. The benchmark
    scripts are available at this link: [https://aka.ms/aoai/benchmarking](https://aka.ms/aoai/benchmarking).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 购买 PTU-M 后，如果您希望评估其性能并确定如平均延迟、最大 TPM 或 RPM 等指标，您可以使用微软提供的基准脚本进行精确的吞吐量计算。基准脚本可以通过以下链接获得：[https://aka.ms/aoai/benchmarking](https://aka.ms/aoai/benchmarking)。
- en: 'AOAI guarantees a PTU uptime SLA of 99.9% and a token generation latency SLA
    of 99%, ensuring consistent and predictable throughput. The gpt-4o models support
    50 deployable increments (Regional Deployment), with a maximum input throughput
    of 2,500 TPM and an output limit of 833 TPM per PTU, alongside a latency target
    of 25 tokens per second. In comparison, the gpt-4o-mini model offers 25 deployable
    (Regional Deployment) increments but delivers significantly higher maximum input
    and output rates of 37,000 TPM and 12,333 TPM per PTU, with a latency target of
    33 tokens per second. These performance metrics apply uniformly across the three
    available deployment options. For detailed information on throughput and token
    latency for each model, visit: [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/provisioned-throughput#how-much-throughput-per-ptu-you-get-for-each-model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/provisioned-throughput#how-much-throughput-per-ptu-you-get-for-each-model)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI 保证 PTU 的正常运行时间 SLA 为 99.9%，并且令牌生成延迟 SLA 为 99%，确保一致且可预测的吞吐量。gpt-4o 模型支持
    50 个可部署增量（区域部署），每个 PTU 的最大输入吞吐量为 2,500 TPM，输出限制为 833 TPM，同时每秒的延迟目标为 25 个令牌。相比之下，gpt-4o-mini
    模型提供 25 个可部署（区域部署）增量，但每个 PTU 的最大输入和输出速率大大提高，分别为 37,000 TPM 和 12,333 TPM，延迟目标为每秒
    33 个令牌。这些性能指标适用于所有三种可用的部署选项。有关每个模型的吞吐量和令牌延迟的详细信息，请访问：[https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/provisioned-throughput#how-much-throughput-per-ptu-you-get-for-each-model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/provisioned-throughput#how-much-throughput-per-ptu-you-get-for-each-model)
- en: You can also use the Azure OpenAI “provisioned-managed Utilization V2” metric
    to track the PTU usage when you perform a load test or run your PTU in production.
    This metric is only available in PTU-M and not in Classic PTU.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 Azure OpenAI 的“预配管理使用 V2”指标来跟踪 PTU 使用情况，尤其是在执行负载测试或将 PTU 用于生产时。此指标仅在
    PTU-M 中可用，而不适用于经典 PTU。
- en: If you decide not to continue with the AOAI PTU reservation, simply switch off
    the reservation’s auto renew option as referred in the *Figure 12**.15.* and ensure
    you delete the PTU deployment, as explained earlier. If the reservation isn’t
    auto renewed and you forget to remove the PTU deployment, you will incur hourly
    charges. Here’s how you can avoid renewing a reservation after making a purchase.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您决定不继续使用 AOAI PTU 预留，只需按照*图 12.15*中的说明关闭该预留的自动续订选项，并确保删除 PTU 部署，如前所述。如果预留未自动续订且您忘记删除
    PTU 部署，您将会产生每小时费用。以下是如何在购买后避免续订预留的方法。
- en: Log in to your Azure portal
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到您的 Azure 门户
- en: Search for Reservations
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索预留
- en: Select the specific reservation you wish to prevent from auto-renewing.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您希望防止自动续订的特定预留。
- en: Navigate to the settings menu and select the **Renewal** options.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入设置菜单并选择**续订**选项。
- en: Select **Do** **not renew**
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**不续订**
- en: Click on **Save**
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**
- en: '![Figure 12.15: Turning off AOAI PTU reservation renewal](img/B21019_12_15.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.15: 关闭 AOAI PTU 预留续订](img/B21019_12_15.jpg)'
- en: 'Figure 12.15: Turning off AOAI PTU reservation renewal'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.15: 关闭 AOAI PTU 预留续订'
- en: We have discussed various components of AOAI that enterprises require for their
    business production applications. Next, we will explore some scaling technique
    of AOAI to overcome throughput limitations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了企业在业务生产应用中需要的 AOAI 各种组件。接下来，我们将探索一些 AOAI 的扩展技术，以克服吞吐量限制。
- en: Azure OpenAI Scaling
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure OpenAI 扩展
- en: AOAI typically imposes constraints on the volume of calls permitted. With Azure
    OpenAI, these limitations manifest as token limits (TPM, Tokens Per Minute) and
    restrictions on the number of requests per minute (RPM). Nevertheless, these quotas
    are confined to individual subscriptions, regions, and specific models. As a result,
    numerous customers opt for multiple Azure OpenAI (AOAI) resources across various
    regions to achieve maximum throughput. Although this configuration in a “PAUG”
    setup does not address latency issues, the subsequent section will delve into
    resolving latency problems using PTU.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: AOAI通常会对允许的调用量施加限制。对于Azure OpenAI，这些限制表现为令牌限制（TPM， 每分钟令牌数）和每分钟请求次数限制（RPM）。然而，这些配额是针对单个订阅、区域和特定模型的。因此，许多客户选择跨不同区域部署多个Azure
    OpenAI（AOAI）资源，以实现最大吞吐量。尽管这种在“PAUG”配置中的设置不能解决延迟问题，接下来的部分将深入探讨如何使用PTU来解决延迟问题。
- en: In PAUG when the capacity limits are reached, The AOAI returns a 429 or TooManyRequests
    HTTP status code, along with a Retry-After response header specifying the duration
    of second you should wait before attempting the next request. Handling these errors
    is typically managed on the client-side by SDKs, which is effective when dealing
    with a single API endpoint. However, when utilizing multiple OpenAI endpoints
    to get the maximum throughputs, managing the list of URLs on the client-side becomes
    necessary, which may not be ideal. To address this, a sophisticated load balancing
    mechanism is required, capable of intelligently determining when and which AOAI
    endpoint the traffic should be routed to.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在PAUG中，当容量限制达到时，AOAI会返回一个429或TooManyRequests HTTP状态码，并附带一个Retry-After响应头，指示在尝试下一次请求之前应该等待的秒数。处理这些错误通常由客户端SDK进行管理，这对于处理单个API端点时很有效。然而，当使用多个OpenAI端点以获得最大吞吐量时，管理客户端上的URL列表变得必要，这可能并不是理想的解决方案。为了解决这个问题，需要一种复杂的负载均衡机制，能够智能地判断流量应该路由到哪个AOAI端点。
- en: 'APIM offers a robust solution for developers to securely expose their APIs
    to both external and internal users. With this platform, you can implement a smart
    load balancing strategy that takes into account the “Retry-After” and 429 error
    responses, dynamically rerouting traffic to alternate OpenAI backends that are
    not currently experiencing throttling. Additionally, you have the flexibility
    to establish a priority order for your AOAI endpoints, ensuring that higher priority
    endpoints are utilized first when they are not throttled. In the event of throttling,
    API Management automatically switches to lower priority backends while the higher
    priority ones recover. This approach optimizes resource utilization and minimizes
    disruptions in service delivery. Let me explain this with an example:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: APIM为开发人员提供了一种强大的解决方案，用于安全地向外部和内部用户公开其API。借助此平台，您可以实施智能负载均衡策略，考虑到“Retry-After”和429错误响应，动态地将流量重新路由到当前没有遭遇限速的替代OpenAI后端。此外，您还可以灵活地为AOAI端点建立优先顺序，确保在未被限速时，优先使用高优先级的端点。当发生限速时，API管理会自动切换到低优先级的后端，而高优先级后端则恢复。这种方法优化了资源利用率，减少了服务中断。让我通过一个例子来说明：
- en: Suppose you’ve established multiple AOAI endpoints across various regions, each
    assigned to different priority groups based on your business requirements. For
    instance, endpoints in US East and US East2 are grouped under priority group 1,
    while those in North Central and South Central are categorized under priority
    group 2, and West US and West US3 fall into priority group 3.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已在多个区域建立了多个AOAI端点，每个端点根据您的业务需求被分配到不同的优先级组。例如，位于美国东部和美国东部2的端点被归类为优先级组1，而位于北中部和南中部的端点被归类为优先级组2，位于西部美国和西部美国3的端点则属于优先级组3。
- en: '**Normal Case**: Under typical conditions, AOAI backends in priority group
    1 receive all incoming traffic from Azure API Management (APIM), while those in
    priority groups 2 and 3 remain inactive as standby options.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正常情况**：在典型条件下，优先级组1中的AOAI后端接收来自Azure API管理（APIM）的所有传入流量，而优先级组2和3中的后端保持待命状态，不激活。'
- en: '![Figure 12.16: APIM load balancing in normal scenario](img/B21019_12_16.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图12.16：APIM在正常场景中的负载均衡](img/B21019_12_16.jpg)'
- en: 'Figure 12.16: APIM load balancing in normal scenario'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.16：APIM在正常场景中的负载均衡
- en: '**Throttling Case**: In the event of workload throttling from priority group
    1, resulting in a 429 error code sent to APIM, the traffic will be rerouted to
    priority group 2 by APIM. This redirection ensures continued service delivery
    until priority group 1 becomes healthy again. Typically, priority group 1 will
    be reactivated after the duration specified in the “Retry-After” HTTP header received
    from AOAI.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限流案例**：如果优先级组 1 遇到工作负载限流，导致向 APIM 发送 429 错误码，流量将通过 APIM 被重定向到优先级组 2。这一重定向确保了服务的持续交付，直到优先级组
    1 恢复正常。通常，优先级组 1 会在从 AOAI 收到的“Retry-After”HTTP 头中指定的持续时间后重新激活。'
- en: '![Figure 12.17: APIM load balancing in throttling scenario](img/B21019_12_17.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.17: 限流场景中的 APIM 负载均衡](img/B21019_12_17.jpg)'
- en: 'Figure 12.17: APIM load balancing in throttling scenario'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.17: 限流场景中的 APIM 负载均衡'
- en: When defining priority groups, you have the flexibility to choose a strategy
    that aligns with your business needs. For instance, you can implement a geo-based
    priority group approach where all AOAI resources deployed across US regions are
    combined into priority group 1, and similarly, all AOAI resources deployed across
    Canadian regions are grouped into priority group 2\. In the event that priority
    group 1, consisting of US resources, experiences throttling, traffic can be routed
    to the Canadian region as an alternative. This strategy ensures efficient resource
    utilization and maintains service availability across different geographical locations.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义优先级组时，您可以灵活选择与业务需求一致的策略。例如，您可以实施基于地理位置的优先级组策略，其中部署在美国区域的所有 AOAI 资源合并到优先级组
    1，而部署在加拿大区域的所有 AOAI 资源合并到优先级组 2。如果优先级组 1（包含美国资源）遇到限流，流量可以作为替代方案路由到加拿大区域。这一策略确保了高效的资源利用，并保持不同地理位置的服务可用性。
- en: When addressing latency concerns, PTU emerges as the appropriate solution and
    is typically recommended for production use cases. However, due to its high cost
    and restriction to specific subscriptions, careful sizing is crucial. While a
    PTU sizing calculator is available in the AOAI portal, many customers opt to size
    PTU based on baseline average utilization, offloading peak traffic to PAUG instances.
    This approach allows for a balance between cost, throughput, and latency. Consequently,
    a hybrid approach is adopted where PTU and PAUG instances coexist side by side.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决延迟问题时，PTU 成为合适的解决方案，通常推荐用于生产用例。然而，由于其成本较高且仅限于特定的订阅，精确的规模调整至关重要。虽然 AOAI 门户中提供了
    PTU 尺寸计算器，但许多客户选择基于基线平均利用率来调整 PTU 尺寸，并将高峰期流量卸载到 PAUG 实例。这种方法在成本、吞吐量和延迟之间取得了平衡。因此，采用了
    PTU 和 PAUG 实例并存的混合方案。
- en: Presented below is a reference architecture tailored for deploying both PTU
    and PAUG instances within a single region. In this setup, designate the AOAI PTU
    instance as priority group 1, and the AOAI PAUG instance as priority group 2\.
    Such architecture serves as a reliable HA setup for AOAI.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个针对在单一区域内部署 PTU 和 PAUG 实例的参考架构。在此架构中，将 AOAI PTU 实例指定为优先级组 1，而将 AOAI PAUG
    实例指定为优先级组 2。这种架构为 AOAI 提供了一个可靠的 HA 设置。
- en: '![Figure 12.18: Single Region Scaling with HA](img/B21019_12_18.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.18: 带 HA 的单区域扩展](img/B21019_12_18.jpg)'
- en: 'Figure 12.18: Single Region Scaling with HA'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.18: 带 HA 的单区域扩展'
- en: At times, customers may require a **disaster recovery** (**DR**) solution alongside
    HA for their business applications utilizing AOAI. In such scenarios, deploying
    AOAI PTU and PAUG instances across different regions is recommended. Assign region-specific
    AOAI resources to distinct priority groups based on their respective regions.
    Below is the architecture presented for this kind of setup.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，客户可能需要为其利用 AOAI 的业务应用程序提供**灾难恢复**（**DR**）解决方案，并配合高可用性（HA）。在这种情况下，建议在不同区域部署
    AOAI PTU 和 PAUG 实例。根据各自的区域，将特定区域的 AOAI 资源分配到不同的优先级组。以下是为这种设置提供的架构。
- en: '![Figure 12.19: Multi Region Scaling with HA & DR](img/B21019_12_19.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.19: 带 HA 和 DR 的多区域扩展](img/B21019_12_19.jpg)'
- en: 'Figure 12.19: Multi Region Scaling with HA & DR'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.19: 带 HA 和 DR 的多区域扩展'
- en: In the provided reference architecture, the focus is on an active-standby DR
    strategy, where only the primary region’s priority group 1 is active. In the event
    of failure in that region, the standby region’s priority group 2 becomes active.
    However, for a more active-active DR setup, both region PTU instances can be designated
    as priority group 1, and both PAUG instances assigned to priority group 2\. This
    configuration ensures that PTUs are effectively utilized while PAUG instances
    handle any additional offloaded traffic.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的参考架构中，重点是一个主动-待命的灾备（DR）策略，其中只有主区域的优先级组 1 是活动的。如果该区域发生故障，待命区域的优先级组 2 将变为活动状态。然而，对于一个更为主动-主动的灾备设置，可以将两个区域的
    PTU 实例都指定为优先级组 1，而将两个 PAUG 实例分配给优先级组 2。这个配置确保了 PTU 能有效利用，同时 PAUG 实例处理任何额外的卸载流量。
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter encompasses crucial aspects of operationalizing AOAI, including
    monitoring various metrics such as the number of API calls, latency, sum of prompt
    tokens, and completion tokens etc. Additionally, it discusses AOAI resource quotas,
    outlining different limits across resources and how to manage and allocate quotas
    effectively. Furthermore, the chapter delves into the reserved instance concept
    of AOAI, known as PTU, which is vital for any production workload. Lastly, it
    explores scaling AOAI using multiple endpoints, along with HA and DR strategies,
    all essential components for building enterprise-level generative AI applications.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了将 AOAI 落地实施的关键方面，包括监控各种指标，如 API 调用次数、延迟、提示令牌总数和完成令牌等。此外，还讨论了 AOAI 资源配额，概述了不同资源的限制以及如何有效管理和分配配额。此外，本章深入探讨了
    AOAI 的预留实例概念，即 PTU，这是任何生产工作负载的关键组成部分。最后，本章探讨了如何通过多个终端点扩展 AOAI，并讨论了高可用性（HA）和灾备（DR）策略，这些都是构建企业级生成式
    AI 应用的关键组成部分。
- en: In the upcoming chapter, we will discuss the concept of prompt engineering,
    an essential cornerstone in the development and optimization of generative AI
    models. Prompt engineering encompasses a diverse array of techniques aimed at
    refining and tailoring the input prompts provided to these models, thereby influencing
    the quality, coherence, and relevance of their generated outputs. Throughout this
    chapter, we will explore the most popular techniques and impactful strategies
    within prompt engineering. By delving into these techniques, we aim to provide
    you with comprehensive insights and practical knowledge essential for effectively
    harnessing the capabilities of generative AI models in various applications and
    domains.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论提示工程的概念，这是生成式 AI 模型开发和优化中的一项重要基石。提示工程涵盖了一系列多样化的技术，旨在精炼和定制输入的提示，以影响模型生成输出的质量、一致性和相关性。在本章中，我们将探讨提示工程中最流行的技术和有影响力的策略。通过深入了解这些技术，我们旨在为您提供全面的见解和实用的知识，以有效利用生成式
    AI 模型在各种应用和领域中的能力。
