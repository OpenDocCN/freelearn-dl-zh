- en: DRL Frameworks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DRL 框架
- en: Working through and exploring the code in this book is meant to be a learning
    exercise in how **Reinforcement Learning** (**RL**) algorithms work but also how
    difficult it can be to get them to work. It is because of this difficulty that
    so many open source RL frameworks seem to pop up every day. In this chapter, we
    will explore a couple of the more popular frameworks. We will start with why you
    would want to use a framework and then move on to exploring the more popular frameworks
    such as Dopamine, Keras-RL, TF-Agents, and RL Lib.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中通过和探索代码旨在成为学习如何**强化学习**（**RL**）算法工作的练习，同时也了解让这些算法工作起来的难度。正是因为这种难度，每天都有许多开源的RL框架出现。在本章中，我们将探讨几个更受欢迎的框架。我们将从为什么您想要使用一个框架开始，然后转向探索更受欢迎的框架，如Dopamine、Keras-RL、TF-Agents和RL
    Lib。
- en: 'Here is a quick summary of the main topics we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是本章我们将涵盖的主要主题的简要总结：
- en: Choosing a framework
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个框架
- en: Introducing Google Dopamine
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Google Dopamine
- en: Playing with Keras-RL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩转 Keras-RL
- en: Exploring RL Lib
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 RL Lib
- en: Using TF agents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TF agents
- en: We will use a combination of notebook environments on Google Colab and virtual
    environments depending on the complexity of the examples in this chapter. Jupyter
    Notebooks, which Colab is based on, is an excellent way to demonstrate code. It
    is generally not the preferred way to develop code and this is the reason we avoided
    this method until now.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将结合使用Google Colab上的笔记本环境和虚拟环境，这取决于本章示例的复杂性。Jupyter Notebooks，Colab的基础，是展示代码的绝佳方式。它通常不是开发代码的首选方式，这也是我们为什么直到现在才避免使用这种方法的原因。
- en: In the next section, we look at why you would want to choose a framework.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨为什么您会选择一个框架。
- en: Choosing a framework
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一个框架
- en: As you may have surmised by now, writing your own RL algorithms and functions
    on top of a deep learning framework, such as PyTorch, is not trivial. It is also
    important to remember that the algorithms in this book go back about 30 years
    over the development of RL. That means that any serious new advances in RL take
    substantial effort and time—yes, for both development and especially training.
    Unless you have the time, resources, and incentive for developing your own framework,
    then it is highly recommended to graduate using a mature framework. However, there
    is an ever-increasing number of new and comparable frameworks out there, so you
    may find that you are unable to choose just one. Until one of these frameworks
    achieves true AGI, then you may also need separate frameworks for different environments
    or even different tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如您现在可能已经推测到的，在深度学习框架（如PyTorch）之上编写自己的RL算法和函数并非易事。还重要的是要记住，本书中的算法回顾了RL发展的约30年。这意味着任何RL的重大新进展都需要大量的努力和时间——是的，包括开发和特别是训练。除非您有开发自己框架的时间、资源和动力，那么强烈建议您使用成熟的框架。然而，新的和可比较的框架数量正在不断增加，因此您可能会发现您无法只选择一个。直到这些框架实现真正的AGI，您可能还需要为不同的环境或甚至不同的任务使用不同的框架。
- en: Remember, **AGI** stands for **Artificial General Intelligence** and it really
    is the goal of any RL framework to be AGI. An AGI framework can be trained on
    any environment. An advanced AGI framework may be able to transfer learning across
    tasks. Transfer learning is where an agent can learn one task and then use those
    learnings to accomplish another similar task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，**AGI**代表**人工通用智能**，这确实是任何RL框架的目标。一个AGI框架可以在任何环境中进行训练。一个高级AGI框架可能能够跨任务进行迁移学习。迁移学习是指一个智能体可以学习一个任务，然后利用这些学习来完成另一个类似的任务。
- en: We are going to look at the current most popular frameworks that have the most
    promise, in later sections. It is important to compare the various current frameworks
    to see whether one may be a better choice for you and your team. Therefore, we
    will look at a comparison of the various RL frameworks currently available in
    the following list.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中查看目前最受欢迎且最有潜力的框架。比较各种当前框架，以确定其中一个可能更适合您和您的团队，这一点很重要。因此，我们将查看以下列表中目前可用的各种RL框架的比较。
- en: 'This list features the current most popular frameworks ordered by current popularity
    (by Google), but this list is expected to change with time:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表按当前流行度（由Google）排序，列出当前最受欢迎的框架，但预计随着时间的推移，此列表将发生变化：
- en: '**OpenAI Gym and Baselines**: OpenAI Gym is the framework we have used for
    most of the environments we have explored in this book. This library also has
    a companion called Baselines that provides several agents for, you guessed it,
    baselining the Gym environments. Baselines is also a very popular and good RL
    framework but we have omitted it here in favor of looking at other libraries.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI Gym 和 Baselines**：OpenAI Gym 是我们在本书中探索的大多数环境所使用的框架。这个库还有一个配套的库叫做 Baselines，它为
    Gym 环境提供了几个代理，正如你所猜测的，用于基准测试 Gym 环境。Baselines 也是一个非常受欢迎且优秀的强化学习框架，但在这里我们为了查看其他库而省略了它。'
- en: '**Google Dopamine**: This is a relatively new framework that has gained popularity
    very quickly. This is likely due, in part, to its implementation of the RainbowDQN
    agent. The framework is well developed but has been described as being clunky
    and not very modular. We showcase it here because it is popular and you will likely
    want a closer look at it anyway.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Dopamine**：这是一个相对较新的框架，迅速获得了人气。这可能是部分原因在于其实施了 RainbowDQN 代理。这个框架已经得到了很好的发展，但被描述为笨拙且不太模块化。我们在这里展示它，因为它很受欢迎，你很可能仍然想更仔细地看看它。'
- en: '**ML-Agents**: We have more or less already covered a whole chapter on this
    framework, so we won''t need to explore it here. Unity has developed a very solid
    but not very modular framework. The implementation currently only supports PG
    methods such as PPO and Soft Actor-Critic. ML-Agents on its own, however, it can
    be a great and recommended way to demonstrate RL to development teams or even
    introduce concepts to clients.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML-Agents**：我们已经在某种程度上覆盖了整个关于这个框架的章节，所以在这里我们不需要进一步探索。Unity 开发了一个非常稳固但不太模块化的框架。当前的实现仅支持
    PG 方法，如 PPO 和 Soft Actor-Critic。然而，ML-Agents 本身可以是一个展示强化学习给开发团队或向客户介绍概念的极好且推荐的方式。'
- en: '**RL Lib with the ray-project**: This has strange origins in that it started
    as a parallelization project for Python and evolved into a training platform for
    RL. As such, it tends to favor training regimes that use asynchronous agents such
    as A3C, and it is well suited to complex environments. Not to mention, this project
    is based on PyTorch so it will be worth a look.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL Lib 与 ray-project**：这个项目的起源很奇特，它最初是一个 Python 的并行化项目，后来演变成一个强化学习的训练平台。因此，它倾向于使用异步代理（如
    A3C）的训练制度，非常适合复杂的环境。不仅如此，这个项目基于 PyTorch，所以它值得一看。'
- en: '**Keras-RL**: Keras itself is another deep learning framework that is very
    popular on its own. The deep learning library itself is quite concise and easy
    to use—perhaps in some ways, too easy. However, it can be an excellent way to
    prototype an RL concept or environment and deserves a closer look by us.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras-RL**：Keras 本身也是一个非常流行的深度学习框架。这个深度学习库本身相当简洁且易于使用——也许在某些方面，过于简单。然而，它可以用作原型化强化学习概念或环境的绝佳方式，值得我们进一步关注。'
- en: '**TRFL**: This library, not unlike Keras-RL, is an extension of the TensorFlow
    framework to incorporate RL. TensorFlow is another low-level deep learning framework.
    As such, the code to build any working agent also needs to be quite a low level
    and using this library likely won''t be for you, especially if you enjoy PyTorch.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TRFL**：这个库与 Keras-RL 类似，是 TensorFlow 框架的扩展，以纳入强化学习。TensorFlow 是另一个低级深度学习框架。因此，构建任何有效代理的代码也需要相当低级，使用这个库可能不适合你，尤其是如果你喜欢
    PyTorch。'
- en: '**Tensorforce**: This is another library focused on extending TensorFlow for
    RL. The benefit of using a TF-based solution is cross-compatibility and even the
    ability to port your code to web or mobile. However, building low-level computational
    graphs is not for everyone and does require a higher level of mathematics than
    we covered in this book.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tensorforce**：这是一个专注于扩展 TensorFlow 以用于强化学习的库。使用基于 TF 的解决方案的好处是跨兼容性，甚至可以将你的代码移植到网页或移动设备。然而，构建低级计算图并不适合每个人，并且确实需要比本书中涵盖的更高水平的数学知识。'
- en: '**Horizon**: This framework is from Facebook and is developed on top of PyTorch.
    Unfortunately, the benefits of this framework fall short in several areas including
    not having a `pip` installer. It also lacks tight integration with Gym environments
    so, unless you work at Facebook, you will likely want to avoid this framework.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Horizon**：这个框架来自 Facebook，是在 PyTorch 上开发的。不幸的是，这个框架在多个领域中的优势不足，包括没有 `pip`
    安装程序。它还缺乏与 Gym 环境的紧密集成，所以除非你在 Facebook 工作，否则你可能会想避免使用这个框架。'
- en: '**Coach**: This is one of those sleeper frameworks that could build a substantial
    following of its own someday. There are many useful and powerful features to Coach,
    including a dedicated dashboard and direct support for Kubernetes. This framework
    also currently boasts the largest implementation of RL algorithms and will give
    you plenty of room to explore. Coach is a framework worth exploring on your own.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Coach**：这是那种可能有一天会建立自己大量追随者的隐藏框架之一。Coach有很多有用和强大的功能，包括专门的仪表板和直接支持Kubernetes。这个框架目前也拥有最大的RL算法实现，将为你提供大量的探索空间。Coach是一个值得你自己探索的框架。'
- en: '**MAgent**: This project is similar to RLLib (Ray) in that it specializes in
    training multiple agents asynchronously or in various configurations. It is developed
    on top of TensorFlow and uses its own grid-world designed environments for what
    is coined as real-life simulations. This is a very specialized framework for developers
    or real-life RL solutions.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAgent**：这个项目与RLLib（Ray）类似，因为它专门用于异步或各种配置下训练多个代理。它是基于TensorFlow开发的，并使用自己设计的网格世界环境进行所谓的现实生活模拟。这是一个非常专业的框架，适用于开发者或现实生活中的RL解决方案。'
- en: '**TF-Agents**: This is another RL implementation from Google developed on top
    of TensorFlow. As such, it is a more low-level framework but is more robust and
    capable than the other TF frameworks mentioned here. This framework appears to
    be a strong contender for more serious research and/or production implementations
    and worth a further look from readers looking to do such work.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-Agents**：这是谷歌在TensorFlow之上开发的另一个RL实现。因此，它是一个更底层的框架，但比这里提到的其他TF框架更稳健和强大。这个框架似乎是一个更严肃的研究和/或生产实现的强劲竞争者，值得那些想要进行此类工作的读者进一步关注。'
- en: '**SLM-Lab**: This is another PyTorch-based framework that is actually based
    on top of Ray (RLLib), although it is designed more for pure research. As such,
    it lacks a `pip` installer and assumes the user is pulling code directly from
    a repository. It is likely best to leave this framework to the researchers for
    now.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SLM-Lab**：这是另一个基于PyTorch的框架，实际上是基于Ray（RLLib）之上的，尽管它更多的是为纯研究而设计。因此，它没有`pip`安装程序，并假设用户直接从存储库中拉取代码。现在最好把这个框架留给研究人员。'
- en: '**DeeR**: This is another library that is integrated with Keras and is intended
    to be more accessible. The library is well kept and the documentation is clear.
    However, this framework is intended for those learning RL and if you made it this
    far, you likely already need something more advanced and robust.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeeR**：这是另一个与Keras集成的库，旨在更加易于使用。这个库维护得很好，文档也很清晰。然而，这个框架是为那些学习RL的人设计的，如果你已经走到这一步，你可能已经需要更高级和更稳健的东西了。'
- en: '**Garage**: This is another TF-based framework that has some excellent functionality
    but lacks documentation and any good installation procedures, which makes this
    another good research framework but may be better avoided by those interested
    in developing working agents.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**车库（Garage）**：这是另一个基于TF的框架，它具有一些出色的功能，但缺乏文档和良好的安装流程，这使得它又是一个很好的研究框架，但对于那些对开发工作代理感兴趣的人来说可能更好避免。'
- en: '**Surreal**: This framework is designed more for robotics applications and,
    as such, is more closed. Robotics RL with environments such as Mujoco have been
    shown to be commercially viable. As such, this branch of RL is seeing the impact
    of those trying to take their share. This means that this framework is currently
    free but not open source and the free part is likely to change soon. Still, if
    you are specializing in robotics applications, this may be worth a serious look.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Surreal**：这个框架更多的是为机器人应用而设计的，因此它更加封闭。使用Mujoco等环境进行机器人RL已被证明具有商业可行性。因此，这个RL分支正在看到那些试图分得一杯羹的人的影响。这意味着这个框架目前是免费的，但不是开源的，而且免费的部分可能很快就会改变。尽管如此，如果你专注于机器人应用，这可能值得认真考虑。'
- en: '**RLgraph**: This is perhaps another sleeper project to keep your eye on. This
    library is currently absorbing a ton of commits and changing quickly. It is also
    built with both PyTorch and TensorFlow mappings. We will spend some time looking
    at using this framework in a later section.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RLgraph**：这可能是一个值得关注的潜在项目。这个库目前正在吸收大量的提交，并且变化很快。它也使用PyTorch和TensorFlow映射构建。我们将在后面的部分花时间探讨如何使用这个框架。'
- en: '**Simple RL**: This is perhaps as simple as you can get with an RL framework.
    The project is intended to be very accessible and examples with multiple agents
    can be developed in less than eight lines of code. It can actually be as simple
    as the following block of code taken from the example documentation:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单RL**：这可能是一个RL框架所能达到的最简单形式。该项目旨在非常易于访问，并且可以在少于八行代码的情况下开发出具有多个代理的示例。实际上，它可能就像以下从示例文档中摘取的代码块那样简单：'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With so many frameworks to choose from, we only have time to go over the most
    popular frameworks in this chapter. While frameworks become popular because they
    are well written and tend to work well in a wide variety of environments, until
    we reach AGI, you may still need to explore various frameworks to find an algorithm/agent
    that works for you and your problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有这么多框架可供选择，我们只有时间在本章中概述最受欢迎的框架。虽然框架因为编写得好并且倾向于在各种环境中表现良好而变得流行，但直到我们达到通用人工智能（AGI），你可能仍然需要探索各种框架，以找到适合你和你问题的算法/代理。
- en: To see how this has evolved over time, we can use Google Trends to perform a
    search comparison analysis. Doing this can often give us an indication of how
    popular a particular framework is trending in search terms. More search terms
    means more interest in the framework, which in turn, leads to more development
    and better software.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这种演变，我们可以使用谷歌趋势进行搜索比较分析。这样做通常可以给我们一个特定框架在搜索词中的流行趋势的指示。更多的搜索词意味着对框架的兴趣更大，这反过来又导致更多的开发和更好的软件。
- en: 'The following Google Trends plot shows a comparison of the top five listed
    frameworks:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的谷歌趋势图显示了前五个列表框架的比较：
- en: '![](img/212f143f-e9d7-466a-b94a-e2f6c6d214f3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/212f143f-e9d7-466a-b94a-e2f6c6d214f3.png)'
- en: Google trends comparison of RL frameworks
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌趋势对比RL框架
- en: You can see in the plot the trending increase for RL Lib and Google Dopamine.
    It is also interesting to note that the primary interest in RL development is
    the current greatest in the US and Japan, with Japan taking a special interest
    in ML-Agents.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图中看到RL库和谷歌多巴胺的趋势增长。值得注意的是，对RL开发的主要兴趣目前在美国和日本最为浓厚，日本对ML-Agents特别感兴趣。
- en: ML-Agents' popularity lends itself to several factors, one of which being the
    VP of AI and ML at Unity, Dr. Danny Lange. Dr. Lange lived in Japan for several
    years and is fluent in Japanese and this has likely contributed to this specific
    popularity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents的流行归因于几个因素，其中之一是Unity公司AI和ML副总裁，丹尼·兰格博士。兰格博士在日本居住了几年，日语流利，这很可能促成了这种特定的流行。
- en: It is interesting to note the absence of China in this area, at least for these
    types of frameworks. China's interest in RL is currently very specific to planning
    applications popularized by the defeat of the game of Go by an RL agent. That
    RL agent was developed using an algorithm called Monte Carlo Tree Search, which
    is intended to do a full exploration of complex but finite state spaces. We started
    looking at finite state spaces but took a turn to explore continuous or infinite
    state spaces. These types of agents also transition well to general games and
    robotics, which is not a major interest by the Chinese. Therefore, it remains
    to be seen how or what interest China shows in this area but that will most likely
    influence this space as well when that happens.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，中国在这一领域缺席，至少对于这些类型的框架来说是这样。中国对强化学习的兴趣目前非常具体，专注于由强化学习代理击败围棋游戏而流行起来的规划应用。那个强化学习代理是使用蒙特卡洛树搜索算法开发的，该算法旨在对复杂但有限的状态空间进行全面探索。我们开始研究有限状态空间，但转向探索连续或无限状态空间。这类代理也很好地过渡到通用游戏和机器人技术，而这并不是中国的主要兴趣。因此，我们还需要看看中国在这个领域将如何或表现出什么兴趣，但一旦发生，这很可能会影响这个领域。
- en: In the next section, we look at our first framework and one that we may find
    the most familiar, Google Dopamine.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨我们的第一个框架，可能也是我们最熟悉的框架，谷歌多巴胺。
- en: Introducing Google Dopamine
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍谷歌多巴胺
- en: Dopamine was developed at Google as a platform to showcase the company's latest
    advances in DRL. Of course, there are also other groups at Google doing the same
    thing, so it is perhaps a testament to how varied these platforms still are and
    need to be. In the next exercise, we will use Google Colab to build an example
    that uses Dopamine on the cloud to train an agent.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多巴胺是在谷歌开发的，作为一个平台来展示公司在深度强化学习方面的最新进展。当然，谷歌还有其他团队在做同样的事情，这或许是对这些平台仍然多样化的证明，以及它们需要进一步多样化的证明。在下一个练习中，我们将使用谷歌Colab构建一个示例，使用云上的多巴胺训练一个代理。
- en: To access all of the features on Colab, you will likely need to create a Google
    account with payment authorized. This likely means entering a credit or debit
    card. The plus here is that Google provides $300 US in credits to use the GCP
    platform, of which Colab is one small part.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 Colab 上的所有功能，您可能需要创建一个已授权支付的 Google 账户。这可能意味着输入一张信用卡或借记卡。好处是 Google 为 GCP
    平台提供了 300 美元的信用额度，而 Colab 只是其中的一小部分。
- en: 'Open your browser to [colab.research.google.com](http://colab.research.google.com) and
    follow the next exercise:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您的浏览器到 [colab.research.google.com](http://colab.research.google.com) 并跟随下一个练习：
- en: We will first start by creating a new **Python 3 Notebook**. Be sure to choose
    this by the prompt dialog or through the Colab **File** menu.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建一个新的 **Python 3 笔记本**。请确保通过提示对话框或通过 Colab **文件** 菜单选择此选项。
- en: This notebook is based on a variation by the Dopamine authors and the original
    may be found in the following link: [https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb](https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本是基于 Dopamine 作者的一个变体，原始版本可以在以下链接中找到：[https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb](https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb)。
- en: 'We first need to install several packages to support the training. On a Colab
    notebook, we can pass any command to the underlying shell by prefixing it with
    `!`. Enter the following code in a cell and then run the cell:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要安装几个支持训练的包。在 Colab 笔记本中，我们可以通过在命令前加上 `!` 来将任何命令传递给底层的 shell。在一个单元格中输入以下代码，然后运行该单元格：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we do some imports and set up some global strings in a new cell:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在新的单元格中进行一些导入并设置一些全局字符串：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `@param` function denotes the value as a parameter and this provides a
    helpful textbox on the interface for changing this parameter later. This is a
    cool notebook feature:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`@param` 函数表示该值为参数，并在界面提供了一个有用的文本框，以便稍后更改此参数。这是一个很酷的笔记本功能：'
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we run the preceding command and code in another new cell. This loads
    the data we will use to run the agent on:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在另一个新的单元格中运行前面的命令和代码。这将加载我们将用于在代理上运行的数据：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create a new cell and enter the preceding code and run it. This creates a random
    DQN agent for more or less blindly exploring an environment.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的单元格并输入前面的代码并运行它。这将创建一个用于在环境中进行盲探索的随机 DQN 代理。
- en: 'Next, we want to train the agent by creating a new cell and entering the following
    code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们想要通过创建一个新的单元格并输入以下代码来训练代理：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This can take a while, so if you have authorized payment enabled, you can run
    this example on a GPU instance by just changing the notebook type. You can do
    this by selecting from the menu **Runtime** | **Change runtime type**. A dialog
    will pop up; change the runtime type and close the dialog, as shown in the following
    screenshot:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可能需要一段时间，所以如果您已经启用了支付授权，您可以通过更改笔记本类型来在 GPU 实例上运行此示例。您可以通过选择 **Runtime** | **Change
    runtime type** 菜单来完成此操作。将弹出一个对话框；更改运行时类型并关闭对话框，如图所示：
- en: '![](img/10cffd3e-db75-433f-adb6-ec4a4fef591f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/10cffd3e-db75-433f-adb6-ec4a4fef591f.png)'
- en: Changing the runtime type on Colab
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 上更改运行时类型
- en: After you change the runtime type, you will need to run the whole notebook again.
    To do this, select from the menu **Runtime | Run all** to run all of the cells
    again. You will still need to wait a while for the training to run; it is, after
    all, running an Atari environment but that is the point.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更改运行时类型后，您需要再次运行整个笔记本。为此，请从菜单中选择 **Runtime | Run all** 以再次运行所有单元格。您仍然需要等待一段时间才能完成训练；毕竟，这是在运行
    Atari 环境，但这正是目的所在。
- en: The agent we just built is using a random DQN agent running on the classic Atari
    game, *Asterix*. Dopamine is a powerful framework that is easy to use as we have
    just seen. You can find much more about the library from the source itself, including
    how to output the results from the last example exercise.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建的代理正在使用运行在经典 Atari 游戏 *Asterix* 上的随机 DQN 代理。Dopamine 是一个功能强大且易于使用的框架，正如我们刚刚看到的。您可以从源本身找到有关库的更多信息，包括如何输出最后一个示例练习的结果。
- en: In the next section, we will move away from Colab and explore another framework,
    Keras-RL with regular Python.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将离开 Colab 并探索另一个框架，即使用常规 Python 的 Keras-RL。
- en: Playing with Keras-RL
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩转 Keras-RL
- en: Keras is a very popular deep learning framework on its own and it is heavily
    used by newcomers looking to learn about the basics of constructing networks.
    The framework is considered very high-level and abstracts most of the inner details
    of constructing networks. It only goes to assume that an RL framework built with
    Keras would attempt to do the same thing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个非常流行的深度学习框架，它本身就被那些想要学习构建网络基础知识的新手大量使用。该框架被认为是高度高级和抽象的，它抽象了构建网络的大部分内部细节。因此，可以假设使用
    Keras 构建的强化学习框架会尝试做同样的事情。
- en: This example is dependent on the version of Keras and TensorFlow and may not
    work correctly unless the two can work together. If you encounter trouble, try
    installing a different version of TensorFlow and try again.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例依赖于 Keras 和 TensorFlow 的版本，并且如果这两个版本不能协同工作，可能无法正确运行。如果您遇到问题，请尝试安装 TensorFlow
    的不同版本，然后再次尝试。
- en: 'To run this example, we will start by doing the installation and all of the
    setup in this exercise:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个示例，我们首先将在本练习中进行安装和所有设置：
- en: 'To install Keras, you should create a new virtual environment using Python
    3.6 and use `pip` to install it along with the `keras-rl` framework. The commands
    to do all of this on Anaconda are shown here:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装 Keras，您应该使用 Python 3.6 创建一个新的虚拟环境，并使用 `pip` 安装它以及 `keras-rl` 框架。在 Anaconda
    上执行所有这些命令的命令如下所示：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After all of the packages are installed, open the sample code file, `Chapter_12_Keras-RL.py`,
    as shown here:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装完所有包后，打开示例代码文件，`Chapter_12_Keras-RL.py`，如图所示：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We haven't covered any Keras code but hopefully, the simple nature of the code
    makes it fairly self-explanatory. If anything, the code should feel quite familiar,
    although missing a training loop.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还没有涵盖任何 Keras 代码，但希望代码的简单性使得它相当直观。如果有什么不同的话，代码应该感觉相当熟悉，尽管缺少训练循环。
- en: Notice in the proceeding code block how the model is constructed using something
    called a `Sequential` class. The class is a container for network layers, which
    we then add next interspersed with appropriate activation functions. Note at the
    end of the network, how the last layer uses a linear activation function.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意在先前的代码块中，模型是如何使用名为 `Sequential` 的类构建的。该类是网络层的容器，我们随后添加适当的激活函数。注意在网络的末尾，最后一个层使用的是线性激活函数。
- en: 'Next, we will take a closer look at the construction of memory, policy, and
    agent itself. See the following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将更详细地研究记忆、策略和智能体本身的构建。请参见以下代码：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The interesting thing to note here is how we construct the network model outside
    the agent and feed it as an input to the agent along with the memory and policy.
    This is very powerful and provides for some interesting extensibility.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里值得注意的有趣之处在于我们如何在智能体外部构建网络模型，并将其作为输入与记忆和政策一起提供给智能体。这非常强大，并为一些有趣的扩展提供了可能。
- en: 'At the end of the file, we can find the training code. The training function
    called `fit` is used to iteratively train the agent. All of the code to do this
    is encapsulated in the `fit` function, as the following code shows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件末尾，我们可以找到训练代码。使用名为 `fit` 的训练函数来迭代训练智能体。所有执行此操作的代码都封装在 `fit` 函数中，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The last section of code saves the model and then runs a test on the agent
    with the following code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的最后部分保存了模型，然后使用以下代码对智能体进行了测试：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Run the code as you normally would and watch the visual training output and
    testing as shown in the following diagram:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规运行代码，并观察以下图中所示的视觉训练输出和测试：
- en: '![](img/31a25484-5cd9-43cc-880e-88369ad378b9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31a25484-5cd9-43cc-880e-88369ad378b9.png)'
- en: Example output from Chapter_12_Keras-RL.py
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `Chapter_12_Keras-RL.py` 的示例输出
- en: Keras-RL is a light powerful framework for testing concepts or other ideas quickly. The
    performance of Keras itself is not as powerful as TensorFlow or PyTorch, so any
    serious development should be done using one of those platforms. In the next section,
    we will look at another RL platform based on PyTorch called RLLib.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Keras-RL 是一个轻量级的强大框架，可以快速测试概念或其他想法。Keras 本身的性能并不像 TensorFlow 或 PyTorch 那样强大，因此任何严肃的开发都应该使用这些平台之一。在下一节中，我们将探讨另一个基于
    PyTorch 的强化学习平台，称为 RLLib。
- en: Exploring RL Lib
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 RL Lib
- en: RL Lib is based on the Ray project, which is essentially a Python job-based
    system. RL Lib is more like ML-Agents, where it exposes functionality using config
    files although, in the case of ML-Agents, the structure is completely run on their
    platform. Ray is very powerful but requires a detailed understanding of the configuration
    parameters and setup. As such, the exercise we show here is just to demonstrate
    the power and flexibility of Ray but you are directed to the full online documentation
    for further discovery on your own.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: RL Lib 基于 Ray 项目，本质上是一个基于 Python 作业的系统。RL Lib 更像 ML-Agents，它通过配置文件公开功能，尽管在 ML-Agents
    的情况下，结构完全运行在其平台上。Ray 非常强大，但需要详细了解配置参数和设置。因此，我们在这里展示的练习只是为了展示 Ray 的强大和灵活性，但您被指引到完整的在线文档以进行进一步的自我探索。
- en: 'Open your browser to [colab.research.google.com](http://colab.research.google.com) and
    follow the next exercise:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您的浏览器访问 [colab.research.google.com](http://colab.research.google.com) 并按照下一个练习进行操作：
- en: 'The great thing about using Colab is it can be quite easy to run and set up.
    Create a new Python 3 notebook and enter the following commands:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Colab 的好处是它运行和设置起来相当容易。创建一个新的 Python 3 笔记本并输入以下命令：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: These commands install the framework on the Colab instance. After this is installed,
    you need to restart the runtime by selecting from the menu: **Runtime | Restart
    runtime**.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些命令会在 Colab 实例上安装框架。安装完成后，您需要通过从菜单中选择来重启运行时：**运行时 | 重启运行时**。
- en: 'After the runtime restarts, create a new cell and enter the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行时重启后，创建一个新的单元格并输入以下代码：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That block of code imports the framework and the tune class for hyperparameter
    tuning.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那段代码导入了框架和用于超参数调整的 tune 类。
- en: 'Create a new cell and enter the following code:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的单元格并输入以下代码：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Believe it or not, that's it. That is the remainder of the code to build a DQN
    agent to run and train on the `CartPole` environment. Not to mention the `tune`
    class is set to tune the learning rate hyperparameter, `lr` (`alpha`), using the
    `tune.grid_search` function.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信不信由你，就是这样。这就是构建一个在 `CartPole` 环境中运行和训练的 DQN 代理的代码剩余部分。更不用说 `tune` 类被设置为使用 `tune.grid_search`
    函数调整学习率超参数 `lr` (`alpha`)。
- en: 'Run the last cell and observe the output. The output is extremely comprehensive
    and an example is shown here:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行最后一个单元格并观察输出。输出非常全面，这里展示了一个例子：
- en: '![](img/ab7b843c-19da-41af-85b0-f6cf8a7dd1ff.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab7b843c-19da-41af-85b0-f6cf8a7dd1ff.png)'
- en: Training RLLib on Google Colab
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 上训练 RLLib
- en: As you can see in the preceding screenshot, this is a very powerful framework
    designed to optimize hyperparameter tuning and it provides plenty of options to
    do so. It also allows for multiagent training in various configurations. This
    framework is a must-study for anyone doing serious work or research in RL. In
    the next section, we will look at the last framework, TF-Agents.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个截图所示，这是一个非常强大的框架，旨在优化超参数调整，并提供大量选项来实现这一点。它还允许在多种配置中进行多智能体训练。对于任何在强化学习方面进行严肃工作或研究的人来说，这个框架是必学的。在下一节中，我们将探讨最后一个框架，TF-Agents。
- en: Using TF-Agents
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TF-Agents
- en: The last framework we are going to look at is TF-Agents, a relatively new but
    up-and-coming tool, again, from Google. It seems Google's approach to building
    RL frameworks is a bit like RL itself. They are trying multiple trial and error
    attempts/actions to get the best reward—not entirely a bad idea for Google, and
    considering the resources they are throwing at RL, it may not unexpected to see
    more RL libraries come out.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探讨的最后一个框架是 TF-Agents，这是一个相对较新但正在崛起的工具，同样来自 Google。Google 构建强化学习框架的方法有点像强化学习本身。他们正在尝试多次试错尝试/动作以获得最佳奖励——对于
    Google 来说，这并不是一个坏主意，考虑到他们投入强化学习的资源，看到更多强化学习库的出现可能并不令人意外。
- en: TF-Agents, while newer, is typically seen as more robust and mature. It is a
    framework designed for notebooks and that makes it perfect for trying out various
    configurations, hyperparameters, or environments. The framework is developed on
    TensorFlow 2.0 and works beautifully on Google Colab. It will likely become the de-facto
    platform to teach basic RL concepts and demo RL in the future.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: TF-Agents，虽然较新，但通常被认为更稳健和成熟。这是一个为笔记本设计的框架，这使得它非常适合尝试各种配置、超参数或环境。该框架基于 TensorFlow
    2.0 开发，在 Google Colab 上运行得非常好。它很可能会成为未来教授基本强化学习概念和演示强化学习的默认平台。
- en: There are plenty of notebook examples to show how to use TF-Agents at the TF-Agents
    Colab repository ([https://github.com/tensorflow/agents/tree/master/tf_agents/colabs](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs)). The
    whole repository is a great resource but this section itself can be especially
    useful for those of us that want to see working code examples.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TF-Agents Colab 仓库中有许多笔记本示例，展示了如何使用 TF-Agents（[https://github.com/tensorflow/agents/tree/master/tf_agents/colabs](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs)）。整个仓库是一个很好的资源，但这个部分本身对于我们想要看到工作代码示例的人来说可能特别有用。
- en: 'Open your browser up to the TF-Agents Colab page at the preceding link and
    follow the next exercise:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您的浏览器，访问前面的链接中的 TF-Agents Colab 页面，并遵循下一个练习：
- en: For this exercise, we are going to modify the training environment for one of
    the samples. That should give us enough of an overview of what the code looks
    like and how to make changes yourself later on. Locate `1_dqn_tutorial.ipynb`
    and click on it to open the page. Note that `.ipynb` stands for **I-Python Notebook**;
    I-Python is a server platform for hosting notebooks.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将修改其中一个样本的训练环境。这应该足以让我们了解代码的样子以及如何自己稍后进行修改。定位 `1_dqn_tutorial.ipynb`
    并点击它以打开页面。请注意，`.ipynb` 代表 **I-Python Notebook**；I-Python 是一个用于托管笔记本的服务器平台。
- en: Click the link at the top that says **Run in Google Colab**. This will open
    the notebook in Colab.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击顶部链接，链接上写着 **在 Google Colab 中运行**。这将打开 Colab 中的笔记本。
- en: From the menu, select **Runtime** | **Change runtime type** **to GPU** and then
    click **Save**. We are going to convert this example to use the Lunar Lander from
    Cart Pole. As we know, this will take more compute cycles.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从菜单中选择 **运行时** | **更改运行时类型** **到 GPU**，然后点击 **保存**。我们将把这个示例转换为使用来自 Cart Pole
    的 Lunar Lander。正如我们所知，这将需要更多的计算周期。
- en: 'First, we will want to modify the initial `pip install` commands to import
    the full `gym` package by updating the commands in the first cell to the following:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们希望修改初始 `pip install` 命令，通过更新第一个单元格中的命令来导入完整的 `gym` 包。
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will want to locate the two cells that refer to the **CartPole** environment.
    We want to change all mentions of **CartPole** to **LunarLander**, as shown in
    the following code:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们希望定位到两个提及 **CartPole** 环境的单元格。我们希望将所有提及 **CartPole** 的内容更改为 **LunarLander**，如下所示：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The algorithm this example uses is a simple DQN model. As we know from experience,
    we can''t just run the same hyperparameters for `LunarLander`; therefore, we will
    change them to the following:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个示例使用的算法是一个简单的 DQN 模型。根据我们的经验，我们不能只是为 `LunarLander` 运行相同的超参数；因此，我们将它们更改为以下内容：
- en: '`num_iterations`: 500000 from 20000'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`迭代次数`: 从 20000 变为 500000'
- en: '`initial_collect_steps`: 20000 from 1000'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`初始收集步骤`: 从 1000 变为 20000'
- en: '`collect_steps_per_iteration`: 5 from 1'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`每迭代收集步骤数`: 从 1 变为 5'
- en: '`replay_buffer_max_length`: 250000 from 100000'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`重放缓冲区最大长度`: 从 100000 变为 250000'
- en: '`batch_size`: 32 from 64'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`批量大小`: 从 64 变为 32'
- en: '`learning_rate`: 1e-35 from 1e-3'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`学习率`: 从 1e-3 变为 1e-35'
- en: '`log_interval`: 2000 from 200'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`日志间隔`: 从 200 变为 2000'
- en: '`num_eval_episodes`: 15 from 10'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`评估回合数`: 从 10 变为 15'
- en: '`eval_interval`: 500 from 1000'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`评估间隔`: 从 1000 变为 500'
- en: 'Let''s move on to adjusting the network size. Locate the following line of
    code and change it as shown:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续调整网络大小。定位以下代码行，并按所示进行更改：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Feel free to change other parameters as you like. If you have done your homework,
    working with this example should be very straightforward. One of the great things
    about TF-Agents and Google Colab, in general, is how interactive sample and the
    training output is.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随意更改其他参数。如果你已经完成了作业，使用这个示例应该非常直接。TF-Agents 和 Google Colab 的一般优点之一是样本和训练输出的交互性。
- en: This book was almost entirely written with Google Colab notebooks. However,
    as good as they are, notebooks still lack a few good elements needed for larger
    samples. They also make it difficult for several reasons to use later in other
    examples. Therefore a preference was given to keep the samples in Python files.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书几乎完全是用 Google Colab 笔记本编写的。然而，尽管它们很好，但笔记本仍然缺少一些对于更大样本所需的一些良好元素。它们也使得在多个原因下难以在其他示例中稍后使用。因此，优先考虑将样本保留在
    Python 文件中。
- en: From the menu, select **Runtime** | **Run all**, to run the sample and then
    wait patiently for the output. This may take a while so grab a beverage and relax
    for a while.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从菜单中选择 **运行所有**，以运行示例，然后耐心等待输出。这可能需要一段时间，所以拿一杯饮料放松一下。
- en: 'On the page, you will be able to see several other algorithm forms that we
    have covered and that we did not get time to cover in this book. The following
    is a list of the agent types TF-Agents currently supports and a brief description
    about each:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在页面上，你将能够看到我们已覆盖的几个其他算法形式，以及我们没有时间在本书中涵盖的。以下是目前TF-Agents支持的代理类型列表及其简要描述：
- en: '**DQN**: This is the standard deep Q-learning network agent we have already
    looked at plenty of times. There isn''t a DDQN agent so it looks like you may
    need to just put two DQN agents together.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DQN**：这是我们已经多次探讨的标准深度Q学习网络代理。目前没有DDQN代理，所以看起来你可能需要将两个DQN代理组合在一起。'
- en: '**REINFORCE**: This is the first policy gradient method we looked at.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**REINFORCE**：这是我们首先探讨的策略梯度方法。'
- en: '**DDPG**: This is a PG method, more specifically, the deep deterministic policy
    gradient method.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DDPG**：这是一种PG方法，更具体地说，是深度确定性策略梯度方法。'
- en: '**TD3**: This is best described as a clipped double Q-learning model that uses
    Actor-Critic to better describe the advantages in discrete action spaces. Typically,
    PG methods can perform poorly in discrete action spaces.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TD3**：这最好描述为一个剪裁的双Q学习模型，它使用Actor-Critic来更好地描述离散动作空间中的优势。通常，PG方法在离散动作空间中表现不佳。'
- en: '**PPO**: This is our old friend proximal policy optimization, another PG method.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PPO**：这是我们熟悉的最邻近策略优化，另一种PG方法。'
- en: '**SAC**: This is based on soft Actor-Critic—an off-policy maximum entropy deep
    reinforcement learning with a stochastic actor. The basic reasoning here is the
    agent maximizes expected rewards by being as random as possible.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SAC**：这是基于软Actor-Critic——一种具有随机actor的离策略最大熵深度强化学习方法。这里的推理是，代理通过尽可能随机来最大化预期奖励。'
- en: TF-Agents is a nice stable platform that allows you to build up intuitive samples
    that you can train in the cloud very easily. This will likely make it a very popular
    framework for building proof-of-concept models for a variety of problems. In the
    next section, we will wrap up this chapter with the usual additional exercises.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: TF-Agents是一个稳定且优秀的平台，它允许你轻松地在云端构建直观的样本进行训练。这可能会使其成为构建各种问题概念模型的非常受欢迎的框架。在下一节中，我们将通过通常的附加练习来结束本章。
- en: Exercises
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The exercises in this section are a bit wider in scope in this chapter in hopes
    you look through several frameworks on your own:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的练习在本章中范围更广，希望你能自己查看几个框架：
- en: Take some time and look at one of the frameworks listed earlier but not reviewed
    in this chapter.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 花些时间查看本章中未审查的早期框架之一。
- en: Use SimpleRL to solve a grid-world MDP that is different than the one in the
    example. Be sure to take the time to tune hyperparameters.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SimpleRL解决一个与示例中不同的网格世界MDP。务必花时间调整超参数。
- en: Use Google Dopamine to train an agent to play the LunarLander environment. The
    best choice is likely RainbowDQN or a variation of that.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Google Dopamine训练一个代理来玩LunarLander环境。最佳选择可能是RainbowDQN或其变体。
- en: Use Keras-RL to train an agent to play the Lunar Lander environment; make sure
    to spend time tuning hyperparameters.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras-RL训练一个代理来玩月球着陆环境；确保花时间调整超参数。
- en: Use RL Lib to train an agent to play the Lunar Lander environment; make sure
    to spend time tuning hyperparameters.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用RL Lib训练一个代理来玩月球着陆环境；确保花时间调整超参数。
- en: Modify the Keras-RL example and modify the network structure. Change the number
    of neurons and layers.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改Keras-RL示例并修改网络结构。改变神经元和层的数量。
- en: 'Modify the RL Lib example and change some of the hyperparameters such as the `num`
    workers and the number of GPUs, as shown in the following `tune` code:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改RL Lib示例并更改一些超参数，例如`num`工作者和GPU数量，如下面的`tune`代码所示：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Modify the RLLib example and use a different agent type. You will likely have
    to check the documentation for RLLib to see what other agents are supported.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改RLLib示例并使用不同的代理类型。你可能需要检查RLLib的文档以查看支持的其他代理。
- en: Use TD3 from TF-Agents to train an agent to complete the Lunar Lander environment.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TF-Agents中的TD3训练一个代理来完成月球着陆环境。
- en: Use SAC from TF-Agents and use it train the Lunar Lander environment.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TF-Agents中的SAC并使用它来训练月球着陆环境。
- en: Feel free to perform these exercises with Google Colab or in your favorite IDE.
    If you do use an IDE, you may need to take extra care to install some dependencies.
    In the next and last section of this chapter, we will finish up with the summary.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随意使用 Google Colab 或您喜欢的 IDE 来执行这些练习。如果您使用的是 IDE，可能需要特别注意安装一些依赖项。在下一节和本章的最后一节中，我们将完成总结。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This was a short but intense chapter in which we spent time looking at various
    third-party DRL frameworks. Fortunately, all of these frameworks are all still
    free and open source, and let's hope they stay that way. We started by looking
    at the many growing frameworks and some pros and cons. Then, we looked at what
    are currently the most popular or promising libraries. Starting with Google Dopamine,
    which showcases RainbowDQN, we looked at how to run a quick sample of Google Colab.
    After that, Keras-RL was next, and we introduced ourselves to the Keras framework
    as well as how to use the Keras-RL library. Moving on to RLLib, we looked at the
    powerful automation of the DRL framework that has many capabilities. Finally,
    we finished up this chapter with another entry from Google, TF-Agents, where we
    ran a complete DQN agent using TF-Agents on a Google Colab notebook.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个短暂但紧张的章节，我们花时间审视了各种第三方 DRL 框架。幸运的是，所有这些框架仍然都是免费和开源的，让我们希望它们保持这种状态。我们首先审视了许多正在增长的框架以及一些优缺点。然后，我们研究了目前最受欢迎或最有前途的库。从
    Google Dopamine 开始，它展示了 RainbowDQN，我们探讨了如何在 Google Colab 上运行一个快速示例。之后，Keras-RL
    接下来，我们介绍了 Keras 框架以及如何使用 Keras-RL 库。接着转向 RLLib，我们研究了 DRL 框架强大的自动化功能，它具有许多功能。最后，我们用
    Google 的另一个项目 TF-Agents 完成了这一章，我们在 Google Colab 笔记本上使用 TF-Agents 运行了一个完整的 DQN
    代理。
- en: We have spent plenty of time learning about and using RL and DRL algorithms.
    So much so, we should be fairly comfortable with training and looking to move
    on to more challenging environments.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了大量时间学习和使用强化学习（RL）和深度强化学习（DRL）算法。如此之多，以至于我们应该对训练和寻找更具挑战性的环境感到相当舒适。
- en: 'In the next chapter, we will move on to training agents in more complex environments
    such as the real world. However, instead of the real world, we are going to use
    the next best thing: 3D worlds.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向在更复杂的环境中训练代理，例如现实世界。然而，我们不会使用现实世界，而是将使用下一个最佳选择：3D 世界。
