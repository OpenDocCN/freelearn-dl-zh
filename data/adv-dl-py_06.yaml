- en: Object Detection and Image Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测与图像分割
- en: In [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml), *Advanced Convolutional
    Networks*, we discussed some of the most popular and best performing **convolutional
    neural network **(**CNN**) models. To focus on the architecture specifics of each
    network, we viewed the models in the straightforward context of the classification
    problem. In the universe of computer vision tasks, classification is fairly straightforward,
    as it assigns a single label to an image. In this chapter, we'll shift our focus
    to two more interesting computer vision tasks—object detection and semantic segmentation,
    while the network architecture will take a backseat. We can say that these tasks
    are more complex compared to classification, because the model has to obtain a
    more comprehensive understanding of the image. It has to be able to detect different
    objects as well as their positions on the image. At the same time, the task complexity
    allows for more creative solutions. In this chapter, we'll discuss some of them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)《高级卷积网络》中，我们讨论了一些最受欢迎且性能最佳的**卷积神经网络**（**CNN**）模型。为了专注于每个网络的架构细节，我们以分类问题为背景来查看这些模型。在计算机视觉任务的世界里，分类相对简单，因为它给图像分配一个标签。在本章中，我们将焦点转向两个更有趣的计算机视觉任务——物体检测和语义分割，而网络架构则会退居其次。我们可以说，这些任务比分类更复杂，因为模型需要对图像有更全面的理解。它必须能够检测不同的物体以及它们在图像中的位置。同时，任务的复杂性也为更具创意的解决方案提供了空间。在本章中，我们将讨论其中的一些。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: 'Introduction to object detection:'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体检测介绍：
- en: Approaches to object detection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体检测方法
- en: YOLO
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO
- en: Faster R-CNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faster R-CNN
- en: 'Image segmentation:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割：
- en: U-Net
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net
- en: Mask R-CNN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mask R-CNN
- en: Introduction to object detection
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测介绍
- en: Object detection is the process of finding object instances of a certain class,
    such as faces, cars, and trees, in images or videos. Unlike classification, object
    detection can detect multiple objects as well as their location in the image.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测是找到某个类别物体实例的过程，比如人脸、汽车和树木，适用于图像或视频。与分类不同，物体检测可以检测多个物体及其在图像中的位置。
- en: 'An object detector would return a list of detected objects with the following
    information for each object:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测器将返回一个已检测物体的列表，并为每个物体提供以下信息：
- en: The class of the object (person, car, tree, and so on).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的类别（人、车、树等）。
- en: Probability (or confidence score) in the [0, 1] range, which conveys how confident
    the detector is that the object exists in that location. This is similar to the
    output of a regular classifier.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[0, 1] 范围内的概率（或置信度分数），表示检测器对物体在该位置存在的信心。这类似于常规分类器的输出。'
- en: The coordinates of the rectangular region of the image where the object is located.
    This rectangle is called a **bounding box**.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中物体所在矩形区域的坐标。这个矩形称为**边界框**。
- en: 'We can see the typical output of an object-detection algorithm in the following
    photograph. The object type and confidence score are above each bounding box:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的照片中看到物体检测算法的典型输出。每个边界框上方显示了物体类型和置信度分数：
- en: '![](img/e3e8ed91-d2f9-4711-9aef-361fce94c5c8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3e8ed91-d2f9-4711-9aef-361fce94c5c8.png)'
- en: The output of an object detector
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测器的输出
- en: Next, let's outline the different approaches to solving an object detection
    task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们概述一下不同的物体检测任务解决方法。
- en: Approaches to object detection
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测方法
- en: 'In this section, we''ll outline three approaches:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述三种方法：
- en: '**Classic sliding window**: Here, we''ll use a regular classification network
    (classifier). This approach can work with any type of classification algorithm,
    but it''s relatively slow and error-prone:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典滑动窗口**：在这里，我们将使用常规的分类网络（分类器）。这种方法可以与任何类型的分类算法配合使用，但它相对较慢且容易出错：'
- en: 'Build an image pyramid: This is a combination of different scales of the same
    image (see the following photograph). For example, each scaled image can be two
    times smaller than the previous one. In this way, we''ll be able to detect objects
    regardless of their size in the original image.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建图像金字塔：这是同一图像的不同尺度的组合（参见下图）。例如，每个缩放后的图像可以比前一个小两倍。通过这种方式，我们将能够检测到无论大小如何的物体。
- en: 'Slide the classifier across the whole image: That is, we''ll use each location
    of the image as an input to the classifier, and the result will determine the
    type of object that is in the location. The bounding box of the location is just
    the image region that we used as input.'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类器滑动整个图像：也就是说，我们将图像的每个位置作为分类器的输入，结果将决定该位置的物体类型。该位置的边界框就是我们用作输入的图像区域。
- en: 'We''ll have multiple overlapping bounding boxes for each object: We''ll use
    some heuristics to combine them in a single prediction.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个物体，我们将有多个重叠的边界框：我们将使用一些启发式方法将它们合并成一个单一的预测结果。
- en: 'Here is a diagram of the sliding window approach:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是滑动窗口方法的示意图：
- en: '![](img/4781245d-f125-4945-a2fd-51a3d4da219e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4781245d-f125-4945-a2fd-51a3d4da219e.png)'
- en: Sliding window plus image pyramid object detection
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口加图像金字塔物体检测
- en: '**Two-stage detection methods**: These methods are very accurate, but relatively
    slow. As the name suggests, they involve two steps:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两阶段检测方法**：这些方法非常精确，但相对较慢。顾名思义，它们涉及两个步骤：'
- en: A special type of CNN, called a **Region Proposal Network** (**RPN**), scans
    the image and proposes a number of possible bounding boxes, or regions of interest
    (**RoI**), where objects might be located. However, this network doesn't detect
    the type of the object, but only whether an object is present in the region.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种特殊类型的CNN，称为**区域建议网络**（**RPN**），扫描图像并提出若干可能的边界框或兴趣区域（**RoI**），这些区域可能包含物体。然而，这个网络并不检测物体的类型，而仅仅判断区域内是否存在物体。
- en: The regions of interest are sent to the second stage for object classification,
    which determines the actual object in each bounding box.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 感兴趣的区域会被送到第二阶段进行物体分类，该阶段确定每个边界框中的实际物体。
- en: '**One-stage** (**or one-shot**) **detection methods**: Here, a single CNN produces
    both the object type and the bounding box. These approaches are usually faster,
    but less accurate compared to the two-stage methods.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一阶段**（**或一击式**）**检测方法**：在这种方法中，单个CNN同时输出物体类型和边界框。这些方法通常速度较快，但相较于两阶段方法，精度较低。'
- en: In the next section, we'll introduce the YOLO—an accurate, yet efficient one-stage
    detection algorithm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍YOLO——一种精确而高效的一阶段检测算法。
- en: Object detection with YOLOv3
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YOLOv3进行物体检测
- en: 'In this section, we''ll discuss one of the most popular detection algorithms,
    called YOLO. The name is an acronym for the popular motto **you only live once**,
    which reflects the one-stage nature of the algorithm. The authors have released three versions
    with incremental improvements of the algorithm. We''ll only discuss the latest,
    v3 ( for more details, see *YOLOv3: An Incremental Improvement*, [https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一节中，我们将讨论一种最受欢迎的检测算法，称为YOLO。这个名字是流行格言**you only live once**的缩写，反映了该算法的一阶段特性。作者已经发布了三版该算法，随着版本的更新逐步改进。我们将仅讨论最新版本v3（更多详情，请参见*YOLOv3:
    An Incremental Improvement*，[https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)）。'
- en: The algorithm starts with the so-called **backbone** network called **Darknet-53** (after
    the number of convolutional layers). It is trained to classify the ImageNet dataset,
    just as the networks in [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*. It is fully convolutional (no pooling layers)
    and uses residual connections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法从所谓的**主干**网络**Darknet-53**开始（根据卷积层的数量命名）。它经过训练以对ImageNet数据集进行分类，类似于[第3章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)中的网络，*高级卷积神经网络*。它是完全卷积的（没有池化层），并使用残差连接。
- en: 'The following diagram shows the backbone architecture:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了主干网络架构：
- en: '![](img/bd1d4252-1a0c-4f41-8a0e-310cdfd4afc5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd1d4252-1a0c-4f41-8a0e-310cdfd4afc5.png)'
- en: The Darknet-53 model (source: https://arxiv.org/abs/1804.02767)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Darknet-53模型（来源：[https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)）
- en: Once the network is trained, it will serve as a base for the following object
    detection training phase. This is a case of feature extraction transfer learning,
    which we described in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*. The fully connected layers of the backbone
    are replaced with new randomly initialized convolutional and fully connected layers.
    The new fully connected layers will output the bounding boxes, object classes,
    and confidence scores of all detected objects in just a single pass.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络训练完成，它将作为后续目标检测训练阶段的基础。这是特征提取迁移学习的一个例子，我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)中描述了这一过程，*理解卷积神经网络*。主干的全连接层将被新的随机初始化的卷积层和全连接层替换。新的全连接层将仅通过一次传递输出所有检测到的物体的边界框、物体类别和置信度得分。
- en: 'For example, the bounding boxes in the image of people on the crosswalk at
    the beginning of this section were generated using a single network pass. YOLOv3
    predicts boxes at three different scales. The system extracts features from those
    scales using a similar concept to feature pyramid networks (for more information,
    see *Feature Pyramid Networks for Object Detection*, [https://arxiv.org/abs/1612.03144](https://arxiv.org/abs/1612.03144)).
    In the detection phase, the network is trained with the common objects in context (*Microsoft
    COCO: Common Objects in Context*, [https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312), [http://cocodataset.org](http://cocodataset.org))
    object detection dataset.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，本节开始时行人在人行道上的图像中的边界框是通过单次网络传递生成的。YOLOv3在三种不同的尺度上预测边界框。该系统使用类似于特征金字塔网络的概念从这些尺度中提取特征（有关更多信息，请参见*特征金字塔网络用于目标检测*，[https://arxiv.org/abs/1612.03144](https://arxiv.org/abs/1612.03144)）。在检测阶段，网络使用常见的上下文对象进行训练（*Microsoft
    COCO: 常见上下文中的对象*，[https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312)，[http://cocodataset.org](http://cocodataset.org)）目标检测数据集。'
- en: 'Next, let''s see how YOLO works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看YOLO是如何工作的：
- en: 'Split the image into a grid of *S×S* cells (in the following diagram, we can
    see a 3×3 grid):'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像分割成*S×S*单元格（在下图中，我们可以看到一个3×3的网格）：
- en: The network treats the center of each grid cell as the center of the region,
    where an object might be located.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络将每个网格单元的中心视为该区域的中心，其中可能会有一个物体。
- en: An object might lie entirely within a cell. Then, its bounding box will be smaller
    than the cell. Alternatively, it can span over multiple cells and the bounding
    box will be larger. YOLO covers both cases.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个物体可能完全位于一个单元格内。那么，它的边界框将小于该单元格。或者，它可以跨越多个单元格，边界框会更大。YOLO涵盖了这两种情况。
- en: The algorithm can detect multiple objects in a grid cell with the help of **anchor
    boxes** (more on that later), but an object is associated with one cell only (a
    one-to-*n* relation). That is, if the bounding box of the object covers multiple
    cells, we'll associate the object with the cell, where the center of the bounding
    box lies. For example, the two objects in the following diagram span multiple
    cells, but they are both assigned to the central cell, because their centers lie
    in it.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法可以借助**锚框**（稍后详细介绍）在一个网格单元中检测多个物体，但每个物体只与一个单元格相关联（一个一对*n*的关系）。也就是说，如果物体的边界框覆盖了多个单元格，我们会将该物体与边界框中心所在的单元格关联。例如，以下图中的两个物体跨越了多个单元格，但它们都分配给了中心单元格，因为它们的边界框中心位于该单元格中。
- en: Some of the cells may contain an object and others might not. We are only interested
    in the ones that do.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些单元格可能包含物体，其他的则可能没有。我们只关心那些包含物体的单元格。
- en: 'The following diagram shows a 3×3 cell grid with 2 objects and their bounding
    boxes (dashed lines). Both objects are associated with the middle cell, because
    the centers of their bounding boxes lie in that cell:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个3×3单元格网格，包含2个物体及其边界框（虚线）。这两个物体都与中间的单元格相关联，因为它们的边界框中心位于该单元格内：
- en: '![](img/51d5c3d5-1325-4153-b367-4c45da92f509.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51d5c3d5-1325-4153-b367-4c45da92f509.png)'
- en: An object detection YOLO example with a 3x3 cell grid and 2 objects
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个目标检测的YOLO示例，使用3x3的单元格网格和2个物体
- en: 'Тhe network will output multiple possible detected objects for each grid cell.
    For example, if the grid is 3×3, then the output will contain 9 possible detected
    objects. For the sake of clarity, let''s discuss the output data (and its corresponding
    label) for a single grid cell/detected object. It is an array with values, *[b[x],
    b[y], b[h], b[w], p[c], c[1], c[2], ..., c[n]]*, where the values are as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络将为每个网格单元输出多个可能的检测对象。例如，如果网格是3×3，那么输出将包含9个可能的检测对象。为了清晰起见，我们先讨论单个网格单元/检测对象的输出数据（及其相应标签）。它是一个数组，包含值
    *[b[x], b[y], b[h], b[w], p[c], c[1], c[2], ..., c[n]]*，其中各个值的含义如下：
- en: '*b[x], b[y], b[h], b[w]* describes the object bounding box, if an object exists,
    then *b[x]* and *b[y]* are the coordinates of the center of the box. They are
    normalized in the [0, 1] range with respect to the size of the image. That is,
    if the image is of size 100 x 100 and *b[x] = 20* and *b[y] = 50*, their normalized
    values will be 0.2 and 0.5\. Basically, *b[h]* and *b[w]* represent the box height
    and width. They are normalized with respect to the grid cell. If the bounding
    box is larger than the cell, its value will be greater than 1\. Predicting the
    box parameters is a regression task.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b[x], b[y], b[h], b[w]* 描述了对象的边界框。如果存在对象，那么 *b[x]* 和 *b[y]* 是框的中心坐标。它们在 [0,
    1] 范围内相对于图像的大小进行归一化。也就是说，如果图像的大小是100 x 100，而 *b[x] = 20* 和 *b[y] = 50*，那么它们的归一化值将是0.2和0.5。基本上，*b[h]*
    和 *b[w]* 代表框的高度和宽度。它们相对于网格单元进行归一化。如果边界框大于单元格，则其值将大于1。预测框的参数是一个回归任务。'
- en: '*p[c]* is a confidence score in the [0, 1] range. The labels for the confidence
    score are either 0 (not present) or 1 (present), making this part of the output
    a classification task. If an object is not present, we can discard the rest of
    the array values.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[c]* 是一个置信度分数，范围为 [0, 1]。置信度分数的标签要么为0（不存在），要么为1（存在），因此这部分输出是一个分类任务。如果对象不存在，我们可以丢弃数组中的其他值。'
- en: '*c[1], c[2], ..., c[n]* is a one-hot encoding of the object class. For example,
    if we have car, person, tree, cat, and dog classes, and the current object is
    of the cat type, its encoding will be *[0, 0, 0, 1, 0]*. If we have *n* possible
    classes, the size of the output array for one cell will be *5 + n* (9 in our example).'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c[1], c[2], ..., c[n]* 是对象类别的独热编码。例如，如果我们有汽车、行人、树木、猫和狗这几类，并且当前对象是猫类，那么它的编码将是
    *[0, 0, 0, 1, 0]*。如果我们有 *n* 个可能的类别，则每个单元格输出数组的大小将是 *5 + n* （在我们的例子中是9）。'
- en: The network output/labels will contain *S×S* such arrays. For example, the length
    of the YOLO output for a 3×3 cell grid and four classes would be *3*3*9 = 81*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 网络输出/标签将包含 *S×S* 这样的数组。例如，对于一个3×3单元格网格和四个类别，YOLO输出的长度将是 *3*3*9 = 81*。
- en: Let's address the scenario with multiple objects in the same cell. Thankfully,
    YOLO proposes an elegant solution to this problem. We'll have multiple candidate
    boxes (known as **anchor boxes** or priors) with a slightly different shape associated
    with each cell. In the following diagram, we can see the grid cell (the square,
    uninterrupted line) and two anchor boxes—vertical and horizontal (the dashed lines).
    If we have multiple objects in the same cell, we'll associate each object with
    one of the anchor boxes. Conversely, if an anchor box doesn't have an associated
    object, it will have a confidence score of zero. This arrangement will also change
    the network output. We'll have multiple output arrays per grid cell (one output
    array per anchor box). To extend our previous example, let's assume we have a
    *3×3* cell grid with 4 classes and 2 anchor boxes per cell. Then, we'll have *3*3*2
    = 18* output bounding boxes and a total output length of *3*3*2*9 = 162*. Because we
    have a fixed number of cells (*S×S*) and a fixed number of anchor boxes per cell,
    the size of the network output doesn't change with the number of detected objects.
    Instead, the output will indicate whether an object is present in all possible
    anchor boxes.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看一下同一单元格中有多个对象的情况。幸运的是，YOLO提出了一个优雅的解决方案。每个单元格将与多个候选框（称为**锚框**或先验框）关联，每个锚框具有略微不同的形状。在下面的图示中，我们可以看到网格单元（方形，实线）和两个锚框——竖直的和水平的（虚线）。如果一个单元格内有多个对象，我们将把每个对象与其中一个锚框关联。相反，如果一个锚框没有关联对象，它的置信度分数将为零。这个安排也将改变网络的输出。每个网格单元将有多个输出数组（每个锚框一个输出数组）。为了扩展我们之前的例子，假设我们有一个
    *3×3* 的单元格网格，4个类别，并且每个单元格有2个锚框。那么我们将有 *3*3*2 = 18* 个输出边界框，总输出长度为 *3*3*2*9 = 162*。由于我们有固定数量的单元格
    (*S×S*) 和每个单元格固定数量的锚框，因此网络输出的大小不会因检测到的对象数量而变化。相反，输出将指示对象是否存在于所有可能的锚框中。
- en: 'In the following diagram, we can see a grid cell with two anchor boxes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图示中，我们可以看到一个网格单元格和两个锚框：
- en: '![](img/d15b17d8-2f69-4320-ac15-753e15fcf584.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d15b17d8-2f69-4320-ac15-753e15fcf584.png)'
- en: Grid cell (the square, uninterrupted line) with two anchor boxes (the dashed
    lines)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 网格单元格（方形，未中断的线）和两个锚框（虚线）
- en: 'The only question now is how to choose the proper anchor box for an object
    during training (during inference, the network will choose by itself). We''ll
    do this with the help of **Intersection over Union** (**IoU**). This is just the
    ratio between the area of the intersection of the object bounding box/anchor box
    and the area of their union:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在唯一的问题是如何在训练过程中选择物体的正确锚框（在推理过程中，网络会自己选择）。我们将通过**交并比**（**IoU**）来完成这个任务。这只是物体边界框/锚框的交集面积与它们的并集面积之间的比率：
- en: '![](img/0468041c-b463-4ca2-8b08-cf907bd50aec.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0468041c-b463-4ca2-8b08-cf907bd50aec.png)'
- en: Intersection over Union
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 交并比
- en: We'll compare the bounding box of each object to all anchor boxes, and assign
    the object to the anchor box with the highest IoU. Since the anchor boxes have
    varying sizes and shapes, IoU assures that the object will be assigned to the
    anchor box that most closely resembles its footprint on the image.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较每个物体的边界框与所有锚框，并将物体分配给IoU最高的锚框。由于锚框具有不同的大小和形状，IoU保证物体将被分配给最符合其在图像上足迹的锚框。
- en: 'Now that we (hopefully) know how YOLO works, we can use it for predictions.
    However, the output of the network might be noisy—that is, the output includes
    all possible anchor boxes for each cell, regardless of whether an object is present
    in them. Many of the boxes will overlap and actually predict the same object.
    We''ll get rid of the noise using **non-maximum suppression**. Here''s how it
    works:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们（希望）已经了解YOLO是如何工作的，我们可以用它来进行预测。然而，网络的输出可能会很嘈杂——也就是说，输出包括每个单元格的所有可能的锚框，无论其中是否存在物体。许多框会重叠，并且实际上预测相同的物体。我们将通过**非极大值抑制**来去除噪声。它是这样工作的：
- en: Discard all bounding boxes with a confidence score of less than or equal to 0.6.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃所有置信度得分小于或等于0.6的边界框。
- en: From the remaining bounding boxes, pick the one with the highest possible confidence
    score.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从剩余的边界框中，选择具有最高置信度得分的框。
- en: Discard any box whose IoU >= 0.5 with the box we selected in the previous step.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃与我们在上一步中选择的框的IoU >= 0.5的任何框。
- en: If you are worried that the network output/groundtruth data will become too
    complex or large, don't be. CNNs work well with the ImageNet dataset, which has
    1,000 categories, and therefore 1,000 outputs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你担心网络输出/真实值数据会变得过于复杂或庞大，别担心。卷积神经网络（CNN）在ImageNet数据集上表现良好，该数据集有1,000个类别，因此有1,000个输出。
- en: 'For more information about YOLO, check out the original sequence of papers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于YOLO的信息，请查阅原始的系列论文：
- en: '*You Only Look Once: Unified, Real-Time Object Detection* ([https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640))
    by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你只看一次：统一的实时物体检测*（[https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)），由Joseph
    Redmon、Santosh Divvala、Ross Girshick和Ali Farhadi提出'
- en: '*YOLO9000: Better, Faster, Stronger* ([https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242))
    by Joseph Redmon and Ali Farhadi'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLO9000：更好、更快、更强*（[https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242)），由Joseph
    Redmon和Ali Farhadi提出'
- en: '*YOLOv3: An Incremental Improvement* ([https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767))
    by Joseph Redmon and Ali Farhadi'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*YOLOv3：一种增量改进*（[https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)），由Joseph
    Redmon和Ali Farhadi提出'
- en: Now that we've introduced the theory of the YOLO algorithm, in the next section,
    we'll discuss how to use it in practice.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了YOLO算法的理论，在接下来的部分，我们将讨论如何在实际中使用它。
- en: A code example of YOLOv3 with OpenCV
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV的YOLOv3代码示例
- en: 'In this section, we''ll demonstrate how to use the YOLOv3 object detector with
    OpenCV. For this example, you''ll need `opencv-python` 4.1.1 or higher, and 250
    MB of disk space for the pretrained YOLO network. Let''s begin with the following
    steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用YOLOv3物体检测器与OpenCV。对于这个示例，你需要`opencv-python` 4.1.1或更高版本，并且需要250MB的磁盘空间来存储预训练的YOLO网络。让我们从以下步骤开始：
- en: 'Start with the imports:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从导入开始：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add some boilerplate code that downloads and stores several configuration and
    data files. We''ll start with the YOLOv3 network configuration `yolo_config` and
    `weights`, and we''ll use them to initialize the `net` network. We''ll use the
    YOLO author''s GitHub and personal website to do this:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一些模板代码，用于下载和存储多个配置和数据文件。我们从YOLOv3网络配置`yolo_config`和`weights`开始，并用它们初始化`net`网络。我们将使用YOLO作者的GitHub和个人网站来完成这一步：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we''ll download the names of the COCO dataset classes that the network
    can detect. We''ll also load them from the file. The dataset as presented in the
    COCO paper contains 91 categories. However, the dataset on the website contains
    only 80\. YOLO uses the 80-category version:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将下载网络可以检测的COCO数据集类别的名称。我们还将从文件中加载它们。COCO论文中展示的数据集包含91个类别。然而，网站上的数据集仅包含80个类别。YOLO使用的是80类别版本：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, download a test image from Wikipedia. We''ll also load the image from
    the file in the `blob` variable:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从Wikipedia下载一张测试图像。我们还将从文件中加载图像到`blob`变量中：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Feed the image to the network and do the inference:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像输入网络并进行推理：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Iterate over the classes and anchor boxes and prepare them for the next step:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历类别和锚框，并为下一步做好准备：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Remove the noise with non-max suppression. You can experiment with different
    values for `score_threshold` and `nms_threshold` to see how the detected objects
    change:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用非最大抑制去除噪声。你可以尝试不同的`score_threshold`和`nms_threshold`值，看看检测到的物体是如何变化的：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Draw the bounding boxes and their captions on the image:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像上绘制边界框及其标签：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we can display the detected objects with the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下代码显示检测到的物体：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If everything goes alright, this code block will produce the same image that
    we saw at the beginning of the *Introduction to object detection* section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，这段代码将生成与我们在*目标检测介绍*部分开始时看到的相同图像。
- en: This concludes our discussion about YOLO. In the next section, we'll introduce
    a two-stage object detector called Faster R-CNN (R-CNN stands for Regions with
    CNN).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于YOLO的讨论。在下一部分，我们将介绍一种名为Faster R-CNN的两阶段目标检测器（R-CNN代表区域与CNN）。
- en: Object detection with Faster R-CNN
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN的目标检测
- en: 'In this section, we''ll discuss a two-stage object detection algorithm called
    Faster R-CNN (*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
    Networks*, [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)).
    It is an evolution of the earlier two-stage detectors Fast R-CNN ([https://arxiv.org/abs/1504.08083](https://arxiv.org/abs/1504.08083))
    and R-CNN (*Rich feature hierarchies for accurate object detection and semantic
    segmentation*, [https://arxiv.org/abs/1311.2524](https://arxiv.org/abs/1311.2524)).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们将讨论一种名为Faster R-CNN的两阶段目标检测算法（*Faster R-CNN: Towards Real-Time Object
    Detection with Region Proposal Networks*，[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)）。它是早期两阶段检测器Fast
    R-CNN（[https://arxiv.org/abs/1504.08083](https://arxiv.org/abs/1504.08083)）和R-CNN（*Rich
    feature hierarchies for accurate object detection and semantic segmentation*，[https://arxiv.org/abs/1311.2524](https://arxiv.org/abs/1311.2524)）的演变。'
- en: 'We''ll start by outlining the general structure of Faster R-CNN, which is displayed
    in the following diagram:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述Faster R-CNN的一般结构，如下图所示：
- en: '![](img/7796c454-91ea-4eb6-b3e9-824fe7451d48.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7796c454-91ea-4eb6-b3e9-824fe7451d48.png)'
- en: The structure of Faster R-CNN; source: https://arxiv.org/abs/1506.01497
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN的结构；来源： [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)
- en: Let's keep that figure in mind while we explain the algorithm. Like YOLO, Faster
    R-CNN starts with a backbone classification network trained on ImageNet, which serves
    as a base for the different modules of the model. The authors of the paper experimented
    with VGG16 and ZF net (*Visualizing and Understanding Convolutional Networks*, [https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf))
    backbones. However, recent implementations use more contemporary architectures
    such as ResNets. The backbone net serves as a backbone (get it?) to the two other
    components of the model—the **region proposal network** (**RPN**) and the detection
    network. In the next section, we'll discuss the RPN.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解释算法时，请记住这个图。像YOLO一样，Faster R-CNN从在ImageNet上训练的主干分类网络开始，作为模型各个模块的基础。论文的作者使用了VGG16和ZF网络（*Visualizing
    and Understanding Convolutional Networks*，[https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)）作为主干。然而，近期的实现使用了更现代的架构，如ResNets。主干网络作为模型的两个其他组件——**区域提议网络**（**RPN**）和检测网络的支撑部分。在下一部分，我们将讨论RPN。
- en: Region proposal network
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 区域提议网络
- en: 'In the first stage, the RPN takes an image (of any size) as input and will output a
    set of rectangular regions of interest (RoIs), where an object might be located.
    The RPN itself is created by taking the first *p* (13 in the case of VGG and 5
    for ZF net) convolutional layers of the backbone model (see the preceding diagram).
    Once the input image is propagated to the last shared convolutional layer, the
    algorithm takes the feature map of that layer and slides another small net over
    each location of the feature map. The small net outputs whether an object is present at
    any of the *k* anchor boxes over each location (the concept of anchor box is the
    same as in YOLO). This concept is illustrated on the left-hand side image of the
    following diagram, which shows a single location of the RPN sliding over a single
    feature map of the last convolutional layer:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，RPN接收一张图像（任意大小）作为输入，并输出一组矩形兴趣区域（RoIs），其中可能包含物体。RPN本身是通过提取主干模型的前* p *（VGG的情况下是13，ZF网是5）卷积层创建的（见前图）。一旦输入图像传播到最后的共享卷积层，算法就会获取该层的特征图，并在每个特征图位置滑动另一个小网络。小网络输出在每个位置上的*k*锚框中是否存在物体（锚框的概念与YOLO中相同）。这一概念在下图的左侧图像中得以体现，显示了RPN在最后一个卷积层的单个特征图上滑动的一个位置：
- en: '![](img/1e60633c-846c-4c74-9a36-ff7c0633a697.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e60633c-846c-4c74-9a36-ff7c0633a697.png)'
- en: 'Left: RPN proposals over a single location; Right: example detections using
    RPN proposals (the labels are artificially enhanced). Source: https://arxiv.org/abs/1506.01497'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 左：RPN提议在单个位置上的结果；右：使用RPN提议的检测示例（标签经过人工增强）。来源：[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)
- en: 'The small net is fully connected to an *n×n* region at the same location over
    all input feature maps (*n = 3* according to the paper).  For example, if the
    final convolutional layer has 512 feature maps, then the small net input size
    at one location is 512 x 3 x 3 = 4,608\. Each sliding window is mapped to a lower
    dimensional (512 for VGG and 256 for ZF net) vector. This vector itself serves
    as input to the following two parallel fully connected layers:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 小网络与所有输入特征图上同一位置的*n×n*区域完全连接（根据论文，*n = 3*）。例如，如果最后的卷积层有512个特征图，那么在某个位置的小网络输入大小就是512
    x 3 x 3 = 4,608。每个滑动窗口被映射到一个低维（VGG为512，ZF网为256）的向量。这个向量本身作为输入，传递给以下两个并行的全连接层：
- en: A classification layer with *2k* units organized into *k* 2-unit binary softmax
    outputs. The output of each softmax represents a confidence score of whether an
    object is located in each of the *k* anchor boxes. The paper refers to the confidence
    score as **objectness**, which measures whether the anchor box content belongs
    to a set of objects versus background. During training, an object is assigned
    to an anchor box based on the IoU formula in the same way as in YOLO.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个分类层，包含*2k*单元，组织为*k* 2单元的二元softmax输出。每个softmax的输出表示该物体是否位于每个*k*锚框中的置信度分数。论文将置信度分数称为**物体性（objectness）**，它衡量锚框的内容是否属于一组物体，而非背景。在训练过程中，物体会根据IoU公式分配给锚框，这与YOLO中的方法相同。
- en: A regression layer with *4k* units organized into *k* 4-unit RoI coordinates.
    2 of the 4 units represent the coordinates of the RoI center in the [0:1] range
    relative to the whole image. The other two coordinates represent the height and
    width of the region, relative to the whole image (again, similar to YOLO).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个回归层，包含* 4k *单元，组织为*k* 4单元的RoI坐标。4个单元中的2个表示相对于整张图像的RoI中心坐标，范围为[0:1]。另外两个坐标表示区域的高度和宽度，相对于整张图像（同样类似于YOLO）。
- en: The authors of the paper experimented with three scales and three aspect ratios,
    resulting in nine possible anchor boxes over each location. The typical H×W size
    of the final feature map is around 2,400, which results in 2,400*9 = 21,600 anchor
    boxes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者实验了三种尺度和三种纵横比，结果在每个位置上得到了九个可能的锚框。最终特征图的典型H×W大小约为2,400，这就导致了2,400*9 = 21,600个锚框。
- en: In theory, we slide the small net over the feature map of the last convolutional
    layer. However, the small net weights are shared along all locations. Because
    of this, the sliding can be implemented as a cross-channel convolution. Therefore,
    the network can produce output for all anchor boxes in a single image pass. This
    is an improvement over Fast R-CNN, which requires a separate network pass for
    each anchor box.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们将小网络滑动到最后一个卷积层的特征图上。然而，小网络的权重在所有位置上是共享的。因此，滑动操作可以实现为跨通道卷积。因此，网络可以在一次图像传递中对所有锚框进行输出。这相对于Fast
    R-CNN是一个改进，后者需要为每个锚框进行单独的网络传递。
- en: 'The RPN is trained with backpropagation and stochastic gradient descent (what
    a surprise!). The shared convolutional layers are initialized with the weights
    of the backbone net and the rest are initialized randomly. The samples of each
    mini-batch are extracted from a single image, which contains many positive (objects)
    and negative (background) anchor boxes. The sampling ratio between the two types is
    1:1. Each anchor is assigned a binary class label (of being an object or not).
    There are two kinds of anchors with positive labels: the anchor/anchors with the
    highest IoU overlap with a groundtruth box or an anchor that has an IoU overlap
    of higher than 0.7 with any groundtruth box. If the IoU ratio of an anchor is
    lower than 0.3, the box is assigned a negative label. Anchors that are neither
    positive nor negative do not participate in the training.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: RPN通过反向传播和随机梯度下降进行训练（多么令人惊讶！）。共享的卷积层以主干网络的权重初始化，其余层随机初始化。每个小批量的样本来自单一图像，该图像包含许多正样本（物体）和负样本（背景）锚框。两种样本之间的采样比例为1:1。每个锚框都会被分配一个二进制类别标签（表示是否为物体）。有两种带有正标签的锚框：与一个真实框（groundtruth）具有最大IoU重叠的锚框，或与任意真实框的IoU重叠大于0.7的锚框。如果锚框的IoU比率低于0.3，则该框被分配为负标签。既不是正样本也不是负样本的锚框不会参与训练。
- en: 'As the RPN has two output layers (classification and regression), the training
    uses the following composite cost function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RPN有两个输出层（分类和回归），因此训练使用以下复合成本函数：
- en: '![](img/e9b2d900-ba01-43b4-9ece-37e05fb81db0.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9b2d900-ba01-43b4-9ece-37e05fb81db0.png)'
- en: 'Let''s discuss it in detail:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论一下：
- en: '*i* is the index of the anchor in the mini-batch.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i* 是小批量中锚框的索引。'
- en: '*p[i]* is the classification output, which represents the predicted probability
    of anchor *i* being an object. Note *p[i]^** is the target data for the same (0
    or 1).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p[i]* 是分类输出，表示锚框 *i* 是物体的预测概率。注意 *p[i]^** 是相同的目标数据（0或1）。'
- en: '*t[i]* is the regression output vector of size 4, which represents the RoI
    parameters. As in YOLO, the  *t[i]^** is the target vector for the same.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t[i]* 是大小为4的回归输出向量，表示RoI参数。与YOLO一样， *t[i]^** 是相同的目标向量。'
- en: '*L[cls]* is a cross-entropy loss for the classification layer. *N[cls]* is
    a normalization term equal to the mini-batch size.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[cls]* 是分类层的交叉熵损失。*N[cls]* 是一个归一化项，等于小批量的大小。'
- en: '*L[reg]* is the regression loss. ![](img/2e31c521-9a23-4e85-a1a2-cdd4d558d832.png),
    where R is the mean absolute error (see the *Cost functions *section in [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of Neural
    Networks*). *N[reg]* is a normalization term equal to the total number of anchor
    locations (around 2,400).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[reg]* 是回归损失。![](img/2e31c521-9a23-4e85-a1a2-cdd4d558d832.png)，其中R是平均绝对误差（请参阅[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中的*成本函数*部分，*神经网络的基本原理*）。
    *N[reg]* 是一个归一化项，等于锚框位置的总数（大约2400个）。'
- en: Finally, the classification and regression components of the cost function are
    combined with the help of the *λ* parameter*.* Since *N[reg]* ~ 2400 and *N[cls]* =
    256, *λ* is set to 10 to preserve the balance between the two losses.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分类和回归的成本函数部分通过*λ*参数结合。由于*N[reg]* ~ 2400 和 *N[cls]* = 256，*λ* 被设定为10，以保持两者损失之间的平衡。
- en: Detection network
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测网络
- en: Now that we've discussed the RPN, let's focus on the detection network. To do
    this, we'll go back to the diagram *The structure of Faster R-CNN* at the beginning
    of the *Object detection with Faster R-CNN* section. Let's recall that in the
    first stage, the RPN already generated the RoI coordinates. The detection network
    is a regular classifier, which determines the type of object (or background) in
    the current RoI. Both the RPN and the detection net share their first convolutional
    layers, borrowed from the backbone net. But the detection net also incorporates
    the proposed regions from the RPN, along with the feature maps of the last shared
    layer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了RPN，让我们聚焦于检测网络。为了做到这一点，我们将回到*Faster R-CNN结构*的图示，这在*使用Faster R-CNN进行目标检测*部分的开头。让我们回顾一下，在第一阶段，RPN已经生成了RoI坐标。检测网络是一个常规的分类器，它决定当前RoI中的物体类型（或背景）。RPN和检测网络共享它们的第一个卷积层，这些层是从主干网络借用的。但检测网络还结合了来自RPN的提议区域，以及最后一个共享层的特征图。
- en: 'But how do we combine the inputs? We can do this with the help of **Region
    of Interest** (**RoI**) max pooling, which is the first layer of the second part
    of the detection network. An example of this operation is displayed in the following
    diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何将输入组合起来呢？我们可以借助**感兴趣区域**（**RoI**）最大池化来实现，这是检测网络第二部分的第一层。这个操作的一个示例如下图所示：
- en: '![](img/4f742144-c9af-41a6-9594-7b195e81742c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f742144-c9af-41a6-9594-7b195e81742c.png)'
- en: An example of *2×2* RoI max pooling with a 10×7 feature map and a 5×5 region
    of interest (blue rectangle)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*2×2*的RoI最大池化示例，使用10×7的特征图和一个5×5的兴趣区域（蓝色矩形）'
- en: For the sake of simplicity, we'll assume that we have a single *10×7* feature
    map and a single RoI. As we learned in the *Region proposal network* section,
    the RoI is defined by its coordinates, width, and height. The operation converts
    these parameters to actual coordinates on the feature map. In this example, the
    region size is *h×w = 5×5*. The RoI max pooling is further defined by its output
    height, *H*, and width, *W*. In this example, *H×W = 2×2*, but in practice the
    values could be larger, such as 7×7\. The operation splits the *h×w* RoI into
    a grid with (*h / H)×(w / W)* subregions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们假设有一个单一的*10×7*特征图和一个单一的RoI。正如我们在*区域提议网络*部分学到的那样，RoI由其坐标、宽度和高度定义。该操作将这些参数转换为特征图上的实际坐标。在此示例中，区域大小为*h×w
    = 5×5*。RoI最大池化进一步由其输出高度*H*和宽度*W*定义。在这个例子中，*H×W = 2×2*，但在实际应用中，这些值可能会更大，如7×7。该操作将*h×w*的RoI划分为一个网格，网格的大小为(*h
    / H)×(w / W)*子区域。
- en: As we can see from the example, the subregions might have different sizes. Once
    this is done, each subregion is downsampled to a single output cell by taking
    the maximum value of that region. In other words, RoI pooling can transform inputs
    with arbitrary sizes into a fixed-size output window. In this way, the transformed
    data can propagate through the network in a consistent format.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从示例中看到的，子区域可能具有不同的大小。一旦完成，每个子区域将通过获取该区域的最大值，降采样为单个输出单元。换句话说，RoI池化可以将任意大小的输入转换为固定大小的输出窗口。通过这种方式，转化后的数据可以以一致的格式在网络中传播。
- en: 'As we mentioned in the *Object detection with Faster R-CNN* section, the RPN
    and the detection network share their initial layers. However, they start their
    life as separate networks. The training alternates between the two in a four-step
    process:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*使用Faster R-CNN进行目标检测*部分中提到的，RPN和检测网络共享它们的初始层。然而，它们一开始是独立的网络。训练过程在两者之间交替进行，按照四步流程进行：
- en: Train the RPN, which is initialized with the ImageNet weights of the backbone
    net.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练RPN，初始化时使用骨干网络的ImageNet权重。
- en: Train the detection network, using the proposals from the freshly trained RPN
    from *step 1*. The training also starts with the weights of the ImageNet backbone
    net. At this point, the two networks don't share weights.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练检测网络，使用从*步骤1*中训练好的RPN提议。训练也从ImageNet骨干网络的权重开始。此时，两个网络并不共享权重。
- en: Use the detection net shared layers to initialize the weights of the RPN. Then,
    train the RPN again, but freeze the shared layers and fine-tune the RPN-specific
    layers only. The two networks share weights now.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检测网络的共享层来初始化RPN的权重。然后，再次训练RPN，但冻结共享层，只微调RPN特定的层。此时，两个网络共享权重。
- en: Train the detection net by freezing the shared layers and fine-tuning the detection-net-specific
    layers only.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过冻结共享层，仅微调检测网络特定层来训练检测网络。
- en: Now that we've introduced Faster R-CNN, in the next section, we'll discuss how
    to use it in practice with the help of a pretrained PyTorch model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了Faster R-CNN，在接下来的章节中，我们将讨论如何在实际中使用它，借助预训练的PyTorch模型。
- en: Implementing Faster R-CNN with PyTorch
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch实现Faster R-CNN
- en: 'In this section, we''ll use a pretrained PyTorch Faster R-CNN with a ResNet50
    backbone for object detection. This example requires PyTorch 1.3.1, `torchvision`
    0.4.2, and `python-opencv` 4.1.1:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个预训练的PyTorch Faster R-CNN，搭载ResNet50骨干网络进行目标检测。此示例需要PyTorch 1.3.1、`torchvision`
    0.4.2和`python-opencv` 4.1.1：
- en: 'We''ll start with the imports:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入开始：
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, we'll continue with downloading the input image, and we'll define the
    class names in the COCO dataset. This step is the same as the one we implemented
    in the *A code example of YOLOv3 with OpenCV* section. The path to the download
    image is stored in the  `image_file = 'source_2.png'` variable, and the class
    names are stored in the `classes` list. This implementation uses the full 91 COCO
    categories.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将继续下载输入图像，并定义 COCO 数据集中的类别名称。这个步骤与我们在 *YOLOv3与OpenCV的代码示例* 部分实现的相同。下载的图像路径存储在
    `image_file = 'source_2.png'` 变量中，类别名称存储在 `classes` 列表中。该实现使用了完整的 91 个 COCO 类别。
- en: 'We''ll load the pretrained Faster R-CNN model, and we''ll set it to evaluation
    mode:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载预训练的 Faster R-CNN 模型，并将其设置为评估模式：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we''ll read the image file with OpenCV:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用 OpenCV 读取图像文件：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll define the PyTorch `transform`  sequence, we''ll transform the image
    to a PyTorch compatible tensor, and we''ll feed it to the net. The network output
    is stored in the `output` variable. As we discussed in the *Region Proposal Network* section, `output`
    contains three components: `boxes` for the bounding box parameters, `classes`
    for the object class, and `scores` for confidence scores. The model applies NMS
    internally, and there is no need to do it in the code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义 PyTorch `transform` 序列，将图像转换为 PyTorch 兼容的张量，并将其传递给网络。网络的输出存储在 `output`
    变量中。正如我们在 *区域提议网络* 部分讨论的，`output` 包含三个部分：`boxes` 是边界框参数，`classes` 是物体类别，`scores`
    是置信度分数。模型内部应用了 NMS，因此代码中无需再执行此操作：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Before we continue with displaying the detected objects, we''ll define a set
    of random colors for each class of the COCO dataset:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们继续展示检测到的物体之前，我们将为 COCO 数据集的每个类别定义一组随机颜色：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We iterate over each bounding box and draw it on the image:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历每个边界框，并将其绘制在图像上：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Drawing the bounding boxes involves the following steps:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制边界框涉及以下步骤：
- en: Filter the boxes with a confidence score of less than 0.5 to prevent noisy detections.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉置信度低于 0.5 的框，以防止出现噪声检测。
- en: The bounding `box` parameters (extracted from `output['boxes']`) contain the
    top-left and bottom-right absolute (pixel) coordinates of the bounding box on
    the image. They are only transformed in tuples to fit the OpenCV format.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界框 `box` 参数（从 `output['boxes']` 提取）包含图像中边界框的左上角和右下角的绝对（像素）坐标。它们只是以元组的形式转换，以适应
    OpenCV 格式。
- en: Extract the class name and the color for the bounding box.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取类别名称和边界框的颜色。
- en: Draw the bounding box and the class name.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制边界框和类别名称。
- en: 'Finally, we can display the detection result with the following code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下代码显示检测结果：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This code will produce the following result (the passengers on the bus are
    also detected):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将产生以下结果（公交车上的乘客也被检测到）：
- en: '![](img/11a81f52-51d6-4535-9387-97d7612aa4a8.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11a81f52-51d6-4535-9387-97d7612aa4a8.png)'
- en: Faster R-CNN object detection
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 物体检测
- en: This concludes the section about object detection. To summarize, we discussed
    two of the most popular detection models—YOLO and Faster R-CNN. In the next section,
    we'll talk about image segmentation—you can think of it as classification on the
    pixel level.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节关于物体检测的内容已结束。总结来说，我们讨论了两种最流行的检测模型——YOLO 和 Faster R-CNN。在下一部分，我们将讨论图像分割——你可以将它视为像素级别的分类。
- en: Introducing image segmentation
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入图像分割
- en: 'Image segmentation is the process of assigning a class label (such as person,
    car, or tree) to each pixel of an image. You can think of it as classification,
    but on a pixel level—instead of classifying the entire image under one label,
    we''ll classify each pixel separately. There are two types of segmentation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是将一个类标签（如人、车或树）分配给图像的每个像素的过程。你可以将其视为分类，但它是在像素级别——而不是对整个图像进行分类，而是单独对每个像素进行分类。图像分割有两种类型：
- en: '**Semantic segmentation**: This assigns a class to each pixel, but doesn''t
    differentiate between object instances. For example, the middle image in the following
    screenshot shows a semantic segmentation mask, where the pixels of each vehicle
    have the same value. Semantic segmentation can tell us that a pixel is part of
    a vehicle, but cannot make a distinction between two vehicles.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**：这为每个像素分配一个类别，但不会区分物体实例。例如，下面截图中的中间图像显示了一个语义分割掩码，其中每个车辆的像素值相同。语义分割可以告诉我们某个像素是车辆的一部分，但不能区分两辆车。'
- en: '**Instance segmentation**: This assigns a class to each pixel and differentiates
    between object instances. For example, the image on the right in the following
    screenshot shows an instance segmentation mask, where each vehicle is segmented
    as a separate object.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例分割**：这为每个像素分配一个类别，并区分物体实例。例如，以下截图右侧的图像展示了一个实例分割掩码，其中每辆车被分割为一个独立的物体。'
- en: 'The following screenshot shows an example of semantic and instance segmentation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了语义分割和实例分割的示例：
- en: '![](img/6dd1e561-e943-41ad-a682-4edee1fede0d.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dd1e561-e943-41ad-a682-4edee1fede0d.png)'
- en: 'Left: input image; middle: semantic segmentation; right: instance segmentation;
    source: http://sceneparsing.csail.mit.edu/'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：输入图像；中间：语义分割；右侧：实例分割；来源： http://sceneparsing.csail.mit.edu/
- en: To train a segmentation algorithm, we'll need a special type of groundtruth
    data, where the labels of each image are the segmented version of the image.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个分割算法，我们需要一种特殊类型的真实数据，其中每张图像的标签是图像的分割版本。
- en: The easiest way to segment an image is by using the familiar sliding-window
    technique, which we described in the *Approaches to object detection* section.
    That is, we'll use a regular classifier and we'll slide it in either direction
    with stride 1\. After we get the prediction for a location, we'll take the pixel
    that lies in the middle of the input region and we'll assign it with the predicted
    class. Predictably, this approach is very slow because of the large number of
    pixels in an image (even a 1024×1024 image has more than 1 million pixels). Thankfully,
    there are faster and more accurate algorithms, which we'll discuss in the following sections.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的图像分割方法是使用我们在*目标检测方法*部分中描述的滑动窗口技术。也就是说，我们将使用常规分类器，并以步幅为1的方式在任意方向滑动它。在我们得到某个位置的预测后，我们会取输入区域中间的像素，并将其分配给预测类别。可以预见，这种方法非常慢，因为图像中像素的数量非常庞大（即使是1024×1024的图像，也有超过100万个像素）。幸运的是，已经有更快且更准确的算法，我们将在接下来的部分讨论这些算法。
- en: Semantic segmentation with U-Net
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用U-Net进行语义分割
- en: 'The first approach to segmentation we''ll discuss is called U-Net (*U-Net:
    Convolutional Networks for Biomedical Image Segmentation*, [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)).
    The name comes from the visualization of the network architecture. U-Net is a
    type of **fully convolutional network** (**FCN**), called so because it contains
    only convolutional layers and doesn''t have any fully connected layers. An FCN
    takes the whole image as input, and outputs its segmentation map in a single pass. We
    can separate an FCN into two virtual components (in reality, this is just a single
    network):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将讨论的第一种分割方法叫做U-Net（*U-Net: 用于生物医学图像分割的卷积网络*，[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)）。这个名字来源于网络架构的可视化。U-Net是一种**全卷积网络**（**FCN**），之所以叫这个名字，是因为它只包含卷积层，没有全连接层。FCN将整张图像作为输入，并在一次传递中输出其分割图。我们可以将FCN分为两个虚拟组件（实际上这只是一个网络）：'
- en: The encoder is the first part of the network. It is similar to a regular CNN,
    without the fully connected layers at the end. The role of the encoder is to learn
    highly abstract representations of the input image (nothing new here).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器是网络的第一部分。它类似于常规的卷积神经网络（CNN），只是最后没有全连接层。编码器的作用是学习输入图像的高度抽象表示（这里没有什么新东西）。
- en: The decoder is the second part of the network. It starts after the encoder and
    uses it as input. The role of the decoder is to translate these abstract representations
    into the segmented groundtruth data. To do this, the decoder uses the opposite
    of the encoder operations. This includes transposed convolutions (the opposite
    of convolutions) and unpooling (the opposite of pooling).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器是网络的第二部分。它在编码器之后开始，并使用编码器的输出作为输入。解码器的作用是将这些抽象表示转换为分割的真实数据。为此，解码器使用与编码器操作相反的操作。这包括转置卷积（卷积的相反操作）和反池化（池化的相反操作）。
- en: 'With that introduction, here is U-Net in all its glory:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍完这些，接下来是U-Net的全部精华：
- en: '![](img/e64e443f-6ab9-4879-8f65-4640edd938c9.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e64e443f-6ab9-4879-8f65-4640edd938c9.png)'
- en: The U-Net architecture; source: https://arxiv.org/abs/1505.04597
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net架构；来源： https://arxiv.org/abs/1505.04597
- en: Each blue box corresponds to a multichannel feature map. The number of channels
    is denoted on top of the box, and the feature map size is at the lower-left edge
    of the box. White boxes represent copied feature maps. The arrows denote the different
    operations (displayed on the legend as well). The left part of the *U* is the
    encoder, and the right part is the decoder.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 每个蓝色框表示一个多通道特征图。框上方表示通道数量，框的左下角表示特征图的大小。白色框表示复制的特征图。箭头表示不同的操作（图例中也有显示）。*U*的左侧是编码器，右侧是解码器。
- en: 'Next, let''s segment (get it?) the U-Net modules:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来分割（理解了吗？）U-Net模块：
- en: '**Encoder**: the network takes as input a 572×572 RGB image. From there, it
    continues like a regular CNN with alternating convolutional and max pooling layers.
    The encoder consists of four blocks of the following layers.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：网络以一个572×572的RGB图像作为输入。从这里开始，它像一个常规的卷积神经网络（CNN），交替进行卷积和最大池化层操作。编码器由以下四个模块组成。'
- en: Two consecutive cross-channel unpadded 3×3 convolutions with stride 1.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个连续的跨通道未填充3×3卷积，步幅为1。
- en: A 2×2 max pooling layer.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个2×2的最大池化层。
- en: ReLU activations.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU激活。
- en: Each downsampling step doubles the number of feature maps.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个下采样步骤都会将特征图的数量翻倍。
- en: The final encoder convolution ends with 1,024 28×28 feature maps.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后的编码器卷积结束时得到1,024个28×28的特征图。
- en: '**Decoder**: This is symmetrical to the encoder. The decoder takes the innermost
    28×28 feature maps and simultaneously upsamples and converts them to a 388×388
    segmentation map. It contains four upsampling blocks:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：它与编码器对称。解码器以最内层的28×28特征图作为输入，并同时进行上采样，将其转换为一个388×388的分割图。它包含四个上采样模块：'
- en: The upsampling works with 2×2 transposed convolutions with stride 2 ([Chapter
    2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)*, Understanding Convolutional Networks*),
    denoted by green vertical arrows.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上采样通过2×2的转置卷积（步幅为2）进行（[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)*，理解卷积网络*），用绿色垂直箭头表示。
- en: The output of each upsampling step is concatenated with the cropped high-resolution
    feature maps of the corresponding encoder step (grey horizontal arrows). The cropping
    is necessary because of the loss of border pixels in every convolution.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个上采样步骤的输出与对应编码器步骤的裁剪高分辨率特征图进行拼接（灰色水平箭头）。裁剪是必要的，因为每个卷积步骤都会丢失边缘像素。
- en: Each transposed convolution is followed by two regular convolutions to smooth
    the expanded image.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个转置卷积后面跟着两个常规卷积，用于平滑扩展后的图像。
- en: The upsampling steps halve the number of feature maps. The final output uses
    a 1×1 bottleneck convolution to map the 64-component feature map tensor to the
    desired number of classes. The authors of the paper have demonstrated the binary
    segmentation of medical images of cells.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上采样步骤将特征图的数量减半。最终输出使用1×1瓶颈卷积将64个分量的特征图张量映射到所需的类别数量。论文的作者展示了细胞医学图像的二分类分割。
- en: The network output is a softmax over each pixel. That is, the output contains
    as many independent softmax operations as the number of pixels. The softmax output
    for one pixel determines the pixel class. The U-Net is trained like a regular
    classification network. However, the cost function is a combination of the cross-entropy
    losses of the softmax outputs over all pixels.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络输出是对每个像素的softmax运算。也就是说，输出包含与像素数量相同的独立softmax运算。一个像素的softmax输出决定了该像素的类别。U-Net的训练方式与常规分类网络相同。然而，损失函数是所有像素的softmax输出的交叉熵损失的组合。
- en: We can see that because of the valid (unpadded) convolutions of the network,
    the output segmentation map is smaller than the input image (388 versus 572).
    However, the output map is not a rescaled version of the input image. Instead,
    it has a one-to-one scale compared to the input, but only covers the central part
    of the input tile.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，由于网络有效（未填充）卷积的作用，输出的分割图像比输入图像要小（388 对 572）。然而，输出图像并不是输入图像的缩放版本。相反，它与输入图像具有一对一的尺度，但仅覆盖输入图像的中央部分。
- en: 'This is illustrated in the following diagram:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下图中有所示意：
- en: '![](img/7c8e4167-93be-4ae8-9044-c7872536d575.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c8e4167-93be-4ae8-9044-c7872536d575.png)'
- en: An overlap-tile strategy for segmenting large images; source: https://arxiv.org/abs/1505.04597
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于分割大图像的重叠拼块策略；来源：[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)
- en: The unpadded convolutions are necessary, so the network doesn't produce noisy
    artifacts at the borders of the segmentation map. This makes it possible to segment
    images with arbitrary large sizes using the so called overlap-tile strategy. The
    input image is split in overlapping input tiles, like the one on the left of the
    preceding diagram. The segmentation map of the small light area in the image on
    the right requires the large light area (one tile) on the left image as input.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 必须使用无填充卷积，以避免网络在分割图的边缘产生噪声伪影。这使得能够使用所谓的重叠瓦片策略对任意大小的图像进行分割。输入图像被分割成重叠的输入瓦片，就像前面图示中左侧的瓦片一样。右侧图像中小的光亮区域的分割图需要左侧图像中大的光亮区域（一个瓦片）作为输入。
- en: The next input tile overlaps with the previous one in such a way that their
    segmentation maps cover adjacent areas of the image. To predict the pixels in
    the border region of the image, the missing context is extrapolated by mirroring
    the input image. In the next section, we'll discuss Mask R-CNN—a model, which
    extends Faster R-CNN for instance segmentation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个输入瓦片与前一个瓦片重叠，使得它们的分割图覆盖图像的相邻区域。为了预测图像边缘区域的像素，通过镜像输入图像来推算缺失的上下文。在下一部分中，我们将讨论
    Mask R-CNN——一个扩展 Faster R-CNN 用于实例分割的模型。
- en: Instance segmentation with Mask R-CNN
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Mask R-CNN 进行实例分割
- en: 'Mask R-CNN ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))
    is an extension of Faster R-CNN for instance segmentation. Faster R-CNN has two
    outputs for each candidate object: bounding box parameters and class labels. In
    addition to these, Mask R-CNN adds a third output—an FCN that produces a binary
    segmentation mask for each RoI. The following diagram shows the structure of Mask
    R-CNN:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))
    是 Faster R-CNN 在实例分割上的扩展。Faster R-CNN 为每个候选对象输出两个结果：边界框参数和类别标签。除了这些，Mask R-CNN
    增加了第三个输出——一个全卷积网络（FCN），为每个 RoI 生成二值分割掩码。下图展示了 Mask R-CNN 的结构：
- en: '![](img/8e225a2f-e379-4d89-8080-6c256cd70229.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e225a2f-e379-4d89-8080-6c256cd70229.png)'
- en: Mask R-CNN
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN
- en: The RPN produces anchors in five scales and three aspect ratios. The segmentation
    and classification paths both use the RoI predictions of the RPN, but otherwise
    are independent of each other. The segmentation path produces *I* *m×m* binary
    segmentation masks, one for each of the *I* classes. At training or inference,
    only the mask related to the predicted class of the classification path is considered
    and the rest are discarded. The class prediction and segmentation are parallel
    and decoupled—the classification path predicts the class of the segmented object,
    and the segmentation path determines the mask.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: RPN 在五种尺度和三种长宽比下生成锚框。分割和分类路径都使用 RPN 的 RoI 预测，但它们之间是独立的。分割路径为每个 *I* 类生成 *I* *m×m*
    二值分割掩码。在训练或推理时，仅考虑与分类路径预测的类别相关的掩码，其他掩码会被丢弃。类别预测和分割是并行且解耦的——分类路径预测分割对象的类别，而分割路径则确定掩码。
- en: 'Mask R-CNN replaces the RoI max pooling operation with a more accurate RoI
    align layer. The RPN outputs the anchor box center, and its height and width as
    four floating point numbers. Then, the RoI pooling layer translates them to integer
    feature map cell coordinates (quantization). Additionally, the division of the
    RoI to *H×W* bins also involves quantization. The RoI example from the *Object
    detection with Faster R-CNN* section shows that the bins have different sizes
    (3×3, 3×2, 2×3, 2×2). These two quantization levels can introduce misalignment
    between the RoI and the extracted features. The following diagram shows how RoI
    alignment solves this problem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 用更精确的 RoI 对齐层替代了 RoI 最大池化操作。RPN 输出锚框中心及其高度和宽度，作为四个浮动点数值。然后，RoI 池化层将这些数值转换为整数特征图单元格坐标（量化）。此外，RoI
    被划分为 *H×W* 个 bins，也涉及到量化问题。来自 *使用 Faster R-CNN 进行物体检测* 部分的 RoI 示例展示了这些 bins 大小不同（3×3、3×2、2×3、2×2）。这两种量化级别可能会导致
    RoI 与提取特征之间的错位。下图展示了 RoI 对齐如何解决这个问题：
- en: '![](img/e676d19c-e656-4212-82e0-98b12eba6157.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e676d19c-e656-4212-82e0-98b12eba6157.png)'
- en: RoI align example; source: https://arxiv.org/abs/1703.06870
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RoI 对齐示例；来源：[https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)
- en: The dashed lines represent the feature map cells. The region with solid lines
    in the middle is a 2×2 RoI overlaid on the feature map. Note that it doesn't match
    the cells exactly. Instead, it is located according to the RPN prediction without
    quantization. In the same way, a cell of the RoI (the black dots) doesn't match
    one particular cell of the feature map. The RoI align operation computes the value
    of an RoI cell with a bilinear interpolation of its adjacent cells. In this way,
    RoI align is more accurate than RoI pooling.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线代表特征图的单元格。中间实线区域是一个 2×2 的 RoI，叠加在特征图上。请注意，它并没有完全匹配单元格，而是根据 RPN 预测的位置没有量化。以相同的方式，RoI
    的一个单元（黑点）并不匹配特征图的某个特定单元。RoI 对齐操作通过双线性插值计算 RoI 单元的值，从而比 RoI 池化更加精确。
- en: At training, an RoI is assigned a positive label if it has IoU with a groundtruth
    box of at least 0.5, and negative otherwise. The mask target is the intersection
    between an RoI and its associated groundtruth mask. Only the positive RoIs participate
    in the segmentation path training.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，如果 RoI 与一个真实框的交并比（IoU）至少为 0.5，则该 RoI 被分配为正标签，否则为负标签。掩码目标是 RoI 与其关联的真实掩码的交集。只有正
    RoI 参与分割路径的训练。
- en: Implementing Mask R-CNN with PyTorch
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现 Mask R-CNN
- en: 'In this section, we''ll use a pretrained PyTorch Mask R-CNN with a ResNet50
    backbone for instance segmentation. This example requires PyTorch 1.1.0, torchvision
    0.3.0, and OpenCV 3.4.2\. This example is very similar to the one we implemented
    in the *Implementing Faster R-CNN with PyTorch* section. Because of this, we''ll
    omit some parts of the code to avoid repetition. Let''s start:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用一个具有 ResNet50 主干的预训练 PyTorch Mask R-CNN 进行实例分割。此示例需要 PyTorch 1.1.0、torchvision
    0.3.0 和 OpenCV 3.4.2。本示例与我们在 *使用 PyTorch 实现 Faster R-CNN* 部分中实现的示例非常相似。因此，为了避免重复，我们将省略一些代码部分。开始吧：
- en: The imports, `classes`, and `image_file` are the same as in the Faster R-CNN
    example.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入、`classes` 和 `image_file` 与 Faster R-CNN 示例相同。
- en: 'The first difference between the two examples is that we''ll load the Mask
    R-CNN pretrained model:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个示例之间的第一个区别是我们将加载 Mask R-CNN 预训练模型：
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We feed the input image to the network and obtain the `output` variable:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将输入图像传递给网络并获得 `output` 变量：
- en: '[PRE17]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Besides `boxes`, `classes`, and `scores`, `output` contains an additional `masks` component
    for the predicted segmentation masks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `boxes`、`classes` 和 `scores`，`output` 还包含一个额外的 `masks` 组件，用于预测的分割掩码。
- en: 'We iterate over the masks and overlay them on the image. The image and the
    mask are `numpy` arrays, and we can implement the overlay as a vector operation.
    We''ll display both the bounding boxes and the segmentation masks:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历掩码并将其叠加到图像上。图像和掩码是 `numpy` 数组，我们可以将叠加操作实现为向量化操作。我们将显示边界框和分割掩码：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can display the segmentation result as follows:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以如下显示分割结果：
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This example will produce the image on the right as follows (the original on
    the left is for comparison):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将产生右侧的图像，如下所示（左侧的原始图像用于比较）：
- en: '![](img/dc987e79-92a8-46e3-b97d-7810f262710b.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc987e79-92a8-46e3-b97d-7810f262710b.png)'
- en: Mask R-CNN instance segmentation
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 实例分割
- en: 'We can see that each segmentation mask is defined only within its bounding
    box, where all values of the segmentation mask are greater than zero. To obtain
    the actual pixels that belong to the object, we apply the mask only over the pixels,
    whose segmentation confidence score is greater than 0.5 (this code snippet is
    part of step 4 of the Mask R-CNN code example):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个分割掩码只在其边界框内定义，所有掩码的值都大于零。为了获得属于物体的实际像素，我们只对分割置信度得分大于 0.5 的像素应用掩码（这段代码是
    Mask R-CNN 代码示例中的第 4 步的一部分）：
- en: '[PRE20]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This concludes the section of the chapter devoted to image segmentation (in
    fact, it concludes the chapter itself).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了本章关于图像分割的部分（事实上，也结束了整章内容）。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed object detection and image segmentation. We started
    with the one-shot detection algorithm, YOLO, and then we continued with the two-stage
    Faster R-CNN algorithm. Next, we discussed the semantic segmentation network architecture,
    U-Net. Finally, we talked about Mask R-CNN—an extension of Faster R-CNN for instance
    segmentation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了目标检测和图像分割。我们从单次检测算法 YOLO 开始，然后继续讨论了两阶段的 Faster R-CNN 算法。接下来，我们讨论了语义分割网络架构
    U-Net。最后，我们谈到了 Mask R-CNN —— Faster R-CNN 的扩展，用于实例分割。
- en: In the next chapter, we'll explore new types of ML algorithms called generative
    models. We can use them to generate new content, such as images. Stay tuned—it
    will be fun!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨一种新的机器学习算法类型，称为生成模型。我们可以利用它们生成新的内容，如图像。敬请期待——这将会很有趣！
