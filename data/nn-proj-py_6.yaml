- en: Sentiment Analysis of Movie Reviews Using LSTM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LSTM 进行电影评论情感分析
- en: In previous chapters, we looked at neural network architectures, such as the
    basic MLP and feedforward neural networks, for classification and regression tasks.
    We then looked at CNNs, and we saw how they are used for image recognition tasks.
    In this chapter, we will turn our attention to **recurrent neural networks** (**RNNs**)
    (in particular, to **long short-term memory** (**LSTM**) networks) and how they
    can be used in sequential problems, such as **Natural Language Processing** (**NLP**).
    We will develop and train a LSTM network to predict the sentiment of movie reviews
    on IMDb.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们探讨了神经网络架构，例如基本的 MLP 和前馈神经网络，适用于分类和回归任务。随后我们看了 CNN，并了解了它们在图像识别任务中的应用。本章将重点讨论**递归神经网络**（**RNNs**）（特别是**长短期记忆网络**（**LSTM**）），以及它们如何用于顺序问题，如**自然语言处理**（**NLP**）。我们将开发并训练一个
    LSTM 网络，用于预测 IMDb 上电影评论的情感。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: Sequential problems in machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的顺序问题
- en: NLP and sentiment analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理与情感分析
- en: Introduction to RNNs and LSTM networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 和 LSTM 网络简介
- en: Analysis of the IMDb movie reviews dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMDb 电影评论数据集分析
- en: Word embeddings
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入
- en: A step-by-step guide to building and training an LSTM network in Keras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中构建和训练 LSTM 网络的分步指南
- en: Analysis of our results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们结果的分析
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The Python libraries required for this chapter are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的 Python 库如下：
- en: matplotlib 3.0.2
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib 3.0.2
- en: Keras 2.2.4
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 2.2.4
- en: seaborn 0.9.0
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seaborn 0.9.0
- en: scikit-learn 0.20.2
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 0.20.2
- en: The code for this chapter can be found in the GitHub repository for the book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的 GitHub 仓库中找到。
- en: 'To download the code onto your computer, you may run the following `git clone`
    command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要将代码下载到您的计算机上，您可以运行以下 `git clone` 命令：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the process is complete, there will be a folder entitled `Neural-Network-Projects-with-Python`.
    Enter the folder by running the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 完成过程后，将会生成一个名为 `Neural-Network-Projects-with-Python` 的文件夹。运行以下命令进入该文件夹：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要在虚拟环境中安装所需的 Python 库，请运行以下命令：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that you should have installed Anaconda on your computer first, before
    running this command. To enter the virtual environment, run the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在运行此命令之前，您应该先在计算机上安装 Anaconda。要进入虚拟环境，请运行以下命令：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Navigate to the `Chapter06` folder by running the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令进入 `Chapter06` 文件夹：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following file is located in the folder:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文件位于文件夹中：
- en: '`lstm.py`: This is the main code for this chapter'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lstm.py`：这是本章的主要代码'
- en: 'To run the code, simply execute the `lstm.py` file:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行代码，只需执行 `lstm.py` 文件：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sequential problems in machine learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的顺序问题
- en: '**Sequential problems** are a class of problem in machine learning in which
    the order of the features presented to the model is important for making predictions.
    Sequential problems are commonly encountered in the following scenarios:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序问题**是机器学习中的一类问题，其中呈现给模型的特征顺序对于做出预测至关重要。顺序问题常见于以下场景：'
- en: NLP, including sentiment analysis, language translation, and text prediction
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理，包括情感分析、语言翻译和文本预测
- en: Time series predictions
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列预测
- en: 'For example, let''s consider the text prediction problem, as shown in the following
    screenshot, which falls under NLP:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑文本预测问题，如下图所示，它属于自然语言处理范畴：
- en: '![](img/a095e669-07db-4705-b0df-32802fff8f36.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a095e669-07db-4705-b0df-32802fff8f36.png)'
- en: 'Human beings have an innate ability for this, and it is trivial for us to know
    that the word in the blank is probably the word *Japanese*. The reason for this
    is that as we read the sentence, we process the words as a sequence. The sequence
    of the words captures the information required to make the prediction. By contrast,
    if we discard the sequential information and only consider the words individually,
    we get a *bag of words,* as shown in the following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 人类天生具有这种能力，我们很容易知道空白处的词可能是*Japanese*。原因在于，当我们阅读句子时，我们将词汇处理为一个序列。这个词序列包含了进行预测所需的信息。相比之下，如果我们忽略序列信息，仅仅把单词当作独立的个体来看，就得到了一个*词袋*，如下面的示意图所示：
- en: '![](img/342284d6-22cd-487b-8f5e-e4adf78c8dca.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/342284d6-22cd-487b-8f5e-e4adf78c8dca.png)'
- en: We can see that our ability to predict the word in the blank is now severely
    impacted. Without knowing the sequence of words, it is impossible to predict the
    word in the blank.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在我们预测空白处单词的能力受到严重影响。如果不知道单词的顺序，就无法预测空白处的单词。
- en: Besides text predictions, sentiment analysis and language translation are also
    sequential problems. In fact, many NLP problems are sequential problems, because
    the languages that we speak are sequential in nature, and the sequence conveys
    context and other subtle nuances.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文本预测，情感分析和语言翻译也是顺序问题。实际上，许多NLP问题都是顺序问题，因为我们所说的语言本身就是顺序性的，顺序传达了上下文和其他细微的差别。
- en: Sequential problems also occur naturally in time series problems. Time series
    problems are common in stock markets. Often, we wish to know whether a particular
    stock will rise or fall on a certain day. This problem is accurately defined as
    a time series problem, because knowing the movement of the stocks in the preceding
    hours or minutes is often crucial to predicting whether the stock will rise or
    fall. Today, machine learning methods are being heavily applied in this domain,
    with algorithmic trading strategies driving the buying and selling of stocks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序问题在时间序列问题中也自然发生。时间序列问题在股市中很常见。我们常常希望知道某只股票在某天是否会上涨或下跌。这个问题被准确地定义为时间序列问题，因为了解股票在前几个小时或几分钟的变化通常对预测股票是涨是跌至关重要。今天，机器学习方法在这个领域得到了广泛应用，算法交易策略推动着股票的买卖。
- en: In this chapter, we will focus on NLP problems. In particular, we will create
    a neural network for sentiment analysis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论NLP问题。特别地，我们将为情感分析创建一个神经网络。
- en: NLP and sentiment analysis
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP与情感分析
- en: NLP is a subfield in **artificial intelligence** (**AI**) that is concerned
    with the interaction of computers and human languages. As early as the 1950s,
    scientists were interested in designing intelligent machines that could understand
    human languages. Early efforts to create a language translator focused on the
    rule-based approach, where a group of linguistic experts handcrafted a set of
    rules to be encoded in machines. However, this rule-based approach produced results
    that were sub-optimal, and, often, it was impossible to convert these rules from
    one language to another, which meant that scaling up was difficult. For many decades,
    not much progress was made in NLP, and human language was a goal that AI couldn't
    reach—until the resurgence of deep learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是**人工智能**（**AI**）的一个子领域，专注于计算机与人类语言的交互。早在1950年代，科学家们就对设计能够理解人类语言的智能机器产生了兴趣。早期的语言翻译工作集中在基于规则的方法上，其中一组语言学专家手工编写了一套规则，并将其编码到机器中。然而，这种基于规则的方法产生的结果并不理想，而且通常无法将这些规则从一种语言转换到另一种语言，这意味着规模化变得困难。在许多年代里，NLP领域进展缓慢，人类语言一直是AI无法达到的目标——直到深度学习的复兴。
- en: With the proliferation of deep learning and neural networks in the image classification
    domain, scientists began to wonder whether the powers of neural networks could
    be applied to NLP. In the late '00s, tech giants, including Apple, Amazon, and
    Google, applied LSTM networks to NLP problems, and the results were astonishing.
    The ability of AI assistants, such as Siri and Alexa, to understand multiple languages
    spoken in different accents was the result of deep learning and LSTM networks.
    In recent years, we have also seen a massive improvement in the abilities of text
    translation software, such as Google Translate, which is capable of producing
    translations as good as human language experts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习和神经网络在图像分类领域的普及，科学家们开始思考神经网络的能力是否可以应用于自然语言处理（NLP）。在2000年代末，科技巨头如苹果、亚马逊和谷歌将LSTM网络应用于NLP问题，结果令人震惊。AI助手（如Siri和Alexa）能够理解不同口音说出的多种语言，正是得益于深度学习和LSTM网络。近年来，我们也看到了文本翻译软件（如Google
    Translate）能力的巨大提升，它能够提供与人类语言专家相媲美的翻译结果。
- en: '**Sentiment analysis** is also an area of NLP that benefited from the resurgence
    of deep learning. Sentiment analysis is defined as the prediction of the positivity
    of a text. Most sentiment analysis problems are classification problems (positive/neutral/negative)
    and not regression problems.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感分析**也是NLP的一个领域，受益于深度学习的复兴。情感分析被定义为对文本的积极性预测。大多数情感分析问题都是分类问题（积极/中立/消极），而不是回归问题。'
- en: There are many practical applications of sentiment analysis. For example, modern
    customer service centers use sentiment analysis to predict the satisfaction of
    customers through the reviews they provide on platforms such as Yelp or Facebook.
    This allows businesses to step in immediately whenever customers are dissatisfied,
    allowing the problem to be addressed as soon as possible, and preventing customer
    churn.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析有许多实际应用。例如，现代客户服务中心通过情感分析预测客户在Yelp或Facebook等平台上提供的评论中的满意度。这使得企业能够在客户不满时立即介入，及时解决问题，避免客户流失。
- en: Sentiment analysis has also been applied in the domain of stocks trading. In
    2010, scientists showed that by sampling the sentiment in Twitter (positive versus
    negative tweets), we can predict whether the stock market will rise. Similarly,
    high-frequency trading firms use sentiment analysis to sample the sentiment of
    news related to certain companies, and execute trades automatically, based on
    the positivity of the news.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析也已应用于股票交易领域。2010年，科学家们通过分析Twitter上的情感（积极与消极的推文），证明我们可以预测股市是否会上涨。同样，高频交易公司利用情感分析来分析与特定公司相关的新闻情感，并根据新闻的积极性自动执行交易。
- en: Why sentiment analysis is difficult
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么情感分析很困难
- en: 'Early efforts in sentiment analysis faced many hurdles, due to the presence
    of subtle nuances in human languages. The same word can often covey a different
    meaning, depending on the context. Take for example the following two sentences:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析的早期工作面临许多障碍，因为人类语言中存在微妙的差异。同一个词往往会根据上下文传达不同的意义。举个例子，看看以下两句话：
- en: '![](img/5060c2e1-acc7-4763-a93f-5a2630eba114.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5060c2e1-acc7-4763-a93f-5a2630eba114.png)'
- en: We know that the sentiment of the first sentence is negative, as it probably
    means that the building is literally on fire. On the other hand, we know that
    the sentiment of the second sentence is positive, since it is unlikely that the
    person is literally on fire. Instead, it probably means that the person is on
    a *hot streak*, and this is positive. The rule-based approach toward sentiment
    analysis suffers because of these subtle nuances, and it is incredibly complex
    to encode this knowledge in a rule-based manner.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道第一句话的情感是消极的，因为它很可能意味着大楼真的着火了。另一方面，我们知道第二句话的情感是积极的，因为不太可能那个人真的着火了。相反，它可能意味着那个人正在经历一个*好时光*，这是积极的。基于规则的情感分析方法由于这些微妙的差异而遭遇困境，而且以规则化的方式编码这些知识非常复杂。
- en: 'Another reason sentiment analysis is difficult is because of sarcasm. Sarcasm
    is commonly used in many cultures, especially in an online medium. Sarcasm is
    difficult for computers to understand. In fact, even humans fail to detect sarcasm
    at times. Take for example the following sentence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析困难的另一个原因是讽刺。讽刺在许多文化中被广泛使用，尤其是在在线环境中。讽刺对计算机来说是难以理解的。事实上，即便是人类有时也难以察觉讽刺。举个例子，看一下以下这句话：
- en: '![](img/fa2c5812-d49e-49ee-8dd2-c9cf0b647c02.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa2c5812-d49e-49ee-8dd2-c9cf0b647c02.png)'
- en: You can probably detect sarcasm in the preceding sentence, and come to the conclusion
    that the sentiment is negative. However, it is not easy for a program to understand
    that.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能能在前一句话中检测到讽刺，并得出结论情感是消极的。然而，对于程序来说，要理解这一点并不容易。
- en: In the next section, we will look at RNNs and LSTM nets, and how they have been
    used to tackle sentiment analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨RNN和LSTM网络，以及它们如何被用于解决情感分析问题。
- en: RNN
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN
- en: 'Up until now, we have used neural networks such as the MLP, feedforward neural
    network, and CNN in our projects. The constraint faced by these neural networks
    is that they only accept a fixed input vector such as an image, and output another
    vector. The high-level architecture of these neural networks can be summarized
    by the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们在项目中使用了神经网络，如MLP、前馈神经网络和CNN。这些神经网络面临的限制是它们只能接受一个固定的输入向量，如图像，并输出另一个向量。这些神经网络的高层架构可以通过以下图示总结：
- en: '![](img/8f373841-27d5-4ef7-aee5-75cd6eca7761.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f373841-27d5-4ef7-aee5-75cd6eca7761.png)'
- en: 'This restrictive architecture makes it difficult for CNNs to work with sequential
    data. To work with sequential data, the neural network needs to take in specific
    bits of the data at each time step, in the sequence that it appears. This provides
    the idea for an RNN. An RNN has high-level architecture, as shown in the following
    diagram:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种限制性的架构使得 CNN 难以处理顺序数据。为了处理顺序数据，神经网络需要在每个时间步获取数据的特定部分，并按其出现的顺序进行处理。这为 RNN 提供了灵感。RNN
    的高层架构如下图所示：
- en: '![](img/afa90262-19e2-491e-8c06-6076e52cce9b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afa90262-19e2-491e-8c06-6076e52cce9b.png)'
- en: From the previous diagram, we can see that an RNN is a multi-layered neural
    network. We can break up the raw input, splitting it into time steps. For example,
    if the raw input is a sentence, we can break up the sentence into individual words
    (in this case, every word represents a time step). Each word will then be provided
    in the corresponding layer in the RNN as **Input**. More importantly, each layer
    in an RNN passes its output to the next layer. The intermediate output passed
    from layer to layer is known as the hidden state. Essentially, the hidden state
    allows an RNN to maintain a memory of the intermediate states from the sequential
    data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中我们可以看到，RNN 是一个多层神经网络。我们可以将原始输入拆分成时间步。例如，如果原始输入是一句话，我们可以将这句话拆分成单独的词（在这种情况下，每个词代表一个时间步）。然后，每个词将作为
    **输入** 提供给 RNN 中相应的层。更重要的是，RNN 中的每一层将其输出传递给下一层。从层到层传递的中间输出被称为隐藏状态。本质上，隐藏状态使得 RNN
    能够保持对顺序数据中间状态的记忆。
- en: What's inside an RNN?
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 中包含了什么？
- en: 'Let''s now take a closer look at what goes on inside each layer of an RNN.
    The following diagram depicts the mathematical function inside each layer of an
    RNN:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们仔细看看 RNN 每一层内部发生了什么。下图展示了 RNN 每一层内部的数学函数：
- en: '![](img/94651e9b-3a1d-4ffa-b914-830c5b8e82d7.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94651e9b-3a1d-4ffa-b914-830c5b8e82d7.png)'
- en: 'The mathematical function of an RNN is simple. Each layer *t *within an RNN
    has two inputs:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的数学函数非常简单。RNN 中每一层 *t* 都有两个输入：
- en: The input from the time step *t*
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自时间步 *t* 的输入
- en: The hidden state passed from the previous layer *t-1*
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从前一层传递过来的隐藏状态 *t-1*
- en: 'Each layer in an RNN simply sums up the two inputs and applies a *tanh* function
    to the sum. It then outputs the result, to be passed as a hidden state to the
    next layer. It''s that simple! More formally, the output hidden state of layer
    *t* is this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 中的每一层简单地将两个输入相加，并对总和应用 *tanh* 函数。然后它输出结果，作为隐藏状态传递给下一层。就是这么简单！更正式地说，层 *t*
    的输出隐藏状态为：
- en: '![](img/1077d130-7e09-41e7-8909-2feb6bae8f02.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1077d130-7e09-41e7-8909-2feb6bae8f02.png)'
- en: 'But what exactly is the *tanh* function? The *tanh *function is the hyperbolic
    tangent function, and it simply squashes a value between **1** and **-1**. The
    following graph illustrates this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，*tanh* 函数到底是什么呢？*tanh* 函数是双曲正切函数，它将一个值压缩到 **1** 和 **-1** 之间。下图说明了这一点：
- en: '![](img/97058633-3829-4297-b6e8-c11d927ad0ab.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97058633-3829-4297-b6e8-c11d927ad0ab.png)'
- en: The tanh function is a good choice as a non-linear transformation of the combination
    of the current input and the previous hidden state, because it ensures that the
    weights don't diverge too rapidly. It has also other nice mathematical properties,
    such as being easily differentiable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: tanh 函数作为当前输入和前一隐藏状态的非线性转换是一个不错的选择，因为它可以确保权重不会过快地发散。它还有其他一些优良的数学性质，比如易于求导。
- en: 'Finally, to get the final output from the last layer in the RNN, we simply
    apply a *sigmoid* function to it:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了从 RNN 的最后一层得到最终输出，我们只需对其应用 *sigmoid* 函数：
- en: '![](img/278c8e5e-8dcd-450e-b2eb-bf90e5ff7aa4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/278c8e5e-8dcd-450e-b2eb-bf90e5ff7aa4.png)'
- en: In the previous equation, *n *is the index of the last layer in the RNN. Recall
    from previous chapters that the *sigmoid* function produces an output between
    0 and 1, hence providing the probabilities for each class as a prediction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*n* 是 RNN 中最后一层的索引。从前面的章节中回顾，*sigmoid* 函数会产生一个介于 0 和 1 之间的输出，因此为每个类别提供概率作为预测。
- en: We can see that if we stack these layers together, the final output from an
    RNN depends on the non-linear combination of the inputs at different time steps.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果我们将这些层堆叠在一起，RNN 的最终输出依赖于不同时间步输入的非线性组合。
- en: Long- and short-term dependencies in RNNs
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 中的长短期依赖
- en: The architecture of an RNN makes it ideal for handling sequential data. Let's
    take a look at some concrete examples, to understand how an RNN handles different
    lengths of sequential data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的架构使其非常适合处理序列数据。我们来看一些具体的例子，了解 RNN 如何处理不同长度的序列数据。
- en: 'Let''s first take a look at a short piece of text as our sequential data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一段简短的文本作为我们的序列数据：
- en: '![](img/c28bc829-1bc6-4cc4-891b-2822035d7491.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c28bc829-1bc6-4cc4-891b-2822035d7491.png)'
- en: 'We can treat this short sentence as sequential data by breaking it down into
    five different inputs, with each word at each time step. This is illustrated in
    the following diagram:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将这句话拆解成五个不同的输入，将每个单词作为每个时间步，来把它视作序列数据。下图说明了这一点：
- en: '![](img/080f8b94-57ff-458f-bc19-468b7a905241.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/080f8b94-57ff-458f-bc19-468b7a905241.png)'
- en: 'Now, suppose that we are building a simple RNN to predict whether is it snowing
    based on this sequential data. The RNN would look something like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们正在构建一个简单的 RNN 来预测是否下雪，基于这个序列数据。RNN 会像这样工作：
- en: '![](img/f1694093-ef42-428a-aa8d-bab48e18164b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1694093-ef42-428a-aa8d-bab48e18164b.png)'
- en: The critical piece of information in the sequence is the word **HOT**, at time
    step 4 (**t[4]**[,]circled in red). With this piece of information, the RNN is
    able to easily predict that it is not snowing today. Notice that the critical
    piece of information came just shortly before the final output. In other words,
    we would say that there is a short-term dependency in this sequence.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 序列中的关键信息是第 4 个时间步（**t[4]**，[红圈标注])的单词**HOT**。有了这条信息，RNN 能够轻松预测今天不会下雪。请注意，关键信息出现在最终输出之前不久。换句话说，我们可以说这个序列中存在短期依赖。
- en: 'Clearly, RNNs have no problems with short-term dependencies. But what about
    long-term dependencies? Let''s take a look now at a longer sequence of text. Let''s
    use the following paragraph as an example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，RNN 在处理短期依赖时没有问题。但长期依赖怎么办呢？让我们现在来看一个更长的文本序列。我们以以下段落为例：
- en: '![](img/febffd6f-8a0e-41b8-bece-f47008b67a48.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/febffd6f-8a0e-41b8-bece-f47008b67a48.png)'
- en: 'Our goal is to predict whether the customer liked the movie. Clearly, the customer
    liked the movie but not the cinema, which was the main complaint in the paragraph.
    Let''s break up the paragraph into a sequence of inputs, with each word at each
    time step (32 time steps for 32 words in the paragraph). The RNN would look this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是预测客户是否喜欢这部电影。显然，客户喜欢这部电影，但不喜欢影院，这也是段落中的主要投诉。让我们把段落分解成一系列输入，每个单词在每个时间步（段落中
    32 个单词对应 32 个时间步）。RNN 会是这样处理的：
- en: '![](img/a2e3ea44-bff6-448e-9913-13426c6cb967.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2e3ea44-bff6-448e-9913-13426c6cb967.png)'
- en: The critical words **liked the movie** appeared between time steps 3 and 5\.
    Notice that there is a significant gap between the critical time steps and the
    output time step, as the rest of the text was largely irrelevant to the prediction
    problem (whether the customer liked the movie). In other words, we say that there
    is a long-term dependency in this sequence. Unfortunately, RNNs do not work well
    with long-term dependency sequences. RNNs have a good short-term memory, but a
    bad long-term memory. To understand why this is so, we need to understand the
    **vanishing gradient problem** when training neural networks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字**liked the movie**出现在第 3 到第 5 个时间步之间。请注意，关键时间步和输出时间步之间存在显著的间隔，因为其余文本对预测问题（客户是否喜欢这部电影）大多是无关的。换句话说，我们说这个序列中存在长期依赖。不幸的是，RNN
    在处理长期依赖序列时效果不好。RNN 具有良好的短期记忆，但长期记忆较差。为了理解这一点，我们需要理解在训练神经网络时的**消失梯度问题**。
- en: The vanishing gradient problem
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消失梯度问题
- en: The vanishing gradient problem is a problem when training deep neural networks
    using gradient-based methods such as backpropagation. Recall in previous chapters,
    we discussed the backpropagation algorithm in training neural networks. In particular,
    the `loss` function provides information on the accuracy of our predictions, and
    allows us to adjust the weights in each layer, to reduce the loss.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度问题是在使用基于梯度的训练方法（如反向传播）训练深度神经网络时出现的问题。回想一下，在之前的章节中，我们讨论了神经网络训练中的反向传播算法。特别地，`loss`
    函数提供了我们预测准确性的反馈，并使我们能够调整每一层的权重，以减少损失。
- en: 'So far, we have assumed that backpropagation works perfectly. Unfortunately,
    that is not true. When the loss is propagated backward, the loss tends to decrease
    with each successive layer:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设反向传播能够完美地工作。不幸的是，事实并非如此。当损失向后传播时，损失在每一层逐步减少：
- en: '![](img/b4f16081-a20b-45b3-bd17-1f0491d6404e.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4f16081-a20b-45b3-bd17-1f0491d6404e.png)'
- en: As a result, by the time the loss is propagated back toward the first few layers,
    the loss has already diminished so much that the weights do not change much at
    all. With such a small loss being propagated backward, it is impossible to adjust
    and train the weights of the first few layers. This phenomenon is known as the
    vanishing gradient problem in machine learning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，当损失向前传播到前几层时，损失已经减小到几乎没有变化，因此权重几乎没有发生变化。由于如此小的损失被反向传播，根本无法调整和训练前几层的权重。这个现象在机器学习中被称为梯度消失问题。
- en: Interestingly, the vanishing gradient problem does not affect CNNs in computer
    vision problems. However, when it comes to sequential data and RNNs, the vanishing
    gradient can have a significant impact. The vanishing gradient problem means that
    RNNs are unable to learn from early layers (early time steps), which causes it
    to have poor long-term memory.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，梯度消失问题并不会影响计算机视觉问题中的CNN。然而，当涉及到序列数据和RNN时，梯度消失会产生重大影响。梯度消失问题意味着RNN无法从早期层（早期时间步）中学习，这导致它在长期记忆上的表现不佳。
- en: To address this problem, Hochreiter and others proposed a clever variation of
    the RNN, known as the **long short-term memory** (**LSTM**) network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Hochreiter等人提出了一种巧妙的RNN变体，称为**长短期记忆**（**LSTM**）网络。
- en: The LSTM network
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络
- en: LSTMs are a variation of RNNs, and they solve the long-term dependency problem
    faced by conventional RNNs. Before we dive into the technicalities of LSTMs, it
    is useful to understand the intuition behind them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是RNN的一种变体，它解决了传统RNN面临的长期依赖问题。在深入探讨LSTM的技术细节之前，了解其背后的直觉是很有帮助的。
- en: LSTMs – the intuition
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM – 直觉
- en: 'As we explained in the previous section, LSTMs were designed to overcome the
    problem with long-term dependencies. Let''s assume we have this movie review:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中所解释的，LSTM被设计用来克服长期依赖问题。假设我们有以下这篇电影评论：
- en: '![](img/5025b4a0-5ce9-46b6-b1b8-2cfaa5cfe2c9.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5025b4a0-5ce9-46b6-b1b8-2cfaa5cfe2c9.png)'
- en: 'Our task is to predict whether the reviewer liked the movie. As we read this
    review, we immediately understand that this review is positive. In particular,
    the following words (highlighted) are the most important:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是预测评论者是否喜欢这部电影。当我们阅读这篇评论时，我们立刻明白这篇评论是积极的。特别是以下这些（突出显示的）单词最为重要：
- en: '![](img/bc8f9769-2b2a-425c-b41b-421a64229f4f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc8f9769-2b2a-425c-b41b-421a64229f4f.png)'
- en: If we think about it, only the highlighted words are important, and we can ignore
    the rest of the words. This is an important strategy. By selectively remembering
    certain words, we can ensure that our neural network does not get bogged down
    by too many unnecessary words that do not provide much predictive power. This
    is an important distinction of LSTMs over conventional RNNs. Conventional RNNs
    have a tendency to remember everything (even unnecessary inputs) that results
    in the inability to learn from long sequences. By contrast, LSTMs selectively
    remember important inputs (such as the preceding highlighted text), and this allows
    them to handle both short- and long-term dependencies.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细想想，只有突出显示的单词才是重要的，其余的可以忽略不计。这是一个重要策略。通过有选择地记住某些单词，我们可以确保神经网络不会被太多不必要的单词所困扰，这些不必要的单词并没有提供太多预测能力。这是LSTM相较于传统RNN的一个重要区别。传统的RNN倾向于记住所有内容（即使是无用的输入），这会导致它无法从长序列中学习。相比之下，LSTM有选择地记住重要的输入（如前面突出显示的文本），这使得它能够处理短期和长期的依赖。
- en: The ability of LSTMs to learn from both short- and long-term dependencies gives
    it its name, **long short-term memory** (**LSTM**).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM能够同时学习短期和长期依赖，因此得名**长短期记忆**（**LSTM**）。
- en: What's inside an LSTM network?
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络内部是什么？
- en: LSTMs have the same repeating structure of RNNs that we have seen previously.
    However, LSTMs differ in their internal structure.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM具有与我们之前看到的RNN相同的重复结构。然而，LSTM在其内部结构上有所不同。
- en: 'The following diagram shows a high-level overview of the repeating unit of
    an LSTM:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了LSTM重复单元的高层次概览：
- en: '![](img/c496ae83-c098-434f-984e-5019fb236cc2.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c496ae83-c098-434f-984e-5019fb236cc2.png)'
- en: The preceding diagram might look complicated to you now, but, don't worry, as
    we'll go through everything step by step. As we mentioned in the previous section,
    LSTMs have the ability to selectively remember important inputs and to forget
    the rest. The internal structure of an LSTM allows it to do that.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表现在看起来可能会有点复杂，但别担心，我们会一步一步地讲解。正如我们在上一节提到的，LSTM 具有选择性记住重要输入并忘记其余部分的能力。LSTM
    的内部结构使它能够做到这一点。
- en: An LSTM differs from a conventional RNN in that it has a cell state, in addition
    to the hidden state. You can think of the cell state as the current memory of
    the LSTM. It flows from one repeating structure to the next, conveying important
    information that has to be retained at the moment. In contrast, the hidden state
    is the overall memory of the entire LSTM. It contains everything that we have
    seen so far, both important and unimportant information.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 与传统的 RNN 不同，除了隐藏状态外，还拥有一个单元状态。你可以将单元状态看作是 LSTM 的当前记忆。它从一个重复结构流向下一个，传递必须保留的重要信息。相比之下，隐藏状态是整个
    LSTM 的总记忆。它包含了我们到目前为止看到的所有信息，包括重要和不重要的内容。
- en: 'How does the LSTM release information between the hidden state and the cell
    state? It does so via three important gates:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 如何在隐藏状态和单元状态之间释放信息？它通过三个重要的门来实现：
- en: Forget gate
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门
- en: Input gate
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门
- en: Output gate
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门
- en: Just like physical gates, the three gates restrict the flow of information from
    the hidden state to the cell state.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 就像物理门一样，这三个门限制了信息从隐藏状态流向单元状态。
- en: Forget gate
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遗忘门
- en: 'The **Forget gate (f)** of an LSTM is highlighted in the following diagram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**遗忘门（f）** 在以下图中突出显示：'
- en: '![](img/4818dad7-3ddd-49ec-b302-1e64e9f340f0.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4818dad7-3ddd-49ec-b302-1e64e9f340f0.png)'
- en: The **Forget gate (f)** forms the first part of the LSTM repeating unit, and
    its role is to decide how much data we should forget or remember from the previous
    cell state. It does so by first concatenating the **Previous Hidden State** **(****h[t−1]****)**
    and the current **Input** **(x[t]****)**, then passing the concatenated vector
    through a sigmoid function. Recall that the sigmoid function outputs a vector
    with values between 0 and 1\. A value of 0 means to stop the information from
    passing through (forget), and a value of 1 means to pass the information through
    (remember).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**遗忘门（f）** 形成 LSTM 重复单元的第一部分，其作用是决定我们应该忘记或记住从前一个单元状态中获取多少数据。它通过首先拼接**前一个隐藏状态（h[t-1]）**和当前**输入（x[t]）**，然后将拼接后的向量传递通过一个
    sigmoid 函数来实现这一点。回想一下，sigmoid 函数输出一个值介于 0 和 1 之间的向量。0 的值意味着停止信息的传递（忘记），而 1 的值意味着通过信息（记住）。'
- en: 'The output of the forget gate, *f,* is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门的输出，*f，*如下所示：
- en: '![](img/70b53f38-c533-4e23-9ab4-3daa49b7117b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70b53f38-c533-4e23-9ab4-3daa49b7117b.png)'
- en: Input gate
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入门
- en: 'The next gate is the **Input gate (i)**. The **Input gate (i)** controls how
    much information to pass to the current cell state. The input gate of an LSTM
    is highlighted in the following diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个门是**输入门（i）**。**输入门（i）**控制将多少信息传递给当前的单元状态。LSTM 的输入门在以下图中突出显示：
- en: '![](img/99ef8fca-77fd-4ab4-b03e-bdb4a516ee90.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99ef8fca-77fd-4ab4-b03e-bdb4a516ee90.png)'
- en: Just like the forget gate, the **Input gate (i)** takes as input the concatenation
    of the **Previous Hidden State (h[t-1]****)** and the current **Input (x[t]****)**.
    It then passes two copies of the concatenated vector through a sigmoid function
    and a tanh function, before multiplying them together.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与遗忘门类似，**输入门（i）**的输入是**前一个隐藏状态（h[t-1]）**和当前**输入（x[t]）**的拼接。然后它将拼接后的向量通过一个 sigmoid
    函数和一个 tanh 函数，再将它们相乘。
- en: 'The output of the input gate, *i, *is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门的输出，*i，*如下所示：
- en: '![](img/2c50b499-d1cd-4f90-ada2-8ce9a399d8dd.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c50b499-d1cd-4f90-ada2-8ce9a399d8dd.png)'
- en: 'At this point, we have what is required to compute the current cell state (**C[t]**)
    to be output. This is illustrated in the following diagram:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已具备计算当前单元状态（**C[t]**）所需的信息，输出如下图所示：
- en: '![](img/a735add6-042e-447c-a3e8-eec2751d1ff3.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a735add6-042e-447c-a3e8-eec2751d1ff3.png)'
- en: 'The current cell state *C[t]* is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的单元状态 *C[t]* 如下所示：
- en: '![](img/924d4067-4019-49cf-bf2c-df5f5320dfd0.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/924d4067-4019-49cf-bf2c-df5f5320dfd0.png)'
- en: Output gate
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出门
- en: 'Finally, the output gate controls how much information is to be retained in
    the hidden state. The output gate is highlighted in the following diagram:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出门控制要在隐藏状态中保留多少信息。输出门在以下图中突出显示：
- en: '![](img/d4ab5c42-4c8c-41e6-bd1d-9079314aafa3.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4ab5c42-4c8c-41e6-bd1d-9079314aafa3.png)'
- en: 'First, we concatenate the **Previous Hidden State (h[t−1]****)** and the current
    **Input (x[t])**, and pass it through a sigmoid function. Then, we take the current
    cell state (*C[t]*) and pass it through a tanh function. Finally, we take the
    multiplication of the two, which is passed to the next repeating unit as the hidden
    state (*h[t]*). This process is summarized by the following equation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将**前一个隐藏状态（h[t−1]）**和当前**输入（x[t]）**连接起来，并通过sigmoid函数传递。然后，我们将当前的单元状态(*C[t]*)传递通过tanh函数。最后，我们将两者相乘，结果传递到下一个重复单元作为隐藏状态(*h[t]*)。这个过程可以通过以下方程式总结：
- en: '![](img/92ecd107-228b-434f-85ee-54dd723b56af.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92ecd107-228b-434f-85ee-54dd723b56af.png)'
- en: Making sense of this
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解这一点
- en: Many beginners to LSTMs often get intimidated by the mathematical formulas involved.
    Although it is useful to understand the mathematical functions behind LSTMs, it
    is often difficult (and not very useful) to try to relate the intuition behind
    LSTMs and the mathematical formulas. Instead, it is more useful to understand
    LSTMs at a high level, and then to apply a black box algorithm, as we shall see
    in the later sections.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 许多LSTM的初学者常常会被涉及的数学公式吓到。虽然理解LSTM背后的数学函数是有用的，但试图将LSTM的直觉与数学公式联系起来往往很困难（并且不太有用）。相反，从高层次理解LSTM，然后应用黑箱算法会更加有用，正如我们将在后续章节中看到的那样。
- en: The IMDb movie reviews dataset
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IMDb电影评论数据集
- en: At this point, let's take a quick look at the IMDb movie reviews dataset before
    we start building our model. It is always a good practice to understand our data
    before we build our model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，让我们在开始构建模型之前快速查看IMDb电影评论数据集。在构建模型之前，了解我们的数据总是一个好习惯。
- en: The IMDb movie reviews dataset is a corpus of movie reviews posted on the popular
    movie reviews website [https://www.imdb.com/](https://www.imdb.com/). Each movie
    review has a label indicating whether the review is positive (1) or negative (0).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb电影评论数据集是一个收录在著名电影评论网站[https://www.imdb.com/](https://www.imdb.com/)上的电影评论语料库。每个电影评论都有一个标签，指示该评论是正面（1）还是负面（0）。
- en: 'The IMDb movie reviews dataset is provided in Keras, and we can import it by
    simply calling the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb电影评论数据集在Keras中提供，我们可以通过简单地调用以下代码来导入它：
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can print out the first movie review as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下打印出第一条电影评论：
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We''ll see the following output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We see a sequence of numbers, because Keras has already encoded the words as
    numbers as part of the preprocessing. We can convert the review back to words,
    using the built-in word-to-index dictionary provided by Keras as part of the dataset:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一串数字，因为Keras在预处理过程中已经将单词编码为数字。我们可以使用Keras作为数据集一部分提供的内置单词到索引的字典将评论转换回单词：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can show the original review in words:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将原始评论显示为文字：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We''ll see the following output:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Clearly, the sentiment of this review is negative! Let''s make sure by printing
    the `y` value:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这条评论的情感是负面的！让我们通过打印`y`值来确认：
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We''ll see the following output:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A `y` value of `0` refers to a negative review and a `y` value of `1` refers
    to a positive review. Let''s take a look at an example of a positive review:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`y`值为`0`表示负面评论，`y`值为`1`表示正面评论。我们来看一个正面评论的例子：'
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We''ll get the following output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE15]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To check the sentiment of the review, try this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查评论的情感，请尝试如下操作：
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We get the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Representing words as vectors
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将词语表示为向量
- en: 'So far, we have looked at what RNNs and LSTM networks represent. There remains
    an important question we need to address: how do we represent words as input data
    for our neural network? In the case of CNNs, we saw how images are essentially
    three-dimensional vectors/matrixes, with dimensions represented by the image width,
    height, and the number of channels (three channels for color images). The values
    in the vectors represent the intensity of each individual pixel.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了RNN和LSTM网络表示的内容。还有一个我们需要解决的重要问题：我们如何将单词表示为神经网络的输入数据？在CNN的例子中，我们看到图像本质上是三维向量/矩阵，其维度由图像的宽度、高度和通道数（彩色图像有三个通道）表示。向量中的值代表每个像素的强度。
- en: One-hot encoding
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: How do we create a similar vector/matrix for words so that they can be used
    as input to our neural network? In earlier chapters, we saw how categorical variables
    such as the day of week can be one-hot encoded to numerical variables by creating
    a new feature for each variable. It may be tempting to think that we can also
    one-hot encode our sentences in this manner, but such a method has significant
    disadvantages.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何为单词创建类似的向量/矩阵，以便它们可以作为神经网络的输入呢？在前面的章节中，我们看到如何通过为每个变量创建一个新特征，将诸如星期几这样的类别变量进行独热编码，转换为数值变量。可能会有人认为我们也可以通过这种方式对句子进行独热编码，但这种方法有显著的缺点。
- en: 'Let''s consider phrases such as the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下短语：
- en: Happy, excited
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Happy, excited
- en: Happy
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Happy
- en: Excited
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Excited
- en: 'The following diagram shows a one-hot encoded two-dimensional representation
    of these phrases:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了这些短语的独热编码二维表示：
- en: '![](img/d146e4e6-ee6c-4319-9fe7-73f6885b8b6b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d146e4e6-ee6c-4319-9fe7-73f6885b8b6b.png)'
- en: In this vector representation, the phrase **"Happy*,*** **excited"** has a value
    of **1** for both axes, because both the words **"Happy"** and **"Excited"** are
    present in the phrase. Similarly, the phrase **Happy** has a value of **1** for
    the **Happy** axis and a value of **0** for the **Excited** axis, because it only
    contains the word **Happy**.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种向量表示中，短语**“Happy”**和**“excited”**的两个轴都为**1**，因为短语中既有**“Happy”**也有**“Excited”**。类似地，短语**Happy**的**Happy**轴为**1**，**Excited**轴为**0**，因为它只包含单词**Happy**。
- en: 'The full two-dimensional vector representation is shown in the following table:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的二维向量表示如下表所示：
- en: '| **Happy** | **Excited** |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **Happy** | **Excited** |'
- en: '| 1 | 1 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 |'
- en: '| 1 | 0 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 |'
- en: '| 0 | 1 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 |'
- en: There are several problems with this one-hot encoded representation. Firstly,
    the number of axes depends on the number of unique words in our dataset. As we
    can imagine, there are tens of thousands of unique words in the English dictionary.
    If we were to create an axis for each word, then the size of our vector would
    quickly grow out of hand. Secondly, such a vector representation would be extremely
    sparse (full of zeros). This is because most words appear only once in each sentence/paragraph.
    It is difficult to train a neural network on such a sparse vector.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这种独热编码表示有几个问题。首先，轴的数量取决于数据集中唯一单词的数量。我们可以想象，英语词典中有成千上万的唯一单词。如果我们为每个单词创建一个轴，那么我们的向量大小将迅速膨胀。其次，这种向量表示会非常稀疏（充满零）。这是因为大多数单词在每个句子/段落中只出现一次。在这样的稀疏向量上训练神经网络是非常困难的。
- en: Finally, and perhaps most importantly, such a vector representation does not
    take into consideration the similarity of words. In our preceding example, **Happy**
    and **Excited** are both words that convey positive emotions. However, this one-hot
    encoded representation does not take this similarity into consideration. Thus,
    important information is lost when words are represented in this form.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也是最重要的，向量表示并没有考虑单词之间的相似性。在我们之前的例子中，**Happy**和**Excited**都是传达积极情感的词。然而，这种独热编码表示并没有考虑到这一相似性。因此，当词语以这种形式表示时，重要的信息就会丢失。
- en: As we can see, there are significant disadvantages associated with one-hot encoded
    vectors. In the next section, we'll look at **word embeddings**, which overcome
    these disadvantages.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，独热编码的向量存在显著的缺点。在接下来的章节中，我们将探讨**词嵌入**，它能够克服这些缺点。
- en: Word embeddings
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embeddings are a learned form of vector representation for words. The main
    advantage of word embeddings is that they have fewer dimensions than the one-hot
    encoded representation, and they place similar words close to one another.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种学习得到的单词向量表示形式。词嵌入的主要优势在于它们比独热编码表示的维度要少，并且它们将相似的单词彼此靠近。
- en: 'The following diagram shows an example of a word embedding:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了一个词嵌入的示例：
- en: '![](img/66dd6536-a7c8-4cc8-8852-d88aac6101d1.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66dd6536-a7c8-4cc8-8852-d88aac6101d1.png)'
- en: Notice that the learned word embedding knows that the words **"Elated"**, **"Happy"**,
    and **"Excited"** are similar words, and hence should be placed near each other.
    Similarly, the words **"Sad"**, **"Disappointed"**, **"Angry"**, and **"Furious"** are
    on the opposite ends of the spectrum, and should be placed far away.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，学习到的词嵌入知道**“Elated”**、**“Happy”**和**“Excited”**是相似的词，因此它们应该彼此靠近。同样，**“Sad”**、**“Disappointed”**、**“Angry”**和**“Furious”**处于词汇的对立面，它们应该远离彼此。
- en: We won't go into detail regarding the creation of the word embeddings, but essentially
    they are trained using supervised learning algorithms. Keras also provides a convenient
    API for training our own word embeddings. In this project, we will train our word
    embeddings on the IMDb movie reviews dataset.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讲解词嵌入的创建过程，但本质上，它们是通过监督学习算法进行训练的。Keras还提供了一个方便的API，用于训练我们自己的词嵌入。在这个项目中，我们将在IMDb电影评论数据集上训练我们的词嵌入。
- en: Model architecture
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'Let''s take a look at the model architecture of our IMDb movie review sentiment
    analyzer, shown in the following diagram:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看IMDb电影评论情感分析器的模型架构，如下图所示：
- en: '![](img/c6ceb135-9662-4be0-9145-2b4411613192.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6ceb135-9662-4be0-9145-2b4411613192.png)'
- en: This should be fairly familiar to you by now! Let's go through each component
    briefly.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这一部分应该对你很熟悉了！让我们简要回顾一下每个组件。
- en: Input
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入
- en: The input to our neural network shall be IMDb movie reviews. The reviews will
    be in the form of English sentences. As we've seen, the dataset provided in Keras
    has already encoded the English words into numbers, as neural networks require
    numerical inputs. However, there remains a problem we need to address. As we know,
    movie reviews have different lengths. If we were to represent the reviews as a
    vector, then different reviews would have different vector lengths, which is not
    acceptable for a neural network. Let's keep this in mind for now, and we'll see
    how we can address this issue as we build our neural network.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们神经网络的输入将是IMDb电影评论。评论将以英语句子的形式出现。正如我们所见，Keras提供的数据集已经将英语单词编码成数字，因为神经网络需要数值输入。然而，我们仍然面临一个问题需要解决。正如我们所知道的，电影评论的长度是不同的。如果我们将评论表示为向量，不同的评论将会有不同的向量长度，这对于神经网络来说是不可接受的。我们暂时记住这一点，随着神经网络的构建，我们将看到如何解决这个问题。
- en: Word embedding layer
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入层
- en: The first layer in our neural network is the word embedding layer. As we've
    seen earlier, word embeddings are a learned form of vector representation for
    words. The word embedding layer takes in words as input, and then outputs a vector
    representation of these words. The vector representation should place similar
    words close to one another, and dissimilar words distant from one another. The
    word embedding layer learns this vector representation during training.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们神经网络的第一层是词嵌入层。正如我们之前所看到的，词嵌入是单词的**学习型**向量表示形式。词嵌入层接收单词作为输入，然后输出这些单词的向量表示。向量表示应该将相似的单词彼此接近，将不相似的单词相隔较远。词嵌入层在训练过程中学习这种向量表示。
- en: LSTM layer
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM层
- en: The LSTM layer takes as input the vector representation of the words from the
    word embedding layer, and learns how to classify the vector representation as
    positive or negative. As we've seen earlier, LSTMs are a variation of RNNs, which
    we can think of as multiple neural networks stacked on top of one another.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层将词嵌入层的单词向量表示作为输入，并学习如何将这些向量表示分类为正面或负面。正如我们之前看到的，LSTM是RNN的一种变体，我们可以把它看作是多个神经网络堆叠在一起。
- en: Dense layer
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: The next layer is the dense layer (fully connected layer). The dense layer takes
    as input the output from the LSTM layer, and transforms it into a fully connected
    manner. Then, we apply a sigmoid activation on the dense layer, so that the final
    output is between 0 and 1.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 下一层是全连接层（dense layer）。全连接层接收LSTM层的输出，并将其转化为全连接方式。然后，我们对全连接层应用sigmoid激活函数，使得最终输出在0和1之间。
- en: Output
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出
- en: The output is a probability between 0 and 1, representing the probability that
    the movie review is positive or negative. A probability near to 1 means that the
    movie review is positive, while a probability near to 0 means that the movie review
    is negative.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个介于0和1之间的概率，表示电影评论是正面的还是负面的。接近1的概率意味着电影评论是正面的，而接近0的概率意味着电影评论是负面的。
- en: Model building in Keras
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中构建模型
- en: We're finally ready to start building our model in Keras. As a reminder, the
    model architecture that we're going to use is shown in the previous section.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好在Keras中开始构建我们的模型了。作为提醒，我们将使用的模型架构已在上一节中展示。
- en: Importing data
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'First, let''s import the dataset. The IMDb movie reviews dataset is already
    provided in Keras, so we can import it directly:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入数据集。IMDb电影评论数据集已经在Keras中提供，我们可以直接导入：
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `imdb` class has a `load_data` main function, which takes in the following
    important argument:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`imdb`类有一个`load_data`主函数，它接收以下重要参数：'
- en: '`num_words`: This is defined as the maximum number of unique words to be loaded.
    Only the *n *most common unique words (as they appear in the dataset) will be
    loaded. If *n *is small, the training time will be faster at the expense of accuracy.
    Let''s set `num_words = 10000`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_words`：这定义为要加载的唯一单词的最大数量。只会加载前*n*个最常见的唯一单词（根据数据集中的出现频率）。如果*n*较小，训练时间会更快，但准确性会受到影响。我们将`num_words
    = 10000`。'
- en: The `load_data` function returns two tuples as the output. The first tuple holds
    the training set, while the second tuple holds the testing set. Note that the
    `load_data` function splits the data equally and randomly into training and testing
    sets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_data`函数返回两个元组作为输出。第一个元组包含训练集，第二个元组包含测试集。请注意，`load_data`函数会将数据均等且随机地分割为训练集和测试集。'
- en: 'The following code imports the data, with the previously mentioned parameters:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码导入了数据，并使用了前面提到的参数：
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s do a quick check to see the amount of data we have:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速检查一下我们拥有的数据量：
- en: '[PRE20]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We''ll see the following output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/f6e43246-75a5-45ca-a83c-7928975691e4.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6e43246-75a5-45ca-a83c-7928975691e4.png)'
- en: We can see that we have `25000` training and testing samples each.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们有`25000`个训练和测试样本。
- en: Zero padding
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零填充
- en: Before we can use the data as input to our neural network, we need to address
    an issue. Recall that in the previous section, we mentioned that movie reviews
    have different lengths, and therefore the input vectors have different sizes.
    This is an issue, as neural networks only accept fixed-size vectors.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将数据作为输入用于神经网络之前，需要解决一个问题。回想上一节提到的，电影评论的长度不同，因此输入向量的大小也不同。这是一个问题，因为神经网络只接受固定大小的向量。
- en: To address this issue, we are going to define a `maxlen` parameter. The `maxlen` parameter
    shall be the maximum length of each movie review. Reviews that are longer than
    `maxlen` will be truncated, and reviews that are shorter than `maxlen` will be
    padded with zeros.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将定义一个`maxlen`参数。`maxlen`参数将是每个电影评论的最大长度。评论长度超过`maxlen`的将被截断，长度不足`maxlen`的将被零填充。
- en: 'The following diagram illustrates the zero padding process:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了零填充过程：
- en: '![](img/cd494e9a-92be-4cfc-9c9b-5558270af72a.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd494e9a-92be-4cfc-9c9b-5558270af72a.png)'
- en: Using zero padding, we ensure that the input will have a fixed vector length.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零填充，我们确保输入具有固定的向量长度。
- en: 'As always, Keras provides a handy function to perform zero padding. Under the
    Keras `preprocessing` module, there''s a `sequence` class that allows us to perform
    preprocessing for sequential data. Let''s import the `sequence` class:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，Keras提供了一个方便的函数来执行零填充。在Keras的`preprocessing`模块下，有一个`sequence`类，允许我们对序列数据进行预处理。让我们导入`sequence`类：
- en: '[PRE21]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `sequence` class has a `pad_sequences` function that allows us to perform
    zero padding on our sequential data. Let''s truncate and pad our training and
    testing data using a `maxlen` of `100`. The following code shows how we can do
    this:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`sequence`类有一个`pad_sequences`函数，允许我们对序列数据进行零填充。我们将使用`maxlen = 100`来截断并填充我们的训练和测试数据。以下代码展示了我们如何做到这一点：'
- en: '[PRE22]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s verify the vector length after zero padding:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们验证零填充后的向量长度：
- en: '[PRE23]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We''ll see the following output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/bd7a4488-6528-492b-88d7-2c21a2bdb401.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd7a4488-6528-492b-88d7-2c21a2bdb401.png)'
- en: Word embedding and LSTM layers
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入和LSTM层
- en: With our input preprocessed, we can now turn our attention to model building.
    As always, we will use the `Sequential` class in Keras to build our model. Recall
    that the `Sequential` class allows us to stack layers on top of one another, making
    it really easy to build complex models layer by layer.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理完成后，我们可以开始构建模型。像往常一样，我们将使用Keras中的`Sequential`类来构建模型。回想一下，`Sequential`类允许我们将各层堆叠在一起，使得逐层构建复杂模型变得非常简单。
- en: 'As always, let''s define a new `Sequential` class:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，让我们定义一个新的`Sequential`类：
- en: '[PRE24]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now add the word embedding layer to our model. The word embedding layer
    can be constructed directly from the `keras.layers` as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将词嵌入层添加到我们的模型中。词嵌入层可以直接通过`keras.layers`构建，如下所示：
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `Embedding` class takes the following important arguments:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding`类需要以下重要参数：'
- en: '**`input_dim`: **The input dimensions of the word embedding layer. This should
    be the same as the `num_words` parameter that we used when we loaded in our data.
    Essentially, this is the maximum number of unique words in our dataset.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`input_dim`：** 词嵌入层的输入维度。它应该与我们在加载数据时使用的`num_words`参数相同。本质上，这是数据集中唯一词汇的最大数量。'
- en: '`output_dim`: The output dimensions of the word embedding layer. This should
    be a hyperparameter to be fine-tuned. For now, let''s use a value of `128`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dim`：词嵌入层的输出维度。这应该是一个需要调优的超参数。目前，我们使用`128`作为输出维度。'
- en: 'We can add an embedding layer with the previously mentioned parameters to our
    sequential model as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面提到的参数的嵌入层添加到我们的顺序模型中，如下所示：
- en: '[PRE26]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Similarly, we can add a `LSTM` layer directly from `keras.layers` as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以直接从`keras.layers`中添加一个`LSTM`层，如下所示：
- en: '[PRE27]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `LSTM` class takes the following important arguments:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM`类接受以下重要参数：'
- en: '`units`: This refers to the number of recurring units in the `LSTM` layer.
    A larger number of units results in a more complex model, at the expense of training
    time and overfitting. For now, let''s use a typical value of `128` for the number
    of units.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`units`：这指的是`LSTM`层中循环单元的数量。更多的单元数会使得模型更加复杂，但也会增加训练时间，并可能导致过拟合。目前，我们使用`128`作为单位的典型值。'
- en: '`activation`: This refers to the type of activation function applied to the
    cell state and the hidden state. The default value is the tanh function.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：这指的是应用于单元状态和隐藏状态的激活函数类型。默认值是tanh函数。'
- en: '`recurrent_activation`: This refers to the type of activation function applied
    to the forget, input, and output gates. The default value is the `sigmoid` function.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recurrent_activation`：这指的是应用于遗忘门、输入门和输出门的激活函数类型。默认值是`sigmoid`函数。'
- en: You might notice that the kind of activation function is rather limited in Keras.
    Instead of selecting individual activations for the forget, input, and output
    gates, we are limited to choosing a single activation function for all three gates.
    This is unfortunately a limitation that we need to work with. However, the good
    news is that this deviation from theory does not significantly affect our results.
    The LSTM that we build in Keras is perfectly able to learn from the sequential
    data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，Keras中可用的激活函数种类比较有限。我们无法为遗忘门、输入门和输出门选择各自独立的激活函数，而只能为所有三个门选择一个共同的激活函数。这不幸是我们需要适应的一个限制。然而，好消息是，这一理论上的偏差并不会显著影响我们的结果。我们在Keras中构建的LSTM完全能够从顺序数据中学习。
- en: 'We can add an `LSTM` layer with the previously mentioned parameters to our
    sequential model as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面提到的参数的`LSTM`层添加到我们的顺序模型中，如下所示：
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we add a `Dense` layer with `sigmoid` as the `activation` function.
    Recall that the purpose of this layer is to ensure that the output of our model
    has a value between `0` and `1`, representing the probability that the movie review
    is positive. We can add a `Dense` layer as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一个`Dense`层，并将`sigmoid`作为`activation`激活函数。回想一下，这一层的目的是确保我们的模型输出的值在`0`到`1`之间，表示电影评论是正面的概率。我们可以如下添加`Dense`层：
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `Dense` layer is the final layer in our neural network. Let''s verify the
    structure of our model by calling the `summary()` function:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense`层是我们神经网络中的最后一层。让我们通过调用`summary()`函数来验证模型的结构：'
- en: '[PRE30]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We get the following output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '![](img/46b595da-5c3a-43ee-8d10-1d99999d405c.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46b595da-5c3a-43ee-8d10-1d99999d405c.png)'
- en: Nice! We can see that the structure of our Keras model matches the model architecture
    in the diagram that we introduced at the start of the previous section.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们可以看到，我们的Keras模型结构与上一节开始时介绍的模型架构图一致。
- en: Compiling and training models
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和训练模型
- en: 'With the model building complete, we''re ready to compile and train our model.
    By now, you should be familiar with the model compilation in Keras. As always,
    there are certain parameters we need to decide when we compile our model. They
    are as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型构建完成，我们准备好编译并训练我们的模型了。到现在为止，你应该已经熟悉了Keras中的模型编译过程。和往常一样，在编译模型时我们需要决定一些参数，它们如下：
- en: '**Loss function**: We use a `binary_crossentropy` loss function when the target
    output is binary and a `categorical_crossentropy` loss function when the target
    output is multi-class. Since the sentiment of movie reviews in this project is
    **binary** (that is, positive or negative), we will use a `binary_crossentropy`
    loss function.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：当目标输出是二分类时，我们使用`binary_crossentropy`损失函数；当目标输出是多分类时，我们使用`categorical_crossentropy`损失函数。由于本项目中电影评论的情感是**二元**的（即正面或负面），因此我们将使用`binary_crossentropy`损失函数。'
- en: '**Optimizer**: The choice of optimizer is an interesting problem in LSTMs.
    Without getting into the technicalities, certain optimizers may not work for certain
    datasets, due to the vanishing gradient and the **exploding gradient problem**
    (the opposite of the vanishing gradient problem). It is often impossible to know
    beforehand which optimizer works better for the dataset. Therefore, the best way
    to know is to train different models using different optimizers, and to use the
    optimizer that gives the best results. Let''s try the `SGD`, `RMSprop`, and the
    `adam` optimizer.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：优化器的选择是LSTM中一个有趣的问题。无需深入技术细节，某些优化器可能对某些数据集无效，这可能是由于梯度消失和**梯度爆炸问题**（与梯度消失问题相反）。通常，很难预先知道哪个优化器对于数据集表现更好。因此，最好的方法是使用不同的优化器训练不同的模型，并选择结果最好的优化器。让我们尝试使用`SGD`、`RMSprop`和`adam`优化器。'
- en: 'We can compile our model as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式编译我们的模型：
- en: '[PRE31]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s train our model for `10` epochs, using the testing set as the validation
    data. We can do so as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练我们的模型`10`个周期，使用测试集作为验证数据。我们可以按以下方式进行：
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `scores` object returned is a Python dictionary that provides the training
    and validation accuracy and the loss per epoch.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的`scores`对象是一个Python字典，提供了每个周期的训练和验证准确率以及损失。
- en: Before we go on to analyze our results, let's put all our code into a single
    function. This allows us to easily test and compare the performance of different
    optimizers.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析我们的结果之前，先将所有代码放入一个函数中。这使我们能够轻松测试和比较不同优化器的性能。
- en: 'We define a `train_model()` function that takes in an `Optimizer` as an argument:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`train_model()`函数，它接受一个`Optimizer`作为参数：
- en: '[PRE33]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Using this function, let''s train three different models using three different
    optimizers, the `SGD`, `RMSprop`, and the `adam` optimizer:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，我们将使用三种不同的优化器训练三种不同的模型，分别是`SGD`、`RMSprop`和`adam`优化器：
- en: '[PRE34]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Analyzing the results
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析结果
- en: 'Let''s plot the validation accuracy per epoch for the three different models.
    First, we plot for the model trained using the `sgd` optimizer:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制三个不同模型的每个周期验证准确率。首先，我们绘制使用`sgd`优化器训练的模型：
- en: '[PRE35]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We get the following output:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/7f1825b8-fcce-41c9-9eb9-b9f9abd20cac.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f1825b8-fcce-41c9-9eb9-b9f9abd20cac.png)'
- en: Did you notice anything wrong? The training and validation accuracy is stuck
    at 50%! Essentially, this shows that the training has failed and our neural network
    performs no better than a random coin toss for this binary classification task.
    Clearly, the `sgd` optimizer is not suitable for this dataset and this LSTM network.
    Can we do better if we use another optimizer? Let's try the `RMSprop` optimizer.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到有什么问题吗？训练和验证准确率停留在50%！实际上，这表明训练失败了，我们的神经网络在这个二分类任务上的表现不比随机投掷硬币好。显然，`sgd`优化器不适合这个数据集和这个LSTM网络。如果使用另一个优化器能做得更好吗？让我们尝试使用`RMSprop`优化器。
- en: 'We plot the training and validation accuracy for the model trained using the
    `RMSprop` optimizer, as shown in the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了使用`RMSprop`优化器训练的模型的训练和验证准确率，如下所示：
- en: '[PRE36]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We get the following output:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/2cc437e5-ae37-4787-a4ce-02b21068f538.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cc437e5-ae37-4787-a4ce-02b21068f538.png)'
- en: That's much better! Within 10 epochs, our model is able to achieve a training
    accuracy of more than 95% and a validation accuracy of around 85%. That's not
    bad at all. Clearly, the `RMSprop` optimizer performs better than the `sgd` optimizer
    for this task.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这好多了！在10个周期内，我们的模型能够达到超过95%的训练准确率和约85%的验证准确率。这个结果还不错。显然，`RMSprop`优化器在这个任务上表现得比`sgd`优化器更好。
- en: 'Finally, let''s try the `adam` optimizer and see how it performs. We plot the
    training and validation accuracy for the model trained using the `adam` optimizer,
    as shown in the following code:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试`adam`优化器，看看它的表现如何。我们绘制了使用`adam`优化器训练的模型的训练和验证准确率，如下所示：
- en: '[PRE37]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We get the following output:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/342e8c75-01db-4fa2-ac2a-737051be217f.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/342e8c75-01db-4fa2-ac2a-737051be217f.png)'
- en: The `adam` optimizer does pretty well. From the preceding graph, we can see
    that the `Training Accuracy` is almost 100% after `10` epochs, while the `Validation
    Accuracy` is around 80%. This gap of 20% suggests that overfitting is happening
    when the `adam` optimizer is used.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`adam`优化器表现得相当不错。从前面的图表可以看出，`训练准确率`在`10`轮后几乎达到了100%，而`验证准确率`大约为80%。这种20%的差距表明，使用`adam`优化器时可能出现了过拟合。'
- en: By contrast, the gap between training and validation accuracy is smaller for
    the `RMSprop` optimizer. Hence, we conclude that the `RMSprop` optimizer is the
    most optimal for this dataset and the LSTM network, and we shall use the model
    built using the `RMSprop` optimizer from this point onward.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，`RMSprop`优化器的训练准确率和验证准确率之间的差距较小。因此，我们得出结论，`RMSprop`优化器对于该数据集和LSTM网络是最优的，从现在起我们将使用基于`RMSprop`优化器构建的模型。
- en: Confusion matrix
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: In [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml), *Diabetes Prediction
    with Multilayer Perceptrons*, we saw how the confusion matrix is a useful visualization
    tool to evaluate the performance of our model. Let's also use the confusion matrix
    to evaluate the performance of our model in this project.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml)中，*多层感知机的糖尿病预测*，我们看到混淆矩阵是一个有用的可视化工具，用来评估模型的表现。让我们也使用混淆矩阵来评估我们在这个项目中的模型表现。
- en: 'To recap, these are the definitions of the terms in the confusion matrix:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这些是混淆矩阵中各个术语的定义：
- en: '**True negative**: The actual class is negative (negative sentiment), and the
    model also predicted negative'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负类**：实际类别为负（负面情感），模型也预测为负'
- en: '**False positive**: The actual class is negative (negative sentiment), but
    the model predicted positive'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**：实际类别为负（负面情感），但模型预测为正'
- en: '**False negative**: The actual class is positive (positive sentiment), but
    the model predicted negative'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**：实际类别为正（正面情感），但模型预测为负'
- en: '**True positive**: The actual class is positive (positive sentiment), and the
    model predicted positive'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正类**：实际类别为正（正面情感），模型预测为正'
- en: We want our false positive and false negative numbers to be as low as possible,
    and for the true negative and true positive numbers to be as high as possible.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望假阳性和假阴性的数量尽可能低，真负类和真正类的数量尽可能高。
- en: 'We can construct a confusion matrix using the `confusion_matrix` class from
    `sklearn`, using `seaborn` for visualization:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sklearn`中的`confusion_matrix`类构建混淆矩阵，并利用`seaborn`进行可视化：
- en: '[PRE38]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We get the following output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '![](img/f968623a-9280-45d6-b958-9c3851e9f407.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f968623a-9280-45d6-b958-9c3851e9f407.png)'
- en: From the preceding confusion matrix, we can see that most of the testing data
    was classified correctly, with the number of true negatives and true positives
    at around 85%. In other words, our model is 85% accurate at predicting sentiment
    for movie reviews. That's pretty impressive!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵可以看出，大多数测试数据被正确分类，真负类和真正类的比例约为85%。换句话说，我们的模型在预测电影评论情感时的准确率为85%。这相当令人印象深刻！
- en: 'Let''s take a look at some of the wrongly classified samples, and see where
    the model got it wrong. The following code captures the index of the wrongly classified
    samples:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些被错误分类的样本，看看模型哪里出错了。以下代码获取了错误分类样本的索引：
- en: '[PRE39]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Let's first take a look at the false positives. As a reminder, false positives
    refer to movie reviews that were negative but that our model wrongly classified
    as positive.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们来看假阳性。为了提醒大家，假阳性指的是那些实际上为负面的电影评论，但我们的模型错误地将其分类为正面。
- en: 'We have selected an interesting false positive; this is shown as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一个有趣的假阳性；如下所示：
- en: '[PRE40]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Even as a human, it is hard to predict the sentiment of this movie review! The
    first sentence of the movie probably sets the tone of the reviewer. However, it
    is written in a really subtle manner, and it is difficult for our model to pick
    out the intention of the sentence. Furthermore, the middle of the review praises
    the movie, before ending with the conclusion that the `movie gets very twisted
    at points and is hard to really understand`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 即使作为人类，预测这条电影评论的情感也是困难的！电影的第一句可能设定了评论者的基调。然而，它写得非常微妙，我们的模型很难捕捉到这句话的意图。此外，评论的中段称赞了电影，但最后得出结论，“电影在某些时刻变得非常复杂，真的很难理解”。
- en: 'Now, let''s take a look at some false negatives:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一些假阴性：
- en: '[PRE41]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This review is definitely on the fence, and it looked pretty neutral, with the
    reviewer presenting the good and bad of the movie. Another point to note is that,
    at the start of the review, the reviewer quoted another reviewer (`I hate reading
    reviews that say something like 'don't waste your time this film stinks on ice'`).
    Our model probably didn't understand that this quote is not the opinion of this
    reviewer. Quoted text is definitely a challenge for most NLP models.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇评论确实有些模棱两可，整体看起来比较中立，评论者呈现了电影的优缺点。另一个需要注意的点是，在评论的开头，评论者引用了另一位评论者的话（`我讨厌看到像'别浪费时间，这部电影糟透了'这种评论'`）。我们的模型可能未能理解这句话并不是该评论者的观点。引用文本对大多数NLP模型来说确实是一个挑战。
- en: 'Let''s take a look at another false negative:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看另一个假阴性：
- en: '[PRE42]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This movie review can be considered a *rant* against other movie reviews, similar
    to the previous review that we showed. The presence of multiple negative words
    in the movie probably misled our model, and our model did not understand that
    the review was ranting against all the other negative reviews. Statistically speaking,
    such reviews are relatively rare, and it is difficult for our model to learn the
    true sentiment of such reviews.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇电影评论可以看作是对其他电影评论的*抱怨*，类似于我们之前展示的评论。电影中多次出现负面词汇，可能误导了我们的模型，导致模型未能理解该评论是在反对所有其他负面评论。从统计学角度看，这类评论相对较少，且我们的模型很难学会准确理解此类评论的真正情感。
- en: Putting it all together
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合总结
- en: 'We have covered a lot in this chapter. Let''s consolidate all our code here:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容已经涉及了很多内容。让我们在这里整合所有的代码：
- en: '[PRE43]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we created an LSTM-based neural network that can predict the
    sentiment of movie reviews with 85% accuracy. We first looked at the theory behind
    recurrent neural networks and LSTMs, and we understood that they are a special
    class of neural network designed to handle sequential data, where the order of
    the data matters.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们创建了一个基于LSTM的神经网络，能够以85%的准确率预测电影评论的情感。我们首先了解了循环神经网络和LSTM的理论，并认识到它们是一类专门设计用于处理序列数据的神经网络，其中数据的顺序至关重要。
- en: We also looked at how we can convert sequential data such as a paragraph of
    text into a numerical vector, as input for neural networks. We saw how word embeddings
    can reduce the dimensionality of such a numerical vector into something more manageable
    for training neural networks, without necessarily losing information. A word embedding
    layer does this by learning which words are similar to one another, and it places
    such words in a cluster, in the transformed vector.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解了如何将如一段文本这样的序列数据转换为神经网络的数值向量输入。我们看到，词嵌入可以将这种数值向量的维度减少到一个更易于训练神经网络的可管理范围，而不一定会丢失信息。词嵌入层通过学习哪些词语彼此相似，并将这些词语聚集到一个簇中，来实现这一点。
- en: We also looked at how we can easily construct a LSTM neural network in Keras,
    using the `Sequential` model. We also investigated the effect of different optimizers
    on the LSTM, and we saw how the LSTM is unable to learn from the data when certain
    optimizers are used. More importantly, we saw that tuning and experimenting is
    an essential part of the machine learning process, in order to maximize our results.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了如何在Keras中使用`Sequential`模型轻松构建LSTM神经网络。我们还研究了不同优化器对LSTM的影响，发现当使用某些优化器时，LSTM无法从数据中学习。更重要的是，我们看到调参和实验是机器学习过程中至关重要的一部分，能够最大化我们的结果。
- en: Lastly, we analyzed our results, and we saw how LSTM-based neural networks fail
    to detect sarcasm and other subtleties in our language. NLP is an extremely challenging
    subfield of machine learning that researchers are still working on today.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们分析了我们的结果，发现基于LSTM的神经网络未能检测到讽刺和我们语言中的其他细微差别。自然语言处理（NLP）是机器学习中一个极具挑战性的子领域，研究人员至今仍在努力攻克。
- en: In the next chapter, [Chapter 7](9223bc03-bd68-42df-93ff-32c5d0f7e246.xhtml),
    *Implementing a Facial Recognition System with Neural Networks,* we'll look at
    **Siamese neural networks**, and how they can be used to create a face recognition
    system.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第七章](9223bc03-bd68-42df-93ff-32c5d0f7e246.xhtml)，*使用神经网络实现人脸识别系统*中，我们将探讨**孪生神经网络**，以及它们如何用于创建人脸识别系统。
- en: Questions
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are sequential problems in machine learning?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在机器学习中，什么是序列问题？
- en: Sequential problems are a class of problem in machine learning in which the
    order of the features presented to the model is important for making predictions.
    Examples of sequential problems include NLP problems (for example, speech and
    text) and time series problems.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 序列问题是机器学习中的一类问题，其中呈现给模型的特征顺序对预测结果至关重要。序列问题的例子包括 NLP 问题（例如语音和文本）和时间序列问题。
- en: What are some reasons that make it challenging for AI to solve sentiment analysis
    problems?
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些原因使得 AI 在解决情感分析问题时面临挑战？
- en: Human languages often contain words that have different meanings, depending
    on the context. It is therefore important for a machine learning model to fully
    understand the context before making a prediction. Furthermore, sarcasm is common
    in human languages, which is difficult for an AI-based model to comprehend.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 人类语言中往往包含根据上下文有不同含义的单词。因此，在做出预测之前，机器学习模型需要充分理解上下文。此外，讽刺在人的语言中很常见，这对于基于 AI 的模型来说是难以理解的。
- en: How is an RNN different than a CNN?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN 和 CNN 有何不同？
- en: RNNs can be thought of as multiple, recursive copies of a single neural network.
    Each layer in an RNN passes its output to the next layer as input. This allows
    an RNN to use sequential data as input.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 可以被看作是一个单一神经网络的多个递归副本。RNN 中的每一层将其输出作为输入传递给下一层。这使得 RNN 可以使用序列数据作为输入。
- en: What is the hidden state of an RNN?
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN 的隐藏状态是什么？
- en: The intermediate output passed from layer to layer in an RNN is known as the
    hidden state. The hidden state allows an RNN to maintain a memory of the intermediate
    states from the sequential data.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 中从一层传递到另一层的中间输出被称为隐藏状态。隐藏状态使得 RNN 能够保留来自序列数据的中间状态的记忆。
- en: What are the disadvantages of using an RNN for sequential problems?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 RNN 处理序列问题有哪些缺点？
- en: RNNs suffer from the vanishing gradient problem, which results in features early
    on in the sequence being "forgotten" due to the small weights assigned to them.
    Therefore, we say that RNNs have a long-term dependency problem.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 会遭遇梯度消失问题，这导致序列中较早的特征由于分配给它们的小权重而被“遗忘”。因此，我们说 RNN 存在长期依赖问题。
- en: How is an LSTM network different than a conventional RNN?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 网络与传统 RNN 有何不同？
- en: LSTM networks are designed to overcome the long-term dependency problem in conventional
    RNNs. An LSTM network contains three gates (input, output, and forget gates),
    which allows it to place emphasis on certain features (that is, words), regardless
    of when the feature appears in the sequence.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络旨在克服传统 RNN 中的长期依赖问题。一个 LSTM 网络包含三个门（输入门、输出门和遗忘门），使其能够强调某些特征（即单词），无论该特征在序列中的出现位置如何。
- en: What is the disadvantage of one-hot encoding words to transform them to numerical
    inputs?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 one-hot 编码将单词转化为数值输入的缺点是什么？
- en: The dimensionality of a one-hot encoded word vector tends to be huge (due to
    the amount of different words in a language), which makes it difficult for the
    neural network to learn from the vector. Furthermore, a one-hot encoded vector
    does not take into consideration the relationships between similar words in a
    language.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: one-hot 编码的单词向量的维度往往非常庞大（由于语言中有大量不同的单词），这使得神经网络难以从该向量中学习。此外，one-hot 编码的向量没有考虑语言中相似单词之间的关系。
- en: What are word embeddings?
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词向量嵌入是什么？
- en: Word embeddings are a learned formed of vector representation for words. The
    main advantage of word embeddings is that they have smaller dimensions than the
    one-hot encoded representation, and they place similar words close to one another.
    Word embeddings are usually the first layer in an LSTM-based neural network.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量嵌入是一种对单词的学习式向量表示。词向量嵌入的主要优点是它们比 one-hot 编码的表示维度更小，而且它们将相似的单词彼此靠近。词向量嵌入通常是基于
    LSTM 的神经网络中的第一层。
- en: What important preprocessing step is required when working with textual data?
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理文本数据时需要什么重要的预处理步骤？
- en: Textual data often has uneven lengths, which results in vectors of different
    sizes. Neural networks are unable to accept vectors of different sizes as input.
    Therefore, we apply zero padding as a preprocessing step, to truncate and pad
    vectors evenly.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据通常具有不均匀的长度，这会导致向量大小不同。神经网络无法接受大小不同的向量作为输入。因此，我们应用零填充作为预处理步骤，以便均匀地截断和填充向量。
- en: Tuning and experimenting is often an essential part of the machine learning
    process. What experimenting have we done in this project?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调优和实验通常是机器学习过程中不可或缺的一部分。在这个项目中，我们做了哪些实验？
- en: In this project, we experimented with different optimizers (the `SGD`, `RMSprop`,
    and `adam` optimizers) for training our neural network. We found that the `SGD`
    optimizer was unable to train the LSTM network, while the `RMSprop` optimizer
    had the best accuracy.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们尝试了不同的优化器（`SGD`、`RMSprop` 和 `adam` 优化器）来训练我们的神经网络。我们发现，`SGD` 优化器无法训练
    LSTM 网络，而 `RMSprop` 优化器的准确度最好。
