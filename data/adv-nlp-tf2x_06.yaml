- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Text Summarization with Seq2seq Attention and Transformer Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Seq2seq注意力机制和Transformer网络进行文本摘要
- en: 'Summarizing a piece of text challenges a deep learning model''s understanding
    of language. Summarization can be considered a uniquely human ability, where the
    gist of a piece of text needs to be understood and phrased. In the previous chapters,
    we have built components that can help in summarization. First, we used BERT to
    encode text and perform sentiment analysis. Then, we used a decoder architecture
    with GPT-2 to generate text. Putting the Encoder and Decoder together yields a
    summarization model. In this chapter, we will implement a seq2seq Encoder-Decoder
    with Bahdanau Attention. Specifically, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一篇文本挑战了深度学习模型对语言的理解。摘要可以看作是一个独特的人类能力，需要理解文本的要点并加以表述。在前面的章节中，我们构建了有助于摘要的组件。首先，我们使用BERT对文本进行编码并执行情感分析。然后，我们使用GPT-2的解码器架构来生成文本。将编码器和解码器结合起来，形成了一个摘要模型。在本章中，我们将实现一个带有Bahdanau注意力机制的seq2seq编码器-解码器。具体来说，我们将涵盖以下主题：
- en: Overview of extractive and abstractive text summarization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取式和抽象式文本摘要概述
- en: Building a seq2seq model with attention to summarize text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用带有注意力机制的seq2seq模型进行文本摘要
- en: Improving summarization with beam search
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过束搜索改进摘要
- en: Addressing beam search issues with length normalizations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过长度归一化解决束搜索问题
- en: Measuring the performance of summarization with ROUGE metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ROUGE指标衡量摘要性能
- en: A review of state-of-the-art summarization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最新摘要技术的回顾
- en: The first step of this journey begins with understanding the main ideas behind
    text summarization. It is important to understand the task before building a model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的第一步是理解文本摘要背后的主要思想。在构建模型之前，理解任务本身至关重要。
- en: Overview of text summarization
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本摘要概述
- en: The core idea in summarization is to condense long-form text or articles into
    a short representation. The shorter representation should contain the main idea
    of crucial information from the longer form. A single document can be summarized.
    This document could be long or may contain just a couple of sentences. An example
    of a short document summarization is generating a headline from the first few
    sentences of an article. This is called **sentence compression**. When multiple
    documents are being summarized, they are usually related. They could be the financial
    reports of a company or news reports about an event. The generated summary could
    itself be long or short. A shorter summary would be desirable when generating
    a headline. A lengthier summary would be something like an abstract and could
    have multiple sentences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要的核心思想是将长篇文本或文章浓缩为简短的表示形式。简短的表示应包含长文本中的关键信息。单个文档可以被总结，这个文档可以很长，也可以只有几句话。一个短文档摘要的例子是从文章的前几句话生成标题。这被称为**句子压缩**。当总结多个文档时，它们通常是相关的。它们可以是公司财务报告，或者关于某个事件的新闻报道。生成的摘要可以长也可以短。当生成标题时，通常希望摘要较短；而较长的摘要则像摘要部分，可能包含多句话。
- en: 'There are two main approaches when summarizing text:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 总结文本时有两种主要的方法：
- en: '**Extractive summarization**: Phrases or sentences from the articles are selected
    and put together to create a summary. A mental model for this approach is using
    a highlighter on the long-form text, and the summary is the highlights put together.
    Extractive summarization is a more straightforward approach as sentences from
    the source text can be copied, which leads to fewer grammatical issues. The quality
    of the summarization is also easier to measure using metrics such as ROUGE. This
    metric is detailed later in this chapter. Extractive summarization was the predominant
    approach before deep learning and neural networks.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取式摘要**：从文章中选取短语或句子，并将其组合成摘要。这种方法的思维模型类似于在长篇文本上使用荧光笔，摘要即是这些重点内容的组合。提取式摘要是一种更直接的方法，因为可以直接复制源文本中的句子，从而减少语法问题。摘要的质量也可以通过诸如ROUGE等指标来衡量。该指标将在本章后面详细介绍。提取式摘要在深度学习和神经网络出现之前是主要的方法。'
- en: '**Abstractive summarization**: A person may use the full vocabulary available
    in a language while summarizing an article. They are not restricted to only using
    words from the article. The mental model is that the person is penning a new piece
    of text. The model must have some understanding of the meaning of different words
    so that the model can use them in the summary. Abstractive summarization is quite
    hard to implement and evaluate. The advent of the seq2seq architecture made significant
    improvements to the quality of abstractive summarization models.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象式摘要**：在摘要一篇文章时，个人可能会使用该语言中所有可用的词汇。他们不局限于仅使用文章中的词语。其心理模型是，人们正在撰写一篇新的文本。模型必须对不同词语的意义有所理解，才能在摘要中使用这些词汇。抽象式摘要相当难以实现和评估。Seq2Seq架构的出现大大提升了抽象式摘要模型的质量。'
- en: 'This chapter focuses on abstractive summarization. Here are some examples of
    summaries that our model can generate:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论抽象式摘要。以下是我们模型生成的摘要示例：
- en: '| Source text | Generated summary |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 生成的摘要 |'
- en: '| american airlines group inc said on sunday it plans to raise ## billion by
    selling shares and convertible senior notes , to improve the airline''s liquidity
    as it grapples with travel restrictions caused by the coronavirus . | american
    airlines to raise ## **bln** convertible **bond issue** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 美国航空集团公司周日表示，它计划通过出售股票和可转换优先票据筹集##亿美元，以改善航空公司在应对因冠状病毒引起的旅行限制时的流动性。| 美国航空将通过**可转换债券发行**筹集##**亿美元**
    |'
- en: '| sales of newly-built single-family houses occurred at a seasonally adjusted
    annual rate of ## in may , that represented a #.#% increase from the downwardly
    revised pace of ## in april . | **new home** sales **rise** in may |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 新建独栋住宅的销售在5月按季节调整后的年化速度为##，与4月经过下调修正后的##相比，增长了#.#%。| **新房**销售在5月**上涨** |'
- en: '| jc penney will close another ## stores for good . the department store chain
    , which filed for bankruptcy last month , is inching toward its target of closing
    ## stores . | jc penney to close **more** stores |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| JC Penney 将永久关闭更多##家门店。这家上个月申请破产的百货商店连锁，正朝着关闭##家门店的目标迈进。| JC Penney 将关闭**更多**门店
    |'
- en: The source text was pre-processed to be all in lowercase, and numbers were replaced
    with placeholder tokens to prevent the model from inventing numbers in the summary.
    The generated summaries have some words highlighted. Those words were not present
    in the source text. The model was able to propose these words in the summary.
    Thus, the model is an abstractive summarization model. So, how can such a model
    be built?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 原文在预处理时已全部转换为小写，并且将数字替换为占位符标记，以防止模型在摘要中捏造数字。生成的摘要中有些词被高亮显示，这些词在原文中并未出现。模型能够在摘要中提出这些词。因此，模型是一个抽象式摘要模型。那么，如何构建这样一个模型呢？
- en: 'One way of looking at the summarization problem is that the model is *translating*
    an input sequence of tokens into a smaller set of output tokens. The model learns
    the output lengths based on the supervised examples provided. Another well-known
    problem is mapping an input sequence to an output sequence – the problem of Neural
    Machine Translation or NMT. In NMT, the input sequence could be a sentence from
    the source language, and the output could be a sequence of tokens in the target
    language. The process for translation is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种看待摘要生成问题的方法是，模型将输入的标记序列*转换*为较小的输出标记集合。模型根据提供的监督样本学习输出长度。另一个著名的问题是将输入序列映射到输出序列——即神经机器翻译（NMT）问题。在NMT中，输入序列可能是源语言中的一句话，输出则可能是目标语言中的一系列标记。翻译过程如下：
- en: Convert the input text into tokens
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入文本转换为标记
- en: Learn embeddings for these tokens
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这些标记学习嵌入
- en: Pass the token embeddings through an encoder to calculate the hidden states
    and outputs
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编码器传递标记嵌入，计算隐藏状态和输出
- en: Use the hidden states with the attention mechanism for generating a context
    vector for the inputs
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带有注意力机制的隐藏状态生成输入的上下文向量
- en: Pass encoder outputs, hidden states, and context vectors to the decoder part
    of the network
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出、隐藏状态和上下文向量传递给网络的解码器部分
- en: Generate the outputs from left to right using an autoregressive model
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自回归模型从左到右生成输出
- en: Google AI published a tutorial on NMT using a seq2seq attention model in July
    2017\. This model uses a left-to-right encoder with GRU cells. The Decoder also
    uses GRU cells. In summarization, the piece of text to be summarized is a prerequisite.
    This may or may not be valid for machine translation. In some cases, the translation
    is performed on the fly. In that case, a left-to-right encoder is useful. However,
    if the entire text to be translated or summarized is available from the outset,
    a bi-directional Encoder can encode context from both sides of a given token.
    BiRNN in the Encoder leads to much better performance of the overall model. The
    NMT tutorial code serves as inspiration for the seq2seq attention model and the
    attention tutorial referenced previously. Before we work on the model, let's look
    at the datasets that are used for this purpose.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Google AI在2017年7月发布了一个关于使用seq2seq注意力模型进行神经机器翻译（NMT）的教程。该模型使用带有GRU单元的从左到右的编码器。解码器也使用GRU单元。在文本摘要中，被总结的文本是前提条件。这对于机器翻译来说可能有效，也可能无效。在某些情况下，翻译是实时进行的。在这种情况下，从左到右的编码器是有用的。然而，如果待翻译或总结的完整文本从一开始就可以获取，那么双向编码器可以从给定词元的两侧编码上下文。编码器中的双向RNN（BiRNN）可以显著提升整体模型的性能。NMT教程中的代码为seq2seq注意力模型和之前提到的注意力教程提供了灵感。在我们进行模型构建之前，让我们先了解一下为此目的使用的数据集。
- en: Data loading and pre-processing
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据加载与预处理
- en: There are several summarization-related datasets available for training. These
    datasets are available through the TensorFlow Datasets or `tfds` package, which
    we have used in the previous chapters as well. The datasets that are available
    differ in length and style. The CNN/DailyMail dataset is one of the most commonly
    used datasets. It was published in 2015, with approximately a total of 1 million
    news articles. Articles from CNN, starting in 2007, and Daily Mail, starting in
    2010, were collected until 2015\. The summaries are usually multi-sentence. The
    Newsroom dataset, available from [https://summari.es](https://summari.es), contains
    over 1.3 million news articles from 38 publications. However, this dataset requires
    that you register to download it, which is why it is not used in this book. The
    wikiHow data set contains full Wikipedia article pages and the summary sentences
    for those articles. The LCSTS data set contains Chinese language data collected
    from Sina Weibo with paragraphs and their one-sentence summaries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个与摘要相关的数据集可用于训练。这些数据集可以通过TensorFlow Datasets或`tfds`包获得，我们在前面的章节中也使用过这些包。可用的数据集在长度和风格上有所不同。CNN/DailyMail数据集是最常用的数据集之一。该数据集于2015年发布，包含约100万篇新闻文章。收集的文章来自CNN（自2007年起）和Daily
    Mail（自2010年起），直到2015年为止。摘要通常是多句话的。Newsroom数据集可以通过[https://summari.es](https://summari.es)获取，包含来自38家出版物的超过130万篇新闻文章。然而，这个数据集需要注册后才能下载，因此本书中未使用此数据集。wikiHow数据集包含完整的维基百科文章页面以及这些文章的摘要句子。LCSTS数据集包含从新浪微博收集的中文数据，包含段落及其单句摘要。
- en: Another popular dataset is the Gigaword dataset. It provides the first one or
    two sentences of a news story and has the headline of the story as the summary.
    This dataset is quite large, with just under 4 million rows. This dataset was
    published in a paper titled *Annotated Gigaword* by Napoles et al. in 2011\. It
    is quite easy to import this dataset using `tfds`. Given the large size of the
    dataset and long training times for the model, the training code is stored in
    Python files, while the inference code is in an IPython notebook. This pattern
    was used in the previous chapter as well. The code for training is in the `s2s-training.py`
    file. The top part of the file contains the imports and a method called `setupGPU()`
    to initialize the GPU. The file contains a main function, which provides the control
    flow, and several functions that perform specific actions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的数据集是Gigaword数据集。它提供了新闻故事的前一到两句话，并以新闻标题作为摘要。这个数据集相当庞大，包含了近400万行数据。该数据集在2011年由Napoles等人在一篇名为*Annotated
    Gigaword*的论文中发布。使用`tfds`导入这个数据集非常容易。考虑到数据集的庞大规模以及模型的长时间训练，训练代码存储在Python文件中，而推理代码则在IPython笔记本中。上一章也使用了这种模式。训练代码在`s2s-training.py`文件中。该文件的顶部包含导入语句以及一个名为`setupGPU()`的方法，用于初始化GPU。文件中还有一个主函数，提供控制流，并包含多个执行特定操作的函数。
- en: 'The dataset needs to be loaded first. The code for loading the data is in the
    `load_data()` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集需要首先加载。加载数据的代码在`load_data()`函数中：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The corresponding section in the main function looks like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数中对应的部分如下所示：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Only the training dataset is being loaded. The validation dataset contains
    approximately 190,000 examples, while the test split contains over 1,900 examples.
    In contrast, the training set contains over 3.8 million examples. Depending on
    the internet connection, downloading the dataset may take a while:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 仅加载训练数据集。验证数据集包含约 190,000 个样本，而测试集包含超过 1,900 个样本。相比之下，训练集包含超过 380 万个样本。根据网络连接情况，下载数据集可能需要一些时间：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The warning about insecure requests can be safely ignored. The data is now ready
    to be tokenized and vectorized.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不安全请求的警告可以安全忽略。数据现在已经准备好进行分词和向量化处理。
- en: Data tokenization and vectorization
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分词和向量化
- en: 'The Gigaword dataset has been already cleaned, normalized, and tokenized using
    the StanfordNLP tokenizer. All the data is converted into lowercase and normalized
    using the StanfordNLP tokenizer, as seen in the preceding examples. The main task
    in this step is to create a vocabulary. A word-based tokenizer is the most common
    choice in summarization. However, we will use a subword tokenizer in this chapter.
    A subword tokenizer provides the benefit of limiting the size of the vocabulary
    while minimizing the number of unknown words. *Chapter 3*, *Named Entity Recognition
    (NER) with BiLSTMs, CRFs, and Viterbi Decoding*, on BERT, described different
    types of tokenizers. Consequently, models such specifically the part as BERT and
    GPT-2 use some variant of a subword tokenizer. The `tfds` package provides a way
    for us to create a subword tokenizer, initialized from a corpus of text. Since
    generating the vocabulary requires running it over all of the training data, this
    process can be slow. After initialization, the tokenizer can be persisted to disk
    for future use. The code for this process is defined in the `get_tokenizer()`
    function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Gigaword 数据集已经使用 StanfordNLP 分词器进行了清洗、规范化和分词。所有数据都已转换为小写，并使用 StanfordNLP 分词器进行了规范化，正如前面的示例所示。本步骤的主要任务是创建词汇表。基于单词的分词器是摘要生成中最常见的选择。然而，本章将使用子词分词器。子词分词器的优点是可以在最小化未知词数量的同时限制词汇表的大小。*第3章*，*使用
    BiLSTMs、CRFs 和维特比解码进行命名实体识别（NER）*，介绍了不同类型的分词器。因此，像 BERT 和 GPT-2 这样的模型使用某种变体的子词分词器。`tfds`
    包提供了一种方法，可以从文本语料库初始化并创建一个子词分词器。由于生成词汇表需要遍历所有训练数据，因此这个过程可能比较慢。初始化后，分词器可以保存到磁盘以供将来使用。这个过程的代码在
    `get_tokenizer()` 函数中定义：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This method checks to see if a subword tokenizer is saved and loads it. If no
    tokenizer exists on disk, it creates one by feeding in the articles and summaries
    combined. Note that creating a new tokenizer took over 20 minutes on my machine.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法检查是否已保存子词分词器并加载它。如果磁盘上没有分词器，则通过将文章和摘要合并输入来创建一个新的分词器。请注意，在我的机器上创建新分词器花费了超过
    20 分钟。
- en: Hence, it is a good idea to do this process only once and persist the results
    for future use. The GitHub folder for this chapter contains a saved version of
    the tokenizer to save some of your time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最好只执行一次这个过程，并将结果保存以供将来使用。本章的 GitHub 文件夹包含了已保存的分词器版本，以节省一些时间。
- en: 'Two additional tokens that denote the start and end of a sequence are added
    to the vocabulary after its creation. These tokens help the model start and end
    the inputs and outputs. The end of sequence token provides a way for the Decoder,
    which generates the summary, to signal the end of the summary. The main method
    at this point looks like so:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建词汇表后，会向其中添加两个额外的标记，表示序列的开始和结束。这些标记帮助模型开始和结束输入输出。结束标记为生成摘要的解码器提供了一种信号，表示摘要的结束。此时，主要方法如下所示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Articles and their summaries can be tokenized using the tokenizer. Articles
    can be of varying lengths and will need to be truncated at a maximum length. A
    maximum token length of 128 has been chosen as the Gigaword dataset only contains
    a few sentences from the article. Note that 128 tokens are not the same as 128
    words due to the subword tokenizer. Using a subword tokenizer minimizes the presence
    of unknown tokens during summary generation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 文章及其摘要可以使用分词器进行分词。文章的长度各异，需要在最大长度处进行截断。由于 Gigaword 数据集仅包含文章中的少数几句话，因此选择了最大令牌长度为
    128。请注意，128 个令牌并不等同于 128 个单词，因为使用的是子词分词器。使用子词分词器可以最小化生成摘要时出现未知令牌的情况。
- en: 'Once the tokenizer is ready, both the article and summary texts need to be
    tokenized. Since the summary will be fed to the Decoder one token at a time, the
    provided summary text will be shifted right by adding a `start` token, as shown
    previously. An `end` token will be appended to the summary to let the Decoder
    learn how to signal the end of the summary''s generation. The `encode()` method
    in the file `seq2seq.py` defines the vectorization step:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分词器准备好，文章和摘要文本都需要进行分词。由于摘要将一次性传递给解码器一个标记，提供的摘要文本会通过添加一个 `start` 标记向右偏移，正如之前所示。一个
    `end` 标记将被附加到摘要末尾，以便让解码器学会如何标识摘要生成的结束。文件 `seq2seq.py` 中的 `encode()` 方法定义了向量化步骤：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since this is a Python function working on the contents of the text of tensors,
    another function needs to be defined. This can be passed to the dataset to be
    applied to all the rows of the data. This function is also defined in the same
    file as the `encode` function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个处理张量文本内容的 Python 函数，因此需要定义另一个函数。这个函数可以传递给数据集，以便应用到数据的所有行。这个函数也在与 `encode`
    函数相同的文件中定义：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Going back to the main function in the `s2s-training.py` file, the dataset
    can be vectorized with the help of the preceding functions like so:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 `s2s-training.py` 文件中的主函数，数据集可以借助之前的函数进行向量化，如下所示：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that shuffling the dataset is recommended. By shuffling the dataset, it
    is easier for the model to converge and not overfit to batches. However, this
    adds to the training time. This has been commented out here as this is an optional
    step. Shuffling records in batches while training models for production use cases
    is recommended. The last step in preparing the data is batching it, as shown in
    the last step here. Now, we are ready to build the model and train it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，建议对数据集进行打乱。通过打乱数据集，模型更容易收敛，并且不容易对批次过拟合。然而，这会增加训练时间。这里将其注释掉，因为这是一个可选步骤。在为生产用例训练模型时，建议在批次中打乱记录。准备数据的最后一步是将其分批，如此处的最后一步所示。现在，我们可以开始构建模型并进行训练。
- en: Seq2seq model with attention
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Seq2seq 模型与注意力机制
- en: 'The summarization model has an Encoder part with a bidirectional RNN and a
    unidirectional decoder part. There is an attention layer that helps the Decoder
    focus on specific parts of the input while generating an output token. The overall
    architecture is shown in the following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要模型有一个包含双向 RNN 的编码器部分和一个单向解码器部分。存在一个注意力层，它帮助解码器在生成输出标记时集中关注输入的特定部分。整体架构如下图所示：
- en: '![](img/B16252_06_01.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_01.png)'
- en: 'Figure 6.1: Seq2seq and attention model'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：Seq2seq 和注意力模型
- en: 'These layers are detailed in the following subsections. All the code for these
    parts of the model are in the file `seq2seq.py`. All the layers use common hyperparameters
    specified in the main function in the `s2s-training.py` file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层将在以下小节中详细介绍。模型的所有代码都在文件 `seq2seq.py` 中。这些层使用在 `s2s-training.py` 文件主函数中指定的通用超参数：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code and architecture for this section have been inspired by the paper
    titled *Get To The Point: Summarization with Pointer-Generator Networks* by Abigail
    See, Peter Liu, and Chris Manning, published in April 2017\. The fundamental architecture
    is easy to follow and provides impressive performance for a model that can be
    trained on a desktop with a commodity GPU.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '这一部分的代码和架构灵感来自2017年4月由 Abigail See、Peter Liu 和 Chris Manning 发表的论文 *Get To
    The Point: Summarization with Pointer-Generator Networks*。基础架构易于理解，并且对于可以在普通桌面
    GPU 上训练的模型提供了令人印象深刻的性能。'
- en: Encoder model
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器模型
- en: 'The detailed architecture of the Encoder layer is shown in the following diagram.
    Tokenized and vectorized input is fed through an embedding layer. Embeddings for
    the tokens generated by the tokenizer are learned from scratch. It is possible
    to use a set of pre-trained embeddings like GloVe and use the corresponding tokenizer.
    While using a pre-trained set of embeddings can help with the accuracy of the
    model, a word-based vocabulary would have many unknown tokens, as we saw in the
    IMDb example and GloVe vectors earlier. The unknown tokens would impact the ability
    of the model to create summaries with words it hasn''t seen before. If the summarization
    model is used on daily news, there can be several unknown words, like names of
    people, places, or new products:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器层的详细架构如下面的图所示。经过分词和向量化的输入会通过嵌入层。分词器生成的令牌的嵌入是从头开始学习的。也可以使用一组预训练的嵌入，例如GloVe，并使用相应的分词器。虽然使用预训练的嵌入集可以提高模型的准确性，但基于词汇的词汇表会有很多未知令牌，正如我们在IMDb示例和之前的GloVe向量中看到的那样。这些未知令牌会影响模型生成之前未见过的词语的摘要的能力。如果将摘要模型用于日常新闻，可能会有多个未知词汇，例如人名、地名或新产品名：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_02.png)'
- en: 'Figure 6.2: Encoder architecture'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：编码器架构
- en: 'The embedding layer has a dimension of 128, as configured in the hyperparameters.
    These hyperparameters have been chosen to resemble those in the paper. We then
    create an embedding singleton that can be used by both the Encoder and the Decoder.
    The code for the class is in the `seq2seq.py` file:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的维度为128，正如在超参数中配置的那样。这些超参数的选择是为了与论文中的配置相似。接下来，我们创建一个嵌入单例，可以被编码器和解码器共享。该类的代码在`seq2seq.py`文件中：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Input sequences will be padded to a fixed length of 128\. Hence, a masking
    parameter is passed to the embedding layer so that the embedding layer ignores
    the mask tokens. Next, let''s define an `Encoder` class and instantiate the embedding
    layer in the constructor:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列将填充到固定长度128。因此，将一个掩码参数传递给嵌入层，以使嵌入层忽略掩码令牌。接下来，让我们在构造函数中定义一个`Encoder`类并实例化嵌入层：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The constructor takes a number of parameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受多个参数：
- en: '**Size of the vocabulary**: In the present case, this is 32,899 tokens.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇表大小**：在当前情况下，词汇表大小为32,899个令牌。'
- en: '**Embedding dimensions**: This is 128 dimensions. Feel free to experiment with
    a larger or smaller embedding dimension. Smaller dimensions would reduce the model''s
    size and memory required for training the model.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入维度**：这是128维。可以尝试使用更大或更小的嵌入维度。较小的维度可以减少模型的大小和训练模型所需的内存。'
- en: '**Encoder units**: The number of forward and backward units in the bidirectional
    layer. 256 units will be used for a total of 512 units.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器单元**：双向层中正向和反向单元的数量。将使用256个单元，总共512个单元。'
- en: '**Batch size**: The size of the input batches. 64 records will be in one batch.
    A larger batch would make training go faster but would need more memory on the
    GPU. So, this number can be adjusted based on the capacity of the training hardware.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理大小**：输入批次的大小。64条记录将组成一个批次。较大的批次可以加速训练，但会需要更多的GPU内存。因此，这个数字可以根据训练硬件的容量进行调整。'
- en: 'The output of the embedding layer is fed to a bidirectional RNN layer. There
    are 256 GRU units in each direction. The bidirectional layer in Keras provides
    options on how to combine the output of the forward and backward layer. In this
    case, we concatenate the outputs of the forward and backward GRU cells. Hence,
    the output will be 512-dimensional. Furthermore, the hidden states are also needed
    for the attention mechanism to work, so a parameter is passed to retrieve the
    output states. The bidirectional GRU layer is configured like so:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输出被传递给一个双向RNN层。每个方向有256个GRU单元。Keras中的双向层提供了如何组合正向和反向层输出的选项。在这种情况下，我们将正向和反向GRU单元的输出进行连接。因此，输出将是512维的。此外，注意机制需要隐藏状态，所以需要传递一个参数来获取输出状态。双向GRU层的配置如下：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A dense layer with ReLU activation is also set up. The two layers return their
    hidden layers. However, the Decoder and attention layers require one vector of
    hidden states. We pass the hidden states through the dense layer and convert the
    dimensions from 512 into 256, which is expected by the Decoder and attention modules.
    This completes the constructor for the Encoder class. Given this is a custom model
    with specific ways to compute the model, a `call()` method is defined that operates
    on a batch of inputs to produce the output and hidden states. This method takes
    in hidden states to seed the bidirectional layer:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 还设置了一个带有 ReLU 激活函数的全连接层。这两个层返回它们的隐藏层。然而，解码器和注意力层需要一个隐藏状态向量。我们将隐藏状态通过全连接层，并将维度从
    512 转换为 256，这也是解码器和注意力模块所期望的。这完成了编码器类的构造器。鉴于这是一个自定义模型，计算模型的方式很具体，因此定义了一个 `call()`
    方法，该方法操作一批输入以生成输出和隐藏状态。这个方法接收隐藏状态以初始化双向层：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, the input is passed through the embedding layer. The output is fed to
    the bidirectional layer, and the output and hidden states are retrieved. The two
    hidden states are concatenated and fed through the dense layer to create the output
    hidden state. Lastly, a utility method to return initial hidden states is defined:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，输入通过嵌入层传递。输出被送入双向层，随后获取输出和隐藏状态。这两个隐藏状态被连接并通过全连接层处理，最终生成输出隐藏状态。最后，定义一个实用方法以返回初始隐藏状态：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This completes the code for the Encoder. Before going into the Decoder, an attention
    layer needs to be defined, which will be used in the Decoder. Bahdanau's attention
    formulation will be used for this. Note that TensorFlow/Keras does not provide
    an attention layer out of the box. However, this simple attention layer code should
    be entirely reusable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了编码器的代码。在进入解码器之前，需要定义一个将在解码器中使用的注意力层。将使用 Bahdanau 的注意力公式。请注意，TensorFlow/Keras
    并未提供现成的注意力层。不过，这段简单的注意力层代码应该是完全可复用的。
- en: Bahdanau attention layer
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bahdanau 注意力层
- en: Bahdanau et al. published this form of global attention in 2015\. It has been
    widely used in Transformer models, as we saw in the previous chapters. Now, we
    are going to implement an attention layer from scratch. This part of the code
    is inspired by the NMT tutorial published by the TensorFlow team.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau 等人于2015年发布了这种形式的全局注意力机制。正如我们在前面的章节中看到的，它已被广泛应用于 Transformer 模型中。现在，我们将从零开始实现一个注意力层。这部分代码灵感来源于
    TensorFlow 团队发布的 NMT 教程。
- en: The core idea behind attention is to let the Decoder see all the inputs and
    focus on the most relevant inputs while predicting the output token. A global
    attention mechanism allows the Decoder to see all the inputs. This global version
    of the attention mechanism will be implemented. At an abstract level, the purpose
    of the attention mechanism maps a set of values to a given query. It does this
    by providing a relevance score of each of these values for a given query.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的核心思想是让解码器能够看到所有输入，并在预测输出标记时专注于最相关的输入。全局注意力机制允许解码器看到所有输入。这个全局版本的注意力机制将被实现。从抽象层面来说，注意力机制的目的是将一组值映射到给定的查询上。它通过为每个值提供一个相关性评分，来评估它们对于给定查询的重要性。
- en: 'In our case, the query is the Decoder''s hidden state, and the values are the
    Encoder outputs. We are interested in figuring out which inputs can best help
    in generating the next token from the Decoder. The first step is computing a score
    using the Encoder output and the Decoder''s previous hidden state. If this is
    the first step of decoding, then the hidden states from the Encoder are used to
    seed the Decoder. A corresponding weight matrix is multiplied by the Encoder''s
    output and Decoder''s hidden state. The output is passed through a *tanh* activation
    function and multiplied by another weight matrix to produce the final score. The
    following equation shows this formulation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，查询是解码器的隐藏状态，而值是编码器的输出。我们希望找出哪些输入能最好地帮助解码器生成下一个标记。第一步是使用编码器输出和解码器的前一个隐藏状态计算一个分数。如果这是解码的第一步，那么将使用编码器的隐藏状态来初始化解码器。一个对应的权重矩阵与编码器输出和解码器的隐藏状态相乘。输出通过
    *tanh* 激活函数并与另一个权重矩阵相乘，从而生成最终分数。以下方程展示了这一公式：
- en: '![](img/B16252_06_001.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_001.png)'
- en: 'Matrices *V*, *W*[1], and *W*[2] are trainable. Then, to understand the alignment
    between the Decoder output and the Encoder outputs, a softmax is computed:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *V*、*W*[1] 和 *W*[2] 是可训练的。接下来，为了理解解码器输出和编码器输出之间的对齐关系，计算一个 softmax：
- en: '![](img/B16252_06_002.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_002.png)'
- en: 'The last step is to produce a context vector. The context vector is produced
    by multiplying the attention weights by the Encoder outputs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是生成上下文向量。上下文向量是通过将注意力权重与编码器输出相乘得到的：
- en: '![](img/B16252_06_003.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_003.png)'
- en: These are all the computations in the attention layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是注意力层中的所有计算过程。
- en: 'The first step is setting up the constructor for the attention class:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段是为注意力类设置构造函数：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `call()` method of the `BahdanauAttention` class implements the equations
    shown previously with some additional code to manage the tensor shapes. This is
    shown here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`BahdanauAttention`类的`call()`方法实现了前面显示的方程式，并附加了一些额外代码来管理张量形状。如下所示：'
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The only thing we have left to do is implement the Decoder model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下的唯一任务就是实现解码器模型。
- en: Decoder model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器模型
- en: 'The detailed Decoder model is shown in the following diagram:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图展示了详细的解码器模型：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_03.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_03.png)'
- en: 'Figure 6.3: Detailed decoder architecture'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：详细的解码器架构
- en: Hidden states from the Encoder are used to initialize the hidden states of the
    Decoder. The start token initiates the summaries being generated. The hidden states
    of the Decoder, along with the Encoder output, are used to compute the attention
    weights and the context vector. The context vector, along with the embeddings
    of the output token, are concatenated and passed through the unidirectional GRU
    cell. The output of the GRU cell is passed through a dense layer, with a softmax
    activation function to get the output token. This process is repeated token by
    token.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的隐藏状态用于初始化解码器的隐藏状态。起始标记符启动生成摘要的过程。解码器的隐藏状态以及编码器的输出一起用于计算注意力权重和上下文向量。上下文向量与输出标记符的嵌入向量一起拼接，并通过单向GRU单元。GRU单元的输出经过一个密集层，并使用Softmax激活函数来获得输出标记符。这个过程是逐个标记符重复进行的。
- en: Note that the Decoder functions differently during training and inference. During
    training, the output token from the Decoder is used to calculate the loss but
    is not fed back into the Decoder to produce the next token. Instead, the next
    token from the ground truth is fed into the Decoder at each time step. This process
    is called **teacher forcing**. The output tokens generated by the Decoder are
    only fed back in during inference when summaries are being generated.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，解码器在训练和推理过程中有不同的工作方式。在训练时，解码器的输出标记符用于计算损失，但不会反馈回解码器以生成下一个标记符。相反，训练时每一步都会将真实标签中的下一个标记符输入到解码器中。这种过程叫做**教师强迫**。解码器生成的输出标记符仅在推理时，即生成摘要时，才会被反馈回解码器。
- en: 'A `Decoder` class is defined in the `seq2seq.py` file. The constructor for
    this class sets up the dimensions and the various layers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`Decoder`类定义在`seq2seq.py`文件中。该类的构造函数设置了维度和各个层：'
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The embedding layer in the Decoder is not shared with the Encoder. This is a
    design choice. It is common in summarization to use a shared embedding layer.
    The structure of the articles and their summaries is slightly different in the
    Gigaword dataset as news headlines are not proper sentences but fragments of sentences.
    During training, using different embedding layers gave better results than shared
    embeddings. It is possible that, on the CNN/DailyMail dataset, shared embeddings
    give better results than on the Gigaword dataset. In the case of machine translation,
    the Encoder and Decoder are seeing different languages, so having separate embedding
    layers is a best practice. You are encouraged to try out both versions on different
    datasets and build your own intuition. The preceding commented code makes it easy
    to switch back and forth between shared and separate embeddings between the Encoder
    and Decoder.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器中的嵌入层与编码器是分开的。这是一个设计选择。在摘要任务中，通常使用共享的嵌入层。Gigaword数据集中的文章及其摘要结构稍有不同，因为新闻标题并非完整的句子，而是句子的片段。在训练过程中，使用不同的嵌入层比共享嵌入层取得了更好的结果。可能在CNN/DailyMail数据集中，使用共享嵌入层会比在Gigaword数据集上取得更好的结果。在机器翻译的情况下，编码器和解码器处理的是不同的语言，因此分开的嵌入层是最佳实践。建议你在不同的数据集上尝试两种版本，建立自己的直觉。前面的注释代码使得在编码器和解码器之间轻松切换共享和分开的嵌入层。
- en: 'The next part of the Decoder is the computation that calculates the output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的下一部分是计算输出的过程：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The computation is fairly straightforward. The model looks like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 计算过程相对简单。模型如下所示：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The Encoder model contains 4.9M parameters, while the Decoder model contains
    13.5M parameters for a total of 18.4M parameters. Now, we are ready to train the
    model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器模型包含490万个参数，而解码器模型包含1350万个参数，总共有1840万个参数。现在，我们准备好训练模型了。
- en: Training the model
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'There are a number of steps to be performed in training that require a custom
    training loop. First, let''s define a method that executes one step of the training
    loop. This method is defined in the `s2s-training.py` file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，有一些步骤需要自定义的训练循环。首先，让我们定义一个执行训练循环一步的函数。这个函数定义在`s2s-training.py`文件中：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is a custom training loop that uses `GradientTape`, which tracks the different
    variables of the model and calculates the gradients. The preceding function runs
    once for each batch of inputs. Inputs are passed through the Encoder to get the
    final encoding and the last hidden state. The Decoder is initialized with the
    last Encoder hidden state, and summaries are generated one token at a time. However,
    the generated token is not fed back into the Decoder. Instead, the actual token
    is fed back. This method is known as **Teacher Forcing**. A custom loss function
    is defined in the `seq2seq.py` file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个自定义的训练循环，使用`GradientTape`来跟踪模型的不同变量并计算梯度。前面的函数每处理一个输入批次就执行一次。输入通过编码器（Encoder）进行处理，得到最终的编码和最后的隐藏状态。解码器（Decoder）使用最后一个编码器隐藏状态进行初始化，并且一次生成一个token的摘要。然而，生成的token不会反馈到解码器中，而是将实际的token反馈回去。这种方法被称为**教师强制**（Teacher
    Forcing）。自定义损失函数定义在`seq2seq.py`文件中：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The key to the loss function is to use a mask to handle summaries of varying
    lengths. The last part of the model is using an optimizer. The Adam optimizer
    is being used here, with a learning rate schedule that reduces the learning rate
    over epochs of training. The concept of learning rate annealing was covered in
    previous chapters. The code for the optimizer is inside the main function in the
    `s2s-training.py` file:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的关键在于使用掩码来处理不同长度的摘要。模型的最后部分使用了一个优化器。这里使用的是Adam优化器，并且采用了一个学习率调度器，使学习率在训练的各个epoch中逐渐减小。学习率退火的概念在之前的章节中有讲解。优化器的代码位于`s2s-training.py`文件的主函数中：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Since the model is going to be trained for a long time, it is important to
    set up checkpoints that can be used to restart training in case issues occur.
    Checkpoints also provide us with an opportunity to adjust some of the training
    parameters across runs. The next part of the main function sets up the checkpointing
    system. We looked at checkpoints in the previous chapter. We will extend what
    we''ve learned and set up an optional command-line argument that specifies if
    training needs to be restarted from a specific checkpoint:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型将训练很长时间，因此设置检查点非常重要，这样可以在出现问题时重新启动训练。检查点还为我们提供了一个调整训练参数的机会。主函数的下一部分设置了检查点系统。我们在上一章中看到了检查点的内容。我们将扩展所学内容，并设置一个可选的命令行参数，指定是否需要从特定检查点重新启动训练：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If training needs to be restarted from a checkpoint, then a command-line argument
    in the form `–-checkpoint <dir>` can be specified while invoking the training
    script. If no argument is supplied, then a new checkpoint directory will be created.
    Training with 1.5M records takes over 3 hours. Running 10 iterations will take
    over a day and a half. The Pointer-Generator model we referenced earlier in this
    chapter was trained for 33 epochs, which took over 4 days of training. However,
    it is possible to see some results after 4 epochs of training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练需要从检查点重新开始，那么在调用训练脚本时，可以指定一个命令行参数，形式为`–-checkpoint <dir>`。如果未提供参数，则会创建一个新的检查点目录。使用150万条记录进行训练需要超过3小时。运行10次迭代将需要超过一天半的时间。本章前面提到的Pointer-Generator模型训练了33个epoch，训练时间超过了4天。然而，在训练4个epoch后，已经可以看到一些结果。
- en: 'Now, the last part of the main function is to start the training process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，主函数的最后部分是开始训练过程：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The training loop prints the loss every 100 batches and saves a checkpoint
    every second epoch. Feel free to adjust these settings as needed. The following
    command can be used to start training:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环每100个批次打印一次损失，并且每经过第二个epoch保存一次检查点。根据需要，您可以随意调整这些设置。以下命令可以用来开始训练：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of this script should be something similar to:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本的输出应该类似于：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This sample run used only 2,000 samples since we edited this line:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例运行只使用了2000个样本，因为我们编辑了这一行：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If training is being restarted from a checkpoint, then the command line will
    be:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练是从检查点重新开始的，则命令行将是：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With this comment, the model is hydrated from the checkpoint directory we used
    in the training step. Training continues from that point. Once the model has finished
    training, we are ready to generate the summaries. Note that the model we'll be
    using in the next section was trained for 8 epochs with 1.5M records. Using all
    3.8M records and training for more epochs would give better results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个注释，模型从我们在训练步骤中使用的检查点目录中加载。训练从那个点继续。一旦模型完成训练，我们就可以开始生成摘要。请注意，我们将在下一部分使用的模型已经训练了8个epoch，使用了150万个记录。如果使用所有380万个记录并训练更多的epoch，结果会更好。
- en: Generating summaries
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成摘要
- en: The critical thing to note while generating summaries is that a new inference
    loop will need to be built. Recall that *teacher forcing* was used during training,
    and the output of the Decoder was not used in predicting the next token. While
    generating summaries, we would like to use the generated tokens in predicting
    the next token. Since we would like to play with various input texts and generate
    summaries, we will use the code in the `generating-summaries.ipynb` IPython notebook.
    After importing and setting everything up, the tokenizer needs to be instantiated.
    The *Setup Tokenization* section of the notebook loads the tokenizers and sets
    up the vocabulary by adding start and end token IDs. Similar to when we loaded
    the data, the data encoding method is set up to encode the input articles.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成摘要时，关键要注意的是需要构建一个新的推理循环。回想一下，在训练期间使用了*教师强迫*，并且解码器的输出并未用于预测下一个token。而在生成摘要时，我们希望使用生成的token来预测下一个token。由于我们希望尝试各种输入文本并生成摘要，因此我们将使用`generating-summaries.ipynb`
    IPython笔记本中的代码。导入并设置好所有内容后，接下来需要实例化tokenizer。笔记本中的*设置分词*部分加载tokenizer，并通过添加开始和结束token的ID来设置词汇表。类似于我们加载数据时，数据编码方法将被设置为对输入文章进行编码。
- en: 'Now, we must hydrate the model from the saved checkpoint. All of the model
    objects are created first:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须从保存的检查点中加载模型。首先创建所有模型对象：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, a checkpoint with the appropriate checkpoint directory is defined:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义一个带有适当检查点目录的检查点：
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, the last checkpoint is checked:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查最后一个检查点：
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Since checkpoints are stored after every alternate epoch, this checkpoint corresponds
    to 8 epochs of training. Checkpoints can be loaded and tested with the following
    code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于检查点是在每个间隔epoch后存储的，因此这个检查点对应的是8个epoch的训练。可以使用以下代码加载并测试检查点：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That's it! The model is now ready for inference.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！模型现在已经准备好进行推理。
- en: '**Checkpoints and variable names**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查点和变量名称**'
- en: 'It is possible that the second command may give an error if it cannot match
    the names of the variables in the checkpoint with the names in the model. This
    can happen as we did not explicitly name the layers when they were instantiated
    in the model. TensorFlow will provide a dynamically generated name for the layer
    when the model is instantiated:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第二个命令无法将检查点中的变量名与模型中的变量名匹配，可能会出现错误。这种情况可能发生，因为我们在实例化模型时没有显式命名层。TensorFlow将在实例化模型时为层动态生成名称：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Variable names in the checkpoint can be inspected with:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令检查检查点中的变量名称：
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If the model is instantiated again, these names may change, and restore from
    checkpoint may fail. There are two solutions to prevent this. A quick fix is to
    restart the notebook kernel. A better fix is to edit the code and add names to
    each layer in the Encoder and Decoder constructors before training. This ensures
    that checkpoints will always find the variables. An example of this approach is
    shown for the `fc1` layer in the Decoder:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型再次实例化，这些名称可能会发生变化，恢复检查点时可能会失败。为了避免这种情况，有两种解决方案。一种快速的解决方法是重启笔记本内核。更好的解决方法是编辑代码，在训练之前为Encoder和Decoder构造函数中的每一层添加名称。这可以确保检查点始终能找到变量。以下是为Decoder中的`fc1`层使用这种方法的示例：
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Inference can be done via the greedy search or beam search algorithms. Both
    of these methods will be demonstrated here. Before going into the code for generating
    summaries, a convenience method for plotting attention weights will be defined.
    This helps in providing some intuition on what inputs contributed to a given token
    being generated in the summary:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 推理可以通过贪心搜索或束搜索算法来完成。这两种方法将在这里演示。在生成摘要的代码之前，将定义一个便捷的方法来绘制注意力权重图。这有助于提供一些直觉，帮助理解哪些输入对生成摘要中的特定token有贡献：
- en: '[PRE38]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: A plot is configured with the input sequence as the columns and the output summary
    tokens as the rows. Feel free to play with different color scales to get a better
    idea of the strength of the association between the tokens.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 配置了一个图表，其中输入序列作为列，输出摘要词元作为行。可以尝试不同的颜色比例，以更好地理解词元之间关联的强度。
- en: We have covered much ground and possibly trained a network for hours. It is
    time to see the fruits of our labor!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走了很远的路，可能训练了数小时的网络。是时候看看我们的努力成果了！
- en: Greedy search
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贪心搜索
- en: 'Greedy search uses the highest probability token at each time step to construct
    the sequence. The predicted token is fed back into the model to generate the next
    token. This is the same model that was used in the previous chapter while generating
    characters in the char-RNN model:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 贪心搜索在每个时间步使用概率最高的词元来构造序列。预测的词元被反馈到模型中，以生成下一个词元。这与前一章中生成字符的char-RNN模型使用的模型相同：
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first part of the code encodes the inputs the same way they were encoded
    during training. These inputs are passed through the Encoder to the final encoder
    output and the last hidden state. The Decoder''s initial hidden state is set to
    the last hidden state of the Encoder. Now, the process of generating the output
    tokens begins. First, the inputs are fed to the Decoder, which generates a prediction,
    the hidden state, and the attention weights. Attention weights are added to a
    running list of attention weights per time step. This generation continues until
    whichever comes earlier; producing an end-of-sequence token, or producing 50 tokens.
    The resulting summary and attention plot are returned. A summarization method
    is defined, which calls this greedy search algorithm, plots the attention weights,
    and converts the generated tokens into proper words:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一部分对输入进行与训练时相同的编码。这些输入通过编码器传递到最终的编码器输出和最后的隐藏状态。解码器的初始隐藏状态设置为编码器的最后隐藏状态。现在，生成输出词元的过程开始了。首先，将输入传递给解码器，解码器生成预测、隐藏状态和注意力权重。注意力权重被添加到每个时间步的运行注意力权重列表中。生成过程将继续，直到以下任一情况更早发生：产生序列结束标记，或生成50个词元。最终的摘要和注意力图将被返回。定义了一种摘要方法，调用这种贪心搜索算法，绘制注意力权重，并将生成的词元转换成正确的单词：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding method has a spot where we can plug in beam search later. Let''s
    test the model:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法中有一个地方可以稍后插入束搜索。让我们来测试一下模型：
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/B16252_06_04.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_04.png)'
- en: 'Figure 6.4: Attention plot for an example summary'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：一个示例摘要的注意力图
- en: 'Let''s take a look at the generated summary:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看生成的摘要：
- en: '*bulgarian president summons french ambassador over remarks on iraq*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*保加利亚总统召见法国大使，因其对伊拉克的评论*'
- en: It is a pretty good summary! The most surprising part is that the model was
    able to identify the Bulgarian president, even though Bulgaria is not mentioned
    anywhere in the source text. It contains other words not found in the original
    text. These are highlighted in the preceding output. The model was able to change
    the tense of the word *summoned* to *summons*. The word *remarks* never appears
    in the source text. The model was able to infer this from a number of input tokens.
    The notebook contains many examples, both good and bad, of summaries generated
    by the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当不错的摘要！最令人惊讶的是，模型能够识别出保加利亚总统，尽管在源文本中没有提到保加利亚。它包含了一些在原文中找不到的词。这些词在前面的输出中已被高亮显示。模型能够将词语*召唤*的时态从*召唤*改为*召唤*。词语*评论*在源文本中从未出现过。模型能够从多个输入词元中推断出这一点。这个笔记本包含了许多由模型生成的摘要示例，有好的，也有差的。
- en: 'Here is an example of a piece of challenging text for the model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型面临的一个具有挑战性的文本示例：
- en: '**Input**: *charles kennedy , leader of britain''s third-ranked liberal democrats
    , announced saturday he was quitting with immediate effect and would not stand
    in a new leadership election . us president george w. bush on saturday called
    for extending tax cuts adopted in his first term , which he said had bolstered
    economic growth.*'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：*查尔斯·肯尼迪，英国排名第三的自由民主党领袖，周六宣布他将立即辞职，并且不会参加新的领导选举。美国总统乔治·W·布什周六呼吁延长他第一任期内通过的减税政策，他表示该政策促进了经济增长。*'
- en: '**Predicted summary**: *kennedy quits to be a step toward new term*'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测的摘要**：*肯尼迪辞职是朝着新任期迈出的第一步*'
- en: 'In this article, there are two seemingly unrelated sentences. The model is
    trying to make sense of them but messes it up. There are other examples where
    the model doesn''t do so well:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，有两个看似不相关的句子。模型试图理解它们，但弄得一团糟。还有其他一些例子，模型的表现不太好：
- en: '**Input**: *jc penney will close another ## stores for good . the department
    store chain , which filed for bankruptcy last month , is inching toward its target
    of closing ## stores.*'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：*JC Penney将永久关闭另外##家门店。这家上个月申请破产的百货商店连锁，正在朝着关闭##家门店的目标迈进。*'
- en: '**Predicted summary**: *jc penney to close another ## stores for #nd stores*'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测摘要**：*JC Penney将关闭另外##家门店，另外#nd家门店*'
- en: In this example, the model repeats itself, attending to the same positions.
    In fact, this is a common problem with summarization models. One solution to prevent
    repetition is to add coverage loss. Coverage loss keeps a running total of the
    attention weights across time steps and feeds it back to the attention mechanism,
    as a way to clue it in to previously attended positions. Furthermore, coverage
    loss terms are added to the overall loss equation to penalize repetition. Training
    the model for much longer would also help in this particular case. Note that Transformer-based
    models suffer a little less from repetition.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，模型重复了自己，关注相同的位置。实际上，这是总结模型的一个常见问题。防止重复的一个解决方案是添加覆盖损失。覆盖损失保持跨时间步的注意力权重的累计总和，并将其反馈到注意力机制中，作为提示之前关注过的位置。此外，覆盖损失项被添加到整体损失方程中，以惩罚重复。将模型训练得更久也有助于解决这个特定问题。注意，基于Transformer的模型在重复方面的表现稍微好一些。
- en: 'The second example is the model inventing something:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个例子是模型发明了一些东西：
- en: '**Input**: *the german engineering giant siemens is working on a revamped version
    of its defective tram car , of which the ### units sold so far worldwide are being
    recalled owing to a technical fault , a company spokeswoman said on tuesday.*'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：*德国工程巨头西门子正在改进其有缺陷的电车版本，至今全球售出的###台因技术故障被召回，周二公司女发言人表示。*'
- en: '**Predicted summary**: *siemens to launch reb-made cars*'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测摘要**：*西门子将推出reb-made汽车*'
- en: 'The model invents *reb-made*, which is incorrect:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 模型发明了*reb-made*，这是错误的：
- en: '![](img/B16252_06_05.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_05.jpg)'
- en: 'Figure 6.5: Model invents the word "reb-made"'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：模型发明了“reb-made”这个词
- en: Looking at the preceding attention plot, the new word is being generated by
    attending to *revamped*, *version*, *defective*, and *tram*. This made-up word
    garbles the summary generated.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 查看前面的注意力图，可以看到新词通过关注*改进*、*版本*、*缺陷*和*电车*生成。这一发明的词语使得生成的摘要变得混乱。
- en: As noted earlier, using beam search can help in further improving the accuracy
    of the translations. We will try some of these challenging examples after implementing
    the beam search algorithm.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用束搜索可以进一步提高翻译的准确性。在实现束搜索算法后，我们将尝试一些具有挑战性的例子。
- en: Beam search
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Beam search（束搜索）
- en: Beam search uses multiple paths or beams to generate tokens and tries to minimize
    the overall conditional probability. At each time step, all the options are evaluated,
    and the cumulative conditional probabilities are evaluated over all the time steps
    so far. Only the top *k* beams, where *k* is the beam width, are kept; the rest
    are pruned for the next time step. Greedy search is a special case of beam search
    with a beam width of 1\. In fact, this property serves as a test case for the
    beam search algorithm. The code for this section can be found in the *Beam Search*
    section of the IPython notebook.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 束搜索使用多个路径或束来生成标记，并尝试最小化总体条件概率。在每个时间步长，所有选项都会被评估，累计的条件概率也会在所有时间步长中进行评估。只有前*k*束，其中*k*是束宽度，会被保留，其余的会在下一时间步长中剪枝。贪心搜索是束搜索的特例，其束宽度为1。事实上，这个特性为束搜索算法提供了一个测试用例。该部分的代码可以在IPython笔记本的*束搜索*部分找到。
- en: 'A new method called `beam_search()` is defined. The first part of this method
    is similar to greedy search, where inputs are tokenized and passed through the
    Encoder. The main difference between this algorithm and the greedy search algorithm
    is the core loop, which processes one token at a time. In beam search, a token
    needs to be generated for every beam. This makes beam search slower than greedy
    search, and running time increases in proportion to beam width. At each time step,
    for each of the *k* beams, the top *k* tokens are generated, sorted, and pruned
    back to *k* items. This step is performed until each beam generates an end of
    sequence token or has generated the maximum number of tokens. If there are *m*
    tokens to be generated, then beam search would require *k * m* runs of the Decoder
    to generate the output sequence. The main loop is shown in the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个名为 `beam_search()` 的新方法。此方法的第一部分与贪婪搜索相似，输入被分词并通过编码器传递。该算法与贪婪搜索算法的主要区别在于核心循环，它一次处理一个标记。在
    Beam search 中，每个 beam 都需要生成一个标记。这使得 Beam search 比贪婪搜索更慢，并且运行时间与 beam 宽度成正比。每个时间步骤，对于每个
    *k* 个 beams，生成最优的 *k* 个标记，对其进行排序并剪枝至 *k* 项。这个步骤会持续执行，直到每个 beam 生成结束标记或生成了最大数量的标记。如果需要生成
    *m* 个标记，那么 Beam search 将需要执行 *k * m* 次解码器的运行，以生成输出序列。主循环代码如下：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'At the start, there is only one beam within the start token. A list to keep
    track of the beams generated is then defined. The list of tuples stores the attention
    plots, tokens, last hidden state, and the overall cost of the beam. Conditional
    probability requires a product of all probabilities. Given that all probabilities
    are numbers between 0 and 1, the conditional probability could become very small.
    Instead, logs of the probabilities are added together, as shown in the preceding
    highlighted code. The best beams minimize this score. Finally, a small section
    is inserted that prints all the top beams with their scores once the function
    completes its execution. This part is optional and can be removed:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，只有一个 beam 在开始标记内。接着定义了一个列表来跟踪生成的 beams。该元组列表存储注意力图、标记、最后的隐藏状态以及该 beam 的总体代价。条件概率需要所有概率的乘积。由于所有概率值都介于
    0 和 1 之间，条件概率可能变得非常小。相反，概率的对数会被相加，如前面的高亮代码所示。最佳的 beams 会最小化这个分数。最后，插入一个小部分，在函数执行完毕后打印所有顶级
    beams 及其分数。这个部分是可选的，可以删除：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'At the end, the function returns the best beam:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，函数返回最佳的 beam：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `summarize()` method is extended so that you can generate greedy and beam
    search, like so:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`summarize()` 方法被扩展，因此你可以生成贪婪搜索和 Beam 搜索，示例如下：'
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s re-run the Siemens tram car example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新运行西门子有轨电车的示例：
- en: '**Greedy search summary**: *siemens to launch reb-made cars*'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪搜索总结**：*西门子将推出重新制作的汽车*'
- en: '**Beam search summary**: *siemens working on revamped european tram car*'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Beam search 总结**：*西门子正在研发改进版的欧洲有轨电车*'
- en: 'The beam search summary contains more detail and represents the text better.
    It introduces a new word, *european*, which may or may not be accurate in the
    current context. Contrast the following attention plot with the one shown previously:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Beam search 生成的总结包含更多细节，并且更好地表示了文本。它引入了一个新词 *european*，但在当前语境中，这个词的准确性尚不确定。请将下图与之前的注意力图进行对比：
- en: '![](img/B16252_06_06.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_06.jpg)'
- en: 'Figure 6.6: Attention plot of summary generated by beam search'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：通过 Beam 搜索生成的总结的注意力图
- en: 'The summary generated by beam search covers more concepts from the source text.
    For the JC Penney example, beam search makes the output better:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Beam search 生成的总结涵盖了源文本中的更多概念。对于 JC Penney 的示例，Beam search 使输出更好：
- en: '**Greedy search summary**: *jc penney to close another ## stores stores for
    #nd stores*'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪搜索总结**：*JC Penney 将关闭另外一些门店*'
- en: '**Beam search summary**: *jc penney to close ## more stores*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Beam search 总结**：*JC Penney 将关闭更多门店*'
- en: The beam search summary is more concise and grammatically correct. These examples
    were generated with a beam width of 3\. The notebook contains several other examples
    for you to play with. You will notice that generally, beam search improves the
    results, but it reduces the length of the output. Beam search suffers from issues
    where the score of sequences is not normalized for the sequence length, and repeatedly
    attending to the same input tokens has no penalty.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Beam search 生成的总结更简洁且语法正确。这些示例是使用宽度为 3 的 beam 生成的。该笔记本包含了若干其他示例供你尝试。你会注意到，通常情况下，Beam
    search 改进了结果，但会减少输出的长度。Beam search 存在一些问题，其中序列的分数没有根据序列长度进行标准化，并且反复关注相同的输入标记没有惩罚。
- en: The most significant improvement at this point will come from training the model
    for longer and on more examples. The model that was used for these examples was
    trained for 22 epochs on 1.5M samples out of 3.8M from the Gigaword dataset. However,
    it is important to have beam search and various penalties in your back pocket
    to improve the quality of your model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最显著的改进将来自于对模型进行更长时间和更多样本的训练。用于这些示例的模型在Gigaword数据集的380万样本中训练了22个epoch，使用了150万样本。然而，重要的是在提升模型质量时，能随时使用束搜索和各种惩罚方法。
- en: There are two specific penalties that address these issues, both of which will
    be discussed in the next section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个特定的惩罚措施针对这些问题，它们将在下一节中讨论。
- en: Decoding penalties with beam search
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用束搜索的解码惩罚
- en: 'Wu et al. proposed two penalties in the seminal paper *Google''s Neural Machine
    Translation System*, published in 2016\. These penalties are:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 吴等人于2016年在开创性论文*Google的神经机器翻译系统*中提出了两种惩罚方法。这些惩罚措施是：
- en: '**Length normalization**: Aimed at encouraging longer or short summaries.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长度归一化**：旨在鼓励生成更长或更短的摘要。'
- en: '**Coverage normalization**: Aimed at penalizing generation if the output focuses
    too much on the same part of the input sequence. As per the pointer-generator
    paper, this is best added during training for the last few iterations of training.
    This will not be implemented in this section.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖度归一化**：旨在惩罚当输出过度集中于输入序列的某一部分时的生成。根据指针生成器论文，最好在训练的最后几轮中加入这一项。此部分不会在本节中实现。'
- en: 'These methods are inspired by NMT and must be adapted for the needs of summarization.
    At a high level, the score can be represented by the following formula:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法受NMT的启发，必须根据摘要生成的需求进行调整。从高层次来看，分数可以通过以下公式表示：
- en: '![](img/B16252_06_004.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_004.png)'
- en: 'For example, the beam search algorithm naturally produces shorter sequences.
    The length penalty is important for NMT as the output sequence should address
    the input text. This is different from summarization, where shorter outputs are
    preferred. Length normalization computes a factor based on a parameter and the
    current token number. The cost of the beam is divided by this factor to calculate
    a length-normalized score. The paper proposes the following empirical formula:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，束搜索算法通常会生成较短的序列。长度惩罚对于NMT（神经机器翻译）非常重要，因为输出序列应该对应输入文本。这与摘要生成不同，在摘要生成中，更短的输出更受欢迎。长度归一化根据参数和当前的标记数量计算一个因子。束的代价除以这个因子，得到一个长度归一化的分数。论文提出了以下经验公式：
- en: '![](img/B16252_06_005.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_005.png)'
- en: Smaller values of alpha produce shorter sequences, and larger values produce
    longer sequences. Values of ![](img/B16252_06_006.png) are between 0 and 1\. The
    conditional probability score is divided by the preceding quantity to give the
    normalized score for a beam. The `length_wu()` method normalizes the score using
    this parameter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的alpha值生成较短的序列，较大的值生成较长的序列。值范围为0到1之间。条件概率分数除以前述值，得到归一化后的束分数。`length_wu()`方法使用此参数进行分数归一化。
- en: 'Note that all the code for this part is in the *Beam Search with Length Normalizations*
    section of the notebook:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有与此部分相关的代码都在笔记本中的*带有长度归一化的束搜索*部分：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'It is easy to implement in the code. A new beam search method with normalizations
    is created. Most of the code is the same as in the previous implementation. The
    key change for enabling length normalization involves adding an alpha parameter
    to the method signature and updating the computation of the score so that it uses
    the aforementioned method:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中实现起来非常简单。创建了一种新的带有归一化的束搜索方法。大部分代码与之前的实现相同。启用长度归一化的关键变化是，在方法签名中加入一个alpha参数，并更新分数的计算方式，以使用上述方法：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, the score is normalized like so (around line 60 in the code):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，分数会像这样归一化（代码中的大约第60行）：
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s try the settings out on some examples. First, we will try placing a
    length normalization penalty on the Siemens example:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在一些示例上应用这些设置。首先，我们将尝试在西门子的示例中加入长度归一化惩罚：
- en: '**Greedy search**: *siemens to launch reb-made cars*'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪搜索**：*西门子将推出重新制造的汽车*'
- en: '**Beam search**: *siemens working on revamped european tram car*'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**束搜索**：*西门子正在研发改进版的欧洲电车*'
- en: '**Beam search with length penalties**: *siemens working on new version of defective
    tram car*'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有长度惩罚的束搜索**：*西门子正在研发新版有缺陷的电车*'
- en: 'A beam size of 5 and an alpha of 0.8 was used to generate the preceding example.
    Length normalization generates longer summaries, which corrects some of the challenges
    that are faced by the summaries generated purely by beam search:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 生成上述示例时，使用了5的束宽和0.8的alpha值。长度归一化会生成更长的摘要，从而解决了纯粹通过束搜索生成的摘要面临的一些挑战：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_07.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图，描述自动生成](img/B16252_06_07.jpg)'
- en: 'Figure 6.7: Beam search with length normalization produces a great summary'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：带长度归一化的束搜索生成了一个很好的摘要
- en: 'Now, let''s take a look at a more complex contemporary example, which is not
    in the training set at all:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个更复杂的现代例子，它完全不在训练集当中：
- en: '**Input**: *the uk on friday said that it would allow a quarantine-free international
    travel to some low-risk countries falling in its green zone list of an estimated
    ## nations . uk transport secretary said that the us will fall within the red
    zone*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：*英国在周五表示，将允许前往一些低风险国家的国际旅行免于隔离，这些国家在其绿色区域名单上，共计估计##个国家。英国交通部长表示，美国将被划入红色区域*'
- en: '**Greedy search**: *uk to allow free travel to low-risk countries*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪心搜索**：*英国将允许低风险国家自由旅行*'
- en: '**Beam search**: *britain to allow free travel to low-risk countries*'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**束搜索**：*英国将允许低风险国家自由旅行*'
- en: '**Beam search with length normalization**: *britain to allow quarantines free
    travel to low-risk countries*'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带长度归一化的束搜索**：*英国将允许低风险国家免隔离自由旅行*'
- en: The best summary uses beam search and length normalization. Note that beam search
    alone was removing a very important word, "quarantines", before "free travel."
    This changed the meaning of the summary. With length normalization, the summary
    contains all the right details. Note that the Gigaword dataset has very short summaries
    in general, and beam search is making them even shorter. Hence, we use larger
    values of alpha. Generally, smaller values of alpha are used for summarization
    and larger values for NMT. You can try different values of the length normalization
    parameter and beam width to build some intuition. Note that the formulation for
    the length penalty was empirical. It should also be experimented with.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳摘要使用了束搜索和长度归一化。请注意，单独使用束搜索时，去掉了一个非常重要的词——"quarantines"（隔离），使得“自由旅行”前的意思发生了变化。采用长度归一化后，摘要包含了所有正确的细节。请注意，Gigaword数据集的摘要通常非常简短，而束搜索则使它们变得更短。因此，我们使用了更大的alpha值。通常，对于摘要生成使用较小的alpha值，而对于NMT则使用较大的值。你可以尝试不同的长度归一化参数和束宽值，以建立一些直觉。值得注意的是，长度惩罚的公式是经验性的，也应进行实验。
- en: The penalty adds a new parameter that needs to be tuned in addition to beam
    size. Selecting the right parameters requires a better way of evaluating summaries
    than human inspection. This is the focus of the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚项添加了一个新的参数，除了束宽外，还需要调优。选择合适的参数需要比人工检查更好的评估方法，这也是下一节的重点。
- en: Evaluating summaries
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要评估
- en: 'When people write summaries, they use inventive language. Human-written summaries
    often use words that are not present in the vocabulary of the text being summarized.
    When models generate abstractive summaries, they may also use words that are different
    from the words used in the ground truth summaries provided. There is no real way
    to do an effective semantic comparison of the ground truth summary and the generated
    summary. In summarization problems, a human evaluation step is often involved,
    which is where a qualitative check of the generated summaries is done. This method
    is both unscalable and expensive. There are approximations that uses n-gram overlaps
    and the longest common subsequence matches after stemming and lemmatization. The
    hope is that such pre-processing helps bring ground truth and generated summaries
    closer together for evaluation. The most common metric used for evaluating summaries
    is **Recall-Oriented Understudy for Gisting Evaluation**, also referred to as
    **ROUGE**. In machine translation, metrics such as **Bilingual Evaluation Understudy**
    (**BLEU**) and **Metric for Evaluation of Translation with Explicit Ordering**
    (**METEOR**) are used. BLEU relies mainly on precision, as precision is very important
    for translation. In summarization, recall is more important. Consequently, ROUGE
    is the metric of choice for evaluating summarization models. It was proposed by
    Chin-Yew Lin in 2004 in a paper titled *Rouge: A Package for Automatic Evaluation
    of Summaries*.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '当人们写摘要时，他们会使用富有创意的语言。人工编写的摘要通常使用文本中没有出现的词汇。当模型生成抽象摘要时，也可能使用与提供的真实摘要中不同的词汇。实际上，没有一种有效的方法来进行真实摘要和生成摘要的语义比较。在摘要问题中，通常会涉及人工评估步骤，在该步骤中对生成的摘要进行定性检查。这种方法既不具有扩展性，也很昂贵。有一些近似方法使用n-gram重叠和最长公共子序列匹配，这些都在词干提取和词形还原后进行。希望这种预处理有助于将真实摘要和生成摘要在评估中拉得更近。评估摘要时最常用的度量是**召回导向的概括评估**，也称为**ROUGE**。在机器翻译中，使用的度量包括**双语评估替代方法**（**BLEU**）和**显式排序翻译评估度量**（**METEOR**）。BLEU主要依赖于精确度，因为精确度对翻译非常重要。在摘要中，召回率更为重要。因此，ROUGE是评估摘要模型的首选度量。它由Chin-Yew
    Lin于2004年在一篇名为*Rouge: A Package for Automatic Evaluation of Summaries*的论文中提出。'
- en: ROUGE metric evaluation
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROUGE度量评估
- en: 'A summary that''s generated by a model should be readable, coherent, and factually
    correct. In addition, it should be grammatically correct. Human evaluation of
    summaries can be a mammoth task. If a person took 30 seconds to evaluate one summary
    in the Gigaword dataset, then it would take over 26 hours for one person to check
    the validation set. Since abstractive summaries are being generated, this human
    evaluation work will need to be done every time summaries are produced. The ROUGE
    metric tries to measure various aspects of an abstractive summary. It is a collection
    of four metrics:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成的摘要应该是可读的、连贯的，并且事实正确。此外，它还应该语法正确。对摘要进行人工评估可能是一项艰巨的任务。如果一个人需要30秒来评估Gigaword数据集中的一个摘要，那么一个人需要超过26小时来检查验证集。由于正在生成抽象摘要，每次生成摘要时，都需要进行这种人工评估工作。ROUGE度量试图衡量抽象摘要的各个方面。它是四个度量的集合：
- en: '**ROUGE-N** is the n-gram recall between a generated summary and the ground
    truth or reference summary. "N" at the end of the name specifies the length of
    the n-gram. It is common to report ROUGE-1 and ROUGE-2\. The metric is calculated
    as the ratio of matching n-grams between the ground truth summary and the generated
    summary, divided by the total number of n-grams in the ground truth. This formulation
    is oriented toward recall. If multiple reference summaries exist, the ROUGE-N
    metric is calculated pairwise for each reference summary, and the maximum score
    is taken. In our example, only one reference summary exists.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-N**是生成的摘要与真实摘要或参考摘要之间的n-gram召回率。名称末尾的"N"指定n-gram的长度。通常报告ROUGE-1和ROUGE-2。该度量的计算方法是将生成摘要与真实摘要之间匹配的n-gram的比率，除以真实摘要中n-gram的总数。此公式侧重于召回率。如果存在多个参考摘要，则对每个参考摘要计算ROUGE-N度量，并取最大得分。在我们的示例中，只有一个参考摘要。'
- en: '**ROUGE-L** uses the **longest common subsequence** (**LCS**) between the generated
    summary and the ground truth to calculate the metric. Often, the sequences are
    stemmed prior to computing the LCS. Once the length of the LCS is known, precision
    is calculated by dividing it by the length of the reference summary; recall is
    calculated by dividing by the length of the generated score. The F1 score, which
    is the harmonic mean of precision and recall, is also calculated and reported.
    The F1 score provides a way for us to balance precision and recall. Since the
    LCS already includes common n-grams, choosing an n-gram length is not required.
    This particular version of ROUGE-L is called the sentence-level LCS score. There
    is a summary-level score as well, for cases when the summary contains more than
    one sentence. It is used for the CNN and DailyMail datasets, among others. The
    summary-level score matches each sentence in the ground truth with all the generated
    sentences to calculate the union LCS precision and recall. Details of the method
    can be found in the paper referenced previously.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-L**使用生成的摘要与参考摘要之间的**最长公共子序列**（**LCS**）来计算该指标。通常，在计算LCS之前会对序列进行词干提取。一旦LCS的长度已知，精确度通过将其除以参考摘要的长度来计算；召回率则通过将其除以生成摘要的长度来计算。还会计算并报告F1分数，它是精确度和召回率的调和平均值。F1分数为我们平衡精确度和召回率提供了一种方式。由于LCS已经包括了公共的n-gram，因此不需要选择n-gram的长度。此版本的ROUGE-L被称为句子级LCS分数。还有一个摘要级别的分数，适用于摘要包含多个句子的情况。它用于CNN和DailyMail数据集等其他数据集。摘要级别的分数通过将参考摘要中的每个句子与所有生成的句子匹配来计算联合LCS精确度和召回率。方法的详细信息可以在前述论文中找到。'
- en: '**ROUGE-W** is a weighted version of the previous metric, where contiguous
    matches in the LCS are weighted higher than if the tokens were separated by some
    other tokens in the middle.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-W**是前述指标的加权版本，其中LCS中的连续匹配被赋予比其他令牌之间有间隔的匹配更高的权重。'
- en: '**ROUGE-S** uses skip-bigram co-occurrence statistics. A skip-bigram allows
    there to be arbitrary gaps between two tokens. Precision and recall are calculated
    using this measure.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE-S**使用跳跃二元组共现统计。跳跃二元组允许两个令牌之间存在任意间隙。使用此度量计算精确度和召回率。'
- en: 'The paper that proposed these metrics also contained code, in Perl, for calculating
    these metrics. This requires generating text files with references and generating
    summaries. Google Research has published a full Python implementation that is
    available from their GitHub repository: [https://github.com/google-research/google-research](https://github.com/google-research/google-research).
    The `rouge/` directory contains the code for these metrics. Please follow the
    installation instructions from the repository. Once installed, we can evaluate
    greedy search, beam search, and beam search with length normalization to judge
    their quality using the ROUGE-L metric. The code for this part is in the ROUGE
    Evaluation section.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 提出这些指标的论文还包含了用于计算这些指标的Perl代码。这需要生成带有参考的文本文件并生成摘要。Google Research已经发布了完整的Python实现，并可从其GitHub仓库获取：[https://github.com/google-research/google-research](https://github.com/google-research/google-research)。`rouge/`目录包含这些指标的代码。请按照仓库中的安装说明进行操作。安装完成后，我们可以使用ROUGE-L指标评估贪心搜索、束搜索和带长度归一化的束搜索来判断它们的质量。此部分的代码位于ROUGE评估部分。
- en: 'The scorer library can be imported and initialized like so:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 可以像下面这样导入并初始化评分库：
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A version of the `summarize()` method, called `summarize_quietly()`, is used
    to summarize pieces of text without printing any outputs like attention plots.
    Random samples from the validation test will be used to measure the performance.
    The code for loading the data and the quiet summarization method can be found
    in the notebook and should be run prior to running metrics. Evaluation can be
    run using a greedy search, as shown in the following code fragment:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`summarize()`方法的一个版本，名为`summarize_quietly()`，用于在不打印任何输出（如注意力图）的情况下总结文本。将使用验证测试中的随机样本来衡量性能。加载数据和安静总结方法的代码可以在笔记本中找到，并应在运行指标之前执行。评估可以通过贪心搜索来进行，如以下代码片段所示：'
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'While the validation set contains close to 190,000 records, the preceding code
    runs metrics on 1,000 records. The code also randomly prints out summaries for
    about 1% of the samples. The results of this evaluation should look similar to
    these:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集包含接近190,000条记录，而前面的代码仅在1,000条记录上运行指标。该代码还会随机打印约1%样本的摘要。此评估的结果应该类似于以下内容：
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This is not a bad start since we have high precision, but the recall is low.
    The current leaderboard for the Gigaword dataset has 36.74 as the highest ROUGE-L
    F1 score, as per paperswithcode.com. Let''s run the same test with beam search
    and see the results. The code here is identical to the preceding code, with the
    only difference being that a beam width of 3 is being used:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个坏的开始，因为我们有高精度，但召回率较低。根据 paperswithcode.com 网站，Gigaword 数据集当前的排行榜上最高的 ROUGE-L
    F1 分数为 36.74。让我们用束搜索（beam search）重新运行相同的测试，看看结果。这里的代码与前面的代码相同，唯一的区别是使用了宽度为 3 的束搜索：
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'It seems that the precision has improved considerably at the expense of recall.
    Overall, the F1 score shows a slight decrease. Beam search does produce shorter
    summaries, which could be the reason for the decrease in recall. Adjusting length
    normalization could help with this. Another hypothesis could be to try bigger
    beams. Trying a bigger beam size of 5 produces this result:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来精度大幅提高，但以牺牲召回率为代价。总体而言，F1 分数略有下降。束搜索确实生成了较短的摘要，这可能是召回率下降的原因。调整长度归一化可能有助于解决这个问题。另一种假设是尝试更大的束宽度。尝试使用更大的束宽度
    5 产生了这个结果：
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'There is a significant improvement in precision and a further decrease in recall.
    Now, let''s try some length normalization. Running beam search with an alpha of
    0.7 gives us the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 精度有了显著提高，但召回率进一步下降。现在，让我们尝试一些长度归一化。使用 alpha 为 0.7 的束搜索给我们带来了以下结果：
- en: '[PRE55]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'By running a larger beam width of 5 with the same alpha, we obtain this result:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用更大的束宽度 5 并保持相同的 alpha，我们得到了这个结果：
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: There is a considerable increase in recall due to there being a decline in precision.
    Overall, for a basic model trained only on a slice of data, the performance is
    quite good. A score of 27.3 would yield a spot on the top 20 of the leaderboard.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率有了显著提升，但精度却有所下降。总体来看，对于仅在一部分数据上训练的基础模型，性能相当不错。27.3 的得分将使其跻身排行榜前 20 名。
- en: Seq2seq-based text summarization was the main approach prior to the advent of
    Transformer-based models. Now, Transformer-based models, which include both the Encoder
    and Decoder parts, are used for summarization. The next section reviews state-of-the-art
    approaches to summarization.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Seq2seq 的文本摘要方法是 Transformer 模型出现之前的主要方法。现在，基于 Transformer 的模型（包括 Encoder
    和 Decoder 部分）被用于摘要生成。下一部分将回顾摘要生成的前沿技术。
- en: Summarization – state of the art
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要生成 — 目前的前沿技术
- en: 'Today, the predominant approach to summarization uses the full Transformer
    architecture. Such models are quite big, often ranging from 223M parameters to
    over a billion in the case of GPT-3\. Google Research published a paper at ICML
    in June 2020 titled *PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive
    Summarization*. This paper sets the benchmark for state-of-the-art results as
    of the time of writing. The key innovation proposed by this model is a specific
    pre-training objective for summarization. Recall that BERT was pre-trained using
    a **masked language model** (**MLM**) objective, where tokens were randomly masked
    and the model had to predict them. The PEGASUS model proposed a **Gap Sentence
    Generation** (**GSG**) pre-training objective, where important sentences are completely
    replaced with a special masking token, and the model has to generate the sequence.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '今天，主流的摘要生成方法使用的是完整的 Transformer 架构。这些模型相当庞大，参数数量从 2.23 亿到 GPT-3 的超过十亿不等。谷歌研究团队在
    2020 年 6 月的 ICML 会议上发布了一篇名为 *PEGASUS: Pre-training with Extracted Gap-sentences
    for Abstractive Summarization* 的论文。这篇论文为撰写时的最新技术成果设定了基准。这一模型提出的关键创新是专门针对摘要生成的预训练目标。回顾一下，BERT
    是使用 **掩码语言模型**（**MLM**）目标进行预训练的，其中某些 token 会被随机掩码，模型需要预测这些 token。PEGASUS 模型则提出了
    **Gap Sentence Generation**（**GSG**）预训练目标，其中重要的句子会完全被特殊的掩码 token 替代，模型需要生成该序列。'
- en: The importance of the sentence is judged using the ROUGE1-F1 score of a given
    score compared to the entire document. A certain number of top-scoring sentences
    are masked from the input, and the model needs to predict them. Additional details
    can be found in the aforementioned paper. The base Transformer model is very similar
    to the BERT configurations. The pre-training objective makes a significant difference
    to the ROUGE1/2/L-F1 scores and sets new records on many of the datasets.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 句子的重点通过与整个文档进行 ROUGE1-F1 分数的比较来判断。某些得分较高的句子会从输入中屏蔽，模型需要预测它们。更多的细节可以在前述论文中找到。基础的
    Transformer 模型与 BERT 配置非常相似。预训练目标对 ROUGE1/2/L-F1 分数有显著影响，并在许多数据集上创下新纪录。
- en: These models are quite large and training them on a desktop is not realistic.
    Often, the models are pre-trained on humongous datasets for several days at a
    time. Thankfully, pre-trained versions of such models are available through libraries
    like HuggingFace.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型相当庞大，无法在桌面电脑上训练。通常，这些模型会在巨大的数据集上预训练几天。幸运的是，通过像HuggingFace这样的库，预训练版本的模型可以使用。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Summarizing text is considered a uniquely human trait. Deep learning NLP models
    have made great strides in this area in the past 2-3 years. Summarization remains
    a very hot area of research within many applications. In this chapter, we built
    a seq2seq model from scratch that can summarize sentences from news articles and
    generate a headline. This model obtains fairly good results due to its simplicity.
    We were able to train the model for a long period of time due to learning rate
    annealing. By checkpointing the model, training was made resilient as it could
    be restarted from the last checkpoint in case of failure. Post-training, we improved
    our generated summaries through a custom implementation of beam search. As beam
    search has a tendency to provide short summaries, length normalization techniques
    were used to make the summaries even better.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要被认为是人类独有的特征。深度学习NLP模型在过去的2-3年里在这一领域取得了巨大进展。摘要仍然是许多应用中的热门研究领域。在本章中，我们从零开始构建了一个seq2seq模型，它可以总结新闻文章中的句子并生成标题。由于其简洁性，该模型取得了相当不错的结果。我们通过学习率衰减，能够长时间训练该模型。通过模型检查点，训练变得更加稳健，因为在出现故障时，可以从上次检查点重新开始训练。训练后，我们通过自定义实现的束搜索提高了生成的摘要质量。由于束搜索倾向于生成简短的摘要，我们使用了长度归一化技术，使得摘要更加完善。
- en: 'Measuring the quality of generated summaries is a challenge in abstractive
    summarization. Here is a random example from the validation dataset:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量生成摘要的质量是抽象摘要中的一大挑战。这里是来自验证数据集的一个随机示例：
- en: '**Input**: *the french soccer star david ginola on saturday launched his anti-land
    mines campaign on behalf of the international committee for the red cross which
    has taken him on as a sort of poster boy for the cause .*'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**: *法国足球明星大卫·吉诺拉于周六代表国际红十字会发起了反地雷运动，该组织将他作为该运动的“代言人”。*'
- en: '**Ground truth**: *soccer star joins red cross effort against land mines*'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实情况**: *足球明星加入红十字会反地雷运动*'
- en: '**Beam search (5/0.7)**: *former french star ginola launches anti-land mine
    campaign*'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**束搜索 (5/0.7)**: *前法国足球明星吉诺拉发起反地雷运动*'
- en: The generated summary is very comparable to the ground truth. However, matching
    token by token would give us a very low score. ROUGE metrics that use n-grams
    and the LCS allow us to measure the quality of the summaries.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的摘要与真实情况非常相似。然而，逐字匹配会给我们一个非常低的分数。使用n-gram和LCS的ROUGE指标可以帮助我们衡量摘要的质量。
- en: Finally, we took a quick look at the current state-of-the-art models for summarization.
    Large models that are pre-trained on even larger datasets are ruling the roost.
    Unfortunately, training a model of such size is often beyond the resources of
    a single individual.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们快速浏览了当前最先进的摘要模型。那些在更大数据集上进行预训练的大型模型已经主导了这一领域。不幸的是，训练如此规模的模型通常超出了单个个人的资源范围。
- en: Now, we will move on to a very new and exciting area of research – multi-modal
    networks. Thus far, we have only treated text in isolation. But is a picture really
    worth a thousand words? We shall find out when we try to caption images and answer
    questions about them in the next chapter.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将转向一个全新且令人兴奋的研究领域——多模态网络。到目前为止，我们只单独处理了文本。但一张图片真的是千言万语吗？我们将在下一章尝试给图片添加标题并回答相关问题时，找到答案。
