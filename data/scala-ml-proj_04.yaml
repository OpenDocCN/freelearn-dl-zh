- en: Population-Scale Clustering and Ethnicity Prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人口规模的聚类与民族预测
- en: Understanding variations in genome sequences assists us in identifying people
    who are predisposed to common diseases, curing rare diseases, and finding the
    corresponding population group of individuals from a larger population group.
    Although classical machine learning techniques allow researchers to identify groups
    (that is, clusters) of related variables, the accuracy and effectiveness of these
    methods diminish for large and high-dimensional datasets such as the whole human
    genome.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 理解基因组序列的变异有助于我们识别易患常见疾病的人群、治愈罕见疾病，并从更大的群体中找到对应的目标群体。尽管经典的机器学习技术可以帮助研究人员识别相关变量的群体（即簇），但这些方法在处理如整个基因组这类大规模和高维度数据集时，准确性和有效性会下降。
- en: On the other hand, **Deep Neural Networks** (**DNNs**) form the core of **deep
    learning** (**DL**) and provide algorithms to model complex, high-level abstractions
    in data. They can better exploit large-scale datasets to build complex models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**深度神经网络** (**DNNs**) 形成了**深度学习** (**DL**)的核心，并提供了建模复杂、高层次数据抽象的算法。它们能够更好地利用大规模数据集来构建复杂的模型。
- en: In this chapter, we apply the K-means algorithm to large-scale genomic data
    from the 1000 Genomes project analysis aimed at clustering genotypic variants
    at the population scale. Finally, we train an H2O-based DNN model and a Spark-based
    random forest model for predicting geographic ethnicity. The theme of this chapter
    is *give me your genetic variants data and I will tell your ethnicity*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将应用K-means算法对来自1000基因组计划分析的大规模基因组数据进行聚类，旨在在人群规模上聚类基因型变异。最后，我们训练一个基于H2O的DNN模型和一个基于Spark的随机森林模型，用于预测地理民族。本章的主题是*给我你的基因变异数据，我会告诉你你的民族*。
- en: 'Nevertheless, we will configure H2O so that the same setting can be used in
    upcoming chapters too. Concisely, we will learn the following topics throughout
    this end-to-end project:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们将配置H2O，以便在接下来的章节中也可以使用相同的设置。简而言之，我们将在这个端到端项目中学习以下内容：
- en: Population-scale clustering and geographic ethnicity prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口规模的聚类与地理民族预测
- en: The 1000 Genomes project, a deep catalog of human genetic variants
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1000基因组计划——一个深入的人的基因变异目录
- en: Algorithms and tools
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法与工具
- en: Using K-means for population-scale clustering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-means进行人口规模的聚类
- en: Using H2O for ethnicity prediction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用H2O进行民族预测
- en: Using random forest for ethnicity prediction
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林进行民族预测
- en: Population scale clustering and geographic ethnicity
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人口规模聚类与地理民族
- en: '**Next-generation genome sequencing** (**NGS**) reduces overhead and time for
    genomic sequencing, leading to big data production in an unprecedented way. In
    contrast, analyzing this large-scale data is computationally expensive and increasingly
    becomes the key bottleneck. This increase in NGS data in terms of number of samples
    overall and features per sample demands solutions for massively parallel data
    processing, which imposes extraordinary challenges on machine learning solutions
    and bioinformatics approaches. The use of genomic information in medical practice
    requires efficient analytical methodologies to cope with data from thousands of
    individuals and millions of their variants.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**下一代基因组测序** (**NGS**)减少了基因组测序的开销和时间，以前所未有的方式产生了大数据。相比之下，分析这些大规模数据在计算上非常昂贵，且逐渐成为关键瓶颈。随着NGS数据中样本数量和每个样本特征的增加，这对大规模并行数据处理提出了需求，从而对机器学习解决方案和生物信息学方法带来了前所未有的挑战。在医疗实践中使用基因组信息需要高效的分析方法来应对来自数千人及其数百万变异的数据。'
- en: One of the most important tasks is the analysis of genomic profiles to attribute
    individuals to specific ethnic populations, or the analysis of nucleotide haplotypes
    for disease susceptibility. The data from the 1000 Genomes project serves as the
    prime source to analyze genome-wide **single nucleotide polymorphisms** (**SNPs**)
    at scale for the prediction of the individual's ancestry with regards to continental
    and regional origins.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一项最重要的任务是分析基因组特征，以便将个体归类为特定的民族群体，或分析核苷酸单倍型与疾病易感性的关系。来自1000基因组计划的数据作为分析全基因组**单核苷酸多态性**
    (**SNPs**)的主要来源，旨在预测个体的祖先背景，涉及大陆和区域的起源。
- en: Machine learning for genetic variants
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因变异的机器学习
- en: Research has revealed that population groups from Asia, Europe, Africa, and
    America can be separated based on their genomic data. However, it is more challenging
    to accurately predict the haplogroup and the continent of origin, that is, geography,
    ethnicity, and language. Other research shows that the Y chromosome lineage can
    be geographically localized, forming the evidence for (geographically) clustering
    the human alleles of the human genotypes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，来自亚洲、欧洲、非洲和美洲的群体可以根据其基因组数据进行区分。然而，准确预测单倍群和起源大陆（即，地理、民族和语言）的难度更大。其他研究表明，Y
    染色体谱系可以在地理上定位，为（地理）聚类人类基因型中的人类等位基因提供证据。
- en: Thus, the clustering of individuals is correlated with geographic origin and
    ancestry. Since race depends on ancestry as well, the clusters are also correlated
    with the more traditional concepts of race, but the correlation is not perfect
    since genetic variation occurs according to probabilistic principles. Therefore,
    it does not follow a continuous distribution in different races and rather overlaps
    across or spills into different populations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，个体的聚类与地理来源和祖先有关系。由于种族也依赖于祖先，聚类与更传统的种族概念之间也存在关联，但这种关联并不完全准确，因为基因变异是按照概率原则发生的。因此，它并不遵循在不同种族间的连续分布，而是呈现出交叉或重叠的现象。
- en: As a result, the identification of ancestry, or even race, may prove to be useful
    for biomedical reasons, but any direct assessment of disease-related genetic variation
    will ultimately yield more accurate and beneficial information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确定祖先，甚至是种族，可能对生物医学有一定的用处，但任何直接评估与疾病相关的基因变异，最终将提供更为准确和有益的信息。
- en: The datasets provided by various genomics projects, such as **The Cancer Genome
    Atlas** (**TCGA**), **International Cancer Genome Consortium (ICGC)**, **1000
    Genomes Projects**, and **Personal Genome Project** (**PGP**), dispose of large-scale
    data. For fast processing of such data, ADAM and Spark-based solutions have been
    proposed and are now widely used in genomics data analytics research.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 各种基因组学项目提供的数据集，如**癌症基因组图谱**（**TCGA**）、**国际癌症基因组联盟**（**ICGC**）、**1000基因组计划**以及**个人基因组计划**（**PGP**），都包含了大规模数据。为了快速处理这些数据，已提出基于
    ADAM 和 Spark 的解决方案，并且这些解决方案现在广泛应用于基因组数据分析研究。
- en: 'Spark forms the most efficient data-processing framework and, in addition,
    provides primitives for in-memory cluster computing, for example, for querying
    the user data repeatedly. This makes Spark an excellent candidate for machine
    learning algorithms that outperform the Hadoop-based MapReduce framework. By using
    the genetic variants dataset from the 1000 Genomes project, we will try to answer
    the following questions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 形成了最有效的数据处理框架，并且提供了内存集群计算的基本构件，例如用于反复查询用户数据的功能。这使得 Spark 成为机器学习算法的优秀候选框架，其性能超过了基于
    Hadoop 的 MapReduce 框架。通过使用来自 1000 基因组计划的基因变异数据集，我们将尝试回答以下问题：
- en: How is human genetic variation distributed geographically among different population
    groups?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类的基因变异在不同群体之间的地理分布是怎样的？
- en: Can we use the genomic profile of individuals to attribute them to specific
    populations or derive disease susceptibility from their nucleotide haplotype?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否利用个体的基因组信息，将其归类为特定的群体，或从其核苷酸单倍型中推导出疾病易感性？
- en: Is the individual's genomic data suitable to predict geographic origin (that
    is, the population group for an individual)?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个体的基因组数据是否适合预测其地理来源（即，个体所属的群体）？
- en: In this project, we addressed the preceding questions in a scalable and more
    efficient way. Particularly, we examined how we applied Spark and ADAM for large-scale
    data processing, H2O for K-means clustering of the whole population to determine
    inter- and intra-population groups, and MLP-based supervised learning by tuning
    more hyperparameters to more accurately predict the population group for an individual
    according to the individual's genomic data. Do not worry at this point; we will
    provide the technical details on working with these technologies in a later section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们以一种可扩展且更高效的方式解决了前述问题。特别地，我们研究了如何应用 Spark 和 ADAM 进行大规模数据处理，使用 H2O 对整个群体进行
    K-means 聚类以确定群体内外的组别，以及通过调节更多超参数来进行基于 MLP 的监督学习，以更准确地根据个体的基因组数据预测该个体所属的群体。现在不必担心；我们将在后续部分提供关于这些技术的工作细节。
- en: However, before getting started, let's take a brief journey to the 1000 Genomes
    Project dataset to provide you with some justification on why interoperating these
    technologies is really important.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在开始之前，让我们简要了解一下1000基因组项目的数据集，以便为您提供一些关于为什么跨技术互操作如此重要的理由。
- en: 1000 Genomes Projects dataset description
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1000基因组项目数据集描述
- en: The data from the 1000 Genomes project is a very large catalog of human genetic
    variants. The project aims to determine genetic variants with frequencies higher
    than 1% in the populations studied. The data has been made openly available and
    freely accessible through public data repositories to scientists worldwide. Also,
    the data from the 1000 Genomes project is widely used to screen variants discovered
    in exome data from individuals with genetic disorders and in cancer genome projects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1000基因组项目的数据是一个非常庞大的人类遗传变异目录。该项目旨在确定在研究的人群中频率超过1%的遗传变异。数据已经公开，并通过公共数据仓库向全球科学家自由访问。此外，1000基因组项目的数据广泛用于筛选在遗传性疾病个体的外显子数据中发现的变异，以及在癌症基因组项目中的应用。
- en: 'The genotype dataset in **Variant Call Format** (**VCF**) provides the data
    of human individuals (that is, samples) and their genetic variants, and in addition,
    the global allele frequencies as well as the ones for the super populations. The
    data denotes the population''s region for each sample which is used for the predicted
    category in our approach. Specific chromosomal data (in VCF format) may have additional
    information denoting the super-population of the sample or the sequencing platform
    used. For multiallelic variants, each alternative **allele frequency** (**AF**)
    is presented in a comma-separated list, shown as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**变异调用格式**（**VCF**）中的基因型数据集提供了人类个体（即样本）及其遗传变异的数据，此外，还包括全球的等位基因频率，以及各超人群的等位基因频率。数据指明了每个样本所属的人群地区，这些信息用于我们方法中的预测类别。特定的染色体数据（以VCF格式呈现）可能包含其他信息，表明样本的超人群或所使用的测序平台。对于多等位基因变异，每个替代**等位基因频率**（**AF**）以逗号分隔的列表形式呈现，如下所示：'
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The AF is calculated as the quotient of **Allele Count** (**AC**) and **Allele
    Number** (**AN**) and NS is the total number of samples with data, whereas `_AF`
    denotes the AF for a specific region.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**等位基因频率**（**AF**）是通过**等位基因计数**（**AC**）与**等位基因总数**（**AN**）的商计算得出的，而NS则是具有数据的样本总数，而`_AF`表示特定区域的AF值。'
- en: The 1000 Genomes Project started in 2008; the consortium consisted of more than
    400 life scientists and phase 3 finished in September 2014 covering `2,504` individuals
    from 26 populations (that is, ethnic backgrounds) in total. In total, over 88
    million variants (84.7 million **single nucleotide polymorphisms** (**SNPs**),
    3.6 million short insertions/deletions (indels), and 60,000 structural variants)
    have been identified as high-quality haplotypes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 1000基因组计划始于2008年；该联盟由400多名生命科学家组成，第三阶段于2014年9月完成，共涵盖了来自26个人群（即种族背景）的`2,504`个个体。总共识别出了超过8800万个变异（8470万个**单核苷酸多态性**（**SNPs**）、360万个短插入/缺失（indels）和6万个结构变异），这些变异被确定为高质量的单倍型。
- en: In short, 99.9% of the variants consist of SNPs and short indels. Less important
    variants—including SNPs, indels, deletions, complex short substitutions, and other
    structural variant classes—have been removed for quality control. As a result,
    the third phase release leaves 84.4 million variants.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，99.9%的变异是由SNPs和短插入/缺失（indels）组成的。较不重要的变异——包括SNPs、indels、缺失、复杂的短替代以及其他结构变异类别——已被去除以进行质量控制。因此，第三阶段发布的数据保留了8440万个变异。
- en: 'Each of the 26 populations has about 60-100 individuals from Europe, Africa,
    America (South and North), and Asia (South and East). The population samples are
    grouped into super-population groups according to their predominant ancestry:
    East Asian (**CHB**, **JPT**, **CHS**, **CDX**, and **KHV**), European (**CEU**,
    **TSI**, **FIN**, **GBR**, and **IBS**), African (**YRI**, **LWK**, **GWD**, **MSL**,
    **ESN**, **ASW**, and **ACB**), American (**MXL**, **PUR**, **CLM**, and **PEL**),
    and South Asian (**GIH**, **PJL**, **BEB**, **STU**, and **ITU**). For details,
    refer to *Figure 1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这26个人群中的每一个大约有60到100个来自欧洲、非洲、美洲（南美和北美）以及亚洲（南亚和东亚）的个体。这些人群样本根据其主要祖先的来源被分为超人群组：东亚人群（**CHB**，**JPT**，**CHS**，**CDX**，和**KHV**）、欧洲人群（**CEU**，**TSI**，**FIN**，**GBR**，和**IBS**）、非洲人群（**YRI**，**LWK**，**GWD**，**MSL**，**ESN**，**ASW**，和**ACB**）、美洲人群（**MXL**，**PUR**，**CLM**，和**PEL**）以及南亚人群（**GIH**，**PJL**，**BEB**，**STU**，和**ITU**）。具体内容请参考*图1*：
- en: '![](img/6c184827-d03e-4b8d-a33b-7a1084dbf66b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c184827-d03e-4b8d-a33b-7a1084dbf66b.png)'
- en: 'Figure 1: Geographic ethnic groups from 1000 Genomes project''s release 3 (source
    http://www.internationalgenome.org/)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：来自 1000 基因组项目发布版 3 的地理种族群体（来源 [http://www.internationalgenome.org/](http://www.internationalgenome.org/)）
- en: The released datasets provide the data for 2,504 healthy adults (18 years and
    older, third project phase); only reads with at least 70 **base pairs** (**bp**)
    have been used until more advanced solutions are available. All genomic data from
    all samples were combined to attribute all variants to a region. However, note
    that specific haplotypes may not occur in the genomes of a particular region;
    that is, the multi-sample approach allows attributing variants to an individual's
    genotype even if the variants are not covered by sequencing reads from that sample.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 发布的数据集提供了来自 2,504 名健康成年人的数据（年龄 18 岁及以上，第三阶段项目）；只有至少 70 **碱基对** (**bp**) 的读取被使用，直到有更先进的解决方案可用为止。所有来自所有样本的基因组数据被整合，以便将所有变异归因于某个区域。然而，值得注意的是，特定的单倍型可能不会出现在某个特定区域的基因组中；也就是说，多样本方法允许将变异归因于个体的基因型，即使这些变异未被该样本的测序读取覆盖。
- en: 'In other words, overlapping reads are provided and the single sample genomes
    have not necessarily been consolidated. All individuals were sequenced using both
    of these:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，提供的是重叠的读取数据，并且单一样本基因组未必已被整合。所有个体均使用以下两种技术进行测序：
- en: Whole-genome sequencings (*mean depth = 7.4x*, where *x* is the number of reads,
    on average, that are likely to be aligned at a given reference *bp*)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全基因组测序 (*平均深度 = 7.4x*，其中 *x* 表示在给定参考 *bp* 上，平均可能对齐的读取数量)
- en: Targeted exome sequencing (*mean depth = 65.7x*)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 靶向外显子组测序 (*平均深度 = 65.7x*)
- en: 'In addition, individuals and their first-degree relatives such as an adult
    offspring were genotyped using high-density SNP microarrays. Each genotype comprises
    all 23 chromosomes and a separate panel file denotes the sample and population
    information. *Table 1* gives an overview of the different releases of the 1000
    Genomes project:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，个体及其直系亲属（如成人后代）通过高密度 SNP 微阵列进行基因分型。每个基因型包含所有 23 对染色体，并且一个单独的面板文件记录了样本和种群信息。*表
    1* 给出了 1000 基因组项目不同发布版的概览：
- en: '**Table 1 – Statistics of the 1000 Genomes project''s genotype dataset** **(source:**
    [http://www.internationalgenome.org/data](http://www.internationalgenome.org/data)**)**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1 – 1000 基因组项目基因型数据集统计** **（来源：** [http://www.internationalgenome.org/data](http://www.internationalgenome.org/data)
    **）**'
- en: '| **1000 genome release** | **Variants** | **Individual** | **Populations**
    | **File format** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **1000 基因组发布版** | **变异** | **个体** | **种群** | **文件格式** |'
- en: '| Phase 3 | Phase 3 | 2,504 | 26 | VCF |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 第 3 阶段 | 第 3 阶段 | 2,504 | 26 | VCF |'
- en: '| Phase 1 | 37.9 million | 1,092 | 14 | VCF |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 第 1 阶段 | 3,790 万 | 1,092 | 14 | VCF |'
- en: '| Pilot | 14.8 million | 179 | 4 | VCF |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 试点 | 1,480 万 | 179 | 4 | VCF |'
- en: The AF in the five super-population groups, **EAS=East Asian**, **EUR=European**,
    **AFR=African**, **AMR=American**, **SAS=South Asian** populations are calculated
    from allele numbers (AN, range= [0, 1]).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 计算五个超级种群组中的等位基因频率（AF）：**EAS=东亚**，**EUR=欧洲**，**AFR=非洲**，**AMR=美洲**，**SAS=南亚**，这些频率来自等位基因数（AN，范围
    = [0, 1]）。
- en: See the details of the panel file at [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅面板文件的详细信息：[ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel)。
- en: Algorithms, tools, and techniques
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法、工具和技术
- en: Large-scale data from release 3 of the 1000 Genomes project contributes to 820
    GB of data. Therefore, ADAM and Spark are used to pre-process and prepare the
    data (that is, training, testing, and validation sets) for the MLP and K-means
    models in a scalable way. Sparkling water transforms the data between H2O and
    Spark.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 1000 基因组项目发布版 3 的大规模数据贡献了 820 GB 的数据。因此，使用 ADAM 和 Spark 来以可扩展的方式预处理和准备数据（即训练、测试和验证集），以供
    MLP 和 K-means 模型使用。Sparkling water 用于在 H2O 和 Spark 之间转换数据。
- en: Then, K-means clustering, the MLP (using H2O) are trained. For the clustering
    and classification analysis, the genotypic information from each sample is required
    using the sample ID, variation ID, and the count of the alternate alleles where
    the majority of variants that we used were SNPs and indels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 K-means 聚类和多层感知机（MLP，使用 H2O）进行训练。对于聚类和分类分析，需要使用样本 ID、变异 ID 以及替代等位基因的计数，这些基因型信息来自每个样本，我们所使用的大多数变异为
    SNP 和插入缺失（indels）。
- en: Now, we should know the minimum info about each tool used such as ADAM, H2O,
    and some background information on the algorithms such as K-means, MLP for clustering,
    and classifying the population groups.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该了解每个工具的最基本信息，如 ADAM、H2O 以及关于算法的一些背景信息，如 K-means、MLP 用于聚类和分类人口群体。
- en: H2O and Sparkling water
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O 和 Sparkling water
- en: 'H2O is an AI platform for machine learning. It offers a rich set of machine
    learning algorithms and a web-based data processing UI that comes as both open
    sources as well as commercial. Using H2O, it''s possible to develop machine learning
    and DL applications with a wide range of languages, such as Java, Scala, Python,
    and R:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 是一个机器学习的人工智能平台。它提供了丰富的机器学习算法和一个基于网页的数据处理用户界面，既有开源版本，也有商业版本。使用 H2O，可以通过多种语言（如
    Java、Scala、Python 和 R）开发机器学习和深度学习应用：
- en: '![](img/96b2598b-a253-48cd-8213-de1386bb9314.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96b2598b-a253-48cd-8213-de1386bb9314.png)'
- en: 'Figure 2: The H2O compute engine and available features (source: https://h20.ai/)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：H2O 计算引擎及其可用功能（来源：https://h20.ai/）
- en: It also has the ability to interface with Spark, HDFS, SQL, and NoSQL databases.
    In short, H2O works with R, Python, and Scala on Hadoop/Yarn, Spark, or laptop.
    On the other hand, Sparkling water combines the fast, scalable ML algorithms of
    H2O with the capabilities of Spark. It drives the computation from Scala/R/Python
    and utilizes the H2O flow UI. In short, Sparkling *water = H2O + Spark*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它还具有与 Spark、HDFS、SQL 和 NoSQL 数据库接口的能力。简而言之，H2O 可在 Hadoop/Yarn、Spark 或笔记本电脑上与
    R、Python 和 Scala 配合使用。另一方面，Sparkling water 将 H2O 的快速、可扩展机器学习算法与 Spark 的功能结合起来。它通过
    Scala/R/Python 驱动计算，并利用 H2O flow 用户界面。简而言之，Sparkling *water = H2O + Spark*。
- en: 'Throughout the next few chapters, we will explore and the wide rich features
    of H2O and Sparkling water; however, I believe it would be useful to provide a
    diagram of all of the functional areas that it covers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将探索 H2O 和 Sparkling water 的广泛丰富功能；然而，我认为提供一张涵盖所有功能领域的图表会更有帮助：
- en: '![](img/dbb845b1-18f8-431e-97d1-195b6b8d84ca.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dbb845b1-18f8-431e-97d1-195b6b8d84ca.png)'
- en: 'Figure 3: A glimpse of available algorithms and the supported ETL techniques
    (source: https://h20.ai/)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：可用算法和支持的 ETL 技术概览（来源：https://h20.ai/）
- en: 'This is a list of features and techniques curated from the H2O website. It
    can be used for wrangling data, modeling using the data, and scoring the resulting
    models:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从 H2O 网站整理出的功能和技术列表。它可以用于数据清洗、使用数据建模以及评分结果模型：
- en: Process
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流程
- en: Model
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型
- en: The scoring tool
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分工具
- en: Data profiling
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据概况
- en: '**Generalized linear models** (**GLM**)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广义线性模型**（**GLM**）'
- en: Predict
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Summary statistics
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结统计
- en: Decision trees
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Confusion matrix
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Aggregate, filter, bin, and derive columns
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合、过滤、分箱和推导列
- en: '**Gradient boosting machine** (**GBM**)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升机**（**GBM**）'
- en: AUC
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC
- en: Slice, log transform, and anonymize
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切片、对数变换和匿名化
- en: K-means
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means
- en: Hit ratio
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命中率
- en: Variable creation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量创建
- en: Anomaly detection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: PCA/PCA score
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA/PCA 评分
- en: DL
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（DL）
- en: Multimodel scoring
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模型评分
- en: Training and validation sampling plan
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证抽样计划
- en: Naive Bayes
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Grid search
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索
- en: 'The following figure shows how to provide a clear method of describing the
    way in which H2O Sparkling water can be used to extend the functionality of Apache
    Spark. Both H2O and Spark are open source systems. Spark MLlib contains a great
    deal of functionality, while H2O extends this with a wide range of extra functionalities,
    including DL. It offers tools to transform, model, and score the data, as we can
    find in Spark ML. It also offers a web-based user interface to interact with:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了如何清晰地描述 H2O Sparkling water 如何被用来扩展 Apache Spark 的功能。H2O 和 Spark 都是开源系统。Spark
    MLlib 包含大量功能，而 H2O 在此基础上扩展了许多额外功能，包括深度学习（DL）。它提供了数据转换、建模和评分的工具，正如我们在 Spark ML
    中看到的那样。它还提供了一个基于网页的用户界面来进行交互：
- en: '![](img/b10c6dbb-b830-491c-9d73-d0b27bdde21c.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b10c6dbb-b830-491c-9d73-d0b27bdde21c.png)'
- en: 'Figure 4: Sparkling water extends H2O and interoperates with Spark (source:
    https://h20.ai/)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Sparkling water 扩展 H2O，并与 Spark 互操作（来源：https://h20.ai/）
- en: 'The following figure shows how H2O integrates with Spark. As we already know,
    Spark has master and worker servers; the workers create executors to do the actual
    work. The following steps occur to run a Sparkling water-based application:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 H2O 如何与 Spark 集成。正如我们所知，Spark 有主节点和工作节点；工作节点创建执行器来执行实际的工作。以下是运行基于 Sparkling
    water 的应用程序的步骤：
- en: Spark's submit command sends the Sparkling water JAR to the Spark master
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 的提交命令将 Sparkling water JAR 发送到 Spark 主节点
- en: The Spark master starts the workers and distributes the JAR file
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark主节点启动工作节点并分发JAR文件
- en: The Spark workers start the executor JVMs to carry out the work
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark工作节点启动Executor JVM以执行工作
- en: The Spark executor starts an H2O instance
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark执行器启动一个H2O实例
- en: 'The H2O instance is embedded with the Executor JVM, and so it shares the JVM
    heap space with Spark. When all of the H2O instances have started, H2O forms a
    cluster, and then the H2O flow web interface is made available:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: H2O实例嵌入在Executor JVM中，因此它与Spark共享JVM堆内存。当所有H2O实例启动后，H2O将形成一个集群，并提供H2O flow网页界面：
- en: '![](img/10b221b6-aa8f-4e8f-ab55-a9da2d90afc3.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10b221b6-aa8f-4e8f-ab55-a9da2d90afc3.png)'
- en: 'Figure 5: How Sparkling water fits into the Spark architecture (source: http://blog.cloudera.com/blog/2015/10/how-to-build-a-machine-learning-app-using-sparkling-water-and-apache-spark/)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Sparkling Water如何融入Spark架构（来源：http://blog.cloudera.com/blog/2015/10/how-to-build-a-machine-learning-app-using-sparkling-water-and-apache-spark/）
- en: 'The preceding figure explains how H2O fits into the Spark architecture and
    how it starts, but what about data sharing? Now the question would be: how does
    data pass between Spark and H2O? The following diagram explains this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图解释了H2O如何融入Spark架构以及如何启动，但数据共享又该如何处理呢？现在的问题是：数据如何在Spark和H2O之间传递？下面的图解释了这一点：
- en: '![](img/8b0ab4ad-f630-4cc0-aa39-d8253355e303.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b0ab4ad-f630-4cc0-aa39-d8253355e303.png)'
- en: 'Figure 6: Data passing mechanism between Spark and H2O'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Spark和H2O之间的数据传递机制
- en: To get a clearer view of the preceding figure, a new H2O RDD data structure
    has been created for H2O and Sparkling water. It is a layer based at the top of
    an H2O frame, each column of which represents a data item and is independently
    compressed to provide the best compression ratio.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地查看前面的图，已经为H2O和Sparkling Water创建了一个新的H2O RDD数据结构。它是基于H2O框架顶部的一个层，每一列代表一个数据项，并独立压缩，以提供最佳的压缩比。
- en: ADAM for large-scale genomics data processing
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ADAM用于大规模基因组数据处理
- en: Analyzing DNA and RNA sequencing data requires large-scale data processing to
    interpret the data according to its context. Excellent tools and solutions have
    been developed at academic labs, but often fall short on scalability and interoperability.
    By this means, ADAM is a genomics analysis platform with specialized file formats
    built using Apache Avro, Apache Spark and Parquet.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 分析DNA和RNA测序数据需要大规模的数据处理，以根据其上下文解释数据。优秀的工具和解决方案已经在学术实验室中开发出来，但往往在可扩展性和互操作性方面存在不足。因此，ADAM是一个基因组分析平台，采用了Apache
    Avro、Apache Spark和Parquet构建的专用文件格式。
- en: However, large-scale data processing solutions such as ADAM-Spark can be applied
    directly to the output data from a sequencing pipeline, that is, after quality
    control, mapping, read preprocessing, and variant quantification using single
    sample data. Some examples are DNA variants for DNA sequencing, read counts for
    RNA sequencing, and so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像ADAM-Spark这样的规模化数据处理解决方案可以直接应用于测序管道的输出数据，也就是说，在质量控制、比对、读段预处理和变异定量之后，使用单一样本数据进行处理。举几个例子，DNA测序的DNA变异、RNA测序的读数计数等。
- en: 'See more at [http://bdgenomics.org/](http://bdgenomics.org/) and the related
    publication: Massie, Matt and Nothaft, Frank et al., ADAM: Genomics Formats and
    Processing Patterns for Cloud Scale Computing, UCB/EECS-2013-207, EECS Department,
    University of California, Berkeley.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更多内容见[http://bdgenomics.org/](http://bdgenomics.org/)以及相关的出版物：Massie, Matt 和
    Nothaft, Frank 等人，《ADAM：用于云规模计算的基因组格式与处理模式》，UCB/EECS-2013-207，加利福尼亚大学伯克利分校电子工程与计算机科学系。
- en: In our study, ADAM is used to achieve the scalable genomics data analytics platform
    with support for the VCF file format so that we can transform genotype-based RDD
    into a Spark DataFrame.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，使用ADAM实现了支持VCF文件格式的可扩展基因组数据分析平台，从而将基于基因型的RDD转化为Spark DataFrame。
- en: Unsupervised machine learning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习
- en: Unsupervised learning is a type of machine learning algorithm used for grouping
    related data objects and finding hidden patterns by inferencing from unlabeled
    datasets—that is, training sets consisting of input data without labels.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种机器学习算法，用于通过推理未标记数据集（即由没有标签的输入数据组成的训练集）来对相关数据对象进行分组并发现隐藏的模式。
- en: Let's see a real-life example. Suppose you have a large collection of non-pirated
    and totally legal MP3 files in a crowded and massive folder on your hard drive.
    Now, what if you could build a predictive model that helps you automatically group
    together similar songs and organize them into your favorite categories, such as
    country, rap, and rock?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个现实生活中的例子。假设你在硬盘上有一个装满非盗版且完全合法的MP3文件的大文件夹。如果你能建立一个预测模型，帮助你自动将相似的歌曲分组并组织成你喜欢的类别，比如乡村、说唱和摇滚，如何？
- en: This is an act of assigning an item to a group so that an MP3 is added to the
    respective playlist in an unsupervised way. For classification, we assume that
    you are given a training dataset of correctly labeled data. Unfortunately, we
    do not always have that luxury when we collect data in the real world.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将项目分配到组中的行为，以便MP3文件能够以无监督的方式添加到相应的播放列表中。对于分类，我们假设你提供了一个正确标记的数据训练集。不幸的是，当我们在现实世界中收集数据时，我们并不总是有这种奢侈的条件。
- en: 'For example, suppose we would like to divide a huge collection of music into
    interesting playlists. How can we possibly group together songs if we do not have
    direct access to their metadata? One possible approach is a mixture of various
    ML techniques, but clustering is often at the heart of the solution:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想将大量的音乐分成有趣的播放列表。如果我们没有直接访问它们的元数据，我们如何可能将歌曲分组呢？一种可能的方法是混合使用各种机器学习技术，但聚类通常是解决方案的核心：
- en: '![](img/dd15ee54-52ae-47a9-8dcf-646723794d3a.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd15ee54-52ae-47a9-8dcf-646723794d3a.png)'
- en: 'Figure 7: Clustering data samples at a glance'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：聚类数据样本一览
- en: In other words, the main objective of unsupervised learning algorithms is to
    explore unknown/hidden patterns in input data that is unlabeled. Unsupervised
    learning, however, also comprehends other techniques to explain the key features
    of the data in an exploratory way to find the hidden patterns. To overcome this
    challenge, clustering techniques are used widely to group unlabeled data points
    based on certain similarity measures in an unsupervised way.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，无监督学习算法的主要目标是探索输入数据中未知/隐藏的模式，这些数据是没有标签的。然而，无监督学习也包含其他技术，以探索性方式解释数据的关键特征，寻找隐藏的模式。为了克服这一挑战，聚类技术被广泛应用于基于某些相似性度量，无监督地对未标记数据点进行分组。
- en: Population genomics and clustering
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人群基因组学与聚类
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus, a simple definition
    of clustering can be thought of as the process of organizing objects into groups
    whose members are similar in some way, as shown in.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是将数据样本或数据点划分并放入相应的同质类或聚类中。因此，聚类的简单定义可以被看作是将对象组织成一些成员在某种方式上相似的组的过程，如下所示。
- en: This way, a cluster is a collection of objects that have some similarity between
    them and are dissimilar to the objects belonging to other clusters. If collections
    of genetic variants are given, clustering algorithms put these objects into a
    group based on similarity—that is, population groups or super-population groups.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，聚类就是将一些在某种程度上彼此相似的对象集合在一起，并与属于其他聚类的对象不相似的集合。如果给定的是遗传变异集合，聚类算法会基于相似性将这些对象放入一个组中——也就是人口组或超级人口组。
- en: How does K-means work?
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值算法是如何工作的？
- en: A clustering algorithm, such as K-means, locates the centroid of the group of
    data points. However, to make clustering accurate and effective, the algorithm
    evaluates the distance between each point from the centroid of the cluster.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法，如K均值算法，定位数据点组的质心。然而，为了使聚类更加准确和有效，算法会评估每个点与聚类质心的距离。
- en: 'Eventually, the goal of clustering is to determine intrinsic grouping in a
    set of unlabeled data. For example, the K-means algorithm tries to cluster related
    data points within the predefined **three** (that is, *k = 3*) clusters as shown
    in *Figure 8*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，聚类的目标是确定一组未标记数据中的内在分组。例如，K均值算法尝试将相关的数据点聚类到预定义的**三**个（即*k = 3*）聚类中，如*图 8*所示：
- en: '![](img/e9b8ab25-2f24-4635-b875-9710eb439de4.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9b8ab25-2f24-4635-b875-9710eb439de4.png)'
- en: 'Figure 8: The results of a typical clustering algorithm and a representation
    of the cluster centers'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：典型聚类算法的结果及聚类中心的表示
- en: 'In our case, using a combined approach of Spark, ADAM and H2O are capable of
    processing large amounts of variant data points. Suppose, we have n data points
    (x[i], i=1, 2… n, example, genetic variants) that need to be partitioned into
    *k* clusters. Then K-means assigns a cluster to each data point and aiming to
    find the positions *μ[i]*, *i=1...k* of the clusters that minimize the distance
    from the data points to the cluster. Mathematically, K-means tries to achieve
    the goal by solving an equation—that is, an optimization problem:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，结合使用Spark、ADAM和H2O的方法能够处理大量不同的数据点。假设我们有n个数据点（x[i]，i=1, 2… n，举例来说，是遗传变异），这些数据点需要被分为*k*个簇。然后，K-means将每个数据点分配给一个簇，目标是找到簇的位置*μ[i]*，*i=1...k*，以最小化数据点到簇的距离。从数学上讲，K-means试图通过解一个方程来实现这一目标——也就是一个优化问题：
- en: '![](img/bb521b9d-0f9d-49ed-b2f7-e642e98ee905.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb521b9d-0f9d-49ed-b2f7-e642e98ee905.png)'
- en: In the preceding equation, *c[i]* is the set of data points that assigned to
    cluster *i* and *d(x,μ[i])=∥x−μ[i]∥[2]²* is the Euclidean distance to be calculated.
    The algorithm computes this distance between data points and the center of the
    k clusters by minimizing the **Within-Cluster Sum of Squares** (that is, **WCSS**),
    where *c[i]* is the set of points belonging to cluster *i*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*c[i]*是分配给簇*i*的数据点集合，*d(x,μ[i])=∥x−μ[i]∥[2]²*是需要计算的欧几里得距离。该算法通过最小化**簇内平方和**（即**WCSS**），计算数据点与k个簇中心之间的距离，其中*c[i]*是属于簇*i*的点的集合。
- en: 'Therefore, we can understand that the overall clustering operation using K-means
    is not a trivial one but an NP-hard optimization problem. Which also means that
    K-means algorithm not only tries to find the global minima but also often is stuck
    in different solutions. The K-means algorithm proceeds by alternating between
    two steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以理解，使用K-means进行的总体聚类操作并非一项简单的任务，而是一个NP难优化问题。这也意味着K-means算法不仅尝试找到全局最小值，而且经常陷入不同的解中。K-means算法通过交替执行两个步骤进行：
- en: '**Cluster assignment step**: Assign each observation to the cluster whose mean
    yields the least **WCSS**. The sum of squares is the squared Euclidean distance.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇分配步骤**：将每个观测值分配给使得均值产生最小**WCSS**的簇。平方和是平方欧几里得距离。'
- en: '**Centroid update step**: Calculate the new means to be the centroids of the
    observations in the new clusters.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质心更新步骤**：计算新簇中观测值的均值作为质心。'
- en: 'In a nutshell, the overall approach of K-means training can be described in
    following figure:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，K-means训练的整体方法可以用下图描述：
- en: '![](img/aab181a6-0476-41d9-822d-bb2c0feeab04.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aab181a6-0476-41d9-822d-bb2c0feeab04.png)'
- en: 'Figure 9: Overall approach of the K-means algorithm process'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：K-means算法过程的总体方法
- en: DNNs for geographic ethnicity prediction
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于地理族群预测的DNN
- en: '**Multilayer Perceptron** (**MLP**) is an example of a DNN that is a feed-forward
    neural network; that is, there are only connections between the neurons from different
    layers. There is one (pass through) input layer, one or more layers of **linear
    threshold units** (**LTUs**) (called **hidden layers**), and one final layer of
    LTUs (called the **output layer**).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）是一个示例DNN，它是一个前馈神经网络；即神经元之间只有不同层之间的连接。它有一个（通过）输入层，一个或多个**线性阈值单元**（**LTUs**）（称为**隐藏层**），以及一个LTU的最终层（称为**输出层**）。'
- en: Each layer, excluding the output layer, involves a bias neuron and is fully
    connected to the next layer, forming a fully connected bipartite graph. The signal
    flows exclusively from the input to the output, that is, one-directional (**feed-forward**).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层（输出层除外）都涉及一个偏置神经元，并且与下一层完全连接，形成一个完全连接的二分图。信号只从输入流向输出，即单向(**前馈**)
- en: 'Until recently, an MLP was trained using the back-propagation training algorithm,
    but now the optimized version (that is, Gradient Descent) uses a reverse-mode
    auto diff; that is, the neural networks are trained with SGD using back-propagation
    as a gradient computing technique. Two layers of abstraction are used in DNN training
    for solving classification problems:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，MLP是通过反向传播训练算法进行训练的，但现在优化版本（即梯度下降）使用反向模式自动微分；即神经网络通过SGD和反向传播作为梯度计算技术进行训练。在DNN训练中，为了解决分类问题，使用了两层抽象：
- en: '**Gradient computation**: Using back-propagation'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度计算**：使用反向传播'
- en: '**Optimization level**: Using SGD, ADAM, RMSPro, and Momentum optimizers to
    compute the gradient computed earlier'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化层级**：使用SGD、ADAM、RMSPro和Momentum优化器来计算之前计算的梯度'
- en: In each training cycle, the algorithm feeds the data into the network and computes
    the state and output for every neuron in the consecutive layers. The approach
    then measures the output error over the network, that is, the gap between the
    expected output and the current output, and the contribution from each neuron
    in the last hidden layer towards the neuron's output error.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期中，算法将数据输入到网络中，并计算每个神经元在连续层中的状态和输出。然后，算法衡量网络中的输出误差，即期望输出与当前输出之间的差距，并计算最后一个隐藏层中每个神经元对该误差的贡献。
- en: 'Iteratively, the output error is propagated back to the input layer through
    all hidden layers and the error gradient is calculated across all connection weights
    during backward propagation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步地，输出误差通过所有隐藏层反向传播到输入层，并在反向传播过程中计算所有连接权重的误差梯度：
- en: '![](img/a6998423-d881-4f26-a9e3-c868dc81c503.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6998423-d881-4f26-a9e3-c868dc81c503.png)'
- en: 'Figure 10: A modern MLP consisting of input layer, ReLU, and softmax'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：由输入层、ReLU 和 softmax 组成的现代 MLP
- en: For a multiclass classification task, the output layer is typically determined
    by a shared softmax function (see *Figure 2* for more) in contrast to individual
    activation functions, and each output neuron provides the estimated probability
    for the corresponding class.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类别分类任务，输出层通常由一个共享的 softmax 函数决定（更多内容请参见*图 2*），与单独的激活函数不同，每个输出神经元提供对应类别的估计概率。
- en: Additionally, we will be using tree ensembles such as random forest for the
    classification. At this moment, I believe we can skip the basic introduction of
    RF since we have covered it in detail in [Chapter 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml),
    *Analyzing Insurance Severity Claims*, [Chapter 2](4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml),
    *Analyzing and Predicting Telecommunication Churn*, and [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml),
    *High-Frequency Bitcoin Price Prediction from Historical Data*. Well, it is time
    for the being stared. Nonetheless, it is always good to have your programming
    environment ready before getting your hands dirty.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用树集成方法，例如随机森林来进行分类。目前，我认为我们可以跳过随机森林的基础介绍，因为我们已经在[第 1 章](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml)《分析保险赔付严重性》、[第
    2 章](4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml)《分析和预测电信流失》和[第 3 章](51e66c26-e12b-4764-bbb7-444986c05870.xhtml)《基于历史数据的高频比特币价格预测》中详细讲解过它。好了，现在是时候开始了。不过，在动手之前，确保你的编程环境已经准备好总是一个好习惯。
- en: Configuring programming environment
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置编程环境
- en: In this section, we describe how to configure our programming environment so
    that we can interoperate with Spark, H2O, and Adam. Note that using H2O on a laptop
    or desktop is quite resource intensive. Therefore, make sure that your laptop
    has at least 16 GB of RAM and enough storage.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何配置我们的编程环境，以便能够与 Spark、H2O 和 ADAM 互操作。请注意，在笔记本电脑或台式机上使用 H2O 会消耗大量资源。因此，请确保你的笔记本至少有
    16GB 的 RAM 和足够的存储空间。
- en: 'Anyway, I am going to make this project a Maven project on Eclipse. However,
    you can try to define the same dependencies in SBT too. Let us define the properties
    tag on a `pom.xml` file for a Maven-friendly project:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我打算在 Eclipse 上将这个项目设置为 Maven 项目。不过，你也可以尝试在 SBT 中定义相同的依赖项。让我们在 `pom.xml`
    文件中定义属性标签，以便构建一个适合 Maven 的项目：
- en: '[PRE1]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then we can the latest version of the Spark 2.2.1 version (any 2.x version
    or even higher should work fine):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 Spark 2.2.1 的最新版本（任何 2.x 版本甚至更高版本都应该能正常工作）：
- en: '[PRE2]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we need to declare the dependencies for H2O and Sparkling water that match
    the version specified in the properties tag. Later versions might also work, and
    you can try:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要声明 H2O 和 Sparkling Water 的依赖项，确保它们与属性标签中指定的版本匹配。较新版本可能也能正常工作，你可以尝试：
- en: '[PRE3]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let''s define ADAM and its dependencies:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们定义 ADAM 及其依赖项：
- en: '[PRE4]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When I tried this on a Windows machine, additionally I had to install `joda-time`
    dependencies. Let us do it (but depending your platform, it might not be needed):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在 Windows 机器上尝试时，我还需要安装 `joda-time` 依赖项。让我们来做这件事（但根据你的平台，可能不需要）：
- en: '[PRE5]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once you create a Maven project in Eclipse (manually from the IDE or using `$
    mvn install)`, all the required dependencies will be downloaded! We are ready
    to code now!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在 Eclipse 中创建了一个 Maven 项目（无论是通过 IDE 手动创建还是使用 `$ mvn install`），所有必需的依赖项将会被下载！我们现在可以开始编码了！
- en: 'Wait! How about seeing the UI of H2O on the browser? For this, we have to manually
    download the H2O JAR somewhere in our computer and run it as a regular `.jar`
    file. In short, it''s a three-way process:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！如何在浏览器中查看H2O的UI界面？为此，我们必须手动下载H2O的JAR文件并将其作为常规的`.jar`文件运行。简而言之，这是一个三步过程：
- en: Download the **Latest Stable Release** H[2]O from [https://www.h2o.ai/download/](https://www.h2o.ai/download/).
    Then unzip it; it contains everything you need to get started.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[https://www.h2o.ai/download/](https://www.h2o.ai/download/)下载**最新稳定版**H[2]O。然后解压，它包含了开始所需的一切。
- en: From your terminal/command prompt, run the `.jar` using `java -jar h2o.jar`.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从你的终端/命令提示符中，使用`java -jar h2o.jar`运行`.jar`文件。
- en: 'Point your browser to `http://localhost:54321`:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在浏览器中输入`http://localhost:54321`：
- en: '![](img/bbe1f494-edb7-4737-980a-1d485ebda72a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbe1f494-edb7-4737-980a-1d485ebda72a.png)'
- en: 'Figure 11: The UI of H2O FLOW'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：H2O FLOW的UI界面
- en: This shows the available features of the latest version (that is, h2o-3.16.0.4
    as of 19 January 2018) of H2O. However, I am not going to explain everything here,
    so let's stop exploring because I believe for the time being this much knowledge
    about H2O and Sparking water will be enough.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了最新版本（即截至2018年1月19日的h2o-3.16.0.4版）H2O的可用功能。然而，我不打算在这里解释所有内容，所以让我们停止探索，因为我相信目前为止，这些关于H2O和Sparkling
    Water的知识已经足够。
- en: Data pre-processing and feature engineering
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: I already stated that all the 24 VCF files contribute 820 GB of data. Therefore,
    I decided to use the genetic variant of chromosome Y only one two make the demonstration
    clearer. The size is around 160 MB, which is not meant to pose huge computational
    challenges. You can download all the VCF files as well as the panel file from
    [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经说明所有的24个VCF文件贡献了820 GB的数据。因此，我决定只使用Y染色体的遗传变异来使演示更加清晰。其大小约为160 MB，这样不会带来巨大的计算挑战。你可以从[ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/)下载所有的VCF文件以及面板文件。
- en: 'Let us get started. We start by creating `SparkSession`, the gateway for the
    Spark application:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。我们从创建`SparkSession`开始，这是Spark应用程序的网关：
- en: '[PRE6]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then let''s show Spark the path of both VCF and the panel file:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们告诉Spark VCF文件和面板文件的路径：
- en: '[PRE7]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We process the panel file using Spark to access the target population data
    and identify the population groups. We first create a set of the populations that
    we want to predict:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Spark处理面板文件，以访问目标种群数据并识别种群组。我们首先创建一个我们想要预测的种群集合：
- en: '[PRE8]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we need to create a map of sample ID → population so that we can filter
    out the samples we are not interested in:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要创建一个样本ID → 种群的映射，以便筛选出我们不感兴趣的样本：
- en: '[PRE9]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that the panel file produces the sample ID of all individuals, population
    groups, ethnicities, super population groups, and the genders shown as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，面板文件会生成所有个体的样本ID、种群组、族群、超种群组以及性别，具体如下所示：
- en: '![](img/ed1be322-c2e8-4982-a8f6-50c9546d37ef.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed1be322-c2e8-4982-a8f6-50c9546d37ef.png)'
- en: 'Figure 12: Contents of a sample panel file'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：样本面板文件的内容
- en: 'Then load the ADAM genotypes and filter the genotypes so that we''re left with
    only those in the populations we''re interested in:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载ADAM基因型，并筛选基因型，使我们仅保留那些在我们感兴趣的种群中的基因型：
- en: '[PRE10]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next job would be converting the `Genotype` objects into our own `SampleVariant`
    objects to try to conserve memory. Then, the `genotype` object is converted into
    a `SampleVariant` object that contains only the data we need for further processing:
    the sample ID, which uniquely identifies a particular sample; a variant ID, which
    uniquely identifies a particular genetic variant; and a count of alternate alleles
    (only when the sample differs from the reference genome).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的工作是将`Genotype`对象转换为我们自己的`SampleVariant`对象，以尽量节省内存。然后，将`genotype`对象转换为一个`SampleVariant`对象，其中只包含我们进一步处理所需的数据：样本ID（唯一标识特定样本）；变异ID（唯一标识特定遗传变异）；以及替代等位基因的计数（仅在样本与参考基因组不同的情况下）。
- en: 'The signature that prepares a sample variant is given here; it takes `sampleID`,
    `variationId`, and the `alternateCount`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 准备样本变异的签名如下所示；它接受`sampleID`、`variationId`和`alternateCount`：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alright! Let us find `variantID` from the `genotype` file. A `varitantId` is
    a `String` type consisting of the name, start, and the end position in the chromosome:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！让我们从`genotype`文件中找到`variantID`。`variantId`是一个`String`类型，由名称、起始位置和染色体上的终止位置组成：
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once we have the `variantID`, we should hunt for the alternate count. In the
    `genotype` file, the objects that do not have an allele reference are roughly
    genetic alternates:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`variantID`，就应该寻找替代计数。在 `genotype` 文件中，那些没有等位基因参考的对象大致上是遗传变体：
- en: '[PRE13]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Lastly, we construct a simple variant object. For this, we need to intern sample
    IDs as they will be repeated a lot in a VCF file:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建一个简单的变体对象。为此，我们需要对样本 ID 进行内联，因为它们在 VCF 文件中会频繁出现：
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Excellent! We have been able to construct simple variants. Now, the next challenging
    task is to prepare `variantsRDD` before we are able to create the `variantsBySampleId`
    RDD:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们已经能够构建简单的变体了。现在，下一项具有挑战性的任务是准备 `variantsRDD`，然后我们才能创建 `variantsBySampleId`
    RDD：
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then we have to group the variants by sample ID so that we can process the
    variants sample by sample. After that, we can get the total number of samples
    to be used to find variants that are missing for some samples. Lastly, we have
    to group the variants by variant ID and filter out those variants that are missing
    from some samples:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须按样本 ID 对变体进行分组，以便逐个样本处理变体。之后，我们可以获得将用于查找缺失变体的样本总数。最后，我们必须按变体 ID 对变体进行分组，并过滤掉那些在某些样本中缺失的变体：
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now let''s make a map of variant ID → count of samples with an alternate count
    of greater than zero. Then we filter out those variants that are not in our desired
    frequency range. The objective here is simply to reduce the number of dimensions
    in the dataset to make it easier to train the model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个变体 ID → 替代计数大于零的样本计数的映射。然后我们过滤掉那些不在所需频率范围内的变体。这里的目标只是减少数据集中的维度数量，以便更容易训练模型：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The total number of samples (or individuals) has been determined before grouping
    them based on their variant IDs and filtering out variants without support by
    the samples to simplify the data pre-processing and to better cope with the very
    large number of variants (in total 84.4 million).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于变体 ID 对样本进行分组并过滤掉没有支持的变体之前，样本（或个体）的总数已确定，以简化数据预处理并更好地应对大量变体（总计 8440 万个）。
- en: '*Figure 13* shows a conceptual view of a genotype variants collection in the
    1000 Genomes project and exposes the feature extraction process from the same
    data to train our K-means and MLP models:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13* 显示了 1000 基因组项目中基因型变体集合的概念视图，并展示了从相同数据中提取特征，以训练我们的 K-means 和 MLP 模型的过程：'
- en: '![](img/5006da23-8d31-45d6-a5a6-dfeb88d14337.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5006da23-8d31-45d6-a5a6-dfeb88d14337.png)'
- en: 'Figure 13: Conceptual view of the genotype variants collection in the 1000
    Genomes project'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：1000基因组项目中基因型变体集合的概念视图
- en: 'The specified range is arbitrary and was chosen because it includes a reasonable
    number of variants, but not too many. To be more specific, for each variant, the
    frequency for alternate alleles have been calculated, and variants with less than
    12 alternate alleles have been excluded, leaving about 3 million variants in the
    analysis (for 23 chromosome files):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 指定的范围是任意的，选择它是因为它包含了一个合理数量的变体，但不至于太多。具体来说，对于每个变体，都计算了替代等位基因的频率，排除了少于 12 个替代等位基因的变体，分析中留下了约
    300 万个变体（来自 23 个染色体文件）：
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once we has `filteredVariantsBySampleId`, the next task is to sort the variants
    for each sample ID. Each sample should now have the same number of sorted variants:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`filteredVariantsBySampleId`，下一个任务是对每个样本 ID 的变体进行排序。每个样本现在应该具有相同数量的排序变体：
- en: '[PRE19]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All items in the RDD should now have the same variants in the same order. The
    final task is to use `sortedVariantsBySampleId` to construct an RDD of `Row` containing
    the region and the alternate count:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，RDD 中的所有项应该具有相同顺序的相同变体。最终任务是使用`sortedVariantsBySampleId`构建一个包含区域和替代计数的 `Row`
    类型 RDD：
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Therefore, we can just use the first one to construct our header for the training
    data frame:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只需使用第一个来构建训练数据框架的头部：
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Well done! Up to this point, we have our RDD and the header `StructType`. So
    now, we can play with both H2O and the Spark deep/machine learning algorithm with
    minimal adjustment/conversion. The overall flow of this end-to-end project can
    be seen in the following figure:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！到目前为止，我们已经得到了我们的 RDD 和 `StructType` 头信息。现在，我们可以通过最小的调整/转换来使用 H2O 和 Spark
    的深度/机器学习算法。整个端到端项目的流程如下图所示：
- en: '![](img/47f8c70a-7a52-495c-bbc2-afbab44c1ab1.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47f8c70a-7a52-495c-bbc2-afbab44c1ab1.png)'
- en: 'Figure 14: The pipeline of the overall approach'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：整体方法的管道流程
- en: Model training and hyperparameter tuning
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练与超参数调优
- en: 'Once we have `rowRDD` and the header, the next task is to construct the rows
    of our Schema DataFrame from the variants using the header and `rowRDD`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`rowRDD`和表头，接下来的任务是使用表头和`rowRDD`构建Schema DataFrame的行：
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/9e459b56-504f-4465-a5c5-ebd78266d610.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e459b56-504f-4465-a5c5-ebd78266d610.png)'
- en: 'Figure 15: A snapshot of the training dataset containing features and the label
    (that is, Region) columns'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：包含特征和标签（即Region）列的训练数据集快照
- en: In the preceding DataFrame, only a few columns, including the label, are shown
    so that it fits on the page.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的DataFrame中，仅显示了少数列，包括标签，以便适合页面。
- en: Spark-based K-means for population-scale clustering
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Spark的K-means用于大规模人口聚类
- en: 'In a previous section, we have seen how the K-means work. So we can directly
    dive into the implementation. Since the training will be unsupervised, we need
    to drop the label column (that is, `Region`):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经了解了K-means的工作原理。所以我们可以直接进入实现部分。由于训练是无监督的，我们需要删除标签列（即`Region`）：
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/34b50280-9728-4f15-8048-2b5e56d2f801.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34b50280-9728-4f15-8048-2b5e56d2f801.png)'
- en: 'Figure 16: A snapshot of the training dataset for K-means without the label
    (that is, Region)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：没有标签（即Region）的K-means训练数据集快照
- en: 'Now, we have seen in [Chapters 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml), *Analyzing
    Insurance Severity Claims* and [Chapter 2](4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml), *Analyzing
    and Predicting Telecommunication Churn* that Spark expects two columns (that is,
    features and label) for supervised training, and for unsupervised training, it expects only
    a single column containing the features. Since we dropped the label column, we
    now need to amalgamate the entire variable column into a single `features` column.
    So for this, we will again use the `VectorAssembler()` transformer. At first,
    let''s select the columns to be embedded into a vector space:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经在[第1章](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml)，《分析保险严重性索赔》和[第2章](4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml)，《分析与预测电信流失》中看到，Spark期望监督训练时有两列（即特征和标签），而无监督训练时它只期望包含特征的单列。由于我们删除了标签列，现在需要将整个变量列合并为一个`features`列。因此，我们将再次使用`VectorAssembler()`转换器。首先，我们选择要嵌入向量空间的列：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then we instantiate the `VectorAssembler()` transformer, specifying the input
    columns and the output column:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化`VectorAssembler()`转换器，指定输入列和输出列：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let''s see how it looks:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看它的效果：
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/a26cbeac-f5c1-423c-984d-1b5417fda8b5.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a26cbeac-f5c1-423c-984d-1b5417fda8b5.png)'
- en: 'Figure 17: A snapshot of the feature vectors for the K-means'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：K-means特征向量的快照
- en: 'Since our dataset is very highly dimensional, we can use some dimensionality
    algorithms such as PCA. So let''s do it by instantiating a `PCA()` transformer
    as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集维度非常高，我们可以使用一些降维算法，如PCA。因此，我们通过实例化一个`PCA()`转换器来实现，如下所示：
- en: '[PRE27]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we transform the assembled DataFrame (that is, assembled) and the top
    50 principle components. You can adjust the number though. Finally, to avoid the
    ambiguity, we renamed the `pcaFeatures` column to `features`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们转换组合后的DataFrame（即已组合的）和前50个主成分。你可以调整这个数量。最后，为了避免歧义，我们将`pcaFeatures`列重命名为`features`：
- en: '[PRE28]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/6b822396-5778-4322-9013-44a867bc6a2f.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b822396-5778-4322-9013-44a867bc6a2f.png)'
- en: 'Figure 18: A snapshot of the top 50 principal components as the most important
    features'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：前50个主成分的快照，作为最重要的特征
- en: 'Excellent! Everything went smoothly. Finally, we are ready to train the K-means
    algorithm:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！一切顺利。最后，我们准备好训练K-means算法了：
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'So let''s evaluate clustering by computing the **Within-Set Sum of Squared
    Errors** (**WSSSE**):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们通过计算**组内平方误差和**（**WSSSE**）来评估聚类效果：
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Determining the number of optimal clusters
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定最佳聚类数
- en: The beauty of clustering algorithms such as K-means is that they do the clustering
    on the data with an unlimited number of features. They are great tools to use
    when you have raw data and would like to know the patterns in that data. However,
    deciding on the number of clusters prior doing the experiment might not be successful
    and may sometimes lead to an overfitting or underfitting problem.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 像K-means这样的聚类算法的优点是，它们可以对具有无限特征数的数据进行聚类。当你拥有原始数据并且希望了解数据中的模式时，它们是非常有用的工具。然而，在进行实验之前决定聚类数可能并不成功，有时甚至会导致过拟合或欠拟合的问题。
- en: On the other hand, one common thing to all three algorithms (that is, K-means,
    bisecting K-means, and Gaussian mixture) is that the number of clusters must be
    determined in advance and supplied to the algorithm as a parameter. Hence, informally,
    determining the number of clusters is a separate optimization problem to be solved.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，K-means、二分K-means和高斯混合这三种算法的一个共同点是，聚类数必须事先确定，并作为参数传递给算法。因此，非正式地说，确定聚类数是一个独立的优化问题，需要解决。
- en: 'Now we will use a heuristic approach based on the Elbow method. We start from
    K = 2 clusters, then we run the K-means algorithm for the same dataset by increasing
    K and observing the value of the cost function WCSS:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用基于肘部法则的启发式方法。从K=2个聚类开始，然后通过增加K值运行K-means算法，观察成本函数WCSS的变化：
- en: '[PRE31]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'At some point, a big drop in cost function can be observed, but then the improvement
    became marginal with the increasing value of `k`. As suggested in cluster analysis
    literature, we can pick the `k` after the last big drop of WCSS as an optimal
    one. Now, let''s see the WCSS values for a different number of clusters between
    2 and 20 for example:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些时刻，可以观察到成本函数有一个大幅下降，但随着`k`值增加，改进变得非常微小。正如聚类分析文献中所建议的，我们可以选择在WCSS的最后一次大幅下降之后的`k`值作为最优值。现在，让我们来看一下在2到20之间的不同聚类数的WCSS值，例如：
- en: '[PRE32]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now let us discuss how to take advantage of the Elbow method for determining
    the number of clusters. As shown next, we calculated the cost function, WCSS,
    as a function of a number of clusters for the K-means algorithm applied to Y chromosome
    genetic variants from the selected population groups.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何利用肘部法则来确定聚类数。如下面所示，我们计算了成本函数WCSS，作为K-means算法在选定人群组的Y染色体基因变异上的聚类数函数。
- en: 'It can be observed that a somewhat **big drop** occurs when `k = 9` (which
    is not a drastic drop though). Therefore, we choose the number of clusters to
    be 10, as shown in *Figure 10*:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，当`k = 9`时出现了一个相对**较大的下降**（尽管这并不是一个剧烈的下降）。因此，我们选择将聚类数设置为10，如*图10*所示：
- en: '![](img/4260d5c6-46f4-4302-9948-ab8cbd4190b8.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4260d5c6-46f4-4302-9948-ab8cbd4190b8.png)'
- en: Figure 19: Number of clusters as a function of WCSS
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：聚类数与WCSS的关系
- en: Using H2O for ethnicity prediction
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O进行种族预测
- en: Up to this point, we have seen how to cluster genetic variants. We have also
    used the Elbow method and found the number of optimal `k`, the tentative number
    cluster. Now we should explore another task that we planned at the beginning—that
    is, ethnicity prediction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何聚类基因变异。我们还使用了肘部法则，找到了最优`k`值和初步的聚类数。现在我们应该探索另一个我们一开始计划的任务——即种族预测。
- en: 'In the previous K-means section, we prepared a Spark DataFrame named `schemaDF`.
    That one cannot be used with H2O. However, an additional conversion is necessary.
    We use the `asH2OFrame()` method to convert the Spark DataFrame into an H2O frame:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的K-means部分中，我们准备了一个名为`schemaDF`的Spark数据框。这个数据框不能直接用于H2O。然而，我们需要进行额外的转换。我们使用`asH2OFrame()`方法将Spark数据框转换为H2O框架：
- en: '[PRE33]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, one important thing you should remember while using H2O is that if you
    do not convert the label column into categorical, it will treat the classification
    task as regression. To get rid of this, we can use the `toCategoricalVec()` method
    from H2O. Since H2O frames are resilient, we can further update the same frame:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一件重要的事情你需要记住，当使用H2O时，如果没有将标签列转换为类别型，它会将分类任务视为回归任务。为了避免这种情况，我们可以使用H2O的`toCategoricalVec()`方法。由于H2O框架是弹性的，我们可以进一步更新相同的框架：
- en: '[PRE34]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now our H2O frame is ready to train an H2O-based DL model (which is DNN, or
    to be more specific, a deep MLP). However, before we start the training, let''s
    randomly split the DataFrame into 60% training, 20% test, and 20% validation data
    using the H2O built-in `FrameSplitter()` method:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的H2O框架已准备好训练基于H2O的深度学习模型（即DNN，或者更具体地说，是深度MLP）。然而，在开始训练之前，让我们使用H2O内置的`FrameSplitter()`方法随机将数据框拆分为60%的训练集、20%的测试集和20%的验证集：
- en: '[PRE35]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Fantastic! Our train, test, and validation sets are ready, so let us set the
    parameters for our DL model:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们的训练集、测试集和验证集已经准备好，现在让我们为我们的深度学习模型设置参数：
- en: '[PRE36]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding setting, we have specified an MLP having three hidden layers
    with 128, 256 and 512 neurons respectively. So altogether, there are five layers
    including the input and the output layer. The training will iterate up to 200
    epoch. Since we have used too many neurons in the hidden layer, we should use
    the dropout to avoid overfitting. To avoid achieve a better regularization, we
    used the l1 regularization.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的设置中，我们已经指定了一个具有三层隐藏层的MLP，隐藏层神经元数分别为128、256和512。因此，总共有五层，包括输入层和输出层。训练将迭代最多200次。由于隐藏层神经元数量较多，我们应该使用dropout来避免过拟合。为了获得更好的正则化效果，我们使用了L1正则化。
- en: The preceding setting also states that we will train the model using the training
    set, and additionally the validation set will be used to validate the training.
    Finally, the response column is `Region`. On the other hand, the seed is used
    to ensure reproducibility.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的设置还表明，我们将使用训练集来训练模型，同时使用验证集来验证训练结果。最后，响应列为`Region`。另一方面，种子用于确保结果的可复现性。
- en: 'So all set! Now let''s train the DL model:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪！现在，让我们来训练深度学习模型：
- en: '[PRE37]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Depending on your hardware configuration, it might take a while. Therefore,
    it is time to  rest and get some coffee maybe! Once we have the trained model,
    we can see the training error:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的硬件配置，这可能需要一段时间。因此，现在是时候休息一下，喝杯咖啡了！一旦我们得到训练好的模型，我们可以查看训练误差：
- en: '[PRE38]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Unfortunately, the training was not that great! Nevertheless, we should try
    with different combination of hyperparameters. The error turns out to be high
    though, but let us not worry too much and evaluate the model, compute some model
    metrics, and evaluate model quality:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，训练结果并不理想！不过，我们应该尝试不同的超参数组合。尽管误差较高，但让我们不要过于担心，先评估一下模型，计算一些模型指标，并评估模型质量：
- en: '[PRE39]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Not so high accuracy! However, you should try with other VCF files and by tuning
    the hyperparameters too. For example, after reducing the neurons in the hidden
    layers and with l2 regularization and 100 epochs, I had about 20% improvement:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率不高！不过，你应该尝试其他VCF文件，并调整超参数。例如，在减少隐藏层神经元数量、使用L2正则化和100轮训练后，我的准确率提高了大约20%：
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Another improvement clue is here. Apart from these hyperparameters, another
    advantage of using H2O-based DL algorithms is that we can take the relative variable/feature
    importance. In previous chapters, we have seen that when using a Random Forest
    algorithm in Spark, it is also possible to compute the variable importance.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进的线索在这里。除了这些超参数外，使用基于H2O的深度学习算法的另一个优势是我们可以获取相对的变量/特征重要性。在前面的章节中，我们已经看到，在Spark中使用随机森林算法时，也可以计算变量的重要性。
- en: 'Therefore, the idea is that if your model does not perform well, it would be
    worth dropping less important features and doing the training again. Now, it is
    possible to find the feature importance during supervised training. I have observed
    this feature importance:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基本思路是，如果模型表现不佳，值得删除一些不重要的特征并重新进行训练。现在，在有监督训练过程中，我们可以找到特征的重要性。我观察到以下特征重要性：
- en: '![](img/ba65fb17-4d3b-43ad-ae89-0b2780157865.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba65fb17-4d3b-43ad-ae89-0b2780157865.png)'
- en: Figure 20: Relative feature importance using H2O
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：使用H2O计算的相对特征重要性
- en: Now the question would be why don't you drop them and try training again and
    observe if the accuracy has increased or not? Well, I leave it up to the readers.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，为什么不尝试删除这些特征，然后重新训练并观察准确性是否提高？这个问题留给读者思考。
- en: Using random forest for ethnicity prediction
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行种族预测
- en: In the previous section, we have seen how to use H2O for ethnicity prediction.
    However, we could not achieve better prediction accuracy. Therefore, H2O is not
    mature enough to compute all the necessary performance metrics.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们已经看到了如何使用H2O进行种族预测。然而，我们未能取得更好的预测准确性。因此，H2O目前还不够成熟，无法计算所有必要的性能指标。
- en: So why don't we try Spark-based tree ensemble techniques such as Random Forest
    or GBTs? Because we have seen that in most cases, RF shows better predictive accuracy,
    so let us try with that one.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不尝试基于Spark的树集成技术，比如随机森林或梯度提升树（GBT）呢？因为我们已经看到在大多数情况下，随机森林（RF）表现出更好的预测精度，所以我们就从这个开始试试。
- en: 'In the K-means section, we''ve already prepared the Spark DataFrame named `schemaDF`.
    Therefore, we can simply transform the variables into feature vectors that we
    described before. Nevertheless, for this, we need to exclude the label column.
    We can do it using the `drop()` method as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在K-means部分，我们已经准备好了名为`schemaDF`的Spark DataFrame。因此，我们可以简单地将变量转换成我们之前描述的特征向量。然而，为了实现这一点，我们需要排除标签列。我们可以使用`drop()`方法来做到这一点，如下所示：
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'At this point, you can further reduce the dimensionality and extract the most
    principal components using PCA or any other feature selector algorithm. However,
    I will leave it up to you. Since Spark expects the label column to be numeric,
    we have to convert the ethnic group name into numeric. We can use `StringIndexer()`
    for this. It is straightforward:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，您可以进一步降低维度，并使用PCA或任何其他特征选择算法提取最主要的成分。然而，我将把这个留给您决定。由于Spark期望标签列是数值型的，我们必须将民族名称转换为数值。我们可以使用`StringIndexer()`来完成这一操作。这是非常简单的：
- en: '[PRE42]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then we randomly split the dataset for training and testing. In our case, let''s
    use 75% for the training and the rest for the testing:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们随机划分数据集进行训练和测试。在我们的例子中，假设我们使用75%作为训练数据，其余作为测试数据：
- en: '[PRE43]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Since this this a small dataset, considering this fact, we can `cache` both
    the train and test set for faster access:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个小数据集，考虑到这一点，我们可以`缓存`训练集和测试集，以便更快地访问：
- en: '[PRE44]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now let''s create a `paramGrid` for searching through decision tree''s `maxDepth`
    parameter for the best model:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个`paramGrid`，用于在决策树的`maxDepth`参数中进行搜索，以获得最佳模型：
- en: '[PRE45]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then we set up the 10-fold cross validation for an optimized and stable model.
    This will reduce the chances of overfitting:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们设置10折交叉验证，以获得优化且稳定的模型。这将减少过拟合的可能性：
- en: '[PRE46]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Well, now we are ready for the training. So let''s train the random forest
    model with the best hyperparameters setting:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们准备进行训练了。那么让我们使用最佳超参数设置来训练随机森林模型：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now that we have the cross-validated and the best model, why don't we evaluate
    the model using the test set. Why not? First, we compute the prediction DataFrame
    for each instance. Then we use the `MulticlassClassificationEvaluator()` to evaluate
    the performance since this is a multiclass classification problem.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了交叉验证和最佳模型，为什么不使用测试集来评估模型呢？为什么不呢？首先，我们为每个实例计算预测DataFrame。然后我们使用`MulticlassClassificationEvaluator()`来评估性能，因为这是一个多类分类问题。
- en: 'Additionally, we compute performance metrics such as `accuracy`, `precision`,
    `recall`, and `f1` measure. Note that using RF classifier, we can get `weightedPrecision`
    and the `weightedRecall`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还计算了性能指标，如`准确率`、`精确率`、`召回率`和`F1`值。请注意，使用RF分类器时，我们可以获得`加权精确率`和`加权召回率`：
- en: '[PRE48]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/40c0900b-2076-47ed-beb4-0bdc16c67382.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40c0900b-2076-47ed-beb4-0bdc16c67382.png)'
- en: Figure 21: Raw prediction probability, true label, and the predicted label using
    random forest
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：使用随机森林的原始预测概率、真实标签和预测标签
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now let''s compute the classification `accuracy`, `precision`, `recall`, `f1`
    measure and error on test data:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算分类的`准确率`、`精确率`、`召回率`、`F1`值以及测试数据上的错误率：
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we print the performance metrics:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印出性能指标：
- en: '[PRE51]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Yes, it turns out to be a better performer. This is bit unexpected since we
    hoped to have better predictive accuracy from a DL model, but we did not. As I
    already stated, we can still try with other parameters of H2O. Anyway, we can
    now see around 25% improvement using random forest. However, probably, it can
    still be improved.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，结果证明它的表现更好。这有点出乎意料，因为我们原本希望从深度学习模型中获得更好的预测准确性，但并没有得到。如我之前所说，我们仍然可以尝试使用H2O的其他参数。无论如何，现在我们看到使用随机森林大约提高了25%。不过，可能仍然可以进一步改进。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to interoperate with a few big data tools such as
    Spark, H2O, and ADAM for handling a large-scale genomics dataset. We applied the
    Spark-based K-means algorithm to genetic variants data from the 1000 Genomes project
    analysis, aiming to cluster genotypic variants at the population scale.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何与一些大数据工具（如Spark、H2O和ADAM）进行交互，以处理大规模基因组数据集。我们应用了基于Spark的K-means算法，分析了来自1000基因组计划的数据，旨在对人群规模的基因型变异进行聚类。
- en: Then we applied an H2O-based DL algorithm and Spark-based Random Forest models
    to predict geographic ethnicity. Additionally, we learned how to install and configure
    H2O for DL. This knowledge will be used in later chapters. Finally and importantly,
    we learned how to use H2O to compute variable importance in order to select the
    most important features in a training set.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应用基于H2O的深度学习（DL）算法和基于Spark的随机森林模型来预测地理族群。此外，我们还学习了如何安装和配置H2O进行深度学习。这些知识将在后续章节中使用。最后，也是最重要的，我们学会了如何使用H2O计算变量重要性，以便选择训练集中最重要的特征。
- en: In the next chapter, we will see how effectively we can use the **Latent Dirichlet
    Allocation** (**LDA**) algorithm for finding useful patterns in data. We will
    compare other topic modeling algorithms and the scalability power of LDA. In addition,
    we will utilize **Natural Language Processing** (**NLP**) libraries such as Stanford
    NLP.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何有效地使用**潜在狄利克雷分配**（**LDA**）算法来发现数据中的有用模式。我们将比较其他主题建模算法以及LDA的可扩展性。同时，我们还将利用**自然语言处理**（**NLP**）库，如斯坦福NLP。
