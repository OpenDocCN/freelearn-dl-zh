- en: '*Chapter 4*: Automating Document Processing Workflows'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：自动化文档处理工作流'
- en: In the previous chapter, we were introduced to **Amazon Comprehend** and **Amazon
    Comprehend Medical**, and we covered how to use these services to derive insights
    from text. We also spent some time understanding how Natural Language Processing
    algorithms work, the different types of insights you can uncover, and we also
    ran code samples trying out the Amazon Comprehend APIs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了**Amazon Comprehend**和**Amazon Comprehend Medical**，并讲解了如何使用这些服务从文本中提取洞察。我们还花了一些时间了解自然语言处理算法如何工作、可以揭示的不同类型的洞察，并且运行了代码示例，尝试使用
    Amazon Comprehend APIs。
- en: In this chapter, we will walk through our first real-world use case of automating
    a document management workflow that many organizations struggle with today. We
    put together this solution based on our collective experience and the usage trends
    we have observed in our careers. Fasten your seat belts and get ready to experience
    architecting an end-to-end AI solution one building block at a time and watch
    it taking shape in front of you. We expect to be hands-on throughout the course
    of this chapter, but we have all the code samples we need to get going.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示我们的第一个现实世界用例——自动化文档管理工作流，这是许多组织目前面临的挑战。我们根据我们的集体经验以及在职业生涯中观察到的使用趋势，整合了这一解决方案。系好安全带，准备好逐步体验如何构建一个端到端的
    AI 解决方案，并看到它在你面前逐渐成型。我们预计在本章的整个过程中将进行实践，但我们已经准备好了所有必要的代码示例，以便开始。
- en: We will dive deep into how you can automate document processing with **Amazon
    Textract** and then we will cover how you can set up compliance and control in
    the documents using **Amazon Comprehend**. Lastly, we will talk about architecture
    best practices while designing **real-time document processing** workflows versus
    **batch processing.** We will provide detailed code samples, designs, and development
    approaches, and a step-by-step guide on how to set up and run these examples along
    with access to GitHub repositories.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨如何使用**Amazon Textract**自动化文档处理，接着介绍如何使用**Amazon Comprehend**在文档中设置合规性和控制。最后，我们将讨论在设计**实时文档处理**工作流与**批处理**时的架构最佳实践。我们将提供详细的代码示例、设计和开发方法，并提供逐步指南，帮助您设置并运行这些示例，同时提供对
    GitHub 仓库的访问。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Automating document processing workflows
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化文档处理工作流
- en: Setting up compliance and control
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置合规性和控制
- en: Processing real-time document workflows versus batch document workflows
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时文档工作流与批量文档工作流的处理
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you will need access to an AWS account. Please make sure to
    follow the instructions specified in the *Technical requirements* section in [*Chapter
    2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing Amazon Textract*,
    to create your AWS account, and log in to the AWS Management Console before trying
    the steps in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容需要访问 AWS 账户。请确保按照[*第二章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)《介绍
    Amazon Textract》中的*技术要求*部分的说明创建 AWS 账户，并登录 AWS 管理控制台，然后再尝试本章中的步骤。
- en: 'The Python code and sample datasets for a walk-through of this chapter''s code
    are provided at the following link: [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004).
    Please use the instructions in the following sections along with the code in the
    repository to build the solution.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章代码的 Python 代码和示例数据集可以通过以下链接获取：[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004)。请根据以下章节中的说明，结合仓库中的代码来构建解决方案。
- en: Check out the following video to see the Code in Action at [https://bit.ly/3GlcCet](https://bit.ly/3GlcCet).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，观看代码的实际应用：[https://bit.ly/3GlcCet](https://bit.ly/3GlcCet)。
- en: Automating document processing workflows
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化文档处理工作流
- en: We have discussed in the previous chapter how Amazon Textract can help us digitize
    scanned documents such as PDF and images by extracting text from any document.
    We also covered how Amazon Comprehend can help us extract insights from these
    documents, including entities, **Personal Identifiable Information** (**PII**),
    and sentiments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中讨论了Amazon Textract如何通过从任何文档中提取文本，帮助我们数字化扫描文档（如PDF和图像）。我们还介绍了Amazon Comprehend如何帮助我们从这些文档中提取洞察，包括实体、**个人身份信息**（**PII**）和情感。
- en: Now, these services can be used together in an architecture to automate the
    document processing workflows for most organizations, be it a financial organization
    or healthcare, which we will cover in [*Chapter 12*](B17528_12_Final_SB_ePub.xhtml#_idTextAnchor141),
    *AI and NLP in Healthcare*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些服务可以在架构中一起使用，以自动化大多数组织的文档处理工作流，无论是金融机构还是医疗机构，关于这一点我们将在[*第12章*](B17528_12_Final_SB_ePub.xhtml#_idTextAnchor141)，*医疗行业中的AI和自然语言处理*中讨论。
- en: Let's start with a fictitious bank, *LiveRight Pvt Ltd.*, whose customers are
    applying for home loans. We all know this loan origination process involves more
    than 400 documents to be submitted and reviewed by the bank before approval is
    forthcoming for your home loan. Automating this process will make it easier for
    banks as well as customers to get loans. The challenge with automating these workflows
    is that there are more than 1,000 templates for the loan origination process and
    going with any **Optical Character Recognition** (**OCR**) system will require
    managing these templates. Moreover, these OCR template-based approaches are not
    scalable and break with format changes. That's why we have Amazon Textract to
    extract text from any documents, enabling these documents to be automated and
    processed in hours rather than months or weeks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个虚构的银行*LiveRight Pvt Ltd.*开始，该银行的客户正在申请住房贷款。我们都知道，这个贷款发放过程涉及到超过400份文件，需要银行在批准贷款之前提交并审核。这一过程的自动化将使银行和客户更容易获得贷款。自动化这些工作流的挑战在于，贷款发放过程有超过1000个模板，而使用任何**光学字符识别**（**OCR**）系统都需要管理这些模板。此外，这些基于OCR模板的方法不具备可扩展性，并且在格式变化时会出现问题。这就是为什么我们有Amazon
    Textract，它可以从任何文档中提取文本，使这些文档能够在数小时内而非数月或数周内完成自动化处理。
- en: You have extracted the data from these forms or semi-structured documents. You
    will now want to set up compliance and control on the data extracted from these
    documents; for example, making sure that if the data is PII, you can mask it for
    further processing. You will also want to extract the entities if you want to
    focus on the loan approval process, for example, the loan amount or the details
    of the requester. This is where Amazon Comprehend can help. In fact, you can perform
    custom classification of the documents submitted and the custom entities based
    on your requirements with Amazon Comprehend; for example, documents extracted
    by Textract and sent to Amazon Comprehend for custom classification to classify
    whether the document submitted is a driving license or W2 form.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从这些表单或半结构化文档中提取了数据。接下来，你将需要对从这些文档中提取的数据进行合规性和控制设置；例如，确保如果数据是个人身份信息（**PII**），你可以进行掩码处理以供进一步处理。如果你想专注于贷款批准流程，还需要提取实体，例如贷款金额或请求人的详细信息。这正是Amazon
    Comprehend可以帮助的地方。事实上，你可以使用Amazon Comprehend根据你的需求对提交的文档和自定义实体进行分类；例如，通过Textract提取的文档发送至Amazon
    Comprehend进行自定义分类，判断该文档是驾照还是W2表格。
- en: 'The following is the architecture of how you can use Amazon Textract and Amazon
    Comprehend together to automate your existing document flow:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何将Amazon Textract和Amazon Comprehend一起使用，以自动化现有文档流程的架构：
- en: '![Figure 4.1 – Automating document processing workflows'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1 – 自动化文档处理工作流'
- en: '](img/B17528_04_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_04_01.jpg)'
- en: Figure 4.1 – Automating document processing workflows
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 自动化文档处理工作流
- en: In this architecture, you have documents coming in, and these documents may
    be financial documents, legal documents, mortgage applications, and so on. You
    send these documents to Amazon Textract to extract text from these documents.
    Once you have extracted text from these documents, you can send this text to Amazon
    Comprehend to extract insights. These insights can classify these documents based
    on document type, it can identify PII from these documents, or it can be **named
    entity recognition** (**NER**) using custom entity recognition. We cover custom
    entities in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162), *Auditing
    Named Entity Recognition Workflows*, and document classification in [*Chapter
    15*](B17528_15_Final_SB_ePub.xhtml#_idTextAnchor178), *Classifying Documents and
    Setting up Human in the Loop for Active Learning*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，您将接收文档，这些文档可能是财务文档、法律文档、按揭申请等。您将这些文档发送到 Amazon Textract，从中提取文本。提取文本后，您可以将其发送到
    Amazon Comprehend，以提取见解。这些见解可以根据文档类型对这些文档进行分类，识别文档中的个人可识别信息（PII），或者通过自定义实体识别进行**命名实体识别**（**NER**）。我们在[*第14章*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162)中讲解了自定义实体，[*第15章*](B17528_15_Final_SB_ePub.xhtml#_idTextAnchor178)中讲解了文档分类，*审计命名实体识别工作流*以及*文档分类和设置人类介入进行主动学习*。
- en: In this section, we covered how you can easily and quickly set up an automated
    document processing workflow with Amazon Textract and Amazon Comprehend by using
    these services together. In the next section, we will talk about how you can use
    these services together to set up compliance and control for LiveRight Pvt Ltd.,
    especially by means of masking or redacting the PII data in their forms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何通过将 Amazon Textract 和 Amazon Comprehend 两项服务结合使用，快速轻松地设置自动化文档处理工作流。在下一节中，我们将讨论如何将这些服务结合使用，为
    LiveRight Pvt Ltd. 设置合规性和控制，特别是通过屏蔽或删除其表单中的 PII 数据。
- en: Setting up compliance and control
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置合规性和控制
- en: 'In this section, we will talk about how LiveRight Pvt Ltd. can set up compliance
    and control as well as automate their loan origination process using Amazon Textract
    and Amazon Comprehend. We will walk you through the following architecture using
    code samples in a Jupyter notebook:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用 Amazon Textract 和 Amazon Comprehend 为 LiveRight Pvt Ltd. 设置合规性和控制，并自动化其贷款发放流程。我们将通过在
    Jupyter notebook 中使用代码示例来逐步讲解以下架构：
- en: '![Figure 4.2 – Setting up compliance and control'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 设置合规性和控制'
- en: '](img/B17528_04_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_04_02.jpg)'
- en: Figure 4.2 – Setting up compliance and control
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 设置合规性和控制
- en: 'We will walk you through this architecture using a single document and sample
    code. However, this architecture can be automated to process a large number of
    documents using the **step function** and **lambda functions** in a serverless
    manner. In this architecture, we will show you the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个单一文档和示例代码来引导您完成这个架构。然而，这个架构可以通过 **Step Function** 和 **Lambda 函数** 以无服务器的方式自动处理大量文档。在这个架构中，我们将展示以下内容：
- en: How you can upload a sample document and extract the text using `.txt` or `.csv`
    files back to an **Amazon S3 bucket**.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何上传示例文档并使用 `.txt` 或 `.csv` 文件将提取的文本返回到 **Amazon S3 存储桶**。
- en: Then, we will show you how you can use **Amazon Comprehend's** real-time or
    sync API to detect PII.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将展示如何使用 **Amazon Comprehend** 的实时或同步 API 来检测 PII。
- en: We will then cover how you can use the Amazon Comprehend PII detection job to
    mask and redact the PII in the extracted text/CSV file in **Amazon S3**.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何使用 Amazon Comprehend 的 PII 检测作业，在 **Amazon S3** 中对提取的文本/CSV 文件中的
    PII 数据进行屏蔽和删除。
- en: How you can find the redacted document text in Amazon S3 as an output of the
    Comprehend PII detection job.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在 Amazon S3 中找到已删除 PII 数据的文档文本，作为 Comprehend PII 检测作业的输出。
- en: So, let's get started with setting up the notebook.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始设置 notebook。
- en: Setting up to solve the use case
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置以解决用例
- en: 'If you have not done so in the previous chapters, you will first have to create
    an Amazon SageMaker Jupyter notebook and set up **Identity and Access Management**
    (**IAM**) permissions for that notebook role to access the AWS services we will
    use in this notebook. After that, you will need to clone the GitHub repository
    ([https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services)).
    Please perform the following steps to complete these tasks before we can execute
    the cells from our notebook:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在前面的章节中没有这样做，你需要先创建一个 Amazon SageMaker Jupyter notebook，并为该 notebook 角色设置**身份和访问管理**（**IAM**）权限，以便访问我们将在本
    notebook 中使用的 AWS 服务。之后，你需要克隆 GitHub 仓库（[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services)）。请按以下步骤完成这些任务，之后我们才能执行
    notebook 中的单元格：
- en: Follow the instructions documented in the *Create an Amazon SageMaker Jupyter
    notebook instance* section within the *Setting up your AWS environment* section
    in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing
    Amazon Textract*, to create your Jupyter notebook instance.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[*第 2 章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)中[*设置 AWS 环境*]部分的*创建
    Amazon SageMaker Jupyter notebook 实例*章节中的说明，创建你的 Jupyter notebook 实例。
- en: IAM Role Permission while Creating Amazon SageMaker Jupyter Notebooks
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建 Amazon SageMaker Jupyter Notebooks 时的 IAM 角色权限
- en: Accept the default option for the IAM role at notebook creation time to allow
    access to any S3 bucket.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在创建 notebook 时，接受 IAM 角色的默认选项，以允许访问任何 S3 桶。
- en: Once you have created the notebook instance and its status is **InService**,
    click on **Open Jupyter** in the **Actions** menu heading for the notebook instance.![Figure
    4.3 – Opening the Jupyter notebook
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建完 notebook 实例并且其状态为**InService**后，点击 notebook 实例的**操作**菜单中的**打开 Jupyter**。![图
    4.3 – 打开 Jupyter notebook
- en: '](img/B17528_04_03.jpg)'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_04_03.jpg)'
- en: Figure 4.3 – Opening the Jupyter notebook
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.3 – 打开 Jupyter notebook
- en: This will take you to the home folder of your notebook instance.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将带你进入 notebook 实例的主文件夹。
- en: Click on **New** and then select **Terminal**, as shown in the following screenshot:![Figure
    4.4 – Opening Terminal in a Jupyter notebook
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**新建**，然后选择**终端**，如下图所示：![图 4.4 – 在 Jupyter notebook 中打开终端
- en: '](img/B17528_04_04.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_04_04.jpg)'
- en: Figure 4.4 – Opening Terminal in a Jupyter notebook
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.4 – 在 Jupyter notebook 中打开终端
- en: In the terminal window, first, type `cd SageMaker` and then type `git clone
    https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services`,
    as shown in the following screenshot:![Figure 4.5 – git clone command
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端窗口中，首先键入 `cd SageMaker`，然后键入 `git clone https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services`，如下图所示：![图
    4.5 – git clone 命令
- en: '](img/B17528_04_05.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17528_04_05.jpg)'
- en: Figure 4.5 – git clone command
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.5 – git clone 命令
- en: Now, exit the terminal window, go back to the home folder, and you will see
    a folder called `Chapter 04`. Click the folder and you should see a notebook called
    `Chapter 4 Compliance and control.ipynb`.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，退出终端窗口，返回主文件夹，你会看到一个名为`Chapter 04`的文件夹。点击该文件夹，你应该会看到一个名为`Chapter 4 Compliance
    and control.ipynb`的 notebook。
- en: Open this notebook by clicking it.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击此 notebook 打开它。
- en: Next, we will cover the additional IAM prerequisites.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍其他的 IAM 先决条件。
- en: Additional IAM prerequisites
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他 IAM 先决条件
- en: To train the Comprehend custom entity recognizer and to set up real-time endpoints,
    we have to enable additional policies and update the trust relationships for our
    SageMaker notebook role. To do this, attach `AmazonS3FullAccess`, `TextractFullAccess`,
    and `ComprehendFullAccess` policies to your Amazon SageMaker Notebook IAM Role.
    To execute this step, please refer to *Changing IAM permissions and trust relationships
    for the Amazon SageMaker notebook execution role* in the *Setting up your AWS
    environment* section in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027),
    *Introducing Amazon Textract*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练 Comprehend 自定义实体识别器并设置实时端点，我们必须启用额外的策略，并更新 SageMaker notebook 角色的信任关系。为此，请将
    `AmazonS3FullAccess`、`TextractFullAccess` 和 `ComprehendFullAccess` 策略附加到你的 Amazon
    SageMaker Notebook IAM 角色。要执行此步骤，请参阅[*第 2 章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)中[*设置
    AWS 环境*]部分的*更改 IAM 权限和信任关系以便 Amazon SageMaker notebook 执行角色*，*介绍 Amazon Textract*。
- en: Now that we have the necessary IAM roles and notebook set up in the Amazon SageMaker
    notebook instance, let's jump to the code walk-through.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在 Amazon SageMaker notebook 实例中设置了所需的 IAM 角色和 notebook，接下来让我们进入代码讲解部分。
- en: Automating documents for control and compliance
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化文档控制与符合性
- en: 'In this section, we will give a code walk-through of the architecture we discussed
    for automating documents using Amazon Textract and setting compliance and control
    with PII masking using Amazon Comprehend in *Figure 14.2* using this notebook:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过代码讲解我们讨论的架构，该架构用于通过 Amazon Textract 自动化文档处理，并通过 Amazon Comprehend
    实现符合性和控制，使用 PII 屏蔽，参考*图 14.2*，使用这个笔记本：
- en: Execute the cell under *Step 1 – Setup and install libraries* in the Jupyter
    notebook you just set up at the following link, [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb),
    to ensure that you have the libraries needed for the notebook. Note that in this
    cell, you are getting the Amazon SageMaker execution role for the notebook along
    with the SageMaker session. You are setting up boto3 libraries to call Amazon
    Textract, Amazon Comprehend, and Amazon S3 APIs. You are also using the SageMaker
    session to access the default SageMaker S3 bucket where you will be storing the
    data for this lab using a prefix or folder.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行你刚刚在以下链接设置的 Jupyter 笔记本中，*第 1 步 – 设置并安装库*下的单元格，[https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb)，以确保你拥有笔记本所需的库。请注意，在此单元格中，你正在获取笔记本的
    Amazon SageMaker 执行角色以及 SageMaker 会话。你正在设置 boto3 库，以调用 Amazon Textract、Amazon
    Comprehend 和 Amazon S3 API。你还将使用 SageMaker 会话访问默认的 SageMaker S3 存储桶，用于将数据以前缀或文件夹的方式存储到该存储桶中。
- en: 'Now, we will start with the sample bank statement. Execute the cells under
    *Step 2*, *Extracting text from a sample document* in the Jupyter notebook, to
    display the sample document to extract text and redact the PII:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从示例银行对账单开始。执行 Jupyter 笔记本中的*第 2 步*，*从示例文档中提取文本*下的单元格，显示示例文档，以提取文本并编辑 PII
    信息：
- en: '[PRE0]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will get the following response:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下响应：
- en: '![Figure 4.6 – Sample bank statement](img/B17528_04_06.jpg)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.6 – 示例银行对账单](img/B17528_04_06.jpg)'
- en: Figure 4.6 – Sample bank statement
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.6 – 示例银行对账单
- en: 'Now we will invoke Amazon Textract''s Detect Document Text Sync API, [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text),
    which extracts only text from documents in near-real time to extract data from
    the sample bank statement using the following code:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将调用 Amazon Textract 的 Detect Document Text Sync API，[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text)，它可以近实时地从文档中提取文本，用于使用以下代码从示例银行对账单中提取数据：
- en: '[PRE1]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You get a JSON response from Amazon Textract using the Detect Document Text
    Sync API.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将从 Amazon Textract 获取一个 JSON 响应，使用 Detect Document Text Sync API。
- en: 'Now we will extract text from this JSON response using the Amazon Textract
    parser library we installed in *Step 1*. Run the following code to parse the Textract
    JSON response to text:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用在*第 1 步*中安装的 Amazon Textract 解析器库从这个 JSON 响应中提取文本。运行以下代码将 Textract JSON
    响应解析为文本：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have the extracted text from the Textract JSON response, let's move
    on to the next step.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们已经从 Textract JSON 响应中提取了文本，让我们进入下一步。
- en: In this step, we will save the extracted text from the bank statement to a text/CSV
    file and upload it to Amazon S3 for processing with the Amazon Comprehend batch
    job. Run the notebook cell *Step 3*, *Save the extracted text to a text/CSV file
    and upload it to an Amazon S3 bucket*, to save the data in a text file and then
    upload it to Amazon S3.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将把从银行对账单中提取的文本保存到一个文本/CSV 文件，并上传到 Amazon S3，以便使用 Amazon Comprehend 批量作业进行处理。运行笔记本单元格*第
    3 步*，*将提取的文本保存到文本/CSV 文件并上传到 Amazon S3 存储桶*，以将数据保存到文本文件中，然后上传到 Amazon S3。
- en: 'Now that we have extracted the text from bank statements, converted it into
    a text file, and uploaded it to Amazon S3, in this step, we will detect PII from
    the text using Amazon Comprehend Detect PII Sync APIs. Run the notebook cell *Step
    4*, *Check for PII using the Amazon Comprehend Detect PII Sync API*, to call the
    Comprehend APIs by passing the extracted text from Amazon Textract:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经从银行对账单中提取了文本，将其转换为文本文件并上传到 Amazon S3，在此步骤中，我们将使用 Amazon Comprehend Detect
    PII Sync API 检测文本中的 PII。运行笔记本单元格*步骤 4*，*使用 Amazon Comprehend Detect PII Sync API
    检查 PII*，通过传递来自 Amazon Textract 提取的文本来调用 Comprehend API：
- en: 'a) First, initialize the boto3 handle for Amazon Comprehend:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 首先，初始化 Amazon Comprehend 的 boto3 句柄：
- en: '[PRE3]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'b) Then, call Amazon Comprehend and pass it the aggregated text from our sample
    bank statement image to Comprehend detect PII entities: [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities):'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) 然后，调用 Amazon Comprehend，并将我们示例银行对账单图像中聚合的文本传递给 Comprehend，以检测 PII 实体：[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities):'
- en: '[PRE4]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You will get a response with identifying PII from the text, which will be redacted
    in the next step using the Amazon Comprehend PII analysis job.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将收到一个响应，识别文本中的 PII，并将在下一步中使用 Amazon Comprehend PII 分析任务删除这些内容。
- en: '![Figure 4.7 – PII detection using Amazon Comprehend in a bank statement](img/B17528_04_07.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.7 – 使用 Amazon Comprehend 在银行对账单中进行 PII 检测](img/B17528_04_07.jpg)'
- en: Figure 4.7 – PII detection using Amazon Comprehend in a bank statement
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.7 – 使用 Amazon Comprehend 在银行对账单中进行 PII 检测
- en: We will mask/redact these 15 PII entities we found in the sample bank statement.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将对在示例银行对账单中找到的 15 个个人身份信息（PII）实体进行屏蔽/删除。
- en: 'Next, we will call the `StartPiiEntitiesDetectionJob` API to start an asynchronous
    PII entity detection job for a collection of documents. For this example, we are
    just using one document sample. You can redact a large number of documents using
    this job. Run the notebook cell *Step 5*, *Mask PII using the Amazon Comprehend
    PII Analysis Job*, to set up and start the PII redaction analysis job with Amazon
    Comprehend:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将调用 `StartPiiEntitiesDetectionJob` API 来启动一个异步的 PII 实体检测任务，适用于一组文档。在本示例中，我们只使用一个文档样本。您可以使用此任务删除大量文档。运行笔记本单元格*步骤
    5*，*使用 Amazon Comprehend PII 分析任务屏蔽 PII*，以设置并启动 Amazon Comprehend 的 PII 删除分析任务：
- en: 'a) Then job requires the S3 location of documents to be redacted and the S3
    location of where you want the redacted output. Run the following cell to specify
    the location of the S3 text file we want to be redacted:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 然后，任务需要指定待屏蔽文档的 S3 位置以及您希望输出已屏蔽内容的 S3 位置。运行以下单元格，指定我们希望屏蔽的 S3 文本文件位置：
- en: '[PRE5]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'b) Now we will call `comprehend.start_pii_entities_detection_job` by setting
    parameters for redaction and passing the input S3 location where data is stored
    by running the following notebook cell:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 现在我们将通过设置删除参数并传递数据存储的输入 S3 位置来调用 `comprehend.start_pii_entities_detection_job`，运行以下笔记本单元格：
- en: '[PRE6]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Refer to the API documentation for more details: [https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html).'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参考 API 文档：[https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html)。
- en: 'c) The job will take roughly 6-7 minutes. The following code is to check the
    status of the job. The cell execution will be completed once the job is complete:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 该任务大约需要 6-7 分钟。以下代码用于检查任务的状态。一旦任务完成，单元格执行也会完成：
- en: '[PRE7]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You will get a JSON response, and this job will take 5-6 minutes. You can go
    and grab a coffee until the notebook cell is running and you have a response.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将收到一个 JSON 响应，且该任务大约需要 5-6 分钟。在此期间，您可以去喝杯咖啡，直到笔记本单元格运行并返回响应。
- en: 'Once the job is successful, we will now show you the extracted, redacted document
    output in this step. Run the notebook cell *Step 6*, *View the redacted/masked
    output in the Amazon S3 bucket*, to extract the output from the Amazon S3 bucket:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦任务成功，我们将在此步骤中向您展示提取的、已屏蔽的文档输出。运行笔记本单元格*步骤 6*，*查看 Amazon S3 存储桶中的已屏蔽/已删除输出*，以从
    Amazon S3 存储桶中提取输出：
- en: '[PRE8]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will get the following redacted bank statement:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将获得以下已屏蔽的银行对账单：
- en: '![Figure 4.8 – Redacted bank statement using the Amazon Comprehend PII Redaction
    job'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – 使用 Amazon Comprehend PII 删除任务屏蔽的银行对账单](img/B17528_04_08.jpg)'
- en: '](img/B17528_04_08.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_04_08.jpg)'
- en: Figure 4.8 – Redacted bank statement using the Amazon Comprehend PII Redaction
    job
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 使用 Amazon Comprehend PII Redaction 任务对银行对账单进行脱敏处理
- en: In the output, you can see that the Amazon Comprehend PII job has masked the
    PII data, such as an address, name, SSN, and bank account number identified using
    the Amazon Comprehend Detect PII entity.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，你可以看到 Amazon Comprehend PII 任务已经对 PII 数据（如地址、姓名、社会保障号码和银行账号）进行了脱敏处理，这些数据是通过
    Amazon Comprehend Detect PII 实体识别出来的。
- en: In this section, we walked you through an end-to-end conceptual architecture
    for automating documents for compliance and control. In the next section, we will
    talk about best practices for real-time document processing workflows versus batch
    processing workflows.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们为你展示了一个端到端的概念性架构，旨在实现文档的合规性和控制自动化。在下一节中，我们将讨论实时文档处理工作流与批量文档处理工作流的最佳实践。
- en: Processing real-time document workflows versus batch document workflows
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理实时文档工作流与批量文档工作流
- en: In this section, we will talk about some best practices while architecting solutions
    using Amazon Textract for real-time workflows versus batch processing document
    workflows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在使用 Amazon Textract 构建实时工作流与批量处理文档工作流的解决方案时的一些最佳实践。
- en: 'Let''s compare the Textract real-time APIs against the batch APIs we discussed
    in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing
    Amazon Textract*, with the help of the following table:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 Textract 的实时 API 与我们在 [*第二章*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027)
    中讨论的批量 API 做一个对比，*介绍 Amazon Textract*，通过以下表格帮助理解：
- en: '![Figure 4.9 – Textract sync APIs versus batch APIs](img/B17528_04_09.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – Textract 同步 API 与批量 API 对比](img/B17528_04_09.jpg)'
- en: Figure 4.9 – Textract sync APIs versus batch APIs
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – Textract 同步 API 与批量 API 对比
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The pricing of Textract is based on which of the three different APIs you are
    going to use out of Analyze Document (forms, table), Detect Text (text extraction),
    and Analyze Expense (invoices and receipts). You will not be charged irrespective
    of whether you use the sync or async (batch) implementation of these, so, feel
    free to design your architecture based on your need for real-time processing versus
    batch processing as pricing is based on the number of documents processed with
    one of the three APIs, irrespective of batch or real-time mode. Check prices here:
    [https://aws.amazon.com/textract/pricing/](https://aws.amazon.com/textract/pricing/).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Textract 的定价是基于你将使用的三种不同 API 中的哪一种，分别是 Analyze Document（表单、表格）、Detect Text（文本提取）和
    Analyze Expense（发票和收据）。无论你使用的是同步（sync）还是异步（async，批量）实现，都不会收取额外费用。因此，可以根据是否需要实时处理与批量处理来设计架构，因为定价是基于处理的文档数量，而与批量或实时模式无关。查看价格详情请访问：[https://aws.amazon.com/textract/pricing/](https://aws.amazon.com/textract/pricing/)。
- en: For example, LiveRight pvt Ltd*.* can use the batch or real-time implementation
    of the detect text API to detect text from their bank statements to process millions
    of documents.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，LiveRight pvt Ltd*.* 可以使用 Detect Text API 的批量或实时实现来从其银行对账单中提取文本，处理数百万份文档。
- en: 'We covered architecture in *Figure 14.2*. This architecture implemented the
    Amazon Textract Detect Text Sync API in the code walk-through. Now, let''s see
    how we can automate the architecture through Lambda functions for scale to process
    multiple documents:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *图 14.2* 中介绍了架构。该架构实现了 Amazon Textract Detect Text Sync API 的代码演示。现在，让我们看看如何通过
    Lambda 函数实现架构自动化，以便扩展处理多个文档：
- en: '![Figure 4.10 – Synchronous document processing workflow](img/B17528_04_10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 同步文档处理工作流](img/B17528_04_10.jpg)'
- en: Figure 4.10 – Synchronous document processing workflow
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 同步文档处理工作流
- en: 'In the preceding architecture, we walked you through how you can process scanned
    images using the proposed synchronous document processing workflow using the sync
    APIs of Amazon Textract. Here are the steps for this architecture:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述架构中，我们演示了如何使用 Amazon Textract 的同步 API，通过同步文档处理工作流处理扫描图像。以下是该架构的步骤：
- en: Documents uploaded to Amazon S3 will send a message to an Amazon SQS queue to
    analyze a document. Amazon SQS is a serverless managed queuing service that polls
    the documents into the queue.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传到 Amazon S3 的文档会向 Amazon SQS 队列发送消息以进行文档分析。Amazon SQS 是一种无服务器的托管队列服务，它会将文档轮询到队列中。
- en: A Lambda function is invoked synchronously with an event that contains a queue
    message.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Lambda 函数会通过包含队列消息的事件同步调用。
- en: The Lambda function then calls Amazon Textract sync APIs and stores the Textract
    output or response in either Amazon S3 or response metadata in the Amazon DynamoDB
    table. Amazon DynamoDB is a NoSQL database managed by AWS that is like a key/value
    store.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，Lambda 函数调用 Amazon Textract 同步 API，并将 Textract 输出或响应存储在 Amazon S3 或 Amazon
    DynamoDB 表中的响应元数据里。Amazon DynamoDB 是一个由 AWS 管理的 NoSQL 数据库，类似于键值存储。
- en: You control the throughput of your pipeline by controlling the batch size and
    Lambda concurrency.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过控制批处理大小和 Lambda 并发性来控制管道的吞吐量。
- en: 'Now we will walk you through the following architecture best practices for
    scaling multi-page scanned documents, which can be PDF or images using batch APIs
    of Amazon Textract:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将向您展示以下架构最佳实践，帮助您扩展多页扫描文档（这些文档可以是 PDF 或图片），并使用 Amazon Textract 的批处理 API
    进行处理：
- en: '![Figure 4.11 – Batch document processing workflow'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11 – 批量文档处理工作流程](img/B17528_04_11.jpg)'
- en: '](img/B17528_04_11.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17528_04_11.jpg)'
- en: Figure 4.11 – Batch document processing workflow
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 批量文档处理工作流程
- en: 'In the preceding diagram, we have an architecture to walk through how batch
    processing workflow works with Amazon Textract batch jobs:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图表中，我们展示了一个架构，展示了 Amazon Textract 批处理作业如何工作：
- en: Multipage PDFs and images are uploaded in Amazon S3\. These documents are sent
    to the **Amazon Simple Queue Service** (**SQS**) queue.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多页 PDF 和图片上传到 Amazon S3。这些文档被发送到 **Amazon 简单队列服务**（**SQS**）队列。
- en: A job scheduler Lambda function runs at a certain frequency, for example, every
    5 minutes, and polls for messages in the SQS queue.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个作业调度器 Lambda 函数以一定的频率运行，例如每 5 分钟一次，并轮询 SQS 队列中的消息。
- en: For each message in the queue, it submits an Amazon Textract job to process
    the document and continues submitting these jobs until it reaches the maximum
    limit of concurrent jobs in your AWS account.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于队列中的每条消息，它会提交一个 Amazon Textract 作业来处理文档，并继续提交这些作业，直到达到 AWS 账户中并发作业的最大限制。
- en: As Amazon Textract finishes processing a document, it sends a completion notification
    to an **Amazon Simple Notification Service** (**SNS**) topic.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 Amazon Textract 完成文档处理时，它会向 **Amazon 简单通知服务**（**SNS**）主题发送一个完成通知。
- en: SNS then triggers the job scheduler Lambda function to start the next set of
    Amazon Textract jobs.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SNS 然后触发作业调度器 Lambda 函数以启动下一组 Amazon Textract 作业。
- en: SNS also sends a message to an SQS queue, which is then processed by a Lambda
    function to get results from Amazon Textract. The results are then stored in a
    relevant dataset, for example, DynamoDB or Amazon S3.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SNS 还会向 SQS 队列发送一条消息，然后由 Lambda 函数处理，以获取 Amazon Textract 的结果。结果会被存储在相关数据集中，例如
    DynamoDB 或 Amazon S3。
- en: This GitHub link, [https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing](https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing),
    has code samples to implement both the suggested architecture and it also has
    some additional components to backfill in case the documents already exist in
    the Amazon S3 bucket. Please feel free to set up and use this if you have large
    documents to experiment with.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 GitHub 链接，[https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing](https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing)，包含了实现建议架构的代码示例，并且还有一些额外的组件，用于填充那些已经存在于
    Amazon S3 存储桶中的文档。如果您有大文档进行实验，请随时设置并使用这个解决方案。
- en: You can also use the following GitHub solution, [https://github.com/aws-samples/amazon-textract-textractor](https://github.com/aws-samples/amazon-textract-textractor),
    to implement large-scale document processing with Amazon Comprehend insights.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下 GitHub 解决方案，[https://github.com/aws-samples/amazon-textract-textractor](https://github.com/aws-samples/amazon-textract-textractor)，通过
    Amazon Comprehend 提供的洞察实现大规模文档处理。
- en: In this section, we covered architecture best practices for using real-time
    processing or batch processing with Amazon Textract. We also presented some already-existing
    GitHub implementations for large-scale document processing with Amazon Textract.
    Now, let's summarize what we have covered in this chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了使用实时处理或批处理与 Amazon Textract 的架构最佳实践。我们还展示了已经存在的一些 GitHub 实现，用于大规模文档处理。现在，让我们总结一下本章所涵盖的内容。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered how you can use Amazon Textract to automate your
    existing documents. We introduced a fictional bank use case with the help of *LiveRight
    Pvt Ltd*. We showed you how using an architecture can help banks automate their
    loan origination process and set up compliance and control with Amazon Comprehend.
    We also covered code samples using a sample bank statement, and how you can extract
    data from the scanned bank statement and save it into a `CSV.text` file in Amazon
    S3 for further analysis. Then, we showed you how you can use Amazon Comprehend
    to detect PII using a sync API and how you can redact that sample bank data text/CSV
    in Amazon S3 using an Amazon Comprehend batch PII redaction job.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何使用 Amazon Textract 自动化您现有的文档。我们通过 *LiveRight Pvt Ltd* 提供的一个虚构银行用例展示了如何使用架构帮助银行自动化其贷款起源流程，并通过
    Amazon Comprehend 设置合规性和控制。我们还介绍了使用样本银行对账单的代码示例，以及如何从扫描的银行对账单中提取数据并将其保存到 Amazon
    S3 中的 `CSV.text` 文件进行进一步分析。然后，我们展示了如何使用 Amazon Comprehend 使用同步 API 检测 PII，并如何使用
    Amazon Comprehend 批处理 PII 擦除作业在 Amazon S3 中擦除该样本银行数据文本/CSV。
- en: We then covered some architecture patterns for using real-time processing document
    workflows versus batch processing workflows. We also provided some GitHub implementations
    that can be used to process large-scale documents.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后涵盖了一些关于使用实时处理文档工作流与批处理工作流的架构模式。我们还提供了一些 GitHub 实现，可用于处理大规模文档。
- en: In this chapter, you learned the differences between when to use and how to
    use real-time APIs versus batch APIs for document automation. You also learned
    how you can set up PII redaction with Amazon Comprehend PII jobs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了何时以及如何使用实时 API 与批处理 API 来进行文档自动化的区别。您还学习了如何使用 Amazon Comprehend PII
    作业设置 PII 擦除。
- en: In the next chapter, we will look at a different use case, but one that's equally
    popular among enterprises looking to leverage NLP to maximize their business value
    by building smart search indexes. We will cover how you can use Amazon Textract
    and Amazon Comprehend along with Amazon Elasticsearch and Amazon Kendra to create
    a quick NLP-based search. We will introduce the use case, discuss how to design
    the architecture, establish the prerequisites, and walk through in detail the
    various steps required to build the solution.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一个不同的用例，但它同样受企业欢迎，企业希望利用 NLP 极大化其业务价值，通过构建智能搜索索引。我们将介绍如何使用 Amazon
    Textract 和 Amazon Comprehend 以及 Amazon Elasticsearch 和 Amazon Kendra 来创建快速基于 NLP
    的搜索。我们将介绍用例，讨论如何设计架构，建立先决条件，并详细讲解构建解决方案所需的各个步骤。
- en: Further reading
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Building a serverless document scanner using Amazon Textract and AWS Amplify*,
    by Moheeb Zara ([https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/](https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/))'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Amazon Textract 和 AWS Amplify 构建无服务器文档扫描仪*，作者 Moheeb Zara ([https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/](https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/))'
- en: '*Automatically extract text and structured data from documents with Amazon
    Textract*, by Kashif Imran and Martin Schade ([https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/](https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/))'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Amazon Textract 从文档中自动提取文本和结构化数据*，作者 Kashif Imran 和 Martin Schade ([https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/](https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/))'
