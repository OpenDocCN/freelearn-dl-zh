- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Improving the Emotional Intelligence Deficiencies of Chatbots
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升聊天机器人的情感智能缺陷
- en: Emotions remain irrational and subjective. AI algorithms never forsake rationality
    and objectivity. Cognitive dissonance ensues, which complicates the task of producing
    an efficient chatbot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 情感依然是非理性和主观的。AI算法从不放弃理性和客观性。由此产生的认知失调，使得生产一个高效的聊天机器人变得更加复杂。
- en: In *Chapter 14*, *Preparing the Input of Chatbots with Restricted Boltzmann
    Machines (RBMs) and Principal Component Analysis (PCA)*, we built a rational chained
    algorithm process with an RBM and a PCA approach. From there, we extracted critical
    objective data on a market segment. From that market segment and its features,
    we then designed a dialog in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI
    Chatbot*. The dialog was rational, and we produced a probable choice of services
    for the user. We did this out of good faith, to make the path from a request to
    its outcome as short as possible. It was a *yes* path in which everything went
    smoothly.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第十四章*，*使用限制玻尔兹曼机（RBM）和主成分分析（PCA）准备聊天机器人输入*中，我们构建了一个理性的链式算法流程，结合了RBM和PCA方法。由此，我们提取了一个市场细分的关键客观数据。根据这个市场细分及其特征，我们设计了*第十五章*，*设置认知NLP
    UI/CUI聊天机器人*中的对话。该对话是理性的，我们为用户提供了一个可能的服务选择。我们是出于善意来做这一切，希望能尽可能缩短从请求到结果的路径。这是一个*是*的路径，一切顺利进行。
- en: In this chapter, we will confront human nature with unexpected reactions. A
    *no* path will challenge our dialog. One of the problems we face resides in emotional
    polysemy, confusing emotional signals from a user.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将面对人性带来的意外反应。*否*的路径将挑战我们的对话。我们面临的问题之一是情感的多义性，用户的情感信号容易混淆。
- en: We will address the *no* unexpected path with information drawn from *Chapter
    14*, *Preparing the Input of Chatbots with Restricted Boltzmann Machines (RBM)
    and Principal Component Analysis (PCA)* and *Chapter 15*, *Setting up a Cognitive
    NLP UI/CUI Chatbot*, and go into the world of data logging.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过从*第十四章*，*使用限制玻尔兹曼机（RBM）和主成分分析（PCA）准备聊天机器人输入*和*第十五章*，*设置认知NLP UI/CUI聊天机器人*中获取的信息，来应对*否*的意外路径，并进入数据记录的世界。
- en: Data logging will provide critical contextual data to satisfy the user. The
    goal will be to create emotions, not just react randomly to a user's emotional
    state.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据记录将提供关键的背景数据，以满足用户需求。目标将是创造情感，而不仅仅是随机地对用户的情感状态做出反应。
- en: Finally, we will open the door to researching ways to generate text automatically
    through RNN-LSTM approaches. The idea will be to create automatic dialogs in the future
    based on data logging.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将开启通过RNN-LSTM方法自动生成文本的研究之门。这个想法是基于数据记录，未来创造自动对话。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Emotional polysemy
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感多义性
- en: Small talk
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 闲聊
- en: Data logging
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据记录
- en: Creating emotions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创造情感
- en: Exploring RNN-LSTM approaches
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索RNN-LSTM方法
- en: We will first explore the difference between simply reacting to emotions and
    creating emotions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨简单地对情感做出反应与创造情感之间的区别。
- en: From reacting to emotions, to creating emotions
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从对情感的反应，到情感的创造
- en: Designing a chatbot that reacts to what a user expresses is one thing. But creating
    emotions during a dialog like a human does requires deeper understanding of how
    a chatbot manages emotions. Let's start with emotional polysemy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个能够对用户表达做出反应的聊天机器人是一回事。但像人类一样在对话中创造情感则需要更深入地理解聊天机器人如何管理情感。让我们从情感的多义性开始。
- en: Solving the problems of emotional polysemy
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决情感多义性问题
- en: We will be enhancing the emotional intelligence of a chatbot starting by addressing
    the issue of emotional polysemy. We are used to defining polysemy with words,
    not emotions, in the sense that polysemy is the capacity of a word to have multiple
    meanings. In *Chapter 6*, *How to Use Decision Trees to Enhance K-Means Clustering*,
    we explored the confusion that arose with the word "coach." "Coach" can mean a
    bus or a sports trainer, which leads to English to French translation issues.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过解决情感的多义性问题来增强聊天机器人的情感智能。我们习惯于用词语来定义多义性，而不是情感，因为多义性是指一个词可以有多个含义。在*第六章*，*如何使用决策树增强K均值聚类*中，我们探讨了“coach”这个词带来的困惑。“Coach”可以指巴士或体育教练，这导致了英语到法语翻译的问题。
- en: 'Polysemy also applies to the interpretation of emotions by artificial intelligence.
    We will explore this domain with two examples: greetings and affirmations.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多义性同样适用于人工智能对情感的解读。我们将通过两个例子来探索这一领域：问候语和肯定语。
- en: Then we will go through the speech recognition and facial analysis as silver
    bullet solutions fallacies that mislead us into thinking it's easy to read emotions
    on faces.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将深入探讨语音识别和面部分析作为“灵丹妙药”解决方案的谬误，这些谬误误导我们认为面部表情的情感解读很简单。
- en: The greetings problem example
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问候问题示例
- en: To implement this chapter, open Dialogflow and go to the agent named `cogfilm+<your
    unique ID>` created in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一章节，请打开Dialogflow，进入在*第15章*，*设置认知NLP UI/CUI聊天机器人*中创建的名为`cogfilm+<你的唯一ID>`的代理。
- en: Suppose somebody types "Hi" to a chatbot agent. Almost everybody will think
    that this is a good beginning. But is it really?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某人向聊天机器人发送"嗨"。几乎每个人都会认为这是一个好的开始。但真的如此吗？
- en: 'Let''s explore some of the many possible interpretations:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一些可能的解释：
- en: '**"Hi" meaning the person is very tense and irritated**: This could be the
    case of a top manager who never uses "Hi" to say hello, does not like chatbots,
    or doubts that the one that is being tested is worth anything at all.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"嗨"意味着这个人非常紧张和烦躁**：这可能是一个高层经理，从不使用"嗨"来打招呼，不喜欢聊天机器人，或者怀疑正在测试的这个机器人是否有任何价值。'
- en: This manager usually says, "Good morning," "Good afternoon," and "Good evening."
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个经理通常会说："早上好"、"下午好"和"晚上好"。
- en: '**"Hi" meaning "So what?"**: It is more like, "Yeah, hi." This could be a person,
    P, who dislikes person Q who just said good morning to P.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"嗨"意味着"那又怎样？"**：这更像是"嗯，嗨"。这可能是一个人P，讨厌刚刚对他说"早安"的Q。'
- en: '**"Hi," meaning "I''m in trouble."**: This could be a usually chirpy, happy
    person who says, "Hello, everyone. How are things going today?" But today, it''s
    just a brief "Hi." This will trigger alert reactions from others, such as "Are you
    okay?," "Is something wrong?"'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"嗨"，意味着"我有麻烦了"**：这可能是一个通常很开朗、快乐的人，他们说："大家好，今天一切还好吗？" 但是今天，他们只是简单地说了一个"嗨"。这会引起其他人的警觉反应，比如"你还好吗？"
    "发生了什么事吗？"'
- en: '**"Hi," meaning "I''m trying to be nice."**: This could be a person that is
    usually grumpy in the morning and just sits down and stares down a laptop until
    the caffeine in their coffee kicks in. But today, this person comes in totally
    in shape, wide awake, and says, "Hi." This might trigger alert reactions from others
    such as, "Somebody had a great evening or night! Am I wrong?", with some laughter
    from the others.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"嗨"，意味着"我在试图友好"**：这可能是一个通常早上脾气暴躁、坐下来盯着笔记本直到咖啡因起作用的人。但今天，这个人完全精神饱满，清醒过来并说："嗨"。这可能引发其他人的警觉反应，比如"某人昨晚过得很愉快！我错了吗？"并伴随着一些笑声。'
- en: I could go on with literally hundreds of other situations and uses of "Hi."
    Why? Because humans have an indefinite number of behaviors that can be reflected
    in that first "Hi" in an encounter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以继续列举成百上千种其他"Hi"的使用情况。为什么？因为人类有无数的行为可以反映在与他人初次见面时的那个"Hi"中。
- en: This could apply to ending a conversation without saying "bye" or saying it
    in many ways. The way a person says goodbye to another person in the morning can have
    an incredible number of significations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于没有说"再见"或以多种方式说再见的情况。一个人在早晨和别人告别的方式可以有非常多的含义。
- en: This is therefore one of our challenges. Before we go further with this, let's
    look at one more challenge by considering the affirmation example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这也是我们面临的挑战之一。在进一步探讨之前，让我们再看一个挑战，考虑一下确认示例。
- en: The affirmation example
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确认示例
- en: Suppose somebody types or says "Yes" in a chatbot. Does that really mean "Yes"?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某人向聊天机器人发送或说"是的"。这真的是"是的"吗？
- en: '**"Yes", as in "Yeah, whatever."**: The user hates the chatbot. The dialog
    is boring. The user is thinking that if they do not say "Yes" and get it over
    with, this dialog will go on forever.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"是的"，就像"随便吧"**：用户讨厌这个聊天机器人。对话很无聊。用户想，如果他们不说"是的"然后赶快结束，这段对话会一直持续下去。'
- en: '**"Yes", as in "I''m afraid to say no."**: The user does not want to say "Yes."
    The question could be, "Are you satisfied with your job?" The user could fear the
    answers are logged and monitored. The user fears sanctions. Although this person
    hates their job, the answer will be "Yes" or might even be "I sure do!"'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"是的"，就像"我不敢说不一样"**：用户并不想说"是的"。问题可能是："你对你的工作满意吗？" 用户可能担心答案被记录和监控。用户害怕受到制裁。尽管这个人讨厌他们的工作，回答仍然会是"是的"，或者甚至可能是"我当然喜欢！"'
- en: '**"Yes" as a good faith "yes" that a person regrets right after**: A person
    says "Yes" to a purchase, stimulated by the ad pressure at that moment in the
    chatbot. But minutes later, the same person thinks, "Why did I say yes and buy
    that?" Therefore, some platforms allow refunds even before the product or service
    is delivered.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“Yes”作为一个善意的“是”，但在之后让人后悔**：一个人对购买说“是”，是因为聊天机器人中广告的压力。但几分钟后，那个同样的人会想：“我怎么会说是并买了这个？”因此，一些平台允许在产品或服务交付之前就进行退款。'
- en: Just as for "Hi," I could list hundreds of situations of emotional polysemy
    with "Yes."
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就像“Hi”一样，我可以列出数百个情感多义性的情况，涉及“Yes”。
- en: Now, that we have understood the challenge at hand, let's explore the silver
    bullet fallacies mentioned previously.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经理解了眼前的挑战，让我们来探讨一下之前提到的银弹谬论。
- en: The speech recognition fallacy
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语音识别的谬误
- en: Many editors and developers believe that speech recognition will solve the problem
    of emotional intelligence by detecting the tone of a voice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 许多编辑和开发者相信，语音识别通过检测语音的语气可以解决情感智能的问题。
- en: However, emotional polysemy applies to the tone of voice, as well. Human beings
    tend to hide their emotions when they feel threatened, and open up when they trust
    their environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，情感多义性同样适用于语气。人类在感到威胁时往往会隐藏情绪，而在信任环境时则会敞开心扉。
- en: Let's go back to the "Hi" and "Yes" examples.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到“Hi”和“Yes”的例子。
- en: '**"Hi" in a chirpy tone**: A person, X, comes into an office, for example.
    Somebody says, "Oh, hi there! Great to see you!" Person Y answers "Hi" in a very
    happy tone. Google Home or Amazon Alexa, in their research lab, produces 0.9 probability
    that the conversation is going well.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**用兴奋的语气说“Hi”**：例如，一个人X进入办公室。有人说：“哦，嗨！很高兴见到你！”人Y用非常开心的语气回答“Hi”。谷歌Home或亚马逊Alexa在他们的研究实验室中会生成0.9的概率，认为对话进行得很顺利。'
- en: This could be true. Or it could be false.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是真的，也可能是假的。
- en: For example, person Y hates person X. Person X knows it and says, "Great to
    see you!" on purpose. Person Y knows that person X knows that they hate each other
    but won't give in to bursting out first. So person "Y" answers "Hi" in a super-happy
    tone.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Y讨厌X。X知道这一点并故意说：“很高兴见到你！”Y知道X知道他们彼此讨厌，但不会先爆发情绪。所以Y用非常开心的语气回答“Hi”。
- en: At that point, many turn to facial analysis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在那时，许多人转向面部分析。
- en: The facial analysis fallacy
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面部分析的谬误
- en: Emotional polysemy also applies to facial analysis. Many think that deep learning
    facial analysis will solve the polysemy problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 情感多义性也适用于面部分析。许多人认为深度学习面部分析可以解决多义性的问题。
- en: I saw a post recently by a developer with a picture of an obviously forced smile
    with the text stating that happiness could be detected with DL facial analysis!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近看到一篇开发者的帖子，帖子里有一张显然是强迫微笑的照片，配文说幸福感可以通过深度学习面部分析来检测！
- en: 'Let''s take two basic facial expressions and explore them: a smile and a frown.
    By now, you know that emotional polysemy will apply to both cases.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看两种基本的面部表情并探讨它们：微笑和皱眉。到现在为止，你应该知道情感多义性同样适用于这两种情况。
- en: A smile
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一种微笑
- en: If somebody smiles and a DL facial analysis algorithm detects that smile, it
    means the person is happy. Is this true? Maybe. Maybe not.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人微笑，并且深度学习（DL）面部分析算法检测到这个微笑，意味着这个人很高兴。这是真的吗？也许是真的，也许不是。
- en: Maybe the person is happy. Maybe the smile is ironic, meaning "Yeah, sure, dream
    if you want, but I don't agree!" It could mean "Get out of my way," or, "I'm happy
    because I'm going to hurt you," or, "I'm happy to see you." Who knows?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这个人真的很高兴。也许这个微笑是讽刺的，意味着“是的，当然，梦想一下，如果你愿意，但我不同意！”它也可能意味着“给我让开”或者“我很高兴因为我要伤害你”或“我很高兴见到你”。谁知道呢？
- en: The truth is that nobody knows, and sometimes even the person that smiles doesn't
    know. Sometimes a person will think, "Why did I smile at her/him? I hate her/him!"
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，没人知道，有时甚至连微笑的人自己也不知道。有时一个人会想：“我为什么要对她/他微笑？我讨厌她/他！”
- en: A frown
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一种皱眉
- en: If somebody frowns and a DL facial analysis algorithm detects that frown, it
    means the person is sad or unhappy. Is that true? Maybe. Maybe not.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人皱起眉头，并且深度学习面部分析算法检测到这个皱眉，这意味着这个人感到悲伤或不高兴。这是真的吗？也许是真的，也许不是。
- en: Maybe the person is happy that day. Things are going smoothly, and the person
    just forgot a book, for example, at home before coming to this location. Maybe
    the second after the person will smile, thinking, "So what? It's a great day and
    I don't care!"
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这个人那天很高兴，事情进展顺利，而这个人可能只是忘记了带一本书，比如，来到这个地点之前把书忘在了家里。也许就在下一秒，这个人会微笑，心想：“那又怎么样？今天真是个好日子，我根本不在乎！”
- en: Maybe the person is unhappy. Maybe the person is having a great time watching
    some kind of ball game, and their favorite player missed something. The second
    after, the person thinks "Oh, so what? My team is winning anyway," and smiles.
    Some people just frown if they're thinking hard, but it doesn't mean they're unhappy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这个人不高兴。也许这个人正在愉快地看某种球赛，而他们最喜欢的球员错过了某个机会。紧接着，这个人想：“哦，那又怎样？反正我的队伍在赢，”然后笑了起来。有些人在思考时会皱眉，但这并不意味着他们不高兴。
- en: We can now see that there are thousands of cases of emotional polysemy that
    occur with words, tone of voice, and facial expressions, and therefore there is
    no magical solution that is going to suddenly overcome the inherent difficulty
    that AI have when it comes to interpreting people's emotions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，词语、语调和面部表情上存在成千上万种情感多义性的案例，因此没有什么神奇的解决方案可以突然克服人工智能在解读人类情感时所面临的固有困难。
- en: We will now explore some realistic solutions to this problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨一些现实的解决方案。
- en: Small talk
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小对话
- en: Small talk is not a silver bullet to solve the emotional intelligence problem
    of chatbots at all. In fact, even without speaking about chatbots, we all suffer
    from emotional distress in one situation or another. Small talk adds little unimportant
    phrases to a dialog such as "wow," "cool," "oops," "great," and more.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 小对话根本不是解决聊天机器人情商问题的灵丹妙药。事实上，即使不谈论聊天机器人，我们在某种情况下都会遭遇情感困扰。小对话只会在对话中加入一些无关紧要的短语，比如“哇”，“酷”，“哎呀”，“太棒了”等。
- en: '*We do not need to seek perfection, but show goodwill*. Every human knows the
    difficulty of emotional intelligence and polysemy. A human can accept an error
    in a dialog if goodwill is shown by the other party to make up for that error.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们不需要追求完美，而是要展示善意*。每个人都知道情商和多义性的问题。有时候，如果对方在对话中展现了善意来弥补错误，人类是可以接受对话中的错误的。'
- en: Small talk is a little step in making amends to show goodwill.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 小对话是弥补过错的一小步，展示了善意。
- en: 'To achieve the "making customers happy" purpose, scroll down the main menu
    to **Small Talk**, click on that option, and enable it, as shown in the following
    screenshot. We will be focusing on **Courtesy** and **Emotions**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现“让顾客开心”的目标，请向下滚动主菜单到**小对话**，点击该选项并启用，如下图所示。我们将专注于**礼貌**和**情感**：
- en: '![](img/B15438_16_01.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_01.png)'
- en: 'Figure 16.1: Small Talk in menu'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1：菜单中的小对话
- en: Courtesy
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 礼貌
- en: Courtesy will help make a conversation smoother when things go wrong. Emotional
    intelligence is not answering 100% correctly every time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 礼貌有助于在出现问题时让对话更加顺畅。情商并不是每次都能100%正确地回答。
- en: '*Emotional intelligence (EI) is adjusting to an environment, correcting a mistake
    made, and trying to ease the tension at all times.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*情商（EI）是适应环境、纠正错误，并始终努力缓解紧张情绪。*'
- en: 'First, click on **Enable**, which will trigger small talk responses during
    a dialog:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，点击**启用**，这将在对话过程中触发小对话回应：
- en: '![](img/B15438_16_02.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_02.png)'
- en: 'Figure 16.2: Enabling Small Talk'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2：启用小对话
- en: 'You will notice that the **Courtesy** progress bar is at 0%. We need to increase
    EI:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到**礼貌**的进度条显示为0%。我们需要提升情商：
- en: '![](img/B15438_16_03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_03.png)'
- en: 'Figure 16.3: Small talk themes'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3：小对话主题
- en: 'We will carefully answer the first possible "question" a user can ask or a
    phrase they might express: **That''s bad.**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仔细回答用户可能提出的第一个“问题”或他们可能表达的短语：**这很糟糕。**
- en: We are in trouble here! This is the worst-case scenario. We are going to have
    to work hard to make up for this.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在遇到麻烦了！这是最糟糕的情况。我们得努力弥补这个问题。
- en: Emotional polysemy makes the situation extremely difficult to deal with. The
    one thing we do not want to do is to pretend our bot is intelligent.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 情感多义性使得情形变得极其难以处理。我们最不想做的就是假装我们的机器人是聪明的。
- en: 'I would recommend two courses of action:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议采取两种行动方案：
- en: 'First, answer carefully, saying that we need to investigate this with something
    like:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，仔细回答，表示我们需要进一步调查，可以这样说：
- en: '*I am very sorry about this. Could you please describe why it''s bad? We will
    regularly check our history log and try to improve all the time. You can also
    send us an email at <your customer service email address>. We will answer as soon
    as possible.*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*对此我非常抱歉。您能否请描述一下为什么这很糟糕？我们会定期检查我们的历史日志，并不断改进。您也可以通过<您的客户服务邮箱>给我们发邮件，我们会尽快回复。*'
- en: 'You can enter this answer as follows and click on the **SAVE** button:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按以下方式输入这个回答并点击**保存**按钮：
- en: '![](img/B15438_16_04.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_04.png)'
- en: 'Figure 16.4: Courtesy'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：礼貌
- en: You will notice the **Courtesy** progress bar has jumped up to 17%. We have
    covered a critical area of a dialog. Default answers are provided when we don't
    fill everything in, but they are random, which makes it better to enter your own
    phrases if you activate this function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，**礼貌**进度条已跳至17%。我们已经覆盖了对话中的一个关键领域。当我们没有填充完整时，默认答案会被提供，但它们是随机的，因此，如果你启用此功能，最好输入自己的短语。
- en: 'Now test the dialog by entering "That''s bad" in the test console at the top
    right. You will see the response appear:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在右上角的测试控制台中输入“That's bad”来测试对话。你会看到回应出现：
- en: '![](img/B15438_16_05.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_05.png)'
- en: 'Figure 16.5: Default response'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：默认响应
- en: 'If you type "bad" instead of "That''s bad," it will work too, thanks to the
    ML functionality of Dialogflow:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你输入“bad”而不是“That's bad”，也能正常工作，因为 Dialogflow 的机器学习功能：
- en: '![](img/B15438_16_06.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_06.png)'
- en: 'Figure 16.6: Default response'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：默认响应
- en: '**Data logging** will tremendously help to boost the quality of a chatbot.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据日志记录**将极大地帮助提升聊天机器人的质量。'
- en: We will explore data logging in the next section. But let's check our emotions
    first.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节探讨数据日志记录。但是，首先让我们检查一下我们的情感。
- en: Emotions
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感
- en: 'We will deal with the first reaction: **Ha ha ha!** If we go back to emotional
    polysemy issues, knowing the user can say this at any time, we are in trouble
    again!'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理第一个反应：**哈哈哈！** 如果我们回到情感多义性的问题，知道用户随时都能说出这句话，我们又陷入麻烦了！
- en: '![](img/B15438_16_07.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_07.png)'
- en: 'Figure 16.7: Managing Emotions'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：管理情感
- en: Is the user happy, or are they making fun of the chatbot? Who knows? Even with
    facial analysis and tone analysis, a quick "Ha ha ha!" is very difficult to interpret.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用户是高兴，还是在取笑聊天机器人？谁知道呢？即使有面部分析和语调分析，一个快速的“哈哈哈！”也很难解释。
- en: I would suggest a careful low-profile answer such as "Well, that's cheerful!",
    for example.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议给出一个低调谨慎的回答，比如“嗯，这真是愉快啊！”。
- en: This will get the user to think that the chatbot has a sense of humor. When
    you click on **SAVE**, the **Emotions** progress bar will jump up.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以让用户觉得聊天机器人有幽默感。当你点击**保存**时，**情感**进度条会跳升。
- en: You will notice that beyond the variants Dialogflow detects, you can also enter
    variants directly in your responses. Also, if the user enters a phrase that is
    not in the dialog, there is a fallback intent in the intents list.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，除了 Dialogflow 检测到的变体之外，你还可以直接在响应中输入变体。此外，如果用户输入的短语不在对话中，意图列表中会有一个回退意图。
- en: Small talk might make a dialog smoother, but it is only one of the components
    of emotional intelligence, in a chatbot or in everyday life.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 闲聊可能使对话更流畅，但它只是情商的一部分，无论是在聊天机器人中还是在日常生活中。
- en: Data logging will take us a step further.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据日志记录将帮助我们迈出更大的一步。
- en: Data logging
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据日志记录
- en: In *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*, we took the context
    of a dialog into account using follow-up intents. However, even follow-up intents
    will not provide solutions to unexpected answers on the part of a user.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第15章*，*设置认知 NLP UI/CUI 聊天机器人*中，我们通过后续意图考虑了对话的上下文。然而，即使是后续意图也不能提供应对用户意外答案的解决方案。
- en: To enhance a dialog, data logging will create a long-term memory for the chatbot
    by remembering the key aspects of a dialog.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强对话，数据日志记录通过记住对话中的关键方面，为聊天机器人创建长期记忆。
- en: 'A user and a Dialogflow designer have to agree to the terms of the Google Dialogflow data
    logging features, as described on this page: [https://cloud.google.com/dialogflow/docs/data-logging](https://cloud.google.com/dialogflow/docs/data-logging).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和 Dialogflow 设计者必须同意 Google Dialogflow 数据日志记录功能的条款，如本页面所述：[https://cloud.google.com/dialogflow/docs/data-logging](https://cloud.google.com/dialogflow/docs/data-logging)。
- en: Privacy is a serious matter. However, you will notice that when you use a search
    engine for a given product, you end up viewing or receiving ads related to the
    search. This is data logging.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私是一个严肃的问题。然而，你会注意到，当你使用搜索引擎搜索某个产品时，最终你会看到或收到与搜索相关的广告。这就是数据日志记录。
- en: Making this decision depends on your goal and target audience. Suppose the user
    accepts the terms of the agreement. Now, data logging is activated. Then, data
    logging will provide the chatbot with long-term memory.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 做出这个决定取决于你的目标和目标受众。假设用户接受了协议条款。那么，数据日志记录将被激活，接着数据日志记录将为聊天机器人提供长期记忆。
- en: The rest of this chapter explores data logging, with the assumption of it having
    been clearly accepted by the user.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将探讨数据日志记录，假设用户已经清楚地接受了这一功能。
- en: Google Cloud, like all chatbot platforms (Amazon, Microsoft, and others), offers
    logs to improve chatbots. Many functions, interfaces, and services provide great
    support to boost the quality of dialogs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud，像所有聊天机器人平台（亚马逊、微软等）一样，提供日志功能来改进聊天机器人。许多功能、接口和服务为提升对话质量提供了极大的支持。
- en: Data logging can drive cognitive-adaptive dialogs beyond speech recognition
    tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据日志记录可以推动认知适应性对话超越语音识别任务。
- en: 'We will explore one way of doing this through the history of a dialog. Go to
    **History**:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过对话历史的方式来探索一种实现方法。请转到**历史**：
- en: '![](img/B15438_16_08.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_08.png)'
- en: 'Figure 16.8: Dialog history option in the menu'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：菜单中的对话历史选项
- en: 'You will see a list of past conversations:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到过去对话的列表：
- en: '![](img/B15438_16_09.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_09.png)'
- en: 'Figure 16.9: Dialog history'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9：对话历史
- en: Notice the **All platforms** list, which contains information for Google Assistant
    and other platforms. You can deploy your chatbot by clicking on **See how it works
    on Google Assistant** on the right-hand side of the screen. From there, you can
    follow the instructions and have it running on smartphones, Google Home, and elsewhere.
    Also, you will have advanced log data to improve the chatbot.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意**所有平台**列表，其中包含Google Assistant和其他平台的信息。您可以通过点击屏幕右侧的**查看Google Assistant上的工作方式**来部署聊天机器人。从那里，您可以按照指示操作，使其在智能手机、Google
    Home等设备上运行。此外，您还将获得先进的日志数据，以改进聊天机器人。
- en: 'If you tested "That''s bad" in the *Courtesy* section, the history of the interactions
    will be present:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在*礼貌*部分测试了“那太糟了”，则交互历史将会出现：
- en: '![](img/B15438_16_10.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_10.png)'
- en: 'Figure 16.10: Chatbot interactions'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：聊天机器人交互
- en: One way to know the username is to ask the user their name when an issue comes
    up. This can come in handy to customize a dialog. We can thus have a special dialog
    for this person or this category of persons. We can thus ask the person to state
    their name in their response with an email address, for example. When we analyze
    the data logs manually or with scripts in the **Fulfillment** section, we can
    track the problem down and improve the chatbot on a personal level.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 了解用户名的一种方式是在出现问题时询问用户他们的名字。这对于定制对话非常有用。我们可以为此人或此类人群设置特殊的对话。举例来说，我们可以要求用户在回答中提供姓名和电子邮件地址。当我们手动或通过脚本分析**履行**部分的数据日志时，我们可以追踪问题并在个人层面上改进聊天机器人。
- en: Having completed the **Small Talk** sections and then activated the data log
    authorization for your use of data logging, we can proceed to create emotions.
    Google will continue to improve our chatbot with our data logging features.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完成**小聊**部分并激活数据日志授权后，我们可以继续创建情感。谷歌将通过我们的数据日志功能继续改进聊天机器人。
- en: If we know which user said what, we can improve the dialog, as we will see in
    the next section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道哪个用户说了什么，我们就可以改进对话，正如我们将在下一节中看到的那样。
- en: Creating emotions
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建情感
- en: When the user enters ambiguous responses involving emotional polysemy, it is
    difficult for a chatbot to consider the hundreds of possibilities described in
    the previous sections.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户输入涉及情感多义性的模糊回答时，聊天机器人很难考虑前面章节中描述的数百种可能性。
- en: In this section, we will focus on a user trying to obtain a service such as
    access to a movie on a streaming platform.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论用户尝试获取某种服务（例如在流媒体平台上观看电影）的情况。
- en: 'An efficient chatbot should *create emotions in the user*. The most effective
    method is to:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高效的聊天机器人应该*激发用户的情感*。最有效的方法是：
- en: Generate *customer satisfaction*. Customer satisfaction is the ultimate emotion
    a chatbot should try to produce in a frictionless and expected dialog. If the
    customer is not satisfied with an answer, tensions and frustration will build
    up.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成*客户满意度*。客户满意度是聊天机器人在无摩擦、可预期的对话中应尽力产生的最终情感。如果客户对回答不满意，紧张和沮丧情绪会积累。
- en: Use functions such as the RBM-PCA approach of *Chapter 14*, *Preparing the Input
    of Chatbots with Restricted Boltzmann Machines (RBMs) and Principal Component
    Analysis (PCA)*, to suggest options that shorten the dialog path, thus its duration
    making the user "happy."
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用像*第14章*中RBM-PCA方法这样的功能，*使用受限玻尔兹曼机（RBM）和主成分分析（PCA）准备聊天机器人的输入*，建议选项以缩短对话路径，从而缩短对话持续时间，使用户“开心”。
- en: We will now explore the *no* path of the dialog encountered in *Chapter 15,
    Setting Up a Cognitive NLP UI/CUI Chatbot*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨在*第15章，设置认知NLP UI/CUI聊天机器人*中遇到的对话*否*路径。
- en: 'To access the *no* path of the dialog, go to **Intents**, click on the **choose_movie**
    intent and click on **Add follow-up intent**, and click on **no** in the drop-down
    menu:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问对话的*否*路径，请转到**意图**，点击**choose_movie**意图，然后点击**添加后续意图**，在下拉菜单中选择**no**：
- en: '![](img/B15438_16_11.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_11.png)'
- en: 'Figure 16.11: Adding a follow-up intent'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11：添加后续意图
- en: 'A **choose_movie - no** option should now appear:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**choose_movie - no**选项现在应该出现：'
- en: '![](img/B15438_16_12.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_12.png)'
- en: 'Figure 16.12: Follow-up options'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12：后续选项
- en: Click on **choose_movie - no**.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**choose_movie - no**。
- en: 'Google has entered several default "no" variants, as shown in the following
    screenshot:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌已经进入了几个默认的“否”变体，如下截图所示：
- en: '![](img/B15438_16_13.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_13.png)'
- en: 'Figure 16.13: Dialogflow training phrases'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13：Dialogflow训练短语
- en: This "no" response comes as a surprise to the chatbot. In *Chapter 14*, this
    market segment was explored. Something has gone wrong!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“否”回应令聊天机器人感到意外。在*第14章*中，这一市场细分已经被探讨过。某些事情出了问题！
- en: The chatbot was working on a specific market segment, the "action" superhero
    fan type viewer. The answer being "no" means that we need to examine the other
    features available.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人正在处理一个特定的市场细分，即“动作”超级英雄粉丝类型的观众。回答“否”意味着我们需要检查其他可用的特性。
- en: 'The features in *Chapter 14*, *Preparing the Input of Chatbots with Restricted
    Boltzmann Machines (RBMs) and Principal Component Analysis (PCA)*, in the `RBM.py`
    program were:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第14章*中，*使用限制玻尔兹曼机（RBMs）和主成分分析（PCA）准备聊天机器人的输入*，在`RBM.py`程序中的特性是：
- en: '[PRE0]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The "action" feature predicted so far groups several features:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 目前预测的“动作”特性组合包括以下几个特性：
- en: Action = {happiness, action, violence}
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 动作 = {幸福，动作，暴力}
- en: 'The following features were not taken into account:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下特性没有被考虑：
- en: '{love, family, horizons}'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '{爱情，家庭，视野}'
- en: 'Since we want to keep the path short, we must find a way to ask a question
    that:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望保持路径简短，必须找到一种方式来提问，使得：
- en: Covers these three features
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖这三个特性
- en: Can use an existing feature matrix for another marketing segment
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以为另一个市场细分使用现有的特性矩阵
- en: 'The viewer also may have:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 观众可能还具有：
- en: Recently seen enough action movies
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近看过足够多的动作电影
- en: Progressively grown out of the superhero period of their life and be looking
    for other types of movies
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从超级英雄的生活阶段逐渐成长，开始寻找其他类型的电影
- en: In both cases, the viewer's market segment might overlap with another segment
    that contains family-love values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，观众的市场细分可能与包含家庭-爱情价值观的另一个细分市场重叠。
- en: 'As we saw in the *Adding fulfillment functionality to an agent* section in
    *Chapter 15*, *Setting up a Cognitive NLP UI/CUI Chatbot*, we can use a script
    to:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第15章*的*为代理添加履行功能*部分中看到的，*设置认知NLP UI/CUI聊天机器人*，我们可以使用脚本来：
- en: Cover these three features
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖这三个特性
- en: Use an existing feature matrix for another marketing segment
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为另一个市场细分使用现有的特性矩阵
- en: 'Classical marketing segments take age into account. Let''s continue in this
    direction and prepare for the possibility that the viewer, a young superhero fan,
    is growing a bit older and entering another age-movie-type segment that overlaps
    with the one used in `RBM.py` in *Chapter 14*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的市场营销细分考虑了年龄因素。我们继续沿这个方向进行，准备好面对这样一种可能性：观众，这位年轻的超级英雄粉丝，逐渐变老并进入另一个与`RBM.py`中使用的年龄-电影类型细分重叠的细分市场，这发生在*第14章*：
- en: '[PRE1]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We should add some love-family features in the matrix with the corresponding
    movies. We will then obtain another marketing segment. In the end, the chatbot
    will manage many marketing segments, which is the standard practice on many streaming
    platforms.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在矩阵中加入一些爱情-家庭特性，并配上相应的电影。然后我们将获得另一个市场细分。最终，聊天机器人将管理多个市场细分，这是许多流媒体平台的标准做法。
- en: 'A variant of the chart in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI
    Chatbot*, could be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*第15章*中*设置认知NLP UI/CUI聊天机器人*的图表变体可能如下：'
- en: '| **MOVIE/FEATURE** | LOVE | HAPPINESS | FAMILY | HORIZONS | ACTION | VIOLENCE
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **电影/特性** | 爱情 | 幸福 | 家庭 | 视野 | 动作 | 暴力 |'
- en: '| 24H in Kamba | 1 | 1 | 0 | 0 | 1 | 1 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 坎巴24小时 | 1 | 1 | 0 | 0 | 1 | 1 |'
- en: '| Lost | 1 | 1 | 0 | 1 | 1 | 1 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 迷失 | 1 | 1 | 0 | 1 | 1 | 1 |'
- en: '| Cube Adventures | 1 | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 魔方冒险 | 1 | 0 | 0 | 0 | 0 | 1 |'
- en: '| A Holiday | 1 | 1 | 0 | 1 | 1 | 1 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 节日 | 1 | 1 | 0 | 1 | 1 | 1 |'
- en: '| Jonathan Brooks | 1 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 乔纳森·布鲁克斯 | 1 | 0 | 0 | 0 | 1 | 1 |'
- en: '| The Melbourne File | 1 | 1 | 0 | 1 | 1 | 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 墨尔本档案 | 1 | 1 | 0 | 1 | 1 | 0 |'
- en: '| WNC Detectives | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| WNC侦探 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Stars | 1 | 1 | 0 | 1 | 1 | 0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 明星 | 1 | 1 | 0 | 1 | 1 | 0 |'
- en: '| Space II | 1 | 1 | 1 | 0 | 1 | 0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 太空II | 1 | 1 | 1 | 0 | 1 | 0 |'
- en: '| Zone 77 | 1 | 0 | 0 | 1 | 1 | 1 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 区域77 | 1 | 0 | 0 | 1 | 1 | 1 |'
- en: 'This feature matrix contains a movie with the missing features from the previous
    matrix: Space II.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征矩阵包含了上一矩阵中缺失特征的电影：《太空II》。
- en: 'A streaming platform contains many marketing segments:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 流媒体平台包含许多营销细分：
- en: '*M* = {*s*[1], *s*[2], … *s*[n]}'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*M* = {*s*[1], *s*[2], … *s*[n]}'
- en: Many of these marketing segments contain variants, merged features, combinations,
    and more.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些营销细分包含了不同的变体、合并特征、组合等。
- en: 'Since data logging has been activated, from this point on we now have the following
    information:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据日志记录已被激活，从此时起我们现在可以获得以下信息：
- en: Whether this viewer has seen one of the several movies available in this marketing
    segment. This constitutes another tricky issue since some viewers may want to
    watch a movie again.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否该观众已观看过此营销细分中提供的几部电影之一。这是另一个棘手的问题，因为某些观众可能会想重新观看某部电影。
- en: The viewer's new marketing segment.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观众的全新营销细分。
- en: 'Building a chatbot for a streaming platform will take months of designing with
    many build possibilities. For this example, we will focus on the age progression
    scenario, keep the dialog path as short as possible, and provide the following
    response:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为流媒体平台构建聊天机器人将需要数月的设计工作，并有许多构建可能性。对于这个示例，我们将专注于年龄进展场景，尽可能缩短对话路径，并提供以下响应：
- en: '"Would you like to watch SPACE II? It''s a blockbuster with a family that has
    an adventure in space. There is some action but it''s mostly the story of a family
    that tries to survive in space."'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '"你想看《太空II》吗？这是一个讲述一家人在太空冒险的大片。虽然有些动作场面，但它主要讲述的是一家人如何在太空中生存的故事。"'
- en: 'Scroll down to the **Text Response** section and enter the response as follows,
    then click on **SAVE** to trigger the training process:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动至**文本响应**部分，输入如下响应，然后点击**保存**以触发训练过程：
- en: '![](img/B15438_16_14.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_14.png)'
- en: 'Figure 16.14: A look at the training process'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14：训练过程概览
- en: If the viewer answers "yes," then the dialog will lead to the movie's page.
    To continue in this direction, go back to *Chapter 15*, *Setting Up a Cognitive
    NLP UI/CUI Chatbot*, and a "yes" follow-up exchange to this part of the dialog
    as you wish.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果观众回答"是"，则对话将引导至该电影的页面。要继续此方向，请返回到*第15章*，*设置认知NLP UI/CUI聊天机器人*，并根据需要添加对该对话部分的"是"后续交换。
- en: We have added some emotional intelligence to the agent. We will now explore
    the future of chatbot architecture through text augmentation with **recurrent
    neural networks** (**RNNs**).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为代理添加了一些情感智能。接下来，我们将通过文本增强与**递归神经网络**（**RNNs**）一起探索聊天机器人架构的未来。
- en: An RNN can process sequential data such as sequences of words, events, and more.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: RNN可以处理序列数据，例如单词序列、事件序列等。
- en: RNN research for future automatic dialog generation
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对未来自动对话生成的RNN研究
- en: The future of chatbots lies in producing dialogs automatically, based on data
    logging dialogs, their cognitive meanings, the personal profile of a user, and
    more. As RNNs progress, we will get closer to this approach. There are many generative
    approaches that can produce automatic sequences of sounds and texts. Understanding
    an RNN is a good place to start.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人未来的发展方向是基于数据记录的对话、其认知意义、用户个人资料等，自动生成对话。随着RNN的进展，我们将越来越接近这一方法。有许多生成方法可以自动生成声音和文本序列。理解RNN是一个很好的起点。
- en: An RNN model is based on sequences, in this case, words. It analyzes anything
    in a sequence, including images. To speed the mind-dataset process up, data augmentation
    can be applied here, exactly as it is to images in other models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型基于序列，在此案例中是单词。它分析序列中的任何内容，包括图像。为了加速思维数据集的处理，可以在这里应用数据增强，就像在其他模型中对图像所做的那样。
- en: 'A first look at its graph data flow structure shows that an RNN is a neural
    network like the others previously explored. The following diagram shows a conceptual
    view of an RNN:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 初步查看其图形数据流结构显示，RNN是一种与之前探讨过的其他神经网络类似的神经网络。以下图表展示了RNN的概念性视图：
- en: '![](img/B15438_16_15.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_15.png)'
- en: 'Figure 16.15: Data flow structure'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15：数据流结构
- en: The *y* inputs (test data) go to the loss function (**Loss_Train**). The *x*
    inputs (training data) will be transformed through weights and biases into logits
    with a softmax function.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 输入（测试数据）进入损失函数（**Loss_Train**）。*x* 输入（训练数据）将通过权重和偏差转换为带有softmax函数的logits。'
- en: 'Looking at the RNN area of the graph shows the following **basic_lstm_cell**:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图表中的RNN区域显示以下**基本_lstm_单元**：
- en: '![](img/B15438_16_16.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_16.png)'
- en: 'Figure 16.16: The basic_lstm_cell—the RNN area of the graph'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.16：基本的 lstm_cell——图中 RNN 区域
- en: The LSTM cell of an RNN contains "forget" gates that will prevent vanishing
    gradients when the sequences become too long for an RNN unit.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的 LSTM 单元包含“遗忘”门，当序列变得对 RNN 单元过长时，它们可以防止梯度消失。
- en: RNNs at work
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 的应用
- en: 'An RNN contains functions that take the output of a layer and feed it back
    to the input in sequences simulating time. This feedback process takes information
    in a sequence, for example:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 包含一些函数，这些函数接收一层的输出，并将其反馈给输入，模拟时间的序列。这个反馈过程接受一个序列中的信息，例如：
- en: '*The* -> movie -> was -> **interesting** -> but -> I -> didn''t -> like ->
    *it*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*The* -> 电影 -> 很 -> **有趣** -> 但 -> 我 -> 不 -> 喜欢 -> *它*'
- en: An RNN will unroll a stack of words into a sequence and parse a window of words to
    the right and the left. For example, in this sentence, an RNN can start with interesting
    (bold) and then read the words on the right and left (in italic). These are some
    of the hyperparameters of the RNN.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 会将一堆单词展开成序列，并解析左右窗口中的单词。例如，在这句话中，RNN 可以从 *interesting*（加粗）开始，然后读取左右的单词（斜体）。这些是
    RNN 的一些超参数。
- en: This sequence aspect opens the door to sequence prediction. Instead of recognizing
    a whole pattern of data at the same time, it is recognizing the sequence of data,
    as in this example.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这种序列特性为序列预测打开了大门。与其同时识别整个数据模式，不如识别数据的序列，就像这个例子一样。
- en: 'A network with no RNN will recognize the following vector as a week, a pattern
    just like any other:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有 RNN 的网络会将以下向量识别为一周，这就像其他任何模式一样：
- en: Monday
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 星期一
- en: Tuesday
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 星期二
- en: Wednesday
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 星期三
- en: Thursday
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 星期四
- en: Friday
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 星期五
- en: Saturday
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 星期六
- en: Sunday
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 星期天
- en: 'An RNN will explore the same data in a sequence by unrolling streams of data:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 会通过展开数据流来按顺序探索相同的数据：
- en: Monday -> Tuesday -> Wednesday -> Thursday -> Friday -> Saturday -> Sunday
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 星期一 -> 星期二 -> 星期三 -> 星期四 -> 星期五 -> 星期六 -> 星期天
- en: The main difference lies in the fact that once trained, the network will predict
    the word that follows; if Wednesday is the input, Thursday could be one of the
    outputs. This is shown in the next section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于，一旦训练完成，网络会预测下一个单词；如果输入是星期三，星期四可能是输出之一。这将在下一节中展示。
- en: RNN, LSTM, and vanishing gradients
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN、LSTM 和梯度消失
- en: To simulate sequences and memory, an RNN and an LSTM will use backpropagation
    algorithms. An LSTM is an improved version of RNN in some cases.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟序列和记忆，RNN 和 LSTM 会使用反向传播算法。在某些情况下，LSTM 是 RNN 的改进版。
- en: An RNN often has problems with gradients when calculating them over deeper and
    deeper layers in the network. Sometimes, it vanishes (too close to 0) due to the sequence
    property, just like us when a memory sequence becomes too long.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在计算更深层次的网络梯度时经常会遇到问题。有时，由于序列的特性，它的梯度会消失（接近 0），就像我们在记忆序列变得太长时一样。
- en: The backpropagation (just like us with a sequence) becomes less efficient. There
    are many backpropagation algorithms, such as vanilla backpropagation, which is
    commonly used. This algorithm performs efficient backpropagation because it updates
    the weights after every training pattern.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播（就像我们在序列中一样）变得不那么高效了。有许多反向传播算法，如常用的基础反向传播。该算法执行高效的反向传播，因为它在每个训练模式后都会更新权重。
- en: One way to force the gradient not to vanish is to use a ReLU activation function,
    *f*(*x*) = max(0, *x*), forcing values on the model so that it will not get stuck.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强制梯度不消失的方法是使用 ReLU 激活函数，*f*(*x*) = max(0, *x*)，通过这种方式迫使模型上的值不至于停滞。
- en: Another way is to use an LSTM cell containing a forget gate between the input
    and the output cells, a bit like us when we get stuck in a memory sequence, and
    we say "whatever" and move on.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用一个包含遗忘门的 LSTM 单元，该遗忘门位于输入和输出单元之间，有点像我们在记忆序列中卡住时，会说“算了”然后继续前进。
- en: The LSTM cell will act as a memory gate with 0 and 1 values, for example. This
    cell will forget some information to have a fresh view of the information it has
    unrolled into a sequence. In recent TensorFlow versions (2.0 and above), you can
    choose to use RNN or LSTM units in a layer. Your choice will depend on several
    factors. The key factor is the behavior of the gradient. If it vanishes in the
    RNN units, you might want to improve your model or move to LSTM units.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元将充当一个具有 0 和 1 值的记忆门，例如。这个单元会遗忘一些信息，以便对它已经展开成序列的信息有一个新的视角。在最近的 TensorFlow
    版本（2.0 及以上）中，你可以选择在某一层使用 RNN 或 LSTM 单元。你的选择将取决于多个因素，关键因素是梯度的行为。如果在 RNN 单元中梯度消失，你可能需要改进你的模型，或者切换到
    LSTM 单元。
- en: The key idea of an RNN to bear in mind is that it unrolls information into sequences, remembering
    the past to predict the future. The main idea of an LSTM relies upon its "forget"
    gate, avoiding the vanishing gradient. In TensorFlow 2.x, the choice of RNN or
    LSTM units can be made in a few lines.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 需要牢记的 RNN 关键思想是，它会将信息展开为序列，记住过去的内容来预测未来。LSTM 的主要思想依赖于其“遗忘”门，避免梯度消失。在 TensorFlow
    2.x 中，您可以通过几行代码选择使用 RNN 或 LSTM 单元。
- en: Let's run an example on Google Colaboratory.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Google Colaboratory 上运行一个示例。
- en: Text generation with an RNN
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RNN 进行文本生成
- en: To view the program, log into your Dialogflow account, upload `text_generation_tf2.ipynb`
    (located in the `CH16` directory in the GitHub repository of this book) to your
    Google Colaboratory environment, and save it in your drive, as explained in the
    *Getting started with Google Colaboratory* section in *Chapter 13*, *Visualizing
    Networks with TensorFlow 2.x and TensorBoard*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看程序，登录到您的 Dialogflow 账户，上传 `text_generation_tf2.ipynb`（该文件位于本书 GitHub 仓库的
    `CH16` 目录中）到您的 Google Colaboratory 环境，并将其保存在您的 Google 云端硬盘中，正如 *第13章* 中 *Google
    Colaboratory 入门* 部分所解释的那样，*使用 TensorFlow 2.x 和 TensorBoard 可视化网络*。
- en: This TensorFlow authors' program has been well designed for educational purposes.
    The program starts by setting up TensorFlow 2.x and the necessary libraries.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这款 TensorFlow 作者编写的程序设计得非常适合教学目的。程序通过设置 TensorFlow 2.x 和必要的库开始运行。
- en: In this section, we will thus focus on the main points of the program that you
    can then explore, run, and modify.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将重点关注程序的主要部分，您可以在之后探索、运行和修改它。
- en: Vectorizing the text
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化文本
- en: 'The main entry step to an RNN consists of taking the sequence of words, the
    strings, and converting them into a **numerical representation**:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的主要输入步骤是将一系列单词（字符串）转换为 **数字表示**：
- en: '[PRE2]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We obtain a numerical value for each character:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个字符获得一个数字值：
- en: '[PRE3]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will notice that this "dictionary" can be interpreted in two ways:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，这个“字典”可以有两种解释方式：
- en: character2number
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: character2number
- en: integer2character
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: integer2character
- en: The RNN will run its calculations but the predictions will come out in characters.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 会运行其计算，但预测结果将以字符的形式输出。
- en: 'For example, the program can take the first sequence of the loaded text and
    produce the mapped integers of the text as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，程序可以将加载文本的第一序列提取出来，并生成对应的整数映射，如下所示：
- en: '[PRE4]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example, the result is:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，结果是：
- en: '[PRE5]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The RNN will run through numerical sequences, integer segments, or windows of the
    text to train and then make predictions. To do this, the program creates examples
    and targets as for all neural networks that have training batches.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 会处理数字序列、整数片段或文本的窗口进行训练，然后做出预测。为此，程序会创建示例和目标，就像所有神经网络在训练批次中的操作一样。
- en: Building the model
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型
- en: Building neural networks with TensorFlow 2 has become so simple to write in
    a few lines that you can even miss seeing them in the example programs!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2 构建神经网络已经变得非常简单，几行代码就可以完成，甚至可能在示例程序中错过它们！
- en: 'Let''s clarify some basic concepts before getting to those few lines:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入这些几行代码之前，让我们先澄清一些基本概念：
- en: A **sequential** model contains a pile or stack of layers.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序**模型包含一堆或一组层。'
- en: '**Embedding** takes the number of each character and stores it in a vector.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**将每个字符的数字表示存储在一个向量中。'
- en: '**GRU** stands for gated recurrent unit. A GRU contains gates that manage hidden
    units, keeping some information and forgetting other information. An RNN GRU can
    sometimes get confused when the sequences become long and thus mismanage the gradient,
    which then disappears. The more efficient LSTM units are part of a recurrent network
    unit as well with feedback connections with a cell, an input gate, an output gate,
    and a forget gate. But in the end the choice of the types units will always be
    yours depending on the context of your project. In any case, the key concept to
    keep in mind is that recurrent networks manage sequences of data, keeping the
    past in mind while forgetting some information.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRU** 代表门控递归单元。GRU 包含一些门控，用来管理隐藏单元，保留一些信息并遗忘其他信息。当序列变长时，RNN GRU 有时会出现混乱，从而错误地处理梯度，导致梯度消失。更高效的
    LSTM 单元同样是递归网络单元的一部分，具备与单元的反馈连接、输入门、输出门以及遗忘门。但最终，选择何种类型的单元始终取决于您的项目背景。无论如何，必须牢记的核心概念是递归网络用于管理数据序列，在保持过去记忆的同时忘记一些信息。'
- en: A **dense** layer, in this case, is the output layer.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集**层，在此指的是输出层。'
- en: A **timestep** is a predefined sequence length. In another model, it could be
    actual time if we are working on time-dependent data.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间步长**是预定义的序列长度。在另一个模型中，如果我们在处理时间相关的数据，它可能是真实的时间。'
- en: 'A sequential model is built in three layers only:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个顺序模型只建立了三个层次：
- en: '[PRE6]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And that''s it! You can replace the basic `rnn_units` with an LSTM layer if
    the model requires it during the training phase. Once the model is built, the
    model:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！如果模型在训练阶段需要，你可以将基础的 `rnn_units` 替换为LSTM层。一旦模型建立，模型将：
- en: Looks an embedding up, as in a "dictionary."
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看起来像是字典中的嵌入。
- en: Runs the GRU for a timestep.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行GRU一个时间步。
- en: The dense layer will then generate **logits** (see *Chapter 2*, *Building a
    Reward Matrix – Designing Your Datasets*) to produce a prediction using a likelihood
    function, a probability distribution.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稠密层接着会生成**logits**（参见*第2章*，*构建奖励矩阵 – 设计数据集*），使用似然函数，一个概率分布来进行预测。
- en: 'The following figure of the TensorFlow author''s program sums the process up:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示总结了TensorFlow作者程序的过程：
- en: '![A drawing of the data passing through the model](img/B15438_16_17.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![数据通过模型的传递图示](img/B15438_16_17.png)'
- en: 'Figure 16.17: TensorFlow model'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.17：TensorFlow模型
- en: Generating text
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成文本
- en: 'After trying and training the model, the program will generate text automatically,
    for example:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试和训练模型后，程序将自动生成文本，例如：
- en: '[PRE7]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you''ll notice, `ROMEO:` has been set up as the starting string. It then
    shows that the following predictions come from the initial text written by Shakespeare
    and are loaded at the beginning of the program:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`ROMEO:` 已被设置为起始字符串。接着显示出以下的预测来源于莎士比亚的初始文本，并在程序开始时加载：
- en: '[PRE8]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can go back to the beginning of the program and change the URL. Instead
    of loading Shakespeare, change it to your own text:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以返回程序的开始部分并更改URL。将莎士比亚的内容改为你自己的文本：
- en: '[PRE9]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Before running the program, go to **Runtime** -> **Change runtime type**:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行程序之前，进入**运行时** -> **更改运行时类型**：
- en: '![](img/B15438_16_18.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_18.png)'
- en: 'Figure 16.18: Runtime type'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.18：运行时类型
- en: 'Click on **Change runtime type**:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**更改运行时类型**：
- en: '![](img/B15438_16_19.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_16_19.png)'
- en: 'Figure 16.19: Notebook settings'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.19：笔记本设置
- en: I recommend using the GPU. Also, verify that **Omit code cell output when saving
    this notebook** is not checked if you want to save your notebook with the results
    produced when you run the program.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议使用GPU。此外，如果你想在保存笔记本时保存程序运行结果，确保**保存此笔记本时忽略代码单元输出**未被勾选。
- en: You are now ready to explore and do your own research to contribute to the future
    of automatic text generation!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已准备好探索并进行自己的研究，为自动文本生成的未来做出贡献！
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Emotional polysemy makes human relationships rich and excitingly unpredictable.
    However, chatbots remain machines and do not have the ability to manage wide ranges
    of possible interpretations of a user's phrases.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 情感多义性使得人际关系丰富而充满令人兴奋的不可预测性。然而，聊天机器人仍然是机器，无法处理用户短语的广泛可能解释。
- en: Present-day technology requires hard work to get a cognitive NPL CUI chatbot
    up and running. Small talk will make the conversation smoother. It goes beyond
    being a minor feature; courtesy and pleasant emotional reactions are what make
    a conversation go well.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现有技术要求大量工作才能让一个认知NPL CUI聊天机器人正常运行。小型对话能让交流更加顺畅。它不仅仅是一个次要特性；礼貌和愉快的情感反应是使对话顺利进行的关键。
- en: We can reduce the limits of present-day technology by creating emotions in the
    users through a meaningful dialog that creates a warmer experience. Customer satisfaction
    constitutes the core of an efficient chatbot. One way to achieve this goal is
    to implement cognitive functions based on data logging. We saw that when a user
    answers "no" when we expect "yes," the chatbot needs to adapt, exactly the way
    we humans do.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过有意义的对话创造用户的情感，从而减少现有技术的局限性，这种对话能够带来更温暖的体验。客户满意度构成了高效聊天机器人的核心。一种实现这一目标的方法是基于数据日志的认知功能实现。当用户在我们期待“是”的时候回答“不是”，我们看到了聊天机器人需要适应，就像我们人类一样。
- en: Cognitive data logging can be achieved through the preparation we explored in
    *Chapter 14*, *Preparing the Input of Chatbots with Restricted Boltzmann Machines
    (RBMs) and Principal Component Analysis (PCA)*, the cognitive dialog of *Chapter
    15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*, and the adaptive dialog built
    in this chapter. In our example, the viewer changed market segments, and the chatbot
    logged the new profile. Dialogflow-fulfillment scripts can manage the whole adaptive
    process, though that is beyond the scope of this book.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 认知数据记录可以通过我们在*第14章*中探讨的准备工作实现，*通过受限玻尔兹曼机(RBM)和主成分分析(PCA)准备聊天机器人的输入*，以及在*第15章*中探讨的认知对话，*搭建认知NLP
    UI/CUI聊天机器人*，以及本章构建的自适应对话。在我们的例子中，观众更改了市场细分，聊天机器人记录了新的用户画像。Dialogflow-fulfillment脚本可以管理整个自适应过程，尽管这超出了本书的讨论范围。
- en: We looked at the study of sequences of data through RNNs eventually leading
    to automatic dialogs. Chatbots, using cognitive approaches such as the RBM-PCA
    and the adaptive data logging inferences of this chapter, will one day build their
    own dialogs.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了通过RNN处理数据序列的过程，最终实现自动对话。使用认知方法的聊天机器人，例如本章中的RBM-PCA和自适应数据记录推理，将来能够构建自己的对话系统。
- en: The following chapters will explore ways to achieve higher levels of artificial
    intelligence through genes, biological neurons, and qubits. The next chapter explores genetic
    algorithms and then implements them into a hybrid neural network.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将探讨通过基因、生物神经元和量子比特来实现更高层次人工智能的方法。下一章将探讨遗传算法，并将其应用到混合神经网络中。
- en: Questions
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: When a chatbot fails to provide a correct response, a hotline with actual humans
    needs to take over the conversation. (Yes | No)
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当聊天机器人无法提供正确的回应时，需要由实际的人工客服接管对话。（是 | 否）
- en: Small talk serves no purpose in everyday life or with chatbots. It is best to
    just get to the point. (Yes | No)
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日常生活中或与聊天机器人交互时，小谈话没有任何意义。最好直接切入主题。（是 | 否）
- en: Data logging can be used to improve speech recognition. (Yes | No)
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据记录可以用来提高语音识别的准确性。（是 | 否）
- en: The history of a chatbot agent's conversations will contain valuable information.
    (Yes | No)
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聊天机器人代理的对话历史将包含有价值的信息。（是 | 否）
- en: Present-day technology cannot make use of the data logging of a user's dialogs.
    (Yes | No)
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现代技术无法利用用户对话的数据记录。（是 | 否）
- en: An RNN uses sequences of data to make predictions. (Yes | No)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN通过数据序列进行预测。（是 | 否）
- en: An RNN can generate the dialog flow of a chatbot automatically for all applications.
    (Yes | No)
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN可以自动生成聊天机器人的对话流，适用于所有应用程序。（是 | 否）
- en: Further reading
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Information on RNNs: [https://www.tensorflow.org/tutorials/recurrent](https://www.tensorflow.org/tutorials/recurrent)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于RNN的更多信息：[https://www.tensorflow.org/tutorials/recurrent](https://www.tensorflow.org/tutorials/recurrent)
- en: 'More on text generation: [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于文本生成的更多内容：[https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)
