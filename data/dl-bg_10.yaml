- en: Deep Autoencoders
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: This chapter introduces the concept of deep belief networks and the significance
    of this type of deep unsupervised learning. It explains such concepts by introducing
    deep autoencoders along with two regularization techniques that can help create
    robust models. These regularization techniques, batch normalization and dropout,
    have been known to facilitate the learning of deep models and have been widely
    adopted. We will demonstrate the power of a deep autoencoder on MNIST and on a
    much harder dataset known as CIFAR-10, which contains color images.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了深度信念网络的概念及这种深度无监督学习方式的重要性。通过引入深度自编码器以及两种有助于创建稳健模型的正则化技术，来解释这些概念。这些正则化技术——批量归一化和丢弃法，已被证明能促进深度模型的学习，并广泛应用。我们将在MNIST数据集和一个更具挑战性的彩色图像数据集CIFAR-10上展示深度自编码器的强大能力。
- en: By the end of this chapter, you will appreciate the benefits of making deep
    belief networks by observing the ease of modeling and quality of the output that
    they provide. You will be able to implement your own deep autoencoder and prove
    to yourself that deeper models are better than shallow models for most tasks.
    You will become familiar with batch normalization and dropout strategies for optimizing
    models and maximizing performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够体会到构建深度信念网络的好处，观察到它们在建模和输出质量方面的优势。你将能够实现自己的深度自编码器，并证明对于大多数任务，深层模型比浅层模型更优。你将熟悉批量归一化和丢弃法策略，以优化模型并最大化性能。
- en: 'This chapter is organized as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结构如下：
- en: Introducing deep belief networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍深度信念网络
- en: Making deep autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度自编码器
- en: Exploring latent spaces with deep autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度自编码器探索潜在空间
- en: Introducing deep belief networks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度信念网络
- en: In machine learning, there is a field that is often discussed when talking about
    **deep learning** (**DL**), called **deep belief networks** (**DBNs**) (Sutskever,
    I., and Hinton, G. E. (2008)). Generally speaking, this term is used also for
    a type of machine learning model based on graphs, such as the well-known **Restricted
    Boltzmann Machine**. However, DBNs are usually regarded as part of the DL family,
    with deep autoencoders as one of the most notable members of that family.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，有一个领域在讨论**深度学习**（**DL**）时常常被提及，叫做**深度信念网络**（**DBNs**）（Sutskever, I. 和
    Hinton, G. E. (2008)）。一般而言，这个术语也用于指基于图的机器学习模型，例如著名的**限制玻尔兹曼机**。然而，DBNs通常被视为深度学习家族的一部分，其中深度自编码器是该家族中最为突出的成员之一。
- en: Deep autoencoders are considered DBNs in the sense that there are latent variables
    that are only visible to single layers in the forward direction. These layers
    are usually many in number compared to autoencoders with a single pair of layers.
    One of the main tenets of DL and DBNs in general is that during the learning process,
    there is different knowledge represented across different sets of layers. This
    knowledge representation is learned by *feature learning* without a bias toward
    a specific class or label. Furthermore, it has been demonstrated that such knowledge
    appears to be hierarchical. Consider images, for example; usually, layers closer
    to the input layer learn features that are of low order (that is, edges), while
    deeper layers learn higher-order features, that is, well-defined shapes, patterns,
    or objects (Sainath, T. N., et.al. (2012)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器被视为DBNs的一种形式，因为在前向传播过程中，有些潜在变量仅对单层可见。这些层的数量通常比单层自编码器要多。深度学习（DL）和DBNs的一项核心原则是，在学习过程中，不同层次之间代表着不同的知识。这些知识表示是通过*特征学习*获得的，并且没有偏向特定的类别或标签。此外，研究表明，这种知识表现出层次结构。例如，考虑图像；通常，靠近输入层的层学习的是低阶特征（如边缘），而更深的层学习的是高阶特征，即明确的形状、模式或物体（Sainath,
    T. N. 等人（2012））。
- en: In DBNs, as in most DL models, the interpretability of the feature space can
    be difficult. Usually, looking at the weights of the first layer can offer information
    about the features learned and or the looks of the feature maps; however, due
    to high non-linearities in the deeper layers, interpretability of the feature
    maps has been a problem and careful considerations need to be made (Wu, K., et.al. (2016)).
    Nonetheless, despite this, DBNs are showing excellent results in feature learning.
    In the next few sections, we will cover deeper versions of autoencoders on highly
    complex datasets. We will be introducing a couple of new types of layers into
    the mix to demonstrate how deep a model can be.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度信念网络（DBN）中，与大多数深度学习模型一样，特征空间的可解释性可能很困难。通常，查看第一层的权重可以提供有关学习的特征和/或特征图外观的信息；然而，由于更深层中的高非线性，特征图的可解释性一直是一个问题，需要谨慎考虑（Wu,
    K., 等人（2016））。尽管如此，DBN在特征学习方面仍然表现出色。在接下来的几节中，我们将介绍在高度复杂数据集上的更深层版本的自编码器。我们将引入几种新型的层，以展示模型可以有多深。
- en: Making deep autoencoders
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度自编码器
- en: An autoencoder can be called *deep* so long as it has more than one pair of
    layers (an encoding one and a decoding one). Stacking layers on top of each other
    in an autoencoder is a good strategy to improve its power for feature learning
    in finding unique latent spaces that can be highly discriminatory in classification
    or regression applications. However, in [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*,
    we covered how to stack layers onto an autoencoder, and we will do that again,
    but this time we will use a couple of new types of layers that are beyond the
    dense layers we have been using. These are the **batch normalization** and **dropout **layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 只要自编码器有不止一对层（一个编码层和一个解码层），它就可以被称为*深度*自编码器。在自编码器中堆叠层是提高其特征学习能力的好策略，能够找到在分类或回归应用中具有高度判别性的独特潜在空间。然而，在[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml)《自编码器》中，我们已经介绍了如何在自编码器上堆叠层，我们将再次这样做，但这次我们将使用一些新的层类型，这些层超出了我们之前使用的全连接层。它们是**批量归一化**和**丢弃**层。
- en: There are no neurons in these layers; however, they act as mechanisms that have
    very specific purposes during the learning process that can lead to more successful
    outcomes by means of preventing overfitting or reducing numerical instabilities.
    Let's talk about each of these and then we will continue to experiment with both
    of these on a couple of important datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层中没有神经元；然而，它们作为具有非常具体目的的机制，在学习过程中发挥作用，通过防止过拟合或减少数值不稳定性，能够带来更成功的结果。让我们讨论一下这些层，然后我们将在几个重要的数据集上继续实验这两种层。
- en: Batch normalization
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: 'Batch normalization has been an integral part of DL since it was introduced
    in 2015 (Ioffe, S., and Szegedy, C. (2015)). It has been a major game-changer
    because it has a couple of nice properties:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化自2015年被引入深度学习（Ioffe, S., 和 Szegedy, C. (2015)）以来，已经成为深度学习的一个重要组成部分。它是一项重大变革，因为它具有几个优点：
- en: It can prevent the problem known as **vanishing gradient** or **exploding gradient**,
    which is very common in recurrent networks (Hochreiter, S. (1998)).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以防止被称为**消失梯度**或**爆炸梯度**的问题，这在递归网络中非常常见（Hochreiter, S. (1998)）。
- en: It can lead to faster training by acting as a regularizer to the learning model
    (Van Laarhoven, T. (2017)).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过充当学习模型的正则化器，从而加快训练过程（Van Laarhoven, T. (2017)）。
- en: 'A summary of these properties and the block image we will use to denote batch
    normalization are shown in *Figure 8.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性的总结以及我们将用于表示批量归一化的块图像显示在*图8.1*中：
- en: '![](img/37f526d5-f4cd-42bf-b81a-c1f884b173bb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37f526d5-f4cd-42bf-b81a-c1f884b173bb.png)'
- en: Figure 8.1 – Batch normalization layer main properties
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 批量归一化层的主要属性
- en: The authors of *batch norm*, as it is often called by data scientists, introduced
    this simple mechanism to accelerate training or model convergence by providing
    stability to the calculation of gradients and how they affect the update of the
    weights across different layers of neurons. This is because they can prevent gradient
    vanishing or explosion, which is a natural consequence of gradient-based optimization
    operating on DL models. That is, the deeper the model is, the way the gradient
    affects the layers and individual units in deeper layers can have the effect of
    large updates or very small updates that can lead to variable overflow or numerical-zero
    values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 被数据科学家称为*批量归一化*的机制，通过为梯度计算提供稳定性，并调整它们如何影响神经网络不同层权重的更新，从而加速了训练或模型的收敛。这是因为它可以防止梯度消失或爆炸，这是基于梯度的优化在深度学习模型中的自然结果。也就是说，模型越深，梯度如何影响更深层的层和单个单位，可能会导致大幅度更新或非常小的更新，这可能导致变量溢出或数值为零。
- en: 'As illustrated at the top of *Figure 8.2*, batch normalization has the ability
    to regulate the boundaries of the input data by normalizing the data that goes
    in so that the output follows a normal distribution. The bottom of the figure
    illustrates where batch normalization is applied, that is, within the neuron right
    before sending the output to the next layer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 8.2*顶部所示，批量归一化具有通过归一化输入数据来调节输入数据边界的能力，从而使输出遵循正态分布。图底部说明了批量归一化应用的位置，即在神经元内部，在将输出发送到下一层之前：
- en: '![](img/a4f3dd44-a3c2-4505-8287-996983c125dc.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f3dd44-a3c2-4505-8287-996983c125dc.png)'
- en: Figure 8.2 – Batch normalization on a simple autoencoder
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 简单自编码器的批量归一化
- en: 'Consider having a (mini-)batch of data, ![](img/4e0de339-6188-4267-a7d6-f8310f64141f.png),
    of size ![](img/f575e97f-0ad0-4d83-b281-0f35baff870a.png), which allows us to
    define the following equations. First, the mean of the batch, at layer ![](img/e5cd3a66-efd6-4e6f-9fce-a8fdd940b0bd.png),
    can be calculated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑有一个（小）批次的数据，![](img/4e0de339-6188-4267-a7d6-f8310f64141f.png)，大小为![](img/f575e97f-0ad0-4d83-b281-0f35baff870a.png)，这允许我们定义以下方程式。首先，批次的均值，在层![](img/e5cd3a66-efd6-4e6f-9fce-a8fdd940b0bd.png)上，可以按如下方式计算：
- en: '![](img/30413e7c-0c75-4ebe-9c23-f8005cae11eb.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30413e7c-0c75-4ebe-9c23-f8005cae11eb.png)'
- en: 'The corresponding standard deviation is calculated as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的标准差可以按如下方式计算：
- en: '![](img/cdb6c81f-6cb2-4c00-9901-f0389a4bc6c1.png).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/cdb6c81f-6cb2-4c00-9901-f0389a4bc6c1.png)。'
- en: 'Then, we can normalize every unit ![](img/76f8290e-3700-4064-9e45-9d2ee62a754e.png) in
    layer ![](img/cb840e84-cf93-45a2-a7b2-29bb2eae0783.png) as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按如下方式归一化层![](img/cb840e84-cf93-45a2-a7b2-29bb2eae0783.png)中的每个单位![](img/76f8290e-3700-4064-9e45-9d2ee62a754e.png)：
- en: '![](img/a0fa2623-dd01-4b71-88d1-5d93f35557dc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0fa2623-dd01-4b71-88d1-5d93f35557dc.png)'
- en: 'Here, ![](img/1227909d-af4a-4fac-b9ec-397992525707.png) is a constant introduced
    merely for numerical stability, but can be altered as needed. Finally, the normalized
    neural output of every unit ![](img/90c9d851-81ed-4cda-8c3e-7f34a7552b23.png) in
    layer ![](img/474e8170-88cb-4656-9737-d964b318a040.png), ![](img/db17d453-c78e-44b6-9c54-265c14cb0674.png),
    can be calculated before it goes into the activation function as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/1227909d-af4a-4fac-b9ec-397992525707.png)是为了数值稳定性引入的常数，但可以根据需要进行调整。最后，每个单位![](img/90c9d851-81ed-4cda-8c3e-7f34a7552b23.png)在层![](img/474e8170-88cb-4656-9737-d964b318a040.png)的归一化神经输出，![](img/db17d453-c78e-44b6-9c54-265c14cb0674.png)，可以在进入激活函数之前按如下方式计算：
- en: '![](img/3443d3f0-ccf0-422c-81a3-f8af404ca4c8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3443d3f0-ccf0-422c-81a3-f8af404ca4c8.png)'
- en: Here, ![](img/4cb094b3-7c28-451f-bce4-1cd64bdd67e1.png) and ![](img/1ac6ba44-eb6b-43d4-aac8-e42df3047cea.png) are
    parameters that need to be learned for each neural unit. After this, any choice
    of activation function at unit ![](img/899c2cb8-2ba1-4548-9d22-37a526fb0f50.png) in
    layer ![](img/6e5c045c-0614-444b-acd3-7110b61ada78.png) will receive the normalized
    input, [![](img/c8199dba-4ba0-4160-99f1-f26f0912be69.png)], and produce an output
    that is optimally normalized to minimize the loss function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/4cb094b3-7c28-451f-bce4-1cd64bdd67e1.png)和![](img/1ac6ba44-eb6b-43d4-aac8-e42df3047cea.png)是每个神经单位需要学习的参数。之后，层![](img/6e5c045c-0614-444b-acd3-7110b61ada78.png)中单位![](img/899c2cb8-2ba1-4548-9d22-37a526fb0f50.png)的任何激活函数选择都会接收到归一化的输入，[![](img/c8199dba-4ba0-4160-99f1-f26f0912be69.png)]，并产生一个输出，该输出经过最佳归一化，以最小化损失函数。
- en: 'One easy way to look at the benefits is to imagine the normalization process:
    although it occurs at each unit, the learning process itself determines the best
    normalization that is required to maximize the performance of the model (loss
    minimization). Therefore, it has the capability to nullify the effects of the
    normalization if it is not necessary for some feature or latent spaces, or it
    can also use the normalization effects. The important point to remember is that,
    when batch normalization is used, the learning algorithm will learn to use normalization
    optimally.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看收益的一种简单方法是想象归一化过程：虽然它发生在每个单元上，但学习过程本身决定了所需的最佳归一化方式，以最大化模型的性能（最小化损失）。因此，它能够在某些特征或潜在空间中，如果归一化不必要，就消除归一化的效果，或者它也可以利用归一化的效果。需要记住的一个重要点是，当使用批量归一化时，学习算法会学会如何最优化地使用归一化。
- en: 'We can use `tensorflow.keras.layers.BatchNormalization` to create a batch normalization
    layer as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tensorflow.keras.layers.BatchNormalization`来创建一个批量归一化层，如下所示：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is obviously done using the functional paradigm. Consider the following
    example of a dataset corresponding to movie reviews, called *IMDb* (Maas, A. L.,
    et al. (2011)), which we will explain in more detail in [Chapter 13](a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml),
    *Recurrent Neural Networks*. In this example, we are simply trying to prove the
    effects of adding a batch normalization layer as opposed to not having one. Take
    a close look at the following code fragment:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是使用函数式编程范式完成的。考虑以下示例，它是一个关于电影评论的数据集，名为*IMDb*（Maas, A. L., 等人，2011），我们将在[第13章](a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml)《循环神经网络》中详细解释。在这个例子中，我们只是尝试证明添加批量归一化层与不添加的效果。仔细看看下面的代码片段：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And we proceed with building the model:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们继续构建模型：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this code fragment, batch normalization is placed right before the activation
    layer. This will, therefore, normalize the input to the activation function, which
    in this case is a `sigmoid.` Similarly, we can build the same model without a
    batch normalization layer as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，批量归一化层被放置在激活层之前。因此，它将归一化输入到激活函数，在本例中是`sigmoid`。类似地，我们也可以构建一个不带批量归一化层的相同模型，如下所示：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we train both models and plot their performance as they minimize the loss
    function, we will notice quickly that having batch normalization pays off, as
    shown in *Figure 8.3*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练这两个模型，并绘制它们在最小化损失函数时的表现，我们会很快注意到，使用批量归一化会带来显著的效果，如*图 8.3*所示：
- en: '![](img/12072ed9-ec0b-4784-a47d-b726124dc480.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12072ed9-ec0b-4784-a47d-b726124dc480.png)'
- en: Figure 8.3 – Comparison of learning progress with and without batch normalization
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 带有和不带批量归一化的学习进度比较
- en: The figure indicates that having batch normalization has the effect of reducing
    the loss function both in training and in validation sets of data. These results
    are consistent with many other experiments that you can try on your own! However,
    as we said before, it is not necessarily a guarantee that this will happen all
    the time. This is a relatively modern technique that has proven to function properly
    so far, but this does not mean that it works for everything that we know of.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图表表明，使用批量归一化的效果是在训练集和验证集的损失函数都减少。这些结果与许多你可以自己尝试的实验是一致的！然而，正如我们之前所说的，这并不一定意味着每次都会发生这种情况。这是一种相对现代的技术，迄今为止已证明它能正常工作，但这并不意味着它对我们已知的所有情况都有效。
- en: We highly recommend that in all your models, you first try to solve the problem
    with a model that has no batch normalization, and then once you feel comfortable
    with the performance you have, come back and use batch normalization to see if
    you can get a slight boost in **performance** and **training speed**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议在所有模型中，首先尝试使用没有批量归一化的模型来解决问题，然后在你对现有性能感到满意时，再回来使用批量归一化，看看是否能略微提升**性能**和**训练速度**。
- en: Let's say that you tried batch normalization and you were rewarded with a boost
    in performance, speed, or both, but you have now discovered that your model was
    overfitting all this time. Fear not! There is another interesting and novel technique,
    known as **dropout**. This can offer a model an alternative to reduce overfitting,
    as we will discuss in the following section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你尝试了批量归一化，并且得到了性能、速度或两者的提升，但现在你发现模型一直在过拟合。别担心！还有一种有趣且新颖的技术，叫做**随机失活**。正如我们在接下来的部分所讨论的，它可以为模型提供一种减少过拟合的替代方法。
- en: Dropout
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机失活
- en: 'Dropout is a technique published in 2014 that became popular shortly after
    that year (Srivastava, N., Hinton, G., et.al. (2014)). It came as an alternative
    to combat overfitting, which is one of its major properties, and can be summarized
    as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种于2014年发布的技术，并在发布后不久迅速流行开来（Srivastava, N., Hinton, G., 等（2014））。它作为一种应对过拟合的替代方法，而过拟合正是它的主要特性之一，具体可以总结如下：
- en: It can reduce the chances of overfitting.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以减少过拟合的机会。
- en: It can lead to better generalization.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以提高模型的泛化能力。
- en: It can reduce the effect of dominating neurons.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以减少主导神经元的影响。
- en: It can promote neuron diversity.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以促进神经元的多样性。
- en: It can promote better neuron teamwork.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以促进更好的神经元协作。
- en: 'The block image we will use for dropout, along with its main properties, is
    shown in *Figure 8.4*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Dropout中使用的块状图像及其主要特性如*图8.4*所示：
- en: '![](img/20c83967-c1c8-4cbc-a56d-0702da606ca5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20c83967-c1c8-4cbc-a56d-0702da606ca5.png)'
- en: Figure 8.4 – Dropout layer properties
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – Dropout层特性
- en: 'A dropout strategy works because it enables the network to search for an alternative
    hypothesis to solve the problem by disconnecting a particular number of neurons
    that represent certain hypotheses (or models) within a network itself. One easy
    way to look at this strategy is by thinking about the following: Imagine that
    you have a number of experts that are tasked with passing judgment on whether
    an image contains a cat or a chair. There might be a large number of experts that
    moderately believe that there is a chair in the image, but it only takes one expert
    to be particularly loud and fully convinced that there is a cat to persuade the
    decision-maker into listening to this particularly loud expert and ignoring the
    rest. In this analogy, experts are neurons.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout策略之所以有效，是因为它通过断开表示某些假设（或模型）的特定神经元，使网络能够寻找替代假设来解决问题。用一个简单的方式来看待这种策略：假设你有一群专家负责判断一张图片中是否包含猫或椅子。可能有大量专家认为图片中有椅子，但只需要有一个专家特别大声、非常确信图片中有猫，就足以说服决策者去听这个特别大声的专家，而忽视其他专家。在这个类比中，专家就是神经元。
- en: 'There might be some neurons that are particularly convinced (sometimes incorrectly,
    due to overfitting on irrelevant features) of a certain fact about the information,
    and their output values are particularly high compared to the rest of the neurons
    in that layer, so much so that the deeper layers learn to listen more to that
    particular layer, thus perpetuating overfitting on deeper layers. **Dropout**
    is the mechanism that will select a number of neurons in a layer and completely
    disconnect them from the layer so that no input flows into those neurons nor is
    there output coming out of those neurons, as shown in *Figure 8.5*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有一些神经元特别确信（有时由于对无关特征的过拟合，错误地确信）某个事实，而它们的输出值与该层中其他神经元相比特别高，甚至以至于更深层的网络学会更多地依赖这一层，从而导致更深层的过拟合。**Dropout**是一个机制，它会选择该层中的一部分神经元，并将它们完全从该层断开，使得没有输入流入这些神经元，也没有输出从这些神经元流出，如*图8.5*所示：
- en: '![](img/7b2a57b2-b790-49fc-9060-b8f35c0a5ced.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b2a57b2-b790-49fc-9060-b8f35c0a5ced.png)'
- en: Figure 8.5 – Dropout mechanism over the first hidden layer. Dropout here disconnects
    one neuron from the layer
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – Dropout机制在第一个隐藏层上的应用。这里的Dropout断开了一个神经元与该层的连接
- en: 'In the preceding diagram, the first hidden layer has a dropout rate of one
    third. This means that, completely at random, one third of the neurons will be
    disconnected. *Figure 8.5* shows an example of when the second neuron in the first
    hidden layer is disconnected: no input from the input layer goes in, and no output
    comes out of it. The model is completely oblivious to its existence; for all practical
    purposes, this is a different neural network!'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，第一个隐藏层的Dropout率为三分之一。这意味着，完全随机地，三分之一的神经元将会被断开。*图8.5*展示了当第一个隐藏层的第二个神经元被断开时的情况：没有来自输入层的输入进入，也没有输出从它那里出来。模型完全不知道它的存在；从实际操作的角度来看，这就像是一个不同的神经网络！
- en: 'However, the neurons that are disconnected are only disconnected for one training
    step: their weights are unchanged for one training step, while all other weights
    are updated. This has a few interesting implications:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，被断开的神经元仅在一次训练步骤中断开：它们的权重在一次训练步骤内保持不变，而所有其他权重会被更新。这有一些有趣的影响：
- en: Due to the random selection of neurons, those *troublemakers* that tend to dominate
    (overfit) on particular features are bound to be selected out at some point, and
    the rest of the neurons will learn to process feature spaces without those *troublemakers*.
    This leads to the prevention and reduction of overfitting, while promoting collaboration
    among diverse neurons that are experts in different things.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于神经元的随机选择，那些倾向于主导（过拟合）某些特征的*麻烦制造者*最终会被选中，而其余的神经元将学会在没有这些*麻烦制造者*的情况下处理特征空间。这有助于防止和减少过拟合，同时促进不同神经元之间的协作，它们在不同领域具有专业知识。
- en: Due to the constant ignorance/disconnection of neurons, the network has the
    potential of being fundamentally different – it is almost as if we are training
    multiple neural networks in every single step without actually having to make
    many different models. It all happens because of dropout.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于神经元的持续忽略/断开连接，网络有可能从根本上发生变化——几乎就像我们在每一步训练中都在训练多个神经网络，而实际上并不需要创建许多不同的模型。这一切都是由于
    dropout 的原因。
- en: It is usually recommended to use dropout in deeper networks to ameliorate the
    traditional problem of overfitting that is common in DL.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常建议在更深的网络中使用 dropout，以改善深度学习中常见的过拟合问题。
- en: 'To show the difference in performance when using dropout, we will use the exact
    same dataset as in the previous section, but we will add an additional layer in
    the autoencoder as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示使用 dropout 时性能的差异，我们将使用与上一节相同的数据集，但我们将在自编码器中添加一个额外的层，如下所示：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code, the dropout rate is 10%, meaning that 10% of the neurons in the
    dense layer `e14` are disconnected multiple times at random during training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，dropout 率为 10%，意味着在训练过程中，`e14` 密集层中 10% 的神经元会被随机断开连接多次。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The decoder is left exactly the same as before, and the baseline model simply
    does not contain a dropout layer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与之前完全相同，基线模型仅不包含 dropout 层：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we choose `''adagrad''` and we perform training over 100 epochs and compare
    the performance results, we can obtain the performance shown in *Figure 8.6*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择`'adagrad'`并在 100 个 epoch 上进行训练并比较性能结果，我们可以获得*图 8.6*中所示的性能：
- en: '![](img/e582e979-2bb1-45a8-989d-454a99610665.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e582e979-2bb1-45a8-989d-454a99610665.png)'
- en: Figure 8.6 – Autoencoder reconstruction loss comparing models with dropout and
    without
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 比较带有 dropout 和不带 dropout 的模型的自编码器重建损失
- en: 'Here is the full code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整代码：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we define the model with dropout like so:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们像这样定义带有 dropout 的模型：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we compile it, train it, store the training history, and clear the variables
    to re-use them as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对其进行编译、训练，存储训练历史记录，并清除变量以便重新使用，如下所示：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And then we do the same for a model without dropout:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对一个不带 dropout 的模型进行相同的操作：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next we gather the training data and plot it like so:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们收集训练数据并像这样绘制它：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From *Figure 8.6* we can see that the performance of the model with dropout
    is superior than without. This suggests that training without dropout has a higher
    chance of overfitting, the reason being that the learning curve is worse on the
    validation set when dropout is not used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 8.6*中我们可以看到，带有 dropout 的模型表现优于不带 dropout 的模型。这表明，训练时没有使用 dropout 更容易发生过拟合，原因是当不使用
    dropout 时，验证集上的学习曲线较差。
- en: As mentioned earlier, the `adagrad` optimizer has been chosen for this particular
    task. We made this decision because it is important for you to learn more optimizers,
    one at a time. Adagrad is an adaptive algorithm; it performs updates with respect
    to the frequency of features (Duchi, J., et al. (2011)). If features occur frequently,
    the updates are small, while larger updates are done for features that are out
    of the ordinary.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`adagrad` 优化器已被选择用于此任务。我们做出这个决定是因为你应该逐步学习更多的优化器。Adagrad 是一种自适应算法；它根据特征的频率来进行更新（Duchi,
    J. 等人，2011）。如果某个特征出现频率较高，更新就会较小，而对于那些不常见的特征，则会进行较大的更新。
- en: It is recommended to use Adagrad when the **dataset is sparse**. For example,
    in word embedding cases such as the one in this example, frequent words will cause
    small updates, while rare words will require larger updates.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当**数据集稀疏**时，建议使用 Adagrad。例如，在像本例中这样的词嵌入任务中，频繁出现的词会导致小的更新，而稀有词则需要较大的更新。
- en: Finally, it is important to mention that `Dropout(rate)` belongs to the `tf.keras.layers.Dropout`
    class. The rate that is taken as a parameter corresponds to the rate at which
    neurons will be disconnected at random at every single training step for the particular
    layer on which dropout is used.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要提到的是，`Dropout(rate)`属于`tf.keras.layers.Dropout`类。作为参数传入的rate值对应着每次训练步骤中该层的神经元将随机断开的比率。
- en: It is recommended that you use a dropout rate between **0.1 and 0.5** to achieve
    significant changes to your network's performance. And it is recommended to use
    dropout **only in deep networks**. However, these are empirical findings and your
    own experimentation is necessary.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐使用介于**0.1和0.5**之间的dropout率，以实现对网络性能的显著改进。并且建议**仅在深度网络中使用dropout**。不过，这些是经验性的发现，您需要通过自己的实验来验证。
- en: Now that we have explained these two relatively new concepts, dropout and batch
    normalization, we will create a deep autoencoder network that is relatively simple
    and yet powerful in finding latent representations that are not biased toward
    particular labels.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了这两个相对较新的概念——dropout（丢弃法）和batch normalization（批量归一化），接下来我们将创建一个相对简单但强大的深度自编码器网络，用于发现不偏向特定标签的潜在表示。
- en: Exploring latent spaces with deep autoencoders
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度自编码器探索潜在空间
- en: Latent spaces, as we defined them in [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*,
    are very important in DL because they can lead to powerful decision-making systems
    that are based on assumed rich latent representations. And, once again, what makes
    the latent spaces produced by autoencoders (and other unsupervised models) rich
    in their representations is that they are not biased toward particular labels.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间，正如我们在[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml) *自编码器*中所定义的那样，在深度学习中非常重要，因为它们可以导致基于假设丰富潜在表示的强大决策系统。而且，正是由于自编码器（和其他无监督模型）产生的潜在空间不偏向特定标签，使得它们在表示上非常丰富。
- en: In [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*,
    we explored the MNIST dataset, which is a standard dataset in DL, and showed that
    we can easily find very good latent representations with as few as four dense
    layers in the encoder and eight layers for the entire autoencoder model. In the
    next section, we will take on a much more difficult dataset known as CIFAR-10,
    and then we will come back to explore the latent representation of the `IMDB`
    dataset, which we have already explored briefly in the previous sections of this
    chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml) *自编码器*中，我们探讨了MNIST数据集，这是深度学习中的标准数据集，并展示了通过仅使用四个密集层的编码器和整个自编码器模型的八层，我们能够轻松地找到非常好的潜在表示。在下一节中，我们将处理一个更为复杂的数据集——CIFAR-10，之后我们会回到探索`IMDB`数据集的潜在表示，该数据集我们在本章前面部分已经简要探讨过。
- en: CIFAR-10
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: 'In 2009, the *Canadian Institute for Advanced Research (CIFAR)* released a
    very large collection of images that can be used to train DL models to recognize
    a variety of objects. The one we will use in this example is widely known as CIFAR-10,
    since it has only 10 classes and a total of 60,000 images; *Figure 8.7* depicts
    samples of each class:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，*加拿大高级研究院（CIFAR）*发布了一个非常大的图像集合，可以用来训练深度学习模型识别各种物体。我们将在本例中使用的这个数据集被广泛称为CIFAR-10，因为它仅包含10个类别，总共有60,000张图像；*图8.7*展示了每个类别的样本：
- en: '![](img/4e66fc36-252d-4f4b-b881-690e8d57122c.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e66fc36-252d-4f4b-b881-690e8d57122c.png)'
- en: Figure 8.7 – Sample images from the CIFAR-10 dataset. The number indicates the
    numeric value assigned to each class for convenience
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 来自CIFAR-10数据集的样本图像。数字表示每个类别分配的数值，方便起见。
- en: Every image in the dataset is 32 by 32 pixels using 3 dimensions to keep track
    of the color details. As can be seen from the figure, these small images contain
    other objects beyond those labeled, such as text, background, structures, landscapes,
    and other partially occluded objects, while preserving the main object of interest
    in the foreground. This makes it more challenging than MNIST, where the background
    is always black, images are grayscale, and there is only one number in every image.
    If you have never worked in computer vision applications, you may not know that
    it is exponentially more complicated to deal with CIFAR-10 compared to MNIST.
    Therefore, our models need to be more robust and deep in comparison to MNIST ones.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每张图像为32x32像素，使用3个维度来跟踪颜色细节。从图中可以看到，这些小图像包含了标签之外的其他物体，如文本、背景、结构、景观以及其他部分遮挡的物体，同时保留了前景中的主要兴趣物体。这使得它比MNIST更具挑战性，因为MNIST的背景始终是黑色的，图像是灰度的，每张图像中只有一个数字。如果你从未接触过计算机视觉应用，可能不知道与MNIST相比，处理CIFAR-10要复杂得多。因此，我们的模型需要比MNIST模型更具鲁棒性和深度。
- en: 'In TensorFlow and Keras, we can easily load and prepare our dataset with the
    following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow和Keras中，我们可以使用以下代码轻松加载和准备我们的数据集：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code outputs the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出如下：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This says that we have one-sixth of the dataset (~16%) separated for test purposes,
    while the rest is used for training. The 3,072 dimensions come from the number
    of pixels and channels: ![](img/13725260-8e91-4e8f-bb67-204f0f8917c4.png). The
    preceding code also normalizes the data from the range [0, 255] down to [0.0,
    1.0] in floating-point numbers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示我们有六分之一的数据显示为测试数据（约16%），其余的用于训练。3,072维来自于像素和通道的数量：![](img/13725260-8e91-4e8f-bb67-204f0f8917c4.png)。上述代码还将数据从[0,
    255]的范围归一化到[0.0, 1.0]的浮动数值。
- en: 'To move on with our example, we will propose a deep autoencoder with the architecture
    shown in *Figure 8.8*, which will take a 3,072-dimensional input and will encode
    it down to 64 dimensions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续我们的示例，我们将提出一个如*图8.8*所示架构的深度自编码器，它将接受一个3,072维的输入，并将其编码为64维：
- en: '![](img/a5b1d547-5beb-43a1-8ce4-eda0b318876d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5b1d547-5beb-43a1-8ce4-eda0b318876d.png)'
- en: Figure 8.8 – Architecture of a deep autoencoder on the CIFAR-10 dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – CIFAR-10数据集上深度自编码器的架构
- en: This architecture uses 17 layers in the encoder and 15 layers in the decoder.
    Dense layers in the diagram have the number of neurons written in their corresponding
    block. As can be seen, this model implements a series of strategic batch normalization
    and dropout strategies throughout the process of encoding the input data. In this
    example, all dropout layers have a 20% dropout rate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构在编码器中使用了17层，在解码器中使用了15层。图中的密集层对应块内写有神经元的数量。可以看到，该模型在编码输入数据的过程中实现了一系列战略性的批量归一化和丢弃层策略。在此示例中，所有丢弃层的丢弃率为20%。
- en: 'If we train the model for 200 epochs using the standard `adam` optimizer and
    the standard binary cross-entropy loss, we could obtain the training performance
    shown in *Figure 8.9*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用标准的`adam`优化器和标准的二元交叉熵损失函数训练模型200个epoch，我们可以获得如*图8.9*所示的训练性能：
- en: '![](img/39b4e755-a820-4fe6-9432-eaf516ec6a13.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39b4e755-a820-4fe6-9432-eaf516ec6a13.png)'
- en: Figure 8.9 – Reconstruction of the loss of the deep autoencoder model on CIFAR-10
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 深度自编码器模型在CIFAR-10上的损失重建
- en: 'Here is the full code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We define the model as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型定义如下：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next we define the decoder portion of the model like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将模型的解码器部分定义如下：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We put it together in an autoencoder model, compile it and train it like so:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们组合成一个自编码器模型，进行编译并像这样训练：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The model performance shown in *Figure 8.9* converges nicely and loss decays
    both on the training and test sets, which implies that the model is not overfitting
    and continues to adjust the weights properly over time. To visualize the model''s
    performance on unseen data (the test set), we can simply pick samples from the
    test set at random, such as the ones in *Figure 8.10*, which produce the output
    shown in *Figure 8.11*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.9*所示的模型性能很好地收敛，训练集和测试集上的损失都在减少，这意味着模型没有过拟合，并且随着时间的推移，继续适当调整权重。为了可视化模型在未见数据（测试集）上的表现，我们可以简单地随机选择测试集中的样本，如*图8.10*中的样本，这些样本产生的输出如*图8.11*所示：
- en: '![](img/a502db01-f2ff-4c8f-987d-d8877f7c0e70.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a502db01-f2ff-4c8f-987d-d8877f7c0e70.png)'
- en: Figure 8.10 – Sample input from the test set of CIFAR-10
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – CIFAR-10测试集中的样本输入
- en: '![](img/5befa42e-5531-46a3-89c2-6fedf3830087.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5befa42e-5531-46a3-89c2-6fedf3830087.png)'
- en: Figure 8.11 – Sample output (reconstructions) from the samples given in Figure
    8.10
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 来自图8.10中样本的输出（重构）
- en: You can see from *Figure 8.11* that the reconstructions correctly address the
    color spectrum of the input data. However, it is clear that the problem is much
    harder in reconstruction terms than MNIST. Shapes are blurry although they seem
    to be in the correct spatial position. A level of detail is evidently missing
    in the reconstructions. We can make the autoencoder deeper, or train for longer,
    but the problem might not be properly solved. We can justify this performance
    with the fact that we deliberately chose to find a latent representation of size
    64, which is smaller than a tiny 5 by 5 image: ![](img/861e2ca8-20e9-4276-94b4-bb24d061553c.png).
    If you think about it and reflect on this, then it is clear that it is nearly
    impossible, as 3,072 to 64 represents a 2.08% compression!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图8.11*中可以看到，重构正确地处理了输入数据的色谱。然而，很明显，问题在重构方面比MNIST更难。形状模糊，尽管它们似乎位于正确的空间位置。重构中显然缺少一些细节。我们可以让自编码器更深，或者训练更长时间，但问题可能并没有得到正确解决。我们可以通过一个事实来解释这个性能，那就是我们故意选择了一个大小为64的潜在表示，这个大小甚至小于一个只有5×5像素的图像：![](img/861e2ca8-20e9-4276-94b4-bb24d061553c.png)。如果你仔细想一想，反思一下，就会明白这是几乎不可能的，因为从3,072到64意味着压缩了2.08%！
- en: 'A solution to this would be not to make the model larger, but to acknowledge
    that the latent representation size might not be large enough to capture relevant
    details of the input to have a good reconstruction. The current model might be
    too aggressive in reducing the dimensionality of the feature space. If we use
    UMAP to visualize the 64-dimensional latent vectors in 2 dimensions, we will obtain
    the plot shown in *Figure 8.12*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法不是让模型变得更大，而是要认识到潜在表示的大小可能不足以捕捉输入的相关细节，从而进行良好的重构。当前的模型可能在降低特征空间的维度时过于激进。如果我们使用UMAP将64维的潜在向量可视化为2维，我们会得到如*图8.12*所示的图形：
- en: '![](img/c9f94d24-4eb7-457f-9ae4-4d0d8dbbefe2.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9f94d24-4eb7-457f-9ae4-4d0d8dbbefe2.png)'
- en: Figure 8.12 – UMAP two-dimensional representation of the latent vectors in the
    test set
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 测试集中的潜在向量的UMAP二维表示
- en: We have not spoken about UMAP before, but we will briefly state that this a
    ground-breaking data visualization tool that has been proposed recently and is
    starting to gain attention (McInnes, L., et al. (2018)). In our case, we simply
    used UMAP to visualize the data distribution since we are not using the autoencoder
    to encode all the way down to two dimensions. *Figure 8.12* indicates that the
    distribution of the classes is not sufficiently clearly defined so as to enable
    us to observe separation or well-defined clusters. This confirms that the deep
    autoencoder has not captured sufficient information for class separation; however,
    there are still clearly defined groups in some parts of the latent space, such
    as the clusters on the bottom middle and left, one of which is associated with
    a group of airplane images, for example. This **deep belief network **has acquired
    knowledge about the input space well enough to make out some different aspects
    of the input; for example, it knows that airplanes are quite different from frogs,
    or at least that they might appear in different conditions, that is, a frog will
    appear against green backgrounds while an airplane is likely to have blue skies
    in the background.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有提到UMAP，但我们简要说明一下，UMAP是最近提出的一种开创性数据可视化工具，正在开始引起关注（McInnes, L., 等人（2018））。在我们的案例中，我们仅仅使用UMAP来可视化数据分布，因为我们并没有让自编码器将数据编码到二维。*图8.12*表明，类的分布没有足够清晰的定义，无法让我们观察到分离或明显的聚类。这确认了深度自编码器并没有捕捉到足够的信息来进行类的分离；然而，在潜在空间的某些部分，仍然可以看到明显定义的组，例如位于底部中间和左侧的聚类，其中一个聚类与一组飞机图像相关。例如，这个**深度信念网络**已经足够了解输入空间的信息，能够区分输入的某些不同特征；例如，它知道飞机与青蛙有很大区别，或者至少它们可能出现在不同的环境中，也就是说，青蛙会出现在绿色背景下，而飞机可能出现在蓝色天空的背景下。
- en: '**Convolutional neural networks** (**CNNs**)are a much better alternative for
    most computer vision and image analysis problems such as this one. We will get
    there in due time in [Chapter 12](c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml),
    *Convolutional Neural Networks*. Be patient for now as we gently introduce different
    models one by one. You will see how we can make a convolutional autoencoder that
    can achieve much better performance than an autoencoder that uses fully connected
    layers. For now, we will continue with autoencoders for a little longer.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）对于大多数计算机视觉和图像分析问题（如本例）是一个更好的选择。我们将在[第 12 章](c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml)中讲到*卷积神经网络*。请耐心等待，我们将逐步介绍不同的模型。你将看到我们如何制作一个卷积自编码器，它能比使用全连接层的自编码器实现更好的性能。目前，我们将继续使用自编码器，稍微再多讲一会儿。'
- en: 'The model introduced in *Figure 8.8* can be produced using the functional approach;
    the encoder can be defined as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.8*中介绍的模型可以通过函数式方法实现；编码器可以定义如下：'
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Notice that all the dropout layers have a 20% rate every time. The 17 layers
    go from mapping an input of `inpt_dim=3072` dimensions down to `ltnt_dim = 64`
    dimensions. The last activation function of the encoder is the hyperbolic tangent `tanh`,
    which provides an output in the range [-1,1]; this choice is only made for convenience
    in visualizing the latent space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有的丢弃层每次的丢弃率为 20%。这 17 层将输入 `inpt_dim=3072` 维度映射到 `ltnt_dim = 64` 维度。编码器的最后一个激活函数是双曲正切函数
    `tanh`，其输出范围为 [-1,1]；这个选择仅仅是为了方便可视化潜在空间。
- en: 'Next, the definition of the decoder is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，解码器的定义如下：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The last layer of the decoder has a `sigmoid` activation function that maps
    back to the input space range, that is, [0.0, 1.0]. Finally, we can train the
    `autoencoder` model as previously defined using the `binary_crossentropy` loss
    and `adam` optimizer for 200 epochs like so:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的最后一层具有 `sigmoid` 激活函数，将其映射回输入空间范围，即[0.0, 1.0]。最后，我们可以像以前定义的那样，使用 `binary_crossentropy`
    损失和 `adam` 优化器，训练 `autoencoder` 模型 200 个 epoch，如下所示：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The results have been previously shown in *Figures 8.9* to 8.*11*. However,
    it is interesting to revisit MNIST, but this time using a deep autoencoder, as
    we will discuss next.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果已在*图 8.9*到*图 8.11*中展示过。然而，重新审视 MNIST 是很有意思的，但这次我们将使用深度自编码器，接下来会讨论。
- en: MNIST
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST
- en: 'The `MNIST` dataset is a good example of a dataset that is less complex than
    CIFAR-10, and that can be approached with a deep autoencoder. Previously, in [Chapter
    7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*, we discussed shallow
    autoencoders and showed that adding layers was beneficial. In this section, we
    go a step further to show that a deep autoencoder with dropout and batch normalization
    layers can perform better at producing rich latent representations. *Figure 8.13*
    shows the proposed architecture:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST` 数据集是一个很好的示例，它的复杂度低于 CIFAR-10，可以使用深度自编码器进行处理。在[第 7 章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml)中，*自编码器*，我们讨论了浅层自编码器，并展示了增加层数是有益的。在本节中，我们更进一步，展示了具有丢弃层（dropout）和批量归一化层（batch
    normalization）的深度自编码器如何在生成丰富的潜在表示方面表现得更好。*图 8.13*展示了提出的架构：'
- en: '![](img/45e95b72-b8c6-4ea4-9c14-742b2d699963.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45e95b72-b8c6-4ea4-9c14-742b2d699963.png)'
- en: Figure 8.13 – Deep autoencoder for MNIST
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 用于 MNIST 的深度自编码器
- en: 'The number of layers and the sequence of layers is the same as in *Figure 8.8*;
    however, the number of neurons in the dense layers and the latent representation
    dimensions have changed. The compression rate is from 784 to 2, or 0.25%:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 层的数量和层的顺序与*图 8.8*中的相同；然而，密集层中的神经元数量和潜在表示的维度发生了变化。压缩率从 784 降到 2，或 0.25%：
- en: '![](img/a6763900-9f9b-405e-a06e-928342690342.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6763900-9f9b-405e-a06e-928342690342.png)'
- en: Figure 8.14 – MNIST original sample digits of the test set
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – 测试集的 MNIST 原始样本数字
- en: 'And yet, the reconstructions are very good, as shown in *Figure 8.14* and in
    *Figure 8.15*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重建结果非常好，如*图 8.14*和*图 8.15*所示：
- en: '![](img/6842fab6-7196-442d-9768-48f4f29ab489.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6842fab6-7196-442d-9768-48f4f29ab489.png)'
- en: Figure 8.15 – Reconstructed MNIST digits from the original test set shown in
    *Figure 8.14*
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – 从原始测试集重建的 MNIST 数字，如*图 8.14*所示
- en: 'The reconstructions shown in the figure show a level of detail that is very
    good, although it seems blurry around the edges. The general shape of the digits
    seems to be captured well by the model. The corresponding latent representations
    of the test set are shown in *Figure 8.16*:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示的重建图像具有非常好的细节，尽管在边缘看起来有些模糊。模型似乎很好地捕捉到了数字的整体形状。测试集的相应潜在表示显示在*图8.16*中：
- en: '![](img/a3ad8424-e567-49ed-808e-d340eac53c77.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3ad8424-e567-49ed-808e-d340eac53c77.png)'
- en: Figure 8.16 – Latent representation of MNIST digits in the test set, partially
    shown in *Figure 8.14*
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 测试集中MNIST数字的潜在表示，部分展示于*图8.14*中
- en: From the preceding plot, we can see that there are well-defined clusters; however,
    it is important to point out that the autoencoder knows nothing about labels and
    that these clusters have been learned from the data alone. This is the power of
    autoencoders at their best. If the encoder model is taken apart and re-trained
    with labels, the model is likely to perform even better. However, for now, we
    will leave it here and continue with a type of *generative* model in [Chapter
    9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*, which
    comes next.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到有清晰的聚类；然而，需要指出的是，自动编码器对标签一无所知，这些聚类完全是从数据中学习得出的。这正是自动编码器的强大之处。如果将编码器模型拆开并使用标签重新训练，模型的性能可能会更好。但现在，我们暂且停在这里，继续讨论下一章中的一种*生成*模型——[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自动编码器*。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This intermediate chapter showed the power of deep autoencoders when combined
    with regularization strategies such as dropout and batch normalization. We implemented
    an autoencoder that has more than 30 layers! That's *deep*! We saw that in difficult
    problems a deep autoencoder can offer an unbiased latent representation of highly
    complex data, as most deep belief networks do. We looked at how dropout can reduce
    the risk of overfitting by ignoring (disconnecting) a fraction of the neurons
    at random in every learning step. Furthermore, we learned that batch normalization
    can offer stability to the learning algorithm by gradually adjusting the response
    of some neurons so that activation functions and other connected neurons don't
    saturate or overflow numerically.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了深度自动编码器在结合正则化策略（如dropout和批量归一化）时的强大功能。我们实现了一个超过30层的自动编码器！这真是*深度*！我们看到，在复杂问题中，深度自动编码器能够提供对高度复杂数据的无偏潜在表示，就像大多数深度信念网络一样。我们探讨了dropout如何通过在每次学习步骤中随机忽略（断开）一部分神经元，来降低过拟合的风险。此外，我们还了解了批量归一化如何通过逐步调整某些神经元的响应，提供学习算法的稳定性，确保激活函数和其他连接的神经元不会饱和或在数值上溢出。
- en: At this point, you should feel confident applying batch normalization and dropout
    strategies in a deep autoencoder model. You should be able to create your own
    deep autoencoders and apply them to different tasks where a rich latent representation
    is required for data visualization purposes, data compression or dimensionality
    reduction problems, and other types of data embeddings where a low-dimensional
    representation is required.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你应该能够自信地在深度自动编码器模型中应用批量归一化和dropout策略。你应该能够创建自己的深度自动编码器，并将其应用于需要丰富潜在表示的数据可视化、数据压缩、降维问题，以及其他类型的嵌入式数据，其中需要低维表示。
- en: '[Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders,* will
    continue with autoencoders but from a *generative* *modeling* perspective. Generative
    models have the ability to generate data by sampling a probability density function,
    which is quite interesting. We will specifically discuss the variational autoencoder
    model as a better alternative to a deep autoencoder in the presence of noisy data.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自动编码器*，将从*生成* *建模*的角度继续讨论自动编码器。生成模型能够通过从概率密度函数中采样来生成数据，这非常有趣。我们将特别讨论变分自动编码器模型，作为在存在噪声数据时，深度自动编码器的更好替代方案。'
- en: Questions and answers
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与解答
- en: '**Which regularization strategy discussed in this chapter alleviates overfitting
    in deep models?**'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**本章讨论的哪种正则化策略可以缓解深度模型中的过拟合？**'
- en: Dropout.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout。
- en: '**Does adding a batch normalization layer make the learning algorithm have
    to learn more parameters? **'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加批量归一化层会使学习算法需要学习更多的参数吗？**'
- en: Actually, no. For every layer in which dropout is used, there will be only two
    parameters for every neuron to learn: ![](img/5db1f077-f826-4c6a-8b08-ca701527c5c0.png).
    If you do the math, the addition of new parameters is rather small.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，并不是这样。对于每一层使用 dropout 的情况，每个神经元需要学习的参数只有两个：![](img/5db1f077-f826-4c6a-8b08-ca701527c5c0.png)。如果你做一下计算，你会发现新增的参数其实相对较少。
- en: '**What other deep belief networks are out there?**'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**还有哪些其他的深度置信网络？**'
- en: Restricted Boltzmann machines, for example, are another very popular example
    of deep belief networks. [Chapter 10](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=33&action=edit), *Restricted
    Boltzmann Machines*, will cover these in more detail.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机（Restricted Boltzmann Machines），例如，是另一种非常流行的深度置信网络实例。[第10章](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=33&action=edit)，*限制玻尔兹曼机*，将详细介绍这些内容。
- en: '**How come deep autoencoders perform better on MNIST than on CIFAR-10?**'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么深度自编码器在 MNIST 数据集上的表现优于 CIFAR-10？**'
- en: 'Actually, we do not have an objective way of saying that deep autoencoders
    are better on these datasets. We are biased in thinking about it in terms of clustering
    and data labels. Our bias in thinking about the latent representations in *Figure
    8.12* and *Figure 8.16* in terms of labels is precluding us from thinking about
    other possibilities. Consider the following for CIFAR-10: what if the autoencoder
    is learning to represent data according to textures? Or color palettes? Or geometric
    properties? Answering these questions is key to understanding what is going on
    inside the autoencoder and why it is learning to represent the data in the way
    it does, but requires more advanced skills and time. In summary, we don''t know
    for sure whether it is underperforming or not until we answer these questions;
    otherwise, if we put on our lenses of classes, groups, and labels, then it might
    just seem that way.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们并没有一个客观的方式来判断深度自编码器在这些数据集上的表现是否更好。我们在考虑时往往带有偏见，习惯于从聚类和数据标签的角度思考。我们在考虑
    *图8.12* 和 *图8.16* 中潜在表示时，将其与标签关联的偏见，使得我们无法考虑其他可能性。以 CIFAR-10 为例：如果自编码器正在学习根据纹理、颜色调色板或几何属性来表示数据呢？回答这些问题是理解自编码器内部工作原理及其如何学习表示数据的关键，但这需要更高级的技能和时间。总之，在我们回答这些问题之前，我们无法确定它是否表现不佳；否则，如果我们用类别、组和标签的视角来看问题，可能会误以为是这样。
- en: References
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Sutskever, I., & Hinton, G. E. (2008). Deep, narrow sigmoid belief networks
    are universal approximators. *Neural computation*, 20(11), 2629-2636.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever, I., & Hinton, G. E. (2008). 深度、狭窄的 sigmoid 置信网络是通用近似器。*神经计算*，20(11)，2629-2636。
- en: Sainath, T. N., Kingsbury, B., & Ramabhadran, B. (2012, March). Auto-encoder
    bottleneck features using deep belief networks. In 2012 *IEEE international conference
    on acoustics, speech and signal processing (ICASSP)* (pp. 4153-4156). IEEE.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sainath, T. N., Kingsbury, B., & Ramabhadran, B. (2012年3月). 使用深度置信网络的自编码器瓶颈特征。发表于
    2012年 *IEEE 国际声学、语音与信号处理大会 (ICASSP)* (第4153-4156页)。IEEE。
- en: Wu, K., & Magdon-Ismail, M. (2016). Node-by-node greedy deep learning for interpretable
    features. *arXiv preprint* arXiv:1602.06183.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu, K., & Magdon-Ismail, M. (2016). 节点逐一贪婪深度学习用于可解释特征。*arXiv 预印本* arXiv:1602.06183。
- en: 'Ioffe, S., & Szegedy, C. (2015, June). Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift. In *International Conference
    on Machine Learning (ICML)* (pp. 448-456).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe, S., & Szegedy, C. (2015年6月). 批量归一化：通过减少内部协变量偏移加速深度网络训练。发表于 *国际机器学习大会
    (ICML)* (第448-456页)。
- en: 'Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov,
    R. (2014). Dropout: a simple way to prevent neural networks from overfitting.
    *The journal of machine learning research*, 15(1), 1929-1958.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov,
    R. (2014). Dropout：一种简单的防止神经网络过拟合的方法。*机器学习研究期刊*，15(1)，1929-1958。
- en: Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for
    online learning and stochastic optimization. *Journal of machine learning research*,
    12(Jul), 2121-2159.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duchi, J., Hazan, E., & Singer, Y. (2011). 在线学习与随机优化的自适应子梯度方法。*机器学习研究期刊*，12(Jul)，2121-2159。
- en: McInnes, L., Healy, J., & Umap, J. M. (2018). Uniform manifold approximation
    and projection for dimension reduction. *arXiv preprint* arXiv:1802.03426.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McInnes, L., Healy, J., & Umap, J. M. (2018). 用于降维的统一流形逼近与投影。*arXiv 预印本* arXiv:1802.03426。
- en: 'Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011,
    June). Learning word vectors for sentiment analysis. In *Proceedings of the 49th
    annual meeting of the association for computational linguistics*: *Human language
    technologies*-volume 1 (pp. 142-150). Association for Computational Linguistics.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011,
    June). 学习情感分析的词向量。在*计算语言学年会第49届年会*: *人类语言技术*-volume 1 (pp. 142-150). 计算语言学协会。'
- en: Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent
    neural nets and problem solutions. International Journal of Uncertainty, *Fuzziness
    and Knowledge-Based Systems*, 6(02), 107-116.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S. (1998). 在学习递归神经网络期间的梯度消失问题及其解决方案。不确定性、*模糊性和基于知识的系统*国际期刊, 6(02),
    107-116。
- en: Van Laarhoven, T. (2017). L2 regularization versus batch and weight normalization.
    *arXiv preprint* arXiv:1706.05350.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Laarhoven, T. (2017). L2正则化对批处理和权重规范化的比较。*arXiv预印本* arXiv:1706.05350。
