- en: Recurrent-Type Neural Networks - Language Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归类型神经网络 - 语言建模
- en: '** Recurrent neural networks** (**RNNs**) are a class of deep learning architectures
    that are widely used for natural language processing. This set of architectures
    enables us to provide contextual information for current predictions and also
    have specific architecture that deals with long-term dependencies in any input
    sequence. In this chapter, we''ll demonstrate how to make a sequence-to-sequence
    model, which will be useful in many applications in NLP. We will demonstrate these
    concepts by building a character-level language model and see how our model generates
    sentences similar to original input sequences.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNNs**）是一类广泛用于自然语言处理的深度学习架构。这类架构使我们能够为当前的预测提供上下文信息，并且具有处理任何输入序列中长期依赖性的特定架构。在本章中，我们将展示如何构建一个序列到序列模型，这将在NLP的许多应用中非常有用。我们将通过构建一个字符级语言模型来展示这些概念，并查看我们的模型如何生成与原始输入序列相似的句子。'
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The intuition behind RNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNNs背后的直觉
- en: LSTM networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM网络
- en: Implementation of the language model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型的实现
- en: The intuition behind RNNs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNNs背后的直觉
- en: All the deep learning architectures that we have dealt with so far have no mechanism
    to memorize the input that they have received previously. For instance, if you
    feed a **feed-forward neural network** (**FNN**) with a sequence of characters
    such as **HELLO**, when the network gets to **E**, you will find that it didn't
    preserve any information/forgotten that it just read **H**. This is a serious
    problem for sequence-based learning. And since it has no memory of any previous
    characters it read, this kind of network will be very difficult to train to predict
    the next character. This doesn't make sense for lots of applications such as language
    modeling, machine translation, speech recognition, and so on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的所有深度学习架构都没有机制来记住它们之前接收到的输入。例如，如果你给**前馈神经网络**（**FNN**）输入一串字符，例如**HELLO**，当网络处理到**E**时，你会发现它没有保留任何信息/忘记了它刚刚读取的**H**。这是基于序列的学习的一个严重问题。由于它没有记住任何它读取过的先前字符，这种网络将非常难以训练来预测下一个字符。这对于许多应用（如语言建模、机器翻译、语音识别等）来说是没有意义的。
- en: For this specific reason, we are going to introduce RNNs, a set of deep learning
    architectures that do preserve information and memorize what they have just encountered.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个特定的原因，我们将介绍RNNs，这是一组能够保存信息并记住它们刚刚遇到的内容的深度学习架构。
- en: Let's demonstrate how RNNs should work on the same input sequence of characters, **HELLO**.
    When the RNN cell/unit receives **E** as an input, it also receives that character
    **H**, which it received earlier. This feeding of the present character along
    with the past one as an input to the RNN cell gives a great advantage to these
    architectures, which is short-term memory; it also makes these architectures usable
    for predicting/guessing the most likely character after **H**, which is **L**,
    in this specific sequence of characters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示RNNs如何在相同的字符输入序列**HELLO**上工作。当RNN单元接收到**E**作为输入时，它也接收到先前输入的字符**H**。这种将当前字符和先前字符一起作为输入传递给RNN单元的做法为这些架构提供了一个巨大优势，即短期记忆；它还使这些架构能够用于预测/推测在这个特定字符序列中**H**之后最可能的字符，即**L**。
- en: We have seen that previous architectures assign weights to their inputs; RNNs
    follow the same optimization process of assigning weights to their multiple inputs,
    which is the present and past. So in this case, the network will assign two different
    matrices of weights to each one of them. In order to do that, we will be using
    **gradient descent** and a heavier version of backpropagation, which is called **backpropagation
    through time** (**BPTT**).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，先前的架构将权重分配给它们的输入；RNNs遵循相同的优化过程，将权重分配给它们的多个输入，包括当前输入和过去输入。因此，在这种情况下，网络将为每个输入分配两个不同的权重矩阵。为了做到这一点，我们将使用**梯度下降**和一种更重的反向传播版本，称为**时间反向传播**（**BPTT**）。
- en: Recurrent neural networks architectures
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络架构
- en: 'Depending on our background of using previous deep learning architectures,
    you will find out why RNNs are special. The previous architectures that we have
    learned about are not flexible in terms of their input or training. They accept
    a fixed-size sequence/vector/image as an input and produce another fixed-size
    one as an output. RNN architectures are somehow different, because they enable
    you to feed a sequence as input and get another sequence as output, or to have
    sequences in the input only/output only as shown in *Figure 1*. This kind of flexibility
    is very useful for multiple applications such as language modeling and sentiment
    analysis:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对以前深度学习架构的了解，你会发现 RNN 是特别的。我们学过的前几种架构在输入或训练方面并不灵活。它们接受固定大小的序列/向量/图像作为输入，并产生另一个固定大小的输出。RNN
    架构则有所不同，因为它们允许你输入一个序列并得到另一个序列作为输出，或者仅在输入/输出中使用序列，如*图 1*所示。这种灵活性对于多种应用，如语言建模和情感分析，非常有用：
- en: '![](img/567e14af-e7cf-4439-bdf0-7a72a595a1f9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/567e14af-e7cf-4439-bdf0-7a72a595a1f9.png)'
- en: 'Figure 1: Flexibility of RNNs in terms of shape of input or output (http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：RNN 在输入或输出形状上的灵活性（[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)）
- en: The intuition behind these set of architectures is to mimic the way humans process
    information. In any typical conversation your understanding of someone's words
    is totally dependent on what he said previously and you might even be able to
    predict what he's going to say next based on what he just said.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构的直观原理是模仿人类处理信息的方式。在任何典型的对话中，你对某人话语的理解完全依赖于他之前说了什么，甚至可能根据他刚刚说的内容预测他接下来会说什么。
- en: The exact same process should be followed in the case of RNNs. For example,
    imagine you want translate a specific word in a sentence. You can't use traditional
    FNNs for that, because they won't be able to use the translation of previous words
    as an input with the current word that we want to translate, and this may result
    in an incorrect translation because of the lack of contextual information around
    this word.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RNN 的情况下，应该遵循完全相同的过程。例如，假设你想翻译句子中的某个特定单词。你不能使用传统的 FNN，因为它们无法将之前单词的翻译作为输入与当前我们想翻译的单词结合使用，这可能导致翻译错误，因为缺少与该单词相关的上下文信息。
- en: 'RNNs do preserves information about the past and they have some kind of loops
    to allow the previously learned information to be used for the current prediction
    at any given point:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 保留了关于过去的信息，并且它们具有某种循环结构，允许在任何给定时刻将之前学到的信息用于当前的预测：
- en: '![](img/5975b720-dd6b-44f2-b227-732b2aa481d8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5975b720-dd6b-44f2-b227-732b2aa481d8.png)'
- en: 'Figure 2: RNNs architecture which has loop to persist information for past
    steps (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：具有循环结构的 RNN 架构，用于保留过去步骤的信息（来源：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)）
- en: In *Figure 2*, we have some neural networks called *A* which receives an input
    *X[t]* and produces and output *h[t]*. Also, it receives information from past
    steps with the help of this loop.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 2*中，我们有一些神经网络称为*A*，它接收输入 *X[t]* 并生成输出 *h[t]*。同时，它借助这个循环接收来自过去步骤的信息。
- en: 'This loop seems to unclear, but if we used the unrolled version of *Figure
    2*, you will find out that it''s very simple and intuitive, and that the RNN is
    nothing but a repeated version of the same network (which could be normal FNN),
    as shown in *Figure 3*:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环看起来似乎不太清楚，但如果我们使用*图 2*的展开版本，你会发现它非常简单且直观，RNN 其实就是同一网络的重复版本（这可以是普通的 FNN），如*图
    3*所示：
- en: '![](img/8fdae554-aebd-49fb-a8ef-24071579e7fe.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fdae554-aebd-49fb-a8ef-24071579e7fe.png)'
- en: 'Figure 3: An unrolled version of the recurrent neural network architecture
    (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：递归神经网络架构的展开版本（来源：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)）
- en: This intuitive architecture of RNNs and its flexibility in terms of input/output
    shape make them a good fit for interesting sequence-based learning tasks such
    as machine translation, language modeling, sentiment analysis, image captioning,
    and more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的这种直观架构及其在输入/输出形状上的灵活性，使得它们非常适合处理有趣的基于序列的学习任务，如机器翻译、语言建模、情感分析、图像描述等。
- en: Examples of RNNs
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 的示例
- en: Now, we have an intuitive understanding of how RNNs work and how it's going
    to be useful in different interesting sequence-based examples. Let's have a closer
    look of some of these interesting examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对循环神经网络（RNN）的工作原理有了直观的理解，也了解它在不同有趣的基于序列的例子中的应用。让我们更深入地了解一些这些有趣的例子。
- en: Character-level language models
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符级语言模型
- en: Language modeling is an essential task for many applications such as speech
    recognition, machine translation and more. In this section, we'll try to mimic
    the training process of RNNs and get a deeper understanding of how these networks
    work. We'll build a language model that operate over characters. So, we will feed
    our network with a chunk of text with the purpose of trying to build a probability
    distribution of the next character given the previous ones which will allow us
    to generate text similar to the one we feed as an input in the training process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模是许多应用中一个至关重要的任务，如语音识别、机器翻译等。在本节中，我们将尝试模拟RNN的训练过程，并更深入地理解这些网络的工作方式。我们将构建一个基于字符的语言模型。所以，我们将向网络提供一段文本，目的是尝试建立一个概率分布，用于预测给定前一个字符后的下一个字符的概率，这将使我们能够生成类似于我们在训练过程中输入的文本。
- en: For example, suppose we have a language with only four letters as its vocabulary, **helo**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个词汇表仅包含四个字母，**helo**。
- en: 'The task is to train a recurrent neural network on a specific input sequence
    of characters such as **hello**. In this specific example, we have four training
    samples:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是训练一个循环神经网络，处理一个特定的字符输入序列，如**hello**。在这个具体的例子中，我们有四个训练样本：
- en: The probability of the character **e** should be calculated given the context
    of the first input character **h**,
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定第一个输入字符**h**的上下文，应该计算字符**e**的概率，
- en: The probability of the character **l** should be calculated given the context
    of **he**,
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定**he**的上下文，应该计算字符**l**的概率，
- en: The probability of the character **l** should be calculated given the context
    of **hel**, and
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定**hel**的上下文，应该计算字符**l**的概率，
- en: Finally the probability of the character **o** should be calculated given the
    context of **hell**
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，给定**hell**的上下文，应该计算字符**o**的概率。
- en: 'As we learned in previous chapters, machine learning techniques in general
    which deep learning is a part of, only accept real-value numbers as input. So,
    wee need somehow convert or encode or input character to a numerical form. To
    do this, we will use one-hot-vector encoding which is a way to encode text by
    have a vector of zeros except for a single entry in the vector, which is the index
    of the character in the vocabulary of this language that we are trying to model
    (in this case **helo**). After encoding our training samples, we will provide
    them to the RNN-type model one at a time. At each given character, the output
    of the RNN-type model will be a 4-dimensional vector (the size of the vector corresponds
    to the size of the vocab) which represents the probability of each character in
    the vocabulary being the next one after the given input character. *Figure 4*
    clarifies this process:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中学到的那样，机器学习技术，深度学习也属于其中的一部分，一般只接受实数值作为输入。因此，我们需要以某种方式将输入字符转换或编码为数字形式。为此，我们将使用one-hot向量编码，这是一种通过将一个向量中除一个位置外其他位置填充为零的方式来编码文本，其中该位置的索引表示我们试图建模的语言（在此为**helo**）中的字符索引。在对训练样本进行编码后，我们将逐个提供给RNN类型的模型。在给定的每个字符时，RNN类型模型的输出将是一个四维向量（该向量的大小对应于词汇表的大小），表示词汇表中每个字符作为下一个字符的概率。*图
    4* 清楚地说明了这个过程：
- en: '![](img/ed9df466-611d-4550-aa61-c2ac49e16102.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed9df466-611d-4550-aa61-c2ac49e16102.jpeg)'
- en: 'Figure 4: Example of RNN-type network with one-hot-vector encoded characters
    as an input and the output will be distribution over the vocab representing the
    most likely character after the current one (source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：RNN类型网络的示例，输入为通过one-hot向量编码的字符，输出是词汇表中的分布，表示当前字符后最可能的字符（来源：http://karpathy.github.io/2015/05/21/rnn-effectiveness/）
- en: As shown in *Figure 4*, you can see that we fed the first character in our input
    sequence **h** to the model and the output was 4-dimensional vector representing
    the confidence about the next character. So it has a confidence of **1.0** of
    **h** being the next character after the input **h**, a confidence of **2.2**
    of **e** being the next character, a confidence of **-3.0** to **l** being the
    next character, and finally a confidence of **4.1** to **o** being the next character.
    In this specific example, we know the correct next character will be **e**,based
    on our training sequence **hello**. So our primary goal while training this RNN-type
    network is increase the confidence of **e** being the next character and decrease
    the confidence of other characters. To do this kind of optimization we will be
    using gradient descent and backpropagation algorithms to update the weights and
    influence the network to produce a higher confidence for our correct next character, **e**,
    and so on, for the other 3 training examples.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4*所示，你可以看到我们将输入序列中的第一个字符**h**喂给模型，输出是一个四维向量，表示下一个字符的置信度。所以它对**h**作为下一个字符的置信度是**1.0**，对**e**是**2.2**，对**l**是**-3.0**，对**o**是**4.1**。在这个特定的例子中，我们知道下一个正确字符是**e**，基于我们的训练序列**hello**。所以，我们在训练这个RNN类型网络时的主要目标是增加**e**作为下一个字符的置信度，并减少其他字符的置信度。为了进行这种优化，我们将使用梯度下降和反向传播算法来更新权重，影响网络产生更高置信度的正确下一个字符**e**，并以此类推，处理其他三个训练例子。
- en: As you can see the output of the RNN-type network produces a confidence distribution
    over all the characters of the vocab being the next one. We can turn this confidence
    distribution into a probability distribution such that the increase of one characters
    probability being the next one will result in decreasing the others probabilities
    because the probability needs to sum up to 1\. For this specific modification
    we can use a standard softmax layer to every output vector.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，RNN类型网络的输出会产生一个对所有词汇中字符的置信度分布，表示下一个字符可能性。我们可以将这种置信度分布转化为概率分布，使得某个字符作为下一个字符的概率增加时，其他字符的概率会相应减少，因为概率总和必须为1。对于这种特定的修改，我们可以对每个输出向量使用一个标准的Softmax层。
- en: For generating text from these kind of networks, we can feed an initial character
    to the model and get a probability distribution over the characters that are likely
    to be next, and then we can sample from these characters and feed it back as an
    input to the model. We'll be able to get a sequence of characters by repeating
    this process over and over again as many times as we want to generate a text with
    a desired length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从这些类型的网络生成文本，我们可以将一个初始字符输入模型，并得到一个关于下一个字符可能性的概率分布，然后我们可以从这些字符中采样并将其反馈作为输入给模型。通过重复这一过程多次，我们就能生成一个具有所需长度的字符序列。
- en: Language model using Shakespeare data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用莎士比亚数据的语言模型
- en: From the preceding example, we can get the model to generate text. But the network
    will surprise us, as it's not only going to generate text but also it's going
    to learn the style and the structure in training data. We can demonstrate this
    interesting process by training an RNN-type model on specific kind of text that
    has structure and style in it, such as the following Shakespeare work.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中，我们可以得到生成文本的模型。但网络会让我们惊讶，因为它不仅仅会生成文本，还会学习训练数据中的风格和结构。我们可以通过训练一个RNN类型的模型来展示这一有趣的过程，使用具有结构和风格的特定文本，例如以下的莎士比亚作品。
- en: 'Let''s have a look at a generated output from the trained network:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看从训练好的网络生成的输出：
- en: 'Second Senator:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第二位参议员：
- en: They are away this miseries, produced upon my soul,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 他们远离了我灵魂上的痛苦，
- en: Breaking and strongly should be buried, when I perish
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我死去时，打破并强烈应当埋葬
- en: The earth and thoughts of many states.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 许多国家的地球与思想。
- en: In spite of the fact that the network only knows how to produce one single character
    at a time, it was able to generate a meaningful text and names that actually have
    the structure and style of Shakespeare work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网络一次只知道如何生成一个字符，但它还是能够生成有意义的文本和实际具有莎士比亚作品风格和结构的名字。
- en: The vanishing gradient problem
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失问题
- en: 'While training these sets of RNN-type architectures, we use gradient descent
    and backprogagation through time, which introduced some successes for lots of
    sequence-based learning tasks. But because of the nature of the gradient and due
    to using fast training strategies, it could be shown that the gradient values
    will tend to be too small and vanish. This process introduced the vanishing gradient
    problem that many practitioners fall into. Later on in this chapter, we will discuss
    how researchers approached these kind of problems and produced variations of the
    vanilla RNNs to overcome this problem:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练这些RNN类型架构时，我们使用梯度下降和通过时间的反向传播，这些方法为许多基于序列的学习任务带来了成功。但是，由于梯度的性质以及使用快速训练策略，研究表明梯度值往往会变得过小并消失。这一过程引发了许多从业者遇到的梯度消失问题。接下来，在本章中，我们将讨论研究人员如何解决这些问题，并提出了传统RNN的变种来克服这个问题：
- en: '![](img/79e4dcaf-8767-4631-94d6-2f24a58024d1.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79e4dcaf-8767-4631-94d6-2f24a58024d1.png)'
- en: 'Figure 5: Vanishing gradient problem'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：梯度消失问题
- en: The problem of long-term dependencies
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期依赖问题
- en: Another challenging problem faced by researchers is the long-term dependencies
    that one can find in text. For example, if someone feeds a sequence like *I used
    to live in France and I learned how to speak...* the next obvious word in the
    sequence is the word French.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员面临的另一个挑战性问题是文本中的长期依赖。例如，如果有人输入类似 *我曾经住在法国，并且我学会了如何说……* 的序列，那么接下来的显而易见的词是
    French。
- en: 'In these kind of situation vanilla RNNs will be able to handle it because it
    has short-term dependencies, as shown in *Figure 6*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，传统RNN能够处理短期依赖问题，如*图6*所示：
- en: '![](img/568df409-a768-4696-ae1a-d6e371d6cb6e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/568df409-a768-4696-ae1a-d6e371d6cb6e.png)'
- en: 'Figure 6: Showing short-term dependencies in the text (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：展示文本中的短期依赖（来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/）
- en: 'Another example, if someone started the sequence by saying that *I used to
    live in France..* and then he/she start to describe the beauty of living there
    and finally he ended the sequence by *I learned to speak French*. So, for the
    model to predict the language that he/she learned at the end of the sequence,
    the model needs to have some information about the early words *live* and *France*.
    The model won''t be able to handle these kind of situation, if it doesn''t manage
    to keep track of long term dependencies in the text:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，如果某人开始输入 *我曾经住在法国……* 然后描述住在那里的一些美好经历，最后以 *我学会了说法语* 结束序列。那么，为了让模型预测他/她在序列结束时学会的语言，模型需要保留早期词汇
    *live* 和 *France* 的信息。如果模型不能有效地跟踪文本中的长期依赖，它就无法处理这种情况：
- en: '![](img/12e35c64-5d9d-4ecc-b969-4e627147b77d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12e35c64-5d9d-4ecc-b969-4e627147b77d.png)'
- en: 'Figure 7: The challenge of long-term dependencies in text (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：文本中长期依赖问题的挑战（来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/）
- en: To handle vanishing gradients and long-term dependencies in the text, researchers
    introduced a variation of the vanilla RNN network called **Long Short Term Networks **(**LSTM**).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理文本中的梯度消失和长期依赖问题，研究人员引入了一种名为**长短时记忆网络**（**LSTM**）的变种网络。
- en: LSTM networks
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络
- en: 'LSTM, a variation of an RNN that is used to help learning long term dependencies
    in the text. LSTMs were initially introduced by Hochreiter & Schmidhuber (1997)
    (link: [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)),
    and many researchers worked on it and produced interesting results in many domains.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是一种RNN的变种，用于帮助学习文本中的长期依赖。LSTM最初由Hochreiter和Schmidhuber（1997年）提出（链接：[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)），许多研究者在此基础上展开了工作，并在多个领域取得了有趣的成果。
- en: These kind of architectures will be able to handle the problem of long-term
    dependencies in the text because of its inner architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构能够处理文本中长期依赖的问题，因为它们的内部架构设计使然。
- en: 'LSTMs are similar to the vanilla RNN as it has a repeating module over time,
    but the inner architecture of this repeated module is different from the vanilla
    RNNs. It includes more layers for forgetting and updating information:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM与传统的RNN相似，都具有一个随着时间重复的模块，但这个重复模块的内部结构与传统RNN不同。它包括更多的层，用于遗忘和更新信息：
- en: '![](img/9ae7087a-fced-4f5c-86c8-84855ad8301a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ae7087a-fced-4f5c-86c8-84855ad8301a.png)'
- en: 'Figure 8: The repeating module in a standard RNN containing a single layer
    (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：标准RNN中包含单一层的重复模块（来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/）
- en: 'As mentioned previously, the vanilla RNNs have a single NN layer, but the LSTMs
    have four different layers interacting in a special way. This special kind of
    interaction is what makes LSTM, work very well for many domains, which we''ll
    see while building our language model example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，基础RNN只有一个神经网络层，而LSTM有四个不同的层以特殊的方式相互作用。这种特殊的交互方式使得LSTM在许多领域中表现得非常好，正如我们在构建语言模型示例时会看到的那样：
- en: '![](img/a385710b-f1db-4c91-af43-244a22b863b8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a385710b-f1db-4c91-af43-244a22b863b8.png)'
- en: 'Figure 9: The repeating module in an LSTM containing four interacting layers
    (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：LSTM中包含四个交互层的重复模块（来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/）
- en: For more details about the mathematical details and how the four layers are
    actually interacting with each other, you can have a look at this interesting
    tutorial: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数学细节以及四个层是如何相互作用的更多信息，可以参考这个有趣的教程：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: Why does LSTM work?
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么LSTM有效？
- en: The first step in our vanilla LSTM architecture it to decide which information
    is not necessary and it will work by throwing it away to leave more room for more
    important information. For this, we have a layer called **forget gate layer**,
    which looks at the previous output *h[t-1]* and the current input *x[t]* and decides
    which information we are going to throw away.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础LSTM架构的第一步是决定哪些信息是不必要的，它通过丢弃这些信息，为更重要的信息留出更多空间。为此，我们有一个叫做**遗忘门层**的层，它查看前一个输出*h[t-1]*和当前输入*x[t]*，并决定我们要丢弃哪些信息。
- en: 'The next step in the LSTM architecture is to decide which information is worth
    keeping/persisting and storing in the cell. This is done in two steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM架构中的下一步是决定哪些信息值得保留/持久化并存储到细胞中。这是通过两个步骤完成的：
- en: A layer called **input gate layer**, which decides which values of the previous
    state of the cell needs to be updated
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个叫做**输入门层**的层，决定了哪些来自前一状态的值需要被更新
- en: The second step is to generate a set of new candidate values that will be added
    to the cell
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是生成一组新的候选值，这些值将被添加到细胞中
- en: Finally, we need to decide what the LSTM cell is going to output. This output
    will be based on our cell state, but will be a filtered version.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要决定LSTM单元将输出什么。这个输出将基于我们的细胞状态，但会是一个经过筛选的版本。
- en: Implementation of the language model
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的实现
- en: 'In this section, we''ll build a language model that operates over characters.
    For this implementation, we will use an Anna Karenina novel and see how the network
    will learn to implement the structure and style of the text:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个基于字符的语言模型。在这个实现中，我们将使用《安娜·卡列尼娜》小说，并观察网络如何学习实现文本的结构和风格：
- en: '![](img/29c6dfef-8eee-49f1-a36c-b6826af3c065.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29c6dfef-8eee-49f1-a36c-b6826af3c065.jpeg)'
- en: 'Figure 10: General architecture for the character-level RNN (source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：字符级RNN的一般架构（来源：http://karpathy.github.io/2015/05/21/rnn-effectiveness/）
- en: 'This network is based off of Andrej Karpathy''s post on RNNs (link: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))
    and implementation in Torch (link: [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)).
    Also, there''s some information here at r2rt (link: [http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html))
    and from Sherjil Ozairp (link: [https://github.com/sherjilozair/char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow))
    on GitHub. The following is the general architecture of the character-wise RNN.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络基于Andrej Karpathy关于RNN的文章（链接：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)）和在Torch中的实现（链接：[https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)）。此外，这里还有一些来自r2rt的资料（链接：[http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html)）以及Sherjil
    Ozairp（链接：[https://github.com/sherjilozair/char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow)）在GitHub上的内容。以下是字符级RNN的一般架构。
- en: 'We''ll build a character-level RNN trained on the Anna Karenina novel (link:
    [https://en.wikipedia.org/wiki/Anna_Karenina](https://en.wikipedia.org/wiki/Anna_Karenina))[.](https://en.wikipedia.org/wiki/Anna_Karenina)
    It''ll be able to generate new text based on the text from the book. You will
    find the `.txt` file included with the assets of this implementation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个基于《安娜·卡列尼娜》小说的字符级RNN（链接：[https://en.wikipedia.org/wiki/Anna_Karenina](https://en.wikipedia.org/wiki/Anna_Karenina)）。它将能够基于书中的文本生成新的文本。你将在这个实现的资源包中找到`.txt`文件。
- en: 'Let’s start by importing the necessary libraries for this character-level implementation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入实现字符级别操作所需的库：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To start off, we need to prepare the dataset by loading it and converting it
    in to integers. So, we will convert the characters into integers and then encode
    them as integers which makes it straightforward and easy to use as input variables
    for the model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过加载数据集并将其转换为整数来准备数据集。因此，我们将字符转换为整数，然后将其编码为整数，这使得它可以作为模型的输入变量，直观且易于使用：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So, let''s have look at the first 200 characters from the Anna Karenina text:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下《安娜·卡列尼娜》文本中的前200个字符：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have also converted the characters to a convenient form for the network,
    which is integers. So, let''s have a look at the encoded version of the characters:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将字符转换为适合网络使用的便捷形式，即整数。因此，让我们来看一下这些字符的编码版本：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since the network is working with individual characters, it's similar to a classification
    problem in which we are trying to predict the next character from the previous
    text. Here's how many classes our network has to pick from.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网络处理的是单个字符，因此它类似于一个分类问题，我们试图从之前的文本中预测下一个字符。以下是我们网络需要选择的类别数。
- en: 'So, we will be feeding the model a character at a time, and the model will
    predict the next character by producing a probability distribution over the possible
    number of characters that could come next (vocab), which is equivalent to a number
    of classes the network needs to pick from:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将一次喂给模型一个字符，模型将通过对可能出现的下一个字符（词汇表中的字符）的概率分布进行预测，从而预测下一个字符，这相当于网络需要从中选择的多个类别：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since we'll be using stochastic gradient descent to train our model, we need
    to convert our data into training batches.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用随机梯度下降来训练我们的模型，因此我们需要将数据转换为训练批次。
- en: Mini-batch generation for training
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于训练的小批次生成
- en: 'In this section, we will divide our data into small batches to be used for
    training. So, the batches will consist of many sequences of desired number of
    sequence steps. So, let''s look at a visual example in *Figure 11*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把数据分成小批次以供训练使用。因此，这些批次将包含许多具有所需序列步数的序列。让我们在*图11*中查看一个可视化示例：
- en: '![](img/77f38b3c-ced1-4784-af60-897d92f968ad.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77f38b3c-ced1-4784-af60-897d92f968ad.png)'
- en: 'Figure 11: Illustration of how batches and sequences would look like (source:
    http://oscarmore2.github.io/Anna_KaRNNa_files/charseq.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：批次和序列的可视化示例（来源：[http://oscarmore2.github.io/Anna_KaRNNa_files/charseq.jpeg](http://oscarmore2.github.io/Anna_KaRNNa_files/charseq.jpeg)）
- en: 'So, now we need to define a function that will iterate through the encoded
    text and generate the batches. In this function we will be using a very nice mechanism
    of Python called **yield** (link: [https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/)).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义一个函数，该函数将遍历编码后的文本并生成批次。在这个函数中，我们将使用Python中的一个非常棒的机制，叫做**yield**（链接：[https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/)）。
- en: A typical batch will have *N × M* characters, where *N* is the number of sequences
    and *M* is, number of sequence steps. For getting the number of possible batches
    in our dataset, we can simply divide the length of the data by the desired batch
    size and after getting this number of possible batches, we can drive how many
    characters should be in each batch.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的批次将包含*N × M*个字符，其中*N*是序列的数量，*M*是序列步数的数量。为了获得数据集中可能的批次数，我们可以简单地将数据的长度除以所需的批次大小，然后在得到这个可能的批次数后，我们可以确定每个批次应该包含多少个字符。
- en: After that, we need to split the dataset we have into a desired number of sequences
    (*N*). We can use `arr.reshape(size)`. We know we want *N* sequences (`num_seqs` is
    used in, following code), let's make that the size of the first dimension. For
    the second dimension, you can use -1 as a placeholder in the size; it'll fill
    up the array with the appropriate data for you. After this, you should have an
    array that is *N × (M * K)*, where *K* is the number of batches.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们需要将现有数据集拆分成所需数量的序列（*N*）。我们可以使用 `arr.reshape(size)`。我们知道我们需要 *N* 个序列（在后续代码中使用
    `num_seqs`），让我们将其作为第一维的大小。对于第二维，你可以使用 -1 作为占位符，它会为你填充适当的数据。这样，你应该得到一个形状为 *N ×
    (M * K)* 的数组，其中 *K* 是批次的数量。
- en: 'Now that we have this array, we can iterate through it to get the training
    batches, where each batch has *N × M* characters. For each subsequent batch, the
    window moves over by `num_steps`. Finally, we also want to create both the input
    and output arrays for ours to be used as the model input. This step of creating
    the output values is very easy; remember that the targets are the inputs shifted
    over one character. You''ll usually see the first input character used as the
    last target character, so something like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这个数组，可以通过它进行迭代以获取训练批次，每个批次包含 *N × M* 个字符。对于每个后续批次，窗口会向右移动 `num_steps`。最后，我们还需要创建输入和输出数组，以便将它们用作模型输入。这一步创建输出值非常简单；记住，目标是将输入移位一个字符。你通常会看到第一个输入字符作为最后一个目标字符使用，像这样：
- en: '![](img/2b300aec-cfbe-4d7f-a835-c7ef2c9944a3.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b300aec-cfbe-4d7f-a835-c7ef2c9944a3.png)'
- en: Where *x* is the input batch and *y* is the target batch.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是输入批次，*y* 是目标批次。
- en: 'The way I like to do this window is to use range to take steps of size `num_steps`,
    starting from 0 to `arr.shape[1]`, the total number of steps in each sequence.
    That way, the integers you get from the range always point to the start of a batch,
    and each window is `num_steps` wide:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢通过使用 range 函数来做这个窗口，步长为 `num_steps`，从 0 到 `arr.shape[1]`，也就是每个序列的总步数。这样，你从
    range 函数得到的整数始终指向一个批次的开始，每个窗口宽度为 `num_steps`：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So, let''s demonstrate this using this function by generating a batch of 15
    sequences and 50 sequence steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们使用这个函数来演示，通过生成一个包含 15 个序列和 50 个序列步骤的批次：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next up, we'll be looking forward to building the core of this example, which
    is the LSTM model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将着手构建本示例的核心部分，即 LSTM 模型。
- en: Building the model
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: Before diving into building the character-level model using LSTMs, it is worth
    mentioning something called **Stacked LSTM**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入使用 LSTM 构建字符级模型之前，值得提到一个叫做 **堆叠 LSTM** 的概念。
- en: Stacked LSTMs are useful for looking at your information at different time scales.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠 LSTM 对于在不同时间尺度上查看信息非常有用。
- en: Stacked LSTMs
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠 LSTM
- en: '"Building a deep RNN by stacking multiple recurrent hidden states on top of
    each other. This approach potentially allows the hidden state at each level to
    operate at different timescale" How to Construct Deep Recurrent Neural Networks
    (link: https://arxiv.org/abs/1312.6026), 2013'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '“通过将多个递归隐藏状态堆叠在一起构建深度 RNN。这种方法可以使每个层次的隐藏状态在不同的时间尺度上运行。” ——《如何构建深度递归神经网络》（链接:
    https://arxiv.org/abs/1312.6026），2013年'
- en: '"RNNs are inherently deep in time, since their hidden state is a function of
    all previous hidden states. The question that inspired this paper was whether
    RNNs could also benefit from depth in space; that is from stacking multiple recurrent
    hidden layers on top of each other, just as feedforward layers are stacked in
    conventional deep networks". Speech Recognition With Deep RNNs (link: [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)),
    2013'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '“RNN 本质上在时间上是深度的，因为它们的隐藏状态是所有先前隐藏状态的函数。启发本文的一个问题是，RNN 是否也能从空间深度中受益；也就是将多个递归隐藏层堆叠在一起，就像在传统深度网络中堆叠前馈层一样。”
    ——《深度 RNN 的语音识别》（链接: [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)），2013年'
- en: Most researchers are using stacked LSTMs for challenging sequence prediction
    problems. A stacked LSTM architecture can be defined as an LSTM model comprised
    of multiple LSTM layers. The preceding LSTM layer provides a sequence output rather
    than a single-value output to the LSTM layer as follows.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究人员都在使用堆叠 LSTM 来解决具有挑战性的序列预测问题。堆叠 LSTM 架构可以定义为由多个 LSTM 层组成的 LSTM 模型。前面的
    LSTM 层为 LSTM 层提供序列输出，而不是单一的值输出，如下所示。
- en: 'Specifically, it''s one output per input time step, rather than one output
    time step for all input time steps:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，它是每个输入时间步都有一个输出，而不是所有输入时间步都只有一个输出时间步：
- en: '![](img/fcf09a5d-b4e6-4c54-826b-58aed18b75a7.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcf09a5d-b4e6-4c54-826b-58aed18b75a7.png)'
- en: 'Figure 12: Stacked LSTMs'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：堆叠 LSTM
- en: So in this example, we will be using this kind of stacked LSTM architecture,
    which gives better performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这个例子中，我们将使用这种堆叠 LSTM 架构，它能提供更好的性能。
- en: Model architecture
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'This is where you''ll build the network. We''ll break it into parts so that
    it''s easier to reason about each bit. Then, we can connect them with the whole
    network:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你将构建网络。我们将把它分成几个部分，这样更容易理解每个部分。然后，我们可以将它们连接成一个完整的网络：
- en: '![](img/9b4a2032-77d5-4c59-a4de-0a4cf19bccfd.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b4a2032-77d5-4c59-a4de-0a4cf19bccfd.png)'
- en: 'Figure 13: Character-level model architecture'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：字符级模型架构
- en: Inputs
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入
- en: 'Now, let''s start by defining the model inputs as placeholders. The inputs
    of the model will be training data and the targets. We will also use a parameter
    called `keep_probability` for the dropout layer, which helps the model avoid overfitting:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始定义模型输入作为占位符。模型的输入将是训练数据和目标。我们还将使用一个叫做`keep_probability`的参数用于 dropout
    层，帮助模型避免过拟合：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Building an LSTM cell
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个 LSTM 单元
- en: In this section, we will write a function for creating the LSTM cell, which
    will be used in the hidden layer. This cell will be the building block for our
    model. So, we will create this cell using TensorFlow. Let's have a look at how
    we can use TensorFlow to build a basic LSTM cell.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将编写一个函数来创建 LSTM 单元，这将用于隐藏层。这个单元将是我们模型的构建块。因此，我们将使用 TensorFlow 来创建这个单元。让我们看看如何使用
    TensorFlow 构建一个基本的 LSTM 单元。
- en: 'We call the following line of code to create an LSTM cell with the parameter
    `num_units` representing the number of units in the hidden layer:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用以下代码行来创建一个 LSTM 单元，参数`num_units`表示隐藏层中的单元数：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To prevent overfitting, we can use something called **dropout**, which is a
    mechanism for preventing the model from overfitting the data by decreasing the
    model''s complexity:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，我们可以使用一种叫做**dropout**的技术，它通过减少模型的复杂度来防止模型过拟合数据：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we mentioned before, we will be using the stacked LSTM architecture; it
    will help us to look at the data from different angles and has been practically found
    to perform better. In order to define a stacked LSTM in TensorFlow, we can use
    the `tf.contrib.rnn.MultiRNNCell` function (link: [https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell)):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们将使用堆叠 LSTM 架构；它将帮助我们从不同角度查看数据，并且在实践中已被证明能表现得更好。为了在 TensorFlow 中定义堆叠
    LSTM，我们可以使用`tf.contrib.rnn.MultiRNNCell`函数（链接：[https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell)）：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Initially for the first cell, there will be no previous information, so we
    need to initialize the cell state to be zeros. We can use the following function
    to do that:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，对于第一个单元，没有前一个信息，因此我们需要将单元状态初始化为零。我们可以使用以下函数来实现：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, let''s put it all together and create our LSTM cell:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们把所有部分结合起来，创建我们的 LSTM 单元：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: RNN output
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 输出
- en: Next up, we need to create the output layer, which is responsible for reading
    the output of the individual LSTM cells and passing them through a fully connected
    layer. This layer has a softmax output for producing a probability distribution
    over the likely character to be next after the input one.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建输出层，负责读取各个 LSTM 单元的输出并通过全连接层传递。这个层有一个 softmax 输出，用于生成可能出现的下一个字符的概率分布。
- en: As you know, we have generated input batches for the network with size *N ×
    M* characters, where *N* is the number of sequences in this batch and *M* is the
    number of sequence steps. We have also used *L* hidden units in the hidden layer
    while creating the model. Based on the batch size and number of hidden units,
    the output of the network will be a *3D* Tensor with size *N × M × L*, and that's
    because we call the LSTM cell *M* times, one for each sequence step. Each call
    to LSTM cell produces an output of size *L*. Finally, we need to do this as many
    as number of sequences *N* as the we have.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，我们为网络生成了输入批次，大小为 *N × M* 字符，其中 *N* 是该批次中的序列数，*M* 是序列步数。我们在创建模型时也使用了 *L*
    个隐藏单元。根据批次大小和隐藏单元的数量，网络的输出将是一个 *3D* Tensor，大小为 *N × M × L*，这是因为我们调用 LSTM 单元 *M*
    次，每次处理一个序列步。每次调用 LSTM 单元会产生一个大小为 *L* 的输出。最后，我们需要做的就是执行 *N* 次，即序列的数量。
- en: So we pass this *N × M × L* output to a fully connected layer (which is the
    same for all outputs with the same weights), but before doing this, we reshape
    the output to a *2D* tensor, which has a shape of *(M * N) × L*. This reshaping
    will make things easier for us when operating on the output, because the new shape
    will be more convenient; the values of each row represents the *L* outputs of
    the LSTM cell, and hence it's one row for each sequence and step.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这个 *N × M × L* 的输出传递给一个全连接层（所有输出使用相同的权重），但在此之前，我们将输出重新调整为一个 *2D* 张量，形状为
    *(M * N) × L*。这个重新调整形状将使我们在处理输出时更加简便，因为新的形状会更方便；每一行的值代表了 LSTM 单元的 *L* 个输出，因此每一行对应一个序列和步骤。
- en: After getting the new shape, we can connect it to the fully connected layer
    with the softmax by doing matrix multiplication with the weights. The weights
    created in the LSTM cells and the weight that we are about to create here have
    the same name by default, and TensorFlow will raise an error in such a case. To
    avoid this error, we can wrap the weight and bias variables created here in a
    variable scope using the TensorFlow function `tf.variable_scope()`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取新形状之后，我们可以通过矩阵乘法将其与权重相乘，将其连接到带有 softmax 的全连接层。LSTM 单元中创建的权重和我们在这里即将创建的权重默认使用相同的名称，这样
    TensorFlow 就会抛出错误。为避免这个错误，我们可以使用 TensorFlow 函数 `tf.variable_scope()` 将在这里创建的权重和偏置变量封装在一个变量作用域内。
- en: 'After explaining the shape of the output and how we are going to reshape it,
    to make things easier, let''s go ahead and code this `build_model_output` function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了输出的形状以及如何重新调整形状后，为了简化操作，我们继续编写这个 `build_model_output` 函数：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training loss
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练损失
- en: Next up is the training loss. We get the logits and targets and calculate the
    softmax cross-entropy loss. First, we need to one-hot encode the targets; we're
    getting them as encoded characters. Then, we reshape the one-hot targets, so it's
    a *2D* tensor with size *(M * N) × C*, where *C* is the number of classes/characters
    we have. Remember that we reshaped the LSTM outputs and ran them through a fully
    connected layer with *C* units. So, our logits will also have size *(M * N) × C*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练损失。我们获取 logits 和 targets，并计算 softmax 交叉熵损失。首先，我们需要对 targets 进行 one-hot
    编码；我们得到的是编码后的字符。然后，我们重新调整 one-hot targets 的形状，使其成为一个 *2D* 张量，大小为 *(M * N) × C*，其中
    *C* 是我们拥有的类别/字符数。记住，我们已经调整了 LSTM 输出的形状，并通过一个具有 *C* 单元的全连接层。于是，我们的 logits 也将具有大小
    *(M * N) × C*。
- en: 'Then, we run the `logits` and `targets` through `tf.nn.softmax_cross_entropy_with_logits`
    and find the mean to get the loss:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将 `logits` 和 `targets` 输入到 `tf.nn.softmax_cross_entropy_with_logits` 中，并计算其均值以获得损失：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Optimizer
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器
- en: Finally, we need to use an optimization method that will help us learn something
    from the dataset. As we know, vanilla RNNs have exploding and vanishing gradient
    issues. LSTMs fix only one issue, which is the vanishing of the gradient values,
    but even after using LSTM, some gradient values explode and grow without bounds.
    In order to fix this problem, we can use something called **gradient clipping**,
    which is a technique to clip the gradients that explode to a specific threshold.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要使用一种优化方法，帮助我们从数据集中学习到一些东西。正如我们所知，普通的 RNN 存在梯度爆炸和梯度消失的问题。LSTM 仅解决了其中一个问题，即梯度值的消失，但即使使用了
    LSTM，仍然有一些梯度值会爆炸并且无限增大。为了解决这个问题，我们可以使用一种叫做**梯度裁剪**的技术，它可以将爆炸的梯度裁剪到一个特定的阈值。
- en: 'So, let''s define our optimizer by using the Adam optimizer for the learning
    process:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们通过使用 Adam 优化器来定义我们的优化器，用于学习过程：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Building the network
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络
- en: 'Now, we can put all the pieces together and build a class for the network.
    To actually run data through the LSTM cells, we will use `tf.nn.dynamic_rnn` (link:
    [https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn)).
    This function will pass the hidden and cell states across LSTM cells appropriately
    for us. It returns the outputs for each LSTM cell at each step for each sequence
    in the mini-batch. It also gives us the final LSTM state. We want to save this
    state as `final_state`, so we can pass it to the first LSTM cell in the the next
    mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state
    we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot
    encode the inputs before going into the RNN:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将所有部分组合起来，构建一个网络的类。为了真正将数据传递到LSTM单元，我们将使用`tf.nn.dynamic_rnn`（链接：[https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn)）。这个函数会适当地传递隐藏状态和单元状态给LSTM单元。它返回每个序列中每个LSTM单元在每个步骤的输出。它还会给我们最终的LSTM状态。我们希望将这个状态保存为`final_state`，以便在下一次mini-batch运行时将其传递给第一个LSTM单元。对于`tf.nn.dynamic_rnn`，我们传入从`build_lstm`获得的单元和初始状态，以及我们的输入序列。此外，我们需要对输入进行one-hot编码，然后才能进入RNN：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Model hyperparameters
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型超参数
- en: 'As with any deep learning architecture, there are a few hyperparameters that
    someone can use to control the model and fine-tune it. The following is the set
    of hyperparameters that we are using for this architecture:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 和任何深度学习架构一样，有一些超参数可以用来控制模型并进行微调。以下是我们为这个架构使用的超参数集：
- en: Batch size is the number of sequences running through the network in one pass.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次大小是每次通过网络运行的序列数量。
- en: The number of steps is the number of characters in the sequence the network
    is trained on. Larger is better typically; the network will learn more long-range
    dependencies, but will take longer to train. 100 is typically a good number here.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤数是网络训练过程中序列中的字符数量。通常，越大越好；网络将学习更多的长程依赖，但训练时间也会更长。100通常是一个不错的数字。
- en: The LSTM size is the number of units in the hidden layers.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的大小是隐藏层中单元的数量。
- en: Architecture number layers is the number of hidden LSTM layers to use.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构层数是要使用的隐藏LSTM层的数量。
- en: Learning rate is the typical learning rate for training.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率是训练中典型的学习率。
- en: And finally, the new thing that we call keep probability is used by the dropout
    layer; it helps the network to avoid overfitting. So if your network is overfitting,
    try decreasing this.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们引入了一个新的概念，叫做保持概率，它由dropout层使用；它帮助网络避免过拟合。如果你的网络出现过拟合，尝试减小这个值。
- en: Training the model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now, let's kick off the training process by providing the inputs and outputs
    to the built model and then use the optimizer to train the network. Don't forget
    that we need to use the previous state while making predictions for the current
    state. Thus, we need to pass the output state back to the network so that it can
    be used during the prediction of the next input.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过提供输入和输出给构建的模型来启动训练过程，然后使用优化器训练网络。不要忘记，在为当前状态做出预测时，我们需要使用前一个状态。因此，我们需要将输出状态传递回网络，以便在预测下一个输入时使用。
- en: 'Let''s provide initial values for our hyperparameters (you can tune them afterwards
    depending on the dataset you are using to train this architecture):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为超参数提供初始值（你可以在之后根据训练该架构使用的数据集调整这些值）：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At the end of the training process, you should get an error close to this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的最后，你应该得到一个接近以下的错误：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Saving checkpoints
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存检查点
- en: 'Now, let''s load the checkpoints. For more about saving and loading checkpoints,
    you can check out the TensorFlow documentation ([https://www.tensorflow.org/programmers_guide/variables](https://www.tensorflow.org/programmers_guide/variables)):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载检查点。关于保存和加载检查点的更多信息，你可以查看TensorFlow文档（[https://www.tensorflow.org/programmers_guide/variables](https://www.tensorflow.org/programmers_guide/variables)）：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Generating text
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成文本
- en: We have a trained model based on our input dataset. The next step is to use
    this trained model to generate text and see how this model learned the style and
    structure of the input data. To do this, we can start with some initial characters
    and then feed the new, predicted one as an input in the next step. We will repeat
    this process until we get a text with a specific length.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个基于输入数据集训练的模型。下一步是使用这个训练好的模型生成文本，并看看这个模型是如何学习输入数据的风格和结构的。为此，我们可以从一些初始字符开始，然后将新预测的字符作为下一个步骤的输入。我们将重复这个过程，直到生成特定长度的文本。
- en: In the following code, we have also added extra statements to the function to
    prime the network with some initial text and start from there.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们还向函数添加了额外的语句，以便用一些初始文本为网络预热并从那里开始。
- en: 'The network gives us predictions or probabilities for each character in the
    vocab. To reduce noise and only use the ones that the network is more confident
    about, we''re going to only choose a new character from the top *N* most probable
    characters in the output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 网络为我们提供了词汇中每个字符的预测或概率。为了减少噪声并只使用网络更加自信的字符，我们将只从输出中选择前*N*个最可能的字符：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s start the sampling process using the latest checkpoint saved:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用保存的最新检查点进行采样过程：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, it''s time to sample using this latest checkpoint:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用这个最新的检查点进行采样的时间到了：
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see that we were able to generate some meaningful words and some meaningless
    words. In order to get more results, you can run the model for more epochs and
    try to play with the hyperparameters.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们能够生成一些有意义的词汇和一些无意义的词汇。为了获得更多结果，你可以让模型训练更多的epochs，并尝试调整超参数。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We learned about RNNs, how they work, and why they have become a big deal. We
    trained an RNN character-level language model on fun novel datasets and saw where
    RNNs are going. You can confidently expect a large amount of innovation in the
    space of RNNs, and I believe they will become a pervasive and critical component
    of intelligent systems.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了RNN，它们是如何工作的，以及为什么它们变得如此重要。我们在有趣的小说数据集上训练了一个RNN字符级语言模型，并看到了RNN的发展方向。你可以自信地期待在RNN领域会有大量的创新，我相信它们将成为智能系统中无处不在且至关重要的组成部分。
