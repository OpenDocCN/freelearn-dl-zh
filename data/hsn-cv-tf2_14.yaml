- en: Assessments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Answers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: The answers to the assessment questions found at the end of each chapter are
    shared in the following sections.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 每章末尾的评估问题的答案将在以下部分中共享。
- en: Chapter 1
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章
- en: '**Which of the following tasks does not belong to computer vision: a web search
    of images similar to a query, a 3D scene reconstruction from image sequences,
    or the animation of a video character?**'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**以下哪个任务不属于计算机视觉：基于查询图像进行相似图片的网络搜索、从图像序列中重建 3D 场景，还是动画视频角色？**'
- en: The *latter*, which instead belongs to the domain of **computer graphics.**
    Note, however, that increasingly, computer vision algorithms are helping artists
    to generate or animate content more efficiently (such as the *motion capture*
    methods, for instance, which record actors performing some actions and transfer
    the motions to virtual characters).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*后者*，它属于 **计算机图形学** 的领域。然而，值得注意的是，越来越多的计算机视觉算法正在帮助艺术家更高效地生成或动画化内容（例如，*动作捕捉*
    方法，记录演员执行某些动作并将这些动作转移到虚拟角色身上）。'
- en: '**Which** **activ****ation** **function did the original perceptrons use?**'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原始感知机使用了哪种激活函数？**'
- en: The `step` function.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`step` 函数。'
- en: '**Suppose we want to train a method to detect whether a handwritten digit is
    a *4*. How should we adapt the network implemented in the chapter for this task?**'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假设我们想训练一种方法来检测手写数字是否为 *4*。我们应该如何调整本章中实现的网络以完成这个任务？**'
- en: In the chapter, we trained a classification network to identify pictures of
    digits from `0` to `9`. Therefore, the network had to predict the proper class
    among 10, hence, an output vector of 10 values (one for each class score/probability).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们训练了一个分类网络来识别数字图片，范围从 `0` 到 `9`。因此，网络必须在 10 个类别中预测出正确的类别，因此，输出向量包含 10
    个值（每个类别一个得分/概率）。
- en: In this question, we define a different classification task. We want the network
    to identify whether an image contains a *4* or *not a 4*. This is a **binary classification**,
    and the network should, therefore, be edited to *output only two values*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们定义了一个不同的分类任务。我们希望网络识别图像中是否包含 *4* 或 *不是 4*。这是一个 **二分类任务**，因此，网络应进行编辑，*仅输出两个值*。
- en: Chapter 2
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章
- en: '**What is Keras compared to TensorFlow? What is its purpose?**'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Keras 与 TensorFlow 有什么区别？它的目的是什么？**'
- en: Keras was designed as a wrapper around other deep learning libraries to make
    development easier. TensorFlow is now fully integrated with Keras through `tf.keras`.
    It is best practice to use this module to create models in TensorFlow 2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 被设计为其他深度学习库的封装器，以便简化开发。TensorFlow 现在通过 `tf.keras` 与 Keras 完全集成。在 TensorFlow
    2 中，最佳实践是使用该模块创建模型。
- en: '**Why does TensorFlow use graphs? How can they be created manually?**'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么 TensorFlow 使用图？如何手动创建图？**'
- en: TensorFlow relies on graphs to ensure model performance and portability. In
    TensorFlow 2, the best way to create graphs manually is to employ the `tf.function`
    decorator.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 依赖于图来确保模型的性能和可移植性。在 TensorFlow 2 中，手动创建图的最佳方式是使用 `tf.function` 装饰器。
- en: '**What is the difference between eager execution mode and lazy execution mode?**'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**急切执行模式和延迟执行模式有什么区别？**'
- en: In lazy execution mode, no computation is performed until the user specifically
    asks for a result. In eager execution mode, every operation is run when it is
    defined. While the former can be faster thanks to graph optimizations, the latter
    is easier to use and easier to debug. In TensorFlow 2, lazy execution mode has
    been deprecated in favor of eager execution mode.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在延迟执行模式下，直到用户特别请求结果时才会执行计算。而在急切执行模式下，每个操作在定义时都会执行。虽然前者由于图优化可能更快，但后者更易于使用和调试。在
    TensorFlow 2 中，延迟执行模式已被弃用，转而支持急切执行模式。
- en: '**How do you log information in TensorBoard, and how do you display it? **'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如何在 TensorBoard 中记录信息，如何显示它？**'
- en: 'To log information in TensorBoard, you can use the `tf.keras.callbacks.TensorBoard`
    callback and pass it to the `.fit` method when training a model. To log information
    manually, you can use the `tf.summary` module. To display information, launch
    the following command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 TensorBoard 中记录信息，您可以使用 `tf.keras.callbacks.TensorBoard` 回调函数，并将其传递给 `.fit`
    方法以训练模型。要手动记录信息，您可以使用 `tf.summary` 模块。要显示信息，请启动以下命令：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `model_logs` is the directory where TensorBoard logs are stored. This
    command will output a URL. Navigate to this URL to monitor training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`model_logs` 是存储 TensorBoard 日志的目录。此命令将输出一个 URL。导航到该 URL 以监控训练过程。
- en: '**What are the main differences between TensorFlow 1 and 2?**'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**TensorFlow 1 和 2 之间的主要区别是什么？**'
- en: TensorFlow 2 focuses on simplicity by removing graph management from the hands
    of the user. It also uses eager execution by default, making models easier to
    debug. Nevertheless, it still maintains its performance thanks to AutoGraph and
    `tf.function`. It also integrates deeply with Keras, making model creation easier
    than ever.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2 通过将图管理从用户手中移除，专注于简化操作。它默认使用即时执行（eager execution），使得模型更容易调试。然而，它依然通过
    AutoGraph 和 `tf.function` 保持了高性能。同时，它与 Keras 深度集成，使得模型创建比以往任何时候都更简单。
- en: Chapter 3
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章
- en: '**Why does the output of a convolutional layer have a smaller width and height
    than the input, unless it is padded?**'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么卷积层的输出宽度和高度会比输入小，除非进行了填充？**'
- en: The spatial dimensions of the output of a convolutional layer represent the
    number of valid positions the kernels could take when sliding over the input tensors,
    vertically and horizontally. Since kernels span over *k* × *k* pixels (if square),
    the number of positions they can take over the input image without being partially
    out of it can only be equal to (if *k* = 1), or less than, the image dimensions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层输出的空间维度表示内核在垂直和水平方向上滑动时能够取到的有效位置数量。由于内核跨越 *k* × *k* 像素（如果是正方形），它们可以在输入图像上取的位置数量只能等于（如果
    *k* = 1）或小于图像的维度。
- en: This is expressed by the equations presented in the chapter, to compute the
    output dimensions based on the layer's hyper parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过本章中提出的方程来表达的，方程用于基于层的超参数计算输出维度。
- en: '**What would be the output of a max-pooling layer with a receptive field of
    (2, 2) and a stride of 2 on the input matrix in Figure 3-6?**'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在图3-6中的输入矩阵上，具有（2，2）感受野和步幅为2的最大池化层输出会是什么？**'
- en: '![](img/b377c7f8-43ec-4690-b38b-7be67231f8a7.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b377c7f8-43ec-4690-b38b-7be67231f8a7.png)'
- en: '**How could LeNet-5 be implemented using the Keras Functional API in a non-object-oriented
    manner ?**'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如何通过 Keras Functional API 以非面向对象的方式实现 LeNet-5？**'
- en: 'The code is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**How does L1/L2 regularization affect the networks?**'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L1/L2 正则化如何影响网络？**'
- en: '**L1 regularization** forces the layers to which it is applied to bring toward
    zero the values of the parameters linked to less important features; that is,
    to ignore less meaningful features (such as features tied to dataset noise).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1 正则化** 强制应用它的层将与不重要特征相关联的参数值拉向零；也就是说，忽略那些不重要的特征（如与数据集噪声相关的特征）。'
- en: '**L2 regularization** compels the layers to keep their variables low, and,
    hence, more homogeneously distributed. It prevents the network from developing
    a small set of parameters with large values that overly influence its predictions.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2 正则化** 强制层将其变量保持在较低的水平，从而使它们更加均匀分布。它防止网络形成一小组具有大值的参数，这些参数会过度影响模型的预测。'
- en: Chapter 4
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章
- en: '**Which TensorFlow Hub module can be used to instantiate an Inception classifier
    for ImageNet?**'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**哪个 TensorFlow Hub 模块可以用来实例化一个用于 ImageNet 的 Inception 分类器？**'
- en: The model at [https://tfhub.dev/google/tf2-preview/inception_v3/classification/2](https://tfhub.dev/google/tf2-preview/inception_v3/classification/2)
    can be directly used to classify ImageNet-like images, as this classification
    model was pretrained over this dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 位于 [https://tfhub.dev/google/tf2-preview/inception_v3/classification/2](https://tfhub.dev/google/tf2-preview/inception_v3/classification/2)
    的模型可以直接用于分类类似 ImageNet 的图像，因为该分类模型是在该数据集上预训练的。
- en: '**How can the first three residual macro-blocks of a ResNet-50 model from Keras
    Applications be frozen?**'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如何冻结 Keras Applications 中 ResNet-50 模型的前三个残差宏块？**'
- en: 'The code is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**When is transfer learning discouraged?**'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**何时不建议使用迁移学习？**'
- en: Transfer learning may not be beneficial when the *domains* are too dissimilar
    and the target data has a structure that is completely different to the source
    data structure. As mentioned in the chapter, while CNNs can be applied to images,
    text, and audio files, transferring weights trained for one modality to another
    is not encouraged.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *领域* 差异过大且目标数据的结构与源数据结构完全不同时，迁移学习可能并不会带来好处。正如本章所述，尽管 CNN 可以应用于图像、文本和音频文件，但将针对一种模态训练的权重迁移到另一种模态是不推荐的。
- en: Chapter 5
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章
- en: '**What is the difference between a bounding box, an anchor box, and a ground
    truth box?**'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**边界框、锚框和真实框之间有什么区别？**'
- en: A **bounding box** is the smallest rectangle enclosing an object. An **anchor
    box** is a bounding box with a specific size. For each position in the image grid,
    there are usually several anchor boxes with different aspect ratios—square, vertical
    rectangle, and horizontal rectangle. By refining the size and the position of
    the anchor box, the object detection model generates predictions. A **ground truth
    box** is a bounding box corresponding to a specific object in the training set.
    If a model is trained perfectly, it generates predictions that are very close
    to ground truth boxes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**边界框**是包围物体的最小矩形。**锚框**是具有特定大小的边界框。对于图像网格中的每个位置，通常会有几个具有不同纵横比的锚框——正方形、竖直矩形和横向矩形。通过调整锚框的大小和位置，目标检测模型可以生成预测结果。**地面实况框**是与训练集中某个特定物体对应的边界框。如果一个模型训练得很好，它会生成非常接近地面实况框的预测结果。'
- en: '**What is the role of the feature extractor?**'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征提取器的作用是什么？**'
- en: A feature extractor is a CNN that converts an image into a feature volume. The
    feature volume is usually smaller in dimension than the input image and contains
    meaningful features that can be passed to the remainder of the network in order
    to generate predictions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取器是一个 CNN，它将图像转换为特征体积。特征体积的维度通常比输入图像小，并且包含可以传递给网络其他部分的有意义特征，从而生成预测结果。
- en: '**Which of the following models should you choose: YOLO or Faster R-CNN?**'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**你应该选择以下哪种模型：YOLO 还是 Faster R-CNN？**'
- en: If speed is the priority, you should pick YOLO as it is the fastest architecture.
    If accuracy is paramount, you should choose Faster R-CNN as it generates the best
    predictions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果速度是优先考虑的因素，你应该选择 YOLO，因为它是最快的架构。如果准确性至关重要，你应该选择 Faster R-CNN，因为它生成最好的预测结果。
- en: '**When are anchor boxes used?**'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**何时使用锚框？**'
- en: Before anchor boxes, box prediction dimensions were generated using the output
    of the network. As object sizes vary (a person usually fits in a vertical rectangle,
    while a car fits in a horizontal rectangle), anchor boxes were introduced. Using
    this technique, each anchor box is able to specialize for one object ratio, leading
    to more precise predictions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在锚框出现之前，框的预测维度是通过网络的输出生成的。由于物体的大小不同（一个人通常适合竖直矩形，而一辆车适合横向矩形），因此引入了锚框。使用这种技术，每个锚框可以专门化为一个物体比例，从而生成更精确的预测。
- en: Chapter 6
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章
- en: '**What is the partic****ularity of autoencoders?**'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自编码器的特性是什么？**'
- en: Autoencoders are encoders-decoders whose **inputs and targets are the same**.
    Their goal is to properly encode and then decode images without impacting their
    quality, despite their *bottleneck* (that is, their latent space of lower dimensionality).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是输入和目标相同的编码器-解码器。它们的目标是正确编码并解码图像，而不影响图像的质量，尽管它们有*瓶颈*（即其低维度的潜在空间）。
- en: '**Which classification architecture are fully convolutional networks (FCNs)
    based on?**'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完全卷积网络（FCNs）基于哪种分类架构？**'
- en: '**FCNs** use **VGG-16** as the feature extractor.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**FCNs** 使用 **VGG-16** 作为特征提取器。'
- en: '**How can a semantic segmentation model be trained so that it does not ignore
    small classes?**'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如何训练语义分割模型，以便它不忽略小类别？**'
- en: '**Per-class weighing** can be applied to the cross-entropy loss, thereby penalizing
    more heavy pixels from smaller classes that are misclassified. Losses that are
    not affected by the classes'' proportions can also be used instead, such as **Dice**.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别加权**可以应用于交叉熵损失，从而对较小类别的重像素进行更多的惩罚，这些像素被误分类。也可以使用不受类别比例影响的损失函数，例如**Dice**。'
- en: Chapter 7
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章
- en: '**Given an `a = [1, 2, 3]` tensor and a `b = [4, 5, 6]` tensor, how can a `tf.data`
    pipeline that would output each value separately, from `1` to `6`, be built?**'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**给定一个 `a = [1, 2, 3]` 的张量和一个 `b = [4, 5, 6]` 的张量，如何构建一个 `tf.data` 管道，以便将每个值从
    `1` 到 `6` 单独输出？**'
- en: 'The code is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**According to the documentation of `tf.data.Options`, how can you ensure that
    a dataset always returns samples in the same order, run after run?**'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**根据 `tf.data.Options` 的文档，如何确保数据集在每次运行时始终以相同的顺序返回样本？**'
- en: The `.experimental_deterministic` attribute of `tf.data.Options` should be set
    to `True` before being passed to the dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Options` 的 `.experimental_deterministic` 属性应该在传递给数据集之前设置为 `True`。'
- en: '**Which domain adaptation methods that we introduced can be used when no target
    annotations are available for training?**'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在没有目标注释用于训练时，我们介绍的哪些领域自适应方法可以使用？**'
- en: Unsupervised domain adaptation methods should be considered, such as *Learning
    Transferable Features with Deep Adaptation Networks*, by Mingsheng Long et al.
    (from Tsinghua University, China), or **Domain-Adversarial Neural Networks** (**DANN**),
    by Yaroslav Ganin et al. (from Skoltech).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 应该考虑使用无监督领域适应方法，例如*通过深度适应网络学习可转移特征*，该方法由中国清华大学的龙铭生等人提出，或者是**领域对抗神经网络**（**DANN**），由Skoltech的Yaroslav
    Ganin等人提出。
- en: '**What role does the discriminator play in GANs?**'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**判别器在GAN中扮演什么角色？**'
- en: It plays against the generator, trying to distinguish fake images from real
    images. The discriminator can be considered as a **trainable loss function** to
    guide the generator—the generator tries to minimize how *correct* the discriminator
    is, with both networks becoming better and better at their task as the training
    proceeds.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它与生成器对抗，试图区分假图像和真实图像。判别器可以看作是一个**可训练的损失函数**，用于引导生成器——生成器试图最小化判别器的*正确*程度，随着训练的进行，两个网络在各自的任务上会变得越来越好。
- en: Chapter 8
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章
- en: '**What are the main advantages of LSTMs over the simple RNN architecture?**'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LSTM相对于简单RNN架构的主要优点是什么？**'
- en: LSTMs suffer less from gradient vanishing and are more capable of storing long-term
    relationships in recurrent data. While they require more computing power, this
    usually leads to better predictions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM在梯度消失方面的影响较小，且能够更好地存储递归数据中的长期关系。尽管它们需要更多的计算能力，但这通常会导致更好的预测结果。
- en: '**How is a CNN used when it is applied before the LSTM?**'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**当CNN应用在LSTM之前时，它的作用是什么？**'
- en: The CNN acts as a feature extractor and reduces the dimensionality of the input
    data. By applying a pretrained CNN, we extract meaningful features from the input
    images. The LSTM trains faster since those features have a much smaller dimensionality
    than the input image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: CNN作为特征提取器，减少了输入数据的维度。通过应用预训练的CNN，我们可以从输入图像中提取有意义的特征。LSTM训练得更快，因为这些特征的维度比输入图像小得多。
- en: '**What is vanishing gradient and why does it occur? Why is it a problem?**'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**什么是梯度消失，为什么会发生？为什么这是一个问题？**'
- en: When backpropagating the error in RNNs, we need to go back through the time
    steps as well. If there are many time steps, the information slowly fades away
    due to the way in which the gradient is computed. It is a problem since it makes
    it harder for the network to learn how to generate good predictions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中反向传播误差时，我们还需要回溯时间步。如果时间步数很多，由于梯度计算的方式，信息会逐渐消失。这是一个问题，因为它使得网络更难学习如何生成好的预测结果。
- en: '**What are some of the workarounds for the vanishing gradient problem?**'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解决梯度消失问题有哪些方法？**'
- en: One workaround is to use truncated backpropagation, which is a technique described
    in the chapter. Another option is to use LSTMs instead of simple RNNs, as they
    suffer less from gradient vanishing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方法是使用截断反向传播，这是一种在本章中描述的技术。另一种选择是使用LSTM而不是简单的RNN，因为LSTM在梯度消失方面的影响较小。
- en: Chapter 9
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章
- en: '**When measuring a model''s inference speed, should you measure with single
    or multiple images?**'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在测量模型推理速度时，应该使用单张图片还是多张图片？**'
- en: Multiple images should be used to avoid measure bias.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用多张图片，以避免测量偏差。
- en: '**Is a model with `float32` weights larger or smaller than one with `float16`
    weights?**'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**具有`float32`权重的模型比具有`float16`权重的模型大还是小？**'
- en: '`Float16` weights use about half the space of `float32` weights. On compatible
    devices, they can also be faster.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Float16`权重使用的空间大约是`float32`权重的一半。在兼容设备上，它们也可以更快。'
- en: '**On iOS devices, should you use Core ML or TensorFlow Lite? What about Android
    devices?**'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在iOS设备上，应该使用Core ML还是TensorFlow Lite？安卓设备呢？**'
- en: On iOS devices, we recommend using Core ML where possible as it is available
    natively and is tightly integrated with the hardware. On Android devices, TensorFlow
    Lite should be used as there is no alternative.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在iOS设备上，我们建议尽可能使用Core ML，因为它原生支持并且与硬件紧密集成。在Android设备上，应该使用TensorFlow Lite，因为没有其他替代方案。
- en: '**What are the benefits and limitations of running a model in the browser?**'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在浏览器中运行模型有哪些好处和局限性？**'
- en: It does not require any installation on the user side and does not require computing
    power on the server side, making the application almost infinitely scalable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它不需要在用户端安装任何东西，也不需要在服务器端占用计算能力，使得应用几乎可以无限扩展。
- en: '**What is the most important requirement for embedded devices running deep
    learning algorithms?**'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入式设备运行深度学习算法的最重要要求是什么？**'
- en: On top of computing power, the most important requirement is power consumption,
    since most embedded devices run on batteries.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算能力外，最重要的需求是功耗，因为大多数嵌入式设备依赖电池供电。
