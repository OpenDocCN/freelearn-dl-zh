- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Advanced Regularization in Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的高级正则化
- en: A full book could be written about regularization in **natural language processing**
    (**NLP**). NLP is a wide field that consists of many topics, ranging from simple
    classification such as review ranking to complex models and solutions such as
    ChatGPT. This chapter will merely scratch the surface of what can reasonably be
    done with simple NLP solutions such as classification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 **自然语言处理**（**NLP**）的正则化可以写成一本完整的书。NLP 是一个广泛的领域，涵盖了许多主题，从简单的分类任务（如评论排序）到复杂的模型和解决方案（如
    ChatGPT）。本章仅会略微触及使用简单 NLP 解决方案（如分类）能够合理完成的内容。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Regularization using a word2vec embedding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 word2vec 嵌入的正则化
- en: Data augmentation using word2vec
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 word2vec 的数据增强
- en: Zero-shot inference with pre-trained models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型进行零-shot 推理
- en: Regularization with BERT embeddings
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BERT 嵌入的正则化
- en: Data augmentation using GPT-3
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPT-3 的数据增强
- en: By the end of this chapter, you will be able to take advantage of advanced methods
    for NLP tasks such as word embeddings and transformers, as well as be able to
    use data augmentation to generate synthetic training data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够利用高级方法处理 NLP 任务，如词嵌入和 transformers，并能使用数据增强生成合成训练数据。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use various NLP solutions and tools, so we will require
    the following libraries:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用各种 NLP 解决方案和工具，因此我们需要以下库：
- en: NumPy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: pandas
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: scikit-learn
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: Gensim
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gensim
- en: NLTK
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK
- en: PyTorch
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Transformers
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers
- en: OpenAI
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI
- en: Regularization using a word2vec embedding
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 word2vec 嵌入的正则化
- en: In this recipe, we will use a pre-trained word2vec embedding to improve the
    results of a task thanks to transfer learning. We will compare the results to
    the initial *Training a GRU* recipe from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206),
    on the IMDb dataset for review classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用预训练的 word2vec 嵌入，借助迁移学习来提高任务的结果。我们将结果与 [*第 8 章*](B19629_08.xhtml#_idTextAnchor206)中的
    *训练 GRU* 任务进行比较，数据集为 IMDb 的评论分类。
- en: Getting ready
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: A word2vec is a rather old type of word embedding in the NLP landscape and has
    been widely used in many NLP tasks. While recent techniques are sometimes more
    powerful, the word2vec approach remains efficient and cost-effective.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 是自然语言处理（NLP）领域中一种相对较旧的词嵌入方法，已广泛应用于许多 NLP 任务。尽管近期的技术有时更强大，但 word2vec
    方法仍然高效且具有成本效益。
- en: Without getting into the details of word2vec, a commonly used model is a 300-dimensional
    embedding; each word in the vocabulary is embedded into a vector of 300 values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入讨论 word2vec 的细节，一个常用的模型是 300 维的嵌入；词汇表中的每个单词都会被嵌入到一个包含 300 个值的向量中。
- en: 'word2vec is usually trained on a large corpus of texts. There are two main
    approaches for training a word2vec that can be roughly described as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 通常在大规模的文本语料库上进行训练。训练 word2vec 的主要方法有两种，基本可以描述如下：
- en: '**Continuous bag of words** (**CBOW**): Uses the context of surrounding words
    in a sentence to predict a missing word'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋模型**（**CBOW**）：使用句子中周围词的上下文来预测缺失的词'
- en: '**skip-gram**: Uses a word to predict its surrounding context'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**skip-gram**：使用一个词来预测其周围的上下文'
- en: 'An example of these two approaches is proposed in *Figure 9**.1*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的示例见 *图 9.1*：
- en: '![Figure 9.1 – An example of training data for both the CBOW (left) and skip-gram
    (right) methods](img/B19629_09_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – CBOW（左）和 skip-gram（右）方法的训练数据示例](img/B19629_09_01.jpg)'
- en: Figure 9.1 – An example of training data for both the CBOW (left) and skip-gram
    (right) methods
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – CBOW（左）和 skip-gram（右）方法的训练数据示例
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In practice, CBOW is usually easier to train, while skip-gram may have better
    performance for rarer words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中，CBOW 通常更容易训练，而 skip-gram 对稀有词的表现可能更好。
- en: The goal is not to train our own word2vec, but simply to reuse a trained one
    and take advantage of transfer learning to get a performance boost in our predictions.
    In this recipe, instead of training our own embedding before feeding the **gated
    recurrent unit** (**GRU**), we will simply reuse a pre-trained word2vec embedding,
    and then only train our GRU on top of these embeddings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 目标不是训练我们自己的 word2vec，而是简单地重用一个已经训练好的模型，并利用迁移学习来提升我们预测的性能。在这个步骤中，我们将不再训练自己的嵌入，而是直接重用一个预训练的
    word2vec 嵌入，然后只在这些嵌入的基础上训练我们的 GRU。
- en: 'For that, we will again work on the IMDb dataset classification task: a dataset
    containing texts of movie reviews as inputs and associated binary labels, positive
    or negative. The dataset for this can be downloaded with the Kaggle API:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将再次进行IMDb数据集分类任务：这是一个包含电影评论文本作为输入和相关二进制标签（正面或负面）的数据集。可以通过Kaggle API下载此数据集：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following command will install the required libraries:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将安装所需的库：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it…
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In this recipe, we will train a GRU for binary classification on the IMDb review
    dataset. Compared to the original recipe, the main difference is in *step 5*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将训练一个GRU模型，用于在IMDb评论数据集上进行二分类。与原始食谱相比，主要的区别在于*第5步*：
- en: 'Import the following necessary libraries:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下必要的库：
- en: '`torch` and some related modules and classes for the neural network'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`及其相关模块和类，用于神经网络'
- en: '`train_test_split` and `LabelEncoder` from `scikit-learn` for preprocessing'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自`scikit-learn`的`train_test_split`和`LabelEncoder`进行预处理
- en: '`AutoTokenizer` from `transformers` to tokenize the reviews'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用来自`transformers`的`AutoTokenizer`来标记化评论
- en: '`pandas` to load the dataset'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`用于加载数据集'
- en: '`numpy` for data manipulation'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`用于数据处理'
- en: '`matplotlib` for visualization'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`用于可视化'
- en: '`gensim` for the word2vec embedding and `nltk` for work tokenization'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`gensim`进行word2vec嵌入，使用`nltk`进行文本标记化处理
- en: 'If you haven’t done so yet, you will need to add the `nltk.download(''punkt'')`
    line to download some required utility instances, as shown here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，你需要添加`nltk.download('punkt')`这一行，以下载一些必要的工具实例，如下所示：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load the pre-trained word2vec model, which contains a 300-dimension embedding.
    The model is about 1.6 GB and may take some time to download, depending on your
    available bandwidth:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的word2vec模型，该模型包含300维的嵌入。该模型大约有1.6GB，下载可能需要一些时间，具体取决于你的带宽：
- en: '[PRE3]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Load the data from the CSV file with `pandas`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`从CSV文件加载数据：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Split the data into train and test sets using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数将数据拆分为训练集和测试集，测试集大小为20%，并指定随机状态以确保可复现性：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Implement the dataset’s `TextClassificationDataset` class, which handles the
    data. This is where the word2vec embedding is computed:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现数据集的`TextClassificationDataset`类，它处理数据。此处计算word2vec嵌入：
- en: '[PRE10]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'At instantiation, each input movie is converted into an embedding in two ways
    with the `embed` method:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化时，每个输入电影通过`embed`方法以两种方式转换为嵌入：
- en: Each movie review is tokenized with a word tokenizer (basically splitting sentences
    into words).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个电影评论都通过一个单词标记器进行标记化（基本上是将句子分割为单词）。
- en: Then, a vector of `max_words` length is computed, containing the word2vec embeddings
    of `max_words` first words in the review. If the review is shorter than `max_words`
    words, the vector is filled with zero padding.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，计算一个`max_words`长度的向量，包含评论中前`max_words`个单词的word2vec嵌入。如果评论少于`max_words`个单词，则使用零填充该向量。
- en: 'Then, we must instantiate the `TextClassificationDataset` objects for the train
    and test sets, as well as the related data loaders. The maximum number of words
    is set to `64`, as is the batch size:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须为训练集和测试集实例化`TextClassificationDataset`对象，以及相关的数据加载器。最大单词数设置为`64`，批处理大小也设置为：
- en: '[PRE42]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we must implement the GRU classifier model. Since the embedding was computed
    at the data loading step, this model is directly computing a three-layer GRU,
    followed by a fully connected layer with a sigmoid activation function:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须实现GRU分类模型。由于嵌入是在数据加载步骤中计算的，因此该模型直接计算一个三层GRU，并随后应用一个带有sigmoid激活函数的全连接层：
- en: '[PRE52]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next, we must instantiate the GRU model. The embedding dimension, defined by
    the word2vec model, is `300`. We have chosen a hidden dimension of `32` so that
    each GRU layer is made up of 32 units:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须实例化GRU模型。由word2vec模型定义的嵌入维度为`300`。我们选择了`32`作为隐藏维度，因此每个GRU层由32个单元组成：
- en: '[PRE72]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Then, we must instantiate the optimizer as an `Adam` optimizer with a learning
    rate of `0.001`; the loss is defined as the binary cross-entropy loss since this
    is a binary classification task:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须实例化优化器为`Adam`优化器，学习率为`0.001`；损失定义为二元交叉熵损失，因为这是一个二分类任务：
- en: '[PRE81]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Train the model for `20` epochs using the `train_model` function and store
    the loss and accuracy for both the train and test sets at each epoch. The implementation
    of the `train_model` function can be found in this book’s GitHub repository at
    [https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_model` 函数训练模型 `20` 个 epoch，并在每个 epoch 存储训练集和测试集的损失和准确性。`train_model`
    函数的实现可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_09/chapter_09.ipynb)：
- en: '[PRE83]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Here is the typical output after 20 epochs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 20 个 epoch 后的典型输出：
- en: '[PRE87]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Plot the BCE loss for the train and test sets:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练集和测试集的 BCE 损失图：
- en: '[PRE88]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Here is the plot for it:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的绘制结果：
- en: '![Figure 9.2 – Binary cross-entropy loss as a function of the epoch](img/B19629_09_02.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 二元交叉熵损失随 epoch 变化](img/B19629_09_02.jpg)'
- en: Figure 9.2 – Binary cross-entropy loss as a function of the epoch
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 二元交叉熵损失随 epoch 变化
- en: As we can see, while the train loss keeps decreasing over the 20 epochs, the
    test loss soon reaches a minimum at around 5 epochs, to then increase, indicating
    overfitting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，虽然训练损失在 20 个 epoch 中持续减少，但测试损失在约 5 个 epoch 后达到了最小值，然后开始增加，表明出现了过拟合。
- en: 'Plot the accuracy for the train and test sets:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练集和测试集的准确性图：
- en: '[PRE92]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Here is the plot for this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该图的绘制结果：
- en: '![Figure 9.3 – Accuracy as a function of the epoch](img/B19629_09_03.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 准确度随 epoch 变化](img/B19629_09_03.jpg)'
- en: Figure 9.3 – Accuracy as a function of the epoch
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 准确度随 epoch 变化
- en: As expected with the loss, the accuracy keeps increasing for the train set.
    For the test set, it reaches a maximum value of about 81% (against 77% without
    word2vec embedding, as shown in the previous chapter). Word2vec embedding allowed
    us to improve the results slightly, though the results may improve much more if
    we adjust the other hyperparameters accordingly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如损失所示，训练集的准确性不断提高。对于测试集，它的最大值约为 81%（相比于前一章中的 77%，未使用 word2vec 嵌入）。word2vec
    嵌入使我们略微提高了结果，尽管如果我们调整其他超参数，结果可能会有更大改善。
- en: There’s more…
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: While we used the embeddings as an array in this recipe, they can be used differently;
    for example, we can use an average of all the embeddings in a sentence or other
    statistical information.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本教程中将嵌入作为数组使用，但它们也可以以不同的方式使用；例如，我们可以使用句子中所有嵌入的平均值或其他统计信息。
- en: Also, while word2vec works well enough in many cases here, a few embeddings
    can be derived with a more specialized approach, such as doc2vec, which is sometimes
    more powerful for documents and longer texts.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管 word2vec 在许多情况下已经表现得足够好，但可以通过更专业的方法，如 doc2vec，来推导一些嵌入，doc2vec 对文档和长文本的处理有时更为强大。
- en: See also
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The Wikipedia article about word2vec is a valuable resource as it specifies
    many relevant publications: [https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3](https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 word2vec 的维基百科文章是一个有价值的资源，因为它列出了许多相关的出版物：[https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3](https://en.wikipedia.org/wiki/Word2vec#cite_note-:1-3)。
- en: 'This documentation from Google is also useful: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Google 的文档也很有用：[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)。
- en: Data augmentation using word2vec
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 word2vec 进行数据增强
- en: One way to regularize a model and get better performance is to have more data.
    Collecting data is not always easy or possible, but synthetic data can be an affordable
    way to improve performance. We’ll do this in this recipe.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化模型并提高性能的一种方法是拥有更多的数据。收集数据并不总是容易或可能的，但合成数据可以是一种负担得起的提高性能的方式。我们将在本教程中做到这一点。
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Using word2vec embeddings, you can generate new, synthetic data that has a close
    semantic meaning. By doing this, it is fairly easy for a given word to get the
    most similar words in a given vocabulary.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 word2vec 嵌入，你可以生成具有相似语义的新合成数据。通过这种方式，对于给定的单词，可以很容易地找到词汇表中最相似的词。
- en: In this recipe, using word2vec and a few parameters, we’ll see how we can generate
    new sentences with a close semantic meaning. We will only apply it to a given
    sentence as an example and propose how to integrate it into a full training pipeline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，使用 word2vec 和一些参数，我们将看到如何生成具有相似语义的新句子。我们仅将其应用于给定的句子作为示例，并提出如何将其集成到完整的训练流程中。
- en: The only required libraries are `numpy` and `gensim`, both of which can be installed
    with `pip install` `numpy gensim`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的库仅为`numpy`和`gensim`，这两个库都可以通过`pip install` `numpy gensim`安装。
- en: How to do it…
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Here are the steps to complete this recipe:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此配方的步骤如下：
- en: 'The first step is to import the necessary libraries – `numpy` for random calls
    and `gensim` for word2vec model loading:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是导入必要的库——`numpy`用于随机调用，`gensim`用于加载word2vec模型：
- en: '[PRE96]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Load a pre-trained word2vec model. This may take some time if the model hasn’t
    been downloaded and stored in a local cache already. Also, this is a rather large
    model, so it may take some time to load in memory:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个预训练的word2vec模型。如果模型尚未下载并存储在本地缓存中，这可能需要一些时间。此外，这是一个相当大的模型，因此加载到内存中可能需要一些时间：
- en: '[PRE98]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Implement the `replace_words_with_similar` function so that you can randomly
    replace `word` with another semantically close word:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`replace_words_with_similar`函数，以便你可以随机地将`word`替换为另一个语义相近的词：
- en: '[PRE101]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Hopefully, the comments are self-explanatory, but here is what this function
    does:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 希望注释已经不言自明，但以下是该函数的作用：
- en: It splits the input text into words by using a simple split (a word tokenizer
    can be used as well)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过使用简单的分割将输入文本拆分成单词（也可以使用词法分析器）。
- en: 'For each word, it checks for the following:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个词，它检查以下内容：
- en: If the word is in the word2vec vocabulary
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果该词在word2vec词汇表中
- en: If the word is not in a list of stop words (to be defined)
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果该词不在停用词列表中（待定义）
- en: If a random probability is above the threshold probability (to draw random words)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果随机概率高于阈值概率（用于抽取随机词）
- en: 'If a word fulfills the previous checks, the following is computed:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个词符合前面的检查，则计算如下：
- en: '`top_similar` most similar words'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_similar` 最相似的词'
- en: One of those words is picked randomly
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这些词中随机选择一个
- en: If the similarity score of this word is above a given threshold, add it to the
    output sentence
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果该词的相似度得分超过给定阈值，则将其添加到输出句子中
- en: If no updated word has been added, just add the original word so that the overall
    sentence remains logical
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有添加更新的词，就直接添加原始词，以使整体句子保持逻辑通顺
- en: 'The parameters are as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '`sim_threshold`: The similarity threshold'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sim_threshold`：相似度阈值'
- en: '`probability`: The probability for a word to be replaced with a similar word'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`probability`：词被替换为相似词的概率'
- en: '`top_similar`: The number of similar words to compute for a given word'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_similar`：计算给定词的相似词的数量'
- en: '`stop_words`: A list of words not to be replaced in case some words are specifically
    important or have several meanings'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop_words`：一个不应被替换的词列表，以防某些词特别重要或有多重含义'
- en: 'Apply the `replace_words_with_similar` function we just implemented to a given
    sentence:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们刚刚实现的`replace_words_with_similar`函数应用于给定的句子：
- en: '[PRE131]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The code output is as follows. It allows us to change a few words while keeping
    the overall meaning:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出如下。这允许我们在保持整体意思不变的情况下改变一些词：
- en: '[PRE136]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Thanks to this data augmentation technique, it is possible to generate more
    diverse data so that we can make the models more robust and regularized.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 借助这种数据增强技术，能够生成更多样化的数据，从而使我们能够使模型更强大并进行正则化。
- en: There’s more…
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'One way to add such a data generation function to a classification task would
    be to add it at the data-loading step. This would generate synthetic data on-the-fly
    and could allow us to regularize the model. It could be added to the dataset class,
    as shown here:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 向分类任务添加数据生成功能的一种方法是在数据加载步骤中添加它。这将实时生成合成数据，并可能允许我们对模型进行正则化。它可以被添加到数据集类中，如下所示：
- en: '[PRE137]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: See also
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: Documentation about the `most_similar` function of the word2vec model can be
    found at [https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有关word2vec模型的`most_similar`函数的文档可以在[https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.xhtml)找到。
- en: Zero-shot inference with pre-trained models
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练模型进行零样本推理
- en: The NLP field has faced many major advances in the last few years, which means
    that many pre-trained, efficient models can be reused. These pre-trained, freely
    available models allow us to approach some NLP tasks with zero-shot inference
    since we can reuse those models. We’ll try this approach in this recipe.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，NLP领域经历了许多重大进展，这意味着许多预训练的高效模型可以重复使用。这些预训练的、免费提供的模型使我们能够以零样本推理的方式处理一些NLP任务，因为我们可以重复使用这些模型。我们将在本配方中尝试这种方法。
- en: Note
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We sometimes use zero-shot inference (or zero-shot learning) and few-shot learning.
    Zero-shot learning means being able to perform a task without any training for
    this specific task; few-shot learning means performing a task while training only
    on a few samples.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时会使用零-shot推理（或零-shot学习）和少-shot学习。零-shot学习意味着在没有针对特定任务的任何训练的情况下完成任务；少-shot学习则意味着在仅用少量样本进行训练的情况下完成任务。
- en: Zero-shot inference is the act of reusing pre-trained models without doing any
    fine-tuning. There are many very powerful, free-to-use models available that can
    do just as well as a trained model of our own. Since the available models are
    trained on huge datasets with massive computational power, it is sometimes hard
    to compete with an in-house model that’s been trained on much less data and with
    less computational power.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot推理是指在没有任何微调的情况下重用预训练模型。许多非常强大的、可以自由使用的模型已经可以做到和我们自己训练的模型一样好。由于这些可用模型是在庞大的数据集上训练的，且拥有巨大的计算能力，因此有时很难与我们自己训练的模型竞争，因为我们自己训练的模型可能使用的数据更少，计算能力也较低。
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: That being said, sometimes, training on small, well-curated, task-specific data
    can do wonders and provide much better performance. It’s all a matter of context.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，有时候，在小而精心策划、特定任务的数据上进行训练也能产生奇迹，并提供更好的性能。这完全取决于上下文。
- en: Also, we sometimes have data without labels, so supervised learning is not a
    possibility. In such cases, labeling a small subsample of the data ourselves and
    evaluating a zero-shot approach against this data can be useful.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有时会遇到没有标签的数据，因此监督学习就不可行。在这种情况下，我们自己为数据中的一个小子集标注标签，并针对这些数据评估零-shot方法可能会有用。
- en: Getting ready
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: In this recipe, we will reuse a pre-trained model on the Tweets dataset and
    classify tweets as negative, neutral, or positive. Since no training is needed,
    we will directly evaluate the model on the test set so that it can be compared
    with the results we obtained with a simple RNN in the *Training an RNN* recipe
    from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将重用在Tweets数据集上预训练的模型，并将推文分类为负面、中性或正面。由于不需要训练，我们将直接在测试集上评估该模型，以便与我们在[*第8章*](B19629_08.xhtml#_idTextAnchor206)中的*训练RNN*配方中获得的结果进行比较。
- en: To do so, we need to download the dataset locally. It can be downloaded with
    the Kaggle API and then unzipped with the following command
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要将数据集下载到本地。可以通过Kaggle API下载，然后使用以下命令解压：
- en: '[PRE138]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'The libraries that are required to run this recipe can be installed with the
    following command:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此配方所需的库可以通过以下命令安装：
- en: '[PRE139]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: How to do it…
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Here are the steps to perform this recipe:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这是执行此配方的步骤：
- en: 'Import the following necessary functions and models:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下必要的函数和模型：
- en: '`numpy` for data manipulation'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`numpy`进行数据处理
- en: '`pandas` for loading the data'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据
- en: '`train_test_split` from `scikit-learn` to split the dataset'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`的`train_test_split`来拆分数据集
- en: '`accuracy_score` from `scikit-learn` to compute the accuracy score'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`的`accuracy_score`来计算准确度评分
- en: '`pipeline` from `transformers` for instantiating the zero-shot classifier'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`transformers`的`pipeline`来实例化零-shot分类器
- en: 'Here is the code for this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这段代码：
- en: '[PRE140]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'Load the dataset. In our case, the only columns of interest are `text` for
    the features and `airline_sentiment` for the labels:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集。在我们的例子中，唯一感兴趣的列是`text`（特征）和`airline_sentiment`（标签）：
- en: '[PRE141]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Here is the output of this code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这段代码的输出：
- en: '![Figure 9.4 – First five rows of the dataset for the considered columns](img/B19629_09_04.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 所考虑列的数据集的前五行](img/B19629_09_04.jpg)'
- en: Figure 9.4 – First five rows of the dataset for the considered columns
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 所考虑列的数据集的前五行
- en: 'Split the data into train and test sets, with the same parameters as in the
    *Regularization using a word2vec embedding* recipe so that the results can be
    compared: `test_size` set to `0.2` and `random_state` set to `0`. Since there
    is no training, we will only use the test set:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集，使用与*使用word2vec嵌入的正则化*配方中相同的参数，以便可以进行比较：`test_size`设置为`0.2`，`random_state`设置为`0`。由于不需要训练，我们只会使用测试集：
- en: '[PRE144]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Instantiate the classifier using a `transformers` pipeline with the following
    parameters:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下参数通过`transformers`管道实例化分类器：
- en: '`task="zero-shot-classification"`: This will instantiate a zero-shot classification
    pipeline'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task="zero-shot-classification"`：这将实例化一个零-shot分类管道'
- en: '`model="facebook/bart-large-mnli"`: This will specify the model to be used
    for this pipeline'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model="facebook/bart-large-mnli"`：这将指定用于该管道的模型'
- en: 'Here is the code for this:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这段代码：
- en: '[PRE147]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Note
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When this is first called, it may download several files and the model itself,
    and it may take some time.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当首次调用时，它可能会下载一些文件以及模型本身，可能需要一些时间。
- en: 'Store the candidate labels in an array. These candidate labels are needed for
    zero-shot classification:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将候选标签存储在数组中。这些候选标签是零-shot分类所需的：
- en: '[PRE148]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Compute the predictions on the test set and store them in an array:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上计算预测结果并将其存储在数组中：
- en: '[PRE149]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: Refer to the *There’s more…* section for a few details about what the classifier
    does, as well as its outputs.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*更多内容...*部分，了解分类器的功能以及它的输出。
- en: 'Compute the accuracy score of the predictions:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测的准确率：
- en: '[PRE162]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'The computed accuracy score is as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的准确率如下：
- en: '[PRE163]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: We got an accuracy score of 74.5%, which is equivalent to the results we had
    after training a simple RNN in the *Training an RNN* recipe from [*Chapter 8*](B19629_08.xhtml#_idTextAnchor206).
    Without any training costs and large, labeled datasets, we can get the same performance
    thanks to this zero-shot classification.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了74.5%的准确率，这与我们在[*第8章*](B19629_08.xhtml#_idTextAnchor206)中使用简单RNN训练后的结果相当。通过这种零-shot分类，我们无需任何训练成本和大规模的标注数据集，就能获得相同的性能。
- en: Note
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Zero-shot learning comes with a cost since pre-trained language models are usually
    rather large and may require large computational power to run at scale.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot学习需要付出代价，因为预训练的语言模型通常相当庞大，且可能需要较大的计算能力来大规模运行。
- en: There’s more…
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'Let’s look at an example of the input and output of `classifier` to get a better
    understanding of what it does:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个`classifier`的输入和输出示例，以更好地理解它的工作原理：
- en: '[PRE164]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'The output of this code is as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出如下：
- en: '[PRE165]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'Here’s what we can see:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到以下内容：
- en: 'An input sentence: `I love to learn` `about regularization`'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入句子：`I love to learn` `about regularization`
- en: 'Candidate labels: `positive`, `negative`, and `neutral`'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选标签：`positive`，`negative`，和`neutral`
- en: 'The result is a dictionary with the following key values:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含以下键值的字典：
- en: '`''sequence''`: The input sequence'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''sequence''`：输入序列'
- en: '`''labels''`: The input candidate labels'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''labels''`：输入的候选标签'
- en: '`''scores''`: A list of scores, one for each label, sorted in decreasing order'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''scores''`：每个标签对应的得分列表，按降序排序'
- en: Note
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since the scores are always sorted in decreasing order, the labels may have
    a different order.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 由于得分总是按降序排序，标签的顺序可能会有所不同。
- en: 'In the end, the predicted class can be computed with the following code, which
    will retrieve the `argmax` values of the scores and the associated label:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，预测的类别可以通过以下代码计算，该代码将检索得分的`argmax`值及其相关标签：
- en: '[PRE166]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: In our case, that would output `positive`.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，输出将是`positive`。
- en: See also
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The model card of the bart-large-mnli model: [https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bart-large-mnli模型的模型卡：[https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)
- en: 'A tutorial from Hugging Face about zero-shot classification: [https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification](https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face关于零-shot分类的教程：[https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification](https://huggingface.co/course/chapter1/3?fw=pt#zero-shot-classification)
- en: 'The documentation about the `transformers` pipelines, which allows us to do
    much more than zero-shot classification: [https://huggingface.co/docs/transformers/main_classes/pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`transformers`流水线的文档，它可以让我们做更多的不仅仅是零-shot分类：[https://huggingface.co/docs/transformers/main_classes/pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
- en: Regularization with BERT embeddings
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT嵌入的正则化
- en: Similar to how we used a pre-trained word2vec model to compute the embeddings,
    it is possible to use the embeddings of a pre-trained BERT model, a transformer-based
    model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们如何使用预训练的word2vec模型来计算嵌入，我们也可以使用预训练的BERT模型的嵌入，这是一个基于transformer的模型。
- en: In this recipe, after quickly explaining the BERT model, we will train a model
    using BERT embeddings.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们会在快速介绍BERT模型后，使用BERT嵌入训练一个模型。
- en: '**BERT** stands for **Bidirectional Encoder Representation from Transformers**
    and is a model that was proposed by Google in 2018\. It was first deployed in
    late 2019 in Google Search for English queries, as well as for many other languages.
    The BERT model has been proven effective in several NLP tasks, including text
    classification and question-answering.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**代表**双向编码器表示从变换器**，是谷歌在2018年提出的模型。它在2019年底首次在Google搜索中用于英文查询，并且支持许多其他语言。BERT模型已被证明在多个NLP任务中有效，包括文本分类和问答。'
- en: Before quickly explaining what BERT is, let’s take a step back and look at what
    **attention mechanisms** and **transformers** are.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速解释什么是BERT之前，让我们先回顾一下什么是**注意力机制**和**变换器**。
- en: Attention mechanisms are widely used in NLP, and more and more in other fields
    such as computer vision, since their introduction in 2017\. The high-level idea
    of an attention mechanism is to compute a weight for each input token concerning
    other tokens in the given sequence. Compared to RNNs, which process inputs as
    sequences, the attention mechanism considers the whole sequence at once. This
    allows attention-based models to handle long-range dependencies in sequences more
    efficiently since the attention mechanism can be considered agnostic to the sequence
    length.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制广泛应用于NLP领域，并且在计算机视觉等其他领域的应用也日益增多，自2017年首次提出以来。注意力机制的高级概念是计算每个输入标记相对于给定序列中其他标记的权重。与按序列处理输入的RNN相比，注意力机制同时考虑整个序列。这使得基于注意力的模型能够更有效地处理序列中的长程依赖，因为注意力机制可以不考虑序列长度。
- en: Transformers are a type of neural network based on self-attention. They usually
    start with an embedding, as well as an absolute positional encoding that attention
    layers are trained on. Those layers usually use multi-head attention to capture
    various aspects of the input sequence. For more details, you can read the original
    paper, *Attention Is All You Need* (refer to the *See also* section for the paper).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是一种基于自注意力的神经网络。它们通常以嵌入向量开始，并且具有绝对位置编码，注意力层基于此进行训练。这些层通常使用多头注意力来捕捉输入序列的不同方面。更多详情可以参考原始论文《Attention
    Is All You Need》（可参阅*另见*部分）。
- en: Note
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since BERT uses absolute positional encodings, you are advised to use padding
    on the right, if any padding is used.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BERT使用的是绝对位置编码，如果使用填充，建议将填充放在右侧。
- en: 'The BERT model was built on top of `transformers` and is made of 12 transformer-based
    encoding layers for the base model (24 layers for the large model), for about
    110 million parameters. More interestingly, it was pre-trained in an unsupervised
    fashion, using two methods:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型建立在`transformers`之上，由12层基于变换器的编码层构成（大模型有24层），大约有1.1亿个参数。更有趣的是，它是以无监督方式预训练的，使用了两种方法：
- en: '**Masked language**: 15% of the tokens in a sequence are randomly masked, and
    the model is trained to predict the masked tokens'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩蔽语言模型**：序列中15%的标记被随机掩蔽，模型被训练去预测这些被掩蔽的标记。'
- en: '**Next sentence prediction**: Given two sentences, the model is trained to
    predict whether they are consecutive in a given text'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一句预测**：给定两句话，模型被训练预测它们是否在给定文本中是连续的。'
- en: 'This pre-training approach is summarized in the following diagram, which was
    extracted from the BERT paper called *BERT: Pre-training of Deep Bidirectional
    Transformers for* *Language Understanding*:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '这种预训练方法在下图中进行了总结，图示来自BERT论文《BERT: 语言理解的深度双向变换器预训练》：'
- en: '![Figure 9.5 – BERT pre-training diagram proposed in the original article,
    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](img/B19629_09_05.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 文章中提出的BERT预训练图示，BERT: 语言理解的深度双向变换器预训练](img/B19629_09_05.jpg)'
- en: 'Figure 9.5 – BERT pre-training diagram proposed in the original article, BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '图9.5 – 文章中提出的BERT预训练图示，BERT: 语言理解的深度双向变换器预训练'
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While word2vec embedding is context-free (no matter the context, a word embedding
    remains the same), BERT gives a different embedding for a given word, depending
    on its surroundings. This makes sense since a given word may have a different
    meaning in two sentences (for example, apple or Apple can be either a fruit or
    a company, depending on the context).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然word2vec嵌入是无上下文的（无论上下文如何，词嵌入保持不变），但BERT根据周围环境为给定词语提供不同的嵌入。这是有道理的，因为一个给定的词在两个句子中的意思可能不同（例如，apple或Apple可以是水果也可以是公司，取决于上下文）。
- en: Getting ready
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will reuse the Tweets dataset, which can be downloaded
    and then unzipped locally with the following command line:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本食谱，我们将重用Tweets数据集，可以通过以下命令行下载并解压到本地：
- en: '[PRE167]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: The necessary libraries can be installed with `pip install torch scikit-learn`
    `transformers pandas`.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 必要的库可以通过`pip install torch scikit-learn` `transformers pandas`安装。
- en: How to do it…
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'In this recipe, we will train a simple logistic regression on top of pre-trained
    BERT embeddings:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将在预训练BERT嵌入上训练一个简单的逻辑回归模型：
- en: 'Make the required imports:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '`torch` for device management if you have a GPU'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有GPU，可以使用`torch`进行设备管理。
- en: The `train_test_split` method and the `LogisticRegression` class from `scikit-learn`
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`方法和`scikit-learn`中的`LogisticRegression`类。
- en: The related `BERT` classes from `transformers`
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers`中的相关`BERT`类。'
- en: '`pandas` for data loading'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据。
- en: 'Here is the code for this:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相关代码：
- en: '[PRE168]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Load the dataset with `pandas`:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据集：
- en: '[PRE169]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'Split the dataset into train and test sets, keeping the same parameters as
    in the *Zero-shot inference* recipe with pre-trained models, so that we can compare
    their performance later:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和测试集，保持与*零-shot推理*食谱中预训练模型相同的参数，以便稍后比较它们的表现：
- en: '[PRE170]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'Instantiate the tokenizer and the BERT model. Instantiating the model is a
    multi-step process:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化tokenizer和BERT模型。实例化模型是一个多步骤的过程：
- en: First, instantiate the model’s configuration with the `BertConfig` class.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`BertConfig`类实例化模型的配置。
- en: Then, instantiate `BertModel` with random weights.
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用随机权重实例化`BertModel`。
- en: Load the weights of the pre-trained model (this will display a warning since
    not all the weights will have been loaded).
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练模型的权重（这将显示一个警告，因为并非所有权重都已加载）。
- en: Load the model on the GPU, if any, and set the model to `eval` mode.
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有GPU，将模型加载到GPU上，并将模型设置为`eval`模式。
- en: 'Here is the code for this:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相关代码：
- en: '[PRE173]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: You may get some warning messages since some layers have no pre-trained weights.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会收到一些警告消息，因为某些层没有预训练权重。
- en: 'Compute the embeddings for the train and test sets. This is a two-step process:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集和测试集的嵌入。这是一个两步过程：
- en: Compute the tokens with the tokenizer (and optionally load the tokens on the
    GPU, if any).
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用tokenizer计算令牌（并可以选择将令牌加载到GPU上，如果有的话）。
- en: Then, compute the embeddings.
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，计算嵌入。
- en: For more details about the inputs and outputs of the BERT model, check out the
    *There’s more…* subsection of this recipe.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 有关BERT模型输入输出的更多细节，请查看本食谱中的*更多…*小节。
- en: 'Here is the code for this:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相关代码：
- en: '[PRE174]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'Then, instantiate and train a logistic regression model. It may require more
    iterations than the default model allows. Here, it has been set to `10,000`:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，实例化并训练一个逻辑回归模型。它可能需要比默认模型更多的迭代次数。在这里，已将其设置为`10,000`：
- en: '[PRE175]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Finally, print the accuracy on the train and test sets:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，打印训练集和测试集的准确性：
- en: '[PRE178]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'You should have output results similar to the following:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似于以下的输出结果：
- en: '[PRE184]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: We get a final result of about 79% accuracy on the test set and 80% on the train
    set. As a comparison, using zero-shot inference and a simple RNN on this same
    dataset both provided 74% accuracy.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上得到了大约79%的最终准确率，在训练集上得到了80%。作为对比，使用零-shot推理和简单的RNN在同一数据集上提供的准确率为74%。
- en: There’s more…
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: To get a better understanding of what the tokenizer computes and what the BERT
    model outputs, let’s have a look at an example.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解tokenizer计算的内容以及BERT模型输出的内容，让我们看一个例子。
- en: 'First, let’s apply the tokenizer to a sentence:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将tokenizer应用于一个句子：
- en: '[PRE185]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'This outputs the following:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE186]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'As we can see, the tokenizer returns three outputs:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，tokenizer返回三个输出：
- en: '`input_ids`: This is the index of tokens in the vocabulary.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`：这是词汇表中令牌的索引。'
- en: '`token_type_ids`: The sentence number. This is only useful for paired sentences,
    as in the original training of BERT.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`：句子的编号。这对于配对句子才有用，就像BERT最初训练时一样。'
- en: '`attention_mask`: This is where the model will focus. As we can see, it’s only
    been set to `1` for actual tokens, and then to `0` for padding.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`：这是模型将关注的地方。正如我们所看到的，它仅对实际的 token 设置为 `1`，然后对填充设置为 `0`。'
- en: 'These three lists are fed to the BERT model so that it can compute its output.
    The output is made up of the following two tensors:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这三列数据将传入 BERT 模型，以便它计算其输出。输出由以下两个张量组成：
- en: '`last_hidden_state`: The values of the last hidden state, whose shape is `[batch_size,`
    `max_length, 768]`'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`：最后隐藏状态的值，其形状为 `[batch_size,` `max_length, 768]`'
- en: '`pooler_output`: The pooled values of the outputs over the sequence steps,
    whose shape is `[``batch_size, 768]`'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`：序列步骤输出的池化值，其形状为 `[batch_size, 768]`'
- en: 'Many other types of embeddings exist and can be more or less powerful, depending
    on the task at hand. For example, OpenAI also proposes embeddings that can be
    made available using an API. For example, the following code allows us to have
    embeddings for a given sentence:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他类型的嵌入存在，它们的强度可能因任务的不同而有所不同。例如，OpenAI 还提供了可以通过 API 提供的嵌入。例如，以下代码允许我们为给定的句子获取嵌入：
- en: '[PRE187]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: This would return a 1,536-dimensional embedding for the given sentence that
    could then be used for classification or other tasks.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个 1,536 维的嵌入，可以用于分类或其他任务。
- en: 'Of course, to use these embeddings, you would need to do the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，要使用这些嵌入，你需要做以下几步：
- en: Install the `openai` library with `pip` `install openai`
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `pip` 安装 `openai` 库：`pip install openai`
- en: Create an API key on the OpenAI website
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 OpenAI 网站上创建 API 密钥
- en: Provide a working payment method
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供有效的支付方式
- en: See also
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The paper introducing transformers, *Attention is all you* *need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Transformer 的论文，*Attention is all you* *need*：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: 'The BERT model card: [https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 模型卡：[https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)
- en: 'The BERT paper: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 论文：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: 'For more information about the OpenAI embeddings, here is the official documentation:
    [https://platform.openai.com/docs/guides/embeddings/use-cases](https://platform.openai.com/docs/guides/embeddings/use-cases)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 OpenAI 嵌入的更多信息，请参见官方文档：[https://platform.openai.com/docs/guides/embeddings/use-cases](https://platform.openai.com/docs/guides/embeddings/use-cases)
- en: Data augmentation using GPT-3
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-3 进行数据增强
- en: Generative models are becoming more and more powerful, especially in NLP. Using
    these to generate new, synthetic data sometimes allows us to significantly improve
    the results we get and regularize models. We’ll learn how to do this in this recipe.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型正变得越来越强大，特别是在自然语言处理（NLP）领域。使用这些模型生成新的合成数据，有时可以显著提升我们的结果并对模型进行正则化。在本食谱中，我们将学习如何做到这一点。
- en: Getting ready
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: While models such as BERT are effective at tasks such as text classification,
    they usually do not perform very well when it comes to text generation.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 BERT 这样的模型在文本分类等任务中有效，但在文本生成方面，它们通常表现不佳。
- en: Other types of models, such as **generative pre-trained transformer** (**GPT**)
    models, can be quite impressive at generating new data. In this recipe, we will
    use the OpenAI API and GPT-3.5 to generate synthetic yet realistic data. Having
    more data is key to having more regularization in our models, and data generation
    is one way to collect more data.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的模型，如**生成预训练变换器**（**GPT**）模型，在生成新数据方面可以非常出色。在本食谱中，我们将使用 OpenAI API 和 GPT-3.5
    来生成合成但现实的数据。拥有更多的数据是我们模型进行更多正则化的关键，而数据生成是收集更多数据的一种方式。
- en: For this recipe, you will need to install the OpenAI library with `pip` `install
    openai`.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本食谱，你需要使用 `pip install openai` 安装 OpenAI 库。
- en: Also, since the API we will use is not free, it is necessary to create an OpenAI
    account with a generated API key and a working payment method.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，由于我们将使用的 API 不是免费的，因此需要创建一个 OpenAI 账户，并生成 API 密钥和有效的支付方式。
- en: Creating an API key
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 API 密钥
- en: You can easily create an API key in your profile by accessing the **API** **keys**
    section.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问 **API** **keys** 部分轻松在个人资料中创建一个 API 密钥。
- en: In the *There’s more…* section, we will provide a free alternative – that is,
    using GPT-2 to generate new data – but it will have less realistic results. For
    that to work, you must have Hugging Face’s `transformers` library, which you can
    install with `pip` `install transformers`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在*更多内容...*部分，我们将提供一个免费的替代方法——即使用GPT-2生成新数据——但它的结果会不那么真实。要使其工作，你必须安装Hugging Face的`transformers`库，可以通过`pip
    install transformers`来安装。
- en: How to do it…
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'In this recipe, we will simply query GPT-3.5 to generate a few positive and
    negative movie reviews so that we have more data to train a movie review classification
    model. Of course, this can be derived from any classification task, as well as
    many other NLP tasks:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将简单地查询GPT-3.5生成几个正面和负面的电影评论，这样我们就可以获得更多的数据来训练电影评论分类模型。当然，这也可以从任何分类任务中得到，甚至许多其他NLP任务：
- en: 'Import the `openai` library, as follows:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`openai`库，如下所示：
- en: '[PRE188]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'Provide your OpenAI API key:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供你的OpenAI API密钥：
- en: '[PRE189]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: Note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is just a code example – never share your API key on public repositories.
    Use alternatives such as environment variables instead.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个代码示例——永远不要在公开的代码库中分享你的API密钥。可以使用环境变量等替代方法。
- en: 'Generate three positive examples using the `ChatCompletion` API:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ChatCompletion` API生成三个正面示例：
- en: '[PRE190]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'There are several parameters here:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个参数：
- en: '`model`: `gpt-3.5-turbo`, which performs well and is cost-efficient. It is
    based on GPT-3.5.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`: `gpt-3.5-turbo`，它表现良好且成本高效，基于GPT-3.5。'
- en: '`messages`: There can be three types of messages:'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`messages`: 消息可以有三种类型：'
- en: '`system`: A formatting message, followed by alternating user and assistant
    messages'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`system`: 格式化消息，接下来是交替的用户和助手消息。'
- en: '`user`: A user message'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user`: 用户消息'
- en: '`assistant`: An assistant message; we won’t use this'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assistant`: 助手消息；我们不会使用这个'
- en: '`max_tokens`: The maximum number of tokens in the output.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_tokens`: 输出中令牌的最大数量。'
- en: '`temperature`: This is usually between 0 and 2\. A larger value means more
    randomness.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature`: 通常在0到2之间。值越大，随机性越强。'
- en: '`n`: The number of desired outputs.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`: 所需输出的数量。'
- en: 'Now, we can display the output-generated sentences:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以显示生成的句子：
- en: '[PRE201]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Similarly, let’s generate and display three negative examples of movie reviews:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样地，让我们生成并显示三个负面的电影评论示例：
- en: '[PRE206]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: 'The following code shows the three reviews that were generated:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了生成的三个评论：
- en: '[PRE225]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: The generated examples are well written and probably good enough to be written
    by a human. Also, it would be possible to generate more neutral, more random,
    longer, or shorter examples if needed, which is quite convenient.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的示例写得很好，可能足够自然，像是人类写的。此外，如果需要的话，也可以生成更中立、更随机、更长或更短的示例，这非常方便。
- en: There’s more…
  id: totrans-493
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Alternatively, it is possible to use the GPT-2 model for free, even though the
    results are less realistic. Let’s learn how to do this.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以免费使用GPT-2模型，尽管结果不如真实。让我们来学习如何做到这一点。
- en: 'First, let’s instantiate a text generation pipeline based on GPT-2:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们基于GPT-2实例化一个文本生成管道：
- en: '[PRE226]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: 'This generates some text. The behavior is not the same, and it only handles
    text completion, so you must propose the beginning of a piece of text for the
    model to complete it automatically:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一些文本。行为不完全相同，它仅处理文本补全，因此你必须为模型提供文本的开头，以便它自动完成：
- en: '[PRE227]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: 'This outputs the following three reviews:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下三个评论：
- en: '[PRE228]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: As we can see, the results are less interesting and realistic than with GPT-3,
    but they can still be useful if we apply some manual filtering.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，结果不如GPT-3那么有趣和真实，但如果我们进行一些手动筛选，仍然可以有用。
- en: See also
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Chat completion documentation from OpenAI: [https://platform.openai.com/docs/guides/chat](https://platform.openai.com/docs/guides/chat)'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的聊天完成文档：[https://platform.openai.com/docs/guides/chat](https://platform.openai.com/docs/guides/chat)
- en: 'Text completion documentation from OpenAI: [https://platform.openai.com/docs/guides/completion](https://platform.openai.com/docs/guides/completion)'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的文本补全文档：[https://platform.openai.com/docs/guides/completion](https://platform.openai.com/docs/guides/completion)
- en: 'Text generation documentation from HuggingFace: [https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation)'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace的文本生成文档：[https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation)
- en: 'GPT-2 model card: [https://huggingface.co/gpt2](https://huggingface.co/gpt2)'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2模型卡：[https://huggingface.co/gpt2](https://huggingface.co/gpt2)
