- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: An Overview of Regularization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化概述
- en: Let’s embark on a journey into the world of regularization in machine learning.
    I hope you will learn a lot and find as much joy in reading this book as I did
    in writing it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始进入机器学习正则化的世界。我希望你能学到很多，并像我在写这本书时那样享受阅读的乐趣。
- en: Regularization is important for any individual willing to deploy robust **machine
    learning** (**ML**) models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化对于任何希望部署强健**机器学习**（**ML**）模型的人来说都至关重要。
- en: This chapter will introduce some context and key concepts about regularization
    before diving deeper into it in the next chapters. At this point, you may have
    many questions about this book and about regularization in general. What is regularization?
    Why do we need regularization for production-grade ML models? How do we diagnose
    the need for regularization? What are the limits of regularization? What are the
    approaches to regularization?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将在深入探讨正则化之前，介绍一些背景知识和关键概念。此时，你可能对本书和正则化本身有许多疑问。什么是正则化？为什么我们需要为生产级的机器学习模型进行正则化？如何诊断是否需要正则化？正则化的局限性是什么？正则化的方法有哪些？
- en: All the foundational knowledge about regularization will be provided in this
    chapter in the hope of answering all these questions. Not only will this give
    you a high-level understanding of what regularization is but it will also allow
    you to fully appreciate the methods and techniques proposed in the next chapters
    of this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将提供有关正则化的所有基础知识，旨在回答所有这些问题。这不仅会让你对正则化有一个高层次的理解，而且还会让你充分理解本书接下来几章提出的方法和技巧。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introducing regularization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍正则化
- en: Developing intuition about regularization on a toy dataset
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在玩具数据集上发展正则化的直觉
- en: Introducing the key concepts of underfitting, overfitting, bias, and variance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍欠拟合、过拟合、偏差和方差的关键概念
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will have the opportunity to generate a toy dataset, display
    it, and train basic linear regression on that data. Therefore, the following Python
    libraries will be required:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将有机会生成一个玩具数据集，展示它，并在该数据上训练基本的线性回归。因此，以下Python库是必需的：
- en: NumPy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: scikit-learn
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Introducing regularization
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍正则化
- en: “Regularization in ML is a technique used to improve the generalization performance
    of a model by adding additional constraints to the model’s parameters. This forces
    the model to use simpler representations and helps reduce the risk of overfitting.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “机器学习中的正则化是一种通过为模型的参数添加额外约束来提高模型泛化能力的技术。这样可以迫使模型使用更简单的表示方式，并帮助减少过拟合的风险。
- en: Regularization can also help improve the performance of a model on unseen data
    by encouraging the model to learn more relevant, generalizable features.”
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化还可以通过鼓励模型学习更相关、具有更强泛化能力的特征，帮助提高模型在未见数据上的表现。
- en: 'This definition of regularization, arguably good enough, was actually generated
    by the famous GPT-3 model when given the following prompt: *Detailed definition
    of regularization in machine learning*. Even more astonishing, this definition
    passed several plagiarism tests, meaning it’s actually fully original text. Do
    not worry if you do not yet understand all the words in this definition from GPT-3;
    it is not meant for beginners. But you will fully understand it by the end of
    this chapter.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于正则化的定义，虽然可以说已经足够好，但实际上是由著名的GPT-3模型在给定以下提示时生成的：“机器学习中正则化的详细定义”。更令人惊讶的是，这个定义通过了几次抄袭测试，这意味着它实际上是完全原创的。如果你现在还不理解GPT-3给出的这个定义中的所有词汇，不用担心；它并非面向初学者的定义。但到本章结束时，你将完全理解它。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**GPT-3**, short for **Generative Pre-trained Transformer 3**, is a 175 billion-parameter
    model proposed by OpenAI and is available for use at [platform.openai.com/playground](https://platform.openai.com/playground).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3**，全称**生成式预训练变换器3**，是由OpenAI提出的一个1750亿参数的模型，并且可以通过[platform.openai.com/playground](https://platform.openai.com/playground)使用。'
- en: You can easily imagine that, to get such a result, not only has GPT-3 been trained
    on a large amount of data but it is really carefully regularized, so that it won’t
    just reproduce a learned text but will instead generate a new one.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以很容易地想象，为了得到这样的结果，GPT-3不仅在大量数据上进行了训练，而且经过了精心的正则化处理，这样它就不会只是简单地复述已学的文本，而是会生成新的内容。
- en: 'This is exactly what regularization is about: being able to generalize and
    produce acceptable results when faced with an unknown situation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是正则化的核心：能够在面对未知情况时进行泛化，并产生可接受的结果。
- en: Why is regularization so crucial to ML? The key to successfully deploying ML
    in production lies in the model’s ability to effectively adapt to and accommodate
    new data. Once a model is in production, it will not be fed with well-known, average
    input data. Most likely, the production model will face unseen data, exceptional
    scenarios, a drift in feature distribution, or evolving customer behavior. While
    a well-regularized ML model may not guarantee its robustness in handling various
    scenarios, a poorly regularized model is almost certain to encounter failure upon
    its initial deployment in production.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么正则化对机器学习如此重要？成功将机器学习部署到生产环境的关键在于模型能否有效适应并处理新数据。一旦模型投入生产，它将不会接收到已知的、标准化的输入数据。生产中的模型很可能会面对未见过的数据、异常情景、特征分布的变化或不断变化的客户行为。虽然一个正则化良好的机器学习模型可能无法保证其在处理各种场景时的鲁棒性，但一个正则化不良的模型几乎可以确定会在初次部署时遇到失败。
- en: Let’s now have a look at a few examples of models that failed during deployment
    in recent years, so that we can fully understand why regularization is so important.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看几个近年在部署过程中失败的模型示例，以便我们能充分理解为什么正则化如此重要。
- en: Examples of models that did not pass the deployment test
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有通过部署测试的模型示例
- en: The last few years have been full of examples of models that failed in the first
    days of deployment. According to a Gartner report from 2020 ([https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline](https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline)),
    more than 50% of AI prototypes will *not* make it to production deployment. Not
    all failures were only due to regularization issues, but some certainly were.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年充满了在部署初期就失败的模型示例。根据2020年Gartner报告（[https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline](https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline))，超过50%的人工智能原型将*无法*进入生产部署。并非所有的失败都仅仅是因为正则化问题，但有些确实是。
- en: 'Let’s have a quick look at some failed attempts at deploying models in production
    over the last few years:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下过去几年中一些在生产中部署失败的尝试：
- en: Amazon had to stop using its AI recruitment model because it was reportedly
    discriminating against women ([https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe](https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe))
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊因其人工智能招聘模型涉嫌歧视女性而被迫停止使用该模型（[https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe](https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe))
- en: Microsoft’s chatbot Tay was shut down after only 16 hours of production after
    posting offensive tweets ([https://en.wikipedia.org/wiki/Tay_(chatbot)](https://en.wikipedia.org/wiki/Tay_(chatbot)))
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软的聊天机器人Tay在投入生产仅16小时后因发布冒犯性推文而被关闭（[https://en.wikipedia.org/wiki/Tay_(chatbot)](https://en.wikipedia.org/wiki/Tay_(chatbot)))
- en: IBM’s Watson was providing unsafe cancer treatment recommendations to patients
    ([https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science](https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science))
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM 的 Watson 给患者提供了不安全的癌症治疗建议（[https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science](https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science)）
- en: 'Those are just a few examples that made the headlines from tech giants. The
    number of projects that have experienced failure yet remain undisclosed to the
    public is staggering. These failures often involve smaller companies and confidential
    initiatives. But still, there are several lessons to learn from those examples:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些来自科技巨头头条新闻的例子。经历失败但仍未对公众披露的项目数量令人震惊。这些失败通常涉及较小的公司和保密的计划。但仍然有几个例子可以从中学到教训：
- en: '**Amazon’s case**: The input data was biased against women, as was the model'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊的案例**：输入数据对女性存在偏见，而模型也存在这种偏见'
- en: '**Microsoft’s case**: The model was probably too sensitive to new data since
    it was feeding on new tweets'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微软的案例**：由于它在新推文上的反馈过于敏感'
- en: '**IBM’s case**: The model was perhaps trained on too much synthetic or unrealistic
    data, and not able to adapt to edge cases and unseen data'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IBM 的案例**：该模型可能过度训练于合成或不现实的数据，而不能适应边缘案例和未见过的数据'
- en: Regularization serves as a valuable approach to enhance the success rate of
    ML models in production. Effective regularization techniques can prevent AI recruitment
    models from exhibiting gender biases, either by eliminating certain features or
    incorporating synthetic data. Additionally, proper regularization enables chatbots
    to maintain an appropriate level of sensitivity toward new tweets. It also equips
    models to handle edge cases and previously unseen data proficiently, even when
    trained on synthetic data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化作为增强 ML 模型在生产中成功率的宝贵方法。有效的正则化技术可以通过消除某些特征或整合合成数据来防止 AI 招聘模型展现性别偏见。此外，适当的正则化使得聊天机器人能够保持对新推文的适当敏感性。它还使模型能够在训练于合成数据的情况下，熟练处理边缘案例和以前未见过的数据。
- en: As a disclaimer, there may be many other ways to overcome or prevent these kinds
    of failures that are not mutually exclusive with regularization. For example,
    having good-quality data is key. Everyone in the AI field knows the adage *garbage
    in,* *garbage out*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明，可能还有许多其他克服或预防这些失败的方法，并且这些方法与正则化并不是互斥的。例如，拥有高质量的数据至关重要。AI 领域的每个人都知道这句格言
    *garbage in, garbage out*。
- en: MLOps (a field that is getting more and more mature every day) and ML engineering
    best practices are also key to success for many projects. Subject matter knowledge
    can sometimes also make a difference.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps（一个日益成熟的领域）和 ML 工程的最佳实践也是许多项目成功的关键。主题知识有时也可能产生影响。
- en: Depending on the context of the project, many other parameters may impact the
    success of an ML project, but anything besides regularization is beyond the scope
    of this book.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据项目的上下文，许多其他参数可能会影响 ML 项目的成功，但是除了正则化之外的任何内容都超出了本书的范围。
- en: Now that we understand the need for regularization for production-level ML models,
    let’s take a step back and gain some intuition about what regularization is with
    a simple example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了在生产级 ML 模型中为什么需要正则化，让我们退一步，通过一个简单的例子对正则化有些直觉。
- en: Intuition about regularization
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对正则化的直觉
- en: Regularization has been defined and mentioned already in this book, but let’s
    try now to develop some intuition about what it really is.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化已经在本书中定义和提到过，但现在让我们尝试发展一些关于它究竟是什么的直觉。
- en: 'Let us consider a typical, real-world use case: the real estate price per square
    meter, as a function of the surface of an apartment (or house) in the city of
    Paris. The goal, from a business perspective, is to be able to predict the price
    per square meter, given the apartment’s surface.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个典型的现实世界用例：巴黎市一个公寓（或房屋）的平方米房价作为其表面积的函数。从业务角度来看，目标是能够预测每平方米的价格，给定公寓的表面积。
- en: We will first need some imports, as well as a helper function to plot the data
    more conveniently.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一些导入，以及一个更方便地绘制数据的辅助函数。
- en: 'The `plot_data()` function simply plots the provided data and adds axis labels
    and a legend if needed:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_data()` 函数简单地绘制提供的数据，并在需要时添加轴标签和图例：'
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following code will now allow us to generate and display our first toy
    dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码现在将允许我们生成和显示我们的第一个玩具数据集：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the plot for it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图表：
- en: '![Figure 1.1 – Price per square meter as a function of the apartment surface](img/B19629_01_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 每平方米价格随公寓面积变化的图](img/B19629_01_01.jpg)'
- en: Figure 1.1 – Price per square meter as a function of the apartment surface
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 每平方米价格随公寓面积变化的图
- en: Even if this is a toy dataset, for the sake of pedagogy, we can assume this
    data would have been collected on the real estate market.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 即便这只是一个示例数据集，为了教学目的，我们可以假设这些数据是从房地产市场中收集的。
- en: We can notice a downward trend in the price per square meter as the apartment
    surface increases. Indeed, in Paris, the small surfaces are much more in demand
    (perhaps because there are many students or because the price is more affordable).
    That could explain why the price per square meter is actually higher for smaller
    surfaces.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着公寓面积的增加，每平方米价格呈下降趋势。事实上，在巴黎，小面积的公寓需求更大（可能是因为有很多学生，或者因为价格更实惠）。这或许能解释为什么小面积公寓的每平方米价格实际上更高。
- en: 'For simplicity, we will omit all the typical ML workflow. Here, we just perform
    a linear regression on this data and display the result with the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们将省略所有典型的机器学习工作流程。在这里，我们仅对这些数据执行线性回归，并使用以下代码显示结果：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 1.2 – Price per square meter as a function of the apartment surface
    and resulting fit curve](img/B19629_01_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 每平方米价格随公寓面积变化及拟合曲线](img/B19629_01_02.jpg)'
- en: Figure 1.2 – Price per square meter as a function of the apartment surface and
    resulting fit curve
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 每平方米价格随公寓面积变化及拟合曲线
- en: Good enough! The fit seems to capture the downward trend. While it’s not so
    close to all the given data samples, the business seems happy about it for the
    moment, as the model performances are within their expectations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 足够好了！拟合似乎捕捉到了下降趋势。虽然与所有给定的数据样本不完全吻合，但目前商业方面对其表示满意，因为模型的表现符合预期。
- en: 'Thanks to this new model, the company has now acquired a few more clients.
    From those clients, the company collected some new data from larger apartment
    sales, so our dataset now looks like the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 借助这个新模型，公司现在获得了更多客户。从这些客户那里，公司收集了一些新的数据，涉及较大公寓的销售，因此我们的数据集现在看起来如下：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the plot for it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图表：
- en: '![Figure 1.3 – Updated price per square meter as a function of the apartment
    surface](img/B19629_01_03.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 每平方米价格随公寓面积变化的更新图](img/B19629_01_03.jpg)'
- en: Figure 1.3 – Updated price per square meter as a function of the apartment surface
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 每平方米价格随公寓面积变化的更新图
- en: 'This actually changes everything; this is the typical failing deployment test.
    There is no global downward trend anymore: with larger apartment surfaces, the
    price per square meter actually seems to follow an upward trend. One simple business
    explanation could be the following: larger apartments may be less common and thus
    have a higher price.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上改变了所有情况；这是典型的失败部署测试。现在已经没有全局的下降趋势了：随着公寓面积的增大，每平方米价格似乎跟随上升趋势。一个简单的商业解释可能是：大面积的公寓可能较为稀缺，因此价格更高。
- en: 'Without confidence, for the sake of trying, we try to reuse the exact same
    method as we did previously: linear regression. The result would be the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 没有信心的情况下，为了尝试，我们重新使用之前相同的方法：线性回归。结果将如下：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the plot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图表：
- en: '![Figure 1.4 – Example of underfitting: the data complexity is not fully captured
    by the model](img/B19629_01_04.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 欠拟合示例：模型未能完全捕捉数据的复杂性](img/B19629_01_04.jpg)'
- en: 'Figure 1.4 – Example of underfitting: the data complexity is not fully captured
    by the model'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 欠拟合示例：模型未能完全捕捉数据的复杂性
- en: As expected, linear regression was not able to capture the complexity of the
    data anymore, leading to this situation. This is called **underfitting**; the
    model is not able to fully capture the complexity of the data. Indeed, with just
    the surface as a parameter, the best the model can do is a straight line, which
    is not enough for this data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，线性回归已无法捕捉数据的复杂性，导致了这种情况。这被称为**欠拟合**；模型未能完全捕捉数据的复杂性。事实上，仅以面积作为参数，模型能做到的最好就是一条直线，这对于这些数据来说是不够的。
- en: 'One way for linear regression to capture more complexity is to provide more
    features. Given that our current input data is limited to the surface, a potential
    straightforward approach could involve utilizing the raised-to-the-power surface.
    For the sake of this example, let’s take a rather extreme approach and add all
    the features from `power1` to `power15`, and make them fit this dataset. This
    can be done pretty easily with the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归捕捉更多复杂性的一个方法是提供更多特征。鉴于我们当前的输入数据仅限于表面，可能的直接方法是利用指数表面。为了这个示例，我们采取了一个相对极端的方法，添加了从`power1`到`power15`的所有特征，并使它们拟合该数据集。这可以通过以下代码轻松实现：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output for it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的输出：
- en: '![Figure 1.5 – Example of overfitting: the model is capturing the noise in
    the data](img/B19629_01_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 过拟合示例：模型捕捉到数据中的噪声](img/B19629_01_05.jpg)'
- en: 'Figure 1.5 – Example of overfitting: the model is capturing the noise in the
    data'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 过拟合示例：模型捕捉到数据中的噪声
- en: 'As we could expect by adding so many degrees of freedom in our model, the fit
    is now going exactly through all of the data points. Indeed, without going into
    the mathematical details, the model has more parameters than data points on which
    it trains, and is capable of going through all those points. Is it a good fit,
    though? We can imagine it does not only capture the global trend of the data but
    also the noise. This is called **overfitting**: the model is too close to the
    data, and may not be able to make correct predictions for new, unseen data. Any
    new data point would not be on the curve, and the behavior outside the training
    range is totally unpredictable.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的那样，随着在模型中添加了如此多的自由度，拟合现在正好通过所有数据点。事实上，不深入探讨数学细节，模型的参数比它训练的数据点还多，因此能够通过所有这些点。但是，这真的是一个好的拟合吗？我们可以想象，它不仅捕捉到了数据的全局趋势，还捕捉到了噪声。这被称为**过拟合**：模型过于贴近数据，可能无法对新的、未见过的数据做出正确的预测。任何新的数据点都不会在曲线上，且训练范围之外的行为完全不可预测。
- en: 'Finally, a more reasonable approach to this situation would be only taking
    the surface up to the power of 2, for example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，对于这种情况，一个更合理的方法是只取表面到2次方，例如：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the output for it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的输出：
- en: '![Figure 1.6 – Example of right fitting: the model is capturing the overall
    trend but not the noise](img/B19629_01_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 正确拟合示例：模型捕捉到整体趋势但没有捕捉到噪声](img/B19629_01_06.jpg)'
- en: 'Figure 1.6 – Example of right fitting: the model is capturing the overall trend
    but not the noise'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 正确拟合示例：模型捕捉到整体趋势，但没有捕捉到噪声
- en: 'The fit seems much more acceptable now. It does capture the global trend of
    the data: at first downward for small surfaces, and then upward for larger surfaces.
    Moreover, it does not try to capture the noise coming from all the data points,
    making it more robust for predicting new, unseen data. Finally, the behavior is
    quite predictable beyond the training range (for example, surface < 15![](img/Formula_01_001.png)
    or surface > 40![](img/Formula_01_002.png)).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拟合看起来要更可接受了。它确实捕捉到了数据的全局趋势：最初对小表面呈下降趋势，然后对较大表面呈上升趋势。此外，它不再试图捕捉所有数据点中的噪声，使得它在预测新的、未见过的数据时更具鲁棒性。最终，超出训练范围的行为（例如，表面
    < 15![](img/Formula_01_001.png) 或表面 > 40![](img/Formula_01_002.png)）相当可预测。
- en: 'This is typically the kind of desired behavior from a good model: neither underfitting
    nor overfitting. Removing some of the raised-to-the-power features allowed our
    model to generalize better; we effectively regularized our model.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是一个好的模型所期望的行为：既不欠拟合也不过拟合。通过移除一些指数特征，我们的模型能够更好地泛化；我们实际上对模型进行了正则化。
- en: 'To summarize this example, we explored several concepts here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这个示例，我们在这里探讨了几个概念：
- en: We visualized examples of underfitting, overfitting, and well-regularized models
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们展示了欠拟合、过拟合和良好正则化模型的示例
- en: By adding a raised-to-the-power surface to our features, we were able to go
    from underfitting to overfitting
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过向特征中添加指数表面，我们成功地将模型从欠拟合过渡到过拟合
- en: Finally, by removing most of the raised-to-the-power features (keeping only
    square features), we were able to go from overfitting to a well-regularized model,
    effectively adding regularization
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，通过移除大部分指数特征（仅保留平方特征），我们成功将模型从过拟合调整为良好正则化的模型，有效地加入了正则化。
- en: Hopefully, you now have a good understanding of underfitting, overfitting, and
    regularization, as well as why this is so important in ML. We can now build upon
    this by providing a more formal definition of the key concepts of regularization.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在对欠拟合、过拟合和正则化有了很好的理解，也理解了它们在机器学习中的重要性。现在我们可以基于此进一步构建，给出正则化关键概念的更正式定义。
- en: Key concepts of regularization
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化的关键概念
- en: Having gained some intuition regarding what constitutes a suitable fit, as well
    as understanding examples of underfitting and overfitting, let us now delve into
    a more precise definition and explore key concepts that enable us to better comprehend
    regularization.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得了一些关于何为合适拟合的直觉，并理解了欠拟合和过拟合的例子后，让我们更精确地定义这些概念，并探讨正则化的关键概念，帮助我们更好地理解正则化。
- en: Bias and variance
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差和方差
- en: 'Bias and variance are two key concepts when talking about regularization. We
    can define two main kinds of errors a model can have:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差是讨论正则化时的两个关键概念。我们可以定义模型可能产生的两种主要错误：
- en: '**Bias** is how bad a model is at capturing the general behavior of the data'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**是模型在捕捉数据的整体行为方面的不足'
- en: '**Variance** is how bad a model is at being robust to small input data fluctuations'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**是模型对小的输入数据波动的鲁棒性差的表现'
- en: 'Those two concepts, in general, are not mutually exclusive. If we take a step
    back from ML, there is a very common figure to visualize bias and variance, assuming
    the model’s goal is to hit the center of a target:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个概念通常不是互相排斥的。如果我们从机器学习退后一步，存在一个非常常见的图示来可视化偏差和方差，假设模型的目标是击中目标的中心：
- en: '![Figure 1.7 – Visualization of bias and variance](img/B19629_01_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 偏差和方差的可视化](img/B19629_01_07.jpg)'
- en: Figure 1.7 – Visualization of bias and variance
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 偏差和方差的可视化
- en: 'Let’s describe those four cases:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述这四种情况：
- en: '**High bias and low variance**: The model is hitting away from the center of
    the target, but in a very consistent manner'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高偏差和低方差**：模型偏离目标中心，但方式非常一致'
- en: '**Low bias and high variance**: The model is, on average, hitting the center
    of the target, but is quite noisy and inconsistent in doing so'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低偏差和高方差**：模型平均上击中目标的中心，但在此过程中非常嘈杂且不一致'
- en: '**High bias and high variance**: The model is hitting away from the center
    in a noisy way'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高偏差和高方差**：模型以一种嘈杂的方式偏离中心'
- en: '**Low bias and low variance**: The best of both worlds – the model is hitting
    the center of the target consistently'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低偏差和低方差**：两者兼得——模型稳定地击中目标中心'
- en: Underfitting and overfitting
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: We saw a very classic approach to bias and variance definition.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一个非常经典的偏差和方差定义方法。
- en: 'But now, what does that mean in terms of ML? How does that relate to regularization?
    Well, before we get there, we will first revisit bias and variance in a more typical
    ML case: linear regression of real estate prices.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在，这在机器学习中意味着什么呢？它与正则化有什么关系？在我们深入探讨之前，我们首先将回顾偏差和方差在一个更典型的机器学习案例中的表现：房地产价格的线性回归。
- en: Let’s have a look at how a model would behave in all those cases on our data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型在这些情况下如何在我们的数据上表现。
- en: High bias and low variance
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高偏差和低方差
- en: 'The model is robust to data fluctuations but could not capture the high-level
    behavior of the data. Refer to the following graph:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对数据波动具有鲁棒性，但无法捕捉到数据的高层次行为。请参考下图：
- en: '![Figure 1.8 – High bias and low variance in practice for linear regression](img/B19629_01_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 线性回归中的高偏差和低方差实践](img/B19629_01_08.jpg)'
- en: Figure 1.8 – High bias and low variance in practice for linear regression
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 线性回归中的高偏差和低方差实践
- en: This is *underfitting*, as we faced earlier, in *Figure 1**.4*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*欠拟合*，正如我们之前在*图 1.4*中所遇到的那样。
- en: Low bias and high variance
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低偏差和高方差
- en: 'The model did capture the global behavior of the data, but could not stay robust
    to input data fluctuations. Refer to the following graph:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 模型确实捕捉到了数据的全局行为，但无法保持对输入数据波动的鲁棒性。请参考下图：
- en: '![Figure 1.9 – Low bias and high variance in practice for linear regression](img/B19629_01_09.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 线性回归中的低偏差和高方差实践](img/B19629_01_09.jpg)'
- en: Figure 1.9 – Low bias and high variance in practice for linear regression
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 线性回归中的低偏差和高方差实践
- en: This is *overfitting*, the case we faced previously, in *Figure 1**.5*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*过拟合*，正如我们之前在*图 1.5*中遇到的情况。
- en: High bias and high variance
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高偏差和高方差
- en: 'The model could neither capture the global behavior nor be robust enough to
    input data fluctuations. This case never happens, or high variance is hidden behind
    high bias, but it could look something like the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 模型既无法捕捉全局行为，也不足够稳健，无法应对数据波动。这种情况通常不会发生，或者说高方差被隐藏在高偏差背后，但可能看起来像以下这样：
- en: '![Figure 1.10 – High bias and high variance in practice for linear regression:
    this most likely never actually happens on such data](img/B19629_01_10.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10 – 线性回归中的高偏差和高方差：这种情况在这样的数据中几乎不会发生](img/B19629_01_10.jpg)'
- en: 'Figure 1.10 – High bias and high variance in practice for linear regression:
    this most likely never actually happens on such data'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 线性回归中的高偏差和高方差：这种情况在这样的数据中几乎不会发生
- en: Low bias and low variance
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低偏差和低方差
- en: The model could both capture the global data behavior and be robust enough for
    data fluctuation. This is the end goal when training a model. This is the case
    we faced in *Figure 1**.6*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能够既捕捉全局数据行为，又足够稳健以应对数据波动。这是训练模型时的最终目标。这正是我们在*图 1.6*中遇到的情况。
- en: '![Figure 1.11 – Low bias and low variance in practice for linear regression:
    the ultimate goal](img/B19629_01_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.11 – 线性回归中的低偏差和低方差：最终目标](img/B19629_01_11.jpg)'
- en: 'Figure 1.11 – Low bias and low variance in practice for linear regression:
    the ultimate goal'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – 线性回归中的低偏差和低方差：最终目标
- en: Of course, the goal is almost always to get both low bias and low variance,
    even if it’s not always possible. Let’s see how regularization is a means toward
    this goal.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，目标几乎总是希望得到低偏差和低方差，即便这并不总是可能的。让我们看看正则化如何作为实现这个目标的手段。
- en: Regularization – from overfitting to underfitting
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化——从过拟合到欠拟合
- en: In light of all those examples, we can now get a really clear understanding
    of what regularization is.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上这些例子，我们现在可以清楚地理解正则化的含义。
- en: If we look again at the definition provided by GPT-3, regularization is what
    allows us to prevent a model from overfitting by adding constraints to the model.
    Indeed, *adding regularization allows us to reduce variance* in a model, and therefore,
    to have a less overfitting model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次回顾GPT-3给出的定义，正则化通过向模型添加约束来防止模型过拟合。实际上，*加入正则化使我们能够降低模型的方差*，从而使模型的过拟合程度减轻。
- en: 'We can go one step further: what if regularization is added to an already well-trained
    model (that is, low bias and low variance)? In other words, what happens if constraints
    are added to a model that works well? Intuitively, it would degrade the overall
    performance. It would not allow the model to fully grasp the data behavior, and
    thus add bias to the model.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步推测：如果正则化被加入到一个已经训练良好的模型中（即低偏差和低方差的模型），会怎样呢？换句话说，如果在一个表现良好的模型中添加约束，会发生什么呢？直觉上，这会降低模型的整体表现。它不会让模型完全理解数据的行为，因此会增加模型的偏差。
- en: 'Indeed, here comes a fundamental drawback of regularization: **adding regularization
    increases** **model bias**.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这里正是正则化的一个根本缺点：**加入正则化会增加** **模型偏差**。
- en: 'This can be summarized in one figure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用一张图来总结：
- en: '![Figure 1.12 – A high variance model (bottom); the same model with more regularization
    and the right level of fitting (middle); and the same model with even more regularization,
    now underfitting (top)](img/B19629_01_12.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – 一个高方差模型（底部）；相同的模型加入更多正则化并达到适当拟合水平（中间）；同样的模型加入更多正则化，现在发生了欠拟合（顶部）](img/B19629_01_12.jpg)'
- en: Figure 1.12 – A high variance model (bottom); the same model with more regularization
    and the right level of fitting (middle); and the same model with even more regularization,
    now underfitting (top)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – 一个高方差模型（底部）；相同的模型加入更多正则化并达到适当拟合水平（中间）；同样的模型加入更多正则化，现在发生了欠拟合（顶部）
- en: 'This is what is called the **bias-variance trade-off**, a very important concept.
    Indeed, adding regularization is always a balance:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的**偏差-方差权衡**，一个非常重要的概念。事实上，加入正则化总是一个平衡：
- en: We need to have enough regularization so that the model generalizes well and
    is not sensitive to small data fluctuations and noise
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要有足够的正则化，使模型具有良好的泛化能力，并且不容易受小幅度数据波动和噪声的影响
- en: We need to not have too much regularization so that the model is free enough
    to fully capture the complexity of the data in all cases
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要避免过多的正则化，以确保模型在任何情况下都能足够自由地完全捕捉数据的复杂性
- en: As we go further in the book, more and more tools and techniques will be provided
    to diagnose our model and find the right bias-variance balance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在书中的深入，更多的工具和技术将被提供来诊断我们的模型并找到合适的偏差-方差平衡。
- en: 'Throughout the chapters, we will see many ways to regularize a model. We think
    of regularization as just adding direct constraints to a model, but there are
    many indirect regularization methods that may help a model better generalize.
    A non-exhaustive list of the existing regularization methods could be the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到许多正则化模型的方法。我们通常认为正则化就是直接向模型添加约束，但实际上有许多间接的正则化方法可以帮助模型更好地泛化。现有正则化方法的非详尽清单可能包括以下内容：
- en: Adding constraints to the model architecture
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型架构添加约束
- en: Adding constraints to the model training, such as the loss
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向模型训练中添加约束，如损失函数
- en: Adding constraints from the input data by engineering it differently
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过不同的工程方法对输入数据添加约束
- en: Adding constraints from the input by generating more samples
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过生成更多样本来增加输入的约束
- en: Other regularization methods may be proposed, but the book will mostly focus
    on those methods for various cases, such as structured and unstructured data,
    linear models, tree-based models, deep learning models, **natural language processing**
    (**NLP**) problems, and computer vision problems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会提出其他正则化方法，但本书将主要关注适用于各种情况的方法，比如结构化和非结构化数据、线性模型、基于树的模型、深度学习模型、**自然语言处理**（**NLP**）问题和计算机视觉问题。
- en: 'As good as a model can be with the right regularization method, most tasks
    have a hard limit on the possible performances a model can achieve: this is what
    we call **unavoidable bias**. Let’s have a look at what it is.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型采用了正确的正则化方法，许多任务对模型可以实现的表现仍有一个硬性上限：这就是我们所说的**不可避免的偏差**。让我们来看看它是什么。
- en: Unavoidable bias
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不可避免的偏差
- en: In almost any task, there is unavoidable bias. For example, in *Figure 1**.13*,
    there are both Shetland Sheepdogs and Rough Collie dogs. Can you say with 100%
    accuracy which is which?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有任务中，都有不可避免的偏差。例如，在*图 1.13*中，既有设得兰牧羊犬，也有粗毛牧羊犬。你能以100%的准确度说出哪一只是哪一只吗？
- en: '![Figure 1.13 – Random pictures of both Shetland Sheepdogs and Rough Collie
    dogs](img/B19629_01_13.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.13 – 设得兰牧羊犬和粗毛牧羊犬的随机图片](img/B19629_01_13.jpg)'
- en: Figure 1.13 – Random pictures of both Shetland Sheepdogs and Rough Collie dogs
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – 设得兰牧羊犬和粗毛牧羊犬的随机图片
- en: From the preceding figure, can you tell which is which? Most likely not. If
    you are a trained expert in dogs, you may have a lower error rate. But the odds
    are that given a large enough number of images, you might be wrong with some images.
    The lowest level of error an expert can achieve is what is called **human-level
    error**. In most cases, human-level error gives a very good idea of the lowest
    error a model can achieve. Anytime you are evaluating a model, it is a good idea
    to know (or to wonder) what a human could possibly do on such a task.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，你能分辨出哪一只是哪一只吗？大概率是不能的。如果你是一个经过训练的犬类专家，你可能会有更低的错误率。但即便如此，给定足够数量的图片，你也可能会在某些图片上出错。专家能够达到的最低错误率被称为**人类水平错误**。在大多数情况下，人类水平错误能很好地反映出模型可能达到的最低错误。每当你在评估一个模型时，了解（或思考）人类在这种任务中的表现是一个很好的思路。
- en: 'Indeed, some tasks are much easier than others:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有些任务比其他任务容易得多：
- en: Humans perform well at classifying dogs and cats, as does AI
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类在分类狗和猫方面表现得很好，AI也是如此
- en: Humans perform well at classifying songs, as does AI
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类在分类歌曲方面表现得很好，AI也是如此
- en: Humans perform quite poorly at hiring people (at least, not all recruiters will
    agree on a candidate), as does AI
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类在招聘人员方面表现得相当差（至少，并不是所有招聘人员都会就候选人达成一致），AI也是如此
- en: Another possible source to compute the unavoidable bias is the Bayes error.
    Most commonly impossible to compute on complex AI tasks, the Bayes error is the
    lowest possible error rate a classifier can achieve. Most of the time, the Bayes
    error is lower than the human-level error, but much harder to estimate. This would
    be the actual theoretical limitation of any model performance.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算不可避免的偏差的另一个可能来源是贝叶斯误差。贝叶斯误差通常在复杂的AI任务中无法计算，它是分类器可以达到的最低错误率。大多数时候，贝叶斯误差低于人类水平误差，但估算起来要困难得多。这将是任何模型性能的实际理论限制。
- en: The Bayes error and the human-level error are unavoidable biases. They indicate
    the irreducible error of any model and are a key concept when evaluating whether
    a model needs more or less regularization.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯误差和人类级别的误差是不可避免的偏差。它们表示任何模型的不可减少误差，是评估模型是否需要更多或更少正则化的关键概念。
- en: Diagnosing bias and variance
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差与方差的诊断
- en: We usually define bias and variance using figures with a more or less accurate
    fit on some data, as we did earlier on the apartment price data. Although useful
    to explain the concepts, in real life, we mostly have highly dimensional datasets.
    By using a simple Titanic dataset, we provided a dozen features, so making this
    kind of visual inspection is simply impossible.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用一些数据上的拟合图来定义偏差和方差，就像我们之前在公寓价格数据中做的那样。虽然这些图有助于解释概念，但在现实生活中，我们通常处理的是高维数据集。通过使用简单的
    Titanic 数据集，我们提供了十几个特征，因此进行这种视觉检查几乎是不可能的。
- en: Let’s assume we are training a dogs and cats classification model with balanced
    data. A good method is to compare the evaluation metric (whether it be accuracy,
    F-score, or whatever you deemed relevant) for both the training set and validation
    set.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在训练一个猫狗分类模型，并且数据是平衡的。一种好的方法是比较训练集和验证集上的评估指标（无论是准确率、F 分数，或是任何你认为相关的指标）。
- en: Note
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the concepts of evaluation metrics or training and validation sets are not
    clear, they will be explained in more detail in [*Chapter 2*](B19629_02.xhtml#_idTextAnchor034).
    Put simply, the model is trained on the training set. The metric is the value
    used to evaluate the trained model and is computed on both the training and validation
    sets.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果评估指标或训练集与验证集的概念不清楚，它们将在[*第 2 章*](B19629_02.xhtml#_idTextAnchor034)中有更详细的解释。简而言之，模型是在训练集上进行训练的。评估指标是用来评估训练好的模型的值，并且是在训练集和验证集上计算的。
- en: 'For example, let’s assume we get the following results:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们得到以下结果：
- en: '![Figure 1.14 – Hypothetical accuracy on training and validation sets](img/B19629_01_14.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.14 – 假设在训练集和验证集上的准确性](img/B19629_01_14.jpg)'
- en: Figure 1.14 – Hypothetical accuracy on training and validation sets
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 – 假设在训练集和验证集上的准确性
- en: If we think about the expected human-level error for such a task, we expect
    a much higher accuracy. Indeed, most humans can recognize a dog from a cat with
    very high confidence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑到这类任务的预期人类级别错误，我们会预期更高的准确性。事实上，大多数人能够非常自信地从猫和狗中辨认出狗。
- en: 'So, in this case, the performances of training and validation sets are far
    below the human-level error rate. This is typical of a **high-bias** scenario:
    the evaluation metric is bad on both training and validation sets. In such cases,
    the model needs to be less regularized.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练集和验证集的表现远低于人类级别的错误率。这是典型的**高偏差**情形：评估指标在训练集和验证集上都很差。在这种情况下，模型需要更少的正则化。
- en: 'Let’s now assume that after adding lower regularization (perhaps adding raised-to-the-power
    features as we did in the *Intuition about regularization* section), we have the
    following results:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设在添加了较低的正则化（可能像我们在*正则化直觉*一节中所做的那样，添加了指数特征）之后，我们得到了以下结果：
- en: '![Figure 1.15 – Hypothetical accuracy after adding more features](img/B19629_01_15.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.15 – 添加更多特征后的假设准确性](img/B19629_01_15.jpg)'
- en: Figure 1.15 – Hypothetical accuracy after adding more features
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15 – 添加更多特征后的假设准确性
- en: 'Those results are better; the validation set now has an accuracy of 89%. Nevertheless,
    there are two issues here:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果更好；验证集现在的准确率为 89%。然而，这里有两个问题：
- en: 'The score on the train set is way too good: it is literally perfect'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集上的得分好得过头了：它简直是完美的
- en: The score on the validation is still far below the human-level error rate, which
    we would expect to be at least 95%
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集上的得分仍然远低于我们预期的至少 95% 的人类错误率
- en: 'This is typical of a **high variance** scenario: results are really good (usually
    too good) on the training set, and far below on the validation set. In such cases,
    the model needs more regularization.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是典型的**高方差**情形：在训练集上的结果非常好（通常过好），而在验证集上则远低于预期。在这种情况下，模型需要更多的正则化。
- en: 'After adding regularization, let’s assume we now have the following results:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 添加正则化后，假设我们现在得到了以下结果：
- en: '![Figure 1.16 – Hypothetical accuracy after adding regularization](img/B19629_01_16.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.16 – 添加正则化后的假设准确性](img/B19629_01_16.jpg)'
- en: Figure 1.16 – Hypothetical accuracy after adding regularization
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16 – 添加正则化后的假设准确性
- en: 'This seems much better: both training and validation sets have an accuracy
    that seems close to human-level performance. Perhaps with more data, a better
    model, or some other improvements, results could get a little better, but overall,
    this seems like a solid result.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来好多了：训练集和验证集的准确度似乎接近人类水平的表现。也许通过更多的数据、一个更好的模型或其他改进，结果可以稍微提升一些，但总体来看，这似乎是一个稳固的结果。
- en: 'In most cases, diagnosing high bias and high variance is simple, and the method
    is always the same:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，诊断高偏差和高方差是很简单的，方法始终是相同的：
- en: Evaluate your model on both the training and validation set.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集和验证集上评估你的模型。
- en: Compare the results with each other, as well as with the human-level error rate.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果相互比较，并与人类水平的误差率进行对比。
- en: 'From that point, there are mostly three cases:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点来看，通常有三种情况：
- en: '**High bias/underfitting**: Both training and validation sets exhibit poor
    performance'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高偏差/欠拟合**：训练集和验证集的表现都很差'
- en: '**High variance/overfitting**: The training set performance is far better than
    the validation set performance; validation set performance is well below the human-level
    error rate'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高方差/过拟合**：训练集表现远好于验证集表现；验证集表现远低于人类水平的误差率'
- en: '**Good fit**: Both training and validation sets exhibit performance close to
    the human-level error rate'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**良好的拟合**：训练集和验证集的表现接近人类水平的误差率'
- en: Note
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Most of the time, it is proposed to use this technique with training and test
    sets, instead of training and validation sets. While the reasoning holds true,
    doing such optimization on the test set directly may lead to overfitting, and
    thus overestimate the actual performance of a model. In this book, though, we
    will use the test set for simplification in the next chapters.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，建议在训练集和测试集上使用该技术，而不是训练集和验证集。尽管这一推理是成立的，但直接在测试集上进行优化可能会导致过拟合，从而高估模型的实际表现。然而，在本书中，我们将简化处理，接下来的章节将使用测试集。
- en: Having all the key concepts of regularization in mind, you might now start to
    understand why regularization might indeed require a whole book. While diagnosing
    a need for regularization is usually fairly easy, choosing the right regularization
    method can be really challenging. Let’s now categorize the regularization approaches
    that will be covered in this book.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 记住所有正则化的关键概念后，你现在可能开始理解为什么正则化确实可能需要一本书来讲解。虽然诊断出正则化的需求通常相对简单，但选择合适的正则化方法却可能非常具有挑战性。接下来，让我们对本书中将要讨论的正则化方法进行分类。
- en: Regularization – a multi-dimensional problem
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化——一个多维度的问题
- en: 'Having the right diagnosis for a model is crucial, as it allows us to choose
    the strategy more carefully to improve the model. But from any diagnosis, many
    paths are possible to improve the model. Those paths can be separated into three
    main categories, as proposed in the following figure:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行正确的诊断至关重要，因为它使我们能够更仔细地选择策略来改进模型。但是，从任何诊断出发，都有许多途径可以改进模型。这些途径可以分为三大类，正如下图所示：
- en: '![Figure 1.17 – A proposed categorization of regularization types: data, model
    architecture, and model training](img/B19629_01_17.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.17 – 提出的正则化类型分类：数据、模型架构和模型训练](img/B19629_01_17.jpg)'
- en: 'Figure 1.17 – A proposed categorization of regularization types: data, model
    architecture, and model training'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17 – 提出的正则化类型分类：数据、模型架构和模型训练
- en: 'At the data level, we may have the following tools for regularization:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据层面，我们可能会使用以下工具进行正则化：
- en: Adding more data, either synthetic or real
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多数据，无论是合成数据还是真实数据
- en: Adding more features
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多特征
- en: Feature engineering
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Data preprocessing
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Indeed, the data is of extreme importance in ML in general, and regularization
    is no exception. We will see many examples throughout the book of regularizing
    data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，数据在机器学习中极为重要，正则化也不例外。在本书中，我们将看到许多正则化数据的例子。
- en: 'At the model level, the following methods may be used for regularization:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型层面，可以使用以下方法进行正则化：
- en: Choosing a more or less simple architecture
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择更简单或更复杂的架构
- en: In deep learning, many architectural designs allow regularization (for example,
    dropout)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习中，许多架构设计允许正则化（例如，Dropout）
- en: 'The model complexity may strongly impact regularization. An overly complicated
    architecture can easily lead to overfitting, while a too simplistic one may underfit,
    as depicted in the following figure:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂度可能对正则化产生很大影响。过于复杂的架构很容易导致过拟合，而过于简单的架构则可能导致欠拟合，正如下图所示：
- en: '![Figure 1.18 – Possible visualization of error as a function of model complexity,
    for both training and validation sets](img/B19629_01_18.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图1.18 – 模型复杂度与训练集和验证集误差之间关系的可视化示意图](img/B19629_01_18.jpg)'
- en: Figure 1.18 – Possible visualization of error as a function of model complexity,
    for both training and validation sets
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18 – 模型复杂度与训练集和验证集误差之间关系的可视化示意图
- en: 'Finally, at the training level, some of the methods to regularize are as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在训练层面，一些正则化的方法如下：
- en: Adding penalization
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加惩罚项
- en: Weight initialization
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Transfer learning
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移学习
- en: Early stopping
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止
- en: Early stopping is a very common way to avoid overfitting by preventing the model
    from getting too close to the training set.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止是一种非常常见的防止过拟合的方法，通过防止模型过度贴合训练集来避免过拟合。
- en: 'There may be many ways to regularize, as it is a multi-dimensional problem:
    data, model architecture, and model training are just high-level categories. Even
    though those categories are just some examples and more may exist or be defined,
    most – if not all – of the techniques this book will cover will fall into one
    of those categories.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的方法可能有很多，因为这是一个多维度的问题：数据、模型架构和模型训练仅仅是高层次的分类。即使这些分类只是一些例子，且可能还存在或定义更多的分类，但本书将要涵盖的大多数——如果不是所有——技术都会属于这些分类中的某一类。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by demonstrating, with several real-world examples,
    that regularization is the key to success in ML in a production environment. Along
    with several other methods and best practices, a robustly regularized model is
    necessary for production. In production, unseen data and edge cases will appear
    on a regular basis, thus any deployed model must have an acceptable response to
    such cases.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过几个实际案例开始本章，展示了在生产环境中，正则化是机器学习成功的关键。结合其他一些方法和最佳实践，稳健的正则化模型是生产中不可或缺的。在生产环境中，未见过的数据和极端情况将定期出现，因此，任何部署的模型都必须能接受这些情况的响应。
- en: We then walked through some key concepts of regularization. Overfitting and
    underfitting are two common problems in ML and relate somehow to bias and variance.
    Indeed, an overfitting model has high variance, while an underfitting model has
    high bias. Thus, to perform well, a model is required to have low bias and low
    variance. We explained how, no matter how good a model can get, unavoidable bias
    limits its performance. Those key concepts allowed us to propose a method to diagnose
    bias and variance using the performance of both the training and validation sets,
    as well as human-level error estimation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们讲解了一些正则化的关键概念。过拟合和欠拟合是机器学习中的两个常见问题，且与偏差和方差有一定关系。实际上，过拟合的模型具有高方差，而欠拟合的模型具有高偏差。因此，为了表现良好，模型需要具有低偏差和低方差。我们解释了无论一个模型多么优秀，不可避免的偏差都限制了它的性能。这些关键概念使我们提出了一种方法，通过训练集和验证集的表现，以及人类水平的误差估计，来诊断偏差和方差。
- en: 'This led us to what regularization is: regularization is adding constraints
    to a model so that it generalizes well to new data, and is not too sensitive to
    small data fluctuations. Regularization is a great tool to make an overfitting
    model a robust model. Although, due to the bias-variance trade-off, we must not
    regularize too much to avoid having an underfitting model.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们了解了正则化的定义：正则化是为模型添加约束，使其能够很好地泛化到新数据，并且不会对小数据波动过于敏感。正则化是将过拟合模型转化为稳健模型的一个重要工具。然而，由于偏差-方差权衡，我们不能过度正则化，以免得到一个欠拟合的模型。
- en: 'Finally, we categorized the different ways of regularizing that this book will
    cover. They mainly fall into three categories: the data, the model architecture,
    and the model training.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将本书将要涵盖的不同正则化方法进行了分类。它们主要分为三类：数据、模型架构和模型训练。
- en: This chapter did not include any recipes in order to build the foundational
    knowledge that will be required to fully understand the remainder of this book,
    but the next chapters will comprise recipes and will be more solution-oriented.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本章并未包括任何实用技巧，而是建立了理解本书其余内容所需的基础知识，但接下来的章节将包括实用技巧，并将更多聚焦于解决方案。
