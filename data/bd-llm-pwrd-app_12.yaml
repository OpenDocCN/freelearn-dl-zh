- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Responsible AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的AI
- en: In Part 2 of this book, we covered multiple applications of **large language
    models** (**LLMs**), gathering also a deeper understanding of how many factors
    could influence their behavior and outputs. In fact, LLMs open the doors to a
    new set of risks and biases to be taken into account while developing LLM-powered
    applications, in order to mitigate them with defensive attacks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二部分，我们涵盖了大型语言模型（LLMs）的多种应用，并深入了解了哪些因素可能影响其行为和输出。事实上，LLMs为在开发LLM驱动的应用程序时考虑一系列新的风险和偏见打开了大门，以便通过防御性攻击来减轻它们。
- en: In this chapter, we are going to introduce the fundamentals of the discipline
    behind mitigating the potential harms of LLMs – and AI models in general – which
    is Responsible AI. We will then move on to the risks associated with LLMs and
    how to prevent or at least mitigate them using proper techniques. By the end of
    this chapter, you will have a deeper understanding of how to prevent LLMs from
    making your application potentially harmful.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍减轻大型语言模型（LLMs）以及AI模型潜在危害的学科基础——负责任的AI。然后，我们将继续探讨与LLMs相关的风险以及如何使用适当的技术来预防或至少减轻这些风险。到本章结束时，你将更深入地了解如何防止LLMs使你的应用程序可能产生有害的结果。
- en: 'We will cover the following key topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下关键主题：
- en: What is Responsible AI and why do we need it?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是负责任的AI以及为什么我们需要它？
- en: Responsible AI architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任的AI架构
- en: Regulations surrounding Responsible AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任的AI的相关法规
- en: What is Responsible AI and why do we need it?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是负责任的AI以及为什么我们需要它？
- en: Responsible AI refers to the ethical and accountable development, deployment,
    and use of AI systems. It involves ensuring fairness, transparency, privacy, and
    avoiding biases in AI algorithms. Responsible AI also encompasses considerations
    for the social impact and consequences of AI technologies, promoting accountability
    and human-centric design. Responsible AI plays a crucial role in steering decisions
    toward positive and fair results. This involves prioritizing people and their
    objectives in the design of systems while upholding enduring values such as fairness,
    reliability, and transparency.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI指的是AI系统的道德和负责任的开发、部署和使用。它包括确保公平性、透明度、隐私性，并在AI算法中避免偏见。负责任的AI还包括对AI技术的社会影响和后果的考虑，促进问责制和以人为本的设计。负责任的AI在引导决策向积极和公平的结果方向发展方面发挥着关键作用。这涉及到在设计系统时优先考虑人和他们的目标，同时维护持久的价值，如公平性、可靠性和透明度。
- en: 'Some ethical implications of Responsible AI are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI的一些伦理影响包括：
- en: '**Bias**: AI systems can inherit biases present in their training data. These
    biases can lead to discriminatory outcomes, reinforcing existing inequalities.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见**：人工智能系统可能会继承其训练数据中存在的偏见。这些偏见可能导致歧视性结果，加剧现有的不平等。'
- en: '**Explainability**: Black-box models (such as LLMs) lack interpretability.
    Efforts are being made to create more interpretable models to enhance trust and
    accountability.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：黑盒模型（如LLMs）缺乏可解释性。正在努力创建更多可解释的模型以增强信任和问责制。'
- en: '**Data protection**: Collecting, storing, and processing data responsibly is
    essential. Consent, anonymization, and data minimization principles should guide
    AI development.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据保护**：负责任地收集、存储和处理数据至关重要。同意、匿名化和数据最小化原则应指导AI开发。'
- en: '**Liability**: Determining liability for AI decisions (especially in critical
    domains) remains a challenge. Legal frameworks need to evolve to address this.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**责任**：确定AI决策的责任（特别是在关键领域）仍然是一个挑战。法律框架需要发展以应对这一挑战。'
- en: '**Human oversight**: AI should complement human decision-making rather than
    replace it entirely. Human judgment is essential, especially in high-stakes contexts.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类监督**：AI应补充人类决策而不是完全取代它。人类判断在高风险环境中尤为重要。'
- en: '**Environmental impact**: Training large models consumes significant energy.
    Responsible AI considers environmental impacts and explores energy-efficient alternatives.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境影响**：训练大型模型消耗大量能源。负责任的AI考虑环境影响并探索节能的替代方案。'
- en: '**Security**: Ensuring AI systems are secure and resistant to attacks is crucial.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：确保AI系统安全且能够抵御攻击至关重要。'
- en: 'As an example of addressing these implications, Microsoft has established a
    framework called the Responsible AI Standard ([https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf)),
    outlining six principles:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决这些影响的例子，微软建立了一个名为“负责任的AI标准”（[https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf)）的框架，概述了六个原则：
- en: Fairness
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性
- en: Reliability and safety
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠性和安全性
- en: Privacy and security
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私和安全
- en: Inclusiveness
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包容性
- en: Transparency
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 透明度
- en: Accountability
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 责任
- en: In the context of generative AI, Responsible AI would mean creating models that
    respect these principles. For instance, the generated content should be fair and
    inclusive, not favoring any particular group or promoting any form of discrimination.
    The models should be reliable and safe to use. They should respect user’s privacy
    and security. The process of generation should be transparent, and there should
    be mechanisms for accountability.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI的背景下，负责任的AI意味着创建尊重这些原则的模型。例如，生成的内容应该是公平和包容的，不偏袒任何特定群体或促进任何形式的歧视。模型应该可靠且安全使用。它们应该尊重用户的隐私和安全。生成过程应该是透明的，并且应该有问责机制。
- en: Responsible AI architecture
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的AI架构
- en: 'Generally speaking, there are many levels at which we can intervene to make
    a whole LLM-powered application safer and more robust: the model level, the metaprompt
    level, and the user interface level. This architecture can be illustrated as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，我们可以从许多层面上进行干预，使整个由LLM驱动的应用程序更安全、更健壮：模型级别、元提示级别和用户界面级别。这个架构可以如下表示：
- en: '![](img/B21714_12_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_12_01.png)'
- en: 'Figure 12.1: Illustration of different mitigation layers for LLM-powered applications'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：LLM驱动的应用程序的不同缓解层示意图
- en: Of course, it is not always possible to work at all levels. For example, in
    the case of ChatGPT, we consume a pre-built application with a black-box model
    and a fixed UX, so we have little room for intervention only at the metaprompt
    level. On the other hand, if we leverage open-source models via an API, we can
    act up to the model level to incorporate Responsible AI principles. Let’s now
    see a description of each layer of mitigation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并不是总是在所有层面上都能工作。例如，在ChatGPT的情况下，我们使用了一个带有黑盒模型和固定UX的预构建应用程序，所以我们只能在元提示级别上有限地进行干预。另一方面，如果我们通过API利用开源模型，我们可以作用到模型级别以纳入负责任的AI原则。现在让我们看看缓解的每一层的描述。
- en: Model level
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型级别
- en: The very first level is the model itself, which is impacted by the training
    dataset we train it with. In fact, if the training data is biased, the model will
    inherit a biased vision of the world.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个级别是模型本身，它受到我们用来训练它的训练数据集的影响。实际上，如果训练数据有偏见，模型将继承一个有偏见的对世界的看法。
- en: 'One example was covered in the paper *Men Also Like Shopping: Reducing Gender
    Bias Amplification using Corpus-level Constraints* by *Zhao et al.*, where authors
    show an example of model bias in the field of computer vision, as shown in the
    following illustration:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《男性也喜欢购物：使用语料库级别的约束减少性别偏见放大》中有一个例子，作者展示了计算机视觉领域的模型偏见的一个例子，如下所示：
- en: '![](img/B21714_12_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_12_02.png)'
- en: 'Figure 12.2: Example of sexism and bias of a vision model. Adapted from [https://aclanthology.org/D17-1323.pdf](https://aclanthology.org/D17-1323.pdf),
    licensed under CC BY 4.0'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：视觉模型性别歧视和偏见的例子。改编自[https://aclanthology.org/D17-1323.pdf](https://aclanthology.org/D17-1323.pdf)，许可协议为CC
    BY 4.0
- en: The model wrongly identifies a man cooking as a woman, since it associates the
    activity of cooking with women with a greater probability, given the bias of the
    examples the model was trained on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型错误地将一个正在烹饪的男性识别为女性，因为它将烹饪活动与女性的关联概率更高，这是基于模型训练时使用的示例中的偏见。
- en: Another example traces back to the first experiments with ChatGPT, in December
    2022, when it exhibited some sexist and racist comments. A recent tweet highlighted
    this example, asking ChatGPT to create a Python function assessing a person’s
    aptitude as a scientist based on their race and gender.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可以追溯到2022年12月ChatGPT的首次实验，当时它表现出一些性别歧视和种族歧视的评论。最近的一条推文强调了这一例子，要求ChatGPT创建一个Python函数，根据一个人的种族和性别评估其作为科学家的能力。
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_12_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B21714_12_03.png)'
- en: 'Figure 12.3: Inner bias of ChatGPT back in December 2022\. Source: [https://twitter.com/spiantado/status/1599462375887114240](https://twitter.com/spiantado/status/1599462375887114240)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：ChatGPT在2022年12月之前的内部偏见。来源：[https://twitter.com/spiantado/status/1599462375887114240](https://twitter.com/spiantado/status/1599462375887114240)
- en: As you can see, the model created a function that linked the probability of
    being a good scientist to race and gender, which is something that the model shouldn’t
    have created in the first place.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，该模型创建了一个将成为优秀科学家的概率与种族和性别联系起来的函数，这是模型一开始就不应该创建的东西。
- en: 'To act at the model level, there are some areas that researchers and companies
    should look at:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要在模型层面采取行动，研究人员和企业应关注的某些领域包括：
- en: '**Redact and curate training data**: The primary goal of language modeling
    is to faithfully represent the language found in the training corpus. As a result,
    it is crucial to edit and carefully select the training data. For example, in
    the scenario of the vision model previously described, the training dataset should
    have been curated in such a way that a man cooking did not represent a minority.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑和整理训练数据**：语言模型的主要目标是忠实代表训练语料库中的语言。因此，编辑和仔细选择训练数据至关重要。例如，在之前描述的视觉模型场景中，训练数据集应该被整理成这样，即一个正在烹饪的男性不代表少数群体。'
- en: '**Note**'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**'
- en: There are various toolkits available to developers to make training datasets
    more “responsible.” A great open-source example is the Python Responsible AI Toolbox,
    a collection of tools and libraries designed to help developers incorporate Responsible
    AI practices into their workflows. These tools aim to address various aspects
    of AI development, including fairness, interpretability, privacy, and security,
    to ensure that AI systems are safe, trustworthy, and ethical. Specifically, the
    toolkit includes resources to examine datasets for potential biases and ensure
    that models are fair and inclusive, providing metrics to assess group fairness
    and tools to mitigate identified biases; other tools specifically focus on analyzing
    the balance of the dataset, providing metrics and techniques to address imbalances
    that could lead to biased model performance.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开发者有多种工具包可供使用，以使训练数据集更加“负责任”。一个很好的开源例子是Python负责任AI工具箱，这是一个旨在帮助开发者将负责任AI实践融入其工作流程的工具和库集合。这些工具旨在解决AI开发的各个方面，包括公平性、可解释性、隐私和安全，以确保AI系统安全、值得信赖且符合伦理。具体来说，工具箱包括检查数据集潜在偏见的资源，并确保模型公平和包容，提供评估群体公平性的指标和减轻识别到的偏见的工具；其他工具专门关注分析数据集的平衡，提供解决可能导致模型性能偏见的失衡的指标和技术。
- en: '**Fine-tune language models**: Adjust weightings to prevent bias and implement
    checks to filter harmful language. There are many open-source datasets with this
    goal, and you can also find a list of aligned fine-tuning datasets at the following
    GitHub repository: [https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-](https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调语言模型**：调整权重以防止偏见并实施检查以过滤有害语言。有许多开源数据集旨在实现这一目标，你还可以在以下GitHub仓库中找到对齐的微调数据集列表：[https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-](https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-).'
- en: '**Use reinforcement learning with human feedback** (**RLHF**): As covered in
    *Chapter 1*, RLHF is an additional layer of LLMs’ training that consists of adjusting
    a model’s weights according to human feedback. This technique, in addition to
    making the model more “human-like,” is also pivotal in making it less biased,
    since any harmful or biased content will be penalized by the human feedback.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用带有人类反馈的强化学习**（**RLHF**）：如第1章所述，RLHF是LLMs训练的额外一层，它包括根据人类反馈调整模型权重。这项技术除了使模型更“像人”之外，对于减少偏见也至关重要，因为任何有害或偏见的内容都会受到人类反馈的惩罚。'
- en: OpenAI employs this strategy to avoid language models generating harmful or
    toxic content, ensuring that the models are geared toward being helpful, truthful,
    and benign. This is part of the whole training process of OpenAI’s models before
    they are released to the public (specifically, ChatGPT went through this development
    stage before being accessible).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI采用这种策略来避免语言模型生成有害或有毒内容，确保模型旨在提供帮助、真实和良善。这是OpenAI模型在公开发布之前整个训练过程的一部分（特别是，ChatGPT在可访问之前经历了这个发展阶段）。
- en: Making LLMs align with human principles and preventing them from being harmful
    or discriminatory is a top priority among companies and research institutes that
    are in the process of developing LLMs. It is also the first layer of mitigation
    toward potential harms and risks, yet it might be not enough to fully mitigate
    the risk of adopting LLM-powered applications. In the next section, we are going
    to cover the second layer of mitigation, which is the one related to the platform
    adopted to host and deploy your LLMs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使LLMs符合人类原则，防止它们有害或歧视，是正在开发LLMs的公司和研究机构的首要任务。这也是减轻潜在危害和风险的第一层缓解措施，但可能不足以完全缓解采用LLM驱动应用的风险。在下一节中，我们将介绍第二层缓解措施，即与用于托管和部署LLMs的平台相关的措施。
- en: Metaprompt level
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元提示级别
- en: 'In *Chapter 4*, we learned how the prompt and, more specifically, the metaprompt
    or system message associated with our LLM is a key component to make our LLM-powered
    application successful, to the point that a new whole discipline has arisen in
    the last few months: prompt engineering.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*中，我们学习了提示符以及更具体地说，与我们的LLM相关的元提示或系统消息是如何成为使我们的LLM驱动应用成功的关键组件，以至于在过去的几个月里，一个全新的学科已经兴起：提示工程。
- en: 'Since the metaprompt can be used to instruct a model to behave as we wish,
    it is also a powerful tool to mitigate any harmful output it might generate. The
    following are some guidelines on how to leverage prompt engineering techniques
    in that sense:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于元提示可以用来指导模型以我们希望的方式行事，因此它也是减轻其可能产生的任何有害输出的强大工具。以下是一些关于如何利用提示工程技术在这一点上的一些指南：
- en: '**Clear guidelines**: Providing clear instructions and guidelines to the AI
    model about what it can and cannot do. This includes setting boundaries on the
    type of content it can generate, ensuring it respects user privacy, and ensuring
    it does not engage in harmful or inappropriate behavior.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明确指南**：向AI模型提供明确的指示和指南，说明它可以做什么以及不可以做什么。这包括对其可以生成的内容类型设定界限，确保它尊重用户隐私，并确保它不参与有害或不适当的行为。'
- en: '**Transparency**: Being transparent about how the AI model works, its limitations,
    and the measures in place to ensure responsible use. This helps build trust with
    users and allows them to make informed decisions about using AI.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度**：关于AI模型的工作方式、其局限性以及确保负责任使用的措施，保持透明度。这有助于建立与用户的信任，并使他们能够就使用AI做出明智的决定。'
- en: '**Ensure grounding**: Implementing grounding strategies on top of the provided
    data can ensure the model does not hallucinate or provide harmful information.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保基础**：在提供的数据之上实施基础策略，可以确保模型不会产生幻觉或提供有害信息。'
- en: Note that, due to its centrality in these new application architectures, the
    prompt is also a potential subject of **prompt injection**; henceforth, it should
    also include some defensive techniques to prevent this attack.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于其在这些新应用架构中的核心地位，提示符也可能成为**提示注入**的潜在目标；因此，它也应该包括一些防御技术来防止这种攻击。
- en: '**Definition**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Prompt injection stands as a form of attack on LLMs, wherein an AI employing
    a specific metaprompt for a task is deceived by adversarial user input, leading
    it to execute a task diverging from its original purpose.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt injection被视为对LLMs的一种攻击形式，其中，一个为特定任务使用特定元提示的AI被对抗性用户输入欺骗，导致其执行偏离原始目的的任务。
- en: 'Prompt injection can be of different types:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入可以是不同类型的：
- en: '**Prompt leakage** (or direct prompt injection): When there is a malicious
    activity that accesses the meta prompt of an LLM and changes it. For example,
    from the defined metaprompt “You are an AI assistant that translates everything
    to French,” an attacker could leak the prompt and change it to “You are an AI
    assistant that translates everything to German.”'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示泄露**（或直接提示注入）：当存在恶意活动访问LLM的元提示并更改它时。例如，从定义的元提示“你是一个将一切翻译成法语的AI助手”中，攻击者可以泄露提示并将其更改为“你是一个将一切翻译成德语的AI助手”。'
- en: '**Goal hijacking** (or indirect prompt injection): When the malicious activity
    finds target prompts to feed the model with that are capable of bypassing the
    metaprompt instructions. In this context, there are plenty of prompts that have
    been tested as capable of jailbracking the metaprompt instructions. An example
    of one of these prompts, which emerged in the first few months after ChatGPT’s
    launch, has been coined as **Do Anything Now** (**DAN**) and is meant to bypass
    the content safety restrictions embedded within ChatGPT.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标劫持**（或间接提示注入）：当恶意活动找到可以绕过元提示指令的目标提示来喂养模型时。在这种情况下，有大量经过测试的提示被认为能够绕过元提示指令。这些提示中的一个，在ChatGPT发布后的前几个月出现，被称为**现在做任何事情**（**DAN**），目的是绕过ChatGPT内嵌的内容安全限制。'
- en: 'The following lines are the start of one of the versions of this prompt (you
    can find a whole repository about DAN prompts at [https://github.com/0xk1h0/ChatGPT_DAN#chatgpt-dan-and-other-jailbreaks](https://github.com/0xk1h0/ChatGPT_DAN#chatgpt-dan-and-other-jailbreaks)):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些提示版本的开头（你可以在[https://github.com/0xk1h0/ChatGPT_DAN#chatgpt-dan-and-other-jailbreaks](https://github.com/0xk1h0/ChatGPT_DAN#chatgpt-dan-and-other-jailbreaks)找到关于DAN提示的整个存储库）：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are some defensive techniques you can use to prevent prompt injections.
    One of the most remarkable of these techniques is called Adversarial Prompt Detector.
    It consists of enforcing the desired behavior through the instruction given to
    the model. While this doesn’t necessarily provide a comprehensive solution, it
    underscores the effectiveness of a well-formulated prompt.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用一些防御性技术来防止提示注入。其中最引人注目的技术之一被称为对抗性提示检测器。它通过向模型提供的指令来强制执行所需的行为。虽然这并不一定提供全面的解决方案，但它强调了良好构建提示的有效性。
- en: The third and final mitigation layer is at the user interface level, and we
    are going to cover it in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第三和最后的缓解层是在用户界面级别，我们将在下一节中介绍它。
- en: User interface level
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户界面级别
- en: The user interface represents the last mile for an LLM-powered application to
    mitigate the potential associated risks. In fact, the way the user can actually
    interact with the LLM in the backend is a powerful tool to control the incoming
    and outgoing tokens.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面代表了由LLM驱动的应用减轻潜在相关风险的最后一公里。实际上，用户在后台与LLM实际交互的方式是一个强大的工具，可以控制传入和传出的令牌。
- en: For example, in *Chapter 9*, while examining some code-related scenarios, we
    saw how the StarCoder model is used in GitHub as a completion copilot for the
    user. In this case, the user has a closed-ended experience, in the sense that
    they cannot ask direct questions to the model; rather, it receives suggestions
    based on the code it writes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在**第九章**中，当我们检查一些与代码相关的场景时，我们看到了StarCoder模型如何在GitHub上作为用户的完成协作者被使用。在这种情况下，用户有一个封闭式的体验，从意义上说，他们不能直接向模型提问；相反，它根据所编写的代码接收建议。
- en: Another example is in *Chapter 7*, where we developed a movie recommendation
    application with a UX that encourages the user to insert some hardcoded parameters,
    rather than asking an open-ended question.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是在**第七章**中，我们开发了一个具有用户界面UX的电影推荐应用，该界面鼓励用户插入一些硬编码的参数，而不是提出一个开放式的问题。
- en: 'Generally speaking, there are some principles that you might want to take into
    account while designing the UX for your LLM-powered application:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，在设计你的LLM驱动应用UX时，你可能需要考虑以下原则：
- en: '**Disclose the LLM’s role in the interaction**: This can help make people aware
    that they are interacting with an AI system that might also be inaccurate.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**披露LLM在交互中的作用**：这有助于让人们意识到他们正在与一个可能也不准确的AI系统进行交互。'
- en: '**Cite references and sources**: Let the model disclose to the user the retrieved
    documentation that has been used as the context to respond. This holds true if
    there is a vector search within a custom VectorDB, as well as when we provide
    the model with external tools, such as the possibility to navigate the web (as
    we saw with our GlobeBotter assistant in *Chapter 6*).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用参考文献和来源**：让模型向用户披露用于响应的上下文检索到的文档。这适用于在自定义VectorDB中进行向量搜索时，以及当我们向模型提供外部工具时，例如导航网络的可能性（正如我们在**第六章**中看到的GlobeBotter助手）。'
- en: '**Show the reasoning process**: This helps the user to decide whether the ratio
    behind the response is coherent and useful for its purpose. It is also a way to
    be transparent and provide the user with all the necessary information about the
    output it is given. In *Chapter 8*, we covered a similar scenario while asking
    the LLM to show the reasoning as well as the SQL query run against the provided
    database when given a user’s query:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**展示推理过程**：这有助于用户判断响应背后的比率是否连贯且对目的有用。这也是一种透明的方式，向用户提供关于其输出的所有必要信息。在*第8章*中，当我们要求LLM展示推理以及针对提供的数据库运行的SQL查询时，我们覆盖了类似的场景：'
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_12_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图 自动生成的描述](img/B21714_12_04.png)'
- en: 'Figure 12.4: Example of transparency with DBCopilot'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：使用DBCopilot的透明度示例
- en: '**Show the tools used**: When we extend an LLM’s capabilities with external
    tools, we want to make sure the model uses them properly. Henceforth, it is a
    best practice to inform the user about which tool the model uses and how. We saw
    an example of that in *Chapter 10*, while examining the case of the agentic approach
    to building multimodal applications.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**展示所使用的工具**：当我们通过外部工具扩展LLM的功能时，我们希望确保模型正确使用这些工具。因此，告知用户模型使用哪些工具以及如何使用，是一种最佳实践。在*第10章*中，我们看到了一个例子，当时我们正在检查构建多模态应用代理方法的案例。'
- en: '**Prepare pre-defined questions**: Sometimes, LLMs don’t know the answer –
    or even worse, hallucinate – simply because users don’t know how to properly ask
    a question. To address this risk, a best practice (especially in conversational
    applications) is that of encouraging the users with pre-defined questions to start
    with, as well as follow-up questions given a model’s answer. This can reduce the
    risk of poorly written questions as well as give a better UX to the user. An example
    of this technique can be found in Bing Chat, a web copilot developed by Microsoft
    and powered by GPT-4:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备预定义问题**：有时，LLM不知道答案——或者更糟糕的是，因为用户不知道如何正确提问，所以产生了幻觉。为了应对这种风险，一种最佳实践（尤其是在对话应用中）是鼓励用户从预定义问题开始，并根据模型的答案提出后续问题。这可以减少编写不良问题的风险，并为用户提供更好的用户体验。这种技术的例子可以在微软开发的Bing
    Chat中找到，这是一个由GPT-4驱动的网络副驾驶：'
- en: '![A screenshot of a chat  Description automatically generated](img/B21714_12_05.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图 自动生成的描述](img/B21714_12_05.png)'
- en: 'Figure 12.5: UX of Bing Chat with pre-defined questions'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：带有预定义问题的Bing Chat用户体验
- en: '**Provide system documentation**: Making users aware of the type of AI system
    they interact with is a pivotal step if you want to embed Responsible AI within
    your application. To achieve that, you might want to educate the users with comprehensive
    system documentation, covering the system’s capabilities, constraints, and risks.
    For example, develop a “learn more” page for easy access to this information within
    the system.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供系统文档**：如果要在您的应用程序中嵌入负责任的AI，让用户了解他们所交互的AI系统的类型是一个关键步骤。为了实现这一点，您可能希望用全面的系统文档教育用户，涵盖系统的能力、限制和风险。例如，开发一个“了解更多”页面，以便在系统中轻松访问这些信息。'
- en: '**Publish user guidelines and best practices**: Facilitate effective system
    utilization for users and stakeholders by disseminating best practices, such as
    crafting prompts and reviewing generated content before acceptance. Integrate
    these guidelines and best practices directly into the UX whenever feasible.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布用户指南和最佳实践**：通过传播最佳实践，如在接受之前制作提示和审查生成的内容，以促进用户和利益相关者有效利用系统。在可行的情况下，将这些指南和最佳实践直接集成到用户体验中。'
- en: It is important to establish a systematic approach to assess the effectiveness
    of implemented mitigations in addressing potential harms, as well as document
    measurement results and regularly review them to iteratively enhance a system’s
    performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一种系统性的方法来评估实施缓解措施在应对潜在危害方面的有效性很重要，同时记录测量结果并定期审查它们，以迭代地提高系统的性能。
- en: Overall, there are different levels where you could intervene to mitigate risks
    associated with LLMs. From the model level to UX, it is pivotal to incorporate
    these considerations and best practices while developing your LLM-powered application.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，在降低LLM相关风险的不同层面上，您都可以进行干预。从模型级别到用户体验，在开发您的LLM驱动应用程序时，将这些考虑因素和最佳实践纳入其中至关重要。
- en: However, it’s important to note that Responsible AI is not just about the technology
    itself but also its use and impact on society. Therefore, it’s crucial to consider
    ethical aspects and societal implications when developing and deploying these
    systems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要注意，负责任的人工智能不仅仅是关于技术本身，还包括其使用和对社会的影响。因此，在开发和部署这些系统时，考虑伦理方面和社会影响至关重要。
- en: Regulations surrounding Responsible AI
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的人工智能的监管
- en: Regulation of AI is becoming increasingly systematic and stringent, with numerous
    proposals on the table.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的监管正变得越来越系统化和严格，桌上有许多提案。
- en: In the United States, the government, particularly under the Biden-Harris administration,
    has proactively implemented measures to ensure responsible AI usage. This includes
    initiatives like the Blueprint for an AI Bill of Rights, an AI Risk Management
    Framework, and a National AI Research Resource roadmap. President Biden’s Executive
    Order emphasizes eliminating bias in federal agencies’ use of new technologies,
    including AI. Collaborative efforts from agencies like the Federal Trade Commission
    and the Equal Employment Opportunity Commission showcase a commitment to protecting
    Americans from AI-related harm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，政府，尤其是在拜登-哈里斯政府领导下，积极采取措施确保人工智能的负责任使用。这包括像人工智能权利法案蓝图、人工智能风险管理框架和国家人工智能研究资源路线图等倡议。拜登总统的行政命令强调消除联邦机构使用新技术（包括人工智能）中的偏见。联邦贸易委员会和公平就业机会委员会等机构的协作努力展示了保护美国人免受人工智能相关伤害的承诺。
- en: 'In Europe, the European Commission proposed the **Artificial Intelligence Act**
    (**AI Act**), which seeks to establish a comprehensive regulatory framework for
    AI that applies to the following stakeholders:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧洲，欧盟委员会提出了**人工智能法案**（**AI Act**），旨在为以下利益相关者建立一个全面的人工智能监管框架：
- en: '**Providers**: Organizations or individuals who develop, deploy, or offer AI
    systems in the EU are subject to the AI Act. This includes both private and public
    entities.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供者**：在欧盟开发、部署或提供人工智能系统的组织或个人受人工智能法案的约束。这包括私营和公共实体。'
- en: '**Users**: Users who utilize AI systems within the EU fall under the scope
    of the regulation. This includes businesses, government agencies, and individuals.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**：在欧盟内使用人工智能系统的用户属于该规定的范围。这包括企业、政府机构和个人。'
- en: '**Importers**: Entities that import AI systems into the EU market are also
    subject to compliance with the AI Act.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进口商**：将人工智能系统进口到欧盟市场的实体也必须遵守人工智能法案。'
- en: '**Distributors**: Distributors who place AI systems on the EU market are responsible
    for ensuring that these systems comply with the regulation.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分销商**：将人工智能系统投放欧盟市场的分销商有责任确保这些系统符合规定。'
- en: '**Third-country entities**: Even entities located outside the EU that provide
    AI services or products to EU residents are subject to certain provisions of the
    AI Act.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三国实体**：即使位于欧盟以外的实体，只要向欧盟居民提供人工智能服务或产品，也受人工智能法案某些条款的约束。'
- en: 'By categorizing AI systems by risk, the AI Act outlines the development and
    use of requirements to promote human-centric and trustworthy AI. The Act aims
    to safeguard health, safety, fundamental rights, democracy, the rule of law, and
    the environment. It empowers citizens to file complaints, establishes an EU AI
    Office for enforcement, and mandates member states to appoint national supervisory
    authorities for AI. The Act aligns with Responsible AI principles, emphasizing
    fairness, accountability, transparency, and ethics. The idea is to ensure that:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按风险对人工智能系统进行分类，人工智能法案概述了促进以人为本和值得信赖的人工智能的开发和使用要求。该法案旨在保护健康、安全、基本权利、民主、法治和环境。它赋予公民提起投诉的权利，设立欧盟人工智能办公室以执行，并要求成员国任命人工智能的国家监管机构。该法案与负责任的人工智能原则相一致，强调公平、问责制、透明度和伦理。其目的是确保：
- en: Providers of generative AI systems must train, design, and develop their systems
    with state-of-the-art safeguards against generating content that breaches EU laws.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式人工智能系统的提供者必须使用最先进的保障措施来训练、设计和开发他们的系统，以防止生成违反欧盟法律的内容。
- en: Providers are required to document and provide a publicly available detailed
    summary of their use of copyrighted training data.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供者必须记录并提供一份公开的详细摘要，说明他们使用受版权保护训练数据的情况。
- en: Providers must adhere to more stringent transparency obligations.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供者必须遵守更严格的透明度义务。
- en: If a generative AI system has been used to create “deep fakes,” users who created
    such content must disclose that it was generated or manipulated by AI.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果生成式人工智能系统被用于创建“深度伪造”，则创建此类内容的用户必须披露该内容是由人工智能生成或操纵的。
- en: 'The AI Act represents a significant step toward ensuring that AI technologies
    are developed and used in a way that benefits society, while respecting fundamental
    human rights and values. In 2023, amid the rapid growth of generative AI technologies,
    significant strides were made regarding the AI Act:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能法案是确保人工智能技术以造福社会、尊重基本人权和价值观的方式开发和使用的重大步骤。在2023年，随着生成式人工智能技术的快速增长，在人工智能法案方面取得了重大进展：
- en: By June 14, 2023, the European Parliament had endorsed its stance on the AI
    Act, securing 499 votes in favor, 28 against, and 93 abstentions.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至2023年6月14日，欧洲议会已经批准了其对人工智能法案的立场，获得499票赞成，28票反对，93票弃权。
- en: Noteworthy amendments were introduced to the proposal for a regulation, titled
    the AI Act, with the aim of establishing unified regulations on AI and modifying
    certain European Union legislative acts.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对名为人工智能法案的法规提案进行了重要的修正，旨在建立关于人工智能的统一法规，并修改某些欧盟立法法案。
- en: Approved in December 2023, the AI Act allows a grace period of 2 to 3 years
    for preparation before its activation.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2023年12月获得批准的人工智能法案允许在激活前有2到3年的准备宽限期。
- en: These developments signify the ongoing progress of the AI Act toward its implementation,
    positioning the EU as a potential trailblazer in introducing oversight or regulation
    for generative AI, given the advanced negotiations within the European Commission.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进展标志着人工智能法案向其实施的持续进步，鉴于欧洲委员会内进行的先进谈判，使欧盟成为引入对生成式人工智能监督或监管的潜在先驱。
- en: Overall, governments around the world are scrambling to figure out how to approach
    the questions posed by AI. These advancements reflect a growing recognition of
    the need for Responsible AI and the role of government in ensuring it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，世界各国政府都在急于弄清楚如何应对人工智能提出的问题。这些进展反映了人们对负责任人工智能的需求日益增长，以及政府在确保其发挥作用的角色。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the “dark side” of generative AI technologies, exposing
    its associated risks and biases, such as hallucinations, harmful content, and
    discrimination. To reduce and overcome those risks, we introduced the concept
    of Responsible AI, starting with a deep dive into the technical approach we can
    have while developing LLM-powered applications; we covered the different levels
    of risk mitigation – model, metaprompt, and UX – and then moved on to the broader
    topic of institutional regulations. In this context, we examined the advancements
    that have been carried out by governments in the last year, with a focus on the
    AI Act.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了生成式人工智能技术的“阴暗面”，揭示了其相关的风险和偏见，例如幻觉、有害内容和歧视。为了减少和克服这些风险，我们介绍了负责任人工智能的概念，从深入探讨我们在开发由大型语言模型（LLM）驱动的应用程序时可以采取的技术方法开始；我们涵盖了不同级别的风险缓解措施——模型、元提示和用户体验（UX）——然后转向更广泛的主题，即机构法规。在此背景下，我们审视了政府在去年所取得的进展，重点关注人工智能法案。
- en: Responsible AI is an evolving field of research, and it definitely has an interdisciplinary
    flavor. There will probably be an acceleration at the regulation level to address
    it in the near future.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任人工智能是一个不断发展的研究领域，它肯定具有跨学科的特点。预计在不久的将来，在监管层面将加速应对它。
- en: In the next and final chapter, we are going to cover all the emerging trends
    and innovations happening in the generative AI field with a glimpse of what we
    could expect from the near future.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和最后一章中，我们将涵盖生成式人工智能领域正在发生的所有新兴趋势和创新，并展望我们可能从近期未来期待的内容。
- en: References
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Reducing Gender Bias Amplification using Corpus-level Constraints: [https://browse.arxiv.org/pdf/1707.09457.pdf](https://browse.arxiv.org/pdf/1707.09457.pdf)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语料库级别的约束减少性别偏见放大：[https://browse.arxiv.org/pdf/1707.09457.pdf](https://browse.arxiv.org/pdf/1707.09457.pdf)
- en: 'ChatGPT racist and sexist outputs: [https://twitter.com/spiantado/status/1599462375887114240](https://twitter.com/spiantado/status/1599462375887114240)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT 种族主义和性别歧视的输出：[https://twitter.com/spiantado/status/1599462375887114240](https://twitter.com/spiantado/status/1599462375887114240)
- en: 'GitHub repository for an aligned dataset: [https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-](https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub 对齐数据集的仓库：[https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-](https://github.com/Zjh-819/LLMDataHub#general-open-access-datasets-for-alignment-)
- en: 'AI Act: [https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf](https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能法案：[https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf](https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf)
- en: 'Prompt hijacking: [https://arxiv.org/pdf/2211.09527.pdf](https://arxiv.org/pdf/2211.09527.pdf)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示劫持：[https://arxiv.org/pdf/2211.09527.pdf](https://arxiv.org/pdf/2211.09527.pdf)
- en: 'AI Act: [https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能法案：[https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)
- en: 'Blueprint for an AI Bill of Rights: [https://www.whitehouse.gov/ostp/ai-bill-of-rights/](https://www.whitehouse.gov/ostp/ai-bill-of-rights/)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能权利法案蓝图：[https://www.whitehouse.gov/ostp/ai-bill-of-rights/](https://www.whitehouse.gov/ostp/ai-bill-of-rights/)
- en: Join our community on Discord
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code214329708533108046.png)'
