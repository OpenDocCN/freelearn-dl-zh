- en: Extract, Transform, Load
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取、转换、加载
- en: Training and testing DL models requires data. Data is usually hosted on different
    distributed and remote storage systems. You need them to connect to the data sources
    and perform data retrieval so that you can start the training phase and you would
    probably need to do some preparation before feeding your model. This chapter goes
    through the phases of the **Extract**, **Transform**, **Load** (**ETL**) process
    applied to DL. It covers several use cases for which the DeepLearning4j framework
    and Spark would be used. The use cases presented here are related to batch data
    ingestion. Data streaming will be covered in the next chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试深度学习模型需要数据。数据通常存储在不同的分布式和远程存储系统中。你需要连接到数据源并执行数据检索，以便开始训练阶段，同时可能需要做一些准备工作再将数据输入模型。本章介绍了应用于深度学习的**提取**、**转换**、**加载**（**ETL**）过程的各个阶段。它涵盖了使用DeepLearning4j框架和Spark的若干用例，这些用例与批量数据摄取有关。数据流处理将在下一章介绍。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Training data ingestion through Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Spark摄取训练数据
- en: Data ingestion from a relational database
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从关系型数据库中摄取数据
- en: Data ingestion from a NoSQL database
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从NoSQL数据库摄取数据
- en: Data ingestion from S3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从S3摄取数据
- en: Training data ingestion through Spark
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Spark摄取训练数据
- en: The first section of this chapter introduces the DeepLearning4j framework and
    then presents some use cases of training data ingestion from files using this
    framework along with Apache Spark.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分介绍了DeepLearning4j框架，并展示了使用该框架和Apache Spark从文件中摄取训练数据的一些用例。
- en: The DeepLearning4j framework
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepLearning4j框架
- en: Before jumping into the first example, let's quickly introduce the DeepLearning4j
    ([https://deeplearning4j.org/](https://deeplearning4j.org/)) framework. It is
    an open source (released under the Apache license 2.0 ([https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0))),
    distributed deep learning framework written for the JVM. Being integrated since
    its earliest releases with Hadoop and Spark, it takes advantage of such distributed
    computing frameworks to speed up network training. It is written in Java, so is
    compatible with any other JVM language (including Scala of course), while the
    underlying computations are written in lower level languages, such as C, C++,
    and CUDA. The DL4J API gives flexibility when composing deep neural networks.
    So it is possible to combine different network implementations as needed in a
    distributed, production-grade infrastructure on top of distributed CPUs or GPUs.
    DL4J can import neural net models from most of the major ML or DL Python frameworks
    (including TensorFlow and Caffe) via Keras ([https://keras.io/](https://keras.io/)),
    bridging the gap between the Python and the JVM ecosystems in terms of toolkits
    for data scientists in particular, but also for data engineers and DevOps. Keras
    represents the DL4J's Python API.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入第一个示例之前，我们先简单介绍一下DeepLearning4j（[https://deeplearning4j.org/](https://deeplearning4j.org/)）框架。它是一个开源的（基于Apache
    2.0许可证发布，[https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)）分布式深度学习框架，专为JVM编写。自最早版本起，DeepLearning4j便与Hadoop和Spark集成，利用这些分布式计算框架加速网络训练。该框架用Java编写，因此与任何其他JVM语言（当然也包括Scala）兼容，而底层计算则使用低级语言（如C、C++和CUDA）编写。DL4J的API提供了在构建深度神经网络时的灵活性。因此，可以根据需要将不同的网络实现组合在一起，部署到基于分布式CPU或GPU的生产级基础设施上。DL4J可以通过Keras（[https://keras.io/](https://keras.io/)）导入来自大多数主流机器学习或深度学习Python框架（包括TensorFlow和Caffe）的神经网络模型，弥合Python与JVM生态系统之间的差距，尤其是为数据科学家提供工具，同时也适用于数据工程师和DevOps。Keras代表了DL4J的Python
    API。
- en: 'DL4J is modular. These are the main libraries that comprise this framework:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J是模块化的。以下是构成该框架的主要库：
- en: '**Deeplearning4j**: The neural network platform core'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Deeplearning4j**：神经网络平台核心'
- en: '**ND4J**: The NumPy ([http://www.numpy.org/](http://www.numpy.org/)) porting
    for the JVM'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4J**：JVM版NumPy（[http://www.numpy.org/](http://www.numpy.org/)）'
- en: '**DataVec**: A tool for ML ETL operations'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataVec**：用于机器学习ETL操作的工具'
- en: '**Keras import**: To import pre-trained Python models implemented in Keras'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras导入**：导入在Keras中实现的预训练Python模型'
- en: '**Arbiter**: A dedicated library for multilayer neural networks hyperparameter
    optimization'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arbiter**：用于多层神经网络超参数优化的专用库'
- en: '**RL4J**: The implementation of deep reinforcement learning for the JVM'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL4J**：JVM版深度强化学习实现'
- en: We are going to explore almost all of the features of DL4J and its libraries,
    starting from this chapter and across the other chapters of this book.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从本章开始，探索 DL4J 及其库的几乎所有功能，并贯穿本书的其他章节。
- en: The reference release for DL4J in this book is version 0.9.1.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的 DL4J 参考版本是 0.9.1。
- en: Data ingestion through DataVec and transformation through Spark
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 DataVec 获取数据并通过 Spark 进行转化
- en: 'Data can come from many sources and in many types, for example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以来自多个来源，并且有多种类型，例如：
- en: Log files
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志文件
- en: Text documents
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本文件
- en: Tabular data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格数据
- en: Images
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像
- en: Videos
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频
- en: When working with neural nets, the end goal is to convert each data type into
    a collection of numerical values in a multidimensional array. Data could also
    need to be pre-processed before it can be used to train or test a net. Therefore,
    an ETL process is needed in most cases, which is a sometimes underestimated challenge
    that data scientists have to face when doing ML or DL. That's when the DL4J DataVec
    library comes to the rescue. After data is transformed through this library API,
    it comes into a format (vectors) understandable by neural networks, so DataVec
    quickly produces open standard compliant vectorized data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经网络时，最终目标是将每种数据类型转换为多维数组中的数值集合。数据在被用于训练或测试神经网络之前，可能还需要预处理。因此，在大多数情况下，需要进行
    ETL 过程，这是数据科学家在进行机器学习或深度学习时面临的一个有时被低估的挑战。这时，DL4J DataVec 库就发挥了重要作用。通过该库的 API 转换后的数据，转换为神经网络可理解的格式（向量），因此
    DataVec 可以快速生成符合开放标准的向量化数据。
- en: DataVec supports out-of-the-box all the major types of input data (text, CSV,
    audio, video, image) with their specific input formats. It can be extended for
    specialized input formats not covered by the current release of its API. You can
    think about the DataVec input/output format system as the same way Hadoop MapReduce uses
    `InputFormat` implementations to determine the logical *InputSplits* and the `RecordReaders`
    implementation to use. It also provides `RecordReaders` to serialize data. This
    library also includes facilities for feature engineering, data cleanup, and normalization.
    They work with both static data and time series. All of the available functionalities
    can be executed on Apache Spark through the DataVec-Spark module.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DataVec 开箱即用地支持所有主要类型的输入数据（文本、CSV、音频、视频、图像）及其特定的输入格式。它可以扩展以支持当前 API 版本中未涵盖的专业输入格式。你可以将
    DataVec 的输入/输出格式系统与 Hadoop MapReduce 中的 `InputFormat` 实现进行类比，用于确定逻辑的 *InputSplits*
    和 `RecordReaders` 实现的选择。它还提供 `RecordReaders` 来序列化数据。这个库还包括特征工程、数据清理和归一化的功能。它们可以处理静态数据和时间序列数据。所有可用的功能都可以通过
    DataVec-Spark 模块在 Apache Spark 上执行。
- en: 'If you want to know more about the Hadoop MapReduce classes mentioned previously,
    you can have a look at the following official online Javadocs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于前面提到的 Hadoop MapReduce 类的信息，你可以查看以下官方在线 Javadocs：
- en: '| **Class name** | **Link** |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **类名** | **链接** |'
- en: '| `InputFormat` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputFormat.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputFormat.html)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `InputFormat` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputFormat.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputFormat.html)
    |'
- en: '| `InputSplits` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputSplit.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputSplit.html)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `InputSplits` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputSplit.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputSplit.html)
    |'
- en: '| `RecordReaders` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/RecordReader.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/RecordReader.html)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `RecordReaders` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/RecordReader.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/RecordReader.html)
    |'
- en: 'Let''s see a practical code example in Scala. We want to extract data from
    a CSV file that contains some e-shop transactions and have the following columns:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个 Scala 中的实际代码示例。我们想从一个包含电子商店交易的 CSV 文件中提取数据，并且该文件包含以下列：
- en: '`DateTimeString`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DateTimeString`'
- en: '`CustomerID`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomerID`'
- en: '`MerchantID`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MerchantID`'
- en: '`NumItemsInTransaction`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumItemsInTransaction`'
- en: '`MerchantCountryCode`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MerchantCountryCode`'
- en: '`TransactionAmountUSD`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TransactionAmountUSD`'
- en: '`FraudLabel`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FraudLabel`'
- en: Then, we perform some transformation over them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对这些数据进行一些转化操作。
- en: 'We need to import the required dependencies (Scala, Spark, DataVec, and DataVec-Spark)
    first. Here is a complete list for a Maven POM file (but, of course, you can use
    SBT or Gradle):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所需的依赖项（Scala、Spark、DataVec 和 DataVec-Spark）。以下是 Maven POM 文件的完整列表（当然，您也可以使用
    SBT 或 Gradle）：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first thing to do in the Scala application is to define the input data
    schema, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala 应用程序中的第一步是定义输入数据模式，如下所示：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If input data is numeric and appropriately formatted then a `CSVRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/csv/CSVRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/csv/CSVRecordReader.html))
    may be satisfactory. If, however, the input data has non-numeric fields, then
    a schema transformation will be required. DataVec uses Apache Spark to perform
    transform operations. Once we have the input schema, we can define the transformation
    we want to apply to the input data. Just a couple of transformations are described
    in this example. We can remove some columns that are unnecessary for our net,
    for example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据是数值的并且格式正确，那么可以使用`CSVRecordReader` ([https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/csv/CSVRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/csv/CSVRecordReader.html))。然而，如果输入数据包含非数值字段，则需要进行模式转换。DataVec
    使用 Apache Spark 执行转换操作。一旦我们有了输入模式，我们可以定义要应用于输入数据的转换。这个例子中描述了一些转换。例如，我们可以删除一些对我们的网络不必要的列：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Filter the `MerchantCountryCode` column in order to get the records related
    to USA and Canada only, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤`MerchantCountryCode`列以获取仅与美国和加拿大相关的记录，如下所示：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this stage, the transformations are only defined, but not applied yet (of
    course we need to get the data from the input file first). So far, we have used
    DataVec classes only. In order to read the data and apply the defined transformations,
    the Spark and DataVec-Spark API need to be used.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，仅定义了转换，但尚未应用（当然，我们首先需要从输入文件中获取数据）。到目前为止，我们仅使用了 DataVec 类。为了读取数据并应用定义的转换，需要使用
    Spark 和 DataVec-Spark API。
- en: 'Let''s create the `SparkContext` first, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建`SparkContext`，如下所示：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can read the CSV input file and parse the data using a `CSVRecordReader`,
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以读取 CSV 输入文件并使用`CSVRecordReader`解析数据，如下所示：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then execute the transformation defined earlier, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行先前定义的转换，如下所示：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, let''s collect the data locally, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们本地收集数据，如下所示：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The input data is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据如下所示：
- en: '![](img/7f7520ca-540e-4f55-b1dd-e0d37e062822.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f7520ca-540e-4f55-b1dd-e0d37e062822.png)'
- en: 'The processed data is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的数据如下所示：
- en: '![](img/1080235c-2b0a-4427-b254-894a85f593e6.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1080235c-2b0a-4427-b254-894a85f593e6.png)'
- en: The full code of this example is part of the source code included with the book.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的完整代码包含在书籍附带的源代码中。
- en: Training data ingestion from a database with Spark
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 从数据库中进行训练数据摄取
- en: Sometimes data has been previously ingested and stored into a database by some
    other application, so you would need to connect to a database in order to use
    it for training or testing purposes. This section describes how to get data from
    a relational database and a NoSQL database. In both cases, Spark would be used.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据之前已被其他应用程序摄入并存储到数据库中，因此您需要连接到数据库以便用于训练或测试目的。本节描述了如何从关系数据库和 NoSQL 数据库获取数据。在这两种情况下，都将使用
    Spark。
- en: Data ingestion from a relational database
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从关系数据库中摄取数据
- en: 'Suppose the data is stored in a table called `sparkexample` in a MySQL ([https://dev.mysql.com/](https://dev.mysql.com/))
    schema with the name `sparkdb`. This is the structure of that table:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据存储在 MySQL ([https://dev.mysql.com/](https://dev.mysql.com/)) 架构名称为 `sparkdb`
    的表 `sparkexample` 中。这是该表的结构：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It contains the same data as, for the example, in *Training data ingestion
    through Spark*, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含与 *使用 Spark 进行训练数据摄取* 中相同的数据，如下所示：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The dependencies to add to the Scala Spark project are the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到 Scala Spark 项目中的依赖项如下所示：
- en: Apache Spark 2.2.1
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.2.1
- en: Apache Spark SQL 2.2.1
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark SQL 2.2.1
- en: The specific JDBC driver for the MySQL database release used
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于 MySQL 数据库发布的特定 JDBC 驱动程序
- en: 'Let''s now implement the Spark application in Scala. In order to connect to
    the database, we need to provide all of the needed parameters. Spark SQL also
    includes a data source that can read data from other databases using JDBC, so
    the required properties are the same as for a connection to a database through
    traditional JDBC; for example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现 Scala 中的 Spark 应用程序。为了连接到数据库，我们需要提供所有必要的参数。Spark SQL 还包括一个数据源，可以使用 JDBC
    从其他数据库读取数据，因此所需的属性与通过传统 JDBC 连接到数据库时相同；例如：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We need to check that the JDBC driver for the MySQL database is available,
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要检查 MySQL 数据库的 JDBC 驱动是否可用，如下所示：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now create a `SparkSession`, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个 `SparkSession`，如下所示：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the implicit conversions, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 导入隐式转换，如下所示：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can finally connect to the database and load the data from the `sparkexample`
    table to a DataFrame, as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你可以连接到数据库并将 `sparkexample` 表中的数据加载到 DataFrame，如下所示：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Spark automatically reads the schema from a database table and maps its types
    back to Spark SQL types. Execute the following method on the DataFrame:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 会自动从数据库表中读取模式，并将其类型映射回 Spark SQL 类型。对 DataFrame 执行以下方法：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It returns the exact same schema as for the table `sparkexample`; for example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回与表` sparkexample`相同的模式；例如：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the data is loaded into the DataFrame, it is possible to run SQL queries
    against it using the specific DSL as shown in the following example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被加载到 DataFrame 中，就可以使用特定的 DSL 执行 SQL 查询，如下例所示：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It is possible to increase the parallelism of the reads through the JDBC interface.
    We need to provide split boundaries based on the DataFrame column values. There
    are four options available (`columnname`, `lowerBound`, `upperBound`, and `numPartitions`)
    to specify the parallelism on read. They are optional, but they must all be specified
    if any of them is provided; for example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 JDBC 接口增加读取的并行性。我们需要根据 DataFrame 列值提供拆分边界。有四个选项可用（`columnname`，`lowerBound`，`upperBound`
    和 `numPartitions`），用于指定读取时的并行性。它们是可选的，但如果提供其中任何一个，必须全部指定；例如：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: While the examples in this section refer to a MySQL database, they apply the
    same way to any commercial or open source RDBMS for which a JDBC driver is available.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节中的示例参考了 MySQL 数据库，但它们适用于任何具有 JDBC 驱动的商业或开源关系型数据库。
- en: Data ingestion from a NoSQL database
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 NoSQL 数据库中获取数据
- en: Data can also come from a NoSQL database. In this section, we are going to explore
    the code to implement in order to consume the data from a MongoDB ([https://www.mongodb.com/](https://www.mongodb.com/))
    database.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据也可以来自 NoSQL 数据库。在本节中，我们将探讨实现代码，以便从 MongoDB ([https://www.mongodb.com/](https://www.mongodb.com/))
    数据库中消费数据。
- en: 'The collection `sparkexample` of the `sparkmdb` database contains the same
    data as for the examples in *Data ingestion through DataVec and transformation
    through Spark* and *Data ingestion from a relational database* sections, but in
    the form of BSON documents; for example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparkmdb` 数据库中的 `sparkexample` 集合包含与 *通过 DataVec 获取数据和通过 Spark 转换* 以及 *从关系型数据库获取数据*
    部分中的示例相同的数据，但以 BSON 文档的形式；例如：'
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The dependencies to add to the Scala Spark project are the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 需要添加到 Scala Spark 项目的依赖项如下：
- en: Apache Spark 2.2.1
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.2.1
- en: Apache Spark SQL 2.2.1
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark SQL 2.2.1
- en: The MongoDB connector for Spark 2.2.0
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.2.0 的 MongoDB 连接器
- en: 'We need to create a Spark Session, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个 Spark 会话，如下所示：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Specify the connection to the database. After the session as been created,
    it is possible to use it to load data from the `sparkexample` collection through
    the `com.mongodb.spark.MongoSpark` class, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 指定连接到数据库的方式。在创建会话后，可以使用它通过 `com.mongodb.spark.MongoSpark` 类从 `sparkexample`
    集合加载数据，如下所示：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The returned DataFrame has the same structure as for the `sparkexample` collection.
    Use the following instruction:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的 DataFrame 具有与 `sparkexample` 集合相同的结构。使用以下指令：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It prints the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它会打印出以下输出：
- en: '![](img/ef9e643a-a52b-499d-950f-17dc5fe4aa7a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef9e643a-a52b-499d-950f-17dc5fe4aa7a.png)'
- en: 'Of course, the retrieved data is that in the DB collection, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，检索到的数据就是数据库集合中的数据，如下所示：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It returns the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回如下内容：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is also possible to run SQL queries on the DataFrame. We need first to create
    a case class to define the schema for the DataFrame, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在 DataFrame 上运行 SQL 查询。我们首先需要创建一个 case 类来定义 DataFrame 的模式，如下所示：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we load the data, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载数据，如下所示：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We must register a temporary view for the DataFrame, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为 DataFrame 注册一个临时视图，如下所示：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Before we can execute an SQL statement, for example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们执行 SQL 语句之前，例如：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Use the following instruction:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下指令：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It returns the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回如下内容：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Data ingestion from S3
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 S3 获取数据
- en: Nowadays, there's a big chance that the training and test data are hosted in
    some cloud storage system. In this section, we are going to learn how to ingest
    data through Apache Spark from an object storage such as Amazon S3 ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    or S3-based (such as Minio, [https://www.minio.io/](https://www.minio.io/)). The
    Amazon simple storage service (which is more popularly known as Amazon S3) is
    an object storage service part of the AWS cloud offering. While S3 is available
    in the public cloud, Minio is a high performance distributed object storage server
    compatible with the S3 protocol and standards that has been designed for large-scale
    private cloud infrastructures.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，训练和测试数据很可能托管在某些云存储系统中。在本节中，我们将学习如何通过 Apache Spark 从对象存储（如 Amazon S3（[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)）或基于
    S3 的存储（如 Minio，[https://www.minio.io/](https://www.minio.io/)））摄取数据。Amazon 简单存储服务（更常被称为
    Amazon S3）是 AWS 云服务的一部分，提供对象存储服务。虽然 S3 可用于公共云，Minio 是一个高性能分布式对象存储服务器，兼容 S3 协议和标准，专为大规模私有云基础设施设计。
- en: 'We need to add to the Scala project the Spark core and Spark SQL dependencies,
    and also the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 Scala 项目中添加 Spark 核心和 Spark SQL 依赖项，以及以下内容：
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: They are the AWS Java JDK core and S3 libraries, plus the Apache Hadoop module
    for AWS integration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是 AWS Java JDK 核心库和 S3 库，以及用于 AWS 集成的 Apache Hadoop 模块。
- en: For this example, we need to have already created one existing bucket on S3
    or Minio. For the readers not familiar with the S3 object storage, a bucket is
    similar to a file system directory, where users can store objects (data and the
    metadata that describe it). Then we need to upload a file in that bucket that
    would need to be read by Spark. The file used for this example is one generally
    available for download at the MonitorWare website ([http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)).
    It contains HTTP requests log entries in ASCII format. For the purpose of this
    example, we are assuming that the name of the bucket is `dl4j-bucket` and the
    uploaded file name is `access_log`. The first thing to do in our Spark program
    is to create a `SparkSession`, as follows
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们需要已经在 S3 或 Minio 上创建了一个现有的存储桶。对于不熟悉 S3 对象存储的读者，存储桶类似于文件系统目录，用户可以在其中存储对象（数据及其描述的元数据）。然后，我们需要在该存储桶中上传一个文件，Spark
    将需要读取该文件。此示例使用的文件通常可以从 MonitorWare 网站下载（[http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)）。它包含以
    ASCII 格式记录的 HTTP 请求日志条目。为了这个示例，我们假设存储桶的名称是 `dl4j-bucket`，上传的文件名是 `access_log`。在我们的
    Spark 程序中，首先要做的是创建一个 `SparkSession`，如下所示
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In order to reduce noise on the output, let's set the log level for Spark to
    `WARN`, as follows
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少输出中的噪音，让我们将 Spark 的日志级别设置为 `WARN`，如下所示
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that the `SparkSession` has been created, we need to set up the S3 or Minio
    endpoint and the credentials for Spark to access it, plus some other properties,
    as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `SparkSession` 已创建，我们需要设置 S3 或 Minio 端点和凭据，以便 Spark 访问它，并设置其他一些属性，如下所示：
- en: '[PRE34]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is the meaning of the properties that have been set for the minimal configuration:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为最小配置设置的属性的含义：
- en: '`fs.s3a.endpoint`: The S3 or Minio endpoint.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs.s3a.endpoint`：S3 或 Minio 端点。'
- en: '`fs.s3a.access.key`: The AWS or Minio access key ID.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs.s3a.access.key`：AWS 或 Minio 访问密钥 ID。'
- en: '`fs.s3a.secret.key`: The AWS or Minio secret key.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs.s3a.secret.key`：AWS 或 Minio 秘密密钥。'
- en: '`fs.s3a.path.style.access`: Enables S3 path style access while disabling the
    default virtual hosting behavior.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs.s3a.path.style.access`：启用 S3 路径风格访问，同时禁用默认的虚拟主机行为。'
- en: '`fs.s3a.connection.ssl.enabled`: Specifies if SSL is enabled at the endpoint.
    Possible values are `true` and `false`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs.s3a.connection.ssl.enabled`：指定是否在端点启用了 SSL。可能的值是`true`和`false`。'
- en: '`fs.s3a.impl`: The implementation class of the `S3AFileSystem` that is used.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs.s3a.impl`：所使用的 `S3AFileSystem` 实现类。'
- en: 'We are now ready to read the `access_log` file (or any other file) from a S3
    or Minio bucket and load its content into a RDD, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好从 S3 或 Minio 存储桶中读取`access_log`文件（或任何其他文件），并将其内容加载到 RDD 中，如下所示：
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It is also possible to convert the RDD into a DataFrame and show the content
    on the output, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将RDD转换为DataFrame，并按如下方式显示输出内容：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will provide the following output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供以下输出：
- en: '![](img/91d783d2-5436-41da-b551-c8a31b0b7cc0.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91d783d2-5436-41da-b551-c8a31b0b7cc0.png)'
- en: Once data has been loaded from objects stored into S3 or Minio buckets, any
    operation available in Spark for RDDs and Datasets can be used.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从存储在S3或Minio桶中的对象加载数据，就可以使用Spark对RDD和数据集的任何操作。
- en: Raw data transformation with Spark
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行原始数据转换
- en: Data coming from a source is often raw data. When we talk about raw data we
    mean data that is in a format that can't be used as is for the training or testing
    purposes of our models. So, before using, we need to make it tidy. The cleanup
    process is done through one or more transformations before giving the data as
    input for a given model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来自一个来源时，通常是原始数据。当我们谈论原始数据时，指的是那些无法直接用于训练或测试模型的数据格式。因此，在使用之前，我们需要将其整理。清理过程通过一个或多个转换步骤完成，才能将数据作为输入提供给特定的模型。
- en: For data transformation purposes, the DL4J DataVec library and Spark provide
    several facilities. Some of the concepts described in this section have been explored
    in the *Data ingestion through DataVec and transformation through Spark* section,
    but now we are going to add a more complex use case.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数据转换的目的，DL4J的DataVec库和Spark提供了多种功能。此部分描述的一些概念已在*通过DataVec进行数据摄取并通过Spark进行转换*部分中探讨过，但现在我们将添加一个更复杂的使用案例。
- en: 'To understand how to use Datavec for transformation purposes, let''s build
    a Spark application for web traffic log analysis. The dataset used is generally
    available for download at the MonitorWare website ([http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)).
    They are HTTP requests log entries in ASCII format. There is one line per request,
    with the following columns:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何使用Datavec进行数据转换，我们来构建一个用于网站流量日志分析的Spark应用程序。所使用的数据集可以从MonitorWare网站下载（[http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)）。这些数据是ASCII格式的HTTP请求日志条目，每个请求一行，包含以下列：
- en: The host making the request. A hostname or an internet address
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发出请求的主机。可以是主机名或互联网地址
- en: A timestamp in the format *DD/Mon/YYYY:HH:MM:SS*, where *DD* is the day of the
    month, *Mon* is the name of the month, *YYYY* is the year and *HH:MM:SS* is the
    time of day using a 24-hour clock. The timezone is -*0800*
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个时间戳，格式为*DD/Mon/YYYY:HH:MM:SS*，其中*DD*是日，*Mon*是月的名称，*YYYY*是年份，*HH:MM:SS*是使用24小时制的时间。时区为-*0800*
- en: The HTTP request given in quotes
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引号中的HTTP请求
- en: The HTTP reply code
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP回复代码
- en: The total of bytes in the reply
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回复中的字节总数
- en: 'Here''s a sample of the log content used:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例日志内容：
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The first thing to do in our application is to define the schema of the input
    data, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用程序中，首先要做的是定义输入数据的模式，如下所示：
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Start a Spark context, as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个Spark上下文，如下所示：
- en: '[PRE39]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Load the file, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 加载文件，如下所示：
- en: '[PRE40]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'A web log file could contain some invalid lines that don''t follow the preceding
    schema, so we need to include some logic to discard those lines that are useless
    for our analysis, for example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个网页日志文件可能包含一些无效的行，这些行不符合前述的模式，因此我们需要加入一些逻辑来丢弃那些对我们的分析没有用的行，例如：
- en: '[PRE41]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We are applying a regular expression to filter the log lines that match the
    expected format. We can now start to parse the raw data using a DataVec `RegexLineRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/regex/RegexLineRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/regex/RegexLineRecordReader.html)).
    We need to define a `regex` for formatting the lines, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用正则表达式来过滤出符合预期格式的日志行。现在我们可以开始使用DataVec的`RegexLineRecordReader`（[https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/regex/RegexLineRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/regex/RegexLineRecordReader.html)）来解析原始数据。我们需要定义一个`regex`来格式化这些行，如下所示：
- en: '[PRE42]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Through the DataVec-Spark library, it is also possible to check the quality
    of the data before defining the transformations. We can use the `AnalyzeSpark`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/spark/transform/AnalyzeSpark.html](https://deeplearning4j.org/datavecdoc/org/datavec/spark/transform/AnalyzeSpark.html))
    class for this purpose, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过DataVec-Spark库，还可以在定义转换之前检查数据质量。我们可以使用`AnalyzeSpark`（[https://deeplearning4j.org/datavecdoc/org/datavec/spark/transform/AnalyzeSpark.html](https://deeplearning4j.org/datavecdoc/org/datavec/spark/transform/AnalyzeSpark.html)）类来实现这一目的，如下所示：
- en: '[PRE43]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following is the output produced by the data quality analysis:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据质量分析产生的输出：
- en: '[PRE44]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'From this, we notice that on `139` lines (out of `1546`), the `replyBytes`
    field isn''t an integer as expected. Here are a couple of those lines:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们注意到，在`139`行（共`1546`行）中，`replyBytes`字段并不是预期的整数类型。以下是其中几行：
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'So, the first transformation to do is to clean up the `replyBytes` field by
    replacing any non-integer entries with the value `0`. We use the `TransformProcess`
    class as for the example in the *Data ingestion through DataVec and transformation
    through Spark* section, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一个要进行的转换是清理`replyBytes`字段，将所有非整数条目替换为`0`。我们使用`TransformProcess`类，方法与*通过DataVec进行数据摄取和通过Spark进行转换*部分中的示例相同，如下所示：
- en: '[PRE46]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, we can apply any other transformation, for example, grouping by host
    and pulling out summary metrics (count the number of entries, count the number
    of unique requests and HTTP reply codes, total the values in the `replyBytes`
    field); for example:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以应用其他任何转换，例如按主机分组并提取汇总指标（计算条目数量、计算唯一请求和HTTP回复代码的数量、对`replyBytes`字段的值求和）；例如：
- en: '[PRE47]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Rename a number of columns, as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 重命名若干列，如下所示：
- en: '[PRE48]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Filter out all hosts that requested fewer than 1 million bytes in total, as
    follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选出所有请求的字节总数少于100万的主机，如下所示：
- en: '[PRE49]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can now execute the transformations, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以执行转换，如下所示：
- en: '[PRE50]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can also perform some analysis on the final data, as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对最终数据进行一些分析，如下所示：
- en: '[PRE51]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The final data schema is shown as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最终数据模式如下所示：
- en: '[PRE52]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following shows that the result count is two:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示结果计数为二：
- en: '[PRE53]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following code shows the result of the analysis:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了分析结果：
- en: '[PRE54]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored different ways of ingesting data from files, relational
    and NoSQL databases, and S3-based object storage systems using the DeepLearning4j
    DataVec library and the Apache Spark (core and Spark SQL modules) framework, and
    showed some examples of how to transform the raw data. All of the examples presented represent
    data ingestion and transformation in a batch fashion.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了通过DeepLearning4j DataVec库和Apache Spark（核心模块和Spark SQL模块）框架从文件、关系型数据库、NoSQL数据库和基于S3的对象存储系统摄取数据的不同方式，并展示了一些如何转换原始数据的示例。所有呈现的示例代表了批处理方式的数据摄取和转换。
- en: The next chapter will focus on ingesting and transforming data to train or test
    your DL model in streaming mode.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将专注于摄取和转换数据，以在流模式下训练或测试您的DL模型。
