- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Enhancing NLP Tasks Using LLMs with spacy-llm
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spacy-llm增强NLP任务
- en: In this chapter, we will build upon the knowledge gained in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087)
    , and explore how to integrate **large language models** ( **LLMs** ) into spaCy
    pipelines using the **spacy-llm** library. We will start by understanding the
    basics of LLMs and prompt engineering, and how these powerful models can be leveraged
    to perform a wide range of NLP tasks within spaCy. We’ll demonstrate how to configure
    and use pre-built LLM tasks such as text summarization, and then take a step further
    by creating a custom task to extract contextual information from text. This will
    involve using Jinja templates for prompt creation and writing custom spaCy components
    that can efficiently handle complex NLP tasks. By the end of this chapter, you
    will have a deeper understanding of how to enhance traditional NLP pipelines with
    the flexibility and power of LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将基于在[*第6章*](B22441_06.xhtml#_idTextAnchor087)中获得的知识，探讨如何使用**spacy-llm**库将**大型语言模型**（**LLMs**）集成到spaCy管道中。我们将从理解LLMs和提示工程的基础知识开始，以及这些强大的模型如何在spaCy中执行各种NLP任务。我们将演示如何配置和使用预构建的LLM任务，如文本摘要，然后进一步创建一个自定义任务，从文本中提取上下文信息。这涉及到使用Jinja模板创建提示，并编写自定义spaCy组件，以高效地处理复杂的NLP任务。到本章结束时，你将更深入地了解如何利用LLMs的灵活性和强大功能来增强传统的NLP管道。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: LLMs and prompt engineering basics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs和提示工程基础知识
- en: Text summarization with LLMs and spaCy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs和spaCy进行文本摘要
- en: Creating custom LLM tasks with Jinja template
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Jinja模板创建自定义LLM任务
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be working with spaCy and the **spacy-llm** library
    to create and run our pipelines. You can find the code used in this chapter at
    [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用spaCy和**spacy-llm**库来创建和运行我们的管道。你可以在此章节中找到使用的代码：[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)。
- en: LLMs and prompt engineering basics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs和提示工程基础知识
- en: As we saw in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) , **language modeling**
    is the task of predicting the next token given the sequence of previous tokens.
    The example we used was that given the sequence of words **Yesterday I visited
    a** , a language model can predict the next token to be something such as **church**
    , **hospital** , **school** , and so on. Conventional language models are usually
    trained in a supervised manner to perform a specific task. **Pre-trained language
    models** ( **PLM** ) are trained in a self-supervised manner, with the aim of
    learning a generic representation of the language. These PLM models are then fine-tuned
    to perform a specific downstream task. This self-supervised pre-training made
    PLM models much more powerful than regular language models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第6章*](B22441_06.xhtml#_idTextAnchor087)中看到的，**语言建模**是预测给定先前标记序列的下一个标记的任务。我们使用的例子是，给定单词序列**昨天我访问了一个**，语言模型可以预测下一个标记可能是**教堂**、**医院**、**学校**等。传统的语言模型通常以监督方式训练以执行特定任务。**预训练语言模型**（**PLM**）以自监督方式训练，目的是学习语言的通用表示。然后对这些PLM模型进行微调以执行特定的下游任务。这种自监督的预训练使PLM模型比常规语言模型更强大。
- en: The LLMs are an evolution of PLMs that have many more model parameters and larger
    training datasets. The GPT-3 model, for example, has 175B parameters. Its successor,
    GPT3.5, was the base for the ChatGPT model released in November 2022. LLMs can
    serve as general-purpose tools, capable of tasks from language translation to
    coding assistance. Their ability to understand and generate human-like text has
    led to impactful applications in medicine, education, science, math, law, and
    more. In medicine, LLMs support doctors with evidence-based recommendations and
    enhance patient interactions. In education, they customize learning experiences
    and assist teachers in creating content. In science, LLMs speed up research and
    scientific writing. In law, they analyze legal documents and clarify complex terms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）是PLMs（预训练语言模型）的进化，拥有更多的模型参数和更大的训练数据集。例如，GPT-3模型拥有1750亿个参数。其继任者GPT3.5是2022年11月发布的ChatGPT模型的基础。LLMs可以作为通用工具，能够执行从语言翻译到编码辅助的各种任务。它们理解和生成类似人类文本的能力，在医学、教育、科学、数学、法律等领域产生了重大影响。在医学领域，LLMs为医生提供基于证据的建议，并增强患者互动。在教育领域，它们定制学习体验，并协助教师创建内容。在科学领域，LLMs加速研究和科学写作。在法律领域，它们分析法律文件并阐明复杂术语。
- en: 'We can also use LLMs for regular NLP tasks such as **Named Entity Recognition**
    ( **NER** ), text categorization, and text summarization. Basically, these models
    can do almost anything we ask them to. But this doesn’t come for free since training
    them requires extensive computational resources, and the large number of layers
    and parameters make them produce answers much more slowly than non-LLM models.
    LLMs can also **hallucinate** : produce responses that at first seem plausible
    but are in fact incorrect or not aligned with the facts or context. This phenomenon
    occurs because the models generate text based on patterns learned from their training
    data, rather than verifying information against an external source. As a result,
    they might create statements that sound reasonable but are misleading, inaccurate,
    or entirely fictional. Given all that, LLMs are useful but we should always analyze
    if they are the best solution to the project at hand.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用LLMs进行常规NLP任务，如**命名实体识别**（**NER**）、文本分类和文本摘要。基本上，这些模型可以完成我们要求的几乎所有事情。但这是有代价的，因为训练它们需要大量的计算资源，大量的层和参数使它们产生答案的速度比非LLM模型慢得多。LLMs也可能**产生幻觉**：产生看似合理但实际上不正确或与事实或上下文不一致的响应。这种现象发生是因为模型根据从训练数据中学习的模式生成文本，而不是通过外部来源验证信息。因此，它们可能会创建听起来合理但实际上具有误导性、不准确或完全虚构的陈述。鉴于所有这些，LLMs是有用的，但我们始终应该分析它们是否是手头项目的最佳解决方案。
- en: 'To interact with LLMs, we use prompts. **Prompts** should guide the models
    to generate answers or make the model take action. Prompts usually have these
    elements:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要与LLMs交互，我们使用提示。**提示**应引导模型生成答案或使模型采取行动。提示通常包含以下元素：
- en: '**Instruction** : The task you want the model to execute'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令**：您希望模型执行的任务'
- en: '**Context** : External information or additional context that should be useful
    to produce better answers'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文**：对产生更好答案有用的外部信息或附加上下文'
- en: '**Input data** : The input/question we want to be answered'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据**：我们想要得到答案的输入/问题'
- en: '**Output indicator** : The type of format we want to see as the model output'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出指示器**：我们希望模型输出的格式类型'
- en: With **spacy-llm** , we define prompts as tasks. When building spaCy pipelines
    with LLMs, each LLM component is defined using a **task** and a **model** . The
    **task** defines the prompt and functionality to parse the resulting response.
    The **model** defines the LLM model and how to connect to it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**spacy-llm**，我们将提示定义为任务。当使用LLMs构建spaCy管道时，每个LLM组件都使用一个**任务**和一个**模型**来定义。**任务**定义了提示和解析结果的提示和功能。**模型**定义了LLM模型以及如何连接到它。
- en: Now that you know what LLMs are and how to interact with them, let’s use a **spacy-llm**
    component in a pipeline. In the next section, we’re going to create a pipeline
    to summarize texts using an LLM.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了LLMs是什么以及如何与它们交互，让我们在管道中使用一个**spacy-llm**组件。在下一节中，我们将创建一个使用LLM来总结文本的管道。
- en: Text summarization with LLMs and spacy-llm
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLMs和spacy-llm进行文本摘要
- en: 'Each **spacy-llm** component has a task definition. spaCy has some pre-defined
    tasks, and we can also create own tasks. In this section, we’re going to use the
    **spacy.Summarization.v1** task. Each task is defined using a prompt. Here is
    the prompt for this task, available at [https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja](https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja)
    :'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 **spacy-llm** 组件都有一个任务定义。spaCy 有一些预定义的任务，我们也可以创建自己的任务。在本节中，我们将使用 **spacy.Summarization.v1**
    任务。每个任务都是通过一个提示来定义的。以下是该任务的提示，可在 [https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja](https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja)
    找到：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**spacy-llm** uses **Jinja** templates to define the instructions and examples.
    Jinja uses placeholders to insert data dynamically in the templates. The most
    common placeholders are **{{ }}** , **{% %}** , and **{# #}** . **{{ }}** is used
    to add variables or expressions, **{% %}** is used with flow control statements,
    and **{# #}** is used to add comments. Let’s see how some of these placeholders
    are used in the **spacy.Summarization.v1** template.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**spacy-llm** 使用 **Jinja** 模板来定义指令和示例。Jinja 使用占位符在模板中动态插入数据。最常见的占位符是 **{{ }}**
    ，**{% %}** 和 **{# #}** 。**{{ }}** 用于添加变量或表达式，**{% %}** 与流程控制语句一起使用，**{# #}** 用于添加注释。让我们看看这些占位符如何在
    **spacy.Summarization.v1** 模板中使用。'
- en: 'We can ask the model to output a summary with a certain maximum number of words.
    The default value for **max_n_words** is **null** . If this parameter is set in
    our config, the template will include this number:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以要求模型输出具有特定最大单词数的摘要。**max_n_words** 的默认值是 **null**。如果我们在配置中设置了此参数，模板将包括此数字：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Few-shot prompting** is a technique that consists of providing some examples
    (usually 1 to 5) of desirable inputs and outputs to show the model how we want
    the results. These examples help the model better understand the patterns without
    the need for fine-tuning with lots of examples. The **spacy.Summarization.v1**
    task has an **examples** parameter to generate few-shot examples.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Few-shot prompting** 是一种技术，它包括提供一些期望的输入和输出示例（通常为 1 到 5 个），以展示我们希望模型如何生成结果。这些示例有助于模型更好地理解模式，而无需使用大量示例进行微调。**spacy.Summarization.v1**
    任务有一个 **examples** 参数来生成少量示例。'
- en: 'Now that we know how the summarization template task works it’s time to work
    on the other element of **spacy-llm** , the **model** . We’r e going to use Anthropic’s
    **Claude 2** model. To make sure the credentials to connect to this model are
    available, you can open the console on your computer and run **export ANTHROPIC_API_KEY="..."**
    . Now let’s install the package with the **python3 -m pip install spacy-llm==0.7.2**
    command. We will load the pipeline using a **config.cfg** file (if you need a
    refresher, you can go back to the *Working with spaCy config.cfg files* section
    in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) ). Let’s build the configuration
    file:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了总结模板任务的工作原理，是时候开始处理 **spacy-llm** 的其他元素了，即 **模型**。我们将使用 Anthropic 的
    **Claude 2** 模型。为了确保连接到该模型的凭据可用，您可以在计算机上的控制台中运行 **export ANTHROPIC_API_KEY="..."**
    命令。现在让我们使用 **python3 -m pip install spacy-llm==0.7.2** 命令安装该包。我们将使用 **config.cfg**
    文件来加载管道（如果您需要复习，可以回到 [*第 6 章*](B22441_06.xhtml#_idTextAnchor087) 中的 *使用 spaCy
    config.cfg 文件* 部分）。让我们构建配置文件：
- en: 'First, we’ll define the **nlp** section, where we should define the language
    and the components of our pipeline. We’re only using the **llm** component:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义 **nlp** 部分，其中我们应该定义我们的管道的语言和组件。我们只使用 **llm** 组件：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, it’s time to specify the **components** section. To initialize the **llm**
    component, we’ll write **factory = "llm"** ; then, we will specify the task and
    the model:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候指定 **components** 部分。为了初始化 **llm** 组件，我们将写入 **factory = "llm"**；然后，我们将指定任务和模型：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To load this pipeline, we’ll pass the path to this config file to the **assemble**
    method from **spacy_llm.util** . Let’s ask the model to summarize the *LLMs and
    prompt engineering basics* section of this chapter:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加载此管道，我们将通过 **spacy_llm.util** 的 **assemble** 方法传递此配置文件的路径。让我们要求模型总结本章的 *LLMs
    和提示工程基础* 部分：
- en: '[PRE4]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The **spacy.Summarization.v1** task adds the summary to the **._.summary**
    extension attribute by default. Here is the response from the model:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**spacy.Summarization.v1** 任务默认将摘要添加到 **._.summary** 扩展属性中。以下是模型的响应：'
- en: '[PRE5]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Nice, right? You can check the other parameters to customize the task here [https://spacy.io/api/large-language-models#summarization-v1](https://spacy.io/api/large-language-models#summarization-v1)
    . Some of the other available **spacy-llm** tasks include **spacy.EntityLinker.v1**
    , **spacy.NER.v3** , **spacy.SpanCat.v3** , **spacy.TextCat.v3** , and **spacy.Sentiment.v1**
    . But beyond using these pre-built tasks, you can also create your own, which
    not only enhances the power of **spacy-llm** but also provides an organized way
    to follow best practices when building NLP pipelines. Let’s learn how to do that
    in the next section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，对吧？你可以检查其他参数来在此处自定义任务 [https://spacy.io/api/large-language-models#summarization-v1](https://spacy.io/api/large-language-models#summarization-v1)
    。一些其他可用的 **spacy-llm** 任务包括 **spacy.EntityLinker.v1** , **spacy.NER.v3** , **spacy.SpanCat.v3**
    , **spacy.TextCat.v3** , 和 **spacy.Sentiment.v1** 。但除了使用这些预构建的任务之外，你还可以创建自己的任务，这不仅增强了
    **spacy-llm** 的功能，而且为构建 NLP 管道时遵循最佳实践提供了一种有组织的方式。让我们在下一节中学习如何做到这一点。
- en: Creating custom spacy-llm tasks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义 spacy-llm 任务
- en: 'In this section, we’re going to create a task where, given a quote from [https://dummyjson.com/docs/quotes](https://dummyjson.com/docs/quotes)
    , the model should provide the context of the quote. Here’s an example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个任务，给定一个来自 [https://dummyjson.com/docs/quotes](https://dummyjson.com/docs/quotes)
    的引用，模型应提供引用的上下文。以下是一个示例：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first step of creating a custom **spacy-llm** task is to create the prompt
    and save it as a Jinja template. Here is the template for this task:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义 **spacy-llm** 任务的第一个步骤是创建提示并将其保存为 Jinja 模板。以下是此任务的模板：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We’ll save this in a file named **templates/quote_context_extract.jinja** .
    The next step is to create the class for the task. This class should implement
    two functions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个保存到名为 **templates/quote_context_extract.jinja** 的文件中。下一步是创建任务的类。这个类应该实现两个函数：
- en: '**generate_prompts(docs: Iterable[Doc]) -> Iterable[str]** : This function
    converts a list of spaCy **Doc** objects into a list of prompts.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**generate_prompts(docs: Iterable[Doc]) -> Iterable[str]** : 这个函数将 spaCy **Doc**
    对象列表转换为提示列表。'
- en: '**parse_responses(docs: Iterable[Doc], responses: Iterable[str]) -> Iterable[Doc]**
    : This function parses LLM outputs into a spaCy **Doc** object.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**parse_responses(docs: Iterable[Doc], responses: Iterable[str]) -> Iterable[Doc]**
    : 这个函数将 LLM 输出解析为 spaCy **Doc** 对象。'
- en: 'The **generate_prompts()** method will use our Jinja template and **parse_responses()**
    method will receive the model’s response and add the context extension attribution
    to our **Doc** . Let’s create the **QuoteContextExtractTask** class:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**generate_prompts()** 方法将使用我们的 Jinja 模板，而 **parse_responses()** 方法将接收模型的响应并添加上下文扩展属性到我们的
    **Doc** 对象。让我们创建 **QuoteContextExtractTask** 类：'
- en: 'First, we import all the methods we’ll need and set the directory for the templates:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所有需要的函数并设置模板的目录：
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let’s create a method to read the text of the Jinja template:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个方法来读取 Jinja 模板中的文本：
- en: '[PRE9]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we can start to create the **QuoteContextExtractTask** class. Let’s
    start creating the **__init__()** method. The class should be initiated with the
    name of the Jinja template and a field string to set the name of the extension
    attribute the task will add to the **Doc** object:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以开始创建 **QuoteContextExtractTask** 类。让我们从创建 **__init__()** 方法开始。这个类应该使用
    Jinja 模板的名称和一个字段字符串来设置任务将添加到 **Doc** 对象的扩展属性名称：
- en: '[PRE10]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we will create a method to build the prompt. Jinja uses an **Environment**
    object to load the templates from the file. We will use the **from_string()**
    method to build the template from text and yield it. Each time this method runs
    internally, it will render the template replacing the **{{text}}** variable of
    the template with the value of **doc.text** :'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '现在我们将创建一个方法来构建提示。Jinja 使用 **Environment** 对象从文件中加载模板。我们将使用 **from_string()**
    方法从文本构建模板并生成它。每次此方法内部运行时，它将渲染模板，用模板中的 **{{text}}** 变量的值替换模板的 **doc.text** :'
- en: '[PRE11]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now write the last method of the class. The **parse_responses()** method
    will add the response of the model to the **Doc** object. First, we create a helper
    method to add the extension attribute if it doesn’t exist. To set the extension
    attribute, we will use Python’s **setattr ()** method so we can dynamically set
    the attribute using the class field variable:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以编写类的最后一个方法。**parse_responses()** 方法将添加模型的响应到 **Doc** 对象。首先，我们创建一个辅助方法来添加扩展属性，如果它不存在。为了设置扩展属性，我们将使用
    Python 的 **setattr ()** 方法，这样我们就可以使用类字段变量动态设置属性：
- en: '[PRE12]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To use this class in our **config.cfg** file, we need to add the task to the
    **spacy-llm** **llm_tasks** register:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在**config.cfg**文件中使用这个类，我们需要将任务添加到**spacy-llm**的**llm_tasks**注册表中：
- en: '[PRE13]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And we’re done! Save this to a **quote.py** class. Here is the full code that
    should be in this script:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！将此保存到**quote.py**类中。以下是应该在这个脚本中的完整代码：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s test our custom task by first creating the **config_custom_task.cfg**
    file:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过首先创建**config_custom_task.cfg**文件来测试我们的自定义任务：
- en: '[PRE15]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can assemble the **nlp** object using this file and print out the
    context for a quote. Don’t forget to import the **QuoteContextExtractTask** from
    **quote.py** so spaCy knows where to load this from:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用这个文件组装**nlp**对象，并打印出引语的上下文。别忘了从**quote.py**导入**QuoteContextExtractTask**，这样spaCy就知道从哪里加载这个任务：
- en: '[PRE16]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this section, you’ve created a custom **spacy-llm** task to extract the context
    from a given quote. This approach not only allows you to craft highly specific
    NLP tasks tailored to your needs but also provides a structured way to integrate
    best practices in software engineering, such as modularity and reusability, into
    your NLP pipelines.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你创建了一个自定义的**spacy-llm**任务来从给定的引语中提取上下文。这种方法不仅允许你根据需求定制高度具体的NLP任务，而且还提供了一种结构化的方式将软件工程的最佳实践，如模块化和可重用性，集成到你的NLP管道中。
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how to leverage **large language models** ( **LLMs**
    ) within spaCy pipelines using the **spacy-llm** library. We reviewed the basics
    of LLMs and prompt engineering, emphasizing their role as versatile tools capable
    of performing a wide range of NLP tasks, from text categorization to summarization.
    However, we also noted the limitations of LLMs, such as their high computational
    cost and the tendency to hallucinate. We then demonstrated how to integrate LLMs
    into spaCy pipelines by defining tasks and models. Specifically, we implemented
    a summarization task and, subsequently, created a custom task to extract the context
    from a quote. This process involved creating Jinja templates for prompts and defining
    methods for generating and parsing responses.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何利用**大型语言模型**（**LLMs**）在spaCy管道中使用**spacy-llm**库。我们回顾了LLMs和提示工程的基础知识，强调它们作为多功能工具在执行各种NLP任务中的作用，从文本分类到摘要。然而，我们也指出了LLMs的局限性，例如它们的高计算成本和产生幻觉的倾向。然后，我们展示了如何通过定义任务和模型将LLMs集成到spaCy管道中。具体来说，我们实现了一个摘要任务，随后创建了一个自定义任务来从引语中提取上下文。这个过程涉及到创建提示的Jinja模板和定义生成和解析响应的方法。
- en: In the next chapter, we’re going back to more traditional machine learning and
    learn how to annotate data and train a pipeline component from scratch using spaCy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回到更传统的机器学习，学习如何使用spaCy从头开始标注数据和训练管道组件。
