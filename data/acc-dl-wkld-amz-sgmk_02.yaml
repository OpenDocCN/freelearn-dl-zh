- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Deep Learning Frameworks and Containers on SageMaker
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker 上的深度学习框架和容器
- en: Amazon SageMaker supports many popular ML and DL frameworks. Framework support
    in SageMaker is achieved using prebuilt Docker containers for inference and training
    tasks. Prebuilt SageMaker containers provide a great deal of functionality, and
    they allow you to implement a wide range of use cases with minimal coding. There
    are also real-life scenarios where you need to have a custom, runtime environment
    for training and/or inference tasks. To address these cases, SageMaker provides
    a flexible **Bring-Your-Own** (**BYO**) container feature.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 支持许多流行的机器学习（ML）和深度学习（DL）框架。SageMaker 中的框架支持通过使用预构建的 Docker
    容器来实现推理和训练任务。预构建的 SageMaker 容器提供了大量的功能，并允许你在最少的编码工作下实现广泛的用例。也有一些实际场景，需要为训练和/或推理任务提供定制的运行时环境。为了解决这些情况，SageMaker
    提供了灵活的**自带容器**（**BYO**）功能。
- en: In this chapter, we will review key supported DL frameworks and corresponding
    container images. Then, we will focus our attention on the two most popular DL
    frameworks, TensorFlow and PyTorch, and learn how to use them in Amazon SageMaker.
    Additionally, we will review a higher-level, state-of-the-art framework, Hugging
    Face, for NLP tasks, and its implementation for Amazon SageMaker.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾支持的主要深度学习框架及其对应的容器镜像。然后，我们将重点关注两个最流行的深度学习框架——TensorFlow 和 PyTorch，并学习如何在
    Amazon SageMaker 中使用它们。此外，我们还将回顾一个更高层次的、用于自然语言处理任务的最先进框架 Hugging Face，以及它在 Amazon
    SageMaker 上的实现。
- en: Then, we will understand how to use and extend prebuilt SageMaker containers
    based on your use case requirements, as well as learning about the SageMaker SDK
    and toolkits, which simplify writing training and inference scripts that are compatible
    with Amazon SageMaker.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将理解如何根据你的用例需求使用和扩展预构建的 SageMaker 容器，以及了解 SageMaker SDK 和工具包，这些工具简化了编写与
    Amazon SageMaker 兼容的训练和推理脚本的过程。
- en: In later sections, we will dive deeper into how to decide whether to use prebuilt
    SageMaker containers or BYO containers. Then, we will develop a SageMaker-compatible
    BYO container.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们将深入探讨如何决定是使用预构建的 SageMaker 容器还是 BYO 容器。接着，我们将开发一个与 SageMaker 兼容的 BYO
    容器。
- en: 'These topics will be covered in the following sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将涵盖这些主题：
- en: Exploring DL frameworks on SageMaker
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 SageMaker 上的深度学习框架
- en: Using SageMaker DL containers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker DL 容器
- en: Developing BYO containers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发 BYO 容器
- en: By the end of this chapter, you will be able to decide which container strategy
    to choose based on your specific problem requirements and chosen DL framework.
    Additionally, you will understand the key aspects of training and inference script
    development, which are compatible with Amazon SageMaker.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够根据具体问题要求和选定的深度学习框架，决定选择哪种容器策略。此外，你将理解与 Amazon SageMaker 兼容的训练和推理脚本开发的关键方面。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In the *Using SageMaker DL containers* and *Developing BYO containers* sections,
    we will provide walk-through code samples, so you can develop practical skills.
    Full code examples are available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用 SageMaker DL 容器*和*开发 BYO 容器*部分，我们将提供操作代码示例，以帮助你培养实践技能。完整的代码示例可以在 [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/)
    查看。
- en: 'To follow along with this code, you will need the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随这段代码，你需要以下内容：
- en: An AWS account and IAM user with the permissions to manage Amazon SageMaker
    resources.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 AWS 账户和具有管理 Amazon SageMaker 资源权限的 IAM 用户。
- en: Python 3 and the SageMaker SDK ([https://pypi.org/project/sagemaker/](https://pypi.org/project/sagemaker/))
    installed on your development machine.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的开发机器上安装 Python 3 和 SageMaker SDK ([https://pypi.org/project/sagemaker/](https://pypi.org/project/sagemaker/))。
- en: Docker installed on your development machine.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的开发机器上安装 Docker。
- en: To use SageMaker P2 instances for training purposes, you will likely need to
    request a service limit increase on your AWS account. For more details, please
    view [https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若要使用 SageMaker P2 实例进行训练，你可能需要请求 AWS 账户的服务限制增加。有关更多详情，请查看 [https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml)。
- en: Exploring DL frameworks on SageMaker
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 SageMaker 上的深度学习框架
- en: 'At the time of writing this book, Amazon SageMaker supports the following frameworks,
    where DL frameworks are marked with an asterisk:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Amazon SageMaker 支持以下框架，其中以星号标记的为深度学习框架：
- en: scikit-learn
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: SparkML Serving
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkML 服务
- en: Chainer*
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chainer*
- en: Apache MXNet*
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache MXNet*
- en: Hugging Face*
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face*
- en: PyTorch*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch*
- en: TensorFlow*
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow*
- en: Reinforcement learning containers – including TensorFlow- and PyTorch-enabled
    containers
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习容器 – 包括支持 TensorFlow 和 PyTorch 的容器
- en: XGBoost
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: The preceding list of supported frameworks could change in the future. Be sure
    to check the official SageMaker documentation at [https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上述支持的框架列表可能会在未来发生变化。请务必查看官方 SageMaker 文档：[https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml)。
- en: 'In this book, we will primarily focus on the two most popular choices: **TensorFlow**
    and **PyTorch**. Both are open source frameworks with a large and vibrant communities.
    Depending on the specific use case or model architecture, one or the other framework
    might have a slight advantage. However, it’s safe to assume that both frameworks
    are comparable in terms of features and performance. In many practical scenarios,
    the choice between TensorFlow or PyTorch is made based on historical precedents
    or individual preferences.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将主要聚焦于两种最受欢迎的选择：**TensorFlow** 和 **PyTorch**。这两者都是开源框架，拥有庞大且充满活力的社区。根据具体的使用场景或模型架构，某一个框架可能会略有优势。然而，可以认为这两个框架在功能和性能上是相当的。在许多实际场景中，选择
    TensorFlow 或 PyTorch 通常是基于历史惯例或个人偏好。
- en: Another framework that we will discuss in this book is **Hugging Face**. This
    is a high-level framework that provides access to SOTA models, training, and inference
    facilities for NLP tasks (such as text classification, translation, and more).
    Hugging Face is a set of several libraries (transformers, datasets, tokenizers,
    and accelerate) designed to simplify building SOTA NLP models. Under the hood,
    Hugging Face libraries use TensorFlow and PyTorch primitives (collectively known
    as “backends”) to perform computations. Users can choose which backend to use
    based on specific runtime requirements. Given its popularity, Amazon SageMaker
    has recently added support for the Hugging Face libraries in separate prebuilt
    containers for training and inference tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们还将讨论另一个框架：**Hugging Face**。这是一个高层次框架，提供了用于 NLP 任务（如文本分类、翻译等）的 SOTA 模型、训练和推理设施。Hugging
    Face 是一组旨在简化构建 SOTA NLP 模型的多个库（transformers、datasets、tokenizers 和 accelerate）。在底层，Hugging
    Face 库使用 TensorFlow 和 PyTorch 的基础组件（统称为“后端”）来执行计算。用户可以根据特定的运行时需求选择使用哪一个后端。鉴于其流行性，Amazon
    SageMaker 最近为 Hugging Face 库提供了支持，提供了分别用于训练和推理任务的预构建容器。
- en: Container sources
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 容器源
- en: Sources of SageMaker DL containers are available on the public GitHub repository
    at [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers).
    In certain cases, it can be helpful to review relevant Dockerfiles to understand
    the runtime configuration of prebuilt containers. Container images are available
    in AWS public registries at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 深度学习容器的源代码可以在 [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)
    的公开 GitHub 仓库中找到。在某些情况下，查看相关的 Dockerfile 可以帮助你了解预构建容器的运行时配置。容器镜像可以在 AWS 公开注册表中找到，地址是
    [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)。
- en: 'For each of the supported frameworks, SageMaker provides separate training
    and inference containers. We have separate containers for these two tasks because
    of the following considerations:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个支持的框架，SageMaker 提供了分别用于训练和推理的容器。我们为这两个任务提供了不同的容器，考虑因素如下：
- en: Training and inference tasks might have different runtime requirements. For
    example, you might choose to run your training and inference tasks on different
    compute platforms. This will result in different sets of accelerators and performance
    optimization tweaks in your container, depending on your specific task.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和推理任务可能有不同的运行时要求。例如，您可能会选择在不同的计算平台上运行训练和推理任务。根据您的具体任务，这将导致容器中使用不同的加速器和性能优化调整。
- en: Training and inference tasks require different sets of auxiliary scripts; for
    instance, standing up a model server in the case of inference tasks. Not separating
    training and inference containers could result in bloated container sizes and
    intricate APIs.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和推理任务需要不同的辅助脚本；例如，在推理任务中启动模型服务器。如果不将训练和推理容器分开，可能会导致容器体积过大和 API 变得复杂。
- en: For this reason, we will always explicitly identify the container we are using
    depending on the specific task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们总是根据具体任务明确标识所使用的容器。
- en: Specific to DL containers, AWS also defines separate GPU-based and CPU-based
    containers. GPU-based containers require the installation of additional accelerators
    to be able to run computations on GPU devices (such as the CUDA toolkit).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 DL 容器，AWS 还定义了独立的基于 GPU 和基于 CPU 的容器。基于 GPU 的容器需要安装额外的加速器，以便能够在 GPU 设备上运行计算（如
    CUDA 工具包）。
- en: Model requirements
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型要求
- en: When choosing a SageMaker DL container, always consider the model requirements
    for compute resources. For the majority of SOTA models, it’s recommended that
    you use GPU-based compute instances to achieve acceptable performance. Choose
    your DL container accordingly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 SageMaker DL 容器时，始终考虑模型对计算资源的要求。对于大多数 SOTA（最先进的）模型，建议使用基于 GPU 的计算实例，以实现可接受的性能。根据这一点选择适合的
    DL 容器。
- en: TensorFlow containers
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 容器
- en: 'A TensorFlow container has two major versions: 1.x (maintenance mode) and 2.x
    (the latest version). Amazon SageMaker supports both versions and provides inference
    and training containers. In this book, all of the code examples and general commentary
    are done assuming TensorFlow v2.x.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 容器有两个主要版本：1.x（维护模式）和 2.x（最新版本）。Amazon SageMaker 支持这两个版本，并提供推理和训练容器。本书中的所有代码示例和一般评论都假设使用
    TensorFlow v2.x。
- en: AWS updates with frequently supported minor TensorFlow versions. The latest
    supported major version is 2.10.0.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 会频繁更新支持的小版本 TensorFlow。最新支持的主版本是 2.10.0。
- en: PyTorch containers
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 容器
- en: Amazon SageMaker provides inference and training containers for PyTorch. The
    latest version is 1.12.1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供了用于 PyTorch 的推理和训练容器。最新版本是 1.12.1。
- en: Hugging Face containers
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face 容器
- en: 'AWS provides Hugging Face containers in two flavors: PyTorch and TensorFlow
    backends. Each backend has separate training and inference containers.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了两种版本的 Hugging Face 容器：PyTorch 和 TensorFlow 后端。每个后端都有独立的训练和推理容器。
- en: Using SageMaker Python SDK
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Python SDK
- en: 'AWS provides a convenient Python SDK that simplifies interactions with supported
    DL frameworks via the Estimator, Model, and Predictor classes. Each supported
    framework has a separate module with the implementation of respective classes.
    For example, here is how you import Predict, Estimator, and Model classes for
    the PyTorch framework:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了一个方便的 Python SDK，它通过 Estimator、Model 和 Predictor 类简化了与支持的 DL 框架的交互。每个支持的框架都有一个单独的模块，包含各自类的实现。例如，下面是如何为
    PyTorch 框架导入 Predict、Estimator 和 Model 类：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following diagram shows the SageMaker Python SDK workflow:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 SageMaker Python SDK 的工作流程：
- en: '![Figure 2.1 – How SageMaker Python SDK works with image URIs  ](img/B17519_02_01.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – SageMaker Python SDK 如何与镜像 URI 配合使用](img/B17519_02_01.jpg)'
- en: Figure 2.1 – How SageMaker Python SDK works with image URIs
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – SageMaker Python SDK 如何与镜像 URI 配合使用
- en: 'To build a better intuition, let’s do a quick example of how to run a training
    job using a PyTorch container with a specific version using SageMaker Python SDK.
    For a visual overview, please refer to *Figure 2.1*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们做一个简单的示例，展示如何使用 SageMaker Python SDK 使用特定版本的 PyTorch 容器运行训练作业。有关视觉概述，请参见
    *图 2.1*：
- en: 'First, we decide which framework to use and import the respective `Pytorch`
    `estimator` class:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们决定使用哪个框架，并导入相应的 `Pytorch` `estimator` 类：
- en: '[PRE1]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When instantiating the PyTorch `estimator` object, we need to provide several
    more parameters including the framework version and the Python version:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化 PyTorch `estimator` 对象时，我们需要提供更多参数，包括框架版本和 Python 版本：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When executing this code, SageMaker Python SDK automatically validates user
    input, including the framework version and the Python version. If the requested
    container exists, then SageMaker Python SDK retrieves the appropriate container
    image URI. If there is no container with the requested parameters, SageMaker Python
    SDK will throw an exception.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行此代码时，SageMaker Python SDK 会自动验证用户输入，包括框架版本和 Python 版本。如果请求的容器存在，SageMaker
    Python SDK 将检索相应的容器镜像 URI。如果没有符合请求参数的容器，SageMaker Python SDK 会抛出异常。
- en: 'During the `fit()` call, a correct container image URI will be provided to
    the SageMaker API, so the training job will be running inside the SageMaker container
    with PyTorch v1.8 and Python v3.7 installed. Since we are requesting a GPU-based
    instance, a training container with the CUDA toolkit installed will be used:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `fit()` 调用过程中，正确的容器镜像 URI 会被提供给 SageMaker API，因此训练作业将运行在安装了 PyTorch v1.8 和
    Python v3.7 的 SageMaker 容器中。由于我们请求的是基于 GPU 的实例，将使用安装了 CUDA 工具包的训练容器：
- en: '[PRE3]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using custom images
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义镜像
- en: Please note that if, for some reason, you would prefer to provide a direct URI
    to your container image, you can do it using the `image_uri` parameter that is
    supported by the `model` and `estimator` classes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果由于某些原因您更倾向于提供容器镜像的直接 URI，您可以通过 `image_uri` 参数来实现，该参数在 `model` 和 `estimator`
    类中受支持。
- en: Now, let’s take a deep dive into SageMaker DL containers, starting with the
    available prebuilt containers for the TensorFlow, PyTorch, and Hugging Face frameworks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解 SageMaker DL 容器，从可用的 TensorFlow、PyTorch 和 Hugging Face 框架的预构建容器开始。
- en: Using SageMaker DL containers
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker DL 容器
- en: Amazon SageMaker supports several container usage patterns. Also, it provides
    you with Training and Inference Toolkits that simplify using prebuilt containers
    and developing BYO containers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 支持多种容器使用模式。此外，它为您提供了训练和推理工具包，以简化使用预构建容器和开发 BYO 容器的过程。
- en: In this section, we will learn how to choose the most efficient container usage
    pattern for your use case and how to use the available SageMaker toolkits to implement
    it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何选择最有效的容器使用模式来满足您的用例需求，并如何使用可用的 SageMaker 工具包来实现它。
- en: Container usage patterns
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器使用模式
- en: Amazon SageMaker provides you with the flexibility to choose whether to use
    prebuilt containers “as is” (known as **Script Mode**), **BYO containers**, or
    modify prebuilt containers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供了灵活性，允许您选择是否使用预构建容器的“原样”（称为 **脚本模式**）、**BYO 容器**，或修改预构建容器。
- en: Typically, the choice of approach is driven by specific model runtime requirements,
    available resources, and engineering expertise. In the next few subsections, we
    will discuss when to choose one approach over another.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，方法的选择取决于特定的模型运行时需求、可用资源以及工程专业知识。在接下来的几个子章节中，我们将讨论何时选择一种方法而不是另一种方法。
- en: Script Mode
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚本模式
- en: In script mode, you define which prebuilt container you’d like to use and then
    provide one or more scripts with the implementation of your training or inference
    logic. Additionally, you can provide any other dependencies (proprietary or public)
    that will be exported to the containers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本模式下，您需要定义要使用的预构建容器，然后提供一个或多个脚本来实现您的训练或推理逻辑。此外，您还可以提供任何其他依赖项（无论是专有的还是公共的），这些依赖项将被导出到容器中。
- en: Both training and inference containers in script mode come with preinstalled
    toolkits that provide common functionality such as downloading data to containers
    and model artifacts, starting jobs, and others. We will look at further details
    of the SageMaker **Inference Toolkit** and **Training Toolkit** later in this
    chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本模式下的训练和推理容器都预装了工具包，提供了常见功能，如将数据下载到容器和模型工件、启动作业等。我们将在本章稍后详细介绍 SageMaker **推理工具包**和
    **训练工具包**。
- en: 'Script Mode is suitable for the following scenarios:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本模式适用于以下场景：
- en: Prebuilt containers satisfy your runtime requirements, or you can install any
    dependencies without needing to rebuild the container
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预构建容器满足您的运行时需求，或者您可以安装任何依赖项，而无需重建容器
- en: You want to minimize the time spent on developing and testing your containers
    or you don’t have the required expertise to do so
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望最小化开发和测试容器所花费的时间，或者您没有所需的专业知识来进行此操作
- en: In the following sections, we will review how to prepare your first training
    and inference scripts and run them on SageMaker in script mode.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾如何准备您的第一个训练和推理脚本，并以脚本模式在 SageMaker 上运行它们。
- en: Modifying prebuilt containers
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改预构建容器
- en: Another way to use SageMaker’s prebuilt containers is to modify them. In this
    case, you will use one of the prebuilt containers as a base image for your custom
    container.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SageMaker 预构建容器的另一种方式是对其进行修改。在这种情况下，您将使用其中一个预构建容器作为自定义容器的基础镜像。
- en: 'Modifying prebuilt containers can be beneficial in the following scenarios:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下场景中，修改预构建容器可能会带来好处：
- en: You need to add additional dependencies (for instance, ones that need to be
    compiled from sources) or reconfigure the runtime environment
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要添加额外的依赖项（例如，必须从源代码编译的依赖项）或重新配置运行时环境
- en: You want to minimize the development and testing efforts of your container and
    rely for the most part on the functionality of the base container tested by AWS
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望最小化容器的开发和测试工作，并在大部分情况下依赖 AWS 测试过的基础容器的功能
- en: 'Please note that when you extend a prebuilt container, you will be responsible
    for the following aspects:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当你扩展预构建容器时，你需要负责以下几个方面：
- en: Creating the Dockerfile with the implementation of your runtime environment
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建包含运行时环境实现的 Dockerfile
- en: Building and storing your container in a Container registry such as **Amazon
    Elastic Container Registry** (**ECR**) or private Docker registries
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在容器注册中心（例如**Amazon Elastic Container Registry**（**ECR**）或私有 Docker 注册中心）中构建并存储容器
- en: Later in this chapter, we see an example of how to extend a prebuilt PyTorch
    container for a training task.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章稍后的部分，我们将看到如何扩展一个预构建的 PyTorch 容器来执行训练任务的示例。
- en: BYO containers
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自带容器（BYO containers）
- en: 'There are many scenarios in which you might need to create a custom container,
    such as the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要创建自定义容器的许多场景，包括以下几种：
- en: You have unique runtime requirements that cannot be addressed by extending the
    prebuilt container
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有独特的运行时需求，这些需求无法通过扩展预构建容器来解决
- en: You want to compile frameworks and libraries from sources for specific hardware
    platforms
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望为特定的硬件平台编译框架和库
- en: You are using DL frameworks that are not supported natively by SageMaker (for
    instance, JAX)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你使用的深度学习框架是 SageMaker 本地不支持的（例如，JAX）
- en: Building a custom container compatible with SageMaker inference and training
    resources requires development efforts, an understanding of Docker containers,
    and specific SageMaker requirements. Therefore, it’s usually recommended that
    you consider script mode or extending a prebuilt container first and choose to
    use a BYO container only if the first options do not work for your particular
    use case.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个与 SageMaker 推理和训练资源兼容的自定义容器需要开发工作、对 Docker 容器的理解以及特定的 SageMaker 要求。因此，通常建议首先考虑脚本模式或扩展一个预构建容器，只有在前两者无法满足你的具体用例时，才选择使用自带容器（BYO）。
- en: SageMaker toolkits
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 工具包
- en: To simplify the development of custom scripts and containers that are compatible
    with Amazon SageMaker, AWS created Python toolkits for training and inference
    tasks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化与 Amazon SageMaker 兼容的自定义脚本和容器的开发，AWS 创建了用于训练和推理任务的 Python 工具包。
- en: 'Toolkits provide the following benefits:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 工具包提供以下好处：
- en: Establish consistent runtime environments and locations for storing code assets
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一致的运行时环境和存储代码资产的目录位置
- en: '`ENTRYPOINT` scripts to run tasks when the container is started'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ENTRYPOINT` 脚本，用于容器启动时执行任务'
- en: Understanding these toolkits helps to simplify and speed up the development
    of SageMaker-compatible containers, so let’s review them in detail.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些工具包有助于简化并加速与 SageMaker 兼容容器的开发，因此让我们详细回顾一下它们。
- en: The Training Toolkit
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练工具包
- en: 'The SageMaker Training Toolkit has several key functions:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 训练工具包有几个关键功能：
- en: 'It establishes a consistent runtime environment, setting environment variables
    and a directory structure to store the input and output artifacts of model training:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它建立了一个一致的运行时环境，设置了环境变量和一个目录结构来存储模型训练的输入和输出结果：
- en: '![Figure 2.2 – The directory structure in SageMaker-compatible containers ](img/B17519_02_02.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – SageMaker 兼容容器中的目录结构](img/B17519_02_02.jpg)'
- en: Figure 2.2 – The directory structure in SageMaker-compatible containers
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – SageMaker 兼容容器中的目录结构
- en: 'The Training Toolkit sets up the following directories in the training container:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工具包会在训练容器中设置以下目录：
- en: The `/opt/ml/input/config` directory with the model hyperparameters and the
    network layout used for distributed training as JSON files.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/opt/ml/input/config` 目录，存放模型超参数和用于分布式训练的网络布局（以 JSON 文件形式）。'
- en: The `/opt/ml/input/data` directory with input data when S3 is used as data storage.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 S3 作为数据存储时，`/opt/ml/input/data` 目录存放输入数据。
- en: The `/opt/ml/code/` directory, containing code assets to run training job.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/opt/ml/code/` 目录，包含用于运行训练任务的代码资产。'
- en: The `/opt/ml/model/` directory, containing the resulting model; SageMaker automatically
    copies it to Amazon S3 after training completion.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/opt/ml/model/` 目录，包含生成的模型；SageMaker 在训练完成后会自动将其复制到 Amazon S3。'
- en: It executes the entrypoint script and handles success and failure statuses.
    In the case of a training job failure, the output will be stored in `/opt/ml/output/failure`.
    For successful executions, the toolkit will write output to the `/opt/ml/success`
    directory.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它执行入口脚本并处理成功和失败状态。如果训练任务失败，输出将存储在`/opt/ml/output/failure`。对于成功的执行，工具包会将输出写入`/opt/ml/success`目录。
- en: 'By default, all prebuilt training containers already have a training toolkit
    installed. If you wish to use it, you will need to install it on your container
    by running the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有预构建的训练容器都已安装训练工具包。如果你希望使用它，你需要通过运行以下命令将其安装到容器中：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Also, you will need to copy all of the code dependencies into your container
    and define a special environmental variable in your main training script, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你需要将所有代码依赖项复制到容器中，并在主训练脚本中定义一个特殊的环境变量，如下所示：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The training toolkit package is available in the PyPI ([pypi.org](http://pypi.org))
    package and the SageMaker GitHub repository ([https://github.com/aws/sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工具包可在PyPI（[pypi.org](http://pypi.org)）包和SageMaker的GitHub仓库（[https://github.com/aws/sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit)）中找到。
- en: Inference Toolkit
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理工具包
- en: 'The **Inference Toolkit** implements a model serving stack that is compatible
    with SageMaker inference services. It comes together with an open source **Multi-Model
    Server** (**MMS**) to serve models. It has the following key functions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理工具包**实现了一个与SageMaker推理服务兼容的模型服务堆栈。它与开源的**多模型服务器**（**MMS**）一起，用于服务模型。它具有以下关键功能：'
- en: To establish runtime environments, such as directories to store input and output
    artifacts of inference and environmental variables. The directory structure follows
    the layout of the training container.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于建立运行时环境，例如存储推理输入和输出工件的目录和环境变量。目录结构遵循训练容器的布局。
- en: To implement a handler service that is called from the model server to load
    the model into memory, and handle model inputs and outputs.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个处理程序服务，该服务由模型服务器调用，将模型加载到内存中，并处理模型输入和输出。
- en: To implement default serializers and deserializers to handle inference requests.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现默认的序列化器和反序列化器来处理推理请求。
- en: The Inference Toolkit package is available in the PyPi ([pypi.org](http://pypi.org))
    package and the GitHub repository ([https://github.com/aws/sagemaker-inference-toolkit](https://github.com/aws/sagemaker-inference-toolkit)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 推理工具包可在PyPi（[pypi.org](http://pypi.org)）包和GitHub仓库（[https://github.com/aws/sagemaker-inference-toolkit](https://github.com/aws/sagemaker-inference-toolkit)）中找到。
- en: Developing for script mode
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发脚本模式
- en: Now that we have an understanding of SageMaker’s container ecosystem, let’s
    implement several learning projects to build practical skills. In this first example,
    we will use SageMaker script mode to train our custom NLP model and deploy it
    for inference.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对SageMaker的容器生态系统有了理解，接下来让我们实现几个学习项目来培养实际技能。在这个第一个示例中，我们将使用SageMaker脚本模式来训练我们的自定义NLP模型并部署它进行推理。
- en: Problem overview
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题概述
- en: In this example, we will learn how to develop training and inference scripts
    using the Hugging Face framework. We will leverage prebuilt SageMaker containers
    for Hugging Face (with the PyTorch backend).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何使用Hugging Face框架开发训练和推理脚本。我们将利用SageMaker为Hugging Face（具有PyTorch后端）预构建的容器。
- en: 'We chose to solve a typical NLP task: text classification. We will use the
    `20 Newsgroups` dataset, which assembles ~20,000 newsgroup documents across 20
    different newsgroups (categories). There are a number of model architectures that
    can address this task. Usually, current SOTA models are based on Transformer architecture.
    Autoregressive models such as **BERT** and its various derivatives are suitable
    for this task. We will use a concept known as **transfer learning**, where a model
    that is pretrained for one task is used for a new task with minimal modifications.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了解决一个典型的NLP任务：文本分类。我们将使用`20 Newsgroups`数据集，它汇集了约20,000篇新闻组文档，涵盖20个不同的新闻组（类别）。有多种模型架构可以解决这个任务。通常，当前的SOTA模型基于Transformer架构。像**BERT**及其各种衍生模型这样的自回归模型适合这个任务。我们将使用一种被称为**迁移学习**的概念，其中一个为某个任务预训练的模型可以通过最小修改用于新任务。
- en: 'As a baseline model, we will use model architecture known as **DistilBERT**,
    which provides high accuracy on a wide variety of tasks and is considerably smaller
    than other models (for instance, the original BERT model). To adapt the model
    for a classification task, we would need to add a classification layer, which
    will be trained during our training to recognize articles:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准模型，我们将使用被称为**DistilBERT**的模型架构，它在多种任务中提供了高准确度，并且比其他模型（例如原始的BERT模型）要小得多。为了将该模型适应分类任务，我们需要添加一个分类层，在我们的训练过程中，该层将被训练以识别文章：
- en: '![Figure 2.3 – The model architecture for the text classification task ](img/B17519_02_03.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 文本分类任务的模型架构](img/B17519_02_03.jpg)'
- en: Figure 2.3 – The model architecture for the text classification task
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 文本分类任务的模型架构
- en: 'The Hugging Face Transformers library simplifies model selection and modification
    for fine-tuning in the following ways:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Transformers库通过以下方式简化了模型选择和修改，以便进行微调：
- en: It provides a rich model zoo with a number of pretrained models and tokenizers
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了丰富的模型库，包含许多预训练模型和分词器
- en: It has a simple model API to modify the baseline model for fine-tuning a specific
    task
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一个简单的模型API，用于修改基准模型，以便对特定任务进行微调
- en: It implements inference pipelines, combining data preprocessing and actual inference
    together
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它实现了推理管道，将数据预处理和实际推理结合在一起
- en: The full source code of this learning project is available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本学习项目的完整源代码可以在[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb)找到。
- en: Developing a training script
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发训练脚本
- en: When running SageMaker training jobs, we need to provide a training script.
    Additionally, we might provide any other dependencies. We can also install or
    modify Python packages that are installed on prebuilt containers via the `requirements.txt`
    file.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行SageMaker训练任务时，我们需要提供一个训练脚本。此外，我们还可以提供任何其他依赖项。我们还可以通过`requirements.txt`文件在预构建的容器上安装或修改已安装的Python包。
- en: 'In this example, we will use a new feature of the Hugging Face framework to
    fine-tune a multicategory classifier using the Hugging Face Trainer API. Let’s
    make sure that the training container has the newer Hugging Face Transformer library
    installed. For this, we create the `requirements.txt` file and specify a minimal
    compatible version. Later, we will provide this file to our SageMaker training
    job:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用Hugging Face框架的一个新特性，通过Hugging Face Trainer API对多类别分类器进行微调。让我们确保训练容器中已安装较新的Hugging
    Face Transformer库。为此，我们创建`requirements.txt`文件并指定最小兼容版本。稍后，我们将把这个文件提供给我们的SageMaker训练任务：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we need to develop the training script. Let’s review some key components
    of it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要开发训练脚本。让我们回顾一下其中的一些关键组件。
- en: 'At training time, SageMaker starts training by calling `user_training_script
    --arg1 value1 --arg2 value2 ...`. Here, `arg1..N` are training hyperparameters
    and other miscellaneous parameters provided by users as part of training job configuration.
    To correctly kick off the training process in our script, we need to include `main
    guard` within our script:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，SageMaker通过调用`user_training_script --arg1 value1 --arg2 value2 ...`来启动训练。在这里，`arg1..N`是训练超参数和用户提供的其他杂项参数，这些参数作为训练任务配置的一部分提供。为了在我们的脚本中正确启动训练过程，我们需要在脚本中包含`main
    guard`：
- en: 'To correctly capture the parameters, the training script needs to be able to
    parse command-line arguments. We use the Python `argparse` library to do this:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了正确捕获参数，训练脚本需要能够解析命令行参数。我们使用Python的`argparse`库来实现这一点：
- en: '[PRE7]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `train()` method is responsible for running end-to-end training jobs. It
    includes the following components:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train()`方法负责执行端到端的训练任务。它包含以下组件：'
- en: Calling `_get_tokenized_dataset` to load and tokenize datasets using a pretrained
    DistilBERT tokenizer from the Hugging Face library.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`_get_tokenized_dataset`，使用来自Hugging Face库的预训练DistilBERT分词器加载并分词数据集。
- en: Loading and configuring the DistilBERT model from the Hugging Face model zoo.
    Please note that we update the default configuration for classification tasks
    to adjust for our chosen number of categories.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Hugging Face 模型库加载并配置 DistilBERT 模型。请注意，我们更新了分类任务的默认配置，以调整我们选择的类别数量。
- en: Configuring Hugging Face Trainer and starting the training process.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Hugging Face Trainer 并开始训练过程。
- en: 'Once the training is done, we save the trained model:'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完成后，我们保存训练好的模型：
- en: '[PRE8]'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So far in our script, we have covered key aspects: handling configuration settings
    and model hyperparameters, loading pretrained models, and starting training using
    the Hugging Face Trainer API.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的脚本已经涵盖了几个关键方面：处理配置设置和模型超参数、加载预训练模型，并使用 Hugging Face Trainer API 启动训练。
- en: Starting the training job
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动训练作业
- en: 'Once we have our training script and dependencies ready, we can proceed with
    the training and schedule a training job via SageMaker Python SDK. We start with
    the import of the Hugging Face Estimator object and get the IAM execution role
    for our training job:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了训练脚本和依赖项，就可以开始训练并通过 SageMaker Python SDK 调度训练作业。我们从导入 Hugging Face Estimator
    对象并获取训练作业的 IAM 执行角色开始：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we need to define the hyperparameters of our model and training processes.
    These variables will be passed to our script at training time:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义模型和训练过程的超参数。这些变量将在训练时传递给我们的脚本：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After that, the training job will be scheduled and executed. It will take 10–15
    minutes for it to complete, then the trained model and other output artifacts
    will be added to Amazon S3.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，训练作业将被调度并执行。完成训练需要大约 10 到 15 分钟，训练后的模型和其他输出成果将被添加到 Amazon S3。
- en: Developing an inference script for script mode
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为脚本模式开发推理脚本
- en: Now that we have a trained model, let’s deploy it as a SageMaker real-time endpoint.
    We will use the prebuilt SageMaker Hugging Face container and will only provide
    our inference script. The inference requests will be handled by the **AWS MMS**,
    which exposes the HTTP endpoint.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练好的模型，接下来将其部署为 SageMaker 实时端点。我们将使用预构建的 SageMaker Hugging Face 容器，并只提供推理脚本。推理请求将由
    **AWS MMS** 处理，并暴露 HTTP 端点。
- en: 'When using prebuilt inference containers, SageMaker automatically recognizes
    our inference script. According to SageMaker convention, the inference script
    has to contain the following methods:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预构建推理容器时，SageMaker 会自动识别我们的推理脚本。根据 SageMaker 的约定，推理脚本必须包含以下方法：
- en: '`model_fn(model_dir)` is executed at the container start time to load the model
    into memory. This method takes the model directory as an input argument. You can
    use `model_fn()` to initialize other components of your inference pipeline, such
    as the tokenizer in our case. Note, Hugging Face Transformers have a convenient
    Pipeline API that allows us to combine data preprocessing (in our case, text tokenization)
    and actual inference in a single object. Hence, instead of a loaded model, we
    return an inference pipeline:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_fn(model_dir)` 在容器启动时执行，用于将模型加载到内存中。该方法以模型目录作为输入参数。你可以使用 `model_fn()`
    来初始化推理管道中的其他组件，比如在我们的例子中是分词器。需要注意的是，Hugging Face Transformers 提供了一个方便的 Pipeline
    API，允许我们将数据预处理（在我们这里是文本分词）和实际推理结合成一个对象。因此，我们返回的是一个推理管道，而不是加载的模型：'
- en: '[PRE11]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`transform_fn(inference_pipeline, data, content_type, accept_type)` is responsible
    for running the actual inference. Since we are communicating with an end client
    via HTTP, we also need to do payload deserialization and response serialization.
    In our sample example, we expect a JSON payload and return a JSON payload; however,
    this can be extended to any other formats based on the requirements (for example,
    CSV and Protobuf):'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform_fn(inference_pipeline, data, content_type, accept_type)` 负责执行实际的推理操作。由于我们通过
    HTTP 与最终客户端进行通信，因此我们还需要进行负载反序列化和响应序列化。在我们的示例中，我们预期使用 JSON 负载并返回 JSON 负载；然而，依据需求，这可以扩展为其他格式（例如
    CSV 和 Protobuf）。'
- en: '[PRE12]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Sometimes, combining deserialization, inference, and serialization in a single
    method can be inconvenient. Alternatively, SageMaker supports a more granular
    API:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，将反序列化、推理和序列化结合到一个方法中可能会不方便。作为替代，SageMaker 支持更细粒度的 API：
- en: '`input_fn(request_body, request_content_type)` runs deserialization'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_fn(request_body, request_content_type)` 执行反序列化操作。'
- en: '`predict_fn(deser_input, model)` performs predictions'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_fn(deser_input, model)` 执行预测操作'
- en: '`output_fn(prediction, response_content_type)` runs the serialization of predictions'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_fn(prediction, response_content_type)` 执行预测结果的序列化'
- en: Note that the `transform_fn()` method is mutually exclusive with the `input_fn()`,
    `predict_fn()`, and `output_fn()` methods.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`transform_fn()`方法与`input_fn()`、`predict_fn()`和`output_fn()`方法是互斥的。
- en: Deploying a Text Classification endpoint
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署文本分类终端节点
- en: 'Now we are ready to deploy and test our Newsgroup Classification endpoint.
    We can use the `estimator.create_model()` method to configure our model deployment
    parameters, specifically the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好部署并测试我们的Newsgroup分类终端节点了。我们可以使用`estimator.create_model()`方法来配置我们的模型部署参数，特别是以下内容：
- en: Define the inference script and other dependencies that will be uploaded by
    SageMaker to an endpoint.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义推理脚本和其他依赖项，这些内容将由SageMaker上传至终端节点。
- en: 'Identify the inference container. If you provide the `transformers_version`,
    `pytorch_version`, and `py_version` parameters, SageMaker will automatically find
    an appropriate prebuilt inference container (if it exists). Alternatively, you
    can provide `image_uri` to directly specify the container image you wish to use:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定推理容器。如果提供`transformers_version`、`pytorch_version`和`py_version`参数，SageMaker会自动找到一个适当的预构建推理容器（如果存在）。另外，您可以提供`image_uri`来直接指定希望使用的容器镜像：
- en: '[PRE13]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we define the parameters of our endpoint such as the number and type
    of instances behind it. The `model.deploy()` method starts the inference deployment
    (which, usually, takes several minutes) and returns a `Predictor` object to run
    inference requests:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义终端节点的参数，例如其背后的实例数量和类型。`model.deploy()`方法启动推理部署（通常需要几分钟时间），并返回一个`Predictor`对象以运行推理请求：
- en: '[PRE14]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, let’s explore how to extend pre-built DL containers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨如何扩展预构建的深度学习容器。
- en: Extending the prebuilt containers
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展预构建容器
- en: We will reuse code assets from the script mode example. However, unlike the
    previous container, we will modify our runtime environment and install the latest
    stable Hugging Face Transformer from the GitHub master branch. This modification
    will be implemented in our custom container image.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用脚本模式示例中的代码资产。然而，与之前的容器不同，我们将修改我们的运行时环境，并从GitHub主分支安装最新稳定版的Hugging Face
    Transformer。此修改将在我们的自定义容器镜像中实现。
- en: First off, we need to identify which base image we will use. AWS has published
    all of the available DL containers at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定将使用哪个基础镜像。AWS已在[https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)发布了所有可用的深度学习容器。
- en: Since we plan to use reinstall from scratch HugggingFace Transformer library
    anyway, we might choose the PyTorch base image. At the time of writing, the latest
    PyTorch SageMaker container was `763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-gpu-py38-cu111-ubuntu20.04`.
    Note that this container URI is for the AWS East-1 region and will be different
    for other AWS regions. Please consult the preceding referenced AWS article on
    the correct URI for your region.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们计划重新从头安装HuggingFace Transformer库，因此可以选择PyTorch基础镜像。在撰写时，最新的PyTorch SageMaker容器是`763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-gpu-py38-cu111-ubuntu20.04`。请注意，此容器URI适用于AWS
    East-1区域，其他AWS区域会有所不同。请查阅前述参考的AWS文章以了解适用于您区域的正确URI。
- en: 'To build a new container, we will need to perform the following steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个新的容器，我们需要执行以下步骤：
- en: Create a Dockerfile with runtime instructions.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个包含运行时指令的Dockerfile。
- en: Build the container image locally.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地构建容器镜像。
- en: 'Push the new container image to the **container registry**. In this example,
    we will use ECR as a container registry: a managed service from AWS, which is
    well integrated into the SageMaker ecosystem.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新的容器镜像推送到**容器注册表**。在本示例中，我们将使用ECR作为容器注册表：AWS提供的托管服务，它与SageMaker生态系统深度集成。
- en: First, let’s create a Dockerfile for our extended container.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为我们的扩展容器创建一个Dockerfile。
- en: Developing a Dockerfile for our extended container
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为我们的扩展容器开发Dockerfile。
- en: 'To extend the prebuilt SageMaker container, we need to have at least the following
    components:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展预构建的SageMaker容器，我们至少需要以下组件：
- en: A SageMaker PyTorch image to use as a base.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于作为基础的SageMaker PyTorch镜像。
- en: The required dependencies installed, such as the latest PyTorch and Hugging
    Face Transformers from the latest Git master branch.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装所需的依赖项，如最新版本的PyTorch和来自最新Git主分支的Hugging Face Transformers。
- en: Copy our training script from the previous example into the container.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们之前示例中的训练脚本复制到容器中。
- en: 'Define the `SAGEMAKER_SUBMIT_DIRECTORY` and `SAGEMAKER_PROGRAM` environmental
    variables, so SageMaker knows which training script to execute when the container
    starts:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 `SAGEMAKER_SUBMIT_DIRECTORY` 和 `SAGEMAKER_PROGRAM` 环境变量，以便 SageMaker 知道容器启动时应该执行哪个训练脚本：
- en: '[PRE15]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we are ready to build and push this container image to ECR. You can find
    the `bash` script to do this in the chapter repository.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好构建并将此容器镜像推送到 ECR。你可以在本章的代码库中找到执行此操作的 `bash` 脚本。
- en: Scheduling a training job
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安排训练作业
- en: 'Once we have our extended PyTorch container in ECR, we are ready to execute
    a SageMaker training job. The training job configuration will be similar to the
    script mode example with one notable difference: instead of the `HuggingFaceEstimator`
    object, we will use a generic SageMaker `Estimator` object that allows us to work
    with custom images. Note that you need to update the `image_uri` parameter with
    reference to the image URI in your ECR instance. You can find it by navigating
    to the ECR service on your AWS Console and finding the extended container there:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在 ECR 中拥有扩展的 PyTorch 容器，就可以执行一个 SageMaker 训练作业了。训练作业配置将与脚本模式示例类似，唯一的不同点是：我们将使用一个通用的
    SageMaker `Estimator` 对象，而不是 `HuggingFaceEstimator` 对象，后者可以让我们使用自定义镜像。请注意，您需要更新
    `image_uri` 参数，指向您 ECR 实例中的镜像 URI。你可以通过访问 AWS 控制台中的 ECR 服务来找到它，并在那找到扩展容器：
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After completing the training job, we should expect similar training outcomes
    as those shown in the script mode example.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 完成训练作业后，我们应当期望与脚本模式示例中展示的训练结果类似。
- en: Developing a BYO container for inference
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个用于推理的 BYO 容器
- en: In this section, we will learn how to build a SageMaker-compatible inference
    container using an official TensorFlow image, prepare an inference script and
    model server, and deploy it for inference on SageMaker Hosting.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用官方 TensorFlow 镜像构建一个与 SageMaker 兼容的推理容器，准备推理脚本和模型服务器，并将其部署到 SageMaker
    Hosting 上进行推理。
- en: Problem overview
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题概述
- en: We will develop a SageMaker-compatible container for inference. We will use
    the latest official TensorFlow container as a base image and use AWS MMS as a
    model server. Please note that MMS is one of many ML model serving options that
    can be used. SageMaker doesn’t have any restrictions on a model server other than
    that it should serve models on port `8080`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个与 SageMaker 兼容的推理容器。我们将使用最新的官方 TensorFlow 容器作为基础镜像，并使用 AWS MMS 作为模型服务器。请注意，MMS
    是许多机器学习模型服务选项之一，SageMaker 对模型服务器没有其他限制，唯一要求是它应该在端口`8080`上提供模型服务。
- en: Developing the serving container
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发服务容器
- en: 'When deploying a serving container to the endpoint, SageMaker runs the following
    command:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当将服务容器部署到端点时，SageMaker 会运行以下命令：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To comply with this requirement, it’s recommended that you use the exec format
    of the `ENTRYPOINT` instruction in your Dockerfile.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵守这一要求，建议在 Dockerfile 中使用 `ENTRYPOINT` 指令的执行格式。
- en: 'Let’s review our BYO Dockerfile:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们的 BYO Dockerfile：
- en: We use the latest TensorFlow container as a base
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用最新的 TensorFlow 容器作为基础
- en: We install general and SageMaker-specific dependencies
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们安装通用和 SageMaker 特定的依赖项
- en: We copy our model serving scripts to the container
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将模型服务脚本复制到容器中
- en: We specify `ENTRYPOINT` and the CMD instructions to comply with the SageMaker
    requirements
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们指定 `ENTRYPOINT` 和 CMD 指令，以遵守 SageMaker 的要求
- en: 'Now, let’s put it into action:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始行动：
- en: 'Use the latest official TensorFlow container:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最新的官方 TensorFlow 容器：
- en: '[PRE18]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Install Java, as required by MMS and any other common dependencies.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Java，因为 MMS 和其他常见依赖项都需要它。
- en: 'Copy the entrypoint script to the image:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将入口脚本复制到镜像中：
- en: '[PRE19]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Copy the default custom service file to handle incoming data and inference
    requests:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将默认的自定义服务文件复制到容器中，以处理传入的数据和推理请求：
- en: '[PRE20]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define an entrypoint script and its default parameters:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义入口脚本及其默认参数：
- en: '[PRE21]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this example, we don’t intend to cover MMS and the development of inference
    scripts in detail. However, it’s worth highlighting some key script aspects:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们并不打算详细介绍 MMS 以及推理脚本的开发。然而，值得强调一些关键的脚本方面：
- en: '`dockerd_entrypoint.py` is an executable that starts the MMS server when the
    `serve` argument is passed to it.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dockerd_entrypoint.py` 是一个可执行文件，当传递 `serve` 参数时，它会启动 MMS 服务器。'
- en: '`model_handler.py` implements model-loading and model-serving logics. Note
    that the `handle()` method checks whether the model is already loaded into memory.
    If it’s not, it will load a model into memory once and then proceed to the handling
    serving request, which includes the following:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_handler.py` 实现了模型加载和服务逻辑。请注意，`handle()` 方法检查模型是否已经加载到内存中。如果没有，它会先加载模型到内存，然后继续处理服务请求，包括以下内容：'
- en: Deserializing the request payload
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反序列化请求负载
- en: Running predictions
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行预测
- en: Serializing predictions
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化预测
- en: Deploying the SageMaker endpoint
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署 SageMaker 端点
- en: 'To schedule the deployment of the inference endpoint, we use the generic `Model`
    class from SageMaker Python SDK. Note that since we downloaded the model from
    a public model zoo, we don’t need to provide a `model_data` parameter (hence,
    its value is `None`):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要安排推理端点的部署，我们使用来自 SageMaker Python SDK 的通用 `Model` 类。请注意，由于我们从公共模型库下载了模型，因此无需提供
    `model_data` 参数（因此其值为 `None`）：
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It might take several minutes to fully deploy the endpoint and start the model
    server. Once it’s ready, we can call the endpoint using the `boto3.sagemaker-runtime`
    client, which allows you to construct the HTTP request and send the inference
    payload (or image, in our case) to a specific SageMaker endpoint:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 完全部署端点并启动模型服务器可能需要几分钟。一旦准备就绪，我们可以使用 `boto3.sagemaker-runtime` 客户端调用端点，该客户端允许你构建
    HTTP 请求并将推理负载（或者在我们的案例中是图像）发送到特定的 SageMaker 端点：
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code will, most likely, return an object in the image based on model predictions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码很可能会根据模型预测返回图像中的对象。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we reviewed how SageMaker provides support for the ML and DL
    frameworks using Docker containers. After reading this chapter, you should now
    know how to select the most appropriate DL container usage pattern according to
    your specific use case requirements. We learned about SageMaker toolkits, which
    simplifies developing SageMaker-compatible containers. In later sections, you
    gained practical knowledge of how to develop custom containers and scripts for
    training and inference tasks on Amazon SageMaker.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了 SageMaker 如何使用 Docker 容器支持机器学习和深度学习框架。阅读完本章后，你现在应该知道如何根据特定的使用案例需求选择最合适的深度学习容器使用模式。我们了解了
    SageMaker 工具包，这简化了开发与 SageMaker 兼容的容器。在后续部分，你将获得如何为 Amazon SageMaker 开发自定义容器和脚本进行训练和推理任务的实用知识。
- en: In the next chapter, we will learn about the SageMaker development environment
    and how to efficiently develop and troubleshoot your DL code. Additionally, we
    will learn about DL-specific tools and interfaces that the SageMaker development
    environment provides to simplify the building, deploying, and monitoring of your
    DL models.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习 SageMaker 开发环境，以及如何高效地开发和排查你的深度学习代码。此外，我们还将了解 SageMaker 开发环境提供的深度学习特定工具和接口，这些工具可以简化深度学习模型的构建、部署和监控。
