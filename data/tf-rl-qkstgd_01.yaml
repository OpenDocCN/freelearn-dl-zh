- en: Up and Running with Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习入门
- en: This book will cover interesting topics in deep **Reinforcement Learning** (**RL**),
    including the more widely used algorithms, and will also provide TensorFlow code
    to solve many challenging problems using deep RL algorithms. Some basic knowledge
    of RL will help you pick up the advanced topics covered in this book, but the
    topics will be explained in a simple language that machine learning practitioners
    can grasp. The language of choice for this book is Python, and the deep learning
    framework used is TensorFlow, and we expect you to have a reasonable understanding
    of the two. If not, there are several Packt books that cover these topics. We
    will cover several different RL algorithms, such as **Deep Q-Network** (**DQN**),
    **Deep Deterministic Policy Gradient** (**DDPG**), **Trust Region Policy Optimization**
    (**TRPO**), and **Proximal Policy Optimization** (**PPO**), to name a few. Let's
    dive right into deep RL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将涵盖深度**强化学习**（**RL**）中的有趣主题，包括更广泛使用的算法，并提供TensorFlow代码来解决许多使用深度RL算法的挑战性问题。具备一定的RL基础知识将有助于你掌握本书涵盖的高级主题，但本书将以简明易懂的语言讲解，便于机器学习从业人员理解。本书选择的编程语言是Python，使用的深度学习框架是TensorFlow，我们预计你对这两者有一定的了解。如果没有，也有许多Packt出版的书籍涉及这些主题。我们将介绍几种不同的RL算法，例如**深度Q网络**（**DQN**）、**深度确定性策略梯度**（**DDPG**）、**信任区域策略优化**（**TRPO**）和**近端策略优化**（**PPO**）等。让我们一起深入探索深度RL。
- en: In this chapter, we will delve deep into the basic concepts of RL. We will learn
    the meaning of the RL jargon, the mathematical relationships between them, and
    also how to use them in an RL setting to train an agent. These concepts will lay
    the foundations for us to learn RL algorithms in later chapters, along with how
    to apply them to train agents. Happy learning!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨强化学习的基本概念。我们将学习RL术语的含义，它们之间的数学关系，以及如何在RL环境中使用这些概念来训练智能体。这些概念将为我们在后续章节中学习RL算法奠定基础，并展示如何应用这些算法来训练智能体。祝学习愉快！
- en: 'Some of the main topics that will be covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及的主要主题如下：
- en: Formulating the RL problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建RL问题模型
- en: Understanding what an agent and an environment are
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解什么是智能体（agent）和环境（environment）
- en: Defining the Bellman equation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义贝尔曼方程
- en: On-policy versus off-policy learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线学习与离线学习
- en: Model-free versus model-based training
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模型训练与基于模型的训练
- en: Why RL?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择强化学习（RL）？
- en: 'RL is a sub-field of machine learning where the learning is carried out by
    a trial-and-error approach. This differs from other machine learning strategies,
    such as the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个子领域，其中学习是通过试错法进行的。这与其他机器学习策略不同，例如：
- en: '**Supervised learning**: Where the goal is to learn to fit a model distribution
    that captures a given labeled data distribution'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：目标是学习拟合一个模型分布，该分布能够捕捉给定标签数据分布'
- en: '**Unsupervised learning**: Where the goal is to find inherent patterns in a
    given dataset, such as clustering'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：目标是在给定的数据集中找到内在的模式，例如聚类'
- en: RL is a powerful learning approach, since you do not require labeled data, provided,
    of course, that you can master the learning-by-exploration approach used in RL.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种强大的学习方法，因为你不需要标签数据，前提是你能够掌握RL中采用的通过探索学习的方法。
- en: While RL has been around for over three decades, the field has gained a new
    resurgence in recent years with the successful demonstration of the use of deep
    learning in RL to solve real-world tasks, wherein deep neural networks are used
    to make decisions. The coupling of RL with deep learning is typically referred
    to as deep RL, and is the main topic of discussion of this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习（RL）已经有三十多年的历史，但近年来随着深度学习在RL中成功应用于解决实际任务的演示，RL领域经历了新的复兴。在这些任务中，深度神经网络用于做出决策。RL与深度学习的结合通常被称为深度强化学习（deep
    RL），也是本书讨论的主要内容。
- en: Deep RL has been successfully applied by researchers to play video games, to
    drive cars autonomously, for industrial robots to pick up objects, for traders
    to make portfolio bets, by healthcare practitioners, and copious other examples.
    Recently, Google DeepMind built AlphaGo, a RL-based system that was able to play
    the game Go, and beat the champions of the game easily. OpenAI built another system
    to beat humans in the Dota video game. These examples demonstrate the real-world
    applications of RL. It is widely believed that this field has a very promising
    future, since you can train neural networks to make predictions without providing
    labeled data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（Deep RL）已经成功地被研究人员应用于视频游戏、自动驾驶、工业机器人抓取物体、交易员进行投资组合下注、医疗保健领域等多个场景。最近，Google
    DeepMind构建了AlphaGo，一个基于强化学习（RL）系统，它能够玩围棋，并轻松击败围棋的冠军。OpenAI也构建了另一个系统，在《Dota》视频游戏中战胜人类。这些例子展示了RL在现实世界中的应用。广泛认为，这一领域有着非常光明的未来，因为你可以训练神经网络进行预测，而无需提供标注数据。
- en: Now, let's delve into the formulation of the RL problem. We will compare how
    RL is similar in spirit to a child learning to walk.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨强化学习问题的表述。我们将比较强化学习与孩子学走路之间的相似性。
- en: Formulating the RL problem
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习问题的表述
- en: The basic problem that is solved is training a model to make predictions of
    some pre-defined task without any labeled data. This is accomplished by a trial-and-error
    approach, akin to a baby learning to walk for the first time. A baby, curious
    to explore the world around them, first crawls out of their crib not knowing where
    to go nor what to do. Initially, they take small steps, make mistakes, keep falling
    on the floor, and cry. But, after many such episodes, they start to stand on their
    feet on their own, much to the delight of their parents. Then, with a giant leap
    of faith, they start to take slightly longer steps, slowly and cautiously. They
    still make mistakes, albeit fewer than before.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决的基本问题是训练一个模型，在没有任何标注数据的情况下对某些预定义任务进行预测。这是通过试错方法实现的，类似于婴儿第一次学走路的过程。婴儿由于好奇想探索周围的世界，首先会爬出婴儿床，不知道该去哪儿，也不知道该做什么。最初，他们迈出小步伐，犯错误，摔倒在地上并哭泣。但是，经过许多这样的尝试后，他们开始能够独立站立，令父母感到欣喜。然后，他们凭借一股信念迈出了稍长的步伐，慢慢而小心地走着。他们依然会犯错误，但比之前少得多。
- en: After many more such tries—and failures—they gain more confidence that enables
    them to take even longer steps. With time, these steps get much longer and faster,
    until eventually, they start to run. And that's how they grow up into a child.
    Was any labeled data provided to them that they used to learn to walk? No. they
    learned by trial and error, making mistakes along the way, learning from them,
    and getting better at it with infinitesimal gains made for every attempt. This
    is how RL works, learning by trial and error.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 经过更多次的尝试——和失败——他们变得更加自信，从而能够迈出更长的步伐。随着时间的推移，这些步伐变得越来越长、越来越快，直到最终他们开始奔跑。这就是他们成长为一个孩子的过程。是否给他们提供了任何标注数据供他们学习走路？没有。他们是通过试错学习的，沿途犯错，从中学习，并在每次尝试中取得微小的进步。这就是强化学习的工作原理，通过试错学习。
- en: Building on the preceding example, here is another situation. Suppose you need
    to train a robot using trial and error, this is how to do it. Let the robot wander
    randomly in the environment initially. The good and bad actions are collected
    and a reward function is used to quantify them, thus, a good action performed
    in a state will have high rewards; on the other hand, bad actions will be penalized.
    This can be used as a learning signal for the robot to improve itself. After many
    such episodes of trial and error, the robot would have learned the best action
    to perform in a given state, based on the reward. This is how learning in RL works.
    But we will not talk about human characters for the rest of the book. The child
    described previously is the *agent,* and their surroundings are the *environment*
    in RL parlance. The agent interacts with the environment and, in the process,
    learns to undertake a task, for which the environment will provide a reward.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子基础上，这里是另一种情况。假设你需要通过试错法来训练一个机器人，下面是具体做法。首先，让机器人在环境中随机游走。然后收集好的和不好的行为，并使用奖励函数对其进行量化，因此，在某个状态下执行好的行为会获得高奖励；另一方面，坏行为会受到惩罚。这可以作为机器人自我改进的学习信号。在经历了多次这样的试错过程后，机器人就会根据奖励学会在给定状态下执行最优的行为。这就是强化学习（RL）中的学习方式。但是在本书的其余部分，我们将不再讨论人的特征。之前描述的孩子是*代理*，而他们的周围环境是强化学习术语中的*环境*。代理与环境互动，在这个过程中，学习如何执行任务，环境会提供奖励。
- en: The relationship between an agent and its environment
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理与其环境之间的关系
- en: At a very basic level, RL involves an agent and an environment. An agent is
    an artificial intelligence entity that has certain goals, must remain vigilant
    about things that can come in the way of these goals, and must, at the same time,
    pursue the things that help in the attaining of these goals. An environment is
    everything that the agent can interact with. Let me explain further with an example
    that involves an industrial mobile robot.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从最基本的层面上讲，强化学习涉及一个代理和一个环境。代理是一个人工智能实体，拥有某些目标，必须时刻警惕可能阻碍这些目标的事物，并且必须同时追求有助于实现这些目标的事物。环境是代理可以与之互动的所有事物。让我通过一个涉及工业移动机器人的例子来进一步解释。
- en: For example, in a setting involving an industrial mobile robot navigating inside
    a factory, the robot is the agent, and the factory is the environment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个工业移动机器人在工厂内导航的情境中，机器人是代理，而工厂是环境。
- en: The robot has certain pre-defined goals, for example, to move goods from one
    side of the factory to the other without colliding with obstacles such as walls
    and/or other robots. The environment is the region available for the robot to
    navigate and includes all the places the robot can go to, including the obstacles
    that the robot could crash in to. So the primary task of the robot, or more precisely,
    the agent, is to explore the environment, understand how the actions it takes
    affects its rewards, be cognizant of the obstacles that can cause catastrophic
    crashes or failures, and then master the art of maximizing the goals and improving
    its performance over time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人有一些预定义的目标，例如，要将货物从工厂的一边移动到另一边，并避免与墙壁和/或其他机器人等障碍物发生碰撞。环境是机器人可以导航的区域，包括机器人可以到达的所有地方，也包括可能撞到的障碍物。因此，机器人的主要任务，或者更准确地说，代理的主要任务，是探索环境，理解它所采取的行为如何影响奖励，意识到可能导致灾难性碰撞或失败的障碍物，然后掌握如何最大化目标并随着时间的推移提高其表现。
- en: In this process, the agent inevitably interacts with the environment, which
    can be good for the agent regarding certain tasks, but could be bad for the agent
    regarding other tasks. So, the agent must learn how the environment will respond
    to the actions that are taken. This is a trial-and-error learning approach, and
    only after numerous such trials can the agent learn how the environment will respond
    to its decisions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，代理不可避免地与环境互动，这对代理来说，在某些任务上可能是有益的，但在其他任务上可能是不利的。因此，代理必须学习环境如何响应它所采取的行动。这是一种试错学习方法，只有经过多次这样的尝试，代理才能学会环境如何响应其决策。
- en: Let's now come to understand what the state space of an agent is, and the actions
    that the agent performs to explore the environment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来了解一下代理的状态空间，以及代理执行的动作以探索环境。
- en: Defining the states of the agent
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义代理的状态
- en: In RL parlance, *states* represent the current situation of the agent. For example,
    in the previous industrial mobile robot agent case, the state at a given time
    instant is the location of the robot inside the factory – that is, where it is
    located, its orientation, or more precisely, the pose of the robot. For a robot
    that has joints and effectors, the state can also include the precise location
    of the joints and effectors in a three-dimensional space. For an autonomous car,
    its state can represent its speed, location on a map, distance to other obstacles,
    torques on its wheels, the rpm of the engine, and so on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习术语中，*状态*表示智能体的当前情况。例如，在前述的工业移动机器人智能体案例中，给定时刻的状态是机器人在工厂内的位置——即它所在的位置、朝向，或者更准确地说，是机器人的姿态。对于具有关节和效应器的机器人，状态还可以包括关节和效应器在三维空间中的精确位置。对于自动驾驶汽车，它的状态可以表示速度、在地图上的位置、与其他障碍物的距离、车轮上的扭矩、引擎的转速等等。
- en: States are usually deduced from sensors in the real world; for instance, the
    measurement from odometers, LIDARs, radars, and cameras. States can be a one-dimensional
    vector of real numbers or integers, or two-dimensional camera images, or even
    higher-dimensional, for instance, three-dimensional voxels. There are really no
    precise limitations on states, and the state just represents the current situation
    of the agent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 状态通常通过现实世界中的传感器推导出来；例如，来自里程计、激光雷达、雷达和摄像头的测量值。状态可以是一个一维的实数或整数向量，或者是二维的摄像头图像，甚至是更高维度的，例如三维体素。实际上，状态没有精确的限制，状态仅仅代表智能体当前的情况。
- en: In RL literature, states are typically represented as **s[t]**, where the subscript
    *t* is used to denote the time instant corresponding to the state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习文献中，状态通常表示为**s[t]**，其中下标 *t* 用于表示与该状态对应的时间瞬间。
- en: Defining the actions of the agent
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义智能体的动作
- en: The agent performs actions to explore the environment. Obtaining this action
    vector is the primary goal in RL. Ideally, you need to strive to obtain optimal
    actions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体执行动作以探索环境。获取这个动作向量是强化学习中的主要目标。理想情况下，你需要努力获得最优动作。
- en: An action is the decision an agent takes in a certain state, *s[t]*. Typically,
    it is represented as *a[t]*, where, as before, the subscript *t* denotes the time
    instant. The actions that are available to an agent depends on the problem. For
    instance, an agent in a maze can decide to take a step north, or south, or east,
    or west. These are called **discrete actions**, as there are a fixed number of
    possibilities. On the other hand, for an autonomous car, actions can be the steering
    angle, throttle value, brake value, and so on, which are called **continuous actions**
    as they can take real number values in a bounded range. For example, the steering
    angle can be 40 degrees from the north-south line, and the throttle can be 60%
    down, and so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动作是智能体在某一状态 *s[t]* 下做出的决策。通常，它表示为 *a[t]*，其中，像之前一样，下标 *t* 表示对应的时间瞬间。智能体可用的动作取决于问题。例如，迷宫中的智能体可以决定朝北、南、东或西迈出一步。这些被称为**离散动作**，因为有固定的可能性数量。另一方面，对于自动驾驶汽车，动作可以是转向角度、油门值、刹车值等等，这些被称为**连续动作**，因为它们可以在有限范围内取实数值。例如，转向角度可以是从南北线偏离40度，油门可以是60%的压下程度，等等。
- en: Thus, actions *a[t]*can be either discrete or continuous, depending on the problem
    at hand. Some RL approaches handle discrete actions, while others are suited for
    continuous actions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，动作 *a[t]* 可以是离散的，也可以是连续的，这取决于具体问题。一些强化学习方法处理离散动作，而其他方法则适用于连续动作。
- en: 'A schematic of the **agent** and its interaction with the **environment** is
    shown in the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了**智能体**及其与**环境**的交互示意图：
- en: '![](img/d87cc4cd-cfc4-4651-938b-9059f424a4ae.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d87cc4cd-cfc4-4651-938b-9059f424a4ae.jpg)'
- en: 'Figure 1: Schematic showing the agent and its interaction with the environment'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：示意图，展示了智能体及其与环境的交互
- en: Now that we know what an **agent** is, we will look at the policies that the
    agent learns, what value and advantage functions are, and how these quantities
    are used in RL.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是**智能体**，接下来我们将探讨智能体学习的策略、什么是价值函数和优势函数，以及这些量如何在强化学习（RL）中使用。
- en: Understanding policy, value, and advantage functions
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解策略、价值和优势函数
- en: A **policy** defines the guidelines for an agent's behavior at a given state.
    In mathematical terms, a policy is a mapping from a state of the agent to the
    action to be taken at that state. It is like a stimulus-response rule that the
    agent follows as it learns to explore the environment. In RL literature, it is
    usually denoted as *π(a**[t]|s[t]**) –* that is, it is a conditional probability
    distribution of taking an action *a**[t]*in a given state *s**[t]*. Policies can
    be deterministic, wherein the exact value of *a**[t]* is known at *s**[t]*, or
    can be stochastic where *a**[t]* is sampled from a distribution – typically this
    is a Gaussian distribution, but it can also be any other probability distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略** 定义了代理在给定状态下行为的指导方针。在数学术语中，策略是从代理的一个状态映射到在该状态下要采取的行动的映射。它类似于代理遵循的刺激-反应规则，当其学习探索环境时。在强化学习文献中，它通常表示为
    *π(a**[t]|s[t]**) –*，即在给定状态 *s**[t]* 下采取行动 *a**[t]* 的条件概率分布。策略可以是确定性的，在这种情况下，*a**[t]*
    的确切值在 *s**[t]* 处是已知的，也可以是随机的，其中 *a**[t]* 从一个分布中采样 - 典型情况下为高斯分布，但也可以是任何其他概率分布。'
- en: 'In RL, **value functions** are used to define how good a state of an agent
    is. They are typically denoted by *V(s)* at state *s* and represent the expected
    long-term average rewards for being in that state. *V(s)* is given by the following
    expression where *E[.]* is an expectation over samples:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，**价值函数** 用于定义代理状态的优劣程度。它们通常用 *V(s)* 表示在状态 *s* 处的预期长期平均奖励。*V(s)* 的表达式如下，其中
    *E[.]* 是对样本的期望：
- en: '![](img/3232df6b-c15e-4a66-a3d3-4fa624a0da2c.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3232df6b-c15e-4a66-a3d3-4fa624a0da2c.png)'
- en: 'Note that *V(s)* does not care about the optimum actions that an agent needs
    to take at the state *s*. Instead, it is a measure of how good a state is. So,
    how can an agent figure out the most optimum action *a[t]* to take in a given
    state *s[t]* at time instant *t*? For this, you can also define an action-value
    function given by the following expression:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*V(s)* 不关心代理在状态 *s* 下需要采取的最佳行动。相反，它衡量的是状态的好坏程度。那么，代理如何在给定时间点 *t* 中找出在状态 *s[t]*
    下采取的最佳行动 *a[t]* 呢？为此，您还可以定义一个动作值函数，其表达式如下所示：
- en: '![](img/9df8c060-53f6-47a4-a9c7-d2775167230d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9df8c060-53f6-47a4-a9c7-d2775167230d.png)'
- en: Note that *Q(s,a)* is a measure of how good is it to take action *a* in state
    *s* and follow the same policy thereafter. So, *t* is different from *V(s),* which
    is a measure of how good a given state is. We will see in the following chapters
    how the value function is used to train the agent under the RL setting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 *Q(s,a)* 表示在状态 *s* 中采取行动 *a* 并随后遵循相同策略的优劣程度。因此，*Q(s,a)* 与 *V(s)* 不同，后者衡量的是给定状态的优劣程度。在接下来的章节中，我们将看到如何使用价值函数来训练强化学习设置下的代理。
- en: 'The **advantage function** is defined as the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势函数** 定义如下：'
- en: '*A(s,a) = Q(s,a) - V(s)*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*A(s,a) = Q(s,a) - V(s)*'
- en: This advantage function is known to reduce the variance of policy gradients,
    a topic that will be discussed in depth in a later chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个优势函数被认为能够减少策略梯度的方差，这是后续章节将深入讨论的一个主题。
- en: 'The classic RL textbook is *Reinforcement Lea**rning: An Introduction* by *Richard
    S Sutton* and *Andrew G Barto,* *The MIT Press*, 1998.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '经典的强化学习教材是《强化学习导论》（*Reinforcement Lea**rning: An Introduction*），作者是 *Richard
    S Sutton* 和 *Andrew G Barto*，出版社是 *The MIT Press*，出版年份是 1998 年。'
- en: We will now define what an episode is and its significance in an RL context.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义什么是一个情节及其在强化学习环境中的重要性。
- en: Identifying episodes
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别情节
- en: We mentioned earlier that the agent explores the environment in numerous trials-and-errors
    before it can learn to maximize its goals. Each such trial from start to finish
    is called an **episode**. The start location may or may not always be from the
    same location. Likewise, the finish or end of the episode can be a happy or sad
    ending.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，代理在能够最大化其目标之前需要在环境中进行多次试验和错误的探索。从开始到结束的每次这样的试验被称为一个 **情节**。起始位置可能并不总是相同。同样，情节的结束可以是愉快的或悲伤的结局。
- en: A happy, or good, ending can be when the agent accomplishes its pre-defined
    goal, which could be successfully navigating to a final destination for a mobile
    robot, or successfully picking up a peg and placing it in a hole for an industrial
    robot arm, and so on. Episodes can also have a sad ending, where the agent crashes
    into obstacles or gets trapped in a maze, unable to get out of it, and so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个愉快或良好的结局可以是代理实现其预定义目标的情况，这可能是移动机器人成功导航到最终目的地，或者工业机器人臂成功拾取和放置卡柄到孔中等等。情节也可能有悲伤的结局，例如代理撞到障碍物或被困在迷宫中，无法摆脱等等。
- en: In many RL problems, an upper bound in the form of a fixed number of time steps
    is generally specified for terminating an episode, although in others, no such
    bound exists and the episode can last for a very long time, ending with the accomplishment
    of a goal or by crashing into obstacles or falling off a cliff, or something similar.
    The Voyager spacecraft was launched by NASA in 1977, and has traveled outside
    our solar system – this is an example of a system with an infinite time episode.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多强化学习问题中，通常会指定一个时间步骤的上限来终止一个回合，尽管在其他问题中没有这样的上限，回合可以持续很长时间，直到达成目标，或者撞到障碍物、跌下悬崖，或发生类似情况为止。旅行者号宇宙飞船是由NASA于1977年发射的，它已经飞出我们的太阳系——这是一个无限时间回合系统的例子。
- en: We will next find out what a reward function is and why we need to discount
    future rewards. This reward function is the key, as it is the signal for the agent
    to learn.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将了解什么是奖励函数以及为什么需要折扣未来奖励。这个奖励函数是关键，因为它是代理学习的信号。
- en: Identifying reward functions and the concept of discounted rewards
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定奖励函数和折扣奖励的概念
- en: Rewards in RL are no different from real world rewards – we all receive good
    rewards for doing well, and bad rewards (aka penalties) for inferior performance.
    Reward functions are provided by the environment to guide an agent to learn as
    it explores the environment. Specifically, it is a measure of how well the agent
    is performing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的奖励与现实世界中的奖励没有区别——我们因表现好而获得奖励，因表现差而受到惩罚（即惩罚）。奖励函数由环境提供，指导代理在探索环境时进行学习。具体来说，它衡量的是代理的表现好坏。
- en: The reward function defines what the good and bad things are that can happen
    to the agent. For instance, a mobile robot that reaches its goal is rewarded,
    but is penalized for crashing into obstacles. Likewise, an industrial robot arm
    is rewarded for putting a peg into a hole, but is penalized for being in undesired
    poses that can be catastrophic by causing ruptures or crashes. Reward functions
    are the signal to the agent regarding what is optimum and what isn't. The agent's
    long-term goal is to maximize rewards and minimize penalties.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数定义了代理可能发生的好事和坏事。例如，一个达到目标的移动机器人会获得奖励，但如果撞到障碍物，则会受到惩罚。同样，一个工业机器人臂在将钉子放入孔中时会获得奖励，但如果它处于可能导致破裂或撞击的危险姿势，则会受到惩罚。奖励函数是代理获取关于什么是最优的、什么不是最优的信号。代理的长期目标是最大化奖励并最小化惩罚。
- en: Rewards
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励
- en: In RL literature, rewards at a time instant *t* are typically denoted as *R[t]*.
    Thus, the total rewards earned in an episode is given by *R = r1+ r2 + ... + r[t]*,
    where *t* is the length of the episode (which can be finite or infinite).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习文献中，时间点 *t* 的奖励通常表示为 *R[t]*。因此，整个回合中获得的总奖励为 *R = r1 + r2 + ... + r[t]*，其中
    *t* 是回合的长度（可以是有限的，也可以是无限的）。
- en: 'The concept of discounting is used in RL, where a parameter called the discount
    factor is used, typically represented by *γ* and *0 ≤ γ ≤ 1* and a power of it
    multiplies *r[t]*. *γ = 0*, making the agent myopic, and aiming only for the immediate
    rewards. *γ = 1* makes the agent far-sighted to the point that it procrastinates
    the accomplishment of the final goal. Thus, a value of ***γ*** in the 0-1 range
    (0 and 1 exclusive) is used to ensure that the agent is neither too myopic nor
    too far-sighted. *γ* ensures that the agent prioritizes its actions to maximize
    the total discounted rewards, *R[t]*, from time instant *t*, which is given by
    the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中使用了折扣的概念，其中使用了一个称为折扣因子的参数，通常表示为 *γ* 且 *0 ≤ γ ≤ 1*，它的幂次与 *r[t]* 相乘。*γ =
    0* 使得代理变得短视，只关注即时奖励；*γ = 1* 使得代理变得过于远视，以至于拖延最终目标的实现。因此，使用 0 到 1 范围内（不包括 0 和 1）的
    *γ* 值来确保代理既不会过于短视，也不会过于远视。*γ* 确保代理优先考虑其行动，最大化从时间点 *t* 开始的总折扣奖励 *R[t]*，其计算公式如下：
- en: '![](img/742aef13-26b3-48cf-8e38-36160f778595.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/742aef13-26b3-48cf-8e38-36160f778595.png)'
- en: Since *0 ≤ γ ≤ 1*, the rewards into the distant future are valued much less
    than the rewards that the agent can earn in the immediate future. This helps the
    agent to not waste time and to prioritize its actions. In practice, *γ = 0.9-0.99*
    is typically used in most RL problems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *0 ≤ γ ≤ 1*，远期的奖励相比即时的奖励价值较低。这有助于代理不浪费时间，并优先考虑其行动。在实际应用中，*γ = 0.9-0.99* 通常用于大多数强化学习问题中。
- en: Learning the Markov decision process
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习马尔可夫决策过程
- en: The Markov property is widely used in RL, and it states that the environment's
    response at time *t+1* depends only on the state and action at time *t*. In other
    words, the immediate future only depends on the present and not on the past. This
    is a useful property that simplifies the math considerably, and is widely used
    in many fields such as RL and robotics.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质在强化学习中被广泛应用，表示环境在时间*t+1*时的反应仅依赖于时间*t*的状态和行动。换句话说，未来的即时状态只依赖于当前状态，而不依赖于过去的状态。这一特性大大简化了数学计算，并在强化学习和机器人学等多个领域得到广泛应用。
- en: 'Consider a system that transitions from state *s[0]*to *s[1]*by taking an action
    *a[0]*and receiving a reward *r[1]*, and thereafter from *s[1]*to *s**[2]*taking
    action *a[1]*, and so on until time *t*. If the probability of being in a state
    *s''* at time *t+1* can be represented mathematically as in the following function,
    then the system is said to follow the Markov property:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个系统，它通过采取行动*a[0]*从状态*s[0]*转移到状态*s[1]*并获得奖励*r[1]*，然后再从*s[1]*到*s[2]*，采取行动*a[1]*，如此类推，直到时间*t*。如果在时间*t+1*时，处于状态*s'*的概率可以通过以下函数在数学上表示，那么该系统就被认为遵循马尔可夫性质：
- en: '![](img/8c698ac9-462f-4383-8fb9-436cd12be2a3.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c698ac9-462f-4383-8fb9-436cd12be2a3.png)'
- en: 'Note that the probability of being in state *s[t+1]* depends only on *s[t]*and
    *a[t]*and not on the past. An environment that satisfies the following state transition
    property and reward function as follows is said to be a **Markov Decision Process**
    (**MDP**):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，处于状态*s[t+1]*的概率仅取决于*s[t]*和*a[t]*，与过去无关。满足以下状态转移性质和奖励函数的环境被称为**马尔可夫决策过程**（**MDP**）：
- en: '![](img/58bbc3e1-18cd-4c45-aa1d-c08584402a9c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58bbc3e1-18cd-4c45-aa1d-c08584402a9c.png)'
- en: '![](img/18414740-fb2c-4a10-9f0d-9abaf19a69e7.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18414740-fb2c-4a10-9f0d-9abaf19a69e7.png)'
- en: 'Let''s now define the very foundation of RL: the Bellman equation. This equation
    will help in providing an iterative solution to obtaining value functions.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义强化学习的基础：贝尔曼方程。这个方程将帮助我们提供一个迭代解法来获得价值函数。
- en: Defining the Bellman equation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义贝尔曼方程
- en: The Bellman equation, named after the great computer scientist and applied mathematician
    Richard E. Bellman, is an optimality condition associated with dynamic programming.
    It is widely used in RL to update the policy of an agent.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程以伟大的计算机科学家和应用数学家理查德·E·贝尔曼的名字命名，是与动态规划相关的最优性条件。它在强化学习（RL）中被广泛用于更新智能体的策略。
- en: 'Let''s define the following two quantities:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义以下两个量：
- en: '![](img/aa14f34c-4a15-4c96-801a-1476a857fa70.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa14f34c-4a15-4c96-801a-1476a857fa70.png)'
- en: '![](img/4ed61901-5bfa-49fb-97d7-55fccdbc1a26.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ed61901-5bfa-49fb-97d7-55fccdbc1a26.png)'
- en: 'The first quantity, *P[s,s'']*, is the transition probability from state *s*
    to the new state *s''*. The second quantity, *R[s,s'']*, is the expected reward
    the agent receives from state *s*, taking action *a*, and moving to the new state
    *s''*. Note that we have assumed the MDP property, that is, the transition to
    state at time *t+1* only depends on the state and action at time *t*. Stated in
    these terms, the Bellman equation is a recursive relationship, and is given by
    the following equations respectively for the value function and action-value function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个量，*P[s,s']*，是从状态*s*到新状态*s'*的转移概率。第二个量，*R[s,s']*，是智能体从状态*s*出发，采取行动*a*，然后转移到新状态*s'*时所获得的期望奖励。请注意，我们假设了MDP的属性，即在时间*t+1*时的状态转移仅依赖于时间*t*的状态和行动。用这些术语表达时，贝尔曼方程是一个递归关系，并且对于价值函数和行动价值函数，分别给出如下方程：
- en: '![](img/2146e61f-5056-4e29-ab5e-fe75bacd02ea.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2146e61f-5056-4e29-ab5e-fe75bacd02ea.png)'
- en: '![](img/09e0191c-abb8-470b-a19e-9167c0a852dc.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09e0191c-abb8-470b-a19e-9167c0a852dc.png)'
- en: Note that the Bellman equations represent the value function *V* at a state,
    and as functions of the value function at other states; similarly for the action-value
    function *Q*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，贝尔曼方程表示了状态下的价值函数*V*，并且是其他状态下价值函数的函数；行动价值函数*Q*也是如此。
- en: On-policy versus off-policy learning
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习与离线学习
- en: RL algorithms can be classified as on-policy or off-policy. We will now learn
    about both of these classes and how to distinguish a given RL algorithm into one
    or the other.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法可以分为在线学习和离线学习。接下来我们将了解这两种类型，并讨论如何将给定的强化学习算法区分为其中之一。
- en: On-policy method
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习方法
- en: On-policy methods use the same policy to evaluate as was used to make the decisions
    on actions. On-policy algorithms generally do not have a replay buffer; the experience
    encountered is used to train the model in situ. The same policy that was used
    to move the agent from state at time *t* to state at time *t+1*, is used to evaluate
    if the performance was good or bad. For example, if a robot is exploring the world
    at a given state, if it uses its current policy to ascertain whether the actions
    it took in the current state were good or bad, then it is an on-policy algorithm,
    as the current policy is also used to evaluate its actions. SARSA, A3C, TRPO,
    and PPO are on-policy algorithms that we will be covering in this book.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 策略方法使用与做出动作决策时相同的策略来进行评估。策略算法通常没有重放缓冲区；遇到的经验用于现场训练模型。从状态 *t* 到状态 *t+1* 期间，使用相同的策略来评估性能好坏。例如，如果一个机器人在给定状态下探索世界，它如果使用当前策略来判断其在当前状态下采取的行动是好是坏，那么它就是一个策略算法，因为当前策略也用于评估其行为。SARSA、A3C、TRPO
    和 PPO 是我们将在本书中讨论的策略算法。
- en: Off-policy method
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离策略方法
- en: Off-policy methods, on the other hand, use different policies to make action
    decisions and to evaluate the performance. For instance, many off-policy algorithms
    use a replay buffer to store the experiences, and sample data from this buffer
    to train the model. During the training step, a mini-batch of experience data
    is randomly sampled and used to train the policy and value functions. Coming back
    to the previous robot example, in an off-policy setting, the robot will not use
    the current policy to evaluate its performance, but rather use a different policy
    for exploring and for evaluation. If a replay buffer is used to sample a mini-batch
    of experience data and then train the agent, then it is off-policy learning, as
    the current policy of the robot (which was used to obtain the immediate actions)
    is different from the policy that was used to obtain the samples in the mini-batch
    of experience used to train the agent (as the policy has changed from an earlier
    time instant when the data was collected, to the current time instant). DQN, DDQN,
    and DDPG are off-policy algorithms that we'll look at in later chapters of this
    book.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，离策略方法使用不同的策略来做出决策和评估性能。例如，许多离策略算法使用重放缓冲区来存储经验，并从该缓冲区中采样数据来训练模型。在训练步骤中，会随机采样一小批经验数据，并用于训练策略和价值函数。回到之前的机器人示例，在离策略设置下，机器人不会使用当前策略来评估其性能，而是使用不同的策略进行探索和评估。如果使用重放缓冲区来采样经验数据的小批量，然后训练代理，则这是离策略学习，因为机器人当前的策略（用于获取即时动作的策略）与用于获取小批量经验样本的策略不同（因为策略从数据收集时的早期时刻变更为当前时刻）。DQN、DDQN
    和 DDPG 是我们将在本书后续章节中讨论的离策略算法。
- en: Model-free and model-based training
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无模型和基于模型的训练
- en: RL algorithms that do not learn a model of how the environment works are called
    model-free algorithms. By contrast, if a model of the environment is constructed,
    then the algorithm is called model-based. In general, if value (*V*) or action-value
    (*Q*) functions are used to evaluate the performance, they are called **model-free**
    algorithms as no specific model of the environment is used. On the other hand,
    if you build a model of how the environment transitions from one state to another
    or determines how many rewards the agent will receive from the environment via
    a model, then they are called **model-based** algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不学习环境运作模型的RL算法称为无模型算法。相反，如果构建了环境模型，则该算法称为基于模型的算法。通常，如果使用价值（*V*）或动作价值（*Q*）函数来评估性能，则称其为**无模型**算法，因为没有使用环境的具体模型。另一方面，如果你建立了一个模型，描述环境如何从一个状态转移到另一个状态，或者通过模型决定代理将从环境中获得多少奖励，那么这些算法称为**基于模型**算法。
- en: In model-free algorithms, as aforementioned, we do not construct a model of
    the environment. Thus, the agent has to take an action at a state to figure out
    if it is a good or a bad choice. In model-based RL, an approximate model of the
    environment is learned; either jointly learned along with the policy, or learned
    a priori. This model of the environment is used to make decisions, as well as
    to train the policy. We will learn more about both classes of RL algorithms in
    later chapters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在无模型算法中，如前所述，我们并未构建环境的模型。因此，智能体必须在一个状态下采取行动，判断这是一个好的选择还是一个坏的选择。在基于模型的RL中，我们会学习一个环境的近似模型；该模型可以与策略共同学习，或是事先学习好。这个环境模型用于做决策，并且用于训练策略。我们将在后续章节中深入学习这两类RL算法。
- en: Algorithms covered in this book
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖的算法
- en: 'In [Chapter 2](72e58e29-a6e5-46b4-9d3e-d1baf4dc57f5.xhtml), *Temporal Difference,
    SARSA, and Q-Learning*, we will look into our first two RL algorithms: Q-learning
    and SARSA. Both of these algorithms are tabular-based and do not require the use
    of neural networks. Thus, we will code them in Python and NumPy. In [Chapter 3](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml),
    *Deep Q-Network*, we will cover DQN and use TensorFlow to code the agent for the
    rest of the book. We will then train it to play Atari Breakout. In [Chapter 4](ac0b4811-f9fc-474c-b1d6-3f8a43dc018c.xhtml),
    *Double DQN, Dueling Architectures, and Rainbow*, we will cover double DQN, dueling
    network architectures, and rainbow DQN. In [Chapter 5](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml),
    *Deep Deterministic Policy Gradient*, we will look at our first Actor-Critic RL
    algorithm called DDPG, learn about policy gradients, and apply them to a continuous
    action problem. In [Chapter 6](f9b2e0f4-90ce-4070-b030-b47339ca1aa8.xhtml), *Asynchronous
    Methods – A3C and A2C*, we will investigate A3C, which is another RL algorithm
    that uses a master and several worker processes. In [Chapter 7](7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml),
    *Trust Region Policy Optimization and Proximal Policy Optimization*, we will investigate
    two more RL algorithms: TRPO and PPO. Finally, we will apply DDPG and PPO to train
    an agent to drive a car autonomously in [Chapter 8](f4774497-444a-4174-b7d4-a8ef21928320.xhtml),
    *Deep RL Applied to Autonomous Driving*. From [Chapter 3](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml),
    *Deep Q-Network*, to [Chapter 8](f4774497-444a-4174-b7d4-a8ef21928320.xhtml),
    *Deep RL Applied to Autonomous Driving*, we''ll use TensorFlow agents. Have fun
    learning RL.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](72e58e29-a6e5-46b4-9d3e-d1baf4dc57f5.xhtml)，*时间差分法、SARSA 和 Q-学习*中，我们将探讨前两个强化学习（RL）算法：Q-学习和SARSA。这两个算法都是基于表格的，不需要使用神经网络。因此，我们将用Python和NumPy进行编程。在[第3章](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml)，*深度Q网络*中，我们将介绍DQN，并使用TensorFlow为本书后续章节中的智能体进行编码。然后我们将训练它玩Atari
    Breakout。在[第4章](ac0b4811-f9fc-474c-b1d6-3f8a43dc018c.xhtml)，*双重DQN、对抗网络架构和Rainbow*中，我们将讨论双重DQN、对抗网络架构以及Rainbow
    DQN。在[第5章](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml)，*深度确定性策略梯度*中，我们将探讨我们的第一个演员-评论员RL算法，称为DDPG，学习策略梯度，并将其应用于连续动作问题。在[第6章](f9b2e0f4-90ce-4070-b030-b47339ca1aa8.xhtml)，*异步方法
    – A3C和A2C*中，我们将研究A3C，这是另一个使用主节点和多个工作节点的RL算法。在[第7章](7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml)，*信任区域策略优化和近端策略优化*中，我们将研究另外两种RL算法：TRPO和PPO。最后，在[第8章](f4774497-444a-4174-b7d4-a8ef21928320.xhtml)，*深度强化学习在自动驾驶中的应用*中，我们将应用DDPG和PPO训练一个智能体，来实现自主驾驶。
    从[第3章](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml)，*深度Q网络*到[第8章](f4774497-444a-4174-b7d4-a8ef21928320.xhtml)，*深度强化学习在自动驾驶中的应用*，我们将使用TensorFlow智能体。希望你在学习强化学习的过程中玩得愉快。
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we were introduced to the basic concepts of RL. We understood
    the relationship between an agent and its environment, and also learned about
    the MDP setting. We learned the concept of reward functions and the use of discounted
    rewards, as well as the idea of value and advantage functions. In addition, we
    saw the Bellman equation and how it is used in RL. We also learned the difference
    between an on-policy and an off-policy RL algorithm. Furthermore, we examined
    the distinction between model-free and model-based RL algorithms. All of this
    lays the groundwork for us to delve deeper into RL algorithms and how we can use
    them to train agents for a given task.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了强化学习（RL）的基本概念。我们理解了智能体与环境之间的关系，也学习了马尔可夫决策过程（MDP）的设置。我们学习了奖励函数的概念和折扣奖励的使用，以及价值和优势函数的思想。此外，我们还了解了贝尔曼方程及其在RL中的应用。我们还学习了在策略型和非策略型RL算法之间的区别。此外，我们还考察了无模型和基于模型的RL算法之间的区别。所有这些为我们深入探讨RL算法以及如何利用它们训练智能体执行特定任务奠定了基础。
- en: 'In the next chapter, we will investigate our first two RL algorithms: Q-learning
    and SARSA. Note that in [Chapter 2](72e58e29-a6e5-46b4-9d3e-d1baf4dc57f5.xhtml),
    *Temporal Difference, SARSA, and Q-Learning*, we will be using Python-based agents
    as they are tabular-learning. But from [Chapter 3,](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml) *Deep
    Q-Network*, onward, we will be using TensorFlow to code deep RL agents, as we
    will require neural networks.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨我们首两个 RL 算法：Q-learning 和 SARSA。请注意，在[第2章](72e58e29-a6e5-46b4-9d3e-d1baf4dc57f5.xhtml)，*时序差分、SARSA
    和 Q-Learning* 中，我们将使用基于 Python 的代理，因为它们是表格学习方法。但从[第3章](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml)，*深度
    Q 网络* 开始，我们将使用 TensorFlow 来编写深度 RL 代理，因为我们将需要神经网络。
- en: Questions
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Is a replay buffer required for on-policy or off-policy RL algorithms?
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于基于策略或非基于策略的 RL 算法，是否需要回放缓存？
- en: Why do we discount rewards?
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要对奖励进行折扣？
- en: What will happen if the discount factor is *γ > 1*?
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果折扣因子是 *γ > 1* 会发生什么？
- en: Will a model-based RL agent always perform better than a model-free RL agent,
    since we have a model of the environment states?
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们拥有环境状态的模型，基于模型的 RL 代理是否总是比无模型的 RL 代理表现更好？
- en: What is the difference between RL and deep RL?
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RL 和深度 RL 有什么区别？
- en: Further reading
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Reinforcement Lea**rning: An Introduction*, *Richar**d S. Sutton* and *Andrew
    G. Barto, The MIT Press*, 1998'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习：导论*，*理查德·S·萨顿* 和 *安德鲁·G·巴托*，*MIT出版社*，1998年'
- en: '*Deep Reinforcement Learning Hands-On*, *Maxim Lapan*, *Packt Publishing*,
    2018: [https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习实战*，*马克西姆·拉潘*，*Packt出版公司*，2018年：[https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands)'
