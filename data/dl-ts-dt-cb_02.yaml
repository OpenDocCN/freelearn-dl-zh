- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Getting Started with PyTorch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 PyTorch
- en: In this chapter, we’ll explore **PyTorch**, a leading deep learning library
    in Python.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索 **PyTorch**，一个领先的 Python 深度学习库。
- en: We go over several operations that are useful for understanding how neural networks
    are built using PyTorch. Besides tensor operations, we will also explore how to
    train different types of neural networks. Specifically, we will focus on feedforward,
    recurrent, **long short-term memory**(**LSTM**), and 1D convolutional networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍一些有助于理解如何使用 PyTorch 构建神经网络的操作。除了张量操作，我们还将探讨如何训练不同类型的神经网络。具体来说，我们将重点关注前馈神经网络、循环神经网络、**长短期记忆**（**LSTM**）和
    1D 卷积神经网络。
- en: In later chapters, we will also cover other types of neural networks, such as
    transformers. Here, we will use synthetic data for demonstrative purposes, which
    will help us showcase both the implementation and theory behind each model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们还将介绍其他类型的神经网络，例如 Transformer。这里，我们将使用合成数据进行演示，这将帮助我们展示每种模型背后的实现和理论。
- en: Upon completing this chapter, you will have gained a robust understanding of
    PyTorch, equipping you with the tools for more advanced deep learning projects.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将对 PyTorch 有深入的理解，并掌握进行更高级深度学习项目的工具。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下几种方法：
- en: Installing PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: Basic operations in PyTorch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的基本操作
- en: Advanced operations in PyTorch
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的高级操作
- en: Building a simple neural network with PyTorch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建一个简单的神经网络
- en: Training a feedforward neural network
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练前馈神经网络
- en: Training a recurrent neural network
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环神经网络
- en: Training an LSTM neural network
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 LSTM 神经网络
- en: Training a convolutional neural network
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练卷积神经网络
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before starting, you will need to ensure that your system meets the following
    technical requirements:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，您需要确保您的系统满足以下技术要求：
- en: '**Python 3.9**: You can download Python from [https://www.python.org/downloads/](https://www.python.org/downloads/).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 3.9**：您可以从[https://www.python.org/downloads/](https://www.python.org/downloads/)
    下载 Python。'
- en: 'pip (23.3.1) or Anaconda: These are popular package managers for Python. pip
    comes with Python by default. Anaconda can be downloaded from [https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pip（23.3.1）或 Anaconda：这些是 Python 的常用包管理器。pip 默认与 Python 一起安装。Anaconda 可以从[https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution)下载。
- en: 'torch (2.2.0): The main library we will be using for deep learning in this
    chapter.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torch（2.2.0）：本章中我们将使用的主要深度学习库。
- en: '**CUDA (optional)**: If you have a CUDA-capable GPU on your machine, you can
    install a version of PyTorch that supports CUDA. This will enable computations
    on your GPU and can significantly speed up your deep learning experiments.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CUDA（可选）**：如果您的计算机上有支持 CUDA 的 GPU，您可以安装支持 CUDA 的 PyTorch 版本。这将使您能够在 GPU 上进行计算，并且可以显著加快深度学习实验的速度。'
- en: It’s worth noting that the code presented in this chapter is platform-independent
    and should run on any system with the preceding requirements satisfied.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，本章中介绍的代码是平台无关的，并且应当可以在满足前述要求的任何系统上运行。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub 地址找到：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。
- en: Installing PyTorch
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: To start with PyTorch, we need to install it first. As of the time of writing,
    PyTorch supports Linux, macOS, and Windows platforms. Here, we will guide you
    through the installation process on these operating systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 PyTorch，首先需要安装它。根据写作时的信息，PyTorch 支持 Linux、macOS 和 Windows 平台。在这里，我们将引导您完成这些操作系统上的安装过程。
- en: Getting ready
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: '`PyTorch` is usually installed via `pip` or Anaconda. We recommend creating
    a new Python environment before installing the library, especially if you will
    be working on multiple Python projects on your system. This is to prevent any
    conflicts between different versions of Python libraries that different projects
    may require.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch` 通常通过 `pip` 或 Anaconda 安装。我们建议在安装库之前创建一个新的 Python 环境，特别是当您需要在系统上进行多个
    Python 项目时。这是为了避免不同项目可能需要的 Python 库版本之间发生冲突。'
- en: How to do it…
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: Let’s see how to install `PyTorch`. We’ll describe how to do this using either
    `pip` or `Anaconda`. We’ll also provide some information about how to use a CUDA
    environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何安装`PyTorch`。我们将描述如何使用`pip`或`Anaconda`来完成此操作。我们还将提供有关如何使用CUDA环境的一些信息。
- en: 'If you’re using `pip`, Python’s package manager, you can install PyTorch by
    running the following command in your terminal:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用`pip`，Python的包管理器，你可以在终端中运行以下命令来安装PyTorch：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With the Anaconda Python distribution, you can install PyTorch using the following
    command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Anaconda Python发行版，你可以使用以下命令安装PyTorch：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you have a CUDA-capable GPU on your machine, you can install a version of
    `PyTorch` that supports CUDA to enable computations on your GPU. This can significantly
    speed up your deep learning experiments. The PyTorch website provides a tool that
    generates the appropriate installation command based on your needs. Visit the
    PyTorch website, select your preferences (such as OS, package manager, Python
    version, and CUDA version) in the **Quick Start Locally** section, and then copy
    the generated command into your terminal.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器上有支持CUDA的GPU，你可以安装支持CUDA的`PyTorch`版本，以便在GPU上进行计算。这可以显著加速你的深度学习实验。PyTorch官网提供了一个工具，根据你的需求生成相应的安装命令。访问PyTorch官网，在**Quick
    Start Locally**部分选择你的偏好（如操作系统、包管理器、Python版本和CUDA版本），然后将生成的命令复制到终端中。
- en: How it works…
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'After you’ve installed `PyTorch`, you can verify that everything is working
    correctly by opening a Python interpreter and running the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`PyTorch`后，你可以通过打开Python解释器并运行以下代码来验证一切是否正常工作：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This should output the version of `PyTorch` that you installed. Now, you’re
    ready to start using `PyTorch` for deep learning!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出你安装的`PyTorch`版本。现在，你已经准备好开始使用`PyTorch`进行深度学习了！
- en: In the next sections, we will familiarize ourselves with the basics of `PyTorch`
    and build our first neural network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将熟悉`PyTorch`的基础知识，并构建我们的第一个神经网络。
- en: Basic operations in PyTorch
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的基本操作
- en: Before we start building neural networks with `PyTorch`, it is essential to
    understand the basics of how to manipulate data using this library. In `PyTorch`,
    the fundamental unit of data is the tensor, a generalization of matrices to an
    arbitrary number of dimensions (also known as a multidimensional array).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用`PyTorch`构建神经网络之前，理解如何使用这个库操作数据是至关重要的。在`PyTorch`中，数据的基本单元是张量，它是矩阵的一种推广，支持任意维度（也称为多维数组）。
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: A tensor can be a number (a 0D tensor), a vector (a 1D tensor), a matrix (a
    2D tensor), or any multi-dimensional data (a 3D tensor, a 4D tensor, and so on).
    `PyTorch` provides various functions to create and manipulate tensors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以是一个数字（0维张量），一个向量（1维张量），一个矩阵（2维张量），或者任何多维数据（3维张量、4维张量等等）。`PyTorch`提供了多种函数来创建和操作张量。
- en: How to do it…
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let’s start by importing `PyTorch`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入`PyTorch`开始：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can create a tensor in `PyTorch` using various techniques. Let’s start by
    creating tensors from lists:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用各种技术在`PyTorch`中创建张量。让我们从使用列表创建张量开始：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`PyTorch` can seamlessly integrate with NumPy, allowing for easy tensor creation
    from `NumPy` arrays:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch`可以与`NumPy`无缝集成，允许从`NumPy`数组轻松创建张量：'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`PyTorch` also provides functions to generate tensors with specific values,
    such as zeros or ones:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch`还提供了生成特定值（如零或一）张量的函数：'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These are commonly used methods in `NumPy` that are also available in `PyTorch`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是`NumPy`中常用的方法，`PyTorch`中也可以使用这些方法。
- en: How it works…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Now that we know how to create tensors, let’s look at some basic operations.
    We can perform all standard arithmetic operations on tensors:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何创建张量，让我们来看看一些基本操作。我们可以对张量执行所有标准的算术操作：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can reshape tensors using the `.``reshape()` method:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`.reshape()`方法来重塑张量：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is a brief introduction to tensor operations in `PyTorch`. As you dive
    deeper, you’ll find that `PyTorch` offers various operations to manipulate tensors,
    giving you the flexibility and control needed to implement complex deep learning
    models and algorithms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`PyTorch`中张量操作的简要介绍。随着你深入学习，你会发现`PyTorch`提供了多种操作来处理张量，给予你实现复杂深度学习模型和算法所需的灵活性和控制力。
- en: Advanced operations in PyTorch
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的高级操作
- en: After exploring basic tensor operations, let’s now dive into more advanced operations
    in `PyTorch`, specifically the linear algebra operations that form the backbone
    of most numerical computations in deep learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了基本的张量操作后，现在让我们深入了解 `PyTorch` 中的更高级操作，特别是构成深度学习中大多数数值计算基础的线性代数操作。
- en: Getting ready
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Linear algebra is a subset of mathematics. It deals with vectors, vector spaces,
    and linear transformations between these spaces, such as rotations, scaling, and
    shearing. In the context of deep learning, we deal with high-dimensional vectors
    (tensors), and operations on these vectors play a crucial role in the internal
    workings of models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是数学的一个子集。它涉及向量、向量空间及这些空间之间的线性变换，如旋转、缩放和剪切。在深度学习的背景下，我们处理的是高维向量（张量），对这些向量的操作在模型的内部工作中起着至关重要的作用。
- en: How to do it…
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Let’s start by revisiting the tensors we created in the previous section:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从回顾上一节中创建的张量开始：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The dot product of two vectors is a scalar that measures the vectors’ direction
    and magnitude. In `PyTorch`, we can calculate the dot product of two `1D` tensors
    using the `torch.dot()` function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量的点积是一个标量，衡量向量的方向和大小。在 `PyTorch` 中，我们可以使用 `torch.dot()` 函数计算两个 `1D` 张量的点积：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Unlike element-wise multiplication, matrix multiplication, also known as the
    dot product, is the operation of multiplying two matrices to produce a new matrix.
    `PyTorch` provides the `torch.mm()` function to perform matrix multiplication:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与逐元素相乘不同，矩阵乘法，也叫做点积，是将两个矩阵相乘以产生一个新的矩阵的操作。`PyTorch` 提供了 `torch.mm()` 函数来执行矩阵乘法：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The transpose of a matrix is a new matrix whose rows are the columns of the
    original matrix and whose columns are the rows. You can compute the transpose
    of a tensor using the `.``T` attribute:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的转置是一个新矩阵，它的行是原始矩阵的列，而列是原始矩阵的行。你可以使用 `.T` 属性来计算张量的转置：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are other operations you can perform, such as calculating the determinant
    of a matrix and finding the inverse of a matrix. Let’s look at a couple of these
    operations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以执行其他操作，例如计算矩阵的行列式和求矩阵的逆。让我们看几个这样的操作：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that these two operations are only defined for `2D` tensors (matrices).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这两个操作仅在 `2D` 张量（矩阵）上定义。
- en: How it works…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: '`PyTorch` is a highly optimized library for performing basic and advanced operations,
    particularly linear algebra operations that are crucial in deep learning.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch` 是一个高度优化的库，特别适用于执行基本和高级操作，尤其是深度学习中至关重要的线性代数操作。'
- en: These operations make `PyTorch` a powerful tool for building and training neural
    networks and performing high-level computations in a more general context. In
    the next section, we will use these building blocks to start constructing deep
    learning models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作使得 `PyTorch` 成为构建和训练神经网络以及在更一般的背景下执行高阶计算的强大工具。在下一节中，我们将使用这些构建块开始构建深度学习模型。
- en: Building a simple neural network with PyTorch
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建一个简单的神经网络
- en: This section will build a simple two-layer neural network from scratch using
    only basic tensor operations to solve a time series prediction problem. We aim
    to demonstrate how one might manually implement a feedforward pass, backpropagation,
    and optimization steps without leveraging `PyTorch`’s predefined layers and optimization
    routines.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将从头开始构建一个简单的两层神经网络，仅使用基本的张量操作来解决时间序列预测问题。我们旨在演示如何手动实现前向传播、反向传播和优化步骤，而不依赖于
    `PyTorch` 的预定义层和优化例程。
- en: Getting ready
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We use synthetic data for this demonstration. Suppose we have a simple time
    series data of `100` samples, each with `10` time steps. Our task is to predict
    the next time step based on the previous ones:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用合成数据进行这个演示。假设我们有一个简单的时间序列数据，共 `100` 个样本，每个样本有 `10` 个时间步。我们的任务是根据前面的时间步预测下一个时间步：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, let’s create a neural network.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个神经网络。
- en: How to do it…
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Let’s start by defining our model parameters and their initial values. Here,
    we are creating a simple two-layer network, so we have two sets of weights and
    biases:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义模型参数及其初始值开始。在这里，我们创建了一个简单的两层网络，因此我们有两组权重和偏置：
- en: We use the `requires_grad_()` function to tell `PyTorch` that we want to compute
    gradients with respect to these tensors during the backward pass.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `requires_grad_()` 函数告诉 `PyTorch`，我们希望在反向传播时计算这些张量的梯度。
- en: 'Next, we define our model. For this simple network, we’ll use a sigmoid activation
    function for the hidden layer:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的模型。对于这个简单的网络，我们将在隐藏层使用 sigmoid 激活函数：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we’re ready to train our model. Let’s define the learning rate and the
    number of epochs:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好训练模型了。让我们定义学习率和训练的轮次（epochs）：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This basic code demonstrates the essential parts of a neural network: the forward
    pass, where we compute predictions; the backward pass, where gradients are computed;
    and the update step, where we adjust our weights to minimize the loss.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这段基本代码演示了神经网络的基本部分：前向传播，我们计算预测值；反向传播，我们计算梯度；以及更新步骤，我们调整权重以最小化损失。
- en: There’s more…
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: This chapter is focused on exploring the intricacies of the training process
    of a neural network. In future chapters, we’ll show how to train deep neural networks
    without worrying about most of these details.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点探讨神经网络训练过程的复杂性。在未来的章节中，我们将展示如何训练深度神经网络，而无需担心这些细节。
- en: Training a feedforward neural network
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练前馈神经网络
- en: This recipe walks you through the process of building a feedforward neural network
    using PyTorch.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将带你逐步完成使用 PyTorch 构建前馈神经网络的过程。
- en: Getting ready
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Feedforward neural networks, also known as **multilayer perceptrons** (**MLPs**),
    are one of the simplest types of artificial neural networks. The data flows from
    the input layer to the output layer, passing through hidden layers without any
    loop. In this type of neural network, all hidden units in one layer are connected
    to the units of the following layer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络，也被称为**多层感知器**（**MLPs**），是最简单的人工神经网络之一。数据从输入层流向输出层，经过隐藏层，不包含任何循环。在这种类型的神经网络中，一层的所有隐藏单元都与下一层的单元相连。
- en: How to do it…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Let’s create a simple feedforward neural network using `PyTorch`. First, we
    need to import the necessary `PyTorch` modules:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `PyTorch` 创建一个简单的前馈神经网络。首先，我们需要导入必要的 `PyTorch` 模块：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can define a simple feedforward neural network with one hidden layer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一个带有单一隐藏层的简单前馈神经网络：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, `nn.Module` is the base class for all neural network
    modules in `PyTorch`, and our network is a subclass of it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`nn.Module` 是 `PyTorch` 中所有神经网络模块的基类，我们的网络是它的一个子类。
- en: 'The `forward()` method in this class represents the forward pass of the network.
    This is the computation that the network performs when transforming inputs into
    outputs. Here’s a step-by-step explanation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该类中的 `forward()` 方法表示网络的前向传播过程。这是网络在将输入转换为输出时执行的计算。以下是逐步的解释：
- en: The `forward()` method takes an input tensor `x`. This tensor represents the
    input data. Its shape should be compatible with the network’s layers. In this
    case, as the first linear layer (`self.fc1`) expects `10` input features, the
    last dimension of `x` should be `10`.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法接收一个输入张量 `x`。这个张量表示输入数据，它的形状应该与网络的层兼容。在这里，作为第一个线性层（`self.fc1`）期望有
    `10` 个输入特征，`x` 的最后一个维度应该是 `10`。'
- en: The input tensor is first passed through a linear transformation, represented
    by `self.fc1`. This object is an instance of `PyTorch`’s `nn.Linear` class, and
    it performs a linear transformation that involves multiplying the input data with
    a weight matrix and adding a bias vector. As defined in the `__init__``()` method,
    this layer transforms the 10D space to a 5D space using a linear transformation.
    This reduction is often seen as the neural network “learning” or “extracting”
    features from the input data.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入张量首先通过线性变换处理，由 `self.fc1` 表示。这个对象是 `PyTorch` 的 `nn.Linear` 类的一个实例，它执行线性变换，涉及用权重矩阵乘以输入数据并加上偏置向量。正如在
    `__init__()` 方法中定义的那样，这一层将 10 维空间转化为 5 维空间，使用的是线性变换。这个降维过程通常被视为神经网络“学习”或“提取”输入数据中的特征。
- en: The output of the first layer is then passed through a `torch.relu()`. This
    is a simple non-linearity that replaces negative values in the tensor with zeros.
    This allows the neural network to model more complex relationships between the
    inputs and the outputs.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层的输出随后通过 `torch.relu()` 进行处理。这是一个简单的非线性函数，它将张量中的负值替换为零。这使得神经网络能够建模输入和输出之间更复杂的关系。
- en: The output from the `ReLU``()` function is then passed through another linear
    transformation, `self.fc2`. As before, this object is an instance of `PyTorch`’s
    `nn.Linear` class. This layer reduces the dimensionality of the tensor from `5`
    (the output size of the previous layer) to `1` (the desired output size).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReLU()`函数的输出接着通过另一个线性变换`self.fc2`。和之前一样，这个对象是`PyTorch`的`nn.Linear`类的一个实例。这个层将张量的维度从`5`（前一层的输出大小）缩减到`1`（所需的输出大小）。'
- en: Finally, the output of the second linear layer is returned by the `forward()`
    method. This output can then be used for various purposes, such as computing a
    loss for training the network, or as the final output in an inference task (that
    is when the network is used for prediction).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第二个线性层的输出由`forward()`方法返回。这个输出可以用于多种目的，例如计算用于训练网络的损失，或者作为推理任务中的最终输出（即网络用于预测时）。
- en: How it works…
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: To train the network, we need a dataset to train on, a loss function, and an
    optimizer.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练网络，我们需要一个数据集，一个损失函数，以及一个优化器。
- en: 'Let’s use the same synthetic dataset that we defined for our previous example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与前一个示例相同的合成数据集：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can use the **mean squared error** (**MSE**) loss for our task, which is
    a common loss function for regression problems. PyTorch provides a built-in implementation
    of this loss function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**均方误差**（**MSE**）损失来进行我们的任务，这是回归问题中常用的损失函数。PyTorch提供了这个损失函数的内置实现：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will use **stochastic gradient descent** (**SGD**) as our optimizer. SGD
    is a type of iterative method for optimizing the objective function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**随机梯度下降**（**SGD**）作为我们的优化器。SGD是一种迭代方法，用于优化目标函数：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can train our network. We’ll do this for `100` epochs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练我们的网络。我们将训练`100`个周期：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In each epoch, we perform a forward pass, compute the loss, perform a backward
    pass to calculate gradients, and then update our weights.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期中，我们执行前向传播，计算损失，进行反向传播以计算梯度，然后更新权重。
- en: You have now trained a simple feedforward neural network using `PyTorch`. In
    the upcoming sections, we will dive deeper into more complex network architectures
    and their applications in time series analysis.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经使用`PyTorch`训练了一个简单的前馈神经网络。在接下来的章节中，我们将深入探讨更复杂的网络架构及其在时间序列分析中的应用。
- en: Training a recurrent neural network
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练递归神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are a class of neural networks that
    are especially effective for tasks involving sequential data, such as time series
    forecasting and natural language processing.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNNs**）是一类神经网络，特别适用于涉及序列数据的任务，如时间序列预测和自然语言处理。'
- en: Getting ready
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: RNNs use sequential information by having hidden layers capable of passing information
    from one step in the sequence to the next.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通过具有隐藏层，能够将序列中的信息从一个步骤传递到下一个步骤，从而利用序列信息。
- en: How to do it…
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Similar to the feedforward network, we begin by defining our `RNN` class. For
    simplicity, let’s define a single-layer `RNN`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前馈神经网络，我们首先定义了`RNN`类。为了简化，假设我们定义了一个单层的`RNN`：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, `input_size` is the number of input features per time step, `hidden_size`
    is the number of neurons in the hidden layer, and `output_size` is the number
    of output features. In the `forward()`method, we pass the input `x` and the initial
    hidden state `h0` to the recurrent layer. The RNN returns the output and the final
    hidden state, which we ignore for now. We then take the last output of the sequence
    (`out[:, -1, :]`) and pass it through a fully connected layer to get our final
    output. The hidden states act as the memory of the network, encoding the temporal
    context of the inputs up to the current time step, which is why this type of neural
    network is useful for sequential data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`input_size`是每个时间步的输入特征数量，`hidden_size`是隐藏层中的神经元数量，`output_size`是输出特征的数量。在`forward()`方法中，我们将输入`x`和初始隐藏状态`h0`传递给递归层。RNN返回输出和最终的隐藏状态，我们暂时忽略隐藏状态。然后，我们取序列的最后一个输出（`out[:,
    -1, :]`），并通过一个全连接层得到最终输出。隐藏状态充当网络的记忆，编码输入的时间上下文，直到当前时间步，这也是这种类型的神经网络在序列数据中非常有用的原因。
- en: 'Let’s note some details we used in our code in this example:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们注意一下在代码中使用的一些细节：
- en: '`x.device`: This refers to the device where the `x` tensor is located. In `PyTorch`,
    tensors can be on the CPU or a GPU, and `.device` is a property that tells you
    where the tensor currently resides. This is particularly important when you are
    running computations on a GPU, as all inputs to a computation must be on the same
    device. In the line of code `h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)`,
    we’re ensuring that the initial hidden state tensor `h0` is on the same device
    as the `x` input tensor.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x.device`：指的是张量`x`所在的设备。在`PyTorch`中，张量可以位于CPU或GPU上，而`.device`是一个属性，表示张量当前所在的设备。当你在GPU上进行计算时，所有输入到计算的张量必须位于相同的设备上。在代码行`h0
    = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)`中，我们确保初始隐藏状态张量`h0`与`x`输入张量位于同一设备上。'
- en: '`x.size(0)`: This refers to the size of the `0th` dimension of the tensor `x`.
    In `PyTorch`, `size()` returns the shape of the tensor, and `size(0)` gives the
    size of the first dimension. In the context of this RNN, `x` is expected to be
    a 3D tensor with shape (`batch_size`, `sequence_length`, `num_features`), so `x.size(0)`
    would return the batch size.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x.size(0)`：指的是张量`x`的第`0`维的大小。在`PyTorch`中，`size()`返回张量的形状，而`size(0)`给出第一维的大小。在这个RNN的上下文中，`x`预计是一个形状为（`batch_size`、`sequence_length`、`num_features`）的3D张量，因此`x.size(0)`会返回批次大小。'
- en: How it works…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The training process for RNNs is similar to that of feedforward networks. We’ll
    use the same synthetic dataset, loss function (MSE), and optimizer (SGD) from
    the previous example. However, let’s modify the input data to be 3D, as required
    by the RNN (`batch_size`, `sequence_length`, `num_features`). The three dimensions
    of the input tensor to an RNN represent the following aspects:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的训练过程与前馈网络类似。我们将使用之前示例中的相同合成数据集、损失函数（MSE）和优化器（SGD）。不过，让我们将输入数据修改为3D格式，以满足RNN的要求（`batch_size`、`sequence_length`、`num_features`）。RNN输入张量的三个维度代表以下方面：
- en: '`batch_size`: This represents the number of sequences in one batch of data.
    In time series terms, you can think of one sample as one sub-sequence (for example,
    the sales of the past five days). So, a batch contains multiple such samples or
    sub-sequences, allowing the model to process and learn from multiple sequences
    simultaneously.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：表示每个批次数据中的序列数。在时间序列中，你可以把一个样本看作是一个子序列（例如，过去五天的销售数据）。因此，一个批次包含多个这样的样本或子序列，允许模型同时处理和学习多个序列。'
- en: '`sequence_length`: This is essentially the size of the window you use to look
    at your data. It specifies the number of time steps included in each input sub-sequence.
    For instance, if you’re predicting today’s temperature based on past data, `sequence_length`
    determines how many days back in the past your model looks at each step.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_length`：本质上是你用来观察数据的窗口大小。它指定了每个输入子序列包含的时间步数。例如，如果你是基于过去的数据预测今天的温度，`sequence_length`就决定了模型每次查看的数据向后回溯了多少天。'
- en: '`num_features`: This dimension indicates the number of features (variables)
    in each time step of the data sequence. In the context of time series, a univariate
    series (such as daily temperature at a single location) has one feature per time
    step. In contrast, a multivariate series (such as daily temperature, humidity,
    and wind speed at the same location) has multiple features per time step.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_features`：该维度表示数据序列中每个时间步的特征（变量）数量。在时间序列的上下文中，单变量序列（例如某一地点的每日温度）在每个时间步只有一个特征。相比之下，多变量序列（例如同一地点的每日温度、湿度和风速）在每个时间步有多个特征。'
- en: 'Let’s create a synthetic dataset as an example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个合成数据集作为示例：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we can train our network. We’ll do this for `100` epochs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练我们的网络。我们将进行`100`轮训练：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, we have trained an RNN. This is a big step towards applying these models
    to real-world time series data, which we will discuss in the next chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经训练了一个RNN。这是将这些模型应用于现实世界时间序列数据的一个重要步骤，我们将在下一章讨论这一点。
- en: Training an LSTM neural network
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练LSTM神经网络
- en: RNNs suffer from a fundamental problem of “vanishing gradients” where, due to
    the nature of backpropagation in neural networks, the influence of earlier inputs
    on the overall error diminishes drastically as the sequence gets longer. This
    is especially problematic in sequence processing tasks where long-term dependencies
    exist (i.e., future outputs depend on much earlier inputs).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 存在一个根本问题，即“梯度消失”问题，由于神经网络中反向传播的性质，较早输入对整体误差的影响在序列长度增加时急剧减小。在存在长期依赖的序列处理任务中尤其严重（即未来的输出依赖于很早之前的输入）。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: LSTM networks were introduced to overcome this problem. They use a more complex
    internal structure for each of their cells compared to RNNs. Specifically, an
    LSTM has the ability to decide which information to discard or to store based
    on an internal structure called a cell. This cell uses gates (input, forget, and
    output gates) to control the flow of information into and out of the cell. This
    helps maintain and manipulate the “long-term” information, thereby mitigating
    the vanishing gradient problem.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络被引入以克服这个问题。与 RNN 相比，它们为每个单元使用了更复杂的内部结构。具体来说，LSTM 能够根据一个叫做细胞的内部结构来决定丢弃或保存哪些信息。这个细胞通过门控（输入门、遗忘门和输出门）来控制信息的流入和流出。这有助于保持和操作“长期”信息，从而缓解梯度消失问题。
- en: How to do it…
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We begin by defining our `LSTM` class. For simplicity, we’ll define a single-layer
    `LSTM` network. Note that `PyTorch`’s LSTM expects inputs to be 3D in the format
    `batch_size`, `seq_length`, and `num_features`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义 `LSTM` 类。为了简化起见，我们将定义一个单层的 `LSTM` 网络。请注意，`PyTorch` 的 LSTM 期望输入是 3D 的，格式为
    `batch_size`，`seq_length` 和 `num_features`：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `forward()` method is very similar to the one that we introduced earlier
    for RNNs. The main difference resides in the fact that in the RNNs’ case, we initialized
    a single hidden state `h0`, and passed it to the `RNN` layer along with the input
    `x`. In the LSTM, however, you need to initialize both a hidden state `h0` and
    a cell state `c0` because of the internal structure of LSTM cells. These states
    are then passed as a tuple to the `LSTM` layer along with the input `x`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()` 方法与我们之前介绍的 RNN 方法非常相似。主要的区别在于，在 RNN 的情况下，我们初始化了一个单一的隐藏状态 `h0`，并将其与输入
    `x` 一起传递给 `RNN` 层。而在 LSTM 中，你需要初始化隐藏状态 `h0` 和细胞状态 `c0`，这是因为 LSTM 单元的内部结构。然后，这些状态作为元组与输入
    `x` 一起传递给 `LSTM` 层。'
- en: How it works…
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The training process for LSTM networks is similar to that of feedforward networks
    and RNNs. We’ll use the same synthetic dataset, loss function (MSE), and optimizer
    (SGD) from the previous examples:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络的训练过程与前馈网络和 RNN 的训练过程相似。我们将使用之前示例中的相同合成数据集、损失函数（MSE）和优化器（SGD）：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Training a convolutional neural network
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练卷积神经网络
- en: '**Convolutional neural networks** (**CNNs**) are a class of neural networks
    particularly effective for tasks involving grid-like input data such as images,
    audio spectrograms, and even certain types of time series data.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是一类特别适用于网格状输入数据（如图像、音频谱图，甚至某些类型的时间序列数据）的神经网络。'
- en: Getting ready
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The central idea of CNNs is to apply a convolution operation on the input data
    with convolutional filters (also known as kernels), which slide over the input
    data to produce output feature maps.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的核心思想是使用卷积滤波器（也称为内核）对输入数据进行卷积操作，这些滤波器滑过输入数据并产生输出特征图。
- en: How to do it…
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'For simplicity, let’s define a single-layer `1D` convolutional neural network,
    which is particularly suited for time series and sequence data. In `PyTorch`,
    we can use the `nn.Conv1d` layer for this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们定义一个单层的 `1D` 卷积神经网络，这特别适用于时间序列和序列数据。在 `PyTorch` 中，我们可以使用 `nn.Conv1d`
    层来实现：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the `forward` method, we pass the input through a convolutional layer followed
    by a `ReLU``()` activation function and finally pass it through a fully connected
    layer. The `Conv1d` layer expects an input of shape (`batch_size`, `num_channels`,
    and `sequence_length`). Here, `num_channels` refers to the number of input channels
    (equivalent to the number of features in the time series data), and `sequence_length`
    refers to the number of time steps in each sample.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `forward` 方法中，我们将输入通过卷积层，然后是 `ReLU()` 激活函数，最后通过一个全连接层。`Conv1d` 层期望输入的形状为（`batch_size`，`num_channels`，和
    `sequence_length`）。其中，`num_channels` 指输入通道的数量（相当于时间序列数据中的特征数量），`sequence_length`
    则指每个样本的时间步数。
- en: The `Linear` layer will take the output from the `Conv1d` layer and reduce it
    to the desired output size. The input to the `Linear` layer is calculated as `hidden_size*(seq_length-kernel_size+1)`,
    where `hidden_size` is the number of output channels from the `Conv1d` layer,
    and `seq_length-kernel_size+1` is the output sequence length after the convolution
    operation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Linear`层将接受来自`Conv1d`层的输出，并将其缩减到所需的输出大小。`Linear`层的输入计算为`hidden_size*(seq_length-kernel_size+1)`，其中`hidden_size`是`Conv1d`层的输出通道数，`seq_length-kernel_size+1`是卷积操作后的输出序列长度。'
- en: How it works…
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The training process for `1D` CNNs is similar to the previous network types.
    We’ll use the same loss function (MSE), and optimizer (SGD), but let’s modify
    the input data to be of size (`batch_size`, `sequence_length`, `num_channels`).
    Recall that the number of channels is equivalent to the number of features:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`1D` CNN的训练过程与前面的网络类型类似。我们将使用相同的损失函数（MSE）和优化器（SGD），但我们将修改输入数据的大小为（`batch_size`，`sequence_length`，`num_channels`）。请记住，通道数等于特征的数量：'
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we can train our network. We’ll do this for `100` epochs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练我们的网络。我们将进行`100`个训练周期：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, we iterate over each epoch. After each training cycle,
    we print the error of the model into the console to monitor the training process.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们对每个训练周期进行迭代。每个训练周期结束后，我们将模型的误差打印到控制台，以便监控训练过程。
