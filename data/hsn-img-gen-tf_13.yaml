- en: '*Chapter 10*: Road Ahead'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*: 前路'
- en: This is the final chapter of the book. We have learned about and implemented
    many generative models; and yet there are a lot more models and applications that
    we have not covered as they are beyond the scope of this book. In this chapter,
    we will start by summarizing some of the important techniques that we have learned,
    such as **optimizer and activation functions**, **adversarial loss**, **auxiliary
    loss**, **normalization**, and **regularization**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章。我们学习并实现了许多生成模型，但还有更多模型和应用我们没有涉及，因为它们超出了本书的范围。在本章中，我们将从总结一些我们学过的重要技术开始，如**优化器和激活函数**、**对抗损失**、**辅助损失**、**归一化**和**正则化**。
- en: Then, we will look at some of the common pitfalls when using generative models
    in real-world settings. After that, we will go over some interesting image/video
    generative models and applications. There is no coding in this chapter, but you
    will find that many of the new models that we introduce in this chapter are built
    using techniques we have learned previously. There are also a few links to resources
    where you can read papers and code to explore the technology.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将探讨在实际应用中使用生成模型时常见的一些陷阱。之后，我们将介绍一些有趣的图像/视频生成模型和应用。本章没有编码内容，但你会发现，我们在本章介绍的许多新模型都是使用之前学过的技术构建的。本章还提供了一些链接，供你查阅论文和代码，进一步探索该技术。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Reviewing GANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾GAN
- en: Putting your skills into practice
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的技能付诸实践
- en: Image processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Text to image
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像
- en: Video retargeting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频重定向
- en: Neural rendering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经渲染
- en: Reviewing GANs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾GAN
- en: 'Apart from PixelCNN, which we covered in [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017),
    *Getting Started with Image Generation Using TensorFlow*, which is a CNN, all
    the other generative models we have learned about are based on (variational) autoencoders
    or **generative adversarial networks** (**GANs**). Strictly speaking, a GAN is
    not a network but a training method that makes use of two networks – a generator
    and a discriminator. I tried to fit a lot of content into this book; so, the information
    can be overwhelming. We will now go over a summary of the important techniques
    we have learned, by grouping them into the following categories:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在[*第1章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)中介绍的PixelCNN，*使用TensorFlow进行图像生成入门*，这是一个CNN外，其他所有我们学习过的生成模型都基于（变分）自编码器或**生成对抗网络**（**GANs**）。严格来说，GAN不是一种网络，而是一种训练方法，利用了两个网络——生成器和判别器。我尝试将大量内容融入本书，因此信息可能会让人感到有些压倒性。接下来，我们将通过以下类别总结我们所学的重要技术：
- en: Optimizer and activation functions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器和激活函数
- en: Adversarial loss
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗损失
- en: Auxiliary loss
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助损失
- en: Normalization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化
- en: Regularization
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: Optimizer and activation functions
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器和激活函数
- en: '`0` and the second moment is set to `0.999`. The learning rate for the generator
    is set to `0.0001`, while the discriminator uses a learning rate that is two to
    four times larger than that. The discriminator is the key component in a GAN and
    it needs to learn well before the generator does. WGAN trains the discriminator
    more times than the generator in the training step, and an alternative to that
    is to use a higher learning rate for the discriminator.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`0`和第二矩设定为`0.999`。生成器的学习率设定为`0.0001`，而判别器的学习率是生成器的两到四倍。判别器是GAN中的关键组件，必须在生成器之前学得很好。WGAN在训练步骤中训练判别器的次数多于生成器，另一种方法是为判别器使用更高的学习率。'
- en: On the other hand, the de facto activation function for internal layers is leaky
    ReLU with an alpha of `0.01` or `0.02`. The choice of the generator's output activation
    functions depends on the image normalization, that is, sigmoid for a pixel range
    of `[0, 1]` or Tanh for `[-1, 1]`. On the other hand, the discriminator uses a
    linear output activation function for most adversarial losses, apart from sigmoid
    for the earlier non-saturated loss.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，内部层的实际激活函数是带有`0.01`或`0.02`的泄漏ReLU。生成器的输出激活函数的选择取决于图像的归一化方式，即对于像素范围`[0,
    1]`使用sigmoid，或者对于`[-1, 1]`使用Tanh。另一方面，判别器在大多数对抗损失中使用线性输出激活函数，除了早期非饱和损失使用sigmoid。
- en: Adversarial loss
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗损失
- en: 'We have seen that an autoencoder can be used as a generator in a GAN setting.
    GANs are trained using adversarial loss (sometimes called GAN loss). The following
    table lists some of the popular adversarial losses:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，自编码器可以作为GAN设置中的生成器。GAN通过对抗损失（有时称为GAN损失）进行训练。下表列出了几种流行的对抗损失：
- en: '![Figure 10.1 – Important adversarial loss; σ refers to the sigmoid function'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – 重要的对抗损失；σ指的是sigmoid函数'
- en: '](img/B14538_10_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_10_01.jpg)'
- en: Figure 10.1 – Important adversarial loss; σ refers to the sigmoid function
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 重要的对抗损失；σ指的是sigmoid函数
- en: Non-saturated loss is used in vanilla GANs but it is unstable due to disjoining
    gradients. Wasserstein loss has theory underpinning it that proves it is more
    stable to train with. However, many GAN models choose to use least-square loss,
    which has shown to be stable too. In recent years, hinge loss has become the favorite
    choice of many state-of-the-art models. It is not clear which loss is the best.
    Nevertheless, we have used least-square and hinge loss in many models in this
    book and they seem to train well. So, I would suggest you try them first when
    designing your new GANs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 非饱和损失用于普通的GAN，但由于梯度分离，它不稳定。Wasserstein损失有理论支持，证明它在训练时更稳定。然而，许多GAN模型选择使用最小二乘损失，这也证明是稳定的。近年来，hinge损失成为了许多最先进模型的首选。尚不清楚哪种损失最优。不过，在本书中，我们在许多模型中使用了最小二乘损失和hinge损失，它们似乎能很好地训练。所以，我建议你在设计新的GAN时，首先尝试这两种损失。
- en: Auxiliary loss
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 辅助损失
- en: 'Apart from adversarial loss, which acts as the main loss in GAN training, there
    are various auxiliary losses that help generate better images. Some of them are
    as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对抗损失，作为GAN训练的主要损失外，还有各种辅助损失帮助生成更好的图像。以下是其中一些：
- en: '**Reconstruction loss** *(*[*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder)* to encourage pixel-wise accuracy, this is usually L1
    loss.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重建损失** *(*[*第2章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, 变分自编码器)*
    用于鼓励逐像素的准确性，通常是L1损失。'
- en: '**KL divergence loss** *(*[*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder)* for the **variational autoencoder** (**VAE**) to bring
    the latent vector to a standard, multivariate normal distribution.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KL散度损失** *(*[*第2章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, 变分自编码器)*
    用于**变分自编码器**（**VAE**），将潜在向量转换为标准的多元正态分布。'
- en: '**Cycle consistency loss** *(*[*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation)* for bi-direction image translation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环一致性损失** *(*[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*, 图像到图像的翻译)*
    用于双向图像翻译。'
- en: '**Perceptual loss** *(*[*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*,
    Style Transfer)* measures high-level perceptual and semantic differences between
    images. It can be further divided into two losses:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感知损失** *(*[*第5章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*, 风格迁移)*
    衡量图像之间的高级感知和语义差异。它可以进一步细分为两种损失：'
- en: a) **Feature-matching loss**, which is normally the L2 loss of image features
    extracted from VGG layers. This is also called **perceptual loss**.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) **特征匹配损失**，通常是从VGG层提取的图像特征的L2损失。这也被称为**感知损失**。
- en: b) **Style loss** features are normally derived from VGG features, such as the
    Gram matrix or activation statistics, and are calculated using L2 loss.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) **风格损失**特征通常来自VGG特征，如Gram矩阵或激活统计信息，并使用L2损失计算。
- en: Normalization
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化
- en: 'Layer activations are normalized to stabilize network training. Normalization
    takes the following general form:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 层激活被归一化以稳定网络训练。归一化通常采用以下形式：
- en: '![](img/Formula_10_001.jpg)![](img/Formula_10_002.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_001.jpg)![](img/Formula_10_002.jpg)'
- en: 'Here, *x* is the activation, *µ* is the mean of activations, *σ* is the standard
    deviation of the activations, and *ε* is the fudge factor for numerical stability.
    *γ* and *β* are learnable parameters; there is one pair for each activation channel.
    Many of the different normalizations differ only in how *µ* and *σ* are obtained:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*是激活值，*µ*是激活值的均值，*σ*是激活值的标准差，*ε*是数值稳定性的调整因子。*γ*和*β*是可学习的参数；每个激活通道都有一对。这些不同的归一化方法主要的区别在于*µ*和*σ*是如何获取的：
- en: In **batch normalization** *(*[*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*,
    Generative Adversarial Network)*, the mean and standard deviation are calculated
    across both batch (*N*) and spatial (*H*, *W*) locations, or in other words, (*N*,
    *H*, *W*).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**批量归一化** *（[*第3章*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*，生成对抗网络）*中，均值和标准差是在批量（*N*）和空间（*H*，*W*）位置上计算的，换句话说，就是（*N*，*H*，*W*）。
- en: '**Instance normalization** *(*[*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation)* which is the preferred method nowadays, only uses
    the spatial dimension (*H*, *W*).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例归一化** *（[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*，图像到图像的转换）*，现在是首选方法，仅使用空间维度（*H*，*W*）。'
- en: '**Adaptive instance normalization** (**AdaIN**) *(*[*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*,
    Style Transfer)* serves a different purpose to merge the content and style activation.
    It still uses the equation, except that now the parameters have different meanings.
    *X* is still the activation we consider from the content feature. *γ* and *β*
    are no longer learnable parameters but the mean and standard deviation from the
    style features. Like with instance normalization, the statistics are calculated
    across a spatial dimension of (*H*, *W*).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应实例归一化**（**AdaIN**）*（[*第5章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*，风格迁移）*具有不同的目的，来合并内容和风格激活。它仍然使用相同的方程式，只是现在这些参数具有不同的含义。*X*仍然是我们从内容特征中考虑的激活值。*γ*和*β*不再是可学习的参数，而是来自风格特征的均值和标准差。与实例归一化类似，统计量是在空间维度（*H*，*W*）上计算的。'
- en: '**Spatially-adaptive normalization (SPADE)** ([*Chapter 6*](B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124)*,
    AI Painter*) has one *γ* and *β* value for each of the features (pixels), in other
    words, they have dimension of (H, W, C). They are produced by running convolutional
    layers across segmentation map to normalize pixels from different semantic objects
    separately.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间自适应归一化（SPADE）**（[*第6章*](B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124)*，AI
    Painter）*为每个特征（像素）提供一个*γ*和*β*值，换句话说，它们的维度是（H，W，C）。它们通过在分割图上运行卷积层来分别归一化来自不同语义对象的像素。'
- en: '**Conditional batch normalization** *(*[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation)* is just like batch normalization except
    that *γ* and *β* are now multi-dimensional of (LABELS, C), so there is one set
    of them per class label.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件批量归一化** *（[*第8章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*，用于图像生成的自注意力）*与批量归一化类似，只不过*γ*和*β*现在是多维的（LABELS，C），因此每个类别标签都有一组。'
- en: '`1`.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`1`。'
- en: Regularization
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: 'Apart from adversarial loss and normalization, the other important factor in
    stabilizing GAN training is regularization. Regularization aims to constrain the
    growth of the network weights in order to keep the competition between the generator
    and discriminator in check. This is normally done by adding a loss function that
    uses weights. The two common regularizations used in GANs aim to enforce the 1-Lipschitz
    constraint:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对抗损失和归一化，稳定GAN训练的另一个重要因素是正则化。正则化的目的是约束网络权重的增长，以保持生成器和判别器之间的竞争。这通常通过添加使用权重的损失函数来实现。GAN中常用的两种正则化方法旨在强制执行1-Lipschitz约束：
- en: '**Gradient penalty** *(*[*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*,
    Generative Adversarial Network)* penalizes the growth of gradients, hence the
    weights. However, it is not very commonly used due to the additional backpropagation
    required to calculate the gradients against the input. This slows down the computation
    considerably.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度惩罚** *（[*第3章*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*，生成对抗网络）*惩罚梯度的增长，从而惩罚权重。然而，由于需要额外的反向传播来计算梯度，这一方法并不常用，且会显著减慢计算速度。'
- en: '**Orthogonal regularization ***(*[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation)* aims to make the weights to be orthonormal
    matrices, this is because the matrix norm doesn''t change if it multiplies with
    an orthogonal matrix. This can avoid the vanishing or exploding gradient problems.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正交正则化** *（[*第8章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*，用于图像生成的自注意力）*旨在使权重成为正交矩阵，这是因为如果矩阵与正交矩阵相乘，矩阵的范数不会发生变化。这可以避免梯度消失或爆炸的问题。'
- en: '**Spectral normalization** *(*[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation)* normalizes the layer weights by dividing
    it by its spectral norm. This is different from usual regularizations that use
    loss function to constrain the weights. Spectral normalization is computationally
    efficient, easy to implement, and independent of the training loss. You should
    use it when designing a new GAN.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谱归一化** *(*[*第8章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*，图像生成的自注意力机制)*
    通过将层的权重除以其谱范数来进行归一化。这与常规的正则化方法不同，后者通过损失函数来约束权重。谱归一化在计算上高效，易于实现，并且与训练损失无关。在设计新的GAN时，你应该使用它。'
- en: This concludes the summary of GANs techniques. We will now look at new applications
    and models that we have not explored.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是GAN技术总结的结束。接下来，我们将探讨我们尚未探索过的新应用和模型。
- en: Putting your skills into practice
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将你的技能付诸实践
- en: Now, you can apply the skills you have learned to implement your own image generation
    projects. Before you start, there are some pitfalls you should look out for and
    also some practical advice that you can follow.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以应用所学的技能来实施自己的图像生成项目。在开始之前，有一些陷阱你需要留意，同时也有一些实用的建议你可以遵循。
- en: Don't trust everything you read
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不要相信你所阅读的一切
- en: A new academic paper is published and shows astonishing images generated by
    their model! Take it with a pinch of salt. Usually, these papers handpick the
    best result to showcase and hide the failed examples. Furthermore, the images
    are shrunk down to fit onto the paper, thus the image artifacts may not be visible
    from the paper. Before investing your time in using or re-implementing the information
    in the paper, try to find other resources of the claimed results. This can be
    the author's website or GitHub repository, which may contain the raw, high-definition
    images and videos.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇新的学术论文发布，展示了他们模型生成的惊人图像！对此要保持怀疑态度。通常，这些论文挑选了最好的结果进行展示，并隐藏了失败的示例。此外，图像会被缩小以适应论文版面，因此图像中的伪影可能在论文中不可见。在投入时间使用或重新实现论文中的信息之前，尽量寻找其他声称结果的资源。这可以是作者的网站或GitHub仓库，可能包含原始的高清图像和视频。
- en: How big is your GPU?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的GPU有多大？
- en: Deep learning models, especially GANs, are computationally expensive. Many of
    the state-of-the-art results are produced after training tons of data on multiple
    GPUs for weeks. You will almost certainly need that sort of computing power just
    to attempt to reproduce those results. Therefore, pay attention to the computation
    resources used in the papers to avoid disappointment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型，尤其是GANs，计算成本高昂。许多最先进的结果是在多个GPU上训练大量数据数周后产生的。你几乎肯定需要这种计算能力才能尝试重现这些结果。因此，注意论文中使用的计算资源，以避免失望。
- en: If you don't mind waiting, you can use a single GPU and wait four times longer
    (assuming the original implementation used four GPUs). However, this will usually
    mean the batch size will also have to be reduced by four times, and this can have
    an effect on the results and convergence rate. You may have to reduce the learning
    rate to match the reduced batch size, which further slows down the training time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不介意等待，你可以使用一张GPU并等待四倍的时间（假设原始实现使用了四张GPU）。然而，这通常意味着批量大小也需要减少四倍，这可能会影响结果和收敛速度。你可能需要减少学习率以适应减少的批量大小，这进一步拖慢了训练时间。
- en: Build your model using existing models
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用现有模型构建你的模型
- en: The renowned AI scientist Dr. Andrej Karpathy, in one of his talks in 2019,
    said *"don't be a hero."* When you want to create an AI project, do not invent
    your own model; always start from existing models. Researchers have spent huge
    amounts of time and resources on creating models. Along the way, they may have
    thrown in some tricks as well. Therefore, you should start from existing models,
    then tweak or build on top of them to suit your requirements.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 著名的AI科学家安德烈·卡尔帕西（Dr. Andrej Karpathy）在2019年的一次讲座中说过 *“不要做英雄。”* 当你想创建一个AI项目时，不要自己发明模型；永远从现有模型开始。研究人员花费了大量的时间和资源来创建模型。在这个过程中，他们可能也加入了一些技巧。因此，你应该从现有模型开始，然后根据你的需求进行调整或在其基础上构建。
- en: As we have seen throughout this book, most often, state-of-the-art models do
    not come out of thin air but have been built on top of pre-existing models or
    techniques. There are usually implementations of the model available online, either
    officially by the authors or by re-implementation by enthusiasts in various different
    machine learning frameworks. One useful web resource to find them is the [http://paperswithcode.com/](http://paperswithcode.com/)
    website.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所看到的，大多数最先进的模型并不是凭空出现的，而是建立在现有的模型或技术之上。通常，网上可以找到模型的实现，要么是作者官方发布的，要么是机器学习爱好者在不同框架下的重新实现。一个有用的在线资源是[http://paperswithcode.com/](http://paperswithcode.com/)网站。
- en: Understand the model's limitations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解模型的局限性
- en: A lot of the AI companies I know do not create their own model architecture,
    for the reasons mentioned in the preceding sections. So, what is the point of
    learning to code TensorFlow to create image generation models? The first answer
    to that is that by writing from scratch, you now understand what the layers and
    models are, as well as their limitations. Say someone without knowledge of GANs
    was amazed by what AI could do, so they downloaded pix2pix to train on their own
    dataset to translate images of cats into trees. That did not work, and they had
    no clue why it failed; AI was a black box to them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我认识的许多AI公司并不会创建自己的模型架构，原因就在于前面提到的几点。那么，学习如何编写TensorFlow代码来创建图像生成模型的意义何在呢？首先的答案是，通过从零开始编写代码，您现在能理解各层和模型的构成以及它们的局限性。假设一个不了解GANs的人，对AI能做的事情感到惊讶，于是他们下载了pix2pix，试图在自己的数据集上训练，将猫的图像转换成树木的图像。但这并没有成功，他们也不明白为什么会失败；对他们来说，AI是一个黑盒。
- en: As an AI-educated person, we know that pix2pix requires a paired image dataset,
    and we will need to use CycleGAN for unpaired datasets. The knowledge that you
    have learned will help you choose the right model and the right data to use. Furthermore,
    you will now know how to tweak the model architecture for different image sizes,
    different conditioning, and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作为受过人工智能教育的人，我们知道pix2pix需要成对的图像数据集，而对于未配对的数据集，我们需要使用CycleGAN。您所学到的知识将帮助您选择合适的模型和数据使用。此外，您现在会知道如何根据不同的图像大小、不同的条件等调整模型架构。
- en: We have now looked at some of the common pitfalls in using generative models.
    Now, we will look at some of the interesting applications and models that you
    could use generative models for.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了使用生成模型的一些常见陷阱。接下来，我们将探讨一些有趣的应用和模型，您可以利用生成模型进行操作。
- en: Image processing
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: Out of all the things that image generative models can do, **image processing**
    is probably the one that produces the best results for commercial use. In our
    context, image processing refers to applying some transformation to existing images
    to produce new ones. We will look at the three applications of image processing
    in this section – **image inpainting**, **image compression**, and **image super-resolution**
    (**ISR**).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有图像生成模型能够完成的任务中，**图像处理**可能是最能为商业用途带来最佳效果的一个。在我们的语境中，图像处理指的是对现有图像应用某些变换以生成新图像。我们将在本节中探讨图像处理的三种应用——**图像修复**、**图像压缩**和**图像超分辨率**（**ISR**）。
- en: Image inpainting
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像修复
- en: 'Image inpainting is the process of filling in missing pixels of an image so
    that the result is visually realistic. It has practical applications in image
    editing, such as restoring a damaged image or removing obstructing objects. In
    the following example, you can see how image inpainting is used to remove people
    in the background. We first fill the people in with white pixels, then we use
    a generative model to fill in the pixels:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复是填补图像中缺失像素的过程，使得结果在视觉上真实可信。它在图像编辑中有着实际应用，比如修复损坏的图像或去除遮挡物。在以下示例中，您可以看到如何使用图像修复去除背景中的人物。我们首先用白色像素填充人物，然后使用生成模型填充这些像素：
- en: '![Figure 10.2 – Image inpainting using DeepFillv2 to remove people in the background
    (left) original image, (middle) people filled with white masks, (right) restored
    image'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 使用DeepFillv2进行图像修复，去除背景中的人物（左）原始图像，（中）用白色面罩填充人物，（右）修复后的图像]'
- en: '(source: J. Yu et al., 2018, "Free-Form Image Inpainting with Gated Convolution,"'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '（来源：J. Yu等，2018年，"自由形式图像修复与门控卷积"， '
- en: https://arxiv.org/abs/1806.03589) ](img/B14538_10_02.jpg)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: https://arxiv.org/abs/1806.03589）](img/B14538_10_02.jpg)
- en: 'Figure 10.2 – Image inpainting using DeepFillv2 to remove people in the background
    (left) original image, (middle) people filled with white masks, (right) restored
    image (source: J. Yu et al., 2018, "Free-Form Image Inpainting with Gated Convolution,"
    [https://arxiv.org/abs/1806.03589](https://arxiv.org/abs/1806.03589))'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 使用DeepFillv2进行图像填充以去除背景中的人物（左）原始图像，（中）人物被白色掩模填充，（右）恢复后的图像（来源：J. Yu等人，2018年，《使用门控卷积的自由形式图像填充》，[https://arxiv.org/abs/1806.03589](https://arxiv.org/abs/1806.03589)）
- en: Traditional image inpainting works by finding a background patch with a similar
    texture and then pasting it into the missing regions. However, this usually only
    works for simple textures in a small area. One of the first GANs designed for
    image inpainting is the **context encoder**. Its architecture is similar to an
    autoencoder but trained with adversarial loss in addition to the usual L2 reconstruction
    loss. The result can appear blurry if there is a large area to be filled.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的图像填充方法是通过找到一个具有相似纹理的背景补丁，然后将其粘贴到缺失区域。然而，这通常只适用于小范围内简单纹理的情况。第一个为图像填充设计的GAN之一是**上下文编码器**。它的架构类似于自编码器，但在训练时除了常规的L2重建损失外，还会加入对抗损失。如果需要填充的区域较大，结果可能会显得模糊。
- en: One approach to tackle this is to use two networks (course and fine) to train
    on different scales. Using this approach, **DeepFill** (J. Yu et al., 2018, *Generative
    Image Inpainting with Contextual Attention*, [https://arxiv.org/abs/1801.07892](https://arxiv.org/abs/1801.07892))
    adds an attention layer to better capture the features from a distant spatial
    location.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的一种方法是使用两个网络（粗略和精细）在不同尺度上进行训练。采用这种方法，**DeepFill**（J. Yu等人，2018年，*基于上下文注意力的生成图像填充*，[https://arxiv.org/abs/1801.07892](https://arxiv.org/abs/1801.07892)）增加了一个注意力层，以更好地捕捉来自遥远空间位置的特征。
- en: 'In earlier GANs, a dataset for image inpainting was created by randomly cutting
    out square masks (holes), but the technique does not translate well to real-world
    applications. Yu et al. propose a partial convolution layer to create irregular
    masks. The layer contains a masked convolution like the one we implemented in
    PixelCNN in [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017), *Getting
    Started with Image Generation Using TensorFlow*. The following image examples
    show the results of using a partial convolution-based network:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的GAN中，通过随机裁剪方形掩模（孔洞）来创建图像填充的数据集，但该技术并不适用于实际应用。Yu等人提出了一个部分卷积层来创建不规则的掩模。该层包含一个像我们在PixelCNN中实现的掩模卷积，[*第1章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)，*使用TensorFlow进行图像生成入门*中也有介绍。以下图像示例展示了使用基于部分卷积的网络的结果：
- en: '![Figure 10.3 – Irregular masks and the inpainted results (source: G. Liu et
    al., 2018, "Image Inpainting for Irregular Holes Using Partial Convolutions,"
    https://arxiv.org/abs/1804.07723)](img/B14538_10_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 不规则的掩模和填充后的结果（来源：G. Liu等人，2018年，《使用部分卷积进行不规则孔洞图像填充》，https://arxiv.org/abs/1804.07723)](img/B14538_10_03.jpg)'
- en: 'Figure 10.3 – Irregular masks and the inpainted results (source: G. Liu et
    al., 2018, "Image Inpainting for Irregular Holes Using Partial Convolutions,"
    [https://arxiv.org/abs/1804.07723](https://arxiv.org/abs/1804.07723))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 不规则的掩模和填充后的结果（来源：G. Liu等人，2018年，《使用部分卷积进行不规则孔洞图像填充》，[https://arxiv.org/abs/1804.07723](https://arxiv.org/abs/1804.07723)）
- en: '**DeepFillv2** uses gated convolution to improve and generalize masked convolutions.
    DeepFill uses only a standard discriminator that predicts real or fake images.
    However, this does not work well when there can be many holes in free-form inpainting.
    Therefore, it uses **spectral-normalized PatchGAN** (**SN-PatchGAN**) to encourage
    more realistic inpainting.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepFillv2**使用门控卷积来改进和概括掩模卷积。DeepFill仅使用标准判别器来预测图像的真伪。然而，当自由形式的图像填充中存在多个孔洞时，这种方法效果不佳。因此，它使用**谱归一化的PatchGAN**（**SN-PatchGAN**）来促进更逼真的图像填充。'
- en: 'The following are some additional resources on this topic:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于此主题的额外资源：
- en: 'The TensorFlow v1 source code for DeepFillv1 and v2: [https://github.com/JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepFillv1和v2的TensorFlow v1源代码：[https://github.com/JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting)
- en: 'Interactive inpainting demo where you can use your own photo to play with:
    [https://www.nvidia.com/research/inpainting/](https://www.nvidia.com/research/inpainting/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互动图像填充演示，你可以使用自己的照片进行操作：[https://www.nvidia.com/research/inpainting/](https://www.nvidia.com/research/inpainting/)
- en: Image compression
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像压缩
- en: 'Image compression is the process of transforming images from raw pixels into
    encoded data that is much smaller in size for storage or communication. For example,
    a JPEG file is a compressed image. When we open a JPEG file, the computer will
    reverse the compression process to restore the image pixels. The simplified image
    compression pipeline is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图像压缩是将图像从原始像素转换为编码数据的过程，压缩后的数据体积要小得多，方便存储或通信。例如，JPEG 文件就是一种压缩图像。当我们打开一个 JPEG
    文件时，计算机会逆向压缩过程来恢复图像像素。简化的图像压缩流程如下：
- en: '**Segmentation**: Divide the image into small blocks and each of them will
    be processed individually.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分割**：将图像分成小块，每个小块将单独处理。'
- en: '**Transformation**: Transform raw pixels into representations that are more
    compressible. At this stage, a higher compression rate is normally achieved by
    removing high-frequency content that makes the restored image blurrier. For example,
    consider a segment of a grayscale image containing [255, 250, 252, 251, ...] pixel
    values that are nearly white. The differences between them are so small that the
    human eye cannot pick up on them, so we can just transform all the pixels into
    255\. This will make the data easier to compress.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**变换**：将原始像素转换为更易压缩的表示。在此阶段，通常通过去除高频内容来实现更高的压缩率，这些高频内容会使恢复后的图像变得模糊。例如，考虑一个包含[255,
    250, 252, 251, ...]像素值的灰度图像段，这些像素几乎是白色的。它们之间的差异非常小，以至于人眼无法察觉，因此我们可以将所有像素转换为255。这将使数据更容易压缩。'
- en: '**Quantization**: Use a lower bit number to represent the data. An example
    is to convert a grayscale image with 256-pixel values between [0, 255] into two
    values of black and white of [0, 1].'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**量化**：使用较低的位数表示数据。例如，将一个灰度图像，其中像素值在[0, 255]之间的256个值，转换为黑白两个值[0, 1]。'
- en: '**Symbol encoding** is used to encode data using some efficient coding. One
    of the common ones is known as **run-length coding**. Instead of saving every
    8-bit pixel, we can save just the difference between the pixels. Therefore, instead
    of saving white pixels of [255, 255, 255, …], we can just encode it into something
    such as [255] x 100, which says the white pixel repeats 100 times.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**符号编码**用于使用一些高效的编码来编码数据。其中一种常见的编码方式是**游程编码**。我们可以不保存每个8位像素，而是只保存像素之间的差异。因此，代替保存[255,
    255, 255, …]这样的白色像素，我们可以将其编码为[255] x 100，表示白色像素重复了100次。'
- en: 'A higher compression rate is achieved by using more extreme quantization or
    removing more frequency contents. As a result, this information is lost (hence,
    this is known as lossy compression). The following diagram shows one such GAN
    for image compression:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用更极端的量化或去除更多频率内容来实现更高的压缩率。因此，这些信息会丢失（因此，这被称为有损压缩）。下图展示了一个用于图像压缩的生成对抗网络（GAN）：
- en: '![Figure 10.4 – Generative compression network. The encoder (E) maps the image
    into latent feature'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 – 生成式压缩网络。编码器 (E) 将图像映射到潜在特征'
- en: w. It is quantized by a finite quantizer, q, to obtain representation ŵ , which
    can be encoded
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过有限的量化器q进行量化，以获得表示ŵ，可以编码为比特流。
- en: into a bitstream. The decoder (G) reconstructs the image and D is the discriminator.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为比特流。解码器 (G) 重建图像，D 是判别器。
- en: '(Source: E. Agustsson et al., 2018, "Generative Adversarial Networks for Extreme
    Learned Image Compression," https://arxiv.org/abs/1804.02958)](img/B14538_10_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：E. Agustsson 等，2018 年，“生成对抗网络用于极端学习图像压缩，” [https://arxiv.org/abs/1804.02958](https://arxiv.org/abs/1804.02958))](img/B14538_10_04.jpg)
- en: 'Figure 10.4 – Generative compression network. The encoder (E) maps the image
    into latent feature w. It is quantized by a finite quantizer, q, to obtain representation
    ŵ , which can be encoded into a bitstream. The decoder (G) reconstructs the image
    and D is the discriminator. (Source: E. Agustsson et al., 2018, "Generative Adversarial
    Networks for Extreme Learned Image Compression," [https://arxiv.org/abs/1804.02958](https://arxiv.org/abs/1804.02958))'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 生成式压缩网络。编码器 (E) 将图像映射到潜在特征w。它通过有限的量化器q进行量化，以获得表示ŵ，可以编码为比特流。解码器 (G)
    重建图像，D 是判别器。(来源：E. Agustsson 等，2018 年，“生成对抗网络用于极端学习图像压缩，” [https://arxiv.org/abs/1804.02958](https://arxiv.org/abs/1804.02958))
- en: In general, generative compression uses autoencoder architecture to compress
    an image into small, latent code and that is restored using the decoder.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，生成式压缩使用自编码器架构将图像压缩为较小的潜在代码，并通过解码器恢复。
- en: Image super-resolution
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像超分辨率
- en: We have used the upsampling layer a lot to increase the spatial resolution of
    the activation in the generator (GAN) or decoder (autoencoder). It works by spacing
    out the pixels and filling in the gaps by interpolation. As a result, the enlarged
    image is usually blurry.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经大量使用了上采样层，以增加生成器（GAN）或解码器（自编码器）中激活的空间分辨率。它通过拉开像素并通过插值填补空隙来工作。结果，放大的图像通常会模糊。
- en: In a lot of image applications, we want to enlarge the image while keeping its
    crispness, and this can be done via **image super resolution** (**ISR**). ISR
    aims to increase the image from **low resolution** (**LR**) to **high resolution**
    (**HR**). **Super-Resolution Generative Adversarial Network** (**SRGAN**) (C.
    Ledig et al., 2016, *Photo-Realistic Single Image Super-Resolution Using a Generative
    Adversarial Network*, [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802))
    was the first to use a GAN to do that.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多图像应用中，我们希望在保持清晰度的同时放大图像，这可以通过**图像超分辨率**（**ISR**）来实现。ISR的目标是将图像从**低分辨率**（**LR**）提升到**高分辨率**（**HR**）。**超分辨率生成对抗网络**（**SRGAN**）（C.
    Ledig 等，2016年，*使用生成对抗网络进行照片级单图像超分辨率*，[https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802)）是第一个使用GAN来实现这一点的网络。
- en: SRGAN's architecture is similar to that of DCGAN but uses residual blocks instead
    of a plain convolutional layer. It borrows the perception loss from style transfer
    literature, that is, the content loss calculated from VGG features. In retrospect,
    we knew this was a better measure of visual perception quality rather than pixel-wise
    loss. We can now see how versatile the autoencoder is for various image processing
    tasks. Similar autoencoder architecture can be repurposed for other image processing
    tasks, such as image denoising or deblurring. Next, we will look at an application
    where the input to the model is not images but words.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN的架构类似于DCGAN，但使用残差块而不是普通的卷积层。它借用了风格迁移文献中的感知损失，即通过VGG特征计算的内容损失。回顾来看，我们知道这是一个比像素级损失更好的视觉感知质量衡量标准。现在我们可以看到自编码器在各种图像处理任务中的多功能性。类似的自编码器架构可以重新用于其他图像处理任务，如图像去噪或去模糊。接下来，我们将看看一个应用，其中模型的输入不是图像而是文字。
- en: Text to image
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到图像
- en: 'Text-to-image GANs are conditional GANs. However, instead of using class labels
    as conditions, they use words as the condition to generate images. In earlier
    practice, GANs used word embeddings as the conditions into the generator and discriminator.
    Their architectures are similar to conditional GANs, which we learned about in
    [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084), *Image-to-Image
    Translation*. The difference is merely that the embedding of text is generated
    using a **natural language processing** (**NLP**) preprocessing pipeline. The
    following diagram shows the architecture of a text-conditional GAN:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像GAN是条件GAN。然而，它们并不是使用类标签作为条件，而是使用文字作为生成图像的条件。在早期的实践中，GAN使用词嵌入作为条件输入到生成器和判别器中。它们的架构类似于我们在[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)《图像到图像翻译》中学到的条件GAN。不同之处仅在于，文本的嵌入是通过**自然语言处理**（**NLP**）预处理管道生成的。下图展示了文本条件GAN的架构：
- en: '![Figure 10.5 – Text-conditional convolutional GAN architecture where text
    encoding'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 – 文本条件卷积GAN架构，其中文本编码'
- en: is used by both the generator and discriminator
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和判别器都使用该层
- en: '(Redrawn from: S. Reed et al., 2016, "Generative Adversarial Text to Image
    Synthesis,"'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: （重绘自：S. Reed 等，2016年，“生成对抗文本到图像合成”，
- en: https://arxiv.org/abs/1605.05396)](img/B14538_10_05.jpg)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: https://arxiv.org/abs/1605.05396)](img/B14538_10_05.jpg)
- en: 'Figure 10.5 – Text-conditional convolutional GAN architecture where text encoding
    is used by both the generator and discriminator (Redrawn from: S. Reed et al.,
    2016, "Generative Adversarial Text to Image Synthesis," [https://arxiv.org/abs/1605.05396](https://arxiv.org/abs/1605.05396))'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 文本条件卷积GAN架构，其中文本编码由生成器和判别器共同使用（重绘自：S. Reed 等，2016年，“生成对抗文本到图像合成”，[https://arxiv.org/abs/1605.05396](https://arxiv.org/abs/1605.05396)）
- en: 'Like normal GANs, generated high-resolution images tend to be blurry. **StackGAN**
    resolves this by stacking two networks together. The following diagram shows the
    text and the generated images at different stages of StackGAN and a vanilla GAN:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的GAN类似，生成的高分辨率图像往往会模糊。**StackGAN**通过将两个网络堆叠在一起解决了这个问题。下图展示了StackGAN在不同阶段生成的文本和图像，与普通GAN的对比：
- en: '![Figure 10.6 – Images generated by StackGAN at different generator stages'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.6 – StackGAN在不同生成器阶段生成的图像'
- en: '(source: H. Zhang et al., 2017, "StackGAN: Text to Photo-realistic Image Synthesis
    with Stacked Generative Adversarial Networks," https://arxiv.org/abs/1612.03242)](img/B14538_10_06.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：H. Zhang 等，2017 年，"StackGAN：通过堆叠生成对抗网络生成文本到照片真实图像"，[https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242)）](img/B14538_10_06.jpg)
- en: 'Figure 10.6 – Images generated by StackGAN at different generator stages (source:
    H. Zhang et al., 2017, "StackGAN: Text to Photo-realistic Image Synthesis with
    Stacked Generative Adversarial Networks," [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – StackGAN 在不同生成器阶段生成的图像（来源：H. Zhang 等，2017 年，"StackGAN：通过堆叠生成对抗网络生成文本到照片真实图像"，[https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242)）
- en: The first generator produces a low-resolution image from the word embedding.
    The second generator then takes the generated image and word embedding as input
    conditions to the second generator to produce refined images. The coarse-to-fine
    architecture has appeared in different forms in many high-resolution GANs, as
    we have learned in this book.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个生成器从词嵌入中生成低分辨率图像。第二个生成器则将生成的图像和词嵌入作为输入条件，生成精细化的图像。我们在本书中已经学到，粗到精的架构在许多高分辨率
    GAN 中以不同形式出现。
- en: '**AttnGAN** (T. Xu et al., 2017, *AttnGAN: Fine-Grained Text to Image Generation
    with Attentional Generative Adversarial Networks, at* [https://arxiv.org/abs/1711.10485](https://arxiv.org/abs/1711.10485))
    further improves text-to-image synthesis by using an attention module. The attention
    module is different from the one used in SAGAN ([*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation*) but the principle is the same. There are
    two inputs into the attention module at the start of every stage of the generator
    – word features and image features. It learns to pay attention to different words
    and image regions when moving from coarse to fine generators. Most text-to-image
    models after that have some form of attention mechanism.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**AttnGAN**（T. Xu 等，2017 年，*AttnGAN：通过注意力生成对抗网络进行细粒度文本到图像生成，见于* [https://arxiv.org/abs/1711.10485](https://arxiv.org/abs/1711.10485)）通过使用注意力模块进一步改进了文本到图像的合成。该注意力模块不同于在
    SAGAN 中使用的（[*第 8 章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156) *，图像生成的自注意力*），但原理相同。在生成器的每个阶段开始时，注意力模块有两个输入——词特征和图像特征。它在从粗到精的生成器过程中，学习如何关注不同的词和图像区域。在此之后，大多数文本到图像模型都具有某种形式的注意力机制。'
- en: 'Text to image is still an unsolved problem; it still struggles to generate
    complex real-world images from text. As we can see in the following figure, the
    generated images are still far from perfect. Researchers are beginning to bring
    in recent advancement from NLP to improve text-to-image performance:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像仍然是一个未解决的问题，它仍然难以从文本生成复杂的真实世界图像。如我们在下图中看到的，生成的图像仍然远未完美。研究人员开始引入最近的自然语言处理（NLP）进展，以提高文本到图像的表现：
- en: '![Figure 10.7 – Examples of images generated from the given caption from the
    MS-COCO dataset (A) original images and their image caption in the dataset (B)
    images generated by StackGAN + object pathway (C) images generated by StackGAN
    (source: T. Hinz et al., 2019, "Generating Multiple Objects at Spatially Distinct
    Locations," https://arxiv.org/abs/1901.00686)](img/B14538_10_07.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 从 MS-COCO 数据集中给定标题生成的图像示例（A）数据集中的原始图像及其图像标题（B）由 StackGAN + 对象路径生成的图像（C）由
    StackGAN 生成的图像（来源：T. Hinz 等，2019 年，"在空间上生成多个物体"，https://arxiv.org/abs/1901.00686)](img/B14538_10_07.jpg)'
- en: 'Figure 10.7 – Examples of images generated from the given caption from the
    MS-COCO dataset (A) original images and their image caption in the dataset (B)
    images generated by StackGAN + object pathway (C) images generated by StackGAN
    (source: T. Hinz et al., 2019, "Generating Multiple Objects at Spatially Distinct
    Locations," [https://arxiv.org/abs/1901.00686](https://arxiv.org/abs/1901.00686))'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 从 MS-COCO 数据集中给定标题生成的图像示例（A）数据集中的原始图像及其图像标题（B）由 StackGAN + 对象路径生成的图像（C）由
    StackGAN 生成的图像（来源：T. Hinz 等，2019 年，"在空间上生成多个物体"，[https://arxiv.org/abs/1901.00686](https://arxiv.org/abs/1901.00686)）
- en: Next, we will look at the exciting application of video retargeting.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍视频重定向的激动人心的应用。
- en: Video retargeting
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频重定向
- en: Video synthesis is a broad term used for describing all forms of video generation.
    This can include generating video from random noise or words, to colorize black-and-white
    video, and so on, much like image generation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 视频合成是一个广泛的术语，用于描述所有形式的视频生成。这包括从随机噪声或文字生成视频、为黑白视频上色等等，类似于图像生成。
- en: In this section, we will look at a subgroup of video synthesis known as **video
    retargeting**. We will first look at two applications – face reenactment and pose
    transfer – and then introduce a powerful model that uses motion to generalize
    video targeting.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论视频合成中的一个子领域——**视频重定向**。我们首先介绍两个应用——面部重现和姿势转移，然后介绍一个强大的模型，该模型利用运动来泛化视频目标。
- en: Face reenactment
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 面部重现
- en: '**Face reenactment** was introduced along with face swapping in [*Chapter 9*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175),
    *Video Synthesis*. Face reenactment in video synthesis involves transferring the
    facial expression of the driving video to the face in the target video. This is
    useful in animation and movie making. Recently, Zakharov et al. proposed a generative
    model that requires only a few target 2D images. This is done by using facial
    landmarks as intermediate features, as shown in the following diagram:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**面部重现**与面部交换一起在[*第9章*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175)，《视频合成》中介绍。视频合成中的面部重现涉及将驱动视频的面部表情转移到目标视频中的面部。这在动画制作和电影制作中非常有用。最近，Zakharov等提出了一种生成模型，只需要少量目标2D图像。这是通过使用面部地标作为中间特征来完成的，如下图所示：'
- en: '![Figure 10.8 – Transferring the facial expression from the target image to
    the source image'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.8 – 将目标图像的面部表情转移到源图像](img/B14538_10_08.jpg)'
- en: '(source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning of Realistic
    Neural Talking Head Models," https://arxiv.org/abs/1905.08233)](img/B14538_10_08.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：E. Zakharov等，2019年，《少样本对抗学习真实神经对话头模型》，[https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))](img/B14538_10_08.jpg)
- en: 'Figure 10.8 – Transferring the facial expression from the target image to the
    source image (source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning
    of Realistic Neural Talking Head Models," [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 将目标图像的面部表情转移到源图像（来源：E. Zakharov等，2019年，《少样本对抗学习真实神经对话头模型》，[https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))
- en: 'Let''s briefly look into the model architecture, as shown in the following
    diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要地看一下模型架构，如下图所示：
- en: '![Figure 10.9 – Architecture of few-shot adversarial learning'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.9 – 少样本对抗学习的架构](img/B14538_10_09.jpg)'
- en: '(source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning of Realistic
    Neural Talking Head Models," https://arxiv.org/abs/1905.08233)](img/B14538_10_09.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：E. Zakharov等，2019年，《少样本对抗学习真实神经对话头模型》，[https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))](img/B14538_10_09.jpg)
- en: 'Figure 10.9 – Architecture of few-shot adversarial learning (source: E. Zakharov
    et al., 2019, "Few-Shot Adversarial Learning of Realistic Neural Talking Head
    Models," [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 少样本对抗学习的架构（来源：E. Zakharov等，2019年，《少样本对抗学习真实神经对话头模型》，[https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))
- en: The first thing that you should notice in the preceding diagram is **AdaIN**,
    which we immediately know is a style-based model. Therefore, we can see that the
    landmarks at the top are the content (the target's face shape and pose), while
    the style (the source's face attributes and expression) is extracted from the
    **embedder**. The generator then uses AdaIN to fuse the content and style to reconstruct
    the face.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，你应该注意到**AdaIN**，我们立刻知道它是一个基于风格的模型。因此，我们可以看到顶部的地标是内容（目标的面部形状和姿势），而风格（源的面部特征和表情）则是从**嵌入器**中提取的。生成器然后使用AdaIN将内容和风格融合，以重建面部。
- en: Recently, a similar model has been deployed by NVIDIA to slash the bit rate
    of teleconferencing video transmission. You can view their blog at [https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/](https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/)
    to learn how they use many of the AI techniques, such as ISR, face alignment,
    and face reenactment, in real-world deployment. Next, we will look at how to use
    AI to transfer the pose of a person.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，NVIDIA部署了一个类似的模型，用于大幅降低远程视频会议的视频传输比特率。你可以访问他们的博客，[https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/](https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/)，了解他们如何在实际部署中使用许多AI技术，如ISR、面部对齐和面部重现。接下来，我们将探讨如何利用AI来转移一个人的姿势。
- en: Pose transfer
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 姿势转移
- en: '**Pose transfer** is similar to face reenactment except that now it will transfer
    the body (and head) pose. There are many ways to perform pose transfer but all
    of them involve the use of **body joints** (also known as **keypoints**) as features.
    The following diagram shows one example of images generated from a condition image
    and target pose:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**姿势转移**与面部重演类似，只是这次它将转移身体（和头部）姿势。执行姿势转移的方法有很多种，但所有方法都涉及使用**身体关节**（也称为**关键点**）作为特征。下图展示了从条件图像和目标姿势生成的图像示例：'
- en: '![Figure 10.10 – Transferring target poses onto the condition image'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.10 – 将目标姿势转移到条件图像上'
- en: '(source: Z. Zhu et al., 2019, "Progressive Pose Attention Transfer for Person
    Image'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Z. Zhu等，2019年，“用于人物图像生成的渐进式姿势注意力转移”）
- en: Generation," https://arxiv.org/abs/1904.03349 )](img/B14538_10_10.jpg)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 生成，https://arxiv.org/abs/1904.03349）](img/B14538_10_10.jpg)
- en: 'Figure 10.10 – Transferring target poses onto the condition image (source:
    Z. Zhu et al., 2019, "Progressive Pose Attention Transfer for Person Image Generation,"
    [https://arxiv.org/abs/1904.03349](https://arxiv.org/abs/1904.03349) )'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 – 将目标姿势转移到条件图像上（来源：Z. Zhu等，2019年，“用于人物图像生成的渐进式姿势注意力转移”，[https://arxiv.org/abs/1904.03349](https://arxiv.org/abs/1904.03349)）
- en: Pose transfer has many potential applications, including generating a fashion
    modeling video from single 2D images. This task is more challenging than face
    reenactment due to the huge variety of human poses. Next, we will look at a motion
    model that could generalize both face reenactment and pose transfer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 姿势转移有许多潜在应用，包括从单张二维图像生成时尚模特视频。与面部重演相比，这项任务更具挑战性，因为人体姿势的种类繁多。接下来，我们将看看一种能够推广面部重演和姿势转移的动作模型。
- en: Motion transfer
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作转移
- en: The face reenactment and pose transfer models introduced in the preceding section
    require object-specific priors, in other words, the facial landmark and human
    pose keypoints. Those features are normally extracted by separate models trained
    using a lot of data, which can be expensive to acquire and annotate.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍的面部重演和姿势转移模型需要物体特定的先验知识，换句话说，就是面部地标和人体姿势关键点。这些特征通常通过使用大量数据训练的独立模型来提取，这些数据的获取和标注往往是昂贵的。
- en: 'Recently, an object-agnostic model known as a **first-order motion model**
    (A. Siarohin et al., 2019, *First Order Motion Model for Image Animation*, [https://arxiv.org/abs/2003.00196](https://arxiv.org/abs/2003.00196))
    was introduced. It has rapidly gained popularity for its ease of use as it doesn''t
    need a lot of annotated training data. The following screenshot shows the overall
    architecture of the model, which exploits the motion in video frames:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了一种与物体无关的模型，称为**一阶动作模型**（A. Siarohin等，2019年，《用于图像动画的一阶动作模型》，[https://arxiv.org/abs/2003.00196](https://arxiv.org/abs/2003.00196)）。该模型因其易用性迅速获得了广泛关注，因为它不需要大量注释的训练数据。下图展示了该模型的整体架构，利用了视频帧中的运动信息：
- en: '![Figure 10.11 – First-order motion model that disentangles appearance and
    motion (source:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.11 – 一阶动作模型，解构外观和运动（来源：'
- en: https://aliaksandrsiarohin.github.io/first-order-model-website/)](img/B14538_10_11.jpg)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aliaksandrsiarohin.github.io/first-order-model-website/)](img/B14538_10_11.jpg)'
- en: 'Figure 10.11 – First-order motion model that disentangles appearance and motion
    (source: [https://aliaksandrsiarohin.github.io/first-order-model-website/](https://aliaksandrsiarohin.github.io/first-order-model-website/))'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 – 一阶动作模型，解构外观和运动（来源：[https://aliaksandrsiarohin.github.io/first-order-model-website/](https://aliaksandrsiarohin.github.io/first-order-model-website/)）
- en: In style transfer, an image is disentangled into content and style. Using the
    same terminology, **motion transfer** disentangles a video into appearance and
    motion. The motion module captures the motion of the object in the driving video.
    The generator network uses the appearance from the source image (similar to VGG
    content features) and motion information to create a new target video.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在风格转移中，图像被解构为内容和风格。使用相同的术语，**动作转移**将视频解构为外观和动作。动作模块捕捉驱动视频中物体的运动。生成网络使用源图像的外观（类似于VGG内容特征）和运动信息来创建一个新的目标视频。
- en: As a result, this model requires only a single source image and a driving video.
    It could do many of the video tasks we discussed, including face reenactment,
    pose transfer, and face swapping. You should definitely check out the website
    to see the demo video.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，该模型只需要一个源图像和一个驱动视频。它能够完成我们讨论的许多视频任务，包括面部重演、姿势转移和面部替换。你一定要访问该网站，查看演示视频。
- en: Although video retargeting GANs have improved dramatically in recent years,
    they are still not quite there yet to generate high-resolution images that are
    perfect for video production. An alternative is to combine 3D modeling with 2D
    GANs, which we will discuss in the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管视频重定向GAN在近年来有了显著改进，但它们仍未达到生成适合视频制作的高分辨率图像的水平。另一种替代方法是将三维建模与二维GAN结合，我们将在下一节讨论这一方法。
- en: Neural rendering
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经渲染
- en: '**Rendering** is the process of generating photo-realistic images from 2D or
    3D computer models. The term **neural rendering** has recently emerged to describe
    rendering using a neural network. In traditional 3D rendering, we will need to
    first create a 3D model with a polygon mesh that describes the object''s shape,
    color, and texture. Then, the lighting and camera position will be set and render
    the view into a 2D image.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**渲染**是从二维或三维计算机模型生成逼真图像的过程。**神经渲染**这一术语最近出现，用来描述使用神经网络进行渲染的方式。在传统的三维渲染中，我们需要首先创建一个包含多边形网格的三维模型，该网格描述了物体的形状、颜色和纹理。接着，设置光照和相机位置，将视图渲染为二维图像。'
- en: 'There has been an ongoing research on 3D object generation, but it is still
    not able to generate satisfying results. We can take advantage of the advancement
    of GANs by projecting part of the 3D objects into 2D space. We then use GANs to
    enhance the image in 2D space, for example, to generate a realistic texture using
    style transfer before projecting that back into the 3D model. The top diagram
    in the following figure shows the general pipeline of this approach:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 目前关于三维物体生成的研究仍在进行，但尚未能生成令人满意的结果。我们可以利用GAN的进展，通过将部分三维物体投影到二维空间来改善效果。然后，我们使用GAN在二维空间中增强图像，例如，通过风格迁移生成逼真的纹理，再将其投影回三维模型中。下图的上部示意图展示了这种方法的总体流程：
- en: '![Figure 10.12 – Two common frameworks for neural rendering'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.12 – 神经渲染的两种常见框架'
- en: '(Redrawn from: M-Y. Liu et al., 2020, "Generative Adversarial Networks for
    Image and Video Synthesis: Algorithms and Applications," https://arxiv.org/abs/2008.02793)
    ](img/B14538_10_12.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: （图源：M-Y. Liu 等，2020，《生成对抗网络在图像和视频合成中的算法与应用》，[https://arxiv.org/abs/2008.02793](https://arxiv.org/abs/2008.02793)）](img/B14538_10_12.jpg)
- en: 'Figure 10.12 – Two common frameworks for neural rendering (Redrawn from: M-Y.
    Liu et al., 2020, "Generative Adversarial Networks for Image and Video Synthesis:
    Algorithms and Applications," [https://arxiv.org/abs/2008.02793](https://arxiv.org/abs/2008.02793))'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 神经渲染的两种常见框架（图源：M-Y. Liu 等，2020，《生成对抗网络在图像和视频合成中的算法与应用》，[https://arxiv.org/abs/2008.02793](https://arxiv.org/abs/2008.02793)）
- en: 'Diagram *(b)* shows the framework that uses 3D data as input and 3D differentiable
    operations, such as 3D convolution. Apart from 3D polygons, 3D data can also exist
    in the form of a point cloud that can be obtained from lidar/radar or computer
    vision techniques such as structure from motion. A point cloud is made up of points
    in 3D space that depict the object''s surface. One application of a 3D to 2D deep
    network framework is to render the point cloud into a 2D image, as shown in the
    following figure, where the input is the cloud points obtained from a room:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *(b)* 显示了一个框架，该框架使用三维数据作为输入，并进行三维可微操作，例如三维卷积。除了三维多边形，三维数据还可以以点云的形式存在，点云可以通过激光雷达/雷达或计算机视觉技术（如运动结构重建）获得。点云由三维空间中的点组成，描绘物体的表面。三维到二维深度网络框架的一个应用是将点云渲染为二维图像，如下图所示，其中输入是来自房间的点云：
- en: '![Figure 10.13 – (left) 3D point cloud to 2D rendering, (middle) point cloud
    synthesis'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.13 – （左）三维点云到二维渲染，（中）点云合成图'
- en: image, (right) ground truth
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图像，（右）真实值
- en: '(source: F. Pittaluga et al., 2019, "Revealing Scenes by Inverting Structure
    from Motion Reconstructions," https://arxiv.org/abs/1904.03303)](img/B14538_10_13.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：F. Pittaluga 等，2019，《通过反转运动结构重建揭示场景》，[https://arxiv.org/abs/1904.03303](https://arxiv.org/abs/1904.03303)）](img/B14538_10_13.jpg)
- en: 'Figure 10.13 – (left) 3D point cloud to 2D rendering, (middle) point cloud
    synthesis image, (right) ground truth (source: F. Pittaluga et al., 2019, "Revealing
    Scenes by Inverting Structure from Motion Reconstructions," [https://arxiv.org/abs/1904.03303](https://arxiv.org/abs/1904.03303))'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – （左）三维点云到二维渲染，（中）点云合成图，（右）真实值（来源：F. Pittaluga 等，2019，《通过反转运动结构重建揭示场景》，[https://arxiv.org/abs/1904.03303](https://arxiv.org/abs/1904.03303)）
- en: 'We can also perform rendering in the reverse direction, that is, from a 2D
    image to a 3D object. This is often known as **inverse rendering**. The following
    figure shows examples of 2D to 3D inverse rendering:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以执行逆向渲染，即从 2D 图像渲染到 3D 对象。这通常被称为**逆向渲染**。下图展示了 2D 到 3D 逆向渲染的示例：
- en: '![Figure 10.14 – Given an input of 2D images (first column), the model predicts
    the 3D shape'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.14 – 给定输入的2D图像（第一列），模型预测出3D形状'
- en: and texture and renders them into the same viewpoint (second column). Images
    on
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 并且将纹理渲染到相同的视角（第二列）。右侧的图像展示了
- en: the right show the rendering in three different viewpoints.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧展示了从三个不同视角的渲染效果。
- en: '(Source: Y. Zhang et al., 2020, "Image GANs Meet Differentiable Rendering for
    Inverse Graphics and Interpretable 3D Neural Rendering," https://arxiv.org/abs/2010.09125)](img/B14538_10_14.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：Y. Zhang 等，2020年，“图像 GANs 与可微分渲染相结合，用于逆向图形学与可解释的 3D 神经渲染”，[https://arxiv.org/abs/2010.09125](https://arxiv.org/abs/2010.09125))](img/B14538_10_14.jpg)
- en: 'Figure 10.14 – Given an input of 2D images (first column), the model predicts
    the 3D shape and texture and renders them into the same viewpoint (second column).
    Images on the right show the rendering in three different viewpoints. (Source:
    Y. Zhang et al., 2020, "Image GANs Meet Differentiable Rendering for Inverse Graphics
    and Interpretable 3D Neural Rendering," [https://arxiv.org/abs/2010.09125](https://arxiv.org/abs/2010.09125))'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 – 给定输入的2D图像（第一列），模型预测出3D形状和纹理，并将其渲染到相同的视角（第二列）。右侧的图像展示了从三个不同视角的渲染效果。（来源：Y.
    Zhang 等，2020年，“图像 GANs 与可微分渲染相结合，用于逆向图形学与可解释的 3D 神经渲染”，[https://arxiv.org/abs/2010.09125](https://arxiv.org/abs/2010.09125))
- en: The model by Y. Zhang et al., 2020, uses two *renderers*. One is a differentiable
    graphics renderer to render 2D into 3D, which is outside the scope of this book.
    The other one is a GAN to generate multi-view image data, or more specifically,
    StyleGAN. It is interesting to know why they chose to use StyleGAN. The authors
    learned that StyleGAN could generate faces of slightly different viewing angles
    by changing the latent code. Then, they did an extensive study to find that styles
    in early layers control the camera viewpoint, making it ideal for this task. This
    is also a good example that shows how we could leverage 2D generative models into
    the 3D world.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Y. Zhang 等，2020 年的模型使用了两个 *渲染器*。一个是可微分图形渲染器，用于将 2D 渲染成 3D，这超出了本书的范围。另一个是 GAN，用于生成多视角图像数据，或者更具体地说，是
    StyleGAN。有趣的是，他们选择使用 StyleGAN的原因。作者发现，通过改变潜在代码，StyleGAN 可以生成不同视角的面部图像。然后，他们进行了广泛的研究，发现早期层中的风格控制着相机的视角，使其非常适合这一任务。这也是一个很好的例子，展示了我们如何将
    2D 生成模型引入 3D 世界。
- en: This concludes our introduction to neural rendering. It is an active area and
    there are many more use cases that are yet to be explored.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容结束了我们对神经渲染的介绍。神经渲染是一个活跃的研究领域，仍有许多尚未被探索的应用案例。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Since the inception of GANs and VAEs in 2014, significant advancement has been
    made in 2D image generation. Generating high-fidelity images is still challenging
    in practice as it requires huge amounts of data, computing power, and hyperparameter
    tuning. However, as demonstrated by StyleGAN, it seems that we now have the technology
    to do this, especially in face generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年GAN和VAE的诞生以来，2D图像生成取得了显著的进展。在实践中，生成高保真图像仍然是一个挑战，因为它需要大量的数据、计算能力和超参数调优。然而，正如StyleGAN所展示的那样，我们现在似乎拥有了实现这一目标的技术，尤其是在面部生成方面。
- en: In fact, at the time of writing this book, there haven't really been any major
    breakthroughs in this area since 2018\. With this book, we have included all the
    important techniques leading to BigGAN. These techniques include the use of AdaIN
    and self-attention modules, which are now commonplace even in adjacent fields
    such as video synthesis. This gives us a solid foundation to explore other emerging
    generative technologies.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在撰写本书时，自2018年以来，该领域并未出现任何重大突破。通过本书，我们涵盖了所有通向 BigGAN 的重要技术。这些技术包括 AdaIN 和自注意力模块的使用，这些模块现在在视频合成等相邻领域中也已变得非常普遍。这为我们探索其他新兴的生成技术奠定了坚实的基础。
- en: In this chapter, we looked back at the things we have learned and summarized
    them in different groups, such as losses and normalization techniques. We then
    looked at some practical advice in training generative models. Finally, we touched
    upon some of the upcoming technologies, especially in the area of video retargeting.
    I believe you now have the knowledge, skills, and confidence to explore the new
    and exciting AI world and I wish you all the best in your new adventure. I hope
    you have enjoyed reading this book. I welcome your feedback, which will help me
    improve my writing skills for my next book. Thanks!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了我们所学的内容，并将其总结为不同的类别，例如损失函数和归一化技术。接着，我们探讨了一些训练生成模型的实用建议。最后，我们涉及了一些即将到来的技术，特别是在视频重定向领域。我相信你现在已经具备了探索这个新兴且令人兴奋的人工智能世界所需的知识、技能和信心，祝愿你在新的冒险中一切顺利。希望你喜欢阅读本书。欢迎你的反馈，它将帮助我提升写作技巧，为我的下一本书做准备。谢谢！
