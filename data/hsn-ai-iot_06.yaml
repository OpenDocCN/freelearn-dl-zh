- en: Reinforcement Learning for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的强化学习
- en: '**Reinforcement learning** (**RL**) is very different from both supervised
    and unsupervised learning. It''s the way most living beings learn—interacting
    with the environment. In this chapter, we''ll study different algorithms employed
    for RL. As you progress through the chapter, you''ll do the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）与监督学习和无监督学习有很大不同。它是大多数生物学习的方式——与环境互动。在本章中，我们将研究强化学习中使用的不同算法。随着章节的进展，你将完成以下内容：'
- en: Learn what RL is and how it's different from supervised learning and unsupervised
    learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解什么是强化学习，以及它与监督学习和无监督学习的不同
- en: Lear different elements of RL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习强化学习的不同元素
- en: Learn about some fascinating applications of RL in the real world
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解强化学习在现实世界中的一些迷人应用
- en: Understand the OpenAI interface for training RL agents
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解用于训练强化学习（RL）智能体的OpenAI接口
- en: Learn about Q-learning and use it to train an RL agent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Q学习并用它来训练一个强化学习智能体
- en: Learn  about Deep Q-Networks and employ them to train an agent to play Atari
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解深度Q网络并用它来训练一个智能体玩Atari游戏
- en: Learn about the policy gradient algorithm and use it to
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解策略梯度算法并运用它
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Have you ever observed infants and how they learn to turn over, sit up, crawl,
    and even stand? Have you watched how baby birds learn to fly—the parents throw
    them out of the nest, they flutter for some time, and they slowly learn to fly.
    All of this learning involves a component of the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾观察过婴儿如何学会翻身、坐起、爬行，甚至站立？你有没有看过小鸟如何学会飞翔——父母把它们从巢里扔出去，它们拍打一段时间，慢慢学会飞行。所有这些学习都包含了以下的一个组成部分：
- en: '**Trial and error**: The baby tries different ways and is unsuccessful many
    times before finally succeeding in doing it.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**试错法**：婴儿尝试不同的方法，许多次未能成功，最终才能做到。'
- en: '**Goal-oriented**: All of the efforts are toward reaching a particular goal.
    The goal for the human baby can be to crawl, and for baby bird to fly.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标导向**：所有的努力都指向一个特定的目标。人类婴儿的目标可以是爬行，幼鸟的目标可以是飞翔。'
- en: '**Interaction with the environment**: The only feedback that they get is from
    the environment.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与环境的互动**：它们得到的唯一反馈来自环境。'
- en: This YouTube video is a beautiful video of a child learning to crawl and the
    stages in between [https://www.youtube.com/watch?v=f3xWaOkXCSQ](https://www.youtube.com/watch?v=f3xWaOkXCSQ).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个YouTube视频是一个美丽的视频，展示了一个孩子学习爬行以及中间的各个阶段：[https://www.youtube.com/watch?v=f3xWaOkXCSQ](https://www.youtube.com/watch?v=f3xWaOkXCSQ)。
- en: The human baby learning to crawl or baby bird learning to fly are both examples
    of RL in nature.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人类婴儿学习爬行或幼鸟学习飞行都是自然界中强化学习的例子。
- en: 'RL (in Artificial Intelligence) can be defined as a computational approach
    to goal-directed learning and decision-making, from interaction with the environment,
    under some idealized conditions. Let''s elaborate upon this since we''ll be using
    various computer algorithms to perform the learning—it''s a computational approach.
    In all of the examples that we''ll consider, the agent (learner) has a specific
    goal, which it''s trying to achieve—it''s a goal-directed approach. The agent
    in RL isn''t given any explicit instructions, it learns only from its interaction
    with the environment. This interaction with the environment, as shown in the following
    diagram, is a cyclic process. The **Agent** can sense the state of the **Environment**,
    and the **Agent** can perform specific well-defined actions on the **Environment**; this
    causes two things: first, a change in the state of the environment, and second,
    a reward is generated (under ideal conditions). This cycle continues:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（在人工智能中）可以定义为一种从与环境互动中进行目标导向学习和决策的计算方法，在某些理想化条件下进行。让我们详细说明这一点，因为我们将使用各种计算机算法来执行学习——这是一种计算方法。在我们考虑的所有例子中，智能体（学习者）都有一个具体的目标，试图实现——这是一个目标导向的方法。强化学习中的智能体没有被给予任何明确的指示，它只从与环境的互动中学习。正如下面的图所示，与环境的互动是一个循环过程。**智能体**可以感知**环境**的状态，**智能体**可以对**环境**执行特定的、定义明确的动作；这会导致两件事：首先，环境状态发生变化，其次，生成奖励（在理想条件下）。这个循环继续进行：
- en: '![](img/d4f20501-75cf-40cb-bdb8-69a29e084d98.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4f20501-75cf-40cb-bdb8-69a29e084d98.png)'
- en: The interaction between agent and environment
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体与环境之间的互动
- en: Unlike supervised learning, the **Agent** isn't presented with any examples.
    The **Agent** doesn't know what the correct action is. And unlike unsupervised
    learning, the agent goal isn't to find some inherent structure in the input (the
    learning may find some structure, but that isn't the goal); instead, its goal
    is to maximize the rewards (in the long run).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，**智能体**并没有被提供任何示例。**智能体**不知道正确的动作是什么。与无监督学习不同，智能体的目标不是在输入数据中寻找某种固有的结构（尽管学习过程可能会发现某些结构，但这不是目标）；相反，它的目标是最大化奖励（从长远来看）。
- en: RL terminology
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RL 术语
- en: 'Before learning different algorithms, let''s accustom ourselves to the RL terminology.
    For illustration purposes, let''s consider two examples: an agent finding a route
    in a maze and an agent steering the wheel of a **Self-Driving Car** (**SDC**).
    The two are illustrated in the following diagram:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习不同的算法之前，让我们先了解一下 RL 术语。为了举例说明，我们可以考虑两个例子：一个智能体在迷宫中找到路线，另一个智能体驾驶**自动驾驶汽车**（**SDC**）。这两个例子在下图中进行了说明：
- en: '![](img/8745eea1-ff7e-493e-a5d9-2017ec988da7.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8745eea1-ff7e-493e-a5d9-2017ec988da7.png)'
- en: Two example RL scenarios
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两个示例 RL 场景
- en: 'Before going further, let''s acquaint ourselves with common RL terms:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，让我们熟悉一些常见的 RL 术语：
- en: '**States *s***: The states can be thought of as a set of tokens (or representation)
    that can define all of the possible states the environment can be in. The state
    can be continuous or discrete. For example, in the case of an agent finding a
    path through a maze, the state can be represented by a 4 × 4 array, with a **0**
    representing an empty block, **1** representing a block occupied by the agent,
    and **X** the state that can''t be occupied; the states here are discrete in nature.
    In the case of an agent steering the wheel, the state is the view in front of
    the SDC. The image contains continuous valued pixels.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态 *s***：状态可以看作是一个表示所有可能环境状态的集合（或表示符）。状态可以是连续的，也可以是离散的。例如，在一个智能体寻找迷宫路径的案例中，状态可以由一个4
    × 4的数组表示，其中**0**代表空白块，**1**代表被智能体占据的块，**X**代表不能占据的状态；这里的状态是离散的。对于一个驾驶方向盘的智能体来说，状态就是SDC前方的视图。图像包含连续值的像素。'
- en: '**Actions *a*(*s*)**: Actions are the set of all possible things that the agent
    can do in a particular state. The set of possible actions, ***a***, depends on
    the present state, ***s***. Actions may or may not result in the change of state.
    They can be discrete or continuous. The agent in the maze can perform five discrete
    actions **[up**, **down**, **left**, **right**, **no change]**. The SDC agent,
    on another hand, can rotate the steering wheel in a continuous range of angles.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作 *a*(*s*)**：动作是智能体在某一特定状态下可以执行的所有可能操作的集合。可能的动作集合，***a***，取决于当前的状态，***s***。动作可能导致状态的变化，也可能不会。动作可以是离散的，也可以是连续的。在迷宫中，智能体可以执行五个离散动作：**[上，
    下， 左， 右， 不变]**。另一方面，SDC智能体可以在一个连续的角度范围内旋转方向盘。'
- en: '**Reward *r(s, a, s''*)**: It''s a scalar value returned by the environment
    when the agent selects an action. It defines the goal; the agent gets a higher
    reward if the action brings it near the goal, and a low (or even negative) reward
    otherwise. How we define a reward is totally up to us—in the case of the maze,
    we can define the reward as the Euclidean distance between the agent''s current
    position and goal. The SDC agent reward can be that the car is on the road (positive
    reward) or off the road (negative reward).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励 *r(s, a, s''*)**：当智能体选择一个动作时，环境返回的标量值。它定义了目标；如果动作将智能体带到目标附近，智能体会得到更高的奖励，否则会得到较低（甚至负）的奖励。我们如何定义奖励完全取决于我们——以迷宫为例，我们可以将奖励定义为智能体当前位置与目标之间的欧几里得距离。SDC智能体的奖励可以是汽车在道路上（正奖励）或在道路外（负奖励）。'
- en: '**Policy π(*s*)**: It defines a mapping between each state and the action to
    take in that state. The policy can be deterministic—that is, for each state a
    well-defined policy. Like for the maze agent, a policy can be that if the top
    block is empty, move up. The policy can also be stochastic—that is, where an action
    is taken by some probability. It can be implemented as a simple look-up table,
    or it can be a function dependent on the present state. The policy is the core
    of the RL agent. In this chapter, we''ll learn about different algorithms that
    help the agent to learn the policy.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略 π(*s*)**：它定义了每个状态与在该状态下采取的动作之间的映射。策略可以是确定性的——即对于每个状态都有一个明确的策略。例如，对于迷宫代理，策略可以是如果上方的格子是空的，则向上移动。策略也可以是随机的——即某个动作是由某个概率决定的。它可以通过简单的查找表来实现，或者可以是一个依赖于当前状态的函数。策略是强化学习代理的核心。在本章中，我们将学习帮助代理学习策略的不同算法。'
- en: '**Value function *V*(*s*)**: It defines the goodness of a state in the long
    run. It can be thought of as the total amount of reward the agent can expect to
    accumulate over the future, starting from the state ***s***. You can think of
    it as long-term goodness as opposed to the immediate goodness of rewards. What
    do you think is more important, maximizing the reward or maximizing the value
    function? Yes, you guessed right: just as in chess, we sometimes lose a pawn to
    win the game a few steps later, and so the agent should try to maximize the value
    function. There are two ways in which the value function is normally considered:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数 *V*(*s*)**：它定义了一个状态在长期中的好坏。它可以被看作是代理从状态***s***开始，未来能够积累的奖励的总量。你可以将其视为长期的好处，而不是奖励的即时好处。你认为哪个更重要，最大化奖励还是最大化价值函数？是的，你猜对了：就像下棋时，我们有时会牺牲一个兵去换取几步后赢得比赛一样，代理应该尝试最大化价值函数。价值函数通常有两种考虑方式：'
- en: '**Value function *V*^π(*s*)**: It''s the goodness of state following the policy *π*.
    Mathematically, at state *s*, it''s the expected cumulative reward from following
    the policy, *π*:'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数 *V*^π(*s*)**：它是跟随策略*π*时状态的好坏。数学上，在状态*s*时，它是从跟随策略*π*中预期的累积奖励：'
- en: '![](img/f9885ead-b1ba-41fd-b021-5701678e2699.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9885ead-b1ba-41fd-b021-5701678e2699.png)'
- en: '**Value-state function (or *Q*-function) *Q*^π(*s*, *a*)**: It''s the goodness
    of a state *s*, taking action *a*, and thereafter following policy *π*. Mathematically,
    we can say that for a state-action pair (*s*, *a*), it''s the expected cumulative
    reward from taking action *a* in state *s* and then following policy *π*:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值-状态函数（或 *Q* 函数） *Q*^π(*s*, *a*)**：它是状态 *s* 下采取动作 *a*，然后跟随策略 *π* 时的好坏。数学上，我们可以说，对于一个状态-动作对
    (*s*, *a*)，它是从状态 *s* 中采取动作 *a*，然后跟随策略 *π* 所得到的预期累积奖励：'
- en: '![](img/c6648db3-0b91-4b58-b2a5-9923e080dbba.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6648db3-0b91-4b58-b2a5-9923e080dbba.png)'
- en: '*γ* is the discount factor, and its value determines how much importance we
    give to the immediate rewards as compared to rewards received later on. A high
    value of discount factor decides how far into the future an agent can see. An
    ideal choice of *γ* in many successful RL algorithms has been a value of *0.97*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*γ*是折扣因子，它的值决定了我们在比较即时奖励和后续奖励时对即时奖励赋予多大的重要性。折扣因子的高值决定了代理能看到多远的未来。许多成功的强化学习算法中，*γ*的理想选择值通常为*0.97*。'
- en: '**Model of the environment**: It''s an optional element. It mimics the behavior
    of the environment, and it contains the physics of the environment; in other words,
    it defines how the environment will behave. The model of the environment is defined
    by the transition probability to the next state.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境模型**：它是一个可选元素，模拟环境的行为，并包含环境的物理规律；换句话说，它定义了环境将如何表现。环境模型由转移到下一个状态的概率定义。'
- en: An RL problem is mathematically formulated as a **Markov Decision Process**
    (**MDP**), and it follows the Markov property— that is, *the current state completely
    characterizes the state of the world*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题在数学上被公式化为**马尔可夫决策过程**（**MDP**），并且遵循马尔可夫性质——即*当前状态完全表征世界的状态*。
- en: Deep reinforcement learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: 'RL algorithms can be classified into two, based on what they iterate/approximate:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法可以根据它们所迭代/逼近的内容分为两类：
- en: '**Value-based methods**: In these methods, the algorithms take the action that
    maximizes the value function. The agent here learns to predict how good a given
    state or action would be. Hence, here, the aim is to find the optimal value. An
    example of the value-based method is Q-learning. Consider, for example, our RL
    agent in a maze: assuming that the value of each state is the negative of the
    number of steps needed to reach from that box to the goal, then, at each time
    step, the agent will choose the action that takes it to a state with optimal value,
    as in the following diagram. So, starting from a value of **-6**, it''ll move
    to **-5**, **-4**, **-3**, **-2**, **-1**, and eventually reach the goal with
    the value **0**:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于价值的方法**：在这些方法中，算法选择最大化价值函数的动作。这里，智能体学习预测一个给定状态或动作的好坏。因此，目标是找到最优的价值。一个基于价值的方法的例子是Q学习。例如，考虑我们的强化学习智能体在迷宫中的情况：假设每个状态的值是从该方格到达目标所需步数的负数，那么，在每个时间步，智能体将选择一个动作，带它到达具有最优值的状态，如下图所示。所以，从值为**-6**的状态开始，它将移动到**-5**、**-4**、**-3**、**-2**、**-1**，最终到达值为**0**的目标：'
- en: '![](img/bacdaff7-2312-4005-840d-a6056cabe610.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bacdaff7-2312-4005-840d-a6056cabe610.png)'
- en: The maze world with the value of each box
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具有每个方格值的迷宫世界
- en: '**Policy-based methods**: In these methods, the algorithms predict the best
    policy which maximizes the value function. The aim is to find the optimal policy.
    An example of the policy-based method is policy gradients. Here, we approximate
    the policy function, which allows us to map each state to the best corresponding
    action.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的方法**：在这些方法中，算法预测最大化价值函数的最佳策略。目标是找到最优策略。一个基于策略的方法的例子是策略梯度。在这里，我们近似策略函数，从而将每个状态映射到最佳的对应动作。'
- en: We can use neural networks as a function approximator to get an approximate
    value of either policy or value. When we use deep neural networks as a policy
    approximator or value approximator, we call it **deep reinforcement learning**
    (**DRL**). DRL has, in the recent past, given very successful results, hence,
    in this chapter, our will focus will be on DRL.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用神经网络作为函数逼近器来获取策略或价值的近似值。当我们使用深度神经网络作为策略逼近器或价值逼近器时，我们称之为**深度强化学习**（**DRL**）。在最近的研究中，DRL取得了非常成功的结果，因此，在本章中，我们将重点讨论DRL。
- en: Some successful applications
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些成功的应用
- en: 'In the last few years, RL has been successfully used in a variety of tasks,
    especially in game-playing and robotics. Let''s acquaint ourselves with some success
    stories of RL before learning its algorithms:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，强化学习在各种任务中取得了成功，尤其是在游戏和机器人领域。在学习其算法之前，让我们先了解一些强化学习的成功案例：
- en: '**AlphaGo Zero**: Developed by Google''s DeepMind team, the AlphaGo Zero *Mastering
    the game of Go without any human knowledge*, starts from an absolutely blank slate
    (**tabula rasa**). The AlphaGo Zero uses one neural network to approximate both
    the move probabilities and value. This neural network takes as input the raw board
    representation. It uses a Monte Carlo Tree search guided by the neural network
    to select the moves. The reinforcement learning algorithm incorporates look-ahead
    search inside the training loop. It was trained for 40 days using a 40-block residual
    CNN and, over the course of training, it played about 29 million games (a big
    number!). The neural network was optimized on Google Cloud using TensorFlow, with
    64 GPU workers and 19 CPU parameter servers. You can access the paper here: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaGo Zero**：由谷歌的DeepMind团队开发的AlphaGo Zero，*通过完全没有人类知识的方式掌握围棋*，从一个完全空白的起点开始（**tabula
    rasa**）。AlphaGo Zero使用一个神经网络来同时近似走棋概率和价值。这个神经网络以原始的棋盘表示为输入，使用由神经网络引导的蒙特卡洛树搜索来选择走棋。强化学习算法将前瞻性搜索整合到训练循环中。它使用40个区块的残差CNN训练了40天，在此过程中，它进行了大约2900万场比赛（一个非常庞大的数字！）。该神经网络在谷歌云上使用TensorFlow进行了优化，配备了64个GPU工作节点和19个CPU参数服务器。你可以在这里访问论文：[https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)。'
- en: '**AI-controlled sailplanes**: Microsoft developed a controller system that
    can run on many different autopilot hardware platforms such as Pixhawk and Raspberry
    Pi 3\. It can keep the sailplane in the air without using a motor, by autonomously
    finding and catching rides on naturally occurring thermals. The controller helps
    the sailplane to operate on its own; it detects and uses thermals to travel without
    the aid of a motor or a person. They implemented it as a partially observable
    MDP. They employ the Bayesian reinforcement learning and use the Monte Carlo tree
    search to search for the best action. They''ve divided the whole system into level
    planners—a high-level planer that makes a decision based on experience and a low-level
    planner that uses Bayesian reinforcement learning to detect and latch onto thermals
    in real time. You can see the sailplane in action at Microsoft News: [https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/](https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI控制的滑翔机**：微软开发了一种控制系统，可以在多种不同的自动驾驶硬件平台上运行，如Pixhawk和Raspberry Pi 3。它可以通过自动寻找并搭乘自然发生的上升气流，使滑翔机在空中飞行而不使用发动机。该控制器帮助滑翔机自行操作；它检测并利用上升气流在没有发动机或人员帮助的情况下飞行。他们将其实现为部分可观察的MDP（马尔可夫决策过程）。他们采用贝叶斯强化学习，并使用蒙特卡罗树搜索来寻找最佳动作。他们将整个系统分为两个级别的规划者——一个基于经验做出决策的高级规划者和一个利用贝叶斯强化学习实时检测并捕捉上升气流的低级规划者。您可以在微软新闻上查看滑翔机的操作：[https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/](https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/)。'
- en: '**Locomotion behavior**: In the paper *Emergence of Locomotion Behaviours in
    Rich Environments* ([https://arxiv.org/pdf/1707.02286.pdf](https://arxiv.org/pdf/1707.02286.pdf)),
    DeepMind researchers provided the agents with rich and diverse environments. The
    environments presented a spectrum of challenges at different levels of difficulty.
    The agent was provided with difficulties in increasing order; this led the agent
    to learn sophisticated locomotion skills without performing any reward engineering.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运动行为**：在论文*《丰富环境中运动行为的出现》*（[https://arxiv.org/pdf/1707.02286.pdf](https://arxiv.org/pdf/1707.02286.pdf)）中，DeepMind的研究人员为代理提供了丰富且多样的环境。这些环境提供了不同难度等级的挑战。代理面临的困难按顺序递增，这促使代理在没有执行任何奖励工程的情况下学会了复杂的运动技能。'
- en: Simulated environments
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟环境
- en: 'Since RL involves trial and error, it makes sense to train our RL agent first
    in a simulated environment. While a large number of applications exist that can
    be used for the creation of an environment, some popular ones include the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习（RL）涉及试错过程，因此在模拟环境中首先训练我们的RL代理是很有意义的。虽然有大量的应用可以用于创建环境，但一些流行的应用包括以下内容：
- en: '**OpenAI gym**: It contains a collection of environments that we can use to
    train our RL agents. In this chapter, we''ll be using the OpenAI gym interface.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI gym**：它包含了一系列我们可以用来训练RL代理的环境。在本章中，我们将使用OpenAI gym接口。'
- en: '**Unity ML-Agents SDK**: It allows developers to transform games and simulations
    created using the Unity editor into environments where intelligent agents can
    be trained using DRL, evolutionary strategies, or other machine learning methods
    through a simple-to-use Python API. It works with TensorFlow and provides the
    ability to train intelligent agents for two-dimensional/three-dimensional and
    VR/AR games. You can learn more about it here: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unity ML-Agents SDK**：它允许开发者通过易于使用的Python API，将使用Unity编辑器创建的游戏和模拟转换为可以通过深度强化学习（DRL）、进化策略或其他机器学习方法训练智能代理的环境。它与TensorFlow兼容，并提供训练二维/三维以及虚拟现实（VR）/增强现实（AR）游戏中智能代理的能力。您可以在此了解更多：[https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)。'
- en: '**Gazebo**: In Gazebo, we can build three-dimensional worlds with physics-based
    simulation. Gazebo along with **Robot Operating System** (**ROS)** and the OpenAI
    gym interface is gym-gazebo and can be used to train RL agents. To know more about
    this, you can refer to the whitepaper: [http://erlerobotics.com/whitepaper/robot_gym.pdf](http://erlerobotics.com/whitepaper/robot_gym.pdf).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gazebo**：在Gazebo中，我们可以构建具有基于物理模拟的三维世界。Gazebo与**机器人操作系统**（**ROS**）和OpenAI
    Gym接口一起使用，称为gym-gazebo，可以用来训练RL代理。有关更多信息，您可以参考白皮书：[http://erlerobotics.com/whitepaper/robot_gym.pdf](http://erlerobotics.com/whitepaper/robot_gym.pdf)。'
- en: '**Blender**** learning environment**: It''s a Python interface for the Blender
    game engine, and it also works over OpenAI gym. It has it''s base Blender. A free
    three-dimensional modeling software with an integrated game engine, this provides
    an easy-to-use, powerful set of tools for creating games. It provides an interface
    to the Blender game engine, and the games themselves are designed in Blender.
    We can then create the custom virtual environment to train an RL agent on a specific
    problem ([https://github.com/LouisFoucard/gym-blender](https://github.com/LouisFoucard/gym-blender)).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Blender**学习环境：这是Blender游戏引擎的Python接口，它也可以在OpenAI gym上使用。它以Blender为基础。Blender是一个免费的三维建模软件，集成了游戏引擎，提供了一套易于使用、功能强大的工具，用于创建游戏。它提供了一个Blender游戏引擎的接口，而游戏本身是在Blender中设计的。我们可以创建自定义虚拟环境，以便在特定问题上训练强化学习（RL）智能体（[https://github.com/LouisFoucard/gym-blender](https://github.com/LouisFoucard/gym-blender)）。'
- en: OpenAI gym
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI gym
- en: 'OpenAI gym is an open source toolkit to develop and compare RL algorithms.
    It contains a variety of simulated environments that can be used to train agents
    and develop new RL algorithms. To start, you''ll first have to install `gym`.
    For Python 3.5+, you can install `gym` using `pip`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI gym是一个开源工具包，用于开发和比较RL算法。它包含多种模拟环境，可用于训练智能体并开发新的RL算法。首先，你需要安装`gym`。对于Python
    3.5及以上版本，可以使用`pip`安装`gym`：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'OpenAI gym supports various environments, from simple text-based to three-dimensional.
    The environments supported in the latest version can be grouped as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI gym支持多种环境，从简单的基于文本的到三维环境。最新版本中支持的环境可以分为以下几类：
- en: '**Algorithms**: It contains environments that involve performing computations
    such as addition. While we can easily perform the computations on a computer,
    what makes these problems interesting as an RL problem is that the agent learns
    these tasks purely by example.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：它包含涉及执行计算任务的环境，如加法运算。尽管我们可以轻松地在计算机上进行计算，但作为一个RL问题，这些问题的有趣之处在于智能体仅通过示例学习这些任务。'
- en: '**Atari**: This environment provides a wide variety of classical Atari/arcade
    games.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Atari**：此环境提供各种经典的Atari/街机游戏。'
- en: '**Box2D**: It contains robotics tasks in two dimensions such as a car racing
    agent or bipedal robot walk.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Box2D**：它包含二维机器人任务，如赛车代理或双足机器人行走。'
- en: '**Classic control**: This contains the classical control theory problems, such
    as balancing a cart pole.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典控制**：包含经典的控制理论问题，如平衡推车摆杆。'
- en: '**MuJoCo**: This is proprietary (you can get a one-month free trial). It supports
    various robot simulation tasks. The environment includes a physics engine, hence,
    it''s used for training robotic tasks.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MuJoCo**：这是一个专有的物理引擎（你可以获得一个为期一个月的免费试用）。它支持多种机器人模拟任务。该环境包含物理引擎，因此用于训练机器人任务。'
- en: '**Robotics**: This environment too uses the physics engine of MuJoCo. It simulates
    goal-based tasks for fetch and shadow-hand robots.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人学**：这个环境也使用MuJoCo的物理引擎。它模拟基于目标的任务，如取物和影子手机器人任务。'
- en: '**Toy text**: It''s a simple text-based environment—very good for beginners.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩具文本**：这是一个基于文本的简单环境，非常适合初学者。'
- en: 'To get a complete list of environments under these groups, you can visit: [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).
    The best part of the OpenAI interface is that all of the environments can be accessed
    with the same minimum interface. To get a list of all available environments in
    your installation, you can use the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取这些组下所有环境的完整列表，你可以访问：[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)。OpenAI接口的最棒之处在于，所有环境都可以通过相同的最小接口进行访问。要获取你安装中所有可用环境的列表，你可以使用以下代码：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will provide a list of all installed environments along with their environment
    ID, which is a string. It''s also possible to add your own environment in the
    `gym` registry. To create an environment, we use the `make` command with the environment
    name passed as a string. For example, to create a game using the Pong environment,
    the string we need will be `Pong-v0`. The `make` command creates the environment,
    and the `reset` command is used to activate the environment. The `reset` command
    returns the environment in an initial state. The state is represented as an array:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供所有已安装环境的列表及其环境ID，ID为字符串类型。你还可以在`gym`注册表中添加你自己的环境。要创建一个环境，我们使用`make`命令，并将环境名称作为字符串传递。例如，要创建一个使用Pong环境的游戏，我们需要的字符串是`Pong-v0`。`make`命令创建环境，而`reset`命令用于激活该环境。`reset`命令将环境恢复到初始状态。该状态以数组形式表示：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The state space of `Pong-v0` is given by an array of the size 210×160×3, which
    actually represents the raw pixel values for the Pong game. On the other hand,
    if you create a **Go9×9-v0** environment, the state is defined by a 3×9×9 array.
    We can visualize the environment using the `render` command. The following diagram
    shows the rendered environment for the **Pong-v0** and **Go9x9-v0** environments
    at the initial state:.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pong-v0`的状态空间由一个210×160×3的数组表示，这实际上代表了Pong游戏的原始像素值。另一方面，如果你创建一个**Go9×9-v0**环境，状态则由一个3×9×9的数组定义。我们可以使用`render`命令来可视化环境。下图展示了**Pong-v0**和**Go9x9-v0**环境在初始状态下的渲染环境：'
- en: '![](img/b2bc09c8-c223-4b72-84a3-c6ea180afe56.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2bc09c8-c223-4b72-84a3-c6ea180afe56.png)'
- en: The rendered environments for Pong-v0 and Go9x9-v0
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pong-v0**和**Go9x9-v0**的渲染环境'
- en: The `render` commands pop up a window. If you want to display the environment
    inline, then you can use Matplotlib inline and change the `render` command to
    `plt.imshow(env.render(mode='rgb_array'))`. This will show the environment inline
    in the Jupyter Notebook.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`render`命令会弹出一个窗口。如果你想在内联显示环境，可以使用Matplotlib的内联模式，并将`render`命令更改为`plt.imshow(env.render(mode=''rgb_array''))`。这将在Jupyter
    Notebook中内联显示环境。'
- en: 'The environment contains the `action_space` variable, which determines the
    possible actions in the environment. We can select a random action using the `sample()`
    function. The selected action can affect the environment using the `step` function.
    The `step` function performs the selected action on the environment; it returns
    the changed state, the reward, a Boolean informing whether the game is over or
    not, and some information about the environment that can be useful for debugging,
    but isn''t used while working with RL agents. The following code shows a game
    of Pong with the agent playing a random move. We''re storing the state at each
    time step in an array, `frames`, so that we can later see the game:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 环境包含`action_space`变量，它决定了环境中可能的动作。我们可以使用`sample()`函数随机选择一个动作。选择的动作可以通过`step`函数影响环境。`step`函数在环境上执行所选动作；它返回改变后的状态、奖励、一个布尔值表示游戏是否结束，以及一些有助于调试但在与强化学习智能体互动时不会用到的环境信息。以下代码展示了一个Pong游戏，其中智能体执行一个随机动作。我们在每个时间步将状态存储在一个数组`frames`中，以便稍后查看游戏：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These frames can be displayed as a continuously playing GIF-style image in
    the Jupyter Notebook with the help of the animation function in Matplotlib and
    IPython:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些帧可以借助Matplotlib和IPython中的动画函数，在Jupyter Notebook中以持续播放的GIF风格图像显示：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Normally, to train an agent, we''ll need a very large number of steps, and
    so it won''t be feasible to store the state space at each step. We can either
    choose to store after every 500th (or any other number you wish) step in the preceding
    algorithm. Instead, we can use the OpenAI gym wrapper to save the game as a video.
    To do so, we need to first import wrappers, then create the environment, and finally
    use Monitor. By default, it will store the video of 1, 8, 27, 64, and so on and
    then every 1,000^(th) episode (episode numbers with perfect cubes); each training,
    by default, is saved in one folder. The code to do it is as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，训练一个智能体需要大量的步骤，因此在每个步骤保存状态空间是不可行的。我们可以选择在前述算法中的每500步（或任何其他你希望的步数）后进行存储。相反，我们可以使用OpenAI
    gym的包装器将游戏保存为视频。为此，我们首先需要导入包装器，然后创建环境，最后使用Monitor。默认情况下，它将存储1、8、27、64等的每个视频，以及每1,000^(次)的回合（回合数为完美的立方数）；每次训练默认保存在一个文件夹中。实现此功能的代码如下：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you want to use the same folder in the next training, you can choose the `force=True`
    option in the `Monitor` method call. In the end, we should close the environment
    using the `close` function:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在下次训练中使用相同的文件夹，可以在`Monitor`方法调用中选择`force=True`选项。最后，我们应该使用`close`函数关闭环境：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding codes are available in the  `OpenAI_practice.ipynb` Jupyter Notebook in
    the folder for [Chapter 6](01e534ff-b0a2-4b5e-bc9a-fd65c527ac7d.xhtml), *Reinforcement
    Learning for IoT,* in GitHub.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码可以在`OpenAI_practice.ipynb` Jupyter Notebook中找到，位于GitHub的[第6章](01e534ff-b0a2-4b5e-bc9a-fd65c527ac7d.xhtml)，*物联网强化学习*文件夹内。
- en: Q-learning
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: In his doctoral thesis, *Learning from delayed rewards*, Watkins introduced
    the concept of Q-learning in the year 1989\. The goal of Q-learning is to learn
    an optimal action selection policy. Given a specific state, *s*, and taking a
    specific action, *a*, Q-learning attempts to learn the value of the state *s*. In
    its simplest version, Q-learning can be implemented with the help of look-up tables.
    We maintain a table of values for every state (row) and action (column) possible
    in the environment. The algorithm attempts to learn the value—that is, how good
    it is to take a particular action in the given state.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的博士论文《从延迟奖励中学习》中，Watkins在1989年提出了Q学习的概念。Q学习的目标是学习一个最优的动作选择策略。给定一个特定的状态，*s*，并采取一个特定的动作，*a*，Q学习试图学习状态*s*的值。在最简单的版本中，Q学习可以通过查找表来实现。我们维护一个表，记录环境中每个状态（行）和动作（列）的值。算法试图学习这个值——即在给定状态下采取特定动作的好坏。
- en: 'We start by initializing all of the entries in the Q-table to *0*; this ensures
    all states a uniform (and hence equal chance) value. Later, we observe the rewards
    obtained by taking a particular action and, based on the rewards, we update the
    Q-table. The update in Q-value is performed dynamically with the help of **the
    Bellman Equation,** given by the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将Q表中的所有条目初始化为*0*；这确保了所有状态都有相同的（因此是平等的）价值。后来，我们观察采取特定动作所获得的奖励，并根据奖励更新Q表。Q值的更新是动态进行的，通过**贝尔曼方程**来帮助实现，方程如下：
- en: '![](img/fd672f88-b11d-4994-9ef5-79075fd547a1.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd672f88-b11d-4994-9ef5-79075fd547a1.png)'
- en: 'Here, *α* is the learning rate. This shows the basic Q-learning algorithm:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*α*是学习率。以下是基本的Q学习算法：
- en: '![](img/4dfc4179-734a-465e-8e72-b60d8cc3af20.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dfc4179-734a-465e-8e72-b60d8cc3af20.png)'
- en: Simple Q-learning algorithm
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的Q学习算法
- en: If you're interested, you can read the 240 pages Watkins doctoral thesis here: [http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣，你可以在这里阅读Watkins的240页博士论文：[http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)。
- en: 'At the end of learning, we''ll have a good Q-table, with optimal policy. An
    important question here is: how do we choose the action at the second step? There
    are two alternatives; first, we choose the action randomly. This allows our agent
    to explore all of the possible actions with equal probability but, at the same
    time, ignoring the information it has already learned. The second way is we choose
    the action for which the value is maximum; initially, all of the actions have
    the same Q-value but, as the agent will learn, some actions will get high value
    and others low value. In this case, the agent is exploiting the knowledge it has
    already learned. So what''s better: exploration or exploitation? This is called
    the **exploration-exploitation trade-off**. A natural way to solve this problem
    is by relying on what the agent has learned, but at the same time sometimes just
    explore. This is achieved via the use of the **epsilon greedy algorithm**. The
    basic idea is that the agent chooses the actions randomly with the probability, *ε*,
    and exploits the information learned in previous episodes by a probability, (*1-ε*).
    The algorithm chooses the best option (greedy) most of the time (*1-ε*) but sometimes
    (*ε*) it makes a random choice. Let''s now try to implement what we learned in
    a simple problem.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 学习结束时，我们将拥有一个好的Q表，并且有最优策略。这里有一个重要问题：我们如何选择第二步的动作？有两种选择；首先，我们随机选择一个动作。这使得我们的智能体能够以相等的概率探索所有可能的动作，但同时忽略了它已经学到的信息。第二种方式是我们选择具有最大值的动作；最初，所有动作的Q值相同，但随着智能体的学习，一些动作将获得高值，另一些则获得低值。在这种情况下，智能体正在利用它已经学到的知识。那么，哪个更好呢：探索还是利用？这就是所谓的**探索-利用权衡**。解决这个问题的一种自然方式是依赖于智能体已经学到的知识，但有时也需要进行探索。这是通过使用**ε-贪婪算法**实现的。基本的想法是，智能体以概率*ε*随机选择动作，而以概率（*1-ε*）利用在之前的回合中学到的信息。该算法大多数时候（*1-ε*）选择最好的选项（贪婪），但有时（*ε*）会做出随机选择。现在让我们尝试在一个简单的问题中实现我们学到的东西。
- en: Taxi drop-off using Q-tables
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Q表的出租车下车
- en: 'The simple Q-learning algorithm involves maintaining a table of the size *m*×*n*,
    where *m* is the total number of states and *n* the total number of possible actions.
    Therefore, we choose a problem from the toy-text group since their `state` space
    and `action` space is small. For illustrative purposes, we choose the `Taxi-v2` environment.
    The goal of our agent is to choose the passenger at one location and drop them
    off at another. The agent receives *+20* points for a successful drop-off and
    loses *1* point for every time step it takes. There''s also a 10-point penalty
    for illegal pick-up and drop-off. The state space has walls shown by **|** and
    four location marks, **R**, **G**, **Y**, and **B** respectively. The taxi is
    shown by box: the pick-up and drop-off location can be either of these four location
    marks. The pick-up point is colored blue, and the drop-off is colored purple.
    The `Taxi-v2` environment has a state space of size *500* and action space of
    size *6*, making a Q-table with *500×6=3000* entries:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 Q 学习算法涉及维护一个大小为 *m*×*n* 的表，其中 *m* 为状态总数，*n* 为可能的动作总数。因此，我们从玩具文本组中选择了一个问题，因为它们的
    `state` 空间和 `action` 空间都很小。为了便于说明，我们选择了 `Taxi-v2` 环境。我们的智能体目标是选择一个位置的乘客并将其送到另一个位置。智能体成功送客后会获得
    *+20* 分，每走一步会失去 *1* 分。如果进行非法的接送操作，还会扣除 10 分。状态空间中有墙壁（用 **|** 表示）和四个位置标记，分别是 **R**、**G**、**Y**
    和 **B**。出租车用框表示：接送位置可以是这四个标记中的任何一个。接客点用蓝色表示，送客点用紫色表示。`Taxi-v2` 环境的状态空间大小为 *500*，动作空间大小为
    *6*，因此 Q 表的大小为 *500×6=3000* 个条目：
- en: '![](img/e24e0dd9-8879-439b-ba1c-796ed9c1541b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e24e0dd9-8879-439b-ba1c-796ed9c1541b.png)'
- en: Taxi drop-off environment
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车接送环境
- en: 'In the taxi drop-off environment, the taxi is denoted by the yellow box. The
    location mark, R, is the pick-up position, and G is the drop-off location:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在出租车接送环境中，出租车用黄色框表示。位置标记 R 是接客点，G 是送客点：
- en: 'We start by importing the necessary modules and creating our environment. Since,
    here, we just need to make a look-up table, using TensorFlow won''t be necessary.
    As mentioned previously, the `Taxi-v2` environment has *500* possible states and
    *6* possible actions:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的模块并创建我们的环境。由于这里只需要创建查找表，因此使用 TensorFlow 并不是必需的。如前所述，`Taxi-v2` 环境有 *500*
    种可能的状态和 *6* 种可能的动作：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We initialize the Q-table of the size (*300×6*) with all zeros, and define
    the hyperparameters: *γ*, the discount factor, and *α*, the learning rate. We
    also set the values for maximum episodes (one episode means one complete run from
    reset to done=`True`) and maximum steps in an episode the agent will learn for:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将大小为 (*300×6*) 的 Q 表初始化为全零，并定义超参数：*γ* 为折扣因子，*α* 为学习率。我们还设定了最大轮次（一个轮次意味着从重置到完成=`True`
    的一次完整运行）和智能体将在每一轮中学习的最大步数：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, for each episode, we choose the action with the highest value, perform
    the action, and update the Q-table based on the received rewards and future state
    using the Bellman Equation:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，对于每一轮，我们选择具有最高值的动作，执行该动作，并根据收到的奖励和未来的状态，使用贝尔曼方程更新 Q 表：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s now see how the learned agent works:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看一下学习型智能体是如何工作的：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following diagram shows the agent behavior in a particular example. The
    empty car is shown as a yellow box, and the car with the passenger is shown by
    a green box. You can see that, in the given case, the agent picks up and drops
    off the passenger in 11 steps, and the desired location is marked (**B**) and
    the destination is marked (**R**):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在一个特定示例中智能体的行为。空车用黄色框表示，载有乘客的车用绿色框表示。从图中可以看出，在给定的情况下，智能体在 11 步内完成接送乘客，目标位置标记为
    (**B**)，目的地标记为 (**R**)：
- en: '![](img/404c987f-7024-4e67-a00c-aab1d910e40b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/404c987f-7024-4e67-a00c-aab1d910e40b.png)'
- en: Agent picking up and dropping off a passenger using the learned Q-table
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习到的 Q 表，智能体接送乘客
- en: Cool, right? The complete code is available in the `Taxi_drop-off.ipynb` file
    available at GitHub.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 酷吧？完整的代码可以在 GitHub 上找到，文件名为 `Taxi_drop-off.ipynb`。
- en: Q-Network
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q 网络
- en: The simple Q-learning algorithm involves maintaining a table of the size *m*×*n*,
    where *m* is the total number of states and *n* the total number of possible actions.
    This means we can't use it for large state space and action space. An alternative
    is to replace the table with a neural network acting as a function approximator,
    approximating the Q-function for each possible action. The weights of the neural
    network in this case store the Q-table information (they match a given state with
    the corresponding action and its Q-value). When the neural network that we use
    to approximate the Q-function is a deep neural network, we call it a **Deep Q-Network**
    (**DQN**).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 Q 学习算法涉及维持一个大小为 *m*×*n* 的表格，其中 *m* 是状态的总数，*n* 是可能动作的总数。这意味着我们不能将其用于大规模的状态空间和动作空间。一个替代方法是用神经网络替换表格，作为一个函数逼近器，逼近每个可能动作的
    Q 函数。在这种情况下，神经网络的权重存储着 Q 表格的信息（它们将给定状态与相应的动作及其 Q 值匹配）。当我们用深度神经网络来逼近 Q 函数时，我们称其为
    **深度 Q 网络** (**DQN**)。
- en: The neural network takes the state as its input and calculates the Q-value of
    all of the possible actions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络以状态为输入，计算所有可能动作的 Q 值。
- en: Taxi drop-off using Q-Network
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q-网络进行出租车下车
- en: 'If we consider the preceding *Taxi drop-off* example, our neural network will
    consist of *500* input neurons (the state represented by *1×500* one-hot vector)
    and *6* output neurons, each neuron representing the Q-value for the particular
    action for the given state.  The neural network will here approximate the Q-value
    for each action. Hence, the network should be trained so that its approximated
    Q-value and the target Q-value are same. The target Q-value as obtained from the
    Bellman Equation is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑前面的 *出租车下车* 示例，我们的神经网络将由 *500* 个输入神经元组成（状态由 *1×500* 的 one-hot 向量表示），以及
    *6* 个输出神经元，每个神经元代表给定状态下某一特定动作的 Q 值。神经网络将在此处逼近每个动作的 Q 值。因此，网络应该经过训练，使得其逼近的 Q 值与目标
    Q 值相同。目标 Q 值由贝尔曼方程给出，如下所示：
- en: '![](img/f09c0b5b-3f34-4b90-ac56-6ef4863af836.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f09c0b5b-3f34-4b90-ac56-6ef4863af836.png)'
- en: 'We train the neural network so that the square error of the difference between
    the target *Q* and predicted *Q* is minimized—that is, the neural network minimizes
    the following loss function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练神经网络，使得目标 *Q* 和预测 *Q* 之间的平方误差最小化——也就是说，神经网络最小化以下损失函数：
- en: '![](img/4dd26570-6e18-4b2d-a7f1-98d2dddfa5ff.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dd26570-6e18-4b2d-a7f1-98d2dddfa5ff.png)'
- en: 'The aim is to learn the unknown Q[target] function. The weights of `QNetwork`
    are updated using backpropagation so that this loss is minimized. We make the
    neural network, `QNetwork`, to approximate the Q-value. It''s a very simple single-layer
    neural network, with methods to provide action and their Q-values (`get_action`),
    train the network (`learnQ`), and get the predicted Q-value (`Qnew`):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是学习未知的 Q[target] 函数。通过反向传播更新 `QNetwork` 的权重，以使损失最小化。我们使神经网络 `QNetwork` 来逼近
    Q 值。它是一个非常简单的单层神经网络，具有提供动作及其 Q 值（`get_action`）、训练网络（`learnQ`）以及获取预测 Q 值（`Qnew`）的方法：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We now incorporate this neural network in our earlier code where we trained
    an RL agent for the *Taxi drop-off* problem. We''ll need to make some changes;
    first, the state returned by the OpenAI step and reset function in this case is
    just the numeric identification of state, so we need to convert it into a one-hot
    vector. Also, instead of a Q-table update, we''ll now get the new Q-predicted
    from `QNetwork`, find the target Q, and train the network so as to minimize the
    loss. The code is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这个神经网络集成到之前训练 RL 代理解决 *出租车下车* 问题的代码中。我们需要进行一些更改；首先，OpenAI 步骤和重置函数返回的状态只是状态的数字标识符，所以我们需要将其转换为一个
    one-hot 向量。此外，我们不再使用 Q 表格更新，而是从 `QNetwork` 获取新的 Q 预测值，找到目标 Q 值，并训练网络以最小化损失。代码如下：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This should have done a good job but, as you can see, even after training for
    *1,000* episodes, the network has a high negative reward, and if you check the
    performance of the network, it appears to just take random steps. Yes, our network
    hasn''t learned anything; the performance is worse than Q-table. This can also
    be verified from the reward plot while training—ideally, the rewards should increase
    as the agent learns, but nothing of the sort happens here; the rewards increase
    and decrease like a random walk around the mean (the complete code for this program
    is in the `Taxi_drop-off_NN.ipynb` file available at GitHub):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这本应该做得很好，但正如你所看到的，即使训练了*1,000*个回合，网络的奖励依然很低，如果你查看网络的表现，似乎它只是随便走动。是的，我们的网络什么都没学到；表现比Q表还差。这也可以从训练过程中的奖励图表验证——理想情况下，随着代理的学习，奖励应该增加，但这里并没有发生这种情况；奖励像是围绕平均值的随机漫步（该程序的完整代码可以在`Taxi_drop-off_NN.ipynb`文件中找到，文件在GitHub上可用）：
- en: '![](img/13ebed29-bce7-456d-a4d4-23ad3eb5dda4.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13ebed29-bce7-456d-a4d4-23ad3eb5dda4.png)'
- en: Total reward per episode obtained by the agent as it learns
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在学习过程中每个回合获得的总奖励
- en: What happened? Why is the neural network failing to learn, and can we make it
    better?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？为什么神经网络没有学到东西，我们能改进它吗？
- en: 'Consider the scenario when the taxi should go west to pick up and, randomly,
    the agent chose west; the agent gets a reward and the network will learn that,
    in the present state (represented by a one-hot vector), going west is favorable.
    Next, consider another state similar to this one (correlated state space): the
    agent again makes the west move, but this time it results in a negative reward,
    so now the agent will unlearn what it had learned earlier. Hence, similar state-actions
    but divergent targets confuse the learning process. This is called **catastrophic
    forgetting**. The problem arises here because consecutive states are highly correlated
    and so, if the agent learns in sequence (as it does here), this extremely correlated
    input state space won''t let the agent learn.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 假设出租车需要往西走去接客，而代理随机选择了西行；代理获得了奖励，网络会学习到，在当前状态下（通过一个独热编码表示），西行是有利的。接下来，考虑另一个与此相似的状态（相关状态空间）：代理再次做出西行动作，但这次却获得了负奖励，所以现在代理会忘记之前学到的东西。因此，相似的状态-动作对但目标不同会混淆学习过程。这被称为**灾难性遗忘**。问题的产生是因为连续状态高度相关，因此，如果代理按顺序学习（如本例），这个高度相关的输入状态空间会妨碍代理的学习。
- en: 'Can we break the correlation between the input presented to the network? Yes,
    we can: we can construct a **replay buffer**, where we first store each state,
    its corresponding action, and the consecutive reward and resultant state (state,
    action, reward, new state). The actions, in this case, are chosen completely randomly,
    thereby ensuring a wide range of actions and resultant states. The replay buffer
    will finally consist of a large list of these tuples (*S*, *A*, *R*, *S''*). Next,
    we present the network with these tuples randomly (instead of sequentially); this
    randomness will break the correlation between consecutive input states. This is
    called **experience replay**. It not only resolves the issues with correlation
    in input state space but also allows us to learn from the same tuples more than
    once, recall rare occurrences, and in general, make better use of the experience.
    In one way, you can say that, by using a replay buffer, we''ve reduced the problem
    of the supervised learning (with the replay buffer as an input-output dataset),
    where the random sampling of input ensures that the network is able to generalize.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打破输入数据与网络之间的关联吗？可以：我们可以构建一个**回放缓冲区**，在这里我们首先存储每个状态、其对应的动作、连续奖励和结果状态（状态、动作、奖励、新状态）。在这种情况下，动作是完全随机选择的，从而确保了动作和结果状态的多样性。回放缓冲区最终将由这些元组（*S*，*A*，*R*，*S'*）组成一个大列表。接下来，我们将这些元组随机地输入到网络中（而不是按顺序输入）；这种随机性将打破连续输入状态之间的关联。这被称为**经验回放**。它不仅解决了输入状态空间中的关联问题，还使我们能够多次从相同的元组中学习，回顾稀有事件，并且通常能更好地利用经验。从某种意义上说，通过使用回放缓冲区，我们已经减少了监督学习中的问题（回放缓冲区作为输入输出数据集），其中输入的随机采样确保了网络能够进行泛化。
- en: Another problem with our approach is that we're updating the target Q immediately.
    This too can cause harmful correlations. Remember that, in Q-learning, we're trying
    to minimize the difference between the *Q[target]* and the currently predicted
    *Q*. This difference is called a **temporal difference** (**TD**) error (and hence
    Q-learning is a type of **TD learning**). At present, we update our *Q[target]*
    immediately, hence there exists a correlation between the target and the parameters
    we're changing (weights through *Q[pred]*). This is almost like chasing a moving
    target and hence won't give a generalized direction. We can resolve the issue
    by using **fixed Q-targets**—that is, use two networks, one for predicting *Q* and
    another for target *Q*. Both are exactly the same in terms of architecture, with
    the predicting Q-Network changing weights at each step, but the weight of the
    target Q-Network is updated after some fixed learning steps. This provides a more
    stable learning environment.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法的另一个问题是，我们立即更新目标Q。这也会导致有害的相关性。请记住，在Q学习中，我们试图最小化*Q[target]*与当前预测的*Q*之间的差异。这个差异被称为**时序差分**（**TD**）误差（因此Q学习是一种**TD学习**）。目前，我们立即更新我们的*Q[target]*，因此目标与我们正在更改的参数之间（通过*Q[pred]*进行的权重）存在相关性。这几乎就像在追逐一个移动的目标，因此不会给出一个通用的方向。我们可以通过使用**固定的Q目标**来解决这个问题——即使用两个网络，一个用于预测*Q*，另一个用于目标*Q*。这两个网络在架构上完全相同，预测Q网络在每一步中都会改变权重，而目标Q网络的权重会在固定的学习步骤后更新。这提供了一个更加稳定的学习环境。
- en: 'Finally, we make one more small change: right now our epsilon has had a fixed
    value throughout learning. But, in real life, this isn''t so. Initially, when
    we know nothing, we explore a lot but, as we become familiar, we tend to take
    the learned path. The same can be done in our epsilon-greedy algorithm, by changing
    the value of epsilon as the network learns through each episode, so that epsilon
    decreases with time.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们做一个小小的改变：目前，我们的epsilon在整个学习过程中都有一个固定值。但在现实生活中并非如此。最初，当我们一无所知时，我们会进行大量探索，但随着我们变得熟悉，我们倾向于采取已学到的路径。在我们的epsilon贪婪算法中也可以做到这一点，通过随着网络在每个回合中学习，逐步改变epsilon的值，使得epsilon随时间减少。
- en: Equipped with these tricks, let's now build a DQN to play an Atari game.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了这些技巧后，现在我们来构建一个DQN来玩Atari游戏。
- en: DQN to play an Atari game
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN来玩Atari游戏
- en: 'The DQN we''ll learn here is based on a DeepMind paper ([https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)). At
    the heart of DQN is a deep convolutional neural network that takes as input the
    raw pixels of the game environment (just like any human player would see), captured
    one screen at a time, and as output, returns the value for each possible action.
    The action with the maximum value is the chosen action:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里学习的DQN基于DeepMind的论文（[https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)）。DQN的核心是一个深度卷积神经网络，它以游戏环境的原始像素为输入（就像任何人类玩家看到的一样），每次捕捉一屏幕，并将每个可能动作的值作为输出。值最大的动作就是选择的动作：
- en: 'The first step is to get all of the modules we''ll need:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是获取我们所需的所有模块：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We chose the Breakout game from the list of OpenAI Atari games—you can try
    the code for other Atari games; the only change you may need to do would be in
    the preprocessing step. The input space of Breakout—our input space—consists of
    210×160 pixels, with 128 possible colors for each pixel. It''s an enormously large
    input space. To reduce the complexity, we''ll choose a region of interest in the
    image, convert it into grayscale, and resize it to an image of the size *80×80*.
    We do this using the `preprocess` function:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择了OpenAI Atari游戏列表中的Breakout游戏——你可以尝试其他Atari游戏的代码；你可能唯一需要更改的地方是预处理步骤。Breakout的输入空间——即我们的输入空间——由210×160个像素组成，每个像素有128种可能的颜色。这是一个非常庞大的输入空间。为了减少复杂性，我们将选择图像中的一个感兴趣区域，将其转换为灰度图像，并将其调整为大小为*80×80*的图像。我们通过`preprocess`函数来实现这一点：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following screenshot shows the environment before and after the preprocessing:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了预处理前后的环境：
- en: '![](img/947614bc-85de-436a-93be-2b01ae49a075.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/947614bc-85de-436a-93be-2b01ae49a075.png)'
- en: The original environment, size 210× 160 (colored image) and the processed environment,
    size 80×80 (grayscale)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 原始环境的大小为210×160（彩色图像），处理后的环境大小为80×80（灰度图像）。
- en: 'As you can see from the preceding diagram, it isn''t possible to tell whether
    the ball is coming down or going up. To deal with this problem, we combine four
    consecutive states (due to four unique actions) as one input. We define a function, `update_state`,
    that appends the current environment observation to the previous state array:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前面的图示中可以看到，无法判断球是下落还是上升。为了解决这个问题，我们将四个连续的状态（由于四个独特的动作）结合为一个输入。我们定义了一个函数`update_state`，它将当前环境的观察结果追加到前一个状态数组中：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The function appends the processed new state in the sliced state, ensuring
    that the final input to the network consists of four frames. In the following
    screenshot, you can see the four consecutive frames. This is the input to our
    DQN:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将处理过的新状态追加到切片的状态中，确保网络的最终输入由四个帧组成。在以下截图中，你可以看到这四个连续的帧。这是我们DQN的输入：
- en: '![](img/2bbec24a-38d5-4bc4-92ac-0340ca53e978.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bbec24a-38d5-4bc4-92ac-0340ca53e978.png)'
- en: The input to DQN four consecutive game-states (frames)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: DQN的输入是四个连续的游戏状态（帧）
- en: 'We create a DQN that we define in the class DQN; it consists of three convolutional
    layers, the output of the last convolutional layer is flattened, and it''s then
    followed by two fully connected layers. The network, as in the previous case,
    tries to minimize the difference between *Q[target] *and *Q[predicted]*. In the
    code, we''re using the RMSProp optimizer, but you can play around with other optimizers:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个在`class DQN`中定义的DQN；它由三层卷积层组成，最后一层卷积层的输出被展平，然后是两个全连接层。该网络和之前的情况一样，试图最小化*Q[target]*和*Q[predicted]*之间的差异。在代码中，我们使用的是RMSProp优化器，但你也可以尝试其他优化器：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The necessary methods that we require for this class are discussed in the following
    steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在接下来的步骤中讨论了该类所需的必要方法：
- en: 'We add a method to return the predicted Q-values:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加了一个方法来返回预测的Q值：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We need a method to determine the action with maximum value. In this method,
    we also implemented the epsilon-greedy policy, and the value of epsilon is changed
    in the main code:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要一个方法来确定具有最大值的动作。在这个方法中，我们还实现了epsilon-贪婪策略，且epsilon的值在主代码中会发生变化：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We need a method to update the weights of the network so as to minimize the
    loss. The function can be defined as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要一个方法来更新网络的权重，以最小化损失。该函数可以定义如下：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Copy the model weights to the fixed Q-Network:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型权重复制到固定的Q网络中：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Besides these methods, we need some helper functions to save the learned network,
    load the saved network, and set the TensorFlow session:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了这些方法，我们还需要一些辅助函数来保存学习到的网络、加载保存的网络，并设置TensorFlow会话：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To implement the DQN algorithm, we use a `learn` function; it picks a random
    sample from the experience replay buffer and updates the Q-Network, using target
    Q from the target Q-Network:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现DQN算法，我们使用一个`learn`函数；它从经验重放缓冲区中随机选择一个样本，并使用目标Q网络中的目标Q来更新Q网络：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Well, all of the ingredients are ready, so let''s now decide the hyperparameters
    for our DQN and create our environment:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好了，所有的元素都准备好了，现在让我们决定DQN的超参数并创建我们的环境：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And finally, the following is the code that calls then fills the experience
    replay buffer, plays the game step by step, and trains the model network at every
    step and `target_model` after every four steps:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，以下是调用并填充经验重放缓冲区、逐步执行游戏并在每一步训练模型网络以及每四步训练`target_model`的代码：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can see that now the reward is increasing with episodes, with an average
    reward of **20** by the end, though it can be higher, then we had only learned
    few thousand episodes and even our replay buffer with a size between (50,00 to
    5,000,000):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在奖励随着回合数增加，最终的平均奖励是**20**，尽管它可能更高，但我们仅仅学习了几千个回合，甚至我们的重放缓冲区的大小在（50,000到5,000,000）之间：
- en: '![](img/4f2a3911-d5d0-49e6-a497-be4aa063ce9b.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f2a3911-d5d0-49e6-a497-be4aa063ce9b.png)'
- en: Average rewards as the agent learn
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体学习过程中的平均奖励
- en: 'Let''s see how our agent plays, after learning for about 2,700 episodes:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们的智能体在学习了大约2,700个回合后是如何表现的：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You can see the video of the learned agent here: [https://www.youtube.com/watch?v=rPy-3NodgCE](https://www.youtube.com/watch?v=rPy-3NodgCE).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到学习过的智能体的视频：[https://www.youtube.com/watch?v=rPy-3NodgCE](https://www.youtube.com/watch?v=rPy-3NodgCE)。
- en: Cool, right? Without telling it anything, it learned to play a decent game after
    only 2,700 episodes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，对吧？没有告诉它任何信息，它仅仅通过2,700个回合学会了如何玩一款不错的游戏。
- en: 'There are some things that can help you to train the agent better:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以帮助你更好地训练智能体：
- en: Since training takes a lot of time, unless you have a strong computational resource,
    it's better to save the model and restart the saved model.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练需要大量时间，除非你有强大的计算资源，否则最好保存模型并重新启动保存的模型。
- en: In the code, we used `Breakout-v0` and OpenAI gym, in this case, repeats the
    same step in the environment for consecutive (randomly chosen `1`, `2`, `3` or
    `4`) frames. You can instead choose `BreakoutDeterministic-v4`, the one used by
    the DeepMind team; here, the steps are repeated for exactly four consecutive frames.
    The agent hence sees and selects the action after every fourth frame.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中，我们使用了`Breakout-v0`和OpenAI gym，在这种情况下，环境中会对连续（随机选择的`1`、`2`、`3`或`4`）帧重复相同的步骤。你也可以选择`BreakoutDeterministic-v4`，这是DeepMind团队使用的版本；在这里，步骤会恰好在连续的四帧中重复。因此，智能体在每第四帧后看到并选择动作。
- en: Double DQN
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重DQN
- en: Now, recall that, we're using a max operator to both select an action and to
    evaluate an action. This can result in overestimated values for an action that
    may not be an ideal one. We can take care of this problem by decoupling the selection
    from evaluation. With Double DQN, we have two Q-Networks with different weights;
    both learn by random experience, but one is used to determine the action using
    the epsilon-greedy policy and the other to determine its value (hence, calculating
    the target Q).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下，我们使用最大值操作符来选择一个动作并评估这个动作。这可能导致一个可能并非最理想的动作被高估。我们可以通过将选择和评估解耦来解决这个问题。通过Double
    DQN，我们有两个权重不同的Q网络；这两个网络都通过随机经验进行学习，但一个用于通过epsilon-greedy策略来确定动作，另一个用于确定其值（因此，计算目标Q值）。
- en: 'To make it clearer, let''s first see the case of the DQN. The action with maximum
    Q-value is selected; let *W* be the weight of the DQN, then what we''re doing
    is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明，让我们先看一下DQN的情况。选择具有最大Q值的动作；设*W*为DQN的权重，那么我们正在做的是：
- en: '![](img/82f81b0f-9082-4204-b661-6e9b18499ca6.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82f81b0f-9082-4204-b661-6e9b18499ca6.png)'
- en: 'The superscript *W* tells the weights used to approximate the Q-value. In Double
    DQN, the equation changes to the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上标*W*表示用于近似Q值的权重。在Double DQN中，方程变为如下：
- en: '![](img/64ff7e73-19e9-4c73-a3b7-1f9dfc40e701.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64ff7e73-19e9-4c73-a3b7-1f9dfc40e701.png)'
- en: 'Note the change: now the action is chosen using the Q-Network with the weights
    *W*, and max Q-value is predicted using a Q-Network with weights *W''.* This reduces
    the overestimation and helps us to train the agent quickly and more reliably.
    You can access the *Deep Reinforcement Learning with Double Q-Learning* paper
    here: [https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意变化：现在，动作是通过使用权重为*W*的Q网络选择的，并且最大Q值是通过使用权重为*W'*的Q网络预测的。这减少了过估计，有助于我们更快且更可靠地训练智能体。你可以在这里访问*Deep
    Reinforcement Learning with Double Q-Learning*论文：[https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)。
- en: Dueling DQN
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对战DQN
- en: Dueling DQN decouples the Q-function into the value function and advantage function.
    The value function is the same as discussed earlier ; it represents the value
    of the state independent of action. The advantage function, on the other hand,
    provides a relative measure of the utility (advantage/goodness) of action *a*
    in the state *s:*
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对战DQN将Q函数解耦为价值函数和优势函数。价值函数和之前讨论的相同；它表示状态的价值，与动作无关。另一方面，优势函数提供了动作*a*在状态*s*中的相对效用（优势/好处）的度量：
- en: '![](img/4cf3f468-8c90-4d54-b1dc-5321a9cc9823.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cf3f468-8c90-4d54-b1dc-5321a9cc9823.png)'
- en: In Dueling DQN, the same convolutional is used to extract features but, in later
    stages, it's separated into two separate networks, one providing the value and
    another providing the advantage. Later, the two stages are recombined using an
    aggregating layer to estimate the Q-value. This ensures that the network produces
    separate estimates for the value function and the advantage function. The intuition
    behind this decoupling of value and advantage is that, for many states, it's unnecessary
    to estimate the value of each action choice. For example, in the car race, if
    there's no car in front, then the action turn left or turn right is not required
    and so there's no need to estimate the value of these actions on the given state.
     This allows it to learn which states are valuable, without having to determine
    the effect of each action for each state.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在Dueling DQN中，使用相同的卷积层来提取特征，但在后期阶段，它被分成两个独立的网络，一个提供价值，另一个提供优势。随后，两个阶段通过聚合层重新组合，以估计Q值。这确保了网络为价值函数和优势函数生成独立的估计值。这种价值和优势的解耦直觉是，对于许多状态，估计每个动作选择的价值并非必要。例如，在赛车中，如果前方没有车，那么选择“向左转”或“向右转”就没有必要，因此在给定状态下无需估计这些动作的价值。这使得网络可以学习哪些状态是有价值的，而不必为每个状态确定每个动作的效果。
- en: 'At the aggregate layer, the value and advantage are combined such that it''s
    possible to recover both *V* and *A* uniquely from a given *Q*. This is achieved
    by enforcing that the advantage function estimator has zero advantage at the chosen
    action:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合层中，价值和优势被结合在一起，使得可以从给定的*Q*中唯一地恢复出*V*和*A*。这是通过强制要求优势函数估计器在所选动作下的优势为零来实现的：
- en: '![](img/a235ba01-25da-4810-b326-fc80f8b57e79.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a235ba01-25da-4810-b326-fc80f8b57e79.png)'
- en: 'Here, *θ* is the parameter of the common convolutional feature extractor, and *α*
    and *β* are the parameters for the advantage and value estimator network. The
    Dueling DQN too was proposed by Google''s DeepMind team. You can read the complete
    paper at *arXiv*: [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581).
    The authors found that changing the preceding `max` operator with an average operator
    increases the stability of the network. The advantage, in this case, changes only
    as fast as the mean. Hence, in their results, they used the aggregate layer given
    by the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*θ*是通用卷积特征提取器的参数，*α*和*β*是优势和值估计器网络的参数。Dueling DQN也是由谷歌DeepMind团队提出的。你可以在*arXiv*上阅读完整的论文：[https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)。作者发现，用平均操作替换先前的`max`操作可以提高网络的稳定性。在这种情况下，优势的变化速度仅与均值变化速度相同。因此，在他们的结果中，使用了以下给出的聚合层：
- en: '![](img/bd2bd8f7-e556-4009-aa53-b1d457099849.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd2bd8f7-e556-4009-aa53-b1d457099849.png)'
- en: 'The following screenshot shows the basic architecture of a Dueling DQN:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了Dueling DQN的基本架构：
- en: '![](img/04aeee5f-ab54-4e73-873f-66b6fece0574.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04aeee5f-ab54-4e73-873f-66b6fece0574.png)'
- en: The basic architecture of Dueling DQN
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Dueling DQN的基本架构
- en: Policy gradients
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: In the Q-learning-based methods, we generated a policy after estimating a value/Q-function.
    In policy-based methods, such as the policy gradient, we approximate the policy
    directly.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于Q学习的方法中，我们在估计价值/Q函数之后生成策略。在基于策略的方法中，如策略梯度方法，我们直接逼近策略。
- en: Continuing as earlier, here, we use a neural network to approximate the policy.
    In the simplest form, the neural network learns a policy for selecting the actions
    that maximize the rewards by adjusting its weights using steepest gradient ascent,
    hence the name policy gradients.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 按照之前的方法，我们在这里使用神经网络来逼近策略。在最简单的形式下，神经网络通过使用最陡梯度上升法调整权重来学习选择最大化奖励的动作策略，因此得名“策略梯度”。
- en: 'In policy gradients, the policy is represented by a neural network whose input
    is a representation of states and whose output is action selection probabilities.
    The weights of this network are the policy parameters that we need to learn. The
    natural question arises: how should we update the weights of this network? Since
    our goal is to maximize rewards, it makes sense that our network tries to maximize
    the expected rewards per episode:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度中，策略由一个神经网络表示，其输入是状态的表示，输出是动作选择的概率。该网络的权重是我们需要学习的策略参数。自然会产生一个问题：我们应该如何更新这个网络的权重？由于我们的目标是最大化奖励，因此可以理解，我们的网络试图最大化每个回合的期望奖励：
- en: '![](img/05ad2539-cfd2-4f99-8e91-9edc0d186e10.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05ad2539-cfd2-4f99-8e91-9edc0d186e10.png)'
- en: 'Here, we''ve taken a parametrized stochastic policy *π—*that is, the policy
    determines the probability of choosing an action *a* given state *s*, and the
    neural network parameters are *θ*. *R* represents the sum of all of the rewards
    in an episode. The network parameters are then updated using gradient ascent:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了一个参数化的随机策略 *π*——也就是说，策略决定了在给定状态 *s* 的情况下选择动作 *a* 的概率，神经网络的参数是 *θ*。*R*
    代表一个回合中所有奖励的总和。然后，使用梯度上升法更新网络参数：
- en: '![](img/1198f6cd-5308-4e80-8911-e375a002e057.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1198f6cd-5308-4e80-8911-e375a002e057.png)'
- en: 'Here, *η* is the learning rate. Using the policy gradient theorem, we get the
    following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*η* 是学习率。通过策略梯度定理，我们得到如下公式：
- en: '![](img/0911f1c5-bb9d-46c1-ade7-90cab47fdc5c.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0911f1c5-bb9d-46c1-ade7-90cab47fdc5c.png)'
- en: 'Hence, instead of maximizing the expected return, we can use loss function
    as log-loss (expected action and predicted action as labels and logits respectively)
    and the discounted reward as the weight to train the network. For more stability,
    it has been found that adding a baseline helps in variance reduction. The most
    common form of the baseline is the sum of the discounted rewards, resulting in
    the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，替代最大化期望回报，我们可以使用损失函数作为对数损失（将期望动作和预测动作作为标签和 logits），并将折扣奖励作为权重来训练网络。为了增加稳定性，研究发现添加基线有助于减少方差。最常见的基线形式是折扣奖励的总和，结果如下所示：
- en: '![](img/2871ebfe-531e-4f6e-acbd-4e71ca5af1ea.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2871ebfe-531e-4f6e-acbd-4e71ca5af1ea.png)'
- en: 'The baseline *b*(*s[t]*) is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 *b*(*s[t]*) 如下所示：
- en: '![](img/5d1d07d4-39cf-4400-876e-297c54fe5302.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d1d07d4-39cf-4400-876e-297c54fe5302.png)'
- en: Here, *γ* is the discount factor.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*γ* 是折扣因子。
- en: Why policy gradients?
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择策略梯度？
- en: Well, first of all, policy gradients, like other policy-based methods, directly
    estimate the optimal policy, without any need to store additional data (experience
    replay buffer). Hence, it's simple to implement. Secondly, we can train it to
    learn true stochastic policies. And finally, it's well suited for continuous action-space.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，政策梯度方法像其他基于策略的方法一样，直接估计最优策略，无需存储额外的数据（经验回放缓冲区）。因此，它的实现非常简单。其次，我们可以训练它来学习真正的随机策略。最后，它非常适合连续动作空间。
- en: Pong using policy gradients
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用策略梯度玩 Pong 游戏
- en: 'Let''s try to use policy gradients to play a game of Pong. The Andrej Karpathy
    blog post, at [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/) inspires
    the implementation here. Recall that, in *Breakout*, we used four-game frames
    stacked together as input so that the game dynamics are known to the agent; here,
    we use the difference between two consecutive game frames as the input to the
    network. Hence, our agent has information about the present state and the previous
    state with it:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用策略梯度来玩 Pong 游戏。这里的实现灵感来自 Andrej Karpathy 的博客文章，文章地址：[http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)。回想一下，在
    *Breakout* 游戏中，我们使用了四个游戏帧堆叠在一起作为输入，从而使代理能够了解游戏的动态；而在这里，我们使用连续两帧游戏图像的差异作为输入。因此，我们的代理同时拥有当前状态和前一个状态的信息：
- en: 'The first step, as always, is importing the modules necessary. We import TensorFlow,
    Numpy, Matplotlib, and `gym` for the environment:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，和往常一样，我们需要导入必要的模块。我们导入了 TensorFlow、Numpy、Matplotlib 和 `gym` 作为环境：
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We build our neural network, the `PolicyNetwork`; it takes as input the state
    of the game, and outputs the action selection probabilities. Here, we build a
    simple two-layered perceptron, with no biases. `weights` are initialized randomly
    using the `Xavier` initialization. The hidden layer uses the `ReLU` activation
    function, and the output layer uses the `softmax` activation function. We use
    the `tf_discount_rewards` method defined later to calculate the baseline. And
    finally, we''ve used TensorFlow `tf.losses.log_loss` with calculated action probabilities
    as predictions, and chosen one-hot action vector as labels and discounted reward
    corrected by variance as weight:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建了我们的神经网络，即 `PolicyNetwork`；它以游戏状态作为输入，并输出动作选择概率。在这里，我们构建了一个简单的两层感知器，且没有偏置。`weights`
    使用 `Xavier` 初始化方法随机初始化。隐藏层使用 `ReLU` 激活函数，输出层使用 `softmax` 激活函数。我们使用稍后定义的 `tf_discount_rewards`
    方法来计算基线。最后，我们使用 TensorFlow 的 `tf.losses.log_loss` 来计算预测的动作概率，并选择一个热编码的动作向量作为标签，同时将折扣奖励和方差校正后的奖励作为权重：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The class has methods to calculate the action probabilities (`tf_policy_forward`
    and `predict_UP`), calculate the baseline using `tf_discount_rewards`, update
    the weights of the network (`update`), and finally set the session (`set_session`),
    then load and save the model:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类有方法来计算动作概率（`tf_policy_forward` 和 `predict_UP`），使用 `tf_discount_rewards` 计算基线，更新网络权重（`update`），并最终设置会话（`set_session`），然后加载和保存模型：
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that `PolicyNetwork` is made, we make a `preprocess` function to the game
    state; we won''t process the complete 210×160 state space—instead, we''ll reduce
    it to an 80×80 state space, in binary, and finally flatten it:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，`PolicyNetwork`已经创建，我们为游戏状态创建了一个`preprocess`函数；我们不会处理完整的210×160状态空间——而是将其缩减为80×80的二值状态空间，最后将其展平：
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s define some variables that we''ll require to hold state, labels, rewards,
    and action space size. We initialize the game state and instantiate the policy
    network:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一些变量，来保存状态、标签、奖励和动作空间大小。我们初始化游戏状态并实例化策略网络：
- en: '[PRE30]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we start the policy gradient algorithm. For each episode, the agent first
    plays the game, storing the states, rewards, and actions chosen. Once a game is
    over, it uses all of the stored data to train itself (just like in supervised
    learning). And it repeats this process for as many episodes as you want:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开始实施策略梯度算法。对于每一集，智能体首先进行游戏，存储状态、奖励和选择的动作。一旦游戏结束，它就会使用所有存储的数据来进行训练（就像监督学习一样）。然后它会重复这一过程，直到达到你想要的集数：
- en: '[PRE31]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: After training for 7,500 episodes, it started winning some games. After 1,200
    episodes the winning rate improved, and it was winning 50% of the time. After
    20,000 episodes, the agent was winning most games. The complete code is available
    at GitHub in the `Policy gradients.ipynb` file. And you can see the game played
    by the agent after learning for 20,000 episodes here: [https://youtu.be/hZo7kAco8is](https://youtu.be/hZo7kAco8is).
    Note that, this agent learned to oscillate around its position; it also learned
    to pass the force created by its movement to the ball and has learned that the
    other player can be beaten only by attacking shots.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练了7,500集后，智能体开始赢得一些游戏。在训练了1,200集后，胜率有所提高，达到了50%。经过20,000集训练后，智能体已经能够赢得大部分游戏。完整代码可在GitHub的
    `Policy gradients.ipynb` 文件中找到。你可以在这里查看智能体经过20,000集学习后进行的游戏：[https://youtu.be/hZo7kAco8is](https://youtu.be/hZo7kAco8is)。请注意，这个智能体学会了在自己的位置附近振荡；它还学会了将自己运动产生的力量传递给球，并且知道只有通过进攻性击球才能击败对手。
- en: The actor-critic algorithm
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论家算法
- en: 'In the policy gradient method, we introduced the baseline to reduce variance,
    but still, both action and baseline (look closely: the variance is the expected
    sum of rewards, or in other words, the goodness of the state or its value function)
    were changing simultaneously. Wouldn''t it be better to separate the policy evaluation
    from the value evaluation? That''s the idea behind the actor-critic method. It
    consists of two neural networks, one approximating the policy, called the **actor-network**,
    and the other approximating the value, called the **critic-network**. We alternate
    between a policy evaluation and a policy improvement step, resulting in more stable
    learning. The critic uses the state and action values to estimate a value function,
    which is then used to update the actor''s policy network parameters so that the
    overall performance improves. The following diagram shows the basic architecture
    of the actor-critic network:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度方法中，我们引入了基线来减少方差，但依然是动作和基线（仔细看：方差是预期奖励的总和，换句话说，它是状态的好坏或其价值函数）同时变化。是不是应该将策略评估和价值评估分开呢？这正是演员-评论家方法的思想。它由两个神经网络组成，一个用于近似策略，称为**演员网络**，另一个用于近似价值，称为**评论家网络**。我们在策略评估和策略改进步骤之间交替进行，从而实现更稳定的学习。评论家使用状态和动作值来估计价值函数，接着用它来更新演员的策略网络参数，使得整体性能得以提升。下图展示了演员-评论家网络的基本架构：
- en: '![](img/bdfe8a1a-de28-4c04-998a-e55bd0840078.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdfe8a1a-de28-4c04-998a-e55bd0840078.png)'
- en: Actor-critic architecture
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家架构
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about RL and how it's different from supervised
    and unsupervised learning. The emphasis of this chapter was on DRL, where deep
    neural networks are used to approximate the policy function or the value function
    or even both. This chapter introduced OpenAI gym, a library that provides a large
    number of environments to train RL agents. We learned about the value-based methods
    such as Q-learning and used it to train an agent to pick up and drop passengers
    off in a taxi. We also used a DQN to train an agent to play a Atari game . This
    chapter then moved on to policy-based methods, specifically policy gradients.
    We covered the intuition behind policy gradients and used the algorithm to train
    an RL agent to play Pong.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了强化学习（RL）以及它与监督学习和无监督学习的区别。本章的重点是深度强化学习（DRL），在该方法中，深度神经网络用于近似策略函数或价值函数，甚至两者兼而有之。本章介绍了OpenAI
    Gym，这是一个提供大量环境来训练RL代理的库。我们学习了基于价值的方法，如Q-learning，并利用它训练一个代理来接载和放下出租车中的乘客。我们还使用了DQN来训练一个代理玩Atari游戏。接着，本章介绍了基于策略的方法，特别是策略梯度。我们讨论了策略梯度背后的直觉，并使用该算法训练一个RL代理来玩Pong游戏。
- en: In the next chapter, we'll explore generative models and learn the secrets behind
    generative adversarial networks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索生成模型，并学习生成对抗网络背后的秘密。
