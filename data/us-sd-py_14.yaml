- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Generating Video Using Stable Diffusion
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç¨³å®šæ‰©æ•£ç”Ÿæˆè§†é¢‘
- en: Harnessing the power of the Stable Diffusion model, we can generate high-quality
    images using techniques such as LoRA, text embedding, and ControlNet. A natural
    progression from static images is toward dynamic content, that is, videos. Can
    we generate consistent videos using the Stable Diffusion model?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹çš„åŠ›é‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡LoRAã€æ–‡æœ¬åµŒå…¥å’Œæ§åˆ¶ç½‘ç­‰æŠ€æœ¯ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚ä»é™æ€å›¾åƒçš„è‡ªç„¶å‘å±•æ˜¯åŠ¨æ€å†…å®¹ï¼Œå³è§†é¢‘ã€‚æˆ‘ä»¬èƒ½å¦ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´çš„è§†é¢‘ï¼Ÿ
- en: The Stable Diffusion modelâ€™s UNet architecture, while effective for single-image
    processing, lacks contextual awareness when dealing with multiple images. Consequently,
    generating identical or consistently related images with the same prompt and parameters
    but different seeds is challenging. The resulting images may vary significantly
    in color, shape, or style due to the randomness introduced by the modelâ€™s nature.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£æ¨¡å‹çš„ UNet æ¶æ„è™½ç„¶å¯¹å•å›¾å¤„ç†æœ‰æ•ˆï¼Œä½†åœ¨å¤„ç†å¤šå›¾æ—¶ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚å› æ­¤ï¼Œä½¿ç”¨ç›¸åŒçš„æç¤ºå’Œå‚æ•°ä½†ä¸åŒçš„ç§å­ç”Ÿæˆç›¸åŒæˆ–æŒç»­ç›¸å…³çš„å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç”±äºæ¨¡å‹æœ¬èº«çš„éšæœºæ€§å¼•å…¥ï¼Œç”Ÿæˆçš„å›¾åƒåœ¨é¢œè‰²ã€å½¢çŠ¶æˆ–é£æ ¼ä¸Šå¯èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚
- en: One might consider an image-to-image pipeline or a ControlNet approach, where
    a video clip is segmented into individual images, and each image is processed
    sequentially. However, maintaining consistency across the entire sequence, especially
    when applying significant changes (such as transforming a realistic video into
    a cartoon), remains a challenge. Even with pose alignment, the output video may
    still exhibit noticeable flickering.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªäººå¯èƒ½ä¼šè€ƒè™‘å›¾åƒåˆ°å›¾åƒçš„ç®¡é“æˆ–ControlNetæ–¹æ³•ï¼Œå…¶ä¸­è§†é¢‘ç‰‡æ®µè¢«åˆ†å‰²æˆå•ä¸ªå›¾åƒï¼Œæ¯ä¸ªå›¾åƒä¾æ¬¡å¤„ç†ã€‚ç„¶è€Œï¼Œåœ¨æ•´ä¸ªåºåˆ—ä¸­ä¿æŒä¸€è‡´æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åº”ç”¨é‡å¤§å˜åŒ–ï¼ˆå¦‚å°†çœŸå®è§†é¢‘è½¬æ¢ä¸ºå¡é€šï¼‰æ—¶ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å³ä½¿æœ‰å§¿æ€å¯¹é½ï¼Œè¾“å‡ºè§†é¢‘ä»å¯èƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„é—ªçƒã€‚
- en: 'A breakthrough came with the publication of *AnimateDiff: Animating Your Personalized
    Text-to-Image Diffusion Models without Specific Tuning* [1] by Yuwei Gao and colleagues.
    This work paved the way for generating consistent images from text, thereby enabling
    the creation of short videos.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 'éšç€Yuwei GaoåŠå…¶åŒäº‹å‘è¡¨çš„ *AnimateDiff: Animating Your Personalized Text-to-Image
    Diffusion Models without Specific Tuning* [1] çš„å‡ºç‰ˆï¼Œå–å¾—äº†çªç ´ã€‚è¿™é¡¹å·¥ä½œä¸ºä»æ–‡æœ¬ç”Ÿæˆä¸€è‡´å›¾åƒé“ºå¹³äº†é“è·¯ï¼Œä»è€Œä½¿å¾—åˆ›å»ºçŸ­è§†é¢‘æˆä¸ºå¯èƒ½ã€‚'
- en: 'In this chapter, we will explore the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¢è®¨ä»¥ä¸‹å†…å®¹ï¼š
- en: The principles of text-to-video generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„åŸç†
- en: Practical applications of AnimateDiff
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AnimateDiff çš„å®é™…åº”ç”¨
- en: Utilizing Motion LoRA to control animation motion
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ Motion LoRA æ§åˆ¶åŠ¨ç”»è¿åŠ¨
- en: By the end of this chapter, you will understand the theoretical aspects of video
    generation, the inner workings of AnimateDiff, and why this methodology is effective
    in creating consistent and coherent images. With the provided sample code, you
    will be able to generate a 16-frame video. You can then apply Motion LoRA to manipulate
    the animationâ€™s motion.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†ç†è§£è§†é¢‘ç”Ÿæˆçš„ç†è®ºæ–¹é¢ã€AnimateDiff çš„å†…éƒ¨å·¥ä½œåŸç†ä»¥åŠä¸ºä»€ä¹ˆè¿™ç§æ–¹æ³•åœ¨åˆ›å»ºä¸€è‡´å’Œè¿è´¯çš„å›¾åƒæ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚é€šè¿‡æä¾›çš„ç¤ºä¾‹ä»£ç ï¼Œä½ å°†èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ª
    16 å¸§çš„è§†é¢‘ã€‚ç„¶åä½ å¯ä»¥åº”ç”¨ Motion LoRA æ¥æ“çºµåŠ¨ç”»çš„è¿åŠ¨ã€‚
- en: Please note that the results of this chapter cannot be fully appreciated in
    a static format like paper or PDF. For the best experience, we encourage you to
    engage with the associated sample code, run it, and observe the generated video.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæœ¬ç« çš„ç»“æœæ— æ³•åœ¨é™æ€æ ¼å¼å¦‚è®ºæ–‡æˆ–PDFä¸­å®Œå…¨æ¬£èµã€‚ä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œæˆ‘ä»¬é¼“åŠ±ä½ å‚ä¸ç›¸å…³çš„ç¤ºä¾‹ä»£ç ï¼Œè¿è¡Œå®ƒï¼Œå¹¶è§‚å¯Ÿç”Ÿæˆçš„è§†é¢‘ã€‚
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: In this chapter, we will employ `AnimateDiffPipeline`, available in the `Diffusers`
    library, to generate videos. You wonâ€™t need to install any extra tools or packages,
    as Diffusers (after version 0.23.0) offers all the required components and classes.
    Throughout the chapter, I will guide you through the usage of these features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `Diffusers` åº“ä¸­å¯ç”¨çš„ `AnimateDiffPipeline` ç”Ÿæˆè§†é¢‘ã€‚ä½ ä¸éœ€è¦å®‰è£…ä»»ä½•é¢å¤–çš„å·¥å…·æˆ–åŒ…ï¼Œå› ä¸º Diffusersï¼ˆä»ç‰ˆæœ¬
    0.23.0 å¼€å§‹ï¼‰æä¾›äº†æ‰€æœ‰å¿…éœ€çš„ç»„ä»¶å’Œç±»ã€‚åœ¨æ•´ä¸ªç« èŠ‚ä¸­ï¼Œæˆ‘å°†æŒ‡å¯¼ä½ ä½¿ç”¨è¿™äº›åŠŸèƒ½ã€‚
- en: 'To export the result in MP4 video format, you will also need to install the
    `opencv-python` package:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä»¥ MP4 è§†é¢‘æ ¼å¼å¯¼å‡ºç»“æœï¼Œä½ è¿˜éœ€è¦å®‰è£… `opencv-python` åŒ…ï¼š
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Also, note that the `AnimateDiffPipeline` will require at least 8 GB of VRAM
    to generate a 16-frame 256x256 video clip.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¯·æ³¨æ„ï¼Œ`AnimateDiffPipeline` ç”Ÿæˆä¸€ä¸ª 16 å¸§ 256x256 çš„è§†é¢‘ç‰‡æ®µè‡³å°‘éœ€è¦ 8 GB çš„ VRAMã€‚
- en: The principles of text-to-video generation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„åŸç†
- en: The Stable Diffusion UNet, while effective for generating single images, falls
    short when it comes to generating consistent images due to its lack of contextual
    awareness. Researchers have proposed solutions to overcome this limitation, such
    as incorporating temporal information from the preceding one or two frames. However,
    this approach still fails to ensure pixel-level consistency, leading to noticeable
    differences between consecutive images and flickering in the generated video.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion UNetè™½ç„¶æœ‰æ•ˆäºç”Ÿæˆå•ä¸ªå›¾åƒï¼Œä½†ç”±äºå…¶ç¼ºä¹ä¸Šä¸‹æ–‡æ„è¯†ï¼Œåœ¨ç”Ÿæˆä¸€è‡´å›¾åƒæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç ”ç©¶äººå‘˜å·²ç»æå‡ºäº†å…‹æœè¿™ä¸€å±€é™æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ä»å‰ä¸€æˆ–ä¸¤ä¸ªå¸§ä¸­ç»“åˆæ—¶é—´ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä»ç„¶æ— æ³•ç¡®ä¿åƒç´ çº§ä¸€è‡´æ€§ï¼Œå¯¼è‡´è¿ç»­å›¾åƒä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œå¹¶åœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­äº§ç”Ÿé—ªçƒã€‚
- en: 'To address this inconsistency problem, the authors of AnimateDiff trained a
    separated motion model â€“ a zero-initialized convolution side model â€“ similar to
    the ControlNet model. Further, rather than controlling an image, the motion model
    is applied to a series of continuous frames, as shown in *Figure 14**.1*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªä¸ä¸€è‡´æ€§é—®é¢˜ï¼ŒAnimateDiffçš„ä½œè€…è®­ç»ƒäº†ä¸€ä¸ªåˆ†ç¦»çš„è¿åŠ¨æ¨¡å‹â€”â€”ä¸€ä¸ªç±»ä¼¼äºControlNetæ¨¡å‹çš„é›¶åˆå§‹åŒ–å·ç§¯ä¾§æ¨¡å‹ã€‚è¿›ä¸€æ­¥åœ°ï¼Œè€Œä¸æ˜¯æ§åˆ¶ä¸€ä¸ªå›¾åƒï¼Œè¿åŠ¨æ¨¡å‹è¢«åº”ç”¨äºä¸€ç³»åˆ—è¿ç»­çš„å¸§ï¼Œå¦‚å›¾*å›¾14.1*æ‰€ç¤ºï¼š
- en: '![Figure 14.1: The architecture of AnimatedDiff](img/B21263_14_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾14.1ï¼šAnimatedDiffçš„æ¶æ„](img/B21263_14_01.jpg)'
- en: 'Figure 14.1: The architecture of AnimatedDiff'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾14.1ï¼šAnimatedDiffçš„æ¶æ„
- en: The process involves training a motion modeling module on video datasets to
    extract motion priors while keeping the base Stable Diffusion model frozen. Motion
    priors are the prior knowledge about motion in order to guide the generation or
    customization of videos. During the training stage, a **motion module** (also
    called **Motion UNet**) is added to the Stable Diffusion UNet. Similar to normal
    Stable Diffusion V1.5 UNet training, this Motion UNet will work on all frames
    simultaneously. We can treat them as images in one batch from the same video clip.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹æ¶‰åŠåœ¨è§†é¢‘æ•°æ®é›†ä¸Šè®­ç»ƒè¿åŠ¨å»ºæ¨¡æ¨¡å—ä»¥æå–è¿åŠ¨å…ˆéªŒï¼ŒåŒæ—¶ä¿æŒåŸºç¡€Stable Diffusionæ¨¡å‹å†»ç»“ã€‚è¿åŠ¨å…ˆéªŒæ˜¯å…³äºè¿åŠ¨çš„çŸ¥è¯†ï¼Œä»¥ä¾¿æŒ‡å¯¼è§†é¢‘çš„ç”Ÿæˆæˆ–å®šåˆ¶ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œä¸€ä¸ª**è¿åŠ¨æ¨¡å—**ï¼ˆä¹Ÿç§°ä¸º**è¿åŠ¨UNet**ï¼‰è¢«æ·»åŠ åˆ°Stable
    Diffusion UNetä¸­ã€‚ä¸æ­£å¸¸çš„Stable Diffusion V1.5 UNetè®­ç»ƒç±»ä¼¼ï¼Œè¿™ä¸ªè¿åŠ¨UNetå°†åŒæ—¶å¤„ç†æ‰€æœ‰å¸§ã€‚æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è§†ä¸ºæ¥è‡ªåŒä¸€è§†é¢‘ç‰‡æ®µçš„ä¸€æ‰¹å›¾åƒã€‚
- en: For instance, if we feed in a video with 16 frames, the motion module with attention
    headers will be trained to consider all 16 frames. If we look into the implementation
    source code, `TransformerTemporalModel` [4] is the core component of `MotionModules`
    [3].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªåŒ…å«16å¸§çš„è§†é¢‘ï¼Œå¸¦æœ‰æ³¨æ„åŠ›å¤´çš„è¿åŠ¨æ¨¡å—å°†è¢«è®­ç»ƒä»¥è€ƒè™‘æ‰€æœ‰16å¸§ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹å®ç°æºä»£ç ï¼Œ`TransformerTemporalModel`
    [4]æ˜¯`MotionModules` [3]çš„æ ¸å¿ƒç»„ä»¶ã€‚
- en: During the inference, the time when we want to generate videos, the motion module
    will be loaded and its weights will be merged into Stable Diffusion UNet. When
    we want to generate a video with 16 frames, the pipeline will first initialize
    16 random latents with Gaussian noise â€“ ğ’©(0,1). Without the motion module, the
    Stable Diffusion UNet will remove noise and generate 16 independent images. However,
    with the help of the motion module with the Transformer attention header built
    inside, the motion UNet attempts to create 16 correlated frames. You may ask,
    why are the images correlated? That is because the frames in the training video
    are correlated. After the denoising stage, the decoder D from the VAE will convert
    the 16 latents into pixel images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå½“æˆ‘ä»¬æƒ³è¦ç”Ÿæˆè§†é¢‘æ—¶ï¼Œå°†åŠ è½½è¿åŠ¨æ¨¡å—å¹¶å°†å…¶æƒé‡åˆå¹¶åˆ°Stable Diffusion UNetä¸­ã€‚å½“æˆ‘ä»¬æƒ³è¦ç”Ÿæˆä¸€ä¸ªåŒ…å«16å¸§çš„è§†é¢‘æ—¶ï¼Œç®¡é“å°†é¦–å…ˆä½¿ç”¨é«˜æ–¯å™ªå£°â€”â€”ğ’©(0,1)åˆå§‹åŒ–16ä¸ªéšæœºæ½œåœ¨å€¼ã€‚å¦‚æœæ²¡æœ‰è¿åŠ¨æ¨¡å—ï¼ŒStable
    Diffusion UNetå°†å»é™¤å™ªå£°å¹¶ç”Ÿæˆ16ä¸ªç‹¬ç«‹çš„å›¾åƒã€‚ç„¶è€Œï¼Œå€ŸåŠ©å†…ç½®çš„Transformeræ³¨æ„åŠ›å¤´çš„è¿åŠ¨æ¨¡å—çš„å¸®åŠ©ï¼Œè¿åŠ¨UNetè¯•å›¾åˆ›å»º16ä¸ªç›¸å…³çš„å¸§ã€‚ä½ å¯èƒ½æƒ³çŸ¥é“ï¼Œä¸ºä»€ä¹ˆè¿™äº›å›¾åƒæ˜¯ç›¸å…³çš„ï¼Ÿé‚£æ˜¯å› ä¸ºè®­ç»ƒè§†é¢‘ä¸­çš„å¸§æ˜¯ç›¸å…³çš„ã€‚åœ¨å»å™ªé˜¶æ®µä¹‹åï¼ŒVAEä¸­çš„è§£ç å™¨Då°†æŠŠ16ä¸ªæ½œåœ¨å€¼è½¬æ¢ä¸ºåƒç´ å›¾åƒã€‚
- en: The Motion UNet is responsible for introducing correlations between successive
    frames in the generated video. It is similar to the correlation of different areas
    in one image. This is because the attention header pays attention to different
    parts of the image, and the model learned this knowledge during the training stage.
    Likewise, during the video generation, the model learned the correlation between
    frames during the training stage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿åŠ¨UNetè´Ÿè´£åœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­å¼•å…¥è¿ç»­å¸§ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å®ƒç±»ä¼¼äºä¸€ä¸ªå›¾åƒä¸­ä¸åŒåŒºåŸŸçš„ç›¸å…³æ€§ã€‚è¿™æ˜¯å› ä¸ºæ³¨æ„åŠ›å¤´å…³æ³¨å›¾åƒçš„ä¸åŒéƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ äº†è¿™äº›çŸ¥è¯†ã€‚åŒæ ·ï¼Œåœ¨è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ äº†å¸§ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
- en: At its core, this approach involves designing an attention mechanism that operates
    on a sequence of continuous images. By learning the relationships between frames,
    AnimateDiff can generate more consistent and coherent images from text. Furthermore,
    since the base Stable Diffusion model remains locked, various Stable Diffusion
    extension techniques, such as LoRA, textual embedding, ControlNet, and image-to-image
    generation, can be applied to AnimateDiff as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ ¸å¿ƒä¸Šï¼Œè¿™ç§æ–¹æ³•æ¶‰åŠè®¾è®¡ä¸€ä¸ªåœ¨è¿ç»­å›¾åƒåºåˆ—ä¸Šæ“ä½œçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡å­¦ä¹ å¸§ä¹‹é—´çš„å…³ç³»ï¼ŒAnimateDiffå¯ä»¥ä»æ–‡æœ¬ç”Ÿæˆæ›´ä¸€è‡´å’Œè¿è´¯çš„å›¾åƒã€‚æ­¤å¤–ï¼Œç”±äºåŸºç¡€Stable
    Diffusionæ¨¡å‹ä¿æŒé”å®šçŠ¶æ€ï¼Œå„ç§Stable Diffusionæ‰©å±•æŠ€æœ¯ï¼Œå¦‚LoRAã€æ–‡æœ¬åµŒå…¥ã€ControlNetå’Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆï¼Œä¹Ÿå¯ä»¥åº”ç”¨äºAnimateDiffã€‚
- en: Anything that works for standard Stable Diffusion, in theory, should also work
    for AnimateDiff to AnimateDiff as well!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è®ºä¸Šï¼Œä»»ä½•é€‚ç”¨äºæ ‡å‡†Stable Diffusionçš„ä¸œè¥¿ï¼Œä¹Ÿåº”è¯¥é€‚ç”¨äºAnimateDiffåˆ°AnimateDiffã€‚
- en: Before moving on to the next section, be aware that the AnimateDiff model requires
    a minimum of 12 GB of VRAM to generate a 16-frame, 256x256 video clip. To truly
    grasp the concept, writing code to utilize AnimateDiff is highly recommended.
    Now, letâ€™s proceed to generate a short video (in GIF and MP4 format) using AnimateDiff.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›å…¥ä¸‹ä¸€èŠ‚ä¹‹å‰ï¼Œè¯·æ³¨æ„ï¼ŒAnimateDiffæ¨¡å‹ç”Ÿæˆä¸€ä¸ª16å¸§ã€256x256çš„è§†é¢‘ç‰‡æ®µè‡³å°‘éœ€è¦12 GBçš„VRAMã€‚ä¸ºäº†çœŸæ­£ç†è§£è¿™ä¸ªæ¦‚å¿µï¼Œç¼–å†™ä»£ç æ¥åˆ©ç”¨AnimateDiffæ˜¯éå¸¸æ¨èçš„ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨AnimateDiffç”Ÿæˆä¸€ä¸ªçŸ­è§†é¢‘ï¼ˆGIFå’ŒMP4æ ¼å¼ï¼‰ã€‚
- en: Practical applications of AnimateDiff
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AnimateDiffçš„å®é™…åº”ç”¨
- en: The original AnimateDiff code and model were released as a standalone GitHub
    repository [2]. While the author provided sample code and Google Colab to demonstrate
    the results, users still needed to manually pull the code and download the model
    file to use it, being cautious about package versions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„AnimateDiffä»£ç å’Œæ¨¡å‹ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„GitHubä»“åº“[2]å‘å¸ƒã€‚è™½ç„¶ä½œè€…æä¾›äº†ç¤ºä¾‹ä»£ç å’ŒGoogle Colabæ¥å±•ç¤ºç»“æœï¼Œä½†ç”¨æˆ·ä»ç„¶éœ€è¦æ‰‹åŠ¨æ‹‰å–ä»£ç å¹¶ä¸‹è½½æ¨¡å‹æ–‡ä»¶æ¥ä½¿ç”¨ï¼ŒåŒæ—¶è¦å°å¿ƒåŒ…çš„ç‰ˆæœ¬ã€‚
- en: 'In November 2023, Dhruv Nair [9] merged an AnimateDiff Pipeline for Diffusers,
    allowing users to generate video clips using the AnimateDiff pertained model without
    leaving the `Diffusers` package. Hereâ€™s how to use the AnimatedDiff pipeline from
    Diffusers:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°2023å¹´11æœˆï¼ŒDhruv Nair [9] å°†AnimateDiff Pipelineåˆå¹¶åˆ°Diffusersä¸­ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ä¸ç¦»å¼€`Diffusers`åŒ…çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨AnimateDiffæŒä¹…åŒ–æ¨¡å‹ç”Ÿæˆè§†é¢‘ç‰‡æ®µã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨Diffusersä¸­çš„AnimatedDiffç®¡é“çš„æ–¹æ³•ï¼š
- en: 'Install this specific version of Diffusers with the integrated AnimateDiff
    code:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é›†æˆçš„AnimateDiffä»£ç å®‰è£…è¿™ä¸ªç‰¹å®šçš„Diffusersç‰ˆæœ¬ï¼š
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At the time of writing this chapter, the version of Diffusers with the latest
    AnimateDiff code is 0.23.0\. By specifying this version number, you can ensure
    that the sample code runs smoothly and error-free, as it was tested against this
    particular version.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ’°å†™æœ¬ç« æ—¶ï¼ŒåŒ…å«æœ€æ–°AnimateDiffä»£ç çš„Diffusersç‰ˆæœ¬æ˜¯0.23.0ã€‚é€šè¿‡æŒ‡å®šè¿™ä¸ªç‰ˆæœ¬å·ï¼Œæ‚¨å¯ä»¥ç¡®ä¿ç¤ºä¾‹ä»£ç èƒ½å¤Ÿé¡ºåˆ©ä¸”æ— é”™è¯¯åœ°è¿è¡Œï¼Œå› ä¸ºå®ƒå·²ç»é’ˆå¯¹è¿™ä¸ªç‰¹å®šç‰ˆæœ¬è¿›è¡Œäº†æµ‹è¯•ã€‚
- en: 'You can also try installing the latest version of Diffusers, as it may have
    added more features to the pipeline by the time you read this:'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°è¯•å®‰è£…Diffusersçš„æœ€æ–°ç‰ˆæœ¬ï¼Œå› ä¸ºåˆ°æ‚¨é˜…è¯»è¿™ç¯‡æ–‡æ¡£çš„æ—¶å€™ï¼Œå®ƒå¯èƒ½å·²ç»å¢åŠ äº†æ›´å¤šåŠŸèƒ½ï¼š
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load up the motion adapter. We will use the pre-trained motion adapter model
    from the author of the original paper:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½è¿åŠ¨é€‚é…å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸå§‹è®ºæ–‡ä½œè€…çš„é¢„è®­ç»ƒè¿åŠ¨é€‚é…å™¨æ¨¡å‹ï¼š
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Load up an AnimateDiff pipeline from a Stable Diffusion v1.5-based checkpoint
    model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»åŸºäºStable Diffusion v1.5çš„æ£€æŸ¥ç‚¹æ¨¡å‹åŠ è½½ä¸€ä¸ªAnimateDiffç®¡é“ï¼š
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use a proper scheduler. The scheduler plays an important role in the process
    of generating coherent images. An comparative study conducted by the author of
    the paper shows different schedulers can lead to different results. Experimentation
    shows that the `EulerAncestralDiscreteScheduler` scheduler with the following
    setting can generate relatively good results:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆé€‚çš„è°ƒåº¦å™¨ã€‚è°ƒåº¦å™¨åœ¨ç”Ÿæˆè¿è´¯å›¾åƒçš„è¿‡ç¨‹ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚ä½œè€…è¿›è¡Œçš„ä¸€é¡¹æ¯”è¾ƒç ”ç©¶è¡¨æ˜ï¼Œä¸åŒçš„è°ƒåº¦å™¨å¯ä»¥å¯¼è‡´ä¸åŒçš„ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œå…·æœ‰ä»¥ä¸‹è®¾ç½®çš„`EulerAncestralDiscreteScheduler`è°ƒåº¦å™¨å¯ä»¥ç”Ÿæˆç›¸å¯¹è¾ƒå¥½çš„ç»“æœï¼š
- en: '[PRE13]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To optimize VRAM usage, you can employ two strategies. First, use `pipe.enable_vae_slicing()`
    to configure the VAE to decode one frame at a time, thereby reducing memory consumption.
    Additionally, utilize `pipe.enable_model_cpu_offload()` to offload idle sub-models
    to the CPU, further decreasing VRAM usage.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¼˜åŒ–VRAMä½¿ç”¨ï¼Œæ‚¨å¯ä»¥é‡‡ç”¨ä¸¤ç§ç­–ç•¥ã€‚é¦–å…ˆï¼Œä½¿ç”¨`pipe.enable_vae_slicing()`é…ç½®VAEä¸€æ¬¡è§£ç ä¸€å¸§ï¼Œä»è€Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨`pipe.enable_model_cpu_offload()`å°†ç©ºé—²å­æ¨¡å‹å¸è½½åˆ°CPUï¼Œè¿›ä¸€æ­¥é™ä½VRAMä½¿ç”¨ã€‚
- en: 'Generate coherent images:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè¿è´¯çš„å›¾åƒï¼š
- en: '[PRE24]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, you should be able to see a GIF file generated using the 16 frames produced
    by AnimateDiff. This GIF uses 16 256x256 images. You can apply the image super-resolution
    techniques introduced in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214) to upscale
    the image and create a 512x512 GIF. I will not duplicate the code in this chapter.
    It is highly recommended to leverage the skills learned in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214)
    to further enhance the quality of video generation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°ä½¿ç”¨AnimateDiffç”Ÿæˆçš„16å¸§äº§ç”Ÿçš„GIFæ–‡ä»¶ã€‚è¿™ä¸ªGIFä½¿ç”¨äº†16å¼ 256x256çš„å›¾åƒã€‚ä½ å¯ä»¥åº”ç”¨åœ¨[*ç¬¬11ç« *](B21263_11.xhtml#_idTextAnchor214)ä¸­ä»‹ç»çš„å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯æ¥æå‡å›¾åƒå¹¶åˆ›å»ºä¸€ä¸ª512x512çš„GIFã€‚æˆ‘ä¸ä¼šåœ¨æœ¬ç« ä¸­é‡å¤ä»£ç ã€‚å¼ºçƒˆå»ºè®®åˆ©ç”¨åœ¨[*ç¬¬11ç« *](B21263_11.xhtml#_idTextAnchor214)ä¸­å­¦åˆ°çš„æŠ€èƒ½æ¥è¿›ä¸€æ­¥æé«˜è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚
- en: Utilizing Motion LoRA to control animation motion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ©ç”¨è¿åŠ¨LoRAæ§åˆ¶åŠ¨ç”»è¿åŠ¨
- en: Besides the motion adapter model, the author of the paper also introduced Motion
    LoRA to control the motion style. Motion LoRA is the same LoRA adapter we introduced
    in [*Chapter 8*](B21263_08.xhtml#_idTextAnchor153). As mentioned before, the AnimateDiff
    pipeline supports all other community-shared LoRAs. You can find these Motion
    LoRAs on the authorâ€™s Hugging Face repository [8].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†è¿åŠ¨é€‚é…å™¨æ¨¡å‹ä¹‹å¤–ï¼Œè®ºæ–‡çš„ä½œè€…è¿˜ä»‹ç»äº†è¿åŠ¨LoRAæ¥æ§åˆ¶è¿åŠ¨é£æ ¼ã€‚è¿åŠ¨LoRAä¸æˆ‘ä»¬[*ç¬¬8ç« *](B21263_08.xhtml#_idTextAnchor153)ä¸­ä»‹ç»çš„ç›¸åŒçš„LoRAé€‚é…å™¨ã€‚å¦‚å‰æ‰€è¿°ï¼ŒAnimateDiffç®¡é“æ”¯æŒæ‰€æœ‰å…¶ä»–ç¤¾åŒºå…±äº«çš„LoRAã€‚ä½ å¯ä»¥åœ¨ä½œè€…çš„Hugging
    Faceä»“åº“ [8] ä¸­æ‰¾åˆ°è¿™äº›è¿åŠ¨LoRAã€‚
- en: These Motion LoRAs can be used to control the camera view. Here, we will use
    zoom-in LoRA â€“ `guoyww/animatediff-motion-lora-zoom-in` â€“ as an example. The zoom-in
    will guide the model to generate a video with zoom-in motion.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¿åŠ¨LoRAå¯ä»¥ç”¨æ¥æ§åˆ¶æ‘„åƒæœºè§†è§’ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ”¾å¤§LoRA â€“ `guoyww/animatediff-motion-lora-zoom-in`
    â€“ ä½œä¸ºç¤ºä¾‹ã€‚æ”¾å¤§å°†å¼•å¯¼æ¨¡å‹ç”Ÿæˆå¸¦æœ‰æ”¾å¤§è¿åŠ¨çš„è§†é¢‘ã€‚
- en: 'The usage is simply one additional line of code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ–¹æ³•ç®€å•ï¼Œåªéœ€é¢å¤–ä¸€è¡Œä»£ç ï¼š
- en: '[PRE43]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is the complete generation code. We are mostly reusing the code from the
    previous section:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å®Œæ•´çš„ç”Ÿæˆä»£ç ã€‚æˆ‘ä»¬ä¸»è¦é‡ç”¨äº†ä¸Šä¸€èŠ‚ä¸­çš„ä»£ç ï¼š
- en: '[PRE44]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You should see a zoom-in GIF clip is generated under the same folder, named
    `animation_origin_256_w_lora_zoom_in.gif` and an MP4 video clip is generated named
    `animation_origin_256_w_lora_zoom_in.mp4`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸‹çœ‹åˆ°ä¸€ä¸ªåä¸º`animation_origin_256_w_lora_zoom_in.gif`çš„æ”¾å¤§GIFå‰ªè¾‘å’Œä¸€ä¸ªåä¸º`animation_origin_256_w_lora_zoom_in.mp4`çš„MP4è§†é¢‘å‰ªè¾‘è¢«ç”Ÿæˆã€‚
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Every day, the quality and duration of text-to-video samples circulating on
    social networks are improving. Itâ€™s likely that by the time you read this chapter,
    the function of the technologies metioned in this chapter will have surpassed
    what was described here. However, one constant is the concept of training a model
    to apply an attention mechanism to a sequence of images.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å¤©åœ¨ç¤¾äº¤ç½‘ç»œä¸Šæµä¼ çš„æ–‡æœ¬åˆ°è§†é¢‘æ ·æœ¬çš„è´¨é‡å’ŒæŒç»­æ—¶é—´éƒ½åœ¨ä¸æ–­æé«˜ã€‚å¾ˆå¯èƒ½åœ¨ä½ é˜…è¯»è¿™ä¸€ç« çš„æ—¶å€™ï¼Œæœ¬ç« ä¸­æåˆ°çš„æŠ€æœ¯çš„åŠŸèƒ½å·²ç»è¶…è¿‡äº†è¿™é‡Œæ‰€æè¿°çš„å†…å®¹ã€‚ç„¶è€Œï¼Œä¸€ä¸ªä¸å˜çš„æ˜¯è®­ç»ƒæ¨¡å‹å°†æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨äºä¸€ç³»åˆ—å›¾åƒçš„æ¦‚å¿µã€‚
- en: At the time of writing, OpenAIâ€™s Sora [9] has just been released. This technology
    can generate high-quality videos based on the Transformer Diffusion architecture.
    This is a similar methodology to that used in AnimatedDiff, which combines the
    Transformer and diffusion models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼ŒOpenAIçš„Sora [9]åˆšåˆšå‘å¸ƒã€‚è¿™é¡¹æŠ€æœ¯å¯ä»¥æ ¹æ®Transformer Diffusionæ¶æ„ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚è¿™ä¸AnimatedDiffä¸­ä½¿ç”¨çš„æ–¹æ³•ç±»ä¼¼ï¼Œå®ƒç»“åˆäº†Transformerå’Œæ‰©æ•£æ¨¡å‹ã€‚
- en: What sets AnimatedDiff apart is its openness and adaptability. It can be applied
    to any community model with the same base checkpoint version, a feature not currently
    offered by any other solution. Furthermore, the authors of the paper have completely
    open-sourced the code and model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: AnimatedDiffä¸ä¼—ä¸åŒçš„åœ°æ–¹åœ¨äºå…¶å¼€æ”¾æ€§å’Œé€‚åº”æ€§ã€‚å®ƒå¯ä»¥åº”ç”¨äºä»»ä½•å…·æœ‰ç›¸åŒåŸºç¡€æ£€æŸ¥ç‚¹ç‰ˆæœ¬çš„ç¤¾åŒºæ¨¡å‹ï¼Œè¿™æ˜¯ç›®å‰ä»»ä½•å…¶ä»–è§£å†³æ–¹æ¡ˆéƒ½æ²¡æœ‰æä¾›çš„åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡çš„ä½œè€…å·²ç»å®Œå…¨å¼€æºäº†ä»£ç å’Œæ¨¡å‹ã€‚
- en: This chapter primarily discussed the challenges of text-to-image generation,
    then introduced AnimatedDiff, explaining how and why it works. We also provided
    a sample code to use the AnimatedDiff pipeline from the Diffusers package to generate
    a GIF clip from 16 coherent images on your own GPU.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸»è¦è®¨è®ºäº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œç„¶åä»‹ç»äº†AnimatedDiffï¼Œè§£é‡Šäº†å®ƒæ˜¯å¦‚ä½•ä»¥åŠä¸ºä»€ä¹ˆèƒ½å·¥ä½œçš„ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨DiffusersåŒ…ä¸­çš„AnimatedDiffç®¡é“åœ¨è‡ªå·±çš„GPUä¸Šä»16å¼ è¿è´¯çš„å›¾åƒç”ŸæˆGIFå‰ªè¾‘ã€‚
- en: In the next chapter, we will explore the solutions for generating text descriptions
    from an image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä»å›¾åƒç”Ÿæˆæ–‡æœ¬æè¿°çš„è§£å†³æ–¹æ¡ˆã€‚
- en: References
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai,
    *AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without
    Specific* *Tuning*: [https://arxiv.org/abs/2307.04725](https://arxiv.org/abs/2307.04725)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éƒ­å®‡ä¼Ÿï¼Œæ¨ç­–æºï¼Œé¥¶å®‰æ¯…ï¼Œç‹è€€è¾‰ï¼Œä¹”å®‡ï¼Œæ—å¤§åï¼Œæˆ´åšï¼Œ*AnimateDiffï¼šæ— éœ€ç‰¹å®š* *è°ƒæ•´* *çš„ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹*ï¼š[https://arxiv.org/abs/2307.04725](https://arxiv.org/abs/2307.04725)
- en: 'Original AnimateDiff code repository: [https://github.com/guoyww/AnimateDiff](https://github.com/guoyww/AnimateDiff)'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŸå§‹ AnimateDiff ä»£ç ä»“åº“ï¼š[https://github.com/guoyww/AnimateDiff](https://github.com/guoyww/AnimateDiff)
- en: 'Diffusers Motion modules implementation: [https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diffusers åŠ¨ä½œæ¨¡å—å®ç°ï¼š[https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50)
- en: )
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Hugging Face Diffusers TransformerTemporalModel implementation: [https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face Diffusers TransformerTemporalModel å®ç°ï¼š[https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41)
- en: '[4] Dhruv Nair, [https://github.com/DN6](https://github.com/DN6)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4] Dhruv Nairï¼Œ[https://github.com/DN6](https://github.com/DN6)'
- en: 'AnimateDiff proposal pull request: [https://github.com/huggingface/diffusers/pull/5413](https://github.com/huggingface/diffusers/pull/5413)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AnimateDiff ææ¡ˆæ‹‰å–è¯·æ±‚ï¼š[https://github.com/huggingface/diffusers/pull/5413](https://github.com/huggingface/diffusers/pull/5413)
- en: 'animatediff-motion-adapter-v1-5-2: [https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2](https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2)'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: animatediff-motion-adapter-v1-5-2ï¼š[https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2](https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2)
- en: 'Yuwei Guoâ€™s Hugging Face repository: [https://huggingface.co/guoyww](https://huggingface.co/guoyww)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éƒ­å®‡ä¼Ÿçš„ Hugging Face ä»“åº“ï¼š[https://huggingface.co/guoyww](https://huggingface.co/guoyww)
- en: 'Video generation models as world simulators: [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼š[https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)
