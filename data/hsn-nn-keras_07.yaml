- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: In the previous chapter, we marveled over the visual cortex and leveraged some
    insights from the way it processes visual signals to inform the architecture of
    **Convolutional Neural Networks** (**CNNs**), which form the base of many state-of-the-art
    computer vision systems. However, we do not understand the world around us with
    vision alone. Sound, for one, also plays a very important role. More specifically,
    we humans love to communicate and express intricate thoughts and ideas through
    sequences of symbolic reductions and abstract representations. Our built-in hardware
    allows us to interpret vocalizations or demarcations thereof, composing the base
    of human thought and collective understandings, upon which more complex representations
    (such as human languages, for instance) may be composed. In essence, these sequences
    of symbols are reduced representations of the world around us, through our own
    lenses, which we use to navigate our surroundings and effectively express ourselves.
    It stands to reason that we would want machines to understand this manner of processing
    sequential information, as it could help us to resolve many problems we face with
    such sequential tasks in the real world. But what kind of problems?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们惊叹于视觉皮层的功能，并借鉴了它处理视觉信号的方式来构建**卷积神经网络**（**CNNs**）的架构，后者构成了许多先进计算机视觉系统的基础。然而，我们并非仅通过视觉来理解周围的世界。声音，尤其是，也在其中扮演着非常重要的角色。更具体地说，我们人类喜欢通过符号化的简化序列和抽象的表现来沟通和表达复杂的思想与观念。我们内置的硬件使我们能够解读语音或其标记，构成了人类思维和集体理解的基础，而更复杂的表现形式（例如人类语言）可以在其之上构建。从本质上讲，这些符号序列是我们通过自己视角对周围世界的简化表示，我们用它们来导航环境并有效地表达自己。显而易见，我们希望机器能理解这种处理顺序信息的方式，因为它可以帮助我们解决现实世界中许多涉及顺序任务的问题。那么，具体来说，是什么问题呢？
- en: 'Following are the topics that will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章将涵盖的主题：
- en: Modeling sequences
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模序列
- en: Summarizing different types of sequence processing tasks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结不同类型的序列处理任务
- en: Predicting an output per time step
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时间步预测输出
- en: Backpropagation through time
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播通过时间
- en: Exploding and vanishing gradients
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度爆炸与消失
- en: GRUs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU
- en: Building character-level language models in keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中构建字符级语言模型
- en: Statistics of character modeling
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符建模的统计
- en: The purpose of controlling stochastically
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机控制的目的
- en: Testing different RNN models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试不同的RNN模型
- en: Building a SimpleRNN
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建简单的RNN
- en: Building GRUs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建GRU
- en: On processing reality sequentially
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序处理现实
- en: Bi-directional layer in Keras
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras中的双向层
- en: Visualizing output values
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化输出值
- en: Modeling sequences
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模序列
- en: Perhaps you want to get the right translation for your order in a restaurant
    while visiting a foreign country. Maybe you want your car to perform a sequence
    of movements automatically so that it is able to park by itself. Or maybe you
    want to understand how different sequences of adenine, guanine, thymine, and cytosine
    molecules in the human genome lead to differences in biological processes occurring
    in the human body. What's the commonality between these examples? Well, these
    are all sequence modeling tasks. In such tasks, the training examples (being vectors
    of words, a set of car movements generated by on-board controls, or configuration
    of *A*, *G*, *T*, and *C* molecules) are essentially multiple time-dependent data
    points of a possibly varied length.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你希望在访问外国时，能够正确地翻译你在餐馆中的点餐。也许你希望你的汽车能够自动执行一系列动作，从而能自己停车。或者你可能想要理解人类基因组中腺嘌呤、鸟嘌呤、胸腺嘧啶和胞嘧啶分子在不同序列中的变化是如何导致人体内生物过程的差异的。这些例子之间有什么共同点呢？嗯，这些都是序列建模任务。在这些任务中，训练示例（无论是单词的向量、由车载控制生成的一系列车动作，还是*A*、*G*、*T*和*C*分子的配置）本质上都是一组具有时间依赖性的数据点，长度可能各不相同。
- en: 'Sentences, for example, are composed of words, and the spatial configuration
    of these words allude not only to what has been said, but also to what is yet
    to come. Try and fill in the following blank:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，句子是由单词组成的，这些单词的空间配置不仅暗示了已说出的内容，还暗示了未说出的内容。试着填入以下空白：
- en: '*Don''t judge a book by its ___.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*不要以貌取书。*'
- en: How did you know that the next word would be *cover*? You simply look at the
    words and their relative positions and performed some sort of Bayesian inference,
    leveraging the sentences you have previously seen and their apparent similarity
    to the example at hand. In essence, you used your internal model of the English
    language to predict the most probable word to follow. Here, *language model* simply
    refers to the probability of a particular configuration of words occurring together
    in a given sequence. Such models are the fundamental components of modern speech
    recognition and machine translation systems, and simply rely on modeling the likelihood
    of sequences of words.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你是怎么知道下一个词是*cover*的？你只是看了看单词及其相对位置，并进行了一些贝叶斯推断，利用你之前看到的句子以及它们与当前示例的明显相似性。本质上，你使用了你对英语语言的内部模型来预测最可能的下一个单词。这里的*语言模型*仅指在给定序列中，特定单词组合一起出现的概率。这些模型是现代语音识别和机器翻译系统的基础组件，依赖于建模单词序列的可能性。
- en: Using RNNs for sequential modeling
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN进行序列建模
- en: The field of natural language understanding is a common area where **recurrent
    neural networks** (**RNNs**) tend to excel. You may imagine tasks such as recognizing
    named entities and classifying the predominant sentiment in a given piece of text.
    However, as we mentioned, RNNs are applicable to a broad spectrum of tasks that
    involve modeling time-dependent sequences of data. Generating music is also a
    sequence modeling task as we tend to distinguish music from a cacophony by modeling
    the sequence of notes that are played in a given tempo.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言理解领域是**循环神经网络**（**RNNs**）通常表现优异的一个领域。你可以想象一些任务，比如识别命名实体或分类给定文本中的主要情感。然而，正如我们提到的，RNNs适用于广泛的任务，这些任务涉及建模时间依赖的序列数据。生成音乐也是一个序列建模任务，因为我们通过建模在给定节奏下演奏的音符序列来区分音乐和杂音。
- en: RNN architectures are even applicable for some visual intelligence tasks, such
    as video activity recognition. Recognizing whether a person is cooking, running,
    or robbing a bank in a given video is essentially modeling sequences of human
    movements and matching them to specific classes. In fact, RNNs have been deployed
    for some very interesting use cases, including generating text in Shakespearean
    style, creating realistic (but incorrect) algebraic papers, and even producing
    source code for the Linux operating system with proper formatting.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RNN架构甚至适用于一些视觉智能任务，比如视频活动识别。识别一个人在给定视频中是做饭、跑步还是抢银行，本质上是在建模人类运动的序列，并将其与特定类别进行匹配。事实上，RNNs已经被应用于一些非常有趣的用例，包括生成莎士比亚风格的文本、创建现实（但错误的）代数论文，甚至为Linux操作系统生成格式正确的源代码。
- en: 'So, what makes these networks so versatile at performing these seemingly diverse
    tasks? Well, before we answer this, let''s refresh our memory on some of the difficulties
    we faced using neural nets so far:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么让这些网络在执行这些看似不同的任务时如此多才多艺呢？在回答这个问题之前，让我们回顾一下迄今为止使用神经网络时遇到的一些困难：
- en: '![](img/a3ef793d-f2f1-4923-a5b9-0e1680930241.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3ef793d-f2f1-4923-a5b9-0e1680930241.png)'
- en: Fake algebraic geometry, generated by RNN, courtesy of Andrej Karpathy
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由RNN生成的假代数几何图形，感谢Andrej Karpathy
- en: 'Which means:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：
- en: '![](img/ee7a9951-c981-4b4f-b1d8-9b3e02bf45eb.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee7a9951-c981-4b4f-b1d8-9b3e02bf45eb.png)'
- en: What's the catch?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有什么关键点吗？
- en: A problem with all of the networks we have built so far is that they only accepted
    inputs and outputs of fixed sizes for given training examples. We have always
    had to specify our input shape, defining the dimensions of the tensor entering
    our network, which in turn returns a fixed size output in terms of a class probability
    score, for example. Moreover, the hidden layers in our networks each had their
    own weights and activations, which behaved somewhat independently of each other,
    without identifying relationships between successive input values. This holds
    true for both the feedforward and the CNNs that we have familiarized ourselves
    with in previous chapters. For each network we built, we used non-sequential training
    vectors, which would propagate through a constant number of layers and produce
    a single output.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们构建的所有网络存在一个问题，即它们只接受固定大小的输入和输出，用于给定的训练样本。我们一直需要指定输入的形状，定义进入网络的张量的维度，而网络则返回固定大小的输出，例如一个类别概率分数。此外，我们网络中的每一层都有自己的权重和激活函数，它们在某种程度上是独立的，未能识别连续输入值之间的关系。这对于我们在前几章中熟悉的前馈网络和卷积神经网络（CNN）都适用。对于我们构建的每个网络，我们使用了非序列化的训练向量，这些向量会通过固定数量的层进行传播并产生单一的输出。
- en: 'While we did see some multi-output models to visualize the intermediate layers
    of CNNs, we never really modified our architecture to operate over a sequence
    of vectors. This basically prohibited us from sharing any time-dependent information
    that may affect the likelihood of our predictions. Discarding time-dependent information
    has got us by so far for the tasks we dealt with. In the case of image classification,
    the fact that your neural network saw the image of a cat at the last iteration
    does not really help it classify the current image it is viewing because the class
    probabilities of these two instances are not temporally related. However, this
    approach already caused us some trouble for the use case of sentiment analysis.
    Recall in [Chapter 3](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml), *Signal Processing
    - Data Analysis with Neural Networks*, that we classified movie reviews by treating
    each review as a bag of undirected words (that is, not in their sequential order).
    This approach entailed transforming each review into a fixed-length vector that''s
    defined by the size of our vocabulary (that is, the number of unique words in
    the corpus, which we had chosen to be 12,000 words). While useful, this is certainly
    not the most efficient or scalable form of representing information, as a sentence
    of any given length must be represented by a 12,000-dimensional vector. The simple
    feedforward network we trained (attaining an accuracy just above 88 %) incorrectly
    classified the sentiment of one of the reviews, which has been reproduced here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们确实看到了一些多输出模型用来可视化卷积神经网络（CNN）的中间层，但我们从未真正修改我们的架构以适应处理一系列向量。这基本上使得我们无法共享任何可能影响预测可能性的时间依赖信息。舍弃时间依赖信息至今对我们所处理的任务没有造成太大问题。在图像分类的情况下，神经网络在最后一次迭代中看到了一只猫的图像，这对其分类当前图像并没有什么帮助，因为这两个实例的类别概率在时间上并没有关联。然而，这种方法已经在情感分析的用例中给我们带来了一些麻烦。回顾[第3章](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml)，《信号处理
    - 使用神经网络的数据分析》，我们通过将每条影评视为一个无向词袋（即，词语不按顺序排列）来进行分类。这种方法涉及将每条影评转化为一个固定长度的向量，长度由我们词汇表的大小定义（即语料库中的唯一词汇数，我们选择的是12,000个词）。虽然这种方法有用，但它显然不是最有效或最具可扩展性的表示信息的方式，因为任何给定长度的句子必须通过一个12,000维的向量来表示。我们训练的简单前馈网络（精度略高于88%）错误地分类了其中一条影评的情感，以下是这条影评的重现：
- en: '![](img/0d2f4a36-47d1-46d5-a154-564f4aff2342.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d2f4a36-47d1-46d5-a154-564f4aff2342.png)'
- en: Our network seemed to have gotten confused due to the (unnecessarily) complex
    sentence with several long-term dependencies and contextual valence shifters.
    In retrospect, we noted unclear double negatives referring to various entities
    such as the director, actor, and the movie itself; yet we were able to make out
    that the overall sentiment of the review was clearly positive. Why? Simply because
    we are able to track concepts that are relevant to the general sentiment of the
    review, as we read it word for word. In our minds, we are able to assess how each
    new word of the review we see affects the general meaning of the statement we
    have read so far. In this manner, we adjust our sentiment score for a review as
    we read along and come across new information (such as adjectives or negations)
    that may affect this score at a given time step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络似乎因为（不必要地）复杂的句子，包含了几个长期依赖关系和上下文效价转换器而变得困惑。回顾起来，我们注意到有不清晰的双重否定，指代了如导演、演员和电影本身等不同实体；然而我们能够看出这篇评论的总体情感显然是积极的。为什么？因为我们能够跟踪与评论总体情感相关的概念，逐字阅读时，我们的大脑能够评估我们所阅读的每个新词如何影响已读语句的整体意义。通过这种方式，我们在阅读过程中会根据新的信息（如形容词或否定词）调整评论的情感得分，这些信息可能在特定的时间步骤上影响得分。
- en: Just like in CNNs, we want our network to be able to use representations that
    have been learned on a certain segment of the input, which are then usable later
    on in other segments and examples. In other words, we need to be able to share
    the weights of our network from previous time steps to connect bits of information
    as we sequentially go over our input review. This is pretty much what RNNs allow
    us to do. These layers leverage the additional information that's encoded in successive
    events, which it does by looping over a sequence of input values. Depending on
    the architectural implementation, RNNs can save relevant information in its memory
    (also referred to as its state) and use this information to perform predictions
    at subsequent time steps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在卷积神经网络（CNN）中一样，我们希望网络能够使用在输入的某个片段上学到的表示，并能在后续的其他片段和示例中使用。换句话说，我们需要能够共享网络在前几个时间步骤中学到的权重，以便在按顺序阅读输入评论时将信息片段连接起来。这正是RNN所允许我们做的。这些层利用了连续事件中编码的额外信息，方法是遍历输入值的序列。根据架构实现的不同，RNN可以将相关信息保存在其记忆（也称为状态）中，并使用这些信息在随后的时间步骤中进行预测。
- en: 'This mechanism is notably different from the networks we saw earlier, which
    processed each training iteration independently and did not maintain any state
    between predictions. There are several different implementations of recurrent
    neural networks, ranging from **Gated Recurrent Units** (**GRUs**), stateful and
    stateless **Long Short-Term Memory** (**LSTM**) networks, bi-directional units,
    and many more. As we will soon discover, each of these architectures help to address
    a certain type of problem, building on the shortcomings of each other:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制与我们之前看到的网络显著不同，后者每次训练迭代是独立的，并且在预测之间没有保持任何状态。循环神经网络有多种不同的实现方式，从**门控循环单元**（**GRUs**）、有状态和无状态的**长短期记忆**（**LSTM**）网络、双向单元等，种类繁多。正如我们很快会发现的那样，这些架构中的每一种都有助于解决某类问题，并在彼此的不足之上构建：
- en: '![](img/7d9ac677-7a6d-493d-8769-4c6a1737b07c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d9ac677-7a6d-493d-8769-4c6a1737b07c.png)'
- en: Basic RNN architecture
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的RNN架构
- en: 'Now, let''s have a look at how the RNN architecture differentiates itself from
    the other networks that we have seen so far by unrolling it through time. Let''s
    consider a new time series problem: speech recognition. This task can be performed
    by computers to identify the flow of words during a segment of human speech. This
    can be used to transcribe the speech itself, translate it, or use it as input
    for instructions, similar to the manner in which we instruct each other. Such
    applications form the base of systems such as Siri or Alexa and perhaps more complex
    and cognitive virtual assistants of the future. So, how can an RNN decode the
    sequence of decomposed vibrations that are recorded by the microphone on your
    computer into a string variable corresponding to the input speech?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看RNN架构如何通过时间展开，区别于我们之前看到的其他网络。让我们考虑一个新的时间序列问题：语音识别。计算机可以执行这个任务，通过识别人类语音段落中单词的流动。它可以用于转录语音本身、翻译语音，或将其用作输入指令，类似于我们彼此间的指令方式。这类应用构成了像Siri或Alexa这样的系统的基础，甚至可能是未来更复杂、更具认知能力的虚拟助手的基础。那么，RNN是如何解码通过电脑麦克风录制下来的分解震动序列，转化为与输入语音相对应的字符串变量的呢？
- en: 'Let''s consider a simplified theoretical example. Imagine that our training
    data maps a sequence of human vocalizations to a set of human-readable words.
    In other words, you show your network an audio clip and it spits out a transcript
    of whatever was said within. We task an RNN to go over a segment of speech by
    treating it as sequences of vectors (representing sound bytes). The network can
    then try to predict what words of the English language these sound bytes may represent
    at each time step:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简化的理论示例。假设我们的训练数据将一系列人类发声映射到一组可读的单词。换句话说，你向网络展示一个音频片段，它会输出其中说的内容的文字记录。我们将一个RNN任务分配给它，处理一段语音，将其视为一系列向量（表示声音字节）。然后，网络可以尝试预测这些声音字节在每个时间步骤可能代表的英语单词：
- en: '![](img/d74466a5-cfac-4d7a-b9d7-a03245b7de68.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d74466a5-cfac-4d7a-b9d7-a03245b7de68.png)'
- en: Consider the set of vectors that represent the sound byte for the words *today
    is a nice day*. A recurrent layer will unfold this sequence in several time steps.
    In the first time step, it will take the vector representing vocalization for
    the first word in the sequence as input (that is, *today*), compute a dot product
    with the layer weights, and pass the product through a non-linear activation function
    (commonly tanh for RNNs) to output a prediction. This prediction corresponds to
    a word that the network thinks it has heard. At the second time step, the recurrent
    layer receives the next sound byte (that is, for the word *is*) in the sequence,
    along with the activation values from the first time step. Both of these values
    are then squashed through the activation function to produce a prediction for
    the current time step. This basically allows the layer to leverage information
    from the previous time steps to inform the prediction at the current time step.
    This process is repeated as the recurrent layer receives each vocalization in
    a given sequence, along with the activation values from previous vocalizations.
    The layer may compute a Softmax probability score for each word in our dictionary,
    picking the one with the highest value as output for the given layer. This word
    corresponds to what our network thinks it has heard at this time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑表示*今天是个好日子*这句话的声音字节向量集合。一个递归层将在几个时间步骤中展开这个序列。在第一个时间步骤，它将接受表示序列中第一个单词（即*今天*）的向量作为输入，与层权重进行点积运算，并将结果通过一个非线性激活函数（通常是RNN使用的tanh）输出一个预测值。这个预测值对应着网络认为它所听到的单词。在第二个时间步骤，递归层接收下一个声音字节（即单词*是*），以及来自第一个时间步骤的激活值。然后，这两个值都会通过激活函数进行压缩，产生当前时间步骤的预测。这基本上使得该层能够利用先前时间步骤的信息来指导当前时间步骤的预测。这个过程会随着递归层接收每个音节的发声，并结合之前发声的激活值反复进行。该层可能会为字典中的每个单词计算一个Softmax概率分数，选择具有最高值的单词作为当前层的输出。这个单词就是网络认为它在这个时间点所听到的内容。
- en: Temporarily shared weights
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 临时共享权重
- en: Why is it useful to temporally connect activations? Well, as we pointed out
    earlier, each word affects the probability distribution of the next word to come.
    If our sentence began with the word *Yesterday*, it is much more likely to be
    followed by the word *was*, than the word *is*, reflecting the use of the past
    tense. Such syntactic information can be passed along through recurrent layers
    to inform the predictions of the network at each step by using what the network
    has output in previous time steps. As our network trains on given segments of
    speech, it will adjust its layer weights to minimize the difference between what
    it predicts and the true value of each output by (hopefully) learning such grammatical
    and syntactic rules, among other things. Importantly, the recurrent layer's weights
    are temporally shared, allowing activations from previous time steps to have influence
    over predictions of subsequent time steps. Doing so, we no longer treat each prediction
    in isolation, but as a function of the network's activations at previous time
    steps, along with some input at the current time step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么将激活连接到时间序列上有用？正如我们之前提到的，每个词都会影响下一个词的概率分布。如果我们的句子以单词*Yesterday*开头，那么它更可能接着是*was*，而不是*is*，这反映了过去时的使用。这种句法信息可以通过递归层传递，以通过利用网络在先前时间步输出的内容，来指导网络在每个时间步的预测。当我们的网络在给定的语音片段上进行训练时，它会调整其层权重，以最小化预测值与每个输出的真实值之间的差异，通过（希望）学习这些语法和句法规则，以及其他内容。重要的是，递归层的权重是时间共享的，使得先前时间步的激活对后续时间步的预测产生影响。这样，我们不再将每个预测视为孤立的，而是将其视为网络在先前时间步的激活和当前时间步的输入的函数。
- en: The actual workflow of speech recognition models may be a bit more complex than
    what we described previously, which involves data normalization techniques such
    as the Fourier transformation, which lets us decompose audio signals into their
    constituent frequencies. In essence, we always try to normalize our input data
    with the goal to better represent data to our neural networks, as this helps it
    to converge faster to encode useful predictive rules. The key take-away from this
    example is that recurrent layers can leverage earlier temporal information to
    inform its predictions at the current time step. As we progress through this chapter,
    we will see how these architectures can be adopted for modeling different lengths
    of sequence input and output data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别模型的实际工作流程可能比我们之前描述的要复杂一些，其中涉及数据标准化技术，如傅里叶变换，它可以将音频信号分解成其组成的频率。实际上，我们总是试图对输入数据进行标准化，以更好地向神经网络表示数据，因为这有助于加速其收敛，从而编码有用的预测规则。从这个例子中得到的关键点是，递归层可以利用早期的时间信息来指导当前时间步的预测。随着本章的推进，我们将看到如何将这些架构应用于不同长度的序列输入和输出数据建模。
- en: Sequence modeling variations in RNNs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN中序列建模的变体
- en: The speech recognition example consists of modeling a synchronized many-to-many
    sequence, where we predicted many sets of vocalizations to many words that correspond
    to these vocalizations. We can use a similar architecture for the task of video
    captioning, where we would want to sequentially label each frame of the video
    with the dominant object within. This is yet another synchronized many-to-many
    sequence, as we output a prediction at each time step that corresponds to the
    input frame of the video.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别示例包含了建模一个同步的多对多序列，其中我们预测了许多语音集与这些语音对应的多个单词。我们可以使用类似的架构来进行视频字幕任务，在该任务中，我们希望顺序地为视频的每一帧标注其中的主要物体。这又是一个同步的多对多序列，因为我们在每个时间步都输出一个与视频输入帧对应的预测。
- en: Encoding many-to-many representations
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码多对多表示
- en: 'We can also have a semi-synchronized many-to-many sequence in the case of machine
    translation. This use case is semi-synchronized, as we do not immediately output
    a prediction at each time step. Instead, we use the encoder section of our RNN
    to capture the entire phrase so that it can be translated before we proceed and
    actually translate it. This lets us find better representations of the input data
    in the output language, instead of just translating each word at a time. The latter
    approach is not very robust and often leads to inaccurate translations. In the
    following example, an RNN translates the French phrase *C''est pas mal!* into
    the equivalent term in English, *It''s nice!*, which is a much more accurate translation
    than the literal, *It''s not bad!*. Hence, RNNs can help us to decipher the peculiar
    rules that are applied to complementing a person in the French language. This
    may help avoid quite a few misunderstandings:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器翻译的情况下，我们还可以使用半同步的多对多序列。这种用例是半同步的，因为我们不会在每个时间步骤上立即输出预测。相反，我们使用RNN的编码器部分来捕获整个短语，以便在我们继续并实际翻译之前先进行翻译。这使我们能够找到输入数据在目标语言中的更好表示，而不是逐个翻译每个单词。后一种方法并不稳健，通常会导致不准确的翻译。在以下示例中，RNN将法语短语*C'est
    pas mal!*翻译成英文的对应词*It's nice!*，这比字面意思的*It's not bad!*要准确得多。因此，RNN可以帮助我们解码法语中用于赞美一个人的独特规则，这可能有助于避免不少误解：
- en: '![](img/0dc153e8-dbf9-4326-8147-380991d5df01.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dc153e8-dbf9-4326-8147-380991d5df01.png)'
- en: Many-to-one
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多对一
- en: 'Similarly, you can also have a many-to-one architecture to address tasks such
    as attributing many sequences of words forming a sentence to one corresponding
    sentiment score. This is just like what we had to do in our previous exercise
    with the IMDb dataset. Last time, our approach involved representing each review
    as an undirected bag-of-words. With RNNs, we can approach this problem by modeling
    a review as a directed sequence of individual words, in their correct order, hence
    leveraging the spatial information from the arrangement of words to inform our
    sentiment score. Here is a simplified example of a many-to-one RNN architecture
    for sentiment classification:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你也可以使用多对一架构来处理一些任务，例如将多个词序列（构成一个句子）映射到一个对应的情感分数。这就像我们在上一次练习中使用IMDb数据集时所做的那样。上次，我们的方法是将每个评论表示为无向词袋。使用RNN时，我们可以通过将评论建模为一个按正确顺序排列的有向单词序列来处理这个问题，从而利用单词排列中的空间信息来帮助我们获得情感分数。以下是一个简化的多对一RNN架构示例，用于情感分类：
- en: '![](img/e5504705-2713-4b8a-bc3c-3854837869a7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5504705-2713-4b8a-bc3c-3854837869a7.png)'
- en: One-to-many
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对多
- en: 'Finally, different variations of sequential tasks may demand different architectures.
    Another commonly used architecture is the one-to-many RNN model, which we would
    use for the use case of music generation or image captioning. For music generation,
    we essentially feed our network one input note, make it predict the next note
    in the sequence, and then leverage its very own prediction as input for the next
    time step:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不同类型的序列任务可能需要不同的架构。另一个常用的架构是“一对多”RNN模型，我们通常在音乐生成或图像标题生成的场景中使用它。对于音乐生成，我们基本上将一个输入音符馈送给网络，预测序列中的下一个音符，然后将其预测作为下一个时间步骤的输入：
- en: '![](img/f8f6a4c0-d250-4103-9ede-c2ba360e3e4b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8f6a4c0-d250-4103-9ede-c2ba360e3e4b.png)'
- en: One-to-many for image captioning
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对多用于图像标题生成
- en: Another novel example of a one-to-many architecture is what is commonly used
    for the task of image captioning. This is when we show our network an image and
    ask it to describe what is going on with a small caption. To do this, we essentially
    feed our network one image at a time to output many words corresponding to what
    is going on in the image. Commonly, you may stack a recurrent layer on top of
    a CNN that has already been trained on some entities (objects, animals, people,
    and so on). Doing so, you could use the recurrent layer to intake the output values
    of the convolutional network all together, and sequentially go over the image
    to output meaningful words corresponding to a description of the input image.
    This is a more complex setup that we will elaborate on in later chapters. For
    now, it is useful to know that LSTM networks (shown as follows) are a type of
    RNN that's inspired by semantic and episodic divisions of the human memory structure
    and will be the prime topic of discussion in [Chapter 6](62bc2e63-11f3-43ab-a3ae-967c6603c306.xhtml), *Long-Short
    Term Memory Networks*. In the following diagram, we can see how the network is
    able to pick up on the fact that there are several giraffes standing about, leveraging
    the output it receives from the CNN.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的一对多架构示例是图像描述任务中使用的架构。当我们将一张图片展示给网络，并要求它用简短的文字描述图片内容时，就会用到这种架构。为了实现这一点，我们实际上是一次性向网络输入一张图像，并输出与图像内容相关的多个单词。通常，你可能会在已经在某些实体（物体、动物、人物等）上训练过的卷积神经网络（CNN）上叠加一个递归层。这样，你可以利用递归层一次性处理卷积网络的输出值，并依次扫描图像，输出与输入图像描述相关的有意义的单词。这是一个更复杂的设置，我们将在后续章节中详细说明。目前，值得了解的是，LSTM网络（如下所示）是一种受到人类记忆结构中的语义和情节划分启发的RNN类型，并将在[第六章](62bc2e63-11f3-43ab-a3ae-967c6603c306.xhtml)中作为主要讨论主题，*长短期记忆网络*。在下图中，我们可以看到网络如何利用从CNN获得的输出，识别出有几只长颈鹿站在周围。
- en: Summarizing different types of sequence processing tasks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结不同类型的序列处理任务
- en: 'Now, we have familiarized ourselves with the basic idea of what a recurrent
    layer does and have gone over some specific examples of use cases (from speech
    recognition, machine translation, and image captioning) where variations of such
    time-dependent models may be used. The following diagram provides a visual summary
    of some of the sequential tasks we discussed, along with the type of RNN that''s
    suited for the job:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经熟悉了递归层的基本原理，并回顾了一些具体的使用案例（如语音识别、机器翻译和图像描述），在这些案例中，可以使用此类时间依赖模型的变体。下图提供了我们讨论的一些序列任务的视觉总结，以及适用于这些任务的RNN类型：
- en: '![](img/0bd4c08a-3dbf-4d48-9fb8-d7800c486b63.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bd4c08a-3dbf-4d48-9fb8-d7800c486b63.png)'
- en: Next, we will dive deeper into the governing equations, as well as the learning
    mechanism behind RNNs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨RNN的控制方程以及其学习机制。
- en: How do RNNs learn?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN是如何学习的？
- en: As we saw previously, for virtually all neural nets, you can break down the
    learning mechanism into two separate parts. The forward propagation equations
    govern the rules that allow data to propagate forward in our neural network, all
    of the way to the network predictions. The backpropagation of errors are defined
    by equations (such as the loss function and the optimizer), which allow our model's
    prediction errors to move backward through our model's layers, adjusting the weights
    on each layer toward the correct prediction values.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，对于几乎所有神经网络，你可以将学习机制分解为两个独立的部分。前向传播方程控制数据如何在神经网络中向前传播，一直到网络的预测结果。误差反向传播由方程（如损失函数和优化器）定义，它们允许模型的预测误差在模型的各层之间向后传播，调整每一层的权重，直到达到正确的预测值。
- en: This is essentially the same for RNNs, yet with a few architectural variations
    to account for time-dependent information flows. To do this, RNNs can leverage
    an internal state, or *memory*, to encode useful time-dependent representations.
    First, let's have a look at the forward pass of data in a recurrent layer. A recurrent
    layer basically combines the input vector that's entering the layer with a state
    vector to produce a new output vector at each time step. Soon, we will see how
    iteratively updating these state vectors can be leveraged to preserve temporally
    relevant information in a given sequence.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RNN来说，基本上是一样的，但有一些架构上的变动，用以应对时间依赖的信息流。为了做到这一点，RNN可以利用一个内部状态，或*记忆*，来编码有用的时间依赖表示。首先，我们来看看递归层中数据的前向传递。递归层基本上是将输入向量与状态向量结合，在每个时间步产生一个新的输出向量。很快，我们将看到如何通过迭代更新这些状态向量来保留给定序列中与时间相关的信息。
- en: A generic RNN layer
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个通用的RNN层
- en: 'The following diagram hopefully familiarizes this process. On the left, the
    gray arrow in the diagram illustrates how activations from current time steps
    are sent forward to future time steps. This holds true for all RNNs, forming a
    distinct signature of their architecture. On the right-hand side, you will notice
    a reduced representation of the RNN unit. This is one of the most common demarcations
    of RNNs that you will find in countless computer science research papers:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示应该有助于你熟悉这一过程。在左侧，图中的灰色箭头展示了当前时间步的激活是如何被传递到未来时间步的。这对于所有RNN都适用，构成了它们架构的独特标志。在右侧，你会看到RNN单元的简化表示。这是你在无数计算机科学研究论文中看到的RNN最常见的划分方式：
- en: '![](img/30b9a91b-026c-4fe1-a6c4-93fa6eb8a63f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30b9a91b-026c-4fe1-a6c4-93fa6eb8a63f.png)'
- en: To sequence, or not to sequence?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化，还是不序列化？
- en: The RNN layer essentially processes its input values in a time-dependent and
    sequential manner. It employs a state (or memory), which allows us to address
    sequence modeling tasks in a novel way. However, there are quite a few examples
    where approaching non-sequential data in a sequential manner has allowed us to
    address standard use cases in more efficient ways. Take the example of the research
    conducted by DeepMind on steering the attention of a network on images.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RNN层本质上是以时间依赖和顺序的方式处理输入值。它采用一种状态（或记忆），这使得我们能够以一种新颖的方式解决序列建模任务。然而，也有许多例子表明，以顺序的方式处理非顺序数据，能够让我们以更高效的方式解决标准应用场景。以DeepMind关于引导网络注意力集中于图像的研究为例。
- en: Instead of simply applying a computation-heavy CNN for image classification,
    DeepMind researchers showed how RNNs that have been trained through reinforcement
    learning can be used to perform the same function and achieve even better accuracy
    at more complex tasks such as classifying cluttered images, along with other dynamic
    visual control problems. One of the main architectural take backs from their work
    was that their RNN effectively extracted information from images or videos by
    adaptively selecting sequence or regions to process at a high resolution, thereby
    reducing the redundant computational complexity of processing an entire image
    at a high resolution. This is pretty neat, as we don't necessarily need to process
    all of the parts of an image to perform classification. Most of what we need is
    usually centered around a local area of the image in question: [https://deepmind.com/research/publications/recurrent-models-visual-attention/](https://deepmind.com/research/publications/recurrent-models-visual-attention/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的研究人员展示了如何通过强化学习训练的RNN来代替简单的计算密集型CNN进行图像分类，且能够在更复杂的任务中达到更高的准确度，如对杂乱图像的分类，以及其他动态视觉控制问题。他们研究中的主要架构启示之一是，RNN能够通过自适应选择要处理的序列或区域，以高分辨率提取图像或视频中的信息，从而减少以高分辨率处理整个图像所带来的冗余计算复杂性。这非常巧妙，因为我们并不一定需要处理图像的所有部分来进行分类。我们通常需要的内容往往集中在图像的局部区域：[https://deepmind.com/research/publications/recurrent-models-visual-attention/](https://deepmind.com/research/publications/recurrent-models-visual-attention/)。
- en: Forward propagation
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'So, how does information actually flow through this-here RNN architecture?
    Let''s use a demonstrative example to introduce the forward pass operations in
    RNNs. Imagine the simple task of predicting the next word in a phrase. Suppose
    our phrase is: *to be or not to be*. As the words enter the network, we can break
    down the computations that are performed at each time step into two conceptual
    categories. In the following diagram, you can visualize each arrow as performing
    a computation (or dot product operation) on a given set of values:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，信息是如何在这个 RNN 架构中流动的呢？让我们通过一个示例来介绍 RNN 中的前向传播操作。假设我们要预测短语中的下一个单词。设定我们的短语为：*to
    be or not to be*。随着单词进入网络，我们可以将每个时间步执行的计算分为两类概念性操作。在以下图示中，你可以将每个箭头视为在给定的数值集上执行一次计算（或点积操作）：
- en: '![](img/ecf33ff7-1e17-4e70-b5cd-a5c3983031d3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecf33ff7-1e17-4e70-b5cd-a5c3983031d3.png)'
- en: We can see that, in a recurrent cell, computations occur both vertically and
    horizontally as data propagates through it. It is important to remember that all
    parameters (or weight matrices) of the layer are temporally shared, meaning that
    the same parameters are used for computations at every time step. At the first
    time step, our layer will use these sets of parameters to compute two output values.
    One of these is the layer's activation value at the current time step, whereas
    the other represents the predicted value at the current time step. Let's start
    with the first one.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在一个递归单元中，计算既是纵向的也是横向的，数据在其中传播。需要记住的是，层的所有参数（或权重矩阵）都是时间共享的，意味着在每个时间步使用相同的参数进行计算。在第一个时间步，我们的层将使用这些参数集来计算两个输出值。其中一个是当前时间步的层激活值，而另一个则表示当前时间步的预测值。让我们从第一个开始。
- en: Computing activations per time step
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个时间步计算激活值
- en: 'The following equation denotes the activation of a recurrent layer at time, *t*.
    The term *g* denotes the non-linear activation function that''s chosen for the
    recurrent layer, which is conventionally a tanh function. Inside the brackets,
    we find two matrix-level multiplications being performed and then being added
    up along with a bias term (*ba*):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程表示在时间 *t* 时刻的递归层激活值。术语 *g* 表示所选的非线性激活函数，通常为 tanh 函数。在括号内，我们执行两个矩阵级别的乘法运算，然后将它们与偏置项
    (*ba*) 相加：
- en: '*at = g [ (W^(ax) x x^t ) + (Waa x a(t-1)) + ba ]*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*at = g [ (W^(ax) x x^t ) + (Waa x a(t-1)) + ba ]*'
- en: 'The term (*W^(ax)*) governs the transformation of our input vector, ![](img/ebdcf4e7-33d9-4f4e-b3f1-6e2a1b117f3d.png),
    at time, *t*, as it enters the recurrent layer. This matrix of weights is temporally
    shared, meaning that we use the same weight matrix at each time step. Then, we
    get to the term (*Waa*), which refers to the temporally shared weight matrix governing
    the activations from the previous time step. At the first time step, (*Waa*) is
    randomly initialized with very small values (or zeros), since we don''t actually
    have any activation weights to compute with just yet. The same holds for the value
    (*a<0>*), which is also initialized as a zeroed vector. Hence, at time step one,
    we our equation will look something like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 (*W^(ax)*) 控制输入向量 ![](img/ebdcf4e7-33d9-4f4e-b3f1-6e2a1b117f3d.png) 在时间 *t*
    时进入递归层的变换。这个权重矩阵是时间共享的，意味着我们在每个时间步使用相同的权重矩阵。接下来，我们看到术语 (*Waa*)，它表示控制来自前一个时间步的激活值的时间共享权重矩阵。在第一个时间步，(*Waa*)
    会随机初始化为非常小的值（或零），因为我们此时还没有激活权重可以计算。对于值 (*a<0>*) 也是如此，它被初始化为零向量。因此，在第一个时间步，我们的方程将看起来像这样：
- en: '*a1 = tanH [ (W^(ax) x x1 ) + (Waa x a(0)) + ba ]*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*a1 = tanH [ (W^(ax) x x1 ) + (Waa x a(0)) + ba ]*'
- en: Simplifying the activation equation
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化激活方程
- en: 'We can further simplify this equation by stacking the two weight matrices (Wax
    and Waa) horizontally into a single matrix (W[a]) that defines all of the weights
    (or the state) of a recurrent layer. We will also vertically stack the two vectors
    representing the activations from the previous time step (*a(t-1)*) and the input
    at the current time ( ![](img/d3b3e016-18de-4d98-be08-c5d9a8826797.png) t ) to
    form a new matrix that we denote as *[a(t-1), ![](img/45557d34-7a42-4cc5-b4ea-eb504c46f4eb.png)
    t ]* . This lets us simplify our previous activation, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将两个权重矩阵（Wax 和 Waa）水平堆叠成一个单一矩阵（W[a]），进一步简化这个方程，这个矩阵定义了递归层的所有权重（或状态）。我们还将代表前一个时间步激活（*a(t-1)*)
    和当前时间步输入（ ![](img/d3b3e016-18de-4d98-be08-c5d9a8826797.png) t ）的两个向量垂直堆叠，形成一个新矩阵，我们将其表示为
    *[a(t-1), ![](img/45557d34-7a42-4cc5-b4ea-eb504c46f4eb.png) t ]* 。这让我们可以简化之前的激活表达式，
    如下所示：
- en: '*at = tanH [ (W![](img/e76fecd2-fe5a-4716-a1da-048104147a2e.png) x ![](img/b7d3a7c5-8985-4076-9db2-0ab3f5c850ca.png)
    t ) + (Waa x a(t-1)) + ba ] or at = tanH (W![](img/5f597f64-f660-4896-be74-16289a777546.png)a(t-1),
    ![](img/71373feb-6c48-4dcb-9caa-3bd3750e0dd8.png) t ] + ba )*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*at = tanH [ (W![](img/e76fecd2-fe5a-4716-a1da-048104147a2e.png) x ![](img/b7d3a7c5-8985-4076-9db2-0ab3f5c850ca.png)
    t ) + (Waa x a(t-1)) + ba ] 或 at = tanH (W![](img/5f597f64-f660-4896-be74-16289a777546.png)a(t-1),
    ![](img/71373feb-6c48-4dcb-9caa-3bd3750e0dd8.png) t ] + ba )*'
- en: 'Conceptually, since the height of the two matrices (W![](img/7ee6ef33-8386-4d3a-a8eb-31f499fa003f.png))
    remains constant, we are able to stack them horizontally in the manner we did.
    The same goes for the length of the input (![](img/22c27f03-9e5f-401d-80ba-870c948b2871.png) t)
    and activation vector (*a(t-1)*), which also remains constant as data propagates
    through an RNN. Now, the computation step can be denoted as the weight matrix
    (W[a]) is multiplied both with the activation from the previous time step, as
    well as with the input from the current time step, after which a bias term is
    added and the whole term is passed through a non-linear activation function. We
    can visualize this process unfolding through time with the new weight matrix,
    as shown in the following diagram:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，由于两个矩阵（W![](img/7ee6ef33-8386-4d3a-a8eb-31f499fa003f.png)）的高度保持不变，我们能够像上面那样水平堆叠它们。同样，输入的长度（![](img/22c27f03-9e5f-401d-80ba-870c948b2871.png)
    t）和激活向量（*a(t-1)*) 也保持不变，因为数据在 RNN 中传播。现在，计算步骤可以表示为：权重矩阵（W[a]）既与前一个时间步的激活相乘，也与当前时间步的输入相乘，然后加上偏置项，整个项通过非线性激活函数传递。我们可以通过新的权重矩阵，按时间展开此过程，如下图所示：
- en: '![](img/3e5e7f94-b73d-42bb-9ed6-7a9c089fb868.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e5e7f94-b73d-42bb-9ed6-7a9c089fb868.png)'
- en: In essence, the use of the temporally shared weight parameters (such as *Wa*
    and *Wya*) allows us to leverage information from earlier on in the sequence to
    inform predictions at later time- teps. Now, you know how activations are iteratively
    computed for each time step as data flows through a recurrent layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，使用时间共享的权重参数（如 *Wa* 和 *Wya*）使我们能够利用序列前面的信息来为后续时间步的预测提供依据。现在，你已经知道了如何在每个时间步迭代计算激活值，数据在递归层中流动。
- en: Predicting an output per time step
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测每个时间步的输出
- en: 'Next, we will look at the equation that leverages the activation value that
    we just calculated to produce a prediction (![](img/94bdb4d8-346d-4392-8547-4550156203ea.png)
    at the given time step (*t*). This is represented like so:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到一个方程，它利用我们刚刚计算出的激活值来产生预测（在给定时间步（*t*）下）。这个过程可以表示如下：
- en: '*![](img/c2da3b60-fd3d-41a0-b36f-2deb2800f3f5.png) = g [ (Way x at) + by ]*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/c2da3b60-fd3d-41a0-b36f-2deb2800f3f5.png) = g [ (Way x at) + by ]*'
- en: This tells us is that our layer's prediction at a time step is determined by
    computing a dot product of yet another temporally shared output matrix of weights,
    along with the activation output (*at*) we just computed using the earlier equation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，层在某一时间步的预测是通过计算一个临时共享输出矩阵与我们刚刚计算出的激活输出（*at*）的点积来确定的。
- en: 'Due to the sharing of the weight parameters, information from previous time
    steps is preserved and passed through the recurrent layer to inform the current
    prediction. For example, the prediction at time step three leverages information
    from the previous time steps, as shown by the green arrow here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于共享权重参数，前一个时间步的信息得以保留，并通过递归层传递，用于当前的预测。例如，第三个时间步的预测利用了前一个时间步的信息，正如这里绿色箭头所示：
- en: '![](img/020d9e3e-b7ad-4fed-b41b-db712bab664b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/020d9e3e-b7ad-4fed-b41b-db712bab664b.png)'
- en: 'To formalize these computations, we mathematically show the relation between
    the predicted output at the third time step with respect to the activations at
    previous time steps, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形式化这些计算，我们数学地展示了第三个时间步的预测输出与前几个时间步的激活之间的关系，如下所示：
- en: '*![](img/c003db51-5aab-473c-9e57-5bea6611fa35.png) = sigmoid [ (Way x a3*)*
    + by* ]'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*![](img/c003db51-5aab-473c-9e57-5bea6611fa35.png) = sigmoid [ (Way x a3*)*
    + by* ]'
- en: 'Where *a(3)* is define by the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a(3)*的定义如下：
- en: '*a3 = sigmoid (W![](img/49ef1f9f-c46d-4975-92fa-0cf0963c59d0.png)a(2), ![](img/da73e7b9-513b-4b84-a764-8c01ef19d543.png)**3
    ] + ba )*'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a3 = sigmoid (W![](img/49ef1f9f-c46d-4975-92fa-0cf0963c59d0.png)a(2), ![](img/da73e7b9-513b-4b84-a764-8c01ef19d543.png)**3
    ] + ba )*'
- en: 'Where *a**(2)* is defined by the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a**(2)*的定义如下：
- en: '*a2 =* *sigmoid* *(W**![](img/89282f8e-619a-418d-9988-d949c010a5aa.png)a(1),
    ![](img/01d42088-c76b-475a-83bd-c1d6d5c4cfcf.png)**2 ] + ba )*'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a2 =* *sigmoid* *(W**![](img/89282f8e-619a-418d-9988-d949c010a5aa.png)a(1),
    ![](img/01d42088-c76b-475a-83bd-c1d6d5c4cfcf.png)**2 ] + ba )*'
- en: 'Where *a(1)* is defined by the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a(1)*的定义如下：
- en: '*a1 = sigmoid (W![](img/3eca3c48-eacc-4485-8d76-da83a1a9fc8c.png)a(0), ![](img/23fc8ac4-b910-41d8-a11e-7dffeba43a8c.png)**1
    ] + ba )*'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a1 = sigmoid (W![](img/3eca3c48-eacc-4485-8d76-da83a1a9fc8c.png)a(0), ![](img/23fc8ac4-b910-41d8-a11e-7dffeba43a8c.png)**1
    ] + ba )*'
- en: Finally, *a(0)* is commonly initialized as a vector of zeros. The main concept
    to understand here is that the RNN layer recursively processes a sequence through
    many time steps before passing the activations forward. Now, you are completely
    familiar with all the equations that govern the forward propagation of information
    in RNNs at a high level. This method, while powerful at modeling many temporal
    sequences, does have its limitations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*a(0)* 通常初始化为零向量。这里要理解的主要概念是，RNN层在将激活值传递到下一层之前，递归地通过多个时间步处理一个序列。现在，你已经完全掌握了RNN中信息前向传播的所有方程式，并且对其有了高层次的理解。尽管这种方法在建模许多时间序列方面非常强大，但它也存在一定的局限性。
- en: The problem of unidirectional information flow
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单向信息流问题
- en: 'A primary limitation is that we are only able to inform our prediction at current
    time steps with activation values from previous time steps, but not from future
    time steps. Why would we want to do this? Well, consider the problem of named
    entity recognition, where we may employ a synchronized many-to-many RNN to predict
    whether each word in our sentence is a named entity (such as the name of a person,
    a place, a product, and so on). We may run into some problems, such as the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的限制是我们只能通过前一时间步的激活值来告知当前时间步的预测，而无法利用未来时间步的数据。那么，为什么我们要这么做呢？请考虑命名实体识别的问题，在这个问题中，我们可能会使用同步的多对多RNN来预测句子中的每个词是否为命名实体（例如一个人名、地名、产品名等）。我们可能会遇到一些问题，如下所示：
- en: The Spartan marched forward, despite the obstacles thrown at him.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管面临各种障碍，斯巴达人依然坚定地向前行进。
- en: The Spartan lifestyle that these people face is unimaginable to many.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些人所面临的斯巴达式生活方式对于许多人来说是难以想象的。
- en: 'As we can see, by looking as the first two words only, we ourselves would not
    be able to tell whether the word Spartan refers to a noun (and hence is a named
    entity) or refers to an adjective. It is only later on, when we read the rest
    of the sentence, that we are able to attribute the correct label on the word.
    Similarly, our network will not be able to accurately predict that the word Spartan
    in the first sentence is a named entity unless we let it leverage activation values
    from future time steps. Since RNNs can learn sequential grammar rules from an
    annotated dataset, it will be able to learn the fact that named entities are often
    followed by verbs (such as marched) rather than nouns (such as lifestyle), and
    hence will be able to accurately predict that the word *Spartan* refers to a named
    entity on the first sentence only. This becomes possible with a specific type
    of RNN known as a bi-directional RNN, which we will look at later on in this chapter.
    It is also noteworthy that an annotated dataset with part of speech tags (tags
    referring to whether a word is a noun, adjective, and so on) will greatly increase
    your network''s ability to learn useful sequential representations, like we want
    it to do here. We can visualize the first part of both our sentences, annotated
    with part of speech tags, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，仅凭前两个词，我们自己是无法判断“Spartan”这个词是作为名词（因此是一个命名实体）使用，还是作为形容词使用。只有在继续阅读完整句子之后，我们才可以为这个词加上正确的标签。同样，我们的网络也无法准确预测第一句中的“Spartan”是一个命名实体，除非它能够利用未来时间步的激活值。由于RNN可以从带注释的数据集中学习序列语法规则，它将能够学习到命名实体通常后面跟随动词（例如
    marched）而非名词（例如 lifestyle）的规律，因此能够准确预测第一句中的“Spartan”是命名实体。这一切在一种特殊类型的RNN——双向RNN的帮助下成为可能，我们将在本章后面讨论它。值得注意的是，带有词性标签（指示一个词是名词、形容词等）的注释数据集，将大大提高你网络学习有效序列表示的能力，正如我们希望它在这里所做的那样。我们可以将两句的第一部分，带有词性标签的注释，可视化如下：
- en: 'The Spartan marched...à:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '斯巴达勇士行进...à:'
- en: '![](img/081b15e7-8f55-49de-a32e-235c6cfc7ff0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/081b15e7-8f55-49de-a32e-235c6cfc7ff0.png)'
- en: 'The Spartan lifestyle... à:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '斯巴达的生活方式...à:'
- en: '![](img/3b55564c-c0b7-40bf-af27-76eb9b8b0bf4.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b55564c-c0b7-40bf-af27-76eb9b8b0bf4.png)'
- en: The sequences of words that precede this provide us with more information on
    the current word than the words that come before it. We will soon see how bi-directional
    RNNs may leverage information from future time steps as well as past time steps
    to compute predictions at the present time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前词语之前的词序列，提供的信息比它前面出现的词语更多。我们很快将看到双向RNN如何利用来自未来时间步以及过去时间步的信息，在当前时间进行预测。
- en: The problems of long-term dependencies
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期依赖问题
- en: 'Another common problem we face with simple recurrent layers is their weakness
    in modeling long-term sequence dependencies. To clarify what we mean by this,
    consider the following examples, which we feed to an RNN word by word to predict
    the next words to come:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用简单的递归层时常遇到的另一个问题是，它们在建模长期序列依赖关系时的弱点。为了澄清我们所说的这一点，考虑以下示例，我们将它们逐词输入到一个RNN中，以预测接下来的词语：
- en: The monkey had enjoyed eating bananas for a while and was eager to have more,
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那只猴子已经享受了一段时间的香蕉美味，并且渴望吃更多。
- en: The monkeys had enjoyed eating bananas for a while and were eager to have more.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猴子们已经享受了一段时间的香蕉美味，并且渴望吃更多。
- en: To predict the word at the 11^(th) time step each sequence, the network must
    remember whether the subject of the sentence (monkey), seen at time-step 2, is
    a singular or plural entity. However, as the model trains and the errors are backpropagated
    through time, the weights for the time steps that are closer to the current time
    step are affected to a larger extent than the weights of earlier time steps. Mathematically
    speaking, this is the problem of vanishing gradients, which is associated with
    extremely small values of the chain rule-based partial derivatives of our loss
    function. The weights in our recurrent layer, which are normally updated in proportion
    to these partial derivatives at each time step, are not *nudged* enough in the
    right direction, prohibiting our network to learn any further. In this manner,
    the model is unable to update the layer weights to reflect long-term grammatical
    dependencies from earlier time steps, like the one reflected in our example. This
    is an especially cumbersome problem, since it significantly affects the backpropagation
    of errors in recurrent layers. Soon, we will see how to partially address this
    problem with more complex architectures such as the GRU and the LSTM networks.
    First, let's understand the process of backpropagation in RNNs, which gives birth
    to this problem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要预测每个序列中第11^(th)时间步的词，网络必须记住句子的主语（猴子），在时间步2看到的，是单数还是复数。然而，当模型训练并且误差通过时间反向传播时，距离当前时间步较近的时间步的权重受到的影响要比较早时间步的权重大得多。从数学角度来看，这就是梯度消失问题，它与我们损失函数的链式法则部分导数的极小值有关。我们递归层中的权重通常会根据每个时间步的这些部分导数进行更新，但它们并没有*足够地*朝着正确的方向微调，这使得我们的网络无法进一步学习。这样，模型无法更新层的权重，以反映早期时间步的长期语法依赖关系，就像我们的例子中所反映的那样。这是一个尤其棘手的问题，因为它显著影响了递归层中误差的反向传播。很快，我们将看到如何通过更复杂的架构（如GRU和LSTM网络）部分解决这个问题。首先，让我们理解RNN中反向传播的过程，它孕育了这个问题。
- en: You may well have wondered how exactly an RNN backpropagates its errors to adjust
    the temporarily shared weights of the layer as it goes over a sequence of inputs.
    This process is even described by an interesting name. Unlike other neural networks
    we have come across, RNNS are known to perform backpropagation through time.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，RNN是如何精确地通过反向传播调整层的暂时共享权重的，尤其是在处理一系列输入时。这个过程甚至有一个有趣的名字。与我们遇到的其他神经网络不同，RNN被称为通过时间进行反向传播。
- en: Backpropagation through time
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过时间反向传播
- en: 'Essentially, we are backpropagating our errors through several time steps,
    reflecting the length of a sequence. As we know, the first thing we need to have
    to be able to backpropagate our errors is a loss function. We can use any variation
    of the cross-entropy loss, depending on whether we are performing a binary task
    per sequence (that is, entity or not, per word à binary cross-entropy) or a categorical
    one (that is, the next word out of the category of words in our vocabulary à categorical
    cross entropy). The loss function here computes the cross-entropy loss between
    a predicted output ![](img/6ebd4a7c-8d96-4cd2-9c2d-2fb746fb420d.png) and actual
    value *(y)*, at time step, *t*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们正在通过多个时间步反向传播我们的误差，反映了一个序列的长度。如我们所知，能够反向传播误差的第一步是必须有一个损失函数。我们可以使用交叉熵损失的任何变体，具体取决于我们是否在每个序列上执行二分类任务（即每个单词是实体还是非实体
    à 二元交叉熵）或分类任务（即从我们的词汇表中选择下一个词 à 分类交叉熵）。这里的损失函数计算的是预测输出 ![](img/6ebd4a7c-8d96-4cd2-9c2d-2fb746fb420d.png)
    和实际值 *(y)* 在时间步 *t* 上的交叉熵损失：
- en: '*![](img/46ec09e1-ff3f-49fc-a4e7-4d2cab07b46a.png)( ![](img/aa137547-d065-4e5f-946c-fcb135fc277b.png)
    ![](img/59cd5142-6f5b-4cb8-b930-f2cb8c7b63ae.png) log ![](img/0f3b1dbe-0f85-4cfb-abdf-c011330710ed.png)
    - [ (1-![](img/15d6e47b-8878-42b1-b060-0af7fabf429c.png)*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/46ec09e1-ff3f-49fc-a4e7-4d2cab07b46a.png)( ![](img/aa137547-d065-4e5f-946c-fcb135fc277b.png)
    ![](img/59cd5142-6f5b-4cb8-b930-f2cb8c7b63ae.png) log ![](img/0f3b1dbe-0f85-4cfb-abdf-c011330710ed.png)
    - [ (1-![](img/15d6e47b-8878-42b1-b060-0af7fabf429c.png)*'
- en: 'This function essentially lets us perform an element-wise loss computation
    of each predicted and actual output, at each time step for our recurrent layer.
    Hence, we generate a loss value at each prediction the network makes, for each
    word (or sequence) it sees. We can then sum up each individual loss value to define
    the overall loss of our recurrent layer, operating over *ty* number of time steps.
    Hence, the overall loss of our network can be denoted as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数本质上让我们在每个时间步对递归层的每个预测和实际输出进行逐元素的损失计算。因此，我们在网络做出每个预测时为每个单词（或序列）生成一个损失值。然后，我们可以将每个单独的损失值加总起来，定义递归层的整体损失，跨越*ty*个时间步。因此，网络的整体损失可以表示为：
- en: '![](img/eae7767c-e6d4-4719-94ee-5b37bb3d6eee.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eae7767c-e6d4-4719-94ee-5b37bb3d6eee.png)'
- en: Using this denotation of the overall loss of our network, we can differentiate
    it with respect to the layer weights at each time step to compute the model's
    errors. We can visualize this process by referring back to our recurrent layer
    diagram. The arrows demarcate the backpropagation of errors through time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种表示网络整体损失的方法，我们可以对每个时间步的层权重求导，以计算模型的误差。我们可以通过回顾递归层的示意图来可视化这个过程。箭头标出了通过时间的错误反向传播。
- en: Visualizing backpropagation through time
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化时间反向传播
- en: 'Here, we backpropagate the errors in our model with respect to the layer weights
    at each time step and adjust the weight matrices, *Way* and *Wa*, as the model
    trains. We are still essentially computing the gradient of the loss function with
    respect to all of the network parameters, and proportionally nudging both the
    weight matrices in the opposite direction for each time step in our sequence:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们根据每个时间步的层权重反向传播模型中的误差，并在模型训练过程中调整权重矩阵*Way*和*Wa*。我们本质上仍然是在计算损失函数相对于网络所有参数的梯度，并在每个时间步中相应地反向调整两个权重矩阵。
- en: '![](img/ca8eeab8-a691-4bd8-baa3-eb46419af982.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca8eeab8-a691-4bd8-baa3-eb46419af982.png)'
- en: Now, we know how RNNs overate over a sequence of vectors and leverage time-dependent
    contingencies to inform predictions at each step.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道RNN是如何在一系列向量上进行操作，并利用时间相关的依赖来在每一步做出预测的。
- en: Exploding and vanishing gradients
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度爆炸与梯度消失
- en: 'Backpropagating the model''s errors in a deep neural network, however, comes
    with its own complexities. This holds equally true for RNNs, facing their own
    versions of the vanishing and exploding gradient problem. As we discussed earlier,
    the activation of neurons in a given time step is dependent on the following equation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深度神经网络中反向传播模型的错误也有其自身的复杂性。对于递归神经网络（RNN）来说，情况也不例外，它们面临着各自版本的梯度消失和梯度爆炸问题。正如我们之前讨论的，给定时间步的神经元激活依赖于以下公式：
- en: '*at = tanH [ (W![](img/49479bca-1721-40de-9026-bc9268c48c87.png) x ![](img/3175fbaa-c555-4dde-8aa9-6641b48527ea.png)
    t ) + (Waa x a(t-1)) + ba ]*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*at = tanH [ (W![](img/49479bca-1721-40de-9026-bc9268c48c87.png) x ![](img/3175fbaa-c555-4dde-8aa9-6641b48527ea.png)
    t ) + (Waa x a(t-1)) + ba ]*'
- en: 'We saw how *Wax* and *Waa* are two separate weight matrices that the RNN layers
    share through time. These matrices are multiplied to the input matrix at current
    time, and the activation from the previous time step, respectively. The dot products
    are then summed up, along with a bias term, and passed through a tanh activation
    function to compute the activation of neurons at current time (*t*). We then used
    this activation matrix to compute the predicted output at current time (![](img/0d6d9ba5-a6ca-4868-b7fc-5db15a607871.png)),
    before passing the activation forward to the next time step:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到*Wax*和*Waa*是RNN层在时间上共享的两个独立的权重矩阵。这些矩阵分别与当前时间步的输入矩阵和前一个时间步的激活进行乘法计算。点积结果然后与偏置项加和，并通过tanh激活函数来计算当前时间步（*t*）的神经元激活。接着，我们使用这个激活矩阵来计算当前时间步的预测输出（![](img/0d6d9ba5-a6ca-4868-b7fc-5db15a607871.png)），然后将激活值传递到下一个时间步：
- en: '*![](img/fcfd4173-60dc-4147-ad63-3e8ba6eaf87c.png) = softmax [ (Way x at) +
    by ]*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/fcfd4173-60dc-4147-ad63-3e8ba6eaf87c.png) = softmax [ (Way x at) +
    by ]*'
- en: Hence, the weight matrices (*Wax*, *Waa*, and *Way*) represent the trainable
    parameters of a given layer. During backpropagation through time, we first compute
    the product of gradients, which represent the changes in the layer weights of
    each time step with respect to the changes in the predicted and actual output.
    Then, we use these products to update the respective layer weights in the opposite
    direction of the change. However, when backpropagating across multiple time steps,
    these products may become infinitesimally small (hence not shifting the layer
    weights significantly), or gargantuanly big (hence overshooting from ideal weights).
    This is mainly true for the activation matrix (*Waa*). It represents the memory
    of our RNN layer since it encodes time-dependent information from previous time
    steps. Let's clarify this notion with a conceptual example to see how updating
    the activation matrix at earlier time steps becomes increasingly hard when dealing
    with long sequences. Suppose you wanted to calculate the gradient of your loss
    at time step three with respect to your layer weights, for example.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，权重矩阵（*Wax*、*Waa* 和 *Way*）代表了给定层的可训练参数。在时间反向传播过程中，我们首先计算梯度的乘积，表示每个时间步层权重相对于预测输出和实际输出变化的变化。然后，我们使用这些乘积来更新相应的层权重，方向与变化相反。然而，当跨多个时间步进行反向传播时，这些乘积可能变得极其微小（因此不会显著地改变层权重），或者变得极其巨大（因此超出了理想权重）。这主要适用于激活矩阵（*Waa*）。它代表了我们
    RNN 层的记忆，因为它编码了来自前一个时间步的时间依赖信息。让我们通过一个概念性的例子来澄清这个概念，看看在处理长序列时，更新早期时间步的激活矩阵是如何变得越来越困难的。假设你想计算在时间步三时的损失梯度，相对于层权重。
- en: Thinking on the gradient level
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从梯度的角度思考
- en: 'The activation matrix at a given time step is a function of the activation
    matrix from the previous time step. Hence, we are forced to recursively define
    the loss at time step three as a product of the sub-gradients of layer weights
    from previous time steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定时间步的激活矩阵是前一个时间步激活矩阵的函数。因此，我们必须递归地将时间步三的损失定义为来自先前时间步的层权重子梯度的乘积：
- en: '![](img/3fa444c5-b97f-42f8-ae0d-3746cb3a7c0e.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fa444c5-b97f-42f8-ae0d-3746cb3a7c0e.png)'
- en: 'Here, (*L*) represents the loss, (*W*) represents the weight matrices of a
    time step, and the *x* values are the inputs at a given time steps. Mathematically,
    this is equivalent to the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，(*L*) 代表损失，(*W*) 代表时间步的权重矩阵，*x* 值是给定时间步的输入。数学上，这相当于以下表达式：
- en: '![](img/90b4c49d-fe88-4e31-841d-4879da3ea31b.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90b4c49d-fe88-4e31-841d-4879da3ea31b.png)'
- en: The derivatives of these functions are stored in a Jacobean matrix, representing
    point-wise derivations of the weight and loss vectors. Mathematically, the derivatives
    of these functions are bound by an absolute value of 1\. However, small derivative
    values (close to 0), over several time-steps of matrix-wise multiplications, degrade
    exponentially, almost vanishing, which in turn prohibits the model from converging.
    The same holds true for large values (larger than 1) in the activation matrix,
    where the gradients will become increasingly large until they are attributed a
    NaN value (not a number), abruptly terminating the training process. How can we
    address these problems?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的导数存储在雅可比矩阵中，表示权重和损失向量的逐点导数。数学上，这些函数的导数的绝对值被限制在 1 以内。然而，小的导数值（接近 0），经过多次时间步的矩阵乘法后，会呈指数下降，几乎消失，这反过来又禁止了模型的收敛。对于激活矩阵中的大值（大于
    1），也是如此，梯度将变得越来越大，直到它们被赋值为 NaN（不是一个数字），从而突然终止训练过程。我们该如何解决这些问题呢？
- en: You can find more information on vanishing gradients at: [http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接中找到关于梯度消失的更多信息：[http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)。
- en: Preventing exploding gradients through clipping
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过裁剪防止梯度爆炸
- en: 'In the case of exploding gradients, the problem is much more evident. Your
    model simply stops training, returning a value error of NaN, corresponding to
    the exploded gradient values. A simple solution to this is to clip your gradients
    by defining an arbitrary upper bound or threshold value to prevent the gradients
    from getting too big. Keras lets us implement this with ease as you can define
    this threshold by manually initiating an optimizer and passing it a `clipvalue`
    or `clipnorm` argument, as shown here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度爆炸的情况下，问题则更加明显。您的模型会直接停止训练，返回NaN值的错误，这对应于爆炸的梯度值。解决这个问题的简单方法是通过定义一个任意的上限或阈值来裁剪梯度，以防梯度变得过大。Keras让我们轻松实现这一点，您可以通过手动初始化优化器并传递`clipvalue`或`clipnorm`参数来定义这个阈值，如下所示：
- en: '![](img/43b2da26-4987-453f-a13e-2fe6ee04da70.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43b2da26-4987-453f-a13e-2fe6ee04da70.png)'
- en: You can then pass the `optimizers` variable to your model when compiling it.
    This idea of clipping gradients is extensively discussed, along with other problems
    that are associated with training RNNs, in the paper titled: *On the difficulty
    of training recurrent neural networks*, which is available at [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以将`optimizers`变量传递给模型进行编译。关于梯度裁剪的这一思想，以及与训练RNN相关的其他问题，已在论文《*训练递归神经网络的难度*》中进行了广泛讨论，您可以在[http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)阅读该论文。
- en: Preventing vanishing gradients with memory
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用记忆防止梯度消失
- en: In the case of vanishing gradients, our network stops learning anything new
    as the weights are insignificantly nudged at each update. The problem is particularly
    cumbersome for the case of RNNs, as they attempt to model long sequences over
    potentially many time steps, and so the model has a very hard time backpropagating
    the errors to nudge the layer weights of earlier time steps. We saw how this can
    affect language modeling tasks such as learning grammar rules and entity-based
    dependencies (with the monkey example). Thankfully, several solutions have been
    devised to address this problem. Some have ventured along the lines of careful
    initialization of the activation matrix, *Waa*, using a ReLU activation function
    to pre-train the layer weights in an unsupervised manner. More commonly, however,
    others have addressed this problem by designing more sophisticated architectures
    that are capable of storing long-term information based on its statistical relevance
    to current events in the sequence. This is essentially the base intuition behind
    more complex RNN variations such as the **Gated Recurrent Units** (**GRUs**) and
    **Long Short-Term Memory** (**LSTM**) networks. Let's see how GRUs address the
    problem of long-term dependencies.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度消失的情况下，我们的网络停止学习新内容，因为在每次更新时权重几乎没有变化。这个问题对于RNN尤其棘手，因为它们尝试在许多时间步长中建模长序列，因此模型在反向传播错误并调整较早时间步的层权重时会遇到很大困难。我们看到这个问题如何影响语言建模任务，例如学习语法规则和基于实体的依赖关系（以猴子示例为例）。幸运的是，已经有一些解决方案被提出以应对这个问题。一些方法试图通过精心初始化激活矩阵*Waa*，使用ReLU激活函数以无监督的方式对层权重进行预训练来解决此问题。然而，更常见的做法是通过设计更复杂的架构来解决此问题，这些架构能够根据当前序列中的事件统计相关性存储长期信息。这本质上是更复杂的RNN变种（如**门控循环单元**（**GRUs**）和**长短期记忆**（**LSTM**）网络）的基本直觉。接下来，我们将看看GRU如何解决长期依赖问题。
- en: GRUs
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GRUs
- en: The GRU can be considered the younger sibling of the LSTM, which we will look
    at [Chapter 6](62bc2e63-11f3-43ab-a3ae-967c6603c306.xhtml), *Long-Short Term Memory
    Networks*. In essence, both leverage similar concepts to modeling long-term dependencies,
    such as remembering whether the subject of the sentence is plural, when generating
    following sequences. Soon, we will see how memory cells and flow gates can be
    used to address the vanishing gradient problem, while better modeling long term
    dependencies in sequence data. The underlying difference between GRUs and LSTMs
    is in the computational complexity they represent. Simply put, LSTMs are more
    complex architectures that, while computationally expensive and time-consuming
    to train, perform very well at breaking down the training data into meaningful
    and generalizable representations. GRUs, on the other hand, while computationally
    less intensive, are limited in their representational abilities compared to LSTM.
    However, not all tasks require heavyset 10-layer LSTMs (like the ones used by
    Siri, Cortana, Alexia, and so on). As we will soon see, character-level language
    modeling can be achieved with quite simple architectures to begin with, producing
    increasingly interesting results with relatively lightweight models such as GRUs.
    The following diagram shows the basic architectural difference between the SimpleRNN
    we have been discussing so far and the GRU.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GRU可以被视为LSTM的“弟弟”，我们将在[第6章](62bc2e63-11f3-43ab-a3ae-967c6603c306.xhtml)中讨论*长短期记忆网络*。本质上，二者都利用类似的概念来建模长期依赖关系，例如在生成后续序列时记住句子的主语是复数。很快，我们将看到如何通过记忆单元和流门来解决消失梯度问题，同时更好地建模序列数据中的长期依赖关系。GRU与LSTM之间的根本区别在于它们所代表的计算复杂度。简单来说，LSTM是更复杂的架构，虽然计算开销大、训练时间长，但能够非常好地将训练数据分解成有意义且具有普适性的表示。而GRU则相对计算负担较轻，但在表示能力上相较于LSTM有所限制。然而，并非所有任务都需要像Siri、Cortana、Alexa等使用的10层LSTM。正如我们很快将看到的，字符级语言建模最初可以通过相对简单的架构实现，利用像GRU这样的轻量级模型，逐渐取得越来越有趣的结果。下图展示了我们迄今为止讨论的SimpleRNN与GRU之间的基本架构差异。
- en: The memory cell
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆单元
- en: 'Again, we have two input values entering the unit, namely the sequence input
    at the current time and the layer activations from the preceding time step. One
    of the main differences in the GRU is the addition of a memory cell (*c*), which
    lets us store some relevant information at a given time step to inform later predictions.
    In practice, this changes how we calculate the activations at a given time step
    (*c^t*, which here is the same as *a^t*) in GRUs:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们有两个输入值进入单元，分别是当前时间步的序列输入和前一时间步的层激活值。GRU的一个主要区别是增加了记忆单元(*c*)，它使我们能够在给定的时间步存储一些相关信息，以便为后续的预测提供依据。实际上，这改变了我们如何计算给定时间步的激活值(*c^t*，在这里与*a^t*相同)的方式：
- en: '![](img/5d45b808-83ca-40ac-ba99-bb81fc3b79b4.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d45b808-83ca-40ac-ba99-bb81fc3b79b4.png)'
- en: 'Going back to the monkey example, a word-level GRU model has the potential
    to better represent the fact that there are several entities in the second sentence
    that''s given here, and hence will remember to use the word **were** instead of
    **was** to complete the sequence:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 回到猴子示例，单词级的GRU模型有潜力更好地表示第二个句子中存在多个实体这一事实，因此能够记住使用**were**而不是**was**来完成序列：
- en: '![](img/3adae2d4-7633-4586-9ad5-15cc5dd89532.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3adae2d4-7633-4586-9ad5-15cc5dd89532.png)'
- en: 'How does this memory cell actually work? Well, the value of (*c^t*) stores
    the activation values (*a^t*) at a given time step (time step 2) and is passed
    forward to subsequent time steps if deemed relevant to the sequence at hand. Once
    the relevance of this activation is lost (that is, a new dependency has been detected
    in the sequence), the memory cell can be updated with a new value of (*c^t*),
    reflecting time-dependent information that may be more relevant:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个记忆单元到底是如何工作的呢？其实，(*c^t*)的值存储了给定时间步（时间步2）时的激活值(*a^t*)，并且如果该值对当前序列相关，则会被传递到后续的时间步。一旦该激活值的相关性丧失（也就是说，序列中检测到新的依赖关系），记忆单元就可以用新的(*c^t*)值进行更新，反映出可能更为相关的时间依赖信息：
- en: '![](img/a4f4ffc2-9b3f-44df-ab6c-1fe7306206bf.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f4ffc2-9b3f-44df-ab6c-1fe7306206bf.png)'
- en: A closer look at the GRU cell
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地了解GRU单元
- en: Representing the memory cell
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示记忆单元
- en: When processing our example sentences, a word-level RNN model may store the
    activations at time step 2 (for the words *monkey* and *monkeys*), and save it
    until time step 11, where it is used to predict the output words *was* and *were*,
    respectively. At each time step, a contender value (*c ^(̴t)*) is generated, which
    attempts to replace the value of the memory cell, (*c^t*). However, as long as
    (*c^t*) remains statistically relevant to the sequence, it is conserved, only
    to be discarded later on for more relevant representation. Let's see how this
    is mathematically implemented, starting with the contender value, (*c ^(̴t)*).
    To implement this parameter, we will initialize a new weight matrix, (*Wc*). Then,
    we will compute the dot product of (*Wc*) with the previous activation (*c^(t-1)*)
    and the input at the current time (![](img/005b1e37-203a-4ae1-b717-95d0ed56f958.png)
    t) and pass the resulting vector through a non-linear activation function such
    as tanh. This operation is strikingly similar to the standard forward propagation
    operation we saw previously.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理我们的示例句子时，基于单词级的RNN模型可能会在时间步2（对于单词*monkey*和*monkeys*）保存激活值，并保存到时间步11，在该时间步，它用于预测输出单词*was*和*were*。在每个时间步，都会生成一个候选值（*c
    ^(̴t)*），该值尝试替换记忆单元的值（*c^t*）。然而，只要（*c^t*）在统计上仍然与序列相关，它就会被保留，直到稍后为了更相关的表示而被丢弃。让我们看看这是如何在数学上实现的，从候选值（*c
    ^(̴t)*）开始。为了实现这个参数，我们将初始化一个新的权重矩阵（*Wc*）。然后，我们将计算（*Wc*）与之前的激活（*c^(t-1)*）以及当前时间的输入（![](img/005b1e37-203a-4ae1-b717-95d0ed56f958.png)
    t）的点积，并将得到的向量通过非线性激活函数（如tanh）。这个操作与我们之前看到的标准前向传播操作非常相似。
- en: Updating the memory value
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新记忆值
- en: 'Mathematically, we can represent this computation as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，我们可以将此计算表示为：
- en: '*c ^(̴t) = tanh ( Wc [ c^(t-1), ![](img/d30f337b-9a65-4151-82d8-203b210d2aee.png)
    t ] + bc)*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*c ^(̴t) = tanh ( Wc [ c^(t-1), ![](img/d30f337b-9a65-4151-82d8-203b210d2aee.png)
    t ] + bc)*'
- en: 'More importantly, the GRU also implements a gate denoted by the Greek alphabet
    gamma (Γu ), which basically computes a dot product of inputs and previous activations
    through yet another non-linear function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，GRU还实现了一个由希腊字母伽马（Γu）表示的门控，它基本上通过另一个非线性函数计算输入与之前激活的点积：
- en: '*Γu = sigmoid ( Wu [ c^(t-1), ![](img/8d63a839-9c80-49b7-80cf-5789501a6bb9.png)
    t ] + bu)*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*Γu = sigmoid ( Wu [ c^(t-1), ![](img/8d63a839-9c80-49b7-80cf-5789501a6bb9.png)
    t ] + bu)*'
- en: 'The purpose of this gate is to determine whether we should update our current
    value (*c^t*) with a candidate value (*c ^(̴t)*). The value of the gate (Γu) can
    be thought of as a binary value. In practice, we know that the sigmoid activation
    function is known for squishing values between zero and one. In fact, the vast
    majority of input values entering a sigmoid activation function will come out
    as either zero or one, hence it is practical to think of the gamma variable as
    a binary value that decides whether to replace (*c^t*) with (*c ^(̴t)*) or not
    at every time step:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个门控的目的是决定是否应当用候选值（*c ^(̴t)*)更新当前值（*c^t*）。门控的值（Γu）可以被看作是一个二进制值。实际上，我们知道sigmoid激活函数以将值压缩在0和1之间而闻名。事实上，进入sigmoid激活函数的绝大多数输入值最终会变成0或1，因此可以实用地将伽马变量视为一个二进制值，它决定是否在每个时间步将（*c^t*）替换为（*c
    ^(̴t)*)。
- en: '![](img/68fa8246-5c30-4b86-ab85-939a6ff30292.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68fa8246-5c30-4b86-ab85-939a6ff30292.png)'
- en: Mathematics of the update equation
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新方程的数学
- en: 'Let''s see how this would work out in practice. We will use our earlier example
    once more, which has been extended here to theoretically demonstrate when a world-level
    GRU model may work:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实践中如何运作。我们将再次使用之前的示例，这里扩展了该示例，以理论上演示何时一个世界级的GRU模型可能有效：
- en: '*The monkey had enjoyed eating bananas for a while and was eager to have more.
    The bananas themselves were the best one could find on this side of the island...*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*猴子曾一度喜欢吃香蕉，并渴望再吃一些。香蕉本身是岛屿这边能找到的最好的...*'
- en: 'As a GRU layer goes over this sequence, it may store the activation values
    at the second time step as (c^t), detecting the presence of a singular entity
    (that is, *monkey*). It will carry forward this representation until it reaches
    a new concept in the sequence (*The bananas*), at which point the update gate
    (Γu) will allow the new candidate activation value (c ̴t) to replace the old value
    in the memory cell (c), reflecting the new plural entity, *bananas*. Mathematically,
    we can tie all of this up by defining how the activation value (ct) is calculated
    in a GRU:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当GRU层遍历这个序列时，它可能会将第二时间步的激活值存储为（c^t），检测到一个单一实体的存在（即*猴子*）。它会将这一表示向前传递，直到遇到序列中的新概念（*香蕉*），此时更新门（Γu）将允许新的候选激活值（c
    ̴t）替换记忆单元中的旧值（c），反映出新的复数实体，即*香蕉*。从数学上讲，我们可以通过定义GRU中激活值（ct）的计算方式来总结这一过程：
- en: '*ct = ( Γu x c ̴t ) + [ ( 1- Γu ) x ct-1 ]*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*ct = ( Γu x c ̴t ) + [ ( 1- Γu ) x ct-1 ]*'
- en: As we can see here, the activation values at a given time step are defined by
    a sum of two terms. The first term reflects the product of the gate value and
    the candidate value. The second term denotes the inverse of the gate value, multiplied
    by the activation from the previous time step. Intuitively, the first term simply
    controls whether to let the update term be included in the equation by being either
    one or zero. The second term controls the potential neutralization of the activation
    of the previous time step (ct-1). Let's have a look at how these two terms work
    together to decide whether or not an update is performed at a given time step.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在给定的时间步，激活值由两项之和定义。第一项反映了门控值和候选值的乘积。第二项表示门控值的反向，乘以前一时间步的激活值。直观地，第一项控制是否将更新项包含在方程中，通过取1或0。第二项则控制是否对上一时间步的激活（ct-1）进行中和。让我们来看一下这两项如何协同工作，以决定在给定的时间步是否进行更新。
- en: Implementing the no-update scenario
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现不更新场景
- en: 'In the case where the value (Γu) is zero, the first term reduces to zero altogether,
    removing the effect of (c ̴t), while the second term simply takes the activation
    value from the previous time step:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当值（Γu）为零时，第一个项完全归零，从而去除（c ̴t）的影响，而第二个项则直接采用上一时间步的激活值：
- en: '[PRE0]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this scenario, no update is performed, and the previous activations (`ct`)
    are preserved and passed forward to the next time step.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，不进行更新，之前的激活值（`ct`）被保留并传递到下一个时间步。
- en: Implementing the update scenario
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现更新场景
- en: 'On the other hand, if the gate holds a `1`, the equation allows c ̴t to become
    the new value of `ct`, since the second term reduces to zero `(( 1-1) x ct-1)`.
    This is what allows us to effectively perform an update to our memory cell, hence
    conserving useful time-dependent representations. The update scenario can be denoted
    mathematically like so:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果门控值为`1`，方程式允许c ̴t成为新的`ct`值，因为第二项归零 `(( 1-1) x ct-1)`。这使得我们能够有效地对记忆单元进行更新，从而保持有用的时间相关表示。更新场景可以用数学公式表示如下：
- en: '[PRE1]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Preserving relevance between time steps
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保持时间步之间的相关性
- en: 'The nature of the two terms that are used to perform our memory update helps
    us to preserve relevant information across multiple time steps. Hence, this implementation
    potentially provides a solution to the vanishing gradient issue by modeling long-term
    dependencies with the use of a memory cell. You may wonder, however, how exactly
    does the GRU assess the relevance of an activation? The update gate simply allows
    the replacement of the activation vector (*c^t*) with the new candidate (*c ^(̴t)*),
    but how do we know how relevant the previous activation (*c^(t-1)*) is to the
    current time step? Well, earlier, we presented a simplified equation for governing
    the GRU unit. A last addition to its implementation is the relevance gate (Γr),
    which helps us to do exactly what it suggests. Hence, we calculate the candidate
    value (*c ^(̴t)*) using this relevance gate (Γr) to incorporate the relevance
    of activation values from the previous time step (*c*^(**t*-1*)) to the current
    one (*c^t*). This helps us to assess how relevant the activations from the previous
    time steps are to the current input sequence at hand and is implemented in a very
    familiar way, as shown in the following diagram:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行记忆更新的两个项的性质帮助我们在多个时间步之间保持相关信息。因此，这种实现通过使用记忆单元来建模长期依赖性，可能为解决梯度消失问题提供了一个方案。然而，你可能会想，GRU
    是如何评估激活的相关性的呢？更新门简单地允许用新的候选值 (*c ^(̴t)*) 替代激活向量 (*c^t*)，但我们如何知道先前的激活 (*c^(t-1)*)
    对当前时间步的相关性呢？好吧，之前我们展示了一个简化的方程来描述 GRU 单元。它实现的最后一部分就是相关性门（Γr），它帮助我们做正如其所示的事情。因此，我们通过这个相关性门（Γr）来计算候选值
    (*c ^(̴t)*)，以便在计算当前时间步的激活时，将先前时间步的激活（*c^(t-1)*) 的相关性纳入其中。这帮助我们评估先前时间步的激活对于当前输入序列的相关性，其实现方式非常熟悉，如下图所示：
- en: '![](img/88c3f787-2091-4af8-8fb0-f1baebc84124.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88c3f787-2091-4af8-8fb0-f1baebc84124.png)'
- en: Formalizing the relevance gate
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 形式化相关性门
- en: 'The following equations show the full spectrum of the GRU equations, including
    the relevance gate term, which is now included in the computation we performed
    earlier to get the contender memory value, (c ̴t ):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程展示了 GRU 方程的完整范围，包括现在包含在我们之前计算候选记忆值（c ̴t）中的相关性门项：
- en: '**Earlier**: *c ̴t = tanh ( Wc [ ct-1, ![](img/1666159b-52ff-45a0-a1d6-2ed0792818e5.png)
    t ] + bc)*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**之前**：*c ̴t = tanh ( Wc [ ct-1, ![](img/1666159b-52ff-45a0-a1d6-2ed0792818e5.png)
    t ] + bc)*'
- en: '**Now**: *c ̴t = tanh ( Wc [ Γr , ct-1, ![](img/8a195fb9-f845-4c0c-b258-dbe27c76fcac.png)
    t ] + bc)*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现在**：*c ̴t = tanh ( Wc [ Γr , ct-1, ![](img/8a195fb9-f845-4c0c-b258-dbe27c76fcac.png)
    t ] + bc)*'
- en: '**Where**: *Γr = sigmoid ( Wr [ ct-1, ![](img/8b555231-db98-4e95-926a-b4ed29399edf.png)
    t ] + br)*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其中**：*Γr = sigmoid ( Wr [ ct-1, ![](img/8b555231-db98-4e95-926a-b4ed29399edf.png)
    t ] + br)*'
- en: 'Not surprisingly, (Γr) is computed by initializing yet another weight matrix
    (*Wr*) and computing its dot product with past activations (*c^(t-1)*) and current
    inputs (![](img/288f2225-c922-4231-9fb6-8a3474d237fa.png) t) before summing them
    through a sigmoid activation function. The equation that''s computing the current
    activation (*c^t*) remains the same, except for the (*c ^(̴t)*) term within it,
    which is now incorporating the relevance gate (Γr) in its calculation:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，（Γr）是通过初始化另一个权重矩阵 (*Wr*) 来计算的，并将其与过去的激活 (*c^(t-1)*) 和当前输入（![](img/288f2225-c922-4231-9fb6-8a3474d237fa.png)
    t）进行点积，然后通过 sigmoid 激活函数求和。计算当前激活 (*c^t*) 的方程保持不变，唯一的不同是其中的 (*c ^(̴t)*) 项，它现在在计算中引入了相关性门（Γr）：
- en: '*ct = ( Γu x c ̴t ) + [ ( 1- Γu ) x ct-1 ]*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*ct = ( Γu x c ̴t ) + [ ( 1- Γu ) x ct-1 ]*'
- en: 'The predicted output at a given time step is calculated in the same manner
    as it was for the SimpleRNN layer. The only difference is that the term (*a^t*)
    is replaced by the term (*c^t*), which denotes the activations of neurons in a
    GRU layer at time step (*t*):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 给定时间步的预测输出的计算方式与 SimpleRNN 层相同。唯一的区别是 (*a^t*) 被 (*c^t*) 替代，后者表示 GRU 层在时间步 (*t*)
    的神经元激活：
- en: '*![](img/f0e96174-8b99-4ab8-8570-59e838271286.png) = softmax [ (Wcy x ct) +
    by ]*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/f0e96174-8b99-4ab8-8570-59e838271286.png) = softmax [ (Wcy x ct) +
    by ]*'
- en: Practically speaking, both terms (*a^t* and *c^t*) can be thought of as synonymous
    in the case of GRUs, but we will later see architectures where this no longer
    applies, such as in LSTMs. For the time being, we have covered the basic equations
    that govern the forward propagation of data in a GRU unit. You've seen how we
    can compute the activations and the output values at each time step and use different
    gates (such as the update and relevance gates) to control the flow of information,
    allowing us to assess and store long-term dependencies. What we saw here is a
    very common implementation that addresses the vanishing gradients problem. However,
    it is but one of potentially many more. Researchers have found this particular
    formulaic implementation to be a successful way to gauge relevance and model sequential
    dependencies for an array of different problems since their introduction in 2014
    by Kyunghyun Cho et al.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，*a^t*和*c^t*这两个术语在 GRU 的情况下可以认为是同义的，但稍后我们会看到一些架构不再适用这种情况，例如 LSTM。暂时来说，我们已经涵盖了控制
    GRU 单元中数据前向传播的基本方程。你已经看到我们如何计算每个时间步骤的激活值和输出值，并使用不同的门（例如更新门和相关性门）来控制信息流，进而评估和存储长期依赖关系。我们看到的这一实现是解决梯度消失问题的常见方式。然而，这只是潜在更多实现中的一种。自
    2014 年 Kyeunghyun Cho 等人提出以来，研究人员发现这种特定的公式化实现是一种成功的方式，用于评估相关性并为各种不同问题建模顺序依赖关系。
- en: Building character-level language models in Keras
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中构建字符级语言模型
- en: Now, we have a good command over the basic learning mechanism of different types
    of RNNs, both simple and complex. We also know a bit about different sequence
    processing use cases, as well as different RNN architectures that permit us to
    model these sequences. Let's combine all of this knowledge and put it to use.
    Next up, we will test these different models on a hands-on task and see how each
    of them do.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经很好地掌握了不同类型 RNN 的基本学习机制，包括简单的和复杂的。我们也了解了一些不同的序列处理用例，以及允许我们对这些序列建模的不同 RNN
    架构。接下来，我们将结合所有这些知识并加以实践。接下来，我们将通过一个实际任务来测试这些不同的模型，并看看它们各自的表现如何。
- en: 'We will explore the simple use case of building a character level language
    model, much like the autocorrect model almost everybody is familiar with, which
    is implemented on word processor applications for almost all devices. A key difference
    will be that we will train our RNN to derive a language model from Shakespeare''s
    Hamlet. Hence, our network will take a sequence of characters from Shakespeare''s
    *Hamlet* as input and iteratively compute the probability distribution of the
    next character to come in the sequence. Let''s make some imports and load in the
    necessary packages:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索一个简单的用例，构建一个字符级语言模型，类似于几乎每个人都熟悉的自动更正模型，它被实现于几乎所有设备的文字处理应用中。一个关键的不同之处在于，我们将训练我们的
    RNN 从莎士比亚的《哈姆雷特》派生语言模型。因此，我们的网络将把莎士比亚的*《哈姆雷特》*中的一系列字符作为输入，并反复计算序列中下一个字符的概率分布。让我们进行一些导入并加载必要的包：
- en: '[PRE2]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading in Shakespeare's Hamlet
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载莎士比亚的《哈姆雷特》
- en: 'We will use the **Natural Language Toolkit **(**NLTK**) package in Python to
    import and preprocess the play, which can be found in the `gutenberg` corpus:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Python 中使用**自然语言工具包**（**NLTK**）来导入并预处理这部戏剧，该剧本可以在`gutenberg`语料库中找到：
- en: '[PRE3]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The string variable (`text`) contains the entire sequence of characters that
    make up the play Hamlet. We will now break it up into shorter sequences that we
    can feed to our recurrent network at successive time steps. To forge the input
    sequences, we will define an arbitrary length of characters that the network sees
    at each time step. We will sample these characters from the text string by iteratively
    sliding over them and collecting sequences of characters (denoting our training
    features), along with the next character of the given sequence (as our training
    labels). Naturally, taking samples over longer sequences allows the network to
    compute more accurate probability distributions, hence reflecting contextual information
    on the character to follow. As a result, however, this is also computationally
    more intensive, both for training the model and to generate predictions during
    testing.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串变量（`text`）包含了构成《哈姆雷特》这部戏剧的整个字符序列。我们现在将其拆分成更短的序列，以便在连续的时间步中将其输入到我们的递归网络。为了构建输入序列，我们将定义一个任意长度的字符序列，网络在每个时间步看到这些字符。我们将通过迭代滑动并收集字符序列（作为训练特征），以及给定序列的下一个字符（作为训练标签），从文本字符串中采样这些字符。当然，采样更长的序列可以让网络计算出更准确的概率分布，从而反映出后续字符的上下文信息。然而，这也使得计算上更加密集，既需要在训练模型时进行更多的计算，也需要在测试时生成预测。
- en: Each of our input sequences (`x`) will correspond to 40 characters, and one
    output character (`y1`) that corresponds to the next character in the sequence.
    We can create this data structure of 11 characters per row by using the range
    function to span by segments characters of our entire string (text) at a time,
    and saving them in a list, as shown here. We can see that we have broken up the
    entire play into about 55, 575 sequences of characters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的每个输入序列（`x`）将对应40个字符和一个输出字符（`y1`），该字符对应序列中的下一个字符。我们可以使用范围函数按段对整个字符串（text）进行分段，从而每次处理11个字符，并将它们保存到一个列表中，如此所示。我们可以看到，我们已经将整个剧本拆分成了大约55,575个字符序列。
- en: Building a dictionary of characters
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建字符字典
- en: 'Now, we will proceed and create a vocabulary, or dictionary of characters,
    for mapping each character to a specific integer. This is a necessary step for
    us to be able to represent these integers as vectors, which we can sequentially
    feed into our network at each time step. We will create two versions of our dictionary:
    one with characters mapped to indices, and the other with indices mapped to characters.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续创建一个词汇表或字符字典，用于将每个字符映射到一个特定的整数。这是我们能够将这些整数表示为向量的必要步骤，这样我们就可以在每个时间步将它们顺序输入到网络中。我们将创建两种版本的字典：一种是字符映射到索引，另一种是索引映射到字符。
- en: 'This is just for the sake of practicality, as we will need both lists for reference:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是出于实用性考虑，因为我们需要这两个列表作为参考：
- en: '[PRE4]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can always check how large your vocabulary is by checking the length of
    the mapping dictionary. In our case, it appears that we have `66` unique characters
    that make up the sequences forming the play Hamlet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查映射字典的长度来查看你的词汇量有多大。在我们的例子中，似乎有`66`个独特的字符组成了《哈姆雷特》这部戏剧的序列。
- en: Preparing training sequences of characters
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备字符训练序列
- en: 'After constructing our dictionary of characters, we will break up the text
    making up Hamlet into a set of sequences that can be fed to our network, with
    a corresponding output character for each sequence:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建好我们的字符字典之后，我们将把构成《哈姆雷特》文本的字符拆分成一组序列，可以将这些序列输入到我们的网络中，并为每个序列提供一个对应的输出字符：
- en: '[PRE5]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We created two lists and looped over our text string to append a sequence of
    40 characters at a time. One list holds the training sequences, while the other
    holds the next character to come, following the 40 characters of the sequence.
    We have implemented an arbitrary sequence length of 40, but you are free to experiment
    with it. Keep in mind that setting too small a sequence will not allow you network
    to look far back enough to inform predictions, whereas setting to big a sequence
    may give your network a hard time converging, as it won't be able to find the
    most efficient representations. Just like the story of Goldilocks and the three
    bears, you will aim for a sequence length that is *just right*, as informed by
    experiments and/or domain knowledge.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个列表，并遍历了我们的文本字符串，每次附加一个 40 个字符的序列。一个列表保存训练序列，另一个列表保存紧随其后的下一个字符，即序列的下一个字符。我们实现了一个任意的序列长度
    40，但你可以自由尝试不同的值。请记住，设置太小的序列长度将无法让你的网络看到足够远的信息来做出预测，而设置过大的序列长度可能会让你的网络很难收敛，因为它无法找到最有效的表示方式。就像《金发姑娘与三只小熊》的故事一样，你的目标是找到一个*恰到好处*的序列长度，这可以通过实验和/或领域知识来决定。
- en: Printing out example sequences
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打印示例序列
- en: 'Similarly, we also arbitrarily choose to stride through our text file with
    a window of one character at a time. This simply means that we can potentially
    sample each character multiple times, just like how our convolutional filter progressively
    sampled an entire image by striding through it in fixed steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们也可以任意选择通过每次一个字符的窗口来遍历我们的文本文件。这意味着我们可以多次采样每个字符，就像我们的卷积滤波器通过固定步长逐步采样整个图像一样：
- en: '[PRE6]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The difference here is that we embed this sequentially in the training data
    itself, instead of letting a layer perform the striding operation while training.
    This is an easier (and more logical) approach with text data, which can be easily
    manipulated to produce the sequences of characters, at desired strides, from our
    entire text of Hamlet. As we can see, each of our lists now stores sequences of
    strings that are sampled at a stride of three steps, from the original text string.
    We printed out the first and second sequences and labels of our training data,
    which demonstrates the sequential nature of its arrangement.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的不同之处在于，我们将这个操作顺序地嵌入到训练数据本身，而不是让一个层在训练过程中执行步进操作。这对于文本数据来说是一种更简单（且更合乎逻辑）的做法，因为文本数据很容易操作，可以从整个《哈姆雷特》的文本中按照所需的步长生成字符序列。正如我们所看到的，我们的每个列表现在存储的是按步长三步采样的字符串序列，来自原始的文本字符串。我们打印出了训练数据的第一个和第二个序列及其标签，这展示了其排列的顺序性。
- en: Vectorizing the training data
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化训练数据
- en: 'The next step is one that your already quite familiar with. We will simply
    vectorize our data by transforming our list of training sequences into a three-dimensional
    tensor representing one-hot encoded training features, with their corresponding
    labels (that is, the next word to come in the sequence). The dimensions of the
    feature matrix can be represented as (*time steps x sequence length x number of
    characters*). In our case, this amounts to 55,575 sequences, each of a length
    of 40\. Hence, our tensor will be composed of 55,575 matrices, each with `40`
    vectors of `66` dimensions, stacked on top of each other. Here, each vector represents
    a single character, in a sequence of 40 characters. It has 66 dimensions, as we
    have one-hot-encoded each character as a vector of zeros, with `1` in the index
    position of that character from our dictionary:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是你已经非常熟悉的步骤。我们将通过将训练序列列表转换为表示 one-hot 编码训练特征的三维张量，并附上相应的标签（即序列中接下来的词语），来简单地向量化我们的数据。特征矩阵的维度可以表示为（*时间步
    x 序列长度 x 字符数*）。在我们的案例中，这意味着 55,575 个序列，每个序列长度为 40。因此，我们的张量将由 55,575 个矩阵组成，每个矩阵有
    `40` 个 `66` 维的向量，彼此堆叠在一起。在这里，每个向量代表一个字符，位于一个 40 个字符的序列中。它有 66 个维度，因为我们已将每个字符作为一个零向量进行了
    one-hot 编码，`1` 位于我们字典中该字符的索引位置：
- en: '[PRE7]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Statistics of character modeling
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符建模的统计数据
- en: We often distinguish words and numbers as being in different realms. As it happens,
    they are not so far apart. Everything can be deconstructed using the universal
    language of mathematics. This is quite a fortunate property of our reality, not
    just for the pleasure of modeling statistical distributions over sequences of
    characters. However, since we are on the topic, we will go ahead and define the
    concept of language models. In essence, language models follow Bayesian logic
    that relates the probability of posterior events (or tokens to come) as a function
    of prior occurrences (tokens that came). With such an assumption, we are able
    to construct a feature space corresponding to the statistical distribution of
    words over a period of time. The RNNs we will build shortly will each construct
    a unique feature space of probability distributions. Then, we are able to feed
    it a sequence of characters and recursively generate the next character to come
    using the distribution schemes.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将单词和数字区分为不同的领域。实际上，它们并没有那么远。任何东西都可以通过数学的普遍语言进行解构。这是我们现实中的一个幸运属性，不仅仅是为了建模统计分布在字符序列上的愉悦。然而，既然我们已经讨论到这个话题，我们将继续定义语言模型的概念。从本质上讲，语言模型遵循贝叶斯逻辑，将后验事件的概率（或未来可能出现的标记）与先前事件的发生（已经出现的标记）联系起来。基于这样的假设，我们能够构建一个特征空间，表示一定时间内单词的统计分布。我们接下来将构建的RNN将为每个模型构建一个独特的概率分布特征空间。然后，我们可以将一个字符序列输入到模型中，并递归地使用该分布方案生成下一个字符。
- en: Modeling character-level probabilities
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模字符级概率
- en: 'In **natural language processing** (**NLP**), the unit of a string is denoted
    as a token. Depending on how you wish to preprocess your string data, you can
    have word tokens or character tokens. We will be working with character tokens
    for the purpose of this example, as our training data is set up to make our network
    predict a single character at a time. Hence, given a sequence of characters, our
    network will output a Softmax probability score for each of the characters in
    our vocabulary of characters. In our case, we initially had 66 total characters
    in Shakespeare''s Hamlet. These included uppercase and lowercase letters, which
    are quite redundant for the task at hand. Hence, to increase our efficiency and
    keep track of less Softmax scores, we will reduce our training vocabulary by converting
    the Hamlet text into lowercase, leaving us with 44 characters. This means that,
    at each network prediction, it will generate a 44-way Softmax output. We can take
    the character with the maximum score (that is, do some greedy sampling) and add
    it to the input sequence, then ask our network what it thinks should come next.
    RNNs are able to learn the general structure of words in the English language,
    as well as punctuation and grammar rules, and even have a flair for inventing
    novel sequences, ranging from cool sounding names to possibly life-saving molecular
    compounds, depending on what sequence you decide to feed it. In fact, RNNs have
    been shown to capture the syntax of molecular representations and can be fine-tuned
    to generate specific molecular targets. This helps researchers considerably in
    tasks such as drug discovery and is a vivid area of scientific research. For further
    reading, check out the following link:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在**自然语言处理**（**NLP**）中，字符串的单位称为标记。根据你希望如何预处理字符串数据，你可以选择单词标记或字符标记。在本例中，我们将使用字符标记，因为我们的训练数据已设置为让网络一次预测一个字符。因此，给定一个字符序列，我们的网络会为每个字符在词汇表中的概率分配一个Softmax分数。在我们的例子中，最初《哈姆雷特》中总共有66个字符，包括大写字母和小写字母，这对于当前任务来说有些冗余。因此，为了提高效率并减少Softmax分数的数量，我们会通过将《哈姆雷特》文本转换为小写来缩减训练词汇量，从而得到44个字符。这意味着在每次网络预测时，它会生成一个44维的Softmax输出。我们可以选择具有最大分数的字符（也就是进行贪婪采样），并将其添加到输入序列中，然后让网络预测接下来应该是什么。RNN能够学习英语单词的一般结构，以及标点符号和语法规则，甚至能够创造新颖的序列，从酷炫的名字到可能具有生命拯救潜力的分子化合物，这取决于你输入给它的序列。事实上，RNN已被证明能够捕捉分子表示法的句法，并且可以微调以生成特定的分子目标。这在药物发现等任务中为研究人员提供了极大的帮助，也是一个充满活力的科学研究领域。有关进一步的阅读，请查看以下链接：
- en: '[https://www.ncbi.nlm.nih.gov/pubmed/29095571](https://www.ncbi.nlm.nih.gov/pubmed/29095571)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.ncbi.nlm.nih.gov/pubmed/29095571](https://www.ncbi.nlm.nih.gov/pubmed/29095571)'
- en: Sampling thresholds
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采样阈值
- en: To be able to generate sequences of Shakespeare-like sentences, we need to devise
    a mannerism to sample our probability distributions. These probability distributions
    are represented by our model's weights and continuously change at successive time
    steps during the training process. Sampling these distributions is akin to peeking
    into the network's idea of Shakespearean text at the end of each training epoch.
    We are essentially using the probability distributions that have been learned
    by our model to generate a sequence of characters. Moreover, depending on the
    sampling strategy we choose, we could potentially introduce some controlled randomness
    in our generated text to force our model to come up with some novel sequences.
    This can result in interesting formulations and is quite entertaining in practice.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够生成类似莎士比亚风格的句子，我们需要设计一种方式来采样我们的概率分布。这些概率分布由我们模型的权重表示，并且在训练过程中会在每个时间步不断变化。采样这些分布就像是在每个训练周期结束时窥视网络对莎士比亚文本的理解。我们基本上是使用模型学习到的概率分布来生成一系列字符。此外，根据我们选择的采样策略，我们有可能在生成的文本中引入一些受控的随机性，以迫使模型生成一些新颖的序列。这可能会导致一些有趣的表达方式，实际上非常有娱乐性。
- en: The purpose of controlling stochasticity
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制随机性的目的
- en: The main concept behind sampling is how you choose control stochasticity (or
    randomness) in selecting the next character from the probability distributions
    for possible characters to come. Various applications may ask for different approaches.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 采样背后的主要概念是如何选择控制随机性（或称随机性）来从可能字符的概率分布中选择下一个字符。不同的应用可能会要求不同的方法。
- en: Greedy sampling
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贪心采样
- en: If you are trying to train an RNN for automatic text completion and correction,
    you will probably be better off going with a greedy sampling strategy. This simply
    means that, at each sampling step, you will choose the next character in the sequence
    based on the character that was attributed the highest probability distribution
    by our Softmax output. This ensures that your network will output predictions
    that likely correspond to words you most commonly use. On the other hand, you
    may want to try a more stratified approach when training an RNN to generate cool
    names, handwriting in a particular person's style, or even producing undiscovered
    molecular compounds. In this case, you wouldn't want to choose the most likely
    characters to come, as this is simply boring. We can instead introduce some controlled
    randomness (or stochasticity) by picking out the next character in a probabilistic
    manner, rather than a fixed one.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在尝试训练一个用于自动文本完成和修正的RNN，使用贪心采样策略可能会更有效。这意味着，在每次采样时，你会根据Softmax输出分配给某个字符的最高概率来选择下一个字符。这样可以确保你的网络输出的预测很可能是你最常用的单词。另一方面，当你训练一个RNN来生成酷炫的名字、模仿某个人的书写风格，甚至生成未发现的分子化合物时，你可能会希望采用更分层的采样方法。在这种情况下，你并不希望选择最可能出现的字符，因为这会显得很无聊。我们可以通过以概率的方式选择下一个字符，而不是固定的方式，从而引入一些受控的随机性（或随机因素）。
- en: Stochastic sampling
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机采样
- en: 'One approach could be, instead of simply choosing the next character based
    on the Softmax output values, to reweight the probability distribution of these
    output values at a given time step. This lets us do things such as assign a proportional
    probability score for any of the characters of our vocabulary to be chosen next.
    As an example, suppose a given character has an assigned probability of 0.25 to
    be the next character in the sequence. We will then choose it one out of four
    times as the next character. In this manner, we are able to systematically introduce
    a little randomness, which gives rise to creative and realistic, albeit artificial
    words and sequences. Playing around by introducing randomness can often be usefully
    informative in the realm of generative modeling, as we will see in later chapters.
    For now, we will implement the controlled introduction of randomness in our sampling
    strategy by introducing a sampling threshold, which lets us redistribute the Softmax
    prediction probabilities of our model, [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法可能是，在选择下一个字符时，不仅仅依赖于 Softmax 输出值，而是对这些输出值的概率分布进行重新加权。这让我们能够做一些事情，比如为我们词汇表中的任何字符分配一个按比例的概率分数，使其成为下一个被选择的字符。举个例子，假设某个字符的下一个字符概率被分配为
    0.25。那么我们将有四分之一的概率选择它作为下一个字符。通过这种方式，我们能够系统地引入一些随机性，这会产生富有创意且逼真的人工词汇和序列。在生成模型的领域中，通过引入随机性进行探索往往能带来有用的信息，正如我们将在后续章节中看到的那样。现在，我们将通过引入采样阈值来实现控制随机性的引入，从而重新分配我们模型的
    Softmax 预测概率，[https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf)：
- en: '[PRE8]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This threshold denotes the entropy of the probability distribution we will be
    using, to sample a given generation from our model. A higher threshold will correspond
    to higher entropy distributions, leading to seemingly unreal and less structured
    sequences. Lower thresholds, on the other hand, will plainly encode English language
    representations and morphology, generating familiar words and terms.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阈值表示我们将使用的概率分布的熵，用于从我们的模型中采样给定的生成结果。较高的阈值会对应较高熵的分布，导致看起来不真实且缺乏结构的序列。另一方面，较低的阈值则会简单地编码英语语言的表示和形态，生成熟悉的单词和术语。
- en: Testing different RNN models
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试不同的 RNN 模型
- en: Now that we have our training data preprocessed and ready in tensor format,
    we can try a slightly different approach than previous chapters. Normally, we
    would go ahead and build a single model and then proceed to train it. Instead,
    we will construct several models, each reflecting a different RNN architecture,
    and train them successively to see how each of them do at the task of generating
    character-level sequences. In essence, each of these models will leverage a different
    learning mechanism and induct its proper language model, based on sequences of
    characters it sees. Then, we can sample the language models that are learned by
    each network. In fact, we can even sample our networks in-between training epochs
    to see how our network is doing at generating Shakespearean phrases at the level
    of each epoch. Before we continue to build our networks, we must go over some
    basic strategies to inform our task of language modeling and sampling. Then, we
    will build some Keras callbacks that let us interact with and sample our model
    while it trains.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将训练数据预处理并准备好以张量格式呈现，可以尝试一种与前几章略有不同的方法。通常，我们会构建一个模型，然后开始训练它。相反，我们将构建几个模型，每个模型反映不同的
    RNN 架构，并依次训练它们，看看每个模型在生成字符级别序列任务中的表现如何。从本质上讲，这些模型将利用不同的学习机制，并根据它们看到的字符序列来推导出其相应的语言模型。然后，我们可以从每个网络学习到的语言模型中进行采样。事实上，我们甚至可以在训练周期之间对我们的网络进行采样，看看我们的网络在每个周期生成莎士比亚短语的表现如何。在我们继续构建网络之前，必须先了解一些基本策略，以指导我们的语言建模和采样任务。然后，我们将构建一些
    Keras 回调函数，允许我们在模型训练过程中与其进行交互并进行采样。
- en: Using custom callbacks to generate text
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义回调函数生成文本
- en: 'Next, we will construct a custom Keras callback that will allow us to use the
    sample function we just constructed to iteratively probe our model at the end
    of each training epoch. As you will recall, callbacks are a class of functions
    that allow operations to be performed on our model (such as saving and testing)
    during the training process. These are very useful functions to visualize how
    a model performs throughout the training process. Essentially, this function will
    take a random sequence of characters from the Hamlet text and then generate 400
    characters to follow on, starting from the given input. It does this for each
    of the five sampling thresholds chosen and prints out the generated results at
    the end of each epoch:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个自定义的Keras回调函数，允许我们使用刚才构建的采样函数，在每个训练周期结束时迭代地探测我们的模型。如你所记得，回调函数是一类可以在训练过程中对我们的模型执行操作（如保存和测试）的函数。这些函数对于可视化模型在训练过程中的表现非常有用。本质上，这个函数将从《哈姆雷特》文本中随机选择一段字符，然后根据给定的输入生成400个后续字符。它会对每个选择的五个采样阈值执行此操作，并在每个周期结束时打印出生成的结果：
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Testing multiple models
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试多个模型
- en: 'The last task on our list is to build a helper function that will train, sample,
    and save a list of RNN models. This function also saves the history objects of
    the model that we used earlier to plot out the loss and accuracy values per epoch,
    which can be useful in case you want to explore different models and their relative
    performances at a later time:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表上的最后一个任务是构建一个辅助函数，该函数将训练、采样并保存一系列RNN模型。这个函数还会保存我们之前用于绘制每个时期的损失和准确度值的历史对象，如果你以后想要探索不同模型及其相对表现时，这会非常有用：
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we can finally proceed to the task of constructing several types of RNNs
    and training them with the helper function to see how different types of RNNs
    perform at generating Shakespeare-like texts.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以继续构建几种类型的RNN并用辅助函数训练它们，看看不同类型的RNN在生成类似莎士比亚的文本时表现如何。
- en: Building a SimpleRNN
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建SimpleRNN
- en: 'The SimpleRNN model in Keras is a basic RNN layer, like the ones we discussed
    earlier. While it has many parameters, most of them are set with excellent defaults
    that will get you by for many different use cases. Since we have initialized the
    RNN layer as the first layer of our model, we must pass it an input shape, corresponding
    to the length of each sequence (which we chose to be 40 characters earlier) and
    the number of unique characters in our dataset (which was 44). While this model
    is computationally compact to run, it gravely suffers from the vanishing gradients
    problem we spoke of. As a result, it has some trouble modeling long-term dependencies:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的SimpleRNN模型是一个基本的RNN层，类似于我们之前讨论的那些。虽然它有许多参数，但大多数已经设置了非常优秀的默认值，适用于许多不同的使用场景。由于我们已经将RNN层初始化为模型的第一层，因此必须为其提供输入形状，表示每个序列的长度（我们之前选择为40个字符）和我们数据集中的独特字符数量（为44）。尽管这个模型在计算上非常紧凑，但它严重受到了我们之前提到的梯度消失问题的影响。因此，它在建模长期依赖关系时存在一定问题：
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that this two-layered model has a final dense layer with a number of neurons
    corresponding to each of the 44 unique characters in our dataset. We equip it
    with a Softmax activation function, which will output a 44-way probability score
    at each time step, corresponding to the likelihood of each character to follow.
    All of the models we build for this experiment will have this final dense layer
    in common. Finally, all RNNs have the ability to remain stateful. This simply
    refers to passing on the layer weights for computations on the subsequent sequences
    of our training data. This feature can be explicitly set in all RNNs, with the
    `stateful` argument, which takes a Boolean value and can be provided when initializing
    a layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个两层模型的最终密集层的神经元数量与我们数据集中44个独特字符的数量相对应。我们为其配备了Softmax激活函数，它将在每个时间步输出一个44维的概率分数，表示每个字符后续出现的可能性。我们为这个实验构建的所有模型都将具有这个共同的最终密集层。最后，所有RNN都有保持状态的能力。这只是指将层的权重传递到后续序列的计算中。这个特性可以在所有RNN中显式设置，使用`stateful`参数，该参数接受布尔值，并可以在初始化层时提供。
- en: Stacking RNN layers
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠RNN层
- en: 'Why have one, when you can have two? All of the recurrent layers in Keras can
    return two different types of tensors, depending on what you wish to accomplish.
    You could either receive a 3D tensor of dimensions as output (`batch_size`, `time_steps`,
    `output_features`) or simply a 2D one, with dimensions of (`time_steps`, `output_features`).
    We query the 3D tensor if we want our model to return entire sequences of successive
    output values on each time step. This is useful if we want to stack an RNN layer
    on top of another, and then ask the first layer to return all of the activations
    to the second layer that''s stacked. Returning all activations essentially means
    returning the activation for each specific time step. These values can be subsequently
    fed into yet another recurrent layer, which aims to encode higher level abstract
    representations from the same input sequence. The following diagram shows the
    mathematical consequences of setting the Boolean argument to **True** or **False**:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只要一个，而不试试两个呢？Keras 中的所有递归层可以根据你想要实现的目标返回两种不同类型的张量。你可以选择接收一个维度为 (`batch_size`，`time_steps`，`output_features`)
    的三维张量输出，或者仅返回一个维度为 (`time_steps`，`output_features`) 的二维张量。如果我们希望模型返回每个时间步的完整输出序列，我们就查询三维张量。如果我们想要将一个
    RNN 层堆叠到另一个层上，并要求第一层返回所有的激活值给第二层，这时返回整个激活值就显得特别重要。返回所有激活值本质上意味着返回每个具体时间步的激活值。这些值可以随后输入到另一个递归层，以从相同的输入序列中编码出更高层次的抽象表示。以下图示展示了将布尔参数设置为
    **True** 或 **False** 的数学效果：
- en: '![](img/03ea7c75-466e-4209-b81f-49a95778f42d.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03ea7c75-466e-4209-b81f-49a95778f42d.png)'
- en: Setting it to true will simply return a tensor with predictions for each time
    step, instead of the prediction from the last time step only. The stacking of
    recurrent layers is quite useful. By stacking RNN layers on top on one another,
    we potentially increase the time-dependent representational value of our network,
    allowing it to memorize more abstract patterns that are potentially present in
    our data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 将其设置为 `true` 将简单地返回每个时间步的预测张量，而不是仅返回最后一个时间步的预测。堆叠递归层非常有用。通过将 RNN 层一个接一个地堆叠在一起，我们可以潜在地增加网络的时间依赖表示能力，使其能够记住数据中可能存在的更抽象模式。
- en: 'On the other hand, if we want it to only return the output at the last time
    step for each input sequence, we can ask it to return a 2D tensor. This is necessary
    when we want to go ahead and actually predict which of the characters from our
    vocabulary is most likely to be next. We can control this implementation with
    the `return_sequences` argument, which is passed when we add a recurrent layer.
    Alternatively, we can set it to false, making our model return the activation
    values from the last time step only, which can be propagated forward for classification:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们只希望它返回每个输入序列在最后一个时间步的输出，我们可以要求它返回一个二维张量。当我们希望进行实际的预测，预测在我们词汇表中下一个最可能的字符时，这是必要的。我们可以通过
    `return_sequences` 参数来控制这个实现，传递该参数时我们添加一个递归层。或者，我们也可以将其设置为 `false`，使得模型仅返回最后一个时间步的激活值，并可以将这些值向前传播用于分类：
- en: '[PRE12]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that the `return_sequences` argument can only be invoked for the penultimate
    hidden layers, and not for the hidden layer preceding the densely connected output
    layer, since the output layer is only tasked with classifying the next sequence
    to come.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`return_sequences` 参数只能用于倒数第二个隐藏层，而不能用于连接到输出层之前的隐藏层，因为输出层只负责分类下一个即将到来的序列。
- en: Building GRUs
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 GRU
- en: 'Excellent at mitigating the vanishing gradients problem, the GRU is a good
    choice for modeling long-term dependencies such as grammar, punctuation, and word
    morphology:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 在缓解梯度消失问题方面表现优异，是建模长期依赖关系（如语法、标点符号和词形变化）的良好选择：
- en: '[PRE13]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Just like the SimpleRNN, we define the dimensions of the input at the first
    layer and return a 3D tensor output to the second GRU layer, which will help retain
    more complex time-dependent representations that are present in our training data.
    We also stack two GRU layers on top of each other to see what the increased representational
    power of our model produces:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 SimpleRNN 一样，我们在第一层定义输入的维度，并将一个三维张量输出到第二个 GRU 层，这将有助于保留我们训练数据中更复杂的时间依赖表示。我们还将两个
    GRU 层堆叠在一起，以查看我们模型增强后的表示能力带来了什么：
- en: '![](img/bc2f701b-6f1c-424b-a062-39703e28e64e.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc2f701b-6f1c-424b-a062-39703e28e64e.png)'
- en: 'Hopefully, this architecture results in realistic albeit novel sequences of
    text that even a Shakespeare expert couldn''t tell apart from the real deal. Let''s
    visualize the model we built here through the following diagram:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这一架构能够生成逼真且新颖的文本序列，即使是莎士比亚的专家也无法分辨与真实的文本有何不同。让我们通过以下图示来可视化我们所构建的模型：
- en: Note that we have also included the line `model.summary()` in our trainer function
    we built earlier to visually depict the structure of the model after it is fit.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还在之前构建的训练函数中加入了`model.summary()`这一行代码，以便在模型训练完成后，直观地展示模型的结构。
- en: Building bi-directional GRUs
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建双向GRU
- en: Next in our models to test is yet another GRU unit, but this time with a twist.
    We nest it within a bi-directional layer, which allows us to feed our model each
    sequence in both the normal and the reverse order. In this manner, our model is
    able to *see* what is yet to come, leveraging future sequence data to inform predictions
    at the current time step. The nature of processing a sequence in a bi-directional
    manner greatly enhances the extracted representations from our data. In fact,
    the order of processing a sequence can have a significant effect on the type of
    representations that are learned after.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要测试的模型是另一个GRU单元，但这一次有所不同。我们将它嵌套在一个双向层内，这使得我们能够同时以正常顺序和反向顺序喂入数据。在这种方式下，我们的模型能够*看到*未来的内容，利用未来的序列数据来为当前时间步做出预测。双向处理序列的方式大大增强了从数据中提取的表示。事实上，处理序列的顺序可能会对学习到的表示类型产生显著影响。
- en: On processing reality sequentially
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序处理现实
- en: 'The notion of changing the order of processing a sequence is quite an intriguing
    one. We humans certainly seem to prefer a certain order of learning things over
    another. The second sentence that''s been reproduced in the following image simply
    makes no sense to us, even though we know exactly what each individual word within
    the sentence means. Similarly, many of us have a hard time reciting the letters
    of the alphabet backward, even though we are extremely familiar with each letter,
    and compose much more complex concepts with them, such as words, ideas, and even
    Keras code:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 改变处理顺序的概念是一个相当有趣的命题。我们人类显然似乎偏好某种特定的学习顺序。以下图片中复制的第二句话对我们来说根本没有意义，尽管我们确切知道句子中的每一个单词是什么意思。同样，许多人难以倒背字母表，尽管我们对每个字母都非常熟悉，并用它们构造出更加复杂的概念，如单词、想法，甚至是Keras代码：
- en: '![](img/1a56a47a-a34a-4dc3-9911-acdd412050a6.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a56a47a-a34a-4dc3-9911-acdd412050a6.png)'
- en: It is very likely that our sequential preferences have to do with the nature
    of our reality, which is sequential and forward-moving by definition. At the end
    of the day, the configuration of the 10^(11) neurons in our brain has been engineered
    by time and natural forces to best encode and represent the deluge of time-dependent
    sensory signals we come across, every living second of our lives. It stands to
    reason that our own neural architecture efficiently implements a mechanism that
    tends to prefer processing signals in a specific order. However, that is not to
    say that we cannot part ways with a learned order, as many pre-school kids take
    up the challenge of reciting the alphabet backward and do so quite successfully.
    Other sequential tasks such as listening to natural language or rhythmic music,
    however, may be harder to process in reverse order. But don't take my word for
    it. Try listening to your favorite song in reverse and see whether you still like
    it as much.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的顺序偏好很可能与我们现实的性质有关，现实本身就是顺序的，并且是前进的。归根结底，我们大脑中大约10^(11)个神经元的配置是由时间和自然力量精心设计的，以最佳方式编码和表示我们每一秒钟所接触到的时间相关的感官信号。可以推测，我们的大脑神经架构有效地实现了一个机制，倾向于按特定顺序处理信号。然而，这并不是说我们不能与已学的顺序分道扬镳，因为许多学前儿童会挑战背诵字母表的倒序，并且做得相当成功。其他顺序性任务，例如听自然语言或节奏感强的音乐，可能更难以倒序处理。但别光听我说，试试将你最喜欢的歌曲倒放，看看你是否还能像以前一样喜欢它。
- en: Benefits of re-ordering sequential data
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新排列顺序数据的好处
- en: 'In some manner, it seems that bi-directional networks are able to potentially
    overcome our own biases in processing information. As you will see, they can learn
    equally useful representations that we would have otherwise not thought to include
    to inform and enhance our predictions. It all depends on how important it is to
    process a given signal, in its sequential order, for the task at hand. In the
    case of our earlier natural language example, this was quite crucial to determine
    the **part-of-speech** (**POS**) tag for the word *Spartan*:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，双向网络似乎能够潜在地克服我们在处理信息时的偏见。正如你所看到的，它们能够学习那些我们原本没有想到要包括的有用表示，从而帮助并增强我们的预测。是否处理一个特定信号、以及如何处理，完全取决于该信号在当前任务中的重要性。在我们之前的自然语言例子中，这一点对于确定**词性**（**POS**）标签尤为重要，比如单词*Spartan*：
- en: '![](img/259ac156-568d-4eb2-938c-498308c58d7f.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/259ac156-568d-4eb2-938c-498308c58d7f.png)'
- en: Bi-directional layer in Keras
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的双向层
- en: Therefore, the bi-directional layer in Keras processes a sequence of data in
    both the normal and reverse sequence, which allows us to pick up on words that
    come later on in the sequence to inform our prediction at the current time.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Keras中的双向层同时按正常顺序和反向顺序处理数据序列，这使得我们能够利用序列后续的词汇信息来帮助当前时间点的预测。
- en: 'Essentially, the bi-directional layer duplicates any layer that''s fed to it
    and uses one copy to process information in the normal sequential order, while
    the other processes data in the reverse order. Pretty neat, no? We can intuitively
    visualize what a bi-directional layer actually does by going through a simple
    example. Suppose you were modeling the two-word sequence **Whats up**, with a
    bi-directional GRU:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，双向层会复制任何输入给它的层，并使用其中一个副本按正常顺序处理信息，而另一个副本则按相反顺序处理数据。是不是很酷？我们可以通过一个简单的例子直观地理解双向层究竟是如何工作的。假设你在用双向GRU模型处理两个单词的序列**Whats
    up**：
- en: '![](img/179d4b85-f770-4bd3-bfb2-bfeb4ba5e7b1.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/179d4b85-f770-4bd3-bfb2-bfeb4ba5e7b1.png)'
- en: 'To do this, you will nest the GRU in a bi-directional layer, which allows Keras
    to generates two versions of the bi-directional model. In the preceding image,
    we stacked two bi-directional layers on top of each other before connecting them
    to a dense output layer, as we did previously:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你需要将GRU嵌套在一个双向层中，这样Keras就能够生成双向模型的两个版本。在前面的图像中，我们将两个双向层叠加在一起，然后将它们连接到一个密集输出层，正如我们之前所做的那样：
- en: '[PRE14]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model that processes the sequence in the normal order is shown in red.
    Similarly, the blue model processes the same sequence in reverse order. Both of
    these models collaborate at each time step to produce a predicted output, with
    respect to the current time step. We can see how these two models receive input
    values and work together to produce the predicted output (![](img/5a07aa4f-cdcc-4d84-96af-8024ce385aa7.png),
    which corresponds to the two respective time steps of our input:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正常顺序处理序列的模型以红色表示。同样，蓝色模型按相反顺序处理相同的序列。这两个模型在每个时间步共同协作，生成针对当前时间步的预测输出。我们可以看到这两个模型是如何接收输入值并一起工作，生成预测输出（![](img/5a07aa4f-cdcc-4d84-96af-8024ce385aa7.png)，这对应我们输入的两个时间步）：
- en: '![](img/353bfcae-f247-43a6-a7bb-b9699b84c1fc.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/353bfcae-f247-43a6-a7bb-b9699b84c1fc.png)'
- en: 'The equations governing the forward propagation of information can be altered
    slightly to account for data entering our RNN, from both the forward and reverse
    sequence layers, at each time step. The backward propagation of errors through
    time is still achieved in the same manner and is done for each orientation of
    GRU layer (red and blue). In the following formulation, we can see how activations
    from both the forward and reverse sequence layers are used to compute a predicted
    output (![](img/5c108a8b-2fe1-40e2-a65a-75cbbc5f1ba6.png)) for a given time step
    (*t*):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 控制前向传播信息的方程可以稍作修改，以适应数据从正向和反向序列层进入RNN的情况，且每个时间步都会如此。误差的反向传播仍然以相同的方式进行，并针对每个GRU层的方向（红色和蓝色）进行处理。在以下的公式中，我们可以看到如何使用来自正向和反向序列层的激活值来计算给定时间步（*t*）的预测输出（![](img/5c108a8b-2fe1-40e2-a65a-75cbbc5f1ba6.png)）：
- en: '![](img/d977fa08-9d31-4a09-a9f0-43130a7c0a7c.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d977fa08-9d31-4a09-a9f0-43130a7c0a7c.png)'
- en: The activation and weight matrices here are simply defined by the model nested
    within the bi-directional layer. As we saw earlier, they will be initialized at
    the first time step and updated by backpropagating errors through time. Hence,
    these are the implemented processes that let us generate a bi-directional network,
    which is an acyclical network where predictions are informed by information flowing
    both forward and backward, corresponding to the ordering of the sequence. One
    key disadvantage with implementing the bi-directional layer is that our network
    needs to see the entire sequence of data before it is able to make a prediction.
    In use cases such as speech recognition, this becomes problematic, since we must
    ensure that the target has ceased to speak before we perform our predictions to
    classify each sound byte as a word. One way to solve this is to keep performing
    predictions on an input sequence iteratively, and update the previous prediction
    iteratively as new information flows in.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的激活和权重矩阵仅由模型在双向层内嵌套定义。正如我们之前看到的，它们将在第一次时间步初始化，并通过时间反向传播误差进行更新。因此，这些是实现双向网络的过程，双向网络是一种无环网络，预测信息由前向和后向流动的信息共同提供，这与序列的顺序相对应。实现双向层的一个关键缺点是，我们的网络需要在能够进行预测之前看到整个数据序列。在语音识别等用例中，这会成为问题，因为我们必须确保目标在进行预测之前已经停止说话，以便将每个声音字节分类为一个单词。解决这个问题的一种方法是对输入序列进行迭代预测，并在新的信息流入时，迭代更新之前的预测。
- en: Implementing recurrent dropout
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现递归丢弃
- en: In earlier chapters, we saw how we can drop out the prediction of a few neurons
    randomly to better distribute representations over our network and avoid the problem
    of overfitting. While our current task at hand does not have much of a negative
    consequence in regards to overfitting, we could not help but briefly introduce
    the specific case of mitigating overfitting in RNNs. This will help our model
    better generate novel sequences, instead of copy-pasting segments from the training
    data.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到我们可以随机丢弃一些神经元的预测，以便更好地分布我们网络中的表示，避免过拟合问题。虽然我们当前的任务在过拟合方面没有太大的负面影响，但我们还是简要介绍了在RNN中缓解过拟合的具体情况。这将帮助我们的模型更好地生成新的序列，而不是从训练数据中复制粘贴片段。
- en: 'However, adding a normal dropout layer here just doesn''t do the trick. It
    introduces too much randomness. This often prohibits our model from converging
    to ideal loss values and encoding useful representations. Instead, we may find
    a confused model that fails to keep track of relevant time-dependent data. What
    does seem to work, on the other hand, is the notion of applying the same dropout
    scheme (or mask) at each time step. This is different from the classic dropout
    operation, which drops neurons on a random basis for each time step. We can use
    this recurrent dropout technique to capture regularized representations, since
    a constant dropout mask is maintained through time. This is one of the most significant
    techniques that helps to prevent overfitting in recurrent layers and is known
    as a **recurrent dropout strategy**. Doing so essentially permits our model to
    representatively encode sequential data without losing valuable information via
    the randomized dropout process:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单纯地添加一个普通丢弃层并不能解决问题。它引入了过多的随机性。这通常会阻止我们的模型收敛到理想的损失值，并编码出有用的表示。另一方面，似乎有效的做法是，在每个时间步应用相同的丢弃方案（或掩码）。这与经典的丢弃操作不同，后者会在每个时间步随机丢弃神经元。我们可以使用这种递归丢弃技术来捕获正则化的表示，因为在时间上保持一个恒定的丢弃掩码。这是帮助防止递归层过拟合的最重要技术之一，通常被称为**递归丢弃策略**。这样做本质上允许我们的模型有代表性地编码顺序数据，而不会通过随机丢弃过程丢失宝贵的信息：
- en: '[PRE15]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The designers of Keras have kindly implemented two dropout-related arguments
    that may be passed when constructing a recurrent layer. The `recurrent_dropout`
    argument accepts a float value that refers to the fraction of neurons upon which
    the same dropout mask will be applied. You can also specify the fraction of input
    values entering a recurrent layer to be randomly dropped to control random noise
    in the data. This can be achieved by passing a float value to the dropout argument
    (different from `recurrent_dropout`), while defining the RNN layer.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的设计者们已经友好地实现了两个与dropout相关的参数，可以在构建递归层时传入。`recurrent_dropout`参数接受一个浮动值，表示同一dropout掩码将应用于神经元的比例。您还可以指定进入递归层的输入值的比例，以随机丢弃部分数据，从而控制数据中的随机噪声。这可以通过传递一个浮动值给dropout参数（与`recurrent_dropout`不同）来实现，同时定义RNN层。
- en: 'For reference you can read the following papers:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，您可以阅读以下论文：
- en: '**A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**: [https://arxiv.org/pdf/1512.05287](https://arxiv.org/pdf/1512.05287.pdf)[.pdf](https://arxiv.org/pdf/1512.05287.pdf)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归神经网络中dropout的理论基础应用**: [https://arxiv.org/pdf/1512.05287](https://arxiv.org/pdf/1512.05287.pdf)[.pdf](https://arxiv.org/pdf/1512.05287.pdf)'
- en: '[http://mlg.eng.cam.ac.uk/yarin/blog_2248.html](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://mlg.eng.cam.ac.uk/yarin/blog_2248.html](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)'
- en: Visualizing output values
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出值的可视化
- en: For the sake of entertainment, we will display some of the more interesting
    results from our own training experiments to conclude this chapter. The first
    screenshot shows the output that's generated by our SimpleRNN model at the end
    of the first epoch (note that the output prints out the first epoch as epoch 0).
    This is simply an implementational issue, denoting the first index position in
    range of *n* epochs. As we can see, even after the very first epoch, the SimpleRNN
    seems to have picked up on word morphology and generates real English words at
    low sampling thresholds.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为了娱乐，我们将展示一些来自我们自己训练实验中的更有趣的结果，作为本章的结尾。第一个截图显示了我们SimpleRNN模型在第一个训练周期结束时生成的输出（请注意，输出显示了第一个周期作为周期0）。这只是一个实现问题，表示在*n*个周期的范围内的第一个索引位置。如我们所见，即使在第一个周期之后，SimpleRNN似乎已经掌握了单词的形态学，并且在较低的采样阈值下生成了真实的英文单词。
- en: 'This is just as we expected. Similarly, higher entropy samples (with a threshold
    of 1.2, for example) produce more stochastic results and generate (from a subjective
    perspective) interesting sounding words (such as *eresdoin*, *harereus*, and *nimhte*):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们所预期的。同样，较高熵值的样本（例如阈值为1.2）会产生更多的随机结果，并生成（从主观角度来看）听起来有趣的单词（如*eresdoin*，*harereus*，和*nimhte*）：
- en: '![](img/76592355-55cd-4b81-ab60-f20f772b6127.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76592355-55cd-4b81-ab60-f20f772b6127.png)'
- en: Visualizing the output of heavier GRU models
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化较重GRU模型的输出
- en: 'In the following screenshot, we present the output from our heavier GRU model,
    which started to produce pretty Shakespeare-sounding strings only after two training
    epochs. It even throws in Hamlet''s name here and there. Note that the loss of
    your network is not the best assessment metric for the purpose of our illustration.
    The models that are shown here had a loss of 1.3, which is still pretty far from
    what we would normally require. You may, of course, keep training your model to
    produce even more comprehensible bits of Shakespeare. However, comparing the performance
    of any model with the loss metric is akin to judging apples and oranges in this
    use case. Intuitively, having reached a loss close to zero simply means that the
    model has memorized Shakespeare''s Hamlet, and won''t really generate novel sequences
    as we want it to do. At the end of the day, you shall remain the best judge of
    its performance for generative tasks such as this one:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，我们展示了我们较重的GRU模型的输出，该模型在经过两个训练周期后才开始生成类似莎士比亚的字符串。它甚至时不时地提到《哈姆雷特》的名字。请注意，网络的损失并不是我们示例中最好的评估指标。这里展示的模型的损失为1.3，这仍然远远低于我们通常要求的水平。当然，您可以继续训练您的模型，以生成更加易于理解的莎士比亚式片段。然而，在这个用例中，使用损失指标来比较任何模型的表现就像是在比较苹果和橘子。直观上，损失接近零仅意味着模型已经记住了莎士比亚的《哈姆雷特》，而不会像我们希望的那样生成新颖的序列。最终，您才是生成任务（如本任务）的最佳评判者：
- en: '![](img/e7fa7810-3b8f-4212-a42a-c23aae919ec4.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7fa7810-3b8f-4212-a42a-c23aae919ec4.png)'
- en: Summary
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about recurren6t neural networks and their aptness
    at processing sequential time-dependent data. The concepts that you have learned
    can now be applied to any time-series dataset that you may stumble upon. While
    this holds true for use cases such as stock market data and time-series in nature,
    it would be unreasonable to expect fantastic results from feeding your network
    real time price changes only. This is simply because the elements that affect
    the market price of stocks (such as investor perception, information networks,
    and available resources) are not nearly reflected to the level that would allow
    proper statistical modeling. The key is representing all relevant information
    in the most *learnable* manner possible for your network to successfully encode
    valuable representations therefrom.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了循环神经网络及其在处理序列时间相关数据方面的适用性。您学到的概念现在可以应用于您可能遇到的任何时间序列数据集。虽然这对股票市场数据和自然时间序列等用例确实有效，但指望仅通过输入网络实时价格变动来获得奇妙的结果是不合理的。这仅仅因为影响股票市场价格的因素（如投资者看法、信息网络和可用资源）远未达到足够的水平，以允许适当的统计建模。关键在于以最可学习的方式表达所有相关信息，以使您的网络能够成功编码其中的有价值表征。
- en: While we did extensively explore the learning mechanisms behind several types
    of RNNs, we also implemented a generative modeling use case in Keras and learned
    to construct custom callbacks that let us generate sequences of data at the end
    of each epoch. Due to spatial limitations, we were forced to leave out some concepts
    from this chapter on RNNs. However, rest assured, these are yet to be elaborated
    upon in the upcoming chapter.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们广泛探讨了几种类型的RNN背后的学习机制，我们还在Keras中实现了一个生成建模用例，并学习构建自定义回调函数，使我们能够在每个epoch结束时生成数据序列。由于空间限制，我们不得不在这一章节中遗漏了一些概念。然而，请放心，这些将在接下来的章节中详细阐述。
- en: In the following chapter, we will learn more about a very popular RNN architecture
    known as the **LSTM network**, and implement it for other exciting use cases.
    These networks are as versatile as RNNs get, and allow us to produce very detailed
    statistical models of languages for use cases such as speech and entity recognition,
    translation, and machine question-answering. For natural language understanding,
    LSTMs (and other RNNs) are often implemented by leveraging concepts such as word
    embeddings, which are dense word vectors that are capable of encoding their semantic
    meaning. LSTMs also tend to do much better at generating novel sequences such
    as pieces of music, but hopefully you will be able to listen for yourself. We
    will also explore the intuition behind attention models briefly and revisit this
    concept in more detail, in a later chapter.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更深入地了解一种非常流行的循环神经网络架构，称为**LSTM网络**，并将其实现到其他令人兴奋的用例中。这些网络像RNN一样多才多艺，使我们能够生成非常详细的语言统计模型，适用于语音和实体识别、翻译以及机器问答等场景。对于自然语言理解，LSTMs（以及其他RNNs）通常通过利用词嵌入等概念来实现，词嵌入是能够编码其语义含义的密集词向量。LSTMs在生成诸如音乐片段等新颖序列方面表现也更为出色，但愿您也能亲自聆听。我们还将简要探讨注意力模型背后的直觉，并在后续章节中更详细地重新审视这一概念。
- en: Finally, before concluding this chapter, we will note a similarity between RNNs
    and a type of CNN we mentioned in earlier chapters. RNNs are a popular choice
    when modeling time series data, yet **one-dimensional convolutional layers** (**Conv1D**)
    also do the trick. The drawback here comes from the fact that CNNs process input
    values independently, not sequentially. As we will see, we can even overcome this
    by combining both convolutional and recurrent layers. This lets the former perform
    a sort of preprocessing on the input sequence before reduced representations are
    passed forward to the RNN layer for sequential processing. But more on that later.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在结束本章之前，我们将注意到RNNs与我们在前几章中提到的一种CNN类型之间的相似性。当建模时间序列数据时，RNNs是一个流行的选择，但**一维卷积层**（**Conv1D**）也能胜任。这里的缺点来自于CNN处理输入值时的独立性，而不是顺序性。正如我们将看到的那样，我们甚至可以通过结合卷积和循环层来克服这一点。这使得前者能够在将减少的表示传递到RNN层进行顺序处理之前对输入序列进行某种预处理。但稍后会详细讨论这一点。
- en: Further reading
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '**GRUs**: [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRUs**: [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)'
- en: '**Neural machine translation**: [https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经机器翻译**: [https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259)'
- en: Exercise
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Train each model on the Hamlet text and use their history objects to compare
    their relative losses. Which one converges faster? What do they learn?
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在《哈姆雷特》文本上训练每个模型，并使用它们的历史对象比较它们的相对损失。哪个模型收敛得更快？它们学到了什么？
- en: Examine the samples that are generated at different entropy distributions, at
    each epoch, to see how each RNN improves upon its language model through time.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查在不同熵分布下生成的样本，并在每个训练轮次中观察每个RNN如何随着时间推移改进其语言模型。
