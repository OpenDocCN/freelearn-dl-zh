- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: The Attention Mechanism and Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制与Transformer
- en: In [*Chapter 6*](B19627_06.xhtml#_idTextAnchor185), we outlined a typical **natural
    language processing** (**NLP**) pipeline, and we introduced **recurrent neural
    networks** (**RNNs**) as a candidate architecture for NLP tasks. But we also outlined
    their drawbacks—they are inherently sequential (that is, not parallelizable) and
    cannot process longer sequences, because of the limitations of their internal
    sequence representation. In this chapter, we’ll introduce the **attention mechanism**,
    which allows a **neural network** (**NN**) to have direct access to the whole
    input sequence. We’ll briefly discuss the attention mechanism in the context of
    RNNs since it was first introduced as an RNN extension. However, the star of this
    chapter will be the **transformer**—a recent NN architecture that relies entirely
    on attention. Transformers have been one of the most important NN innovations
    in the past 10 years. They are at the core of all recent **large language models**
    (**LLMs**), such as ChatGPT ([https://chat.openai.com/](https://chat.openai.com/)),
    and even image generation models such as Stable Diffusion ([https://stability.ai/stable-diffusion](https://stability.ai/stable-diffusion)).
    This is the second chapter in our arc dedicated to NLP and the first of three
    chapters dedicated to transformers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B19627_06.xhtml#_idTextAnchor185)中，我们概述了一个典型的**自然语言处理**（**NLP**）流程，并介绍了**递归神经网络**（**RNNs**）作为NLP任务的候选架构。但我们也概述了它们的缺点——它们本质上是顺序的（即不可并行化），并且由于其内部序列表示的局限性，无法处理更长的序列。在本章中，我们将介绍**注意力机制**，它使**神经网络**（**NN**）可以直接访问整个输入序列。我们将简要讨论RNN中的注意力机制，因为它最初是作为RNN的扩展引入的。然而，本章的主角将是**Transformer**——一种完全依赖于注意力的最新神经网络架构。Transformer在过去10年中成为最重要的神经网络创新之一。它们是所有近期**大型语言模型**（**LLMs**）的核心，例如ChatGPT（[https://chat.openai.com/](https://chat.openai.com/)），甚至是图像生成模型，如Stable
    Diffusion（[https://stability.ai/stable-diffusion](https://stability.ai/stable-diffusion)）。这是我们专注于NLP的章节中的第二章，也是三章中专门讨论Transformer的第一章。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing **sequence-to-sequence** (**seq2seq**) models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍**序列到序列**（**seq2seq**）模型
- en: Understanding the attention mechanism
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解注意力机制
- en: Building transformers with attention
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用注意力机制构建 Transformer
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and the
    Hugging Face Transformers library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    If you don’t have an environment set up with these tools, fret not—the example
    is available as a Jupyter notebook on Google Colab. You can find the code examples
    in the book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter07](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter07).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python、PyTorch和Hugging Face Transformers库（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）来实现本章的示例。如果你还没有配置这些工具的环境，不用担心——该示例作为Jupyter笔记本在Google
    Colab上提供。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter07](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter07)。
- en: Introducing seq2seq models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍seq2seq模型
- en: In [*Chapter 6*](B19627_06.xhtml#_idTextAnchor185), we outlined several types
    of recurrent models, depending on the input/output combinations. One of them is
    indirect many-to-many, or **seq2seq**, where an input sequence is transformed
    into another, different output sequence, not necessarily with the same length
    as the input. One type of seq2seq task is machine translation. The input sequences
    are the words of a sentence in one language, and the output sequences are the
    words of the same sentence translated into another language. For example, we can
    translate the English sequence *tourist attraction* to the German *Touristenattraktion*.
    Not only is the output of a different length but there is no direct correspondence
    between the elements of the input and output sequences. One output element corresponds
    to a combination of two input elements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第6章*](B19627_06.xhtml#_idTextAnchor185) 中，我们概述了几种类型的递归模型，取决于输入/输出的组合。其中一种是间接的多对多任务，或
    **seq2seq**，其中一个输入序列被转换成另一个不同的输出序列，输出序列的长度不一定与输入序列相同。seq2seq 任务的一种类型是机器翻译。输入序列是一个语言中的句子的单词，而输出序列是同一句子翻译成另一种语言的单词。例如，我们可以将英语序列
    *tourist attraction* 翻译成德语 *Touristenattraktion*。输出不仅长度不同，而且输入和输出序列的元素之间没有直接对应关系。一个输出元素对应于两个输入元素的组合。
- en: Another type of indirect many-to-many task is conversational chatbots such as
    ChatGPT, where the initial input sequence is the first user query. After that,
    the whole conversation so far (including both user queries and bot responses)
    serves as an input sequence for the newly generated bot responses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种间接的多对多任务是会话聊天机器人，例如 ChatGPT，其中初始输入序列是用户的第一个查询。之后，整个对话（包括用户的查询和机器人的回复）都作为新生成的机器人的回复的输入序列。
- en: 'In this section, we’ll focus on encoder-decoder seq2seq models (*Sequence to
    Sequence Learning with Neural Networks*, [https://arxiv.org/abs/1409.3215;](https://arxiv.org/abs/1409.3215;)
    *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine
    Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)),
    first introduced in 2014\. They use RNNs in a way that’s especially suited for
    solving indirect many-to-many tasks such as these. The following is a diagram
    of the seq2seq model, where an input sequence `[A, B, C, <EOS>]` is decoded into
    an output sequence `[W, X, Y,` `Z, <EOS>]`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将重点介绍编码器-解码器的 seq2seq 模型（*使用神经网络的序列到序列学习*，[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215);
    *使用 RNN 编码器-解码器进行统计机器翻译的短语表示学习*，[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)），该模型首次于2014年提出。它们使用
    RNN 的方式特别适用于解决间接的多对多任务，例如这些任务。以下是 seq2seq 模型的示意图，其中输入序列 `[A, B, C, <EOS>]` 被解码为输出序列
    `[W, X, Y, Z, <EOS>]`：
- en: '![Figure 7.1 – A seq2seq model (inspired by https://arxiv.org/abs/1409.3215)](img/B19627_07_1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 一个 seq2seq 模型（灵感来源于 https://arxiv.org/abs/1409.3215）](img/B19627_07_1.jpg)'
- en: Figure 7.1 – A seq2seq model (inspired by [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 一个 seq2seq 模型（灵感来源于 [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)）
- en: 'The model consists of two parts:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由两部分组成：
- en: '`<EOS>`—end-of-sequence—token is reached. Let’s assume that the input is a
    textual sequence using word-level tokenization. Then, we’ll use word-embedding
    vectors as the encoder input at each step, and the `<EOS>` token signals the end
    of a sentence. The encoder output is discarded and has no role in the seq2seq
    model, as we’re only interested in the hidden encoder state.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<EOS>`—序列结束—标记已到达。假设输入是使用词级别标记化的文本序列。那么，我们将在每一步使用词嵌入向量作为编码器输入，`<EOS>` 标记表示句子的结束。编码器的输出会被丢弃，在
    seq2seq 模型中没有作用，因为我们只关心隐藏的编码器状态。'
- en: '`<GO>` input signal. The encoder is also an RNN (LSTM or GRU). The link between
    the encoder and the decoder is the most recent encoder’s internal state vector,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/550.png)
    (also known as the `<EOS>` becomes the most probable symbol, the decoding is finished.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<GO>` 输入信号。编码器也是一个 RNN（LSTM 或 GRU）。编码器和解码器之间的联系是编码器最新的内部状态向量， ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/550.png)（也称为
    `<EOS>`，成为最可能的符号，解码完成）。'
- en: An example of an autoregressive model
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型的示例
- en: 'Let’s assume that we want to translate the English sentence *How are you today?*
    into Spanish. We’ll tokenize it as `[how, are, you, today, ?, <EOS>]`. An autoregressive
    model will start with an initial sequence `[<GO>]`. Then, it will generate the
    first word of the translation and will append it to the existing input sequence:
    `[<GO>, ¿]`. The new sequence will serve as new input to the decoder, so it can
    produce the next element and extend the sequence again: `[<GO>, ¿, cómo]`. We’ll
    repeat the same steps until the decoder predicts the `<EOS>` token: `[<GO>, ¿,
    cómo, estás, hoy, ?, <``EOS>]`.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想将英语句子*How are you today?*翻译成西班牙语。我们将其标记为`[how, are, you, today, ?, <EOS>]`。一个自回归模型将从初始序列`[<GO>]`开始。然后，它会生成翻译的第一个词，并将其附加到现有的输入序列中：`[<GO>,
    ¿]`。新的序列将作为解码器的输入，以便生成下一个元素并再次扩展序列：`[<GO>, ¿, cómo]`。我们将重复相同的步骤，直到解码器预测出`<EOS>`标记：`[<GO>,
    ¿, cómo, estás, hoy, ?, <EOS>]`。
- en: 'The training of the model is supervised, as it needs to know both the input
    sequence and its corresponding target output sequence (for example, the same text
    in multiple languages). We feed the input sequence to the encoder, generate the
    thought vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/550.png),
    and use it to initiate the output sequence generation from the decoder. Training
    the decoder uses a process called `[W, X, Y]`, but the current decoder-generated
    output sequence is `[W, X, Z]`. With teacher forcing, the decoder input at step
    *t+1* will be *Y* instead of *Z*. In other words, the decoder learns to generate
    target values `[t+1,...]` given target values `[...,t]`. We can think of this
    in the following way: the decoder input is the target sequence, while its output
    (target values) is the same sequence but shifted one position to the right.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的训练是有监督的，因为它需要知道输入序列及其对应的目标输出序列（例如，多个语言中的相同文本）。我们将输入序列送入编码器，生成思维向量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/550.png)，并利用它启动解码器的输出序列生成。训练解码器使用一种叫做`[W,
    X, Y]`的过程，但当前解码器生成的输出序列是`[W, X, Z]`。通过教师强制法，在步骤*t+1*时，解码器的输入将是*Y*而不是*Z*。换句话说，解码器学习在给定目标值`[...,t]`的情况下生成目标值`[t+1,...]`。我们可以这样理解：解码器的输入是目标序列，而其输出（目标值）是同一序列，但向右移动了一个位置。
- en: 'To summarize, the seq2seq model solves the problem of varying input/output
    sequence lengths by encoding the input sequence into a fixed-length state vector,
    **v**, and then using this vector as a base to generate the output sequence. We
    can formalize this by saying that it tries to maximize the following probability:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，seq2seq模型通过将输入序列编码为固定长度的状态向量**v**，然后使用这个向量作为基础来生成输出序列，从而解决了输入/输出序列长度变化的问题。我们可以通过以下方式形式化这一过程：它尝试最大化以下概率：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munderover><mml:mrow><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/561.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munderover><mml:mrow><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/561.png)'
- en: 'This is equivalent to the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于以下表达式：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/562.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/562.png)'
- en: 'Let’s look at the elements of this formula in more detail:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这个公式的各个元素：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/563.png):
    The conditional probability where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/564.png)
    is the input sequence with length *T* and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/565.png)
    is the output sequence with length *T’*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/563.png)：条件概率，其中![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/564.png)
    是长度为*T*的输入序列，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfenced](img/565.png)
    是长度为*T’*的输出序列。'
- en: '**v**: The fixed-length encoding of the input sequence (the thought vector)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**v**：输入序列的固定长度编码（思维向量）。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>‘</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/566.png):
    The probability of an output word ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math>](img/567.png)
    given prior words *y*, as well as the thought vector, **v**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>‘</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/566.png):
    给定先前的词 *y* 以及思想向量 **v**，输出词 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math>](img/567.png)
    的概率。'
- en: The original seq2seq paper introduces a few tricks to enhance the training and
    performance of the model. For example, the encoder and decoder are two separate
    LSTMs. In the case of machine translations, this makes it possible to train different
    decoders for different languages with the same encoder.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 seq2seq 论文介绍了一些技巧，用于增强模型的训练和性能。例如，编码器和解码器是两个独立的 LSTM。在机器翻译的情况下，这使得可以使用相同的编码器为不同语言训练不同的解码器。
- en: 'Another improvement is that the input sequence is fed to the decoder in reverse.
    For example, `[A,B,C]` -> `[W,X,Y,Z]` would become `[C,B,A]` -> `[W,X,Y,Z]`. There
    is no clear explanation of why this works, but the authors have shared their intuition:
    since this is a step-by-step model, if the sequences were in normal order, each
    source word in the source sentence would be far from its corresponding word in
    the output sentence. If we reverse the input sequence, the average distance between
    input/output words won’t change, but the first input words will be very close
    to the first output words. This will help the model to establish better communication
    between the input and output sequences. However, this improvement also illustrates
    the deficiencies of the hidden state of RNNs (even LSTM or GRU)—the more recent
    sequence elements suppress the available information for the older elements. In
    the next section, we’ll introduce an elegant way to solve this issue once and
    for all.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进是输入序列以反向方式输入到解码器。例如，`[A,B,C]` -> `[W,X,Y,Z]` 将变成 `[C,B,A]` -> `[W,X,Y,Z]`。没有明确的解释说明为什么这样有效，但作者分享了他们的直觉：由于这是一个逐步模型，如果序列按正常顺序排列，源句子中的每个源词将远离其在输出句子中的对应词。如果我们反转输入序列，输入/输出词之间的平均距离不会改变，但第一个输入词会非常接近第一个输出词。这有助于模型在输入和输出序列之间建立更好的通信。然而，这一改进也展示了
    RNN（即使是 LSTM 或 GRU）隐藏状态的不足——较新的序列元素会抑制较老元素的可用信息。在下一节中，我们将介绍一种优雅的方式来彻底解决这个问题。
- en: Understanding the attention mechanism
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解注意力机制
- en: In this section, we’ll discuss several iterations of the attention mechanism
    in the order that they were introduced.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将按引入的顺序讨论注意力机制的几个迭代版本。
- en: Bahdanau attention
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bahdanau 注意力
- en: 'The first attention iteration (*Neural Machine Translation by Jointly Learning
    to Align and Translate*, [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)),
    known as **Bahdanau** attention, extends the seq2seq model with the ability for
    the decoder to work with all encoder hidden states, not just the last one. It
    is an addition to the existing seq2seq model, rather than an independent entity.
    The following diagram shows how Bahdanau attention works:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次注意力迭代（*Neural Machine Translation by Jointly Learning to Align and Translate*，
    [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)），被称为**巴赫达瑙**注意力，扩展了
    seq2seq 模型，使解码器能够与所有编码器隐藏状态进行交互，而不仅仅是最后一个状态。它是对现有 seq2seq 模型的补充，而不是一个独立的实体。下图展示了巴赫达瑙注意力的工作原理：
- en: '![Figure 7.2 – The attention mechanism](img/B19627_07_2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 注意力机制](img/B19627_07_2.png)'
- en: Figure 7.2 – The attention mechanism
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 注意力机制
- en: 'Don’t worry—it looks scarier than it is. We’ll go through this diagram from
    top to bottom: the attention mechanism works by plugging an additional **context
    vector**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/568.png),
    between the encoder and the decoder. The hidden decoder state ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/569.png)
    at time *t* is now a function not only of the hidden state and decoder output
    at step *t-1* but also of the context vector ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——它看起来比实际更复杂。我们将从上到下解析这个图：注意力机制通过在编码器和解码器之间插入一个额外的**上下文向量**，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/568.png)，来实现。隐藏的解码器状态
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/569.png)
    在时间 *t* 上，现在不仅是隐藏状态和解码器输出的函数，还包含上下文向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/571.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/571.png)'
- en: 'Each decoder step has a unique context vector, and the context vector for one
    decoder step is just *a weighted sum of all encoder hidden states*. In this way,
    the encoder can access all input sequence states at each output step *t*, which
    removes the necessity to encode all information of the source sequence into a
    fixed-length thought vector, as the regular seq2seq model does:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解码步骤都有一个独特的上下文向量，而一个解码步骤的上下文向量只是*所有编码器隐藏状态的加权和*。通过这种方式，编码器可以在每个输出步骤 *t* 中访问所有输入序列状态，这消除了像常规
    seq2seq 模型那样必须将源序列的所有信息编码为一个固定长度的思维向量的必要性：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/572.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/572.png)'
- en: 'Let’s discuss this formula in more detail:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这个公式：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png):
    The context vector for a decoder output step *t* out of *T’* total output steps'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)：解码器输出步骤*t*（总共有*T’*个输出步骤）的上下文向量'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png):
    The hidden state vector of encoder step *i* out of *T* total input steps'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png)：编码器步骤*i*（总共有*T*个输入步骤）的隐藏状态向量'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png):
    The scalar weight associated with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png)
    in the context of the current decoder step *t*'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png)：与当前解码器步骤*t*中![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png)相关的标量权重'
- en: Note that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/577.png)
    is unique for both the encoder and decoder steps—that is, the input sequence states
    will have different weights depending on the current output step. For example,
    if the input and output sequences have lengths of 10, then the weights will be
    represented by a 10×10 matrix for a total of 100 weights. This means that the
    attention mechanism will focus the attention (get it?) of the decoder on different
    parts of the input sequence, depending on the current state of the output sequence.
    If ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png)
    is large, then the decoder will pay a lot of attention to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png)
    at step *t*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/577.png)
    对于编码器和解码器步骤都是唯一的——也就是说，输入序列的状态将根据当前的输出步骤具有不同的权重。例如，如果输入和输出序列的长度为 10，那么权重将由一个 10×10
    的矩阵表示，共有 100 个权重。这意味着注意力机制将根据输出序列的当前状态，将解码器的注意力集中在输入序列的不同部分。如果![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png)
    很大，那么解码器将非常关注在步骤 *t* 时的![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png)。
- en: 'But how do we compute the weights ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png)?
    First, we should mention that the sum of all ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/581.png)
    weights for a decoder at step *t* is 1\. We can implement this with a softmax
    operation on top of the attention mechanism:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何计算权重![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png)
    呢？首先，我们需要提到，对于解码器的每个步骤 *t*，所有![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/581.png)
    权重的总和为 1。我们可以通过在注意力机制之上执行 softmax 操作来实现这一点：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/582.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/582.png)'
- en: 'Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/583.png)
    is an alignment score, which indicates how well the input sequence elements around
    position *i* match (or align with) the output at position *t*. This score (represented
    by the weight ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/584.png))
    is based on the previous decoder state ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/585.png)
    (we use ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/585.png)
    because we have not computed ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/587.png)
    yet), as well as the encoder state ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/583.png)
    是一个对齐分数，表示输入序列中位置 *i* 附近的元素与位置 *t* 的输出匹配（或对齐）的程度。这个分数（由权重 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/584.png))
    基于前一个解码器状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/585.png)（我们使用
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/585.png)，因为我们还没有计算
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/587.png)），以及编码器状态
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png)：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/589.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/589.png)'
- en: 'Here, *a* (not alpha) is a differentiable function, which is trained with backpropagation
    together with the rest of the system. Different functions satisfy these requirements,
    but the authors of the paper chose the so-called **additive attention**, which
    combines ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/591.png)
    with the help of vector addition. It exists in two flavors:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*a*（而非 alpha）是一个可微分函数，通过反向传播与系统的其他部分一起训练。不同的函数满足这些要求，但论文的作者选择了所谓的**加性注意力**，它通过向量加法将![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/591.png)结合起来。它有两种变体：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mfenced open="["
    close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/592.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mfenced open="["
    close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/592.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/593.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/593.png)'
- en: In the first formula, **W** is a weight matrix, applied over the concatenated
    vectors ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png),
    and **v** is a weight vector. The second formula is similar, but this time we
    have separate **fully connected** (**FC**) layers (the weight matrices ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/596.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/597.png))
    and we sum ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/574.png).
    In both cases, the alignment model can be represented as a simple **feedforward
    network** (**FFN**) with one hidden layer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个公式中，**W** 是一个权重矩阵，应用于连接向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml><mml:mo>-</mml:mo><mml:mn>1</mml></mml:mrow></mml:msub></mml:math>](img/457.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml></mml:mrow></mml:msub></mml:math>](img/574.png)，而
    **v** 是一个权重向量。第二个公式类似，但这次我们有单独的 **全连接** (**FC**) 层（权重矩阵 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml></mml:mrow></mml:msub></mml:math>](img/596.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml></mml:mrow></mml:msub></mml:math>](img/597.png))，然后我们对
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml><mml:mo>-</mml:mo><mml:mn>1</mml></mml:mrow></mml:msub></mml:math>](img/457.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml></mml:mrow></mml:msub></mml:math>](img/574.png)
    求和。在这两种情况下，对齐模型可以表示为一个简单的 **前馈网络** (**FFN**)，带有一个隐藏层。
- en: 'Now that we know the formulas for ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/575.png),
    let’s replace the latter with the former:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml></mml:mrow></mml:msub></mml:math>](img/570.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml><mml:mo>,</mml:mo><mml:mi>i</mml></mml:mrow></mml:msub></mml:math>](img/575.png)
    的公式，让我们用前者替换后者：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/602.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/602.png)'
- en: 'As a conclusion, let’s summarize the attention algorithm in a step-by-step
    manner as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为总结，下面是一步一步总结的注意力算法：
- en: Feed the encoder with the input sequence and compute the set of hidden states,
    H = {h 1, h 2…h T}.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入序列输入编码器，并计算一组隐藏状态，H = {h 1, h 2…h T}。
- en: Compute the alignment scores, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/603.png),
    that use the decoder state from the preceding step ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png).
    If *t=1*, we’ll use the last encoder state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/605.png),
    as the initial hidden state.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算对齐分数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/603.png)，该对齐分数使用来自前一步解码器状态的值![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)。如果*t=1*，我们将使用最后一个编码器状态，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math>](img/605.png)，作为初始隐藏状态。
- en: Compute the weights ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/606.png).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算权重![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/606.png)。
- en: Compute the context vector ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/607.png).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算上下文向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/607.png)。
- en: Compute the hidden state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>RNN</mml:mtext></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/608.png),
    based on the concatenated vectors ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/609.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)
    and the previous decoder output ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/611.png).
    At this point, we can compute the final output ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/612.png).
    In the case where we need to classify the next word, we’ll use the softmax output,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/613.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/614.png),
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math>](img/615.png)
    is a weight matrix.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算隐藏状态，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>RNN</mml:mtext></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/608.png)，基于连接的向量
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/609.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)
    以及先前的解码器输出 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/611.png)。此时，我们可以计算最终输出
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/612.png)。如果我们需要对下一个单词进行分类，将使用
    softmax 输出，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/613.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/614.png)，其中
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math>](img/615.png)
    是一个权重矩阵。
- en: Repeat *steps 2* to *5* until the end of the sequence.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 5*，直到序列结束。
- en: Next, we’ll discuss a slightly improved version of Bahdanau attention.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论 Bahdanau 注意力的一个稍微改进的版本。
- en: Luong attention
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Luong 注意力
- en: '**Luong attention** (*Effective Approaches to Attention-based Neural Machine
    Translation*, [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025))
    introduces several improvements over Bahdanau attention. Most notably, the alignment
    scores depend on the decoder’s hidden state ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png),
    as opposed to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/617.png)
    in Bahdanau attention. To better understand this, let’s compare the two algorithms:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Luong 注意力** (*Effective Approaches to Attention-based Neural Machine Translation*,
    [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025)) 相比 Bahdanau
    注意力做出了若干改进。最显著的变化是对齐分数依赖于解码器的隐藏状态 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)，而不像
    Bahdanau 注意力中的 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/617.png)。为了更好地理解这一点，我们来比较这两种算法：'
- en: '![Figure 7.3 – Left: Bahdanau attention; right: Luong attention](img/B19627_07_3.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 左：Bahdanau 注意力；右：Luong 注意力](img/B19627_07_3.jpg)'
- en: 'Figure 7.3 – Left: Bahdanau attention; right: Luong attention'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 左：Bahdanau 注意力；右：Luong 注意力
- en: 'We’ll go through a step-by-step execution of Luong attention:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步执行 Luong 注意力的过程：
- en: Feed the encoder with the input sequence and compute the set of encoder hidden
    states ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/618.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/619.png).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入序列传入编码器，并计算编码器的隐藏状态集 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/618.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/619.png)。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>RN</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>decoder</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/620.png):
    Compute the decoder’s hidden state based on the previous decoder’s hidden state
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)
    and the previous decoder’s output ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/611.png)
    (not the context vector, though).'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>RN</mml:mtext><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mrow><mml:mtext>decoder</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/620.png):
    计算解码器的隐藏状态，基于上一个解码器的隐藏状态！[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png)
    和上一个解码器的输出！[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/611.png)（但不是上下文向量）。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/623.png):
    Compute the alignment scores, which use the decoder state from the current step,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/624.png).
    Besides additive attention, the Luong attention paper also proposes two types
    of **multiplicative attention**:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/623.png):
    计算对齐分数，使用当前步骤的解码器状态！[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/624.png)。除了加性注意力之外，Luong
    注意力论文还提出了两种**乘性注意力**：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/625.png):
    **Dot product** without any parameters. In this case, the vectors **s** and **h**
    (represented as column and row matrices) need to have the same sizes.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/625.png):
    **点积**没有任何参数。在这种情况下，向量**s**和**h**（表示为列矩阵和行矩阵）需要具有相同的大小。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/626.png):
    Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/627.png)
    is a trainable weight matrix of the attention layer.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/626.png):
    这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/627.png)
    是注意力层的可训练权重矩阵。'
- en: The multiplication of the vectors as an alignment score measurement has an intuitive
    explanation—as we mentioned in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047),
    the dot product acts as a similarity measure between vectors. Therefore, if the
    vectors are similar (that is, aligned), the result of the multiplication will
    be a large value and the attention will be focused on the current *t,i* relationship.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将向量相乘作为对齐评分的度量有一个直观的解释——正如我们在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中提到的，点积作为向量之间相似性的度量。因此，如果向量相似（即对齐），那么乘积的结果将是一个较大的值，注意力将集中在当前的*t,i*关系上。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/606.png):
    Compute the weights.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/606.png):
    计算权重。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/607.png):
    Compute the context vector.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/607.png):
    计算上下文向量。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/630.png):
    Compute the intermediate vector based on the concatenated vectors ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png).
    At this point, we can compute the final output ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/612.png).
    In the case of classification, we’ll use softmax, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:math>](img/634.png),
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math>](img/615.png)
    is a weight matrix.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/630.png):
    根据连接的向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/570.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    计算中间向量。此时，我们可以计算最终输出 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/612.png)。在分类的情况下，我们将使用
    softmax，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:math>](img/634.png)，其中
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math>](img/615.png)
    是一个权重矩阵。'
- en: Repeat *steps 2* to *6* until the end of the sequence.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*到*步骤6*直到序列结束。
- en: Next, we’ll use Bahdanau and Luong attention as a stepping stone to a generic
    attention mechanism.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用巴赫达努（Bahdanau）和Luong注意力作为通用注意力机制的垫脚石。
- en: General attention
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用注意力
- en: 'Although we’ve discussed the attention mechanism in the context of seq2seq
    with RNNs, it is a general **deep learning** (**DL**) technique in its own right.
    To understand it, let’s start with the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在使用RNN的seq2seq背景下讨论了注意力机制，但它本身就是一种通用的**深度学习**（**DL**）技术。为了理解它，我们从以下图示开始：
- en: '![Figure 7.4 – General attention](img/B19627_07_4.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 通用注意力](img/B19627_07_4.jpg)'
- en: Figure 7.4 – General attention
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 通用注意力
- en: It starts with a query, **q**, executed against a database of key-value pairs,
    **k** and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:msub></mml:math>](img/636.png),
    respectively. Each key, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/637.png),
    has a single corresponding value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/638.png).
    The query, keys, and values are vectors. Because of this, we can represent the
    key-value store as two matrices, **K** and **V**. If we have multiple queries,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/639.png),
    we can also represent them as a matrix, **Q**. Hence, these are often abbreviated
    as **Q**, **K**, and **V**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它从一个查询**q**开始，执行对一组键值对**k**和![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">k</mml:mi></mml:mrow></mml:msub></mml:math>](img/636.png)的查询。每个键![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/637.png)有一个对应的值![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/638.png)。查询、键和值都是向量。因此，我们可以将键值存储表示为两个矩阵**K**和**V**。如果有多个查询![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/639.png)，我们也可以将它们表示为一个矩阵**Q**。因此，这些通常被简写为**Q**、**K**和**V**。
- en: Differences between general attention and Bahdanau/Luong attention
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通用注意力与巴赫达努/Luong注意力的区别
- en: Unlike general attention, the keys **K** and the values **V** of Bahdanau and
    Luong attention are the same thing—that is, these attention models are more like
    **Q**/**V**, rather than **Q**/**K**/**V**. Having separate keys and values provides
    more flexibility to the general attention—the keys specialize in matching the
    input queries, and the values carry the actual information. We can think of the
    Bahdanau vector ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/640.png)
    (or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/641.png)
    in Luong attention) as the query, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/642.png),
    executed against the database of key-value pairs, where the keys/values are the
    hidden states ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/643.png).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与一般的注意力机制不同，Bahdanau 和 Luong 注意力的键 **K** 和值 **V** 是相同的——也就是说，这些注意力模型更像是 **Q**/**V**，而不是
    **Q**/**K**/**V**。分开键和值为一般的注意力机制提供了更多的灵活性——键专注于匹配输入查询，而值则携带实际的信息。我们可以把 Bahdanau
    向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/640.png)（或
    Luong 注意力中的 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/641.png)）视为查询，![
- en: 'General attention uses a multiplicative, rather than additive, mechanism (like
    Luong attention). Here’s how it works:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的注意力使用的是乘法机制，而非加法机制（像 Luong 注意力一样）。以下是它的工作原理：
- en: The starting point is one of the input query vectors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/644.png).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 起始点是其中一个输入查询向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/644.png)。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/645.png):
    Compute the alignment scores, using the dot product between the query, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/646.png),
    and each key vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/647.png).
    As we mentioned in the *Bahdanau attention* section, the dot product acts as a
    similarity measure, and it makes sense to use it on this occasion.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/645.png):
    使用查询向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/646.png)
    和每个关键向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/647.png)
    计算对齐分数。正如我们在*巴达瑙注意力*部分提到的那样，点积充当相似度度量，并且在这种情况下使用它是有意义的。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math>](img/648.png):
    Compute the final weights of each value vector against the query with the help
    of softmax.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math>](img/648.png)：借助
    softmax 计算每个值向量相对于查询的最终权重。'
- en: 'The final attention vector is the weighted addition (that is, an element-wise
    sum) of all value vectors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/649.png):'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的注意力向量是所有值向量的加权和（即元素级别的求和），![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/649.png)：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Attention</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/650.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>注意力</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/650.png)'
- en: 'To better understand the attention mechanism, we’ll use the numerical example
    displayed in the following diagram:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解注意力机制，我们将使用以下图表中显示的数值示例：
- en: '![Figure 7.5 – An attention example with a four-dimensional query executed
    against a key-value store with four vectors](img/B19627_07_5.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 使用四维查询在包含四个向量的键值存储中执行的注意力示例](img/B19627_07_5.jpg)'
- en: Figure 7.5 – An attention example with a four-dimensional query executed against
    a key-value store with four vectors
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 使用四维查询在包含四个向量的键值存储中执行的注意力示例
- en: 'Let’s track it step by step:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地跟踪它：
- en: Execute a four-dimensional query vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0.6,1.2</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1.2,1.8</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/651.png),
    against a key-value store of four four-dimensional vectors.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一个四维查询向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>0.6,1.2</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1.2,1.8</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/651.png)，对一个包含四个四维向量的键值存储进行查询。
- en: "Compute the alignment scores. For example, the first score is ![<math xmlns=\"\
    http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>e</mi><mrow><msub><mi\
    \ mathvariant=\"bold\">q</mi><mn>1</mn></msub><mo>,</mo><msub><mi mathvariant=\"\
    bold\">k</mi><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.6</mn><mo>×</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>0.2</mn></mrow></mfenced><mo>+</mo><mn>1.2</mn><mo>×</mo><mn>0.4</mn><mo>+</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>1.2</mn></mrow></mfenced><mo>×</mo><mn>1.2</mn><mo>+</mo><mn>1.8</mn><mo>×</mo><mn>0.8</mn><mo>=</mo><mn>0</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>.</mo><mn>36</mn></mrow></mrow></math>](img/652.png)![<math\
    \ xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>e</mi><mrow><msub><mi\
    \ mathvariant=\"bold\">q</mi><mn>1</mn></msub><mo>,</mo><msub><mi mathvariant=\"\
    bold\">k</mi><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.6</mn><mo>×</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>0.2</mn></mrow></mfenced><mo>+</mo><mn>1.2</mn><mo>×</mo><mn>0.4</mn><mo>+</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>1.2</mn></mrow></mfenced><mo>×</mo><mn>1.2</mn><mo>+</mo><mn>1.8</mn><mo>×</mo><mn>0.8</mn><mo>=</mo><mn>0</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>.</mo><mn>36</mn></mrow></mrow></math>](img/653.png).\
    \ The rest of the scores are displayed in *Figure 7**.5*. We have intentionally\
    \ selected the query, ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"\
    \ xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced\
    \ open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mn>0.6,1.2</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1.2,1.8</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/654.png),\
    \ to be relatively similar to the second key vector, ![<mml:math xmlns:mml=\"\
    http://www.w3.org/1998/Math/MathML\" xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"\
    ><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced\
    \ open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mn>0.2,0.4</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>0.6,0.6</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/655.png).\
    \ In this way, ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:m=\"\
    http://schemas.openxmlformats.org/officeDocument/2006/math\"><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/656.png)\
    \ has the largest alignment score, ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"\
    \ xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.4</mml:mn></mml:math>](img/657.png),"
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: "计算对齐得分。例如，第一个得分是 ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>e</mi><mrow><msub><mi\
    \ mathvariant=\"bold\">q</mi><mn>1</mn></msub><mo>,</mo><msub><mi mathvariant=\"\
    bold\">k</mi><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.6</mn><mo>×</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>0.2</mn></mrow></mfenced><mo>+</mo><mn>1.2</mn><mo>×</mo><mn>0.4</mn><mo>+</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>1.2</mn></mrow></mfenced><mo>×</mo><mn>1.2</mn><mo>+</mo><mn>1.8</mn><mo>×</mo><mn>0.8</mn><mo>=</mo><mn>0</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>.</mo><mn>36</mn></mrow></mrow></math>](img/652.png)![<math\
    \ xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>e</mi><mrow><msub><mi\
    \ mathvariant=\"bold\">q</mi><mn>1</mn></msub><mo>,</mo><msub><mi mathvariant=\"\
    bold\">k</mi><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.6</mn><mo>×</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>0.2</mn></mrow></mfenced><mo>+</mo><mn>1.2</mn><mo>×</mo><mn>0.4</mn><mo>+</mo><mfenced\
    \ open=\"(\" close=\")\"><mrow><mo>−</mo><mn>1.2</mn></mrow></mfenced><mo>×</mo><mn>1.2</mn><mo>+</mo><mn>1.8</mn><mo>×</mo><mn>0.8</mn><mo>=</mo><mn>0</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>.</mo><mn>36</mn></mrow></mrow></math>](img/653.png)。其余的得分显示在*图\
    \ 7**.5*中。我们故意选择了查询 ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"\
    \ xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced\
    \ open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mn>0.6,1.2</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1.2,1.8</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/654.png)，它与第二个键向量\
    \ ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"\
    ><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced\
    \ open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mn>0.2,0.4</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>0.6,0.6</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/655.png)相对相似。这样，![<mml:math\
    \ xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"\
    ><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/656.png)具有最大的对齐得分，![<mml:math\
    \ xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"\
    ><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi\
    \ mathvariant=\"bold\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.4</mml:mn></mml:math>](img/657.png)，"
- en: and it should have the largest influence over the final result.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它应该对最终结果产生最大的影响。
- en: Compute the weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/658.png),
    with the help of softmax—for example, α q 1,k 2 = exp(2.4)/(exp(0.36) + exp(2.4)
    + exp(0.36) + exp(0.36)) = 0.756\. The key vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/659.png),
    has the largest weight because of its large alignment score. The softmax function
    exaggerates the differences between the inputs, hence the final weight of ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/660.png)
    is even higher compared to the ratio of the input alignment scores.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算权重，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/658.png)，借助
    softmax 函数——例如，α q 1,k 2 = exp(2.4)/(exp(0.36) + exp(2.4) + exp(0.36) + exp(0.36))
    = 0.756。关键向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/659.png)，由于其较大的对齐分数，具有最大的权重。softmax
    函数夸大了输入之间的差异，因此，最终的权重！[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/660.png)甚至比输入对齐分数的比例还要高。
- en: Compute the final result, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">r</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1.98,2.98,3.98,4.98</mml:mn><mml:mo>]</mml:mo></mml:math>](img/661.png),
    which is the weighted element-wise sum of the value vectors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/662.png).
    For example, we can compute the first element of the result as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.098</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>+</mml:mo><mml:mn>0.756</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.048</mml:mn><mml:mo>×</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>0.098</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn><mml:mo>=</mml:mo><mml:mn>1.98</mml:mn></mml:math>](img/663.png).
    We can see that the values of the result are closest to the value vector, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/664.png),
    which, again, reflects the large alignment between the key vector, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/659.png),
    and the input query, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/666.png).
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算最终结果，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">r</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1.98,2.98,3.98,4.98</mml:mn><mml:mo>]</mml:mo></mml:math>](img/661.png)，这是值向量的加权元素级求和，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/662.png)。例如，我们可以计算结果的第一个元素为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.098</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>+</mml:mo><mml:mn>0.756</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.048</mml:mn><mml:mo>×</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>0.098</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn><mml:mo>=</mml:mo><mml:mn>1.98</mml:mn></mml:math>](img/663.png)。我们可以看到结果的值最接近值向量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/664.png)，这再次反映了键向量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/659.png)，和输入查询，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/666.png)之间的大量对齐。
- en: I hope that this example has helped you understand the attention mechanism,
    as this is one of the major DL innovations in the past 10 years. Next, we’ll discuss
    an even more advanced attention version.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个例子能帮助你理解注意力机制，因为这是过去10年深度学习领域的一个重要创新。接下来，我们将讨论一个更先进的注意力版本。
- en: Transformer attention
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 注意力
- en: 'In this section, we’ll discuss the attention mechanism, as it appears in the
    transformer NN architecture (*Attention Is All You Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    Don’t worry—you don’t need to know about transformers yet, as **transformer attention**
    (**TA**) is an independent self-sufficient building block of the entire model.
    It is displayed in the following diagram:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论注意力机制，正如它在 Transformer 神经网络架构中出现的那样（*Attention Is All You Need*，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）。别担心——你现在不需要了解
    Transformer，因为**Transformer 注意力**（**TA**）是整个模型的一个独立且自给自足的构建模块。它在下图中展示：
- en: '![Figure 7.6 – Scaled dot product (multiplicative) TA (inspired by https://arxiv.org/abs/1706.03762)](img/B19627_07_6.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 缩放点积（乘法）TA（灵感来源于 https://arxiv.org/abs/1706.03762）](img/B19627_07_6.jpg)'
- en: Figure 7.6 – Scaled dot product (multiplicative) TA (inspired by [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 缩放点积（乘法）TA（灵感来源于 [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）
- en: 'The TA uses dot product (multiplicative) similarity and follows the general
    attention procedure we introduced in the *General attention* section (as we have
    already mentioned, it is not restricted to RNN models). We can define it with
    the following formula:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TA 使用点积（乘法）相似度，并遵循我们在*通用注意力*部分介绍的通用注意力过程（正如我们之前提到的，它并不限于 RNN 模型）。我们可以用以下公式来定义它：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi
    mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/667.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi
    mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/667.png)'
- en: 'In practice, we’ll compute the TA function over a set of queries simultaneously,
    packed in a matrix **Q** (the keys **K**, the values **V**, and the result are
    also matrices). Let’s discuss the steps of the formula in more detail:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们会同时计算一组查询的 TA 函数，这些查询被打包在一个矩阵**Q**中（键 **K**、值 **V** 和结果也是矩阵）。让我们更详细地讨论公式中的各个步骤：
- en: 'Match the queries, **Q**, against the database (keys **K**) with matrix multiplication
    to produce the alignment scores, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/668.png).
    Matrix multiplication is equivalent to applying dot product similarity between
    each unique pair of query and key vectors. Let’s assume that we want to match
    *m* different queries to a database of *n* values and the query-key vector length
    is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/669.png).
    Then, we have the query matrix, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/670.png),
    with one ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/671.png)-dimensional
    query per row for *m* total rows. Similarly, we have the key matrix, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/672.png),
    with one ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/673.png)-dimensional
    key vector per row for *n* total rows (its transpose is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/674.png)).
    Then, the output matrix will be ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/675.png),
    where one row contains the alignment scores of a single query against all keys
    of the database:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询**Q**与数据库（键**K**）进行矩阵乘法匹配，以生成对齐分数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/668.png)。矩阵乘法等价于在每一对查询和键向量之间应用点积相似度。假设我们希望将*m*个不同的查询与一个*n*个值的数据库进行匹配，且查询-键向量的长度为![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/669.png)。然后，我们有查询矩阵，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">Q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/670.png)，每行包含一个![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/671.png)-维的查询向量，共*m*行。类似地，我们有键矩阵，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/672.png)，每行包含一个![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/673.png)-维的键向量，共*n*行（其转置为![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/674.png))。然后，输出矩阵将为![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/675.png)，其中每一行包含一个查询与数据库中所有键的对齐分数：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup><mo>=</mo><munder><munder><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><msub><mi>q</mi><mn>11</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>q</mi><mrow><mn>1</mn><msub><mi
    mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">k</mi></msub></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>q</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>1</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>q</mi><mrow><mi
    mathvariant="bold-italic">m</mi><msub><mi mathvariant="bold-italic">d</mi><mi
    mathvariant="bold-italic">k</mi></msub></mrow></msub></mtd></mtr></mtable></mfenced><mo
    stretchy="true">⏟</mo></munder><mi mathvariant="bold">Q</mi></munder><mo>∙</mo><munder><munder><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><msub><mi>k</mi><mn>11</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>k</mi><mrow><mn>1</mn><mi
    mathvariant="bold-italic">n</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>k</mi><mrow><msub><mi
    mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">k</mi></msub><mn>1</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>k</mi><mrow><msub><mi
    mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">k</mi></msub><mi
    mathvariant="bold-italic">n</mi></mrow></msub></mtd></mtr></mtable></mfenced><mo
    stretchy="true">⏟</mo></munder><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></munder><mo>=</mo><munder><munder><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><msub><mi>e</mi><mn>11</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>e</mi><mrow><mn>1</mn><mi
    mathvariant="bold-italic">n</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>1</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">n</mi></mrow></msub></mtd></mtr></mtable></mfenced><mo
    stretchy="true">⏟</mo></munder><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi
    mathvariant="normal">⊤</mi></msup></mrow></munder></mrow></mrow></math>](img/676.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup><mo>=</mo><munder><munder><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><msub><mi>q</mi><mn>11</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>q</mi><mrow><mn>1</mn><msub><mi
    mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">k</mi></msub></mrow></msub></mtd></tr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>q</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>1</mn></rrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>q</mi><mrow><mi
    mathvariant="bold-italic">m</mi><msub><mi mathvariant="bold-italic">d</mi><mi
    mathvariant="bold-italic">k</mi></msub></rrow></msub></mtd></tr>'''''''
- en: In other words, we can match multiple queries against multiple database keys
    in a single matrix-matrix multiplication. For example, in the context of translation,
    we can compute the alignment scores of all words of the target sentence over all
    words of the source sentence in the same way.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以通过单个矩阵乘法操作，在单个矩阵乘法中匹配多个查询与多个数据库键。例如，在翻译的上下文中，我们可以计算目标句子中所有单词与源句子中所有单词的对齐分数。
- en: 2. Scale the alignment scores with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math>](img/677.png)
    , where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/678.png)
    is the same vector size as the key vectors in the matrix **K**, which is also
    equal to the size of the query vectors in **Q** (analogously, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/679.png)
    is the vector size of the value vectors **V**). The authors of the paper suspect
    that for large values of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png),
    the dot product grows large in magnitude and pushes the softmax in regions with
    extremely small gradients. This, in turn, leads to the vanishing gradients problem,
    hence the need to scale the results.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 使用![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>1</mml:mn><mml:mtext>/</mml:mtext><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math>](img/677.png)
    缩放对齐分数，其中![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/678.png)
    与矩阵 **K** 中键向量的大小相同，也等于 **Q** 中查询向量的大小（类似地，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/679.png)
    是值向量 **V** 的大小）。文中作者怀疑，在![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png)
    的大值情况下，点积的幅度增大，推动 softmax 函数进入极小梯度区域。这反过来导致梯度消失问题，因此需要对结果进行缩放。
- en: '3. Compute the attention scores with the softmax operation along the rows of
    the matrix (we’ll talk about the **mask** operation later):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 对矩阵的行应用 softmax 操作来计算注意力分数（稍后我们会讨论**掩码**操作）：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mfenced open="(" close=")"><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi
    mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mo>=</mo><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><mtable columnspacing="0.8000em" columnwidth="auto
    auto" columnalign="center center" rowspacing="1.0000ex" rowalign="baseline baseline"><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>11</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>12</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>21</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>22</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>1</mn><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>2</mn><mi
    mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr></mtable></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mtable
    columnspacing="0.8000em" columnwidth="auto auto" columnalign="center center" rowalign="baseline"><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mrow><mi mathvariant="bold-italic">m</mi><mn>1</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>2</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mrow><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></mrow></math>](img/681.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mfenced open="(" close=")"><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi
    mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mo>=</mo><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><mtable columnspacing="0.8000em" columnwidth="auto
    auto" columnalign="center center" rowspacing="1.0000ex" rowalign="baseline baseline"><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>11</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>12</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>21</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>22</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>1</mn><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>2</mn><mi
    mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></tr></mtable></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mtable
    columnspacing="0.8000em" columnwidth="auto auto" columnalign="center center" rowalign="baseline"><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mrow><mi mathvariant="bold-italic">m</mi><mn>1</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>2</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mrow><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></mrow></math>](img/681.png)'
- en: '4. Compute the final attention vector by multiplying the attention scores with
    the values **V**:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 通过将注意力得分与值**V**相乘，计算最终的注意力向量：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mfenced open="(" close=")"><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi
    mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mi
    mathvariant="bold">V</mi><mo>=</mo><mfenced open="[" close="]"><mtable columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" columnalign="center center center" rowspacing="1.0000ex
    1.0000ex" rowalign="baseline baseline baseline"><mtr><mtd><mtable columnspacing="0.8000em"
    columnwidth="auto auto" columnalign="center center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi
    mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>11</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>12</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>21</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>22</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>1</mn><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>2</mn><mi
    mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr></mtable></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mtable
    columnspacing="0.8000em" columnwidth="auto auto" columnalign="center center" rowalign="baseline"><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mrow><mi mathvariant="bold-italic">m</mi><mn>1</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>2</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mrow><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr></mtable></mfenced><mo>∙</mo><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><msub><mi>v</mi><mn>11</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>v</mi><mrow><mn>1</mn><msub><mi
    mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">v</mi></msub></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>v</mi><mrow><mi
    mathvariant="bold-italic">n</mi><mn>1</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>v</mi><mrow><mi
    mathvariant="bold-italic">n</mi><msub><mi mathvariant="bold-italic">d</mi><mi
    mathvariant="bold-italic">v</mi></msub></mrow></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi
    mathvariant="bold">A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow></mrow></math>](img/682.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mfenced open="(" close=")"><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi
    mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mi
    mathvariant="bold">V</mi><mo>=</mo><mfenced open="[" close="]"><mtable columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" columnalign="center center center" rowspacing="1.0000ex
    1.0000ex" rowalign="baseline baseline baseline"><mtr><mtd><mtable columnspacing="0.8000em"
    columnwidth="auto auto" columnalign="center center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi
    mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>11</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>12</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mn>21</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mn>22</mn></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>1</mn><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><msub><mi>e</mi><mrow><mn>2</mn><mi
    mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></tr></mtable></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></tr><mtr><mtd><mtable
    columnspacing="0.8000em" columnwidth="auto auto" columnalign="center center" rowalign="baseline"><mtr><mtd><mrow><mrow><mi
    mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi
    mathvariant="normal">x</mi><mo>(</mo><mo>[</mo><msub><mi>e</mi><mrow><mi mathvariant="bold-italic">m</mi><mn>1</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>,</mo></mrow></mrow></mtd><mtd><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mn>2</mn></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow></mtd></mtr></mtable></mtd><mtd><mo>⋯</mo></mtd><mtd><mrow><mrow><msub><mi>e</mi><mrow><mi
    mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">n</mi></mrow></msub><mo>/</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><mo>]</mo><mo>)</mo></mrow></mrow></mtd></tr></mtable></mfenced><mo>∙</mo><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" columnalign="center center center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline
    baseline baseline"><mtr><mtd><msub><mi>v</mi><mn>11</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>v</mi><mrow><mn>1</mn><msub><mi
    mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">v</mi></msub></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>v</mi><mrow><mi
    mathvariant="bold-italic">n</mi><mn>1</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msub><mi>v</mi><mrow><mi
    mathvariant="bold-italic">n</mi><msub><mi mathvariant="bold-italic">d</mi><mi
    mathvariant="bold-italic">v</mi></msub></mrow></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi
    mathvariant="bold">A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow></mrow></math>](img/682.png)'
- en: 'The full TA uses a collection of such attention blocks and is known as **multi-head
    attention** (**MHA**), as displayed in the following diagram:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的TA使用一组注意力块，称为**多头注意力**（**MHA**），如以下图所示：
- en: '![Figure 7.7 – MHA (inspired by https://arxiv.org/abs/1706.03762)](img/B19627_07_7.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – MHA（灵感来自 https://arxiv.org/abs/1706.03762）](img/B19627_07_7.jpg)'
- en: Figure 7.7 – MHA (inspired by https://arxiv.org/abs/1706.03762)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – MHA（灵感来自 [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）
- en: Instead of a single attention function with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/683.png)-dimensional
    keys, we linearly project the keys, queries, and values *h* times to produce *h*
    different ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/684.png)-,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/684.png)-,
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/686.png)-dimensional
    projections of these values. Then, we apply separate parallel attention blocks
    (or **heads**) over the newly created vectors, which yield a single ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/687.png)-dimensional
    output for each head. Next, we concatenate the head outputs and linearly project
    them to produce the final attention result.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于单一的注意力函数，其中包含![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/683.png)-维度的键，我们将键、查询和数值线性投影*h*次，以生成*h*个不同的![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/684.png)-维，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/684.png)-维和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/686.png)-维的这些数值投影。然后，我们对新创建的向量应用独立的并行注意力块（或**头部**），每个头部生成一个![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/687.png)-维度的输出。接着，我们将这些头部输出连接起来，并进行线性投影，生成最终的注意力结果。
- en: Note
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: By linear projection, we mean applying an FC layer. That is, initially we branch
    the **Q**/**K**/**V** matrices with the help of separate FC operations. In the
    end, we use an FC layer to combine and compress the concatenated head outputs.
    In this case, we follow the terminology used in the original paper.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 线性投影是指应用全连接（FC）层。也就是说，最初我们借助单独的FC操作将**Q**/**K**/**V**矩阵分支处理。最终，我们使用一个FC层来合并并压缩连接后的头部输出。在这种情况下，我们遵循原论文中使用的术语。
- en: 'MHA allows each head to attend to different elements of the sequence. At the
    same time, the model combines the outputs of the heads in a single cohesive representation.
    More precisely, we can define this with the following formula:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: MHA允许每个头部关注序列的不同元素。同时，模型将各个头部的输出合并为一个统一的表示。更精确地说，我们可以通过以下公式来定义这一过程：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi
    mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi
    mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi
    mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:math>](img/688.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi
    mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi
    mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi
    mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:math>](img/688.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">V</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/689.png).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">V</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/689.png)。
- en: 'Let’s look at this in more detail, starting with the heads:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一下，从头部开始：
- en: Each head receives the linearly projected versions of the initial **Q**, **K**,
    and **V** matrices. The projections are computed with the learnable weight matrices
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/690.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/691.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/692.png)
    respectively (again, the projections are FC layers). Note that we have a separate
    set of weights for each component (**Q**, **K**, **V**) and for each head, *i*.
    To satisfy the transformation from ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/695.png),
    the dimensions of these matrices are ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/696.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/697.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/698.png).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个头接收初始**Q**、**K**和**V**矩阵的线性投影版本。这些投影是通过可学习的权重矩阵计算的！[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/690.png)、![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/691.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/692.png)，分别计算出来（再次强调，投影是全连接层）。注意，我们为每个组件（**Q**、**K**、**V**）以及每个头，*i*，拥有一组单独的权重。为了满足从![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png)到![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/695.png)的转换，这些矩阵的维度是![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/696.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/697.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/698.png)。
- en: Once **Q**, **K**, and **V** are transformed, we can compute the attention of
    each head using the regular attention block we described at the beginning of this
    section.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦**Q**、**K**和**V**被转换，我们就可以使用本节开头描述的常规注意力块来计算每个头的注意力。
- en: The final attention result is the linear projection (FC layer with a weight
    matrix ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math>](img/699.png)
    of learnable weights) over the concatenated head outputs ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">h</mml:mi><mml:mi
    mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/700.png).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的注意力结果是线性投影（带有权重矩阵 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math>](img/699.png)，可学习的权重）作用于拼接的头输出
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/700.png)。
- en: 'So far, we’ve assumed that the attention works for different input and output
    sequences. For example, in translation, each word of the translated sentence *attends*
    to the words of the source sentence. However, there is another valid attention
    use case. The transformer also relies on **self-attention** (or **intra-attention**),
    where the queries, **Q**, belong to the same dataset as the keys, **K**, and values,
    **V**, of the query database. In other words, in self-attention, the source and
    the target are the same sequence (in our case, the same sentence). The benefit
    of self-attention is not immediately obvious, as there is no direct task to apply
    it to. On an intuitive level, it allows us to see the relationship between words
    of the same sequence. To understand why this is important, let’s recall the word2vec
    model ([*Chapter 6*](B19627_06.xhtml#_idTextAnchor185)), where we use the context
    of a word (that is, its surrounding words) to learn an embedding vector of said
    word. One of the limitations of word2vec is that the embedding is static (or context
    independent)—we have a single embedding vector for all contexts of the word in
    the whole training corpus. For example, the word *new* will have the same embedding
    vector, regardless of whether we use it in the phrase *new shoes* or *New York*.
    Self-attention allows us to solve this problem by creating a **dynamic embedding**
    (or **context dependent**) of that word. We won’t go into too much detail just
    yet (we’ll do this in the *Building transformers with attention* section), but
    the dynamic embedding works in the following way: we feed the current word into
    the attention block, but also its current immediate surrounding (context). The
    word is the query, **q**, and the context is the **K**/**V** key-value store.
    In this way, the self-attention mechanism allows the model to produce a dynamic
    embedding vector, unique to the current context of the word. This vector serves
    as an input for a variety of downstream tasks. Its purpose is similar to the static
    word2vec embedding, but it is much more expressive and makes it possible to solve
    more complex tasks with greater accuracy.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设注意力机制适用于不同的输入和输出序列。例如，在翻译中，每个翻译句子的单词会 *关注* 源句子的单词。然而，注意力还有另一个有效的应用场景。变换器也依赖于**自注意力**（或**内部注意力**），其中查询
    **Q** 来自于与键 **K** 和值 **V** 相同的数据集。换句话说，在自注意力中，源和目标是相同的序列（在我们的例子中，就是同一句话）。自注意力的好处并不是立刻显而易见的，因为没有直接的任务来应用它。从直观的角度来看，它使我们能够看到同一序列中单词之间的关系。为了理解为什么这很重要，让我们回想一下
    word2vec 模型（[*第 6 章*](B19627_06.xhtml#_idTextAnchor185)），在这个模型中，我们利用一个词的上下文（即它周围的词）来学习该词的嵌入向量。word2vec
    的一个局限性是其嵌入是静态的（或与上下文无关的）——我们为该词在整个训练语料库中的所有上下文使用相同的嵌入向量。例如，无论我们将 *new* 使用在 *new
    shoes* 还是 *New York* 中，它的嵌入向量都是相同的。自注意力使我们能够通过创建该词的**动态嵌入**（或**上下文相关**）来解决这个问题。我们暂时不深入细节（我们将在*构建带注意力机制的变换器*一节中讲解），但是动态嵌入的工作原理如下：我们将当前词输入注意力模块，同时也输入其当前的即时上下文（即周围词）。该词是查询
    **q**，而上下文是 **K**/**V** 键值存储。通过这种方式，自注意力机制使模型能够产生一个动态的嵌入向量，这个向量是针对该词当前上下文的独特表示。这个向量作为各种下游任务的输入。它的目的类似于静态的
    word2vec 嵌入，但它更具表现力，并使得能够以更高的准确度解决更复杂的任务。
- en: 'We can illustrate how self-attention works with the following diagram, which
    shows the multi-head self-attention of the word *economy* (different colors represent
    different attention heads):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下图示来说明自注意力是如何工作的，图中展示了词 *economy* 的多头自注意力（不同的颜色代表不同的注意力头）：
- en: '![Figure 7.8 – Multi-head self-attention of the word “economy” (generated by
    https://github.com/jessevig/bertviz)](img/B19627_07_8.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 词“economy”的多头自注意力（由 https://github.com/jessevig/bertviz 生成）](img/B19627_07_8.jpg)'
- en: Figure 7.8 – Multi-head self-attention of the word “economy” (generated by https://github.com/jessevig/bertviz)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 词“economy”的多头自注意力（由 https://github.com/jessevig/bertviz 生成）
- en: We can see that the strongest link to *economy* comes from the word *market*,
    which makes sense because the two words form a phrase with a unique meaning. However,
    we can also see that different heads attend to different, further, parts of the
    input sequence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，*economy* 与词 *market* 的关联最强，这很有道理，因为这两个词组成了一个具有独特含义的短语。然而，我们也可以看到，不同的注意力头会关注输入序列中的不同、更远的部分。
- en: 'As a conclusion to this section, let’s outline the advantages of the attention
    mechanism compared to the way RNNs process sequences:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本节的总结，让我们概括一下注意力机制相较于 RNN 处理序列的优势：
- en: '**Direct access to the elements of the sequence**: An RNN encodes the information
    of the input elements in a single hidden (thought vector). In theory, it represents
    a distilled version of all sequence elements so far. In practice, it has limited
    representational power—it can only preserve meaningful information for a sequence
    with a maximum length of around 100 tokens before the newest tokens start erasing
    the information of the older ones.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接访问序列元素**：RNN 将输入元素的信息编码为一个单一的隐藏层（思维向量）。理论上，它代表了目前为止所有序列元素的浓缩版本。实际上，它的表示能力有限——它只能在最大长度约为
    100 个标记的序列中保留有意义的信息，在此之后最新的标记开始覆盖旧的标记信息。'
- en: In contrast, the attention mechanism provides direct access to all input sequence
    elements. On one hand, this imposes a strict limit on the maximum sequence length.
    On the other hand, it makes it possible, as of the time of writing book, to have
    transformer-based LLMs, which can process sequences of more than 32,000 tokens.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比之下，注意力机制提供了对所有输入序列元素的直接访问。一方面，这对最大序列长度施加了严格的限制；另一方面，它使得在本书编写时，基于 Transformer
    的 LLM 能够处理超过 32,000 个标记的序列。
- en: '**Parallel processing of the input sequence**: An RNN processes the input sequence
    elements one by one, in the order of their arrival. Therefore, we cannot parallelize
    RNNs. Compare this with the attention mechanism—it consists exclusively of matrix
    multiplication operations, which are **embarrassingly parallel**. This makes it
    possible to train LLMs with billions of trainable parameters over large training
    datasets.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入序列的并行处理**：RNN 按照元素到达的顺序逐一处理输入序列元素。因此，我们无法对 RNN 进行并行化。与此相比，注意力机制完全由矩阵乘法操作构成，**显著并行**。这使得能够在大规模训练数据集上训练具有数十亿可训练参数的
    LLM。'
- en: But these advantages come with one disadvantage—where an RNN preserves the order
    of the sequence elements, the attention mechanism, with its direct access, does
    not. However, we’ll introduce a workaround to that limitation in the *Transformer*
    *encoder* section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这些优势伴随着一个劣势——当 RNN 保留序列元素的顺序时，注意力机制由于其直接访问的特性，则没有保留顺序。然而，我们将在*Transformer*
    *编码器*部分介绍一种解决该限制的变通方法。
- en: This concludes our theoretical introduction to TA. Next, let’s implement it.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 TA 的理论介绍。接下来，让我们实现它。
- en: Implementing TA
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 TA
- en: In this section, we’ll implement MHA, following the definitions from the *Transformer
    attention* section. The code in this section is part of the larger transformer
    implementation, which we’ll discuss throughout the chapter. We won’t include the
    full source code, but you can find it in the book’s GitHub repo.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现 MHA，遵循 *Transformer 注意力* 部分的定义。本节中的代码是更大 Transformer 实现的一部分，我们将在本章中讨论这些内容。我们不会提供完整的源代码，但你可以在本书的
    GitHub 仓库中找到它。
- en: Note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This example is based on [https://github.com/harvardnlp/annotated-transformer](https://github.com/harvardnlp/annotated-transformer).
    Let’s also note that PyTorch has native transformer modules (the documentation
    is available at [https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)).
    Still, in this section, we’ll implement TA from scratch to understand it better.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例基于 [https://github.com/harvardnlp/annotated-transformer](https://github.com/harvardnlp/annotated-transformer)。还需要注意的是，PyTorch
    有原生的 Transformer 模块（文档可在 [https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)
    查阅）。尽管如此，在本节中，我们将从头实现 TA，以便更好地理解它。
- en: 'We’ll start with the implementation of regular scaled dot product attention.
    As a reminder, it implements the formula ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">A</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi
    mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mtext>/</mml:mtext><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfenced><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/701.png), where `query`, `key`,
    and `value`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从常规缩放点积注意力的实现开始。提醒一下，它实现的公式是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">A</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi
    mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mtext>/</mml:mtext><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfenced><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/701.png)，其中 `query`，`key` 和 `value`：
- en: '[PRE0]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `attention` function includes `dropout`, as it is part of the full transformer
    implementation. Once again, we’ll leave the `mask` parameter and its purpose for
    later. Let’s also note a novel detail—the use of the `@` operator (`query @ key.transpose(-2,
    -1)` and `p_attn @ value`), which, as of Python 3.5, is reserved for matrix multiplication.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`attention` 函数包括 `dropout`，因为它是完整的 transformer 实现的一部分。我们暂时将 `mask` 参数及其用途留到后面再讨论。还需要注意一个新细节——`@`
    操作符的使用（`query @ key.transpose(-2, -1)` 和 `p_attn @ value`），自 Python 3.5 起，它已被保留用于矩阵乘法。'
- en: 'Next, let’s continue with the MHA implementation. As a reminder, the implementation
    follows the formula: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math" display="block"><mml:mi
    mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi
    mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi
    mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi
    mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:math>](img/702.png)
    . Here, hea d i = Attention(Q W i Q,'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续实现MHA。提醒一下，实现遵循以下公式：![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math" display="block"><mml:mi
    mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi
    mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi
    mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi
    mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi
    mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mi
    mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:math>](img/702.png)。这里，head
    i = Attention(Q W i Q，
- en: K W i K, V W i V).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: K W i K，V W i V。
- en: 'We’ll implement it as a subclass of `torch.nn.Module`, called `MultiHeadedAttention`.
    We’ll start with the constructor:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其实现为`torch.nn.Module`的子类，名为`MultiHeadedAttention`。我们将从构造函数开始：
- en: '[PRE1]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we use the `clones` function (implemented on GitHub) to create four
    identical FC `self.fc_layers` instances. We’ll use three of them for the `self.attn`
    property.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`clones`函数（在GitHub上实现）来创建四个相同的全连接`self.fc_layers`实例。我们将使用其中三个作为`self.attn`属性。
- en: 'Next, let’s implement the `MultiHeadedAttention.forward` method. Please bear
    in mind that the declaration should be indented, as it is a property of the `MultiHeadedAttention`
    class:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现`MultiHeadedAttention.forward`方法。请记住，声明应该缩进，因为它是`MultiHeadedAttention`类的一个属性：
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We iterate over the `query`/`key`/`value` tensors and their reference projection,
    `self.fc_layers`, and produce `query`/`key`/`value` projections with the following
    snippet:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历`query`/`key`/`value`张量及其参考投影`self.fc_layers`，并使用以下代码片段生成`query`/`key`/`value`的投影：
- en: '[PRE3]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we apply regular attention over the projections using the attention function
    we first defined. Next, we concatenate the outputs of the multiple heads, and
    finally, we feed them to the last FC layer (`self.fc_layers[-1]`) and return the
    results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用我们最初定义的注意力函数，对投影进行常规注意力操作。接下来，我们将多个头的输出进行拼接，最后将它们传递到最后的全连接层（`self.fc_layers[-1]`）并返回结果。
- en: Now that we’ve discussed the TA, let’s continue with the transformer model itself.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了TA，让我们继续深入讲解变换器模型本身。
- en: Building transformers with attention
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建具有注意力机制的变换器
- en: 'We’ve spent the better part of this chapter touting the advantages of the attention
    mechanism. It’s time to reveal the full **transformer** architecture, which, unlike
    RNNs, relies solely on the attention mechanism (*Attention Is All You Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    The following diagram shows two of the most popular transformer flavors, **post-ln**
    and **pre-ln** (or **post-normalization** and **pre-normalization**):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的大部分内容中都在强调注意力机制的优势。现在是时候揭示完整的**变换器**架构了，它与RNN不同，完全依赖于注意力机制（*Attention
    Is All You Need*，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）。下图展示了两种最流行的变换器类型，**后归一化**和**前归一化**（或**后归一化**和**前归一化**）：
- en: '![Figure 7.9 – Left: the original (post-normalization, post-ln) transformer;
    right: pre-normalization (pre-ln) transformer (inspired by https://arxiv.org/abs/1706.03762)](img/B19627_07_9.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 左：原始（后归一化，后ln）变换器；右：前归一化（前ln）变换器（灵感来源于https://arxiv.org/abs/1706.03762）](img/B19627_07_9.jpg)'
- en: 'Figure 7.9 – Left: the original (post-normalization, post-ln) transformer;
    right: pre-normalization (pre-ln) transformer (inspired by https://arxiv.org/abs/1706.03762)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 左：原始（后归一化，后ln）变换器；右：前归一化（前ln）变换器（灵感来源于https://arxiv.org/abs/1706.03762）
- en: It looks scary, but fret not—it’s easier than it seems. In this section, we’ll
    discuss the transformer in the context of the seq2seq task, which we defined in
    the *Introducing seq2seq models* section. That is, it will take a sequence of
    tokens as input, and it will output another, different, token sequence. As with
    the seq2seq model, it has two components—an **encoder** and a **decoder**. We’ll
    start with the encoder (the left-hand component of both sections of the preceding
    diagram).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很复杂，但别担心——其实比想象的更简单。在本节中，我们将以seq2seq任务为背景讨论变换器，该任务我们在*引入seq2seq模型*一节中已经定义。也就是说，它将接受一组标记序列作为输入，并输出另一组不同的标记序列。与seq2seq模型一样，它有两个组成部分——**编码器**和**解码器**。我们将从编码器开始（即前面图中两部分的左侧）。
- en: Transformer encoder
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器编码器
- en: The encoder begins with an input sequence of one-hot-encoded tokens. The most
    popular tokenization algorithms are **byte-pair encoding** (**BPE**), WordPiece,
    and Unigram ([*Chapter 6*](B19627_06.xhtml#_idTextAnchor185)). The tokens are
    transformed into ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/707.png)-dimensional
    embedding vectors. The transformation works in the way we described in [*Chapter
    6*](B19627_06.xhtml#_idTextAnchor185). We have a lookup table (matrix)—the index
    of the one-hot-encoded token indicates the matrix row, which represents the embedding
    vector. The embedding vectors are further multiplied by ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math>](img/708.png).
    They are initialized randomly and are trained with the whole model (this is opposed
    to initializing them with an algorithm such as word2vec).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器以一组独热编码的标记序列作为输入。最常用的标记化算法有**字节对编码**（**BPE**）、WordPiece和Unigram（[*第六章*](B19627_06.xhtml#_idTextAnchor185)）。这些标记会被转换成![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/707.png)-维度的嵌入向量。这一转换方式如我们在[*第六章*](B19627_06.xhtml#_idTextAnchor185)中所描述。我们有一个查找表（矩阵）——独热编码标记的索引指示矩阵的行，该行代表嵌入向量。嵌入向量进一步与![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math>](img/708.png)相乘。它们被随机初始化，并与整个模型一起训练（这与使用word2vec等算法初始化不同）。
- en: The next step adds positional information to the existing embedding vector.
    This is necessary because the attention mechanism doesn’t preserve the order of
    sequence elements. This step modifies the embedding vectors in a way that implicitly
    encodes that information within them.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将位置位置信息添加到现有的嵌入向量中。这是必要的，因为注意力机制无法保留序列元素的顺序。此步骤通过一种方式修改嵌入向量，隐式地将位置信息编码到其中。
- en: The original transformer implementation uses **static positional encoding**,
    represented by special positional encoding vectors with the same size as the token
    embeddings. We add these vectors, using element-wise addition, to all embedding
    vectors of the sequence, depending on their position. The static encoding is unique
    for each position of the sequence but is constant with regard to the elements
    of the sequence. Because of this, we can precompute the positional encodings only
    once and use them subsequently.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Transformer 实现使用**静态位置编码**，通过与词嵌入大小相同的特殊位置编码向量来表示。我们将这些向量使用逐元素加法的方式加到序列中的所有嵌入向量上，具体取决于它们的位置。静态编码对于序列的每个位置都是唯一的，但对于序列元素而言是常量。由于这个原因，我们可以仅预计算位置编码一次，然后在后续使用。
- en: An alternative way to encode positional information is with relative position
    representations
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 编码位置信息的另一种方式是使用相对位置表示
- en: (*Self-Attention with Relative Position Representations*, [https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)).
    Here, the positional information is dynamically encoded in the key-value matrices,
    **K**/**V**, of the attention blocks. Each element of the input sequence has a
    different position in relation to the rest of the elements. Therefore, the relative
    position encoding is computed dynamically for each token. This encoding is applied
    to the **K** and **V** matrices as an additional part of the attention formula.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: (*相对位置表示的自注意力*， [https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155))。在这里，位置信息在注意力块的键值矩阵
    **K**/**V** 中动态编码。输入序列的每个元素相对于其余元素的位置都是不同的。因此，相对位置编码对于每个标记动态计算。此编码作为注意力公式的附加部分应用于
    **K** 和 **V** 矩阵。
- en: 'The rest of the encoder is composed of a stack of *N=6* identical blocks that
    come in two flavors: post-ln and pre-ln. Both types of blocks share the following
    sublayers:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的其余部分由 *N=6* 个相同的块组成，这些块有两种类型：post-ln 和 pre-ln。这两种类型的块共享以下子层：
- en: A multi-head self-attention mechanism, like the one we described in the *Transformer
    attention* section. Since the self-attention mechanism works across the whole
    input sequence, the encoder is **bidirectional** by design. That is, the context
    of the current token includes both the tokens that come before and the ones that
    come after it in the sequence. This is opposed to a regular RNN, which only has
    access to the tokens that came before the current one. Each position in an encoder
    block can attend to all positions in the previous encoder block.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多头自注意力机制，就像我们在*Transformer 注意力*部分描述的那样。由于自注意力机制作用于整个输入序列，编码器在设计上是**双向**的。也就是说，当前标记的上下文包括序列中当前标记之前和之后的标记。这与常规的
    RNN 相对立，后者只能访问当前标记之前的标记。在一个编码器块中，每个位置都可以关注前一个编码器块中的所有位置。
- en: We feed the embedding of each token as a query, **q**, to the multi-head self-attention
    (we can feed the full input sequence in one pass as an input matrix, **Q**). At
    the same time, the embeddings of its context act as the key-value store **K**/**V**.
    The output vector of the multi-head self-attention operation serves as input for
    the rest of the model.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将每个标记的嵌入作为查询 **q** 输入到多头自注意力中（我们可以将整个输入序列作为一个输入矩阵 **Q** 一次性传递）。同时，其上下文的嵌入作为键值存储
    **K**/**V**。多头自注意力操作的输出向量作为模型其余部分的输入。
- en: MHA and activation functions
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: MHA 和激活函数
- en: The MHA produces *h* attention vectors for each of the *h* attention heads for
    each input token. Then, they are linearly projected with an FC layer that combines
    them. The whole attention block doesn’t have an explicit activation function.
    But let’s recall that the attention block ends with a non-linearity, softmax.
    The dot product of the key-value vectors is an additional
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: MHA 为每个输入标记产生 *h* 个注意力向量，分别来自每个 *h* 个注意力头。然后，这些向量通过一个全连接（FC）层进行线性投影并进行结合。整个注意力块没有显式的激活函数。但让我们回想一下，注意力块的结束有一个非线性函数——softmax。键值向量的点积是一个额外的部分。
- en: non-linearity. In that strict sense, the attention block doesn’t need additional
    activation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性。从严格意义上讲，注意力块不需要额外的激活函数。
- en: 'A simple FC FFN, which is defined by the following formula:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的全连接 FFN，由以下公式定义：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>FFN</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>ActivationFunc</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/709.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>FFN</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>ActivationFunc</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/709.png)'
- en: The network is applied to each sequence element, **x**, separately. It uses
    the same set of parameters (![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi
    mathvariant="bold">W</mi><mn>1</mn><mrow /></msubsup></mrow></math>](img/710.png),
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi mathvariant="bold">W</mi><mn>2</mn><mrow
    /></msubsup></mrow></math>](img/711.png), ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/712.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/713.png))
    across different positions, but different parameters across the different encoder
    blocks. The original transformer uses **rectified linear unit** (**ReLU**) activations.
    However, more recent models use one of its variations, such as **sigmoid linear
    units** (**SiLUs**). The role of the FFN is to process the MHA output in a way
    that better fits the input for the next block.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 网络应用于每个序列元素 **x**，并且是独立进行的。它使用相同的参数集 (![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi
    mathvariant="bold">W</mi><mn>1</mn><mrow /></msubsup></mrow></math>](img/710.png)，![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi mathvariant="bold">W</mi><mn>2</mn><mrow
    /></msubsup></mrow></math>](img/711.png)，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/712.png)，和
    ![<mml:math xmlns=mml="http://www.w3.org/1998/Math/MathML" xmlns=m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/713.png))，但不同的编码器块使用不同的参数。原始
    transformer 使用 **整流线性单元**（**ReLU**）激活函数。然而，近年来的模型使用其变种之一，如 **sigmoid 线性单元**（**SiLU**）。FFN
    的作用是以更适合下一个块输入的方式处理 MHA 输出。
- en: 'The difference between pre-ln and post-ln blocks lies in the position of the
    normalization layer. Each post-ln sublayer (both the MHA and FFN) has a residual
    connection around itself and ends with normalization and dropout over the sum
    of that connection and its own output. The normalization layers in the post-ln
    transformer lie after the attention and the FFN, respectively. Therefore, the
    output of each post-ln sublayer is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: pre-ln 和 post-ln 块之间的区别在于归一化层的位置。每个 post-ln 子层（包括 MHA 和 FFN）都有一个残差连接，并以该连接和自身输出的和进行归一化和丢弃。post-ln
    transformer 中的归一化层分别位于注意力机制和 FFN 之后。因此，每个 post-ln 子层的输出如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/714.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/714.png)'
- en: 'In contrast, the pre-ln blocks (the right section of *Figure 7**.9*), in the
    two encoder normalization layers lie before the attention and the FFN, respectively.
    Therefore, the output of each pre-ln sublayer is this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，pre-ln块（*图7.9*的右侧部分）在两个编码器归一化层中，分别位于注意力机制和FFN之前。因此，每个pre-ln子层的输出是这样的：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/715.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/715.png)'
- en: The difference between the two flavors manifests itself during training. Without
    going into too many details, the aptly named paper *Understanding the Difficulty
    of Training Transformers* ([https://arxiv.org/abs/2004.08249](https://arxiv.org/abs/2004.08249))
    suggests that the post-ln transformer’s strong dependency on residual connections
    amplifies the fluctuation caused by parameter changes (for example, adaptive learning
    rate) and destabilizes the training. Because of this, the post-ln training starts
    with a warmup phase with a low learning rate, before ultimately increasing it.
    This is opposed to the usual learning rate schedule that starts with a large value,
    which only decreases with the progression of the training. The pre-ln blocks don’t
    have such a problem and don’t need a warmup phase. However, they could suffer
    from **representation collapse**, where the hidden representation in deeper blocks
    (those closer to the end of the NN) will be similar and thus contribute little
    to model capacity. In practice, both types of blocks are in use.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 两种变体的差异在训练过程中表现得尤为明显。无需深入细节，恰如其分地命名的论文*理解训练变压器的难度*（[https://arxiv.org/abs/2004.08249](https://arxiv.org/abs/2004.08249)）表明，后-ln变压器对残差连接的强烈依赖放大了由于参数变化（例如自适应学习率）引起的波动，从而使训练不稳定。因为这个原因，后-ln训练从一个较低学习率的预热阶段开始，然后逐渐增加学习率。这与通常的学习率调度相反，通常从较大的学习率开始，并随着训练的进行逐渐减小。而pre-ln块则没有这个问题，也不需要预热阶段。然而，它们可能会遭遇**表示崩溃**的问题，即深层块（靠近神经网络末端的块）中的隐藏表示会变得相似，从而对模型容量贡献较小。在实践中，两种类型的块都会被使用。
- en: So far, so good with the encoder. Next, let’s build upon our attention implementation
    by building the encoder as well.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，编码器部分进展顺利。接下来，让我们在构建编码器的过程中，继续完善我们的注意力实现。
- en: Implementing the encoder
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现编码器
- en: 'In this section, we’ll implement the post-ln encoder, which is composed of
    several different submodules. Let’s start with the main class, `Encoder`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现后-ln编码器，它由几个不同的子模块组成。让我们从主要类`Encoder`开始：
- en: '[PRE4]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It stacks `N` instances of `EncoderBlock` (`self.blocks`), followed by a `LayerNorm`
    normalization, `self.norm`. Each instance serves as input to the next, as the
    definition of the `forward` method shows. In addition to the regular input, `x`,
    `forward` also takes as input a `mask` parameter. However, it is only relevant
    to the decoder part, so we won’t focus on it here.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 它堆叠了`N`个`EncoderBlock`实例（`self.blocks`），后面跟着一个`LayerNorm`归一化层（`self.norm`）。每个实例都作为下一个实例的输入，如`forward`方法的定义所示。除了常规的输入`x`外，`forward`方法还接受一个`mask`参数作为输入。但它只与解码器部分相关，因此我们在这里不会关注它。
- en: 'Next, let’s see the implementation of the `EncoderBlock` class:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看`EncoderBlock`类的实现：
- en: '[PRE5]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each encoder block consists of multi-head self-attention (`self.self_attn`)
    and FFN (`self.ffn`) sublayers (`self.sublayers`). Each sublayer is wrapped by
    its residual connection, `SublayerConnection` class and instantiated with the
    familiar `clone` function:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器块由多头自注意力（`self.self_attn`）和 FFN（`self.ffn`）子层（`self.sublayers`）组成。每个子层都被其残差连接（`SublayerConnection`
    类）包裹，并通过熟悉的 `clone` 函数实例化：
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `SublayerConnection.forward` method takes as input the data tensor, `x`,
    and `sublayer`, which is an instance of either `MultiHeadedAttention` or `PositionwiseFFN`
    (it matches the sublayer definition ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/716.png)
    from the *Transformer* *encoder* section).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`SublayerConnection.forward` 方法接受数据张量 `x` 和 `sublayer` 作为输入，`sublayer` 是 `MultiHeadedAttention`
    或 `PositionwiseFFN` 的实例（它与来自 *Transformer* *encoder* 部分的子层定义![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/716.png)相匹配)。'
- en: 'The only component we haven’t defined yet is `PositionwiseFFN`, which implements
    the formula ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>FFN</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>ActivationFunc</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/717.png).
    We’ll use SiLU activation. Let’s add this missing piece:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未定义的唯一组件是 `PositionwiseFFN`，它实现了公式 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>FFN</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>ActivationFunc</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/717.png)。我们将使用
    SiLU 激活函数。现在，让我们补充这一缺失的部分：
- en: '[PRE7]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This concludes our implementation of the encoder. Next, let’s focus our attention
    on the decoder.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对编码器部分的实现完成。接下来，让我们将注意力集中在解码器上。
- en: Transformer decoder
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 解码器
- en: The decoder generates the output sequence, based on a combination of the encoder
    output and its own previously generated sequence of tokens (we can see the decoder
    on the right side of both sections in *Figure 7**.9* at the beginning of the *Building
    transformers with attention* section). In the context of a seq2seq task, the full
    encoder-decoder transformer is an autoregressive model. First, we feed the initial
    sequence—for example, a sentence to translate or a question to answer—to the encoder.
    This can happen in a single pass, if the sequence is short enough to fit the maximum
    size of the query matrix, **Q**. Once the encoder processes all sequence elements,
    the decoder will take the encoder output and start generating the output sequence
    one token at a time. It will append each generated token to the initial input
    sequence. We’ll feed the new, extended sequence to the encoder once again. The
    new output of the encoder will initiate the next token generation step of the
    decoder, and so on. In effect, the target token sequence is the same as the input
    token sequence, shifted by one (similar to the seq2seq decoder).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器基于编码器输出和自身先前生成的标记序列的组合生成输出序列（我们可以在*图7.9*的*构建带注意力的变换器*部分的两侧看到解码器）。在seq2seq任务的背景下，完整的编码器-解码器变换器是一个自回归模型。首先，我们将初始序列——例如，待翻译的句子或待回答的问题——输入给编码器。如果序列足够短，可以适应查询矩阵的最大大小，这可以在一次传递中完成。一旦编码器处理完所有序列元素，解码器将使用编码器的输出开始逐个标记地生成输出序列。它将每个生成的标记附加到初始输入序列中。我们将新的扩展序列再次输入给编码器。编码器的新输出将启动解码器的下一步标记生成，如此循环。实际上，目标标记序列与输入标记序列相同，且位移了一位（类似于seq2seq解码器）。
- en: 'The decoder uses the same embedding vectors and positional encoding as the
    encoder. It continues with a stack of *N=6* identical decoder blocks. Each block
    consists of three sublayers and each sublayer employs residual connections, dropout,
    and normalization. As with the encoder, the blocks come in post-ln and pre-ln
    flavors. The sublayers are as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器使用与编码器相同的嵌入向量和位置编码。它继续通过堆叠*N=6*个相同的解码器块。每个块由三个子层组成，每个子层都使用残差连接、丢弃法和归一化。与编码器一样，这些块有后-ln和前-ln两种形式。子层如下：
- en: 'A masked multi-head self-attention mechanism. The encoder’s self-attention
    is bidirectional—it can attend to all elements of the sequence, regardless of
    whether they come before or after the current element. However, the decoder only
    has a partially generated target sequence. Therefore, the decoder is **unidirectional**—the
    self-attention can only attend to the preceding sequence elements. During inference,
    we have no choice but to run the transformer in a sequential way so that it can
    produce each token of the output sequence one by one. However, during training,
    we can feed the whole target sequence simultaneously, as it’s known in advance.
    To avoid illegal forward attention, we can **mask out** illegal connections by
    setting −∞ on all such values in the input of the attention softmax. We can see
    the mask component in *Figure 7**.6* of the *Transformer attention* section and
    the result of the mask operation here:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种带掩码的多头自注意力机制。编码器的自注意力是双向的——它可以关注序列中的所有元素，无论这些元素是在当前元素之前还是之后。然而，解码器只有部分生成的目标序列。因此，解码器是**单向的**——自注意力只能关注前面的序列元素。在推理过程中，我们别无选择，只能以顺序的方式运行转换器，从而让它一个接一个地生成输出序列中的每个标记。然而，在训练过程中，我们可以同时输入整个目标序列，因为它是提前已知的。为了避免非法的前向注意力，我们可以通过在注意力的软最大输入中将所有非法值设为−∞来**屏蔽**非法连接。我们可以在*图7.6*中的*Transformer注意力*部分看到掩码组件，掩码操作的结果如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi
    mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/718.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi
    mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">n'
- en: A regular attention (not self-attention) mechanism, where the queries come from
    the previous decoder layer, and the keys and values come from the encoder output.
    This allows every position in the decoder to attend all positions in the original
    input sequence. This mimics the typical encoder-decoder attention mechanisms,
    which we discussed in the *Introducing seq2seq* *models* section.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规注意力机制（而不是自注意力机制），查询来自前一个解码器层，键和值来自编码器输出。这使得解码器中的每个位置都能关注原始输入序列中的所有位置。这模拟了我们在*引入seq2seq*
    *模型*部分中讨论的典型编码器-解码器注意力机制。
- en: FFN, which is similar to the one in the encoder.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN，与编码器中的类似。
- en: The decoder ends with an FC layer, followed by a softmax operation, which produces
    the most probable next word of the sentence.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器以一个全连接层结束，接着是softmax操作，它生成句子中最可能的下一个单词。
- en: We can train the full encoder-decoder model using the teacher-forcing process
    we defined in the *Introducing seq2seq* *models* section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在*引入seq2seq* *模型*部分中定义的教师强迫过程来训练完整的编码器-解码器模型。
- en: Next, let’s implement the decoder.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来实现解码器。
- en: Implementing the decoder
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现解码器
- en: 'In this section, we’ll implement the decoder in a similar pattern to the encoder.
    We’ll start with the implementation of the main module, `Decoder`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将以与编码器类似的模式实现解码器。我们将从主模块`Decoder`的实现开始：
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It consists of `N` instances of `DecoderBlock` (`self.blocks`). As we can see
    in the `forward` method, the output of each `DecoderBlock` instance serves as
    input to the next. These are followed by the `self.norm` normalization (an instance
    of `LayerNorm`). The decoder ends with an FC layer (`self.projection`), followed
    by a softmax to produce the most probable next word. Note that the `Decoder.forward`
    method takes an additional parameter, `encoder_states`, which is passed to the
    `DecoderBlock` instances. `encoder_states` represents the encoder output and is
    the link between the encoder and the decoder. In addition, the `source_mask` parameter
    provides the mask of the decoder self-attention.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 它由`N`个`DecoderBlock`实例（`self.blocks`）组成。正如我们在`forward`方法中看到的，每个`DecoderBlock`实例的输出作为下一个实例的输入。这些后面跟着`self.norm`归一化（一个`LayerNorm`实例）。解码器以一个全连接层（`self.projection`）结束，接着是softmax操作，以生成最可能的下一个单词。需要注意的是，`Decoder.forward`方法有一个额外的参数`encoder_states`，该参数会传递给`DecoderBlock`实例。`encoder_states`表示编码器的输出，是编码器和解码器之间的联系。此外，`source_mask`参数提供了解码器自注意力的掩码。
- en: 'Next, let’s implement the `DecoderBlock` class:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来实现`DecoderBlock`类：
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This implementation follows the `EncoderBlock` pattern but is adapted to the
    decoder: in addition to self-attention (`self_attn`), we also have encoder attention
    (`encoder_attn`). Because of this, we instantiate three `sublayers` instances
    (instances of the familiar `SublayerConnection` class): for self-attention, encoder
    attention, and the FFN.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现遵循`EncoderBlock`的模式，但针对解码器进行了调整：除了自注意力（`self_attn`），我们还增加了编码器注意力（`encoder_attn`）。因此，我们实例化了三个`sublayers`实例（即熟悉的`SublayerConnection`类的实例）：分别用于自注意力、编码器注意力和FFN。
- en: We can see the combination of multiple attention mechanisms in the `DecoderBlock.forward`
    method. `encoder_attn` takes as a query the output of the preceding decoder block
    (`x`) and key-value combination from the encoder output (`encoder_states`). In
    this way, regular attention establishes the link between the encoder and the decoder.
    On the other hand, `self_attn` uses `x` for the query, key, and value.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在`DecoderBlock.forward`方法中结合了多种注意力机制。`encoder_attn`以前一个解码器块的输出（`x`）作为查询，并结合编码器输出（`encoder_states`）的键值对。通过这种方式，常规注意力机制在编码器和解码器之间建立了联系。另一方面，`self_attn`使用`x`作为查询、键和值。
- en: This concludes the decoder implementation. We’ll proceed with building the full
    transformer model in the next section.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了解码器的实现。接下来，我们将在下一部分中构建完整的transformer模型。
- en: Putting it all together
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容结合起来
- en: 'We now have implementations of the encoder and the decoder. Let’s combine them
    in the full `EncoderDecoder` class:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了编码器和解码器。接下来让我们将它们组合在一起，构建完整的`EncoderDecoder`类：
- en: '[PRE10]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It combines `encoder`, `decoder`, `source_embeddings`, and `target_embeddings`.
    The `forward` method takes the source sequence and feeds it to `encoder`. Then,
    `decoder` takes its input from the preceding output step (`x=self.target_embeddings(target)`),
    the encoder states (`encoder_states=encoder_output)`, and the source and target
    masks. With these inputs, it produces the predicted next token (word) of the sequence,
    which is also the return value of the `forward` method.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 它将 `encoder`、`decoder`、`source_embeddings` 和 `target_embeddings` 结合在一起。`forward`
    方法接受源序列并将其输入到 `encoder` 中。然后，`decoder` 从前一步的输出中获取输入（`x=self.target_embeddings(target)`），以及编码器的状态（`encoder_states=encoder_output`）和源目标掩码。使用这些输入，它生成预测的下一个词（或标记），这是
    `forward` 方法的返回值。
- en: 'Next, we’ll implement the `build_model` function, which instantiates all the
    classes we implemented to produce a single transformer instance:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `build_model` 函数，该函数实例化我们实现的所有类，生成一个单一的 Transformer 实例：
- en: '[PRE11]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Besides the familiar `MultiHeadedAttention` and `PositionwiseFFN`, we also create
    a `position` variable (an instance of the `PositionalEncoding` class). This class
    implements the static positional encoding we described in the *Transformer encoder*
    section (we won’t include the full implementation here).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了熟悉的 `MultiHeadedAttention` 和 `PositionwiseFFN`，我们还创建了一个 `position` 变量（`PositionalEncoding`
    类的实例）。这个类实现了我们在 *Transformer 编码器* 部分描述的静态位置编码（我们不会在这里包含完整的实现）。
- en: 'Now, let’s focus on the `EncoderDecoder` instantiation: we are already familiar
    with `encoder` and `decoder`, so there are no surprises there. But the embeddings
    are a tad more interesting. The following code instantiates the source embeddings
    (but this is also valid for the target ones):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注 `EncoderDecoder` 的实例化：我们已经熟悉了 `encoder` 和 `decoder`，所以这部分没有什么意外。但嵌入层则更为有趣。以下代码实例化了源嵌入（但这对目标嵌入同样有效）：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can see that they are a sequential list of two components:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，它们是两个组件的顺序列表：
- en: An instance of the `Embeddings` class, which is simply a combination of `torch.nn.Embedding`
    further multiplied by ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math>](img/719.png)
    (we’ll omit the class definition here)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Embeddings` 类的一个实例，它只是 `torch.nn.Embedding` 的组合，进一步乘以 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:math>](img/719.png)（我们这里不包括类定义）'
- en: Positional encoding `c(position)`, which adds the static positional data to
    the embedding vector
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码 `c(position)`，它将静态位置数据添加到嵌入向量中
- en: Once we have the input data preprocessed in this way, it can serve as input
    to the core part of the encoder-decoder.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们以这种方式预处理了输入数据，它就可以作为编码器-解码器核心部分的输入。
- en: In the next section, we’ll discuss the major variants of the transformer architecture.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将讨论 Transformer 架构的主要变体。
- en: Decoder-only and encoder-only models
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅解码器和仅编码器模型
- en: 'So far, we’ve discussed the full encoder-decoder variant of the transformer
    architecture. But in practice, we are going to mostly use two of its variations:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了 Transformer 架构的完整编码器-解码器变体。但在实际应用中，我们主要会使用它的两种变体：
- en: '**Encoder-only**: These models use only the encoder part of the full transformer.
    Encoder-only models are bidirectional, following the properties of encoder self-attention.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅编码器**：这些模型只使用完整 Transformer 的编码器部分。仅编码器模型是双向的，遵循编码器自注意力的特性。'
- en: '**Decoder-only**: These models use only the decoder part of the transformer.
    Decoder-only models are unidirectional, following the properties of the decoder’s
    masked self-attention.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅解码器**：这些模型只使用 Transformer 的解码器部分。仅解码器模型是单向的，遵循解码器的掩蔽自注意力特性。'
- en: I know that these dry definitions sound vague, but don’t worry—in the next two
    sections we’ll discuss one example of each type to make it clear.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这些枯燥的定义听起来有些模糊，但别担心——在接下来的两部分中，我们将讨论每种类型的一个例子来澄清。
- en: Bidirectional Encoder Representations from Transformers
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自 Transformer 的双向编码器表示
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**; see
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)), as the
    name gives away, is an encoder-only (hence bidirectional) model that learns representations.
    These representations serve as a base for solving various downstream tasks (the
    pure BERT model doesn’t solve any specific problem). The following diagram shows
    generic pre-ln and post-ln encoder-only models with softmax outputs (which also
    apply to BERT):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自变压器的双向编码器表示** (**BERT**; 详见 [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805))，顾名思义，是一种仅编码器（因此是双向的）模型，用于学习表示。这些表示作为解决各种下游任务的基础（纯BERT模型不解决任何特定问题）。以下图表显示了通用的前层归一化和后层归一化编码器模型，带有softmax输出（这也适用于BERT）：'
- en: '![Figure 7.10 – Left: post-ln encoder-only model; right: pre-ln encoder-only
    model](img/B19627_07_10.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 左：后层归一化编码器模型；右：前层归一化编码器模型](img/B19627_07_10.jpg)'
- en: 'Figure 7.10 – Left: post-ln encoder-only model; right: pre-ln encoder-only
    model'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 左：后层归一化编码器模型；右：前层归一化编码器模型
- en: BERT model sizes
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型规格
- en: BERT comes in two variations![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>—</mml:mo><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>BASE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/720.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LARGE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/721.png).
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>BASE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/722.png)
    has 12 encoder blocks, each with 12 attention heads, 768-dimensional attention
    vectors (the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/723.png)
    parameter), and a total of 110M parameters. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LARGE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/724.png)
    has 24 encoder blocks, each with 16 attention heads, 1,024-dimensional attention
    vectors, and a total of 340M parameters. The models use WordPiece tokenization
    and have a 30,000-token vocabulary.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 有两种变体![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>—</mml:mo><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>BASE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/720.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LARGE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/721.png)。
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mrow><mml:mrow><mml:mtext>BASE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/722.png)
    包含 12 个编码器块，每个块有 12 个注意力头，768 维注意力向量（![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/723.png)
    参数），总共 110M 参数。 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>BER</mml:mtext><mml:msub><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LARGE</mml:mtext></mml:mrow></mml:msub></mml:math>](img/724.png)
    包含 24 个编码器块，每个块有 16 个注意力头，1,024 维注意力向量，总共 340M 参数。这些模型使用 WordPiece 分词，并有一个 30,000
    个词汇量。
- en: 'Let’s start with the way BERT represents its input data, which is an important
    part of its architecture. We can see an input data representation in the following
    diagram:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从BERT表示其输入数据的方式开始，这是其架构的重要部分。我们可以在以下图表中看到输入数据表示：
- en: '![Figure 7.11 – BERT input embeddings as the sum of the token embeddings, the
    segmentation embeddings, and the position embeddings (source: https://arxiv.org/abs/1810.04805)](img/B19627_07_11.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – BERT 输入嵌入为令牌嵌入、分段嵌入和位置嵌入之和 (来源：https://arxiv.org/abs/1810.04805)](img/B19627_07_11.jpg)'
- en: 'Figure 7.11 – BERT input embeddings as the sum of the token embeddings, the
    segmentation embeddings, and the position embeddings (source: https://arxiv.org/abs/1810.04805)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – BERT输入嵌入作为标记嵌入、分段嵌入和位置嵌入的总和（来源：https://arxiv.org/abs/1810.04805）
- en: 'Because BERT is encoder-only, it has two special modes of input data representation
    so that it can handle a variety of downstream tasks:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BERT是仅编码器模型，它有两种特殊的输入数据表示模式，使其能够处理各种下游任务：
- en: A single sequence (for example, in classification tasks, such as **sentiment
    analysis**, or **SA**)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一序列（例如，在分类任务中，如**情感分析**，或**SA**）
- en: A pair of sequences (for example, machine translation or **question-answering**
    (**QA**) problems)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对序列（例如，机器翻译或**问答**（**QA**）问题）
- en: The first token of every sequence is always a special classification token,
    `[CLS]`. The encoder output, corresponding to this token, is used as the aggregate
    sequence representation for classification tasks. For example, if we want to apply
    SA over the sequence, the output corresponding to the `[CLS]` input token will
    represent the sentiment (positive/negative) output of the model (this example
    is relevant when the input data is a single sequence). This is necessary because
    the `[CLS]` token acts as a query, while all other elements of the input sequence
    act as the key/value store. In this way, all tokens of the sequence participate
    in the weighted attention vector, which serves as input to the rest of the model.
    Selecting another token besides `[CLS]` excludes this token from the attention
    formula, which introduces unfair bias against it and results in an incomplete
    sequence.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 每个序列的第一个标记总是一个特殊的分类标记`[CLS]`。与此标记对应的编码器输出作为分类任务的聚合序列表示。例如，如果我们想对序列进行SA分析，`[CLS]`输入标记对应的输出将表示模型的情感（正面/负面）输出（当输入数据是单一序列时，此示例相关）。这是必要的，因为`[CLS]`标记充当查询，而输入序列的其他所有元素充当键/值存储。通过这种方式，序列中的所有标记都参与加权注意力向量，它作为输入进入模型的其余部分。选择除了`[CLS]`以外的其他标记将把该标记排除在注意力公式之外，这会导致对该标记的不公平偏见，并导致不完整的序列。
- en: If the input data is a pair of sequences, we pack them together in a single
    sequence, separated by a special `[SEP]` token. On top of that, we have additional
    learned segmentation embedding for every token, which indicates whether it belongs
    to sequence *A* or sequence *B*. Therefore, the input embeddings are the sum of
    the token embeddings, the segmentation embeddings, and the position embeddings.
    Here, the token and position embeddings serve the same purpose as they do in the
    regular transformer.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据是一个序列对，我们将它们合并成一个单一的序列，用特殊的`[SEP]`标记分隔。除此之外，我们为每个标记添加了额外学习到的分段嵌入，表示它属于序列
    *A* 还是序列 *B*。因此，输入嵌入是标记嵌入、分段嵌入和位置嵌入的总和。在这里，标记嵌入和位置嵌入的作用与常规的Transformer相同。
- en: Now that we are familiar with the input data representation, let’s continue
    with the training.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了输入数据的表示形式，让我们继续进行训练。
- en: BERT training
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT训练
- en: 'BERT training is a two-step process (this is also valid for other transformer-based
    models):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: BERT训练是一个两步过程（这对其他基于Transformer的模型同样适用）：
- en: '**Pre-training**: Train the model with unlabeled data over different pre-training
    tasks'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练**：在不同的预训练任务上使用无标签数据训练模型。'
- en: '**Fine-tuning**: A form of **transfer learning** (**TL**), where we initialize
    the model with the pre-trained parameters and fine-tune them over a labeled dataset
    of a specific downstream task'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：一种**迁移学习**（**TL**）的形式，我们通过预训练的参数初始化模型，并在特定下游任务的有标签数据集上进行微调。'
- en: 'We can see the pre-training on the left side and the fine-tuning on the right
    side of the following diagram:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的图中看到左侧是预训练，右侧是微调：
- en: '![Figure 7.12 – Left: pre-training; right: fine-tuning (source: https://arxiv.org/abs/1810.04805)](img/B19627_07_12.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 左：预训练；右：微调（来源：https://arxiv.org/abs/1810.04805）](img/B19627_07_12.jpg)'
- en: 'Figure 7.12 – Left: pre-training; right: fine-tuning (source: https://arxiv.org/abs/1810.04805)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 左：预训练；右：微调（来源：https://arxiv.org/abs/1810.04805）
- en: Here, **Tok N** represents the one-hot-encoded input tokens, **E** represents
    the token embeddings, and **T** represents the model output vector. The topmost
    labels represent the different tasks we can use the model for in each of the training
    modes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**Tok N**表示一热编码输入标记，**E**表示标记嵌入，**T**表示模型输出向量。最顶部的标签表示我们可以在每种训练模式下使用模型进行的不同任务。
- en: 'The authors of the paper pre-trained the model using two unsupervised training
    tasks: **masked language modeling** (**MLM**) and **next sentence** **prediction**
    (**NSP**).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的作者通过两个无监督的训练任务对模型进行了预训练：**掩码语言模型**（**MLM**）和**下一句预测**（**NSP**）。
- en: We’ll start with MLM, where the model is presented with an input sequence and
    its goal is to predict a missing word in that sequence. MLM is similar in nature
    to the `[MASK]` token (80% of the time), a random word (10% of the time), or leave
    the word as is (10% of the time). This is necessary because the vocabulary of
    the downstream tasks doesn’t have the `[MASK]` token. On the other hand, the pre-trained
    model might expect it, which could lead to unpredictable behavior.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从MLM（掩码语言模型）开始，其中模型输入一个序列，目标是预测该序列中缺失的单词。MLM本质上类似于`[MASK]`标记（80%的情况）、一个随机单词（10%的情况），或者保持单词不变（10%的情况）。这是必要的，因为下游任务的词汇表中没有`[MASK]`标记。另一方面，预训练模型可能会期望它，这可能导致不可预测的行为。
- en: Next, let’s continue with NSP. The authors argue that many important downstream
    tasks, such as question answering or **natural language inference** (**NLI**),
    are based on understanding the relationship between two sentences, which is not
    directly captured by language modeling.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续讨论NSP（下一句预测）。作者认为，许多重要的下游任务，如问答或**自然语言推理**（**NLI**），基于理解两个句子之间的关系，而这种关系并未被语言建模直接捕捉。
- en: NLI
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: NLI
- en: 'NLI determines whether a sentence, which represents a **hypothesis**, is either
    true (**entailment**), false (**contradiction**), or undetermined (**neutral**)
    given another sentence, called a **premise**. For example, given the premise *I
    am running*, we have the following hypothesis: *I am sleeping* is false; *I am
    listening to music* is undetermined; *I am training* is true.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: NLI（自然语言推理）决定一个句子（表示**假设**）在给定另一个句子（称为**前提**）的情况下，是否为真（**蕴含**）、假（**矛盾**）或未确定（**中立**）。例如，给定前提*我在跑步*，我们有以下假设：*我在睡觉*是假的；*我在听音乐*是未确定的；*我在训练*是真的。
- en: The authors of BERT propose a simple and elegant unsupervised solution to pre-train
    the model to understand sentence relationships (displayed on the left side of
    *Figure 7**.12*). We’ll train the model on binary classification, where each input
    sample starts with a `[CLS]` token and consists of two sequences (let’s use sentences
    for simplicity), *A* and *B*, separated by a `[SEP]` token. We’ll extract sentences
    *A* and *B* from the training corpus. In 50% of the training samples, *B* is the
    actual next sentence that follows *A* (labeled as `is_next`). In the other 50%,
    *B* is a random sentence from the corpus (`not_next`). As we mentioned, the model
    outputs `is_next`/`not_next` labels on the `[CLS]` corresponding input.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的作者提出了一种简单优雅的无监督解决方案来预训练模型，理解句子之间的关系（如*图7.12*左侧所示）。我们将模型训练为二分类任务，每个输入样本以`[CLS]`标记开始，由两个序列（为了简便，我们称之为句子*A*和*B*）组成，这两个序列通过`[SEP]`标记分隔。我们将从训练语料库中提取句子*A*和*B*。在50%的训练样本中，*B*是紧跟*A*的实际下一个句子（标记为`is_next`）。在另外50%的样本中，*B*是语料库中的随机句子（标记为`not_next`）。如我们所提到，模型将在与输入对应的`[CLS]`上输出`is_next`/`not_next`标签。
- en: Next, let’s focus on the fine-tuning task, which follows the pre-training task
    (the right side of *Figure 7**.12*). The two steps are very similar, but instead
    of creating a masked sequence, we simply feed the BERT model with the task-specific
    unmodified input and output and fine-tune all the parameters in an end-to-end
    fashion. Therefore, the model that we use in the fine-tuning phase is the same
    model that we’ll use in the actual production environment.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们专注于微调任务，它是在预训练任务之后进行的（如*图7.12*右侧所示）。这两个步骤非常相似，但不同的是，我们不再创建一个掩码序列，而是将任务特定的未修改输入和输出直接输入BERT模型，并以端到端的方式微调所有参数。因此，我们在微调阶段使用的模型与我们将在实际生产环境中使用的模型相同。
- en: Let’s continue with some of the downstream tasks we can solve with BERT.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探讨一些可以通过BERT解决的下游任务。
- en: BERT downstream tasks
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT下游任务
- en: 'The following diagram shows how to solve several different types of tasks with
    BERT:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了如何通过BERT解决几种不同类型的任务：
- en: '![Figure 7.13 – BERT applications for diﬀerent tasks (source: https://arxiv.org/abs/1810.04805)](img/B19627_07_13.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – BERT在不同任务中的应用（来源：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805))](img/B19627_07_13.jpg)'
- en: 'Figure 7.13 – BERT applications for diﬀerent tasks (source: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805))'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – BERT在不同任务中的应用（来源：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)）
- en: 'Let’s discuss them:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论一下这两个任务：
- en: The top-left scenario illustrates how to use BERT for sentence-pair classification
    tasks, such as NLI. In short, we feed the model with two concatenated sentences
    and only look at the `[CLS]` token output classification, which will output the
    model result. For example, in an NLI task, the goal is to predict whether the
    second sentence is an entailment, a contradiction, or neutral with respect to
    the first one.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左上方的场景展示了如何使用BERT进行句子对分类任务，如NLI。简而言之，我们将两个连接的句子输入模型，并只查看`[CLS]`令牌的输出分类，进而得出模型结果。例如，在NLI任务中，目标是预测第二个句子与第一个句子的关系，是蕴含、矛盾还是中立。
- en: The top-right scenario illustrates how to use BERT for single-sentence classification
    tasks, such as SA. This is very similar to sentence-pair classification. In both
    cases, we’ll extend the encoder with an FC layer and a binary softmax, with *N*
    possible classes (*N* is the number of classes for each task).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右上方的场景展示了如何使用BERT进行单句分类任务，如SA。这与句子对分类非常相似。在这两种情况下，我们都将编码器扩展为一个全连接层（FC层）和一个二进制Softmax，并有*N*个可能的类别（*N*是每个任务的类别数）。
- en: The bottom-left scenario illustrates how to use BERT on a QA dataset. Given
    that sequence *A* is a question and sequence *B* is a passage from *Wikipedia*,
    which contains the answer, the goal is to predict the text span (start and end)
    of the answer within this passage. The model outputs the probability for each
    token of sequence *B* to be either the start or the end of the answer.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左下方的场景展示了如何在QA数据集上使用BERT。假设序列*A*是一个问题，序列*B*是来自*Wikipedia*的段落，包含答案，目标是预测答案在该段落中的文本跨度（起始和结束）。模型输出序列*B*中每个令牌作为答案起始或结束的概率。
- en: The bottom-right scenario illustrates how to use BERT for **named entity recognition**
    (**NER**), where each input token is classified as some type of entity.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右下方的场景展示了如何使用BERT进行**命名实体识别**（**NER**），其中每个输入令牌都被分类为某种类型的实体。
- en: This concludes our section dedicated to the BERT model. Next, let’s focus on
    decoder-only models.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对BERT模型部分的介绍。接下来，让我们聚焦于仅解码器模型。
- en: Generative Pre-trained Transformer
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成预训练变换器
- en: In this section, we’ll discuss a decoder-only model, known as **Generative Pre-trained
    Transformer** (**GPT**; see *Improving Language Understanding by Generative Pre-Training*,
    [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)).
    This is the first of a series of GPT models, released by OpenAI, which led to
    the now-famous GPT-3 and GPT-4.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一个仅解码器模型，称为**生成预训练变换器**（**GPT**；参见*通过生成预训练提升语言理解*，[https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)）。这是OpenAI发布的一系列GPT模型中的第一个，后续推出了如今著名的GPT-3和GPT-4。
- en: GPT model size
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的规模
- en: GPT has 12 decoder layers, each with 12 attention heads, and 768-dimensional
    attention vectors. The FFN is 3,072-dimensional. The model has a total of 117
    million parameters (weights). GPT uses BPE tokenization and has a token vocabulary
    size of 40,000.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: GPT有12个解码器层，每个层有12个注意力头，768维度的注意力向量。FFN的维度为3,072。该模型共有1.17亿个参数（权重）。GPT使用BPE分词，并且具有40,000个令牌词汇表。
- en: 'We can see the GPT decoder-only architecture in the following diagram:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的图中看到GPT仅解码器架构：
- en: '![Figure 7.14 – Left: post-ln decoder-only model; right: pre-ln decoder-only
    model; different outputs for the pre-training and fine-tuning training steps](img/B19627_07_14.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 左：后-ln解码器模型；右：前-ln解码器模型；预训练和微调训练步骤的不同输出](img/B19627_07_14.jpg)'
- en: 'Figure 7.14 – Left: post-ln decoder-only model; right: pre-ln decoder-only
    model; different outputs for the pre-training and fine-tuning training steps'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 左：后-ln解码器模型；右：前-ln解码器模型；预训练和微调训练步骤的不同输出
- en: Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: We discuss the decoder-only architecture in the context of the original GPT
    paper, but it applies to the broad class of decoder-only models.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在原始GPT论文的背景下讨论仅解码器架构，但它适用于广泛的仅解码器模型类。
- en: 'It is derived from the decoder we discussed in the *Transformer decoder* section.
    The model takes as input token embeddings and adds static positional encoding.
    This is followed by a stack of *N* decoder blocks. Each block has two sublayers:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 它源自我们在*Transformer解码器*部分讨论的解码器。该模型将令牌嵌入作为输入，并添加静态位置编码。接着是一个*N*个解码器块的堆叠。每个块有两个子层：
- en: '**Masked multi-head self-attention**: Let’s put emphasis on the masked part.
    It determines the main properties of the decoder-only model—it is unidirectional
    and autoregressive. This is opposed to bidirectional encoder-only models.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩蔽的多头自注意力**：我们重点强调“掩蔽”部分。它决定了仅解码器模型的主要特性——单向性和自回归性。这与双向的编码器模型是对立的。'
- en: '**FFN**: This sublayer has the same purpose as in the encoder-decoder model.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FFN**：这个子层的作用与编码器-解码器模型中的作用相同。'
- en: The sublayers contain the relevant residual links, normalization, and dropout.
    The decoder comes in pre-ln and post-ln flavors.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 子层包含相关的残差链接、归一化和丢弃层。解码器有预-ln和后-ln两种形式。
- en: The model ends with an FC layer, followed by a softmax operation, which can
    be adapted to suit the specific task at hand.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以一个全连接层（FC）结束，后面跟着一个softmax操作，可以根据具体任务进行调整。
- en: The main difference between this decoder and the one in the full encoder-decoder
    transformer is the lack of an attention sublayer, which links the encoder and
    decoder blocks in the full model. Since the current architecture doesn’t have
    an encoder part, the sublayer is obsolete. This makes the decoder very similar
    to the encoder, except for masked self-attention. Hence, the main difference between
    the encoder-only and decoder-only models is that they are bidirectional and unidirectional,
    respectively.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 该解码器与完整编码器-解码器转换器中的解码器的主要区别在于缺少一个注意力子层，该子层在完整模型中连接编码器和解码器部分。由于当前架构没有编码器部分，因此该子层已不再使用。这使得解码器与编码器非常相似，除了掩蔽的自注意力。由此，编码器模型和解码器模型之间的主要区别在于它们分别是双向和单向的。
- en: 'As with BERT, the training of GPT is a two-step process, which consists of
    unsupervised pre-training and supervised fine-tuning. Let’s start with the pre-training,
    which resembles the seq2seq training algorithm (the decoder part) we described
    in the *Introducing seq2seq models* section. As a reminder, we train the original
    seq2seq model to transform an input sequence of tokens into another, different
    output sequence of tokens. Examples of such tasks include machine translation
    and question answering. The original seq2seq training is supervised because matching
    the input and output sequences counts as labeling. Once the full input sequence
    is fed into the seq2seq encoder, the decoder starts generating the output sequence
    one token at a time. In effect, the seq2seq decoder learns to predict the next
    word in the sequence (as opposed to predicting any masked word in the full sequence,
    as with BERT). Here, we have a similar algorithm, but the output sequence is the
    same as the input sequence. From a language modeling point of view, the pre-training
    learns to approximate the conditional probability of the next token, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/725.png),
    given an input sequence of tokens, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/726.png),
    and the model parameters, *θ*: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/727.png).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT类似，GPT的训练也是一个两步过程，包括无监督的预训练和有监督的微调。我们先从预训练开始，它类似于我们在*介绍seq2seq模型*部分描述的seq2seq训练算法（解码器部分）。提醒一下，我们训练原始的seq2seq模型，将输入的标记序列转换为另一个不同的输出标记序列。这类任务的例子包括机器翻译和问答。原始的seq2seq训练是有监督的，因为匹配输入和输出序列相当于标签化。一旦完整的输入序列被送入seq2seq编码器，解码器就开始一次生成一个标记的输出序列。实际上，seq2seq解码器学会了预测序列中的下一个单词（与BERT预测完整序列中任何被遮蔽的单词不同）。在这里，我们有一个类似的算法，但输出序列与输入序列相同。从语言建模的角度来看，预训练学习的是近似下一个标记的条件概率，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/725.png)，给定输入标记序列![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/726.png)和模型参数*θ*：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/727.png)。
- en: 'Let’s illustrate the pre-training with an example. We’ll assume that our input
    sequence is `[[START],` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``,`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/729.png)``,
    ...,` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/730.png)``]`
    and that we’ll denote the training pairs as `{input: label}`. Our training pairs
    are going to be `{[[START]]:` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``}`,
    `{[[START],` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``]:`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/729.png)``}`,
    and `{[[START],` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``,...,`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/730.png)``]:`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math>](img/736.png)``}`.
    We can see the same scenario displayed in the following diagram:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们通过一个例子来说明预训练。我们假设输入序列为`[[START],` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``,`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/729.png)``,
    ...,` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/730.png)``]`，我们将训练对标记为`{input:
    label}`。我们的训练对为`{[[START]]:` `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``}`，`{[[START],`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``]:`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/729.png)``}`，以及`{[[START],`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/728.png)``,...,`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/730.png)``]:`
    `![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math>](img/736.png)``}`。我们可以从下图中看到相同的场景：'
- en: '![Figure 7.15 – GPT pre-training to predict the next word of the same input/output
    sequence](img/B19627_07_15.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – GPT预训练以预测相同输入/输出序列的下一个词](img/B19627_07_15.jpg)'
- en: Figure 7.15 – GPT pre-training to predict the next word of the same input/output
    sequence
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – GPT预训练以预测相同输入/输出序列的下一个词
- en: 'Next, let’s discuss the supervised fine-tuning step, which is similar to BERT
    fine-tuning. The following diagram illustrates how the tasks of sequence classification
    and NLI work in GPT:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论监督式微调步骤，这与BERT的微调类似。以下图示说明了在GPT中，序列分类和NLI任务是如何工作的：
- en: '![Figure 7.16 – GPT fine-tuning; top: text classification; bottom: NLI](img/B19627_07_16.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – GPT微调；上：文本分类；下：自然语言推理（NLI）](img/B19627_07_16.jpg)'
- en: 'Figure 7.16 – GPT fine-tuning; top: text classification; bottom: NLI'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – GPT微调；上：文本分类；下：自然语言推理（NLI）
- en: In both cases, we have special `[START]` and `[EXTRACT]` tokens. The `[EXTRACT]`
    token plays the same role as `[CLS]` in BERT—we take the output of that token
    as the result of the classification. But here, it’s at the end of the sequence,
    rather than the start. Again, the reason for this is that the decoder is unidirectional
    and only has full access to the input sequence at its end. The NLI task concatenates
    the premise and the entailment, separated by a special `[``DELIM]` token.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们都有特殊的`[START]`和`[EXTRACT]`标记。`[EXTRACT]`标记在这里起着与BERT中的`[CLS]`相同的作用——我们将该标记的输出作为分类结果。但是在这里，它位于序列的末尾，而不是开头。再次说明，这样做的原因是解码器是单向的，并且只能在序列末尾完全访问输入序列。NLI任务将前提和蕴含通过一个特殊的`[`DELIM]`标记连接起来。
- en: This concludes our introduction to GPT—the prototypical example of a decoder-only
    model. With this, we’ve introduced the three major transformer architectures—encoder-decoder,
    encoder-only, and decoder-only. This is also a good place to conclude the chapter.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对GPT的介绍——解码器-only模型的典型示例。至此，我们已经介绍了三种主要的变换器架构——编码器-解码器、仅编码器和仅解码器。这也是本章的一个良好总结。
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Our focus in this chapter was the attention mechanism and transformers. We started
    with the seq2seq model, and we discussed Bahdanau and Luong attention in its context.
    Next, we gradually introduced the TA mechanism, before discussing the full encoder-decoder
    transformer architecture. Finally, we focused on encoder-only and decoder-only
    transformer variants.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是注意力机制和变换器。我们从seq2seq模型开始，并讨论了Bahdanau和Luong注意力机制。在此基础上，我们逐步引入了TA机制，然后讨论了完整的编码器-解码器变换器架构。最后，我们专注于仅编码器和仅解码器的变换器变种。
- en: In the next chapter, we’ll focus on LLMs, and we’ll explore the Hugging Face
    transformers library.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '在下一章，我们将专注于LLM，并探讨Hugging Face的变换器库。  '
