- en: Understanding Convolutional Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积网络
- en: In this chapter, we'll discuss **Convolutional Neural Networks** (**CNNs**)
    and their applications in **Computer Vision** (**CV**). CNNs started the modern
    deep learning revolution. They are at the base of virtually all recent CV advancements,
    including **Generative Adversarial Networks** (**GANs**), object detection, image
    segmentation, neural style transfer, and much more. For this reason, we believe
    CNNs deserve an in-depth look that's beyond our basic understanding of them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论**卷积神经网络**（**CNNs**）及其在**计算机视觉**（**CV**）中的应用。CNNs启动了现代深度学习的革命。它们是几乎所有最近CV进展的基础，包括**生成对抗网络**（**GANs**）、目标检测、图像分割、神经风格迁移等。因此，我们认为CNNs值得深入探讨，超越我们对它们的基本理解。
- en: To do this, we'll start with a short recap of the CNN building blocks, that
    is, the convolutional and pooling layers. We'll discuss the various types of convolutions
    in use today since they are reflected in a large number of CNN applications. We'll
    also learn how to visualize the internal state of CNNs. Then, we'll focus on regularization
    techniques and implement a transfer learning example.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将从简短回顾CNN的构建块开始，也就是卷积层和池化层。我们将讨论今天使用的各种类型的卷积，因为它们在大量的CNN应用中有所体现。我们还将学习如何可视化CNN的内部状态。接着，我们将重点讲解正则化技术并实现一个迁移学习的例子。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding CNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CNN
- en: Introducing transfer learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入迁移学习
- en: Understanding CNNs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CNN
- en: 'In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The* *Nuts and
    Bolts of Neural Networks,* we discussed that many NN operations have solid mathematical
    foundations, and convolutions are no exception. Let''s start by defining the mathematical
    convolution:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中，*《神经网络的基础》*，我们讨论了许多NN操作都有坚实的数学基础，卷积也不例外。让我们从定义数学卷积开始：
- en: '![](img/f3bce005-8da4-479a-bb99-b56cc6234765.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3bce005-8da4-479a-bb99-b56cc6234765.png)'
- en: 'Here, we have the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: The convolution operation is denoted with *.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积运算用*表示。
- en: '*f* and *g* are two functions with a common parameter, *t*.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*和*g*是具有共同参数*t*的两个函数。'
- en: The result of the convolution is a third function, *s(t)* (not just a single
    value).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积的结果是第三个函数，*s(t)*（而不仅仅是一个单一的值）。
- en: The convolution of *f* and *g* at value *t* is the integral of the product of *f(t)*
    and the reversed (mirrored) and shifted value of *g(t-τ)*, where *t-τ* represents
    the shift. That is, for a single value of *f* at time *t*, we shift *g* in the
    range ![](img/27cdbe02-f140-4acb-bcea-3114eae42a4a.png) and we compute the product
    *f(t)**g(t-τ)* continuously because of the integral. The integral (and hence the
    convolution) is equivalent to the area under the curve of the product of the two
    functions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*和*g*在值*t*处的卷积是*f(t)*与*g(t-τ)*的逆向（镜像）和位移后的值的乘积的积分，其中*t-τ*表示位移。也就是说，对于时间*t*上*f*的单个值，我们在范围内平移*g*
    ![](img/27cdbe02-f140-4acb-bcea-3114eae42a4a.png)，并且由于积分的缘故，我们不断计算乘积*f(t)**g(t-τ)*。积分（因此卷积）等同于两个函数乘积曲线下的面积。'
- en: 'This is best illustrated in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示最能说明这一点：
- en: '![](img/c8d8a4c1-7a56-460c-a0c3-7460343330a1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8d8a4c1-7a56-460c-a0c3-7460343330a1.png)'
- en: 'Left: a convolution, where *g* is shifted and reversed; right: a step-by-step
    illustration of a convolution operation'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 左：一个卷积，其中*g*被平移并反转；右：卷积运算的逐步演示
- en: In the convolution operation, *g* is shifted and reversed in order to preserve
    the operation's commutative property. In the context of CNNs, we can ignore this
    property and we can implement it without reversing *g*. In this case, the operation
    is called cross-correlation. These two terms are used interchangeably.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积运算中，为了保持运算的交换性，*g*被平移并反转。在CNN的上下文中，我们可以忽略这个特性，并且我们可以在不反转*g*的情况下实现它。在这种情况下，这个运算称为互相关。这两个术语是可以互换使用的。
- en: 'We can define the convolution for discrete (integer) values of *t* with the
    following formula (which is very similar to the continuous case):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下公式为离散（整数）值*t*定义卷积（这与连续情况非常相似）：
- en: '![](img/93ecc38f-ba32-4cc5-b628-ec8f975db95b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93ecc38f-ba32-4cc5-b628-ec8f975db95b.png)'
- en: 'We can also generalize it to the convolution of functions with two shared input
    parameters, *i* and *j*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将其推广到具有两个共享输入参数*i*和*j*的函数卷积：
- en: '![](img/16ae140a-3930-4c85-99f1-04f0c49ff727.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16ae140a-3930-4c85-99f1-04f0c49ff727.png)'
- en: We can derive the formula in a similar manner for three parameters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以类似的方式推导出三个参数的公式。
- en: 'In CNNs, the function *f* is the input of the convolution operation (also referred
    to as the convolutional layer). Depending on the number of input dimensions, we
    have 1D, 2D, or 3D convolutions. А time series input is a 1D vector, an image
    input is a 2D matrix, and a 3D point cloud is a 3D tensor. The function *g*, on
    the other hand, is called a kernel (or filter). It has the same number of dimensions
    as the input data and it is defined by a set of learnable weights. For example,
    a filter of size *n* for a 2D convolution is an *n×n* matrix. The following diagram
    illustrates a 2D convolution with a 2×2filter applied over a single 3×3 slice:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNNs）中，函数 *f* 是卷积操作的输入（也称为卷积层）。根据输入维度的数量，我们有 1D、2D 或 3D 卷积。时间序列输入是 1D
    向量，图像输入是 2D 矩阵，3D 点云是 3D 张量。另一方面，函数 *g* 被称为核（或滤波器）。它的维度与输入数据相同，并由一组可学习的权重定义。例如，2D
    卷积的大小为 *n* 的滤波器是一个 *n×n* 矩阵。下图展示了 2D 卷积，使用 2×2 滤波器应用于单个 3×3 切片：
- en: '![](img/a52c6aef-cfd2-4f27-bc85-20649b591ca1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a52c6aef-cfd2-4f27-bc85-20649b591ca1.png)'
- en: 2D convolution with a 2×2filter applied over a single 3×3 slice
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 卷积，使用 2×2 滤波器应用于单个 3×3 切片
- en: 'The convolution works as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的工作原理如下：
- en: We slide the filter along all of the dimensions of the input tensor.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将滤波器滑动通过输入张量的所有维度。
- en: At every input position, we multiply each filter weight by its corresponding
    input tensor cell at the given location. The input cells, which contribute to
    a single output cell, are called **receptive fields**. We sum all of these values
    to produce the value of a single output cell.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个输入位置，我们将每个滤波器权重与给定位置对应的输入张量单元相乘。贡献到单个输出单元的输入单元称为 **感受野**。我们将所有这些值相加，得到单个输出单元的值。
- en: Unlike fully-connected layers, where each output unit gathers information from
    all of the inputs, the activation of a convolution output cell is determined by
    the inputs in its receptive field. This principle works best for hierarchically
    structured data such as images. For example, neighboring pixels form meaningful
    shapes and objects, but a pixel at one end of the image is unlikely to have a
    relationship with a pixel at another end. Using a fully-connected layer to connect
    all of the input pixels with each output unit is like asking the network to find
    a needle in a haystack. It has no way of knowing whether an input pixel is in
    the receptive field of the output unit or not.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与全连接层不同，在全连接层中，每个输出单元都从所有输入中收集信息，而卷积输出单元的激活是由其感受野中的输入决定的。这个原理最适用于层次结构数据，例如图像。例如，邻近的像素形成有意义的形状和物体，但图像一端的像素与另一端的像素之间不太可能有关系。使用全连接层将所有输入像素与每个输出单元连接，就像是让网络在大海捞针。它无法知道某个输入像素是否在输出单元的感受野内。
- en: The filter highlights some particular features in the receptive field. The output
    of the operation is a tensor (known as a feature map), which marks the locations
    where the feature is detected. Since we apply the same filter throughout the input
    tensor, the convolution is translation invariant; that is, it can detect the same
    features, regardless of their location on the image. However, the convolution
    is neither rotation invariant (it is not guaranteed to detect a feature if it's
    rotated), nor scale invariant (it is not guaranteed to detect the same artifact
    in different scales).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该滤波器在感受野中突出显示某些特定特征。操作的输出是一个张量（称为特征图），标记了检测到特征的位置。由于我们在整个输入张量上应用相同的滤波器，卷积是平移不变的；也就是说，它可以检测到相同的特征，无论它们在图像中的位置如何。然而，卷积既不是旋转不变的（如果特征旋转，它不保证检测到该特征），也不是尺度不变的（它不保证在不同尺度下检测到相同的特征）。
- en: 'In the following diagram, we can see examples of 1D and 3D convolutions (we''ve
    already introduced an example of a 2D convolution):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到 1D 和 3D 卷积的示例（我们已经介绍了 2D 卷积的一个示例）：
- en: '![](img/6f84dd7f-8286-4cd7-9442-c79457e6322e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f84dd7f-8286-4cd7-9442-c79457e6322e.png)'
- en: '1D convolution: The filter (denoted with multicolored lines) slides over a
    single axis; 3D convolution: The filter (denoted with dashed lines) slides over
    three axes'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 卷积：滤波器（用多色线表示）沿单一轴滑动；3D 卷积：滤波器（用虚线表示）沿三个轴滑动
- en: The CNN convolution can have multiple filters, highlighting different features,
    which results in multiple output feature maps (one for each filter). It can also
    gather input from multiple feature maps, for example, the output of a previous
    convolution. The combination of feature maps (input or output) is called a volume.
    In this context, we can also refer to the feature maps as slices. Although the
    two terms refer to the same thing, we can think of the slice as part of the volume,
    whereas the feature map highlights its role as, well, a feature map.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 卷积可以有多个滤波器，用来突出不同的特征，这样就会生成多个输出特征图（每个滤波器一个）。它也可以从多个特征图中收集输入，例如，来自先前卷积的输出。特征图（输入或输出）的组合称为体积。在这个上下文中，我们也可以将特征图称为切片。虽然这两个术语指的是同一事物，但我们可以把切片看作是体积的一部分，而特征图则突出了它作为特征图的角色。
- en: As we mentioned earlier in this section, each volume (as well as the filter)
    is represented by a tensor. For example, a red, green, and blue (RGB) image is
    represented by a 3D tensor of three 2D slices (one slice per color channel). But
    in the context of CNNs, we add one more dimension for the sample index in the
    mini-batch. Here, a 1D convolution would have 3D input and output tensors. Their
    axes can be in either *NCW* or *NWC* order, where *N* is the index of the sample
    in the mini-batch, *C* is the index of the depth slice in the volume, and *W*
    is the vector size of each sample. In the same way, a 2D convolution will be represented
    by *NCHW* or *NHWC* tensors, where *H* and *W* are the height and width of the
    slices. A 3D convolution will have an *NCLHW* or *NLHWC* order, where *L* stands
    for the depth of the slice.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节前面提到的，每个体积（以及滤波器）都是通过张量表示的。例如，一个红、绿、蓝（RGB）图像通过一个包含三个 2D 切片（每个颜色通道一个切片）的
    3D 张量表示。但是，在 CNN 的上下文中，我们为每个样本的 mini-batch 添加了一个维度。在这种情况下，1D 卷积将具有 3D 输入和输出张量。它们的轴可以是
    *NCW* 或 *NWC* 顺序，其中 *N* 是 mini-batch 中样本的索引，*C* 是体积中深度切片的索引，*W* 是每个样本的向量大小。以相同的方式，2D
    卷积将由 *NCHW* 或 *NHWC* 张量表示，其中 *H* 和 *W* 分别是切片的高度和宽度。3D 卷积将采用 *NCLHW* 或 *NLHWC*
    顺序，其中 *L* 表示切片的深度。
- en: We use 2D convolutions to work with RGB images. However, we may consider the
    three colors an additional dimension, hence making the RGB image 3D. Why didn't
    we use 3D convolutions, then? The reason for this is that, even though we can
    think of the input as 3D, the output is still a 2D grid. Had we used 3D convolution,
    the output would also be 3D, which doesn't carry any meaning in the case of 2D
    images.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 2D 卷积来处理 RGB 图像。然而，我们也可以将三种颜色视为一个额外的维度，从而将 RGB 图像视为 3D。那么为什么我们不使用 3D 卷积呢？原因是，尽管我们可以把输入看作是
    3D 的，但输出仍然是一个 2D 网格。如果我们使用 3D 卷积，输出也会是 3D 的，而在 2D 图像的情况下，这没有任何意义。
- en: Let's say we have *n* input and *m* output slices. In this case, we'll apply
    *m* filters across the set of *n* input slices. Each filter will generate a unique
    output slice that highlights the feature that was detected by the filter (*n*
    to *m* relationship).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 *n* 个输入切片和 *m* 个输出切片。在这种情况下，我们会对 *n* 个输入切片应用 *m* 个滤波器。每个滤波器将生成一个独特的输出切片，突出显示该滤波器检测到的特征（*n*
    到 *m* 的关系）。
- en: 'Depending on the relationship of the input and output slice, we get cross-channel
    and depth-wise convolutions, as illustrated in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输入和输出切片的关系，我们可以得到跨通道卷积和深度卷积，如下图所示：
- en: '![](img/56593c82-220f-46c9-a73f-346edd79558d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56593c82-220f-46c9-a73f-346edd79558d.png)'
- en: 'Left: cross-channel convolution; right: depthwise convolution'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：跨通道卷积；右图：深度卷积
- en: 'Let''s discuss their properties:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论它们的属性：
- en: '**Cross-channel convolutions**: One output slice receives input from all of
    the input slices (*n*-to-one relationship). With multiple output slices, the relationship
    becomes *n*-to-*m*. In other words, each input slice contributes to the output
    of each output slice. Each pair of input/output slices uses a separate filter
    slice that''s unique to that pair. Let''s denote the size of the filter (equal width
    and height) with *F*, the depth of the input volume with *C[in]*, and the depth
    of the output volume with *C[out]*. With this, we can compute the total number
    of weights, *W*, in a 2D convolution with the following equation:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨通道卷积**：一个输出切片从所有输入切片接收输入（*n* 到一的关系）。当有多个输出切片时，关系变为 *n* 到 *m*。换句话说，每个输入切片都会对每个输出切片的输出产生贡献。每对输入/输出切片都会使用一个与该对独特的滤波器切片。我们用
    *F* 来表示滤波器的大小（宽度和高度相等），用 *C[in]* 来表示输入体积的深度，用 *C[out]* 来表示输出体积的深度。有了这些，我们可以通过以下公式计算
    2D 卷积的总权重数 *W*：'
- en: '![](img/cbefc433-a0d8-46ef-98c5-9392bf144203.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbefc433-a0d8-46ef-98c5-9392bf144203.png)'
- en: Here, +1 represents the bias weight for each filter. Let's say we have three
    slices and want to apply four 5*×*5 filters to them. If we did this, the convolution
    filter would have a total of *(3*5*5 + 1) * 4 = 304* weights, four output slices
    (output volume with a depth of 4), and one bias per slice. The filter for each
    output slice will have three 5*×*5 filter patches for each of the three input
    slices and one bias for a total of 3*5*5 + 1 = 76 weights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，+1 表示每个滤波器的偏置权重。假设我们有三个切片，并且希望对它们应用四个 5*×*5 的滤波器。如果这样做，卷积滤波器将有总计 *(3*5*5
    + 1) * 4 = 304* 个权重，四个输出切片（深度为4的输出体积），每个切片有一个偏置。每个输出切片的滤波器将对每个输入切片应用三个 5*×*5 的滤波器补丁，并有一个偏置，总共有
    3*5*5 + 1 = 76 个权重。
- en: '**Depthwise convolutions**: Each output slice receives input from a single
    input slice. It''s a kind of reversal of the previous case. In its most simple
    form, we apply a filter over a single input slice to produce a single output slice.
    In this case, the input and output volumes have the same depth, that is, *C*.
    We can also specify a **channel multiplier** (an integer, *m*), where we apply *m* filters
    over a single output slice to produce *m* output slices. This is a case of a one-to-*m*
    relationship. In this case, the total number of output slices is *n * m*. We can
    compute the number of weights, *W*, in a 2D depthwise convolution with the following
    formula:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Depthwise convolutions**：每个输出切片仅接收来自单个输入切片的输入。这是之前情况的逆转。在最简单的形式中，我们对单个输入切片应用一个滤波器，生成一个单一的输出切片。在这种情况下，输入和输出的体积具有相同的深度，即
    *C*。我们还可以指定一个 **通道倍增器**（一个整数，*m*），我们在单个输出切片上应用 *m* 个滤波器，从而生成 *m* 个输出切片。这是一个一对-*m*
    的关系。在这种情况下，输出切片的总数是 *n * m*。我们可以用以下公式计算 2D 深度卷积中的权重 *W*：'
- en: '![](img/86e8086d-cbb7-430e-b548-164c7b897256.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86e8086d-cbb7-430e-b548-164c7b897256.png)'
- en: Here, *m* is the channel multiplier and *+C* represents the biases of each output
    slice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m* 是通道倍增器，*+C* 表示每个输出切片的偏置。
- en: 'The convolution operation is also described by two other parameters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作还通过其他两个参数来描述：
- en: '**S****tride** is the number of positions that we slide the filter over on
    the input slice on each step. By default, the stride is 1\. If it''s larger than
    1, then we call it a **stride convolution**. The largest stride increases the
    receptive field of the output neurons. With stride 2, the size of the output slice
    will be roughly four times smaller than the input. In other words, one output
    neuron will cover the area, which is four times larger, compared to a stride 1
    convolution. The neurons in the following layers will gradually capture input
    from the larger regions of the input image.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stride** 是在每一步中，我们滑动滤波器在输入切片上的位置数。默认情况下，步幅为 1。如果大于 1，则称之为 **步幅卷积**。较大的步幅增加了输出神经元的感受野。步幅为
    2 时，输出切片的大小大约是输入的四分之一。换句话说，步幅为 1 的卷积中，一个输出神经元的感受野是步幅为 2 时的四倍。后续层中的神经元将逐渐捕获来自输入图像较大区域的信息。'
- en: '**Padding** the edges of the input slice with rows and columns of zeros before
    the convolution operation. The most common way to use padding is to produce output
    with the same dimensions as the input. The newly padded zeros will participate
    in the convolution operation with the slice, but they won''t affect the result.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Padding** 在卷积操作之前，用零填充输入切片的边缘，增加行和列。最常见的填充方式是使输出的尺寸与输入相同。新增的零将参与与切片的卷积操作，但不会影响结果。'
- en: 'Knowing the input dimensions and the filter size, we can compute the dimensions
    of the output slices. Let''s say the size of the input slice is *I* (equal height
    and width), the size of the filter is *F*, the stride is *S*, and the padding is *P*.
    Here, the size, *O*, of the output slice is given by the following equation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 知道输入的尺寸和滤波器的大小后，我们可以计算输出切片的尺寸。假设输入切片的大小为 *I*（高度和宽度相等），滤波器的大小为 *F*，步幅为 *S*，填充为
    *P*。此时，输出切片的大小 *O* 由以下公式给出：
- en: '![](img/288c3c31-1000-4908-8163-171bbc19a0d3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/288c3c31-1000-4908-8163-171bbc19a0d3.png)'
- en: 'Besides stride convolutions, we can also use **pooling** operations to increase
    the receptive field of the deeper neurons and reduce the size of the future slices. The
    pooling splits the input slice into a grid, where each grid cell represents a
    receptive field of a number of neurons (just like it does with the convolution).
    Then, a pooling operation is applied over each cell of the grid. Similar to convolution,
    pooling is described by the stride, *S*, and the size of the receptive field, *F*.
    If the size of input slice is *I*, then the formula for the output size of the
    pooling is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了步幅卷积，我们还可以使用**池化**操作来增加深层神经元的感受野，并减少未来切片的大小。池化将输入切片划分为网格，每个网格单元代表多个神经元的感受野（就像卷积操作一样）。然后，在网格的每个单元上应用池化操作。类似于卷积，池化通过步幅，*S*，和感受野的大小，*F*，来描述。如果输入切片的大小为
    *I*，则池化输出大小的公式如下：
- en: '![](img/643541b1-b7a2-44b3-a546-cd28d1b32542.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/643541b1-b7a2-44b3-a546-cd28d1b32542.png)'
- en: 'In practice, only two combinations are used. The first is a 2*×*2 receptive field
    with stride 2, while the second is a 3*×*3 receptive field with stride 2 (overlapping). The
    most common pooling operations are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常只使用两种组合。第一种是步幅为2的2*×*2感受野，第二种是步幅为2的3*×*3感受野（重叠）。最常见的池化操作如下：
- en: '**Max pooling**: This propagates the maximum value of the input values of the
    receptive field.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**: 这个操作传播感受野内输入值的最大值。'
- en: '**Average pooling**: This propagates the average value of the inputs in the
    receptive field.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均池化**: 这个操作传播感受野内输入值的平均值。'
- en: '**Global Average Pooling** (**GAP**): This is the same as average pooling,
    but the pooling region has the same size as the feature map, *I×I*. GAP performs
    an extreme type of dimensionality reduction: the output is a single scalar, which
    represents the average value of the whole feature map.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局平均池化** (**GAP**): 这与平均池化相同，但池化区域与特征图的大小相同，*I×I*。GAP执行一种极端的降维操作：输出是一个单一的标量，表示整个特征图的平均值。'
- en: Typically, we would alternate one or more convolutional layers with one pooling
    (or stride convolution) layer. In this way, the convolutional layers can detect
    features at every level of the receptive field size because the aggregated receptive
    field size of deeper layers is larger than the ones at the beginning of the network.
    The deeper layers also have more filters (hence, a higher volume depth) compared
    to the initial ones. The feature detector at the beginning of the network works
    on a small receptive field. It can only detect a limited number of features, such
    as edges or lines, that are shared among all classes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会交替使用一个或多个卷积层与一个池化（或步幅卷积）层。通过这种方式，卷积层可以在每个感受野大小的层次上检测特征，因为较深层的感受野大小要大于网络初始层的感受野。深层的层也比最初的层拥有更多的滤波器（因此，具有更高的体积深度）。网络初始阶段的特征检测器工作在较小的感受野上，它只能检测到有限的特征，如边缘或线条，这些特征在所有类别中都有共享。
- en: On the other hand, a deeper layer would detect more complex and numerous features.
    For example, if we have multiple classes, such as cars, trees, or people, each
    would have its own set of features, such as tires, doors, leaves, and faces. This
    would require more feature detectors. The output of the final convolution (or
    pooling) is "translated" to the target labels by adding one or more fully connected
    layers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，较深的层会检测到更复杂和更多的特征。例如，如果我们有多个类别，如汽车、树木或人，每个类别都会有一组特征，如轮胎、车门、树叶和面孔。这将需要更多的特征检测器。最终卷积（或池化）的输出通过添加一个或多个全连接层“翻译”到目标标签。
- en: Now that we have had an overview of convolutions, pooling operations, and CNNs,
    in the next section, we'll focus on different types of convolution operations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对卷积、池化操作和卷积神经网络（CNN）有了一个概述，在下一节中，我们将重点讨论不同类型的卷积操作。
- en: Types of convolutions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积类型
- en: So far, we've discussed the most common type of convolution. In the upcoming
    sections, we'll talk about a few of its variations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了最常见的卷积类型。在接下来的章节中，我们将讨论它的一些变体。
- en: Transposed convolutions
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转置卷积
- en: In the convolutional operations we've discussed so far, the output dimensions
    are either equal or smaller than the input dimensions. In contrast, transposed
    convolutions (first proposed in *Deconvolutional Networks* by Matthew D. Zeiler,
    Dilip Krishnan, Graham W. Taylor, and Rob Fergus: [https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf))
    allow us to upsample the input data (their output is larger than the input). This
    operation is also known as **deconvolution**, **fractionally strided convolution**,
    or **sub-pixel convolution**. These names can sometimes lead to confusion. To
    clarify things, note that the transposed convolution is, in fact, a regular convolution
    with a slightly modified input slice or convolutional filter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们目前讨论的卷积操作中，输出的维度要么与输入相等，要么小于输入的维度。与此相反，反卷积（最早由Matthew D. Zeiler、Dilip Krishnan、Graham
    W. Taylor和Rob Fergus在《*反卷积网络*》中提出：[https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)）允许我们对输入数据进行上采样（其输出大于输入）。这种操作也被称为**反卷积**、**分数步幅卷积**或**子像素卷积**。这些名称有时会导致混淆。为了澄清这一点，请注意，反卷积实际上是一个常规卷积，只是对输入切片或卷积滤波器进行稍微修改。
- en: 'For the longer explanation, we''ll start with a 1D regular convolution over
    a single input and output slice:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更长的解释，我们将从一个1D常规卷积开始，考虑单个输入和输出切片：
- en: '![](img/a6a3a798-3dc8-441a-be05-3b5043baa376.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6a3a798-3dc8-441a-be05-3b5043baa376.png)'
- en: 1D regular convolution
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 常规卷积
- en: It uses a filter with a size of 4, stride 2, and padding 2 (denoted with gray
    in the preceding diagram). The input is a vector of size 6 and the output is a
    vector of size 4\. The filter, a vector **f** = [1, 2, 3, 4], is always the same,
    but it's denoted with different colors for each position we apply it to. The respective
    output cells are denoted with the same color. The arrows show which input cells
    contribute to one output cell.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用一个大小为4、步幅为2、填充为2的滤波器（在上面的图中用灰色表示）。输入是一个大小为6的向量，输出是一个大小为4的向量。滤波器是一个向量**f**
    = [1, 2, 3, 4]，它始终是相同的，但我们根据应用它的位置使用不同的颜色表示。对应的输出单元也用相同的颜色表示。箭头表示哪些输入单元贡献给一个输出单元。
- en: The example that is being discussed in this section is inspired by the paper; *Is
    the deconvolution layer the same as a convolutional layer? *([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的示例灵感来源于论文《*反卷积层是否与卷积层相同？*》([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009))。
- en: 'Next, we''ll discuss the same example (1D, single input and output slices,
    filter of size 4, padding 2, and stride 2), but for transposed convolution. The
    following diagram shows two ways we can implement it:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论相同的示例（1D，单个输入和输出切片，滤波器大小为4，填充为2，步幅为2），但对于反卷积。下图展示了我们可以实现它的两种方式：
- en: '![](img/31980d51-3120-43f3-bf47-ac3ec266ef8a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31980d51-3120-43f3-bf47-ac3ec266ef8a.png)'
- en: 'Left: A convolution with stride 2, applied with the transposed filter **f**.
    The 2 pixels at the beginning and the end of the output are cropped; right: A
    convolution with stride 0.5, applied over input data, padded with subpixels. The
    input is filled with 0-valued pixels (gray).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 左：步幅为2的卷积，应用了反卷积滤波器**f**。输出的开始和结束处的2个像素被裁剪；右：步幅为0.5的卷积，应用于输入数据，并通过子像素填充。输入被填充为0值像素（灰色）。
- en: 'Let''s discuss them in detail:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论它们：
- en: 'In the first case, we have a regular convolution with stride 2 and a filter represented
    as transposed row matrix (equivalent to column matrix) with size 4: ![](img/88d1c5eb-7650-46c4-b378-438a9f3fa008.png) (shown
    in the preceding diagram, left). Note that the stride is applied over the output
    layer as opposed to the regular convolution, where we stride over the input. By
    setting the stride larger than 1, we can increase the output size, compared to
    the input. Here, the size of the input slice is *I*, the size of the filter is *F*,
    the stride is *S*, and the input padding is *P*. Due to this, the size, *O*, of
    the output slice of a transposed convolution is given by the following formula:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一个案例中，我们有一个步幅为2的常规卷积，滤波器表示为反转的行矩阵（等同于列矩阵），大小为4：![](img/88d1c5eb-7650-46c4-b378-438a9f3fa008.png)（如上图左所示）。请注意，步幅是应用于输出层的，而常规卷积则是应用于输入层的。通过将步幅设置为大于1，我们可以增加输出的大小，相对于输入。在这里，输入切片的大小为*I*，滤波器的大小为*F*，步幅为*S*，输入的填充为*P*。因此，反卷积的输出切片大小*O*可以通过以下公式计算：
- en: '![](img/5c6c084c-a942-44ce-adbd-190f97c82f4e.png)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/5c6c084c-a942-44ce-adbd-190f97c82f4e.png)'
- en: In this scenario, an input of size 4 produces an output of size *2*(4 - 1) +
    4 - 2*2 = 6*. We also crop the two cells at the beginning and the end of the output
    vector because they only gather input from a single input cell.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，大小为4的输入产生的输出大小为*2*(4 - 1) + 4 - 2*2 = 6*。我们还会裁剪输出向量的开始和结束部分的两个单元，因为它们只收集来自单个输入单元的信息。
- en: In the second case, the input is filled with imaginary 0-valued subpixels between
    the existing ones (shown in the preceding diagram, right). This is where the name
    subpixel convolution comes from. Think of it as padding but within the image itself
    and not only along the borders. Once the input has been transformed in this way,
    a regular convolution is applied.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二种情况下，输入填充了存在的像素之间的虚拟0值子像素（如前面的图所示，右侧）。这就是“子像素卷积”这一名称的由来。可以将其视为一种填充，但它发生在图像内部，而不仅仅是在边缘上。一旦输入以这种方式被转换，就会应用常规卷积。
- en: Let's compare the two output cells, *o*[1] and *o*[3], in both scenarios. As
    shown in the preceding diagram, in either case, *o*[1] receives input from the
    first and the second input cells and *o*[3] receives input from the second and
    third cells. In fact, the only difference between these two cases is the index
    of the weight, which participates in the computation. However, the weights are
    learned during training and, because of this, the index is not important. Therefore,
    the two operations are equivalent.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较两种情况中的两个输出单元，*o*[1]和*o*[3]。如前面的图所示，在这两种情况下，*o*[1]接收来自第一个和第二个输入单元的信息，*o*[3]接收来自第二个和第三个单元的信息。实际上，这两种情况之间唯一的区别是参与计算的权重索引。然而，权重是在训练过程中学习的，因此，索引并不重要。因此，这两种操作是等效的。
- en: 'Next, let''s take a look at a 2D transposed convolution from a subpixel point
    of view (the input is at the bottom). As with the 1D case, we insert 0-valued
    pixels and padding in the input slice to achieve upsampling:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从子像素的角度来看一个2D转置卷积（输入在底部）。与1D情况类似，我们在输入切片中插入0值像素和填充，以实现上采样：
- en: '![](img/1d3ef15e-d1d6-407d-bcad-b696e2b6f812.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d3ef15e-d1d6-407d-bcad-b696e2b6f812.png)'
- en: The first three steps of a 2D transpose convolution with padding 1 and stride
    2: Source: https://github.com/vdumoulin/conv_arithmetic, https://arxiv.org/abs/1603.07285
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 具有填充为1和步幅为2的2D转置卷积的前三个步骤：来源：[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)，[https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)
- en: The backpropagation operation of a regular convolution is a transposed convolution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 常规卷积的反向传播操作是转置卷积。
- en: 1×1 convolutions
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1×1卷积
- en: A 1*×*1 (or pointwise) convolution is a special case of convolution where each
    dimension of the convolution filter is of size 1 (1*×*1 in 2D convolutions and
    1*×*1*×*1 in 3D). At first, this doesn't make sense—a 1*×*1 filter doesn't increase
    the receptive field size of the output neurons. The result of such a convolution would
    be pointwise scaling. But it can be useful in another way—we can use them to change
    the depth between the input and output volumes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1×1（或逐点）卷积是卷积的一种特殊情况，其中卷积滤波器的每个维度的大小为1（2D卷积中的1×1和3D卷积中的1×1×1）。起初，这似乎没有意义——1×1的滤波器并不会增加输出神经元的感受野大小。此类卷积的结果将是逐点缩放。但它在另一个方面是有用的——我们可以用它们来改变输入和输出体积之间的深度。
- en: To understand this, let's recall that, in general, we have an input volume with
    a depth of *D* slices and *M* filters for *M* output slices. Each output slice
    is generated by applying a unique filter over all of the input slices. If we use
    a 1*×*1 filter and *D != M*, we'll have output slices of the same size, but with
    different volume depths. At the same time, we won't change the receptive field
    size between the input and output. The most common use case is to reduce the output
    volume, or *D > M* (dimension reduction), nicknamed the "bottleneck" layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，我们回顾一下，通常情况下，我们有一个深度为*D*切片的输入体积，以及*M*个滤波器用于生成*M*个输出切片。每个输出切片都是通过将唯一的滤波器应用于所有输入切片生成的。如果我们使用1×1的滤波器且*D
    != M*，我们会得到相同大小的输出切片，但深度不同。同时，我们不会改变输入和输出之间的感受野大小。最常见的用例是减少输出体积，或*D > M*（降维），俗称“瓶颈”层。
- en: Depth-wise separable convolutions
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: An output slice in a cross-channel convolution receives input from all of the
    input slices using a single filter. The filter tries to learn features in a 3D
    space, where two of the dimensions are spatial (the height and width of the slice)
    and the third is the channel. Therefore, the filter maps both spatial and cross-channel
    correlations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨通道卷积中，一个输出切片接收来自所有输入切片的输入，使用一个滤波器。该滤波器试图在三维空间中学习特征，其中两个维度是空间维度（切片的高度和宽度），第三个维度是通道。因此，滤波器映射了空间和跨通道的相关性。
- en: '**Depthwise separable convolutions** (**DSC**, *Xception: Deep Learning with
    Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)) can
    completely decouple cross-channel and spatial correlations. A DSC combines two
    operations: a depthwise convolution and a 1*×*1 convolution. In a depthwise convolution,
    a single input slice produces a single output slice, so it only maps spatial (and
    not cross-channel) correlations. With 1*×*1 convolutions, we have the opposite.
    The following diagram represents the DSC:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度可分离卷积**（**DSC**，*Xception: Deep Learning with Depthwise Separable Convolutions*，[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)）可以完全解耦跨通道和空间相关性。DSC结合了两种操作：深度卷积和1×1卷积。在深度卷积中，一个输入切片产生一个输出切片，因此它只映射空间相关性（而不是跨通道相关性）。而在1×1卷积中，我们恰恰相反。以下图示表示了DSC：'
- en: '![](img/79686796-d667-416c-a52f-9c40fd7631d4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79686796-d667-416c-a52f-9c40fd7631d4.png)'
- en: A depthwise separable convolution
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: The DSC is usually implemented without non-linearity after the first (depthwise)
    operation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积（DSC）通常在第一次（深度）操作后没有非线性激活。
- en: Let's compare the standard and depthwise separable convolutions. Imagine that
    we have 32 input and output channels and a filter with a size of 3*×*3\. In a
    standard convolution, one output slice is the result of applying one filter for
    each of the 32 input slices for a total of *32 * 3 * 3 = 288* weights (excluding
    bias). In a comparable depthwise convolution, the filter has only *3 * 3 = 9*
    weights and the filter for the 1*×*1 convolution has *32 * 1 * 1 = 32* weights.
    The total number of weights is *32 + 9 = 41*. Therefore, the depthwise separable
    convolution is faster and more memory-efficient compared to the standard one.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较标准卷积和深度可分离卷积。假设我们有32个输入和输出通道，以及一个大小为3×3的滤波器。在标准卷积中，一个输出切片是对32个输入切片中每一个应用一个滤波器的结果，总共需要*32
    * 3 * 3 = 288*个权重（不包括偏置）。在一个可比的深度卷积中，滤波器只有*3 * 3 = 9*个权重，而1×1卷积的滤波器有*32 * 1 *
    1 = 32*个权重。总权重数是*32 + 9 = 41*。因此，深度可分离卷积相对于标准卷积来说，速度更快且更节省内存。
- en: Dilated convolutions
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 空洞卷积
- en: 'Recall the discrete convolution formula we introduced at the beginning of the *A
    quick recap of CNNs* section. To explain dilated convolutions (*Multi-Scale Context
    Aggregation by Dilated Convolutions*, [https://arxiv.org/abs/1511.07122](https://arxiv.org/abs/1511.07122)),
    let''s start with the following formula:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在《*CNN快速回顾*》章节开始时介绍的离散卷积公式。为了说明空洞卷积（*Multi-Scale Context Aggregation by
    Dilated Convolutions*，[https://arxiv.org/abs/1511.07122](https://arxiv.org/abs/1511.07122)），我们从以下公式开始：
- en: '![](img/61fd057f-883a-456b-85d4-d77cd8589e36.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61fd057f-883a-456b-85d4-d77cd8589e36.png)'
- en: 'We''ll denote the dilated convolution with **[l]*, where *l* is a positive
    integer value called the dilation factor. The key is the way we apply the filter
    over the input. Instead of applying the *n×n* filter over the *n×n* receptive
    field, we apply the same filter sparsely over a receptive field of size *(n*l-1)×(n*l-1)*.
    We still multiply each filter weight by one input slice cell, but these cells
    are at a distance of *l* away from each other. The regular convolution is a special
    case of dilated convolution with *l=1*. This is best illustrated with the following
    diagram:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用**[l]*表示空洞卷积，其中*l*是一个正整数，称为膨胀因子。关键在于我们如何在输入上应用滤波器。不同于在*n×n*感受野上应用*n×n*滤波器，我们将相同的滤波器稀疏地应用于一个大小为*(n*l-1)×(n*l-1)*的感受野。我们依然将每个滤波器权重与一个输入切片单元相乘，但这些单元之间相隔*l*的距离。常规卷积是空洞卷积的一个特例，当*l=1*时。以下图示可以最好地说明这一点：
- en: '![](img/0807af5e-ce74-4b7d-8399-52a33bab68d4.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0807af5e-ce74-4b7d-8399-52a33bab68d4.png)'
- en: 'A dilated convolution with a dilation factor of l=2: Here, the first two steps
    of the operation are displayed. The bottom layer is the input while the top layer
    is the output. Source: https://github.com/vdumoulin/conv_arithmetic'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀因子l=2的空洞卷积：这里显示了操作的前两步。底层是输入层，而顶层是输出层。来源：[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)
- en: Dilated convolutions can increase the receptive field size exponentially without
    losing resolution or coverage. We can also increase the receptive field with stride
    convolutions or pooling but at the cost of resolution and/or coverage. To understand
    this, let's imagine that we have a stride convolution with stride *s>1*. In this
    case, the output slice is *s* times smaller than the input (loss of resolution).
    If we increase *s>n* further (*n* is the size of either the pooling or convolutional
    kernel), we get loss of coverage because some of the areas of the input slice
    will not participate in the output at all. Additionally, dilated convolutions
    don't increase the computation and memory costs because the filter uses the same
    number of weights as the regular convolution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积可以在不损失分辨率或覆盖范围的情况下指数级地增加感受野的大小。我们也可以通过步长卷积或池化来增加感受野，但代价是分辨率和/或覆盖范围的损失。为了理解这一点，我们假设有一个步长为*s>1*的步长卷积。在这种情况下，输出切片比输入小*s*倍（分辨率损失）。如果我们进一步增加*s>n*（*n*为池化或卷积核的大小），则会损失覆盖范围，因为输入切片的某些区域将完全不参与输出。此外，膨胀卷积不会增加计算和内存成本，因为滤波器使用的权重数量与常规卷积相同。
- en: Improving the efficiency of CNNs
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高卷积神经网络（CNNs）的效率
- en: One of the main reasons for the advances in recent **Deep Learning** (**DL**)
    is its ability to run **Neural Networks** (**NNs**) very fast. This is in large
    part because of the good match between the nature of NN algorithms and the specifics
    of **Graphical Processing Units** (**GPUs**). In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*, we underscored the importance of matrix multiplication
    in NNs. As a testament to this, it is possible to transform the convolution into
    a matrix multiplication as well. Matrix multiplication is embarrassingly parallel
    (trust me, this is a term—you can Google it!). The computation of each output
    cell is not related to the computation of any other output cell. Therefore, we
    can compute all of the outputs in parallel.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最近**深度学习**（**DL**）取得进展的主要原因之一是它能够非常快速地运行**神经网络**（**NNs**）。这在很大程度上是因为神经网络算法的性质与**图形处理单元**（**GPU**）的具体特点非常契合。在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)《神经网络的基本原理》中，我们强调了矩阵乘法在神经网络中的重要性。对此的证明是，卷积也可以转化为矩阵乘法。矩阵乘法是极其并行的（相信我，这是一个术语——你可以在Google上查找它！）。每个输出单元的计算与其他任何输出单元的计算无关。因此，我们可以并行计算所有输出。
- en: Not coincidentally, GPUs are well suited for highly parallel operations like
    this. On the one hand, a GPU has a high number of computational cores compared
    to a **Central Processing Unit** (**CPU**). Even though a GPU core is faster than
    a CPU one, we can still compute a lot more output cells in parallel. But what's
    even more important is that GPUs are optimized for memory bandwidth, while CPUs
    are optimized for latency. This means that a CPU can fetch small chunks of memory
    very quickly but will be slow when it comes to fetching large chunks. The GPU does
    the opposite. Because of this, in tasks such as large matrix multiplication for
    NNs, the GPU has an advantage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 并非巧合，GPU非常适合执行像这样的高并行操作。一方面，GPU的计算核心数量远高于**中央处理单元**（**CPU**）。尽管GPU核心的速度比CPU核心快，但我们仍然可以并行计算更多的输出单元。但更重要的是，GPU在内存带宽方面进行了优化，而CPU则在延迟方面进行了优化。这意味着CPU可以非常快速地获取小块内存，但在获取大块内存时会变得缓慢。GPU则相反。因此，在像神经网络（NNs）的大矩阵乘法这样的任务中，GPU具有优势。
- en: Besides hardware specifics, we can optimize CNNs on the algorithmic side as
    well. The majority of computational time in a CNN is devoted to the convolutions
    themselves. Although the implementation of the convolution is straightforward
    enough, in practice, there are more efficient algorithms to achieve the same result.
    Although contemporary DL libraries such as TensorFlow or PyTorch shield the developer
    from such details, in this book, we are aiming for a deeper (pun intended) understanding
    of DL.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件方面的优化，我们也可以在算法方面优化CNN。CNN中的大部分计算时间都花费在卷积操作上。尽管卷积的实现本身相对简单，但实际上有更高效的算法可以实现相同的结果。尽管现代的深度学习库如TensorFlow或PyTorch能够将开发者从这些细节中抽象出来，但在本书中，我们旨在更深入（这可是双关语哦）地理解深度学习。
- en: Because of this, in the next section, we'll discuss two of the most popular
    fast convolution algorithms.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在接下来的部分，我们将讨论两种最流行的快速卷积算法。
- en: Convolution as matrix multiplication
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积作为矩阵乘法
- en: 'In this section, we''ll describe the algorithm that we use to transform convolutions
    into matrix multiplication, just like how it''s implemented in the cuDNN library
    (*cuDNN: Efficient Primitives for Deep Learning*, [https://arxiv.org/abs/1410.0759](https://arxiv.org/abs/1410.0759)).
    To understand this, let''s assume that we perform a cross-channel 2D convolution
    over an RGB input image. Let''s look at the following table for the parameters
    of the convolution:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将描述将卷积转化为矩阵乘法的算法，正如它在cuDNN库中实现的那样（*cuDNN: 高效的深度学习原语*，[https://arxiv.org/abs/1410.0759](https://arxiv.org/abs/1410.0759)）。为了理解这一点，我们假设对RGB输入图像执行跨通道二维卷积。让我们查看下面的表格，了解卷积的参数：'
- en: '| **Parameter** | **Notation** | **Value** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **符号** | **值** |'
- en: '| Mini-batch size | N | 1 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 小批量大小 | N | 1 |'
- en: '| Input feature maps (volume depth) | C | 3 (one for each RGB channel) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 输入特征图（体积深度） | C | 3（每个RGB通道一个）|'
- en: '| Input image height | H | 4 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 输入图像高度 | H | 4 |'
- en: '| Input image width | W | 4 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 输入图像宽度 | W | 4 |'
- en: '| Output feature maps (volume depth) | K | 2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 输出特征图（体积深度）| K | 2 |'
- en: '| Filter height | R | 2 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 滤波器高度 | R | 2 |'
- en: '| Filter width | S | 2 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 滤波器宽度 | S | 2 |'
- en: '| Output feature map height | P | 2 (based on the input/filter sizes) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 输出特征图高度 | P | 2（基于输入/滤波器的大小）|'
- en: '| Output feature map width | Q | 2 (based on the input/filter sizes) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 输出特征图宽度 | Q | 2（基于输入/滤波器的大小）|'
- en: 'For the sake of simplicity, we''ll assume we have zero padding and stride 1\.
    We''ll denote the input tensor with *D* and the convolution filter tensor with
    *F*. The matrix convolution works in the following way:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们假设使用零填充和步幅为1。我们将输入张量表示为*D*，卷积滤波器张量表示为*F*。矩阵卷积按以下方式工作：
- en: We unfold the tensors, *D* and *F*, into the ![](img/b51588a3-c401-4c1c-8b20-b91726e14de5.png) and
    ![](img/93fd8178-c6d7-4996-a1dd-94cf21f0a6a4.png) matrices, respectively.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将张量*D*和*F*分别展开为 ![](img/b51588a3-c401-4c1c-8b20-b91726e14de5.png) 和 ![](img/93fd8178-c6d7-4996-a1dd-94cf21f0a6a4.png)
    矩阵。
- en: Then, we multiply the matrices to get the output matrix, ![](img/bc1fddcd-c0be-424f-9acb-47604f8d3a17.png).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过矩阵相乘来得到输出矩阵， ![](img/bc1fddcd-c0be-424f-9acb-47604f8d3a17.png)。
- en: 'We discussed matrix multiplication in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*. Now, let''s focus on the way we can unfold
    the tensors in matrices. The following diagram shows how to do this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中讨论了矩阵乘法，*神经网络的基础*。现在，让我们关注如何将张量展开成矩阵。下面的图示展示了如何做到这一点：
- en: '![](img/aa86d420-5efa-43ad-bd02-cecad396cc09.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa86d420-5efa-43ad-bd02-cecad396cc09.png)'
- en: Convolution as matrix multiplication; Inspired by https://arxiv.org/abs/1410.0759
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积作为矩阵乘法；灵感来源于 https://arxiv.org/abs/1410.0759
- en: 'Each feature map has a different color (R, G, B). In the regular convolution,
    the filter has a square shape and we apply it over a square input region. In the
    transformation, we unfold each possible square region of *D* into one column of **D***[m]*.
    Then, we unfold each square component of *F* into one row of **F***[m]*. In this
    way, the input and filter data for each output cell is situated in a single column/row
    of the matrices **D***[m]* and **F***[m]*. This makes it possible to compute the
    output value as a matrix multiplication. The dimensions of the transformed input/filter/output
    are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征图有不同的颜色（R，G，B）。在常规卷积中，滤波器是方形的，并且我们将其应用于方形的输入区域。在变换中，我们将*D*中的每个可能的方形区域展开成**D***[m]*的一个列。然后，我们将*F*的每个方形组件展开为**F***[m]*的一个行。通过这种方式，每个输出单元的输入和滤波器数据都位于矩阵**D***[m]*和**F***[m]*的单独列/行中。这使得通过矩阵乘法计算输出值成为可能。变换后的输入/滤波器/输出的维度如下：
- en: dim(**D***[m]*) = CRS*×*NPQ = 12*×*4
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dim(**D***[m]*) = CRS*×*NPQ = 12*×*4
- en: dim(**F***[m]*) = K*×*CRS = 2*×*12
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dim(**F***[m]*) = K*×*CRS = 2*×*12
- en: dim(**O***[m]*) = K*×*NPQ = 2*×*4
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dim(**O***[m]*) = K*×*NPQ = 2*×*4
- en: 'To understand this transformation, let''s learn how to compute the first output
    cell with the regular convolution algorithm:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个变换，让我们学习如何使用常规卷积算法计算第一个输出单元格：
- en: '![](img/9834db46-e86b-421e-8de9-8c1789e9ed47.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9834db46-e86b-421e-8de9-8c1789e9ed47.png)'
- en: 'Next, let''s observe the same formula, but this time, in matrix multiplication
    form:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们观察相同的公式，不过这次是以矩阵乘法的形式：
- en: '![](img/89ec53d1-8092-46af-9893-2faa7aea4d0f.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89ec53d1-8092-46af-9893-2faa7aea4d0f.png)'
- en: If we compare the components of the two equations, we'll see that they are exactly
    the same. That is, *D*[0,0,0,0] = **D**[m][0,0], *F*[0,0,0,0] = **F**[m][0,0], *D*[0,0,0,1]
    = **D**[m][0,1], *F*[0,0,0,1] = **F**[m][0,1], and so on. We can do the same for
    the rest of the output cells. Therefore, the output of the two approaches is the
    same.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较两个方程的组成部分，会发现它们完全相同。即，*D*[0,0,0,0] = **D**[m][0,0]，*F*[0,0,0,0] = **F**[m][0,0]，*D*[0,0,0,1]
    = **D**[m][0,1]，*F*[0,0,0,1] = **F**[m][0,1]，以此类推。我们可以对其余的输出单元进行相同的操作。因此，两种方法的输出是相同的。
- en: One disadvantage of the matrix convolution is increased memory usage. In the
    preceding diagram, we can see that some of the input elements are duplicated multiple
    times (up to RS = 4 times, like D4).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵卷积的一个缺点是增加了内存使用量。在前述图示中，我们可以看到一些输入元素被多次重复（最多可达 RS = 4 次，例如 D4）。
- en: Winograd convolutions
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Winograd 卷积
- en: The Winograd algorithm (*Fast Algorithms for Convolutional Neural Networks*, [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308))
    can provide 2 or 3*×* speedup compared to the direct convolution. To explain this,
    we'll use the same notations that we used in the *Convolution as matrix multiplication* section
    but with a *3×3* (*R=S=3*) filter. We'll also assume that the input slices are
    bigger than *4×4* (*H>4, W>4*).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Winograd 算法（*卷积神经网络的快速算法*，[https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308)）可以提供相较于直接卷积
    2 或 3*×* 的加速。为了解释这一点，我们将使用在 *卷积作为矩阵乘法* 部分中相同的符号，但使用 *3×3*（*R=S=3*）滤波器。我们还假设输入切片大于
    *4×4*（*H>4, W>4*）。
- en: 'Here''s how to compute Winograd convolutions:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是计算 Winograd 卷积的方法：
- en: 'Divide the input image into 4*×*4 tiles that overlap with stride 2, as shown
    in the following diagram:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入图像划分为 4*×*4 个重叠的块，步长为 2，如下图所示：
- en: '![](img/cd4aa445-8508-4aab-bea4-72effd713822.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd4aa445-8508-4aab-bea4-72effd713822.png)'
- en: The input is split into tiles
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入被划分为多个块
- en: The tile size can vary, but for the sake of simplicity, we'll only focus on
    4*×*4 tiles.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小可以有所变化，但为了简便起见，我们只关注 4*×*4 的块。
- en: 'Transform each tile using the following two matrix multiplications:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下两个矩阵乘法变换每个块：
- en: '![](img/b0268ace-bb0f-44da-bbbe-28165f6b82fa.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0268ace-bb0f-44da-bbbe-28165f6b82fa.png)'
- en: In the preceding formula, the matrix **D** is the input slice (the one with
    circle values) while **B** is a special matrix, which results from the specifics
    of the Winograd algorithm (you can find more information about them in the paper
    linked at the beginning of this section).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，矩阵 **D** 是输入切片（带有圆形值的那个），而 **B** 是一个特殊矩阵，它源于 Winograd 算法的特性（您可以在本节开头的论文中找到更多相关信息）。
- en: 'Transform the filter using the following two matrix multiplications:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下两个矩阵乘法变换滤波器：
- en: '![](img/72a0e638-cfe5-4a34-a05e-83cfe8af3884.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72a0e638-cfe5-4a34-a05e-83cfe8af3884.png)'
- en: In the preceding formula, the matrix **F** (the one with dot values) is the
    3*×*3 convolution filter between one input and one output slice. **G** and its
    transpose, ![](img/c62285e7-2871-435d-92d0-75ab2fc9df82.png), are, again, special
    matrices, which result from the specifics of the Winograd algorithm. Note that
    the transformed filter matrix, **F***[t]*, has the same dimensions as the input
    tile, **D***[t]*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，矩阵 **F**（带有点值的那个）是一个输入和输出切片之间的 3*×*3 卷积滤波器。**G** 及其转置 ![](img/c62285e7-2871-435d-92d0-75ab2fc9df82.png)
    是 Winograd 算法的特殊矩阵。请注意，变换后的滤波器矩阵 **F***[t]* 与输入块 **D***[t]* 具有相同的维度。
- en: 'Compute the transformed output as an **element-wise** multiplication (the ![](img/8c5ea580-95d6-4abf-b93b-a2dc5ba2824b.png)symbol)
    of the transformed input and filter:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对变换后的输入和滤波器进行 **元素级** 相乘（即 ![](img/8c5ea580-95d6-4abf-b93b-a2dc5ba2824b.png)
    符号），计算变换后的输出：
- en: '![](img/8c2d3f58-125b-4bc5-9cdc-0d72c1e1b1a6.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c2d3f58-125b-4bc5-9cdc-0d72c1e1b1a6.png)'
- en: 'Transform the output back into its original form:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出转换回其原始形式：
- en: '![](img/c425497c-3ed4-4453-a663-75e676f44a5d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c425497c-3ed4-4453-a663-75e676f44a5d.png)'
- en: '**A** is a transformation matrix, which makes it possible to transform ![](img/17eb8275-2f67-47dc-ba8d-e764d4956c73.png)
    back into the form, which would have resulted from a direct convolution. As shown
    in the preceding formula and in the following diagram, the Winograd convolution
    allows us to compute 2*×*2 output tile simultaneously (four output cells):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**A** 是一个变换矩阵，它使得将 ![](img/17eb8275-2f67-47dc-ba8d-e764d4956c73.png) 转换回从直接卷积得到的形式成为可能。正如前述公式和下图所示，Winograd
    卷积允许我们同时计算 2*×*2 输出块（四个输出单元）：'
- en: '![](img/7f77fb1f-a7f4-41a6-a5ff-7a52ce537597.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f77fb1f-a7f4-41a6-a5ff-7a52ce537597.png)'
- en: The Winograd convolution allows us to compute four output cells simultaneously
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Winograd 卷积允许我们同时计算四个输出单元。
- en: At first glance, it seems that the Winograd algorithm performs a lot more operations
    than direct convolution. So, how is it faster, then? To find out, let's focus
    on the ![](img/199877db-609c-4220-bfef-fea0b783f935.png) transformation. The key
    here is that we have to perform ![](img/62533cbb-4149-4b6f-9a6b-65a4401988db.png) only
    once and then **D***[t]* can participate in the outputs of all *K* (following
    the notation) output slices. Therefore, ![](img/75e04c2e-80be-4bf1-9aa6-3ce4c72140c4.png) is
    amortized among all of the outputs and it doesn't affect the performance as much.
    Next, let's take a look at the ![](img/de88f956-c9af-4ff3-aea8-91343b168a3e.png) transformation. This
    one is even better because, once we compute **F***[t]*, we can apply it *N×P×Q*
    times (across all of the cells of the output slice and all of the images in the
    batch). Therefore, the performance penalty for this transformation is negligible.
    Similarly, the output transformation ![](img/4833fd5c-3623-4e16-be16-96617865bf20.png) is
    amortized over the number of input channels C.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，似乎 Winograd 算法执行的操作比直接卷积多得多。那么，它为什么更快呢？为了找出答案，让我们集中关注 ![](img/199877db-609c-4220-bfef-fea0b783f935.png)
    转换。关键在于，我们只需要执行一次 ![](img/62533cbb-4149-4b6f-9a6b-65a4401988db.png)，然后 **D***[t]*
    可以参与所有 *K*（按照符号表示）输出切片的输出。因此，![](img/75e04c2e-80be-4bf1-9aa6-3ce4c72140c4.png)
    在所有输出中进行摊销，并且它对性能的影响不大。接下来，让我们看看 ![](img/de88f956-c9af-4ff3-aea8-91343b168a3e.png)
    转换。这一步甚至更好，因为一旦我们计算出 **F***[t]*，我们可以将其应用于 *N×P×Q* 次（遍历输出切片的所有单元格以及批次中的所有图像）。因此，这一转换的性能损失可以忽略不计。同样，输出转换
    ![](img/4833fd5c-3623-4e16-be16-96617865bf20.png) 也在输入通道数 C 上进行摊销。
- en: Finally, we'll discuss the element-wise multiplication, ![](img/f2884fb8-3671-4696-86f3-c113be25c24d.png), which
    is applied *P×Q* times across all of the cells of the output slice and takes the
    bulk of the computational time. It consists of 16 scalar multiply operations and
    allows us to compute 2*×*2 output tile, which results in four multiplications
    for one output cell. Let's compare this with the direct convolution, where we
    have to perform *3*3=9* scalar multiplications (each filter element is multiplied
    by each receptive field input cell) for a single output. Therefore, the Winograd
    convolution requires *9/4 = 2.25* fewer operations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论逐元素乘法，![](img/f2884fb8-3671-4696-86f3-c113be25c24d.png)，它应用于输出切片的所有单元格，*P×Q*
    次，并占据了大部分计算时间。它由 16 次标量乘法操作组成，允许我们计算 2*×*2 的输出瓦片，这意味着每个输出单元需要进行四次乘法操作。与直接卷积相比，我们需要执行
    *3*3=9* 次标量乘法（每个滤波器元素与每个感受野输入单元相乘），才能得到一个输出。因此，Winograd 卷积比直接卷积减少了 *9/4 = 2.25*
    次操作。
- en: The Winograd convolution has the most benefits when working with smaller filter
    sizes (for example, 3*×*3). Convolutions with larger filters (for example, 11*×*11)
    can be efficiently implemented with Fast Fourier Transform (FFT) convolutions,
    which are beyond the scope of this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Winograd 卷积在使用较小的滤波器尺寸时（例如，3*×*3）效果最佳。使用较大的滤波器（例如，11*×*11）的卷积可以通过快速傅里叶变换（FFT）卷积高效实现，这超出了本书的讨论范围。
- en: In the next section, we'll try to understand the inner workings of CNNs by visualizing
    their internal state.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将尝试通过可视化 CNN 的内部状态来理解其内部工作原理。
- en: Visualizing CNNs
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化 CNN
- en: One of the criticisms of NNs is that their results aren't interpretable. It's
    common to think of a NN as a black box whose internal logic is hidden from us.
    This could be a serious problem. On the one hand, it's less likely we trust an
    algorithm that works in a way we don't understand, while on the other hand, it's
    hard to improve the accuracy of CNNs if we don't know how they work. Because of
    this, in the upcoming sections, we'll discuss two methods of visualizing the internal
    layers of a CNN, both of which will help us to gain insight into the way they
    learn.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NN）的一个批评是它们的结果不可解释。通常认为神经网络是一个黑盒，其内部逻辑对我们是隐藏的。这可能是一个严重的问题。一方面，我们不太可能信任一个我们不了解其工作原理的算法；另一方面，如果我们不知道神经网络是如何工作的，就很难提高
    CNN 的准确性。正因为如此，在接下来的章节中，我们将讨论两种可视化 CNN 内部层的方法，这两种方法将帮助我们深入了解它们的学习方式。
- en: Guided backpropagation
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引导反向传播
- en: 'Guided backpropagation (*Striving for Simplicity: The All Convolutional Net*, [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806))
    allows us to visualize the features that are learned by a single unit of one layer
    of a CNN. The following diagram shows how the algorithm works:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '引导反向传播（*Striving for Simplicity: The All Convolutional Net*， [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)）使我们能够可视化
    CNN 中某一层的单个单元所学到的特征。下图展示了算法的工作原理：'
- en: '![](img/f578ee79-db08-41d5-82dd-e246b5197edb.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f578ee79-db08-41d5-82dd-e246b5197edb.png)'
- en: Guided backpropagation visualization; Inspired by https://arxiv.org/abs/1412.6806.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 引导反向传播可视化；灵感来自于 [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)。
- en: 'Here is the step-by-step execution:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是逐步执行的过程：
- en: First, we start with a regular CNN (for example, AlexNet, VGG, and so on) with
    ReLU activations.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从一个常规的 CNN 开始（例如，AlexNet、VGG 等），并使用 ReLU 激活函数。
- en: Then, we feed the network with a single image *f^((0))* and propagate it forward
    until we get to the layer, *l*, we're interested in. This could be any network
    layer—hidden or output, convolutional or fully-connected.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将一个单一图像 *f^((0))* 输入网络，并向前传播，直到我们到达感兴趣的层 *l*。这可以是任何网络层——隐藏层或输出层，卷积层或全连接层。
- en: Set all but one activation of the output tensor *f^((l))* of that layer to 0\.
    For example, if we're interested in the output layer of a classification network,
    we'll select the unit with maximum activation (equivalent to the predicted class)
    and we'll set its value to 1\. All of the other units will be set to 0\. By doing
    this, we can isolate the unit in question and see which parts of the input image
    impact it the most.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该层输出张量 *f^((l))* 的所有激活值设置为 0，除了一个。例如，如果我们对分类网络的输出层感兴趣，我们将选择激活值最大（等同于预测类别）的单元，并将其值设置为
    1，其余单元设置为 0。通过这种方式，我们可以隔离出感兴趣的单元，查看哪些输入图像部分对其影响最大。
- en: Finally, we propagate the activation value of the selected unit backward until
    we reach the input layer and the reconstructed image *R^((0))*. The backward pass
    is very similar to the regular backpropagation (but not the same), that is, we
    still use transposed convolution as the backward operation of the forward convolution.
    In this case, though, we are interested in its image restoration properties rather
    than error propagation. Because of this, we aren't limited by the requirement
    to propagate the first derivative (gradient) and we can modify the signal in a
    way that will improve the visualization.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将选定单元的激活值向后传播，直到到达输入层并恢复图像 *R^((0))*。反向传播过程与常规反向传播非常相似（但并不完全相同），即我们仍然使用转置卷积作为前向卷积的反向操作。然而，在这种情况下，我们关注的是图像恢复特性，而不是误差传播。因此，我们不受限于传播一阶导数（梯度）的要求，能够以一种改善可视化的方式修改信号。
- en: 'To understand the backward pass, we''ll use an example convolution with a single
    3*×*3 input and output slices. Let''s assume that we''re using a 1*×*1 filter
    with a single weight equal to 1 (we repeat the input). The following diagram shows
    this convolution, as well as three different ways to implement the backward pass:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解反向传播，我们将使用一个示例卷积，输入和输出都是单个 3*×*3 的切片。假设我们使用一个 1*×*1 的滤波器，并且滤波器的权重为 1（我们重复输入）。下图展示了这个卷积操作，以及实现反向传播的三种不同方式：
- en: '![](img/319cc3f9-9d51-4e9e-828b-e962de89b88c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/319cc3f9-9d51-4e9e-828b-e962de89b88c.png)'
- en: Convolution and the three different ways to reconstruct the image; Inspired
    by https://arxiv.org/abs/1412.6806.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和三种不同的图像重构方式；灵感来自于 [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)。
- en: 'Let''s discuss these three different ways to implement the backward pass in
    detail:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这三种不同的反向传播实现方式：
- en: '**Regular backpropagation**: The backward signal is preconditioned on the input
    image since it also depends on the forward activations ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*, in the *Backpropagation* section). Our network
    uses a ReLU activation function, so the signal will only pass through the units
    that had positive activations in the forward pass.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常规反向传播**：反向信号依赖于输入图像，因为它也依赖于前向激活（见 [第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*，在
    *反向传播* 部分）。我们的网络使用 ReLU 激活函数，因此信号只会通过前向传播中激活值为正的单元。'
- en: '**Deconvolutional network** (*deconvnet*, [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)):
    The backward signal of layer *l* depends only on the backward signal of layer
    *l+1*. A deconvnet will only route the positive values of *l+1* to *l*, regardless
    of what the forward activations are. Theoretically, the signal is not preconditioned
    on the input image at all. In this case, the deconvnet tries to restore the image
    based on its internal knowledge and the image class. However, this is not entirely
    true—if the network contains max-pooling layers, the deconvnet will store the
    so-called **switches** for each pooling layer. Each switch represents a map of
    the units with max activations of the forward pass. This map determines how to
    route the signal through the backward pass (you can read more about this in the
    source paper).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反卷积网络**（*deconvnet*，[https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)）：层
    *l* 的反向信号仅依赖于层 *l+1* 的反向信号。deconvnet 只会将 *l+1* 的正值路由到 *l*，不管正向激活如何。理论上，信号完全不依赖于输入图像。在这种情况下，deconvnet
    会根据其内部知识和图像类别尝试恢复图像。然而，这并不完全正确——如果网络包含最大池化层，deconvnet 会存储每个池化层的所谓 **开关**。每个开关代表正向传播中最大激活值的单元的映射。该映射决定了如何在反向传播中路由信号（你可以在源论文中阅读更多内容）。'
- en: '**Guided backpropagation**: This is a combination of deconvnet and regular
    backprop. It will only route signals that have positive forward activations in
    *l* and positive backward activations in *l+1*. This adds additional guidance
    signal (hence the name) from the higher layers to the regular backprop. In essence,
    this step prevents negative gradients from flowing through the backward pass.
    The rationale is that the units that act as suppressors of our starting unit will
    be blocked and the reconstructed image will be free of their influence. Guided
    backpropagation performs so well that it doesn''t need to use deconvnet switches
    and instead routes the signal to all the units in each pooling region.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引导反向传播**：这是 deconvnet 和常规反向传播的结合。它只会将正向激活在 *l* 层中以及正向反向激活在 *l+1* 层中的信号传递。这为常规反向传播增加了额外的引导信号（因此得名），来自更高层次的指导信号。在本质上，这一步骤防止了负梯度在反向传播中流动。其原理是，作为抑制我们起始单元的单位将被阻止，重建的图像将不再受其影响。引导反向传播表现得如此出色，以至于它不需要使用
    deconvnet 开关，而是将信号传递给每个池化区域中的所有单元。'
- en: 'The following screenshot shows a reconstructed image that was generated using
    guided backpropagation and AlexNet:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了使用引导反向传播和 AlexNet 生成的重建图像：
- en: '![](img/d5ea6a22-529b-43c2-b783-e5cff83efdd8.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5ea6a22-529b-43c2-b783-e5cff83efdd8.png)'
- en: 'From left to right: original image, color reconstruction, and grayscale reconstruction
    using guided backpropagation on AlexNet; these images were generated using https://github.com/utkuozbulak/pytorch-cnn-visualizations.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：原始图像、颜色重建、以及使用引导反向传播在 AlexNet 上的灰度重建；这些图像是通过 [https://github.com/utkuozbulak/pytorch-cnn-visualizations](https://github.com/utkuozbulak/pytorch-cnn-visualizations)
    生成的。
- en: Gradient-weighted class activation mapping
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度加权类激活映射
- en: 'To understand gradient-weighted class activation mapping (*Grad-CAM: Visual
    Explanations from Deep Networks via Gradient-Based Localization*, [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)),
    let''s quote the source paper itself:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '要理解梯度加权类激活映射（*Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based
    Localization*，[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)），让我们引用源论文本身：'
- en: '"Grad-CAM uses the gradients of any target concept (say, the logits for ''dog''
    or even a caption) flowing into the final convolutional layer to produce a coarse
    localization map highlighting the important regions in the image for predicting
    the concept."'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '"Grad-CAM 使用任何目标概念（例如，‘狗’的逻辑回归值或甚至一个标题）的梯度，这些梯度流入最终的卷积层，以生成一个粗略的定位图，突出显示图像中对于预测该概念的重要区域。"'
- en: 'The following screenshot shows the Grad-CAM algorithm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 Grad-CAM 算法：
- en: '![](img/bcef2eda-a402-4ce0-871f-60bdd70bc65b.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcef2eda-a402-4ce0-871f-60bdd70bc65b.png)'
- en: 'Grad-CAM schema; Source: https://arxiv.org/abs/1610.02391'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM 方案；来源：[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)
- en: 'Now, let''s look at how it works:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看它是如何工作的：
- en: First, you start with a classification CNN model (for example, VGG).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你从一个分类 CNN 模型开始（例如，VGG）。
- en: Then, you feed the CNN with a single image and propagate it to the output layer.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你将一张单一的图像输入到 CNN 中，并将其传播到输出层。
- en: Like we did in guided backpropagation, we take the output unit with maximum
    activation (equivalent to the predicted class ***c***), set its value to 1, and
    set all of the other outputs to 0\. In other words, create a one-hot encoded vector, *y^c*,
    of the prediction.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同我们在引导反向传播中所做的那样，我们选取具有最大激活的输出单元（等价于预测类别 ***c***），将其值设为 1，并将其他所有输出设为 0。换句话说，创建一个预测的
    one-hot 编码向量 *y^c*。
- en: Next, compute the gradient of *y^c* with respect to the feature maps, *A^k*,
    of the final convolutional layer, ![](img/01d05e9f-c408-4523-b0aa-7ca630c33961.png),
    using backpropagation. *i* and *j* are the cell coordinates in the feature map.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用反向传播计算 *y^c* 相对于最终卷积层的特征图 *A^k* 的梯度， ![](img/01d05e9f-c408-4523-b0aa-7ca630c33961.png)。其中
    *i* 和 *j* 是特征图中的单元坐标。
- en: 'Then, compute the scalar weight, ![](img/7bd05250-83fc-45f6-ae8d-3900c9a2fce7.png),
    which measures the "importance" of feature map *k* for the predicted class, *c*:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，计算标量权重 ![](img/7bd05250-83fc-45f6-ae8d-3900c9a2fce7.png)，它衡量特征图 *k* 对预测类别 *c*
    的“重要性”：
- en: '![](img/423b3d51-d95b-493b-ace2-5735fb791876.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/423b3d51-d95b-493b-ace2-5735fb791876.png)'
- en: 'Finally, compute a weighted combination between the scalar weights and the
    forward activation feature maps of the final convolutional layer and follow this
    with a ReLU:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算标量权重与最终卷积层的前向激活特征图之间的加权组合，并随后使用 ReLU：
- en: '![](img/8ac9e118-0da1-4eab-8b7d-0716b076b07b.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ac9e118-0da1-4eab-8b7d-0716b076b07b.png)'
- en: 'Note that we multiply the scalar importance weight, ![](img/5c1c7470-1634-4c96-8d46-5d40c6d68f9a.png),
    by the tensor feature map, *A^k*. The result is a heatmap with the same dimensions
    as the feature map (14*×*14 in the case of VGG and AlexNet). It will highlight
    the areas of the feature map with the highest importance to class *c*. The ReLU
    discards the negative activations because we''re only interested in the features
    that increase *y^c*. We can upsample this heatmap back to the size of the input
    image and then superimpose it on it, as shown in the following screenshot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将标量重要性权重 ![](img/5c1c7470-1634-4c96-8d46-5d40c6d68f9a.png) 乘以张量特征图 *A^k*。结果是一个与特征图相同维度的热力图（对于
    VGG 和 AlexNet，维度为 14*×*14）。它会突出显示对类别 *c* 最重要的特征图区域。ReLU 会丢弃负激活，因为我们只关心那些能增加 *y^c*
    的特征。我们可以将这个热力图上采样回输入图像的大小，然后叠加在图像上，如下图所示：
- en: '![](img/decb3007-cf16-4fad-835a-52e5462ac541.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/decb3007-cf16-4fad-835a-52e5462ac541.png)'
- en: 'Left to right: input image; upsampled heat-map; heat-map superimosed on the
    input (RGB); grayscale heat-map. The images were generated using https://github.com/utkuozbulak/pytorch-cnn-visualizations.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：输入图像；上采样后的热力图；热力图叠加在输入图像上（RGB）；灰度热力图。图像是使用 https://github.com/utkuozbulak/pytorch-cnn-visualizations
    生成的。
- en: 'One problem with Grad-CAM is upsampling the heatmap from 14*×*14 to 224*×*224
    because it doesn''t provide a fine-grained perspective of the important features
    for each class. To mitigate this problem, the authors of the paper proposed a
    combination of Grad-CAM and guided backpropagation (displayed in the Grad-CAM
    schema at the beginning of this section). We take the upsampled heatmap and combine
    it with the guided backprop visualization with element-wise multiplication. The
    input image contains two objects: a dog and a cat. Therefore, we can run Grad-CAM
    with both classes (the two rows of the diagram). This example shows how different
    classes detect different relevant features in the same image.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM 的一个问题是将热力图从 14*×*14 上采样到 224*×*224，因为它无法提供每个类别的重要特征的细粒度视角。为了缓解这个问题，论文的作者提出了
    Grad-CAM 和引导反向传播（在本节开始的 Grad-CAM 示意图中显示）的结合。我们将上采样的热力图与引导反向传播的可视化进行逐元素相乘。输入图像包含两种物体：一只狗和一只猫。因此，我们可以对这两种类别运行
    Grad-CAM（图示的两行）。这个例子展示了不同类别在同一图像中检测不同相关特征的方法。
- en: In the next section, we'll discuss how to optimize CNNs with the help of regularization.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何借助正则化来优化 CNN。
- en: CNN regularization
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 正则化
- en: As we discussed in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, a NN can approximate any function. But
    with great power comes great responsibility. The NN may learn to approximate the
    noise of the target function rather than its useful components. For example, imagine
    that we are training a NN to classify whether an image contains a car or not,
    but for some reason, the training set contains mostly red cars. It may turn out
    that the NN will associate the color red with the car, rather than its shape.
    Now, if the network sees a green car in inference mode, it may not recognize it
    as such because the color doesn't match. This problem is referred to as overfitting
    and it is central in machine learning (and even more so in deep networks). In
    this section, we'll discuss several ways to prevent it. Such techniques are collectively known
    as **regularization**.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*中讨论的那样，神经网络（NN）可以逼近任何函数。但强大的能力伴随着巨大的责任。神经网络可能会学会逼近目标函数的噪声部分，而不是有用的成分。例如，假设我们正在训练一个神经网络来分类图像是否包含汽车，但由于某种原因，训练集中的大多数汽车是红色的。结果可能是神经网络将红色与汽车关联起来，而不是它的形状。现在，如果网络在推理模式下看到一辆绿色汽车，可能会因为颜色不匹配而无法识别它。这种问题被称为过拟合，是机器学习中的核心问题（在深度网络中尤其如此）。在本节中，我们将讨论几种防止过拟合的方法。这些技术统称为**正则化**。
- en: 'In the context of NNs, these regularization techniques usually impose some
    artificial limitations or obstacles on the training process to prevent the network
    from approximating the target function too closely. They try to guide the network
    to learn generic rather than specific approximation of the target function in
    the hope that this representation will generalize well on previously unseen examples
    of the test dataset. You may already be familiar with many of these techniques,
    so we''ll keep it short:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的上下文中，这些正则化技术通常会对训练过程施加一些人工的限制或障碍，以防止网络过于精确地逼近目标函数。它们试图引导网络学习目标函数的通用而非特定的逼近方式，希望这种表示方法能够很好地泛化到测试数据集中之前未见过的示例。你可能已经熟悉其中的许多技术，因此我们会简要介绍：
- en: '**Input feature scaling**: ![](img/566be761-e32f-489f-a3f5-cb215784bef2.png). This
    operation scales all of the inputs in the [0, 1] range. For example, a pixel with
    intensity 125 would have a scaled value of ![](img/b247a222-8b4d-4945-9732-e5444b75478b.png).
    Feature scaling is fast and easy to implement.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入特征缩放**：![](img/566be761-e32f-489f-a3f5-cb215784bef2.png)。此操作将所有输入缩放到[0,
    1]范围内。例如，一个强度为125的像素会有一个缩放值为![](img/b247a222-8b4d-4945-9732-e5444b75478b.png)。特征缩放实施起来既快速又简单。'
- en: '**Input standard score**:![](img/a42dd245-b0f6-4a2a-8fc6-e991a4b1f37e.png). Here,
    μ and σ are the mean and standard deviation of all of the training data. They are
    usually computed separately for each input dimension. For example, in an RGB image,
    we would compute the mean *μ* and *σ* for each channel. We should note that *μ*
    and *σ* have to be computed on the training data and then applied to the test
    data. Alternatively, we can compute *μ* and *σ* per sample if it''s not practical
    to compute them over the entire dataset.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入标准分数**：![](img/a42dd245-b0f6-4a2a-8fc6-e991a4b1f37e.png)。这里，μ和σ是所有训练数据的均值和标准差。它们通常是针对每个输入维度单独计算的。例如，在RGB图像中，我们会为每个通道计算均值*μ*和*σ*。我们需要注意的是，*μ*和*σ*必须在训练数据上计算，然后应用到测试数据上。或者，如果不便于在整个数据集上计算，我们可以按样本计算*μ*和*σ*。'
- en: '**Data augmentation**: This is where we artificially increase the size of the
    training set by applying random modifications (rotation, skew, scaling, and so
    on) on the training samples before feeding them to the network.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：这是通过在将训练样本输入网络之前，应用随机修改（如旋转、扭曲、缩放等）来人为地增加训练集的大小。'
- en: '**L2 regularization** (or **weight decay**): Here, we add a special regularization
    term to the cost function. Let''s assume that we''re using MSE ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of NNs*, *Gradient descent* section). Here, the MSE + L2 regularization
    formula is as follows:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2正则化**（或**权重衰减**）：在这里，我们在成本函数中添加一个特殊的正则化项。假设我们使用的是均方误差（MSE）（[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*，*梯度下降*部分）。这里，MSE
    + L2正则化的公式如下：'
- en: '![](img/f0238ce8-2b02-4940-8b97-e734088e66ae.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0238ce8-2b02-4940-8b97-e734088e66ae.png)'
- en: Here, *w[j]* is one of *k* total network weights and λ is the weight decay coefficient.
    The rationale is that if the network weights, *w[j]*, are large, then the cost
    function will also increase. In effect, weight decay penalizes large weights (hence
    the name). This prevents the network from relying too heavily on a few features
    associated with these weights. There is less chance of overfitting when the network
    is forced to work with multiple features. In practical terms, when we compute
    the derivative of the weight decay cost function (the preceding formula) with
    respect to each weight and then propagate it to the weights themselves, the weight
    update rule changes from ![](img/616423c0-4765-4633-992c-40af9f782b5c.png) to
    ![](img/97768311-0e57-46f7-ab19-d63b6d727f0d.png).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w[j]* 是总共 *k* 个网络权重中的一个，λ 是权重衰减系数。其基本原理是，如果网络权重 *w[j]* 较大，那么成本函数也会增加。实际上，权重衰减对大权重进行惩罚（因此得名）。这可以防止网络过于依赖与这些权重相关的少数特征。当网络被迫使用多个特征时，过拟合的可能性较小。在实际应用中，当我们计算权重衰减成本函数（前述公式）相对于每个权重的导数，并将其传播到权重本身时，权重更新规则从
    ![](img/616423c0-4765-4633-992c-40af9f782b5c.png) 变为 ![](img/97768311-0e57-46f7-ab19-d63b6d727f0d.png)。
- en: '**Dropout**: Here, we randomly and periodically remove some of the neurons
    (along with their input and output connections) from the network. During a training
    mini-batch, each neuron has a probability, *p*, of being stochastically dropped.
    This is to ensure that no neuron ends up relying too much on other neurons and
    "learns" something useful for the network instead.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout（丢弃法）**：在此过程中，我们随机并定期从网络中移除一些神经元（连同它们的输入输出连接）。在训练的小批次期间，每个神经元有 *p*
    的概率被随机丢弃。这是为了确保没有神经元过于依赖其他神经元，而是“学习”对网络有用的东西。'
- en: '**Batch Normalization** (**BN**, *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*, [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)): This
    is a way to apply data processing, similar to the standard score, for the hidden
    layers of the network. It normalizes the outputs of the hidden layer for each
    mini-batch (hence the name) in a way that maintains its mean activation value
    close to 0 and its standard deviation close to 1. Let''s say ![](img/b32be79b-c498-426d-ab86-63943dc09652.png) is
    a mini-batch of size *n*. Each sample of *D* is a vector, ![](img/a3c7bc7c-e67d-4c0d-9550-05aa36f4a3a0.png),
    and ![](img/d8826d10-f9e0-4724-a977-eb5da755842e.png)is a cell with an index *k*
    of that vector. For the sake of clarity, we''ll omit the (*k*) superscript in
    the following formulas; that is, we''ll write *x[i]*, but we''ll mean ![](img/a7f2a55b-b380-4a97-92fa-5458f6ff024d.png).
    We can compute BN for each activation, *k*, over the whole minibatch in the following
    way:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化**（**BN**，*Batch Normalization: Accelerating Deep Network Training by
    Reducing Internal Covariate Shift*， [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)）：这是一种应用数据处理的方法，类似于标准分数，适用于网络的隐藏层。它对每个小批次的隐藏层输出进行归一化（因此得名），使得其均值接近0，标准差接近1。假设 ![](img/b32be79b-c498-426d-ab86-63943dc09652.png) 是一个大小为
    *n* 的小批次。每个 *D* 的样本是一个向量，![](img/a3c7bc7c-e67d-4c0d-9550-05aa36f4a3a0.png)，而 ![](img/d8826d10-f9e0-4724-a977-eb5da755842e.png)是该向量的索引为
    *k* 的单元。为了清晰起见，以下公式中我们将省略 (*k*) 上标；也就是说，我们写 *x[i]*，但指代的是 ![](img/a7f2a55b-b380-4a97-92fa-5458f6ff024d.png)。我们可以按照以下方式计算每个激活值
    *k* 在整个小批次上的 BN：'
- en: '[![](img/a7b5373c-098a-413c-833d-83609bcd6a1e.png)]: This is the mini-batch
    mean. We compute *μ* separately for each location, *k*, over all samples.'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[![](img/a7b5373c-098a-413c-833d-83609bcd6a1e.png)]：这是小批次均值。我们分别为每个位置 *k* 计算
    *μ*，然后对所有样本求平均。'
- en: '![](img/d86ffa01-2a3f-43f1-8a3e-efbc2731d556.png): This is the mini-batch standard
    deviation. We compute *σ* separately for each location, *k*, over all samples.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/d86ffa01-2a3f-43f1-8a3e-efbc2731d556.png)：这是小批次标准差。我们分别为每个位置 *k* 计算
    *σ*，然后对所有样本求平均。'
- en: '![](img/7acf3e8f-0a11-47d6-8cc1-0aae11396f2d.png): We normalize each sample. *ε*
    is a constant that''s added for numerical stability.'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/7acf3e8f-0a11-47d6-8cc1-0aae11396f2d.png)：我们对每个样本进行归一化处理。*ε* 是一个常数，用于保证数值稳定性。'
- en: '![](img/7a6d44a8-9740-4439-9462-217732c87adc.png): *γ* and *β* are learnable
    parameters and we compute them over each location, *k* (*γ^((k))* and *β^((k))*),
    over all of the samples of the mini-batch (the same applies for *μ* and *σ*).
    In convolutional layers, each sample, *x*, is a tensor with multiple feature maps.
    To preserve the convolutional property, we compute *μ* and *σ* per location over
    all of the samples, but we use the same *μ* and *σ* in the matching locations
    across all of the feature maps. On the other hand, we compute *γ* and *β* per
    feature map, rather than per location.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/7a6d44a8-9740-4439-9462-217732c87adc.png): *γ* 和 *β* 是可学习的参数，我们在每个位置上计算它们，*k*
    (*γ^((k))* 和 *β^((k))*)，在所有小批量样本中进行计算（*μ* 和 *σ* 也适用）。在卷积层中，每个样本，*x*，是一个包含多个特征图的张量。为了保持卷积特性，我们在所有样本的每个位置上计算
    *μ* 和 *σ*，但在所有特征图中的匹配位置使用相同的 *μ* 和 *σ*。另一方面，我们在每个特征图上计算 *γ* 和 *β*，而不是每个位置上。'
- en: This section concludes our analysis of the structure and inner workings of CNNs.
    At this point, we would normally proceed with some sort of CNN coding example.
    But in this book, we want to do things a little differently. Therefore, we won't
    implement a plain old feed-forward CNN, which you may have already done before.
    Instead, in the next section, you will be introduced to the technique of transfer
    learning—a way to use pretrained CNN models for new tasks. But don't worry—we'll
    still implement a CNN from scratch. We'll do this in [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks.* In this way, we'll be able to create a more
    complex network architecture using our knowledge from that chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结束了我们对卷积神经网络（CNN）结构和内部工作原理的分析。到这时，我们通常会继续进行一些CNN编程示例。但是在本书中，我们想做得稍微不同一些。因此，我们不会实现一个传统的前馈CNN，您可能之前已经做过。相反，在下一节中，您将会接触到**迁移学习**技术——一种使用预训练的CNN模型处理新任务的方法。但请放心，我们仍然会从零开始实现一个CNN。我们将在[第3章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)
    *高级卷积网络* 中实现。通过这种方式，我们将能够利用该章的知识构建一个更复杂的网络架构。
- en: Introducing transfer learning
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍迁移学习
- en: Let's say that we want to train a model on a task that doesn't have readily
    available labeled training data like ImageNet does. Labeling training samples
    could be expensive, time-consuming, and error-prone. So, what does a humble engineer
    do when they want to solve a real ML problem with limited resources? Enter **Transfer
    Learning** (**TL**).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要在一个没有像 ImageNet 那样现成的标签训练数据的任务上训练模型。标记训练样本可能会非常昂贵、耗时且容易出错。那么，当工程师想用有限的资源解决实际的机器学习问题时，该怎么办呢？这时就有了**迁移学习**（**Transfer
    Learning**，简称**TL**）。
- en: TL is the process of applying an existing trained ML model to a new, but related,
    problem. For example, we can take a network trained on ImageNet and repurpose
    it to classify grocery store items. Alternatively, we could use a driving simulator
    game to train a neural network to drive a simulated car and then use the network
    to drive a real car (but don't try this at home!). TL is a general ML concept
    that's applicable to all ML algorithms, but in this context, we'll talk about
    CNNs. Here's how it works.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是将一个已训练好的机器学习模型应用到一个新的但相关的问题的过程。例如，我们可以将一个在 ImageNet 上训练的网络重新用于分类超市商品。或者，我们可以使用驾驶模拟器游戏训练一个神经网络来驾驶模拟车，然后再用这个网络驾驶一辆真实的车（但不要在家尝试！）。迁移学习是一个通用的机器学习概念，适用于所有机器学习算法，但在这个上下文中，我们将讨论卷积神经网络（CNN）。它是如何工作的呢？
- en: We start with an existing pretrained network. The most common scenario is to
    take a pretrained network from ImageNet, but it could be any dataset. TensorFlow
    and PyTorch both have popular ImageNet pretrained neural architectures that we
    can use. Alternatively, we can train our own network with a dataset of our choice.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个已有的预训练网络开始。最常见的做法是从 ImageNet 获取一个预训练的网络，但也可以是任何数据集。TensorFlow 和 PyTorch
    都有流行的 ImageNet 预训练神经网络架构供我们使用。或者，我们也可以使用自己选择的数据集训练一个网络。
- en: The fully-connected layers at the end of a CNN act as translators between the
    network's language (the abstract feature representations learned during training)
    and our language, which is the class of each sample. You can think of TL as a
    translation into another language. We start with the network's features, which
    is the output of the last convolutional or pooling layer. Then, we translate them
    into a different set of classes of the new task. We can do this by removing the
    last fully-connected layer (or all the fully-connected layers) of an existing
    pretrained network and replacing it with another layer, which represents the classes
    of the new problem.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: CNN末尾的全连接层充当网络语言（在训练过程中学习到的抽象特征表示）和我们语言（每个样本的类别）之间的转换器。你可以将迁移学习看作是翻译成另一种语言。我们从网络的特征开始，即最后一层卷积或池化层的输出。然后，我们将它们翻译成新任务的不同类别。我们可以通过去掉现有预训练网络的最后一层全连接层（或所有全连接层），并用另一个层替代它，后者代表新问题的类别。
- en: 'Let''s look at the TL scenario shown in the following diagram:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下图所示的迁移学习（TL）场景：
- en: '![](img/b755ece2-264b-4069-b880-ce0486c1ffef.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b755ece2-264b-4069-b880-ce0486c1ffef.png)'
- en: In TL, we can replace the fully-connected layer(s) of a pretrained net and repurpose
    it/them for a new problem
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们可以替换预训练网络的全连接层，并将其重新用于新问题。
- en: 'However, we cannot do this mechanically and expect the new network to work
    because we still have to train the new layer with data related to the new task.
    Here, we have two options:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不能机械地进行操作并期待新网络能够工作，因为我们仍然需要用与新任务相关的数据训练新层。在这里，我们有两个选项：
- en: '**Use the original part of the network as a feature extractor and only train
    the new layer(s)**: In this scenario, we feed the network a training batch of
    the new data and propagate it forward to see the network''s output. This part
    works just like regular training would. But in the backward pass, we lock the
    weights of the original network and only update the weights of the new layers.
    This is the recommended way to do things when we have limited training data for
    the new problem. By locking most of the network weights, we prevent overfitting
    on the new data.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将原网络部分作为特征提取器，只训练新层**：在这种情况下，我们将网络输入一批新的训练数据并前向传播，以查看网络的输出。这一部分就像常规训练一样工作。但是在反向传播时，我们锁定原始网络的权重，只更新新层的权重。当我们在新问题上有有限的训练数据时，推荐使用这种方法。通过锁定大部分网络权重，我们可以防止在新数据上出现过拟合。'
- en: '**Fine-tune the whole network**: In this scenario, we''ll train the whole network
    and not just the newly added layers at the end. It is possible to update all of
    the network weights, but we can also lock some of the weights in the first layers.
    The idea here is that the initial layers detect general features—not related to
    a specific task—and it makes sense to reuse them. On the other hand, the deeper
    layers may detect task-specific features and it would be better to update them.
    We can use this method when we have more training data and don''t need to worry
    about overfitting.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调整个网络**：在这种情况下，我们将训练整个网络，而不仅仅是最后添加的新层。我们可以更新所有网络的权重，但也可以锁定前几层的部分权重。这里的思路是，初始层检测的是一般特征——与特定任务无关——因此重复使用这些层是有意义的。另一方面，深层可能会检测到与任务相关的特定特征，因此更新这些层会更好。当我们有更多训练数据并且不需要担心过拟合时，可以使用这种方法。'
- en: Implementing transfer learning with PyTorch
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch实现迁移学习
- en: Now that we know what TL is, let's look at whether it works in practice. In
    this section, we'll apply an advanced ImageNet pretrained network on the CIFAR-10
    images with **PyTorch 1.3.1** and the `torchvision` 0.4.2 package. We'll use both
    types of TL. It's preferable to run this example on a GPU.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是迁移学习，让我们来看看它在实践中是否有效。在本节中，我们将使用**PyTorch 1.3.1**和`torchvision` 0.4.2包，将一个高级的ImageNet预训练网络应用于CIFAR-10图像。我们将使用两种类型的迁移学习。最好在GPU上运行此示例。
- en: This example is partially based on [https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例部分基于[https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py)。
- en: 'Let''s get started:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始吧：
- en: 'Do the following imports:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行以下导入：
- en: '[PRE0]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define `batch_size` for convenience:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便起见，定义`batch_size`：
- en: '[PRE1]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the training dataset. We have to consider a few things:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练数据集。我们需要考虑以下几个方面：
- en: The CIFAR-10 images are 32*×*32, while the ImageNet network expects 224*×*224
    input. Since we are using an ImageNet-based network, we'll upsample the 32*×*32
    CIFAR images to 224*×*224.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR-10的图像大小是32×32，而ImageNet网络期望输入为224×224。由于我们使用的是基于ImageNet的网络，我们将CIFAR的32×32图像上采样到224×224。
- en: Standardize the CIFAR-10 data using the ImageNet mean and standard deviation
    since this is what the network expects.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ImageNet的均值和标准差对CIFAR-10数据进行标准化，因为网络需要这些数据格式。
- en: 'We''ll also add some data augmentation in the form of random horizontal or
    vertical flips:'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将添加一些数据增强，形式是随机的水平或垂直翻转：
- en: '[PRE2]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the same steps with the validation/test data, but this time without
    augmentation:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对验证/测试数据执行相同的步骤，但这次不进行数据增强：
- en: '[PRE3]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Choose `device`, preferably a GPU with a fallback on CPU:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`device`，最好选择GPU，如果没有则使用CPU：
- en: '[PRE4]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the training of the model. Unlike TensorFlow, in PyTorch, we have to
    iterate over the training data manually. This method iterates once over the whole
    training set (one epoch) and applies the optimizer after each forward pass:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型的训练过程。与TensorFlow不同，在PyTorch中，我们需要手动遍历训练数据。这个方法会遍历整个训练集一次（一个epoch），并在每次前向传播后应用优化器：
- en: '[PRE5]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the testing/validation of the model. This is very similar to the training
    phase, but we will skip the backpropagation part:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型的测试/验证过程。这与训练阶段非常相似，但我们将跳过反向传播部分：
- en: '[PRE6]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the first TL scenario, where we use the pretrained network as a feature
    extractor:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义第一个TL场景，我们将预训练网络作为特征提取器：
- en: We'll use a popular network known as ResNet-18\. We'll talk about this in detail
    in the *Advanced network architectures* section. PyTorch will automatically download
    the pretrained weights.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用一个流行的网络架构——ResNet-18。我们将在*高级网络架构*部分详细讨论这个内容。PyTorch 会自动下载预训练的权重。
- en: Replace the last network layer with a new layer with 10 outputs (one for each
    CIFAR-10 class).
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用具有10个输出的新层替换网络的最后一层（每个CIFAR-10类别一个输出）。
- en: Exclude the existing network layers from the backward pass and only pass the
    newly added fully-connected layer to the Adam optimizer.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除现有的网络层的反向传播，只将新添加的全连接层传递给Adam优化器。
- en: Run the training for `epochs` and evaluate the network accuracy after each epoch.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练`epochs`次数，并在每个epoch后评估网络的准确度。
- en: Plot the test accuracy with the help of the `plot_accuracy` function. Its definition
    is trivial and you can find it in this book's code repository.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`plot_accuracy`函数绘制测试准确度。该函数的定义非常简单，你可以在本书的代码库中找到它。
- en: 'The following is the `tl_feature_extractor` function, which implements all
    of this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`tl_feature_extractor`函数，它实现了所有这些功能：
- en: '[PRE7]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Implement the fine-tuning approach. This function is similar to `tl_feature_extractor`,
    but here, we''re training the whole network:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现微调方法。这个函数类似于`tl_feature_extractor`，但在这里，我们训练整个网络：
- en: '[PRE8]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can run the whole thing in one of two ways:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过两种方式之一运行整个过程：
- en: Call `tl_fine_tuning()` to use the fine-tuning TL approach for five epochs.
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`tl_fine_tuning()`来使用微调的迁移学习方法，训练五个epochs。
- en: Call `tl_feature_extractor()` to train the network with the feature extractor
    approach for five epochs.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`tl_feature_extractor()`来使用特征提取方法训练网络，训练五个epochs。
- en: 'This is the accuracy of the networks after five epochs for the two scenarios:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这是网络在五个epochs后在两种场景下的准确度：
- en: '![](img/17c9041d-7c61-440d-abc9-4065268247ba.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17c9041d-7c61-440d-abc9-4065268247ba.png)'
- en: 'Left: Feature extraction TL accuracy; right: Fine-tuning TL accuracy'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 左：特征提取TL准确度；右：微调TL准确度
- en: Due to the large size of the chosen `ResNet18` pretrained model, the network
    starts to overfit in the feature extraction scenario.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于选择的`ResNet18`预训练模型较大，在特征提取场景中，网络开始出现过拟合。
- en: Transfer learning with TensorFlow 2.0
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2.0进行迁移学习
- en: In this section, we'll implement the two transfer learning scenarios again,
    but this time using **TensorFlow 2.0.0 (TF)**. In this way, we can compare the
    two libraries. Instead of `ResNet18`, we'll use the `ResNet50V2` architecture
    (more on that in the [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*). In addition to TF, this example also requires
    the TF Datasets 1.3.0 package ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)),
    a collection of various popular ML datasets.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将再次实现这两种迁移学习场景，但这次使用**TensorFlow 2.0.0 (TF)**。通过这种方式，我们可以比较这两个库。我们将使用`ResNet50V2`架构（更多内容参见[第3章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)，*高级卷积网络*）。除了TF，示例还需要TF
    Datasets 1.3.0包（[https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)），这是一个包含各种流行机器学习数据集的集合。
- en: This example is partially based on [https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子部分基于[https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb)。
- en: 'With that, let''s get started:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，让我们开始吧：
- en: 'As usual, first, we need to do the imports:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，首先，我们需要进行导入：
- en: '[PRE9]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we''ll define the mini-batch and input images sizes (the image size is
    determined by the network architecture):'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义小批量和输入图像的大小（图像大小由网络架构决定）：
- en: '[PRE10]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we''ll load the CIFAR-10 dataset with the help of TF datasets. The `repeat()`
    method allows us to reuse the dataset for multiple epochs:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将借助TF数据集加载CIFAR-10数据集。`repeat()`方法允许我们在多个周期中重用数据集：
- en: '[PRE11]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we''ll define the `train_format_sample` and `test_format_sample` functions,
    which will transform the input images into suitable CNN inputs. These functions
    play the same roles that the `transforms.Compose` object plays, which we defined
    in the *Implementing transfer learning with PyTorch* section. The input is transformed
    as follows:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义`train_format_sample`和`test_format_sample`函数，这些函数将把输入图像转换为适合CNN的输入。这些函数起着与我们在*使用PyTorch实现迁移学习*章节中定义的`transforms.Compose`对象相同的作用。输入经过以下转换：
- en: The images are resized to 96*×*96, which is the expected network input size.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像被调整为96×96的大小，这是期望的网络输入大小。
- en: Each image is standardized by transforming its values so that it's in the (-1;
    1) interval.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个图像都通过转换其值进行标准化，使其位于（-1；1）区间内。
- en: The labels are transformed for one-hot encoding.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签会转换为独热编码。
- en: The training images are randomly flipped horizontally and vertically.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练图像会被随机水平和垂直翻转。
- en: 'Let''s look at the actual implementation:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际的实现：
- en: '[PRE12]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next is some boilerplate code that assigns these transformers to the train/test
    datasets and splits them into mini-batches:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一些模板代码，它将这些转换器分配给训练/测试数据集，并将它们分割成小批量：
- en: '[PRE13]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we need to define the feature extraction model:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义特征提取模型：
- en: We'll use Keras for the pretrained network and model definition since it is
    an integral part of TF 2.0.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用Keras来定义预训练网络和模型，因为它是TF 2.0的重要组成部分。
- en: We load the `ResNet50V2` pretrained net, excluding the final fully-connected
    layers.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们加载`ResNet50V2`预训练网络，排除最后的全连接层。
- en: Then, we call `base_model.trainable = False`, which *freezes* all of the network
    weights and prevents them from training.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们调用`base_model.trainable = False`，这将*冻结*所有网络权重，并防止它们参与训练。
- en: Finally, we add a `GlobalAveragePooling2D` operation, followed by a new and
    trainable fully-connected trainable layer at the end of the network.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们添加一个`GlobalAveragePooling2D`操作，并在网络末尾添加一个新的可训练全连接层。
- en: 'The following code implements this:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了这一点：
- en: '[PRE14]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we''ll define the fine-tuning model. The only difference it has from
    the feature extraction is that we only freeze some of the bottom pretrained network
    layers (as opposed to all of them). The following is the implementation:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义微调模型。它与特征提取模型的唯一区别是，我们仅冻结一些底层的预训练网络层（而不是全部冻结）。以下是实现：
- en: '[PRE15]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we''ll implement the `train_model` function, which trains and evaluates
    the models that are created by either the `build_fe_model` or `build_ft_model`
    function:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将实现`train_model`函数，该函数训练和评估由`build_fe_model`或`build_ft_model`函数创建的模型：
- en: '[PRE16]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can run either the feature extraction or fine-tuning TL using the following
    code:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码运行特征提取或微调迁移学习（TL）：
- en: '`train_model(build_ft_model())`'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_model(build_ft_model())`'
- en: '`train_model(build_fe_model())`'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_model(build_fe_model())`'
- en: 'TF will automatically use the machine GPU if one is available; otherwise, it
    will revert to the CPU. The following diagram shows the accuracy of the networks
    after five epochs for the two scenarios:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器有 GPU，TF 将自动使用 GPU；否则，它将回退到 CPU。下图展示了两种场景下网络在五个训练周期后的准确度：
- en: '![](img/3c73d77d-462f-4bfe-8c5d-2e9e7d7f9c64.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c73d77d-462f-4bfe-8c5d-2e9e7d7f9c64.png)'
- en: 'Left: Feature extraction TL; right: Fine-tuning TL'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：特征提取迁移学习；右侧：微调迁移学习
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter with a quick recap of CNNs and discussed transposed,
    depthwise separable, and dilated convolutions. Next, we talked about improving
    the performance of CNNs by representing the convolution as a matrix multiplication
    or with the Winograd convolution algorithm. Then, we focused on visualizing CNNs
    with the help of guided backpropagation and Grad-CAM. Next, we discussed the most
    popular regularization techniques. Finally, we learned about transfer learning
    and implemented the same TL task with both PyTorch and TF as a way to compare
    the two libraries.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 本章一开始我们快速回顾了 CNN，并讨论了转置卷积、深度可分离卷积和膨胀卷积。接着，我们介绍了通过将卷积表示为矩阵乘法或使用 Winograd 卷积算法来提高
    CNN 性能。然后，我们重点讲解了如何借助引导反向传播和 Grad-CAM 可视化 CNN。接下来，我们讨论了最受欢迎的正则化技术。最后，我们学习了迁移学习，并通过
    PyTorch 和 TF 实现了相同的迁移学习任务，以便比较这两个库。
- en: In the next chapter, we'll discuss some of the most popular advanced CNN architectures.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论一些最受欢迎的高级 CNN 架构。
