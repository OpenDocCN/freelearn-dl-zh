- en: '*Chapter 2*: Multi-Armed Bandits'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：多臂赌博机'
- en: 'When you log on to your favorite social media app, chances are that you''ll
    see one of the many versions of the app that are tested at that time. When you
    visit a website, the ads displayed to you are tailored to your profile. In many
    online shopping platforms, the prices are determined dynamically. Do you know
    what all of these have in common? They are often modeled as **multi-armed bandit**
    (**MAB**) problems to identify optimal decisions. A MAB problem is a form of **reinforcement**
    **learning** (**RL**), where the agent makes decisions in a problem horizon that
    consists of a single step. Therefore, the goal is to maximize only the immediate
    reward, and there are no consequences considered for any subsequent steps. While
    this is a simplification over multi-step RL, the agent must still deal with a
    fundamental trade-off of RL: the exploration of new actions that could possibly
    lead to higher rewards versus exploitation of the actions that are known to be
    decent. A wide range of business problems, such as the ones mentioned previously,
    involve optimizing this exploration-exploitation trade-off. Throughout the next
    two chapters, you will understand the implications of this trade-off – which will
    be a recurring theme in almost all RL methods – and learn how to effectively address
    it.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你登录到你最喜欢的社交媒体应用时，你很可能会看到当时正在测试的多种版本之一。当你访问一个网站时，展示给你的广告是根据你的个人资料量身定制的。在许多在线购物平台上，价格是动态决定的。你知道这些现象有什么共同点吗？它们通常被建模为**多臂赌博机**（**MAB**）问题，用于识别最优决策。MAB问题是一种**强化学习**（**RL**）的形式，其中智能体在一个由单一步骤组成的时间范围内做出决策。因此，目标是最大化即时奖励，并且没有考虑任何后续步骤的后果。虽然这比多步骤RL做了简化，但智能体仍然必须处理RL中的一个基本权衡：探索可能导致更高奖励的新动作与利用已知能带来合理奖励的动作之间的权衡。许多商业问题，如前面提到的，涉及到优化这种探索与利用的权衡。在接下来的两章中，你将了解这个权衡的意义——它几乎会成为所有RL方法的反复出现的主题——并学习如何有效地应对它。
- en: In this chapter, we lay the groundwork by solving MAB problems that don't take
    into account the "context" in which the actions are taken, such as the profile
    of the user visiting the website/app of interest, the time of day, and so on.
    To that end, we cover four fundamental exploration strategies. In the next chapter,
    we are going the extend these strategies to solve **contextual MABs**. In both
    chapters, we use online advertising, an important application of bandit problems,
    as our running case study.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过解决不考虑“上下文”的MAB问题来奠定基础，例如访问网站/应用的用户资料、时间等。为此，我们介绍了四种基本的探索策略。在下一章中，我们将扩展这些策略以解决**上下文相关的MAB**问题。在这两章中，我们将使用在线广告——多臂赌博机问题的重要应用——作为我们的持续案例研究。
- en: 'So, let''s get started! Here is what we will specifically cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始吧！在本章中，我们将具体讨论以下内容：
- en: Exploration-exploitation trade-off
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索与利用的权衡
- en: What is a MAB?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是MAB？
- en: Case study – online advertising
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究——在线广告
- en: A/B/n testing as an exploration strategy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A/B/n 测试作为探索策略
- en: '![](img/Formula_02_000.png)-greedy actions for exploration'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_000.png)-贪婪动作用于探索'
- en: Action selection using upper confidence bounds
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用上置信界限进行动作选择
- en: Thompson (posterior) sampling
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汤普森（后验）采样
- en: Exploration-exploitation trade-off
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用的权衡
- en: As we mentioned earlier, RL is all about learning from experience without a
    supervisor labeling correct actions for the agent. The agent observes the consequences
    of its actions, identifies what actions are leading to the highest rewards in
    each situation, and learns from this experience. Now, think about something you
    have learned from your own experience – for example, how to study for a test.
    Chances are you explored different methods until you discovered what works best
    for you. Maybe you studied regularly for your tests first, but then you tested
    whether studying the last night before the test could work well enough – and maybe
    it does for certain types of tests. The point is that you had to **explore** to
    find the method(s) that maximizes your "reward," which is a function of your test
    score, time spent on leisure activities, your anxiety levels before and during
    the test, and so on. In fact, exploration is essential for any learning that is
    based on experience. Otherwise, we may never discover better ways of doing things
    or a way that works at all! On the other hand, we cannot always be trying new
    ways. It would be silly to not exploit what we have already learned! So, there
    is a *trade-off between exploration and exploitation*, and this trade-off is at
    the very center of RL. It is crucial to balance this trade-off for efficient learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，RL完全依赖于从经验中学习，而无需监督员为代理标记正确的行动。代理观察其行动的后果，确定在每种情况下导致最高奖励的行动，并从这种经验中学习。现在，请考虑一下你从自己的经验中学到的东西——例如，如何为考试学习。很可能你探索了不同的方法，直到发现最适合你的方法。也许你先是定期为考试学习，但后来你测试过在考试前夜学习是否足够有效——也许对某些类型的考试确实有效。关键是你必须进行**探索**以找到最大化你的“奖励”的方法（s），这是你的考试成绩、闲暇活动时间、考试前后的焦虑水平等的函数。实际上，探索对基于经验的任何学习都至关重要。否则，我们可能永远不会发现更好的做事方式或根本可行的方式！另一方面，我们不能总是试验新方法。不去利用我们已经学到的东西是愚蠢的！因此，探索和开发之间存在*探索与开发之间的权衡*，这种权衡是RL的核心所在。在MAB中有效平衡这种权衡至关重要。
- en: 'If the exploration-exploitation trade-off is a challenge across all RL problems,
    why do we specifically bring it up in the context of MAB? This is for two main
    reasons:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果探索与开发的权衡在所有RL问题中都是一个挑战，那么为什么我们特别在MAB的背景下提出它？这主要有两个原因：
- en: MAB is one-step RL. Therefore, it allows us to study various exploration strategies
    in isolation from the complexities of multi-step RL, and potentially prove how
    good they are theoretically.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAB是单步RL。因此，它允许我们从多步RL的复杂性中分离出各种探索策略，并在理论上证明它们的优越性。
- en: 'While in multi-step RL we often train the agent offline (and in a simulation)
    and use its policy online, in MAB problems, the agent is often trained and used
    (almost always) online. Therefore, inefficient exploration costs more than just
    computer time: it actually burns real money through bad actions. Therefore, it
    becomes absolutely crucial to balance exploration and exploitation effectively
    in MAB problems.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多步RL中，我们经常在离线状态下训练代理（并在模拟中），并在线使用其策略，在MAB问题中，代理通常在线训练和使用（几乎总是）。因此，低效的探索成本不仅仅是计算机时间：它实际上通过不良行动花费真钱。因此，在MAB问题中有效平衡探索和开发变得非常关键。
- en: With this in mind, now it is time to define what a MAB problem is, and then
    see an example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，现在是时候定义什么是MAB问题，然后看一个例子。
- en: What is a MAB?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是MAB？
- en: A MAB problem is all about identifying the best action among a set of actions
    available to an agent through trial and error, such as figuring out the best look
    for a website among some alternatives, or the best ad banner to run for a product.
    We will focus on the more common variant of MABs where there are ![](img/Formula_02_001.png)
    discrete actions available to the agent, also known as a ![](img/Formula_02_002.png)**-armed
    bandit problem**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MAB问题的关键是在通过试错确定的一组行动中识别最佳行动，例如在一些选择中找出网站的最佳外观或产品的最佳广告横幅。我们将专注于MAB的更常见变体，其中代理可以选择
    ![](img/Formula_02_001.png) 离散行动，也称为 ![](img/Formula_02_002.png)**-臂老虎机问题**。
- en: Let's define the problem in more detail through the example it got its name
    from.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过得名的示例更详细地定义问题。
- en: Problem definition
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题定义
- en: 'The MAB problem is named after the case of a gambler who needs to choose a
    slot machine (bandit) to play from a row of machines:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MAB问题是以需要选择一台老虎机（强盗）来玩的赌徒为案例命名的情况：
- en: When the lever of a machine is pulled, it gives a random reward coming from
    a probability distribution specific to that machine.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当拉动机器的拉杆时，它会根据该机器特定的概率分布给出一个随机奖励。
- en: Although the machines look identical, their reward probability distributions
    are different.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管这些机器看起来相同，但它们的奖励概率分布是不同的。
- en: The gambler is trying to maximize their total reward. So, in each turn, they
    need to decide whether to play the machine that has given the highest average
    reward so far, or to try another machine. Initially, the gambler has no knowledge
    of the machines' reward distributions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 赌博者的目标是最大化他们的总奖励。因此，在每一轮中，他们需要决定是继续玩目前为止提供最高平均奖励的机器，还是尝试其他机器。最初，赌博者并不知道机器的奖励分布。
- en: Clearly, the gambler needs to find a balance between exploiting the one that
    has been the best so far and exploring the alternatives. Why is that needed? Well,
    because the rewards are stochastic. A machine that won't give the highest average
    reward in the long term may have looked like the best just by chance!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，赌博者需要在利用目前为止表现最好的机器和探索其他选择之间找到平衡。为什么需要这样做呢？因为奖励是随机的。一个机器可能在长期内不会提供最高的平均奖励，但由于某种偶然性，它可能在短期内看起来是最好的！
- en: '![Figure 2.1 – MAB problems involve identifying the best lever to pull among
    multiple options'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – MAB问题涉及从多个选项中识别出最好的拉杆'
- en: '](img/B14160_02_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_02_01.jpg)'
- en: Figure 2.1 – MAB problems involve identifying the best lever to pull among multiple
    options
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – MAB问题涉及从多个选项中识别出最好的拉杆
- en: 'So, to summarize what a MAB problem looks like, we can state the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，总结一下MAB问题的特点，我们可以得出以下结论：
- en: The agent takes sequential actions. After each action, a reward is received.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理执行顺序动作。在每次动作之后，会获得奖励。
- en: An action affects only the immediate reward, not the subsequent rewards.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个动作只会影响即时奖励，而不会影响后续奖励。
- en: There is no "state" in the system that changes with the actions that the agent
    takes.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统中没有“状态”，也就是说，代理采取的动作不会改变任何状态。
- en: There is no input that the agent uses to base its decisions on. That will come
    later in the next chapter when we discuss contextual bandits.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理没有任何用于决策的输入。这个问题将在下一章中讨论上下文强盗问题时涉及。
- en: So far, so good! Let's better understand this by actually coding an example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利！让我们通过实际编码一个例子来更好地理解这个问题。
- en: Experimenting with a simple MAB problem
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验一个简单的MAB问题
- en: In this section, you will experience through an example how tricky it could
    be to solve even a simple MAB problem. We will create some virtual slot machines
    and try to maximize the total reward by identifying the luckiest machine. This
    code is available in `Chapter02/Multi-armed bandits.ipynb` on the GitHub repo.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将通过一个例子体验即使是一个简单的MAB问题也可能非常棘手。我们将创建一些虚拟的老虎机，并通过识别最幸运的机器来最大化总奖励。此代码可以在GitHub代码库的`Chapter02/Multi-armed
    bandits.ipynb`中找到。
- en: Setting up the virtual environment
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置虚拟环境
- en: 'Before we start, we suggest you create a virtual environment for the exercise
    using `virtualenv` or using Conda commands. In a folder that you would like to
    place the virtual environment files in, execute the following commands in your
    terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们建议你使用`virtualenv`或Conda命令为练习创建一个虚拟环境。在你希望放置虚拟环境文件的文件夹中，在终端执行以下命令：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will open a browser tab with a Jupyter notebook. Find the `.ipynb` file
    you get from the repo, open it, and set your kernel to be the `rlenv` environment
    we just created.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个浏览器标签，加载一个Jupyter笔记本。找到从代码库中获得的`.ipynb`文件，打开它，并将内核设置为我们刚刚创建的`rlenv`环境。
- en: The bandit exercise
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强盗练习
- en: 'Let''s get started with the exercise:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始练习吧：
- en: 'First, let''s create a class for a single slot machine that gives a reward
    from a normal (Gaussian) distribution with respect to a given mean and standard
    deviation:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们为一个单一的老虎机创建一个类，该老虎机根据给定的均值和标准差从正态（高斯）分布中获取奖励：
- en: '[PRE1]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we create a class that will simulate the game:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个类来模拟游戏：
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A game instance receives a list of slot machines as inputs. It then shuffles
    the order of the slot machines so that you won't recognize which machine gives
    the highest average reward. In each step, you will choose a machine and aim to
    get the highest reward.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个游戏实例接收一组老虎机作为输入。它会打乱这些老虎机的顺序，以便你无法识别哪个机器提供最高的平均奖励。在每一步，你需要选择一个机器并尽量获得最高的奖励。
- en: 'Then, we create some slot machines and a game instance:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一些老虎机和一个游戏实例：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, start playing the game by calling the `user_play()` method of the game
    object:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过调用游戏对象的`user_play()`方法开始游戏：
- en: '[PRE4]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will look like the following:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE5]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you enter your choice, you will observe the reward you got in that round.
    We don''t know anything about the machines, so let''s start with 1:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你输入选择时，你将看到该回合获得的奖励。我们对机器一无所知，所以从1开始吧：
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s play the same machine for a couple more rounds:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再玩几个回合同样的机器：
- en: '[PRE7]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Snap! This in fact looks like the worst machine! It is very unlikely for `slotA`
    or `slotB` machines to give a reward of `-2.8`.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 咔嚓！这看起来确实是最差的机器！`slotA`或`slotB`机器给出`-2.8`奖励的可能性非常小。
- en: Let's check what we have as the first machine in the game (remember that the
    first machine would correspond to index 0 in the `bandits` list) by looking at
    its mean value parameter. Executing `game.bandits[0].mean` gives us `1` as the
    output!
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下游戏中的第一台机器（记住第一台机器对应`bandits`列表中的索引0），通过查看它的均值参数。执行`game.bandits[0].mean`时，我们得到的输出是`1`！
- en: Indeed, we thought we had chosen the best machine although it was the worst!
    Why did that happen though? Well, again, the rewards are stochastic. Depending
    on the variance of the reward distribution, a particular reward could be wildly
    different from the average reward we could expect from that machine. For this
    reason, it is not quite possible to know which lever to pull before we experience
    enough rounds of the game. In fact, with only a few samples, our observations
    could be quite misleading as just happened. In addition, if you play the game
    yourself, you will realize that it is quite difficult to differentiate between
    `slotA` and `slotB`, because their reward distributions are similar. You might
    be thinking, "is this a big deal?". Well, it kind of is, if the difference corresponds
    to significant money and resources, as is the case in many real-world applications.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们以为我们选择了最好的机器，尽管它实际上是最差的！为什么会发生这种情况呢？嗯，原因在于奖励是随机的。根据奖励分布的方差，某个特定奖励可能与我们期望的该机器的平均奖励相差甚远。正因为如此，在我们经历足够的游戏轮数之前，很难知道该拉哪个拉杆。事实上，只有少量的样本，我们的观察结果可能会非常具有误导性，就像刚刚发生的那样。此外，如果你自己玩这个游戏，你会发现很难区分`slotA`和`slotB`，因为它们的奖励分布相似。你可能会想，“这有这么重要吗？”嗯，如果差异对应着显著的金钱和资源，像许多现实世界的应用一样，那是非常重要的。
- en: Next, we will introduce such an application, online advertising, which is going
    to be our running example throughout this chapter and the next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一种应用——在线广告，这是我们在本章及下一章中的示例。
- en: Case study – online advertising
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究——在线广告
- en: Consider a company that wants to advertise a product on various websites through
    digital banners, aiming to attract visitors to the product landing page. Among
    multiple alternatives, the advertiser company wants to find out which banner is
    the most effective and has the maximum **click-through rate** (**CTR**), which
    is defined as the total number of clicks an ad receives divided by the total number
    of impressions (number of times it is shown).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一家公司希望通过数字横幅广告在各大网站上推广产品，目的是吸引访客进入产品的登陆页面。在多个广告选择中，广告主希望找出最有效的横幅，并且拥有最高的**点击率**（**CTR**），点击率定义为广告获得的总点击数除以广告展示的总次数（即广告的曝光次数）。
- en: Every time a banner is about to be shown on a website, it is the advertiser's
    algorithm that chooses the banner (for example, through an API provided by the
    advertiser to the website) and observes whether the impression has resulted in
    a click or not. This is a great use case for a MAB model, which could boost clicks
    and product sales. What we want the MAB model to do is to identify the ad that
    performs the best as early as possible, display it more, and write off the ad(s)
    that is (are) a clear loser(s) as early as possible.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每次横幅广告将在网站上展示时，都是广告主的算法选择横幅（例如，通过广告主提供的API接口）并观察该展示是否导致了点击。这是一个很好的多臂老虎机（MAB）模型应用场景，可以提高点击率和产品销量。我们希望MAB模型尽早识别出表现最好的广告，更多地展示它，并尽早淘汰明显失败的广告。
- en: Tip
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The probability of observing a click or no click after an impression, a binary
    outcome, can be modeled using the Bernoulli distribution. It has a single parameter,
    ![](img/Formula_02_003.png), which is the probability of receiving a click, or
    more generally, observing a 1 as opposed to a 0\. Note that this is a discrete
    probability distribution, whereas the normal distribution we used earlier is a
    continuous one.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示后观察点击与未点击的概率（二元结果），可以使用伯努利分布来建模。它有一个参数，![](img/Formula_02_003.png)，即接收到点击的概率，或者更一般地，观察到1而不是0的概率。注意，这是一个离散概率分布，而我们之前使用的正态分布是一个连续的分布。
- en: 'In the previous example, we had rewards coming from a normal distribution.
    In the online ad case, we have a binary outcome. For each ad version, there is
    a different probability of click (CTR), which the advertiser does not know but
    is trying to discover. So, the rewards will come from different Bernoulli distributions
    for each ad. Let''s code these to use with our algorithms later:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们的奖励来自正态分布。在在线广告的情况中，奖励是二元的。对于每个广告版本，有不同的点击概率（CTR），广告商不知道这个概率，但试图去发现它。所以，奖励将来自每个广告的不同伯努利分布。我们来编写代码，以便稍后与我们的算法一起使用：
- en: 'We start by creating a class to model the ad behavior:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个类来建模广告行为：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s create five different ads (banners) with the corresponding CTRs
    we arbitrarily pick:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们来创建五个不同的广告（横幅），并随便选择相应的CTR：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So far, so good. Now, it is time to implement some exploration strategies to
    maximize the CTR of the ad campaign!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。现在，是时候实施一些探索策略，以最大化广告活动的CTR了！
- en: A/B/n testing
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A/B/n 测试
- en: One of the most common exploration strategies is what is called **A/B testing**,
    which is a method to determine which one of the two alternatives (of online products,
    pages, ads, and so on) performs better. In this type of testing, the users are
    randomly split into two groups to try different alternatives. At the end of the
    testing period, the results are compared to choose the best alternative, which
    is then used in production for the rest of the problem horizon. In our case, we
    have more than two ad versions. So, we will implement what is called **A/B/n testing**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的探索策略之一是所谓的**A/B 测试**，这是一种确定两个备选方案（如在线产品、页面、广告等）中哪个表现更好的方法。在这种测试中，用户会被随机分成两组，尝试不同的备选方案。在测试期结束时，比较结果以选择最佳方案，然后在剩余的时间内用于生产。在我们的例子中，我们有多个广告版本。因此，我们将实施所谓的**A/B/n
    测试**。
- en: We will use A/B/n testing as our baseline strategy for comparison with the more
    advanced methods that we will introduce afterward. Before going into the implementation,
    we need to define some notation that we will use throughout the chapter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用A/B/n 测试作为基准策略，便于与之后介绍的更先进方法进行比较。在进入实现之前，我们需要定义一些符号，这些符号将在本章中使用。
- en: Notation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号
- en: 'Throughout the implementations of various algorithms, we will need to keep
    track of some quantities related to a particular action (the ad chosen for display), ![](img/Formula_02_004.png).
    Now, we define some notation for those quantities. Initially, we drop ![](img/Formula_02_0041.png)
    from our notation for brevity, but at the end of this section, we will put it
    back:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种算法的实现过程中，我们需要跟踪与特定行为（选择展示的广告）相关的一些量，![](img/Formula_02_004.png)。现在，我们为这些量定义一些符号。最初，为了简洁起见，我们省略了![](img/Formula_02_0041.png)，但在本节末，我们会重新添加：
- en: First, we denote the reward (that is, 1 for a click, 0 for no click) received
    after selecting the action, ![](img/Formula_02_0042.png), for the ![](img/Formula_02_007.png)
    time by ![](img/Formula_02_008.png).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们表示在选择动作后收到的奖励（即，点击为1，未点击为0），![](img/Formula_02_0042.png)，为第![](img/Formula_02_007.png)次由![](img/Formula_02_008.png)获得。
- en: 'The average reward observed before the ![](img/Formula_02_009.png) selection
    of this same action is defined as follows:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择此相同行为之前观察到的平均奖励定义如下：
- en: '![](img/Formula_02_010.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_010.jpg)'
- en: This estimates the expected value of the reward that this action yields, ![](img/Formula_02_011.png),
    after ![](img/Formula_02_012.png) observations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这估计了该行为产生的奖励的期望值！[](img/Formula_02_011.png)，在进行![](img/Formula_02_012.png)次观察后。
- en: This is also called the **action value** of ![](img/Formula_02_013.png). Here,
    this is ![](img/Formula_02_014.png) estimates of the action value after selecting
    this action ![](img/Formula_02_0121.png) times.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这也被称为**行为值**！[](img/Formula_02_013.png)。在这里，这就是在选择此行为![](img/Formula_02_0121.png)次后，![](img/Formula_02_014.png)的行为值估计。
- en: Now, we need a bit of simple algebra and we will have a very convenient formula
    to update the action values:![](img/Formula_02_016.jpg)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们需要一些简单的代数运算，就能得到一个非常方便的公式来更新动作值：![](img/Formula_02_016.jpg)
- en: '![](img/Formula_02_017.jpg)![](img/Formula_02_018.jpg)![](img/Formula_02_019.jpg)![](img/Formula_02_020.jpg)![](img/Formula_02_021.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_017.jpg)![](img/Formula_02_018.jpg)![](img/Formula_02_019.jpg)![](img/Formula_02_020.jpg)![](img/Formula_02_021.jpg)'
- en: Remember that ![](img/Formula_02_014.png) is our estimate for the action value
    of ![](img/Formula_02_0131.png) before we take it for the ![](img/Formula_02_0091.png)
    time. When we observe the reward, ![](img/Formula_02_025.png), it gives us another
    signal for the action value. We don't want to discard our previous observations,
    but we also want to update our estimate to reflect the new signal.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，![](img/Formula_02_014.png) 是我们在执行 ![](img/Formula_02_0091.png) 次动作之前对 ![](img/Formula_02_0131.png)
    的动作值的估计。当我们观察到奖励 ![](img/Formula_02_025.png) 时，它为我们提供了一个新的动作值信号。我们不想丢弃之前的观察结果，但我们也希望更新我们的估计以反映这个新信号。
- en: So, we adjust our current estimate, ![](img/Formula_02_0141.png), in the direction
    of the **error** that we calculate based on the latest observed reward, ![](img/Formula_02_027.png),
    with a **step size** of ![](img/Formula_02_028.png) and obtain a new estimate,
    ![](img/Formula_02_029.png). This means, for example, if the latest observed reward
    is greater than our current estimate, we revise the action value estimate upward.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们调整当前估计值 ![](img/Formula_02_0141.png)，使其朝着我们基于最新观察到的奖励 ![](img/Formula_02_027.png)
    计算出的 **误差** 的方向进行调整，步长为 ![](img/Formula_02_028.png)，并获得新的估计值 ![](img/Formula_02_029.png)。这意味着，例如，如果最新观察到的奖励大于我们当前的估计值，我们会将动作值估计向上修正。
- en: For convenience, we define ![](img/Formula_02_030.png).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了方便起见，我们定义 ![](img/Formula_02_030.png)。
- en: Notice that the rate at which we adjust our estimate will get smaller as we
    make more observations due to the ![](img/Formula_02_0281.png) term. So, we put
    less weight on the most recent observations and our estimate for the action value
    for a particular action will settle down over time.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，随着我们进行更多观察，我们调整估计的速率会变得更小，因为有了 ![](img/Formula_02_0281.png) 项。所以，我们会对最新的观察结果赋予较小的权重，并且某个特定动作的动作值估计会随着时间的推移而趋于稳定。
- en: However, this might be a disadvantage if the environment is not stationary but
    is changing over time. In those cases, we would want to use a step size that does
    not diminish over time, such as a fixed step size of ![](img/Formula_02_032.png).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，如果环境不是静态的，而是随时间变化，这可能是一个缺点。在这些情况下，我们希望使用一个不会随时间衰减的步长，例如固定步长 ![](img/Formula_02_032.png)。
- en: Note that this step size must be smaller than 1 for the estimate to converge
    (and larger than 0 for a proper update).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，为了使估计收敛，这个步长必须小于 1（并且大于 0 以确保适当更新）。
- en: Using a fixed value for ![](img/Formula_02_033.png) will make the weights of
    the older observations decrease exponentially as we take action ![](img/Formula_02_0331.png)
    more and more.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定值 ![](img/Formula_02_033.png) 会使得随着我们越来越多地执行动作 ![](img/Formula_02_0331.png)，较早的观察结果的权重呈指数衰减。
- en: 'Let''s bring ![](img/Formula_02_033.png) back to the notation, so we can obtain
    our formula to update the action values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把 ![](img/Formula_02_033.png) 带回符号中，这样我们就能得到更新动作值的公式：
- en: '![](img/Formula_02_036.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_036.jpg)'
- en: Here, ![](img/Formula_02_0332.png) is a number between 0 and 1\. For stationary
    problems, we usually set  ![](img/Formula_02_038.png), where ![](img/Formula_02_039.png)
    is the number of times the action ![](img/Formula_02_0132.png) has been taken
    up to that point (which was denoted by ![](img/Formula_02_041.png) initially).
    In stationary problems, this will help action values converge quicker, due to
    the diminishing ![](img/Formula_02_042.png) term, rather than chasing after noisy
    observations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_02_0332.png) 是一个介于 0 和 1 之间的数。对于静态问题，我们通常设定 ![](img/Formula_02_038.png)，其中
    ![](img/Formula_02_039.png) 是到目前为止已采取的动作 ![](img/Formula_02_0132.png) 的次数（最初表示为
    ![](img/Formula_02_041.png)）。在静态问题中，由于逐渐减少的 ![](img/Formula_02_042.png) 项，这将有助于动作值更快地收敛，而不是追逐噪声观察值。
- en: That's all we need. Without further ado, let's implement an A/B/n test.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的一切。事不宜迟，让我们实现一个 A/B/n 测试。
- en: Application to the online advertising scenario
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用于在线广告场景
- en: 'In our example, we have five different ad versions, which we randomly show
    to the users with equal probabilities. Let''s implement this in Python:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们有五个不同的广告版本，我们以相等的概率随机展示给用户。让我们在 Python 中实现这一点：
- en: 'We start with creating the variables to keep track of the rewards in the experiment:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从创建变量来跟踪实验中的奖励开始：
- en: '[PRE10]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s run the A/B/n test:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们运行A/B/n 测试：
- en: '[PRE11]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Remember that we randomly select an ad to display during the test and observe
    whether it gets a click. We update the counter, the action value estimate, and
    the average reward observed so far.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，我们在测试期间随机选择一个广告进行展示，并观察是否获得点击。我们更新计数器、行动值估计值以及到目前为止观察到的平均奖励。
- en: 'At the end of the test period, we choose the winner as the ad that has achieved
    the highest action value:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试期结束时，我们选择获得最高行动值的广告作为获胜者：
- en: '[PRE12]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We display the winner using a `print` statement:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`print`语句展示获胜者：
- en: '[PRE13]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The outcome is the following:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE14]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this case, the A/B/n test has identified D as the best performing ad, which
    is not exactly correct. Apparently, the test period was not long enough.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，A/B/n 测试将D识别为表现最佳的广告，但这并不完全正确。显然，测试期不够长。
- en: 'Let''s run the best ad identified in the A/B/n test in production:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在生产中运行A/B/n测试识别出的最佳广告：
- en: '[PRE15]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: At this stage, we don't explore any other actions. So, the incorrect selection
    of the ad D will have its impact throughout the production period. We continue
    to record the average reward observed so far to later visualize the ad campaign
    performance.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此阶段，我们不会再探索其他操作。所以，广告D的错误选择将在整个生产期内产生影响。我们继续记录到目前为止观察到的平均奖励，以便之后可视化广告活动表现。
- en: 'Now, time to visualize the results:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，展示结果的时间到了：
- en: 'Let''s create a `pandas` DataFrame to record the results from the A/B/n test:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个`pandas` DataFrame来记录A/B/n 测试的结果：
- en: '[PRE16]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To display the progress of the average rewards, we use Plotly with Cufflinks:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了显示平均奖励的进展，我们使用Plotly和Cufflinks：
- en: '[PRE17]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in the following output:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 2.2 – A/B/n test rewards'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2 – A/B/n 测试奖励'
- en: '](img/B14160_02_02.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_02_02.jpg)'
- en: Figure 2.2 – A/B/n test rewards
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – A/B/n 测试奖励
- en: You can see from *Figure 2.2* that after the exploration ends, the average reward
    is approaching 2.8%, which is the expected CTR for the ad D. On the other hand,
    due to the exploration during the first 10k impressions, in which we tried several
    bad alternatives, the CTR after 100k impressions ended up being 2.71%. We could
    have achieved a higher CTR if the A/B/n test had identified ad E as the best alternative.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图2.2*中可以看到，在探索结束后，平均奖励接近2.8%，这是广告D的预期CTR。另一方面，由于在前10k展示期间进行了一些探索，我们尝试了几个表现不佳的选择，因此在100k展示后，CTR最终为2.71%。如果A/B/n测试能识别广告E作为最佳选择，CTR可能会更高。
- en: That's it! We have just implemented an A/B/n test. Overall, the test was able
    to identify one of the best ads for us, although not the best. Next, we discuss
    the pros and cons of A/B/n testing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们刚刚实现了一个A/B/n测试。总体而言，这个测试帮助我们识别出了一些表现较好的广告，尽管并非最好的。接下来，我们讨论A/B/n测试的优缺点。
- en: Advantages and disadvantages of A/B/n testing
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B/n 测试的优缺点
- en: 'Now, let''s qualitatively evaluate this method and discuss its shortcomings:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对这种方法进行定性评估，并讨论其缺点：
- en: '**A/B/n testing is inefficient as it does not modify the experiment dynamically
    by learning from the observations**. Instead, it explores in a fixed time budget
    with pre-determined probabilities of trying the alternatives. It fails to benefit
    from the early observations in the test by writing off/promoting an alternative
    even though it is obviously underperforming/outperforming the others.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B/n 测试效率低下，因为它不会通过从观察中学习动态地修改实验**。相反，它在固定的时间预算内探索，并且尝试不同选择的概率是预先设定的。它没有利用测试中的早期观察，甚至在某些选择明显表现不佳/表现出色时，也未能及时淘汰或推广这些选择。'
- en: '**It is unable to correct a decision once it''s made**. If, for some reason,
    the test period identifies an alternative as the best incorrectly (mostly because
    of a not-sufficiently long test duration), this selection remains fixed during
    the production period. So, there is no way to correct the decision for the rest
    of the deployment horizon.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它一旦做出决策就无法纠正**。如果由于某些原因，测试期间错误地将某个选择识别为最佳（通常是因为测试时间不足），则该选择在生产期间会一直保持固定。因此，无法在剩余的部署期内纠正该决策。'
- en: '**It is unable to adapt to changes in a dynamic environment**. Related to the
    previous note, this approach is especially problematic for environments that are
    not stationary. So, if the underlying reward distributions change over time, plain
    A/B/n testing has no way of detecting such changes after the selection is fixed.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它无法适应动态环境中的变化**。与前述问题相关，这种方法在非静态环境中尤其有问题。因此，如果基础的奖励分布随时间发生变化，简单的A/B/n测试就无法在选择固定后检测到这些变化。'
- en: '**The length of the test period is a hyperparameter to tune, affecting the
    efficiency of the test**. If this period is chosen to be shorter than needed,
    an incorrect alternative could be declared the best because of the noise in the
    observations. If the test period is chosen to be too long, too much money gets
    wasted in exploration.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试期长度是一个需要调整的超参数，影响测试的效率**。如果选择的期限比所需期限短，由于观察中的噪声，可能会错误地宣布某个错误的替代品是最佳选择。如果测试期限选择得太长，将在探索中浪费太多资金。'
- en: '**A/B/n testing is simple**. Despite all these shortcomings, it is intuitive
    and easy to implement, and therefore widely used in practice.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B/n测试很简单**。尽管存在这些缺点，但它直观且易于实施，因此在实践中被广泛使用。'
- en: So, the vanilla A/B/n testing is a rather naive approach to MAB. Next, let's
    look into some other more advanced approaches that will overcome some of the shortcomings
    of A/B/n testing, starting with ε-greedy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，普通的A/B/n测试对于MAB而言是一种相当幼稚的方法。接下来，让我们探讨一些其他更高级的方法，这些方法将克服A/B/n测试的一些缺点，首先是ε贪婪。
- en: ε-greedy actions
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ε贪婪动作
- en: An easy-to-implement, effective, and widely used approach to the exploration-exploitation
    problem is what is called **ε-greedy** actions. This approach suggests, most of
    the time, greedily taking the action that is the best according to the rewards
    observed by that point in the experiment (that is, with 1-ε probability); but
    once in a while (that is, with ε probability), take a random action regardless
    of the action performances. Here, ε is a number between 0 and 1, usually closer
    to zero (for example, 0.1) to "exploit" in most decisions. This way, the method
    allows continuous exploration of the alternative actions throughout the experiment.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一种易于实施、有效且广泛使用的探索-利用问题方法被称为**ε贪婪**动作。这种方法建议在大多数决策中贪心地选择到目前为止根据观察到的奖励最好的动作（即以1-ε的概率）；但偶尔（即以ε的概率），无论动作表现如何，都会随机选择一个动作。在这里，ε是一个介于0和1之间的数字，通常接近零（例如，0.1），以便在实验中持续探索替代动作。
- en: Application to the online advertising scenario
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用于在线广告场景
- en: 'Now, let''s implement the ε-greedy actions to the online advertising scenario
    that we have:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将ε贪婪动作应用于我们的在线广告场景：
- en: 'We start with initializing the necessary variables for the experiment, which
    will keep track of the action value estimates, the number of times each ad has
    been displayed, and the moving average for the reward:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始初始化实验所需的变量，用于跟踪动作值估计、每个广告显示次数和奖励的移动平均值：
- en: '[PRE18]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we choose 0.1 for ε, but this is a somewhat arbitrary choice. Different
    ε values will lead to different performances, so this should be treated as a hyperparameter
    to be tuned. A more sophisticated approach would be to start with a high ε value
    and gradually reduce it. We'll talk about this a bit more later.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意我们选择0.1作为ε值，但这是一个相对随意的选择。不同的ε值将导致不同的性能，因此应将其视为需要调整的超参数。一个更复杂的方法是从较高的ε值开始，逐渐减少。稍后我们将详细讨论这一点。
- en: 'Next, we run the experiment. Pay attention to how we select a random action
    with ε probability, and the best action otherwise. We update our action value
    estimates according to the rule we previously described:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们进行实验。注意我们如何以ε的概率选择随机动作，否则选择最佳动作。根据我们之前描述的规则更新我们的动作值估计：
- en: '[PRE19]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Run *steps 1 and 2* for different ε values, namely 0.01, 0.05, 0.1, and 0.2\.
    Then, compare how the ε selection affects the performance, as follows:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对不同的ε值（即0.01、0.05、0.1和0.2），运行*步骤1和2*。然后，比较ε选择如何影响性能，如下：
- en: '[PRE20]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This results in the following output:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 2.3 – Exploration using ε-greedy actions'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3 – 使用ε贪婪动作进行探索'
- en: '](img/B14160_02_03.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_02_03.jpg)'
- en: Figure 2.3 – Exploration using ε-greedy actions
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 使用ε贪婪动作进行探索
- en: The best rewards are given by ε=0.05 and ε=0.1 as 2.97%. It turns out the exploration
    with the other two ε values were either too low or too high. In addition, all
    of the ε-greedy policies gave better results than the A/B/n test, particularly
    because the A/B/n test happened to make an incorrect choice in that specific case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳奖励由ε=0.05和ε=0.1分别为2.97%给出。结果表明，使用其他两个ε值的探索效果要么太低要么太高。此外，所有的ε贪婪策略都比A/B/n测试给出了更好的结果，特别是因为在这种情况下A/B/n测试做出了错误的选择。
- en: Advantages and disadvantages of ε-greedy actions
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ε贪婪动作的优缺点
- en: 'Let''s talk about the pros and cons of using ε-greedy actions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论使用ε贪婪动作的利弊：
- en: '**ε-greedy actions and A/B/n tests are similarly inefficient and static in
    allocating the exploration budget**. The ε-greedy approach, too, fails to write
    off actions that are clearly bad and continues to allocate the same exploration
    budget to each alternative. For example, halfway through the experiment, it is
    pretty clear that ad A is performing pretty poorly. It would have been more efficient
    to use the exploration budget to try to differentiate between the rest of the
    alternatives to identify the best. On a related note, if a particular action is
    under-explored/over-explored at any point, the exploration budget is not adjusted
    accordingly.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ε-贪心策略和A/B/n测试在分配探索预算时同样低效且静态**。ε-贪心策略也未能及时淘汰明显不好的动作，而是继续将相同的探索预算分配给每一个备选项。例如，在实验进行到一半时，显然广告A的表现很差。将探索预算用于尝试区分其他备选项，找出最佳选择，效率会更高。相关的一个问题是，如果某个动作在某一时刻被探索过少或过多，探索预算并不会相应调整。'
- en: '**With ε-greedy actions, exploration is continuous, unlike in A/B/n testing**.
    This means if the environment is not stationary, the ε-greedy approach has the
    potential to pick up the changes and modify its selection of the best alternative.
    In stationary environments, though, we can expect the A/B/n testing and the ε-greedy
    approach to perform similarly since they are very similar in nature, except when
    they do the exploration.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用ε-贪心策略时，探索是持续的，这与A/B/n测试不同**。这意味着，如果环境不是静态的，ε-贪心策略有潜力发现变化，并调整对最佳备选项的选择。然而，在静态环境中，我们可以预期A/B/n测试和ε-贪心策略表现相似，因为它们在本质上非常相似，区别仅在于何时进行探索。'
- en: '**The ε-greedy actions approach could be made more efficient by dynamically
    changing the ε value**. For example, you could start with a high ε value to explore
    more at the beginning and gradually decrease it to exploit more later. This way,
    there is still continuous exploration, but not as much as at the beginning when
    there was no knowledge of the environment.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过动态变化ε值，ε-贪心策略可以提高效率**。例如，你可以从较高的ε值开始，初期进行更多的探索，之后逐渐降低ε值以进行更多的利用。这样，仍然会有持续的探索，但不像最开始那样频繁，因为那时对环境还没有了解。'
- en: '**The ε-greedy actions approach could be made more dynamic by increasing the
    importance of more recent observations**. In the standard version, the ![](img/Formula_02_043.png)
    values in the preceding are calculated as simple averages. Remember that, in dynamic
    environments, we could instead use the following formula:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过提高近期观察结果的重要性，可以使ε-贪心策略更加动态**。在标准版本中，前述的 ![](img/Formula_02_043.png) 值是作为简单平均值来计算的。请记住，在动态环境中，我们可以使用以下公式：'
- en: '![](img/Formula_02_044.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_044.jpg)'
- en: This would exponentially diminish the weights of the older observations and
    enable the approach to detect the changes in the environment more easily.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这将以指数方式减小较早观察结果的权重，从而使得该方法能够更容易地察觉环境的变化。
- en: '**Modifying the ε-greedy actions approach introduces new hyperparameters, which
    need to be tuned**. Both of the previous suggestions – gradually diminishing ε
    and using exponential smoothing for *Q* – come with additional hyperparameters,
    and it may not be obvious what values to set these to. Moreover, incorrect selection
    of these hyperparameters may lead to worse results than what the standard version
    would yield.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改ε-贪心策略引入了新的超参数，这些超参数需要调整**。前面提到的两个建议——逐渐减小ε值和使用指数平滑来处理*Q*——都涉及额外的超参数，且可能不容易直观地知道该设置什么值。而且，选择这些超参数不当可能导致比标准版本更差的结果。'
- en: So far, so good! We have used ε-greedy actions to optimize our online advertising
    campaign and obtained better results than A/B/n testing. We have also discussed
    how we can modify this approach to use in a broader set of environments. However,
    ε-greedy selection of the actions is still too static, and we can do better. Now,
    let's look into another approach, upper confidence bounds, which dynamically adjusts
    the exploration of the actions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利！我们已经使用ε-贪心策略优化了我们的在线广告活动，并取得了比A/B/n测试更好的结果。我们还讨论了如何修改该方法，以便在更广泛的环境中使用。然而，ε-贪心策略的动作选择仍然过于静态，我们可以做得更好。现在，让我们看看另一种方法——上置信区间，它能动态调整动作的探索。
- en: Action selection using upper confidence bounds
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用上置信区间进行动作选择
- en: '**Upper confidence bounds** (**UCB**) is a simple yet effective solution to
    the exploration-exploitation trade-off. The idea is that at each time step, we
    select the action that has the highest potential for reward. The potential of
    the action is calculated as the sum of the action value estimate and a measure
    of the uncertainty of this estimate. This sum is what we call the UCB. So, an
    action is selected either because our estimate for the action value is high, or
    the action has not been explored enough (that is, as many times as the other ones)
    and there is high uncertainty about its value, or both.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**上置信界** (**UCB**) 是一种简单而有效的解决探索与利用权衡问题的方法。其思路是在每一个时间步骤中，我们选择潜在回报最高的动作。动作的潜力通过动作值估计和该估计的不确定性度量之和来计算。这个和就是我们所说的
    UCB。因此，选择一个动作的原因可能是我们对该动作值的估计很高，或者该动作还没有得到足够的探索（即，探索的次数不如其他动作），且对其价值的不确定性很高，或者两者兼有。'
- en: 'More formally, we select the action to take at time ![](img/Formula_02_045.png)
    using the following formula:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，我们使用以下公式在时间 ![](img/Formula_02_045.png) 选择要执行的动作：
- en: '![](img/Formula_02_046.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_046.jpg)'
- en: 'Let''s unpack this a little bit:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微拆解一下：
- en: Now, we have used a notation that is slightly different from what we introduced
    earlier. ![](img/Formula_02_047.png) and ![](img/Formula_02_048.png) have essentially
    the same meanings as before. This formula looks at the variable values, which
    may have been updated a while ago, at the time of decision-making, ![](img/Formula_02_0451.png),
    whereas the earlier formula described how to update them.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们使用了一种与之前介绍的略有不同的符号！[](img/Formula_02_047.png) 和 ![](img/Formula_02_048.png)
    的含义与之前基本相同。这个公式关注的是变量值，这些值可能是在决策时已经更新过的！[](img/Formula_02_0451.png)，而之前的公式描述的是如何更新这些值。
- en: In this equation, the square root term is a measure of the uncertainty for the
    estimate of the action value of ![](img/Formula_02_0332.png).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个方程中，平方根项是一个衡量动作值估计不确定性的指标！[](img/Formula_02_0332.png)。
- en: The more we select ![](img/Formula_02_0332.png), the less uncertainty we have
    about its value, and so is the ![](img/Formula_02_052.png) term in the denominator.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择的次数越多，关于 ![](img/Formula_02_0332.png) 的不确定性就越小，因此分母中的 ![](img/Formula_02_052.png)
    项也会减小。
- en: As time passes, however, the uncertainty grows due to the ![](img/Formula_02_053.png)
    term (which makes sense especially if the environment is not stationary), and
    more exploration is encouraged.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，随着时间的推移，由于 ![](img/Formula_02_053.png) 项（特别是在环境不是静态的情况下，这一点是有道理的），不确定性会增加，从而鼓励更多的探索。
- en: On the other hand, the emphasis on uncertainty during decision making is controlled
    by a hyperparameter, ![](img/Formula_02_054_1.png). This obviously requires tuning,
    and a bad selection could diminish the value in the method.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，决策时对不确定性的重视程度是由超参数 ![](img/Formula_02_054_1.png) 控制的。显然，这需要调优，而不恰当的选择可能会降低该方法的效果。
- en: Now, it is time to see UCB in action.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候看看 UCB 的实际应用了。
- en: Application to the online advertising scenario
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用于在线广告场景
- en: 'Follow along to implement the UCB method to optimize the ad display:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 跟着一起实现 UCB 方法来优化广告展示：
- en: 'As usual, let''s initialize the necessary variables first:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，让我们首先初始化必要的变量：
- en: '[PRE21]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, implement the main loop to use UCB for action selection:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，实现主循环来使用 UCB 进行动作选择：
- en: '[PRE22]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that we select the action in each time step with the highest UCB. If an
    action has not been selected yet, it has the highest UCB. We break the ties randomly
    if there are multiple such actions.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们在每个时间步骤中选择具有最高 UCB 的动作。如果一个动作还没有被选择过，那么它将具有最高的 UCB。如果有多个动作具有相同的 UCB，我们将随机打破平局。
- en: 'As mentioned before, different ![](img/Formula_02_054.png) selections will
    lead to different levels of performance. Run *steps 1 and 2* with different selections
    of the ![](img/Formula_02_0541.png) hyperparameter. Then, compare the results,
    as follows:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，不同的 ![](img/Formula_02_054.png) 选择会导致不同的表现水平。使用不同的 ![](img/Formula_02_0541.png)
    超参数选择运行 *步骤 1 和 2*。然后，比较结果，如下所示：
- en: '[PRE23]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following output:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![Figure 2.4 – Exploration using UCB'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4 – 使用 UCB 进行探索'
- en: '](img/B14160_02_04.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_02_04.jpg)'
- en: Figure 2.4 – Exploration using UCB
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 使用 UCB 进行探索
- en: In this case, using UCB for exploration, after some hyperparameter tuning, gave
    a better result (3.07% CTR) than ε-greedy exploration and A/B/n testing! Of course,
    the elephant in the room is how to do this hyperparameter tuning. Interestingly,
    this is itself a MAB problem! First, you have to form a set of plausible ![](img/Formula_02_0541.png)
    values and choose the best one using one of the methods we described so far.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用 UCB 进行探索，在经过一些超参数调优后，取得了比 ε-greedy 探索和 A/B/n 测试更好的结果（3.07% 点击率）！当然，问题的关键在于如何进行超参数调优。有趣的是，这本身就是一个
    MAB 问题！首先，你需要形成一组合理的 ![](img/Formula_02_0541.png) 值，并使用我们到目前为止描述的某种方法选择最佳值。
- en: Tip
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Try hyperparameters in a logarithmic scale, such as [0.01, 0.1, 1, 10], rather
    than a linear scale, such as [0.08, 0.1, 0.12, 0.14]. The former allows exploring
    different orders of magnitude, where we could see significant jumps in performance.
    A search on a linear scale could be used after identifying the right order of
    magnitude.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用对数尺度的超参数，如 [0.01, 0.1, 1, 10]，而不是线性尺度，如 [0.08, 0.1, 0.12, 0.14]。前者允许探索不同数量级的变化，在这些变化中，我们可能会看到显著的性能跳跃。在确定了正确的数量级后，可以使用线性尺度进行搜索。
- en: To make things less complicated, you can use an A/B/n test to choose ![](img/Formula_02_0541.png).
    This might look like an infinite loop – you form a MAB to solve a MAB, which itself
    may have a hyperparameter to tune and so on. Fortunately, once you identify a
    good ![](img/Formula_02_0542.png) value that works for your problem type (for
    example, online advertising), you can usually use the same value over and over
    again in later experiments as far as the reward scale remains similar (for example,
    around 1–3% CTR for online ads).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化事情，你可以使用 A/B/n 测试来选择 ![](img/Formula_02_0541.png)。这可能看起来像一个无限循环——你用 MAB
    来解决一个 MAB 问题，而 MAB 本身可能还需要调优超参数，依此类推。幸运的是，一旦你为你的问题类型（例如，在线广告）找到了一个合适的 ![](img/Formula_02_0542.png)
    值，通常可以在以后的实验中反复使用该值，只要奖励尺度保持相似（例如，在线广告的点击率大约在 1–3% 之间）。
- en: Advantages and disadvantages of using UCBs
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 UCB 的优缺点
- en: 'Finally, let''s discuss some of the pros and cons of the UCB approach:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论一下 UCB 方法的优缺点：
- en: '**UCB is a set-and-forget approach**. It systematically and dynamically allocates
    the budget to alternatives that need exploration. If there are changes in the
    environment – for example, if the reward structure changes because one of the
    ads gets more popular for some reason – the method will adapt its selection of
    the actions accordingly.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UCB 是一种设置后即忘的方法**。它系统地并动态地将预算分配给需要探索的选项。如果环境发生变化——例如，如果某个广告由于某种原因变得更受欢迎，奖励结构发生变化——该方法会相应地调整其行动选择。'
- en: '**UCB can be further optimized for dynamic environments, potentially at the
    expense of introducing additional hyperparameters**. The formula we provided for
    UCB is a common one, but it can be improved – for example, by using exponential
    smoothing to calculate the ![](img/Formula_02_060.png) values. There are also
    more effective estimations of the uncertainty component in literature. These modifications,
    though, could potentially make the method more complicated.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UCB 可以进一步优化以适应动态环境，可能会引入额外的超参数代价**。我们提供的 UCB 公式是一个常见的公式，但它可以得到改进——例如，通过使用指数平滑来计算
    ![](img/Formula_02_060.png) 值。文献中也有更有效的不确定性成分估算方法。然而，这些修改可能会使方法变得更加复杂。'
- en: '**UCB could be hard to tune**. It is somewhat easier to make the call and say,
    "I want to explore 10% of the time, and exploit for the rest" for the ε-greedy
    approach than saying, "I want my ![](img/Formula_02_0543.png) to be 0.729" for
    the UCB approach, especially if you are trying these methods on a brand-new problem.
    When not tuned, a UCB implementation could give unexpectedly bad results.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UCB 可能难以调优**。相比于 ε-greedy 方法中简单地说“我想 10% 的时间用于探索，其余时间用于开发”，要说“我希望我的 ![](img/Formula_02_0543.png)
    为 0.729”对 UCB 方法来说要复杂一些，尤其是在你尝试这些方法来解决一个全新问题时。如果没有调优，UCB 实现可能会给出出乎意料的差结果。'
- en: There you go! You have now implemented multiple approaches to the online advertising
    problem, and using the UCB approach has particularly equipped you to manage the
    exploration effectively in potentially non-stationary environments. Next, we will
    cover another very powerful approach, Thompson sampling, which will be a great
    addition to your arsenal.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在已经实现了多种解决在线广告问题的方法，并且使用 UCB 方法特别有助于在潜在的非平稳环境中有效管理探索。接下来，我们将介绍另一种非常强大的方法——汤普森抽样，它将是你武器库中的一个重要补充。
- en: Thompson (posterior) sampling
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汤普森（后验）抽样
- en: The goal in MAB problems is to estimate the parameter(s) of the reward distribution
    for each arm (that is, the ad to display, in the preceding example). In addition,
    measuring our uncertainty about our estimate is a good way to guide the exploration
    strategy. This problem very much fits into the Bayesian inference framework, which
    is what Thompson sampling leverages. Bayesian inference starts with a prior probability
    distribution – an initial idea, for the parameter ![](img/Formula_02_062.png)
    – and updates this prior distribution as data becomes available. Here, ![](img/Formula_02_062.png)
    refers to the mean and variance for a normal distribution, and to the probability
    of observing a 1 for Bernoulli distribution. So, the Bayesian approach treats
    the parameter as a random variable given the data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在多臂老虎机（MAB）问题中的目标是估计每个臂（即前面例子中的广告）的奖励分布参数。此外，衡量我们对估计的 uncertainty 是指导探索策略的一个好方法。这个问题非常符合贝叶斯推断框架，而汤普森采样正是利用了这一框架。贝叶斯推断从先验概率分布开始——即参数
    ![](img/Formula_02_062.png) 的初始假设——并随着数据的到来更新这个先验分布。这里，![](img/Formula_02_062.png)
    指的是正态分布的均值和方差，或伯努利分布中观察到 1 的概率。因此，贝叶斯方法将参数视为给定数据后的随机变量。
- en: 'The formula for this is given by the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式的表达式如下所示：
- en: '![](img/Formula_02_064.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_064.jpg)'
- en: In this formula, ![](img/Formula_02_065.png) is the **prior distribution** of
    ![](img/Formula_02_062.png), which represents the current hypothesis on its distribution.
    ![](img/Formula_02_067.png) represents the data, with which we obtain a **posterior
    distribution**, ![](img/Formula_02_068.png). This is our updated hypothesis on
    the distribution of the parameter given the data we observe. ![](img/Formula_02_069.png)
    is called the **likelihood** (of observing the data ![](img/Formula_02_0671.png)
    given the parameter) and ![](img/Formula_02_071.png) is called the **evidence**.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/Formula_02_065.png) 是 **先验分布**，表示当前对 ![](img/Formula_02_062.png)
    分布的假设。![](img/Formula_02_067.png) 代表数据，通过这些数据我们可以得到 **后验分布**，即 ![](img/Formula_02_068.png)。这就是我们基于所观察到的数据对参数分布的更新假设。![](img/Formula_02_069.png)
    被称为 **似然函数**（给定参数下观察到数据 ![](img/Formula_02_0671.png) 的概率），而 ![](img/Formula_02_071.png)
    被称为 **证据**。
- en: Next, let's look into how we can implement Thompson sampling for cases with
    0–1 type of outcome, such as what we have in the online advertising scenario.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨如何在0–1类型的结果中实现汤普森采样，例如在在线广告场景中出现的情况。
- en: Application to the online advertising scenario
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线广告场景的应用
- en: In our example, for a given ad ![](img/Formula_02_072.png), observing a click
    is a Bernoulli random variable with parameter ![](img/Formula_02_073.png), which
    we are trying to estimate. Since ![](img/Formula_02_073.png) is essentially the
    probability that the ad ![](img/Formula_02_075.png) is clicked when displayed,
    equivalently, the CTR is between 0 and 1\. Note that many problems other than
    online advertising have such a binary outcome. Therefore, our discussion and the
    formulas here can be extended to such other cases.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，对于给定的广告 ![](img/Formula_02_072.png)，观察一次点击是一个伯努利随机变量，参数为 ![](img/Formula_02_073.png)，我们试图对其进行估计。由于
    ![](img/Formula_02_073.png) 本质上是广告 ![](img/Formula_02_075.png) 显示时被点击的概率，因此CTR介于0和1之间。需要注意的是，除了在线广告外，许多问题也具有这样的二元结果。因此，我们在这里的讨论和公式可以扩展到其他类似的情况。
- en: Details of Thompson sampling
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汤普森采样的细节
- en: 'For now, let''s see how we can use a Bayesian approach to our problem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将贝叶斯方法应用到我们的问题中：
- en: Initially, we don't have any reason to believe that the parameter is high or
    low for a given ad. Therefore, it makes sense to assume that ![](img/Formula_02_0731.png)
    has a uniform distribution over ![](img/Formula_02_077.png).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初，我们没有理由相信给定广告的参数是高还是低。因此，假设 ![](img/Formula_02_0731.png) 在 ![](img/Formula_02_077.png)
    上服从均匀分布是合理的。
- en: Assume that we display the ad ![](img/Formula_02_0721.png) and it results in
    a click. We take this as a signal to update the probability distribution for ![](img/Formula_02_0731.png)
    so that the expected value shifts a little bit toward 1\.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们展示了广告 ![](img/Formula_02_0721.png)，并且该广告获得了点击。我们将此视为一个信号，更新 ![](img/Formula_02_0731.png)
    的概率分布，使得期望值略微向 1 移动。
- en: 'As we collect more and more data, we should also see the variance estimate
    for the parameter shrink. Well, this is exactly how we want to balance exploration
    and exploitation. We did something similar when we used UCB: we used our estimate
    of a parameter together with the associated uncertainty around the estimate to
    guide the exploration. Thompson sampling does exactly the same, using Bayesian
    inference.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着我们收集越来越多的数据，我们还应该看到参数的方差估计值逐渐缩小。这正是我们希望平衡探索和利用的方式。当我们使用 UCB 时，我们做了类似的事情：我们将参数的估计值与该估计值的相关不确定性结合起来，以指导探索。汤普森采样使用贝叶斯推断，正是这样做的。
- en: This method tells us to take a sample from the posterior distribution of the
    parameter, ![](img/Formula_02_080.png). If the expected value of ![](img/Formula_02_0732.png)
    is high, we are likely to get samples closer to 1\. If the variance is high because
    ad ![](img/Formula_02_0722.png) has not been selected many times by that point,
    our samples will also have high variance, which will lead to exploration. At a
    given time step, we take one sample for each ad and select the greatest sample
    to determine the ad to display.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法告诉我们从参数的后验分布中采样，![](img/Formula_02_080.png)。如果！[](img/Formula_02_0732.png)的期望值较高，我们可能会得到更接近1的样本。如果方差较高，因为该广告！[](img/Formula_02_0722.png)在此时尚未被多次选择，我们的样本也将具有较高的方差，这将导致更多的探索。在给定的时间步长内，我们为每个广告采样一次，并选择最大的样本来确定要显示的广告。
- en: 'In our example, the likelihood (chance of an impression resulting in a click)
    is Bernoulli distribution, to which we will apply the logic we described previously.
    Here is what is really going on in less technical terms:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，似然（广告印象转化为点击的概率）是伯努利分布，我们将应用之前描述的逻辑。以下是用较少的技术术语来描述实际情况：
- en: We want to understand what the CTR is for each ad. We have estimates, but we
    are unsure about them, so we associate a probability distribution with each CTR.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要了解每个广告的CTR。我们有估计值，但我们对它们不确定，因此我们会为每个CTR关联一个概率分布。
- en: We update the probability distributions for CTRs as new data come in.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着新数据的到来，我们会更新CTR的概率分布。
- en: When it is time to select an ad, we make a guess about the CTR for each ad –
    that is, sample ![](img/Formula_02_083.png)s. We then pick the ad for which we
    happened to guess the highest CTR.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要选择广告时，我们会对每个广告的CTR做出猜测——也就是采样！[](img/Formula_02_083.png)。然后我们选择我们猜测CTR最高的广告。
- en: If the probability distribution for the CTR of an ad has a high variance, it
    means we are very uncertain about it. This will cause us to make wild guesses
    about that particular ad and select it more often, until the variance reduces
    – that is, we become more certain about it.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某个广告的CTR概率分布方差较大，意味着我们对此非常不确定。这将导致我们对该广告做出较为冒险的猜测，并更频繁地选择它，直到方差减少——也就是我们变得更加确信它。
- en: 'Now, let''s talk about the update rules for the Bernoulli distribution. It
    is okay if you don''t fully grasp the terms here. The preceding explanations should
    tell you about what is going on:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论伯努利分布的更新规则。如果你没有完全理解这里的术语也没关系，前面的解释应该能告诉你发生了什么：
- en: A common choice to use for the prior is beta distribution. If you think for
    a moment, parameter ![](img/Formula_02_084.png) takes values within ![](img/Formula_02_085.png).
    So, we need to use a probability distribution with the same support to model ![](img/Formula_02_086.png),
    which beta distribution has.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常见的先验选择是贝塔分布。如果你稍微思考一下，参数！[](img/Formula_02_084.png)的取值范围在！[](img/Formula_02_085.png)内。因此，我们需要使用一个具有相同支持度的概率分布来建模！[](img/Formula_02_086.png)，贝塔分布正好符合这一要求。
- en: In addition, if we use beta distribution for the prior and plug it in the Bayes
    formula with a Bernoulli likelihood, the posterior also becomes a beta distribution.
    This way, we can use the posterior as the prior for the next update when we observe
    new data.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，如果我们使用贝塔分布作为先验，并将其代入贝叶斯公式与伯努利似然结合，后验分布也会变成贝塔分布。这样，我们就可以在观察到新数据时，将后验分布作为下一次更新的先验。
- en: 'Having the posterior in the same distribution family with the prior is such
    a convenience that it even has a special name: they are called **conjugate distributions**,
    and the prior is called a **conjugate prior** for the likelihood function. Beta
    distribution is a conjugate prior for the Bernoulli distribution. Depending on
    your choice of modeling the likelihood, it is possible to find a conjugate before
    implementing Thompson sampling.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有相同分布族的后验与先验相比，带来了极大的便利，甚至它有一个特殊的名称：它们被称为**共轭分布**，先验被称为似然函数的**共轭先验**。贝塔分布是伯努利分布的共轭先验。根据你选择的似然建模方式，在实现汤普森抽样之前，可以找到一个共轭先验。
- en: Without further ado, let's implement Thompson sampling for our online advertising
    example.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 不再废话，接下来我们来实现汤普森抽样用于在线广告的例子。
- en: Implementation
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: 'The beta distribution for the prior of the ad ![](img/Formula_02_087.png) is
    given by the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 广告！[](img/Formula_02_087.png)的先验的贝塔分布由以下公式给出：
- en: '![](img/Formula_02_088.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_088.jpg)'
- en: 'Here, ![](img/Formula_02_089.png) and ![](img/Formula_02_090.png) are the parameters
    characterizing the beta distribution, and ![](img/Formula_02_091.png) is the gamma
    function. Don''t let this formula scare you! It is actually pretty easy to implement.
    To initialize the prior, we use ![](img/Formula_02_092.png), which makes ![](img/Formula_02_093.png)
    uniformly distributed over ![](img/Formula_02_094.png). Once we observe a reward,
    ![](img/Formula_02_095.png), after selecting the ad ![](img/Formula_02_096.png),
    we obtain the posterior distribution, as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，！[](img/Formula_02_089.png)和！[](img/Formula_02_090.png)是表征贝塔分布的参数，！[](img/Formula_02_091.png)是伽马函数。别让这个公式吓到你！其实它非常容易实现。为了初始化先验，我们使用！[](img/Formula_02_092.png)，这使得！[](img/Formula_02_093.png)在！[](img/Formula_02_094.png)上均匀分布。一旦我们观察到奖励，！[](img/Formula_02_095.png)，选择了广告！[](img/Formula_02_096.png)，我们就能得到如下的后验分布：
- en: '![](img/Formula_02_097.png)s![](img/Formula_02_098.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02_097.png)s![](img/Formula_02_098.png)'
- en: 'Now, let''s do this in Python:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用Python来实现：
- en: 'First, initialize the variables that we will need:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，初始化我们需要的变量：
- en: '[PRE24]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, initialize the main loop with Bayesian updates:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，初始化主循环并进行贝叶斯更新：
- en: '[PRE25]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We sample ![](img/Formula_02_099.png) for each ![](img/Formula_02_100.png) value
    from their corresponding posteriors and display the ad that corresponds to the
    greatest sampled parameter. Once we observe the reward, we make the posterior
    the prior and update it according to the preceding rule to obtain the new posterior.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从各自的后验分布中为每个！[](img/Formula_02_100.png)值抽取样本，并展示与最大抽样参数对应的广告。一旦我们观察到奖励，我们就将后验作为先验，并按照前述规则更新，以获得新的后验。
- en: 'Then, display the results:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，展示结果：
- en: '[PRE26]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This results in the following output:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '![Figure 2.5 – Exploration using Thompson sampling'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 – 使用汤普森抽样的探索'
- en: '](img/B14160_02_05.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_02_05.jpg)'
- en: Figure 2.5 – Exploration using Thompson sampling
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 使用汤普森抽样的探索
- en: Thompson sampling has given a performance that is similar to ε-greedy and UCB
    approaches, right at 3% CTR.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 汤普森抽样的表现与ε-贪心和UCB方法相似，CTR为3%。
- en: Advantages and disadvantages of Thompson sampling
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汤普森抽样的优缺点
- en: 'Thompson sampling is a very competitive approach with one major advantage over
    the ε-greedy and UCB approaches: *Thompson sampling did not require us to do any
    hyperparameter tuning*. This, in practice, has the following benefits:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 汤普森抽样是一个非常有竞争力的方法，其相较于ε-贪心和UCB方法有一个主要的优势：*汤普森抽样不需要我们进行任何超参数调优*。在实践中，这带来了以下好处：
- en: '**Saves significant time** that would have been spent on hyperparameter tuning'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节省大量时间**，这些时间本来会花费在超参数调优上。'
- en: '**Saves significant money** that would have been burned by ineffective exploration
    and incorrect selection of hyperparameters in other approaches.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节省大量资金**，这些资金本来会在其他方法中被浪费在低效的探索和超参数选择错误上。'
- en: In addition, Thompson sampling is shown to be a very competitive choice in many
    benchmarks in literature, and it has gotten increasingly popular over the last
    few years.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文献中展示了汤普森抽样在许多基准测试中是一个非常有竞争力的选择，并且在过去几年中越来越受欢迎。
- en: Awesome job! Now that Thompson sampling is in your toolkit, along with the other
    methods, you are set to go out and solve real-world MAB problems!
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在汤普森抽样已经在你的工具箱中了，和其他方法一起，你已经准备好解决现实世界中的MAB问题！
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered MAB problems, which is one-step RL with many practical
    business applications. Despite its apparent simplicity, it is tricky to balance
    the exploration and exploitation in MAB problems, and any improvements in managing
    this trade-off come with savings in cost and increases in revenue. We have introduced
    four approaches to this end: A/B/n testing, ε-greedy actions, action selection
    using UCB, and Thompson sampling. We implemented these approaches in an online
    advertising scenario and discussed their advantages and disadvantages.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了多臂赌博机（MAB）问题，这是一种一阶强化学习（RL）方法，具有许多实际的商业应用。尽管表面上看似简单，但在MAB问题中平衡探索与利用的难度很大，任何在管理这一权衡方面的改进，都能带来成本节省和收入增加。我们介绍了四种方法来解决这一问题：A/B/n测试、ε-贪婪策略、基于UCB的动作选择和汤普森抽样。我们在在线广告场景中实现了这些方法，并讨论了它们的优缺点。
- en: So far, while making decisions, we have not considered any information about
    the situation in the environment. For example, we have not used any information
    about the users (for example, location, age, previous behavior, and so on) in
    the online advertising scenario that could be available to our decision-making
    algorithm. In the next chapter, you will learn about a more advanced form of MABs,
    namely contextual bandits, which can use to come up with better decisions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在做决策时，我们并没有考虑到环境中的任何情境信息。例如，在在线广告场景中，我们没有使用任何关于用户的信息（如位置、年龄、历史行为等），这些信息可能对我们的决策算法有所帮助。在下一章中，您将学习一种更高级的多臂赌博机形式——上下文强盗问题，它可以利用这些信息来做出更好的决策。
- en: References
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chapelle, O., & Li, L. (2011). An Empirical Evaluation of Thompson Sampling.
    *Advances in Neural Information Processing Systems 24*, (pp. 2249-2257)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapelle, O., & Li, L. (2011). 汤普森抽样的实证评估. *神经信息处理系统进展 24*，(第2249-2257页)
- en: 'Marmerola, G. D. (2017, November 28). *Thompson Sampling for Contextual bandits*.
    Retrieved from Guilherme''s blog: [https://gdmarmerola.github.io/ts-for-contextual-bandits/](https://gdmarmerola.github.io/ts-for-contextual-bandits/)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marmerola, G. D. (2017年11月28日). *汤普森抽样在上下文强盗问题中的应用*. 取自Guilherme的博客：[https://gdmarmerola.github.io/ts-for-contextual-bandits/](https://gdmarmerola.github.io/ts-for-contextual-bandits/)
- en: Russo, D., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018). *A Tutorial
    on Thompson Sampling. Foundations and Trends in Machine Learning*, (pp. 1-96)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russo, D., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018). *汤普森抽样教程.
    机器学习基础与趋势*，(第1-96页)
