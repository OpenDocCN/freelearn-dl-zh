- en: Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: In this chapter, you will learn about artificial neural networks, which forms
    the foundation for all deep learning. We will discuss what makes deep learning
    different from other forms of machine learning and then spend time diving into
    some of its specific and special features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习关于人工神经网络的知识，它是所有深度学习的基础。我们将讨论是什么使深度学习与其他形式的机器学习不同，然后花时间深入探讨一些其特定和特殊的特点。
- en: By the end of this chapter, we will have learned what makes deep learning a
    special subset of machine learning. We'll have an understanding of neural networks,
    how they mimic the brain, and the benefits of hidden layers for discrete element
    detection. We'll create a feedforward neural network, noting the role of the activation
    function in determining variable weights.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将了解是什么使深度学习成为机器学习的一个特殊子集。我们将理解神经网络，它们如何模仿大脑，以及隐藏层在离散元素检测中的优势。我们将创建一个前馈神经网络，并注意激活函数在确定变量权重中的作用。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Contrasting deep learning with machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度学习与机器学习进行对比
- en: Comparing neural networks and the human brain
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较神经网络和人脑
- en: Understanding the role of hidden layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解隐藏层的作用
- en: Creating a feedforward network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建前馈网络
- en: Augmenting our neural network with backpropagation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播增强我们的神经网络
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the source code of this chapter, please refer to the GitHub link at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码请参考GitHub链接：[https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)。
- en: Contrasting deep learning with machine learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将深度学习与机器学习进行对比
- en: One key strength of deep learning not shared by other forms of ML is its ability
    to factor the way variables are related. For instance, if we think back to when
    we were first learning about animals, then we could imagine a simple task where
    we are given five images of cats and five images of dogs; later, when we were
    shown a new image, we would be able to determine whether it was a cat or dog using
    the patterns that we detected from the previous images that we studied. In our
    example, it was the images that were to be classified as either cats or dogs.
    We can consider this example as a training set, and will use the same terminology
    for the classification of images. Mentally, our brain tries to match the images
    with the patterns that form the features of these two different species so that
    we can differentiate between them. This is what happens in deep learning as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个关键优势是其他机器学习方法所没有的，它能够考虑变量之间的关系。例如，如果我们回想起当初学习动物时，可以想象一个简单的任务，我们被给出了五张猫的照片和五张狗的照片；之后，当我们看到一张新图片时，我们能够通过之前学习到的模式来判断这是一只猫还是狗。在我们的例子中，图片是要被分类为猫或狗的对象。我们可以把这个例子看作是一个训练集，并且在分类图片时使用相同的术语。从心理上讲，我们的大脑试图将这些图片与形成这两种不同物种特征的模式匹配，从而帮助我们区分它们。这在深度学习中也是如此。
- en: 'Today, we would find the preceding example task to be quite simple; however,
    think of how a computer would have to learn this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们会发现前面的任务相当简单；然而，想象一下计算机必须如何学习这个任务：
- en: It needs to account for the ways that features are related for dogs compared
    with cats and it needs to do this wherever an animal appears in a photo, however
    much of the image is taken up by the animal.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要考虑狗和猫的特征之间的关系，并且无论动物在照片中的占比多少，都会对每一张照片进行这种处理。
- en: For this, we cannot use the standard machine-learning approach where all input
    is used without consideration for how it is related. The placement and proximity
    of pixels in two-dimensional space must be considered, so we can already see how
    a simple task for humans is already more complex for a machine.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对此，我们不能使用标准的机器学习方法，其中所有输入都被直接使用，而不考虑它们之间的关系。必须考虑像素在二维空间中的位置和相对距离，因此我们已经可以看到，对人类来说是一个简单的任务，机器却变得更加复杂。
- en: Furthermore, just evaluating the input data in the form of two-dimensional arrays
    is not enough. The machine also needs the multiple hidden layer architecture present
    in deep learning to identify multiple different patterns among the data arrays.
    That is to say, the pure signal won't be as helpful as the relationship between
    given signals that are near each other.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，仅以二维数组的形式评估输入数据是不够的。机器还需要深度学习中存在的多个隐藏层架构，以识别数据数组中的多种不同模式。换句话说，单纯的信号并不像相互接近的信号之间的关系那样有用。
- en: Each layer will identify the presence of a different aspect of the image from
    basic shape detection to the length and steepness of color gradients.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层都会识别图像中不同方面的存在，从基本的形状检测到颜色渐变的长度和陡峭度。
- en: When performing a simple regression or classification task, we apply a weight
    to each variable and then use this to predict an outcome. With deep learning,
    there is a middle step where artificial neurons or units are created using all
    the variables. This step creates new features that are a combination of all variables
    with varying weights applied. This activity happens in what is known as a hidden
    layer. After this, the signal is passed on to another hidden layer and the same
    process happens again.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行简单的回归或分类任务时，我们会为每个变量应用一个权重，然后用这个权重预测结果。对于深度学习，存在一个中间步骤，其中人工神经元或单元是基于所有变量创建的。这个步骤创建了新的特征，这些特征是所有变量的组合，并应用了不同的权重。这一过程发生在所谓的隐藏层中。之后，信号会传递到另一个隐藏层，重复相同的过程。
- en: 'Each layer will learn a different aspect from the input. Let''s look at an
    example of this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都会学习输入的不同方面。让我们来看一个例子：
- en: The first layer creates neurons based on the overall size of the object in the
    image by applying different weights to the negative and positive space.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层通过对负空间和正空间应用不同的权重，根据图像中物体的整体大小来创建神经元。
- en: In the second layer, the neurons may be created based on the shape and size
    of the ears and nose.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二层，神经元可能会根据耳朵和鼻子的形状和大小来创建。
- en: In this way, the different characteristics of cats and dogs are captured among
    the entirety of the hidden layers.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这种方式，猫和狗的不同特征会被捕捉到所有隐藏层中。
- en: The weights are assigned randomly at first. This is then checked against the
    actual answers. After several attempts, the model learns that adjusting weights
    in a given direction produces better results, and so it will continue to adjust
    the weights in the same direction until the chosen error rate is minimized.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，权重是随机分配的。然后会与实际答案进行比较。经过多次尝试，模型学会了通过调整权重的方向来获得更好的结果，因此它会继续沿着相同的方向调整权重，直到选定的误差率最小化。
- en: Once this is complete and the weights are learned, a new input can be introduced.
    The model will multiply all variables by the learned weights at every neuron and
    each neuron will use an activation function that determines whether to activate
    or fire and send the signal forward to the next layer. We will go into more detail
    on the various activation functions later in the chapter. For now, let's just
    say that a calculation occurs at each neuron, after which a final value is produced
    at the output node. In this case, the probability is that the image is either
    a dog or a cat.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成并且权重被学习到，可以引入新的输入。模型将把所有变量与每个神经元的学习权重相乘，每个神经元将使用一个激活函数来决定是否激活或发火，并将信号传递到下一层。我们将在本章稍后详细讨论各种激活函数。目前，我们可以简单地说，在每个神经元上都会进行计算，最终在输出节点产生一个值。在这种情况下，概率是图像是狗还是猫。
- en: In this illustration, we may begin to see the power of neural networks and deep
    learning. The model is not evaluating the variables in isolation but rather in
    concert. By contrast, regression models calculate weights for each individual
    variable separately. Regression models can use interaction terms to calculate
    weights for combinations of variables; however, even this doesn't consider all
    variables in the same way as neural networks that evaluate all variables at all
    neurons. The neurons created from all the variables are then used to define the
    next set of neurons in the next layer. In this way, the entirety of the feature
    space is considered and is then partitioned based on themes that emerge after
    evaluating all variables.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，我们可以开始看到神经网络和深度学习的强大力量。模型不是孤立地评估每个变量，而是将它们一起考虑。相比之下，回归模型为每个单独的变量计算权重。回归模型可以使用交互项来计算变量组合的权重；然而，即便如此，它也没有像神经网络那样，在所有神经元中评估所有变量。由所有变量创建的神经元随后用于定义下一层中神经元的集合。通过这种方式，整个特征空间都会被考虑，并根据评估所有变量后出现的主题进行划分。
- en: Comparing neural networks and the human brain
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较神经网络与人脑
- en: Let's consider how a human brain learns in order to see the ways in which a
    neural network is similar and the ways in which it is different.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下人类大脑是如何学习的，从而了解神经网络在某些方面的相似性和不同之处。
- en: Our brain contains a large number of neurons, and each neuron is connected to
    thousands of nearby neurons. As these neurons receive signals, they fire if the
    input contains a certain amount of a given color or a certain amount of a given
    texture. After millions of these interconnected neurons fire, the brain interprets
    the incoming signal as a certain class.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑包含大量的神经元，每个神经元与成千上万个附近的神经元相连接。当这些神经元接收到信号时，如果输入包含一定量的某种颜色或某种纹理，它们会被激活。经过数百万个这样的互联神经元激活后，大脑会将传入的信号解读为某一类。
- en: Of course, these connections are not set permanently but rather change dynamically
    as we continue to have experiences, notice patterns, and discover relationships.
    If we try a new fruit for the first time and discover that it is really sour,
    then all the attributes that help us recognize this fruit are connected with things
    that we know are sour. In the future, whether we will want to experience eating
    this fruit again or not will depend on how much we want to experience this sour
    taste.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些连接不是永久设定的，而是随着我们不断获得经验、注意到模式并发现关系而动态变化的。如果我们第一次尝试一种新水果，发现它非常酸，那么所有帮助我们识别这种水果的特征就会与我们已知的酸味联系在一起。未来，是否愿意再次体验这种水果的味道将取决于我们是否愿意再体验这种酸味。
- en: Another example that shows how the neural network in our brain constantly evolves
    focuses on the types of activities that we find enjoyable. For instance, have
    you ever wondered why babies find shaking a simple toy enjoyable while we do not?
    In our brains, novelty is rewarded by the release of opioids; however, as a given
    piece of stimulus becomes less surprising, a smaller number of neurons are needed
    to interpret this experience, resulting in a less intense response as fewer neurons
    are firing. In this way, we see the dynamic nature of the neural connections in
    our brains, which are in constant flux.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个展示我们大脑中的神经网络如何不断演变的例子聚焦于我们发现愉快的活动类型。例如，你有没有想过，为什么婴儿觉得摇动一个简单的玩具很有趣，而我们却不这么觉得？在我们的脑中，新奇会通过释放类鸦片物质来获得奖励；然而，随着某一刺激变得不再那么令人惊讶，解读这种体验所需的神经元数量会减少，导致反应不那么强烈，因为更少的神经元在激活。通过这种方式，我们看到了大脑中神经连接的动态性质，它们始终处于变化之中。
- en: When we discuss the connections between neurons, we are specifically speaking
    of the synapses between neurons. Artificial neural networks seek to mimic the
    type of learning done by the human brain by creating a massive web of connections
    between constructed neurons and an output node or nodes, in a crude approximation
    of the brain's neurons. Just as the synapses that connect neurons in our brain
    can get stronger or weaker, the weights between neurons can change during the
    training process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论神经元之间的连接时，我们特指的是神经元之间的突触。人工神经网络试图通过创建一个由构造的神经元和一个或多个输出节点之间的巨大连接网络，模拟人脑的学习方式，粗略地近似大脑的神经元。就像大脑中连接神经元的突触可以变强或变弱一样，神经元之间的权重也可以在训练过程中发生变化。
- en: However, unlike the human brain, simple artificial neural networks like those
    that we are studying in this chapter do not start with any inherited weights and
    connections. They begin with a random assignment of weights and connections. Also,
    while the weights can change during the training process, this does not continue
    during the application of the model. At this phase, the weights that were derived
    during the training process are applied and there isn't a continual adjustment.
    This can be contrasted with the human brain. The neural networks in our brains
    do behave similarly to artificial neural networks; however, the adjustments to
    how neurons are connected update constantly. Lastly, the human brain has billions
    of neurons and trillions of connections, while artificial neural networks like
    those we will build shortly have much fewer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与人类大脑不同，我们在本章研究的简单人工神经网络并不以任何遗传的权重和连接开始。它们从权重和连接的随机分配开始。此外，虽然在训练过程中权重会发生变化，但这种变化在模型应用时并不会继续。此时，训练过程中得到的权重会被应用，并且没有持续的调整。这与人类大脑有所不同。我们大脑中的神经网络确实与人工神经网络类似；然而，神经元连接的调整会不断更新。最后，人类大脑拥有数十亿个神经元和万亿级的连接，而我们即将构建的人工神经网络的神经元和连接要少得多。
- en: While there are some significant differences between the way a brain learns
    and the way an artificial neural network learns, by sharing a similar design structure,
    the artificial neural network is capable of solving some extremely complex tasks.
    This idea has continued to develop and improve, and throughout the course of this
    book, we will see just how powerful this idea has been for data science.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大脑学习的方式和人工神经网络学习的方式存在一些显著的差异，通过共享相似的设计结构，人工神经网络能够解决一些极其复杂的任务。这个理念不断发展和改进，在本书的过程中，我们将看到这一理念对数据科学的巨大影响。
- en: Utilizing bias and activation functions within hidden layers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在隐藏层中利用偏置和激活函数
- en: When we described deep learning earlier, we noted that the defining characteristic
    is the presence of hidden layers comprised of neurons that contain the weighted
    sum of all predictor variables in a dataset. We just addressed how this array
    of interconnected neurons is modeled after the human brain. Now let's take a deeper
    dive into what is happening in these hidden layers where neurons are created.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前描述深度学习时，我们提到其定义特征是存在由神经元组成的隐藏层，这些神经元包含数据集中所有预测变量的加权和。我们刚刚讨论了这种互联神经元的结构是如何模仿人类大脑的。现在，让我们更深入地了解这些隐藏层中神经元创建时所发生的事情。
- en: 'At this point, we can deduce the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以推导出以下结论：
- en: We understand that all variables receive a coefficient at random for each neuron
    based on how many units we want to create in each layer.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们理解到，所有变量都会根据我们希望在每一层创建的神经元单元数量，随机为每个神经元分配一个系数。
- en: The algorithm then continues to make changes to these coefficients until it
    minimizes the error rate.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法随后继续调整这些系数，直到最小化误差率。
- en: However, there is one additional coefficient present during this process of
    passing weighted values to the neurons, and that is known as the bias function.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，在这个将加权值传递给神经元的过程中，还有一个附加的系数，这就是所谓的偏置函数。
- en: 'The bias function can simply be thought of as a means of adjusting the shape
    of the line separating the data laterally. For now, let''s simply imagine that
    a straight diagonal line is drawn to separate data points in two-dimensional space.
    In the following example, no matter how we adjust the slope of the line, we cannot
    find a line that bisects the triangles and circles:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置函数可以简单地理解为一种调节数据在横向上分隔的线形的方式。目前，我们可以假设在二维空间中绘制了一条直线，用于将数据点分开。在以下的示例中，不论我们如何调整直线的斜率，我们都无法找到一条能够将三角形和圆形数据点分开的直线：
- en: '![](img/eea521fc-8c58-4542-9f70-9559a02b977c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eea521fc-8c58-4542-9f70-9559a02b977c.png)'
- en: 'However, if we adjust the line slightly so that it intercepts the *y*-axis
    above the center of the plot, then we can fit a line between the two classes of
    points. This is what the bias function does. It adjusts the intercept point to
    allow for a better fitting line:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们稍微调整一下直线，使其与图形的 *y* 轴交点位于图中心的上方，那么我们就可以找到一条在两类数据点之间的最佳拟合线。这就是偏置函数的作用。它通过调整截距点，允许我们找到更合适的拟合线：
- en: '![](img/af4a5ff3-c637-4eaa-a04a-5f431a2b36cf.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af4a5ff3-c637-4eaa-a04a-5f431a2b36cf.png)'
- en: The bias function is a coefficient that adjusts the line along the *x*-axis
    in this way to account for situations where the data requires the line to intersect
    the point where **y** is equal to **0** and **x** perhaps equals **5**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置函数是一个系数，它通过这种方式调整*x*轴上的直线，以应对数据要求直线与**y**等于**0**且**x**可能等于**5**的情况。
- en: All the weighted units and the bias function value are summed within the neuron
    and plotted along a linearly divided space. Calculating where a point is located
    relative to this line determines whether the neuron activates or switches on and
    continues to send a signal forward or if it switches off. Neural networks that
    use this type of function as a threshold to determine what happens with an incoming
    signal are referred to as perceptrons and are the earliest form of artificial
    neural network created.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所有加权单元和偏置函数的值在神经元内被求和，并在一个线性划分的空间中绘制。计算一个点相对于这条直线的位置，决定了神经元是否激活或开关，并继续向前发送信号，或者关闭。如果使用这种类型的函数作为阈值来确定传入信号的处理方式，这种神经网络被称为感知器，是最早的人工神经网络形式。
- en: However, as neural networks have evolved, it has become clearer that using a
    linear model to separate data would not work in all circumstances. As a result,
    there are now a number of available functions that can take place within the neuron
    to determine whether the signal should continue or stop as it passes through.
    These gate functions are referred to as activation functions, as they simulate
    the process of a neuron within the brain being triggered to fire or activate and
    send a signal to a connected neuron or not. At this point, let's explore the variety
    of activation functions available.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着神经网络的发展，越来越明显的是，使用线性模型来分离数据并不适用于所有情况。因此，现在有许多可以在神经元内执行的函数，以决定信号在通过时是否继续传递。这些门控函数被称为激活函数，因为它们模拟了大脑中神经元被触发激活并向连接的神经元发送信号或不发送信号的过程。此时，让我们探索一下可用的各种激活函数。
- en: Surveying activation functions
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查激活函数
- en: The activation functions are the last piece of the neural network that we have
    not covered in depth yet. To review what we know so far, in a neural network,
    we start with an input, as we would with any machine-learning modeling exercise.
    This data consists of a dependent target variable that we would like to predict
    and any number of independent predictor variables that are to be used for this
    prediction task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是我们尚未深入探讨的神经网络的最后一部分。回顾我们到目前为止所知道的，在神经网络中，我们从输入开始，就像我们在任何机器学习建模任务中一样。这些数据包括我们希望预测的目标变量（依赖变量），以及用于此预测任务的任意数量的独立预测变量。
- en: During the training process, the independent variables are weighted and combined
    in simulated neurons. A bias function is also applied during this step and this
    constant value is combined with the weighted independent variable values. At this
    point, an activation function evaluates an aggregation of the values and if it
    is above a set threshold limit, then the neuron fires and the signal is passed
    forward to additional hidden layers, if they exist, or to the output node.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，独立变量在模拟神经元中被加权并组合。此步骤还会应用偏置函数，并将此常数值与加权后的独立变量值结合。在此时，一个激活函数会评估这些值的聚合，如果聚合值超过设定的阈值限制，那么神经元就会被激活，信号将传递给其他隐藏层（如果存在），或者传递到输出节点。
- en: Let's consider the simplest activation function, which is a Heaviside or binary
    step function. This can be imagined visually as two horizontal lines that act
    as the threshold limits on either side of a vertical line splitting the data so
    that the shape is like that of a step. If the value is on the horizontal line
    at 1, then the signal progresses; otherwise, the neuron doesn't fire. We also
    previously mentioned how a diagonal line could be used at this step to linearly
    separate points. When points cannot be separated by either of these simple activation
    functions, then we can use nonlinear alternatives, which we will look at next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来考虑最简单的激活函数——Heaviside 或二元阶跃函数。这可以通过视觉化的方式想象为两条水平线，它们作为阈值限制，位于一条垂直线的两侧，将数据分割，形成像阶梯一样的形状。如果值处于1的水平线上，那么信号会继续传递；否则，神经元就不会激活。我们之前也提到过，如何在这一步使用一条对角线来线性地分离点。当这些简单的激活函数无法分离数据点时，我们可以使用非线性的替代函数，接下来我们将介绍这些函数。
- en: Exploring the sigmoid function
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 sigmoid 函数
- en: The sigmoid function is the classic S-shaped function. This function works especially
    well for logistic regression tasks. While most of the results will be classified
    by the tails on either side of the curve, there is an area in the middle for capturing
    uncertainty about some of the data. The drawback of this shape is that the gradient
    is almost zero at the extremes, so the model may not be able to continue to learn
    as the points get towards either side.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是经典的S型函数。这种函数特别适用于逻辑回归任务。尽管大多数结果会通过曲线两侧的尾部进行分类，但曲线中间有一个区域，用于捕捉部分数据的不确定性。这种形状的缺点是，在极端位置，梯度几乎为零，因此随着数据点向两侧移动，模型可能无法继续学习。
- en: The sigmoid function also contains a derivative value, which means that we can
    use this function along with backpropagation to update the weights after the variables
    pass through additional layers. We will explore backpropagation more in the final
    parts of this chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数还包含一个导数值，这意味着我们可以将该函数与反向传播一起使用，在变量通过额外层之后更新权重。我们将在本章的最后部分更详细地探讨反向传播。
- en: Another advantage of the sigmoid function is that it confines values between
    0 and 1 so that the values are conveniently bound.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数的另一个优点是它将值限制在0和1之间，从而使值方便地被限定。
- en: 'The `sigmoid` function can be defined simply using R code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数可以通过R代码简单地定义：'
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this function, we can see that the dynamic value is the exponent of negative
    `x`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们可以看到动态值是负`x`的指数。
- en: 'Let''s use this sigmoid function on a sequence of values between `-10` and
    `10`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`-10`到`10`之间的数值序列上使用这个Sigmoid函数：
- en: 'To start, we will create a `tibble` containing two columns. One column will
    contain the sequence of numbers and the other will use the results of passing
    this sequence of values as arguments through the `sigmoid` function:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个包含两列的`tibble`。一列将包含数值序列，另一列将使用将这些数值序列作为参数传递给`sigmoid`函数后的结果：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we set up the base of our plot by using the `ggplot()` function and passing
    in the data objects and defining the values that will be used along the *x*- and
    *y*-axes. In this case, we use the sequence of values between `-10` and `10` along
    the *x*-axis and the results of passing these values through the sigmoid function
    along the *y*-axis:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`ggplot()`函数设置图形的基础，通过传入数据对象并定义用于*X*轴和*Y*轴的值。在这个例子中，我们使用`-10`到`10`之间的数值序列作为*X*轴的值，使用将这些数值传递到sigmoid函数后的结果作为*Y*轴的值：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will now add the points and use the `stat_function` feature to connect the
    points and display the sigmoid shape:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将添加数据点，并使用`stat_function`特性将这些点连接起来，展示出S型曲线的形状：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we now look at this shape, we can see why this type of activation function
    works so well for logistic regression. Using the `sigmoid` function to transform
    our values has pushed most of the values close to the extremes of `0` or `1`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在观察这个形状，我们可以理解为什么这种激活函数在逻辑回归中表现得如此出色。通过使用`sigmoid`函数来转换我们的数值，大多数数值已经被推近到`0`或`1`的极端位置：
- en: '![](img/5dce8270-9c4f-4561-a97f-ffcbbd5c8e4e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5dce8270-9c4f-4561-a97f-ffcbbd5c8e4e.png)'
- en: Investigating the hyperbolic tangent function
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究双曲正切函数
- en: The hyperbolic tangent function, which is also known as **tanh**, is very similar
    to the sigmoid function; however, the lower bound of the curve is in negative
    space to better handle data containing negative values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数，也称为**tanh**，与sigmoid函数非常相似；然而，曲线的下限位于负数区域，更适合处理包含负值的数据。
- en: Aside from this one difference, everything else about the hyperbolic tangent
    function is the same as the sigmoid function, and like the sigmoid function, the
    hyperbolic tangent contains a derivative element and can be used with backpropagation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一点，双曲正切函数与sigmoid函数其他方面完全相同，和sigmoid函数一样，双曲正切函数也包含导数元素，可以与反向传播一起使用。
- en: Since tanh is bounded between `-1` and `1`, the gradient is larger and the derivative
    is more pronounced. Being bounded means that tanh is centered around `0`, which
    can be advantageous in a model with a large number of hidden layers as the results
    from a layer are easier for the next layer to use.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于tanh的值在`-1`到`1`之间，它的梯度较大，导数更加明显。被限定的特点使得tanh围绕`0`中心，这对于具有大量隐藏层的模型来说是有利的，因为层之间的结果更容易被下一个层使用。
- en: 'Let''s use the same sequence of values and plot the values after passing all
    of these values through the hyperbolic tangent function, which is included with
    base R and can be called using `tanh()`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用相同的数值序列，并绘制通过双曲正切函数转换后的值，该函数是R的基础函数之一，可以通过`tanh()`调用：
- en: 'As we did in the preceding sigmoid example, we will create a `tibble` with
    the values in our sequence and the transformed values:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们在前面的sigmoid示例中所做的，我们将创建一个`tibble`，其中包含我们的数值序列及其转换后的值：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then set up the base of our plot by passing the dataset along with the values
    to use for the *x*-axis and the *y*-axis to the `ggplot()` function:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过将数据集以及用于*x*轴和*y*轴的值传递给`ggplot()`函数，来设置我们绘图的基础：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lastly, we again add our points and use the `stat_function` feature to connect
    the dots and display our shape:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们再次添加我们的点，并使用`stat_function`功能来连接这些点并显示我们的形状：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we look at this shape, we can see that it is a very similar shape to the
    sigmoid shape that we just plotted; however, note that the *y*-axis now has a
    range from `-1` to `1` rather than `0` to `1`, as was the case with the sigmoid
    shape. As a result, the values are pushed even further to the extremes and negative
    values remain negative after transformation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个形状时，我们可以看到它与我们刚才绘制的sigmoid形状非常相似；然而，请注意，*y*轴现在的范围是`-1`到`1`，而不是sigmoid形状中的`0`到`1`。因此，值被推向极端，负值在转换后仍然是负值：
- en: '![](img/2497a31e-f420-4e7a-aba6-b4859d42df7d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2497a31e-f420-4e7a-aba6-b4859d42df7d.png)'
- en: Plotting the rectified linear units activation function
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制整流线性单元激活函数
- en: '**Rectified Linear Units** (**ReLU**) is a hybrid function that fits a line
    for positive values of *x* while assigning any negative values of *x* with a value
    of 0\. Even though one half of this function is linear, the shape is nonlinear
    and carries with it all the advantages of nonlinearity, such as being able to
    use the derivative for backpropagation.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**整流线性单元**（**ReLU**）是一个混合函数，对于正值的*x*，它拟合一条直线，而对于任何负值的*x*，它将其赋值为0。尽管该函数的一半是线性的，但它的形状是非线性的，并且具有非线性的所有优点，例如可以使用导数进行反向传播。'
- en: Unlike the previous two activation functions, it has no upper bound. This lack
    of a constraint can be helpful to avoid the issue with the sigmoid or tanh function,
    where the gradient becomes very gradual near the extremes and provides little
    information to help the model continue to learn. Another major advantage of ReLU
    is how it leads to sparsity in the neural network because of the drop off at the
    center point. Using signoid or tanh, very few output values from the function
    will be zero, which means that the activation functions will fire, leading to
    a dense network. By contrast, ReLU results in far more output values of zero,
    leading to fewer neurons firing and a much sparser network.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与前两个激活函数不同，ReLU没有上限。这种没有约束的特性有助于避免sigmoid或tanh函数的问题，即在极值附近梯度变得非常平缓，提供的信息很少，无法帮助模型继续学习。ReLU的另一个主要优点是它会导致神经网络的稀疏性，因为在中心点附近有掉头现象。使用sigmoid或tanh时，函数的零值输出非常少，这意味着激活函数会触发，从而导致网络密集。相比之下，ReLU会产生更多的零输出值，导致更少的神经元触发，从而形成一个更稀疏的网络。
- en: ReLU will learn faster than sigmoid and tanh because of its simplicity. The
    ReLU function results in more zero values than sigmoid or tanh, which will improve
    the speed of the training process; however, since we no longer know the unaltered
    value for these points, they cannot be updated during backpropagation later. This
    can be an issue if weights would otherwise be adjusted to pass along relevant
    information during backpropagation; however, it is no longer possible once the
    derivative is set to zero.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU比sigmoid和tanh学习得更快，因为它的简单性。ReLU函数比sigmoid或tanh产生更多的零值，这将提高训练过程的速度；然而，由于我们不再知道这些点的未修改值，它们不能在之后的反向传播中更新。如果权重需要在反向传播过程中调整以传递相关信息，这可能会成为一个问题；然而，一旦导数被设置为零，就无法再进行调整。
- en: 'Let''s write some code to visualize the ReLU function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一些代码来可视化ReLU函数：
- en: 'First, we will define the function. It is simply defined so that if a value
    for *x* is greater than 0, then it sets *x* equal to *y*, and creates a line with
    a slope of `1`; otherwise, it sets *y* to *x* and creates a horizontal line on
    the *x*-axis. This can be coded like this:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义这个函数。它简单地定义为，如果*x*的值大于0，那么它将*x*设置为*y*，并创建一个斜率为`1`的直线；否则，它将*y*设置为*x*，并在*x*轴上创建一条水平线。可以这样编码：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let''s create our dataset again using the same sequence of numbers as
    before and the transformed values after passing this sequence through our ReLU
    function:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们再次使用之前相同的数字序列创建数据集，并将该序列通过我们的ReLU函数后得到的转换值：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, let''s plot these points and connect them to display the shape of this
    activation function:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们绘制这些点并连接它们，以展示该激活函数的形状：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, as we look at this shape, we can see its strengths. Having no upper bounds
    provides the model with more information than sigmoid, where the gradient becomes
    very minimal near the extremes. In addition, converting all negative values to
    `0` results in a much more sparse neural network and faster training time:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们观察这个形状时，可以看到它的优点。没有上界为模型提供了比sigmoid更多的信息，因为sigmoid在极端值附近的梯度变得非常微小。此外，将所有负值转换为`0`会导致神经网络更加稀疏，从而加快训练速度：
- en: '![](img/04d3f782-4dd1-46e9-9255-e68dcf7e19a6.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04d3f782-4dd1-46e9-9255-e68dcf7e19a6.png)'
- en: Calculating the Leaky ReLU activation function
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算Leaky ReLU激活函数
- en: One potential problem with ReLU is known as **dying ReLU**, where, since the
    function assigns a zero value for all negative values, signals can get dropped
    completely before reaching the output node. One way to try to solve this issue
    is to use Leaky ReLU, which assigns a small alpha value when numbers are negative
    so that the signal is not completely lost. Once this constant is applied, the
    values that would otherwise have been zero now have a small slope. This keeps
    the neuron from being fully deactivated so that information can still be passed
    on to improve the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU的一个潜在问题是**死亡ReLU**，由于该函数对所有负值赋值为零，因此信号在到达输出节点之前可能会完全丢失。解决这个问题的一种方法是使用Leaky
    ReLU，当数值为负时，赋一个小的alpha值，从而避免信号完全丢失。应用这个常数后，本应为零的值现在有了一个小的斜率。这可以防止神经元完全失效，使得信息仍然能够传递，从而改善模型。
- en: 'Let''s create a simple example of the Leaky ReLU activation function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的Leaky ReLU激活函数示例：
- en: 'We start by defining the function. We will do this in the exact same way as
    the ReLU function, except that instead of assigning a `0` to all negative values,
    we will instead multiply by a constant to provide a small slope:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从定义函数开始。我们将以与ReLU函数完全相同的方式进行操作，不同之处在于，我们不是将所有负值赋值为`0`，而是乘以一个常数以提供一个小的斜率：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After this, we create the dataset with the sequence of values that we have
    been using in these examples, along with the transformed values:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建包含我们在这些示例中使用的数值序列的数据集，以及转换后的值：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, we plot these points to display the shape of this activation function:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们绘制这些点来展示这个激活函数的形状：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When we look at this shape, we can see why this alternative to ReLU is preferable
    sometimes. Having the slight slope for negative values of **x** combats the dying
    ReLU problem where the neural network becomes too sparse and doesn''t have the
    data needed to converge around a prediction:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看这个形状时，我们可以理解为什么有时这种替代ReLU的方法更可取。对于负值的**x**拥有轻微的斜率可以解决死亡ReLU问题，在该问题中，神经网络变得过于稀疏，缺乏足够的数据来围绕预测收敛：
- en: '![](img/7009590a-75de-4684-9efe-096c48732791.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7009590a-75de-4684-9efe-096c48732791.png)'
- en: Defining the swish activation function
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义swish激活函数
- en: Swish is a more recently developed activation function that aims to leverage
    the strengths of ReLU while also addressing some of its shortcomings. Swish, like
    ReLU, has a lower bound and no upper bound, which is a strength as it can still
    deactivate neurons while preventing values from being forced to converge around
    an upper bound. However, unlike ReLU, the lower bound is still curved, and what's
    more notable is that the line is nonmonotonic, which means that as values for
    `x` decrease, the value for `y` can increase. This is an important feature that
    prevents the dying neuron problem as the derivative can continue to be modified
    across iterations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Swish是一种较新开发的激活函数，旨在发挥ReLU的优势，同时解决它的一些缺点。Swish与ReLU一样，具有下界且没有上界，这是其优势，因为它仍然可以使神经元失效，同时防止数值被迫收敛到上界。然而，与ReLU不同的是，下界仍然是曲线形的，更值得注意的是，这条线是非单调的，这意味着当`x`值减小时，`y`值可以增加。这是一个重要的特点，可以防止死亡神经元问题，因为导数可以在迭代中继续调整。
- en: 'Let''s investigate the shape of this activation function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来研究这个激活函数的形状：
- en: 'Let''s start by defining the function, as we did in other examples. The formula
    simply takes a value and multiplies it by the result of passing the exact same
    value through the sigmoid function:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们像在其他示例中一样，首先定义函数。这个公式简单地接受一个值，并将其乘以通过sigmoid函数传递相同值的结果：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After this, we will create our dataset again with the same sequence of values
    that we have used previously and a corresponding set of transformed values:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将再次创建我们的数据集，使用之前已经使用过的相同值序列，并附上对应的转换值集合：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we can plot these points to display the shape of this function:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以绘制这些点，展示这个函数的形状：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we look at this shape, we will see that, like ReLU, it has a lower bound
    and no upper bound; however, unlike ReLU and all other activation functions, it
    is nonmonotonic—that is, we can see that the values for *y* when *x* is negative
    first decrease and then increase. This feature has been shown to be especially
    beneficial as neural networks get progressively deeper:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察这个形状时，会发现它像ReLU一样有下界，但没有上界；然而，与ReLU及其他激活函数不同，它是非单调的——也就是说，我们可以看到当*x*为负时，*y*的值先减小后增大。这个特性已经证明在神经网络逐渐加深时尤其有益：
- en: '![](img/c46491be-188a-43eb-9303-b59485b73200.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c46491be-188a-43eb-9303-b59485b73200.png)'
- en: Predicting class likelihood with softmax
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用softmax预测类别概率
- en: 'The `softmax()` function is needed when there is more than one target variable.
    Softmax will create probabilities that a particular set of input variables belongs
    to each class. After these results are calculated, they can be used to assign
    input rows to one of the possible target classes. Let''s explore this activation
    function with a slightly different example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当有多个目标变量时，`softmax()`函数是必需的。Softmax将计算输入变量属于每个类别的概率。计算出这些结果后，可以用来将输入行分配到可能的目标类别之一。让我们通过一个略有不同的示例来探索这个激活函数：
- en: 'We will start by defining the function:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从定义函数开始：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, let''s pass a vector of values to the function:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们将一个值的向量传递给函数：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s confirm that the sum of these transformed values equals `1`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们确认这些转换后的值的总和等于`1`：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see that this function will take a set of values and calculate a probability
    that each is a value we are trying to predict so that the sum of all probabilities
    is `1`. This can be used to select the most likely value among a set of more than
    two values for a given target.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个函数将接受一组值，并计算每个值是我们试图预测的值的概率，以便所有概率的总和为`1`。这可以用来从多个可能的值中选择最可能的一个，作为给定目标的预测值。
- en: Creating a feedforward network
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个前馈网络
- en: With an understanding of neural networks, we will now build some simple examples.
    First, we will create the functions needed to create a very simple neural network
    ourselves to better understand what is happening during the modeling process.
    Afterward, we will use the `neuralnet` package to build a neural network that
    solves a task using a simple dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解神经网络的基础上，我们将构建一些简单的示例。首先，我们将创建自己需要的函数，以便创建一个非常简单的神经网络，从而更好地理解建模过程中发生了什么。然后，我们将使用`neuralnet`包，利用一个简单的数据集构建一个神经网络来解决一个任务。
- en: Writing a neural network with Base R
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Base R编写神经网络
- en: 'For this example, we will use Base R to create a very simple neural network
    from scratch to better understand exactly what is happening at each step. In order
    to complete this task we will do the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用Base R从头开始创建一个非常简单的神经网络，以便更好地理解每个步骤发生了什么。为了完成这个任务，我们将执行以下操作：
- en: Define the activation function for the neurons in our model
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义我们模型中神经元的激活函数
- en: Create a function that shows the line after every iteration of learning the
    weights
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个函数，显示每次学习权重后的线条
- en: Make some test data and plot these data values
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作一些测试数据并绘制这些数据值
- en: Update weights using the results of the previous attempt
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前一次尝试的结果更新权重
- en: 'We will use the following steps to do so:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下步骤来实现：
- en: 'First, we code the Heaviside (binary) step activation function to start. We
    will recall that this function evaluates the input and if this value is greater
    than zero, then the output value of the function is `1`; otherwise, the value
    is `0`. We can express this logic in code using the following lines of code:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们编写Heaviside（二进制）阶跃激活函数作为开始。我们会回忆一下这个函数的工作原理，它评估输入值，如果该值大于零，则函数的输出值为`1`；否则，输出值为`0`。我们可以使用以下代码行将这个逻辑转化为代码：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we can create the function for drawing the line using random weights
    at first and then the learned weight as we iterate over and update the values
    passed to the expression portion of the curve function, as shown here:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个绘制直线的函数，首先使用随机权重，然后随着迭代并更新传递给曲线函数的表达式部分的值，最终使用学习到的权重，如下所示：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the expression equation, we can see that everything is relative to `x`. In
    this case, if we just ran `curve((x))`, then we would get a line at exactly 45
    degrees so that the `x` and `y` were always equal and the slope of the line was
    `1`. In the preceding code, we use the weights to change the slope of the line
    relative to `x`. The remainder just defines the plot and the line and the `add`
    argument is used to declare whether the new line produced by the curve function
    should be added to the plot in addition to the line or lines already there.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在表达式方程中，我们可以看到一切都相对于`x`。在这种情况下，如果我们仅仅运行`curve((x))`，那么我们将得到一条45度的直线，这样`x`和`y`总是相等，且直线的斜率为`1`。在前面的代码中，我们使用权重来改变相对于`x`的直线斜率。剩下的部分只是定义了图像和直线，`add`参数用于声明通过曲线函数生成的新直线是否应添加到现有的图像中。
- en: 'With these functions defined, we can now assign some initial values for the
    input, output, learning rate, and weights. The input values are a set of *x* and
    *y* coordinates along with a constant value. The output values are just binary
    flags, in this case, denoting whether the input variable is or is not a member
    of the target class. Lastly, some random weights are included to initialize the
    model, along with a learning rate that will be used for updating weights at every
    iteration:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义了这些函数后，我们现在可以为输入、输出、学习率和权重分配一些初始值。输入值是一组*x*和*y*坐标，并且包含一个常数值。输出值只是二进制标志，在此情况下表示输入变量是否为目标类的成员。最后，一些随机权重被用来初始化模型，同时还包括一个学习率，用于在每次迭代时更新权重：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we can add our first line, which is the guess using the random weights
    along with our output points. We would like to arrive at a line that completely
    bisects the two points that belong to the target class from the two points that
    do not. To start, we apply our initial random weights to the expression equation
    within the `linear_fits()` function to create an initial slope. The `points()`
    function adds our points with squares to the two points that are part of the target
    class and circles for the points that are not part of this class:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以添加我们的第一条线，这条线是通过使用随机权重以及我们的输出点来进行猜测的。我们希望得到一条完全二分目标类的两个点和非目标类的两个点的直线。首先，我们将初始的随机权重应用到`linear_fits()`函数中的表达式方程，创建一个初始的斜率。`points()`函数通过为属于目标类的两个点绘制方形标记，为不属于该类的点绘制圆形标记，来添加我们的点：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As we can tell from the following plot that is generated by the preceding code,
    this first line is extremely far from bisecting the two classes:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面这个由前面的代码生成的图像中我们可以看出，这第一条线与二分这两类数据相差甚远：
- en: '![](img/0069b1d5-2df1-4760-bda8-77c6244fa22f.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0069b1d5-2df1-4760-bda8-77c6244fa22f.png)'
- en: 'Now, we will begin to update the weights by using the values from the corresponding
    input and output values. First, the weights are updated by the learning rate.
    The smaller this number is, the more gradual the changes will be. This is multiplied
    by the target class value minus the product of the result of the artificial neuron
    function, which again is either `0` or `1` since we are using the binary step
    activation function and the first input values. The `linear_fits()` function is
    then used again to draw one more line:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将开始通过使用相应的输入和输出值来更新权重。首先，权重通过学习率进行更新。这个数字越小，变化就越平缓。它与目标类值减去人工神经元函数结果的乘积相乘，该结果仍然是`0`或`1`，因为我们使用的是二值步进激活函数以及输入的初始值。然后，再次使用`linear_fits()`函数绘制一条新的直线：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using the `linear_fits()` function, we have created a new line that is closer
    to bisecting the classes, and yet it is not yet completely dividing the points
    as we would like:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`linear_fits()`函数，我们创建了一条更接近于二分这两类数据的直线，但它还没有完全按照我们希望的方式划分这些点：
- en: '![](img/c91f7a32-4b3e-4c47-bad1-c8c3940700d4.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c91f7a32-4b3e-4c47-bad1-c8c3940700d4.png)'
- en: 'This same operation is repeated for the remainder of the input and output values.
    A different line type is used for the last plot because, as we will see, this
    line solves the problem and finds a slope that separates the two classes. The
    third line is drawn next:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个相同的操作对剩余的输入和输出值重复进行。由于我们将看到，这条线解决了问题，并找到了一个能够分离两类的斜率，因此在最后的图表中使用了不同的线型。接下来绘制第三条线：
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, the third line completely overlaps the second line:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第三行与第二行完全重合：
- en: '![](img/8a627969-1766-4e7f-a96c-943d88aa4127.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a627969-1766-4e7f-a96c-943d88aa4127.png)'
- en: 'The next code is used to draw the fourth line:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下列代码用于绘制第四条线：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The fourth line deviates further away from bisecting the points than the second
    or the third line:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第四条线偏离了第二条和第三条线更远，未能很好地将点分开：
- en: '![](img/3e80384b-d17b-4481-b876-950f7bc24a7e.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e80384b-d17b-4481-b876-950f7bc24a7e.png)'
- en: 'The final line is created using the following line of code:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一条线是通过以下代码生成的：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We see here that the dotted line successfully bisects the square points and
    separates them from the circular points:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到虚线成功地将方形点和圆形点分开：
- en: '![](img/7fb3f623-6478-46fa-a72a-93b6b038ac48.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fb3f623-6478-46fa-a72a-93b6b038ac48.png)'
- en: In this minimal example, we can see what is happening when we fit a neural network.
    The model attempts to separate different classes of variables just like in other
    machine-learning models. In this case, during every iteration, the weights are
    updated depending on whether the neuron fires along with the constant change introduced
    by the learning rate value. Through this process, the goal is to reduce the error
    rate. In this case, we did not define a formal error rate as we could clearly
    see when the line successfully divided the classes. In the next example, we will
    step away from the underlying math and focus on optimizing a slightly more complex
    neural network using an open source dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最小示例中，我们可以看到在拟合神经网络时发生了什么。该模型试图像其他机器学习模型一样将不同类别的变量分开。在这个过程中，每次迭代时，权重会根据神经元是否激活以及学习率值引入的常数变化进行更新。通过这一过程，目标是减少错误率。在本示例中，我们没有定义正式的错误率，因为我们可以清楚地看到当线条成功地分开类别时的效果。在下一个示例中，我们将不再关注底层数学，而是着重于使用一个开源数据集优化一个稍微复杂一些的神经网络。
- en: Creating a model with Wisconsin cancer data
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用威斯康星癌症数据创建模型
- en: For this example, we will use the breast cancer dataset from the University
    of Wisconsin. Details on this dataset can be found at the UCI Machine Learning
    Repository at [http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用来自威斯康星大学的乳腺癌数据集。有关该数据集的详细信息，可以在UCI机器学习库的[http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29)中找到。
- en: 'The dataset can be loaded using the following lines of code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集可以通过以下代码加载：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After loading the data, we will encode the target variable by converting the
    column from a character column with two character values that indicate whether
    or not there were signs of malignancy to a numeric data type holding binary values.
    For this type of neural network, we will need all values to be numeric, including
    the target variable:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据后，我们将通过将包含两个字符值的列转换为数值数据类型，来对目标变量进行编码，以表示是否存在恶性迹象。对于这种类型的神经网络，我们需要所有值都是数字类型，包括目标变量：
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will scale and standardize all of our predictor values. As we mentioned,
    all data needs to be numeric for the neural network, and by scaling and standardizing
    the data, we will increase performance by giving the activation functions a set
    of values that are all constrained within the same boundaries:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将对所有预测变量进行缩放和标准化。正如我们提到的，所有数据必须是数字类型，以便神经网络使用，通过缩放和标准化数据，我们将提高性能，因为这会将激活函数的值约束在相同的范围内：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will partition to train and test, just like in our machine learning
    example. We will use the ID column in `X1` for splitting the data in this step;
    however, afterward, we can drop this column. Here, we will use a `tidyverse` approach
    to simplify the process:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将进行训练和测试数据的划分，就像在我们的机器学习示例中一样。我们将在此步骤中使用`X1`中的ID列来拆分数据；不过，之后我们可以删除这一列。这里，我们将采用`tidyverse`方法来简化过程：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our last data preparation step is to extract all the actual correct responses
    from the test data and then remove this column from the test dataset. This vector
    of values will be used to calculate performance after modeling:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的最后一步数据准备是从测试数据集中提取所有实际的正确响应，然后将这一列从测试数据集中移除。这个值的向量将用于在建模后计算性能：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The data is now completely prepared and ready for modeling with a neural network.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已经完全准备好，可以用于神经网络建模。
- en: 'Next, we need to create the formula syntax for the `neuralnet` package, which
    is a dependent variable `~`. This is similar to the syntax for fitting a linear
    model with R. All independent variables can be connected with the `+` sign between
    each one; however, when there are many independent variables, then it would be
    very tedious to write the name for every column, even in this example, where the
    column names are just X followed by a number. Fortunately, there is a way to expedite
    this process, which we will use in the following steps. First, we will get the
    names for all of the columns in our train set and then, using paste and collapse,
    we will create the string of independent variables to go on the other side of
    our formula from the dependent variable:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为`neuralnet`包创建公式语法，其中包含一个因变量`~`。这与R中拟合线性模型的语法类似。所有自变量可以通过`+`符号连接起来；然而，当自变量有很多时，写出每一列的名称会非常繁琐，即使在这个例子中，列名只是X后跟一个数字。幸运的是，有一种方法可以加速这个过程，我们将在接下来的步骤中使用它。首先，我们将获取训练集中所有列的名称，然后使用paste和collapse来创建自变量的字符串，将它们放在公式中因变量的另一侧：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With this set, we can now fit our model. In this case, we will keep the model
    fairly simple, only using a few of the arguments available for this function.
    Specifically, we include the formula that we just created, defining the dependent
    and independent variables. Next, we indicate that the model will be fit to the
    train data. Choosing the correct number of layers and units per layer involves
    trying a few combinations and comparing performance. In this case, we start with
    two layers containing about half as many units as there are variables. Lastly,
    we note that the activation function should be logistic, which is the sigmoid
    function, and that we are not performing a linear operation:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这组数据，我们现在可以拟合我们的模型。在这个例子中，我们将保持模型相对简单，只使用这个函数可用的一些参数。具体来说，我们包括了刚刚创建的公式，定义了因变量和自变量。接下来，我们指出模型将拟合训练数据。选择正确的层数和每层单元数涉及到尝试几种组合并比较性能。在这个例子中，我们从两层开始，每层的单元数大约是变量数的一半。最后，我们注意到激活函数应该是逻辑函数，也就是sigmoid函数，并且我们不进行线性运算：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With the modeling process complete, we can now use our model to make predictions.
    With the `neuralnet` package, we use the `compute()` function to generate these
    prediction values:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型建模过程完成后，我们现在可以使用我们的模型进行预测。使用`neuralnet`包，我们通过`compute()`函数来生成这些预测值：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'When we pass the model and the test dataset through the `compute()` function,
    we are given a list. The list contains details about the neurons within the model,
    along with the predicted values. In this case, we just want the predicted values,
    so we will pull these from the list. In addition, we will create a set of binary
    predictions. The binary predictions are created by changing values to `1`, if
    the predicted probability is greater than `0.5`; otherwise, the value is changed
    to a `0`. We will use each set of predictions for two different model evaluation
    methods:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们通过`compute()`函数传递模型和测试数据集时，返回的是一个列表。该列表包含了模型中神经元的详细信息以及预测值。在这种情况下，我们只需要预测值，因此我们将从列表中提取这些值。此外，我们还将创建一组二进制预测。二进制预测是通过将预测概率大于`0.5`的值设置为`1`来生成的；否则，值将设置为`0`。我们将使用每一组预测进行两种不同的模型评估方法：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Using our binary predictions, we can easily calculate basic model accuracy—that
    is, we will sum the number of cases where the binary predicted value matches the
    actual value and divide that number by the total number of actual values:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的二进制预测，我们可以轻松计算基本的模型准确性——也就是说，我们将求出二进制预测值与实际值匹配的案例数，并将这个数字除以实际值的总数：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we see that the accuracy is 92.98%, so our basic neural net has performed
    quite well on this data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到准确率为92.98%，所以我们的基本神经网络在这个数据上表现得相当好。
- en: 'We can also look at the breakdown for this accuracy value by using a confusion
    matrix. The simplest way to produce a confusion matrix is to use the `confusionMatrix()`
    function, which is part of the `caret` package. This function requires a table
    containing the predicted valued and the actual values as an argument. In this
    case, we need to use our binary predictions as the results need to fit into one
    of four categories, and as such, levels of granularity are not permitted:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以通过使用混淆矩阵来查看准确度值的分解。生成混淆矩阵的最简单方法是使用`confusionMatrix()`函数，它是`caret`包的一部分。该函数需要一个包含预测值和实际值的表作为参数。在这种情况下，我们需要使用我们的二元预测，因为结果需要适应四种类别之一，因此不允许更高的粒度级别。
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After calling this function, we see that we are provided with a two by two
    grid containing the results. The confusion matrix has categorized our predictions
    into the following four outcomes:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用此函数后，我们会看到提供了一个包含结果的二维网格。混淆矩阵将我们的预测分为以下四种结果：
- en: '**True positives**: Values of `1` correctly predicted to be `1`. The actual
    test target variable contains the value we are predicting and we correctly predicted
    it.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**：值为`1`被正确预测为`1`。实际测试目标变量包含我们预测的值，我们正确地预测了它。'
- en: '**Type I errors**: Values of `0` incorrectly predicted to be values of `1`.
    The actual test target variable does not have the value we are predicting; however,
    we predicted that it will. Also referred to as a false positive.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I型错误**：值为`0`被错误预测为`1`。实际测试目标变量不包含我们预测的值；然而，我们预测它会包含。也被称为假阳性。'
- en: '**Type II errors**: Values of `1` incorrectly predicted to be values of `0`.
    The actual test target variable does have the value we are predicting; however,
    we predicted that it will not. Also referred to as a false negative.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**II型错误**：值为`1`被错误预测为`0`。实际测试目标变量确实包含我们预测的值；然而，我们预测它不会包含。也被称为假阴性。'
- en: '**True negatives**: Values of `0` correctly predicted to be `0`. The actual
    test target variable does not contain the value we are predicting and we correctly
    predicted that it would not have it.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**：值为`0`被正确预测为`0`。实际测试目标变量不包含我们预测的值，我们正确地预测了它不会包含。'
- en: It also includes a number of other statistical measures that are outside the
    scope of this chapter; however, we can note that the accuracy is included and
    the value matches the value that we just calculated ourselves.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包括一些其他的统计度量，这些超出了本章的范围；然而，我们可以注意到，准确度被包含在内，并且该值与我们刚才自己计算的值相匹配。
- en: In addition to using our binary prediction to calculate accuracy, we can also
    use our probabilities so that we take into account the level of certainty for
    each outcome. In order to measure performance using these values, we will use
    the AUC, or area under the curve, score. This compares the probabilities for true
    positive cases with the probabilities for false positive cases. The final result
    is a measure of the confidence that positive values are positive and that negative
    values are negative, or in this case, that negative values are not incorrectly
    labeled positive with high confidence.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用二元预测计算准确度外，我们还可以使用我们的概率值，从而考虑到每个结果的确定性程度。为了使用这些值衡量性能，我们将使用AUC（曲线下面积）得分。它比较了真实阳性案例的概率与假阳性案例的概率。最终结果是一个衡量正值为正且负值为负的信心值，或者在本例中，衡量负值不会被高置信度错误标记为正值。
- en: 'To calculate the AUC score, we can use the `auc()` function, which is part
    of the `Metrics` package. The function takes two arguments—a vector of actual
    values and a vector of predicted probabilities that a record should be classified
    as the target variable based on the model''s interpretation of the independent
    variable for that row:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了计算AUC得分，我们可以使用`auc()`函数，它是`Metrics`包的一部分。该函数接受两个参数——一个实际值向量和一个预测概率向量，该向量表示根据模型对该行独立变量的解释，一个记录应被分类为目标变量的概率：
- en: '[PRE38]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The AUC score of 0.987 is even stronger than the accuracy score calculated previously.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AUC得分为0.987，甚至比之前计算的准确度得分还要强。
- en: This model is already working very well at solving the prediction task using
    this dataset; however, we will now try to add a backpropagation step and see if
    we can improve performance further.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已经在使用该数据集解决预测任务方面表现得非常好；然而，我们现在将尝试添加一个反向传播步骤，看看是否可以进一步提高性能。
- en: Augmenting our neural network with backpropagation
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强我们的神经网络，采用反向传播
- en: 'At this point, we have a working neural network. For this simple example, we
    will add one additional feature of neural networks that can improve performance,
    which is backpropagation. A neural network can learn to solve a task by multiplying
    the variable by values so that the variables are weighted as they pass through
    hidden layers. The backpropagation step allows the model to traverse back through
    layers and adjust the weights that were learned during previous steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经有了一个可运行的神经网络。对于这个简单的示例，我们将加入神经网络的一个附加特性，这个特性可以提高性能，那就是反向传播。神经网络可以通过将变量与值相乘，从而在经过隐藏层时对变量进行加权，学习去解决任务。反向传播步骤允许模型回溯各层，并调整在先前步骤中学习到的权重。
- en: In practical terms, this step is quite straightforward to implement. We simply
    declare that we will use the backpropagation algorithm and indicate the learning
    rate, which controls how much the weights are adjusted. In general, this learning
    rate value should be very low.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实际操作中，这一步是相当直接的实现。我们只需声明将使用反向传播算法，并指明学习率，学习率控制着权重调整的幅度。通常，这个学习率值应该非常低。
- en: 'In the following example, we have to do the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们需要做以下几步：
- en: The `threshold` value and `stepmax` value have to be changed as the model failed
    to converge using the default values.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` 值和 `stepmax` 值必须进行调整，因为使用默认值时模型未能收敛。'
- en: The `threshold` argument defines the value that the error rate must reach before
    the model stops and the `stepmax` argument defines the number of iterations the
    model will run before stopping.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` 参数定义了误差率必须达到的值，模型才会停止；而 `stepmax` 参数定义了模型在停止前将运行的迭代次数。'
- en: 'By changing these values, you can program the model to run longer and stop
    sooner, both of which will help if you run into an error when converging:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更改这些值，你可以编程让模型运行得更久，或者更早停止，如果在收敛时遇到错误，这两者都会有所帮助：
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After running this new version of the model, we can run the same steps again
    to assess performance. First, we will run `compute` on the new model to get new
    predictions. We will once again create a vector of probabilities and binary predictions,
    and as a first step, we will create the table of binary prediction values and
    actual values and pass this to the `confusionMatrix()` function. We will skip
    calculating the accuracy this time around as it is included in the output from
    the call to the `confusionMatrix()` function:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这个新版本的模型后，我们可以重新运行相同的步骤来评估性能。首先，我们将在新模型上运行 `compute` 以获得新的预测值。我们将再次创建一个包含概率和二元预测的向量，并且作为第一步，我们将创建二元预测值和实际值的表格，并将其传递给
    `confusionMatrix()` 函数。由于 `confusionMatrix()` 函数的输出已经包括了准确率，因此这次我们跳过计算准确率：
- en: '[PRE40]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our accuracy has improved, increasing from 92.98% to 94.74%. Let''s now check
    our AUC score. Again, we simply pass the actual values and predicted probabilities
    to the `auc()` function:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的准确率有所提高，从 92.98% 增加到 94.74%。现在，让我们检查一下我们的 AUC 得分。同样，我们只需将实际值和预测概率传递给 `auc()`
    函数：
- en: '[PRE41]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Our AUC score has improved, increasing from 0.987 to 0.993, so we can see that
    backpropagation does improve model performance.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 AUC 得分有所提高，从 0.987 增加到 0.993，因此我们可以看到，反向传播确实提高了模型性能。
- en: That being said, what exactly is happening during this step?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在这一步骤中究竟发生了什么呢？
- en: The backpropagation step takes the derivative of the error rate and uses this
    to update weights based on results.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播步骤取误差率的导数，并利用此导数根据结果更新权重。
- en: The derivative is just the rate at which the current weights impact the error
    rate. So if the derivative rate is `7`, then changing the weights by a single
    unit will result in a change to the error rate that is 7 times larger.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导数只是当前权重对误差率影响的变化率。所以，如果导数率为 `7`，那么改变权重一个单位将导致误差率变化 7 倍。
- en: Using just a feedforward neural network, we can update the initial weights based
    on the final derivative value; however, using backpropagation, we can update the
    weights at every neuron.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用前馈神经网络，我们可以根据最终的导数值更新初始权重；然而，使用反向传播时，我们可以在每个神经元上更新权重。
- en: Using information about how previous changes have impacted the derivative, this
    step either increases or decreases the weights.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过利用先前的变化如何影响导数的信息，这一步要么增加，要么减少权重。
- en: The learning rate is applied so that changes are never dramatic but rather smooth
    and gradual. This process can continue until the error rate is minimized.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率的应用确保了变化不会剧烈，而是平稳和逐步进行。这个过程可以持续，直到误差率最小化。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned that deep learning is differentiated from other
    machine-learning algorithms because of the use of multiple hidden layers. This
    network of hidden layers, which are composed of artificial neurons, was designed
    to mimic the way our brain processes input signals to interpret our environment.
    The units within the hidden layers take in all the independent variables and apply
    some weights to these variables. In this way, each neuron classifies the combination
    of input values in different ways.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解到，深度学习与其他机器学习算法的区别在于使用了多个隐藏层。这个由人工神经元组成的隐藏层网络被设计成模拟我们大脑处理输入信号以解读环境的方式。隐藏层中的单元接收所有独立变量，并对这些变量应用一定的权重。通过这种方式，每个神经元以不同的方式对输入值的组合进行分类。
- en: From understanding the architecture of this type of machine learning from a
    high level, we then took a deeper dive into the actual process of converting the
    input to predictions using this approach. We discussed the various activation
    functions that act as the gate for every neuron, determining whether a signal
    should be passed to the next layer. We then built two feedforward neural networks—one
    using base R for a better understanding of what is happening and another using
    the `neuralnet` package on a larger dataset. Lastly, we applied the backpropagation
    step to further improve our model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次理解这种机器学习架构后，我们深入探讨了使用这种方法将输入转换为预测的实际过程。我们讨论了各种激活函数，它们作为每个神经元的“门”，决定是否将信号传递到下一层。随后，我们构建了两个前馈神经网络——一个使用基础R语言以便更好地理解发生了什么，另一个则在更大的数据集上使用`neuralnet`包。最后，我们应用了反向传播步骤，进一步改进了我们的模型。
- en: As stated, the artificial neural network is the fundamental building block for
    more complex deep learning, and now that we have this understanding, we will move
    on to creating convolutional neural networks for image recognition in the next
    chapter.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，人工神经网络是更复杂的深度学习的基本构建块，现在我们已经掌握了这一理解，接下来将在下一章创建用于图像识别的卷积神经网络。
