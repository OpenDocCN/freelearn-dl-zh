- en: Cross-validation and Parameter Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证与参数调优
- en: Predictive analytics is about making predictions for unknown events. We use
    it to produce models that generalize data. For this, we use a technique called
    cross-validation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析是关于对未知事件进行预测的。我们使用它来生成能够泛化数据的模型。为此，我们使用一种叫做交叉验证的技术。
- en: Cross-validation is a validation technique for assessing the result of a statistical
    analysis that generalizes to an independent dataset that gives a measure of out-of-sample
    accuracy. It achieves the task by averaging over several random partitions of
    the data into training and test samples. It is often used for hyperparameter tuning
    by doing cross-validation for several possible values of a parameter and choosing
    the parameter value that gives the lowest cross-validation average error.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种验证技术，用于评估统计分析结果的泛化能力，能够给出样本外准确度的度量。它通过对数据进行多次随机划分成训练样本和测试样本，然后对这些划分进行平均来完成这项任务。它通常用于超参数调优，即通过对多个可能的参数值进行交叉验证，选择给出最低交叉验证平均误差的参数值。
- en: 'There are two kinds of cross-validation: exhaustive and non-exhaustive. K-fold
    is an example of non-exhaustive cross-validation. It is a technique for getting
    a more accurate assessment of the model''s performance. Using k-fold cross-validation,
    we can do hyperparameter tuning. This is about choosing the best hyperparameters
    for our models. Techniques such as k-fold cross-validation and hyperparameter
    tuning are crucial for building great predictive analytics models. There are many
    flavors or methods of cross-validation, such as holdout cross-validation and k-fold
    cross-validation.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证有两种类型：穷举交叉验证和非穷举交叉验证。K折交叉验证就是非穷举交叉验证的一种例子。它是一种获得更准确的模型性能评估的技术。通过使用K折交叉验证，我们可以进行超参数调优。超参数调优是选择适合我们模型的最佳超参数。像K折交叉验证和超参数调优这样的技术对于构建优秀的预测分析模型至关重要。交叉验证有许多变种或方法，例如保留法交叉验证和K折交叉验证。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Holdout cross-validation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留法交叉验证
- en: K-fold cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: Comparing models with k-fold cross-validation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K折交叉验证比较模型
- en: Introduction to hyperparameter tuning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调优简介
- en: Holdout cross-validation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留法交叉验证
- en: In holdout cross-validation, we hold out a percentage of observations and so
    we get two datasets. One is called the training dataset and the other is called
    the testing dataset. Here, we use the testing dataset to calculate our evaluation
    metrics, and the rest of the data is used to train the model. This is the process
    of holdout cross-validation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在保留法交叉验证中，我们将一部分观察数据保留出来，因此会得到两个数据集。一个被称为训练数据集，另一个被称为测试数据集。在这里，我们使用测试数据集来计算评估指标，剩下的数据则用于训练模型。这就是保留法交叉验证的过程。
- en: The main advantage of holdout cross-validation is that it is very easy to implement
    and it is a very intuitive method of cross-validation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 保留法交叉验证的主要优点是它非常容易实现，是一种非常直观的交叉验证方法。
- en: The problem with this kind of cross-validation is that it provides a single
    estimate for the evaluation metric of the model. This is problematic because some
    models rely on randomness. So in principle, it is possible that the evaluation
    metrics calculated on the test sometimes they will vary a lot because of random
    chance. So the main problem with holdout cross-validation is that we get only
    one estimation of our evaluation metric.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种交叉验证方法的问题在于它只提供了模型评估指标的一个估计值。这个问题在于某些模型依赖于随机性。因此，原则上，由于随机因素的影响，计算出的测试评估指标有时会有很大的波动。因此，保留法交叉验证的主要问题是我们只得到评估指标的一个估计值。
- en: K-fold cross-validation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: In k-fold cross-validation, we basically do holdout cross-validation many times.
    So in k-fold cross-validation, we partition the dataset into *k* equal-sized samples.
    Of these many *k* subsamples, a single subsample is retained as the validation
    data for testing the model, and the remaining *k−1* subsamples are used as training
    data. This cross-validation process is then repeated *k* times, with each of the
    *k* subsamples used exactly once as the validation data. The *k* results can then
    be averaged to produce a single estimation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在K折交叉验证中，我们基本上多次执行保留法交叉验证。因此，在K折交叉验证中，我们将数据集划分为*k*个相等大小的子样本。在这些*k*个子样本中，单个子样本被保留作为验证数据，用于测试模型，其余的*k−1*个子样本用作训练数据。这个交叉验证过程将重复*k*次，每个子样本都恰好作为验证数据使用一次。然后，这*k*个结果可以进行平均，从而得出一个单一的估计值。
- en: 'The following screenshot shows a visual example of 5-fold cross-validation
    (*k=5*) :'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了5折交叉验证的可视化示例（*k=5*）：
- en: '![](img/da18d7be-8cc3-4aa7-a092-e74201a63185.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da18d7be-8cc3-4aa7-a092-e74201a63185.png)'
- en: Here, we see that our dataset gets divided into five parts. We use the first
    part for testing and the rest for training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们的数据集被划分为五个部分。我们使用第一部分进行测试，其余部分用于训练。
- en: 'The following are the steps we follow in the 5-fold cross-validation method:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在5折交叉验证方法中遵循的步骤：
- en: We get the first estimation of our evaluation metrics.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得了评估指标的首次估计。
- en: We use the second part for testing and the rest for training, and we use that
    to get a second estimation of our evaluation metrics.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用第二部分进行测试，其余部分用于训练，并用其获取评估指标的第二次估计。
- en: We use the third part for testing and the rest for training, and so on. In this
    way, we get five estimations of the evaluation metrics.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用第三部分进行测试，其余部分用于训练，依此类推。这样，我们会得到五次评估指标的估计。
- en: In k-fold cross-validation, after the *k* estimations of the evaluation matrix
    have been observed, an average of them is taken. This will give us a better estimation
    of the performance of the model. So, instead of having just one estimation of
    this evaluation metric, we can get *n* number of estimations with k-fold cross-validation,
    and then we can take the average and get a better estimation for the performance
    of the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，观察到* k *次评估矩阵后，我们会取其平均值。这将为我们提供模型性能的更好估计。因此，我们不仅能获得该评估指标的一个估计，而是通过k折交叉验证获得*n*次估计，然后取其平均值，从而为模型性能提供更好的估计。
- en: As seen here, the advantage of the k-fold cross-validation method is that it
    can be used not only for model evaluation but also for hyperparameter tuning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如此所见，k折交叉验证方法的优点在于它不仅可以用于模型评估，还可以用于超参数调优。
- en: In this validation method, the common values for *k* are 5 and 10.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种验证方法中，*k*的常见值为5和10。
- en: 'The following are the variants of k-fold cross-validation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是k折交叉验证的变体：
- en: '**Repeated cross-validation:** In repeated cross-validation, we perform k-fold
    cross-validation many times. So, if we want 30 estimations of our evaluation metrics,
    we can do 5-fold cross-validation six times. So then we will get 30 estimations
    of our evaluation metrics.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复交叉验证：** 在重复交叉验证中，我们会进行多次k折交叉验证。所以，如果我们需要30次评估指标的估计，我们可以进行5折交叉验证六次。这样我们就能得到30次评估指标的估计。'
- en: '**Leave-One-Out (LOO) cross-validation:** In this method, we take the whole
    dataset for training except for one point. We use that one point for evaluation
    and then we repeat this process for every data point in our dataset.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**留一法（LOO）交叉验证：** 在这种方法中，我们使用整个数据集进行训练，除了一个数据点。我们使用这个数据点进行评估，然后对数据集中的每个数据点重复此过程。'
- en: If we have millions of points, this validation method will be really expensive
    computationally. We use repeated k-fold cross-validation in such cases, because
    this validation method will give us comparatively good results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有数百万个数据点，这种验证方法在计算上会非常昂贵。在这种情况下，我们使用重复的k折交叉验证，因为这种验证方法能够提供相对较好的结果。
- en: Implementing k-fold cross-validation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现k折交叉验证
- en: Let's take examples of a `diamond` dataset to understand the implementation
    of k-fold cross-validation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以`diamond`数据集为例来理解k折交叉验证的实现。
- en: 'For performing k-fold cross-validation in `scikit-learn`, we first have to
    import the libraries that we will use. The following code snippet shows the code
    used for importing the libraries:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`中执行k折交叉验证时，我们首先需要导入将要使用的库。以下代码片段展示了用于导入库的代码：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The second step is to prepare the dataset for the `diamond` dataset that we
    will use in this example. The following shows the code used to prepare data for
    this dataset:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是准备我们将在此示例中使用的`diamond`数据集。以下展示了用于准备该数据集数据的代码：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After preparing the data, we will create the objects used for modeling. The
    following shows the code used to create the objects for modeling:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好数据后，我们将创建用于建模的对象。以下展示了用于创建建模对象的代码：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is the same cell that we used in [Chapter 1](765f03c9-7f1e-4eb8-af81-5cf6b0f3d2ee.xhtml),
    *Ensemble Methods for Regression and Classification*. The difference here is that
    we are not using the `train_test_split` function. Here, we are producing the `X`
    matrix, which contains all the features and also has our target feature. So we
    have our `X` matrix and our `y` vector.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在[第1章](765f03c9-7f1e-4eb8-af81-5cf6b0f3d2ee.xhtml)《回归与分类的集成方法》中使用的相同单元格。这里的区别是我们没有使用`train_test_split`函数。在这里，我们正在生成`X`矩阵，其中包含所有特征，同时也包含我们的目标特征。因此，我们有了`X`矩阵和`y`向量。
- en: For training the model, we will instantiate our `RandomForestRegressor` function,
    which we found was the best model in [Chapter 1](765f03c9-7f1e-4eb8-af81-5cf6b0f3d2ee.xhtml), *Ensemble
    Methods for Regression and Classification*, for this dataset. The following shows
    the code used to instantiate the `RandomForestRegressor` function**:**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，我们将实例化我们的`RandomForestRegressor`函数，经过测试，我们发现它是[第1章](765f03c9-7f1e-4eb8-af81-5cf6b0f3d2ee.xhtml)《回归与分类的集成方法》中最适合该数据集的模型。以下展示了实例化`RandomForestRegressor`函数的代码**：**
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To perform k-fold cross-validation, we import the `cross_validate` function
    from the `model_selection` module in `scikit-learn`. The following shows the code
    used to import the `cross_validate` function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行k折交叉验证，我们从`scikit-learn`的`model_selection`模块中导入了`cross_validate`函数。以下展示了导入`cross_validate`函数的代码：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This `cross_validate` function works as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`cross_validate`函数的工作原理如下：
- en: 'We provide the estimator, which will be the `RandomForestRegressor` function.
    The following shows the code used to apply the `RandomForestRegressor` function:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提供了估算器，它将是`RandomForestRegressor`函数。以下展示了应用`RandomForestRegressor`函数的代码：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we pass the `X` object and the `y` object.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们传递了`X`对象和`y`对象。
- en: We provide a set of metrics that we want to evaluate for this model and for
    this dataset. In this case, evaluation is done using the `mean_squared_error`
    function and the `r2` metrics, as shown in the preceding code. Here, we pass the
    value of *k* in `cv`. So, in this case, we will do tenfold cross-validation.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为该模型和数据集提供了一组我们希望评估的指标。在本例中，评估是使用`mean_squared_error`函数和`r2`指标来完成的，如前面的代码所示。在这里，我们传递了`cv`中的*k*值。因此，在本例中，我们将进行十折交叉验证。
- en: 'The output that we get from this `cross_validate` function will be a dictionary
    with the corresponding matrix. For better understanding, the output is converted
    into a dataframe. The following screenshot shows the code use to visualize the
    scores in the dataframe and the dataframe output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个`cross_validate`函数中得到的输出将是一个字典，包含相应的矩阵。为了更好地理解，输出被转换为一个数据框。以下截图展示了用于在数据框中可视化分数的代码以及数据框的输出：
- en: '![](img/9856da4f-8d3d-476e-a72e-661065f019ad.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9856da4f-8d3d-476e-a72e-661065f019ad.png)'
- en: Here, we apply `test_mean_squared_error` and `test_r2`, which were the two metrics
    that we wanted to evaluate. After evaluating, we get the `train_mean_squared_error` value
    and the `test_r2` set. So, we are interested in the testing metrics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用了`test_mean_squared_error`和`test_r2`，这正是我们希望评估的两个指标。评估后，我们得到了`train_mean_squared_error`值和`test_r2`集合。因此，我们对测试指标感兴趣。
- en: To get a better assessment of the performance of a model, we will take an average
    (mean) of all of the individual measurements.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地评估模型的性能，我们将对所有单独的测量结果取平均值（均值）。
- en: 'The following shows the code for getting the `Mean test MSE` and `Mean test
    R-squared` values and output showing their values:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了获取`平均测试MSE`和`平均测试R平方`值的代码，以及输出显示它们的值：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So here, on averaging, we see that the mean of the test MSE is the value that
    we have here and an average of the other metric, which was the R-squared evaluation
    metrics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这里，通过取平均，我们看到测试MSE的均值就是我们这里的值，另外一个指标是R平方评估指标的平均值。
- en: Comparing models with k-fold cross-validation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k折交叉验证比较模型
- en: As k-fold cross-validation method proved to be a better method, it is more suitable
    for comparing models. The reason behind this is that k-fold cross-validation gives
    much estimation of the evaluation metrics, and on averaging these estimations,
    we get a better assessment of model performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k折交叉验证方法被证明是更好的方法，它更适合用来比较模型。其原因在于，k折交叉验证提供了对评估指标的更多估计，通过对这些估计进行平均，我们可以更好地评估模型性能。
- en: 'The following shows the code used to import libraries for comparing models:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了用于导入比较模型的库的代码：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After importing libraries, we''ll import the `diamond` dataset. The following
    shows the code used to prepare this `diamond` dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库之后，我们将导入`diamond`数据集。以下是用于准备这个`diamond`数据集的代码：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we have to prepare objects for modeling after preparing the dataset for
    doing model comparison. The following shows the code used for preparing the objects
    for modeling. Here we have the `X` matrix, showing the features, and the `y` vector,
    which is the target for this dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要在准备好数据集以进行模型比较后，准备建模对象。以下展示了用于准备建模对象的代码。在这里，我们有`X`矩阵，表示特征，以及`y`向量，表示该数据集的目标：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we will compare the KNN model, the random forest model, and the bagging
    model. In these models, using tenfold cross-validation, we will use the `KNNRegressor`,
    `RandomForestRegressor`, and `AdaBoostRegressor` functions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将比较KNN模型、随机森林模型和集成模型。在这些模型中，使用十折交叉验证，我们将使用`KNNRegressor`、`RandomForestRegressor`和`AdaBoostRegressor`函数。
- en: 'Then we will import the `cross_validate` function. The following shows the
    code used to import the `cross_validate` function for these three models:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将导入`cross_validate`函数。以下是用于导入这三个模型的`cross_validate`函数的代码：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to compare the models using the `cross_validate` function.
    The following shows the code block used for comparing these three models:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用`cross_validate`函数比较模型。以下是用于比较这三种模型的代码块：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we see the result of the testing evaluation metrics that we will use
    for every model. We use the `mean_squared_error` function. For every model, we
    use tenfold cross-validation and after getting the result, we get the `test_score`
    variable. This `test_score` variable is the mean-squared error in this case, and
    is shown in the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到将用于每个模型的测试评估指标的结果。我们使用`mean_squared_error`函数。对于每个模型，我们使用十折交叉验证，获得结果后，我们得到`test_score`变量。这个`test_score`变量在此情况下是均方误差，并在以下代码中显示：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following screenshot shows the code for the result that we got after running
    the tenfold cross-validation for every model and also, the table showing the 10
    estimations of the evaluation metrics for the three models:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了运行每个模型的十折交叉验证后得到的结果代码，并且展示了三种模型的10次评估指标估算值的表格：
- en: '![](img/3371935f-fe8f-49f7-97e0-b4f194856d59.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3371935f-fe8f-49f7-97e0-b4f194856d59.png)'
- en: This table shows a lot of variation in estimations of every model. To know the
    actual performance of any model, we take the average of the result. So, in the
    preceding figure, we take the mean of all the values and then plot it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该表展示了每个模型评估值的较大变化。为了了解任何模型的实际表现，我们取结果的平均值。因此，在前面的图表中，我们取所有值的均值后再绘制。
- en: 'The following screenshot shows the code used to take the mean and the graph
    showing the mean MSE values for every model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了用于计算均值的代码，以及显示每个模型的均值MSE值的图表：
- en: '![](img/a47895c7-c5aa-4670-896b-0eddabf074f6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a47895c7-c5aa-4670-896b-0eddabf074f6.png)'
- en: On averaging, the random forest model performs best out of the three models.
    So, after taking an average, the second place goes to the KNN model, and the boosting
    model takes last place.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 经平均后，随机森林模型在这三种模型中表现最好。因此，经过平均后，第二名是KNN模型，集成模型则排名最后。
- en: 'The following figure shows the code used to get the box plot for these evaluation
    metrics and the box plot for the three models:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了用于获取这些评估指标箱线图的代码，以及显示三种模型箱线图的结果：
- en: '![](img/cc62d4f6-ce2f-4866-9f7c-6fb96e95b6a3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc62d4f6-ce2f-4866-9f7c-6fb96e95b6a3.png)'
- en: So, looking at the box plot for these evaluation metrics, we see that, the random
    forest performs the best.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，观察这些评估指标的箱线图，我们看到随机森林表现最佳。
- en: 'To check the degree of variability of these models, we can analyze the standard
    deviation of the test MSE for regression models. The following screenshot shows
    the code used to check the degree of variability and the plot showing the standard
    deviation of these models:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这些模型的变异度，我们可以分析回归模型测试MSE的标准差。以下截图展示了用于检查变异度的代码，以及展示这些模型标准差的图表：
- en: '![](img/e98e483d-748c-4067-ab99-27a387d6d1c6.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e98e483d-748c-4067-ab99-27a387d6d1c6.png)'
- en: The preceding screenshot shows that the most model in this case was the KNN
    model, followed by the boosting model, and random forest model had the least variation.
    So, the random forest model is the best among these three models. Even for this
    dataset, the random forest model performs the best.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图显示，在这个案例中，最常用的模型是KNN模型，其次是提升模型，而随机森林模型的变化最小。所以，在这三种模型中，随机森林模型是最好的。即使对于这个数据集，随机森林模型表现最佳。
- en: Introduction to hyperparameter tuning
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优介绍
- en: The method used to choose the best estimators for a particular dataset or choosing
    the best values for all hyperparameters is called **hyperparameter tuning**. Hyperparameters
    are parameters that are not directly learned within estimators. Their value is
    decided by the modelers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于选择特定数据集的最佳估算器或选择所有超参数最佳值的方法称为**超参数调优**。超参数是那些在估算器中不能直接学习的参数。它们的值由建模人员决定。
- en: For example, in the `RandomForestClassifier` object, there are a lot of hyperparameters,
    such as `n_estimators`, `max_depth`, `max_features`, and `min_samples_split`.
    Modelers decide the values for these hyperparameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在`RandomForestClassifier`对象中，有许多超参数，如`n_estimators`、`max_depth`、`max_features`和`min_samples_split`。建模人员决定这些超参数的值。
- en: Exhaustive grid search
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 穷举网格搜索
- en: One of the most important and generally-used methods for performing hyperparameter
    tuning is called the **exhaustive grid search**. This is a brute-force approach
    because it tries all of the combinations of hyperparameters from a grid of parameter
    values. Then, for each combination of hyperparameters, the model is evaluated
    using k-fold cross-validation and any other specified metrics. So the combination
    that gives us the best metric is the one that is returned by the object that we
    will use in `scikit-learn`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 执行超参数调优的一个非常重要且常用的方法叫做**穷举网格搜索**。这是一种暴力搜索方法，因为它会尝试所有超参数网格中的组合。然后，对于每个超参数组合，使用k折交叉验证和任何其他指定的度量标准来评估模型。所以，给出最佳度量标准的组合将由我们将在`scikit-learn`中使用的对象返回。
- en: Let's take an example of a hyperparameter grid. Here, we try three different
    values, such as 10, 30, and 50, for the `n_estimators` hyperparameter. We will
    try two options, such as auto and square root, for `max_features`, and assign
    four values—5, 10, 20, and 30—for `max_depth`. So, in this case, we will have
    24 hyperparameter combinations. These 24 will be evaluated. For every one of these
    24 combinations, in this case, we use tenfold cross-validation and the computer
    will be training and evaluating 240 models. The biggest shortcoming that grid
    search faces is the curse of dimensionality which will be covered in the coming
    chapters. The curse of dimensionality essentially means that the number of times
    you will have to evaluate your model increases exponentially with the number of
    parameters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个超参数网格的例子。在这里，我们尝试为`n_estimators`超参数选择三个不同的值，如10、30和50。我们为`max_features`选择两个选项，如auto和平方根，并为`max_depth`分配四个值——5、10、20和30。所以，在这种情况下，我们将有24种超参数组合。这24个组合将会被评估。在这24个组合的每一种情况下，我们使用十折交叉验证，计算机将训练并评估240个模型。网格搜索面临的最大缺点是维度灾难，接下来的章节将详细讨论。维度灾难本质上意味着，随着参数数量的增加，你需要评估模型的次数会呈指数级增长。
- en: If certain combinations of hyperparameters are not tested, then different grids
    can be passed to the `GridSearchCV` object. Here, different grids can be passed
    in the form of a list of dictionaries because every grid is a dictionary in `scikit-learn`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些超参数组合没有被测试，则可以将不同的网格传递给`GridSearchCV`对象。在这里，可以通过字典列表的形式传递不同的网格，因为每个网格在`scikit-learn`中都是一个字典。
- en: Don't ever use the entire dataset for tuning parameters, always perform train-test
    split when tuning parameters, otherwise the hyperparameters may be fit to that
    specific dataset and the model won't generalize well to new data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要使用整个数据集来调优参数，调优参数时始终进行训练集-测试集划分，否则超参数可能会过拟合于特定数据集，导致模型无法很好地泛化到新数据。
- en: So, we perform the train-test split and use one part of the dataset to learn
    the hyperparameters of our model; the part that we left for testing should be
    used for the final model evaluation, and later we use the whole dataset to fit
    the model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们进行训练集-测试集划分，并使用数据集的一部分来学习模型的超参数；我们留作测试的部分应当用于最终模型评估，之后再使用整个数据集来拟合模型。
- en: Hyperparameter tuning in scikit-learn
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中的超参数调优
- en: Let's take an example of the `diamond` dataset to understand hyperparameter
    tuning in `scikit-learn`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以`diamond`数据集为例来理解`scikit-learn`中的超参数调整。
- en: 'To perform hyperparameter tuning, we first have to import the libraries that
    we will use. To import the libraries, we will use the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行超参数调整，我们首先需要导入我们将使用的库。导入库的代码如下：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we perform the transformations to the `diamond` dataset that we will
    use in this example. The following shows the code used to prepare data for this
    dataset:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们对将用于本示例的`diamond`数据集进行转换。以下展示了为该数据集准备数据的代码：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After preparing the data, we will create the objects used for modeling. The
    following shows the code used to create the objects for modeling:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好数据后，我们将创建用于建模的对象。以下展示了创建建模对象的代码：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After performing and creating the objects used for modeling, we perform the
    `train_test_split` function. In the preceding codeblock, we will set 0.1(10%)
    of the data for testing, so this portion of the dataset will be used for model
    evaluation after tuning the hyperparameters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行并创建用于建模的对象之后，我们执行`train_test_split`函数。在前面的代码块中，我们将数据的10%（0.1）用于测试，因此这一部分数据将在调整超参数后用于模型评估。
- en: 'We will tune the `RandomForestRegressor` model using the following parameters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下参数来调整`RandomForestRegressor`模型：
- en: '`n_estimators`: This parameter represents the number of trees in the forest.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：此参数表示森林中树的数量。'
- en: '`max_features`: This parameter represents the number of features to consider
    when looking for best split. The possible choices are `n_features`, which corresponds
    to the auto hyperparameter, or the square root of the `log2` of the number of
    features.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：此参数表示在寻找最佳分割时考虑的特征数量。可选择的选项有`n_features`，对应于自动超参数，或者特征数量的`log2`平方根。'
- en: '`max_depth`: This parameter represents the maximum depth of the tree.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：此参数表示树的最大深度。'
- en: Different values with a grid search will be used for these parameters. For `n_estimators`
    and `max_depth`, we will use four different values. For `n_estimators`, we will
    use `[25,50,75,100]` as values, and for `max_depth`, we will use `[10,15,20,30]`
    as values. For `max_features`, we will use the auto and square root.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用不同的网格搜索值来调整这些参数。对于`n_estimators`和`max_depth`，我们将使用四个不同的值。对于`n_estimators`，我们将使用`[25,50,75,100]`作为值，对于`max_depth`，我们将使用`[10,15,20,30]`作为值。对于`max_features`，我们将使用自动和平方根。
- en: 'Now we will instantiate the `RandomForestRegressor` model explicitly without
    any hyperparameters. The following shows the code used to instantiate it:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将显式实例化`RandomForestRegressor`模型，不带任何超参数。以下展示了用于实例化它的代码：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The parameter grid is basically a dictionary where we pass the name of the
    hyperparameter and the values that we want to try. The following code block shows
    the parameter grid with different parameters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数网格基本上是一个字典，其中我们传递超参数的名称和我们想尝试的值。以下代码块展示了具有不同参数的参数网格：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In total, we have four values for `n_estimators`, four values for `max_depth`,
    and two for `max_features`. So, on calculating, there are in total of 32 combinations
    of hyperparameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们有四个`n_estimators`值，四个`max_depth`值，以及两个`max_features`值。因此，计算后，总共有32种超参数组合。
- en: 'To perform hyperparameter tuning in `scikit-learn`, we will use the `GridSearchCV`
    object. The following shows the code used to import the object from `scikit-learn`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在`scikit-learn`中执行超参数调整，我们将使用`GridSearchCV`对象。以下展示了用于从`scikit-learn`导入该对象的代码：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we pass the estimator we want to tune, in this case, `RandomForestRegressor`. The
    following screenshot shows the code used and the output that we get:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们传递我们想要调整的估算器，在此案例中是`RandomForestRegressor`。以下截图展示了使用的代码和我们得到的输出：
- en: '![](img/7c374323-bcaa-4d95-b126-082372f965e8.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c374323-bcaa-4d95-b126-082372f965e8.png)'
- en: Then we pass the parameter grid that we want to try. Here, `refit` means that
    this estimator object will refit using the best parameters that it found using
    this process of grid-search and cross-validation. This is the evaluation metric
    that will be used by this object to evaluate all of the possible combinations
    of hyperparameters. In this case, we will use tenfold cross-validation. So after
    creating that, we can use the `fit` method and pass the training object. Since
    we are using tenfold cross-validation with 32 combinations, the model will evaluate
    320 models.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们传递要尝试的参数网格。在这里，`refit`表示该估计器对象将使用通过网格搜索和交叉验证过程找到的最佳参数进行重新拟合。这是该对象用来评估所有可能超参数组合的评估指标。在这个例子中，我们将使用十折交叉验证。创建完之后，我们可以使用`fit`方法并传递训练对象。由于我们使用的是十折交叉验证，并且有32种组合，模型将评估320个模型。
- en: 'We can get the results using the `cv _results_` attribute from the object that
    we created using the `GridSearchCV` method. The following screenshot shows the
    code used to get the result and output showing the result:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用从`GridSearchCV`方法创建的对象中的`cv_results_`属性来获取结果。以下截图展示了获取结果的代码和输出结果：
- en: '![](img/1c8cf2ba-e2f8-4301-96d2-f7e29f2876c0.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c8cf2ba-e2f8-4301-96d2-f7e29f2876c0.png)'
- en: 'Here, the most important thing is to get `best_params_`. So with `best_params_`,
    we can see the combination of parameters of all 32 combinations. The following
    screenshot shows the input used to get the parameters and the output showing combination
    of parameters that can give the best result:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最重要的是获取`best_params_`。通过`best_params_`，我们可以看到所有32种组合的参数。以下截图展示了获取参数的输入和显示最佳结果的参数组合的输出：
- en: '![](img/99ba11ae-8c28-44e3-bebe-433858fbe813.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99ba11ae-8c28-44e3-bebe-433858fbe813.png)'
- en: Here, we can see that the combination that can give `best_params_` is `max_depth`
    with a value of `20`, `max _features` with a value of auto, and `n_estimators`
    with a value of `100`. So this is the best possible combination of parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到能给出`best_params_`的组合是`max_depth`值为`20`，`max_features`值为auto，`n_estimators`值为`100`。所以这是参数的最佳组合。
- en: 'We can also get the `best_estimator_` object, and this is the complete list
    of hyperparameters. The following screenshot shows the code used to get `best_estimator_` and
    output showing the result:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以获取`best_estimator_`对象，这个对象包含了完整的超参数列表。以下截图展示了获取`best_estimator_`的代码和输出结果：
- en: '![](img/5e34b51a-55e9-4422-a75d-599120926bbf.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e34b51a-55e9-4422-a75d-599120926bbf.png)'
- en: So, as we tuned the hyperparameters of the random forest model, we got different
    values from the values that we got before; when we had values for `n_estimators`
    as `50`, `max_depth` as `16`, and `max_features` as `auto`, the parameters in
    that model were untuned.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在调优了随机森林模型的超参数后，我们得到的值与之前得到的值不同；之前我们设置`n_estimators`为`50`，`max_depth`为`16`，`max_features`为auto时，模型的参数是未调优的。
- en: Comparing tuned and untuned models
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较调优和未调优的模型
- en: 'We can compare the best model that we got while tuning the parameters with
    the best model that we have been using without the help of tuning `n_estimators`
    with a value of `50`, `max_depth` with a value of `16`, and `max_features` as
    `auto`, and in both the cases it was random forest. The following code shows the
    values of the parameters of both the tuned and untuned models:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将调优后的最佳模型与未调优的最佳模型进行比较，后者的`n_estimators`值为`50`，`max_depth`值为`16`，`max_features`为auto，在这两种情况下，都是使用随机森林。以下代码展示了调优和未调优模型的参数值：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To actually see the comparison between the tuned and untuned models, we can
    see the value of the mean-square error. The following screenshot shows the code
    to get the `mean_squared_error` value for the two models, and the plot showing
    the comparison of the MSE value of the two models:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际看到调优和未调优模型之间的比较，我们可以查看均方误差的值。以下截图展示了获取两个模型的`mean_squared_error`值的代码，以及比较两个模型MSE值的图表：
- en: '![](img/3bb189e2-3e82-45bc-911a-302a14c799f4.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bb189e2-3e82-45bc-911a-302a14c799f4.png)'
- en: We can clearly observe here that, on comparison, these tuned parameters perform
    better than the untuned parameters in both of the random forest models.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地观察到，在比较中，这些调优后的参数在两个随机森林模型中表现得比未调优的参数更好。
- en: 'To see the actual difference in values between the two models, we can do a
    little calculation. The following screenshot shows the calculation for getting
    the percentage of improvement and output showing the actual percentage value:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看两个模型之间的实际数值差异，我们可以做一些计算。以下截图展示了如何计算改进的百分比，并输出实际的百分比值：
- en: '![](img/de47b18f-6727-4197-8078-2b10a3457512.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de47b18f-6727-4197-8078-2b10a3457512.png)'
- en: Here we got an improvement of 4.6% for the tuned model over the untuned one
    and this is actually very good. In these models, an improvement of 1%-3% percent
    can also have huge practical implications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，调优后的模型比未调优的模型提高了4.6%，这是一个非常好的结果。在这些模型中，即使是1%-3%的改进也可能产生巨大的实际影响。
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about cross-validation, and different methods of
    cross-validation, including holdout cross-validation and k-fold cross-validation.
    We came to know that k-fold cross-validation is nothing but doing holdout cross-validation
    many times. We implemented k-fold cross-validation using the `diamond` dataset.
    We also compared different models using k-fold cross-validation and found the
    best-performing model, which was the random forest model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了交叉验证及其不同方法，包括保持法交叉验证和k折交叉验证。我们了解到，k折交叉验证其实就是多次进行保持法交叉验证。我们使用`diamond`数据集实现了k折交叉验证。我们还通过k折交叉验证比较了不同的模型，并找到了表现最佳的模型——随机森林模型。
- en: Then, we discussed hyperparameter tuning. We came across the exhaustive grid-search
    method, which is used to perform hyperparameter tuning. We implemented hyperparameter
    tuning again using the `diamond` dataset. We also compared tuned and untuned models,
    and found that tuned parameters make the model perform better than untuned ones.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了超参数调优。我们遇到了穷举网格搜索方法，这是一种用于执行超参数调优的方法。我们再次使用`diamond`数据集实现了超参数调优。我们还比较了调优后的模型和未调优的模型，发现调优后的参数使得模型表现优于未调优的模型。
- en: In the next chapter, we will study feature selection methods, dimensionality
    reduction and **principle component analysis** (**PCA**), and feature engineering.
    We will also learn about a method to improve the model with feature engineering.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习特征选择方法、降维技术和**主成分分析**（**PCA**），以及特征工程。我们还将学习一种通过特征工程改进模型的方法。
