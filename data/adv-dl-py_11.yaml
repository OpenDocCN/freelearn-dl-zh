- en: Sequence-to-Sequence Models and Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到序列模型与注意力机制
- en: In [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding Recurrent
    Networks, *we outlined several types of recurrent models, depending on the input-output
    combinations. One of them is **indirect many-to-many** or **sequence-to-sequence** (**seq2seq**), where
    an input sequence is transformed into another, different output sequence, not
    necessarily with the same length as the input. Machine translation is the most
    popular type of seq2seq task. The input sequences are the words of a sentence
    in one language and the output sequences are the words of the same sentence translated
    into another language. For example, we can translate the English sequence **tourist
    attraction** to the German **touristenattraktion**. Not only is the output sentence
    a different length, but there is no direct correspondence between the elements
    of the input and output sequences. In particular, one output element corresponds
    to a combination of two input elements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml)，*理解递归网络*中，我们概述了几种类型的递归模型，取决于输入输出的组合方式。其中之一是**间接多对多**或**序列到序列**（**seq2seq**），其中输入序列被转化为另一个不同的输出序列，输出序列不一定与输入序列的长度相同。机器翻译是最常见的seq2seq任务类型。输入序列是一个语言中的句子单词，输出序列是相同句子的翻译，转换为另一种语言。例如，我们可以将英语序列**tourist
    attraction**翻译为德语**touristenattraktion**。输出句子不仅长度不同，而且输入和输出序列的元素之间没有直接对应关系。特别地，一个输出元素可能对应输入元素的两个组合。
- en: Machine translation that's implemented with a single neural network is called **neural
    machine translation** (**NMT**). Other types of indirect many-to-many tasks include speech
    recognition, where we take different time frames of an audio input and convert
    them into a text transcript, question-answering chatbots, where the input sequences
    are the words of a textual question and the output sequence is the answer to that
    question, and text summarization, where the input is a text document and the output
    is a short summary of the text's contents.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一神经网络实现的机器翻译称为**神经机器翻译**（**NMT**）。其他类型的间接多对多任务包括语音识别，其中我们将音频输入的不同时间帧转化为文本转录；问答聊天机器人，其中输入序列是文本问题的单词，输出序列是该问题的答案；以及文本摘要，其中输入是文本文档，输出是该文档内容的简短总结。
- en: In this chapter, we'll introduce the attention mechanism—a new type of algorithm
    for seq2seq tasks. It allows direct access to any element of the input sequence.
    This is unlike a **recurrent neural network** (**RNN**), which summarizes the
    whole sequence in a single hidden state vector and prioritizes recent sequence
    elements over older ones.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍注意力机制——一种用于seq2seq任务的新型算法。它允许直接访问输入序列中的任何元素。这不同于**递归神经网络**（**RNN**），后者将整个序列总结为一个隐藏状态向量，并优先处理最近的序列元素，而忽略较早的元素。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing seq2seq models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍seq2seq模型
- en: Seq2seq with attention
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带注意力机制的Seq2seq
- en: Understanding transformers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解变换器（transformers）
- en: 'Transformer language models:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器语言模型：
- en: Bidirectional encoder representations from transformers
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自变换器的双向编码器表示
- en: Transformer-XL
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer-XL
- en: XLNet
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet
- en: Introducing seq2seq models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍seq2seq模型
- en: 'Seq2seq, or encoder-decoder (see *Sequence to Sequence Learning with Neural
    Networks* at [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)),
    models use RNNs in a way that''s especially suited for solving tasks with indirect
    many-to-many relationships between the input and the output. A similar model was
    also proposed in another pioneering paper, *Learning Phrase Representations using
    RNN Encoder-Decoder for Statistical Machine Translation* (go to [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    for more information). The following is a diagram of the seq2seq model. The input
    sequence [**A**, **B**, **C**, **<EOS>**] is decoded into the output sequence
    [**W**, **X**, **Y**, **Z**, **<EOS>**]:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2seq，或称编码器-解码器（参见*神经网络的序列到序列学习*，[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)），模型使用RNN的方式，特别适合解决输入和输出之间存在间接多对多关系的任务。类似的模型也在另一篇开创性论文中提出，*使用RNN编码器-解码器学习短语表示用于统计机器翻译*（更多信息请访问[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)）。以下是seq2seq模型的示意图。输入序列[**A**,
    **B**, **C**, **<EOS>**]被解码为输出序列[**W**, **X**, **Y**, **Z**, **<EOS>**]：
- en: '![](img/9541434b-efa7-46c2-8efb-1b06cafc3664.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9541434b-efa7-46c2-8efb-1b06cafc3664.png)'
- en: A seq2seq model case by https://arxiv.org/abs/1409.3215
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)提供的seq2seq模型案例
- en: 'The model consists of two parts: an encoder and a decoder. Here''s how the
    inference part works:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由两部分组成：编码器和解码器。下面是推理部分的工作方式：
- en: The encoder is an RNN. The original paper uses LSTM, but GRU or other types
    would also work. Taken by itself, the encoder works in the usual way—it reads
    the input sequence, one step at a time, and updates its internal state after each
    step. The encoder will stop reading the input sequence once a special **<EOS>**—end
    of sequence—symbol is reached. If we assume that we use a textual sequence, we'll
    use word-embedding vectors as the encoder input at each step, and the **<EOS>**
    symbol signals the end of a sentence. The encoder output is discarded and has
    no role in the seq2seq model, as we're only interested in the hidden encoder state.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器是一个RNN。原始论文使用的是LSTM，但GRU或其他类型的RNN也可以使用。单独来看，编码器按常规方式工作——它逐步读取输入序列，并在每步后更新其内部状态。当编码器读取到一个特殊的**<EOS>**（序列结束）符号时，它就停止读取输入序列。如果我们假设使用的是文本序列，则在每一步时我们会使用词嵌入向量作为编码器的输入，而**<EOS>**符号表示句子的结束。编码器的输出被丢弃，在seq2seq模型中没有作用，因为我们只关注隐藏的编码器状态。
- en: Once the encoder is finished, we'll signal the decoder so that it can start
    generating the output sequence with a special **<GO>** input signal. The encoder
    is also an RNN (LSTM or GRU). The link between the encoder and the decoder is
    the most recent encoder internal state vector **h***[t ]*(also known as the **thought
    vector**), which is fed as the recurrence relation at the first decoder step.
    The decoder output *y[t+1]* at step *t+1* is one element of the output sequence.
    We'll use it as an input at step *t+2*, then we'll generate new output, and so
    on (this type of model is called **autoregressive**). In the case of textual sequences,
    the decoder output is a softmax over all the words in the vocabulary. At each
    step, we take the word with the highest probability and we feed it as input to
    the next step. Once **<EOS>** becomes the most probable symbol, the decoding is
    finished.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦编码器完成，我们会向解码器发送信号，让它通过特殊的**<GO>**输入信号开始生成输出序列。编码器同样是一个RNN（LSTM或GRU）。编码器和解码器之间的联系是最新的编码器内部状态向量**h**[*t*]（也称为**思维向量**），它作为递归关系在第一个解码器步骤中被送入。解码器在第*t+1*步的输出*y[t+1]*是输出序列中的一个元素。我们会将其作为第*t+2*步的输入，然后生成新的输出，依此类推（这种类型的模型被称为**自回归**）。对于文本序列，解码器输出是对词汇表中所有词的softmax。在每一步中，我们会选择概率最高的词，并将其作为输入送入下一步。一旦**<EOS>**成为最可能的符号，解码过程就结束了。
- en: 'The training of the model is supervised, and the model needs to know both the
    input sequence and its corresponding target output sequence (for example, the
    same text in multiple languages). We feed the input sequence to the decoder, generate
    the thought vector *h*[*t*], and use it to initiate the output sequence generation
    from the decoder. However, the decoder uses a process called **teacher forcing**—the
    decoder input at step *t* is not the decoder output of step *t-1*. Instead, the
    input at step *t* is always the correct character from the target sequence at
    step *t-1*. For example, let''s say that the correct target sequence until step
    *t* is [**W**, **X**, **Y**], but the current decoder-generated output sequence
    is [**W**, **X**, **Z**]. With teacher forcing, the decoder input at step *t+1*
    will be **Y** instead of **Z**. In other words, the decoder learns to generate
    target values [t+1, ...] given target values [..., t]. We can think of this in
    the following way: the decoder input is the target sequence, while its output
    (target values) is the same sequence, but shifted one position to the right.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练是有监督的，模型需要知道输入序列及其对应的目标输出序列（例如，同一文本的多种语言）。我们将输入序列送入解码器，生成思维向量*h*[*t*]，并用它来启动解码器的输出序列生成。然而，解码器采用一种叫做**教师强制**（teacher
    forcing）的方法——在第*t*步时，解码器的输入不是第*t-1*步的解码器输出。而是第*t*步的输入始终是目标序列第*t-1*步的正确字符。例如，假设目标序列在第*t*步时的正确内容为[**W**，**X**，**Y**]，但当前解码器生成的输出序列为[**W**，**X**，**Z**]。使用教师强制时，第*t+1*步的解码器输入将是**Y**，而不是**Z**。换句话说，解码器学会根据目标值[...，t]来生成目标值[t+1，...]。我们可以这样理解：解码器的输入是目标序列，而它的输出（目标值）是相同的序列，只不过右移了一个位置。
- en: 'To summarize, the seq2seq model solves the problem of varying input/output
    sequence lengths by encoding the input sequence in a fixed-length state vector
    and then using this vector as a base to generate the output sequence. We can formalize
    this by saying that it tries to maximize the following probability:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，seq2seq模型通过将输入序列编码为固定长度的状态向量，然后利用该向量作为基础生成输出序列，解决了输入/输出序列长度变化的问题。我们可以将其形式化为，模型尝试最大化以下概率：
- en: '![](img/5570a5ca-5687-4458-bbb3-9677b184229b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5570a5ca-5687-4458-bbb3-9677b184229b.png)'
- en: 'This is equivalent to the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于以下内容：
- en: '![](img/3fe60f77-417e-41e6-99bc-352fcf22bcbe.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fe60f77-417e-41e6-99bc-352fcf22bcbe.png)'
- en: 'Let''s look at the elements of this formula in more detail:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这个公式的各个元素：
- en: '![](img/d7e7a462-5bf5-461e-ad23-3381b31c85e3.png)is the conditional probability
    where ![](img/c8b1de2d-651e-47df-8338-35ba46c96a00.png) is the input sequence
    with length *T* and ![](img/ae11b1ef-58db-42da-9d92-6b94eccdf07f.png) is the output
    sequence with length *T''.*'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/d7e7a462-5bf5-461e-ad23-3381b31c85e3.png)是条件概率，其中![](img/c8b1de2d-651e-47df-8338-35ba46c96a00.png)是长度为*T*的输入序列，![](img/ae11b1ef-58db-42da-9d92-6b94eccdf07f.png)是长度为*T''*的输出序列。'
- en: The element *v *is the fixed-length encoding of the input sequence (the thought
    vector).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元素*v*是输入序列的固定长度编码（思想向量）。
- en: '![](img/8dc7f5de-0839-409a-874f-846a787889bb.png)is the probability of an output
    word *y[T'']* given prior words *y*, as well as the vector *v.*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8dc7f5de-0839-409a-874f-846a787889bb.png)是给定前置单词*y*和向量*v*时，输出词汇*y[T'']*的概率。'
- en: 'The original seq2seq paper introduces a few tricks to enhance the training
    and performance of the model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的seq2seq论文介绍了一些*技巧*来增强模型的训练和性能：
- en: The encoder and decoder are two separate LSTMs. In the case of NMTs, this makes
    it possible to train different decoders with the same encoder.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器是两个独立的LSTM。在NMT的情况下，这使得可以使用相同的编码器训练不同的解码器。
- en: The experiments of the authors of the paper demonstrated that stacked LSTMs
    perform better than the ones with a single layer.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文作者的实验表明，堆叠的LSTM比单层LSTM效果更好。
- en: 'The input sequence is fed to the decoder in reverse. For example, **ABC** ->
    **WXYZ** would become **CBA** -> **WXYZ**. There is no clear explanation of why
    this works, but the authors have shared their intuition: since this is a step-by-step
    model, if the sequences were in normal order, each source word in the source sentence
    would be far from its corresponding word in the output sentence. If we reverse
    the input sequence, the average distance between input/output words won''t change,
    but the first input words will be very close to the first output words. This will
    help the model to establish better *communication* between the input and output
    sequences.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入序列反向馈送给解码器。例如，**ABC** -> **WXYZ**会变成**CBA** -> **WXYZ**。目前没有明确的解释说明为什么这种方法有效，但作者分享了他们的直觉：由于这是一个逐步生成的模型，如果序列是正常顺序，那么源句子中的每个单词都会远离输出句子中相应的单词。如果我们反转输入序列，输入/输出单词之间的平均距离不会改变，但第一个输入单词将非常接近第一个输出单词。这将帮助模型在输入和输出序列之间建立更好的*沟通*。
- en: 'Besides **<EOS>** and **<GO>**, the model also uses the following two special
    symbols (we''ve already encountered them in the *Implementing text classification*
    section of [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding
    Recurrent Networks*):'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了**<EOS>**和**<GO>**，模型还使用以下两个特殊符号（我们已经在[第7章](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml)的*实现文本分类*部分中遇到过它们，*理解递归网络*）：
- en: '**<UNK>**—**unknown**: This is used to replace rare words so that the vocabulary
    size doesn''t grow too large.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<UNK>**—**未知**：用于替换稀有词汇，以避免词汇表过大。'
- en: '**<PAD>**: For performance reasons, we have to train the model with sequences
    of a fixed length. However, this contradicts the real-world training data, where
    the sequences can have arbitrary lengths. To solve this, shorter sequences are
    filled with the special <PAD> symbol.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<PAD>**：出于性能考虑，我们必须使用固定长度的序列来训练模型。然而，这与实际训练数据相矛盾，因为这些序列可能有任意长度。为了解决这个问题，较短的序列会用特殊的<
    PAD >符号填充。'
- en: Now that we've introduced the base seq2seq model architecture, we'll learn how
    to extend it with the attention mechanism.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了基本的seq2seq模型架构，接下来我们将学习如何用注意力机制扩展它。
- en: Seq2seq with attention
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带注意力机制的Seq2seq
- en: The decoder has to generate the entire output sequence based solely on the thought
    vector. For this to work, the thought vector has to encode all of the information
    of the input sequence; however, the encoder is an RNN, and we can expect that
    its hidden state will carry more information about the latest sequence elements
    than the earliest. Using LSTM cells and reversing the input helps, but cannot
    prevent it entirely. Because of this, the thought vector becomes something of
    a bottleneck. As a result, the seq2seq model works well for short sentences, but
    the performance deteriorates for longer ones.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器必须仅基于思维向量生成整个输出序列。为了实现这一点，思维向量必须编码输入序列的所有信息；然而，编码器是一个RNN，我们可以预期它的隐藏状态会比最早的序列元素携带更多关于最新序列元素的信息。使用LSTM单元并反转输入有一定帮助，但不能完全防止这种情况。因此，思维向量成为了某种瓶颈。结果，seq2seq模型在短句子上表现良好，但对于更长的句子性能会下降。
- en: Bahdanau attention
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 巴赫达瑙注意力机制
- en: We can solve this problem with the help of the **attention mechanism** (see *Neural
    Machine Translation by Jointly Learning to Align and Translate* at [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)),
    an extension of the seq2seq model, that provides a way for the decoder to work
    with all encoder hidden states, not just the last one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过借助**注意力机制**（详见《[神经机器翻译：通过联合学习对齐与翻译](https://arxiv.org/abs/1409.0473)》），这是seq2seq模型的一个扩展，来解决这个问题，它提供了一种方式，让解码器不仅能使用最后一个编码器隐藏状态，而是能与所有编码器隐藏状态一起工作。
- en: The type of attention mechanism in this section is called Bahdanau attention,
    after the author of the original paper.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的注意力机制称为巴赫达瑙注意力机制（Bahdanau attention），以原始论文的作者命名。
- en: Besides solving the bottleneck problem, the attention mechanism has some other
    advantages. For one, the immediate access to all previous states helps to prevent
    the vanishing gradients problem. It also allows for some interpretability of the
    results because we can see what parts of the input the decoder was focusing on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能够解决瓶颈问题，注意力机制还有一些其他的优势。首先，立即访问所有之前的状态有助于防止梯度消失问题。它还允许结果的某些可解释性，因为我们可以看到解码器在处理输入时集中注意的部分。
- en: 'The following diagram shows how attention works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了注意力机制是如何工作的：
- en: '![](img/9b2651a9-4318-4b95-bcac-ed0e55d74bdc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b2651a9-4318-4b95-bcac-ed0e55d74bdc.png)'
- en: Attention mechanism
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'Don''t worry—it looks scarier than it actually is. We''ll go through this diagram
    from top to bottom: the attention mechanism works by plugging an additional **context
    vector** **c***[t]* between the encoder and the decoder. The hidden decoder state **s***[t]* at
    time *t* is now a function not only of the hidden state and decoder output at
    step *t-1*, but also of the context vector **c***[t]*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心——它看起来比实际复杂。我们将从上到下逐步解释这个图示：注意力机制通过在编码器和解码器之间插入一个额外的**上下文向量** **c***[t]*
    来工作。在时间 *t* 时，隐藏的解码器状态 **s***[t]* 不仅是步骤 *t-1* 的隐藏状态和解码器输出的函数，还包括上下文向量 **c***[t]*。
- en: '![](img/6ea7c879-71db-4d71-b835-5f96b75842cb.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ea7c879-71db-4d71-b835-5f96b75842cb.png)'
- en: 'Each decoder step has a unique context vector, and the context vector for one
    decoder step is just **a weighted sum of all encoder hidden states**. In this
    way, the encoder has access to all input sequence states at each output step *t*,
    which removes the necessity to encode all information of the source sequence into
    a fixed-length vector, as the regular seq2seq model does:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解码器步骤都有一个独特的上下文向量，而一个解码器步骤的上下文向量仅仅是所有编码器隐藏状态的**加权和**。通过这种方式，编码器可以在每个输出步骤 *t*
    访问所有输入序列的状态，这就不再需要像常规的seq2seq模型那样将源序列的所有信息编码成一个固定长度的向量。
- en: '![](img/eaeedfcf-489c-46b3-a647-835ea61754ec.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eaeedfcf-489c-46b3-a647-835ea61754ec.png)'
- en: 'Let''s discuss this formula in more detail:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这个公式：
- en: '**c***[t]* is the context vector for a decoder output step *t* out of *T''*, the
    total output.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**c***[t]* 是解码器输出步骤 *t*（从 *T''* 总输出中）的上下文向量。'
- en: '**h***[i]* is the hidden state of encoder step *i* out of *T* total input steps.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**h***[i]* 是编码器步骤 *i*（从 *T* 个总输入步骤中）的隐藏状态。'
- en: '*α[t,i]* is the scalar weight associated with *h[i]* in the context of the
    current decoder step *t*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α[t,i]* 是与 *h[i]* 相关的标量权重，在当前解码器步骤 *t* 的上下文中。'
- en: Note that *α[t,i]* is unique for both the encoder and decoder steps—that is, the
    input sequence states will have different weights depending on the current output
    step. For example, if the input and output sequences have lengths of 10, then
    the weights will be represented by a 10 × 10 matrix for a total of 100 weights.
    This means that the attention mechanism will focus the attention (get it?) of
    the decoder on different parts of the input sequence, depending on the current
    state of the output sequence. If *α**[t,i]* is large, then the decoder will pay
    a lot of attention to **h***[i]* at step *t.*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*α[t,i]* 对于编码器和解码器步骤是唯一的——也就是说，输入序列的状态将在不同的输出步骤中具有不同的权重。例如，如果输入和输出序列的长度为
    10，则权重将由一个 10 × 10 的矩阵表示，总共有 100 个权重。这意味着注意力机制将根据当前输出序列的状态，将解码器的注意力集中在输入序列的不同部分。如果
    *α**[t,i]* 较大，则解码器会在步骤 *t* 时非常关注 **h***[i]*。
- en: 'But how do we compute the weights *α**[t,i]*? First, we should mention that
    the sum of all *α**[t,i]* for a decoder at step *t* is 1\. We can implement this
    with a softmax operation on top of the attention mechanism:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何计算权重 *α**[t,i]* 呢？首先，我们应该提到，解码器在步骤 *t* 时，所有 *α**[t,i]* 的总和为 1。我们可以通过在注意力机制上应用
    softmax 操作来实现这一点：
- en: '![](img/e30f5a17-9dc2-4066-a71d-54d47e36b21b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e30f5a17-9dc2-4066-a71d-54d47e36b21b.png)'
- en: 'Here, *e[t,k]* is an alignment model, which indicates how well the input sequence
    elements around position *k* match (or align with) the output at position *t*.
    This score (represented by the weight *α**[t,i]*) is based on the previous decoder
    state **s***[t-1]* (we use **s***[t-1]* because we have not computed **s***[t]* yet),
    as well as the encoder state **h***[i]*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*e[t,k]* 是一个对齐模型，表示输入序列中位置 *k* 附近的元素与输出序列中位置 *t* 的匹配（或对齐）程度。这个得分（由权重 *α**[t,i]*
    表示）基于先前的解码器状态 **s***[t-1]*（我们使用 **s***[t-1]*，因为我们还没有计算 **s***[t]*），以及编码器状态 **h***[i]*：
- en: '![](img/3ba517d8-c552-4d8e-b078-a8bfc6c728de.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ba517d8-c552-4d8e-b078-a8bfc6c728de.png)'
- en: 'Here, *a* (and not alpha) is a differentiable function, which is trained with
    backpropagation together with the rest of the system. Different functions satisfy
    these requirements, but the authors of the paper chose the so-called **additive
    attention**, which combines **s***[t-1]* and **h***[i]* with the help of addition.
    It exists in two flavors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*a*（而不是 alpha）是一个可微分的函数，经过反向传播训练，和系统的其他部分一起学习。满足这些要求的函数有很多，但论文的作者选择了所谓的 **加性注意力**，它通过加法将
    **s***[t-1]* 和 **h***[i]* 结合起来。它有两种变体：
- en: '![](img/10f7fde3-06d9-4d0a-afb9-4140c7d0aa12.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10f7fde3-06d9-4d0a-afb9-4140c7d0aa12.png)'
- en: In the first formula, **W** is a weight matrix, applied over the concatenated
    vectors **s***[t-1]* and **h**[*i*], and **v** is a weight vector. The second
    formula is similar, but this time we have separate fully connected layers (the
    weight matrices **W***[1]* and **W***[2]*) and we sum **s***[t-1]* and **h***[i]*.
    In both cases, the alignment model can be represented as a simple feed-forward
    network with one hidden layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个公式中，**W** 是一个权重矩阵，应用于拼接后的向量 **s***[t-1]* 和 **h**[*i*]，而 **v** 是一个权重向量。第二个公式类似，但这一次我们有了独立的全连接层（权重矩阵
    **W***[1]* 和 **W***[2]*），并且我们对 **s***[t-1]* 和 **h***[i]* 进行了求和。在这两种情况下，对齐模型可以表示为一个简单的前馈网络，包含一个隐藏层。
- en: 'Now that we know the formulas for **c***[t]* and *α**[t,i]*, let''s replace
    the latter in the former:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了 **c***[t]* 和 *α**[t,i]* 的公式，让我们将后者代入前者：
- en: '![](img/57362bc4-26be-4976-9bf4-4e5af81241f7.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57362bc4-26be-4976-9bf4-4e5af81241f7.png)'
- en: 'As a conclusion, let''s summarize the attention algorithm in a step-by-step
    manner as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，下面是按步骤列出的注意力算法：
- en: Feed the encoder with the input sequence and compute the set of hidden states
    [![](img/36ae53cd-e2be-4faf-aa02-08d992c75345.png)].
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入序列传给编码器，并计算隐藏状态集 [![](img/36ae53cd-e2be-4faf-aa02-08d992c75345.png)]。
- en: Compute the alignment scores [![](img/61cc9e65-e8ca-42d7-a2b4-d31ee99edd10.png)],
    which use the decoder state from the preceding step **s***[t-1]*. If *t = 1*,
    we'll use the last encoder state **h***[T]* as the initial hidden state.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算对齐分数 [![](img/61cc9e65-e8ca-42d7-a2b4-d31ee99edd10.png)]，该分数使用来自前一步的解码器状态
    **s***[t-1]*。如果 *t = 1*，我们将使用最后的编码器状态 **h***[T]* 作为初始隐藏状态。
- en: Compute the weights [![](img/42518a65-1b9e-40f0-b9fc-afca0e8d184d.png)].
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算权重 [![](img/42518a65-1b9e-40f0-b9fc-afca0e8d184d.png)]。
- en: Compute the context vector [![](img/50970d59-6f4e-4aed-b40b-a8b4d038d0c6.png)].
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算上下文向量 [![](img/50970d59-6f4e-4aed-b40b-a8b4d038d0c6.png)]。
- en: Compute the hidden state [![](img/ebac8f8c-6818-4c34-aa52-b1d048a40d5d.png)],
    based on the concatenated vectors **s***[t-1]* and **c***[t]* and the previous
    decoder output *y[t-1]*. At this point, we can compute the final output *y[t]*.
    In the case where we need to classify the next word, we'll use the softmax output [![](img/df54b439-ccf4-4204-8fcc-2610d646d756.png)],
    where **W***[y]* is a weight matrix.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于拼接后的向量**s***[t-1]*和**c***[t]*以及前一时刻的解码器输出*y[t-1]*，计算隐藏状态[![](img/ebac8f8c-6818-4c34-aa52-b1d048a40d5d.png)]。此时，我们可以计算最终输出*y[t]*。在需要分类下一个词的情况下，我们将使用softmax输出[![](img/df54b439-ccf4-4204-8fcc-2610d646d756.png)]，其中**W***[y]*是一个权重矩阵。
- en: Repeat steps 2–6 until the end of the sequence.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2到6直到序列结束。
- en: Next, we'll introduce a slightly improved attention mechanism called Luong attention.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一个稍微改进的注意力机制，称为Luong 注意力。
- en: Luong attention
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Luong 注意力
- en: '**Luong attention** (see *Effective Approaches to Attention-based Neural Machine
    Translation* at [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025))
    introduces several improvements over Bahdanau attention. Most notably, the alignment
    scores *e[t]* depend on the decoder hidden state *s[t]*, as opposed to *s[t-1]*
    in Bahdanau attention. To better understand this, let''s compare the two algorithms:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Luong 注意力**（参见[《基于注意力的神经机器翻译的有效方法》](https://arxiv.org/abs/1508.04025)）在Bahdanau注意力上引入了几个改进。最显著的变化是对齐分数*e[t]*依赖于解码器隐藏状态*s[t]*，而不是Bahdanau注意力中的*s[t-1]*。为了更好地理解这一点，我们来比较这两种算法：'
- en: '![](img/817dea7b-4a83-4aa9-8bdc-fcbdc90f1d31.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/817dea7b-4a83-4aa9-8bdc-fcbdc90f1d31.png)'
- en: Left: Bahdanau attention; right: Luong attention
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：Bahdanau 注意力；右侧：Luong 注意力
- en: 'Let''s go through a step-by-step execution of Luong attention:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步执行Luong注意力的过程：
- en: Feed the encoder with the input sequence and compute the set of encoder hidden
    states [![](img/e077b276-6efe-457f-9515-1137e39fe84a.png)].
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入序列输入到编码器，并计算编码器隐藏状态集[![](img/e077b276-6efe-457f-9515-1137e39fe84a.png)]。
- en: Compute the decoder hidden state [![](img/15429df0-ec77-4db3-a7b0-fdba1e708867.png)] based
    on the previous decoder hidden state **s***[t-1]* and the previous decoder output *y[t-1 ]*(not
    the context vector, though).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于前一时刻的解码器隐藏状态**s***[t-1]*和前一时刻的解码器输出*y[t-1]*（但不是上下文向量），计算解码器隐藏状态[![](img/15429df0-ec77-4db3-a7b0-fdba1e708867.png)]。
- en: 'Compute the alignment scores [![](img/13b6047a-f0ae-48be-8286-48c486aa6214.png)],
    which use the decoder state from the current step **s***[t]*. Besides additive
    attention, the Luong attention paper also proposes two types of **multiplicative
    attention**:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算对齐分数[![](img/13b6047a-f0ae-48be-8286-48c486aa6214.png)]，它使用当前步骤的解码器状态**s***[t]*。除了加法注意力，Luong注意力的论文还提出了两种**乘法注意力**：
- en: '[![](img/da9a4ee6-5e97-4197-8d1a-b4889312b65f.png)]: The basic dot product
    without any parameters. In this case, the vectors **s** and **h** need to have
    the same sizes.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/da9a4ee6-5e97-4197-8d1a-b4889312b65f.png)]: 基本的点积没有任何参数。在这种情况下，向量**s**和**h**需要具有相同的大小。'
- en: '[![](img/00643d6d-4cb3-4a6f-a4d6-44ca0b9b31a2.png)]: Here, **W***[m]* is a
    trainable weight matrix of the attention layer.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/00643d6d-4cb3-4a6f-a4d6-44ca0b9b31a2.png)]: 这里，**W***[m]*是注意力层的可训练权重矩阵。'
- en: The multiplication of the vectors as an alignment score measurement has an intuitive
    explanation—as we mentioned in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, the dot product acts as a similarity
    measure between vectors. Therefore, if the vectors are similar (that is, aligned),
    the result of the multiplication will be a large value and the attention will
    be focused on the current *t,i* relationship.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的乘积作为对齐分数度量具有直观的解释——正如我们在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)《神经网络基础》一节中提到的，点积作为向量间相似度的度量。因此，如果向量相似（即对齐），乘积的结果将是一个大值，注意力将集中在当前的*t,i*关系上。
- en: Compute the weights ![](img/8b5847aa-c2ed-484e-b48c-68230e89b406.png).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算权重 ![](img/8b5847aa-c2ed-484e-b48c-68230e89b406.png)。
- en: Compute the context vector ![](img/52730a94-6d3f-4332-929e-966ca7ea7ccf.png).
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算上下文向量 ![](img/52730a94-6d3f-4332-929e-966ca7ea7ccf.png)。
- en: Compute the vector ![](img/d449d1ea-a54f-4e81-ac54-248fea0f30b8.png) based on
    the concatenated vectors **c***[t]* and **s***[t]*. At this point, we can compute
    the final output *y[t]*. In the case of classification, we'll use softmax ![](img/24925d51-6840-4e05-be25-d62447da0b5c.png),
    where **W***[y]* is a weight matrix.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于拼接后的向量**c***[t]*和**s***[t]*，计算向量 ![](img/d449d1ea-a54f-4e81-ac54-248fea0f30b8.png)。此时，我们可以计算最终输出*y[t]*。在分类的情况下，我们将使用softmax
    ![](img/24925d51-6840-4e05-be25-d62447da0b5c.png)，其中**W***[y]*是一个权重矩阵。
- en: Repeat steps 2–7 until the end of the sequence.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-7直到序列结束。
- en: Next, let's discuss some more attention variants. We'll start with **hard**
    and **soft attention**, which relates to the way we compute the context vector
    **c***[t]*. So far, we've described soft attention, where **c***[t]* is a weighted
    sum of all hidden states of the input sequence. With hard attention, we still
    compute the weights *α**[t,i]*, but we only take the hidden state **h***[imax]*
    with the maximum associated weight *α**[t,imax]*. Then, the selected state **h***[imax]*
    serves as the context vector. At first, hard attention seems a little counter-intuitive—after
    all this effort to enable the decoder to have access to all input states, why
    limit it to a single state again? However, hard attention was first introduced
    in the context of image-recognition tasks, where the input sequence represents
    different regions of the same image. In such cases, it makes more sense to choose
    between multiple regions or a single region. Unlike soft attention, hard attention
    is a stochastic process, which is nondifferentiable. Therefore, the backward phase
    uses some tricks to work (this goes beyond the scope of this book).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一些更多的注意力变体。我们将从**硬注意力**和**软注意力**开始，这与我们计算上下文向量 **c*[t]* 的方式有关。到目前为止，我们描述了软注意力，其中
    **c*[t]* 是输入序列所有隐藏状态的加权和。在硬注意力中，我们仍然计算权重 *α*[t,i]*，但只选择具有最大相关权重 *α*[t,imax]* 的隐藏状态
    **h*[imax]*。然后，选定的状态 **h*[imax]* 作为上下文向量。起初，硬注意力似乎有点违反直觉——毕竟为了使解码器能够访问所有输入状态，为什么再次限制到单个状态？然而，硬注意力最初是在图像识别任务的背景下引入的，在这些任务中，输入序列表示同一图像的不同区域。在这种情况下，选择多个区域或单个区域更有意义。与软注意力不同，硬注意力是一个随机过程，是不可微分的。因此，反向阶段使用一些技巧来工作（这超出了本书的范围）。
- en: '**Local attention** represents a compromise between soft and hard attention.
    Whereas these mechanisms take into account either all input hidden vectors (global)
    or just a single input vector, local attention takes a window of vectors, surrounding
    a given input sequence location, and then applies soft attention over this window
    only. But how do we determine the center of the window *p[t]* (known as the **aligned
    position**), based on the current output step *t*? The easiest way is to assume
    that the source and target sequences are roughly monotonically aligned—that is, to
    set *p[t] = t—*following the logic that the input and output sequence positions
    relate to the same thing.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**本地注意力**代表了软注意力和硬注意力之间的一种折衷。而这些机制要么考虑所有输入隐藏向量（全局），要么只考虑单个输入向量，本地注意力则会取一个窗口向量，包围给定的输入序列位置，然后只在此窗口上应用软注意力。但是我们如何确定窗口的中心
    *p[t]*（称为**对齐位置**），基于当前输出步骤 *t*？最简单的方式是假设源序列和目标序列大致单调对齐，即设定 *p[t] = t*，这是因为输入和输出序列位置相关联的逻辑。'
- en: Next, we'll summarize what we have learned so far by introducing a general form
    of the attention mechanism.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过介绍注意力机制的一般形式来总结我们到目前为止所学到的内容。
- en: General attention
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一般注意力
- en: Although we've discussed the attention mechanism in the context of NMT, it is
    a general deep-learning technique that can be applied to any seq2seq task. Let's
    assume that we are working with hard attention. In this case, we can think of
    the vector **s***[t-1]* as a **query** executed against a database of key-value
    pairs, where the **keys** are vectors and the hidden states **h***[i]* are the
    **values. **These are often abbreviated as **Q**, **K**, and **V**, and you can
    think of them as matrices of vectors. The keys **Q** and the values **V** of Luong
    and Bahdanau attention are the same vector—that is, these attention models are
    more like **Q**/**V**, rather than **Q**/**K**/**V**. The general attention mechanism
    uses all three components.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在NMT的背景下讨论了注意力机制，但它是一种通用的深度学习技术，可应用于任何序列到序列的任务。让我们假设我们正在使用硬注意力。在这种情况下，我们可以将向量
    **s*[t-1]* 视为针对键-值对数据库执行的**查询**，其中**键**是向量，隐藏状态 **h*[i]* 是 **值**。这些通常缩写为 **Q**、**K**
    和 **V**，您可以将它们视为向量矩阵。Luong 和 Bahdanau 注意力的键 **Q** 和值 **V** 是相同的向量——也就是说，这些注意力模型更像是
    **Q**/**V**，而不是 **Q**/**K**/**V**。一般的注意力机制使用了所有三个组件。
- en: 'The following diagram illustrates this new general attention:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了这种新的一般注意力：
- en: '![](img/c2cf2330-9fe7-4553-b2db-b6a0e1cbb4df.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2cf2330-9fe7-4553-b2db-b6a0e1cbb4df.png)'
- en: General attention
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一般注意力
- en: 'When we execute the query (**q** *=* **s***[t-1]*) against the database, we''ll
    receive a single match—the key **k***[imax ]*with the maximum weight *α**[t,imax]*.
    Hidden behind this key is the vector **v***[imax] = ***h***[imax]*, which is the
    actual value we''re interested in. But what about soft attention, where all values
    participate? We can think in the same query/key/value terms, but instead of a
    single value, the query results are all values with different weights. We can
    write a generalized attention formula (based on the context vector **c***[t]*
    formula) using the new notation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对数据库执行查询（**q** *=* **s***[t-1]*）时，我们将收到一个单一的匹配项——具有最大权重*α**[t,imax]*的键**k***[imax]*。隐藏在这个键背后的是向量**v***[imax]
    = ***h***[imax]*，它是我们真正感兴趣的实际值。那么，什么是软注意力，所有值都参与其中呢？我们可以用相同的查询/键/值的方式思考，但不同的是，查询结果是所有值，且它们具有不同的权重。我们可以使用新的符号写出一个广义的注意力公式（基于上下文向量**c***[t]*公式）：
- en: '![](img/bc2289ef-df6d-4c70-a593-7b6c7637f45d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc2289ef-df6d-4c70-a593-7b6c7637f45d.png)'
- en: In this generic attention, the queries, keys, and vectors of the database are
    not necessarily related in a sequential fashion. In other words, the database
    doesn't have to consist of the hidden RNN states at different steps, but could
    contain any kind of information instead. This concludes our introduction to the
    theory behind seq2seq models. We'll use this knowledge in the following section,
    where we'll implement a simple seq2seq NMT example.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个通用的注意力机制中，查询、键和数据库中的向量不一定按顺序相关。换句话说，数据库不必由不同步骤中的隐藏RNN状态组成，而可以包含任何类型的信息。这就结束了我们对seq2seq模型背后理论的介绍。我们将在接下来的部分中应用这些知识，那里我们将实现一个简单的seq2seq
    NMT示例。
- en: Implementing seq2seq with attention
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现带有注意力机制的seq2seq
- en: In this section, we'll use PyTorch 1.3.1 to implement a simple NMT example with
    the help of a seq2seq attention model. To clarify, we'll implement a seq2seq attention
    model, like the one we introduced in the *Introducing* *seq2seq models* section, and
    we'll extend it with Luong attention. The model encoder will take as input a text
    sequence (sentence) in one language and the decoder will output the corresponding
    sequence translated into another language.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用PyTorch 1.3.1，借助seq2seq注意力模型实现一个简单的NMT示例。为澄清起见，我们将实现一个seq2seq注意力模型，就像我们在*引入*
    *seq2seq模型*部分中介绍的那样，并将其扩展为Luong注意力。模型的编码器将作为输入处理一个语言中的文本序列（句子），而解码器将输出翻译成另一种语言的相应序列。
- en: We'll only show the most relevant parts of the code, but the full example is
    available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/nmt_rnn_attention](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/nmt_rnn_attention). This
    example is partially based on the PyTorch tutorial at [https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py](https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只展示代码中最相关的部分，但完整示例可以在[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/nmt_rnn_attention](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/nmt_rnn_attention)找到。这个示例部分基于PyTorch教程[https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py](https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py)。
- en: Let's start with the training set. It consists of a large list of sentences
    in both French and English, stored in a text file. The `NMTDataset `class (a subclass
    of `torch.utils.data.Dataset`) implements the necessary data preprocessing. It
    creates a vocabulary with integer indexes of all possible words in the dataset.
    For the sake of simplicity, we won't use embedding vectors, and we'll feed the
    words to the network with their numerical representation. Also, we won't split
    the dataset into training and testing parts, as our goal is to demonstrate the
    work of the seq2seq model. The `NMTDataset` class outputs source-target tuple
    sentences, where each sentence is represented by a 1D tensor of indexes of the
    words in that sentence.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练集开始。它包含一大批法语和英语的句子，存储在文本文件中。`NMTDataset`类（是`torch.utils.data.Dataset`的子类）实现了必要的数据预处理。它创建了一个包含数据集中所有可能单词的整数索引的词汇表。为了简化，我们不使用嵌入向量，而是将单词的数字表示形式输入到网络中。此外，我们不会将数据集拆分为训练集和测试集，因为我们的目标是展示seq2seq模型的工作原理。`NMTDataset`类输出源-目标句子的元组，其中每个句子由该句子中单词索引的1D张量表示。
- en: Implementing the encoder
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现编码器
- en: Next, let's continue with implementing the encoder.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续实现编码器。
- en: 'We''ll start with the constructor:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从构造函数开始：
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The entry point is the `self.embedding` module. It will take the index of each
    word and it will return its assigned embedding vector. We will not use pretrained
    word vectors (such as GloVe), but nevertheless, the concept of embedding vectors
    is the same—it's just that we'll initialize them with random values and we'll
    train them along the way with the rest of the model. Then, we have the `torch.nn.GRU`
    RNN cell itself.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 入口点是`self.embedding`模块。它将获取每个单词的索引，并返回其对应的嵌入向量。我们不会使用预训练的词向量（如GloVe），但嵌入向量的概念是相同的——只是我们将使用随机值初始化它们，并在训练过程中与模型的其余部分一起训练。然后，我们使用`torch.nn.GRU`
    RNN单元。
- en: 'Next, let''s implement the `EncoderRNN.forward` method (please bear in mind
    the indentation):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现`EncoderRNN.forward`方法（请注意缩进）：
- en: '[PRE1]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It represents the processing of a sequence element. First, we obtain the `embedded`
    word vector and then we feed it to the RNN cell.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示序列元素的处理。首先，我们获得`embedded`单词向量，然后将其输入到RNN单元中。
- en: 'We''ll also implement the `EncoderRNN.init_hidden` method, which creates an
    empty tensor with the same size as the hidden RNN state. This tensor serves as
    the first RNN hidden state at the beginning of the sequence (please bear in mind
    the indentation):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将实现`EncoderRNN.init_hidden`方法，该方法创建一个与隐藏RNN状态大小相同的空张量。这个张量作为序列开始时的第一个RNN隐藏状态（请注意缩进）：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we've implemented the encoder, let's continue with the decoder implementation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了编码器，接下来继续实现解码器。
- en: Implementing the decoder
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现解码器
- en: 'Let''s implement the `DecoderRNN` class—a basic decoder without attention.
    Again, we''ll start with the constructor:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现`DecoderRNN`类——一个没有注意力的基础解码器。同样，我们从构造函数开始：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It's similar to the encoder—we have the initial `self.embedding` word embedding
    and the `self.gru` GRU cell. We also have the fully connected `self.out` layer
    with `self.log_softmax` activation, which will output the predicted word in the
    sequence.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 它与编码器类似——我们有初始的`self.embedding`词嵌入和`self.gru` GRU单元。我们还有全连接的`self.out`层，配合`self.log_softmax`激活函数，将输出序列中预测的单词。
- en: 'We''ll continue with the `DecoderRNN.forward` method (please bear in mind the
    indentation):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续实现`DecoderRNN.forward`方法（请注意缩进）：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It starts with the `embedded` vector, which serves as input to the RNN cell.
    The module returns both its new `hidden` state and the `output` tensor, which
    represents the predicted word. The method accepts the void argument `_`, so it
    could match the interface of the attention decoder, which we'll implement in the
    next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它从`embedded`向量开始，作为RNN单元的输入。该模块返回它的新`hidden`状态和`output`张量，后者表示预测的单词。该方法接受空参数`_`，以便与我们将在下一节中实现的注意力解码器接口匹配。
- en: Implementing the decoder with attention
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现带注意力的解码器
- en: Next, we'll implement the `AttnDecoderRNN` decoder with Luong attention. This
    also works in combination with `EncoderRNN`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现带Luong注意力的`AttnDecoderRNN`解码器。它也可以与`EncoderRNN`一起使用。
- en: 'We''ll start with the `AttnDecoderRNN.__init__` method:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`AttnDecoderRNN.__init__`方法开始：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As usual, we have `self.embedding`, but this time, we''ll also add `self.dropout`
    to prevent overfitting. The fully connected `self.attn` and `self.w_c` layers relate
    to the attention mechanism, and we''ll learn how to use them when we look at the `AttnDecoderRNN.forward`
    method, which comes next. ` AttnDecoderRNN.forward` implements the Luong attention
    algorithm we described in the *Seq2seq with attention* section. Let''s start with
    the method declaration and parameter preprocessing:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们有`self.embedding`，但这次我们还将添加`self.dropout`来防止过拟合。全连接层`self.attn`和`self.w_c`与注意力机制相关，接下来我们将在查看`AttnDecoderRNN.forward`方法时学习如何使用它们。`AttnDecoderRNN.forward`实现了我们在*带注意力机制的Seq2seq*部分中描述的Luong注意力算法。我们从方法声明和参数预处理开始：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we''ll compute the current hidden state (`hidden` = **s***[t]*). Please
    bear in mind the indentation, as this code is still part of the `AttnDecoderRNN.forward` method:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算当前的隐藏状态（`hidden` = **s***[t]*）。请注意缩进，因为这段代码仍然是`AttnDecoderRNN.forward`方法的一部分：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we''ll compute the alignment scores (`alignment_scores` = *e[t,i]*),
    following the multiplicative attention formula. Here, `torch.mm` is the matrix
    multiplication and `encoder_outputs` is the encoder outputs (surprise!):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将计算对齐分数（`alignment_scores` = *e[t,i]*），遵循乘法注意力公式。在这里，`torch.mm`是矩阵乘法，`encoder_outputs`是编码器的输出（惊讶吧！）：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we''ll compute softmax over the scores to produce the attention weights
    (`attn_weights` = *a[t,i]*):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对分数计算softmax以产生注意力权重（`attn_weights` = *a[t,i]*）：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we''ll compute the context vector (`c_t` = **c***[t]*) following the
    attention formula:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将根据注意力公式计算上下文向量（`c_t` = **c***[t]*）：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we''ll compute the modified state vector (`hidden_s_t` = ![](img/e679dcd7-79cf-4eed-abe6-a71dc56c242f.png)) by
    concatenating the current hidden state and the context vector:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过连接当前的隐藏状态和上下文向量来计算修改后的状态向量（`hidden_s_t` = ![](img/e679dcd7-79cf-4eed-abe6-a71dc56c242f.png)）：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we''ll compute the next predicted word:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将计算下一个预测词：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We should note that `torch.nn.functional.log_softmax` applies the logarithm
    after a regular softmax. This activation function works in combination with the
    negative log-likelihood loss function `torch.nn.NLLLoss`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，`torch.nn.functional.log_softmax`在常规softmax之后应用对数。这个激活函数与负对数似然损失函数`torch.nn.NLLLoss`配合使用。
- en: 'Finally, the method returns `output`, `hidden`, and `attn_weights`. Later, we''ll
    use `attn_weights` to visualize the attention between the input and output sentences
    (the method `AttnDecoderRNN.forward` ends here):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，方法返回`output`、`hidden`和`attn_weights`。稍后，我们将使用`attn_weights`来可视化输入和输出句子之间的注意力（方法`AttnDecoderRNN.forward`在此结束）：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, let's look at the training process.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看训练过程。
- en: Training and evaluation
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估
- en: Next, let's implement the `train` function. It's similar to other such functions
    that we've implemented in previous chapters; however, it takes into account the
    sequential nature of the input and the teacher forcing principle we described
    in the *Seq2eq with attention* section. For the sake of simplicity, we'll only
    train with a single sequence at a time (a mini batch of size 1).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现`train`函数。它类似于我们在前几章中实现的其他函数；然而，它考虑了输入的顺序性和我们在*Seq2eq with attention*部分描述的教师强制原则。为了简化，我们将一次只训练一个序列（大小为1的小批量）。
- en: 'First, we''ll initiate the iteration over the training set, set up initial
    sequence tensors, and reset the gradients:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将启动训练集的迭代，设置初始序列张量，并重置梯度：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The encoder and decoder parameters are instances of `EncoderRNN` and `AttnDecoderRNN`
    (or `DecoderRNN`), `loss_function` represents the loss (in our case, `torch.nn.NLLLoss`), `encoder_optimizer`
    and `decoder_optimizer` (the names speak for themselves) are instances of `torch.optim.Adam`,
    and `data_loader` is a `torch.utils.data.DataLoader`, which wraps an instance
    of `NMTDataset`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器的参数是`EncoderRNN`和`AttnDecoderRNN`（或`DecoderRNN`）的实例，`loss_function`表示损失（在我们这里是`torch.nn.NLLLoss`），`encoder_optimizer`和`decoder_optimizer`（名称不言自明）是`torch.optim.Adam`的实例，而`data_loader`是一个`torch.utils.data.DataLoader`，它包装了一个`NMTDataset`实例。
- en: 'Next, we''ll do the actual training:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行实际的训练：
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s discuss this in more detail:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论一下：
- en: We feed the full sequence to the encoder and save the hidden states in the `encoder_outputs`
    list.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将完整的序列输入给编码器，并将隐藏状态保存在`encoder_outputs`列表中。
- en: We initiate the decoder sequence with `GO_token` as input.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过`GO_token`作为输入启动解码器序列。
- en: We use the decoder to generate new elements of the sequence. Following the teacher
    forcing principle, the `decoder` input at each step comes from the real target
    sequence `decoder_input = target_tensor[di]`.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用解码器生成序列的新元素。遵循教师强制原则，`decoder`在每个步骤的输入来自实际的目标序列`decoder_input = target_tensor[di]`。
- en: We train the encoder and decoder with `encoder_optimizer.step()` and `decoder_optimizer.step()`,
    respectively.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们分别使用`encoder_optimizer.step()`和`decoder_optimizer.step()`训练编码器和解码器。
- en: 'Similar to `train`, we have an `evaluate` function, which takes an input sequence
    and returns its translated counterpart and its accompanying attention scores.
    We won''t include the full implementation here, but we''ll focus on the encoder/decoder
    part. Instead of teacher forcing, the `decoder` input at each step is the output
    word of the previous step:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`train`，我们有一个`evaluate`函数，它接受一个输入序列并返回其翻译结果和相应的注意力分数。我们不会在这里包括完整的实现，而是关注编码器/解码器部分。不同于教师强制，`decoder`在每个步骤的输入是前一步的输出词：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When we run the full program, it will display several example translations.
    It will also display a map of the attention scores between the elements of the
    input and output sequences, such as the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行完整的程序时，它将显示几个示例翻译。它还会显示输入和输出序列之间元素的注意力分数图，像下面这样：
- en: '![](img/57dbbe1f-33ff-4d26-878f-43736c8bbdbc.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57dbbe1f-33ff-4d26-878f-43736c8bbdbc.png)'
- en: Translation attention scores
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译注意力分数
- en: For example, we can see that the output word **she** focuses its attention to
    the input word **elle** (*she* in French). If we didn't have the attention mechanism
    and only relied on the last encoder hidden state to initiate the translation,
    the output could have been **She's five years younger than me** just as easily.
    Since the word **elle** is furthest away from the end of the sentence, it would
    have been hard to encode it within the last encoder hidden state alone.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到输出单词 **she** 将注意力集中在输入单词 **elle** 上（*she* 在法语中是“她”）。如果没有注意力机制，仅依赖最后的编码器隐藏状态来发起翻译，输出可能也会是
    **She's five years younger than me**。由于单词 **elle** 离句子的末尾最远，单靠最后一个编码器隐藏状态很难编码它。
- en: In the next section, we'll leave the RNNs behind and we'll introduce the transformer—a
    seq2seq model, based solely on the attention mechanism.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将告别 RNN，并引入 Transformer——一个完全基于注意力机制的 seq2seq 模型。
- en: Understanding transformers
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Transformer
- en: We spent the better part of this chapter touting the advantages of the attention
    mechanism. But we still use attention in the context of RNNs—in that sense, it
    works as an addition on top of the core recurrent nature of these models. Since
    attention is so good, is there a way to use it on its own without the RNN part?
    It turns out that there is. The paper *Attention is all you need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))
    introduces a new architecture called **transformer** with encoder and decoder
    that relies solely on the attention mechanism. First, we'll focus our attention
    on the transformer attention (pun intended) mechanism.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分时间里，我们强调了注意力机制的优势。但我们依然在 RNN 的背景下使用注意力——从这个意义上讲，注意力机制是对这些模型核心递归特性的补充。既然注意力机制这么强大，是否可以单独使用它，而不需要
    RNN 部分呢？事实证明，答案是肯定的。论文 *Attention is all you need*（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）介绍了一种全新的架构——**Transformer**，它依赖于仅有的注意力机制，包含编码器和解码器。首先，我们将重点关注
    Transformer 注意力机制（此处有双关意味）。
- en: The transformer attention
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 注意力机制
- en: 'Before focusing on the entire model, let''s take a look at how the transformer
    attention is implemented:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在专注于整个模型之前，先来看看 Transformer 注意力是如何实现的：
- en: '![](img/2b40b98c-6f83-42e5-b820-385222bc3d9e.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b40b98c-6f83-42e5-b820-385222bc3d9e.png)'
- en: 'Left: Scaled dot product (multiplicative) attention; right: Multihead attention; source: https://arxiv.org/abs/1706.03762'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：缩放点积（乘法）注意力；右图：多头注意力；来源：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: 'The transformer uses dot product attention (the left-hand side diagram of the
    preceding diagram), which follows the general attention procedure we introduced
    in the *Seq2seq with attention* section (as we have already mentioned, it is not
    restricted to RNN models). We can define it with the following formula:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 使用点积注意力（前面图示的左侧图），遵循我们在 *带有注意力机制的 Seq2seq* 章节中介绍的通用注意力过程（正如我们已经提到的，这并不限于
    RNN 模型）。我们可以通过以下公式定义它：
- en: '![](img/bd082d35-cc07-4e3a-990d-b8ae2c53ed87.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd082d35-cc07-4e3a-990d-b8ae2c53ed87.png)'
- en: 'In practice, we''ll compute the attention function over a set of queries simultaneously,
    packed in a matrix **Q**. In this scenario, the keys **K**, the values **V**,
    and the result are also matrices. Let''s discuss the steps of the formula in more
    detail:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们将同时对一组查询进行注意力计算，并将它们打包成一个矩阵 **Q**。在这种情况下，键 **K**、值 **V** 和结果也都是矩阵。让我们更详细地讨论公式中的步骤：
- en: 'Match the query **Q** and the database (keys **K**) with matrix multiplication
    to produce the alignment scores ![](img/8e75d95c-8656-4199-8c56-132eba392711.png).
    Let''s assume that we want to match *m* different queries to a database of *n*
    values and the query-key vector length is *d[k]*. Then, we have the matrix ![](img/24eda6ec-aad0-4707-b605-6d259b18c5f0.png)with
    one *d[k]*-dimensional query per row for *m* total rows. Similarly, we have the
    matrix ![](img/8ae0030f-f2ea-4afd-9f1b-89a992250772.png) with one *d[k]*-dimensional
    key per row for *n* total rows. Then, the output matrix will have ![](img/ddc8548b-714a-461f-baac-c51a1e44f9f1.png),
    where one row contains the alignment scores of a single query over all keys of
    the database:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过矩阵乘法将查询 **Q** 与数据库（键 **K**）匹配，生成对齐得分 ![](img/8e75d95c-8656-4199-8c56-132eba392711.png)。假设我们要将
    *m* 个不同的查询与包含 *n* 个值的数据库进行匹配，并且查询-键向量的长度是 *d[k]*。那么，我们得到矩阵 ![](img/24eda6ec-aad0-4707-b605-6d259b18c5f0.png)，其中每行包含一个
    *d[k]* 维的查询，共 *m* 行。类似地，我们得到矩阵 ![](img/8ae0030f-f2ea-4afd-9f1b-89a992250772.png)，每行包含一个
    *d[k]* 维的键，共 *n* 行。然后，输出矩阵将是 ![](img/ddc8548b-714a-461f-baac-c51a1e44f9f1.png)，其中每行包含一个查询在所有数据库键上的对齐得分。
- en: '![](img/3be3cb9d-c46e-48d5-a46d-198ee13549ae.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3be3cb9d-c46e-48d5-a46d-198ee13549ae.png)'
- en: In other words, we can match multiple queries against multiple database keys
    in a single matrix-matrix multiplication. In the context of NMT, we can compute
    the alignment scores of all words of the target sentence over all words of the
    source sentence in the same way.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以通过一次矩阵-矩阵乘法将多个查询与多个数据库键匹配。在NMT的背景下，我们可以用相同的方式计算目标句子所有词与源句子所有词之间的对齐分数。
- en: Scale the alignment scores with ![](img/c494b8f8-a1fd-45e5-a1d6-affc6656a87a.png),
    where *d[k]* is the vector size of the key vectors in the matrix **K**, which
    is also equal to the size of the query vectors in **Q** (analogously, *d[v]* is
    the vector size of the key vectors **V**). The authors of the paper suspect that
    for large values of *d[k]*, the dot product grows large in magnitude and pushes
    the softmax in regions with extremely small gradients, which leads to the infamous
    vanishing gradients problem, hence the need to scale the results.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用![](img/c494b8f8-a1fd-45e5-a1d6-affc6656a87a.png)来缩放对齐分数，其中*d[k]*是矩阵**K**中键向量的向量大小，它也等于**Q**中查询向量的大小（类似地，*d[v]*是键向量**V**的向量大小）。论文的作者怀疑，对于较大的*d[k]*值，点积的幅度会增大，导致softmax在极小梯度的区域内，这就引发了著名的梯度消失问题，因此需要对结果进行缩放。
- en: 'Compute the attention scores with the softmax operation along the rows of the
    matrix (we''ll talk about the mask operation later):'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用softmax操作计算注意力分数（稍后我们会讨论掩码操作）：
- en: '![](img/2a1a4321-f434-4fa4-8e63-5d7f839cd57d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a1a4321-f434-4fa4-8e63-5d7f839cd57d.png)'
- en: Compute the final attention vector by multiplying the attention scores with the
    values **V***:*
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将注意力分数与值**V**相乘来计算最终的注意力向量**V***:*
- en: '![](img/d5492e4a-d60b-49c0-8f3e-0b8736697dcb.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5492e4a-d60b-49c0-8f3e-0b8736697dcb.png)'
- en: We can adapt this mechanism to work with both hard and soft attention.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整这一机制，使其同时适用于硬注意力和软注意力。
- en: 'The authors also propose **multihead attention **(see the right-hand side diagram
    of the preceding diagram). Instead of a single attention function with *d[model]*-dimensional
    keys, we linearly project the keys, queries, and values *h* times to produce *h*
    different *d[k]-*, *d[k]-*, and *d[v]-*dimensional projections of these values.
    Then, we apply separate parallel attention functions (or heads) over the newly
    created vectors, which yield a single *d[v]*-dimensional output for each head.
    Finally, we concatenate the head outputs to produce the final attention result.
    Multihead attention allows each head to attend to different elements of the sequence.
    At the same time, the model combines the outputs of the heads in a single cohesive
    representation. More precisely, we can define this with the following formula:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还提出了**多头注意力**（见前面图示的右侧图）。我们不是使用单一的注意力函数和*d[model]*维度的键，而是将键、查询和值线性投影*h*次，产生*h*个不同的*d[k]*、*d[k]*和*d[v]*维度的这些值的投影。然后，我们对新创建的向量应用独立的并行注意力函数（或头部），每个头部产生一个*d[v]*维度的输出。最后，我们将头部输出拼接在一起，产生最终的注意力结果。多头注意力使每个头部可以关注序列的不同元素，同时，模型将头部的输出结合成一个单一的、统一的表示。更准确地说，我们可以用以下公式定义它：
- en: '![](img/1e7fc963-7dde-4a07-92dc-79b54b4ac1af.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e7fc963-7dde-4a07-92dc-79b54b4ac1af.png)'
- en: 'Let''s look at this in more detail, starting with the heads:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这个过程，从头部开始：
- en: Each head receives the linearly projected versions of the initial **Q**, **K**,
    and **V**. The projections are computed with the learnable weight matrices **W***[i]^Q*, **W***[i]^K*,
    and **W***[i]^V* respectively. Note that we have a separate set of weights for
    each component (**Q**, **K**, **V**) and for each head *i*. To satisfy the transformation
    from *d[model]* to and *d[k]* and *d[v]*, the dimensions of these matrices are
    ![](img/3e533bd3-439d-473e-907b-9c257ed57d38.png),![](img/bf9a3dd8-8d40-4d35-b6bf-68434a50e2de.png),
    and ![](img/4ef902f8-bfe6-4a4a-a979-fcdd6c51b215.png).
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个头部接收初始**Q**、**K**和**V**的线性投影版本。投影是通过可学习的权重矩阵**W** *[i]^Q*、**W** *[i]^K*和**W**
    *[i]^V*分别计算的。注意，我们为每个组件（**Q**、**K**、**V**）和每个头部*i*都有一组独立的权重。为了满足从*d[model]*到*d[k]*和*d[v]*的转换，这些矩阵的维度是![](img/3e533bd3-439d-473e-907b-9c257ed57d38.png)、![](img/bf9a3dd8-8d40-4d35-b6bf-68434a50e2de.png)和![](img/4ef902f8-bfe6-4a4a-a979-fcdd6c51b215.png)。
- en: Once **Q**, **K**, and **V** are transformed, we can compute the attention of
    each head using the regular attention model we described at the beginning of this
    section.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦**Q**、**K**和**V**被转换，我们就可以使用我们在本节开头描述的常规注意力模型来计算每个头部的注意力。
- en: The final attention result is the linear projection (the weight matrix **W***^O*
    of learnable weights) over the concatenated head outputs head[i].
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的注意力结果是对连接的头输出 head[i] 进行线性投影（可学习权重的权重矩阵 **W***^O*）。
- en: 'So far, we''ve demonstrated attention for different input and output sequences.
    For example, we''ve seen that in NMT each word of the translated sentence relates
    to the words of the source sentence. The transformer model also relies on **self-attention**
    (or intra-attention), where the query **Q** belongs to the same dataset as the
    keys **K** and vectors **V** of the query database. In other words, in self-attention,
    the source and the target are the same sequence (in our case, the same sentence).
    The benefit of self-attention is not immediately obvious, as there is no direct
    task to apply it to. On an intuitive level, it allows us to see the relationship
    between words of the same sequence. For example, the following diagram shows the
    multihead self-attention of the verb *making* (different colors represent different
    heads). Many of the attention heads attend to a distant dependency of *making*,
    completing the phrase *making ... more difficult*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经展示了不同输入和输出序列的注意力。例如，我们已经看到，在神经机器翻译（NMT）中，翻译句子的每个词与源句子的词相关。Transformer
    模型也依赖于**自注意力**（或内部注意力），其中查询 **Q** 来自与键 **K** 和查询数据库的向量 **V** 相同的数据集。换句话说，在自注意力中，源和目标是相同的序列（在我们的例子中，是同一句话）。自注意力的好处并不立即显现，因为没有直接的任务可以应用它。从直观层面上看，它让我们能够看到同一序列中词与词之间的关系。例如，以下图示展示了动词
    *making* 的多头自注意力（不同颜色代表不同的头）。许多注意力头关注到 *making* 的远程依赖，完成了短语 *making ... more difficult*：
- en: '![](img/21108f32-fd44-4c77-bcc4-45213f630e0b.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21108f32-fd44-4c77-bcc4-45213f630e0b.png)'
- en: An example of multihead self-attention. Source: https://arxiv.org/abs/1706.03762
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力的示例。来源： [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: The transformer model uses self-attention as a replacement of the encoder/decoder
    RNNs, but more on that in the next section.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型使用自注意力来替代编码器/解码器 RNNs，但更多内容将在下一节中介绍。
- en: The transformer model
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 模型
- en: 'Now that we are familiar with multihead attention, let''s focus on the full
    transformer model, starting with the following diagram:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了多头注意力，让我们集中关注完整的 Transformer 模型，从以下图示开始：
- en: '![](img/362b20db-2833-4ecb-b33c-28711065b014.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/362b20db-2833-4ecb-b33c-28711065b014.png)'
- en: The transformer model architecture. The left-hand side shows the encoder and
    the right-hand side shows the decoder; source: https://arxiv.org/abs/1706.03762
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型架构。左侧显示编码器，右侧显示解码器；来源：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: 'It looks scary, but fret not—it''s easier than it seems. We''ll start with
    the encoder (the left-hand component of the preceding diagram):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很复杂，但别担心——它比看起来要简单。我们从编码器（前图的左侧组件）开始：
- en: It begins with an input sequence of one-hot-encoded words, which are transformed
    into *d[model]*-dimensional embedding vectors. The embedding vectors are further
    multiplied by ![](img/5ec0c397-cafb-43e7-92c8-0e5f2accc144.png).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从一个经过独热编码的词序列开始，这些词被转化为*d[model]*维的嵌入向量。嵌入向量进一步与![](img/5ec0c397-cafb-43e7-92c8-0e5f2accc144.png)相乘。
- en: 'The transformer doesn''t use RNNs, and therefore, it has to convey the positional
    information of each sequence element in some other way. We can do this explicitly
    by augmenting each embedding vector with positional encoding. In short, the positional
    encoding is a vector with the same length *d[model ]*as the embedding vector.
    The positional vector is added (elementwise) to the embedding vector and the result
    is propagated further in the encoder. The authors of the paper introduce the following
    function for each element *i* of the positional vector, when the current word
    has the position *pos* in the sequence:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 不使用 RNNs，因此它必须以其他方式传递每个序列元素的位置信息。我们可以通过显式地将每个嵌入向量与位置编码相结合来做到这一点。简而言之，位置编码是一个与嵌入向量具有相同长度*d[model]*的向量。位置向量与嵌入向量按元素相加，结果在编码器中进一步传播。论文的作者为位置向量的每个元素
    *i* 引入了以下函数，当当前词在序列中的位置为 *pos* 时：
- en: '![](img/f608ed47-8a47-4419-af06-cc22dbb6d3ff.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f608ed47-8a47-4419-af06-cc22dbb6d3ff.png)'
- en: Each dimension of the positional encoding corresponds to a sinusoid. The wavelengths
    form a geometric progression from 2π to 10000 · 2π. The authors hypothesize that
    this function would allow the model to easily learn to attend by relative positions,
    since, for any fixed offset *k*, *PE[pos+k]* can be represented as a linear function
    of *PE[pos]*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码的每个维度对应一个正弦波。波长从 2π 到 10000 · 2π 形成一个几何级数。作者假设这个函数可以让模型轻松地学会按相对位置进行注意力操作，因为对于任何固定的偏移量
    *k*，*PE[pos+k]* 可以表示为 *PE[pos]* 的线性函数。
- en: 'The rest of the encoder is composed of a stack of *N = 6* identical blocks.
    Each block has two sublayers:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的其余部分由一堆 *N = 6* 个相同的块组成。每个块有两个子层：
- en: A multihead self-attention mechanism, like the one we described in the section
    titled *The transformer attention*. Since the self-attention mechanism works across
    the whole input sequence, the encoder is **bidirectional** by design. Some algorithms
    use only the encoder transformer part and are referred to as **transformer encoder**.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多头自注意力机制，类似我们在《*Transformer 注意力*》一节中描述的。由于自注意力机制跨整个输入序列工作，因此编码器是**双向**的设计。一些算法仅使用编码器部分，并被称为**Transformer
    编码器**。
- en: 'A simple, fully connected, feed-forward network, which is defined by the following
    formula:'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的全连接前馈网络，其定义公式如下：
- en: '![](img/b6c24f77-e176-47d3-ac92-12e1e910c157.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6c24f77-e176-47d3-ac92-12e1e910c157.png)'
- en: The network is applied to each sequence element *x* separately. It uses the
    same set of parameters (**W***[1]*, **W***[2]*, *b[1]*, and *b[2]*) across different
    positions, but different parameters across the different encoder blocks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 网络应用于每个序列元素 *x*，并独立处理。它在不同位置使用相同的参数集（**W***[1]*，**W***[2]*，*b[1]*，和 *b[2]*），但在不同的编码器块中使用不同的参数。
- en: 'Each sublayer (both the multihead attention and feed-forward network) has a
    residual connection around itself and ends with normalization over the sum of
    that connection and its own output and the residual connection. Therefore, the
    output of each sublayer is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子层（无论是多头注意力还是前馈网络）都有一个围绕其自身的残差连接，并以归一化结束，归一化的对象是该连接及其自身输出与残差连接的和。因此，每个子层的输出如下：
- en: '![](img/59208993-0830-4b6b-b328-6a6198dff280.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59208993-0830-4b6b-b328-6a6198dff280.png)'
- en: The normalization technique is described in the paper *Layer Normalization*
    ([https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化技术在论文《*层归一化*》中有详细描述（[https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)）。
- en: 'Next, let''s look at the decoder, which is somewhat similar to the encoder:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看解码器，它与编码器有些相似：
- en: The input at step *t* is the decoder's own predicted output word at step *t-1*.
    The input word uses the same embedding vectors and positional encoding as the
    encoder.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤 *t* 的输入是解码器在步骤 *t-1* 的预测输出词。输入词使用与编码器相同的嵌入向量和位置编码。
- en: 'The decoder continues with a stack of *N = 6* identical blocks, which are somewhat
    similar to the encoder blocks. Each block consists of three sublayers and each
    sublayer employs residual connections and normalization. The sublayers are:'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器继续使用一堆 *N = 6* 个相同的块，这些块与编码器块有些相似。每个块由三个子层组成，且每个子层都采用残差连接和归一化。子层包括：
- en: 'A multihead self-attention mechanism. The encoder self-attention can attend
    to all elements of the sequence, regardless of whether they come before or after
    the target element. But the decoder has only a partially generated target sequence.
    Therefore, the self-attention here can only attend to the preceding sequence elements. This
    is implemented by **masking out** (setting to −∞) all values in the input of the
    softmax, which correspond to illegal connections:'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多头自注意力机制。编码器的自注意力可以关注序列中的所有元素，无论它们是出现在目标元素之前还是之后。但解码器只有部分生成的目标序列。因此，这里的自注意力只能关注前面的序列元素。这通过**屏蔽**（将输入中的非法连接设置为
    −∞）实现，屏蔽的是 softmax 输入中所有非法连接对应的值：
- en: '![](img/537f17ea-f0a1-445f-ad32-d658dc974616.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/537f17ea-f0a1-445f-ad32-d658dc974616.png)'
- en: The masking makes the decoder **unidirectional** (unlike the bidirectional encoder).
    Algorithms that work with the decoder are referred to as **transformer decoder
    algorithms**.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 屏蔽操作使解码器变为**单向**（与双向编码器不同）。与解码器相关的算法被称为**Transformer 解码器算法**。
- en: A regular attention mechanism, where the queries come from the previous decoder
    layer, and the keys and values come from the previous sublayer, which represents
    the processed decoder output at step *t-1*. This allows every position in the
    decoder to attend over all positions in the input sequence. This mimics the typical
    encoder-decoder attention mechanisms, which we discussed in the *Seq2seq with
    attention* section.
  id: totrans-211
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常规的注意力机制，其中查询来自前一个解码器层，键和值来自前一个子层，表示在步骤*t-1*时处理过的解码器输出。这使得解码器中的每个位置都能关注输入序列中的所有位置。这模拟了典型的编码器-解码器注意力机制，我们在*Seq2seq
    with attention*部分中讨论过。
- en: A feed-forward network, which is similar to the one in the encoder.
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个前馈网络，类似于编码器中的网络。
- en: The decoder ends with a fully connected layer, followed by a softmax, which
    produces the most probable next word of the sentence.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器以一个全连接层结束，接着是一个softmax层，用于生成最有可能的下一个词。
- en: The transformer uses dropout as a regularization technique. It adds dropout
    to the output of each sublayer before it is added to the sublayer input and normalized.
    It also applies dropout to the sums of the embeddings and the positional encodings
    in both the encoder and decoder stacks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer使用dropout作为正则化技术。在每个子层的输出添加dropout之前，它会先将dropout添加到子层输入，并进行归一化。它还会对编码器和解码器堆栈中嵌入向量和位置编码的和应用dropout。
- en: Finally, let's summarize the benefits of self-attention over the RNN attention
    models we discussed in the *Seq2seq with attention* section. The key advantage
    of the self-attention mechanism is the immediate access to all elements of the
    input sequence, as opposed to the bottleneck thought vector of the RNN models.
    Additionally—the following is a direct quote from the paper—a self-attention layer
    connects all positions with a constant number of sequentially executed operations,
    whereas a recurrent layer requires *O(n)* sequential operations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们总结一下自注意力相较于我们在*Seq2seq with attention*部分讨论的RNN注意力模型的优势。自注意力机制的关键优点是可以直接访问输入序列的所有元素，而不像RNN模型那样有瓶颈的思维向量。此外——以下是来自论文的直接引用——自注意力层通过恒定数量的顺序执行操作连接所有位置，而递归层则需要*O(n)*次顺序操作。
- en: 'In terms of computational complexity, self-attention layers are faster than
    recurrent layers when the sequence length *n* is smaller than the representation
    dimensionality *d*, which is most often the case with sentence representations
    used by state-of-the-art models in machine translations, such as word-piece (see *Google''s
    Neural Machine Translation System: Bridging the Gap between Human and Machine
    Translation* at [https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    and byte-pair (see *Neural Machine Translation of Rare Words with Subword Units*
    at [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)) representations.
    To improve computational performance for tasks involving very long sequences,
    self-attention could be restricted to considering only a neighborhood of size
    *r* in the input sequence centered around the respective output position.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '从计算复杂度上来看，当序列长度*n*小于表示维度*d*时，自注意力层比递归层更快，这在机器翻译中，像word-piece（见*Google''s Neural
    Machine Translation System: Bridging the Gap between Human and Machine Translation*，[https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144)）和byte-pair（见*Neural
    Machine Translation of Rare Words with Subword Units*，[https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)）表示法的句子表示中最为常见。为了提高处理非常长序列的计算性能，自注意力机制可以限制只考虑输入序列中围绕各自输出位置的大小为*r*的邻域。'
- en: This concludes our theoretical description of transformers. In the next section,
    we'll implement a transformer from scratch.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对transformer的理论描述。在下一部分，我们将从头实现一个transformer。
- en: Implementing transformers
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现transformers
- en: 'In this section, we''ll implement the transformer model with the help of PyTorch
    1.3.1\. As the example is relatively complex, we''ll simplify it by using a basic
    training dataset: we''ll train the model to copy a randomly generated sequence
    of integer values—that is, the source and the target sequence are the same and
    the transformer will learn to replicate the input sequence as the output. We won''t
    include the full source code, but you can find it at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/transformer.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/transformer.py).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将借助 PyTorch 1.3.1 实现 transformer 模型。由于示例相对复杂，我们将通过使用一个基本的训练数据集来简化它：我们将训练模型复制一个随机生成的整数值序列——也就是说，源序列和目标序列是相同的，transformer
    将学习将输入序列复制为输出序列。我们不会包括完整的源代码，但你可以在 [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/transformer.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter08/transformer.py)
    找到它。
- en: This example is based on [https://github.com/harvardnlp/annotated-transformer](https://github.com/harvardnlp/annotated-transformer).
    Let's also note that PyTorch 1.2 has introduced native transformer modules (the
    documentation is available at [https://pytorch.org/docs/master/nn.html#transformer-layers](https://pytorch.org/docs/master/nn.html#transformer-layers)).
    Still, in this section we'll implement the transformer from scratch to understand
    it better.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例基于 [https://github.com/harvardnlp/annotated-transformer](https://github.com/harvardnlp/annotated-transformer)。还需要注意的是，PyTorch
    1.2 已经引入了原生的 transformer 模块（文档可以在 [https://pytorch.org/docs/master/nn.html#transformer-layers](https://pytorch.org/docs/master/nn.html#transformer-layers)
    中找到）。尽管如此，在本节中，我们将从头开始实现 transformer 以便更好地理解它。
- en: 'First, we''ll start with the utility function `clone`, which takes an instance
    of `torch.nn.Module` and produces `n` identical deep copies of the same module
    (excluding the original source instance):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从实用函数`clone`开始，该函数接受`torch.nn.Module`的实例并生成`n`个相同的深拷贝（不包括原始源实例）：
- en: '[PRE17]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With this short introduction, let's continue with the implementation of multihead
    attention.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这简短的介绍，让我们继续实现多头注意力。
- en: Multihead attention
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力
- en: 'In this section, we''ll implement multihead attention by following the definitions
    from the *The transformer attention* section. We''ll start with the implementation
    of the regular scaled dot product attention:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将按照*Transformer Attention*部分的定义实现多头注意力。我们将从实现常规的缩放点积注意力开始：
- en: '[PRE18]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As a reminder, this function implements the formula [![](img/f1f24071-0946-4744-8b95-de6569c39aca.png)],
    where **Q** = `query`, **K** = `key`, and **V** = `value`. If a `mask` is available,
    it will also be applied.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这个函数实现了公式 [![](img/f1f24071-0946-4744-8b95-de6569c39aca.png)]，其中 **Q**
    = `query`，**K** = `key`，**V** = `value`。如果有 `mask`，它也会被应用。
- en: 'Next, we''ll implement the multihead attention mechanism as `torch.nn.Module`.
    As a reminder, the implementation follows the following formula:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将作为 `torch.nn.Module` 实现多头注意力机制。作为提醒，实现遵循以下公式：
- en: '![](img/46de1649-0510-4bba-9367-953964022efd.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46de1649-0510-4bba-9367-953964022efd.png)'
- en: 'We''ll start with the `__init__` method:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 `__init__` 方法开始：
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we use the `clones` function to create four identical, fully connected
    `self.fc_layers`. We'll use three of them for the **Q**/**K**/**V** linear projections—
    [![](img/6c2d3e70-f1e8-4854-a624-e9390c0427db.png)]. The fourth fully connected
    layer is to merge the concatenated results of the outputs of the different heads
    **W***^O*. We'll store the current attention results in the `self.attn` property.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用 `clones` 函数来创建四个相同的全连接 `self.fc_layers`。我们将使用其中三个进行 **Q**/**K**/**V**
    的线性投影——[![](img/6c2d3e70-f1e8-4854-a624-e9390c0427db.png)]。第四个全连接层用于合并不同头的输出结果的拼接
    **W***^O*。我们将当前的注意力结果存储在 `self.attn` 属性中。
- en: 'Next, let''s implement the `MultiHeadedAttention.forward` method (please bear
    in mind the indentation):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现`MultiHeadedAttention.forward`方法（请注意缩进）：
- en: '[PRE20]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We iterate over the **Q**/**K**/**V** vectors and their reference projection
    `self.fc_layers` and produce the **Q**/**K**/**V** `projections` with the following
    snippet:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历 **Q**/**K**/**V** 向量及其参考投影 `self.fc_layers`，并使用以下代码片段生成 **Q**/**K**/**V**
    的 `projections`：
- en: '[PRE21]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Then, we apply the regular attention over the projections using the `attention`
    function we first defined, and finally, we concatenate the outputs of multiple
    heads and return the results. Now that we've implemented multihead attention,
    let's continue by implementing the encoder.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用我们首次定义的 `attention` 函数对投影应用常规注意力，最后，我们将多个头的输出拼接并返回结果。现在我们已经实现了多头注意力，让我们继续实现编码器。
- en: Encoder
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: 'In this section, we''ll implement the encoder, which is composed of several
    different subcomponents. Let''s start with the main definition and then dive into
    more details:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现编码器，编码器由多个不同的子组件组成。我们先从主要定义开始，然后深入更多的细节：
- en: '[PRE22]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It is fairly straightforward: the encoder is composed of `self.blocks`: `N`
    stacked instances of `EncoderBlock`, where each serves as input for the next.
    They are followed by `LayerNorm` normalization `self.norm` (we discussed these
    concepts in the *The transformer model* section). The `forward` method takes as
    input the data tensor `x` and an instance of `mask`, which blocks some of the
    input sequence elements. As we discussed in the *The transformer model* section,
    the mask is only relevant to the decoder part of the model, where the future elements
    of the sequence are not available yet. In the encoder, the mask exists only as
    a placeholder.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单：编码器由 `self.blocks` 组成：`N` 个堆叠的 `EncoderBlock` 实例，每个实例作为下一个的输入。它们后面跟着 `LayerNorm`
    归一化 `self.norm`（我们在 *Transformer模型* 部分讨论过这些概念）。`forward` 方法的输入是数据张量 `x` 和 `mask`
    的实例，后者会屏蔽掉一些输入序列元素。正如我们在 *Transformer模型* 部分讨论的那样，mask 仅与模型的解码器部分相关，其中序列的未来元素尚不可用。在编码器中，mask
    仅作为占位符存在。
- en: 'We''ll omit the definition of `LayerNorm` (it''s enough to know that it''s
    a normalization at the end of the encoder) and we''ll focus on `EncoderBlock`
    instead:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将省略 `LayerNorm` 的定义（只需知道它是编码器末尾的归一化），我们将专注于 `EncoderBlock`：
- en: '[PRE23]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As a reminder, each encoder block consists of two sublayers (`self.sublayers`
    instantiated with the familiar `clones` function): a multihead self-attention
    `self_attn` (an instance of `MultiHeadedAttention`), followed by a simple fully
    connected network `ffn` (an instance of `PositionwiseFFN`). Each sublayer is wrapped
    by its residual connection, which is implemented with the `SublayerConnection`
    class:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，每个编码器块由两个子层组成（`self.sublayers` 使用熟悉的 `clones` 函数实例化）：一个多头自注意力 `self_attn`（`MultiHeadedAttention`
    的实例），后跟一个简单的全连接网络 `ffn`（`PositionwiseFFN` 的实例）。每个子层都被它的残差连接包装，残差连接是通过 `SublayerConnection`
    类实现的：
- en: '[PRE24]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The residual connection also includes normalization and dropout (according to
    the definition). As a reminder, it follows the formula [![](img/56ef0b5c-6430-44ef-b14b-aadb5badca9f.png)],
    but for code simplicity, the `self.norm` comes first rather than last. The `SublayerConnection.forward`
    phrase takes as input the data tensor `x` and `sublayer`, which is an instance
    of either `MultiHeadedAttention` or `PositionwiseFFN`. We can see this dynamic
    in the `EncoderBlock.forward` method.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接还包括归一化和丢弃（根据定义）。作为提醒，它遵循公式 [![](img/56ef0b5c-6430-44ef-b14b-aadb5badca9f.png)]，但为了代码简洁，`self.norm`
    放在前面而不是最后。`SublayerConnection.forward` 函数的输入是数据张量 `x` 和 `sublayer`，后者是 `MultiHeadedAttention`
    或 `PositionwiseFFN` 的实例。我们可以在 `EncoderBlock.forward` 方法中看到这个动态。
- en: 'The only component we haven''t defined yet is `PositionwiseFFN`, which implements
    the formula [![](img/a255961a-4105-4c24-9971-41d393304341.png)]. Let''s add this
    missing piece:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一尚未定义的组件是 `PositionwiseFFN`，它实现了公式 [![](img/a255961a-4105-4c24-9971-41d393304341.png)]。让我们添加这个缺失的部分：
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We have now implemented the encoder and all its building blocks. In the next
    section, we'll continue with the decoder definition.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了编码器及其所有构建块。在下一部分，我们将继续解码器的定义。
- en: Decoder
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: 'In this section, we''ll implement the decoder. It follows a pattern that is
    very similar to the encoder:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现解码器。它遵循与编码器非常相似的模式：
- en: '[PRE26]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It consists of `self.blocks`: `N` instances of `DecoderBlock`, where the output
    of each block serves as input to the next. These are followed by the `self.norm`
    normalization (an instance of `LayerNorm`). Finally, to produce the most probable
    word, the decoder has an additional fully connected layer with softmax activation. Note
    that the `Decoder.forward` method takes an additional parameter `encoder_states`,
    which represents the attention vector of the encoder. The `encoder_states` are
    then passed to the `DecoderBlock` instances.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 它由`self.blocks`组成：`N`个`DecoderBlock`实例，每个块的输出作为下一个块的输入。这些后面是`self.norm`归一化（`LayerNorm`的一个实例）。最后，为了生成最可能的单词，解码器有一个额外的全连接层，并使用softmax激活。注意，`Decoder.forward`方法接受一个额外的参数`encoder_states`，它表示编码器的注意力向量。然后，`encoder_states`会传递给`DecoderBlock`实例。
- en: 'Next, let''s implement the `DecoderBlock`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现`DecoderBlock`：
- en: '[PRE27]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is similar to `EncoderBlock`, but with one substantial difference: whereas
    `EncoderBlock` relies only on the self-attention mechanism, here we combine self-attention
    with the regular attention coming from the encoder. This is reflected in the `encoder_attn` module
    and later the `encoder_states` parameter of the `forward` method, as well as the
    additional `SublayerConnection` for the encoder attention values. We can see the
    combination of multiple attention mechanisms in the `DecoderBlock.forward` method.
    Note that `self.self_attn` uses `x` for both query/key/value, while `self.encoder_attn`
    uses `x` as a query and `encoder_states` for keys and values. In this way, the
    regular attention establishes the link between the encoder and the decoder.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`EncoderBlock`类似，但有一个显著的区别：`EncoderBlock`仅依赖于自注意力机制，而这里我们将自注意力与来自编码器的常规注意力结合在一起。这体现在`encoder_attn`模块中，以及之后`forward`方法中的`encoder_states`参数，和用于编码器注意力值的额外`SublayerConnection`。我们可以在`DecoderBlock.forward`方法中看到多个注意力机制的结合。注意，`self.self_attn`将`x`用作查询/键/值，而`self.encoder_attn`将`x`作为查询，`encoder_states`作为键和值。通过这种方式，常规注意力机制在编码器和解码器之间建立了联系。
- en: This concludes the decoder implementation. We'll proceed with building the full
    transformer model in the next section.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了解码器的实现。接下来，我们将在下一节构建完整的transformer模型。
- en: Putting it all together
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容结合在一起
- en: 'We''ll continue with the main `EncoderDecoder` class:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续实现主`EncoderDecoder`类：
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It combines the `Encoder`, `Decoder`, and `source_embeddings/target_embeddings`
    (we'll focus on the embeddings later in this section). The `EncoderDecoder.forward`
    method takes the source sequence and feeds it to `self.encoder`. Then, `self.decoder` takes
    its input from the preceding output step `x=self.target_embeddings(target)`, the
    encoder states `encoder_states=encoder_output`, and the source and target masks.
    With these inputs, it produces the predicted next element (word) of the sequence,
    which is also the return value of the `forward` method.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 它结合了`Encoder`、`Decoder`和`source_embeddings/target_embeddings`（我们稍后会专注于嵌入层）。`EncoderDecoder.forward`方法接受源序列并将其传递给`self.encoder`。然后，`self.decoder`从前一步输出`x=self.target_embeddings(target)`获取输入，使用编码器状态`encoder_states=encoder_output`以及源和目标的掩码。使用这些输入，它生成序列中下一个最可能的元素（单词），即`forward`方法的返回值。
- en: 'Next, we''ll implement the `build_model` function, which combines everything
    we''ve implemented so far into one coherent model:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现`build_model`函数，它将我们迄今为止实现的所有内容结合成一个连贯的模型：
- en: '[PRE29]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Besides the familiar `MultiHeadedAttention` and `PositionwiseFFN`, we also
    create the `position` variable (an instance of the `PositionalEncoding` class).
    This class implements the sinusoidal positional encoding we described in the *The*
    *transformer model* section (we won''t include the full implementation here).
    Now let''s focus on the `EncoderDecoder` instantiation: we are already familiar
    with the encoder and the decoder, so there are no surprises there. But the embeddings
    are a tad more interesting. The following code instantiates the source embeddings
    (but this is also valid for the target ones):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了熟悉的`MultiHeadedAttention`和`PositionwiseFFN`，我们还创建了`position`变量（`PositionalEncoding`类的一个实例）。该类实现了我们在*Transformer模型*部分描述的正弦位置编码（这里不包括完整实现）。现在让我们聚焦于`EncoderDecoder`的实例化：我们已经熟悉了编码器和解码器，所以在这方面没有什么意外。但嵌入层稍微有些不同。以下代码实例化了源嵌入（但对于目标嵌入也是有效的）：
- en: '[PRE30]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can see that they are a sequential list of two components:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它们是由两个组件按顺序排列的列表：
- en: An instance of the `Embeddings` class, which is simply a combination of `torch.nn.Embedding` further
    multiplied by [![](img/139f1f87-f470-4ddd-bc21-128a4113a83a.png)] (we'll omit
    the class definition here)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Embeddings` 类的一个实例，它只是 `torch.nn.Embedding` 的组合，并进一步与 [![](img/139f1f87-f470-4ddd-bc21-128a4113a83a.png)]
    相乘（这里省略了类的定义）'
- en: Positional encoding `c(position)`, which adds the positional sinusoidal data
    to the embedding vector
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码 `c(position)`，它将位置的正弦波数据添加到嵌入向量中
- en: Once we have the input data preprocessed in this way, it can serve as input
    to the core part of the encoder/decoder.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们以这种方式预处理了输入数据，它就可以作为编码器/解码器核心部分的输入。
- en: This concludes our implementation of the transformer. Our goal with this example
    was to provide a supplement to the theoretical base of the sections called *The
    transformer attention* and *The transformer model*. Therefore, we have focused
    on the most relevant parts of the code and omitted a few *ordinary* code sections,
    chief among them the `RandomDataset` data generator for random numerical sequences
    and the `train_model` function, which implements the training. Nevertheless, I
    would encourage the reader to run through the full example step by step so that
    they can gain a better understanding of the way the transformer works.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 Transformer 的实现。我们通过这个示例的目的是为了补充 *Transformer 注意力* 和 *Transformer 模型*
    这两个章节的理论基础。因此，我们专注于代码中最相关的部分，省略了一些 *普通* 的代码部分，主要是 `RandomDataset` 随机数列生成器和实现训练的
    `train_model` 函数。然而，我鼓励读者一步一步地执行完整示例，以便更好地理解 Transformer 的工作原理。
- en: In the next section, we'll talk about some of the state-of-the-art language
    models based on the attention mechanisms we have introduced so far.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一些基于我们迄今为止介绍的注意力机制的最先进的语言模型。
- en: Transformer language models
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 语言模型
- en: 'In [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml), *Language* *Modeling*,
    we introduced several different language models (word2vec, GloVe, and fastText)
    that use the context of a word (its surrounding words) to create word vectors
    (embeddings). These models share some common properties:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)，*语言模型*中，我们介绍了几种不同的语言模型（word2vec、GloVe
    和 fastText），它们使用词语的上下文（周围的词语）来创建词向量（嵌入）。这些模型具有一些共同的特性：
- en: They are context-free (I know it contradicts the previous statement) because they
    create a single global word vector of each word based on all its occurrences in
    the training text. For example, *lead* can have completely different meanings
    in the phrases *lead the way* and *lead atom*, yet the model will try to embed
    both meanings in the same word vector.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是无上下文的（我知道这与之前的说法相矛盾），因为它们根据词在训练文本中出现的所有情况创建每个词的单一全局词向量。例如，*lead* 在短语 *lead
    the way* 和 *lead atom* 中可能有完全不同的含义，但模型会尝试将这两种含义嵌入到同一个词向量中。
- en: They are position-free because they don't take into account the order of the
    contextual words when training for the embedding vectors.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是无位置的，因为它们在训练嵌入向量时没有考虑上下文词语的顺序。
- en: In contrast, it's possible to create transformer-based language models, which
    are both context- and position-dependent. These models will produce different
    word vectors for each unique context of the word, taking into account both the
    current context words and their positions. This leads to a conceptual difference
    between the classic and transformer-based models. Since a model such as word2vec
    creates static context- and position-free embedding vectors, we can discard the
    model and only use the vectors in subsequent downstream tasks. But the transformer
    model creates dynamic vectors based on the context, and therefore, we have to
    include it as part of the task pipeline.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于 Transformer 的语言模型可以同时依赖上下文和位置。这些模型会根据每个词的独特上下文生成不同的词向量，同时考虑当前上下文中的词和它们的位置。这就导致了经典模型与基于
    Transformer 的模型之间的概念性差异。由于像 word2vec 这样的模型创建的是静态、与上下文和位置无关的嵌入向量，我们可以抛弃该模型，仅在后续的下游任务中使用这些向量。但
    Transformer 模型会根据上下文生成动态向量，因此我们必须将其作为任务管道的一部分。
- en: In the following sections, we'll discuss some of the most recent transformer-based
    models.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论一些最新的基于 Transformer 的模型。
- en: Bidirectional encoder representations from transformers
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向编码器表示来自 Transformer
- en: 'The **bidirectional encoder representations from transformers** (**BERT**)
    (see *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding *at [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805))
    model has a very descriptive name. Let''s look at some of the elements that are
    mentioned:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**双向编码器表示的变换器**（**BERT**）（参见 *BERT：用于语言理解的深度双向变换器预训练*，[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)）模型有一个非常描述性的名称。让我们来看一下文中提到的一些元素：'
- en: 'Encoder representations: This model uses only the output of the multilayer
    encoder part of the transformer architecture we described in the *The transformer
    model* section.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器表示：该模型只使用我们在 *变换器模型* 部分描述的变换器架构中的多层编码器部分的输出。
- en: 'Bidirectional: The encoder has an inherent bidirectional nature.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向：编码器具有固有的双向特性。
- en: 'To gain some perspective, let''s denote the number of transformer blocks with *L*,
    the hidden size with *H* (previously denoted with *d[model]*), and the number
    of self-attention heads with *A*. The authors of the paper experimented with two
    BERT configurations: BERT[BASE] (*L *= 12, *H *= 768, *A *= 12, total parameters
    = 110M) and BERT[LARGE] (*L *= 24, *H *= 1024, *A *= 16, total parameters = 340M).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们可以用 *L* 表示变换器块的数量，用 *H* 表示隐藏层大小（之前表示为 *d[model]*），用 *A* 表示自注意力头的数量。论文的作者尝试了两种BERT配置：BERT[BASE]（*L*
    = 12，*H* = 768，*A* = 12，总参数量 = 110M）和BERT[LARGE]（*L* = 24，*H* = 1024，*A* = 16，总参数量
    = 340M）。
- en: 'To better understand the BERT framework, we''ll start with the training, which
    has two steps:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解BERT框架，我们将从训练开始，训练分为两个步骤：
- en: '**Pretraining**: The model is trained on unlabeled data over different pretraining
    tasks.'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练**：该模型在不同的预训练任务上对未标记数据进行训练。'
- en: '**Fine-tuning**: The model is initialized with the pretrained parameters and
    then all parameters are fine-tuned over the labeled dataset of the specific downstream
    task.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**：该模型使用预训练参数初始化，然后在特定下游任务的标注数据集上对所有参数进行微调。'
- en: 'We can see the steps in the following diagram:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中看到这些步骤：
- en: '![](img/0600cac1-7eab-4671-b730-54e10104037c.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0600cac1-7eab-4671-b730-54e10104037c.png)'
- en: 'Left: Pretraining; Right: Fine-tuning Source: https://arxiv.org/abs/1810.04805'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 左：预训练；右：微调 来源：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: These diagrams will serve as references through the next sections, so stay tuned
    for more details. For now, it's enough for us to know that **Tok N** represents
    the one-hot-encoded input tokens, *E* represents the token embeddings, and *T*
    represents the model output vector.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表将作为接下来章节的参考，所以请继续关注更多细节。现在，我们只需要知道 **Tok N** 代表经过一热编码的输入标记，*E* 代表标记嵌入，*T*
    代表模型输出向量。
- en: Now that we have an overview of BERT, let's look at its components.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对BERT有了一个概览，让我们来看一下它的组成部分。
- en: Input data representation
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入数据表示
- en: 'Before going into each training step, let''s discuss the input and output data
    representations, which are shared by the two steps. Somewhat similar to fastText
    (see [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml), *Language Modeling*),
    BERT uses a data-driven tokenization algorithm called WordPiece ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144)).
    This means that, instead of a vocabulary of full words, it creates a vocabulary
    of subword tokens in an iterative process until that vocabulary reaches a predetermined
    size (in the case of BERT, the size is 30,000 tokens). This approach has two main
    advantages:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论每个训练步骤之前，让我们先讨论输入和输出数据表示，这些表示是两个步骤共有的。与 fastText（参见[第6章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)，*语言建模*）有些相似，BERT使用一种数据驱动的分词算法叫做
    WordPiece（[https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144)）。这意味着，BERT并不是使用完整单词的词汇表，而是通过迭代过程创建一个子词标记的词汇表，直到该词汇表达到预定的大小（在BERT中，大小为30,000个标记）。这种方法有两个主要优点：
- en: It allows us to control the size of the dictionary.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使我们能够控制词典的大小。
- en: It handles unknown words by assigning them to the closest existing dictionary
    subword token.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过将未知单词分配给最接近的现有字典子词标记来处理未知词汇。
- en: 'BERT can handle a variety of downstream tasks. To do so, the authors introduced
    a special-input data representation, which can unambiguously represent the following
    as a single-input sequence of tokens:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: BERT可以处理多种下游任务。为此，作者引入了一种特殊的输入数据表示，可以明确无误地将以下内容表示为一个单一输入标记序列：
- en: A single sentence (for example, in classification tasks, such as sentiment analysis)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个句子（例如，在分类任务中，如情感分析）
- en: A pair of sentences (for example, in question-answering problems)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对句子（例如，在问答问题中）
- en: Here, *sentence* not only refers to a linguistic sentence, but can mean any
    contiguous text of arbitrary length.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*句子*不仅指语言学上的句子，还可以指任何长度的连续文本。
- en: 'The model uses two special tokens:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用两个特殊词元：
- en: The first token of every sequence is always a special classification token (`[CLS]`).
    The hidden state corresponding to this token is used as the aggregate sequence
    representation for classification tasks. For example, if we want to apply sentiment
    analysis over the sequence, the output corresponding to the `[CLS]` input token
    will represent the sentiment (positive/negative) output of the model.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个序列的第一个词元总是一个特殊的分类词元（`[CLS]`）。与该词元对应的隐藏状态被用作分类任务的序列总表示。例如，如果我们想对序列应用情感分析，则对应于`[CLS]`输入词元的输出将表示模型的情感（正面/负面）输出。
- en: Sentence pairs are packed together into a single sequence. The second special
    token (`[SEP]`) marks the boundary between the two input sentences (in the case
    that we have two). We further differentiate the sentences with the help of an
    additional learned segmentation embedding for every token indicating whether it
    belongs to sentence A or sentence B. Therefore, the input embeddings are the sum
    of the token embeddings, the segmentation embeddings, and the position embeddings.
    Here, the token and position embeddings serve the same purpose as they do in the
    regular transformer.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子对被组合成一个单一的序列。第二个特殊词元(`[SEP]`)标记了两个输入句子之间的边界（如果我们有两个句子）。我们还通过为每个词元引入一个额外的学习过的分割嵌入来区分句子，指示该词元属于句子A还是句子B。因此，输入嵌入是词元嵌入、分割嵌入和位置嵌入的总和。在这里，词元和位置嵌入的作用与常规变换器中的作用相同。
- en: 'The following diagram displays the special tokens, as well as the input embeddings:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了特殊词元以及输入嵌入：
- en: '![](img/1cafb5c9-2644-4b5a-b454-de3021a73aec.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cafb5c9-2644-4b5a-b454-de3021a73aec.png)'
- en: BERT input representation; the input embeddings are the sum of the token embeddings,
    the segmentation embeddings, and the position embeddings. Source: https://arxiv.org/abs/1810.04805
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: BERT输入表示；输入的嵌入是词元嵌入、分割嵌入和位置嵌入的总和。来源：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: Now that we know how the input is processed, let's look at the pretraining step.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了输入是如何处理的，让我们来看看预训练步骤。
- en: Pretraining
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: 'The pretraining step is illustrated on the left-hand side of the diagram in
    the *Bidirectional encoder representations from transformers* section. The authors
    of the paper trained the BERT model using two unsupervised training tasks: **masked
    language modeling** (**MLM**) and **next sentence prediction** (**NSP**).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练步骤在图表左侧的*基于变换器的双向编码器表示*部分进行了说明。论文的作者使用了两个无监督训练任务来训练BERT模型：**掩码语言模型**（**MLM**）和**下一句预测**（**NSP**）。
- en: 'We''ll start with MLM, where the model is presented with an input sequence
    and its goal is to predict a missing word in that sequence. In this case, BERT
    acts as a **denoising autoencoder** in the sense that it tries to reconstruct
    its intentionally corrupted input. MLM is similar in nature to the CBOW objective
    of the word2vec model (see [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml), *Language
    Modeling*). To solve this task, the BERT encoder output is extended with a fully
    connected layer with softmax activation to produce the most probable word, given
    the input sequence. Each input sequence is modified by randomly masking 15% (according
    to the paper) of the WordPiece tokens. To better understand this, we''ll use an
    example from the paper itself: assuming that the unlabeled sentence is *my dog
    is hairy*, and that, during the random masking procedure, we chose the fourth
    token (which corresponds to `hairy`), our masking procedure can be further illustrated
    by the following points:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从MLM开始，模型会接收一个输入序列，其目标是预测该序列中缺失的词。在这种情况下，BERT充当**去噪自编码器**，意思是它试图重建其故意损坏的输入。MLM本质上类似于word2vec模型的CBOW目标（见[第6章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)，*语言建模*）。为了解决这个任务，BERT编码器的输出通过一个带有softmax激活的全连接层进行扩展，以根据输入序列产生最可能的词。每个输入序列通过随机掩码15%的WordPiece词元进行修改（根据论文）。为了更好地理解这一点，我们将使用论文中的一个例子：假设未标记的句子是*my
    dog is hairy*，并且在随机掩码过程中，我们选择了第四个词元（即`hairy`），我们的掩码过程可以通过以下几点进一步说明：
- en: '**80% of the time**: Replace the word with the `[MASK]` token—for example, *my
    dog is hairy* → *my dog is* `[MASK]`.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**80% 的时间**：将单词替换为 `[MASK]` 标记，例如，*我的狗毛茸茸的* → *我的狗是* `[MASK]`。'
- en: '**10% of the time**: Replace the word with a random word—for example, *my dog
    is hairy* → *my dog is apple*.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**10% 的时间**：将单词替换为随机单词，例如，*我的狗毛茸茸的* → *我的狗是苹果*。'
- en: '**10% of the time**: Keep the word unchanged *my dog is hairy* → *my dog is
    hairy*. The purpose of this is to bias the representation toward the actual observed
    word.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**10% 的时间**：保持单词不变，例如，*我的狗毛茸茸的* → *我的狗毛茸茸的*。这样做的目的是使表示向实际观察到的词偏置。'
- en: Because the model is bidirectional, the `[MASK]` token can appear at any position
    in the input sequence. At the same time, the model will use the full sequence
    to predict the missing word. This is opposed to unidirectional autoregressive
    models (we'll discuss these in the following sections), which always try to predict
    the next word from all preceding words, thereby avoiding the need to have `[MASK]`
    tokens.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模型是双向的，`[MASK]` 标记可以出现在输入序列的任何位置。同时，模型将使用完整序列来预测缺失的单词。这与单向自回归模型相反（我们将在接下来的部分讨论），后者总是试图从所有前面的单词中预测下一个单词，从而避免使用
    `[MASK]` 标记。
- en: 'There are two main reasons why we need this 80/10/10 distribution:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这种 80/10/10 的分布有两个主要原因：
- en: The `[MASK]` token creates a mismatch between pretraining and fine-tuning (we'll
    discuss this in the next section), since it only appears in the former but not
    in the latter—that is, the fine-tuning task will present the model with input
    sequences without the `[MASK]` token. Yet, the model was pretrained to expect
    sequences with `[MASK],` which might lead to undefined behavior.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[MASK]` 标记在预训练和微调之间创建了不匹配（我们将在下一节讨论这一点），因为它仅出现在前者中而不出现在后者中——即，微调任务将向模型呈现不带
    `[MASK]` 标记的输入序列。然而，模型被预训练为期望带有 `[MASK]` 标记的序列，这可能导致未定义的行为。'
- en: BERT assumes that the predicted tokens are independent of each other. To understand
    this, let's imagine that the model tries to reconstruct the input sequence *I
    went* `[MASK]` *with my* `[MASK]`. BERT can predict the sentence *I went cycling
    with my bicycle*, which is a valid sentence. But because the model does not relate
    the two masked words, nothing prevents it from predicting *I went swimming with
    my bicycle*, which is not valid.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 假设预测的标记彼此独立。为了理解这一点，让我们想象模型试图重构输入序列 *I went* `[MASK]` *with my* `[MASK]`。BERT
    可以预测句子 *I went cycling with my bicycle*，这是一个有效的句子。但是因为模型不关联两个被掩盖的单词，它也可以预测 *I
    went swimming with my bicycle*，这是无效的。
- en: With the 80/10/10 distribution, the `transformer` encoder does not know which
    words it will be asked to predict or which have been replaced by random words,
    so it is forced to keep a distributional contextual representation of every input
    token. Additionally, because random replacement only occurs for 1.5% of all tokens
    (that is, 10% of 15%), this does not seem to harm the model's language-understanding
    ability.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在 80/10/10 的分布下，`transformer` 编码器不知道它将被要求预测哪些单词或者哪些单词已被替换为随机单词，因此它被迫保持每个输入标记的分布上下文表示。此外，由于随机替换仅在所有标记的
    1.5% 中发生（即 15% 的 10%），这似乎并不会损害模型的语言理解能力。
- en: One disadvantage of MLM is that, because the model only predicts 15% of the
    words in each batch, it might converge more slowly than pretraining models that use
    all words.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: MLM 的一个缺点是，因为模型仅预测每批次中 15% 的单词，它可能比使用所有单词的预训练模型收敛速度更慢。
- en: Next, let's continue with NSP. The authors argue that many important downstream
    tasks, such as **question answering** (**QA**) and **natural language inference**
    (**NLI**), are based on understanding the relationship between two sentences,
    which is not directly captured by language modeling.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续进行 NSP。作者认为许多重要的下游任务，如**问答** (**QA**) 和**自然语言推理** (**NLI**)，基于对两个句子关系的理解，而这不是语言建模直接捕捉到的。
- en: 'Natural language inference determines whether a sentence, which represents
    a **hypothesis**, is either true (entailment), false (contradiction), or undetermined
    (neutral) given another sentence, called a **premise**. The following table shows
    some examples:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推理确定一句话（表示一个**假设**）在给定另一句话（称为**前提**）时是真实的（蕴含），错误的（矛盾）还是未确定的（中性）。以下表格展示了一些例子：
- en: '| **Premise** | **Hypothesis** | **Label** |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| **前提** | **假设** | **标签** |'
- en: '| I am running | I am sleeping | contradiction |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 我在跑步 | 我在睡觉 | 矛盾 |'
- en: '| I am running | I am listening to music | neutral |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 我在跑步 | 我在听音乐 | 中性 |'
- en: '| I am running | I am training | entailment |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 我正在跑步 | 我正在训练 | 推理 |'
- en: In order to train a model that understands sentence relationships, we pretrain
    for a next-sentence prediction task that can be trivially generated from any monolingual
    corpus. Specifically, each input sequence consists of a starting `[CLS]` token,
    followed by two concatenated sentences, A and B, which are separated by the `[SEP]`
    token (see the diagram in the *Bidirectional encoder representations from transformers*
    section). When choosing the sentences A and B for each pretraining example, 50%
    of the time, B is the actual next sentence that follows A (labeled as `IsNext`),
    and 50% of the time, it is a random sentence from the corpus (labeled as `NotNext`).
    As we mentioned, the model outputs the `IsNext`/`NotNext` labels on the `[CLS]`
    corresponding input.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个能够理解句子关系的模型，我们预训练了一个下一个句子预测任务，这个任务可以从任何单语语料库中轻松生成。具体来说，每个输入序列由一个起始的`[CLS]`标记，后跟两个连接在一起的句子A和B，句子之间用`[SEP]`标记分隔（请参见*基于变换器的双向编码器表示*部分的图示）。在为每个预训练示例选择句子A和B时，50%的时间，B是实际跟随A的下一个句子（标记为`IsNext`），而50%的时间，它是来自语料库的随机句子（标记为`NotNext`）。如前所述，模型会在与`[CLS]`对应的输入上输出`IsNext`/`NotNext`标签。
- en: 'The NSP task is illustrated using the following example:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: NSP任务使用以下示例进行说明：
- en: '`[CLS]` *the man went to* `[MASK]` *store* `[SEP]` *he bought a gallon* `[MASK]`
    *milk [SEP]* with the label `IsNext`.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]` *那个人去了* `[MASK]` *商店* `[SEP]` *他买了一加仑* `[MASK]` *牛奶 [SEP]*，标签为`IsNext`。'
- en: '`[CLS]` *the man* `[MASK]` *to the store* `[SEP]` *penguins* `[MASK]` *are
    flight ##less birds* `[SEP]` with the label `NotNext`. Note the use of the *##less*
    token, which is the result of the WordPiece tokenization algorithm.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]` *那个人* `[MASK]` *去了商店* `[SEP]` *企鹅* `[MASK]` *是不会飞的鸟* `[SEP]`，标签为`NotNext`。请注意`##less`标记的使用，这是WordPiece分词算法的结果。'
- en: Next, let's look at the fine-tuning step.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下微调步骤。
- en: Fine-tuning
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: The fine-tuning task follows the pretraining task, and apart from the input
    preprocessing, the two steps are very similar. Instead of creating a masked sequence,
    we simply feed the BERT model with the task-specific unmodified input and output
    and fine-tune all the parameters in an end-to-end fashion. Therefore, the model
    that we use in the fine-tuning phase is the same model that we'll use in the actual
    production environment.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 微调任务是在预训练任务的基础上进行的，除了输入预处理，两个步骤非常相似。与创建掩码序列不同，我们只是将任务特定的未修改输入和输出直接输入到BERT模型中，并以端到端的方式微调所有参数。因此，在微调阶段使用的模型与我们在实际生产环境中使用的模型是相同的。
- en: 'The following diagram shows how to solve several different types of task with
    BERT:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了如何使用BERT解决几种不同类型的任务：
- en: '![](img/4ba45881-2a02-4dba-9f4f-c22fb3da7ffe.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ba45881-2a02-4dba-9f4f-c22fb3da7ffe.png)'
- en: BERT applications for different tasks; source: https://arxiv.org/abs/1810.04805
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在不同任务中的应用；来源：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: 'Let''s discuss them:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论一下：
- en: The top-left scenario illustrates how to use **BERT** for sentence-pair classification
    tasks, such as NLI. In short, we feed the model with two concatenated sentences
    and only look at the `[CLS]` token output classification, which will output the
    model result. For example, in an NLI task, the goal is to predict whether the
    second sentence is an entailment, a contradiction, or neutral with respect to
    the first one.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左上角的场景说明了如何使用**BERT**进行句对分类任务，如自然语言推理（NLI）。简而言之，我们将两个连接在一起的句子输入模型，并只查看`[CLS]`标记输出的分类结果，该输出将给出模型的结果。例如，在NLI任务中，目标是预测第二个句子相对于第一个句子是蕴含、矛盾还是中立。
- en: The top-right scenario illustrates how to use **BERT** for single-sentence classification
    tasks, such as sentiment analysis. This is very similar to the sentence-pair classification.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右上角的场景说明了如何使用**BERT**进行单句分类任务，如情感分析。这与句对分类非常相似。
- en: 'The bottom-left scenario illustrates how to use **BERT** on the **Stanford
    Question Answering Dataset** (**SQuAD** v1.1, [https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)).
    Given that sequence A is a question and sequence B is a passage from Wikipedia,
    which contains the answer, the goal is to predict the text span (start and end)
    of the answer within this passage. We introduce two new vectors: a start vector
    ![](img/920325e6-3871-4031-844c-3196fe8d17b6.png) and an end vector ![](img/d37a6783-26fe-4ba4-84d0-504cab4d82c2.png),
    where *H* is the hidden size of the model. The probability of each word *i* as
    being the start (or end) of the answer span is computed as a dot product between
    its output vector *T[i]* and *S* (or *E*), followed by a softmax over all the
    words of the sequence *B*: ![](img/2b598dd9-c691-4be3-87aa-16a2a2153ce3.png).
    The score of a candidate span starting from position *i* and spanning to *j* is
    computed as ![](img/911c7f71-5ac0-4cc0-af14-3b5662add47f.png). The output candidate
    is the one with the maximum score, where ![](img/7caa20c6-d192-4825-9a7b-e444a051d85f.png).'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左下方的场景展示了如何在 **斯坦福问答数据集**（**SQuAD** v1.1，[https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)）上使用
    **BERT**。假设序列 A 是一个问题，序列 B 是来自维基百科的段落，包含答案，目标是预测答案在该段落中的文本范围（起始位置和结束位置）。我们引入两个新的向量：一个起始向量
    ![](img/920325e6-3871-4031-844c-3196fe8d17b6.png) 和一个结束向量 ![](img/d37a6783-26fe-4ba4-84d0-504cab4d82c2.png)，其中
    *H* 是模型的隐藏层大小。每个单词 *i* 作为答案范围的起始（或结束）位置的概率通过其输出向量 *T[i]* 与 *S*（或 *E*）的点积计算，然后对序列
    *B* 中所有单词进行 softmax 操作：![](img/2b598dd9-c691-4be3-87aa-16a2a2153ce3.png)。从位置 *i*
    开始并延伸到位置 *j* 的候选范围的得分计算为 ![](img/911c7f71-5ac0-4cc0-af14-3b5662add47f.png)。输出候选范围是具有最高得分的那个，其中
    ![](img/7caa20c6-d192-4825-9a7b-e444a051d85f.png)。
- en: The bottom-right scenario illustrates how to use **BERT** for **named entity
    recognition** (**NER**), where each input token is classified as some type of
    entity.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右下方的场景展示了如何在 **BERT** 中进行 **命名实体识别**（**NER**），其中每个输入标记都会被分类为某种类型的实体。
- en: This concludes our section dedicated to the BERT model. As a reminder, it is
    based on the transformer encoder. In the next section, we'll discuss transformer
    decoder models.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容就介绍完了 BERT 模型。作为提醒，它是基于变换器编码器的。下一部分我们将讨论变换器解码器模型。
- en: Transformer-XL
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer-XL
- en: 'In this section, we''ll talk about an improvement over the vanilla transformer,
    called transformer-XL, where XL stands for extra long (see *Transformer-X**L:
    Attentive Language Models Beyond a Fixed-Length Context *at [https://arxiv.org/abs/1901.02860](https://arxiv.org/abs/1901.02860)).
    To understand the need to improve the regular transformer, let''s discuss some
    of its limitations, one of which comes from the nature of the transformer itself.
    An RNN-based model has the (at least theoretical) ability to convey information
    about sequences of arbitrary length, because the internal RNN state is adjusted
    based on all previous inputs. But the transformer''s self-attention doesn''t have
    such a recurrent component, and is restricted entirely within the bounds of the
    current input sequence. If we had infinite memory and computation, a simple solution
    would be to process the entire context sequence. But in practice, we have limited
    resources, and so we split the entire text into smaller segments and train the
    model only within each segment, as image **(a)** in the following diagram shows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将讨论一个对传统变换器的改进，叫做 transformer-XL，其中 XL 代表额外的长（见 *Transformer-X**L: 超越固定长度上下文的注意力语言模型*，[https://arxiv.org/abs/1901.02860](https://arxiv.org/abs/1901.02860)）。为了理解为什么需要改进常规的变换器，我们先来讨论一下它的一些局限性，其中之一来源于变换器本身的特点。基于
    RNN 的模型具有（至少在理论上）传递任意长度序列信息的能力，因为其内部的 RNN 状态会根据所有先前的输入进行调整。但变换器的自注意力机制没有这样的递归组件，它完全限制在当前输入序列的范围内。如果我们有无限的内存和计算能力，一个简单的解决方案是处理整个上下文序列。但在实践中，我们的资源是有限的，因此我们将整个文本划分为更小的片段，并且只在每个片段内训练模型，如下图中的**（a）**所示：'
- en: '![](img/2acf4b82-935f-4f9e-a94a-b0a3b4e615ef.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2acf4b82-935f-4f9e-a94a-b0a3b4e615ef.png)'
- en: Illustration of the training (a) and evaluation (b) of a regular transformer
    with an input sequence of length 4; note the use of the unidirectional transformer
    decoder. Source: https://arxiv.org/abs/1901.02860
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 普通变换器训练（a）和评估（b）的示意图，输入序列长度为 4；注意使用的是单向变换器解码器。来源：https://arxiv.org/abs/1901.02860
- en: The horizontal axis represents the input sequence [*x[1],..., x[4]*] and the
    vertical axis represents the stacked decoder blocks. Note that element *x[i]*
    can only attend to the elements ![](img/6625a637-0e08-4d99-b338-aed9728d4b6c.png).
    That's because transformer-XL is based on the transformer decoder (and doesn't
    include the encoder), unlike BERT, which is based on the encoder. Therefore, the
    transformer-XL decoder is not the same as the decoder in the *full* encoder-decoder
    transformer, because it doesn't have access to the encoder state, as the regular
    decoder does. In that sense, the transformer-XL decoder is very similar to a general
    transformer encoder, with the exception that it's unidirectional, because of the
    input sequence mask. Transformer-XL is an example of an **autoregressive model**.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 横坐标表示输入序列[*x[1], ..., x[4]*]，纵坐标表示堆叠的解码器块。请注意，元素*x[i]*只能关注元素 ![](img/6625a637-0e08-4d99-b338-aed9728d4b6c.png)。这是因为
    transformer-XL 基于变压器解码器（不包括编码器），而与 BERT 不同，BERT 基于编码器。因此，transformer-XL 的解码器与*全*编码器-解码器变压器中的解码器不同，因为它无法访问编码器状态，而常规解码器是可以访问的。从这个意义上讲，transformer-XL
    的解码器非常类似于一般的变压器编码器，唯一的区别是它是单向的，因为输入序列掩码的存在。Transformer-XL 是一个**自回归模型**的例子。
- en: As the preceding diagram demonstrates, the largest possible dependency length
    is upper-bounded by the segment length, and although the attention mechanism helps
    prevent vanishing gradients by allowing immediate access to all elements of the
    sequence, the transformer cannot fully exploit this advantage, because of the
    limited input segment. Furthermore, the text is usually split by selecting a consecutive
    chunk of symbols without respecting the sentence or any other semantic boundary,
    which the authors of the paper refer to as context fragmentation. To quote the
    paper itself, the model lacks the contextual information needed to well predict
    the first few symbols, leading to inefficient optimization and inferior performance.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图示所示，最大的依赖长度是由片段长度上限的，尽管注意力机制通过允许立即访问序列中的所有元素帮助防止梯度消失，但由于输入片段的限制，变压器（transformer）无法充分利用这一优势。此外，文本通常通过选择连续的符号块来拆分，而不考虑句子或任何其他语义边界，论文的作者将其称为**上下文碎片化**。引用论文中的话，模型缺乏正确预测前几个符号所需的上下文信息，导致优化效率低下和性能不佳。
- en: Another issue of the vanilla transformer is manifested during evaluation, as
    shown on the right-hand side of the preceding diagram. At each step, the model
    takes the full sequence as input, but only makes a single prediction. To predict
    the next output, the transformer is shifted right with a single position, yet
    the new segment (which is the same as the last segment, except for the last value)
    has to be processed from scratch over the full input sequence.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 传统变压器的另一个问题出现在评估过程中，如前面图示的右侧所示。在每一步，模型将完整序列作为输入，但只做出一个预测。为了预测下一个输出，变压器右移一个位置，但新的片段（与最后一个片段相同，唯一不同的是最后的值）必须从头开始在整个输入序列上处理。
- en: Now that we've identified some problems with the transformer model, let's look
    at how to solve them.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经识别出变压器模型中的一些问题，接下来让我们看看如何解决这些问题。
- en: Segment-level recurrence with state reuse
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 片段级别的递归与状态重用
- en: 'Transformer-XL introduces a recurrence relationship in the transformer model.
    During training, the model caches its state for the current segment, and when
    it processes the next segment, it has access to that cached (but fixed) value,
    as we can see in the following diagram:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer-XL 在变压器模型中引入了递归关系。在训练过程中，模型会缓存当前片段的状态，当处理下一个片段时，它可以访问该缓存（但固定）值，正如我们在以下图示中看到的那样：
- en: '![](img/78c7fc5c-275f-4ccc-975d-d483648c44ac.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c7fc5c-275f-4ccc-975d-d483648c44ac.png)'
- en: Illustration of the training (a) and evaluation (b) of transformer-XL with an
    input sequence length of 4. Source: https://arxiv.org/abs/1901.02860
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 训练（a）和评估（b）变压器-XL的示意图，输入序列长度为4。来源：https://arxiv.org/abs/1901.02860
- en: 'During training, the gradient is not propagated through the cached segment.
    Let''s formalize this concept (we''ll use the notation from the paper, which might
    differ slightly from the previous notations in this chapter). We''ll denote two
    consecutive segments of length *L* with ![](img/003a82a2-0d66-4b95-80cc-9cb61ceba9e2.png) and
    ![](img/3886154f-f4ae-43c9-bb52-0fa177d943bc.png) and the *n*th block hidden state
    of the *τ*th segment with ![](img/daf9b9a2-2b97-4d63-8cfe-f485f3c9b0a8.png), where
    *d* is the hidden dimension (equivalent to *d[model]*). To clarify, ![](img/58e25d25-edae-43f1-a5f7-2e996e7e1ea8.png) is
    a matrix with *L* rows, where each row contains the *d*-dimensional self-attention vector
    of each element of the input sequence. Then, the *n*th layer hidden state of the *τ+1*th
    segment is produced by going through the following steps:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，梯度不会通过缓存的片段传播。我们来正式化这个概念（我们将使用论文中的符号，可能与本章之前的符号稍有不同）。我们用![](img/003a82a2-0d66-4b95-80cc-9cb61ceba9e2.png)表示两个连续片段的长度为*L*，用![](img/3886154f-f4ae-43c9-bb52-0fa177d943bc.png)表示另一个片段，用![](img/daf9b9a2-2b97-4d63-8cfe-f485f3c9b0a8.png)表示第*τ*个片段的第*n*层隐藏状态，其中*d*是隐藏维度（等同于*d[model]*）。为了更清楚，![](img/58e25d25-edae-43f1-a5f7-2e996e7e1ea8.png)是一个具有*L*行的矩阵，其中每一行包含输入序列中每个元素的*d*维自注意力向量。然后，*τ+1*个片段的第*n*层隐藏状态是通过以下步骤生成的：
- en: '![](img/65d7d9bf-1a70-4dda-8f8d-aabd242822fe.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65d7d9bf-1a70-4dda-8f8d-aabd242822fe.png)'
- en: Here, ![](img/e009eb26-3070-427c-ae4c-13af2a325006.png) refers to the stop gradient,
    **W[*]** refers to the model parameters (previously denoted with **W^***), and
    ![](img/03843f17-dd65-4127-bcbd-feeabc108c61.png) refers to the concatenation
    of the two hidden sequences along the length dimension. To clarify, the concatenated
    hidden sequences is a matrix with *2L* rows, where each row contains the *d*-dimensional self-attention vector
    of one element of the combined input sequences τ and τ+1\. The paper does a great
    job of explaining the intricacies of the preceding formulas, so the following
    explanation contains some direct quotes. Compared to the standard transformer,
    the critical difference lies in that the key ![](img/2ca40428-2d70-45a3-a648-aa3985b7e98b.png) and
    value ![](img/20bc5956-58c1-45ca-9d59-71039a1c984a.png) are conditioned on the
    extended context ![](img/30c22b63-cdf9-46c8-99c3-3aa9814c8edd.png), and so ![](img/f357cf76-93ea-47cc-85dd-6e7ddc6f9047.png)
    is cached from the previous segment (shown with the green paths in the preceding
    diagram). With this recurrence mechanism applied to every two consecutive segments
    of a corpus, it essentially creates a segment-level recurrence in the hidden states.
    As a result, the effective context that is utilized can go way beyond just two
    segments. However, note that the recurrent dependency between ![](img/c6363dc1-ba34-4cde-bda6-d93e1538e439.png) and
    ![](img/fa984b5b-8e02-45f4-93b0-d88ed5e5c12c.png) shifts one layer downward per
    segment. Consequently, the largest possible dependency length grows linearly with
    respect to the number of layers as well as the segment length—that is, *O*(*N*
    × *L*), as visualized by the shaded area of the preceding diagram.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/e009eb26-3070-427c-ae4c-13af2a325006.png)表示停止梯度，**W[*]**表示模型参数（之前用**W^***表示），而![](img/03843f17-dd65-4127-bcbd-feeabc108c61.png)表示沿长度维度拼接的两个隐藏序列。为了澄清，拼接后的隐藏序列是一个具有*2L*行的矩阵，其中每一行包含一个组合输入序列τ和τ+1的元素的*d*维自注意力向量。论文很好地解释了前面公式的复杂性，因此以下的解释包含一些直接引用。与标准的transformer相比，关键的不同之处在于键![](img/2ca40428-2d70-45a3-a648-aa3985b7e98b.png)和值![](img/20bc5956-58c1-45ca-9d59-71039a1c984a.png)是基于扩展上下文![](img/30c22b63-cdf9-46c8-99c3-3aa9814c8edd.png)来条件化的，因此![](img/f357cf76-93ea-47cc-85dd-6e7ddc6f9047.png)是从前一个片段缓存的（如前面图中的绿色路径所示）。通过将这种递归机制应用于语料库中的每两个连续片段，它实际上在隐藏状态中创建了一个片段级递归。因此，所使用的有效上下文可以远远超出仅仅两个片段。然而，需要注意的是，![](img/c6363dc1-ba34-4cde-bda6-d93e1538e439.png)与![](img/fa984b5b-8e02-45f4-93b0-d88ed5e5c12c.png)之间的递归依赖关系在每个片段中都会向下偏移一层。因此，最大的依赖长度会随着层数和片段长度的增加而线性增长——也就是说，*O*(*N*
    × *L*)，如前面图中的阴影区域所示。
- en: Besides achieving extra-long context and resolving fragmentation, another benefit
    that comes with the recurrence scheme is significantly faster evaluation. Specifically,
    during evaluation, the representations from the previous segments can be reused
    instead of being computed from scratch, as in the case of the vanilla model.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实现超长上下文和解决碎片化问题之外，递归机制的另一个好处是显著加快了评估速度。具体来说，在评估过程中，之前片段的表示可以被重复使用，而不是像传统模型那样从头开始计算。
- en: Finally, note that the recurrence scheme does not need to be restricted to only
    the previous segment. In theory, we can cache as many previous segments as the
    GPU memory allows and reuse all of them as the extra context when processing the
    current segment.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，注意到递归方案不需要仅限于前一个片段。从理论上讲，我们可以根据GPU内存的大小缓存尽可能多的先前片段，并在处理当前片段时将它们作为额外的上下文进行重用。
- en: The recurrence scheme will require a new way to encode the positions of the
    sequence elements. Let's look at this topic next.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 递归方案将需要一种新的方法来编码序列元素的位置。接下来，我们将讨论这个话题。
- en: Relative positional encodings
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相对位置编码
- en: 'The vanilla transformer input is augmented with sinusoidal positional encodings
    (see the *The transformer model* section), which are relevant only within the
    current segment. The following formula shows how to schematically compute the
    states ![](img/8351dc62-260f-4636-934e-bab20e8af900.png) and ![](img/6ebf6a61-eff6-4c68-9cc9-7d92b83d1d0c.png) with
    the current positional encodings:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 普通Transformer输入会加上正弦位置编码（参见*Transformer模型*部分），这些编码仅在当前片段内有效。以下公式展示了如何通过当前的位置信息编码来简要计算状态 ![](img/8351dc62-260f-4636-934e-bab20e8af900.png) 和
    ![](img/6ebf6a61-eff6-4c68-9cc9-7d92b83d1d0c.png)：
- en: '![](img/8fbe4d9a-d78a-48ba-8dcb-d0dc5e377eff.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fbe4d9a-d78a-48ba-8dcb-d0dc5e377eff.png)'
- en: Here, ![](img/a8944859-bec6-40db-90f6-2b25713b139c.png) is the word-embedding
    sequence of *s[τ]*, and *f* is the transformation function. We can see that we
    use the same positional encoding ![](img/6b6471ad-550b-4c79-8b34-42141e5f1c36.png)
    for both ![](img/1664f782-bb9f-4a20-b780-ccd4107ae4b3.png) and ![](img/34dcfc74-e58d-493a-8cf2-08e627b08b38.png).
    Because of this, the model cannot distinguish between the positions of two elements
    of the same position within the different sequences ![](img/a7aeb32f-02b8-470d-8d9d-0475875c6a1f.png) and
    ![](img/4ae0fb1b-6338-42b2-8568-32bab59a85b1.png). To avoid this, the authors
    of the paper propose a new type of **relative** positional encoding scheme. They
    made the observation that when a query vector (or matrix of queries) ![](img/59cab96e-f966-4809-92da-5595bd43f68b.png) attends
    to key vectors ![](img/fc82f157-e241-443c-a3ae-fcfeef222bd3.png), it does not
    need to know the absolute position of each key vector to identify the temporal
    order of the segment. Instead, it is enough to know the relative distance between
    each key vector ![](img/95f61cdd-49ac-49f6-a8e4-21e00433e065.png) and itself ![](img/50ee0566-d28e-41d5-952e-a995dcb752f3.png)—that
    is *i−j*.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/a8944859-bec6-40db-90f6-2b25713b139c.png)是*s[τ]*的词嵌入序列，*f*是转换函数。我们可以看到，我们对 ![](img/1664f782-bb9f-4a20-b780-ccd4107ae4b3.png) 和
    ![](img/34dcfc74-e58d-493a-8cf2-08e627b08b38.png)使用了相同的位置信息编码 ![](img/6b6471ad-550b-4c79-8b34-42141e5f1c36.png)。正因为如此，模型无法区分两个相同位置的元素在不同序列中的位置 ![](img/a7aeb32f-02b8-470d-8d9d-0475875c6a1f.png) 和
    ![](img/4ae0fb1b-6338-42b2-8568-32bab59a85b1.png)。为了解决这个问题，论文的作者提出了一种新的**相对**位置编码方案。他们观察到，当查询向量（或查询矩阵） ![](img/59cab96e-f966-4809-92da-5595bd43f68b.png) 注意到键向量 ![](img/fc82f157-e241-443c-a3ae-fcfeef222bd3.png)时，查询向量不需要知道每个键向量的绝对位置来识别片段的时间顺序。相反，知道每个键向量 ![](img/95f61cdd-49ac-49f6-a8e4-21e00433e065.png) 与其自身 ![](img/50ee0566-d28e-41d5-952e-a995dcb752f3.png)之间的相对距离，即*i−j*，就足够了。
- en: 'The proposed solution is to create a set of relative positional encodings ![](img/ecdc4134-f80d-4b20-a418-57dcff1d65fd.png),
    where each cell of the *i*th row indicates the relative distance between the *i*th
    element and the rest of the elements of the sequence. **R** uses the same sinusoidal
    formula as before, but this time, with relative instead of absolute positions.
    This relative distance is injected dynamically (as opposed to being part of the
    input preprocessing), which makes it possible for the query vector to distinguish
    between the positions of ![](img/a7aeb32f-02b8-470d-8d9d-0475875c6a1f.png) and ![](img/4ae0fb1b-6338-42b2-8568-32bab59a85b1.png).
    To understand this, let''s start with the product absolute position attention
    formula from the *The transformer attention* section, which can be decomposed
    as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的解决方案是创建一组相对位置编码 ![](img/ecdc4134-f80d-4b20-a418-57dcff1d65fd.png)，其中第*i*行的每个单元格表示第*i*个元素与序列中其他元素之间的相对距离。**R**使用与之前相同的正弦公式，但这次使用的是相对位置而非绝对位置。这个相对距离是动态注入的（与输入预处理部分不同），这使得查询向量能够区分 ![](img/a7aeb32f-02b8-470d-8d9d-0475875c6a1f.png) 和 ![](img/4ae0fb1b-6338-42b2-8568-32bab59a85b1.png)的位置信息。为了理解这一点，让我们从*Transformer注意力*部分的绝对位置注意力公式开始，这个公式可以分解如下：
- en: '![](img/d2cea46c-2833-4f4e-a1c6-1ec5b577e235.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2cea46c-2833-4f4e-a1c6-1ec5b577e235.png)'
- en: 'Let''s discuss the components of this formula:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下这个公式的组成部分：
- en: Indicates how much word *i* attends to word *j*, regardless of their current
    position (content-based addressing)—for example, how much the word *tire* relates
    to the word *car*.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表示词汇*i*在多大程度上关注词汇*j*，不考虑它们当前的位置（基于内容的寻址）——例如，词汇*tire*与词汇*car*的关系。
- en: Reflects how much word *i* attends to the word in position *j*, regardless of
    what that word is (content-dependent positional bias)—for example, if the word *i*
    is *cream*, we may want to check the probability that word *j = i - 1* is *ice*.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反映了词汇*i*在多大程度上关注位置*j*的词，无论那个词是什么（基于内容的定位偏置）——例如，如果词汇*i*是*cream*，我们可能希望检查词汇*j
    = i - 1*是否是*ice*。
- en: This step is the opposite of step 2.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一步是步骤2的反向操作。
- en: Indicates how much a word in position *i* should attend to a word in position
    *j*, regardless of what the two words are (global-positioning bias)—for example,
    this value could be low for positions that are far apart.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表示位置*i*的词应多大程度上关注位置*j*的词，无论这两个词是什么（全局定位偏置）——例如，对于相距较远的位置，这个值可能较低。
- en: 'In transformer-XL, this formula is modified to include the relative positional
    embeddings:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformer-XL中，这个公式被修改为包含相对位置嵌入：
- en: '![](img/3fffcb4d-ad6d-430d-a538-58aa25e8f47d.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fffcb4d-ad6d-430d-a538-58aa25e8f47d.png)'
- en: 'Let''s outline the changes with respect to the absolute position formula:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们概述一下相对于绝对位置公式的变化：
- en: Replace all appearances of the absolute positional embedding *U[j]* for computing
    key vectors in terms (2) and (4) with its relative counterpart *R[i−j]*.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将公式（2）和（4）中用于计算关键向量的绝对位置嵌入*U[j]*替换为其相对位置嵌入*R[i−j]*。
- en: Replace the query ![](img/f2b094b1-2640-4acd-93fe-8c489139064b.png) in term
    (3) with a trainable parameter ![](img/763e3944-4c5c-481c-9578-3f64d6a4dfc7.png).
    The reasoning behind this is that the query vector is the same for all query positions;
    therefore, the attentive bias toward different words should remain the same regardless
    of the query position. Similarly, a trainable parameter ![](img/b913a4dc-a09e-4ef8-8f5b-ae5b247722b5.png) substitutes
    ![](img/327fa6b0-167d-434f-bad7-6835c4303d18.png) in term (4).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在公式（3）中，将查询![](img/f2b094b1-2640-4acd-93fe-8c489139064b.png)替换为一个可训练的参数![](img/763e3944-4c5c-481c-9578-3f64d6a4dfc7.png)。这样做的原因是查询向量对于所有查询位置都是相同的；因此，对不同词汇的注意力偏置应保持一致，无论查询位置如何。类似地，可训练参数![](img/b913a4dc-a09e-4ef8-8f5b-ae5b247722b5.png)替代了公式（4）中的![](img/327fa6b0-167d-434f-bad7-6835c4303d18.png)。
- en: Separate **W***[K]* into two weight matrices **W***[K,E]* and **W***[K,R]* to
    produce separate content-based and position-based key vectors.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**W***[K]*分为两个权重矩阵**W***[K,E]*和**W***[K,R]*，以生成分别基于内容和位置的关键向量。
- en: To recap, the segment-level recurrence and relative positional encodings are the
    main improvements of transformer-XL over the vanilla transformer. In the next
    section, we'll look at yet another improvement of transformer-XL.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，segment-level的循环和相对位置编码是transformer-XL相较于普通transformer的主要改进。在接下来的部分中，我们将探讨transformer-XL的另一个改进。
- en: XLNet
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLNet
- en: 'The authors note that bidirectional models with denoising autoencoding pretraining (such
    as BERT) achieve better performance compared to unidirectional autoregressive
    models (such as transformer-XL). But as we mentioned in the *Pretraining* subsection
    of the *Bidirectional encoder representations from transformers* section, the
    `[MASK]` token introduces a discrepancy between the pretraining and fine-tuning
    steps. To overcome these limitations, the authors of transformer-XL propose XLNet (see *XLNet:
    Generalized Autoregressive Pretraining for Language Understanding *at [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)):
    a generalized **autoregressive** pretraining mechanism that enables learning bidirectional
    contexts by maximizing the expected likelihood over all permutations of the factorization
    order. To clarify, XLNet builds upon the transformer decoder model of transformer-XL
    and introduces a smart permutation-based mechanism for bidirectional context flow
    within the autoregressive pretraining step.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '作者指出，具有去噪自编码预训练（如BERT）的双向模型，相较于单向自回归模型（如transformer-XL），表现更好。但正如我们在*双向编码器表示来自transformers*部分的*预训练*小节中提到的，`[MASK]`标记在预训练和微调步骤之间引入了差异。为了解决这些问题，transformer-XL的作者提出了XLNet（参见*XLNet:
    用于语言理解的广义自回归预训练*，详见[https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)）：一种广义的**自回归**预训练机制，通过最大化所有因式分解顺序的排列期望似然，来实现双向上下文学习。为澄清，XLNet建立在transformer-XL的transformer解码器模型之上，并引入了一种基于排列的智能机制，用于在自回归预训练步骤中实现双向上下文流动。'
- en: 'The following diagram illustrates how the model processes the same input sequence
    with different factorization orders. Specifically, it shows a transformer decoder
    with two stacked blocks and segment-level recurrence (the **mem** fields):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了模型如何使用不同的因式分解顺序处理相同的输入序列。具体来说，它展示了一个具有两个堆叠块和分段级别递归（**mem** 字段）的变压器解码器：
- en: '![](img/9bd20c60-51c4-4d3d-aebc-427ac09eed57.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bd20c60-51c4-4d3d-aebc-427ac09eed57.png)'
- en: Predicting *x[3]* over the same input sequence with four different factorization
    orders. Source: https://arxiv.org/abs/1906.08237
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '在相同的输入序列上，预测不同因式分解顺序下的 *x[3]*。来源: [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)'
- en: There are *T*! different orders to perform a valid autoregressive factorization
    over a sequence of length *T*. Let's assume that we have an input sequence of
    [*x[1], x[2], x[3], x[4]*] with length 4\. The diagram shows four of the possible
    4! = 24 factorization orders of that sequence (starting clockwise from the top-left
    corner): [*x[3], x[2], x[4], x[1]*], [*x[2], x[4], x[3], x[1]*], [*x[4], x[3],
    x[1], x[2]*], and [*x[1], x[4], x[2], x[3]*]. Remember that the autoregressive
    model allows the current element to attend only to the preceding elements of the
    sequence. Therefore, under normal circumstances, *x[3]* would be able to attend
    only to *x[1]* and *x[2]*. But the XLNet algorithm trains the model not only with
    the regular sequence, but also with different factorization orders of that sequence.
    Therefore, the model will *see* all four factorization orders as well as the original
    input. For example, with [*x[3], x[2], x[4], x[1]*], *x[3]* will not be able to
    attend to any of the other elements, because it's the first one. Alternatively,
    with [*x[2], x[4], x[3], x[1]*], *x[3]* will be able to attend to *x[2]* and *x[4]*.
    Under the previous circumstances, *x[4]* would have been inaccessible. The black
    arrows in the diagram indicate the elements that *x[3]* can attend to, depending
    on the factorization order (the unavailable elements have no arrows).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个长度为 *T* 的序列，存在 *T*! 种有效的自回归因式分解顺序。假设我们有一个长度为 4 的输入序列 [*x[1], x[2], x[3],
    x[4]*]。图中展示了该序列的四种可能的 4! = 24 种因式分解顺序（从左上角顺时针开始）：[*x[3], x[2], x[4], x[1]*]，[*x[2],
    x[4], x[3], x[1]*]，[*x[4], x[3], x[1], x[2]*]，以及 [*x[1], x[4], x[2], x[3]*]。记住，自回归模型允许当前元素只关注序列中的前一个元素。因此，在正常情况下，*x[3]*
    只能关注 *x[1]* 和 *x[2]*。但 XLNet 算法不仅用常规的序列训练模型，还用该序列的不同因式分解顺序进行训练。因此，模型将“看到”所有四种因式分解顺序以及原始输入。例如，在
    [*x[3], x[2], x[4], x[1]*] 中，*x[3]* 将无法关注其他任何元素，因为它是第一个元素。或者，在 [*x[2], x[4], x[3],
    x[1]*] 中，*x[3]* 将能够关注 *x[2]* 和 *x[4]*。在之前的情况下，*x[4]* 是无法访问的。图中的黑色箭头指示了 *x[3]*
    根据因式分解顺序可以关注的元素（不可用的元素没有箭头）。
- en: But how can this work, and what is the point of the training when the sequence
    will lose its meaning if it's not in its natural order? To answer this, let's
    remember that the transformer has no implicit recurrent mechanism, and instead,
    we convey the position of the elements with the explicit positional encodings.
    Let's also remember that in the regular transformer decoder, we use the self-attention
    mask to limit the access to the sequence elements following the current one. When
    we feed a sequence with another factorization order, say [*x[2], x[4], x[3], x[1]*],
    the elements of the sequence will maintain their original positional encoding
    and the transformer will not lose their correct order. In fact, the input is still
    the original sequence [*x[1], x[2], x[3], x[4]*], but with an **altered attention
    mask** to provide access only to elements *x[2]* and *x[4]*.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这怎么可能有效？当序列如果不按自然顺序排列会失去意义时，训练的意义又何在呢？为了回答这个问题，我们需要记住，变压器没有隐式的递归机制，而是通过显式的位置信息编码来传递元素的位置。还要记住，在常规的变压器解码器中，我们使用自注意力掩码来限制对当前序列元素后续元素的访问。当我们输入一个具有其他因式分解顺序的序列，例如 [*x[2],
    x[4], x[3], x[1]*]，序列的元素将保持其原始的位置编码，变压器不会丢失它们的正确顺序。事实上，输入仍然是原始序列 [*x[1], x[2],
    x[3], x[4]*]，但有一个**修改过的注意力掩码**，只允许访问元素 *x[2]* 和 *x[4]*。
- en: 'To formalize this concept, let''s introduce some notations: ![](img/3a1b5515-8dcb-4562-a081-af1fa4ceadf2.png) is
    the set of all possible permutations of the length-*T* index sequence [1, 2, .
    . . , *T*]; ![](img/8a903d12-8bac-423c-a8e6-3df5d4cee11e.png) is one permutation
    of ![](img/3e93fec1-aaa9-44e0-9a1b-1011ba46b66f.png); ![](img/26099f4b-c07a-4e06-a6ba-6ab9a2bbe328.png) is
    the *t*th element of that permutation; ![](img/40c6c4da-6249-46ba-b758-3c5904d09767.png) are
    the first *t-1* elements of that permutation, and ![](img/2abdc5f8-5de3-429e-af1a-b29d5266370c.png) is
    the probability distribution of the next word ![](img/9b448615-aa0e-4e5e-9684-593bb672c733.png),
    given the current permutation ![](img/3b46504a-f316-4cf4-b448-dcd9ca95e065.png) (the
    autoregressive task, which is the output of the model), where θ are the model
    parameters. Then, the permutation language modeling objective is as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形式化这个概念，我们引入一些符号：![](img/3a1b5515-8dcb-4562-a081-af1fa4ceadf2.png)是长度为*T*的索引序列[1,
    2, . . . , *T*]的所有可能排列的集合；![](img/8a903d12-8bac-423c-a8e6-3df5d4cee11e.png)是![](img/3e93fec1-aaa9-44e0-9a1b-1011ba46b66f.png)的一个排列；![](img/26099f4b-c07a-4e06-a6ba-6ab9a2bbe328.png)是该排列的第*t*个元素；![](img/40c6c4da-6249-46ba-b758-3c5904d09767.png)是该排列的前*t-1*个元素，而![](img/2abdc5f8-5de3-429e-af1a-b29d5266370c.png)是给定当前排列![](img/3b46504a-f316-4cf4-b448-dcd9ca95e065.png)下，下一单词![](img/9b448615-aa0e-4e5e-9684-593bb672c733.png)的概率分布（自回归任务，即模型的输出），其中θ是模型参数。然后，排列语言建模目标如下：
- en: '![](img/68a26555-aaca-4e73-bdad-7aaad53dd649.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68a26555-aaca-4e73-bdad-7aaad53dd649.png)'
- en: It samples different factorization orders of the input sequence one at a time
    and attempts to maximize the probability ![](img/715f3a1e-4047-41f6-9dd9-23af7a3786bb.png)—that
    is, to increase the chance of the model to predict the correct word. The parameters θ
    are shared across all factorization orders; therefore, the model will be able
    to see every possible element ![](img/585d38cf-1b5b-4953-872f-6e839aec3abc.png),
    thereby emulating bidirectional context. At the same time, this is still an autoregressive
    function, which doesn't need `[MASK]` tokens.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 它一次性采样输入序列的不同因式分解顺序，并试图最大化概率![](img/715f3a1e-4047-41f6-9dd9-23af7a3786bb.png)—也就是说，增加模型预测正确单词的概率。参数θ在所有因式分解顺序中是共享的；因此，模型能够看到每一个可能的元素![](img/585d38cf-1b5b-4953-872f-6e839aec3abc.png)，从而模拟双向上下文。同时，这仍然是一个自回归函数，不需要`[MASK]`标记。
- en: 'We need one more piece to fully utilize the permutation-based pretraining.
    We''ll start by defining the probability distribution of the next word ![](img/5799d957-0303-4627-be58-7984f859a9c5.png),
    given the current permutation ![](img/0a1a99a8-7bb7-41f7-ae26-dc6b87be0247.png) (the
    autoregressive task, which is the output of the model) ![](img/1abef19f-67ad-4069-8b73-379d19a45b19.png),
    which is simply the softmax output of the model:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用基于排列的预训练，我们还需要一个组成部分。我们首先定义给定当前排列![](img/0a1a99a8-7bb7-41f7-ae26-dc6b87be0247.png)（自回归任务，即模型的输出）下，下一单词![](img/5799d957-0303-4627-be58-7984f859a9c5.png)的概率分布![](img/1abef19f-67ad-4069-8b73-379d19a45b19.png)，这简单来说就是模型的softmax输出：
- en: '![](img/ba32b664-9c7b-4447-837c-8d61e7de2889.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba32b664-9c7b-4447-837c-8d61e7de2889.png)'
- en: Here, ![](img/b9123cb8-acb5-4793-b2a8-5b424dadacb3.png) acts as the query and ![](img/5e4e171d-48b8-4143-9a62-9938cd9309f9.png) is
    the hidden representation produced by the transformer after the proper masking,
    which acts as the key-value database.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/b9123cb8-acb5-4793-b2a8-5b424dadacb3.png)作为查询，![](img/5e4e171d-48b8-4143-9a62-9938cd9309f9.png)是经过适当掩蔽处理后由变换器生成的隐藏表示，充当键值数据库。
- en: 'Next, let''s assume that we have two factorization orders ![](img/a1d6ae4d-fc14-4d73-abb8-22182e3e58da.png) and
    ![](img/cee30934-7d1d-4415-97e9-76e18ed54cc8.png), where the first two elements
    are the same and the second two are swapped. Let''s also assume that *t = 3—*that
    is, the model has to predict the third element of the sequence. Since ![](img/177c0cfc-ffe1-497b-ad7c-39b542595b68.png), we
    can see that ![](img/651f9b80-ff5e-498d-b921-9c3a06ce647d.png) will be the same
    in both cases. Therefore, ![](img/9aac910e-9bcd-425f-ab54-708546ddc45c.png). But
    this is not a valid result, because in the first case, the model should predict
    *x[4]* and in the second, *x[1]*. Let''s remember that although we predict *x[1]*
    and *x[4]* in position 3, they still maintain their original positional encodings.
    Therefore, we can alter the current formula to include the positional information
    for the predicted element (which will be different for *x[1]* and *x[4]*), but
    exclude the actual word. In other words, we can modify the task of the model from
    *predict the next word* to *predict the next word, given that we know its position*.
    In this way, the formula for the two-sample factorization orders will be different.
    The modified formula is as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设我们有两种因式分解顺序 ![](img/a1d6ae4d-fc14-4d73-abb8-22182e3e58da.png) 和 ![](img/cee30934-7d1d-4415-97e9-76e18ed54cc8.png)，其中前两个元素相同，后两个元素交换。我们还假设 *t
    = 3* ——也就是说，模型需要预测序列中的第三个元素。由于 ![](img/177c0cfc-ffe1-497b-ad7c-39b542595b68.png)，我们可以看到 ![](img/651f9b80-ff5e-498d-b921-9c3a06ce647d.png) 在两种情况下是相同的。因此， ![](img/9aac910e-9bcd-425f-ab54-708546ddc45c.png)。但是这是一个无效的结果，因为在第一种情况下，模型应该预测
    *x[4]*，而在第二种情况下，应该预测 *x[1]*。让我们记住，尽管我们在位置3预测了 *x[1]* 和 *x[4]*，但它们仍然保持其原始的位置编码。因此，我们可以修改当前的公式，加入预测元素的位置相关信息（对于 *x[1]*
    和 *x[4]*，这个信息是不同的），但排除实际单词。换句话说，我们可以将模型的任务从 *预测下一个单词* 修改为 *预测下一个单词，前提是我们知道它的位置*。这样，两种样本因式分解顺序的公式将不同。修改后的公式如下：
- en: '![](img/6dd9628b-fd10-48df-b4f6-8556ead937ac.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dd9628b-fd10-48df-b4f6-8556ead937ac.png)'
- en: 'Here, ![](img/9996236e-51b9-4e90-8f7c-b15b9f2e8da6.png) is the new transformer
    function, which also includes the positional information ![](img/c4e80685-bb95-4c5f-9758-0bd6a64a85b6.png). The
    authors of the paper propose a special mechanism called two-stream self-attention
    to solve this. As the name suggests, it consists of two combined attention mechanisms:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， ![](img/9996236e-51b9-4e90-8f7c-b15b9f2e8da6.png) 是新的变换器函数，它还包括位置相关信息 ![](img/c4e80685-bb95-4c5f-9758-0bd6a64a85b6.png)。论文的作者提出了一种名为“两流自注意力”的特殊机制来解决这个问题。顾名思义，它由两种结合的注意力机制组成：
- en: Content representation [![](img/ceef0807-aa8b-47c3-b1bf-fd581358f7e6.png)],
    which is the attention mechanism we are already familiar with. This representation
    encodes both the context and the content [![](img/c8cd3e08-f801-4f64-b2d5-cde6db23b1dd.png)] itself.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容表示 [![](img/ceef0807-aa8b-47c3-b1bf-fd581358f7e6.png)]，这就是我们已经熟悉的注意力机制。该表示既编码上下文，也编码内容本身
    [![](img/c8cd3e08-f801-4f64-b2d5-cde6db23b1dd.png)]。
- en: Query representation ![](img/e6387d63-4dbd-4c88-b999-b2ee6b4794d7.png), which
    only has access to the contextual information ![](img/550a6616-0477-4b3a-99e6-30a58743f86a.png) and
    the position ![](img/bbe2dc8d-31f8-46e5-91e4-b026d1092c31.png), but not the content
    [![](img/ba4e9b81-5a07-44f2-8011-1451697da9ea.png)], as we mentioned previously.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询表示 ![](img/e6387d63-4dbd-4c88-b999-b2ee6b4794d7.png)，它只访问上下文信息 ![](img/550a6616-0477-4b3a-99e6-30a58743f86a.png) 和位置 ![](img/bbe2dc8d-31f8-46e5-91e4-b026d1092c31.png)，但无法访问内容
    [![](img/ba4e9b81-5a07-44f2-8011-1451697da9ea.png)]，正如我们之前提到的。
- en: I would encourage you to check the original paper for more details. In the next
    section, we'll implement a basic example of a transformer language model.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你查阅原文以了解更多详细信息。在下一节中，我们将实现一个变换器语言模型的基本示例。
- en: Generating text with a transformer language model
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用变换器语言模型生成文本
- en: 'In this section, we''ll implement a basic text-generation example with the
    help of the `transformers` 2.1.1 library ([https://huggingface.co/transformers/](https://huggingface.co/transformers/)),
    released by Hugging Face. This is a well-maintained and popular open source package
    that implements different transformer language models, including BERT, transformer-XL,
    XLNet, OpenAI GPT, GPT-2, and others. We''ll use a pretrained transformer-XL model
    to generate new text based on an initial input sequence. The goal is to give you
    a brief taste of the library:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将借助`transformers` 2.1.1库（[https://huggingface.co/transformers/](https://huggingface.co/transformers/)），由Hugging
    Face发布，实现一个基本的文本生成示例。这个库是一个维护良好的流行开源包，包含了不同的Transformer语言模型，如BERT、transformer-XL、XLNet、OpenAI
    GPT、GPT-2等。我们将使用一个预训练的transformer-XL模型，根据初始输入序列生成新的文本。目标是让你简要了解这个库：
- en: 'Let''s start with the imports:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从导入开始：
- en: '[PRE31]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `TransfoXLLMHeadModel` and `TransfoXLTokenizer` phrases are the implementations
    of the transformer-XL language model and its corresponding tokenizer.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransfoXLLMHeadModel`和`TransfoXLTokenizer`是transformer-XL语言模型及其相应分词器的实现。'
- en: 'Next, we''ll initialize the device and instantiate the `model` and the `tokenizer`.
    Note that we''ll use the `transfo-xl-wt103` pretrained set of parameters, available
    in the library:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化设备并实例化`model`和`tokenizer`。请注意，我们将使用库中可用的`transfo-xl-wt103`预训练参数集：
- en: '[PRE32]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we''ll specify the initial sequence, tokenize it, and turn it into a model-compatible
    input `tokens_tensor`, which contains a list of tokens:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将指定初始序列，对其进行分词，并将其转化为一个与模型兼容的输入`tokens_tensor`，该输入包含一个token列表：
- en: '[PRE33]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we''ll use this token to initiate a loop, where the model will generate
    new tokens of the sequence:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这个token来启动一个循环，在这个循环中，模型将生成序列的新token：
- en: '[PRE34]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We start the loop with the initial sequence of tokens `tokens_tensor`. The model
    uses this to generate the `predictions` (a softmax over all tokens of the vocabulary)
    and `mems` (a variable that stores the previous hidden decoder state for the recurrence
    relation). We extract the index of the most probable word `predicted_index` and
    we convert it to a vocabulary token `predicted_token`. Then, we append it to the
    existing `tokens_tensor` and initiate the loop again with the new sequence. The
    loop ends either after 50 tokens or when the special `[EOS]` token is reached.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从初始的token序列`tokens_tensor`开始循环。模型利用它生成`predictions`（对词汇表中所有token的softmax）和`mems`（存储先前隐藏解码器状态的变量，用于递归关系）。我们提取出最可能的词的索引`predicted_index`，并将其转换为一个词汇表token`predicted_token`。然后，我们将其附加到现有的`tokens_tensor`中，并使用新序列重新启动循环。循环将在生成50个token后结束，或者当遇到特殊的`[EOS]`
    token时结束。
- en: 'Finally, we''ll display the result:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将展示结果：
- en: '[PRE35]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output of the program is as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的输出如下：
- en: '[PRE36]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: With this example, we conclude a long chapter about attention models.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个示例，我们结束了关于注意力模型的长篇讨论。
- en: Summary
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on seq2seq models and the attention mechanism. First,
    we discussed and implemented a regular recurrent encoder-decoder seq2seq model
    and learned how to complement it with the attention mechanism. Then, we talked
    about and implemented a purely attention-based type of model called a **transformer**. We
    also defined multihead attention in their context. Next, we discussed transformer
    language models (such as BERT, transformerXL, and XLNet). Finally, we implemented
    a simple text-generation example using the `transformers` library.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点讨论了seq2seq模型和注意力机制。首先，我们讨论并实现了一个常规的循环编码器-解码器seq2seq模型，并学习了如何用注意力机制来补充它。接着，我们讨论并实现了一种纯粹基于注意力机制的模型，叫做**Transformer**。我们还在其背景下定义了多头注意力。然后，我们讨论了transformer语言模型（如BERT、transformerXL和XLNet）。最后，我们实现了一个简单的文本生成示例，使用`transformers`库。
- en: This chapter concludes our series of chapters with a focus on natural language
    processing. In the next chapter, we'll talk about some new trends in deep learning
    that aren't fully matured yet but hold great potential for the future.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了我们关于自然语言处理的系列章节。在下一章中，我们将讨论一些尚未完全成熟，但对未来充满巨大潜力的深度学习新趋势。
