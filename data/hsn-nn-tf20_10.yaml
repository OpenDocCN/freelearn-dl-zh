- en: Introduction to Object Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测简介
- en: Detecting and classifying objects in images is a challenging problem. So far,
    we have treated the issue of image classification on a simple level; in a real-life
    scenario, we are unlikely to have pictures containing just one object. In industrial
    environments, it is possible to set up cameras and mechanical supports to capture
    images of single objects. However, even in constrained environments, such as an
    industrial one, it is not always possible to have such a strict setup. Smartphone
    applications, automated guided vehicles, and, more generally, any real-life application
    that uses images captured in a non-controlled environment require the simultaneous
    localization and classification of several objects in the input images. Object
    detection is the process of localizing an object into an image by predicting the
    coordinates of a bounding box that contains it, while at the same time correctly
    classifying it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中检测和分类物体是一个具有挑战性的问题。到目前为止，我们在简单层面上处理了图像分类的问题；但在现实场景中，我们不太可能只拥有包含一个物体的图像。在工业环境中，可以设置相机和机械支撑来捕捉单个物体的图像。然而，即使在像工业这样的受限环境中，也不总是能够拥有如此严格的设置。智能手机应用、自动化引导车辆，以及更一般的，任何在非受控环境中捕捉图像的现实应用，都需要在输入图像中同时进行多个物体的定位和分类。物体检测是通过预测包含物体的边界框的坐标来定位图像中的物体，同时正确分类它的过程。
- en: State-of-the-art methods to tackle object detection problems are based on convolutional
    neural networks that, as we will see in this chapter, can be used not only to
    extract meaningful classification features but also to regress the coordinates
    of the bounding box. Being a challenging problem, it is better to start with the
    foundations. Detecting and classifying more than one object at the same time requires
    the convolutional architecture to be designed and trained in a more complicated
    way than the one needed to solve the same problem with a single object. The tasks
    of regressing the bounding box coordinates of a single object and classifying
    the content are called **localization and classification**. Solving this task
    is the starting point to develop more complicated architectures that address the
    object detection task.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 解决物体检测问题的最先进方法基于卷积神经网络，正如我们在本章中将看到的，它不仅可以用于提取有意义的分类特征，还可以回归边界框的坐标。由于这是一个具有挑战性的问题，因此最好从基础开始。检测和分类多个物体比仅解决单一物体问题需要更复杂的卷积架构设计和训练。回归单个物体的边界框坐标并对内容进行分类的任务被称为**定位和分类**。解决此任务是开发更复杂架构以解决物体检测任务的起点。
- en: In this chapter, we will look at both problems; we start from the foundations,
    developing a regression network completely, and then extending it to perform both
    regression and classification. The chapter ends with an introduction to anchor-based
    detectors, since a complete implementation of an object detection network goes
    beyond the scope of this book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究这两个问题；我们从基础开始，完全开发一个回归网络，然后将其扩展为同时执行回归和分类。章节最后将介绍基于锚点的检测器，因为完整实现物体检测网络超出了本书的范围。
- en: The dataset used throughout the chapter is the PASCAL Visual Object Classes
    Challenge 2007.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集是 PASCAL Visual Object Classes Challenge 2007。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Getting the data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Object localization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体定位
- en: Classification and localization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类与定位
- en: Getting the data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: Object detection is a supervised learning problem that requires a considerable
    amount of data to reach good performance. The process of carefully annotating
    images by drawing bounding boxes around the objects and assigning them the correct
    labels is a time-consuming process that requires several hours of repetitive work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测是一个监督学习问题，需要大量的数据才能达到良好的性能。通过在物体周围绘制边界框并为其分配正确标签，仔细注释图像的过程是一个费时的过程，需要几个小时的重复工作。
- en: Fortunately, there are already several datasets for object detection that are
    ready to use. The most famous is the ImageNet dataset, immediately followed by
    the PASCAL VOC 2007 dataset. To be able to use ImageNet, dedicated hardware is
    required since its size and number of labeled objects per image makes the object
    detection task hard to tackle.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，已经有几个现成可用的物体检测数据集。最著名的是ImageNet数据集，紧随其后的是PASCAL VOC 2007数据集。要能够使用ImageNet，需要专门的硬件，因为它的大小和每张图片中标注的物体数量使得物体检测任务难以完成。
- en: 'PASCAL VOC 2007, instead, consists of only 9,963 images in total, each of them
    with a different number of labeled objects belonging to the 20 selected object
    classes. The twenty object classes are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，PASCAL VOC 2007只包含9,963张图像，每张图像中标注的物体数量不同，且属于20个选定的物体类别。20个物体类别如下：
- en: '**Person**: Person'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Person**: 人物'
- en: '**Animal**: Bird, cat, cow, dog, horse, sheep'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Animal**: 鸟、猫、牛、狗、马、羊'
- en: '**Vehicle**: Airplane, bicycle, boat, bus, car, motorbike, train'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vehicle**: 飞机、自行车、船、公共汽车、汽车、摩托车、火车'
- en: '**Indoor**: Bottle, chair, dining table, potted plant, sofa, tv/monitor'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Indoor**: 瓶子、椅子、餐桌、盆栽植物、沙发、电视/显示器'
- en: As described in the official dataset page ([http://host.robots.ox.ac.uk/pascal/VOC/voc2007/](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)),
    the dataset already comes with three splits (train, validation, and test) ready
    to use. The data has been split into 50% for training/validation and 50% for testing.
    The distributions of images and objects by class are approximately equal across
    the training/validation and test sets. In total there are 9,963 images, containing
    24,640 annotated objects.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如官方数据集页面所述（[http://host.robots.ox.ac.uk/pascal/VOC/voc2007/](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)），该数据集已经分为三个部分（训练、验证和测试）可供使用。数据已经被划分为50%的训练/验证集和50%的测试集。各类图像和物体的分布在训练/验证集和测试集之间大致相等。总共有9,963张图像，包含24,640个标注的物体。
- en: 'TensorFlow datasets allow us to download the whole dataset with a single line
    of code (approximately 869 MiB) and to obtain the `tf.data.Dataset` object of
    every split:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow数据集允许我们通过一行代码下载整个数据集（约869 MiB），并获取每个分割的`tf.data.Dataset`对象：
- en: '`(tf2)`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As usual, TensorFlow datasets give us a lot of useful information about the
    dataset format. The output that follows is the result of `print(info)`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，TensorFlow数据集提供了很多关于数据集格式的有用信息。以下输出是`print(info)`的结果：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For every image, there is a `SequenceDict` object that contains the information
    of every labeled object present. Something handy when working with any data related
    project is to visualize the data. In this case in particular, since we are trying
    to solve a computer vision problem, visualizing the images and the bounding boxes
    can help us to have a better understanding of the difficulties the network should
    face during the training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每张图像，都有一个`SequenceDict`对象，其中包含每个标注物体的信息。在处理任何数据相关项目时，查看数据非常方便。在这个案例中，特别是因为我们正在解决一个计算机视觉问题，查看图像和边界框可以帮助我们更好地理解网络在训练过程中应该面对的难题。
- en: 'To visualize the labeled images, we use `matplotlib.pyplot` together with the
    usage of the `tf.image` package; the former is used to display the images, and
    the latter is used to draw the bounding boxes and to convert them to `tf.float32`
    (thus scaling the values in the [0,1] range). Moreover, how to use the `tfds.ClassLabel.int2str` method
    is shown; this method is convenient since it allows us to get the text representation
    of a label from its numerical representation:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化标注图像，我们使用`matplotlib.pyplot`结合使用`tf.image`包；前者用于显示图像，后者用于绘制边界框并将其转换为`tf.float32`（从而将值缩放到[0,1]的范围内）。此外，演示了如何使用`tfds.ClassLabel.int2str`方法；这个方法非常方便，因为它允许我们从标签的数值表示中获取文本表示：
- en: '`(tf2)`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the training set, take five images, draw the bounding box, and then print
    the class:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练集获取五张图像，绘制边界框，然后打印类别：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, plot the image using the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下代码绘制图像：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following images are a collage of the five images produced by the code
    snippet:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像是由代码片段生成的五张图像的拼贴画：
- en: '![](img/a0987746-b915-42a3-80cc-3e059f3178d9.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0987746-b915-42a3-80cc-3e059f3178d9.png)'
- en: Please note that since TensorFlow dataset shuffles the data when it creates
    the TFRecords, it is unlikely that the same execution on a different machine will
    produce the same sequence of images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于TensorFlow数据集在创建TFRecords时会对数据进行打乱，因此在不同机器上执行相同的操作时，不太可能产生相同的图像顺序。
- en: It is also worth noting that partial objects are annotated as full objects;
    for instance, the human hand on the bottom-left image is labeled as a person,
    and the rear wheel of a motorbike present in the bottom right of the picture is
    marked as a motorbike .
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，部分物体被标注为完整物体；例如，左下角图像中的人类手被标记为一个人，图片右下角的摩托车后轮被标记为摩托车。
- en: 'The object detection task is challenging by its nature, but looking at the
    data, we can see that the data itself is hard to use. In fact, the labels printed
    to the standard output for the bottom-right image are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测任务本质上具有挑战性，但通过查看数据，我们可以看到数据本身很难使用。事实上，打印到标准输出的右下角图像的标签是：
- en: Person
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人物
- en: Bird
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟类
- en: 'Thus, the dataset contains full objects annotated and labeled (bird) and also
    partial objects annotated and labeled as the whole object (for example, the human
    hand is labeled as a person). This small example shows how difficult object detection
    is: the network should be able to classify and localize, for example, people from
    their attributes (a hand) or from their complete shape while dealing with the
    problem of occlusion.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集包含了完整的物体标注和标签（鸟类），以及部分物体标注并被标记为完整物体（例如，人类的手被标记为一个人）。这个简单的例子展示了物体检测的困难：网络应该能够根据属性（如手）或完整形状（如人）进行分类和定位，同时解决遮挡问题。
- en: 'Looking at the data gives us a better idea of how challenging the problem is.
    However, before facing the challenge of object detection, it is better to start
    from the foundations by tackling the problem of localization and classification.
    Therefore, we have to filter the dataset objects to extract only the images that
    contain a single labeled object. For this reason, a simple function that accepts
    a `tf.data.Dataset` object as input and applies a filter on it can be defined
    and used. Create a subset of the dataset by filtering the elements: we are interested in
    creating a dataset for object detection and classification, that is, a dataset
    of images with a single object annotated:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据让我们更清楚问题的挑战性。然而，在面对物体检测的挑战之前，最好从基础开始，先解决定位和分类的问题。因此，我们必须过滤数据集中的物体，仅提取包含单个标注物体的图像。为此，可以定义并使用一个简单的函数，该函数接受`tf.data.Dataset`对象作为输入并对其进行过滤。通过过滤元素创建数据集的子集：我们感兴趣的是创建一个用于物体检测和分类的数据集，即一个包含单个标注物体的图像数据集：
- en: '`(tf2)`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using the same snippet as earlier, we can visualize some of the images to check
    if everything goes as we expect:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的代码片段，我们可以可视化一些图像，以检查是否一切如我们所预期：
- en: '![](img/1e52f0ef-e349-41e4-9cc1-69633671986f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e52f0ef-e349-41e4-9cc1-69633671986f.png)'
- en: We can see images that contain only a single object, sampled from the training
    set, drawn using the previous code snippet after applying the `filter` function.
    The `filter` function returns a new dataset that contains only the elements of
    the input dataset that contain a single bounding box, hence the perfect candidates
    to train a single network for classification and localization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到从训练集抽取的、只包含单个物体的图像，使用之前的代码片段应用`filter`函数后绘制出来。`filter`函数返回一个新的数据集，该数据集仅包含输入数据集中包含单个边界框的元素，因此它们是训练单个网络进行分类和定位的完美候选。
- en: Object localization
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体定位
- en: 'Convolutional neural networks (CNNs) are extremely flexible objects—so far,
    we have used them to solve classification problems, making them learn to extract
    features specific to the task. As shown in [Chapter 6](699c6d94-72fa-4636-8e23-6da8928847b6.xhtml), *Image
    Classification Using TensorFlow Hub*, the standard architecture of CNNs designed
    to classify images is made of two parts—the feature extractor, which produces
    a feature vector, and a set of fully connected layers that classifies the feature
    vector in the (hopefully) correct class:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是极其灵活的对象——到目前为止，我们已经使用它们解决分类问题，让它们学习提取特定任务的特征。如在[第6章](699c6d94-72fa-4636-8e23-6da8928847b6.xhtml)《使用TensorFlow
    Hub进行图像分类》中所示，设计用于分类图像的CNN标准架构由两部分组成——特征提取器，它生成特征向量，以及一组全连接层，用于将特征向量分类到（希望是）正确的类别：
- en: '![](img/78335c1a-7738-41d9-8705-3b16ad17e97e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78335c1a-7738-41d9-8705-3b16ad17e97e.png)'
- en: The classifier placed on top of the feature vector can also be seen as the head
    of the network
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 放置在特征向量顶部的分类器也可以看作是网络的头部
- en: The fact that, so far, CNNs have only been used to solve classification problems
    should not mislead us. These types of networks are extremely powerful, and, especially
    in their multilayer setting, they can be used to solve many different kinds of
    problems, extracting information from the visual input.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，卷积神经网络（CNN）仅被用来解决分类问题，这一点不应误导我们。这些类型的网络非常强大，特别是在多层设置下，它们可以用来解决多种不同类型的问题，从视觉输入中提取信息。
- en: For this reason, solving the localization and classification problem is just
    a matter of adding a new head to the network, the localization head.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，解决定位和分类问题的关键只是向网络中添加一个新的头，即定位头。
- en: The input data is an image that contains a single object together with the four
    coordinates of the bounding box. So the idea is to use this information to solve
    the classification and localization problem at the same time by treating the localization
    as a regression problem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据是一张包含单一物体以及边界框四个坐标的图像。因此，目标是利用这些信息通过将定位问题视为回归问题，来同时解决分类和定位问题。
- en: Localization as a regression problem
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将定位视为回归问题
- en: Ignoring for a moment the classification problem and focusing only on the localization
    part, we can think about the localization as the problem of regressing the four
    coordinates of the bounding box that contains the subject of the input image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时忽略分类问题，专注于定位部分，我们可以将定位问题视为回归输入图像中包含物体的边界框的四个坐标的问题。
- en: 'In practice, there is not much difference between training a CNN to solve a
    classification task or a regression task: the architecture of the feature extractor
    remains the same, while the classification head changes and becomes a regression
    head. In the very end, this only means to change the number of output neurons
    from the number of classes to 4, one neuron per coordinate of the bounding box.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，训练 CNN 来解决分类任务或回归任务并没有太大区别：特征提取器的架构保持不变，而分类头则变成回归头。最终，这只是意味着将输出神经元的数量从类别数更改为
    4，每个坐标一个神经元。
- en: The idea is that the regression head should learn to output the correct coordinates
    when certain input features are present.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其理念是，当某些输入特征存在时，回归头应该学习输出正确的坐标。
- en: '![](img/a0293f3f-3b42-4702-bb64-3599c35c2d81.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0293f3f-3b42-4702-bb64-3599c35c2d81.png)'
- en: The AlexNet architecture used as a feature extractor and the classification
    head replaced with a regression head with four output neurons
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AlexNet 架构作为特征提取器，并将分类头替换为一个具有四个输出神经元的回归头
- en: To make the network learn to regress the coordinates of the bounding box of
    an object, we have to express the input/output relationship between the neurons
    and the labels (that is, the four coordinates of the bounding box, present in
    the dataset) using a loss function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使网络学习回归物体边界框的坐标，我们必须使用损失函数来表达神经元和标签之间的输入/输出关系（即数据集中存在的边界框四个坐标）。
- en: 'The L2 distance can be effectively used as the loss function: the goal is to
    regress correctly all the four coordinates and thus minimize the distance between
    the predicted values and the real one, making it tend to zero:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: L2 距离可以有效地用作损失函数：目标是正确回归四个坐标，从而最小化预测值与真实值之间的距离，使其趋近于零：
- en: '![](img/36609293-b950-431f-818c-ffc25791c50b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36609293-b950-431f-818c-ffc25791c50b.png)'
- en: Where the first tuple [![](img/e8fd1f5e-9e42-4b2c-8952-cec3d47769c6.png)] is
    the regression head output, and the second tuple [![](img/be5cedad-3e23-4a0e-8dd2-acd229159641.png)] represents
    the ground truth bounding box coordinates.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个元组 [![](img/e8fd1f5e-9e42-4b2c-8952-cec3d47769c6.png)] 是回归头输出，第二个元组 [![](img/be5cedad-3e23-4a0e-8dd2-acd229159641.png)]
    表示真实的边界框坐标。
- en: Implementing a regression network in TensorFlow 2.0 is straightforward. As shown
    in [Chapter 6](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&action=edit#post_30), *Image
    Classification Using TensorFlow Hub*, TensorFlow Hub can be used to speed up the
    training phase by using it to download and embed a pretrained feature extractor
    in our model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 2.0 中实现回归网络是直接的。如 [第 6 章](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&action=edit#post_30)
    《*使用 TensorFlow Hub 进行图像分类*》所示，可以通过使用 TensorFlow Hub 下载并嵌入预训练的特征提取器来加速训练阶段。
- en: A detail that is worth pointing out is the format that TensorFlow uses to represent
    the bounding box coordinates (and the coordinates in general)—the format used
    is `[ymin, xmin, ymax, xmax]` , and the coordinates are normalized in the [0,1]
    range, in order not to depend on the original image resolution.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 值得指出的一个细节是 TensorFlow 用于表示边界框坐标（以及一般坐标）的方法—使用的格式是`[ymin, xmin, ymax, xmax]`，并且坐标在[0,1]范围内进行归一化，以避免依赖于原始图像分辨率。
- en: Using TensorFlow 2.0 and TensorFlow Hub, we can define and train a coordinate
    regression network on the PASCAL VOC 2007 dataset in a few lines of code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.0 和 TensorFlow Hub，我们可以通过几行代码在 PASCAL VOC 2007 数据集上定义并训练坐标回归网络。
- en: 'Using the Inception v3 network from TensorFlow Hub as the backbone of the coordinate
    regression network, defining the regression model is straightforward. Although
    the network has a sequential structure, we define it using the functional API
    since this will allow us to extend the model easily without the need to rewrite
    it:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自 TensorFlow Hub 的 Inception v3 网络作为坐标回归网络的骨干，定义回归模型是直接的。尽管该网络具有顺序结构，我们通过函数式
    API 定义它，因为这将使我们能够轻松扩展模型，而无需重写：
- en: '`(tf2)`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Moreover, since we decided to use the inception network that needs a 299 x
    299 input image resolution with values in the [0,1] range, we have to add an additional
    step to the input pipeline to prepare the data:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们决定使用需要 299 x 299 输入图像分辨率且值在[0,1]范围内的 Inception 网络，我们需要在输入管道中增加额外的步骤来准备数据：
- en: '`(tf2)`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As previously introduced, the loss function to use is the standard L2 loss,
    which already comes implemented in TensorFlow as a Keras loss that can be found
    in the `tf.losses` package. However, instead of using `tf.losses.MeanSquaredError`,
    it is worth defining the loss function ourselves since there is a detail to highlight.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用的损失函数是标准的 L2 损失，TensorFlow 已经将其作为 Keras 损失实现，可以在`tf.losses`包中找到。然而，值得注意的是，我们自己定义损失函数，而不是使用`tf.losses.MeanSquaredError`，因为有一个细节需要强调。
- en: If we decide to use the implemented **Mean Squared Error** (**MSE**) function,
    we have to take into account that under the hood, the `tf.subtract` operation
    is used. This operation simply computes the subtraction of the left-hand operation
    with the right-hand operand. This behavior is what we are looking for, of course,
    but the subtraction operation in TensorFlow follows the NumPy broadcasting semantic
    (as almost any mathematical operation). This particular semantic broadcasts the
    value of the left-side tensor to the right-side tensor, and if the right-side
    tensor has a dimension of 1 where the value of the left-side tensor is copied.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定使用已实现的**均方误差**（**MSE**）函数，我们必须考虑到，在底层使用了`tf.subtract`操作。该操作仅仅计算左侧操作数与右侧操作数的差值。这种行为是我们所期望的，但
    TensorFlow 中的减法操作遵循 NumPy 的广播语义（几乎所有数学操作都遵循此语义）。这种语义将左侧张量的值广播到右侧张量，如果右侧张量的某个维度为
    1，则会将左侧张量的值复制到该位置。
- en: Since we selected the images with only one object inside, we have a single bounding
    box present in the `"bbox"` attribute. Hence, if we pick a batch size of 32, the
    tensor that contains the bounding box will have a shape of `(32, 1, 4)`. The 1
    in the second position can cause problems in the loss computation and preventing
    the model from converging .
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择的图像中只有一个物体，因此在`"bbox"`属性中只有一个边界框。因此，如果我们选择批处理大小为 32，则包含边界框的张量将具有形状`(32,
    1, 4)`。第二个位置的 1 可能会在损失计算中引起问题，并阻止模型收敛。
- en: 'Thus, we have two options:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个选择：
- en: Define the loss function using Keras, removing the unary dimension by using `tf.squeeze`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 定义损失函数，通过使用`tf.squeeze`去除一维维度
- en: Define the loss function manually
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动定义损失函数
- en: 'In practice, defining the loss function manually allows us to place the `tf.print`
    statements in the body function, which can be used for a raw debugging process
    and, more importantly, to define the training loop in the standard way, making
    the loss function itself taking care of squeezing the unary dimension where needed:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，手动定义损失函数使我们能够在函数体内放置`tf.print`语句，这可以用于原始调试过程，且更重要的是，以标准方式定义训练循环，使得损失函数本身能够处理在需要时去除一维维度。
- en: '`(tf2)`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The training loop is straightforward, and it can be implemented in two different
    ways:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环很简单，可以通过两种不同的方式来实现：
- en: Writing a custom training loop (thus using the `tf.GradientTape` object)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写自定义训练循环（因此使用`tf.GradientTape`对象）
- en: Using the `compile` and `fit` methods of the Keras model, since this is a standard
    training loop that Keras can build for us
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras模型的`compile`和`fit`方法，因为这是Keras为我们构建的标准训练循环。
- en: However, since we are interested in extending this solution in the next sections,
    it is better to start using the custom training loop, as it gives more freedom
    for customization. Moreover, we are interested in visualizing the ground truth
    and the predicted bounding box, by logging them on TensorBoard.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们有兴趣在接下来的章节中扩展此解决方案，最好开始使用自定义训练循环，因为它提供了更多的自定义自由度。此外，我们有兴趣通过在TensorBoard上记录它们来可视化真实值和预测的边界框。
- en: 'Therefore, before defining the training loop, it is worth defining a `draw`
    function that takes a dataset, the model, and the current step, and using them
    to draw both the ground truth and the predicted boxes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在定义训练循环之前，值得定义一个`draw`函数，该函数接受数据集、模型和当前步骤，并利用它们来绘制真实框和预测框：
- en: '`(tf2)`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The training loop for our coordinate regressor (that can also be thought of
    as a region proposal, since it is now aware of the label of the object it''s detecting
    in the images), which also logs on TensorBoard the training loss value and the
    prediction on three images sampled from the training and validation set (using
    the `draw` function), can be easily defined:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的坐标回归器的训练循环（它也可以被视为一个区域提议，因为它现在已经知道它正在图像中检测的物体的标签），同时在TensorBoard上记录训练损失值和来自训练集和验证集的三个样本图像的预测（使用`draw`函数），可以很容易地定义：
- en: 'Define the `global_step` variable, used to keep track of the training iterations,
    followed by the definition of the file writers, used to log the train and validation
    summaries:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`global_step`变量，用于跟踪训练迭代，然后定义文件写入器，用于记录训练和验证摘要：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Following the TensorFlow 2.0 best practice, we can define the training step
    as a function and convert it to its graph representation using `tf.function`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据TensorFlow 2.0的最佳实践，我们可以将训练步骤定义为一个函数，并使用`tf.function`将其转换为图形表示：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the training loop over the batches and invoke the `train_step` function
    on every iteration:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个批次上定义训练循环，并在每次迭代中调用`train_step`函数：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Although the Inception network is used as a fixed feature extractor, the training
    process can take a few hours on a CPU and almost half an hour on a GPU.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用了Inception网络作为固定的特征提取器，但训练过程在CPU上可能需要几个小时，而在GPU上则几乎需要半个小时。
- en: 'The following screenshot shows the visible trend of the loss function during
    the training:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了训练过程中损失函数的可见趋势：
- en: '![](img/065c98e0-6122-4e3a-a8f1-0ae70395309a.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/065c98e0-6122-4e3a-a8f1-0ae70395309a.png)'
- en: We see that from the early training steps, the loss value is close to zero,
    although oscillations are present during the whole training process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，从早期的训练步骤开始，损失值接近零，尽管在整个训练过程中会出现波动。
- en: 'During the training process, in the images tab of TensorBoard, we can visualize
    the images with the regressed and ground truth bounding boxes drawn. Since we
    created two different summary writers (one for the training logs and the other
    for the validation logs), TensorFlow visualizes for us the images logged for the
    two different splits:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，在TensorBoard的图像标签中，我们可以可视化带有回归框和真实边界框的图像。由于我们创建了两个不同的日志记录器（一个用于训练日志，另一个用于验证日志），TensorFlow为我们可视化了两个不同数据集的图像：
- en: '![](img/cf1356bb-95db-44f3-af09-41f9a77c51fd.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf1356bb-95db-44f3-af09-41f9a77c51fd.png)'
- en: The preceding images are samples from the training (first row) and validation
    (second row) set, with the ground truth and regressed bounding boxes. The regressed
    bounding boxes in the training set are close to the ground truth boxes, while
    the regressed boxes on the validation images are different.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像是来自训练集（第一行）和验证集（第二行）的样本，包含真实框和回归边界框。训练集中的回归边界框接近真实框，而验证集中的回归框则有所不同。
- en: 'The training loop previously defined has various problems:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的训练循环存在各种问题：
- en: The only measured metric is the L2 loss
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一被测量的指标是L2损失。
- en: The validation set is never used to measure any numerical score
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集从未用于衡量任何数值分数。
- en: No check for overfitting is present
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有进行过拟合检查。
- en: There is a complete lack of a metric that measures how good the regression of
    the bounding box is, measured on both the training and the validation set
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全缺乏一个衡量回归边界框质量的指标，既没有在训练集上，也没有在验证集上进行衡量。
- en: 'The training loop can, therefore, be improved by measuring an object detection
    metric; measuring the metric also reduces the training time since we can stop
    the training earlier. Moreover, from the visualization of the results, it is pretty
    clear that the model is overfitting the training set, and a regularization layer,
    such as dropout, can be added to address this problem. The problem of regressing
    a bounding box can be treated as a binary classification problem. In fact, there
    are only two possible outcomes: ground truth bounding box matched or not matched.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练循环可以通过测量目标检测指标来改进；测量该指标还可以减少训练时间，因为我们可以提前停止训练。此外，从结果的可视化中可以明显看出，模型正在过拟合训练集，可以添加正则化层（如dropout）来解决这个问题。回归边界框的问题可以视为一个二分类问题。事实上，只有两种可能的结果：真实边界框匹配或不匹配。
- en: Of course, having a perfect match is not an easy task; for this reason, a function
    that measures how good the detected bounding box is with a numerical score (with
    respect to the ground truth) is needed. The most widely used function to measure
    the goodness of localization is the **Intersection over Union** (**IoU**), which
    we will explore in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，达到完美匹配并非易事；因此，需要一个衡量检测到的边界框与真实值之间好坏的数值评分函数。最常用的用于衡量定位好坏的函数是**交并比**（**IoU**），我们将在下一节中详细探讨。
- en: Intersection over Union
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交并比（IoU）
- en: 'Intersection over Union (IoU) is defined as the ratio between the area of overlap
    and the area of union. The following image is a graphical representation of IoU:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 交并比（IoU）定义为重叠区域与并集区域的比率。以下图像是IoU的图示：
- en: '![](img/d078024a-428b-4e25-a723-f3d0c50838d3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d078024a-428b-4e25-a723-f3d0c50838d3.png)'
- en: 'Credits: Jonathan Hui ([https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173))'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 版权归属：Jonathan Hui ([https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173))
- en: 'In practice, the IoU measures *how much* the predicted bounding box overlaps
    with the ground truth. Since IoU is a metric that uses the areas of the objects,
    it can be easily expressed treating the ground truth and the detected area like
    sets. Let A be the set of the proposed object pixels and B the set of true object
    pixels; then IoU is defined as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，IoU衡量的是预测的边界框与真实边界框的*重叠程度*。由于IoU是一个使用物体区域的指标，因此可以很容易地将真实值和检测区域视为集合来表示。设A为提议物体像素的集合，B为真实物体像素的集合；则IoU定义为：
- en: '![](img/79a3faa7-f117-4f73-b9e1-878c13ebcc7e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79a3faa7-f117-4f73-b9e1-878c13ebcc7e.png)'
- en: The IoU value is in the [0,1] range, where 0 is a no-match (no overlap), and
    1 is the perfect match. The IoU value is used as an overlap criterion; usually,
    an IoU value greater than 0.5 is considered as a true positive (match), while
    any other value is regarded as a false positive. There are no true negatives.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: IoU值在[0,1]范围内，其中0表示无匹配（没有重叠），1表示完美匹配。IoU值用作重叠标准；通常，IoU值大于0.5被认为是正匹配（真正例），而其他值被视为假匹配（假正例）。没有真正的负例。
- en: 'Implementing the IoU formula in TensorFlow is straightforward. The only subtlety
    to take into account is that de-normalizing the coordinates is needed since the
    area should be computed in pixels. The conversion in pixel coordinates together
    with the coordinate swapping in a more friendly representation is implemented
    in the `_swap` closure:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现IoU公式非常简单。唯一需要注意的细节是，需要对坐标进行反归一化，因为面积应该以像素为单位来计算。像素坐标的转换以及更友好的坐标交换表示是在`_swap`闭包中实现的：
- en: '`(tf2)`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Transpose the coordinates from `y_min`, `x_min`, `y_max`, and `x_max` in absolute
    coordinates to `x_min`, `y_min`, `x_max`, and `y_max` in pixel coordinates:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将`y_min`、`x_min`、`y_max`和`x_max`的绝对坐标转换为`x_min`、`y_min`、`x_max`和`y_max`的像素坐标：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, compute the width and height of the bounding box:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，计算边界框的宽度和高度：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Average precision
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均精度
- en: A value of IoU greater than a specified threshold (usually 0.5) allows us to
    treat the bounding box regressed as a match.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果IoU值大于指定阈值（通常为0.5），则可以将回归的边界框视为匹配。
- en: 'In the case of single class prediction, having the number of **true positives**
    (**TP**) and **false positives** (**FP**) measured on the dataset allows us to
    compute the average precision as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在单类预测的情况下，计算**真实正例**（**TP**）和**假正例**（**FP**）的数量，能够使我们计算出平均精度，如下所示：
- en: '![](img/377c65ae-3390-47f8-8dbb-9235a35a05bd.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/377c65ae-3390-47f8-8dbb-9235a35a05bd.png)'
- en: In the object detection challenges, the **Average Precision** (**AP**) is often
    measured for different values of IoU. The minimum requirement is to measure the
    AP for an IoU value of 0.5, but achieving good results with a half overlap is
    not sufficient in most real-life scenarios. Usually, in fact, a bounding box prediction
    is required to match at least a value of IoU of 0.75 or 0.85 to be useful.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测挑战中，**平均精度**（**AP**）通常会在不同的IoU值下进行测量。最小要求是对IoU值为0.5时测量AP，但在大多数实际场景中，单纯达到0.5的重叠并不足够。通常情况下，实际上，边界框预测需要至少匹配IoU值为0.75或0.85才能有用。
- en: So far we have treated the AP for the single-class case, but it is worth it
    to treat the more general multi-class object detection scenario.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的是单类情况下的AP，但值得讨论更一般的多类目标检测场景。
- en: Mean Average Precision
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均精度均值
- en: In the case of a multi-class detection, where every bounding box regressed can
    contain one of the available classes, the standard metric used to evaluate the
    performance of the object detector is the **Mean Average Precision** (**mAP**).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类检测的情况下，每个回归的边界框可以包含可用类之一，评估目标检测器性能的标准指标是**平均精度均值**（**mAP**）。
- en: 'Computing it is straightforward—the mAP is the average precision for each class
    in the dataset:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 计算它非常简单——mAP是数据集中每个类别的平均精度：
- en: '![](img/4fe09f9c-4399-4297-a2ed-385872534493.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fe09f9c-4399-4297-a2ed-385872534493.png)'
- en: Knowing the metrics to use for object detection, we can improve the training
    script by adding this measurement on the validation set at the end of every training
    epoch and measuring it on a batch of training data every ten steps. Since the
    model defined so far is just a coordinate regressor without classes, the measured
    metric will be the AP.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 了解用于目标检测的指标后，我们可以通过在每个训练周期结束时，在验证集上添加此测量，并每十步在一批训练数据上进行测量，从而改进训练脚本。由于目前定义的模型仅是一个没有类别的坐标回归器，因此测量的指标将是AP。
- en: 'Implementing the mAP in TensorFlow is trivial since in the `tf.metrics` package
    there is an implementation ready to use. The first parameter of the `update_state`
    method is the true labels; the second parameter is the predicted labels. For instance,
    for a binary classification problem, a possible scenario can be as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现mAP非常简单，因为`tf.metrics`包中已经有现成的实现可用。`update_state`方法的第一个参数是真实标签；第二个参数是预测标签。例如，对于二分类问题，一个可能的场景如下：
- en: '`(tf2)`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It should also be noted that the average precision and the IoU are not object-detection-specific
    metrics, but they can be used whenever a localization task is performed (the IoU)
    and the precision of the detection is measured (the mAP).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 还应注意，平均精度和IoU并不是目标检测专有的指标，但它们可以在执行任何定位任务时使用（IoU）并测量检测精度（mAP）。
- en: In [Chapter 8](51f4dcda-add6-4e58-a660-75f34a7e5593.xhtml), *Semantic Segmentation
    and Custom Dataset Builder*, dedicated to the semantic segmentation task, the
    same metrics are used to measure the segmentation model performance. The only
    difference is that the IoU is measured at the pixel level and not using a bounding
    box. The training loop can be improved; in the next section, a draft of an improved
    training script is presented, but the real improvement is left as an exercise.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](51f4dcda-add6-4e58-a660-75f34a7e5593.xhtml)中，*语义分割与自定义数据集构建器*专门讨论语义分割任务，使用相同的指标来衡量分割模型的性能。唯一的区别是，IoU是以像素级别来衡量的，而不是使用边界框。训练循环可以改进；在下一节中，将展示一个改进后的训练脚本草案，但真正的改进将留作练习。
- en: Improving the training script
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进训练脚本
- en: Measuring the mean average precision (over a single class) requires you to fix
    a threshold for the IoU measurement and to define the `tf.metrics.Precision` object
    that computes the mean average precision over the batches.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 测量平均精度（针对单一类别）需要你为IoU测量设置阈值，并定义`tf.metrics.Precision`对象，该对象计算批次上的平均精度。
- en: 'In order not to change the whole code structure, the `draw` function is used
    not only to draw the ground truth and regressed boxes, but also to measure the
    IoU and log the mean average precision summary:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不改变整个代码结构，`draw`函数不仅用于绘制地面真值和回归框，还用于测量IoU并记录平均精度的总结：
- en: '`(tf2)`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As an exercise (see the *Exercises* section), you can use this code as a baseline
    and restructure them, in order to improve the code organization. After improving
    the code organization, you are also invited to retrain the model and analyze the
    precision plot.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个练习（参见*练习*部分），你可以使用这段代码作为基线并重新组织结构，以便改善代码的组织方式。改善代码组织后，建议重新训练模型并分析精度图。
- en: Object localization alone, without the information about the class of the object
    being localized, has a limited utility, but, in practice, it is the basis of any
    object detection algorithm.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅进行物体定位，而没有关于物体类别的信息，实用性有限，但在实践中，这是任何物体检测算法的基础。
- en: Classification and localization
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类与定位
- en: An architecture like the one defined so far that has no information about the
    class of the object it's localizing is called a** region proposal**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 目前定义的这种架构，没有关于它正在定位的物体类别的信息，称为**区域提议**。
- en: It is possible to perform object detection and localization using a single neural
    network. In fact, there is nothing stopping us adding a second head on top of
    the feature extractor and training it to classify the image and at the same time
    training the regression head to regress the bounding box coordinates.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一神经网络进行物体检测和定位是可行的。事实上，完全可以在特征提取器的顶部添加第二个头，并训练它对图像进行分类，同时训练回归头来回归边界框坐标。
- en: Solving multiple tasks at the same time is the goal of multitask learning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 同时解决多个任务是多任务学习的目标。
- en: Multitask learning
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务学习
- en: 'Rich Caruna defines multi-task learning in his paper *Multi-task learning*
    (1997):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Rich Caruna在他的论文*多任务学习*（1997年）中定义了多任务学习：
- en: '"Multitask Learning is an approach to inductive transfer that improves generalization
    by using the domain information contained in the training signals of related tasks
    as an inductive bias. It does this by learning tasks in parallel while using a
    shared representation; what is learned for each task can help other tasks be learned
    better."'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: “多任务学习是一种归纳迁移方法，它通过利用相关任务训练信号中的领域信息作为归纳偏差来改善泛化能力。它通过并行学习任务，同时使用共享的表示；每个任务学到的内容可以帮助其他任务更好地学习。”
- en: In practice, multi-task learning is a machine learning subfield with the explicit
    goal of solving multiple different tasks, exploiting commonalities and differences
    across tasks. It has been empirically shown that using the same network to solve
    multiple tasks usually results in improved learning efficiency and prediction
    accuracy compared to the performance achieved by the same network trained to solve
    the same tasks separately.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，多任务学习是机器学习的一个子领域，明确的目标是解决多个不同的任务，利用任务之间的共性和差异。经实验证明，使用相同的网络来解决多个任务，通常比使用同一网络分别训练解决每个任务的效果更好，能够提高学习效率和预测准确性。
- en: Multi-task learning also helps to fight the overfitting problem since the neural
    network is less likely to adapt its parameters to solve a specific task, so it
    has to learn how to extract meaningful features that can be useful to solve different
    tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习有助于解决过拟合问题，因为神经网络不太可能将其参数适应于解决一个特定任务，因此它必须学习如何提取对解决不同任务有用的有意义特征。
- en: Double-headed network
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双头网络
- en: In the past few years, several architectures for object detection and classification
    have been developed using a two-step process. The first process was to use a region
    proposal to get regions of the input image that are likely to contain an object.
    The second step was to use a simple classifier on the proposed regions to classify
    the content.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，已经开发了几种用于物体检测和分类的架构，采用两步过程。第一步是使用区域提议获取可能包含物体的输入图像区域。第二步是对提议的区域使用简单的分类器进行分类。
- en: Using a double-headed neural network allows us to have faster inference time,
    since only a single forward pass of a single model is needed to achieve better
    performance overall.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用双头神经网络可以使推理时间更快，因为只需要进行一次单模型的前向传播，就可以实现更好的整体性能。
- en: 'From the architectural side, supposing for simplicity that our feature extractor
    is AlexNet (when, instead, it is the more complex network Inception V3), adding
    a new head to the network changes the model architecture as shown in the following
    screenshot:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构的角度看，假设为了简单起见我们的特征提取器是AlexNet（而实际上是更复杂的网络Inception V3），向网络添加新头会改变模型架构，如下截图所示：
- en: '![](img/01503375-3d15-4fef-a4ff-4e12d4403912.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01503375-3d15-4fef-a4ff-4e12d4403912.png)'
- en: The preceding screenshot is a representation of what a classification and localization
    network looks like. The feature extract part should be able to extract features
    general enough to make the two heads solve the two different tasks, using the
    same shared features.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示了分类和定位网络的样子。特征提取部分应该能够提取足够通用的特征，以使两个头能够使用相同的共享特征解决这两种不同的任务。
- en: 'From the code side, as we used the Keras functional style model definition,
    adding the additional output to the model is straightforward. In fact, it is only
    a matter of adding the desired numbers of layers that compose the new head and
    adding the final layer to the outputs list of the Keras model definition. As may
    be obvious at this point in the book, this second head must end with a number
    of neurons equal to the number of classes the model will be trained to classify.
    In our case, the PASCAL VOC 2007 dataset contains 20 different classes. Therefore,
    we only have to define the model as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码的角度来看，由于我们使用了Keras函数式模型定义，向模型添加额外输出非常简单。实际上，这仅仅是添加组成新头的所需层数，并将最终层添加到Keras模型定义的输出列表中。正如本书到目前为止所展示的，这第二个头必须以等于模型将训练的类别数量的神经元结束。在我们的例子中，PASCAL
    VOC 2007数据集包含20个不同的类别。因此，我们只需要按如下方式定义模型：
- en: '`(tf2)`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf2)`'
- en: 'First, start with the input layer definition:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从输入层定义开始：
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, using TensorFlow Hub, we define the fixed (non-trainable) feature extractor:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用TensorFlow Hub，我们定义固定的（不可训练的）特征提取器：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we define the regression head, which is just a stack of fully connected
    layers that end with four linear neurons (one per bounding box coordinate):'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义回归头，它只是一个由全连接层堆叠组成的部分，最后以四个线性神经元结束（每个边界框坐标一个）：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we define the classification head, which is just a stack of fully connected
    layers that is trained to classify the features extracted by the fixed (non-trainable)
    feature extractor:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义分类头，它只是一个由全连接层堆叠组成的部分，经过训练用于分类由固定（不可训练）特征提取器提取的特征：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we can define the Keras model that will perform classification and
    localization. Please note that the model has a single input and two outputs:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以定义将执行分类和定位的Keras模型。请注意，该模型有一个输入和两个输出：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Using TensorFlow datasets, we have all the information needed to perform both
    the classification and localization easily since every row is a dictionary that
    contains the label for every bounding box present in the image. Moreover, since
    we filtered the dataset in order to have only images with a single object inside,
    we can treat the training of the classification head exactly as the training of
    the classification model, as shown in [Chapter 6](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&action=edit#post_30), *Image
    Classification Using TensorFlow Hub*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow数据集，我们拥有执行分类和定位所需的所有信息，因为每一行都是一个字典，包含图像中每个边界框的标签。此外，由于我们已过滤数据集，确保其中仅包含单个对象的图像，因此我们可以像训练分类模型一样训练分类头，如[第6章](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&action=edit#post_30)所示，*使用TensorFlow
    Hub进行图像分类*。
- en: The implementation of the training script is left as an exercise for you (see
    the *Exercises* section). The only peculiarity of the training process is the
    loss function to use. In order to effectively train the network to perform different
    tasks at the same time, the loss function should contain different terms for each
    different task.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本的实现留作练习（见*练习*部分）。训练过程的唯一特点是要使用的损失函数。为了有效地训练网络同时执行不同任务，损失函数应包含每个任务的不同项。
- en: 'Usually, the weighted sum of different terms is used as a loss function. In
    our case, one term is the classification loss that can easily be the sparse categorical
    cross-entropy loss, and the other is the regression loss (the L2 loss previously
    defined):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用不同项的加权和作为损失函数。在我们的例子中，一项是分类损失，它通常是稀疏类别交叉熵损失，另一项是回归损失（之前定义的L2损失）：
- en: '![](img/9395cba6-22ff-4db6-8f8c-3a9d5d3e36d1.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9395cba6-22ff-4db6-8f8c-3a9d5d3e36d1.png)'
- en: The multiplicative factors [![](img/0006eaf3-b30f-43d7-b932-d3fdf746e9e1.png)] are
    hyperparameters used to give more or less *importance* (strength of the gradient
    update) to the different tasks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法因子 [![](img/0006eaf3-b30f-43d7-b932-d3fdf746e9e1.png)] 是超参数，用来赋予不同任务不同的*重要性*（梯度更新的强度）。
- en: Classifying images with a single object inside and regressing the coordinate
    of the only bounding box present can be applied only in limited real-life scenarios.
    More often, instead, given an input image, it is required to localize and classify
    multiple objects at the same time (the real object detection problem).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一物体的图像进行分类并回归唯一存在的边界框坐标，只能在有限的实际场景中应用。相反，通常情况下，给定输入图像，要求同时定位和分类多个物体（即实际的物体检测问题）。
- en: Over the years, several models for object detection have been proposed, and
    the ones that recently outperformed all the others are all based on the concept
    of anchor. We will explore anchor-based detectors in the following section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已经提出了多种物体检测模型，最近超越所有其他模型的都是基于锚框概念的模型。我们将在下一节探讨基于锚框的检测器。
- en: Anchor-based detectors
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于锚框的检测器
- en: Anchor-based detectors rely upon the concept of anchor boxes to detect objects
    in images in a single pass, using a single architecture.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 基于锚框的检测器依赖锚框的概念，通过单一架构一次性检测图像中的物体。
- en: The intuitive idea of the anchor-based detectors is to split the input image
    into several regions of interests (the anchor boxes) and apply a localization
    and regression network to each of them. The idea is to make the network learn
    not only to regress the coordinates of a bounding box and classify its content,
    but also to use the same network to look at different regions of the image in
    a single forward pass.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 基于锚框的检测器的直观思路是将输入图像划分为多个兴趣区域（锚框），并对每个区域应用定位和回归网络。其核心思想是让网络不仅学习回归边界框的坐标并分类其内容，还要使用同一网络在一次前向传播中查看图像的不同区域。
- en: To train these models, it is required not only to have a dataset with the annotated
    ground truth boxes, but also to add to every input image a new collection of boxes
    that overlap (with the desired amount of IoU) the ground truth boxes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这些模型，不仅需要一个包含注释真实框的数据集，还需要为每张输入图像添加一组新的框，这些框与真实框有一定程度的重叠（具有所需的IoU）。
- en: Anchor-boxes
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 锚框
- en: Anchor-boxes are a discretization of the input image in different regions, also called
    **anchors** or **bounding boxes prior**. The idea behind the concept of anchor-boxes
    is that the input can be discretized in different regions, each of them with a
    different appearance. An input image could contain big and small objects, and
    therefore the discretization should be made at different scales in order to detect
    the same time objects at different resolutions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框是输入图像在不同区域的离散化，也称为**锚点**或**边界框先验**。锚框概念背后的思路是，输入图像可以在不同的区域中离散化，每个区域有不同的外观。一个输入图像可能包含大物体和小物体，因此，离散化应该在不同的尺度下进行，以便同时检测不同分辨率下的物体。
- en: 'When discretizing the input in anchor boxes, the important parameters are as
    follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在将输入离散化为锚框时，重要的参数如下：
- en: '**The grid size:** How the input is evenly divided'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网格大小**：输入图像如何被均匀划分'
- en: '**The box scale levels**: Given the parent box, how to resize the current box'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框的尺度级别**：给定父框，如何调整当前框的大小'
- en: '**The aspect ratio levels**: For every box, the ratio between width and height'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长宽比级别**：对于每个框，宽度与高度之间的比率'
- en: 'The input image can be divided into a grid with cells of equal dimensions,
    for instance, a 4 x 4 grid. Each cell of this grid can then be resized with different
    scales (0.5, 1, 2, ...) and each of them with different levels of aspect ratios
    (0.5, 1, 2, ...). As an example, the following image shows how an image can be
    "covered" by anchor boxes:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像可以被划分为一个均匀尺寸的网格，例如4 x 4网格。这个网格的每个单元格可以用不同的尺度（0.5、1、2等）进行调整，并且每个单元格具有不同的长宽比级别（0.5、1、2等）。例如，以下图片展示了如何用锚框“覆盖”一张图像：
- en: '![](img/7922f964-ec85-43f2-90f6-def4ea0f519f.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7922f964-ec85-43f2-90f6-def4ea0f519f.png)'
- en: The generation of the anchor boxes influences the network performance—the dimension
    of the smaller box represents the dimension of the smaller objects the network
    is able to detect. The same reasoning applies to the larger box as well.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点框的生成影响着网络的性能——较小框的尺寸代表着网络能够检测到的小物体的尺寸。同样的推理也适用于较大框。
- en: In the last few years, anchor-based detectors have demonstrated they are capable
    of reaching astonishing detection performance, being not only accurate, but also
    faster.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，基于锚点的检测器已经证明它们能够达到惊人的检测性能，不仅准确，而且速度更快。
- en: 'The most famous anchor-based detector is **You Only Look Once** (**YOLO**),
    followed by **Single Shot MultiBox Detector** (**SSD**). The following YOLO image detects
    multiple objects in the image, at different scales, with a single forward pass:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的基于锚点的检测器是**You Only Look Once**（**YOLO**），其次是**Single Shot MultiBox Detector**（**SSD**）。以下
    YOLO 图像检测了图像中的多个物体，且在不同的尺度下，仅通过一次前向传播：
- en: '![](img/79700b05-c3c0-4dde-beeb-f8472c0d7817.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79700b05-c3c0-4dde-beeb-f8472c0d7817.png)'
- en: The implementation of an anchor-based detector goes beyond the scope of this
    book due to the theory needed to understand the concepts and the complexity of
    these models. Therefore, only an intuitive idea of what happens when these models
    are used has been presented.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 基于锚点的检测器的实现超出了本书的范围，因为理解这些概念需要相应的理论知识，并且这些模型也非常复杂。因此，仅介绍了使用这些模型时发生的直观想法。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, the problem of object detection was introduced and some basic
    solutions were proposed. We first focused on the data required and used TensorFlow
    datasets to get the PASCAL VOC 2007 dataset ready to use in a few lines of code.
    Then, the problem of using a neural network to regress the coordinate of a bounding
    box was looked at, showing how a convolutional neural network can be easily used
    to produce the four coordinates of a bounding box, starting from the image representation.
    In this way, we build a region proposal, that is, a network able to suggest where
    in the input image a single object can be detected, without producing other information
    about the detected object.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了目标检测的问题，并提出了一些基本的解决方案。我们首先关注了所需的数据，并使用 TensorFlow 数据集获取了可以在几行代码中直接使用的 PASCAL
    VOC 2007 数据集。然后，讨论了如何使用神经网络回归边界框的坐标，展示了如何轻松地利用卷积神经网络从图像表示中生成边界框的四个坐标。通过这种方式，我们构建了区域建议（Region
    Proposal），即一个能够建议在输入图像中检测单个物体的位置的网络，而不产生其他关于检测物体的信息。
- en: After that, the concept of multi-task learning was introduced and how to add
    a classification head next to the regression head was shown by using the Keras
    functional API. Then, we covered a brief introduction about the anchor-based detectors.
    These detectors are used to solve the problem of object detection (the detection
    and classification of multiple objects in a single image) by discretizing the
    input in thousands of regions (anchors).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，介绍了多任务学习的概念，并展示了如何使用 Keras 函数式 API 将分类头与回归头结合。接着，我们简要介绍了基于锚点的检测器。这些检测器通过将输入划分为成千上万个区域（锚点）来解决目标检测的问题（即在单一图像中检测和分类多个物体）。
- en: We used TensorFlow 2.0 and TensorFlow Hub together, allowing us to speed up
    the training process by using the Inception v3 model as a fixed feature extractor.
    Moreover, thanks to the quick execution, mixing pure Python and TensorFlow code
    simplified the way of defining the whole training process.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 TensorFlow 2.0 和 TensorFlow Hub 结合使用，使我们能够通过将 Inception v3 模型作为固定特征提取器来加速训练过程。此外，得益于快速执行，结合纯
    Python 和 TensorFlow 代码简化了整个训练过程的定义。
- en: In the next chapter, we will learn about semantic segmentation and dataset builder.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习语义分割和数据集构建器。
- en: Exercises
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'You can answer all the theoretical questions and, perhaps more importantly,
    struggle to solve all the code challenges that each exercise contains:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以回答所有理论问题，也许更重要的是，努力解决每个练习中包含的所有代码挑战：
- en: In the *Getting the data* section, a filtering function was applied to the PASCAL
    VOC 2007 dataset to select only the images with a single object inside. The filtering
    process, however, doesn't take into account the class balancement.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*获取数据*部分，我们对 PASCAL VOC 2007 数据集应用了过滤函数，仅选择了包含单一物体的图像。然而，过滤过程没有考虑类别平衡问题。
- en: Create a function that, given the three filtered datasets, merges them first
    and then creates three balanced splits (with a tolerable class imbalance, if it
    is not possible to have them perfectly balanced).
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建一个函数，给定三个过滤后的数据集，首先合并它们，然后创建三个平衡的拆分（如果不能完全平衡，可以接受适度的类别不平衡）。
- en: Use the splits created in the previous point to retrain the network for localization
    and classification defined in the chapter. How and why do the performances change?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前面提到的拆分方式，重新训练定位和分类网络。性能变化的原因是什么？
- en: What measures the Intersection over Union metric?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交并比（IoU）度量的是啥？
- en: What does an IoU value of 0.4 represent? A good or a bad match?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IoU 值为 0.4 代表什么？是好的匹配还是差的匹配？
- en: What is the Mean Average Precision? Explain the concept and write the formula.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是平均精度均值（mAP）？请解释这个概念并写出公式。
- en: What is multi-task learning?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是多任务学习？
- en: Does multi-task learning improve or worsen the model performance on single tasks?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多任务学习是如何影响单任务模型的性能的？是提高了还是降低了？
- en: In the domain of object detection, what is an anchor?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标检测领域，什么是锚点？
- en: Describe how an anchor-based detector looks at the input image during training
    and inference.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述一个基于锚点的检测器在训练和推理过程中如何查看输入图像。
- en: Are mAP and IoU object detection metrics only?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: mAP 和 IoU 仅是目标检测的度量标准吗？
- en: To improve the code of the object detection and localization network, add the
    support saving the model at the end of every epoch into a checkpoint and to restore
    the model (and the global step variable) status to continue the training process.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了改进目标检测和定位网络的代码，添加支持在每个训练轮次结束时将模型保存到检查点，并恢复模型（以及全局步骤变量）的状态以继续训练过程。
- en: The code of the localization and regression networks explicitly use a `draw` function
    that not only draws the bounding boxes but also measures the mAP. Improve the
    code quality by creating different functions for each different feature.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位和回归网络的代码显式使用了一个`draw`函数，它不仅绘制边界框，还测量 mAP。通过为每个不同的功能创建不同的函数来改进代码质量。
- en: The code that measures network performance only uses three samples. This is
    wrong, can you explain the reason? Change the code in order to use a single training
    batch during the training and the whole validation set at the end of every epoch.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量网络性能的代码仅使用了三个样本。这是错误的，你能解释原因吗？请修改代码，在训练过程中使用单个训练批次，并在每个训练轮次结束时使用完整的验证集。
- en: 'Define a training script for the model defined in the "Multi-Headed Network
    and Multi-Task learning": train the regression and classification head at the
    same time and measure the training and validation accuracy at the end of every
    training epoch.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为“多头网络和多任务学习”中定义的模型编写训练脚本：同时训练回归和分类头，并在每个训练轮次结束时测量训练和验证准确率。
- en: Filter the PASCAL VOC train, validation, and test datasets to produce only images
    with at least a person inside (there can be other labeled objects present in the
    picture).
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 筛选 PASCAL VOC 训练、验证和测试数据集，仅保留至少包含一个人（图片中可以有其他标注的物体）的图像。
- en: Replace the regression and classification head of the trained localization and
    classification network with two new heads. The classification head should now
    have a single neuron that represents the probability of the image to contain a
    person. The regression head should regress the coordinates of the objects labeled
    as a person.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换训练过的定位和分类网络中的回归和分类头，使用两个新的头部。分类头现在应该只有一个神经元，表示图像包含人的概率。回归头应回归标注为人的物体的坐标。
- en: Apply transfer learning to train the previously defined network. Stop the training
    process when the mAP on the person class stops increasing (with a tolerance of
    +/- 0.2) for 50 steps.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用迁移学习来训练之前定义的网络。当“人”类别的 mAP 停止增长（容忍范围为 +/- 0.2）并持续 50 步时，停止训练过程。
- en: Create a Python script that generates anchor boxes at different resolutions
    and scales.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Python 脚本，用于生成不同分辨率和尺度的锚框。
