- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Optimizing Models with Transfer Learning and Fine-Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习与微调优化模型
- en: As models grow in size (the depth and number of processing modules per layer),
    training them grows exponentially as more time is spent per epoch, and typically,
    more epochs are required to reach optimum performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的增大（每层的深度和处理模块数量），训练它们所需的时间呈指数增长，通常为了达到最佳性能，需要更多的训练轮次（epoch）。
- en: For this reason, **MXNet** provides state-of-the-art pre-trained models via
    **GluonCV** and **GluonNLP** libraries. As we have seen in previous chapters,
    these models can help us solve a variety of problems when our final dataset is
    similar to the one the selected model has been pre-trained on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**MXNet**通过**GluonCV**和**GluonNLP**库提供了最先进的预训练模型。正如我们在前几章中所见，当我们的最终数据集与所选模型的预训练数据集相似时，这些模型可以帮助我们解决各种问题。
- en: However, sometimes this is not good enough, and our final dataset might have
    some nuances that the pre-trained model is not picking up. In these cases, it
    would be ideal to combine the stored knowledge of the pre-trained model with our
    final dataset. This is called transfer learning, where the knowledge of our pre-trained
    model is transferred to a new task (final dataset).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时候这还不够，最终数据集可能存在一些微妙的差异，预训练模型并未能捕捉到这些差异。在这种情况下，将预训练模型所存储的知识与我们的最终数据集相结合是理想的做法。这就是迁移学习，我们将预训练模型的知识转移到一个新的任务（最终数据集）上。
- en: In this chapter, we will learn how to use GluonCV and GluonNLP, which are MXNet
    Gluon libraries that are specific to **Computer Vision** (**CV**) and **Natural
    Language Processing** (**NLP**), respectively. We will also learn how to retrieve
    pre-trained models from their model zoos, and how to optimize our own networks
    by transferring the learnings from these pre-trained models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习如何使用MXNet Gluon库中的GluonCV和GluonNLP，分别针对**计算机视觉**（**CV**）和**自然语言处理**（**NLP**）。我们还将学习如何从它们的模型库中获取预训练模型，并通过迁移这些预训练模型的学习成果来优化我们自己的网络。
- en: 'Specifically, we will cover the following topics in our recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将在本章中涵盖以下主题：
- en: Understanding transfer learning and fine-tuning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解迁移学习与微调
- en: Improving performance for classifying images
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升图像分类性能
- en: Improving performance for segmenting images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升图像分割性能
- en: Improving performance for translating English to German
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升从英语翻译到德语的性能
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*前言*中指定的技术要求外，以下技术要求适用：
- en: Ensure that you have completed the first recipe, *Installing MXNet, Gluon, GluonCV
    and GluonNLP*, from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running*
    *with MXNet*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了第一章的食谱，*安装MXNet、Gluon、GluonCV和GluonNLP*，[*第1章*](B16591_01.xhtml#_idTextAnchor016)，*开始使用MXNet*。
- en: Ensure that you have completed [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121),
    *Understanding Text with Natural* *Language Processing*.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，以及[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*理解自然语言处理中的文本*。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub链接中找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07)。
- en: 'Furthermore, you can access each recipe directly from Google Colab; for example,
    the first recipe of this chapter can be found here: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接从Google Colab访问每个食谱；例如，本章的第一个食谱可以在这里找到：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb)。
- en: Understanding transfer learning and fine-tuning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解迁移学习与微调
- en: In the previous chapters, we saw how we could leverage MXNet, GluonCV, and GluonNLP
    to retrieve pre-trained models in certain datasets (such as ImageNet, MS COCO,
    and IWSLT2015) and use them for our specific tasks and datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们看到如何利用MXNet、GluonCV和GluonNLP来检索特定数据集（如ImageNet、MS COCO和IWSLT2015）中的预训练模型，并将它们应用于我们的特定任务和数据集。
- en: In this recipe, we will introduce a methodology called **transfer learning**,
    which will allow us to combine the information from pre-trained models (on general
    knowledge datasets) and the information from the new domain (the dataset from
    the task we want to solve). There are two main significant advantages to this
    approach. On the one hand, pre-training datasets are typically large-scale (ImageNet-22k
    has 14 million images), and using a pre-trained model saves us that training time.
    On the other hand, we use our specific dataset not only for evaluation but also
    for training the model, improving its performance in the desired scenario. As
    we will discover, there is not always an easy way to achieve this, as it requires
    the capability to obtain a sizable dataset, or even one right way, as it might
    not yield the expected results. We will also explore the optional next step after
    transfer learning, called fine-tuning, where we will try to use our specific dataset
    to modify the model parameters even further. We will put both techniques to the
    test.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍一种称为**迁移学习**的方法，它允许我们结合预训练模型（在通用知识数据集上）的信息和新领域的信息（来自我们希望解决的任务数据集）。这种方法有两个主要显著优势。一方面，预训练数据集通常是大规模的（ImageNet-22k有1400万张图像），使用预训练模型可以节省训练时间。另一方面，我们不仅用我们的特定数据集进行评估，还用它来训练模型，在所需的场景中提高其性能。正如我们将发现的那样，这并不总是一种容易的方式来实现，因为它需要能够获得一个可观的数据集，或者甚至一种正确的方式，因为它可能不会产生预期的结果。我们还将探讨迁移学习之后的可选下一步，称为微调，我们将尝试使用我们的特定数据集进一步修改模型参数。我们将对这两种技术进行测试。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的章节一样，在本教程中，我们将使用一些矩阵运算和线性代数，但这一点一点也不难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将关注以下步骤：
- en: Introducing transfer learning
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入迁移学习
- en: Describing the advantages of transfer learning and when to use it
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述迁移学习的优势及其使用时机
- en: Understanding the fundamentals of representation learning
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解表示学习的基础知识
- en: Focusing on practical applications
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专注于实际应用
- en: Let’s dive into each of these steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每一个步骤。
- en: Introducing transfer learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入迁移学习
- en: In the previous chapters, we learned how to train deep learning neural networks
    from scratch, exploring problems in CV and NLP. As introduced in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052),
    *Solving Regression Problems*, deep learning neural networks try to imitate the
    biological networks in our brains. One interesting point of view is that when
    we (and our brains) learn new tasks, we leverage previous knowledge we have acquired
    in a very strong way. For example, a very good tennis player will become a relatively
    good player at squash with a few hours of play. Transfer learning is a field of
    study that contains different techniques to achieve similar results as in this
    example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们学习了如何从头开始训练深度学习神经网络，探索计算机视觉和自然语言处理中的问题。正如在[*第三章*](B16591_03.xhtml#_idTextAnchor052)中介绍的那样，*解决回归问题*，深度学习神经网络试图模仿我们大脑中的生物网络。一个有趣的观点是，当我们（及我们的大脑）学习新任务时，我们以非常强大的方式利用先前获得的知识。例如，一个非常优秀的网球选手会在几个小时的比赛中成为相对优秀的壁球选手。迁移学习是一个研究领域，其中包含不同的技术，以达到类似于这个例子的结果。
- en: "![Figure 7.1 – Comparison between traditional \uFEFFmachine \uFEFFlearning\
    \ (ML) and transfer learning](img/B16591_07_1.jpg)"
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 传统机器学习（ML）与迁移学习之间的比较](img/B16591_07_1.jpg)'
- en: Figure 7.1 – Comparison between traditional machine learning (ML) and transfer
    learning
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 传统机器学习（ML）与迁移学习之间的比较
- en: In *Figure 7**.1*, we can see a comparison between both paradigms where, in
    transfer learning, the approach to solving Task 2 leverages the knowledge acquired
    while solving Task 1\. This implies, however, that to solve a single desired task
    (Task 2), we are training the model twice (for Task 1 and for Task 2 later). In
    practice, as we will see in the next steps, we will work with pre-trained models
    from MXNet’s GluonCV and GluonNLP model zoos, and therefore, we will only have
    to train the model once, for Task 2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.1*中，我们可以看到两种范式的对比，其中迁移学习解决任务 2 的方法利用了在解决任务 1 时获得的知识。然而，这意味着要解决单个目标任务（任务
    2），我们需要训练两次模型（分别为任务 1 和任务 2）。实际上，正如我们接下来的步骤所示，我们将使用来自 MXNet 的 GluonCV 和 GluonNLP
    模型库中的预训练模型，因此我们只需为任务 2 训练一次模型。
- en: Describing the advantages of transfer learning and when to use it
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述迁移学习的优势以及何时使用它
- en: 'There are several reasons why using transfer learning offers advantages:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习具有多种优势，原因有很多：
- en: '**Faster**: As we leverage pre-trained models from model zoos, the training
    will converge much faster than training from scratch, requiring much fewer epochs
    and less time.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快**：通过利用来自模型库的预训练模型，训练过程会比从头开始训练更快收敛，所需的训练轮次和时间都会大大减少。'
- en: '**More general**: Typically, pre-trained models have been trained in large-scale
    datasets (such as ImageNet); therefore, the parameters (weights) learned are generalistic
    and can then be reused for a large number of tasks. It is an objective that outputs
    from the feature extraction part of the pre-trained model (also known as **representations**),
    learned by training using large-scale datasets that are general and domain-invariant
    (can be reused).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更通用**：通常，预训练模型是使用大规模数据集（如 ImageNet）进行训练的，因此学习到的参数（权重）具有广泛的适用性，能够重用于大量任务。这个目标是通过使用大规模数据集训练得到的、既通用又不依赖特定领域的特征提取部分（也称为**表示**）来实现的。'
- en: '**Requires less data**: To adapt a pre-trained model for a given new task,
    the amount of data required is much less than for training that specific model
    architecture from scratch. This is because representations can be reused (as mentioned
    in the previous point).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要更少的数据**：为了将预训练模型适配到新的任务上，所需的数据量远低于从头开始训练该模型架构的数量。这是因为表示（如前述）可以被重用。'
- en: '**More environmentally friendly**: As the training time, datasets, and compute
    requirements for transfer learning are much lower than training from scratch,
    less pollution is required to train a model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更环保**：由于迁移学习所需的训练时间、数据集和计算资源远低于从头开始训练，训练模型所需的污染也大大减少。'
- en: '**Performance improvements**: It has been proven (for example, in [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf))
    that using transfer learning with small-scale datasets yields strong performance
    improvements, and on large-scale datasets, the same performance point is achieved
    much faster than training from scratch.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能提升**：已有研究证明（例如，[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)）迁移学习在小规模数据集上能带来显著的性能提升，在大规模数据集上，迁移学习能比从头训练更快地达到相同的性能水平。'
- en: In *Figure 7**.2*, different methods to compute representations are analyzed,
    and although specialized networks can reach better performance, this is only possible
    if large-scale datasets, high-end compute resources, and longer training times
    are given.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.2*中，分析了计算表示的不同方法，尽管专用网络可以达到更好的性能，但这只有在拥有大规模数据集、高端计算资源和更长训练时间的情况下才能实现。
- en: '![Figure 7.2 – Comparing different approaches for representations](img/B16591_07_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 比较不同的表示学习方法](img/B16591_07_2.jpg)'
- en: Figure 7.2 – Comparing different approaches for representations
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 比较不同的表示学习方法
- en: 'In a more general setting, there are different ways to achieve transfer learning,
    as shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的场景下，迁移学习有多种实现方法，如下图所示：
- en: '![Figure 7.3 – Different types of transfer learning](img/B16591_07_3.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 不同类型的迁移学习](img/B16591_07_3.jpg)'
- en: Figure 7.3 – Different types of transfer learning
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 不同类型的迁移学习
- en: In *Figure 7**.3*, we can see the different types of transfer learning, depending
    on the similarity of the source and target domain and the availability of source
    and target data. In this chapter, we will explore the usual setting of having
    a pre-trained model in a similar domain to our intended task (equal source and
    target domain), and the tasks will be slightly different, with some amount of
    labeled data in the target domain (**inductive** **transfer learning**).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.3*中，我们可以看到不同类型的迁移学习，这取决于源领域和目标领域的相似性以及源领域和目标领域数据的可用性。在本章中，我们将探讨在与我们目标任务相似的领域中使用预训练模型的常见设置（源领域和目标领域相同），并且任务会稍有不同，同时在目标领域有一定量的标注数据（**归纳**
    **迁移学习**）。
- en: 'Andrew Ng, chief scientist of Baidu and co-founder of Google Brain, said the
    following in a tutorial in NIPS 2016 called *Nuts and Bolts of Building AI Applications
    Using Deep Learning*: “*In the next few years, we’ll see a lot of concrete value
    driven through transfer learning*,” and he was right.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 百度首席科学家、Google Brain的联合创始人Andrew Ng在2016年NIPS的一个教程中说：“*在未来几年，我们将看到通过迁移学习带来大量具体的价值*，”他是对的。
- en: Understanding the fundamentals of representation learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解表示学习的基本原理
- en: In this section, we will answer the question, from a more theoretical point
    of view, about how to use transfer learning and why it works. In [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121),
    *Understanding Text with Natural Language Processing*, we introduced the concept
    of **representations** for features in images using GluonCV and for words/sentences
    in text using GluonNLP.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从更理论的角度回答如何使用迁移学习以及它为何有效的问题。在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，和[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*使用自然语言处理理解文本*中，我们介绍了使用GluonCV提取图像特征的**表示**概念，以及使用GluonNLP提取文本中单词/句子的表示概念。
- en: 'We can revisit, in *Figure 7**.4*, the usual architecture of a CNN architecture:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图7.4*中回顾CNN架构的常见结构：
- en: '![Figure 7.4 – Refresher of Convolutional Neural Networks (CNNs)](img/B16591_07_4.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 卷积神经网络（CNNs）的复习](img/B16591_07_4.jpg)'
- en: Figure 7.4 – Refresher of Convolutional Neural Networks (CNNs)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 卷积神经网络（CNNs）的复习
- en: 'In *Figure 7**.5*, we can revisit the usual Transformer architecture:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.5*中，我们可以回顾Transformer架构的常见结构：
- en: '![Figure 7.5 – Refresh of the Transformer architecture (encoder on the left,
    and decoder on the right)](img/B16591_07_5.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – Transformer架构的复习（左侧是编码器，右侧是解码器)](img/B16591_07_5.jpg)'
- en: Figure 7.5 – Refresh of the Transformer architecture (encoder on the left, and
    decoder on the right)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – Transformer架构的复习（左侧是编码器，右侧是解码器）
- en: The underlying idea is common in both fields; for example, the feature extractor
    part of CNNs and the encoder in Transformers are representations, and the training
    of these network sections is called **representation learning**, an active field
    of study due to the capability of being able to train these networks in both supervised
    and unsupervised settings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这一基本思想在两个领域中都是共通的；例如，CNN的特征提取部分和Transformer中的编码器就是表示，而这些网络部分的训练被称为**表示学习**，这是一项积极的研究领域，因为它具备在监督和无监督设置下训练这些网络的能力。
- en: 'The main idea behind transfer learning is to transfer the representations learned
    in a task to a different task; therefore, we will typically follow the next steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习背后的主要思想是将一个任务中学习到的表示迁移到另一个任务中；因此，我们通常会遵循以下步骤：
- en: Retrieve a pre-trained model from MXNet’s Model Zoo (GluonCV or GluonNLP).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从MXNet的模型库中获取预训练模型（GluonCV或GluonNLP）。
- en: Remove the last layers (typically, a classifier). Keep the parameters in the
    rest of the layers frozen (not updatable during training).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除最后的层（通常是分类器）。将其他层的参数冻结（在训练过程中不可更新）。
- en: Add new layers (a new classifier) corresponding to the new task
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新的层（新的分类器），以适应新任务
- en: Train the updated model (only the new layers, not frozen, will be updated during
    training) with the target data.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标数据训练更新后的模型（只有新添加的层是可更新的，其他冻结的层在训练期间不可更新）。
- en: If we have enough labeled data for the task that we want to solve (target task),
    another step (that can be done after the previous step or substituting it) is
    called **fine-tuning**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有足够的标注数据来解决我们想要解决的任务（目标任务），另一个步骤（可以在前一步之后执行，也可以替代前一步）叫做**微调**。
- en: 'Fine-tuning takes into account that the representations originally learned
    might not fit perfectly with the target task and, therefore, could also improve
    with updating. In this scenario, the steps are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 微调考虑到原始学习的表示可能无法完美适应目标任务，因此，通过更新也能得到改进。在这种情况下，步骤如下：
- en: Unfreeze the weights of the representation network.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻表示网络的权重。
- en: Retrain the network with target data, typically with a smaller learning rate
    as the representations should be close (same domain).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标数据重新训练网络，通常使用较小的学习率，因为表示应该接近（同一领域）。
- en: Both processes (transfer learning and fine-tuning) are summarized visually in
    *Figure 7**.6*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 两个过程（迁移学习和微调）在*图 7.6*中有直观总结。
- en: '![Figure 7.6 – Transfer learning and fine-tuning](img/B16591_07_6.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 迁移学习与微调](img/B16591_07_6.jpg)'
- en: Figure 7.6 – Transfer learning and fine-tuning
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 迁移学习与微调
- en: Both processes can be applied sequentially, with adequate **hyperparameters**
    for each one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 两个过程可以按顺序应用，每个过程都有适当的**超参数**。
- en: Focusing on practical applications
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重点关注实际应用
- en: 'In this section, we will use what we have learned so far about representation
    learning and we will apply it to a practical example: detecting cats and dogs.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用到目前为止所学的表示学习内容，并将其应用于一个实际的示例：检测猫和狗。
- en: 'To do this, we will retrieve a model from the **GluonCV Model Zoo**; we will
    remove the classifier (last layers) and keep the feature extraction stage. We
    will then analyze how the representations of the cats and dogs have been learned.
    To load the model, we can use this code snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将从**GluonCV模型库**中检索一个模型；我们将去除分类器（最后的几层），保留特征提取阶段。然后，我们将分析猫和狗的表示是如何被学习的。要加载模型，我们可以使用以下代码片段：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the previous code snippet, for the `pretrained` parameter, we have assigned
    the value of `True`, indicating that we want the pretrained weights to be retrieved
    (and not only the architecture of the model).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，对于`pretrained`参数，我们已将其赋值为`True`，表示我们希望加载预训练权重（而不仅仅是模型的架构）。
- en: When trained correctly, CNNs learn hierarchical representations of the features
    of the images in the training dataset, with each progressive layer learning more
    and more complex patterns. Therefore, when an image is processed (when processing
    on successive layers), the network can compute more complex patterns associated
    with the network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当正确训练时，CNN会学习训练数据集中图像特征的层次化表示，每一层逐渐学习越来越复杂的模式。因此，当图像被处理时（在连续的层中进行处理），网络能够计算与网络相关的更复杂的模式。
- en: 'Now, we can use a new MXNet library, MXBoard (see the recipe for installation
    instructions), with this model to evaluate the different steps that a dog image
    goes through and see some examples of how a pre-trained model computes its representations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用一个新的MXNet库，MXBoard（请参阅安装说明中的食谱），使用此模型来评估狗图像经过的不同步骤，并查看一些预训练模型计算其表示的示例：
- en: '![Figure 7.7 – Cat and dog representations – convolutional filters](img/B16591_07_7.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 猫与狗的表示 – 卷积滤波器](img/B16591_07_7.jpg)'
- en: Figure 7.7 – Cat and dog representations – convolutional filters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 猫与狗的表示 – 卷积滤波器
- en: In *Figure 7**.7*, we can see the convolutional filters corresponding to the
    first convolutional layer of a ResNet152 pre-trained network (on ImageNet). Please
    note how these filters focus on simple patterns such as specific shapes (vertical
    and horizontal lines, circles, and so on) and specific colors (red blobs).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.7*中，我们可以看到对应于ResNet152预训练网络（在ImageNet上）的第一层卷积层的卷积滤波器。请注意这些滤波器如何专注于简单的模式，如特定的形状（垂直和水平线、圆形等）和特定的颜色（红色斑点）。
- en: 'Let’s analyze the results with a specific image:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一张特定的图像来分析结果：
- en: '![Figure 7.8 – Example image of a dog](img/B16591_07_8.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 一只狗的示例图像](img/B16591_07_8.jpg)'
- en: Figure 7.8 – Example image of a dog
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 一只狗的示例图像
- en: 'We select an image from our *Dogs vs. Cats* dataset, such as the dog depicted
    in *Figure 7**.8*. When passing this image through our network, we will find results
    similar to the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*Dogs vs. Cats*数据集中选择一张图像，例如*图 7.8*中描绘的狗。当将这张图像通过我们的网络时，我们将得到类似以下的结果：
- en: '![Figure 7.9 – Output from convolutional filters](img/B16591_07_9.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 卷积滤波器输出](img/B16591_07_9.jpg)'
- en: Figure 7.9 – Output from convolutional filters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 卷积滤波器输出
- en: In *Figure 7**.9*, we can see the output of the filters in *Figure 7**.7* for
    our dog example. Note how different outputs highlight simple shapes such as the
    eyes or the legs (larger values, closer to white).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.9*中，我们可以看到我们狗的示例在*图7.7*中的过滤器输出。注意不同的输出如何突出显示简单的形状，比如眼睛或腿部（较大的值，接近白色）。
- en: 'Finally, as the image traverses the network, its features are more and more
    compressed, yielding (for ResNet152) a final vector of 2,048 elements. This vector
    can be computed easily with networks retrieved using MXNet’s Model Zoo:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随着图像在网络中传播，它的特征会越来越压缩，最终得到（对于ResNet152）一个包含2,048个元素的向量。这个向量可以通过使用MXNet的模型库轻松计算：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This code excerpt provides the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码示例会输出以下结果：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, we have a `2048` element.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们得到了一个`2048`元素。
- en: How it works…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we introduced the concepts of transfer learning and fine-tuning.
    We explained when it made sense to use these two different techniques and their
    advantages.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇食谱中，我们介绍了迁移学习和微调的概念。我们解释了何时使用这两种不同技术及其优点。
- en: We also explored when these techniques can be useful and their connections to
    representation learning, explaining how representations play a significant role
    in the knowledge being transferred when using these techniques. We used a new
    library, **MXBoard**, to produce visualizations for the representations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了这些技术何时有用及其与表征学习的关系，解释了在使用这些技术时，表征在知识转移中的重要作用。我们使用了一个新的库，**MXBoard**，来生成表征的可视化。
- en: Moreover, we intuitively and practically showed how to apply these techniques
    to CV and NLP tasks and computed a representation for a specific example.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们直观且实践地展示了如何将这些技术应用于计算机视觉和自然语言处理任务，并为一个具体示例计算了表征。
- en: There’s more...
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Transfer learning, including fine-tuning, is an active field of study. In this
    recipe, we have only covered the most useful scenario for deep learning, inductive
    transfer learning. For a more comprehensive but still easy-to-read introduction,
    I recommend reading *Transfer learning: a friendly introduction*, which can be
    found at: [https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习，包括微调，是一个活跃的研究领域。在这个食谱中，我们仅涵盖了深度学习中最有用的场景——归纳迁移学习。想了解更全面但仍易于阅读的介绍，我推荐阅读*迁移学习：友好的介绍*，可以在以下网址找到：[https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w)。
- en: 'Moreover, the concept of transferring knowledge from one system to another
    is not new, and there are references to concepts such as **learning to learn**
    and **knowledge transfer** as early as 1995, when a **Neural Information Processing
    Systems** (**NeurIPS**) workshop on the topic was presented. A summary of the
    workshop can be found here: http://socrates.acadiau.ca/courses/comp/dsilver/nips95_ltl/nips95.workshop.pdf.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，知识从一个系统转移到另一个系统的概念并不新颖，早在1995年就有**学习如何学习**和**知识转移**等概念的引用，那时曾有一个关于这个主题的**神经信息处理系统**（**NeurIPS**）研讨会。该研讨会的总结可以在这里找到：http://socrates.acadiau.ca/courses/comp/dsilver/nips95_ltl/nips95.workshop.pdf。
- en: 'Furthermore, as introduced 21 years later in the same venue, Andrew Ng was
    able to correctly foresee the importance of transfer learning. His 2016 NeurIPS
    tutorial can be found here (jump to 1h 37m for the transfer learning quote): [https://www.youtube.com/watch?v=F1ka6a13S9I](https://www.youtube.com/watch?v=F1ka6a13S9I).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如21年后在同一场合中介绍的，Andrew Ng正确预见了迁移学习的重要性。他的2016年NeurIPS教程可以在这里找到（跳转到1小时37分钟查看迁移学习相关内容）：[https://www.youtube.com/watch?v=F1ka6a13S9I](https://www.youtube.com/watch?v=F1ka6a13S9I)。
- en: Improving performance for classifying images
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升图像分类的性能
- en: After introducing transfer learning and fine-tuning in the previous recipe,
    in this one, we will apply it to **image classification**, a CV task.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇食谱中介绍了迁移学习和微调后，在本篇中，我们将其应用于**图像分类**，这是一个计算机视觉任务。
- en: In the second recipe, *Classifying images with MXNet – GluonCV Model Zoo, AlexNet,
    and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing Images
    with Computer Vision*, we saw how we could use GluonCV to retrieve pre-trained
    models and use them directly for an image classification task. In the first instance,
    we looked at training them from scratch, effectively only leveraging past knowledge
    by using the architecture of the pre-trained model, without leveraging any past
    knowledge contained in the pre-trained weights, which were re-initialized, deleting
    any past information. Afterward, the pre-trained models were used directly for
    the task, effectively also leveraging the weights/parameters of the model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个配方中，*使用MXNet进行图像分类——GluonCV模型库，AlexNet和ResNet*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，我们已经看到如何使用GluonCV检索预训练模型，并直接用于图像分类任务。在第一次的实例中，我们展示了如何从零开始训练模型，实际上仅利用预训练模型的架构，而没有利用任何包含在预训练权重中的过去知识，这些权重已被重新初始化，删除了任何历史信息。之后，预训练模型直接用于任务，实际上也利用了模型的权重/参数。
- en: In this recipe, we will combine the weights/parameters of the model with the
    target dataset, applying the techniques introduced in this chapter, transfer learning
    and fine-tuning. The dataset used for the pre-training was `Dogs vs` `Cats` dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将把模型的权重/参数与目标数据集结合，应用本章介绍的技术——迁移学习和微调。用于预训练的数据集是`Dogs vs` `Cats`数据集。
- en: Getting ready
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 和前面的章节一样，在这个配方中，我们将使用一些矩阵运算和线性代数，但一点也不难。
- en: 'Furthermore, we will be working with text datasets; therefore, we will revisit
    some concepts already seen in the second recipe, *Classifying images with MXNet:
    GluonCV Model Zoo, AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将处理文本数据集；因此，我们将重新审视在第二个配方中已看到的一些概念，*使用MXNet进行图像分类：GluonCV模型库，AlexNet和ResNet*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*。
- en: How to do it...
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将查看以下步骤：
- en: Revisiting the *ImageNet-1k* and *Dogs vs.* *Cats* datasets
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新审视*ImageNet-1k*和*Dogs vs.* *Cats*数据集
- en: Training a **ResNet** model from scratch with *Dogs* *vs Cats*
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始训练一个**ResNet**模型，使用*Dogs* *vs Cats*数据集
- en: Using a pre-trained ResNet model to optimize performance via transfer learning
    from *ImageNet-1k* to *Dogs* *vs Cats*
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的ResNet模型，通过迁移学习从*ImageNet-1k*到*Dogs* *vs Cats*优化性能
- en: Fine-tuning our pre-trained ResNet model on *Dogs* *vs Cats*
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对*Dogs* *vs Cats*数据集上的预训练ResNet模型进行微调
- en: Let’s look at these steps in detail next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细介绍这些步骤。
- en: Revisiting the ImageNet-1k and Dogs vs Cats datasets
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新审视ImageNet-1k和Dogs vs Cats数据集
- en: '*ImageNet-1k* and *Dogs vs Cats* are both image classification datasets; however,
    they are quite different. *ImageNet-1k* is a large-scale dataset containing ~1.2
    million images labeled into 1,000 classes and has been used extensively in research
    and academia for benchmarking. *Dogs vs Cats* is a small-scale dataset containing
    1,400 images depicting either a dog or a cat, and its fame is mostly due to a
    Kaggle competition launched in 2013.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*ImageNet-1k*和*Dogs vs Cats*都是图像分类数据集；然而，它们有很大的不同。*ImageNet-1k*是一个大规模数据集，包含约120万张图像，按1000个类别进行标签，广泛用于研究和学术界的基准测试。*Dogs
    vs Cats*是一个小规模数据集，包含1400张描绘狗或猫的图像，其知名度主要来自于2013年启动的Kaggle竞赛。'
- en: MXNet GluonCV does not provide methods to directly download any of the datasets.
    However, we do not need the *ImageNet-1k* dataset (its size is ~133 GB), only
    the pre-trained parameters for our chosen model. The pre-trained models can be
    downloaded directly from the MXNet GluonCV Model Zoo, we have seen examples in
    previous chapters and we will use them again in this one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet GluonCV不提供直接下载数据集的方法。然而，我们不需要*ImageNet-1k*数据集（它的大小约为133GB），只需要我们选择的模型的预训练参数。预训练模型可以直接从MXNet
    GluonCV模型库下载，我们在前面的章节中见过例子，在本章中也将再次使用它们。
- en: 'Here are some examples from *ImageNet-1k*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些来自*ImageNet-1k*的示例：
- en: '![Figure 7.10 – ImageNet-1k examples](img/B16591_07_10.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – ImageNet-1k示例](img/B16591_07_10.jpg)'
- en: Figure 7.10 – ImageNet-1k examples
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – ImageNet-1k示例
- en: The source of the preceding figure is [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上图的来源是[https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)。
- en: 'For *Dogs vs Cats*, all the information on how to retrieve the dataset can
    be found in the second recipe, *Classifying images with MXNet: GluonCV Model Zoo,
    AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*. Taking that recipe’s code as a reference, we can
    display some examples:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*猫狗*数据集，关于如何获取数据集的所有信息都可以在第二个配方中找到，即*使用 MXNet 进行图像分类：GluonCV 模型动物园、AlexNet
    和 ResNet*，在[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*。根据该配方的代码作为参考，我们可以显示一些示例：
- en: '![Figure 7.11 – Dogs vs Cats dataset](img/B16591_07_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 猫狗数据集](img/B16591_07_11.jpg)'
- en: Figure 7.11 – Dogs vs Cats dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 猫狗数据集
- en: In *Figure 7**.10* and *Figure 7**.11*, we can see how some images from *ImageNet-1k*
    resemble some of the images from *Dogs* *vs Cats*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7**.10*和*图 7**.11*中，我们可以看到*ImageNet-1k*中的一些图像与*猫狗*数据集中的一些图像相似。
- en: Training a ResNet model from scratch with Dogs vs Cats
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始训练一个 ResNet 模型，使用猫狗数据集
- en: 'As described in the second recipe, *Classifying images with MXNet: GluonCV
    Model Zoo, AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, we will be using **softmax cross-entropy**
    as the loss function and **accuracy** and the **confusion matrix** as evaluation
    metrics.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如第二个配方中所述，*使用 MXNet 进行图像分类：GluonCV 模型动物园、AlexNet 和 ResNet*，在[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，我们将使用**softmax
    交叉熵**作为损失函数，以及**accuracy**和**混淆矩阵**作为评估指标。
- en: 'We have the following evolution in the training using the ResNet model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ResNet 模型进行训练的演变如下：
- en: '![Figure 7.12 – ResNet training evolution (training loss and validation loss,
    and validation accuracy) – training from scratch](img/B16591_07_12.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – ResNet 训练演变（训练损失和验证损失，以及验证精度）– 从头开始训练](img/B16591_07_12.jpg)'
- en: Figure 7.12 – ResNet training evolution (training loss and validation loss,
    and validation accuracy) – training from scratch
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – ResNet 训练演变（训练损失和验证损失，以及验证精度）– 从头开始训练
- en: 'Furthermore, for the best iteration, the `accuracy` value obtained in the test
    set is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在最佳迭代中，测试集中得到的`accuracy`值如下：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The confusion matrix is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下：
- en: '![Figure 7.13 – Confusion matrix in Dogs vs Cats for a ResNet model trained
    from scratch](img/B16591_07_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 从头开始训练的 ResNet 模型在猫狗数据集中的混淆矩阵](img/B16591_07_13.jpg)'
- en: Figure 7.13 – Confusion matrix in Dogs vs Cats for a ResNet model trained from
    scratch
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 从头开始训练的 ResNet 模型在猫狗数据集中的混淆矩阵
- en: Both the accuracy value obtained (75%) and *Figure 7**.13* show quite average
    performance after training for several epochs (100, in this example). You are
    encouraged to run your own experiments trying out different hyperparameters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过多次训练（例如，100 次）后，获得的准确度值（75%）和*图 7**.13*显示出相当平均的性能。鼓励您运行自己的实验，尝试不同的超参数设置。
- en: 'Qualitatively, we can also check how well our model is performing with an example
    image. In our case, we chose the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 定性上，我们还可以检查我们的模型在一个示例图像中的表现。在我们的案例中，我们选择了以下内容：
- en: '![Figure 7.14 – Qualitative example of Cats vs Dogs, specifically a cat](img/B16591_07_14.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 猫狗定性示例，具体为猫](img/B16591_07_14.jpg)'
- en: Figure 7.14 – Qualitative example of Cats vs Dogs, specifically a cat
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 猫狗定性示例，具体为猫
- en: 'We can check the output of our model by running this image through it with
    the following code snippet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码片段运行这张图像，检查我们模型的输出：
- en: '[PRE4]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These code statements will give us the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码语句将给出以下输出：
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As can be seen from the results, the image has been correctly classified as
    a cat.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果所示，图像已正确分类为猫。
- en: Using a pre-trained ResNet model to optimize performance via transfer learning
    from ImageNet-1k to Dogs vs Cats
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的 ResNet 模型，通过从 ImageNet-1k 到猫狗数据集的迁移学习来优化性能
- en: 'In the previous recipe, we trained a new model from scratch using our dataset.
    However, this has two important drawbacks:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们使用我们的数据集从头开始训练了一个新模型。然而，这有两个重要的缺点：
- en: A large amount of data is required for training from scratch.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练需要大量的数据。
- en: The training process can take a long time due to the large size of the dataset
    and the number of epochs needed for the model to learn the task.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集的大尺寸和模型学习任务所需的周期数，训练过程可能需要很长时间。
- en: 'Therefore, in this recipe, we will follow a different approach: we will use
    pre-trained models from MXNet GluonCV to solve the task. These models have been
    trained in *ImageNet-1k*, a dataset that contains the classes we are interested
    in (cats and dogs); therefore, we can use those learned representations and easily
    transfer them to *Dogs vs Cats* (same domain).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个食谱中，我们将采取不同的方法：我们将使用来自 MXNet GluonCV 的预训练模型来解决任务。这些模型已经在 *ImageNet-1k*
    数据集上训练过，该数据集包含我们感兴趣的类别（猫和狗）；因此，我们可以利用这些学到的特征，并轻松将其迁移到 *Dogs vs Cats*（相同领域）。
- en: 'For a ResNet model, use the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ResNet 模型，使用以下代码：
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see in the previous code snippet, following the discussion in this
    chapter’s first recipe, *Understanding transfer learning and fine-tuning*, for
    the `pretrained` parameter, we have assigned the value of `True`, indicating that
    we want the pre-trained weights to be retrieved (and not only the architecture
    of the model).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的代码段中看到的，按照本章第一个食谱《理解迁移学习和微调》中的讨论，对于 `pretrained` 参数，我们已将其值设置为 `True`，表示我们希望获取预训练权重（而不仅仅是模型的架构）。
- en: 'In order to adequately evaluate the improvements that transfer learning brings,
    we are going to evaluate our pre-trained model directly (the source task is *ImageNet-1k*)
    before applying transfer learning to *Dogs vs Cats* and after applying it. Therefore,
    using our pre-trained model as is, we obtain the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分评估迁移学习带来的改进，我们将在应用迁移学习到 *Dogs vs Cats* 数据集之前和之后，直接评估我们的预训练模型（源任务是 *ImageNet-1k*）。因此，使用我们当前的预训练模型，我们得到如下结果：
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The confusion matrix is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下：
- en: '![Figure 7.15 – Confusion matrix in Dogs vs Cats for a pre-trained ResNet model](img/B16591_07_15.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 在预训练 ResNet 模型下，Dogs vs Cats 数据集的混淆矩阵](img/B16591_07_15.jpg)'
- en: Figure 7.15 – Confusion matrix in Dogs vs Cats for a pre-trained ResNet model
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 在预训练 ResNet 模型下，Dogs vs Cats 数据集的混淆矩阵
- en: As we can see, our pre-trained Transformer model is already showing good performance
    values as it is the same domain; however, simply using a pre-trained model does
    not yield better performance than training from scratch. The great advantage of
    using pre-trained models is the time savings, as loading one just takes a few
    lines of code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的预训练 Transformer 模型在同一领域下已经显示出良好的表现；然而，单纯使用预训练模型并不能比从零开始训练获得更好的表现。使用预训练模型的巨大优势在于节省时间，因为加载它只需要几行代码。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example. Note how the code slightly differs from the previous qualitative
    image excerpt, as now we need to convert ImageNet classes (the output of our ResNet50
    pre-trained model) to our classes (`0` for cats and `1` for dogs). The new code
    is given as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的图像示例检查模型的定性表现。注意代码与之前的定性图像摘录有所不同，因为现在我们需要将 ImageNet 类别（即我们的预训练 ResNet50
    模型的输出）转换为我们的类别（`0` 代表猫，`1` 代表狗）。新的代码如下所示：
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These code statements will give us the following output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码语句将会给我们以下输出：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As can be seen from the result, the image has been correctly classified as a
    cat.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，该图像已被正确分类为猫。
- en: Now that we have a baseline for comparison, let’s apply transfer learning to
    our task. From the first recipe, *Understanding transfer learning and fine-tuning*,
    the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV
    or GluonNLP), which we have already done.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个基准进行比较，让我们将迁移学习应用到我们的任务中。从第一个食谱《理解迁移学习和微调》中，第一步是从 MXNet 模型库（GluonCV
    或 GluonNLP）中获取一个预训练的模型，这一步我们已经完成。
- en: The second step was to remove the last layers (typically, a classifier), keeping
    the parameters in the rest of the layers frozen (not updatable during training),
    so let’s do it!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是移除最后一层（通常是分类器），保持其他层的参数被冻结（在训练过程中不可更新），所以让我们开始吧！
- en: 'We can replace the classifier with the following snippet:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下代码段替换分类器：
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can freeze the ResNet feature extraction layers with the following snippet:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码段来冻结 ResNet 特征提取层：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can replace the classifier with the following snippet:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下代码段替换分类器：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can apply the usual training process with *Dogs vs Cats*, and we have
    the following evolution in the training using the ResNet model:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以应用通常的训练过程来处理 *Dogs vs Cats* 数据集，并且我们在使用 ResNet 模型的训练中有了以下进展：
- en: '![Figure 7.16 – ResNet training evolution (training loss and validation loss)
    – transfer learning](img/B16591_07_16.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – ResNet 训练演变（训练损失和验证损失）– 迁移学习](img/B16591_07_16.jpg)'
- en: Figure 7.16 – ResNet training evolution (training loss and validation loss)
    – transfer learning
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – ResNet 训练演变（训练损失和验证损失）– 迁移学习
- en: 'Furthermore, for the best iteration, the accuracy obtained in the test set
    is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集上的准确率如下：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The confusion matrix is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下：
- en: '![Figure 7.17 – Confusion matrix in Dogs vs Cats for a ResNet model with transfer
    learning](img/B16591_07_17.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – 使用迁移学习的 ResNet 模型在狗与猫数据集上的混淆矩阵](img/B16591_07_17.jpg)'
- en: Figure 7.17 – Confusion matrix in Dogs vs Cats for a ResNet model with transfer
    learning
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 使用迁移学习的 ResNet 模型在狗与猫数据集上的混淆矩阵
- en: Compared with our previous experiment of training from scratch, this experiment
    yields much higher performance, and it took us literally minutes to get this model
    to start working well for us in our intended task, whereas the training required
    for the previous experiment took hours and required several tries to tune the
    hyperparameters, which can then turn into several days of effort in total.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前从头开始训练的实验相比，这个实验的表现大大提高，并且我们只用了几分钟就让这个模型开始在我们期望的任务中表现良好，而之前的实验需要几个小时，并且需要多次调整超参数，这可能会转化为几天的工作。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用相同的图像示例和代码来定性地检查我们的模型表现。输出结果如下：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As can be seen from the result, the image has been correctly classified as a
    cat.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，图像已经被正确分类为猫。
- en: Fine-tuning our pre-trained ResNet model on Dogs vs Cats
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在狗与猫数据集上微调我们的预训练 ResNet 模型
- en: In the previous recipe, we *froze* the parameters in the encoder layers. However,
    as the dataset we are currently working with (*Dogs vs Cats*) has enough data
    samples, we can *unfreeze* those parameters and train the model, effectively allowing
    the new training process to update the representations (with transfer learning,
    we were working directly with the representations learned for *ImageNet-1k*).
    This process is called fine-tuning.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的步骤中，我们*冻结*了编码器层中的参数。然而，由于我们当前使用的数据集（*狗与猫*）样本足够多，我们可以*解冻*这些参数并训练模型，从而有效地让新的训练过程更新表示（在迁移学习中，我们直接使用了为*ImageNet-1k*学到的表示）。这个过程叫做微调。
- en: 'There are two variants of fine-tuning:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 微调有两种变体：
- en: Applying transfer learning by freezing the layers and unfreezing them afterward
    (fine-tuning after transfer learning)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过冻结层并在之后解冻（迁移学习后的微调）来应用迁移学习
- en: Directly applying fine-tuning without the preliminary step of freezing the layers
    (fine-tuning directly)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接应用微调，而没有冻结层的预处理步骤（直接微调）
- en: Let’s compute both experiments and draw conclusions by comparing the results.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算这两个实验并通过比较结果得出结论。
- en: 'For the first experiment, we can take the network obtained in the previous
    recipe, unfreeze the layers, and restart the training. In MXNet, to unfreeze the
    encoder parameters, we can run the following snippet:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个实验，我们可以取之前步骤中获得的网络，解冻层并重新开始训练。在 MXNet 中，要解冻编码器参数，我们可以运行以下代码片段：
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can apply the usual training process with *Dogs vs Cats*, and we have
    the following evolution in the training using the ResNet model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以应用常规的训练过程与*狗与猫*数据集，并且我们在使用 ResNet 模型进行训练时，得到了以下演变：
- en: '![Figure 7.18 – ResNet training evolution (training loss and validation loss)
    – fine-tuning after transfer learning](img/B16591_07_18.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.18 – ResNet 训练演变（训练损失和验证损失）– 迁移学习后的微调](img/B16591_07_18.jpg)'
- en: Figure 7.18 – ResNet training evolution (training loss and validation loss)
    – fine-tuning after transfer learning
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – ResNet 训练演变（训练损失和验证损失）– 迁移学习后的微调
- en: 'Furthermore, for the best iteration, the accuracy obtained in the test set
    is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集上的准确率如下：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The confusion matrix is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下：
- en: '![Figure 7.19 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    after transfer learning](img/B16591_07_19.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.19 – 使用迁移学习后微调的 ResNet 模型在狗与猫数据集上的混淆矩阵](img/B16591_07_19.jpg)'
- en: Figure 7.19 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    after transfer learning
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 使用迁移学习后微调的 ResNet 模型在狗与猫数据集上的混淆矩阵
- en: Compared with our previous experiment of transfer learning, this experiment
    yields worse performance. This is due to a combination of the size of the dataset
    and the hyperparameters chosen. You are encouraged to try your own experiments.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的迁移学习实验相比，这次实验的表现更差。这是由于数据集的大小和选择的超参数组合。鼓励你尝试自己的实验。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用相同的图像示例和代码，定性地检查我们的模型表现如何。输出如下所示：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As can be seen from the results, the image has been correctly classified as
    a cat.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，图像已被正确分类为猫。
- en: Let’s continue now with the second fine-tuning experiment where, instead of
    applying transfer learning, we apply fine-tuning directly to the whole model (no
    frozen layers).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进行第二次微调实验，这次我们不采用迁移学习，而是直接对整个模型进行微调（没有冻结层）。
- en: 'We need to again retrieve the pre-trained ResNet model for *ImageNet-1k*, with
    the following code snippet for MXNet GluonCV:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要再次获取为*ImageNet-1k*预训练的ResNet模型，MXNet GluonCV的代码片段如下：
- en: '[PRE18]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And now, without freezing, we can apply the training process, which will update
    all layers of our ResNet model, giving the following loss curves:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在没有冻结层的情况下应用训练过程，这将更新我们ResNet模型的所有层，得到如下的损失曲线：
- en: '![Figure 7.20 – ResNet training evolution (training loss and validation loss)
    – fine-tuning without freezing](img/B16591_07_20.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.20 – ResNet 训练过程（训练损失与验证损失）– 微调且未冻结层](img/B16591_07_20.jpg)'
- en: Figure 7.20 – ResNet training evolution (training loss and validation loss)
    – fine-tuning without freezing
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – ResNet 训练过程（训练损失与验证损失）– 微调且未冻结层
- en: 'Furthermore, for the best iteration, the accuracy obtained in the test set
    is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，在测试集上获得的准确率如下：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The value is similar to the previous experiment with transfer learning. For
    the confusion matrix, we have the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值与之前迁移学习实验的结果相似。对于混淆矩阵，结果如下：
- en: '![Figure 7.21 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    without freezing](img/B16591_07_21.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.21 – ResNet 模型在狗与猫分类中的混淆矩阵，采用微调且未冻结层](img/B16591_07_21.jpg)'
- en: Figure 7.21 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    without freezing
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 – ResNet 模型在狗与猫分类中的混淆矩阵，采用微调且未冻结层
- en: As mentioned, compared with our previous fine-tuning experiment, we can see
    how this experiment yields higher performance. Empirically, this has been proven
    to be a repeatable result and has been indicated to be because initially freezing
    the encoder allows for the decoder to learn (using the encoder representations)
    the new task at hand. From an information flow point of view, in this step, there
    is a knowledge transfer from the feature extraction stage to the classifier. In
    the secondary step when the feature extraction stage is unfrozen, the learned
    parameters from the classifier perform auxiliary transfer learning – this time,
    from the classifier to the feature extraction stage.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，与我们之前的微调实验相比，我们可以看到这次实验的性能更高。根据经验，这已被证明是一个可重复的结果，且被认为是因为最初冻结编码器允许解码器（使用编码器表示）学习当前的新任务。从信息流的角度来看，在这一步骤中，知识从特征提取阶段转移到了分类器。在第二步中，当特征提取阶段解冻时，分类器中学到的参数执行辅助的迁移学习——这次是从分类器到特征提取阶段。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用相同的图像示例和代码，定性地检查我们的模型表现如何。输出如下所示：
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As can be seen from the results, the image has been correctly classified as
    a cat.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，图像已被正确分类为猫。
- en: How it works…
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理是…
- en: 'In this recipe, we applied the techniques of transfer learning and fine-tuning,
    introduced at the beginning of the chapter, to the task of image classification,
    which was also presented previously, in the second recipe, *Classifying images
    with MXNet: GluonCV Model Zoo, AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将第一个章节开头介绍的迁移学习和微调技术应用于图像分类任务，这个任务在第二个配方中也有呈现，名为*使用 MXNet 分类图像：GluonCV
    模型库、AlexNet 和 ResNet*，[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)，*计算机视觉下的图像分析*。
- en: 'We revisited two known datasets, *ImageNet-1k* and *Dogs vs Cats*, which we
    intended to combine using knowledge transfer based on the former dataset and refining
    that knowledge with the latter. Moreover, this was achieved by leveraging the
    tools that MXNet GluonCV provided:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新访问了两个已知的数据集，*ImageNet-1k* 和 *Dogs vs Cats*，并打算结合这两个数据集，基于前者数据集的知识转移，并用后者对该知识进行微调。此外，这是通过利用MXNet
    GluonCV提供的工具实现的：
- en: A pre-trained ResNet model for *ImageNet-1k*
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 *ImageNet-1k* 提供的预训练ResNet模型
- en: Tools for easy-to-use access to *Dogs* *vs Cats*
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 便于访问的工具，用于*狗* *与猫* 数据集
- en: Furthermore, we continued using the loss functions and metrics introduced for
    image classification, softmax cross-entropy, accuracy, and the confusion matrix.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们继续使用用于图像分类的损失函数和指标，包括softmax交叉熵、准确率和混淆矩阵。
- en: 'Having all these tools readily available within MXNet and GluonCV allowed us
    to run the following experiments with just a few lines of code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有了MXNet和GluonCV中这些随时可用的工具，我们只需几行代码就能运行以下实验：
- en: Training a model from scratch in *Dogs* *vs Cats*
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始训练 *Dogs* *vs Cats* 中的模型
- en: Using a pre-trained model to optimize performance via transfer learning from
    *ImageNet-1k* to *Dogs* *vs Cats*
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型，通过从 *ImageNet-1k* 到 *Dogs* *vs Cats* 的转移学习优化性能
- en: Fine-tuning our pre-trained model on *Dogs vs Cats* (with and without freezing
    layers)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *Dogs vs Cats* 数据集上微调我们的预训练模型（包括和不包括冻结层）
- en: After running the different experiments, we obtained an effective tie between
    transfer learning and fine-tuning directly (accuracies of 0.985 and 0.98, respectively).
    The actual results obtained when running these experiments might differ based
    on model architecture, datasets, and hyperparameters chosen, so you are encouraged
    to try out different techniques and variations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行不同实验后，我们直接获得了转移学习与微调之间的有效联系（准确率分别为0.985和0.98）。实际运行这些实验时，结果可能会根据模型架构、数据集和选择的超参数有所不同，因此建议大家尝试不同的技术和变种。
- en: There’s more...
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Transfer learning, including fine-tuning, is an active field of research. A
    recent paper published in 2022 explores the latest advances in image classification.
    The paper is titled *Deep Transfer Learning for Image Classification: A survey*,
    and can be found here: [https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey](https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习，包括微调，是一个活跃的研究领域。一篇2022年发布的论文探讨了图像分类领域的最新进展。该论文标题为《深度转移学习用于图像分类：一项综述》，可以在此查阅：[https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey](https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey)。
- en: 'For a more general approach to CV use cases, a recent paper was published,
    *Transfer Learning Methods as a New Approach in Computer Vision Tasks with Small
    Datasets*, where the problem of small datasets is evaluated, and these techniques
    are applied to solve medical imaging tasks. It can be found here: [https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets](https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 关于计算机视觉应用案例的更一般方法，最近发布了一篇论文《转移学习方法作为解决小数据集计算机视觉任务的新方法》，该论文评估了小数据集的问题，并应用这些技术解决医学影像任务。可以在此查阅：[https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets](https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets)。
- en: Improving performance for segmenting images
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高图像分割性能
- en: In this recipe, we will apply transfer learning and fine-tuning to **semantic
    segmentation**, a CV task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将应用转移学习和微调来进行**语义分割**，这是一个计算机视觉任务。
- en: 'In the fourth recipe, *Segmenting objects in images with MXNet: PSPNet and
    DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing Images
    with Computer Vision*, we saw how we could use GluonCV to retrieve pre-trained
    models and use them directly for a semantic segmentation task, effectively leveraging
    past knowledge by using the architecture and the weights/parameters of the pre-trained
    model.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四个配方中，*使用MXNet进行图像对象分割：PSPNet与DeepLab-v3*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)
    *使用计算机视觉分析图像* 中，我们展示了如何使用GluonCV直接获取预训练模型并将其用于语义分割任务，通过使用预训练模型的架构和权重/参数，充分利用过去的知识。
- en: In this recipe, we will continue leveraging the weights/parameters of the model,
    obtained for a task consisting of classifying images among a set of 21 classes
    using semantic segmentation models. The dataset used for the pre-training was
    *MS COCO* (source task) and we will run several experiments to evaluate our models
    in a new (target) task, using the *Penn-Fudan Pedestrian* dataset. In these experiments,
    we will also include knowledge from the target dataset to improve our semantic
    classification performance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将继续利用模型的权重/参数，用于一个任务，该任务包括使用语义分割模型在一组21类图像中对图像进行分类。用于预训练的数据集是*MS COCO*（源任务），我们将运行多个实验来评估我们的模型在一个新的（目标）任务中的表现，使用*Penn-Fudan
    Pedestrian*数据集。在这些实验中，我们还将包含来自目标数据集的知识，以提高我们的语义分类性能。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 至于之前的章节，在本教程中，我们将使用一些矩阵操作和线性代数，但这一点也不难。
- en: 'Furthermore, we will be working with text datasets; therefore, we will revisit
    some concepts already seen in the fourth recipe, *Segmenting objects in images
    with MXNet: PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用文本数据集；因此，我们将重新讨论第四篇教程中已经见过的一些概念，*MXNet中图像分割：PSPNet和DeepLab-v3*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*计算机视觉中的图像分析*。
- en: How to do it...
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将看到以下步骤：
- en: Revisiting the *MS COCO* and *Penn-Fudan* *Pedestrian* datasets
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新审视*MS COCO*和*Penn-Fudan Pedestrian*数据集
- en: Training a **DeepLab-v3** model from scratch with *Penn-Fudan Pedestrian*
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*Penn-Fudan Pedestrian*从头开始训练**DeepLab-v3**模型
- en: Using a pre-trained DeepLab-v3 model to optimize performance via transfer learning
    from *MS COCO* to *Penn-Fudan Pedestrian*
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的DeepLab-v3模型通过从*MS COCO*到*Penn-Fudan Pedestrian*的迁移学习来优化性能。
- en: Fine-tuning our pre-trained DeepLab-v3 model on *Penn-Fudan Pedestrian*
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*Penn-Fudan Pedestrian*上微调我们预训练的DeepLab-v3模型
- en: Let’s look at these steps in detail next.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下接下来的步骤。
- en: Revisiting the MS COCO and Penn-Fudan Pedestrian datasets
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新审视MS COCO和Penn-Fudan Pedestrian数据集
- en: '*MS COCO* and *Penn-Fudan Pedestrian* are both object detection and semantic
    segmentation datasets; however, they are quite different. *MS COCO* is a large-scale
    dataset containing ~150k images labeled into 80 classes (21 main ones) and has
    been used extensively in research and academia for benchmarking. *Penn-Fudan Pedestrian*
    is a small-scale dataset containing 170 images of 423 pedestrians. For this recipe,
    we will focus on the semantic segmentation task.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*MS COCO*和*Penn-Fudan Pedestrian*都是目标检测和语义分割数据集；然而，它们有很大的不同。*MS COCO*是一个大规模数据集，包含约150,000张图像，标记为80类（21个主要类），并且在研究和学术界广泛应用于基准测试。*Penn-Fudan
    Pedestrian*是一个小规模数据集，包含170张图像和423名行人。在本教程中，我们将专注于语义分割任务。'
- en: MXNet GluonCV does not provide methods to directly download any of the datasets.
    However, we do not need the *MS COCO* dataset (its size is ~19 GB), only the pre-trained
    parameters for our chosen model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet GluonCV不提供直接下载任何数据集的方法。然而，我们不需要*MS COCO*数据集（其大小约为19 GB），只需要选择模型的预训练参数。
- en: 'Here are some examples from *MS COCO*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些*MS COCO*的例子：
- en: '![Figure 7.22 – MS COCO example](img/B16591_07_22.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图7.22 – MS COCO示例](img/B16591_07_22.jpg)'
- en: Figure 7.22 – MS COCO example
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 – MS COCO示例
- en: 'For *Penn-Fudan Pedestrian*, all the information on how to retrieve the dataset
    can be found in the fourth recipe, *Segmenting objects in images with MXNet: PSPNet
    and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*. Taking that recipe’s code as a reference, we can
    display some examples:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*Penn-Fudan Pedestrian*，关于如何获取数据集的所有信息可以在第四篇教程中找到，*MXNet中图像分割：PSPNet和DeepLab-v3*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*计算机视觉中的图像分析*。参考该篇教程的代码，我们可以展示一些例子：
- en: '![Figure 7.23 – Penn-Fudan Pedestrian dataset examples](img/B16591_07_23.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图7.23 – Penn-Fudan Pedestrian数据集示例](img/B16591_07_23.jpg)'
- en: Figure 7.23 – Penn-Fudan Pedestrian dataset examples
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23 – Penn-Fudan Pedestrian数据集示例
- en: From *Figures 7.22* and *7.23*, we can see how some images from *MS COCO* resemble
    some of the images from *Penn-Fudan Pedestrian*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图7.22*和*图7.23*，我们可以看到一些*MS COCO*图像与*Penn-Fudan Pedestrian*的图像相似。
- en: Training a DeepLab-v3 model from scratch with Penn-Fudan Pedestrian
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Penn-Fudan Pedestrian从头开始训练DeepLab-v3模型
- en: 'As described in the fourth recipe, *Segmenting objects in images with MXNet:
    PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*, we will be using softmax cross-entropy as the loss
    function and pixel accuracy and **Mean Intersection over Union** (**mIoU**) as
    evaluation metrics.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第四个示例中所述，*使用 MXNet 对图像进行分割：PSPNet 和 DeepLab-v3*，在 [*第 5 章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*
    中，我们将使用 softmax 交叉熵作为损失函数，并使用像素准确率和 **平均交并比**（**mIoU**）作为评估指标。
- en: 'By following the code in our recipe, we have the following evolution while
    training from scratch our *DeepLab-v3* model:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们示例中的代码，我们得到了以下从头开始训练 *DeepLab-v3* 模型时的演变：
- en: '![Figure 7.24 – DeepLab-v3 training evolution (training loss and validation
    loss) – training from scratch](img/B16591_07_24.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.24 – DeepLab-v3 训练演变（训练损失和验证损失）– 从头开始训练](img/B16591_07_24.jpg)'
- en: Figure 7.24 – DeepLab-v3 training evolution (training loss and validation loss)
    – training from scratch
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24 – DeepLab-v3 训练演变（训练损失和验证损失）– 从头开始训练
- en: 'Furthermore, for the best iteration, the pixel accuracy and mIoU values obtained
    in the test set are as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的像素准确率和 mIoU 值如下：
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Even after training for 40 epochs, the evaluation values obtained do not show
    strong performance (an mIoU value of only 0.65).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 即使训练了 40 个周期，评估结果仍未显示出强劲的性能（mIoU 值仅为 0.65）。
- en: 'Qualitatively, we can also check how well our model is performing with an example
    image. In our case, we chose the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 定性地，我们还可以通过一个示例图像来检查模型的表现。在我们的案例中，我们选择了以下图像：
- en: '![Figure 7.25 – Image example of Penn-Fudan Pedestrian for qualitative results](img/B16591_07_25.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.25 – Penn-Fudan 行人数据集的图像示例，用于定性结果](img/B16591_07_25.jpg)'
- en: Figure 7.25 – Image example of Penn-Fudan Pedestrian for qualitative results
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25 – Penn-Fudan 行人数据集的图像示例，用于定性结果
- en: 'We can check the output of our model by running this image through it with
    the following code snippet:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码片段将此图像传入模型来检查模型的输出：
- en: '[PRE22]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code snippet shows the ground truth segmentations and the prediction
    from our model:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段展示了真实标签的分割结果和我们模型的预测结果：
- en: '![Figure 7.26 – Ground truth and prediction from DeepLab-v3 trained from scratch](img/B16591_07_26.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.26 – 从头开始训练的 DeepLab-v3 的真实标签与预测](img/B16591_07_26.jpg)'
- en: Figure 7.26 – Ground truth and prediction from DeepLab-v3 trained from scratch
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.26 – 从头开始训练的 DeepLab-v3 的真实标签与预测
- en: As can be seen from the results, the pedestrians have only started to be correctly
    segmented. To improve the results, we will need to train for more epochs and/or
    adjust the hyperparameters. However, a better, faster, and simpler approach would
    be to use transfer learning and fine-tuning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，行人只有在较晚阶段才开始被正确分割。为了改善结果，我们需要训练更多的周期和/或调整超参数。然而，一个更好、更快、更简单的方法是使用迁移学习并进行微调。
- en: Using a pre-trained DeepLab-v3 model to optimize performance via transfer learning
    from MS COCO to Penn-Fudan Pedestrian
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的 DeepLab-v3 模型，通过从 MS COCO 到 Penn-Fudan 行人的迁移学习优化性能
- en: 'In the previous recipe, we trained a new model from scratch using our dataset.
    However, this has three important drawbacks:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个示例中，我们使用自己的数据集从头开始训练了一个新模型。然而，这种方法有三个重要的缺点：
- en: A large amount of data is required for training from scratch.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练需要大量的数据。
- en: The training process can take a very long time due to the large size of the
    dataset and the number of epochs needed for the model to learn the task.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集的庞大和模型需要的训练周期数，训练过程可能会耗时很长。
- en: The compute resources required might be expensive or difficult to procure.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的计算资源可能会非常昂贵或难以获取。
- en: Therefore, in this recipe, we will follow a different approach. We will use
    pre-trained models from the MXNet GluonCV Model Zoo to solve the task. These models
    have been trained in *MS COCO*, a dataset that contains the classes we are interested
    in (`person` in this case); therefore, we can use those learned representations
    and easily transfer them to *Penn-Fudan Pedestrian* (same domain).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本示例中，我们将采用不同的方法。我们将使用来自 MXNet GluonCV 模型库的预训练模型来解决任务。这些模型已经在 *MS COCO* 数据集上训练过，包含我们感兴趣的类别（在本案例中是
    `person`）；因此，我们可以使用这些学到的表示，并轻松地将它们迁移到 *Penn-Fudan 行人*（同一领域）。
- en: 'For a DeepLab-v3 model, we have the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DeepLab-v3 模型，我们有以下内容：
- en: '[PRE23]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see in the preceding code snippet, following the discussion in this
    chapter’s first recipe, *Understanding Transfer-Learning and Fine-Tuning*, for
    the `pretrained` parameter, we have assigned the value of `True`, indicating that
    we want the pretrained weights to be retrieved (and not only the architecture
    of the model).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的代码片段中看到的，按照本章第一个食谱中讨论的内容，*理解迁移学习和微调*，对于 `pretrained` 参数，我们已将其值设置为 `True`，表示我们希望获取预训练的权重（而不仅仅是模型的架构）。
- en: 'In order to evaluate adequately the improvements that transfer learning brings,
    we are going to directly evaluate our pre-trained model in our target task (the
    task source is *MS COCO*) before applying transfer learning to *Penn-Fudan Pedestrian*
    and after applying it. Therefore, using our pre-trained model as is, we obtain
    the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分评估迁移学习带来的改进，我们将直接在我们的目标任务中评估预训练模型（任务源自*MS COCO*），然后再将迁移学习应用到*Penn-Fudan
    Pedestrian*，并对比应用前后的结果。因此，使用我们当前的预训练模型，我们获得了以下结果：
- en: '[PRE24]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, our pre-trained Transformer model is already showing good performance
    values as it is in the same domain. Moreover, the great advantage of using pre-trained
    models is the time savings, as loading a pre-trained model just takes a few lines
    of code.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的预训练 Transformer 模型在相同领域中已经显示出良好的性能值。此外，使用预训练模型的巨大优势是节省时间，因为加载预训练模型只需要几行代码。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的图像示例和代码检查模型的定性表现。输出结果如下：
- en: '![Figure 7.27 – Ground truth and prediction from a DeepLab-v3 pre-trained model](img/B16591_07_27.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.27 – DeepLab-v3 预训练模型的真实值与预测值](img/B16591_07_27.jpg)'
- en: Figure 7.27 – Ground truth and prediction from a DeepLab-v3 pre-trained model
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27 – DeepLab-v3 预训练模型的真实值与预测值
- en: 'As can be seen from the results, the pedestrians have been correctly segmented.
    Please note a side advantage of using pre-trained models in *Figure 7**.27*: in
    the ground truth image, the people in the background were not segmented, but the
    pre-trained model correctly picked them up (which might explain the low mIoU values).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，行人已被正确地分割。请注意，使用预训练模型的一个附加优点见于*图 7.27*：在真实值图像中，背景中的人没有被分割，但预训练模型正确地识别了它们（这可能解释了较低的
    mIoU 值）。
- en: Now that we have a baseline for comparison, let’s apply transfer learning to
    our task. In the first recipe, *Understanding transfer learning and fine-tuning*,
    the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV
    or GluonNLP), which we have already done.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个比较基准，让我们将迁移学习应用于我们的任务。在第一个食谱中，*理解迁移学习和微调*，第一步是从 MXNet Model Zoo（GluonCV
    或 GluonNLP）中获取预训练模型，而这一部分我们已经完成。
- en: The second step is to remove the last layers (typically, a classifier), keeping
    the parameters in the rest of the layers frozen (not updatable during training),
    so let’s do it!
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是移除最后的几层（通常是分类器），并保持其余层的参数冻结（在训练过程中不可更新），所以让我们开始吧！
- en: 'We can freeze the *DeepLab-v3* feature extraction layers with the following
    snippet:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码片段冻结*DeepLab-v3*的特征提取层：
- en: '[PRE25]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Furthermore, we will also need to replace the segmentation task head. Previously,
    it supported 21 classes from *MS COCO*. For our experiments, two classes are enough,
    `background` and `person`. This is done with the following snippet:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要替换分割任务头。以前，它支持来自*MS COCO*的 21 类。对于我们的实验，两个类别就足够了，`background` 和 `person`。这可以通过以下代码片段完成：
- en: '[PRE26]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can apply the usual training process with *Penn-Fudan Pedestrian*,
    and we have the following evolution in the training using the *DeepLab-v3* model:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用*Penn-Fudan Pedestrian*应用常规的训练过程，并且我们使用*DeepLab-v3*模型时，训练演化如下：
- en: '![Figure 7.28 – DeepLab-v3 training evolution (training loss and validation
    loss) – transfer learning](img/B16591_07_28.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.28 – DeepLab-v3 训练演化（训练损失和验证损失）– 迁移学习](img/B16591_07_28.jpg)'
- en: Figure 7.28 – DeepLab-v3 training evolution (training loss and validation loss)
    – transfer learning
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.28 – DeepLab-v3 训练演化（训练损失和验证损失）– 迁移学习
- en: 'Furthermore, for the best iteration, the evaluation metrics obtained in the
    test set are as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的评估指标如下所示：
- en: '[PRE27]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Compared with our previous experiments of training from scratch and pre-training,
    this experiment yields slightly better performance, and it took us literally minutes
    to get this model to start working for us in our intended task, whereas the training
    required for the training from scratch experiment took hours and required several
    tries to tune the hyperparameters, which turned into several days of effort in
    total.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前从头开始训练和预训练的实验相比，这个实验的表现略好，而且我们花费了几分钟就让这个模型开始在我们预定的任务上工作，而从头开始训练的实验则花费了数小时，并且需要多次尝试调整超参数，最终耗费了几天的时间。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的图像示例和代码检查模型的定性表现。输出结果如下：
- en: '![Figure 7.29 – Ground truth and prediction from the DeepLab-v3 pre-trained
    model with transfer learning](img/B16591_07_29.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.29 – 经过转移学习的 DeepLab-v3 预训练模型的真实标签与预测结果](img/B16591_07_29.jpg)'
- en: Figure 7.29 – Ground truth and prediction from the DeepLab-v3 pre-trained model
    with transfer learning
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29 – 经过转移学习的 DeepLab-v3 预训练模型的真实标签与预测结果
- en: As can be seen from the results, the pedestrians have been correctly segmented.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，行人已经被正确地分割。
- en: Fine-tuning our pre-trained DeepLab-v3 model on Penn-Fudan Pedestrian
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Penn-Fudan 行人数据集上对我们预训练的 DeepLab-v3 模型进行微调
- en: In the previous recipe, we *froze* the parameters in the encoder layers. However,
    with the dataset we are currently working with (*Penn-Fudan Pedestrian*), we can
    *unfreeze* those parameters and train the model, effectively allowing the new
    training process to update the representations (with transfer learning, we were
    working directly with the representations learned for *MS COCO*). As introduced
    in this chapter, this process is called fine-tuning.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配方中，我们*冻结*了编码器层中的参数。然而，在我们当前使用的数据集（*Penn-Fudan 行人数据集*）中，我们可以*解冻*这些参数并训练模型，从而有效地使新的训练过程更新表示（在转移学习中，我们直接使用了为
    *MS COCO* 学习到的表示）。如本章所介绍的，这个过程被称为微调。
- en: 'There are two variants of fine-tuning:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 微调有两种变体：
- en: Apply transfer learning by freezing the layers and unfreezing them afterward.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过冻结层并随后解冻层来应用转移学习。
- en: Directly apply fine-tuning without the preliminary step of freezing the layers.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接应用微调，而不进行冻结层的预备步骤。
- en: Let’s compute both experiments and draw conclusions by comparing the results.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算两个实验并通过比较结果得出结论。
- en: 'For the first experiment, we can take the network obtained in the previous
    recipe, unfreeze the layers, and restart the training. In MXNet, to unfreeze the
    encoder parameters, we can run the following snippet:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个实验，我们可以使用之前配方中获得的网络，解冻层并重新开始训练。在 MXNet 中，要解冻编码器参数，可以运行以下代码片段：
- en: '[PRE28]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we can apply the usual training process with *Penn-Fudan Pedestrian*,
    and we have the following evolution in the training using the *DeepLab-v3* model:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以应用常规的训练过程来训练 *Penn-Fudan 行人数据集*，并且在使用 *DeepLab-v3* 模型时，我们有以下训练演变：
- en: '![Figure 7.30 – DeepLab-v3 training evolution (training loss and validation
    loss) – fine-tuning after transfer learning](img/B16591_07_30.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.30 – DeepLab-v3 训练过程演变（训练损失和验证损失）– 转移学习后的微调](img/B16591_07_30.jpg)'
- en: Figure 7.30 – DeepLab-v3 training evolution (training loss and validation loss)
    – fine-tuning after transfer learning
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.30 – DeepLab-v3 训练过程演变（训练损失和验证损失）– 转移学习后的微调
- en: 'Furthermore, for the best iteration, the evaluation metrics obtained in the
    test set are as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，在测试集上获得的评估指标如下：
- en: '[PRE29]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Compared with our previous experiment in transfer learning, this experiment
    yields ~3% better performance in mIoU, a very good increase taking into account
    the low training time invested.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前在转移学习中的实验相比，这个实验在 mIoU 上的表现提高了约 3%，考虑到投入的训练时间较少，这是一个非常好的提升。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的图像示例和代码检查模型的定性表现。输出结果如下：
- en: '![Figure 7.31 – Ground truth and prediction from the DeepLab-v3 pre-trained
    model with fine-tuning after transfer learning](img/B16591_07_31.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.31 – 经过微调后的 DeepLab-v3 预训练模型的真实标签与预测结果](img/B16591_07_31.jpg)'
- en: Figure 7.31 – Ground truth and prediction from the DeepLab-v3 pre-trained model
    with fine-tuning after transfer learning
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31 – 经过微调后的 DeepLab-v3 预训练模型的真实标签与预测结果
- en: As can be seen from the results, the pedestrians have been correctly segmented.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，行人已经被正确地分割。
- en: Let’s continue now with the second fine-tuning experiment, in which we do not
    apply transfer learning (no frozen layers) and, instead, apply fine-tuning directly
    to the whole model.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进行第二次微调实验，在此实验中，我们不应用迁移学习（没有冻结层），而是直接对整个模型应用微调。
- en: 'We need to retrieve the pre-trained *DeepLab-v3* model for *MS COCO*, with
    the following code snippet for MXNet GluonCV:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要检索用于*MS COCO*的预训练*DeepLab-v3*模型，以下是MXNet GluonCV的代码片段：
- en: '[PRE30]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And now, without freezing, we can apply the training process, which will update
    all layers of our *DeepLab-v3* model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在不冻结的情况下应用训练过程，这将更新我们*DeepLab-v3*模型的所有层：
- en: '![Figure 7.32 – DeepLab-v3 training evolution (training loss and validation
    loss) – fine-tuning without freezing](img/B16591_07_32.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.32 – DeepLab-v3训练演变（训练损失和验证损失） – 未冻结的微调](img/B16591_07_32.jpg)'
- en: Figure 7.32 – DeepLab-v3 training evolution (training loss and validation loss)
    – fine-tuning without freezing
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.32 – DeepLab-v3训练演变（训练损失和验证损失） – 未冻结的微调
- en: 'Furthermore, for the best iteration, the evaluation metrics obtained in the
    test set are as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，在测试集上获得的评估指标如下：
- en: '[PRE31]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Compared with our previous fine-tuning experiment, we can see how these experiments
    yield very similar performance. Empirically, it has been proven that this fine-tuning
    experiment can also yield slightly lower results because initially freezing the
    encoder allows for the decoder to learn (using the encoder representations) the
    new task at hand. From a point of view, in this step, there is a knowledge transfer
    from the encoder to the decoder. In a secondary step, when the encoder is unfrozen,
    the learned parameters from the decoder perform auxiliary transfer learning, this
    time from thedecoder to the encoder.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的微调实验相比，我们可以看到这些实验表现出非常相似的性能。根据经验，已经证明这个微调实验可能会略微降低结果，因为最初冻结编码器使解码器能够使用编码器表示学习当前任务。从某种角度看，在这一步，知识从编码器传递到解码器。在第二步中，当编码器被解冻时，解码器中学习到的参数执行辅助迁移学习，这次是从解码器到编码器。
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的图像示例和代码来检查我们的模型在定性上的表现。输出如下：
- en: '![Figure 7.33 – Ground truth and prediction from the DeepLab-v3 pre-trained
    model with fine-tuning without freezing](img/B16591_07_33.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.33 – DeepLab-v3预训练模型的真实标签与预测（未冻结的微调）](img/B16591_07_33.jpg)'
- en: Figure 7.33 – Ground truth and prediction from the DeepLab-v3 pre-trained model
    with fine-tuning without freezing
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.33 – DeepLab-v3预训练模型的真实标签与预测（未冻结的微调）
- en: As can be seen from the results, the pedestrians have been correctly segmented,
    although, as mentioned, if we look at the person on the right, the arm closer
    to the person on the left could be segmented better. As discussed, sometimes this
    version of fine-tuning yields slightly lower results than other approaches.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，行人已经被正确分割，尽管如前所述，如果我们看右侧的那个人，靠近左侧那个人的手臂可能会被更好地分割。如前所讨论，有时这种微调版本的结果可能会略低于其他方法。
- en: How it works…
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we applied the techniques of transfer learning and fine-tuning,
    introduced at the beginning of the chapter, to the task of image classification,
    which was also presented previously, in the fourth recipe, *Segmenting objects
    in images with MXNet: PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将本章开头介绍的迁移学习和微调技术应用于图像分类任务，这在之前的第四个方案中也有呈现，*使用MXNet进行图像对象分割：PSPNet和DeepLab-v3*，位于[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*。
- en: 'We revisited two known datasets, *MS COCO* and *Penn-Fudan Pedestrian*, which
    we intended to combine using knowledge transfer based on the former dataset and
    refining that knowledge with the latter. Moreover, MXNet GluonCV provided the
    following:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新访问了两个已知的数据集，*MS COCO*和*Penn-Fudan Pedestrian*，我们打算通过基于前者数据集的知识迁移，并利用后者对该知识进行优化来将它们结合起来。此外，MXNet
    GluonCV提供了以下内容：
- en: A pre-trained *DeepLab-v3* model for *MS COCO*
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于*MS COCO*的预训练*DeepLab-v3*模型
- en: Tools for easy-to-use access to *Penn-Fudan Pedestrian*
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于便捷访问*Penn-Fudan Pedestrian*的工具
- en: Furthermore, we continued using the loss functions and metrics introduced for
    semantic segmentation, softmax cross-entropy, pixel accuracy, and mIoU.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们继续使用为语义分割引入的损失函数和指标，如softmax交叉熵、像素准确率和mIoU。
- en: 'Having all these tools readily available within MXNet and GluonCV allowed us
    to run the following experiments with just a few lines of code:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在MXNet和GluonCV中，所有这些工具都已经预先准备好，这使得我们只需几行代码就能进行以下实验：
- en: Training a model from scratch with *Penn-Fudan Pedestrian*
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*Penn-Fudan Pedestrian*从零开始训练模型
- en: Using a pre-trained model to optimize performance via transfer learning from
    *MS COCO* to *Penn-Fudan Pedestrian*
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型通过迁移学习从*MS COCO*到*Penn-Fudan Pedestrian*优化性能
- en: Fine-tuning our pre-trained model on *Penn-Fudan Pedestrian* (with and without
    freezing layers)
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*Penn-Fudan Pedestrian*上微调我们的预训练模型（包括冻结和不冻结层的情况）
- en: After running the different experiments and taking into account the qualitative
    results and the quantitative results, transfer learning (with a pixel accuracy
    of 0.95 and mIoU of 0.88) has been the best experiment for our task. The actual
    results obtained when running these experiments might differ based on model architecture,
    datasets, and hyperparameters chosen, so you are encouraged to try out different
    techniques and variations.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 经过不同实验的运行，并结合定性结果和定量结果，迁移学习（像素准确率为0.95，mIoU为0.88）已经成为我们任务中最佳的实验方法。实际运行这些实验时获得的结果可能会因为模型架构、数据集和所选择的超参数而有所不同，因此我们鼓励你尝试不同的技术和变化。
- en: There’s more...
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Transfer learning, including fine-tuning, is an active field of research. A
    recent paper published in 2022 explores the latest advances in image classification.
    The paper is titled *Deep Transfer Learning for Image Classification: A survey*,
    and can be found here: [https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey](https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '迁移学习，包括微调，是一个活跃的研究领域。2022年发布的一篇论文探讨了图像分类的最新进展。该论文题为*Deep Transfer Learning
    for Image Classification: A survey*，可以在这里找到：[https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey](https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey)。'
- en: 'An interesting paper that combines transfer learning and semantic segmentation
    is *Semantic Segmentation with Transfer Learning for Off-Road Autonomous Driving*,
    in which a change of domain is also studied by the usage of synthetic data. It
    can be found here: [https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving](https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇有趣的论文将迁移学习和语义分割结合起来，题为*Semantic Segmentation with Transfer Learning for Off-Road
    Autonomous Driving*，在这篇论文中，也通过使用合成数据研究了领域的变化。可以在这里找到：[https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving](https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving)。
- en: 'A more general overview is given in this paper: *Learning Transferable Knowledge
    for Semantic Segmentation with Deep Convolutional Neural Network*, accepted for
    **Computer Vision and Pattern Recognition** (**CVPR**) symposium in 2016\. It
    can be found here: [https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文提供了更一般的概述：*Learning Transferable Knowledge for Semantic Segmentation with
    Deep Convolutional Neural Network*，该论文被**计算机视觉与模式识别**（**CVPR**）大会于2016年接收。可以在这里找到：[https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf)。
- en: Improving performance for translating English to German
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进英德翻译的性能
- en: In the previous recipes, we have seen how we can leverage pre-trained models
    and new datasets for transfer learning and fine-tuning applied to CV tasks. In
    this recipe, we will follow a similar approach, but with an NLP task, translating
    from English to German.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们已经看到如何利用预训练模型和新数据集进行迁移学习和微调，应用于计算机视觉任务。在本示例中，我们将采用类似的方法，但针对一个自然语言处理任务，即从英语翻译成德语。
- en: In the fourth recipe, *Translating text from Vietnamese to English*, in [*Chapter
    6*](B16591_06.xhtml#_idTextAnchor121), *Understanding Text with Natural Language
    Processing*, we saw how we could use GluonNLP to retrieve pre-trained models and
    use them directly for a translation task, training them from scratch, effectively
    only leveraging past knowledge by using the architecture of the pre-trained model.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四个食谱中，*从越南语翻译到英语*，来自[*第六章*](B16591_06.xhtml#_idTextAnchor121)，*理解文本与自然语言处理*，我们看到如何使用GluonNLP直接检索预训练的模型，并将它们用于翻译任务，从头开始训练，实际上只是通过使用预训练模型的架构来有效地利用过去的知识。
- en: In this recipe, we will also leverage the weights/parameters of the model, obtained
    for a task consisting of translating text from English to German using **machine
    translation** models. The dataset that we will use for pre-training will be *WMT2014*
    (task source), and we will run several experiments to evaluate our models in a
    new (target) task, using the dataset *WMT2016* dataset (with a ~20% increased
    vocabulary of words and sentences for German-English pairs).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们还将利用模型的权重/参数，这些权重/参数是通过机器翻译模型用于将英文文本翻译成德文的任务获得的。我们将使用*WMT2014*数据集（任务来源）进行预训练，并将进行多个实验，使用*WMT2016*数据集（增加了约20%的德英对词汇和句子）评估我们在新目标任务中的模型。
- en: Getting ready
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的章节一样，在本食谱中，我们将使用一些矩阵运算和线性代数，但这并不难。
- en: 'Furthermore, we will be working with text datasets; therefore, we will revisit
    some concepts already seen in the fourth recipe, *Understanding text datasets
    – load, manage, and visualize Enron Emails dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将处理文本数据集；因此，我们将重新审视在第四个食谱中已经看到的一些概念，*理解文本数据集——加载、管理和可视化Enron邮件数据集*，来自[*第二章*](B16591_02.xhtml#_idTextAnchor029)，*使用MXNet和可视化数据集：Gluon*
    *和DataLoader*。
- en: How to do it...
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将查看以下步骤：
- en: Introducing the *WMT2014* and *WMT2016* datasets
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍*WMT2014*和*WMT2016*数据集
- en: Training a Transformer model from scratch with *WMT2016*
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始训练一个Transformer模型，使用*WMT2016*数据集
- en: Using a pre-trained Transformer model to optimize performance via transfer learning
    from *WMT2014* to *WMT2016*
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的Transformer模型，通过迁移学习将性能从*WMT2014*优化到*WMT2016*
- en: Fine-tuning our pre-trained Transformer model on *WMT2016*
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*WMT2016*上微调我们预训练的Transformer模型
- en: Let’s look at these steps in detail next.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将详细查看这些步骤。
- en: Introducing the WMT2014 and WMT2016 datasets
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍WMT2014和WMT2016数据集
- en: '*WMT2014* and *WMT2016* are multi-modal (multi-language) translation datasets,
    including Chinese, English, and German corpus. *WMT2014* was first introduced
    in 2014 in *Proceedings of the Ninth Workshop on Statistical Machine Translation*,
    as part of the evaluation campaign of the translation models. This workshop was
    upgraded to its own conference in 2016, and *WMT2016* was introduced as part of
    the evaluation campaign of translation models in *Proceedings of the First Conference
    on Machine Translation*. Both datasets are very similar, retrieving information
    from news sources, and the largest difference is the corpus (the size of the vocabulary
    used for both). WMT2014 is about ~140k distinct words, whereas WMT2016 is slightly
    larger with ~150k words, and specifically for German-English pairs, an increase
    of ~20% of words and sentences.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '*WMT2014*和*WMT2016*是多模态（多语言）翻译数据集，包括中文、英语和德语语料库。*WMT2014*首次在2014年《第九届统计机器翻译研讨会论文集》上介绍，作为翻译模型评估活动的一部分。该研讨会在2016年升级为自己的会议，并在《第一次机器翻译会议论文集》中介绍了*WMT2016*，作为翻译模型评估活动的一部分。这两个数据集非常相似，都是从新闻来源中提取信息，最大的区别在于语料库（两个数据集所用词汇量的大小）。WMT2014大约包含~14万个不同的词，而WMT2016略大，包含~15万个词，特别是在德英对中，增加了约20%的词汇和句子。'
- en: 'MXNet GluonNLP provides ready-to-use versions of these datasets. For our case,
    we will work with *WMT2016*, which only contains *train* and *test* splits. We
    will further split the test set to obtain *validation* and *test* splits. Here
    is the code to load the dataset:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet GluonNLP提供了这些数据集的现成版本。在我们的案例中，我们将使用*WMT2016*，它仅包含*train*和*test*划分。我们将进一步拆分测试集，获得*validation*和*test*划分。以下是加载数据集的代码：
- en: '[PRE32]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is the code to generate *validation* and *test* splits:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成*validation*和*test*划分的代码：
- en: '[PRE33]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After splitting, our *WMT2016* datasets provide the following data:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割后，我们的*WMT2016*数据集提供以下数据：
- en: '[PRE34]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: From the large number of instances on each of the datasets, we can confirm that
    these are suitable for our experiments.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个数据集上大量的实例中，我们可以确认这些数据集适合用于我们的实验。
- en: Training a Transformer model from scratch in WMT2016
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在WMT2016中从零开始训练一个Transformer模型
- en: As described in the fourth recipe, *Translating text from Vietnamese to English*,
    in [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), *Understanding Text with Natural
    Language Processing*, we will be using **Perplexity** for our *per-batch computations*
    in training, and **BLEU** for *per-epoch computations*, which will show us the
    evolution of our training process, as part of the typically used training and
    validation losses. We will also use them for quantitative evaluation, and for
    qualitative evaluation, we will choose a sentence (feel free to use any other
    sentence you can come up with).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如第四章所述，*从越南语翻译到英语*，在[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*使用自然语言处理理解文本*中，我们将使用**困惑度**进行*每批次计算*，并使用**BLEU**进行*每轮计算*，这些将展示我们的训练过程演变，作为典型的训练和验证损失的一部分。我们还将它们用于定量评估，对于定性评估，我们将选择一个句子（也可以使用任何你想出的其他句子）。
- en: 'We have the following evolution in the training using the Transformer model:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用Transformer模型进行训练时有以下演变：
- en: '![Figure 7.34 – Transformer training evolution (training loss) – training from
    scratch](img/B16591_07_34.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.34 – Transformer 训练演变（训练损失）– 从零开始训练](img/B16591_07_34.jpg)'
- en: Figure 7.34 – Transformer training evolution (training loss) – training from
    scratch
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.34 – Transformer 训练演变（训练损失）– 从零开始训练
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的损失、困惑度和BLEU得分（乘以100）如下：
- en: '[PRE35]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Current **State-of-the-Art** (**SOTA**) models can yield above 30 points in
    the BLEU score; we reach about halfway with 10 epochs, reaching SOTA performance
    in ~30 epochs.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 当前**最先进的技术**（**SOTA**）模型的BLEU得分可以超过30分；我们在10个epoch后大约达到一半，在大约30个epoch后达到SOTA性能。
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose: `"I learn new things every day"`, and this can
    be verified with the following code:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 从质量上讲，我们也可以通过一个句子示例检查我们的模型表现如何。在我们的案例中，我们选择了：`"I learn new things every day"`，可以通过以下代码进行验证：
- en: '[PRE36]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'These code statements will give us the following output:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码语句将给我们以下输出：
- en: '[PRE37]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The German sentence means *I think that’s the case here*; therefore, as can
    be seen from this result, the text has not been correctly translated from English
    to German, and we would need to invest more time in training to achieve the right
    results.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这句德语意味着*我认为这里是这种情况*；因此，从这个结果可以看出，文本没有正确地从英语翻译成德语，我们需要投入更多的时间来训练，才能获得正确的结果。
- en: Using a pre-trained Transformer model to optimize performance via transfer learning
    from WMT2014 to WMT2016
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的Transformer模型，通过将WMT2014转移到WMT2016进行优化，从而提升性能。
- en: 'In the previous recipe, we trained a new model from scratch using our dataset.
    However, this has two important drawbacks:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用自己的数据集从零开始训练了一个新模型。然而，这有两个重要的缺点：
- en: A large amount of data is required for training from scratch.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始训练需要大量的数据。
- en: The training process can take a very long time due to the large size of the
    dataset and the number of epochs needed for the model to learn the task.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集的庞大规模和模型需要的训练轮数，训练过程可能会非常漫长。
- en: Therefore, in this recipe, we will follow a different approach. We will use
    pre-trained models from MXNet GluonNLP to solve the task. These models have been
    trained on *WMT2014* a very similar dataset, so the representations learned for
    this task can be easily transferred to *WMT2016* (same domain).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将采用不同的方法。我们将使用MXNet GluonNLP中的预训练模型来解决该任务。这些模型已经在与*WMT2014*非常相似的数据集上进行训练，因此为该任务学习到的表示可以很容易地迁移到*WMT2016*（同一领域）。
- en: 'For a Transformer model, we have the following:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个Transformer模型，我们有如下内容：
- en: '[PRE38]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output shows us the size of the vocabulary of the *WMT2014* dataset (the
    pre-trained English to German translation task):'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 输出展示了*WMT2014*数据集的词汇表大小（预训练的英德翻译任务）：
- en: '[PRE39]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This is a subset of the whole corpus available for *WMT2014*. As we can also
    see in the preceding code snippet, following the discussion in this chapter’s
    first recipe, *Understanding transfer learning and fine-tuning*, for the `pretrained`
    parameter, we have assigned the value of `True`, indicating that we want the pretrained
    weights to be retrieved (and not only the architecture of the model).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*WMT2014*数据集的一个子集。正如我们在前面的代码片段中所看到的，按照本章第一个配方*理解迁移学习和微调*中的讨论，对于`pretrained`参数，我们已将其值设置为`True`，表示我们希望检索预训练的权重（而不仅仅是模型的架构）。
- en: 'In order to evaluate adequately the improvements that transfer learning brings,
    we are going to directly evaluate our pre-trained model (task source is *WMT2014*)
    before applying transfer learning to *WMT2016* and after applying it. Therefore,
    using our pre-trained model as is, we obtain the following:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分评估迁移学习带来的改进，我们将直接评估我们的预训练模型（任务源是*WMT2014*）在应用迁移学习到*WMT2016*之前和之后的表现。因此，直接使用我们的预训练模型，我们得到以下结果：
- en: '[PRE40]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As we can see, our pre-trained Transformer model is already showing very good
    performance values as it is the same domain; however, simply using a pre-trained
    model does not yield SOTA performance, which can be achieved if training from
    scratch. The great advantage of using pre-trained models is the time and compute
    savings as loading a pre-trained model just takes a few lines of code.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的预训练Transformer模型已经显示出非常好的性能，因为它属于相同的领域；然而，单纯使用预训练模型并不能达到SOTA（最先进的）性能，这只能通过从零开始训练来实现。使用预训练模型的巨大优势在于节省时间和计算资源，因为加载一个预训练模型只需要几行代码。
- en: 'We can also check how well our model is performing qualitatively with the same
    sentence example and code. The output is given as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的句子示例和代码检查模型的定性表现。输出结果如下：
- en: '[PRE41]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The German sentence means *I learn new things that arise in every case*; therefore,
    as can be seen from the results, the text has not yet been correctly translated
    from English to German, but this time, was much closer than our previous experiment.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这句德语的意思是*我学习在每个案例中出现的新事物*；因此，从结果中可以看出，文本尚未正确地从英语翻译成德语，但这一次，比我们之前的实验更接近了。
- en: Now that we have a baseline for comparison, let’s apply transfer learning to
    our task. In the first recipe, *Understanding transfer learning and fine-tuning*,
    the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV
    or GluonNLP), which we have already done.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个比较的基准，让我们将迁移学习应用到我们的任务中。在第一个配方中，*理解迁移学习和微调*，第一步是从MXNet模型库（GluonCV或GluonNLP）中检索一个预训练的模型，这一步我们已经完成了。
- en: The second step is to remove the last layers (typically, a classifier), keeping
    the parameters in the rest of the layers frozen (not updatable during training),
    so let’s do it!
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是去掉最后一层（通常是分类器），保持其余层的参数冻结（在训练过程中不可更新），让我们来做吧！
- en: 'We can freeze all parameters except the classifier with the following snippet,
    keeping the parameters frozen (we will unfreeze them in a later experiment):'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下代码冻结除分类器之外的所有参数，保持这些参数被冻结（我们将在后续实验中解冻它们）：
- en: '[PRE42]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can apply the usual training process with *WMT2016*, and we have the
    following evolution in the training using the Transformer model:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用*WMT2016*应用常规的训练过程，接着我们可以看到在使用Transformer模型训练时的演变：
- en: '![Figure 7.35 – Transformer training evolution (training loss) – transfer learning](img/B16591_07_35.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.35 – Transformer训练演变（训练损失）– 迁移学习](img/B16591_07_35.jpg)'
- en: Figure 7.35 – Transformer training evolution (training loss) – transfer learning
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.35 – Transformer训练演变（训练损失）– 迁移学习
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的损失、困惑度和BLEU得分（乘以100）如下：
- en: '[PRE43]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Compared with our previous experiments, this experiment yields slightly lower
    numerical performance; however, it took us literally minutes to get this model
    to start working for us in our intended task, whereas training from scratch in
    our previous experiment took hours and required several tries to tune the hyperparameters,
    becoming several days of effort in total.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的实验相比，这次实验的数值表现略低；然而，我们花了几分钟就让这个模型开始在我们预定的任务中发挥作用，而之前的实验从头开始训练则花了几个小时，并且需要多次尝试调整超参数，总共花费了几天的时间。
- en: 'We can also check how well our model is performing qualitatively with the same
    sentence example and code. The output is given as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过相同的句子示例和代码检查模型的定性表现。输出如下：
- en: '[PRE44]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The German sentence means *I learn new things every time*; therefore, as can
    be seen from the results, the text has been almost correctly translated from English
    to German, improving from our previous experiment (pre-trained model), although
    the (better) quantitative results were suggesting otherwise.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这句德语句子意味着*我每次都学到新东西*；因此，从结果可以看出，文本几乎已正确地从英语翻译成德语，相较于我们之前的实验（预训练模型），有所改进，尽管（更好的）定量结果显示了不同的趋势。
- en: Fine-tuning our pre-trained Transformer model on WMT2016
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 WMT2016 上微调我们预训练的 Transformer 模型
- en: In the previous recipe, we froze all the parameters except the classifier. However,
    as the dataset we are currently working with (*WMT2016*) has enough data samples,
    we can unfreeze those parameters and train the model, effectively allowing the
    new training process to update the representations (with transfer learning, we
    were working directly with the representations learned for *WMT2014*). This process,
    as we know, is called fine-tuning.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方案中，我们冻结了所有参数，除了分类器。然而，由于我们目前使用的数据集（*WMT2016*）有足够的数据样本，我们可以解冻这些参数并训练模型，有效地让新的训练过程更新表示（通过迁移学习，我们直接使用了为
    *WMT2014* 学到的表示）。这个过程，正如我们所知，叫做微调。
- en: 'There are two variants of fine-tuning:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 微调有两种变体：
- en: Apply transfer learning by freezing the layers and unfreezing them afterward.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过冻结层并在之后解冻它们来应用迁移学习。
- en: Directly apply fine-tuning without the preliminary step of freezing the layers.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接应用微调，而不需要冻结层的预备步骤。
- en: Let’s compute both experiments and draw conclusions by comparing the results.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算两个实验，并通过比较结果得出结论。
- en: 'For the first experiment, we can take the network obtained in the previous
    recipe, unfreeze the layers, and restart the training. In MXNet, to unfreeze the
    encoder parameters, we can run the following snippet:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个实验，我们可以获取在之前方案中获得的网络，解冻层并重新开始训练。在 MXNet 中，要解冻编码器参数，我们可以运行以下代码片段：
- en: '[PRE45]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, we can apply the usual training process with *WMT2016*, and we have the
    following evolution in the training using the Transformer model:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以应用通常的训练过程，使用 *WMT2016*，我们得到了 Transformer 模型训练中的以下演变：
- en: '![Figure 7.36 – Transformer training evolution (training loss) – fine-tuning
    after transfer learning](img/B16591_07_36.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.36 – Transformer 训练演变（训练损失）– 迁移学习后微调](img/B16591_07_36.jpg)'
- en: Figure 7.36 – Transformer training evolution (training loss) – fine-tuning after
    transfer learning
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.36 – Transformer 训练演变（训练损失）– 迁移学习后微调
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集中的损失、困惑度和 BLEU 分数（乘以 100）如下：
- en: '[PRE46]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Compared with our previous experiment in transfer learning, this experiment
    yields a slightly worse quantitative performance.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的迁移学习实验相比，本次实验的定量表现略差。
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose `"I learn new things every day"`, and the output
    obtained is as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性上讲，我们还可以通过一个句子示例来检查我们的模型表现如何。在我们的例子中，我们选择了 `"I learn new things every day"`，得到的输出如下：
- en: '[PRE47]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The German sentence means *I learn something new every time*; therefore, as
    can be seen from the results, the text has been almost correctly translated from
    English to German.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这句德语句子意味着*我每次都学到新东西*；因此，从结果可以看出，文本几乎已正确地从英语翻译成德语。
- en: Let’s continue now with the second fine-tuning experiment, where we do not apply
    transfer learning (no frozen layers), and instead apply fine-tuning directly to
    the whole model.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进行第二个微调实验，在这个实验中，我们不应用迁移学习（没有冻结层），而是直接对整个模型应用微调。
- en: 'We need to retrieve again the pre-trained Transformer model for *WMT2014*,
    with the following code snippet for MXNet GluonNLP:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重新获取预训练的 Transformer 模型，使用以下 MXNet GluonNLP 代码片段：
- en: '[PRE48]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'And now, without freezing, we can apply the training process, which will update
    all layers of our Transformer model:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在不冻结的情况下，我们可以应用训练过程，这将更新我们 Transformer 模型的所有层：
- en: '![Figure 7.37 – Transformer training evolution (training loss) – fine-tuning
    without freezing](img/B16591_07_37.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.37 – Transformer 训练演变（训练损失）– 不冻结的微调](img/B16591_07_37.jpg)'
- en: Figure 7.37 – Transformer training evolution (training loss) – fine-tuning without
    freezing
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.37 – Transformer 训练演化（训练损失）– 不冻结层进行微调
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于最佳迭代，测试集上获得的损失、困惑度和 BLEU 分数（乘以 100）如下：
- en: '[PRE49]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Compared with our previous fine-tuning experiment, we can see how this experiment
    yields a slightly higher performance. Empirically, however, the opposite results
    were expected (for this experiment to yield a slightly lower performance). This
    has been proven to be a repeatable result because initially freezing the encoder
    allows for the decoder to learn (using the encoder representations) the new task
    at hand. From a point of view, in this step, there is a knowledge transfer from
    the encoder to the decoder. In a secondary step, when the encoder is unfrozen,
    the learned parameters from the decoder perform auxiliary transfer learning –
    this time, from the decoder to the encoder.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的微调实验相比，我们可以看到这个实验的性能略有提升。然而，实际操作中，我们原本预计会得到相反的结果（即该实验性能会略有下降）。这已被证明是一个可重复的结果，因为最初冻结编码器可以使解码器学习（使用编码器的表示）当前的任务。从某种角度来看，在这一步，知识从编码器转移到了解码器。在随后的步骤中，当编码器解冻时，解码器学习到的参数执行辅助的迁移学习——这次是从解码器到编码器。
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose `"I learn new things every day"`, and the output
    obtained is as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们还可以通过一个句子示例来检查模型的表现。在我们的例子中，我们选择了 `"I learn new things every day"`，得到的输出结果如下：
- en: '[PRE50]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The German sentence means *I learn new things every time*; therefore, as can
    be seen from the results, the text has been almost correctly translated from English
    to German.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 这句德语的意思是 *I learn new things every time*；因此，从结果可以看出，文本几乎正确地从英语翻译成了德语。
- en: How it works…
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we applied the techniques of transfer learning and fine-tuning,
    introduced at the beginning of the chapter, to the task of machine translation,
    which was also presented previously, in the fourth recipe, *Translating text from
    Vietnamese to English*, in [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), *Understanding
    Text with Natural* *Language Processing*.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇教程中，我们将第一个章节开头介绍的迁移学习和微调技术应用于机器翻译任务，该任务也在前一篇教程 *从越南语翻译到英语* 中呈现过，详见 [*第6章*](B16591_06.xhtml#_idTextAnchor121)，*通过自然语言处理理解文本*。
- en: 'We explored two new datasets, *WMT2014* and *WMT2016*, which, among other language
    pairs, support translations between German and English. Moreover, MXNet GluonNLP
    provided the following:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了两个新的数据集，*WMT2014* 和 *WMT2016*，这些数据集除了支持其他语言对的翻译外，还支持德语和英语之间的翻译。此外，MXNet
    GluonNLP 提供了以下内容：
- en: A pre-trained Transformer model for *WMT2014*
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对 *WMT2014* 的预训练 Transformer 模型
- en: A data loader ready to be used with *WMT2016*
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个准备好与 *WMT2016* 一起使用的数据加载器
- en: Furthermore, we continued using the metrics introduced for machine translation,
    perplexity, and BLEU.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们继续使用了为机器翻译引入的评估指标：困惑度（perplexity）和 BLEU。
- en: 'Having all these tools readily available within MXNet and GluonNLP allowed
    us to run the following experiments with just a few lines of code:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 和 GluonNLP 提供的所有这些工具使我们能够通过几行代码轻松运行以下实验：
- en: Training a model from scratch with *WMT2016*
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练 *WMT2016* 模型
- en: Using a pre-trained model to optimize performance via transfer learning from
    *WMT2014* to *WMT2016*
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型通过迁移学习优化性能，从 *WMT2014* 到 *WMT2016*
- en: Fine-tuning our pre-trained model on *WMT2016* (with and without freezing layers)
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *WMT2016* 上对预训练模型进行微调（有层冻结和没有冻结层的情况）
- en: We compared the results and derived the best approach for this particular task,
    which was applying transfer learning and fine-tuning afterward.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了结果，并得出了最佳方法，即先应用迁移学习，之后进行微调。
- en: There’s more...
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'In this recipe, we introduced two new datasets, *WMT2014* and *WMT2016*. These
    datasets were introduced as challenges in the **Workshop on Statistical Machine
    Translation** (**WMT**) conference. The results for 2014 and 2016 are the following:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇教程中，我们介绍了两个新的数据集，*WMT2014* 和 *WMT2016*。这些数据集是在 **统计机器翻译研讨会**（**WMT**）会议上作为挑战引入的。2014年和2016年的结果如下：
- en: '**Findings of the 2014 Workshop on Statistical Machine** **Translation:** https://aclanthology.org/W14-3302.pdf'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2014年统计机器翻译研讨会的发现：** https://aclanthology.org/W14-3302.pdf'
- en: '**Findings of the 2016 Conference on Machine Translation (****WMT16):** [https://aclanthology.org/W16-2301.pdf](https://aclanthology.org/W16-2301.pdf)'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2016年机器翻译会议（WMT16）研究成果：** [https://aclanthology.org/W16-2301.pdf](https://aclanthology.org/W16-2301.pdf)'
- en: 'Transfer learning, including fine-tuning, for machine translation is an active
    area of research. A paper published in 2020 explores its applications, titled
    *In Neural Machine Translation, What Does Transfer Learning Transfer?* and can
    be found here: [https://aclanthology.org/2020.acl-main.688.pdf](https://aclanthology.org/2020.acl-main.688.pdf).'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译中的迁移学习，包括微调，是一个活跃的研究领域。一篇发表于2020年的论文探讨了其应用，题为*神经机器翻译中的迁移学习转移了什么？*，可以在这里找到：[https://aclanthology.org/2020.acl-main.688.pdf](https://aclanthology.org/2020.acl-main.688.pdf)。
- en: 'For a more general approach to NLP use cases, a recent paper was published,
    *A Survey on Transfer Learning in Natural Language Processing*, and can be found
    here: [https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing](https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 针对更一般的自然语言处理应用，最近有一篇论文发布，题为*自然语言处理中的迁移学习综述*，可以在这里找到：[https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing](https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing)。
