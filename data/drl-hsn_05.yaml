- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Tabular Learning and the Bellman Equation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格学习和贝尔曼方程。
- en: 'In the previous chapter, you became acquainted with your first reinforcement
    learning (RL) algorithm, the cross-entropy method, along with its strengths and
    weaknesses. In this new part of the book, we will look at another group of methods
    that has much more flexibility and power: Q-learning. This chapter will establish
    the required background shared by those methods.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你初步了解了第一个强化学习（RL）算法——交叉熵法，并了解了它的优缺点。在本书的这一部分，我们将介绍另一组方法，它们具有更多的灵活性和强大功能：Q-learning。本章将为这些方法奠定必要的背景知识。
- en: We will also revisit the FrozenLake environment and explore how new concepts
    fit with this environment and help us to address issues related to its uncertainty.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重新审视FrozenLake环境，探讨新概念如何与这个环境契合，并帮助我们解决与其不确定性相关的问题。
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Review the value of the state and the value of the action, and learn how to
    calculate them in simple cases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾状态的价值和行动的价值，并学习如何在简单的情况下计算它们。
- en: Talk about the Bellman equation and how it establishes the optimal policy if
    we know the values of states
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论贝尔曼方程，以及它如何在我们知道状态的价值时建立最优策略。
- en: Discuss the value iteration method and try it on the FrozenLake environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论价值迭代方法，并在FrozenLake环境中进行尝试。
- en: Do the same for the Q-iteration method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对Q-迭代方法进行相同的操作。
- en: Despite the simplicity of the environments in this chapter, it establishes the
    required preparation for deep Q-learning, which is a very powerful and generic
    RL method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章中的环境简单，但它为深度Q学习（一个非常强大且通用的强化学习方法）奠定了必要的准备。
- en: Value, state, and optimality
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 价值、状态和最优性。
- en: You may remember our definition of the value of the state from Chapter [1](ch005.xhtml#x1-190001).
    This is a very important notion and the time has come to explore it further.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们在第[1章](ch005.xhtml#x1-190001)中对状态价值的定义。这是一个非常重要的概念，现在是时候进一步探讨它了。
- en: This whole part of the book is built around the value of the state and how to
    approximate it. We defined this value as an expected total reward (optionally
    discounted) that is obtainable from the state. In a formal way, the value of the
    state is given by
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的这一部分是围绕状态的价值及如何逼近它展开的。我们将该价值定义为从状态中获得的期望总奖励（可选折扣）。从正式的角度来看，状态的价值由以下公式给出：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq8.png)'
- en: where r[t] is the local reward obtained at step t of the episode.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中r[t]是代理在情节的第t步获得的局部奖励。
- en: 'The total reward could be discounted with 0 < γ < 1 or not discounted (when
    γ = 1); it’s up to us how to define it. The value is always calculated in terms
    of some policy that our agent follows. To illustrate this, let’s consider a very
    simple environment with three states, as shown in Figure [5.1](#x1-83004r1):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总奖励可以折扣，范围为0 < γ < 1，或者不折扣（当γ = 1时）；这取决于我们如何定义它。价值始终是根据代理遵循的某个策略计算的。为了解释这一点，让我们考虑一个非常简单的环境，包含三个状态，如图[5.1](#x1-83004r1)所示：
- en: '![SsSeSerr=ta=n=n==1r2d3d12t ](img/B22150_05_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![SsSeSerr=ta=n=n==1r2d3d12t ](img/B22150_05_01.png)'
- en: 'Figure 5.1: An example of an environment’s state transition with rewards'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：一个环境状态转换及奖励的示例。
- en: The agent’s initial state.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理的初始状态。
- en: The final state that the agent is in after executing action “right” from the
    initial state. The reward obtained from this is 1.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理在执行“右”动作后从初始状态到达的最终状态。从中获得的奖励是1。
- en: The final state that the agent is in after action “down.” The reward obtained
    from this is 2.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理在执行“下”动作后的最终状态。从中获得的奖励是2。
- en: 'The environment is always deterministic — every action succeeds and we always
    start from state 1\. Once we reach either state 2 or state 3, the episode ends.
    Now, the question is, what’s the value of state 1? This question is meaningless
    without information about our agent’s behavior or, in other words, its policy.
    Even in a simple environment, our agent can have an infinite amount of behaviors,
    each of which will have its own value for state 1\. Consider these examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 环境始终是确定性的——每个行动都成功，我们总是从状态1开始。一旦我们到达状态2或状态3，情节结束。现在，问题是，状态1的价值是多少？如果没有关于我们代理行为的信息，或者换句话说，没有其策略，这个问题是没有意义的。即使在一个简单的环境中，我们的代理也可能有无限多的行为，每个行为都有其自身的状态1价值。考虑以下例子：
- en: Agent always goes right
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理始终向右移动。
- en: Agent always goes down
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理始终向下移动。
- en: Agent goes right with a probability of 50% and down with a probability of 50%
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理以50%的概率向右移动，50%的概率向下移动。
- en: Agent goes right in 10% of cases and in 90% of cases executes the “down” action
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '代理在10%的情况下向右，在90%的情况下执行“向下”动作  '
- en: 'To demonstrate how the value is calculated, let’s do it for all the preceding
    policies:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '为了演示如何计算值，让我们对之前的所有策略进行计算：  '
- en: The value of state 1 in the case of the “always right” agent is 1.0 (every time
    it goes left, it obtains 1 and the episode ends)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于“始终向右”代理，状态1的值为1.0（每次它向左走，获得1分，回合结束）  '
- en: For the “always down” agent, the value of state 1 is 2.0
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于“始终向下”代理，状态1的值为2.0  '
- en: For the 50% right/50% down agent, the value is 1.0⋅0.5+2.0⋅0.5 = 1.5
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于50%右/50%下代理，值为1.0⋅0.5 + 2.0⋅0.5 = 1.5  '
- en: For the 10% right/90% down agent, the value is 1.0⋅0.1+2.0⋅0.9 = 1.9
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于10%右/90%下代理，值为1.0⋅0.1 + 2.0⋅0.9 = 1.9  '
- en: 'Now, another question: what’s the optimal policy for this agent? The goal of
    RL is to get as much total reward as possible. For this one-step environment,
    the total reward is equal to the value of state 1, which, obviously, is at the
    maximum at policy 2 (always down).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，另一个问题是：这个代理的最优策略是什么？强化学习的目标是获得尽可能多的总奖励。对于这个一步的环境，总奖励等于状态1的值，显然，在策略2（始终向下）下，总奖励是最大的。  '
- en: Unfortunately, such simple environments with an obvious optimal policy are not
    that interesting in practice. For interesting environments, the optimal policies
    are much harder to formulate and it’s even harder to prove their optimality. However,
    don’t worry; we are moving toward the point when we will be able to make computers
    learn the optimal behavior on their own.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，具有明显最优策略的简单环境在实际中并不那么有趣。对于有趣的环境，最优策略往往更难制定，甚至更难证明它们的最优性。然而，不要担心；我们正在向着让计算机能够自主学习最优行为的方向前进。
- en: From the preceding example, you may have a false impression that we should always
    take the action with the highest reward. In general, it’s not that simple. To
    demonstrate this, let’s extend our preceding environment with yet another state
    that is reachable from state 3\. State 3 is no longer a terminal state but a transition
    to state 4, with a bad reward of -20\. Once we have chosen the “down” action in
    state 1, this bad reward is unavoidable, as from state 3, we have only one exit
    to state 4\. So, it’s a trap for the agent, which has decided that “being greedy”
    is a good strategy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '从前面的例子来看，你可能会产生一个误解，认为我们应该总是采取奖励最高的行动。通常来说，事情并没有那么简单。为了证明这一点，让我们在之前的环境中再增加一个状态，这个状态可以从状态3到达。状态3不再是终结状态，而是一个过渡状态到状态4，且有一个很差的奖励——-20。一旦我们在状态1选择了“向下”这个动作，这个坏奖励是不可避免的，因为从状态3开始，我们只有一个出口——到状态4。所以，对于代理来说，这是一个陷阱，它已经决定“贪婪”是一个好策略。  '
- en: '![SsSeSSerrr=ta=n==n===1r2d34d12−t20 ](img/B22150_05_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![SsSeSSerrr=ta=n==n===1r2d34d12−t20 ](img/B22150_05_02.png)  '
- en: 'Figure 5.2: The same environment, with an extra state added'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.2：同样的环境，增加了一个状态  '
- en: 'With that addition, our values for state 1 will be calculated this way:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '这样一来，我们对于状态1的值计算如下：  '
- en: 'The “always right” agent is the same: 1.0'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“始终向右”代理的值是：1.0  '
- en: The “always down” agent gets 2.0 + (−20) = −18
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“始终向下”代理的值为2.0 + (−20) = −18  '
- en: The 50%/50% agent gets 0.5 ⋅ 1.0 + 0.5 ⋅ (2.0 + (−20)) = −8.5
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '50%/50%代理的值为0.5 ⋅ 1.0 + 0.5 ⋅ (2.0 + (−20)) = −8.5  '
- en: The 10%/90% agent gets 0.1 ⋅ 1.0 + 0.9 ⋅ (2.0 + (−20)) = −16.1
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '10%/90%代理的值为0.1 ⋅ 1.0 + 0.9 ⋅ (2.0 + (−20)) = −16.1  '
- en: 'So, the best policy for this new environment is now policy 1: always go right.
    We spent some time discussing naïve and trivial environments so that you realize
    the complexity of this optimality problem and can appreciate the results of Richard
    Bellman better. Bellman was an American mathematician who formulated and proved
    his famous Bellman equation. We will talk about it in the next section.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '所以，这个新环境的最佳策略现在是策略1：始终向右。我们花了一些时间讨论天真和简单的环境，这样你就能意识到这个最优问题的复杂性，并更好地理解理查德·贝尔曼的结果。贝尔曼是美国数学家，他提出并证明了著名的贝尔曼方程。我们将在下一节讨论它。  '
- en: The Bellman equation of optimality
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '贝尔曼最优性方程  '
- en: To explain the Bellman equation, it’s better to go a bit abstract. Don’t be
    afraid; I’ll provide concrete examples later to support your learning! Let’s start
    with a deterministic case, when all our actions have a 100% guaranteed outcome.
    Imagine that our agent observes state s[0] and has N available actions. Every
    action leads to another state, s[1]…s[N], with a respective reward, r[1]…r[N].
    Also, assume that we know the values, V [i], of all states connected to state
    s[0]. What will be the best course of action that the agent can take in such a
    state?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释贝尔曼方程，最好抽象一些。不要害怕；我会后面提供具体的例子来支持你的学习！让我们从一个确定性情况开始，当我们的所有行动都有 100% 的保证结果时。想象我们的代理观察到状态
    s[0] 并有 N 个可用的行动。每个行动导致另一个状态 s[1]…s[N]，并带有相应的奖励 r[1]…r[N]。还假设我们知道与状态 s[0] 相连的所有状态的值
    V [i]。在这样的状态下，代理可以采取什么最佳行动？
- en: '![rrrr====rrrr123N sssssaaaa0123N====VVVV123N123N ](img/B22150_05_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![rrrr====rrrr123N sssssaaaa0123N====VVVV123N123N ](img/B22150_05_03.png)'
- en: 'Figure 5.3: An abstract environment with N states reachable from the initial
    state'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：从初始状态可达的 N 个状态的抽象环境
- en: 'If we choose the concrete action, a[i], and calculate the value given to this
    action, then the value will be V [0](a = a[i]) = r[i] + V [i]. So, to choose the
    best possible action, the agent needs to calculate the resulting values for every
    action and choose the maximum possible outcome. In other words, V [0] = max[a∈1…N](r[a]
    + V [a]). If we are using the discount factor, γ, we need to multiply the value
    of the next state by gamma: V [0] = max[a∈1…N](r[a] + γV [a]).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择具体的行动 a[i] 并计算给定该行动的值，那么该值将为 V [0](a = a[i]) = r[i] + V [i]。因此，为了选择可能的最佳行动，代理需要计算每个行动的结果值，并选择可能的最大结果。换句话说，V
    [0] = max[a∈1…N](r[a] + V [a])。如果我们使用折现因子 γ，我们需要将下一个状态的值乘以 gamma：V [0] = max[a∈1…N](r[a]
    + γV [a])。
- en: 'This may look very similar to our greedy example from the previous section,
    and, in fact, it is. However, there is one difference: when we act greedily, we
    do not only look at the immediate reward for the action, but at the immediate
    reward plus the long-term value of the state. This allows us to avoid a possible
    trap with a large immediate reward but a state that has a bad value.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来可能与前一节的贪婪示例非常相似，实际上确实如此。然而，有一个区别：当我们贪婪地行动时，我们不仅看即时行动的奖励，还看长期状态值的奖励。这使我们能够避免可能出现的陷阱，即即时奖励很大但状态值很差的情况。
- en: Bellman proved that with that extension, our behavior will get the best possible
    outcome. In other words, it will be optimal. So, the preceding equation is called
    the Bellman equation of value (for a deterministic case).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼证明了通过这种扩展，我们的行为将获得最佳可能的结果。换句话说，它将是最优的。因此，前述方程被称为值的贝尔曼方程（对于确定性情况）。
- en: 'It’s not very complicated to extend this idea for a stochastic case, when our
    actions have the chance of ending up in different states. What we need to do is
    calculate the expected value for every action, instead of just taking the value
    of the next state. To illustrate this, let’s consider one single action available
    from state s[0], with three possible outcomes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法推广到随机情况并不复杂，当我们的行为有可能导致不同状态时。我们需要做的是计算每个行动的期望值，而不仅仅是考虑下一个状态的值。为了说明这一点，让我们考虑从状态
    s[0] 可用的单个行动，有三种可能的结果：
- en: '![rrr===rrr 123 ssssappp0123=123VVV1123 ](img/B22150_05_04.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![rrr===rrr 123 ssssappp0123=123VVV1123 ](img/B22150_05_04.png)'
- en: 'Figure 5.4: An example of the transition from the state in a stochastic case'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：在随机情况下从状态转移的示例
- en: 'Here, we have one action, which can lead to three different states with different
    probabilities. With probability p[1], the action can end up in state s[1], with
    p[2] in state s[2], and with p[3] in state s[3] (p[1] + p[2] + p[3] = 1, of course).
    Every target state has its own reward (r[1], r[2], or r[3]). To calculate the
    expected value after issuing action 1, we need to sum all values, multiplied by
    their probabilities:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个行动，可以以三种不同的概率导致三个不同的状态。以概率 p[1]，该行动可能进入状态 s[1]，以 p[2] 进入状态 s[2]，以 p[3]
    进入状态 s[3]（当然，p[1] + p[2] + p[3] = 1）。每个目标状态都有自己的奖励（r[1]、r[2] 或 r[3]）。要计算发出行动 1
    后的期望值，我们需要将所有值乘以它们的概率并求和：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq9.png)'
- en: or, more formally
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，更正式地说，
- en: '![π (a |s) = P[At = a|St = s] ](img/eq10.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq10.png)'
- en: Here, 𝔼 [s∼S] means taking the expected value over all states in our state space,
    S.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，𝔼 [s∼S] 表示在我们的状态空间 S 中所有状态上取期望值。
- en: 'By combining the Bellman equation, for a deterministic case, with a value for
    stochastic actions, we get the Bellman optimality equation for a general case:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将贝尔曼方程（对于确定性情况）与随机动作的值相结合，我们得到了一般情况的贝尔曼最优性方程：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq11.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq11.png)'
- en: 'Note that p[a,i→j] means the probability of action a, issued in state i, ending
    up in state j. The interpretation is still the same: the optimal value of the
    state corresponds to the action, which gives us the maximum possible expected
    immediate reward, plus the discounted long-term reward for the next state. You
    may also notice that this definition is recursive: the value of the state is defined
    via the values of the immediately reachable states. This recursion may look like
    cheating: we define some value, pretending that we already know it. However, this
    is a very powerful and common technique in computer science and even in math in
    general (proof by induction is based on the same trick). This Bellman equation
    is a foundation not only in RL but also in much more general dynamic programming,
    which is a widely used method for solving practical optimization problems.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，p[a,i→j]表示在状态i下执行动作a后，转移到状态j的概率。解释仍然是一样的：状态的最优值对应于能够给我们最大可能的期望立即奖励，加上下一状态的折扣长期奖励的动作。你可能还会注意到，这一定义是递归的：状态的值是通过立即可达状态的值来定义的。这种递归看起来可能像是作弊：我们定义了一个值，假装我们已经知道它。然而，这在计算机科学甚至数学中都是一种非常强大且常见的技巧（数学归纳法就是基于这种技巧）。这个贝尔曼方程不仅是强化学习的基础，还是更为一般的动态规划的基础，动态规划是一种广泛用于解决实际优化问题的方法。
- en: 'These values not only give us the best reward that we can obtain, but they
    basically give us the optimal policy to obtain that reward: if our agent knows
    the value for every state, then it automatically knows how to gather this reward.
    Thanks to Bellman’s optimality proof, at every state the agent ends up in, it
    needs to select the action with the maximum expected reward, which is a sum of
    the immediate reward and the one-step discounted long-term reward – that’s it.
    So, those values are really useful to know. Before you get familiar with a practical
    way to calculate them, I need to introduce one more mathematical notation. It’s
    not as fundamental as the value of the state, but we need it for our convenience.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值不仅告诉我们可以获得的最佳奖励，而且基本上给出了获取该奖励的最优策略：如果我们的智能体知道每个状态的值，那么它就自动知道如何获取这个奖励。凭借贝尔曼最优性证明，在智能体到达的每个状态中，它需要选择具有最大期望奖励的动作，这个期望奖励是立即奖励与一步折扣后的长期奖励之和——仅此而已。因此，这些值对于了解是非常有用的。在你熟悉一种计算这些值的实际方法之前，我需要介绍一个数学符号。它不像状态值那样基础，但为了方便我们需要它。
- en: The value of the action
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作的值
- en: 'To make our life slightly easier, we can define different quantities, in addition
    to the value of the state, V (s), as the value of the action, Q(s,a). Basically,
    this equals the total reward we can get by executing action a in state s and can
    be defined via V (s). Being a much less fundamental entity than V (s), this quantity
    gave a name to the whole family of methods called Q-learning, because it is more
    convenient. In these methods, our primary objective is to get values of Q for
    every pair of state and action:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的生活稍微轻松一点，我们可以定义不同的量，除了状态值V(s)外，还可以定义动作值Q(s,a)。基本上，这等于我们在状态s下执行动作a所能获得的总奖励，可以通过V(s)来定义。作为一个比V(s)更不基础的量，这个量给整个Q学习方法族命名，因为它更方便。在这些方法中，我们的主要目标是获取每对状态和动作的Q值：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq12.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq12.png)'
- en: 'Q, for this state, s, and action, a, equals the expected immediate reward and
    the discounted long-term reward of the destination state. We also can define V
    (s) via Q(s,a):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个状态s和动作a，Q等于期望的立即奖励和目标状态的折扣长期奖励。我们还可以通过Q(s,a)来定义V(s)：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq13.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq13.png)'
- en: This just means that the value of some state equals to the value of the maximum
    action we can execute from this state.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是意味着某个状态的值等于我们从该状态执行的最大动作的值。
- en: 'Finally, we can express Q(s,a) recursively (which will be used in Chapter [6](#)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以递归地表达Q(s,a)（将在第[6](#)章中使用）：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq14.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq14.png)'
- en: 'In the last formula, the index on the immediate reward, (s,a), depends on the
    environment details:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的公式中，立即奖励的索引(s,a)依赖于环境的具体细节：
- en: If the immediate reward is given to us after executing a particular action,
    a, from state s, index (s,a) is used and the formula is exactly as shown above.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在执行特定动作a时，立即奖励是在状态s下给予我们的，那么使用索引(s,a)，公式与上面展示的完全一致。
- en: 'But if the reward is provided for reaching some state, s′, via action a′, the
    reward will have the index (s′,a′) and will need to be moved into the max operator:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是如果奖励是通过动作a′到达某个状态s′时给予的，奖励将具有索引(s′,a′)，并且需要被移入最大值运算符中：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq15.png)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq15.png)'
- en: That difference is not very significant from a mathematical point of view, but
    it could be important during the implementation of the methods. The first situation
    is more common, so we will stick to the preceding formula.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，这个差异并不大，但在方法实现过程中可能很重要。第一种情况更为常见，因此我们将坚持使用前述公式。
- en: 'To give you a concrete example, let’s consider an environment that is similar
    to FrozenLake, but has a much simpler structure: we have one initial state (s[0])
    surrounded by four target states, s[1], s[2], s[3], s[4], with different rewards:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个具体的例子，让我们考虑一个类似FrozenLake的环境，但结构更简单：我们有一个初始状态(s[0])，周围有四个目标状态s[1]、s[2]、s[3]、s[4]，每个状态的奖励不同：
- en: '![s0 – initial state ssssss012431,s2,s3,s4 – final states ](img/B22150_05_05.png)
    Figure 5.5: A simplified grid-like environment'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![s0 – 初始状态 ssssss012431,s2,s3,s4 – 终态 ](img/B22150_05_05.png) 图 5.5：简化的网格状环境'
- en: 'Every action is probabilistic in the same way as in FrozenLake: with a 33%
    chance that our action will be executed without modifications, but with a 33%
    chance that we will slip to the left, relatively, of our target cell and a 33%
    chance that we will slip to the right. For simplicity, we use discount factor
    γ = 1.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个动作的执行都是有概率的，和FrozenLake中的方式一样：有33%的概率我们的动作将按原样执行，但也有33%的概率我们会相对于目标单元格向左滑动，另外33%的概率我们会向右滑动。为了简化起见，我们使用折扣因子γ
    = 1。
- en: '![sssssuledr000000000000rrrr01234pfoig.3.3.3.3.3.3.3.3.3.3.3.3====twh3333333333331234nt
    ](img/B22150_05_06.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![sssssuledr000000000000rrrr01234pfoig.3.3.3.3.3.3.3.3.3.3.3.3====twh3333333333331234nt
    ](img/B22150_05_06.png)'
- en: 'Figure 5.6: A transition diagram of the grid environment'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：网格环境的转移图
- en: 'Let’s calculate the values of the actions to begin with. Terminal states s[1]…s[4]
    have no outbound connections, so Q for those states is zero for all actions. Due
    to this, the values of the terminal states are equal to their immediate reward
    (once we get there, our episode ends without any subsequent states): V [1] = 1,
    V [2] = 2, V [3] = 3, V [4] = 4.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先计算这些动作的值。终态s[1]…s[4]没有外部连接，因此这些状态的Q值对所有动作均为零。由于这个原因，终态的值等于它们的即时奖励（我们一旦到达终态，回合就结束，没有后续状态）：V
    [1] = 1，V [2] = 2，V [3] = 3，V [4] = 4。
- en: 'The values of the actions for state 0 are a bit more complicated. Let’s start
    with the “up” action. Its value, according to the definition, is equal to the
    expected sum of the immediate reward plus the long-term value for subsequent steps.
    We have no subsequent steps for any possible transition for the “up” action:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 状态0的动作值稍微复杂一些。我们从“向上”动作开始。根据定义，它的值等于立即奖励的期望值加上后续步骤的长期值。对于“向上”动作的任何可能转移，我们没有后续步骤：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq16.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq16.png)'
- en: 'Repeating this for the rest of the s[0] actions results in the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对s[0]的其余动作进行相同的计算，结果如下：
- en: '| Q(s[0],left) | = 0.33 ⋅V [1] + 0.33 ⋅V [2] + 0.33 ⋅V [3] = 1.98 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Q(s[0],left) | = 0.33 ⋅V [1] + 0.33 ⋅V [2] + 0.33 ⋅V [3] = 1.98 |'
- en: '| Q(s[0],right) | = 0.33 ⋅V [4] + 0.33 ⋅V [1] + 0.33 ⋅V [3] = 2.64 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Q(s[0],right) | = 0.33 ⋅V [4] + 0.33 ⋅V [1] + 0.33 ⋅V [3] = 2.64 |'
- en: '| Q(s[0],down) | = 0.33 ⋅V [3] + 0.33 ⋅V [2] + 0.33 ⋅V [4] = 2.97 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Q(s[0],down) | = 0.33 ⋅V [3] + 0.33 ⋅V [2] + 0.33 ⋅V [4] = 2.97 |'
- en: The final value for state s[0] is the maximum of those actions’ values, which
    is 2.97.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 状态s[0]的最终值是这些动作值中的最大值，即2.97。
- en: 'Q-values are much more convenient in practice, as for the agent, it’s much
    simpler to make decisions about actions based on Q than on V . In the case of
    Q, to choose the action based on the state, the agent just needs to calculate
    Q for all available actions using the current state and choose the action with
    the largest value of Q. To do the same using values of the states, the agent needs
    to know not only the values, but also the probabilities for transitions. In practice,
    we rarely know them in advance, so the agent needs to estimate transition probabilities
    for every action and state pair. Later in this chapter, you will see this in practice
    by solving the FrozenLake environment both ways. However, to be able to do this,
    we have one important thing still missing: a general way to calculate V [i] and
    Q[i].'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Q值在实践中要方便得多，因为对于智能体来说，根据 Q 而不是 V 来做决策要简单得多。在 Q 的情况下，智能体只需要使用当前状态计算所有可用动作的 Q
    值，并选择具有最大 Q 值的动作。使用状态值来做相同的选择时，智能体不仅需要知道这些值，还需要知道转移的概率。在实践中，我们很少事先知道这些概率，因此智能体需要为每个动作和状态对估计转移概率。在本章稍后的部分，你将通过两种方法解决
    FrozenLake 环境，亲自看到这一点。然而，要做到这一点，我们还有一个重要的东西缺失：计算 V[i] 和 Q[i] 的通用方法。
- en: The value iteration method
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值迭代方法
- en: 'In the simplistic example you just saw, to calculate the values of the states
    and actions, we exploited the structure of the environment: we had no loops in
    transitions, so we could start from terminal states, calculate their values, and
    then proceed to the central state. However, just one loop in the environment builds
    an obstacle in our approach. Let’s consider such an environment with two states:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在你刚才看到的简单例子中，为了计算状态和动作的值，我们利用了环境的结构：转移中没有循环，因此我们可以从终端状态开始，计算它们的值，然后再处理中央状态。然而，环境中的一个循环就构成了我们方法的障碍。让我们考虑一个有两个状态的环境：
- en: '![ssrrγ12==12= 0.9 ](img/B22150_05_07.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![ssrrγ12==12= 0.9 ](img/B22150_05_07.png)'
- en: 'Figure 5.7: A sample environment with a loop in the transition diagram'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：具有转移图中循环的示例环境
- en: 'We start from state s[1], and the only action we can take leads us to state
    s[2]. We get the reward, r = 1, and the only transition from s[2] is an action,
    which brings us back to s[1]. So, the life of our agent is an infinite sequence
    of states [s[1],s[2],s[1],s[2],…]. To deal with this infinity loop, we can use
    a discount factor: γ = 0.9\. Now, the question is, what are the values for both
    the states? The answer is not very complicated, in fact. Every transition from
    s[1] to s[2] gives us a reward of 1 and every back transition gives us 2\. So,
    our sequence of rewards will be [1,2,1,2,1,2,1,2,…]. As there is only one action
    available in every state, our agent has no choice, so we can omit the max operation
    in formulas (there is only one alternative).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从状态 s[1] 开始，唯一可以采取的动作将我们带到状态 s[2]。我们获得奖励 r = 1，s[2] 中唯一的转移是一个动作，它将我们带回到 s[1]。所以，我们的智能体的生命周期是一个无限的状态序列[s[1],
    s[2], s[1], s[2], …]。为了处理这个无限循环，我们可以使用折扣因子：γ = 0.9。现在，问题是，两个状态的值是多少？其实，答案并不复杂。每次从
    s[1] 转移到 s[2] 都会给我们奖励1，而每次返回转移都会给我们奖励2。因此，我们的奖励序列将是[1, 2, 1, 2, 1, 2, 1, 2, …]。由于每个状态只有一个可用的动作，我们的智能体没有选择的余地，因此我们可以在公式中省略最大值操作（因为只有一个选择）。
- en: 'The value for every state will be equal to the infinite sum:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态的值将等于无限求和：
- en: '| V (s[1]) | = 1 + γ(2 + γ(1 + γ(2 + …))) = ∑ [i=0]^∞1γ^(2i) + 2γ^(2i+1) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| V (s[1]) | = 1 + γ(2 + γ(1 + γ(2 + …))) = ∑ [i=0]^∞1γ^(2i) + 2γ^(2i+1) |'
- en: '| V (s[2]) | = 2 + γ(1 + γ(2 + γ(1 + …))) = ∑ [i=0]^∞2γ^(2i) + 1γ^(2i+1) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| V (s[2]) | = 2 + γ(1 + γ(2 + γ(1 + …))) = ∑ [i=0]^∞2γ^(2i) + 1γ^(2i+1) |'
- en: 'Strictly speaking, we can’t calculate the exact values for our states, but
    with γ = 0.9, the contribution of every transition quickly decreases over time.
    For example, after 10 steps, γ^(10) = 0.9^(10) ≈ 0.349, but after 100 steps, it
    becomes just 0.0000266\. Due to this, we can stop after 50 iterations and still
    get quite a precise estimation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，我们无法计算出我们状态的准确值，但当 γ = 0.9 时，每个转移的贡献会随着时间迅速减少。例如，经过10步后，γ^(10) = 0.9^(10)
    ≈ 0.349，但经过100步后，它就变成了0.0000266。因此，我们可以在50次迭代后停止，仍然可以得到相当精确的估算值：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding example can be used to get the gist of a more general procedure
    called the value iteration algorithm. This allows us to numerically calculate
    the values of the states and values of the actions of Markov decision processes
    (MDPs) with known transition probabilities and rewards. The procedure (for values
    of the states) includes the following steps:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例可以用来概述一个更一般的过程，称为值迭代算法。该算法使我们能够数值地计算具有已知转移概率和奖励的马尔科夫决策过程（MDP）的状态值和动作值。该过程（针对状态值）包括以下步骤：
- en: Initialize the values of all states, V [i], to some initial value (usually zero)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有状态的值V [i]初始化为某个初始值（通常为零）
- en: 'For every state, s, in the MDP, perform the Bellman update:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于MDP中的每个状态s，执行Bellman更新：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq17.png)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq17.png)'
- en: Repeat step 2 for some large number of steps or until changes become too small
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2，进行大量的迭代，或者直到变化变得非常小
- en: Okay, so that’s the theory. In practice, this method has certain obvious limitations.
    First of all, our state space should be discrete and small enough to perform multiple
    iterations over all states. This is not an issue for FrozenLake-4x4 and even for
    FrozenLake-8x8 (it exists in Gym as a more challenging version), but for CartPole,
    it’s not totally clear what to do. Our observation for CartPole is four float
    values, which represent some physical characteristics of the system. Potentially,
    even a small difference in those values could have an influence on the state’s
    value. One of the solutions for that could be discretization of our observation’s
    values; for example, we can split the observation space of CartPole into bins
    and treat every bin as an individual discrete state in space. However, this will
    create lots of practical problems, such as how large bin intervals should be and
    how much data from the environment we will need to estimate our values. I will
    address this issue in subsequent chapters, when we get to the usage of neural
    networks in Q-learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那就是理论。在实际应用中，这种方法有一些明显的局限性。首先，我们的状态空间应该是离散的，并且足够小，以便能够对所有状态进行多次迭代。这对于FrozenLake-4x4，甚至FrozenLake-8x8（作为更具挑战性的版本存在于Gym中）来说不是问题，但对于CartPole来说，应该如何做并不完全清楚。我们的CartPole的观察值是四个浮动值，表示系统的一些物理特性。潜在地，即使这些值之间有很小的差异，也可能影响状态的值。解决这个问题的一种方法是对我们的观察值进行离散化；例如，我们可以将CartPole的观察空间分成多个区间，并将每个区间当作空间中的一个独立离散状态。然而，这会带来很多实际问题，例如区间的大小应该如何确定，估算值时需要多少来自环境的数据。我将在后续章节中解决这个问题，当我们涉及到神经网络在Q-learning中的应用时。
- en: 'The second practical problem arises from the fact that we rarely know the transition
    probability for the actions and rewards matrix. Remember the interface provided
    by Gym to the agent’s writer: we observe the state, decide on an action, and only
    then do we get the next observation and reward for the transition. We don’t know
    (without peeking into Gym’s environment code) what the probability is of getting
    into state s[1] from state s[0] by issuing action a[0]. What we do have is just
    the history from the agent’s interaction with the environment. However, in Bellman’s
    update, we need both a reward for every transition and the probability of this
    transition. So, the obvious answer to this issue is to use our agent’s experience
    as an estimation for both unknowns. Rewards could be used as they are. We just
    need to remember what reward we got on the transition from s[0] to s[1] using
    action a, but to estimate probabilities, we need to maintain counters for every
    tuple (s[0],s[1],a) and normalize them.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个实际问题源于我们很少知道动作和奖励矩阵的转移概率。记住Gym提供给代理人编写者的接口：我们观察状态，决定一个动作，然后才获得下一次观察和转移奖励。我们不知道（除非查看Gym的环境代码）通过执行动作a[0]从状态s[0]进入状态s[1]的概率是多少。我们拥有的只是代理与环境交互的历史。然而，在Bellman更新中，我们需要每个转移的奖励和该转移的概率。因此，解决这个问题的明显方法是将代理的经验作为这两个未知数的估计。奖励可以按原样使用。我们只需要记住在使用动作a从s[0]到s[1]的转移中获得的奖励，但要估算概率，我们需要为每个元组（s[0]，s[1]，a）保持计数器并进行归一化。
- en: Now that you’re familiar with the theoretical background, let’s look at this
    method in practice.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了理论背景，让我们来看一下这个方法的实际应用。
- en: Value iteration in practice
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的值迭代
- en: 'In this section, we will look at how the value iteration method will work for
    FrozenLake. The complete example is in Chapter05/01_frozenlake_v_iteration.py.
    The central data structures in this example are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将研究值迭代方法如何在FrozenLake中工作。完整的示例位于Chapter05/01_frozenlake_v_iteration.py中。该示例中的核心数据结构如下：
- en: 'Reward table: A dictionary with the composite key “source state” + “action”
    + “target state.” The value is obtained from the immediate reward.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励表：一个字典，键是组合的“源状态”+“动作”+“目标状态”。值是从即时奖励获得的。
- en: 'Transitions table: A dictionary keeping counters of the experienced transitions.
    The key is the composite “state” + “action,” and the value is another dictionary
    that maps the “target state” into a count of times that we have seen it.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移表：一个字典，记录了经历的转移次数。键是组合的“状态”+“动作”，值是另一个字典，将“目标状态”映射到我们看到它的次数。
- en: 'For example, if in state 0 we execute action 1 ten times, after three times,
    it will lead us to state 4 and after seven times to state 5\. Then entry with
    the key (0, 1) in this table will be a dict with the contents {4: 3, 5: 7}. We
    can use this table to estimate the probabilities of our transitions.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '例如，如果在状态0下执行动作1十次，三次后会将我们带到状态4，七次后将带到状态5。那么，表中键为(0, 1)的条目将是一个字典，内容为{4: 3, 5:
    7}。我们可以利用这个表来估计我们的转移概率。'
- en: 'Value table: A dictionary that maps a state into the calculated value of this
    state.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值表：一个字典，将一个状态映射到该状态的计算值。
- en: 'The overall logic of our code is simple: in the loop, we play 100 random steps
    from the environment, populating the reward and transition tables. After those
    100 steps, we perform a value iteration loop over all states, updating our value
    table. Then we play several full episodes to check our improvements using the
    updated value table. If the average reward for those test episodes is above the
    0.8 boundary, then we stop training. During the test episodes, we also update
    our reward and transition tables to use all data from the environment.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代码的整体逻辑很简单：在循环中，我们从环境中执行100步随机操作，填充奖励和转移表格。完成这100步后，我们对所有状态执行值迭代循环，更新值表。然后我们进行几个完整回合的测试，检查使用更新后的值表后我们有哪些改进。如果这些测试回合的平均奖励超过0.8的边界值，我们就停止训练。在测试回合中，我们还会更新奖励和转移表格，以使用来自环境的所有数据。
- en: 'Now let’s come to the code. We first import the used packages and define constants.
    Then we define several type aliases. They are not necessary, but make our code
    more readable:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看代码。我们首先导入所需的包并定义常量。然后我们定义几个类型别名。它们不是必需的，但使我们的代码更具可读性：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For the FrozenLake environment, both observation and action spaces are of the
    Box class, so states and actions are represented by int values. We also define
    types for our reward and transition tables’ keys. For the reward table, it is
    a tuple with [State, Action, State] and for the transition table it is [State,
    Action]:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于FrozenLake环境，观察和动作空间都属于Box类，因此状态和动作由整数值表示。我们还为奖励表和转移表的键定义了类型。对于奖励表，键是一个元组，格式为[状态，动作，状态]，而对于转移表，键是[状态，动作]：
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we define the Agent class, which will keep our tables and contain functions
    that we will be using in the training loop. In the class constructor, we create
    the environment that we will be using for data samples, obtain our first observation,
    and define tables for rewards, transitions, and values:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了Agent类，它将保存我们的表格并包含我们将在训练循环中使用的函数。在类的构造函数中，我们创建了一个用于数据采样的环境，获得了第一个观察值，并为奖励、转移和价值定义了表格：
- en: '[PRE3]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function play_n_random_steps is used to gather random experience from the
    environment and update the reward and transition tables. Note that we don’t need
    to wait for the end of the episode to start learning; we just perform N steps
    and remember their outcomes. This is one of the differences between value iteration
    and the cross-entropy method, which can learn only on full episodes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 函数play_n_random_steps用于从环境中收集随机经验并更新奖励和转移表。需要注意的是，我们不需要等到回合结束才能开始学习；我们只执行N步并记录其结果。这是值迭代和交叉熵方法之间的一个区别，后者只能在完整回合中进行学习：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next function (calc_action_value()) calculates the value of the action
    from the state using our transition, reward, and values tables. We will use it
    for two purposes: to select the best action to perform from the state and to calculate
    the new value of the state on value iteration.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数（`calc_action_value()`）使用我们的转移、奖励和值表来计算从状态出发的动作值。我们将它用于两个目的：从状态中选择最佳动作，并计算值迭代中的状态的新值。
- en: 'We do the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做以下操作：
- en: We extract transition counters for the given state and action from the transition
    table. Counters in this table have a form of dict, with target states as the key
    and a count of experienced transitions as the value. We sum all counters to obtain
    the total count of times we have executed the action from the state. We will use
    this total value later to go from an individual counter to probability.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从转移表中提取给定状态和动作的转移计数器。此表中的计数器采用字典形式，目标状态作为键，经历的转移次数作为值。我们将所有计数器相加，以获得从该状态执行该动作的总次数。稍后我们将使用此总值从单个计数器转换为概率。
- en: Then we iterate every target state that our action has landed on and calculate
    its contribution to the total action value using the Bellman equation. This contribution
    is equal to immediate reward plus discounted value for the target state. We multiply
    this sum to the probability of this transition and add the result to the final
    action value.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们遍历每个目标状态，该状态是我们的动作所到达的，并使用贝尔曼方程计算它对总动作值的贡献。这个贡献等于即时奖励加上目标状态的折扣值。我们将此总和乘以此转移的概率，并将结果加到最终的动作值中。
- en: 'This logic is illustrated in the following diagram:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该逻辑在以下图中进行了说明：
- en: '![transit[(s,a)] = {s1:c1,s2:c2} total = c1 + c2 sssaccQ1212(s,a) = tco1tal(rs1
    + γVs1)+ tco2tal(rs2 + γVs2) ](img/B22150_05_08.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![transit[(s,a)] = {s1:c1,s2:c2} total = c1 + c2 sssaccQ1212(s,a) = tco1tal(rs1
    + γVs1)+ tco2tal(rs2 + γVs2) ](img/B22150_05_08.png)'
- en: 'Figure 5.8: The calculation of the state’s value'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：状态值的计算
- en: 'In the preceding diagram, we do a calculation of the value for state s and
    action a. Imagine that, during our experience, we have executed this action several
    times (c[1] + c[2]) and it ends up in one of two states, s[1] or s[2]. How many
    times we have switched to each of these states is stored in our transition table
    as dict {s[1]: c[1], s[2]: c[2]}.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的图中，我们对状态s和动作a的值进行了计算。假设在我们的经验中，我们已经执行了该动作若干次（c[1] + c[2]），并最终进入了两个状态之一，s[1]或s[2]。我们切换到这些状态的次数存储在我们的转移表中，形式为字典{s[1]:
    c[1], s[2]: c[2]}。'
- en: 'Then, the approximate value for the state and action, Q(s,a), will be equal
    to the probability of every state, multiplied by the value of the state. From
    the Bellman equation, this equals the sum of the immediate reward and the discounted
    long-term state value:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，状态和动作的近似值Q(s,a)将等于每个状态的概率，乘以该状态的值。从贝尔曼方程来看，这等于即时奖励与折扣的长期状态值之和：
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next function uses the function that I just described to make a decision
    about the best action to take from the given state. It iterates over all possible
    actions in the environment and calculates the value for every action. The action
    with the largest value wins and is returned as the action to take. This action
    selection process is deterministic, as the play_n_random_steps() function introduces
    enough exploration. So, our agent will behave greedily in regard to our value
    approximation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数使用我刚才描述的函数来决定从给定状态采取最佳行动。它遍历环境中的所有可能动作，并计算每个动作的值。值最大的动作胜出，并作为执行的动作返回。这个动作选择过程是确定性的，因为`play_n_random_steps()`函数引入了足够的探索。因此，我们的智能体将在我们的值近似上表现得贪婪：
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The play_episode() function uses select_action() to find the best action to
    take and plays one full episode using the provided environment. This function
    is used to play test episodes, during which we don’t want to mess with the current
    state of the main environment used to gather random data. So, we use the second
    environment passed as an argument. The logic is very simple and should already
    be familiar to you: we just loop over states accumulating the reward for one episode:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`play_episode()`函数使用`select_action()`来找出最佳的行动，并使用提供的环境播放一个完整的回合。此函数用于播放测试回合，在此期间，我们不希望干扰用于收集随机数据的主要环境的当前状态。因此，我们使用作为参数传递的第二个环境。逻辑非常简单，应该已经很熟悉：我们只需遍历状态并累计一个回合的奖励：'
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The final method of the Agent class is our value iteration implementation and
    it is surprisingly simple, thanks to the functions we already defined. What we
    do is just loop over all states in the environment, then for every state, we calculate
    the values for the states reachable from it, obtaining candidates for the value
    of the state. Then we update the value of our current state with the maximum value
    of the action available from the state:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Agent 类的最终方法是我们的价值迭代实现，感谢我们已经定义的函数，这一方法出奇的简单。我们所做的只是循环遍历环境中的所有状态，然后对于每个状态，我们计算从该状态可达的状态的值，获得状态的价值候选值。然后，我们用可从该状态采取的动作的最大值来更新当前状态的值：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That’s all of our agent’s methods, and the final piece is a training loop and
    the monitoring of the code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们代理的所有方法，最后一部分是训练循环和代码的监控：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We create the environment that we will be using for testing, the Agent class
    instance, and the summary writer for TensorBoard:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了用于测试的环境，Agent 类实例，以及 TensorBoard 的摘要写入器：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The last two lines in the preceding code snippet are the key piece in the training
    loop. We first perform 100 random steps to fill our reward and transition tables
    with fresh data, and then we run value iteration over all states.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段中的最后两行是训练循环的关键部分。我们首先执行 100 次随机步骤，以填充我们的奖励和转移表，并获取新数据，然后对所有状态执行价值迭代。
- en: 'The rest of the code plays test episodes using the value table as our policy,
    then writes data into TensorBoard, tracks the best average reward, and checks
    for the training loop stop condition:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码通过使用价值表作为我们的策略来执行测试回合，然后将数据写入 TensorBoard，跟踪最佳平均奖励，并检查训练循环停止条件：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Okay, let’s run our program:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们运行我们的程序：
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our solution is stochastic, and my experiments usually required 10 to 100 iterations
    to reach a solution, but in all cases, it took less than a second to find a good
    policy that could solve the environment in 80% of runs. If you remember, about
    an hour was needed to achieve a 60% success ratio using the cross-entropy method,
    so this is a major improvement. There are two reasons for that.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案是随机的，我的实验通常需要 10 到 100 次迭代才能找到解决方案，但在所有情况下，都在不到一秒的时间内找到一个可以在 80% 的运行中解决环境的良好策略。如果你还记得，使用交叉熵方法需要大约一个小时才能达到
    60% 的成功率，所以这是一个重大的改进。原因有两个。
- en: First, the stochastic outcome of our actions, plus the length of the episodes
    (6 to 10 steps on average), makes it hard for the cross-entropy method to understand
    what was done right in the episode and which step was a mistake. Value iteration
    works with individual values of the state (or action) and incorporates the probabilistic
    outcome of actions naturally by estimating probability and calculating the expected
    value. So, it’s much simpler for value iteration and requires much less data from
    the environment (which is called sample efficiency in RL).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们动作的随机结果，加上回合的长度（平均 6 到 10 步），使得交叉熵方法很难理解回合中做对了什么，哪个步骤是错误的。价值迭代通过利用每个状态（或动作）的个体值来处理，并通过估计概率和计算期望值自然地结合了动作的概率结果。因此，价值迭代更为简单，且对环境的需求数据量更少（这在强化学习中称为样本效率）。
- en: The second reason is the fact that value iteration doesn’t need full episodes
    to start learning. In an extreme case, we can start updating our values just from
    a single example. However, for FrozenLake, due to the reward structure (we get
    1 only after successfully reaching the target state), we still need to have at
    least one successful episode to start learning from a useful value table, which
    may be challenging to achieve in more complex environments. For example, you can
    try switching the existing code to a larger version of FrozenLake, which has the
    name FrozenLake8x8-v1\. The larger version of FrozenLake can take from 150 to
    1,000 iterations to solve, and, according to TensorBoard charts, most of the time
    it waits for the first successful episode, then it very quickly reaches convergence.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是，价值迭代不需要完整的回合才能开始学习。在极端情况下，我们可以仅从一个例子开始更新我们的值。然而，对于 FrozenLake，由于奖励结构（我们只有成功到达目标状态后才能获得奖励
    1），我们仍然需要至少一个成功的回合来开始从有用的价值表中学习，这在更复杂的环境中可能会很难实现。例如，你可以尝试将现有代码切换到一个更大的 FrozenLake
    版本，名为 FrozenLake8x8-v1。FrozenLake 的大版本可能需要从 150 次到 1,000 次迭代才能解决，并且根据 TensorBoard
    图表，大多数时候它会等待第一次成功的回合，然后非常快速地达到收敛。
- en: 'The following are two charts: the first one shows reward dynamics during training
    on FrozenLake-4x4 and the second is for the 8 × 8 version.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两个图表：第一个显示了在 FrozenLake-4x4 上训练过程中的奖励动态，第二个是 8 × 8 版本的奖励动态。
- en: '![PIC](img/B22150_05_09.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_05_09.png)'
- en: 'Figure 5.9: The reward dynamics for FrozenLake-4x4'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：FrozenLake-4x4 的奖励动态
- en: '![PIC](img/B22150_05_10.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_05_10.png)'
- en: 'Figure 5.10: The reward dynamics on FrozenLake-8x8'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：FrozenLake-8x8 的奖励动态
- en: Now it’s time to compare the code that learns the values of the states, as we
    just discussed, with the code that learns the values of the actions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将学习状态价值的代码与学习动作价值的代码进行比较了，就像我们刚刚讨论的那样。
- en: Q-iteration for FrozenLake
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FrozenLake 的 Q 迭代
- en: 'The whole example is in the Chapter05/02_frozenlake_q_iteration.py file, and
    the differences are really minor:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 整个例子在 Chapter05/02_frozenlake_q_iteration.py 文件中，差异实际上非常小：
- en: The most obvious change is to our value table. In the previous example, we kept
    the value of the state, so the key in the dictionary was just a state. Now we
    need to store values of the Q-function, which has two parameters, state and action,
    so the key in the value table is now a composite of (State, Action) values.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最显著的变化是我们的价值表。在前一个例子中，我们保存了状态的价值，因此字典中的键只是一个状态。现在我们需要存储 Q 函数的值，它有两个参数，状态和动作，因此价值表中的键现在是
    (状态, 动作) 的组合值。
- en: The second difference is in our calc_action_value() function. We just don’t
    need it anymore, as our action values are stored in the value table.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个差异出现在我们的 `calc_action_value()` 函数中。我们不再需要它，因为我们的动作值现在存储在价值表中。
- en: Finally, the most important change in the code is in the agent’s value_iteration()
    method. Before, it was just a wrapper around the calc_action_value() call, which
    did the job of Bellman approximation. Now, as this function has gone and been
    replaced by a value table, we need to do this approximation in the value_iteration()
    method.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，代码中最重要的变化出现在代理的 `value_iteration()` 方法中。之前，它只是 `calc_action_value()` 调用的一个包装器，负责贝尔曼近似的工作。现在，由于这个函数已被移除并由价值表替代，我们需要在
    `value_iteration()` 方法中执行这个近似。
- en: 'Let’s look at the code. As it’s almost the same, I will jump directly to the
    most interesting value_iteration() function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码。由于几乎完全相同，我将直接跳到最有趣的 `value_iteration()` 函数：
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code is very similar to calc_action_value() in the previous example and,
    in fact, it does almost the same thing. For the given state and action, it needs
    to calculate the value of this action using statistics about target states that
    we have reached with the action. To calculate this value, we use the Bellman equation
    and our counters, which allow us to approximate the probability of the target
    state. However, in Bellman’s equation, we have the value of the state; now, we
    need to calculate it differently.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与前一个例子中的 `calc_action_value()` 非常相似，实际上它做的几乎是相同的事情。对于给定的状态和动作，它需要使用我们通过该动作到达的目标状态的统计数据来计算这个动作的价值。为了计算这个值，我们使用贝尔曼方程和我们的计数器，这些计数器允许我们近似目标状态的概率。然而，在贝尔曼方程中，我们有状态的值；现在，我们需要以不同的方式来计算它。
- en: Before, we had it stored in the value table (as we approximated the value of
    the states), so we just took it from this table. We can’t do this anymore, so
    we have to call the select_action method, which will choose for us the action
    with the largest Q-value, and then we take this Q-value as the value of the target
    state. Of course, we can implement another function that can calculate this value
    of the state, but select_action does almost everything we need, so we will reuse
    it here.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们将其存储在价值表中（因为我们近似了状态的价值），所以我们只需要从这个表中获取它。现在我们不能再这样做了，因此我们必须调用 `select_action`
    方法，它会为我们选择具有最大 Q 值的动作，然后我们将这个 Q 值作为目标状态的值。当然，我们可以实现另一个函数来计算这个状态的值，但 `select_action`
    几乎完成了我们需要的所有工作，所以我们在这里会复用它。
- en: 'There is another piece of this example that I’d like to emphasize here. Let’s
    look at our select_action method:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个我想强调的例子。让我们来看一下我们的 `select_action` 方法：
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As I said, we don’t have the calc_action_value method anymore; so, to select
    an action, we just iterate over the actions and look up their values in our values
    table. It could look like a minor improvement, but if you think about the data
    that we used in calc_action_value, it may become obvious why the learning of the
    Q-function is much more popular in RL than the learning of the V-function.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，我们不再有`calc_action_value`方法；因此，为了选择一个动作，我们只需要遍历所有动作并在值表中查找它们的值。看起来这可能是一个小小的改进，但如果你考虑我们在`calc_action_value`中使用的数据，你就能明白为什么Q函数的学习在强化学习中比V函数的学习更受欢迎。
- en: Our calc_action_value function uses both information about the reward and probabilities.
    It’s not a huge problem for the value iteration method, which relies on this information
    during training. However, in the next chapter, you will learn about the value
    iteration method extension, which doesn’t require probability approximation, but
    just takes it from the environment samples. For such methods, this dependency
    on probability adds an extra burden for the agent. In the case of Q-learning,
    what the agent needs to make the decision is just Q-values.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`calc_action_value`函数同时使用了关于奖励和概率的信息。对于价值迭代方法来说，这并不是一个大问题，因为该方法在训练过程中依赖这些信息。然而，在下一章中，你将了解一种价值迭代方法的扩展，它不需要概率的近似，而是直接从环境样本中获取。对于这种方法，依赖概率为智能体增加了额外的负担。在Q学习中，智能体做决策时所需要的只是Q值。
- en: I don’t want to say that V-functions are completely useless, because they are
    an essential part of the actor-critic method, which we will talk about in Part
    3 of this book. However, in the area of value learning, Q-functions are the definite
    favorite. With regard to convergence speed, both our versions are almost identical
    (but the Q-learning version requires four times more memory for the value table).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想说V函数完全没用，因为它们是演员-评论员方法的一个重要部分，而我们将在本书第3部分讨论这一方法。然而，在价值学习领域，Q函数无疑是更受欢迎的。关于收敛速度，我们的两个版本几乎是相同的（但Q学习版本需要的价值表内存是价值迭代版本的四倍）。
- en: 'The following is the output of the Q-learning version and it has no major differences
    from the value iteration version:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Q学习版本的输出，它与价值迭代版本没有重大区别：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'My congratulations; you have made another step toward understanding modern,
    state-of-the-art RL methods! In this chapter, you learned about some very important
    concepts that are widely used in deep RL: the value of the state, the value of
    the action, and the Bellman equation in various forms.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你在理解现代最先进的强化学习（RL）方法上又迈出了重要一步！在这一章中，你了解了一些深度强化学习中广泛应用的非常重要的概念：状态值、动作值以及贝尔曼方程的不同形式。
- en: We also covered the value iteration method, which is a very important building
    block in the area of Q-learning. Finally, you got to know how value iteration
    can improve our FrozenLake solution.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了价值迭代方法，这是Q学习领域一个非常重要的构建块。最后，你了解了价值迭代如何改进我们在FrozenLake中的解决方案。
- en: In the next chapter, you will learn about deep Q-networks, which started the
    deep RL revolution in 2013 by beating humans on lots of Atari 2600 games.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习深度Q网络（DQN），它通过在2013年击败人类玩家的许多Atari 2600游戏，开启了深度强化学习的革命。
