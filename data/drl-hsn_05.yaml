- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Tabular Learning and the Bellman Equation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¡¨æ ¼å­¦ä¹ å’Œè´å°”æ›¼æ–¹ç¨‹ã€‚
- en: 'In the previous chapter, you became acquainted with your first reinforcement
    learning (RL) algorithm, the cross-entropy method, along with its strengths and
    weaknesses. In this new part of the book, we will look at another group of methods
    that has much more flexibility and power: Q-learning. This chapter will establish
    the required background shared by those methods.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç« ä¸­ï¼Œä½ åˆæ­¥äº†è§£äº†ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•â€”â€”äº¤å‰ç†µæ³•ï¼Œå¹¶äº†è§£äº†å®ƒçš„ä¼˜ç¼ºç‚¹ã€‚åœ¨æœ¬ä¹¦çš„è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦ä¸€ç»„æ–¹æ³•ï¼Œå®ƒä»¬å…·æœ‰æ›´å¤šçš„çµæ´»æ€§å’Œå¼ºå¤§åŠŸèƒ½ï¼šQ-learningã€‚æœ¬ç« å°†ä¸ºè¿™äº›æ–¹æ³•å¥ å®šå¿…è¦çš„èƒŒæ™¯çŸ¥è¯†ã€‚
- en: We will also revisit the FrozenLake environment and explore how new concepts
    fit with this environment and help us to address issues related to its uncertainty.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†é‡æ–°å®¡è§†FrozenLakeç¯å¢ƒï¼Œæ¢è®¨æ–°æ¦‚å¿µå¦‚ä½•ä¸è¿™ä¸ªç¯å¢ƒå¥‘åˆï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬è§£å†³ä¸å…¶ä¸ç¡®å®šæ€§ç›¸å…³çš„é—®é¢˜ã€‚
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: Review the value of the state and the value of the action, and learn how to
    calculate them in simple cases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›é¡¾çŠ¶æ€çš„ä»·å€¼å’Œè¡ŒåŠ¨çš„ä»·å€¼ï¼Œå¹¶å­¦ä¹ å¦‚ä½•åœ¨ç®€å•çš„æƒ…å†µä¸‹è®¡ç®—å®ƒä»¬ã€‚
- en: Talk about the Bellman equation and how it establishes the optimal policy if
    we know the values of states
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºè´å°”æ›¼æ–¹ç¨‹ï¼Œä»¥åŠå®ƒå¦‚ä½•åœ¨æˆ‘ä»¬çŸ¥é“çŠ¶æ€çš„ä»·å€¼æ—¶å»ºç«‹æœ€ä¼˜ç­–ç•¥ã€‚
- en: Discuss the value iteration method and try it on the FrozenLake environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºä»·å€¼è¿­ä»£æ–¹æ³•ï¼Œå¹¶åœ¨FrozenLakeç¯å¢ƒä¸­è¿›è¡Œå°è¯•ã€‚
- en: Do the same for the Q-iteration method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹Q-è¿­ä»£æ–¹æ³•è¿›è¡Œç›¸åŒçš„æ“ä½œã€‚
- en: Despite the simplicity of the environments in this chapter, it establishes the
    required preparation for deep Q-learning, which is a very powerful and generic
    RL method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ¬ç« ä¸­çš„ç¯å¢ƒç®€å•ï¼Œä½†å®ƒä¸ºæ·±åº¦Qå­¦ä¹ ï¼ˆä¸€ä¸ªéå¸¸å¼ºå¤§ä¸”é€šç”¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼‰å¥ å®šäº†å¿…è¦çš„å‡†å¤‡ã€‚
- en: Value, state, and optimality
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»·å€¼ã€çŠ¶æ€å’Œæœ€ä¼˜æ€§ã€‚
- en: You may remember our definition of the value of the state from ChapterÂ [1](ch005.xhtml#x1-190001).
    This is a very important notion and the time has come to explore it further.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è¿˜è®°å¾—æˆ‘ä»¬åœ¨ç¬¬[1ç« ](ch005.xhtml#x1-190001)ä¸­å¯¹çŠ¶æ€ä»·å€¼çš„å®šä¹‰ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µï¼Œç°åœ¨æ˜¯æ—¶å€™è¿›ä¸€æ­¥æ¢è®¨å®ƒäº†ã€‚
- en: This whole part of the book is built around the value of the state and how to
    approximate it. We defined this value as an expected total reward (optionally
    discounted) that is obtainable from the state. In a formal way, the value of the
    state is given by
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„è¿™ä¸€éƒ¨åˆ†æ˜¯å›´ç»•çŠ¶æ€çš„ä»·å€¼åŠå¦‚ä½•é€¼è¿‘å®ƒå±•å¼€çš„ã€‚æˆ‘ä»¬å°†è¯¥ä»·å€¼å®šä¹‰ä¸ºä»çŠ¶æ€ä¸­è·å¾—çš„æœŸæœ›æ€»å¥–åŠ±ï¼ˆå¯é€‰æŠ˜æ‰£ï¼‰ã€‚ä»æ­£å¼çš„è§’åº¦æ¥çœ‹ï¼ŒçŠ¶æ€çš„ä»·å€¼ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq8.png)'
- en: where r[t] is the local reward obtained at step t of the episode.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­r[t]æ˜¯ä»£ç†åœ¨æƒ…èŠ‚çš„ç¬¬tæ­¥è·å¾—çš„å±€éƒ¨å¥–åŠ±ã€‚
- en: 'The total reward could be discounted with 0 < Î³ < 1 or not discounted (when
    Î³ = 1); itâ€™s up to us how to define it. The value is always calculated in terms
    of some policy that our agent follows. To illustrate this, letâ€™s consider a very
    simple environment with three states, as shown in FigureÂ [5.1](#x1-83004r1):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»å¥–åŠ±å¯ä»¥æŠ˜æ‰£ï¼ŒèŒƒå›´ä¸º0 < Î³ < 1ï¼Œæˆ–è€…ä¸æŠ˜æ‰£ï¼ˆå½“Î³ = 1æ—¶ï¼‰ï¼›è¿™å–å†³äºæˆ‘ä»¬å¦‚ä½•å®šä¹‰å®ƒã€‚ä»·å€¼å§‹ç»ˆæ˜¯æ ¹æ®ä»£ç†éµå¾ªçš„æŸä¸ªç­–ç•¥è®¡ç®—çš„ã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªéå¸¸ç®€å•çš„ç¯å¢ƒï¼ŒåŒ…å«ä¸‰ä¸ªçŠ¶æ€ï¼Œå¦‚å›¾[5.1](#x1-83004r1)æ‰€ç¤ºï¼š
- en: '![SsSeSerr=ta=n=n==1r2d3d12t ](img/B22150_05_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![SsSeSerr=ta=n=n==1r2d3d12t ](img/B22150_05_01.png)'
- en: 'FigureÂ 5.1: An example of an environmentâ€™s state transition with rewards'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.1ï¼šä¸€ä¸ªç¯å¢ƒçŠ¶æ€è½¬æ¢åŠå¥–åŠ±çš„ç¤ºä¾‹ã€‚
- en: The agentâ€™s initial state.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†çš„åˆå§‹çŠ¶æ€ã€‚
- en: The final state that the agent is in after executing action â€œrightâ€ from the
    initial state. The reward obtained from this is 1.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†åœ¨æ‰§è¡Œâ€œå³â€åŠ¨ä½œåä»åˆå§‹çŠ¶æ€åˆ°è¾¾çš„æœ€ç»ˆçŠ¶æ€ã€‚ä»ä¸­è·å¾—çš„å¥–åŠ±æ˜¯1ã€‚
- en: The final state that the agent is in after action â€œdown.â€ The reward obtained
    from this is 2.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†åœ¨æ‰§è¡Œâ€œä¸‹â€åŠ¨ä½œåçš„æœ€ç»ˆçŠ¶æ€ã€‚ä»ä¸­è·å¾—çš„å¥–åŠ±æ˜¯2ã€‚
- en: 'The environment is always deterministic â€” every action succeeds and we always
    start from state 1\. Once we reach either state 2 or state 3, the episode ends.
    Now, the question is, whatâ€™s the value of state 1? This question is meaningless
    without information about our agentâ€™s behavior or, in other words, its policy.
    Even in a simple environment, our agent can have an infinite amount of behaviors,
    each of which will have its own value for state 1\. Consider these examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒå§‹ç»ˆæ˜¯ç¡®å®šæ€§çš„â€”â€”æ¯ä¸ªè¡ŒåŠ¨éƒ½æˆåŠŸï¼Œæˆ‘ä»¬æ€»æ˜¯ä»çŠ¶æ€1å¼€å§‹ã€‚ä¸€æ—¦æˆ‘ä»¬åˆ°è¾¾çŠ¶æ€2æˆ–çŠ¶æ€3ï¼Œæƒ…èŠ‚ç»“æŸã€‚ç°åœ¨ï¼Œé—®é¢˜æ˜¯ï¼ŒçŠ¶æ€1çš„ä»·å€¼æ˜¯å¤šå°‘ï¼Ÿå¦‚æœæ²¡æœ‰å…³äºæˆ‘ä»¬ä»£ç†è¡Œä¸ºçš„ä¿¡æ¯ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œæ²¡æœ‰å…¶ç­–ç•¥ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚å³ä½¿åœ¨ä¸€ä¸ªç®€å•çš„ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬çš„ä»£ç†ä¹Ÿå¯èƒ½æœ‰æ— é™å¤šçš„è¡Œä¸ºï¼Œæ¯ä¸ªè¡Œä¸ºéƒ½æœ‰å…¶è‡ªèº«çš„çŠ¶æ€1ä»·å€¼ã€‚è€ƒè™‘ä»¥ä¸‹ä¾‹å­ï¼š
- en: Agent always goes right
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†å§‹ç»ˆå‘å³ç§»åŠ¨ã€‚
- en: Agent always goes down
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†å§‹ç»ˆå‘ä¸‹ç§»åŠ¨ã€‚
- en: Agent goes right with a probability of 50% and down with a probability of 50%
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†ä»¥50%çš„æ¦‚ç‡å‘å³ç§»åŠ¨ï¼Œ50%çš„æ¦‚ç‡å‘ä¸‹ç§»åŠ¨ã€‚
- en: Agent goes right in 10% of cases and in 90% of cases executes the â€œdownâ€ action
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ä»£ç†åœ¨10%çš„æƒ…å†µä¸‹å‘å³ï¼Œåœ¨90%çš„æƒ…å†µä¸‹æ‰§è¡Œâ€œå‘ä¸‹â€åŠ¨ä½œ  '
- en: 'To demonstrate how the value is calculated, letâ€™s do it for all the preceding
    policies:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†æ¼”ç¤ºå¦‚ä½•è®¡ç®—å€¼ï¼Œè®©æˆ‘ä»¬å¯¹ä¹‹å‰çš„æ‰€æœ‰ç­–ç•¥è¿›è¡Œè®¡ç®—ï¼š  '
- en: The value of state 1 in the case of the â€œalways rightâ€ agent is 1.0 (every time
    it goes left, it obtains 1 and the episode ends)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯¹äºâ€œå§‹ç»ˆå‘å³â€ä»£ç†ï¼ŒçŠ¶æ€1çš„å€¼ä¸º1.0ï¼ˆæ¯æ¬¡å®ƒå‘å·¦èµ°ï¼Œè·å¾—1åˆ†ï¼Œå›åˆç»“æŸï¼‰  '
- en: For the â€œalways downâ€ agent, the value of state 1 is 2.0
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯¹äºâ€œå§‹ç»ˆå‘ä¸‹â€ä»£ç†ï¼ŒçŠ¶æ€1çš„å€¼ä¸º2.0  '
- en: For the 50% right/50% down agent, the value is 1.0â‹…0.5+2.0â‹…0.5 = 1.5
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯¹äº50%å³/50%ä¸‹ä»£ç†ï¼Œå€¼ä¸º1.0â‹…0.5 + 2.0â‹…0.5 = 1.5  '
- en: For the 10% right/90% down agent, the value is 1.0â‹…0.1+2.0â‹…0.9 = 1.9
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯¹äº10%å³/90%ä¸‹ä»£ç†ï¼Œå€¼ä¸º1.0â‹…0.1 + 2.0â‹…0.9 = 1.9  '
- en: 'Now, another question: whatâ€™s the optimal policy for this agent? The goal of
    RL is to get as much total reward as possible. For this one-step environment,
    the total reward is equal to the value of state 1, which, obviously, is at the
    maximum at policy 2 (always down).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç°åœ¨ï¼Œå¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼šè¿™ä¸ªä»£ç†çš„æœ€ä¼˜ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯è·å¾—å°½å¯èƒ½å¤šçš„æ€»å¥–åŠ±ã€‚å¯¹äºè¿™ä¸ªä¸€æ­¥çš„ç¯å¢ƒï¼Œæ€»å¥–åŠ±ç­‰äºçŠ¶æ€1çš„å€¼ï¼Œæ˜¾ç„¶ï¼Œåœ¨ç­–ç•¥2ï¼ˆå§‹ç»ˆå‘ä¸‹ï¼‰ä¸‹ï¼Œæ€»å¥–åŠ±æ˜¯æœ€å¤§çš„ã€‚  '
- en: Unfortunately, such simple environments with an obvious optimal policy are not
    that interesting in practice. For interesting environments, the optimal policies
    are much harder to formulate and itâ€™s even harder to prove their optimality. However,
    donâ€™t worry; we are moving toward the point when we will be able to make computers
    learn the optimal behavior on their own.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œå…·æœ‰æ˜æ˜¾æœ€ä¼˜ç­–ç•¥çš„ç®€å•ç¯å¢ƒåœ¨å®é™…ä¸­å¹¶ä¸é‚£ä¹ˆæœ‰è¶£ã€‚å¯¹äºæœ‰è¶£çš„ç¯å¢ƒï¼Œæœ€ä¼˜ç­–ç•¥å¾€å¾€æ›´éš¾åˆ¶å®šï¼Œç”šè‡³æ›´éš¾è¯æ˜å®ƒä»¬çš„æœ€ä¼˜æ€§ã€‚ç„¶è€Œï¼Œä¸è¦æ‹…å¿ƒï¼›æˆ‘ä»¬æ­£åœ¨å‘ç€è®©è®¡ç®—æœºèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ æœ€ä¼˜è¡Œä¸ºçš„æ–¹å‘å‰è¿›ã€‚
- en: From the preceding example, you may have a false impression that we should always
    take the action with the highest reward. In general, itâ€™s not that simple. To
    demonstrate this, letâ€™s extend our preceding environment with yet another state
    that is reachable from state 3\. State 3 is no longer a terminal state but a transition
    to state 4, with a bad reward of -20\. Once we have chosen the â€œdownâ€ action in
    state 1, this bad reward is unavoidable, as from state 3, we have only one exit
    to state 4\. So, itâ€™s a trap for the agent, which has decided that â€œbeing greedyâ€
    is a good strategy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä»å‰é¢çš„ä¾‹å­æ¥çœ‹ï¼Œä½ å¯èƒ½ä¼šäº§ç”Ÿä¸€ä¸ªè¯¯è§£ï¼Œè®¤ä¸ºæˆ‘ä»¬åº”è¯¥æ€»æ˜¯é‡‡å–å¥–åŠ±æœ€é«˜çš„è¡ŒåŠ¨ã€‚é€šå¸¸æ¥è¯´ï¼Œäº‹æƒ…å¹¶æ²¡æœ‰é‚£ä¹ˆç®€å•ã€‚ä¸ºäº†è¯æ˜è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬åœ¨ä¹‹å‰çš„ç¯å¢ƒä¸­å†å¢åŠ ä¸€ä¸ªçŠ¶æ€ï¼Œè¿™ä¸ªçŠ¶æ€å¯ä»¥ä»çŠ¶æ€3åˆ°è¾¾ã€‚çŠ¶æ€3ä¸å†æ˜¯ç»ˆç»“çŠ¶æ€ï¼Œè€Œæ˜¯ä¸€ä¸ªè¿‡æ¸¡çŠ¶æ€åˆ°çŠ¶æ€4ï¼Œä¸”æœ‰ä¸€ä¸ªå¾ˆå·®çš„å¥–åŠ±â€”â€”-20ã€‚ä¸€æ—¦æˆ‘ä»¬åœ¨çŠ¶æ€1é€‰æ‹©äº†â€œå‘ä¸‹â€è¿™ä¸ªåŠ¨ä½œï¼Œè¿™ä¸ªåå¥–åŠ±æ˜¯ä¸å¯é¿å…çš„ï¼Œå› ä¸ºä»çŠ¶æ€3å¼€å§‹ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå‡ºå£â€”â€”åˆ°çŠ¶æ€4ã€‚æ‰€ä»¥ï¼Œå¯¹äºä»£ç†æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªé™·é˜±ï¼Œå®ƒå·²ç»å†³å®šâ€œè´ªå©ªâ€æ˜¯ä¸€ä¸ªå¥½ç­–ç•¥ã€‚  '
- en: '![SsSeSSerrr=ta=n==n===1r2d34d12âˆ’t20 ](img/B22150_05_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![SsSeSSerrr=ta=n==n===1r2d34d12âˆ’t20 ](img/B22150_05_02.png)  '
- en: 'FigureÂ 5.2: The same environment, with an extra state added'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 5.2ï¼šåŒæ ·çš„ç¯å¢ƒï¼Œå¢åŠ äº†ä¸€ä¸ªçŠ¶æ€  '
- en: 'With that addition, our values for state 1 will be calculated this way:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬å¯¹äºçŠ¶æ€1çš„å€¼è®¡ç®—å¦‚ä¸‹ï¼š  '
- en: 'The â€œalways rightâ€ agent is the same: 1.0'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œå§‹ç»ˆå‘å³â€ä»£ç†çš„å€¼æ˜¯ï¼š1.0  '
- en: The â€œalways downâ€ agent gets 2.0 + (âˆ’20) = âˆ’18
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'â€œå§‹ç»ˆå‘ä¸‹â€ä»£ç†çš„å€¼ä¸º2.0 + (âˆ’20) = âˆ’18  '
- en: The 50%/50% agent gets 0.5 â‹… 1.0 + 0.5 â‹… (2.0 + (âˆ’20)) = âˆ’8.5
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '50%/50%ä»£ç†çš„å€¼ä¸º0.5 â‹… 1.0 + 0.5 â‹… (2.0 + (âˆ’20)) = âˆ’8.5  '
- en: The 10%/90% agent gets 0.1 â‹… 1.0 + 0.9 â‹… (2.0 + (âˆ’20)) = âˆ’16.1
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '10%/90%ä»£ç†çš„å€¼ä¸º0.1 â‹… 1.0 + 0.9 â‹… (2.0 + (âˆ’20)) = âˆ’16.1  '
- en: 'So, the best policy for this new environment is now policy 1: always go right.
    We spent some time discussing naÃ¯ve and trivial environments so that you realize
    the complexity of this optimality problem and can appreciate the results of Richard
    Bellman better. Bellman was an American mathematician who formulated and proved
    his famous Bellman equation. We will talk about it in the next section.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ‰€ä»¥ï¼Œè¿™ä¸ªæ–°ç¯å¢ƒçš„æœ€ä½³ç­–ç•¥ç°åœ¨æ˜¯ç­–ç•¥1ï¼šå§‹ç»ˆå‘å³ã€‚æˆ‘ä»¬èŠ±äº†ä¸€äº›æ—¶é—´è®¨è®ºå¤©çœŸå’Œç®€å•çš„ç¯å¢ƒï¼Œè¿™æ ·ä½ å°±èƒ½æ„è¯†åˆ°è¿™ä¸ªæœ€ä¼˜é—®é¢˜çš„å¤æ‚æ€§ï¼Œå¹¶æ›´å¥½åœ°ç†è§£ç†æŸ¥å¾·Â·è´å°”æ›¼çš„ç»“æœã€‚è´å°”æ›¼æ˜¯ç¾å›½æ•°å­¦å®¶ï¼Œä»–æå‡ºå¹¶è¯æ˜äº†è‘—åçš„è´å°”æ›¼æ–¹ç¨‹ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºå®ƒã€‚  '
- en: The Bellman equation of optimality
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹  '
- en: To explain the Bellman equation, itâ€™s better to go a bit abstract. Donâ€™t be
    afraid; Iâ€™ll provide concrete examples later to support your learning! Letâ€™s start
    with a deterministic case, when all our actions have a 100% guaranteed outcome.
    Imagine that our agent observes state s[0] and has N available actions. Every
    action leads to another state, s[1]â€¦s[N], with a respective reward, r[1]â€¦r[N].
    Also, assume that we know the values, V [i], of all states connected to state
    s[0]. What will be the best course of action that the agent can take in such a
    state?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è§£é‡Šè´å°”æ›¼æ–¹ç¨‹ï¼Œæœ€å¥½æŠ½è±¡ä¸€äº›ã€‚ä¸è¦å®³æ€•ï¼›æˆ‘ä¼šåé¢æä¾›å…·ä½“çš„ä¾‹å­æ¥æ”¯æŒä½ çš„å­¦ä¹ ï¼è®©æˆ‘ä»¬ä»ä¸€ä¸ªç¡®å®šæ€§æƒ…å†µå¼€å§‹ï¼Œå½“æˆ‘ä»¬çš„æ‰€æœ‰è¡ŒåŠ¨éƒ½æœ‰ 100% çš„ä¿è¯ç»“æœæ—¶ã€‚æƒ³è±¡æˆ‘ä»¬çš„ä»£ç†è§‚å¯Ÿåˆ°çŠ¶æ€
    s[0] å¹¶æœ‰ N ä¸ªå¯ç”¨çš„è¡ŒåŠ¨ã€‚æ¯ä¸ªè¡ŒåŠ¨å¯¼è‡´å¦ä¸€ä¸ªçŠ¶æ€ s[1]â€¦s[N]ï¼Œå¹¶å¸¦æœ‰ç›¸åº”çš„å¥–åŠ± r[1]â€¦r[N]ã€‚è¿˜å‡è®¾æˆ‘ä»¬çŸ¥é“ä¸çŠ¶æ€ s[0] ç›¸è¿çš„æ‰€æœ‰çŠ¶æ€çš„å€¼
    V [i]ã€‚åœ¨è¿™æ ·çš„çŠ¶æ€ä¸‹ï¼Œä»£ç†å¯ä»¥é‡‡å–ä»€ä¹ˆæœ€ä½³è¡ŒåŠ¨ï¼Ÿ
- en: '![rrrr====rrrr123N sssssaaaa0123N====VVVV123N123N ](img/B22150_05_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![rrrr====rrrr123N sssssaaaa0123N====VVVV123N123N ](img/B22150_05_03.png)'
- en: 'FigureÂ 5.3: An abstract environment with N states reachable from the initial
    state'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 5.3ï¼šä»åˆå§‹çŠ¶æ€å¯è¾¾çš„ N ä¸ªçŠ¶æ€çš„æŠ½è±¡ç¯å¢ƒ
- en: 'If we choose the concrete action, a[i], and calculate the value given to this
    action, then the value will be V [0](a = a[i]) = r[i] + V [i]. So, to choose the
    best possible action, the agent needs to calculate the resulting values for every
    action and choose the maximum possible outcome. In other words, V [0] = max[aâˆˆ1â€¦N](r[a]
    + V [a]). If we are using the discount factor, Î³, we need to multiply the value
    of the next state by gamma: V [0] = max[aâˆˆ1â€¦N](r[a] + Î³V [a]).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬é€‰æ‹©å…·ä½“çš„è¡ŒåŠ¨ a[i] å¹¶è®¡ç®—ç»™å®šè¯¥è¡ŒåŠ¨çš„å€¼ï¼Œé‚£ä¹ˆè¯¥å€¼å°†ä¸º V [0](a = a[i]) = r[i] + V [i]ã€‚å› æ­¤ï¼Œä¸ºäº†é€‰æ‹©å¯èƒ½çš„æœ€ä½³è¡ŒåŠ¨ï¼Œä»£ç†éœ€è¦è®¡ç®—æ¯ä¸ªè¡ŒåŠ¨çš„ç»“æœå€¼ï¼Œå¹¶é€‰æ‹©å¯èƒ½çš„æœ€å¤§ç»“æœã€‚æ¢å¥è¯è¯´ï¼ŒV
    [0] = max[aâˆˆ1â€¦N](r[a] + V [a])ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨æŠ˜ç°å› å­ Î³ï¼Œæˆ‘ä»¬éœ€è¦å°†ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ä¹˜ä»¥ gammaï¼šV [0] = max[aâˆˆ1â€¦N](r[a]
    + Î³V [a])ã€‚
- en: 'This may look very similar to our greedy example from the previous section,
    and, in fact, it is. However, there is one difference: when we act greedily, we
    do not only look at the immediate reward for the action, but at the immediate
    reward plus the long-term value of the state. This allows us to avoid a possible
    trap with a large immediate reward but a state that has a bad value.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥å¯èƒ½ä¸å‰ä¸€èŠ‚çš„è´ªå©ªç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼Œå®é™…ä¸Šç¡®å®å¦‚æ­¤ã€‚ç„¶è€Œï¼Œæœ‰ä¸€ä¸ªåŒºåˆ«ï¼šå½“æˆ‘ä»¬è´ªå©ªåœ°è¡ŒåŠ¨æ—¶ï¼Œæˆ‘ä»¬ä¸ä»…çœ‹å³æ—¶è¡ŒåŠ¨çš„å¥–åŠ±ï¼Œè¿˜çœ‹é•¿æœŸçŠ¶æ€å€¼çš„å¥–åŠ±ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé¿å…å¯èƒ½å‡ºç°çš„é™·é˜±ï¼Œå³å³æ—¶å¥–åŠ±å¾ˆå¤§ä½†çŠ¶æ€å€¼å¾ˆå·®çš„æƒ…å†µã€‚
- en: Bellman proved that with that extension, our behavior will get the best possible
    outcome. In other words, it will be optimal. So, the preceding equation is called
    the Bellman equation of value (for a deterministic case).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è´å°”æ›¼è¯æ˜äº†é€šè¿‡è¿™ç§æ‰©å±•ï¼Œæˆ‘ä»¬çš„è¡Œä¸ºå°†è·å¾—æœ€ä½³å¯èƒ½çš„ç»“æœã€‚æ¢å¥è¯è¯´ï¼Œå®ƒå°†æ˜¯æœ€ä¼˜çš„ã€‚å› æ­¤ï¼Œå‰è¿°æ–¹ç¨‹è¢«ç§°ä¸ºå€¼çš„è´å°”æ›¼æ–¹ç¨‹ï¼ˆå¯¹äºç¡®å®šæ€§æƒ…å†µï¼‰ã€‚
- en: 'Itâ€™s not very complicated to extend this idea for a stochastic case, when our
    actions have the chance of ending up in different states. What we need to do is
    calculate the expected value for every action, instead of just taking the value
    of the next state. To illustrate this, letâ€™s consider one single action available
    from state s[0], with three possible outcomes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•æ¨å¹¿åˆ°éšæœºæƒ…å†µå¹¶ä¸å¤æ‚ï¼Œå½“æˆ‘ä»¬çš„è¡Œä¸ºæœ‰å¯èƒ½å¯¼è‡´ä¸åŒçŠ¶æ€æ—¶ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯è®¡ç®—æ¯ä¸ªè¡ŒåŠ¨çš„æœŸæœ›å€¼ï¼Œè€Œä¸ä»…ä»…æ˜¯è€ƒè™‘ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ã€‚ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä»çŠ¶æ€
    s[0] å¯ç”¨çš„å•ä¸ªè¡ŒåŠ¨ï¼Œæœ‰ä¸‰ç§å¯èƒ½çš„ç»“æœï¼š
- en: '![rrr===rrr 123 ssssappp0123=123VVV1123 ](img/B22150_05_04.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![rrr===rrr 123 ssssappp0123=123VVV1123 ](img/B22150_05_04.png)'
- en: 'FigureÂ 5.4: An example of the transition from the state in a stochastic case'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 5.4ï¼šåœ¨éšæœºæƒ…å†µä¸‹ä»çŠ¶æ€è½¬ç§»çš„ç¤ºä¾‹
- en: 'Here, we have one action, which can lead to three different states with different
    probabilities. With probability p[1], the action can end up in state s[1], with
    p[2] in state s[2], and with p[3] in state s[3] (p[1] + p[2] + p[3] = 1, of course).
    Every target state has its own reward (r[1], r[2], or r[3]). To calculate the
    expected value after issuing action 1, we need to sum all values, multiplied by
    their probabilities:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¡ŒåŠ¨ï¼Œå¯ä»¥ä»¥ä¸‰ç§ä¸åŒçš„æ¦‚ç‡å¯¼è‡´ä¸‰ä¸ªä¸åŒçš„çŠ¶æ€ã€‚ä»¥æ¦‚ç‡ p[1]ï¼Œè¯¥è¡ŒåŠ¨å¯èƒ½è¿›å…¥çŠ¶æ€ s[1]ï¼Œä»¥ p[2] è¿›å…¥çŠ¶æ€ s[2]ï¼Œä»¥ p[3]
    è¿›å…¥çŠ¶æ€ s[3]ï¼ˆå½“ç„¶ï¼Œp[1] + p[2] + p[3] = 1ï¼‰ã€‚æ¯ä¸ªç›®æ ‡çŠ¶æ€éƒ½æœ‰è‡ªå·±çš„å¥–åŠ±ï¼ˆr[1]ã€r[2] æˆ– r[3]ï¼‰ã€‚è¦è®¡ç®—å‘å‡ºè¡ŒåŠ¨ 1
    åçš„æœŸæœ›å€¼ï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰å€¼ä¹˜ä»¥å®ƒä»¬çš„æ¦‚ç‡å¹¶æ±‚å’Œï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq9.png)'
- en: or, more formally
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ›´æ­£å¼åœ°è¯´ï¼Œ
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq10.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq10.png)'
- en: Here, ğ”¼ [sâˆ¼S] means taking the expected value over all states in our state space,
    S.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œğ”¼ [sâˆ¼S] è¡¨ç¤ºåœ¨æˆ‘ä»¬çš„çŠ¶æ€ç©ºé—´ S ä¸­æ‰€æœ‰çŠ¶æ€ä¸Šå–æœŸæœ›å€¼ã€‚
- en: 'By combining the Bellman equation, for a deterministic case, with a value for
    stochastic actions, we get the Bellman optimality equation for a general case:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†è´å°”æ›¼æ–¹ç¨‹ï¼ˆå¯¹äºç¡®å®šæ€§æƒ…å†µï¼‰ä¸éšæœºåŠ¨ä½œçš„å€¼ç›¸ç»“åˆï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€èˆ¬æƒ…å†µçš„è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq11.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq11.png)'
- en: 'Note that p[a,iâ†’j] means the probability of action a, issued in state i, ending
    up in state j. The interpretation is still the same: the optimal value of the
    state corresponds to the action, which gives us the maximum possible expected
    immediate reward, plus the discounted long-term reward for the next state. You
    may also notice that this definition is recursive: the value of the state is defined
    via the values of the immediately reachable states. This recursion may look like
    cheating: we define some value, pretending that we already know it. However, this
    is a very powerful and common technique in computer science and even in math in
    general (proof by induction is based on the same trick). This Bellman equation
    is a foundation not only in RL but also in much more general dynamic programming,
    which is a widely used method for solving practical optimization problems.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œp[a,iâ†’j]è¡¨ç¤ºåœ¨çŠ¶æ€iä¸‹æ‰§è¡ŒåŠ¨ä½œaåï¼Œè½¬ç§»åˆ°çŠ¶æ€jçš„æ¦‚ç‡ã€‚è§£é‡Šä»ç„¶æ˜¯ä¸€æ ·çš„ï¼šçŠ¶æ€çš„æœ€ä¼˜å€¼å¯¹åº”äºèƒ½å¤Ÿç»™æˆ‘ä»¬æœ€å¤§å¯èƒ½çš„æœŸæœ›ç«‹å³å¥–åŠ±ï¼ŒåŠ ä¸Šä¸‹ä¸€çŠ¶æ€çš„æŠ˜æ‰£é•¿æœŸå¥–åŠ±çš„åŠ¨ä½œã€‚ä½ å¯èƒ½è¿˜ä¼šæ³¨æ„åˆ°ï¼Œè¿™ä¸€å®šä¹‰æ˜¯é€’å½’çš„ï¼šçŠ¶æ€çš„å€¼æ˜¯é€šè¿‡ç«‹å³å¯è¾¾çŠ¶æ€çš„å€¼æ¥å®šä¹‰çš„ã€‚è¿™ç§é€’å½’çœ‹èµ·æ¥å¯èƒ½åƒæ˜¯ä½œå¼Šï¼šæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå€¼ï¼Œå‡è£…æˆ‘ä»¬å·²ç»çŸ¥é“å®ƒã€‚ç„¶è€Œï¼Œè¿™åœ¨è®¡ç®—æœºç§‘å­¦ç”šè‡³æ•°å­¦ä¸­éƒ½æ˜¯ä¸€ç§éå¸¸å¼ºå¤§ä¸”å¸¸è§çš„æŠ€å·§ï¼ˆæ•°å­¦å½’çº³æ³•å°±æ˜¯åŸºäºè¿™ç§æŠ€å·§ï¼‰ã€‚è¿™ä¸ªè´å°”æ›¼æ–¹ç¨‹ä¸ä»…æ˜¯å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€ï¼Œè¿˜æ˜¯æ›´ä¸ºä¸€èˆ¬çš„åŠ¨æ€è§„åˆ’çš„åŸºç¡€ï¼ŒåŠ¨æ€è§„åˆ’æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºè§£å†³å®é™…ä¼˜åŒ–é—®é¢˜çš„æ–¹æ³•ã€‚
- en: 'These values not only give us the best reward that we can obtain, but they
    basically give us the optimal policy to obtain that reward: if our agent knows
    the value for every state, then it automatically knows how to gather this reward.
    Thanks to Bellmanâ€™s optimality proof, at every state the agent ends up in, it
    needs to select the action with the maximum expected reward, which is a sum of
    the immediate reward and the one-step discounted long-term reward â€“ thatâ€™s it.
    So, those values are really useful to know. Before you get familiar with a practical
    way to calculate them, I need to introduce one more mathematical notation. Itâ€™s
    not as fundamental as the value of the state, but we need it for our convenience.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å€¼ä¸ä»…å‘Šè¯‰æˆ‘ä»¬å¯ä»¥è·å¾—çš„æœ€ä½³å¥–åŠ±ï¼Œè€Œä¸”åŸºæœ¬ä¸Šç»™å‡ºäº†è·å–è¯¥å¥–åŠ±çš„æœ€ä¼˜ç­–ç•¥ï¼šå¦‚æœæˆ‘ä»¬çš„æ™ºèƒ½ä½“çŸ¥é“æ¯ä¸ªçŠ¶æ€çš„å€¼ï¼Œé‚£ä¹ˆå®ƒå°±è‡ªåŠ¨çŸ¥é“å¦‚ä½•è·å–è¿™ä¸ªå¥–åŠ±ã€‚å‡­å€Ÿè´å°”æ›¼æœ€ä¼˜æ€§è¯æ˜ï¼Œåœ¨æ™ºèƒ½ä½“åˆ°è¾¾çš„æ¯ä¸ªçŠ¶æ€ä¸­ï¼Œå®ƒéœ€è¦é€‰æ‹©å…·æœ‰æœ€å¤§æœŸæœ›å¥–åŠ±çš„åŠ¨ä½œï¼Œè¿™ä¸ªæœŸæœ›å¥–åŠ±æ˜¯ç«‹å³å¥–åŠ±ä¸ä¸€æ­¥æŠ˜æ‰£åçš„é•¿æœŸå¥–åŠ±ä¹‹å’Œâ€”â€”ä»…æ­¤è€Œå·²ã€‚å› æ­¤ï¼Œè¿™äº›å€¼å¯¹äºäº†è§£æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚åœ¨ä½ ç†Ÿæ‚‰ä¸€ç§è®¡ç®—è¿™äº›å€¼çš„å®é™…æ–¹æ³•ä¹‹å‰ï¼Œæˆ‘éœ€è¦ä»‹ç»ä¸€ä¸ªæ•°å­¦ç¬¦å·ã€‚å®ƒä¸åƒçŠ¶æ€å€¼é‚£æ ·åŸºç¡€ï¼Œä½†ä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬éœ€è¦å®ƒã€‚
- en: The value of the action
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨ä½œçš„å€¼
- en: 'To make our life slightly easier, we can define different quantities, in addition
    to the value of the state, V (s), as the value of the action, Q(s,a). Basically,
    this equals the total reward we can get by executing action a in state s and can
    be defined via V (s). Being a much less fundamental entity than V (s), this quantity
    gave a name to the whole family of methods called Q-learning, because it is more
    convenient. In these methods, our primary objective is to get values of Q for
    every pair of state and action:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®©æˆ‘ä»¬çš„ç”Ÿæ´»ç¨å¾®è½»æ¾ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸åŒçš„é‡ï¼Œé™¤äº†çŠ¶æ€å€¼V(s)å¤–ï¼Œè¿˜å¯ä»¥å®šä¹‰åŠ¨ä½œå€¼Q(s,a)ã€‚åŸºæœ¬ä¸Šï¼Œè¿™ç­‰äºæˆ‘ä»¬åœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œaæ‰€èƒ½è·å¾—çš„æ€»å¥–åŠ±ï¼Œå¯ä»¥é€šè¿‡V(s)æ¥å®šä¹‰ã€‚ä½œä¸ºä¸€ä¸ªæ¯”V(s)æ›´ä¸åŸºç¡€çš„é‡ï¼Œè¿™ä¸ªé‡ç»™æ•´ä¸ªQå­¦ä¹ æ–¹æ³•æ—å‘½åï¼Œå› ä¸ºå®ƒæ›´æ–¹ä¾¿ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è·å–æ¯å¯¹çŠ¶æ€å’ŒåŠ¨ä½œçš„Qå€¼ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq12.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq12.png)'
- en: 'Q, for this state, s, and action, a, equals the expected immediate reward and
    the discounted long-term reward of the destination state. We also can define V
    (s) via Q(s,a):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªçŠ¶æ€så’ŒåŠ¨ä½œaï¼ŒQç­‰äºæœŸæœ›çš„ç«‹å³å¥–åŠ±å’Œç›®æ ‡çŠ¶æ€çš„æŠ˜æ‰£é•¿æœŸå¥–åŠ±ã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡Q(s,a)æ¥å®šä¹‰V(s)ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq13.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq13.png)'
- en: This just means that the value of some state equals to the value of the maximum
    action we can execute from this state.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯æ„å‘³ç€æŸä¸ªçŠ¶æ€çš„å€¼ç­‰äºæˆ‘ä»¬ä»è¯¥çŠ¶æ€æ‰§è¡Œçš„æœ€å¤§åŠ¨ä½œçš„å€¼ã€‚
- en: 'Finally, we can express Q(s,a) recursively (which will be used in ChapterÂ [6](#)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥é€’å½’åœ°è¡¨è¾¾Q(s,a)ï¼ˆå°†åœ¨ç¬¬[6](#)ç« ä¸­ä½¿ç”¨ï¼‰ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq14.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq14.png)'
- en: 'In the last formula, the index on the immediate reward, (s,a), depends on the
    environment details:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åçš„å…¬å¼ä¸­ï¼Œç«‹å³å¥–åŠ±çš„ç´¢å¼•(s,a)ä¾èµ–äºç¯å¢ƒçš„å…·ä½“ç»†èŠ‚ï¼š
- en: If the immediate reward is given to us after executing a particular action,
    a, from state s, index (s,a) is used and the formula is exactly as shown above.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœåœ¨æ‰§è¡Œç‰¹å®šåŠ¨ä½œaæ—¶ï¼Œç«‹å³å¥–åŠ±æ˜¯åœ¨çŠ¶æ€sä¸‹ç»™äºˆæˆ‘ä»¬çš„ï¼Œé‚£ä¹ˆä½¿ç”¨ç´¢å¼•(s,a)ï¼Œå…¬å¼ä¸ä¸Šé¢å±•ç¤ºçš„å®Œå…¨ä¸€è‡´ã€‚
- en: 'But if the reward is provided for reaching some state, sâ€², via action aâ€², the
    reward will have the index (sâ€²,aâ€²) and will need to be moved into the max operator:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœå¥–åŠ±æ˜¯é€šè¿‡åŠ¨ä½œaâ€²åˆ°è¾¾æŸä¸ªçŠ¶æ€sâ€²æ—¶ç»™äºˆçš„ï¼Œå¥–åŠ±å°†å…·æœ‰ç´¢å¼•(sâ€²,aâ€²)ï¼Œå¹¶ä¸”éœ€è¦è¢«ç§»å…¥æœ€å¤§å€¼è¿ç®—ç¬¦ä¸­ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq15.png)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq15.png)'
- en: That difference is not very significant from a mathematical point of view, but
    it could be important during the implementation of the methods. The first situation
    is more common, so we will stick to the preceding formula.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªå·®å¼‚å¹¶ä¸å¤§ï¼Œä½†åœ¨æ–¹æ³•å®ç°è¿‡ç¨‹ä¸­å¯èƒ½å¾ˆé‡è¦ã€‚ç¬¬ä¸€ç§æƒ…å†µæ›´ä¸ºå¸¸è§ï¼Œå› æ­¤æˆ‘ä»¬å°†åšæŒä½¿ç”¨å‰è¿°å…¬å¼ã€‚
- en: 'To give you a concrete example, letâ€™s consider an environment that is similar
    to FrozenLake, but has a much simpler structure: we have one initial state (s[0])
    surrounded by four target states, s[1], s[2], s[3], s[4], with different rewards:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»™ä½ ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç±»ä¼¼FrozenLakeçš„ç¯å¢ƒï¼Œä½†ç»“æ„æ›´ç®€å•ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªåˆå§‹çŠ¶æ€(s[0])ï¼Œå‘¨å›´æœ‰å››ä¸ªç›®æ ‡çŠ¶æ€s[1]ã€s[2]ã€s[3]ã€s[4]ï¼Œæ¯ä¸ªçŠ¶æ€çš„å¥–åŠ±ä¸åŒï¼š
- en: '![s0 â€“ initial state ssssss012431,s2,s3,s4 â€“ final states ](img/B22150_05_05.png)
    FigureÂ 5.5: A simplified grid-like environment'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![s0 â€“ åˆå§‹çŠ¶æ€ ssssss012431,s2,s3,s4 â€“ ç»ˆæ€ ](img/B22150_05_05.png) å›¾ 5.5ï¼šç®€åŒ–çš„ç½‘æ ¼çŠ¶ç¯å¢ƒ'
- en: 'Every action is probabilistic in the same way as in FrozenLake: with a 33%
    chance that our action will be executed without modifications, but with a 33%
    chance that we will slip to the left, relatively, of our target cell and a 33%
    chance that we will slip to the right. For simplicity, we use discount factor
    Î³ = 1.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåŠ¨ä½œçš„æ‰§è¡Œéƒ½æ˜¯æœ‰æ¦‚ç‡çš„ï¼Œå’ŒFrozenLakeä¸­çš„æ–¹å¼ä¸€æ ·ï¼šæœ‰33%çš„æ¦‚ç‡æˆ‘ä»¬çš„åŠ¨ä½œå°†æŒ‰åŸæ ·æ‰§è¡Œï¼Œä½†ä¹Ÿæœ‰33%çš„æ¦‚ç‡æˆ‘ä»¬ä¼šç›¸å¯¹äºç›®æ ‡å•å…ƒæ ¼å‘å·¦æ»‘åŠ¨ï¼Œå¦å¤–33%çš„æ¦‚ç‡æˆ‘ä»¬ä¼šå‘å³æ»‘åŠ¨ã€‚ä¸ºäº†ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ˜æ‰£å› å­Î³
    = 1ã€‚
- en: '![sssssuledr000000000000rrrr01234pfoig.3.3.3.3.3.3.3.3.3.3.3.3====twh3333333333331234nt
    ](img/B22150_05_06.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![sssssuledr000000000000rrrr01234pfoig.3.3.3.3.3.3.3.3.3.3.3.3====twh3333333333331234nt
    ](img/B22150_05_06.png)'
- en: 'FigureÂ 5.6: A transition diagram of the grid environment'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.6ï¼šç½‘æ ¼ç¯å¢ƒçš„è½¬ç§»å›¾
- en: 'Letâ€™s calculate the values of the actions to begin with. Terminal states s[1]â€¦s[4]
    have no outbound connections, so Q for those states is zero for all actions. Due
    to this, the values of the terminal states are equal to their immediate reward
    (once we get there, our episode ends without any subsequent states): V [1] = 1,
    V [2] = 2, V [3] = 3, V [4] = 4.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å…ˆè®¡ç®—è¿™äº›åŠ¨ä½œçš„å€¼ã€‚ç»ˆæ€s[1]â€¦s[4]æ²¡æœ‰å¤–éƒ¨è¿æ¥ï¼Œå› æ­¤è¿™äº›çŠ¶æ€çš„Qå€¼å¯¹æ‰€æœ‰åŠ¨ä½œå‡ä¸ºé›¶ã€‚ç”±äºè¿™ä¸ªåŸå› ï¼Œç»ˆæ€çš„å€¼ç­‰äºå®ƒä»¬çš„å³æ—¶å¥–åŠ±ï¼ˆæˆ‘ä»¬ä¸€æ—¦åˆ°è¾¾ç»ˆæ€ï¼Œå›åˆå°±ç»“æŸï¼Œæ²¡æœ‰åç»­çŠ¶æ€ï¼‰ï¼šV
    [1] = 1ï¼ŒV [2] = 2ï¼ŒV [3] = 3ï¼ŒV [4] = 4ã€‚
- en: 'The values of the actions for state 0 are a bit more complicated. Letâ€™s start
    with the â€œupâ€ action. Its value, according to the definition, is equal to the
    expected sum of the immediate reward plus the long-term value for subsequent steps.
    We have no subsequent steps for any possible transition for the â€œupâ€ action:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€0çš„åŠ¨ä½œå€¼ç¨å¾®å¤æ‚ä¸€äº›ã€‚æˆ‘ä»¬ä»â€œå‘ä¸Šâ€åŠ¨ä½œå¼€å§‹ã€‚æ ¹æ®å®šä¹‰ï¼Œå®ƒçš„å€¼ç­‰äºç«‹å³å¥–åŠ±çš„æœŸæœ›å€¼åŠ ä¸Šåç»­æ­¥éª¤çš„é•¿æœŸå€¼ã€‚å¯¹äºâ€œå‘ä¸Šâ€åŠ¨ä½œçš„ä»»ä½•å¯èƒ½è½¬ç§»ï¼Œæˆ‘ä»¬æ²¡æœ‰åç»­æ­¥éª¤ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq16.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq16.png)'
- en: 'Repeating this for the rest of the s[0] actions results in the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹s[0]çš„å…¶ä½™åŠ¨ä½œè¿›è¡Œç›¸åŒçš„è®¡ç®—ï¼Œç»“æœå¦‚ä¸‹ï¼š
- en: '| Q(s[0],left) | = 0.33 â‹…V [1] + 0.33 â‹…V [2] + 0.33 â‹…V [3] = 1.98 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Q(s[0],left) | = 0.33 â‹…V [1] + 0.33 â‹…V [2] + 0.33 â‹…V [3] = 1.98 |'
- en: '| Q(s[0],right) | = 0.33 â‹…V [4] + 0.33 â‹…V [1] + 0.33 â‹…V [3] = 2.64 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Q(s[0],right) | = 0.33 â‹…V [4] + 0.33 â‹…V [1] + 0.33 â‹…V [3] = 2.64 |'
- en: '| Q(s[0],down) | = 0.33 â‹…V [3] + 0.33 â‹…V [2] + 0.33 â‹…V [4] = 2.97 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Q(s[0],down) | = 0.33 â‹…V [3] + 0.33 â‹…V [2] + 0.33 â‹…V [4] = 2.97 |'
- en: The final value for state s[0] is the maximum of those actionsâ€™ values, which
    is 2.97.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€s[0]çš„æœ€ç»ˆå€¼æ˜¯è¿™äº›åŠ¨ä½œå€¼ä¸­çš„æœ€å¤§å€¼ï¼Œå³2.97ã€‚
- en: 'Q-values are much more convenient in practice, as for the agent, itâ€™s much
    simpler to make decisions about actions based on Q than on V . In the case of
    Q, to choose the action based on the state, the agent just needs to calculate
    Q for all available actions using the current state and choose the action with
    the largest value of Q. To do the same using values of the states, the agent needs
    to know not only the values, but also the probabilities for transitions. In practice,
    we rarely know them in advance, so the agent needs to estimate transition probabilities
    for every action and state pair. Later in this chapter, you will see this in practice
    by solving the FrozenLake environment both ways. However, to be able to do this,
    we have one important thing still missing: a general way to calculate V [i] and
    Q[i].'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Qå€¼åœ¨å®è·µä¸­è¦æ–¹ä¾¿å¾—å¤šï¼Œå› ä¸ºå¯¹äºæ™ºèƒ½ä½“æ¥è¯´ï¼Œæ ¹æ® Q è€Œä¸æ˜¯ V æ¥åšå†³ç­–è¦ç®€å•å¾—å¤šã€‚åœ¨ Q çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“åªéœ€è¦ä½¿ç”¨å½“å‰çŠ¶æ€è®¡ç®—æ‰€æœ‰å¯ç”¨åŠ¨ä½œçš„ Q
    å€¼ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€å¤§ Q å€¼çš„åŠ¨ä½œã€‚ä½¿ç”¨çŠ¶æ€å€¼æ¥åšç›¸åŒçš„é€‰æ‹©æ—¶ï¼Œæ™ºèƒ½ä½“ä¸ä»…éœ€è¦çŸ¥é“è¿™äº›å€¼ï¼Œè¿˜éœ€è¦çŸ¥é“è½¬ç§»çš„æ¦‚ç‡ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¾ˆå°‘äº‹å…ˆçŸ¥é“è¿™äº›æ¦‚ç‡ï¼Œå› æ­¤æ™ºèƒ½ä½“éœ€è¦ä¸ºæ¯ä¸ªåŠ¨ä½œå’ŒçŠ¶æ€å¯¹ä¼°è®¡è½¬ç§»æ¦‚ç‡ã€‚åœ¨æœ¬ç« ç¨åçš„éƒ¨åˆ†ï¼Œä½ å°†é€šè¿‡ä¸¤ç§æ–¹æ³•è§£å†³
    FrozenLake ç¯å¢ƒï¼Œäº²è‡ªçœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œè¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªé‡è¦çš„ä¸œè¥¿ç¼ºå¤±ï¼šè®¡ç®— V[i] å’Œ Q[i] çš„é€šç”¨æ–¹æ³•ã€‚
- en: The value iteration method
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£æ–¹æ³•
- en: 'In the simplistic example you just saw, to calculate the values of the states
    and actions, we exploited the structure of the environment: we had no loops in
    transitions, so we could start from terminal states, calculate their values, and
    then proceed to the central state. However, just one loop in the environment builds
    an obstacle in our approach. Letâ€™s consider such an environment with two states:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ åˆšæ‰çœ‹åˆ°çš„ç®€å•ä¾‹å­ä¸­ï¼Œä¸ºäº†è®¡ç®—çŠ¶æ€å’ŒåŠ¨ä½œçš„å€¼ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ç¯å¢ƒçš„ç»“æ„ï¼šè½¬ç§»ä¸­æ²¡æœ‰å¾ªç¯ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä»ç»ˆç«¯çŠ¶æ€å¼€å§‹ï¼Œè®¡ç®—å®ƒä»¬çš„å€¼ï¼Œç„¶åå†å¤„ç†ä¸­å¤®çŠ¶æ€ã€‚ç„¶è€Œï¼Œç¯å¢ƒä¸­çš„ä¸€ä¸ªå¾ªç¯å°±æ„æˆäº†æˆ‘ä»¬æ–¹æ³•çš„éšœç¢ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæœ‰ä¸¤ä¸ªçŠ¶æ€çš„ç¯å¢ƒï¼š
- en: '![ssrrÎ³12==12= 0.9 ](img/B22150_05_07.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![ssrrÎ³12==12= 0.9 ](img/B22150_05_07.png)'
- en: 'FigureÂ 5.7: A sample environment with a loop in the transition diagram'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.7ï¼šå…·æœ‰è½¬ç§»å›¾ä¸­å¾ªç¯çš„ç¤ºä¾‹ç¯å¢ƒ
- en: 'We start from state s[1], and the only action we can take leads us to state
    s[2]. We get the reward, r = 1, and the only transition from s[2] is an action,
    which brings us back to s[1]. So, the life of our agent is an infinite sequence
    of states [s[1],s[2],s[1],s[2],â€¦]. To deal with this infinity loop, we can use
    a discount factor: Î³ = 0.9\. Now, the question is, what are the values for both
    the states? The answer is not very complicated, in fact. Every transition from
    s[1] to s[2] gives us a reward of 1 and every back transition gives us 2\. So,
    our sequence of rewards will be [1,2,1,2,1,2,1,2,â€¦]. As there is only one action
    available in every state, our agent has no choice, so we can omit the max operation
    in formulas (there is only one alternative).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»çŠ¶æ€ s[1] å¼€å§‹ï¼Œå”¯ä¸€å¯ä»¥é‡‡å–çš„åŠ¨ä½œå°†æˆ‘ä»¬å¸¦åˆ°çŠ¶æ€ s[2]ã€‚æˆ‘ä»¬è·å¾—å¥–åŠ± r = 1ï¼Œs[2] ä¸­å”¯ä¸€çš„è½¬ç§»æ˜¯ä¸€ä¸ªåŠ¨ä½œï¼Œå®ƒå°†æˆ‘ä»¬å¸¦å›åˆ° s[1]ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“çš„ç”Ÿå‘½å‘¨æœŸæ˜¯ä¸€ä¸ªæ— é™çš„çŠ¶æ€åºåˆ—[s[1],
    s[2], s[1], s[2], â€¦]ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªæ— é™å¾ªç¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æŠ˜æ‰£å› å­ï¼šÎ³ = 0.9ã€‚ç°åœ¨ï¼Œé—®é¢˜æ˜¯ï¼Œä¸¤ä¸ªçŠ¶æ€çš„å€¼æ˜¯å¤šå°‘ï¼Ÿå…¶å®ï¼Œç­”æ¡ˆå¹¶ä¸å¤æ‚ã€‚æ¯æ¬¡ä»
    s[1] è½¬ç§»åˆ° s[2] éƒ½ä¼šç»™æˆ‘ä»¬å¥–åŠ±1ï¼Œè€Œæ¯æ¬¡è¿”å›è½¬ç§»éƒ½ä¼šç»™æˆ‘ä»¬å¥–åŠ±2ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å¥–åŠ±åºåˆ—å°†æ˜¯[1, 2, 1, 2, 1, 2, 1, 2, â€¦]ã€‚ç”±äºæ¯ä¸ªçŠ¶æ€åªæœ‰ä¸€ä¸ªå¯ç”¨çš„åŠ¨ä½œï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“æ²¡æœ‰é€‰æ‹©çš„ä½™åœ°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨å…¬å¼ä¸­çœç•¥æœ€å¤§å€¼æ“ä½œï¼ˆå› ä¸ºåªæœ‰ä¸€ä¸ªé€‰æ‹©ï¼‰ã€‚
- en: 'The value for every state will be equal to the infinite sum:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªçŠ¶æ€çš„å€¼å°†ç­‰äºæ— é™æ±‚å’Œï¼š
- en: '| V (s[1]) | = 1 + Î³(2 + Î³(1 + Î³(2 + â€¦))) = âˆ‘ [i=0]^âˆ1Î³^(2i) + 2Î³^(2i+1) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| V (s[1]) | = 1 + Î³(2 + Î³(1 + Î³(2 + â€¦))) = âˆ‘ [i=0]^âˆ1Î³^(2i) + 2Î³^(2i+1) |'
- en: '| V (s[2]) | = 2 + Î³(1 + Î³(2 + Î³(1 + â€¦))) = âˆ‘ [i=0]^âˆ2Î³^(2i) + 1Î³^(2i+1) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| V (s[2]) | = 2 + Î³(1 + Î³(2 + Î³(1 + â€¦))) = âˆ‘ [i=0]^âˆ2Î³^(2i) + 1Î³^(2i+1) |'
- en: 'Strictly speaking, we canâ€™t calculate the exact values for our states, but
    with Î³ = 0.9, the contribution of every transition quickly decreases over time.
    For example, after 10 steps, Î³^(10) = 0.9^(10) â‰ˆ 0.349, but after 100 steps, it
    becomes just 0.0000266\. Due to this, we can stop after 50 iterations and still
    get quite a precise estimation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¥æ ¼æ¥è¯´ï¼Œæˆ‘ä»¬æ— æ³•è®¡ç®—å‡ºæˆ‘ä»¬çŠ¶æ€çš„å‡†ç¡®å€¼ï¼Œä½†å½“ Î³ = 0.9 æ—¶ï¼Œæ¯ä¸ªè½¬ç§»çš„è´¡çŒ®ä¼šéšç€æ—¶é—´è¿…é€Ÿå‡å°‘ã€‚ä¾‹å¦‚ï¼Œç»è¿‡10æ­¥åï¼ŒÎ³^(10) = 0.9^(10)
    â‰ˆ 0.349ï¼Œä½†ç»è¿‡100æ­¥åï¼Œå®ƒå°±å˜æˆäº†0.0000266ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨50æ¬¡è¿­ä»£ååœæ­¢ï¼Œä»ç„¶å¯ä»¥å¾—åˆ°ç›¸å½“ç²¾ç¡®çš„ä¼°ç®—å€¼ï¼š
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding example can be used to get the gist of a more general procedure
    called the value iteration algorithm. This allows us to numerically calculate
    the values of the states and values of the actions of Markov decision processes
    (MDPs) with known transition probabilities and rewards. The procedure (for values
    of the states) includes the following steps:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„ç¤ºä¾‹å¯ä»¥ç”¨æ¥æ¦‚è¿°ä¸€ä¸ªæ›´ä¸€èˆ¬çš„è¿‡ç¨‹ï¼Œç§°ä¸ºå€¼è¿­ä»£ç®—æ³•ã€‚è¯¥ç®—æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ•°å€¼åœ°è®¡ç®—å…·æœ‰å·²çŸ¥è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±çš„é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰çš„çŠ¶æ€å€¼å’ŒåŠ¨ä½œå€¼ã€‚è¯¥è¿‡ç¨‹ï¼ˆé’ˆå¯¹çŠ¶æ€å€¼ï¼‰åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
- en: Initialize the values of all states, V [i], to some initial value (usually zero)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰çŠ¶æ€çš„å€¼V [i]åˆå§‹åŒ–ä¸ºæŸä¸ªåˆå§‹å€¼ï¼ˆé€šå¸¸ä¸ºé›¶ï¼‰
- en: 'For every state, s, in the MDP, perform the Bellman update:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºMDPä¸­çš„æ¯ä¸ªçŠ¶æ€sï¼Œæ‰§è¡ŒBellmanæ›´æ–°ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq17.png)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq17.png)'
- en: Repeat step 2 for some large number of steps or until changes become too small
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤æ­¥éª¤2ï¼Œè¿›è¡Œå¤§é‡çš„è¿­ä»£ï¼Œæˆ–è€…ç›´åˆ°å˜åŒ–å˜å¾—éå¸¸å°
- en: Okay, so thatâ€™s the theory. In practice, this method has certain obvious limitations.
    First of all, our state space should be discrete and small enough to perform multiple
    iterations over all states. This is not an issue for FrozenLake-4x4 and even for
    FrozenLake-8x8 (it exists in Gym as a more challenging version), but for CartPole,
    itâ€™s not totally clear what to do. Our observation for CartPole is four float
    values, which represent some physical characteristics of the system. Potentially,
    even a small difference in those values could have an influence on the stateâ€™s
    value. One of the solutions for that could be discretization of our observationâ€™s
    values; for example, we can split the observation space of CartPole into bins
    and treat every bin as an individual discrete state in space. However, this will
    create lots of practical problems, such as how large bin intervals should be and
    how much data from the environment we will need to estimate our values. I will
    address this issue in subsequent chapters, when we get to the usage of neural
    networks in Q-learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œé‚£å°±æ˜¯ç†è®ºã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸€äº›æ˜æ˜¾çš„å±€é™æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„çŠ¶æ€ç©ºé—´åº”è¯¥æ˜¯ç¦»æ•£çš„ï¼Œå¹¶ä¸”è¶³å¤Ÿå°ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¯¹æ‰€æœ‰çŠ¶æ€è¿›è¡Œå¤šæ¬¡è¿­ä»£ã€‚è¿™å¯¹äºFrozenLake-4x4ï¼Œç”šè‡³FrozenLake-8x8ï¼ˆä½œä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„ç‰ˆæœ¬å­˜åœ¨äºGymä¸­ï¼‰æ¥è¯´ä¸æ˜¯é—®é¢˜ï¼Œä½†å¯¹äºCartPoleæ¥è¯´ï¼Œåº”è¯¥å¦‚ä½•åšå¹¶ä¸å®Œå…¨æ¸…æ¥šã€‚æˆ‘ä»¬çš„CartPoleçš„è§‚å¯Ÿå€¼æ˜¯å››ä¸ªæµ®åŠ¨å€¼ï¼Œè¡¨ç¤ºç³»ç»Ÿçš„ä¸€äº›ç‰©ç†ç‰¹æ€§ã€‚æ½œåœ¨åœ°ï¼Œå³ä½¿è¿™äº›å€¼ä¹‹é—´æœ‰å¾ˆå°çš„å·®å¼‚ï¼Œä¹Ÿå¯èƒ½å½±å“çŠ¶æ€çš„å€¼ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯å¯¹æˆ‘ä»¬çš„è§‚å¯Ÿå€¼è¿›è¡Œç¦»æ•£åŒ–ï¼›ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†CartPoleçš„è§‚å¯Ÿç©ºé—´åˆ†æˆå¤šä¸ªåŒºé—´ï¼Œå¹¶å°†æ¯ä¸ªåŒºé—´å½“ä½œç©ºé—´ä¸­çš„ä¸€ä¸ªç‹¬ç«‹ç¦»æ•£çŠ¶æ€ã€‚ç„¶è€Œï¼Œè¿™ä¼šå¸¦æ¥å¾ˆå¤šå®é™…é—®é¢˜ï¼Œä¾‹å¦‚åŒºé—´çš„å¤§å°åº”è¯¥å¦‚ä½•ç¡®å®šï¼Œä¼°ç®—å€¼æ—¶éœ€è¦å¤šå°‘æ¥è‡ªç¯å¢ƒçš„æ•°æ®ã€‚æˆ‘å°†åœ¨åç»­ç« èŠ‚ä¸­è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå½“æˆ‘ä»¬æ¶‰åŠåˆ°ç¥ç»ç½‘ç»œåœ¨Q-learningä¸­çš„åº”ç”¨æ—¶ã€‚
- en: 'The second practical problem arises from the fact that we rarely know the transition
    probability for the actions and rewards matrix. Remember the interface provided
    by Gym to the agentâ€™s writer: we observe the state, decide on an action, and only
    then do we get the next observation and reward for the transition. We donâ€™t know
    (without peeking into Gymâ€™s environment code) what the probability is of getting
    into state s[1] from state s[0] by issuing action a[0]. What we do have is just
    the history from the agentâ€™s interaction with the environment. However, in Bellmanâ€™s
    update, we need both a reward for every transition and the probability of this
    transition. So, the obvious answer to this issue is to use our agentâ€™s experience
    as an estimation for both unknowns. Rewards could be used as they are. We just
    need to remember what reward we got on the transition from s[0] to s[1] using
    action a, but to estimate probabilities, we need to maintain counters for every
    tuple (s[0],s[1],a) and normalize them.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå®é™…é—®é¢˜æºäºæˆ‘ä»¬å¾ˆå°‘çŸ¥é“åŠ¨ä½œå’Œå¥–åŠ±çŸ©é˜µçš„è½¬ç§»æ¦‚ç‡ã€‚è®°ä½Gymæä¾›ç»™ä»£ç†äººç¼–å†™è€…çš„æ¥å£ï¼šæˆ‘ä»¬è§‚å¯ŸçŠ¶æ€ï¼Œå†³å®šä¸€ä¸ªåŠ¨ä½œï¼Œç„¶åæ‰è·å¾—ä¸‹ä¸€æ¬¡è§‚å¯Ÿå’Œè½¬ç§»å¥–åŠ±ã€‚æˆ‘ä»¬ä¸çŸ¥é“ï¼ˆé™¤éæŸ¥çœ‹Gymçš„ç¯å¢ƒä»£ç ï¼‰é€šè¿‡æ‰§è¡ŒåŠ¨ä½œa[0]ä»çŠ¶æ€s[0]è¿›å…¥çŠ¶æ€s[1]çš„æ¦‚ç‡æ˜¯å¤šå°‘ã€‚æˆ‘ä»¬æ‹¥æœ‰çš„åªæ˜¯ä»£ç†ä¸ç¯å¢ƒäº¤äº’çš„å†å²ã€‚ç„¶è€Œï¼Œåœ¨Bellmanæ›´æ–°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ¯ä¸ªè½¬ç§»çš„å¥–åŠ±å’Œè¯¥è½¬ç§»çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜çš„æ˜æ˜¾æ–¹æ³•æ˜¯å°†ä»£ç†çš„ç»éªŒä½œä¸ºè¿™ä¸¤ä¸ªæœªçŸ¥æ•°çš„ä¼°è®¡ã€‚å¥–åŠ±å¯ä»¥æŒ‰åŸæ ·ä½¿ç”¨ã€‚æˆ‘ä»¬åªéœ€è¦è®°ä½åœ¨ä½¿ç”¨åŠ¨ä½œaä»s[0]åˆ°s[1]çš„è½¬ç§»ä¸­è·å¾—çš„å¥–åŠ±ï¼Œä½†è¦ä¼°ç®—æ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå…ƒç»„ï¼ˆs[0]ï¼Œs[1]ï¼Œaï¼‰ä¿æŒè®¡æ•°å™¨å¹¶è¿›è¡Œå½’ä¸€åŒ–ã€‚
- en: Now that youâ€™re familiar with the theoretical background, letâ€™s look at this
    method in practice.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»ç†Ÿæ‚‰äº†ç†è®ºèƒŒæ™¯ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªæ–¹æ³•çš„å®é™…åº”ç”¨ã€‚
- en: Value iteration in practice
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„å€¼è¿­ä»£
- en: 'In this section, we will look at how the value iteration method will work for
    FrozenLake. The complete example is in Chapter05/01_frozenlake_v_iteration.py.
    The central data structures in this example are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å€¼è¿­ä»£æ–¹æ³•å¦‚ä½•åœ¨FrozenLakeä¸­å·¥ä½œã€‚å®Œæ•´çš„ç¤ºä¾‹ä½äºChapter05/01_frozenlake_v_iteration.pyä¸­ã€‚è¯¥ç¤ºä¾‹ä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„å¦‚ä¸‹ï¼š
- en: 'Reward table: A dictionary with the composite key â€œsource stateâ€ + â€œactionâ€
    + â€œtarget state.â€ The value is obtained from the immediate reward.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥–åŠ±è¡¨ï¼šä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯ç»„åˆçš„â€œæºçŠ¶æ€â€+â€œåŠ¨ä½œâ€+â€œç›®æ ‡çŠ¶æ€â€ã€‚å€¼æ˜¯ä»å³æ—¶å¥–åŠ±è·å¾—çš„ã€‚
- en: 'Transitions table: A dictionary keeping counters of the experienced transitions.
    The key is the composite â€œstateâ€ + â€œaction,â€ and the value is another dictionary
    that maps the â€œtarget stateâ€ into a count of times that we have seen it.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è½¬ç§»è¡¨ï¼šä¸€ä¸ªå­—å…¸ï¼Œè®°å½•äº†ç»å†çš„è½¬ç§»æ¬¡æ•°ã€‚é”®æ˜¯ç»„åˆçš„â€œçŠ¶æ€â€+â€œåŠ¨ä½œâ€ï¼Œå€¼æ˜¯å¦ä¸€ä¸ªå­—å…¸ï¼Œå°†â€œç›®æ ‡çŠ¶æ€â€æ˜ å°„åˆ°æˆ‘ä»¬çœ‹åˆ°å®ƒçš„æ¬¡æ•°ã€‚
- en: 'For example, if in state 0 we execute action 1 ten times, after three times,
    it will lead us to state 4 and after seven times to state 5\. Then entry with
    the key (0, 1) in this table will be a dict with the contents {4: 3, 5: 7}. We
    can use this table to estimate the probabilities of our transitions.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä¾‹å¦‚ï¼Œå¦‚æœåœ¨çŠ¶æ€0ä¸‹æ‰§è¡ŒåŠ¨ä½œ1åæ¬¡ï¼Œä¸‰æ¬¡åä¼šå°†æˆ‘ä»¬å¸¦åˆ°çŠ¶æ€4ï¼Œä¸ƒæ¬¡åå°†å¸¦åˆ°çŠ¶æ€5ã€‚é‚£ä¹ˆï¼Œè¡¨ä¸­é”®ä¸º(0, 1)çš„æ¡ç›®å°†æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå†…å®¹ä¸º{4: 3, 5:
    7}ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸ªè¡¨æ¥ä¼°è®¡æˆ‘ä»¬çš„è½¬ç§»æ¦‚ç‡ã€‚'
- en: 'Value table: A dictionary that maps a state into the calculated value of this
    state.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å€¼è¡¨ï¼šä¸€ä¸ªå­—å…¸ï¼Œå°†ä¸€ä¸ªçŠ¶æ€æ˜ å°„åˆ°è¯¥çŠ¶æ€çš„è®¡ç®—å€¼ã€‚
- en: 'The overall logic of our code is simple: in the loop, we play 100 random steps
    from the environment, populating the reward and transition tables. After those
    100 steps, we perform a value iteration loop over all states, updating our value
    table. Then we play several full episodes to check our improvements using the
    updated value table. If the average reward for those test episodes is above the
    0.8 boundary, then we stop training. During the test episodes, we also update
    our reward and transition tables to use all data from the environment.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»£ç çš„æ•´ä½“é€»è¾‘å¾ˆç®€å•ï¼šåœ¨å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ä»ç¯å¢ƒä¸­æ‰§è¡Œ100æ­¥éšæœºæ“ä½œï¼Œå¡«å……å¥–åŠ±å’Œè½¬ç§»è¡¨æ ¼ã€‚å®Œæˆè¿™100æ­¥åï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰çŠ¶æ€æ‰§è¡Œå€¼è¿­ä»£å¾ªç¯ï¼Œæ›´æ–°å€¼è¡¨ã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œå‡ ä¸ªå®Œæ•´å›åˆçš„æµ‹è¯•ï¼Œæ£€æŸ¥ä½¿ç”¨æ›´æ–°åçš„å€¼è¡¨åæˆ‘ä»¬æœ‰å“ªäº›æ”¹è¿›ã€‚å¦‚æœè¿™äº›æµ‹è¯•å›åˆçš„å¹³å‡å¥–åŠ±è¶…è¿‡0.8çš„è¾¹ç•Œå€¼ï¼Œæˆ‘ä»¬å°±åœæ­¢è®­ç»ƒã€‚åœ¨æµ‹è¯•å›åˆä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šæ›´æ–°å¥–åŠ±å’Œè½¬ç§»è¡¨æ ¼ï¼Œä»¥ä½¿ç”¨æ¥è‡ªç¯å¢ƒçš„æ‰€æœ‰æ•°æ®ã€‚
- en: 'Now letâ€™s come to the code. We first import the used packages and define constants.
    Then we define several type aliases. They are not necessary, but make our code
    more readable:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹ä»£ç ã€‚æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„åŒ…å¹¶å®šä¹‰å¸¸é‡ã€‚ç„¶åæˆ‘ä»¬å®šä¹‰å‡ ä¸ªç±»å‹åˆ«åã€‚å®ƒä»¬ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†ä½¿æˆ‘ä»¬çš„ä»£ç æ›´å…·å¯è¯»æ€§ï¼š
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For the FrozenLake environment, both observation and action spaces are of the
    Box class, so states and actions are represented by int values. We also define
    types for our reward and transition tablesâ€™ keys. For the reward table, it is
    a tuple with [State, Action, State] and for the transition table it is [State,
    Action]:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºFrozenLakeç¯å¢ƒï¼Œè§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´éƒ½å±äºBoxç±»ï¼Œå› æ­¤çŠ¶æ€å’ŒåŠ¨ä½œç”±æ•´æ•°å€¼è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜ä¸ºå¥–åŠ±è¡¨å’Œè½¬ç§»è¡¨çš„é”®å®šä¹‰äº†ç±»å‹ã€‚å¯¹äºå¥–åŠ±è¡¨ï¼Œé”®æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œæ ¼å¼ä¸º[çŠ¶æ€ï¼ŒåŠ¨ä½œï¼ŒçŠ¶æ€]ï¼Œè€Œå¯¹äºè½¬ç§»è¡¨ï¼Œé”®æ˜¯[çŠ¶æ€ï¼ŒåŠ¨ä½œ]ï¼š
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we define the Agent class, which will keep our tables and contain functions
    that we will be using in the training loop. In the class constructor, we create
    the environment that we will be using for data samples, obtain our first observation,
    and define tables for rewards, transitions, and values:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®šä¹‰äº†Agentç±»ï¼Œå®ƒå°†ä¿å­˜æˆ‘ä»¬çš„è¡¨æ ¼å¹¶åŒ…å«æˆ‘ä»¬å°†åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨çš„å‡½æ•°ã€‚åœ¨ç±»çš„æ„é€ å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç”¨äºæ•°æ®é‡‡æ ·çš„ç¯å¢ƒï¼Œè·å¾—äº†ç¬¬ä¸€ä¸ªè§‚å¯Ÿå€¼ï¼Œå¹¶ä¸ºå¥–åŠ±ã€è½¬ç§»å’Œä»·å€¼å®šä¹‰äº†è¡¨æ ¼ï¼š
- en: '[PRE3]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function play_n_random_steps is used to gather random experience from the
    environment and update the reward and transition tables. Note that we donâ€™t need
    to wait for the end of the episode to start learning; we just perform N steps
    and remember their outcomes. This is one of the differences between value iteration
    and the cross-entropy method, which can learn only on full episodes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°play_n_random_stepsç”¨äºä»ç¯å¢ƒä¸­æ”¶é›†éšæœºç»éªŒå¹¶æ›´æ–°å¥–åŠ±å’Œè½¬ç§»è¡¨ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦ç­‰åˆ°å›åˆç»“æŸæ‰èƒ½å¼€å§‹å­¦ä¹ ï¼›æˆ‘ä»¬åªæ‰§è¡ŒNæ­¥å¹¶è®°å½•å…¶ç»“æœã€‚è¿™æ˜¯å€¼è¿­ä»£å’Œäº¤å‰ç†µæ–¹æ³•ä¹‹é—´çš„ä¸€ä¸ªåŒºåˆ«ï¼Œåè€…åªèƒ½åœ¨å®Œæ•´å›åˆä¸­è¿›è¡Œå­¦ä¹ ï¼š
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next function (calc_action_value()) calculates the value of the action
    from the state using our transition, reward, and values tables. We will use it
    for two purposes: to select the best action to perform from the state and to calculate
    the new value of the state on value iteration.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå‡½æ•°ï¼ˆ`calc_action_value()`ï¼‰ä½¿ç”¨æˆ‘ä»¬çš„è½¬ç§»ã€å¥–åŠ±å’Œå€¼è¡¨æ¥è®¡ç®—ä»çŠ¶æ€å‡ºå‘çš„åŠ¨ä½œå€¼ã€‚æˆ‘ä»¬å°†å®ƒç”¨äºä¸¤ä¸ªç›®çš„ï¼šä»çŠ¶æ€ä¸­é€‰æ‹©æœ€ä½³åŠ¨ä½œï¼Œå¹¶è®¡ç®—å€¼è¿­ä»£ä¸­çš„çŠ¶æ€çš„æ–°å€¼ã€‚
- en: 'We do the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åšä»¥ä¸‹æ“ä½œï¼š
- en: We extract transition counters for the given state and action from the transition
    table. Counters in this table have a form of dict, with target states as the key
    and a count of experienced transitions as the value. We sum all counters to obtain
    the total count of times we have executed the action from the state. We will use
    this total value later to go from an individual counter to probability.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»è½¬ç§»è¡¨ä¸­æå–ç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œçš„è½¬ç§»è®¡æ•°å™¨ã€‚æ­¤è¡¨ä¸­çš„è®¡æ•°å™¨é‡‡ç”¨å­—å…¸å½¢å¼ï¼Œç›®æ ‡çŠ¶æ€ä½œä¸ºé”®ï¼Œç»å†çš„è½¬ç§»æ¬¡æ•°ä½œä¸ºå€¼ã€‚æˆ‘ä»¬å°†æ‰€æœ‰è®¡æ•°å™¨ç›¸åŠ ï¼Œä»¥è·å¾—ä»è¯¥çŠ¶æ€æ‰§è¡Œè¯¥åŠ¨ä½œçš„æ€»æ¬¡æ•°ã€‚ç¨åæˆ‘ä»¬å°†ä½¿ç”¨æ­¤æ€»å€¼ä»å•ä¸ªè®¡æ•°å™¨è½¬æ¢ä¸ºæ¦‚ç‡ã€‚
- en: Then we iterate every target state that our action has landed on and calculate
    its contribution to the total action value using the Bellman equation. This contribution
    is equal to immediate reward plus discounted value for the target state. We multiply
    this sum to the probability of this transition and add the result to the final
    action value.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬éå†æ¯ä¸ªç›®æ ‡çŠ¶æ€ï¼Œè¯¥çŠ¶æ€æ˜¯æˆ‘ä»¬çš„åŠ¨ä½œæ‰€åˆ°è¾¾çš„ï¼Œå¹¶ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—å®ƒå¯¹æ€»åŠ¨ä½œå€¼çš„è´¡çŒ®ã€‚è¿™ä¸ªè´¡çŒ®ç­‰äºå³æ—¶å¥–åŠ±åŠ ä¸Šç›®æ ‡çŠ¶æ€çš„æŠ˜æ‰£å€¼ã€‚æˆ‘ä»¬å°†æ­¤æ€»å’Œä¹˜ä»¥æ­¤è½¬ç§»çš„æ¦‚ç‡ï¼Œå¹¶å°†ç»“æœåŠ åˆ°æœ€ç»ˆçš„åŠ¨ä½œå€¼ä¸­ã€‚
- en: 'This logic is illustrated in the following diagram:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥é€»è¾‘åœ¨ä»¥ä¸‹å›¾ä¸­è¿›è¡Œäº†è¯´æ˜ï¼š
- en: '![transit[(s,a)] = {s1:c1,s2:c2} total = c1 + c2 sssaccQ1212(s,a) = tco1tal(rs1
    + Î³Vs1)+ tco2tal(rs2 + Î³Vs2) ](img/B22150_05_08.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![transit[(s,a)] = {s1:c1,s2:c2} total = c1 + c2 sssaccQ1212(s,a) = tco1tal(rs1
    + Î³Vs1)+ tco2tal(rs2 + Î³Vs2) ](img/B22150_05_08.png)'
- en: 'FigureÂ 5.8: The calculation of the stateâ€™s value'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.8ï¼šçŠ¶æ€å€¼çš„è®¡ç®—
- en: 'In the preceding diagram, we do a calculation of the value for state s and
    action a. Imagine that, during our experience, we have executed this action several
    times (c[1] + c[2]) and it ends up in one of two states, s[1] or s[2]. How many
    times we have switched to each of these states is stored in our transition table
    as dict {s[1]: c[1], s[2]: c[2]}.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨å‰é¢çš„å›¾ä¸­ï¼Œæˆ‘ä»¬å¯¹çŠ¶æ€så’ŒåŠ¨ä½œaçš„å€¼è¿›è¡Œäº†è®¡ç®—ã€‚å‡è®¾åœ¨æˆ‘ä»¬çš„ç»éªŒä¸­ï¼Œæˆ‘ä»¬å·²ç»æ‰§è¡Œäº†è¯¥åŠ¨ä½œè‹¥å¹²æ¬¡ï¼ˆc[1] + c[2]ï¼‰ï¼Œå¹¶æœ€ç»ˆè¿›å…¥äº†ä¸¤ä¸ªçŠ¶æ€ä¹‹ä¸€ï¼Œs[1]æˆ–s[2]ã€‚æˆ‘ä»¬åˆ‡æ¢åˆ°è¿™äº›çŠ¶æ€çš„æ¬¡æ•°å­˜å‚¨åœ¨æˆ‘ä»¬çš„è½¬ç§»è¡¨ä¸­ï¼Œå½¢å¼ä¸ºå­—å…¸{s[1]:
    c[1], s[2]: c[2]}ã€‚'
- en: 'Then, the approximate value for the state and action, Q(s,a), will be equal
    to the probability of every state, multiplied by the value of the state. From
    the Bellman equation, this equals the sum of the immediate reward and the discounted
    long-term state value:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼ŒçŠ¶æ€å’ŒåŠ¨ä½œçš„è¿‘ä¼¼å€¼Q(s,a)å°†ç­‰äºæ¯ä¸ªçŠ¶æ€çš„æ¦‚ç‡ï¼Œä¹˜ä»¥è¯¥çŠ¶æ€çš„å€¼ã€‚ä»è´å°”æ›¼æ–¹ç¨‹æ¥çœ‹ï¼Œè¿™ç­‰äºå³æ—¶å¥–åŠ±ä¸æŠ˜æ‰£çš„é•¿æœŸçŠ¶æ€å€¼ä¹‹å’Œï¼š
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next function uses the function that I just described to make a decision
    about the best action to take from the given state. It iterates over all possible
    actions in the environment and calculates the value for every action. The action
    with the largest value wins and is returned as the action to take. This action
    selection process is deterministic, as the play_n_random_steps() function introduces
    enough exploration. So, our agent will behave greedily in regard to our value
    approximation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå‡½æ•°ä½¿ç”¨æˆ‘åˆšæ‰æè¿°çš„å‡½æ•°æ¥å†³å®šä»ç»™å®šçŠ¶æ€é‡‡å–æœ€ä½³è¡ŒåŠ¨ã€‚å®ƒéå†ç¯å¢ƒä¸­çš„æ‰€æœ‰å¯èƒ½åŠ¨ä½œï¼Œå¹¶è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„å€¼ã€‚å€¼æœ€å¤§çš„åŠ¨ä½œèƒœå‡ºï¼Œå¹¶ä½œä¸ºæ‰§è¡Œçš„åŠ¨ä½œè¿”å›ã€‚è¿™ä¸ªåŠ¨ä½œé€‰æ‹©è¿‡ç¨‹æ˜¯ç¡®å®šæ€§çš„ï¼Œå› ä¸º`play_n_random_steps()`å‡½æ•°å¼•å…¥äº†è¶³å¤Ÿçš„æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å°†åœ¨æˆ‘ä»¬çš„å€¼è¿‘ä¼¼ä¸Šè¡¨ç°å¾—è´ªå©ªï¼š
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The play_episode() function uses select_action() to find the best action to
    take and plays one full episode using the provided environment. This function
    is used to play test episodes, during which we donâ€™t want to mess with the current
    state of the main environment used to gather random data. So, we use the second
    environment passed as an argument. The logic is very simple and should already
    be familiar to you: we just loop over states accumulating the reward for one episode:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`play_episode()`å‡½æ•°ä½¿ç”¨`select_action()`æ¥æ‰¾å‡ºæœ€ä½³çš„è¡ŒåŠ¨ï¼Œå¹¶ä½¿ç”¨æä¾›çš„ç¯å¢ƒæ’­æ”¾ä¸€ä¸ªå®Œæ•´çš„å›åˆã€‚æ­¤å‡½æ•°ç”¨äºæ’­æ”¾æµ‹è¯•å›åˆï¼Œåœ¨æ­¤æœŸé—´ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›å¹²æ‰°ç”¨äºæ”¶é›†éšæœºæ•°æ®çš„ä¸»è¦ç¯å¢ƒçš„å½“å‰çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä½œä¸ºå‚æ•°ä¼ é€’çš„ç¬¬äºŒä¸ªç¯å¢ƒã€‚é€»è¾‘éå¸¸ç®€å•ï¼Œåº”è¯¥å·²ç»å¾ˆç†Ÿæ‚‰ï¼šæˆ‘ä»¬åªéœ€éå†çŠ¶æ€å¹¶ç´¯è®¡ä¸€ä¸ªå›åˆçš„å¥–åŠ±ï¼š'
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The final method of the Agent class is our value iteration implementation and
    it is surprisingly simple, thanks to the functions we already defined. What we
    do is just loop over all states in the environment, then for every state, we calculate
    the values for the states reachable from it, obtaining candidates for the value
    of the state. Then we update the value of our current state with the maximum value
    of the action available from the state:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Agent ç±»çš„æœ€ç»ˆæ–¹æ³•æ˜¯æˆ‘ä»¬çš„ä»·å€¼è¿­ä»£å®ç°ï¼Œæ„Ÿè°¢æˆ‘ä»¬å·²ç»å®šä¹‰çš„å‡½æ•°ï¼Œè¿™ä¸€æ–¹æ³•å‡ºå¥‡çš„ç®€å•ã€‚æˆ‘ä»¬æ‰€åšçš„åªæ˜¯å¾ªç¯éå†ç¯å¢ƒä¸­çš„æ‰€æœ‰çŠ¶æ€ï¼Œç„¶åå¯¹äºæ¯ä¸ªçŠ¶æ€ï¼Œæˆ‘ä»¬è®¡ç®—ä»è¯¥çŠ¶æ€å¯è¾¾çš„çŠ¶æ€çš„å€¼ï¼Œè·å¾—çŠ¶æ€çš„ä»·å€¼å€™é€‰å€¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ç”¨å¯ä»è¯¥çŠ¶æ€é‡‡å–çš„åŠ¨ä½œçš„æœ€å¤§å€¼æ¥æ›´æ–°å½“å‰çŠ¶æ€çš„å€¼ï¼š
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Thatâ€™s all of our agentâ€™s methods, and the final piece is a training loop and
    the monitoring of the code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬ä»£ç†çš„æ‰€æœ‰æ–¹æ³•ï¼Œæœ€åä¸€éƒ¨åˆ†æ˜¯è®­ç»ƒå¾ªç¯å’Œä»£ç çš„ç›‘æ§ï¼š
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We create the environment that we will be using for testing, the Agent class
    instance, and the summary writer for TensorBoard:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºäº†ç”¨äºæµ‹è¯•çš„ç¯å¢ƒï¼ŒAgent ç±»å®ä¾‹ï¼Œä»¥åŠ TensorBoard çš„æ‘˜è¦å†™å…¥å™¨ï¼š
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The last two lines in the preceding code snippet are the key piece in the training
    loop. We first perform 100 random steps to fill our reward and transition tables
    with fresh data, and then we run value iteration over all states.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢ä»£ç ç‰‡æ®µä¸­çš„æœ€åä¸¤è¡Œæ˜¯è®­ç»ƒå¾ªç¯çš„å…³é”®éƒ¨åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆæ‰§è¡Œ 100 æ¬¡éšæœºæ­¥éª¤ï¼Œä»¥å¡«å……æˆ‘ä»¬çš„å¥–åŠ±å’Œè½¬ç§»è¡¨ï¼Œå¹¶è·å–æ–°æ•°æ®ï¼Œç„¶åå¯¹æ‰€æœ‰çŠ¶æ€æ‰§è¡Œä»·å€¼è¿­ä»£ã€‚
- en: 'The rest of the code plays test episodes using the value table as our policy,
    then writes data into TensorBoard, tracks the best average reward, and checks
    for the training loop stop condition:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä¸‹çš„ä»£ç é€šè¿‡ä½¿ç”¨ä»·å€¼è¡¨ä½œä¸ºæˆ‘ä»¬çš„ç­–ç•¥æ¥æ‰§è¡Œæµ‹è¯•å›åˆï¼Œç„¶åå°†æ•°æ®å†™å…¥ TensorBoardï¼Œè·Ÿè¸ªæœ€ä½³å¹³å‡å¥–åŠ±ï¼Œå¹¶æ£€æŸ¥è®­ç»ƒå¾ªç¯åœæ­¢æ¡ä»¶ï¼š
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Okay, letâ€™s run our program:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè®©æˆ‘ä»¬è¿è¡Œæˆ‘ä»¬çš„ç¨‹åºï¼š
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our solution is stochastic, and my experiments usually required 10 to 100 iterations
    to reach a solution, but in all cases, it took less than a second to find a good
    policy that could solve the environment in 80% of runs. If you remember, about
    an hour was needed to achieve a 60% success ratio using the cross-entropy method,
    so this is a major improvement. There are two reasons for that.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯éšæœºçš„ï¼Œæˆ‘çš„å®éªŒé€šå¸¸éœ€è¦ 10 åˆ° 100 æ¬¡è¿­ä»£æ‰èƒ½æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œéƒ½åœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…æ‰¾åˆ°ä¸€ä¸ªå¯ä»¥åœ¨ 80% çš„è¿è¡Œä¸­è§£å†³ç¯å¢ƒçš„è‰¯å¥½ç­–ç•¥ã€‚å¦‚æœä½ è¿˜è®°å¾—ï¼Œä½¿ç”¨äº¤å‰ç†µæ–¹æ³•éœ€è¦å¤§çº¦ä¸€ä¸ªå°æ—¶æ‰èƒ½è¾¾åˆ°
    60% çš„æˆåŠŸç‡ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªé‡å¤§çš„æ”¹è¿›ã€‚åŸå› æœ‰ä¸¤ä¸ªã€‚
- en: First, the stochastic outcome of our actions, plus the length of the episodes
    (6 to 10 steps on average), makes it hard for the cross-entropy method to understand
    what was done right in the episode and which step was a mistake. Value iteration
    works with individual values of the state (or action) and incorporates the probabilistic
    outcome of actions naturally by estimating probability and calculating the expected
    value. So, itâ€™s much simpler for value iteration and requires much less data from
    the environment (which is called sample efficiency in RL).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åŠ¨ä½œçš„éšæœºç»“æœï¼ŒåŠ ä¸Šå›åˆçš„é•¿åº¦ï¼ˆå¹³å‡ 6 åˆ° 10 æ­¥ï¼‰ï¼Œä½¿å¾—äº¤å‰ç†µæ–¹æ³•å¾ˆéš¾ç†è§£å›åˆä¸­åšå¯¹äº†ä»€ä¹ˆï¼Œå“ªä¸ªæ­¥éª¤æ˜¯é”™è¯¯çš„ã€‚ä»·å€¼è¿­ä»£é€šè¿‡åˆ©ç”¨æ¯ä¸ªçŠ¶æ€ï¼ˆæˆ–åŠ¨ä½œï¼‰çš„ä¸ªä½“å€¼æ¥å¤„ç†ï¼Œå¹¶é€šè¿‡ä¼°è®¡æ¦‚ç‡å’Œè®¡ç®—æœŸæœ›å€¼è‡ªç„¶åœ°ç»“åˆäº†åŠ¨ä½œçš„æ¦‚ç‡ç»“æœã€‚å› æ­¤ï¼Œä»·å€¼è¿­ä»£æ›´ä¸ºç®€å•ï¼Œä¸”å¯¹ç¯å¢ƒçš„éœ€æ±‚æ•°æ®é‡æ›´å°‘ï¼ˆè¿™åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ç§°ä¸ºæ ·æœ¬æ•ˆç‡ï¼‰ã€‚
- en: The second reason is the fact that value iteration doesnâ€™t need full episodes
    to start learning. In an extreme case, we can start updating our values just from
    a single example. However, for FrozenLake, due to the reward structure (we get
    1 only after successfully reaching the target state), we still need to have at
    least one successful episode to start learning from a useful value table, which
    may be challenging to achieve in more complex environments. For example, you can
    try switching the existing code to a larger version of FrozenLake, which has the
    name FrozenLake8x8-v1\. The larger version of FrozenLake can take from 150 to
    1,000 iterations to solve, and, according to TensorBoard charts, most of the time
    it waits for the first successful episode, then it very quickly reaches convergence.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªåŸå› æ˜¯ï¼Œä»·å€¼è¿­ä»£ä¸éœ€è¦å®Œæ•´çš„å›åˆæ‰èƒ½å¼€å§‹å­¦ä¹ ã€‚åœ¨æç«¯æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»…ä»ä¸€ä¸ªä¾‹å­å¼€å§‹æ›´æ–°æˆ‘ä»¬çš„å€¼ã€‚ç„¶è€Œï¼Œå¯¹äº FrozenLakeï¼Œç”±äºå¥–åŠ±ç»“æ„ï¼ˆæˆ‘ä»¬åªæœ‰æˆåŠŸåˆ°è¾¾ç›®æ ‡çŠ¶æ€åæ‰èƒ½è·å¾—å¥–åŠ±
    1ï¼‰ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦è‡³å°‘ä¸€ä¸ªæˆåŠŸçš„å›åˆæ¥å¼€å§‹ä»æœ‰ç”¨çš„ä»·å€¼è¡¨ä¸­å­¦ä¹ ï¼Œè¿™åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å¯èƒ½ä¼šå¾ˆéš¾å®ç°ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å°è¯•å°†ç°æœ‰ä»£ç åˆ‡æ¢åˆ°ä¸€ä¸ªæ›´å¤§çš„ FrozenLake
    ç‰ˆæœ¬ï¼Œåä¸º FrozenLake8x8-v1ã€‚FrozenLake çš„å¤§ç‰ˆæœ¬å¯èƒ½éœ€è¦ä» 150 æ¬¡åˆ° 1,000 æ¬¡è¿­ä»£æ‰èƒ½è§£å†³ï¼Œå¹¶ä¸”æ ¹æ® TensorBoard
    å›¾è¡¨ï¼Œå¤§å¤šæ•°æ—¶å€™å®ƒä¼šç­‰å¾…ç¬¬ä¸€æ¬¡æˆåŠŸçš„å›åˆï¼Œç„¶åéå¸¸å¿«é€Ÿåœ°è¾¾åˆ°æ”¶æ•›ã€‚
- en: 'The following are two charts: the first one shows reward dynamics during training
    on FrozenLake-4x4 and the second is for the 8 Ã— 8 version.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸¤ä¸ªå›¾è¡¨ï¼šç¬¬ä¸€ä¸ªæ˜¾ç¤ºäº†åœ¨ FrozenLake-4x4 ä¸Šè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±åŠ¨æ€ï¼Œç¬¬äºŒä¸ªæ˜¯ 8 Ã— 8 ç‰ˆæœ¬çš„å¥–åŠ±åŠ¨æ€ã€‚
- en: '![PIC](img/B22150_05_09.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_05_09.png)'
- en: 'FigureÂ 5.9: The reward dynamics for FrozenLake-4x4'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.9ï¼šFrozenLake-4x4 çš„å¥–åŠ±åŠ¨æ€
- en: '![PIC](img/B22150_05_10.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_05_10.png)'
- en: 'FigureÂ 5.10: The reward dynamics on FrozenLake-8x8'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.10ï¼šFrozenLake-8x8 çš„å¥–åŠ±åŠ¨æ€
- en: Now itâ€™s time to compare the code that learns the values of the states, as we
    just discussed, with the code that learns the values of the actions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™å°†å­¦ä¹ çŠ¶æ€ä»·å€¼çš„ä»£ç ä¸å­¦ä¹ åŠ¨ä½œä»·å€¼çš„ä»£ç è¿›è¡Œæ¯”è¾ƒäº†ï¼Œå°±åƒæˆ‘ä»¬åˆšåˆšè®¨è®ºçš„é‚£æ ·ã€‚
- en: Q-iteration for FrozenLake
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FrozenLake çš„ Q è¿­ä»£
- en: 'The whole example is in the Chapter05/02_frozenlake_q_iteration.py file, and
    the differences are really minor:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä¸ªä¾‹å­åœ¨ Chapter05/02_frozenlake_q_iteration.py æ–‡ä»¶ä¸­ï¼Œå·®å¼‚å®é™…ä¸Šéå¸¸å°ï¼š
- en: The most obvious change is to our value table. In the previous example, we kept
    the value of the state, so the key in the dictionary was just a state. Now we
    need to store values of the Q-function, which has two parameters, state and action,
    so the key in the value table is now a composite of (State, Action) values.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€æ˜¾è‘—çš„å˜åŒ–æ˜¯æˆ‘ä»¬çš„ä»·å€¼è¡¨ã€‚åœ¨å‰ä¸€ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¿å­˜äº†çŠ¶æ€çš„ä»·å€¼ï¼Œå› æ­¤å­—å…¸ä¸­çš„é”®åªæ˜¯ä¸€ä¸ªçŠ¶æ€ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦å­˜å‚¨ Q å‡½æ•°çš„å€¼ï¼Œå®ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼ŒçŠ¶æ€å’ŒåŠ¨ä½œï¼Œå› æ­¤ä»·å€¼è¡¨ä¸­çš„é”®ç°åœ¨æ˜¯
    (çŠ¶æ€, åŠ¨ä½œ) çš„ç»„åˆå€¼ã€‚
- en: The second difference is in our calc_action_value() function. We just donâ€™t
    need it anymore, as our action values are stored in the value table.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå·®å¼‚å‡ºç°åœ¨æˆ‘ä»¬çš„ `calc_action_value()` å‡½æ•°ä¸­ã€‚æˆ‘ä»¬ä¸å†éœ€è¦å®ƒï¼Œå› ä¸ºæˆ‘ä»¬çš„åŠ¨ä½œå€¼ç°åœ¨å­˜å‚¨åœ¨ä»·å€¼è¡¨ä¸­ã€‚
- en: Finally, the most important change in the code is in the agentâ€™s value_iteration()
    method. Before, it was just a wrapper around the calc_action_value() call, which
    did the job of Bellman approximation. Now, as this function has gone and been
    replaced by a value table, we need to do this approximation in the value_iteration()
    method.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä»£ç ä¸­æœ€é‡è¦çš„å˜åŒ–å‡ºç°åœ¨ä»£ç†çš„ `value_iteration()` æ–¹æ³•ä¸­ã€‚ä¹‹å‰ï¼Œå®ƒåªæ˜¯ `calc_action_value()` è°ƒç”¨çš„ä¸€ä¸ªåŒ…è£…å™¨ï¼Œè´Ÿè´£è´å°”æ›¼è¿‘ä¼¼çš„å·¥ä½œã€‚ç°åœ¨ï¼Œç”±äºè¿™ä¸ªå‡½æ•°å·²è¢«ç§»é™¤å¹¶ç”±ä»·å€¼è¡¨æ›¿ä»£ï¼Œæˆ‘ä»¬éœ€è¦åœ¨
    `value_iteration()` æ–¹æ³•ä¸­æ‰§è¡Œè¿™ä¸ªè¿‘ä¼¼ã€‚
- en: 'Letâ€™s look at the code. As itâ€™s almost the same, I will jump directly to the
    most interesting value_iteration() function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ã€‚ç”±äºå‡ ä¹å®Œå…¨ç›¸åŒï¼Œæˆ‘å°†ç›´æ¥è·³åˆ°æœ€æœ‰è¶£çš„ `value_iteration()` å‡½æ•°ï¼š
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code is very similar to calc_action_value() in the previous example and,
    in fact, it does almost the same thing. For the given state and action, it needs
    to calculate the value of this action using statistics about target states that
    we have reached with the action. To calculate this value, we use the Bellman equation
    and our counters, which allow us to approximate the probability of the target
    state. However, in Bellmanâ€™s equation, we have the value of the state; now, we
    need to calculate it differently.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ä¸å‰ä¸€ä¸ªä¾‹å­ä¸­çš„ `calc_action_value()` éå¸¸ç›¸ä¼¼ï¼Œå®é™…ä¸Šå®ƒåšçš„å‡ ä¹æ˜¯ç›¸åŒçš„äº‹æƒ…ã€‚å¯¹äºç»™å®šçš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œå®ƒéœ€è¦ä½¿ç”¨æˆ‘ä»¬é€šè¿‡è¯¥åŠ¨ä½œåˆ°è¾¾çš„ç›®æ ‡çŠ¶æ€çš„ç»Ÿè®¡æ•°æ®æ¥è®¡ç®—è¿™ä¸ªåŠ¨ä½œçš„ä»·å€¼ã€‚ä¸ºäº†è®¡ç®—è¿™ä¸ªå€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹å’Œæˆ‘ä»¬çš„è®¡æ•°å™¨ï¼Œè¿™äº›è®¡æ•°å™¨å…è®¸æˆ‘ä»¬è¿‘ä¼¼ç›®æ ‡çŠ¶æ€çš„æ¦‚ç‡ã€‚ç„¶è€Œï¼Œåœ¨è´å°”æ›¼æ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰çŠ¶æ€çš„å€¼ï¼›ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸åŒçš„æ–¹å¼æ¥è®¡ç®—å®ƒã€‚
- en: Before, we had it stored in the value table (as we approximated the value of
    the states), so we just took it from this table. We canâ€™t do this anymore, so
    we have to call the select_action method, which will choose for us the action
    with the largest Q-value, and then we take this Q-value as the value of the target
    state. Of course, we can implement another function that can calculate this value
    of the state, but select_action does almost everything we need, so we will reuse
    it here.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œæˆ‘ä»¬å°†å…¶å­˜å‚¨åœ¨ä»·å€¼è¡¨ä¸­ï¼ˆå› ä¸ºæˆ‘ä»¬è¿‘ä¼¼äº†çŠ¶æ€çš„ä»·å€¼ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ä»è¿™ä¸ªè¡¨ä¸­è·å–å®ƒã€‚ç°åœ¨æˆ‘ä»¬ä¸èƒ½å†è¿™æ ·åšäº†ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»è°ƒç”¨ `select_action`
    æ–¹æ³•ï¼Œå®ƒä¼šä¸ºæˆ‘ä»¬é€‰æ‹©å…·æœ‰æœ€å¤§ Q å€¼çš„åŠ¨ä½œï¼Œç„¶åæˆ‘ä»¬å°†è¿™ä¸ª Q å€¼ä½œä¸ºç›®æ ‡çŠ¶æ€çš„å€¼ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°å¦ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—è¿™ä¸ªçŠ¶æ€çš„å€¼ï¼Œä½† `select_action`
    å‡ ä¹å®Œæˆäº†æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰å·¥ä½œï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œä¼šå¤ç”¨å®ƒã€‚
- en: 'There is another piece of this example that Iâ€™d like to emphasize here. Letâ€™s
    look at our select_action method:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªæˆ‘æƒ³å¼ºè°ƒçš„ä¾‹å­ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„ `select_action` æ–¹æ³•ï¼š
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As I said, we donâ€™t have the calc_action_value method anymore; so, to select
    an action, we just iterate over the actions and look up their values in our values
    table. It could look like a minor improvement, but if you think about the data
    that we used in calc_action_value, it may become obvious why the learning of the
    Q-function is much more popular in RL than the learning of the V-function.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬ä¸å†æœ‰`calc_action_value`æ–¹æ³•ï¼›å› æ­¤ï¼Œä¸ºäº†é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œæˆ‘ä»¬åªéœ€è¦éå†æ‰€æœ‰åŠ¨ä½œå¹¶åœ¨å€¼è¡¨ä¸­æŸ¥æ‰¾å®ƒä»¬çš„å€¼ã€‚çœ‹èµ·æ¥è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå°å°çš„æ”¹è¿›ï¼Œä½†å¦‚æœä½ è€ƒè™‘æˆ‘ä»¬åœ¨`calc_action_value`ä¸­ä½¿ç”¨çš„æ•°æ®ï¼Œä½ å°±èƒ½æ˜ç™½ä¸ºä»€ä¹ˆQå‡½æ•°çš„å­¦ä¹ åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ¯”Vå‡½æ•°çš„å­¦ä¹ æ›´å—æ¬¢è¿ã€‚
- en: Our calc_action_value function uses both information about the reward and probabilities.
    Itâ€™s not a huge problem for the value iteration method, which relies on this information
    during training. However, in the next chapter, you will learn about the value
    iteration method extension, which doesnâ€™t require probability approximation, but
    just takes it from the environment samples. For such methods, this dependency
    on probability adds an extra burden for the agent. In the case of Q-learning,
    what the agent needs to make the decision is just Q-values.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„`calc_action_value`å‡½æ•°åŒæ—¶ä½¿ç”¨äº†å…³äºå¥–åŠ±å’Œæ¦‚ç‡çš„ä¿¡æ¯ã€‚å¯¹äºä»·å€¼è¿­ä»£æ–¹æ³•æ¥è¯´ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ï¼Œå› ä¸ºè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–è¿™äº›ä¿¡æ¯ã€‚ç„¶è€Œï¼Œåœ¨ä¸‹ä¸€ç« ä¸­ï¼Œä½ å°†äº†è§£ä¸€ç§ä»·å€¼è¿­ä»£æ–¹æ³•çš„æ‰©å±•ï¼Œå®ƒä¸éœ€è¦æ¦‚ç‡çš„è¿‘ä¼¼ï¼Œè€Œæ˜¯ç›´æ¥ä»ç¯å¢ƒæ ·æœ¬ä¸­è·å–ã€‚å¯¹äºè¿™ç§æ–¹æ³•ï¼Œä¾èµ–æ¦‚ç‡ä¸ºæ™ºèƒ½ä½“å¢åŠ äº†é¢å¤–çš„è´Ÿæ‹…ã€‚åœ¨Qå­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“åšå†³ç­–æ—¶æ‰€éœ€è¦çš„åªæ˜¯Qå€¼ã€‚
- en: I donâ€™t want to say that V-functions are completely useless, because they are
    an essential part of the actor-critic method, which we will talk about in Part
    3 of this book. However, in the area of value learning, Q-functions are the definite
    favorite. With regard to convergence speed, both our versions are almost identical
    (but the Q-learning version requires four times more memory for the value table).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸æƒ³è¯´Vå‡½æ•°å®Œå…¨æ²¡ç”¨ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ï¼Œè€Œæˆ‘ä»¬å°†åœ¨æœ¬ä¹¦ç¬¬3éƒ¨åˆ†è®¨è®ºè¿™ä¸€æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨ä»·å€¼å­¦ä¹ é¢†åŸŸï¼ŒQå‡½æ•°æ— ç–‘æ˜¯æ›´å—æ¬¢è¿çš„ã€‚å…³äºæ”¶æ•›é€Ÿåº¦ï¼Œæˆ‘ä»¬çš„ä¸¤ä¸ªç‰ˆæœ¬å‡ ä¹æ˜¯ç›¸åŒçš„ï¼ˆä½†Qå­¦ä¹ ç‰ˆæœ¬éœ€è¦çš„ä»·å€¼è¡¨å†…å­˜æ˜¯ä»·å€¼è¿­ä»£ç‰ˆæœ¬çš„å››å€ï¼‰ã€‚
- en: 'The following is the output of the Q-learning version and it has no major differences
    from the value iteration version:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯Qå­¦ä¹ ç‰ˆæœ¬çš„è¾“å‡ºï¼Œå®ƒä¸ä»·å€¼è¿­ä»£ç‰ˆæœ¬æ²¡æœ‰é‡å¤§åŒºåˆ«ï¼š
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'My congratulations; you have made another step toward understanding modern,
    state-of-the-art RL methods! In this chapter, you learned about some very important
    concepts that are widely used in deep RL: the value of the state, the value of
    the action, and the Bellman equation in various forms.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ï¼Œä½ åœ¨ç†è§£ç°ä»£æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¸Šåˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼åœ¨è¿™ä¸€ç« ä¸­ï¼Œä½ äº†è§£äº†ä¸€äº›æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨çš„éå¸¸é‡è¦çš„æ¦‚å¿µï¼šçŠ¶æ€å€¼ã€åŠ¨ä½œå€¼ä»¥åŠè´å°”æ›¼æ–¹ç¨‹çš„ä¸åŒå½¢å¼ã€‚
- en: We also covered the value iteration method, which is a very important building
    block in the area of Q-learning. Finally, you got to know how value iteration
    can improve our FrozenLake solution.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è®¨è®ºäº†ä»·å€¼è¿­ä»£æ–¹æ³•ï¼Œè¿™æ˜¯Qå­¦ä¹ é¢†åŸŸä¸€ä¸ªéå¸¸é‡è¦çš„æ„å»ºå—ã€‚æœ€åï¼Œä½ äº†è§£äº†ä»·å€¼è¿­ä»£å¦‚ä½•æ”¹è¿›æˆ‘ä»¬åœ¨FrozenLakeä¸­çš„è§£å†³æ–¹æ¡ˆã€‚
- en: In the next chapter, you will learn about deep Q-networks, which started the
    deep RL revolution in 2013 by beating humans on lots of Atari 2600 games.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œä½ å°†å­¦ä¹ æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ï¼Œå®ƒé€šè¿‡åœ¨2013å¹´å‡»è´¥äººç±»ç©å®¶çš„è®¸å¤šAtari 2600æ¸¸æˆï¼Œå¼€å¯äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é©å‘½ã€‚
