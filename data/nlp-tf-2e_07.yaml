- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Understanding Long Short-Term Memory Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解长短期记忆网络
- en: In this chapter, we will discuss the fundamentals behind a more advanced RNN
    variant known as **Long Short-Term Memory Networks** (**LSTMs**). Here, we will
    focus on understanding the theory behind LSTMs, so we can discuss their implementation
    in the next chapter. LSTMs are widely used in many sequential tasks (including
    stock market prediction, language modeling, and machine translation) and have
    proven to perform better than older sequential models (for example, standard RNNs),
    especially given the availability of large amounts of data. LSTMs are designed
    to avoid the problem of the vanishing gradient that we discussed in the previous
    chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论一种更高级RNN变体背后的基本原理，这种变体被称为**长短期记忆网络**（**LSTMs**）。在这里，我们将专注于理解LSTM背后的理论，以便在下一章讨论它们的实现。LSTM广泛应用于许多顺序任务（包括股市预测、语言建模和机器翻译），并且已被证明在大量数据的支持下，比旧的顺序模型（如标准RNN）表现更好。LSTM旨在避免我们在上一章讨论的梯度消失问题。
- en: The main practical limitation posed by the vanishing gradient is that it prevents
    the model from learning long-term dependencies. However, by avoiding the vanishing
    gradient problem, LSTMs have the ability to store memory for longer than ordinary
    RNNs (for hundreds of time steps). In contrast to RNNs, which only maintain a
    single hidden state, LSTMs have many more parameters as well as better control
    over what memory to store and what to discard at a given training step. For example,
    RNNs are not able to decide which memory to store and which to discard, as the
    hidden state is forced to be updated at every training step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失带来的主要实际限制是，它阻止了模型学习长期依赖关系。然而，通过避免梯度消失问题，LSTM能够存储比普通RNN更长时间的记忆（可达数百个时间步）。与只维持单一隐藏状态的RNN不同，LSTM拥有更多的参数，并能更好地控制在每个训练步骤中应该存储哪些记忆、丢弃哪些记忆。例如，RNN无法决定存储哪些记忆以及丢弃哪些记忆，因为隐藏状态在每个训练步骤都会被强制更新。
- en: Specifically, we will discuss what an LSTM is at a very high level and how the
    functionality of LSTMs allows them to store long-term dependencies. Then we will
    go into the actual underlying mathematical framework governing LSTMs and discuss
    an example to highlight why each computation matters. We will also compare LSTMs
    to vanilla RNNs and see that LSTMs have a much more sophisticated architecture
    that allows them to surpass vanilla RNNs in sequential tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将从一个非常高层次的角度讨论LSTM是什么，以及LSTM的功能如何使其能够存储长期依赖关系。然后，我们将深入探讨LSTM背后的实际数学框架，并通过一个例子来强调每个计算的重要性。我们还将比较LSTM和普通RNN，看到LSTM拥有一个更加复杂的架构，使其在顺序任务中超越普通RNN。
- en: Revisiting the problem of the vanishing gradient and illustrating it through
    an example will lead us to understand how LSTMs solve the problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回顾梯度消失问题，并通过一个示例来说明这一问题，我们将理解LSTM是如何解决该问题的。
- en: Thereafter, we will discuss several techniques that have been introduced to
    improve the predictions produced by a standard LSTM (for example, improving the
    quality/variety of generated text in a text generation task). For example, generating
    several predictions at once instead of predicting them one by one can help to
    improve the quality of generated predictions. We will also look at **bidirectional
    LSTMs (BiLSTMs)**, which are an extension to the standard LSTM, that have greater
    capabilities for capturing the patterns present in a sequence than a standard
    LSTM.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，我们将讨论为提高标准LSTM预测结果而引入的几种技术（例如，在文本生成任务中提高生成文本的质量/多样性）。例如，一次生成多个预测，而不是逐个预测，可以帮助提高生成预测的质量。我们还将介绍**双向LSTM（BiLSTMs）**，这是标准LSTM的扩展，它比标准LSTM在捕捉序列中的模式方面具有更强的能力。
- en: Finally, we will discuss two recent LSTM variants. First, we will look at **peephole
    connections**, which introduce more parameters and information to the LSTM gates,
    allowing LSTMs to perform better. Next, we will discuss **Gated Recurrent Units**
    (**GRUs**), which are gaining increasing popularity as they have a much simpler
    structure compared to standard LSTMs and also do not degrade performance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论两个近期的LSTM变种。首先，我们将介绍**窥视孔连接**，它向LSTM门引入了更多的参数和信息，从而使LSTM能够更好地执行任务。接下来，我们将讨论**门控循环单元**（**GRUs**），由于其结构比标准LSTM更简单，并且不会降低性能，GRUs正变得越来越受欢迎。
- en: 'Specifically, this chapter will cover the following main topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主要主题：
- en: Understanding Long Short-Term Memory Networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解长短期记忆网络
- en: How LSTMs solve the vanishing gradient problem
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM如何解决梯度消失问题
- en: Improving LSTMs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进LSTM
- en: Other variants of LSTMs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的其他变种
- en: Transformer models have emerged as a more powerful alternative for sequence
    learning. Transformer models deliver better performance as these models have access
    to the full history of the sequence at a given step, whereas LSTM models can only
    see the previous output at a given step. We will discuss Transformer models in
    detail in *Chapter 10*, *Transformers* and *Chapter 11*, *Image Captioning with
    Transformers*. However, it’s still worth learning about LSTMs as they have laid
    the foundation for next-generation models like Transformers. Additionally, LSTMs
    are still used to some extent, especially when working on time-series problems
    in memory-constrained environments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型已成为一种更强大的序列学习替代方案。Transformer模型提供了更好的性能，因为这些模型可以在给定的步骤访问序列的完整历史，而LSTM模型只能看到给定步骤的前一个输出。我们将在*第10章*《Transformers》和*第11章*《使用Transformer进行图像描述》中详细讨论Transformer模型。然而，学习LSTM仍然值得，因为它们为下一代模型（如Transformer）奠定了基础。此外，LSTM在某些情况下仍被使用，尤其是在内存受限环境中的时间序列问题中。
- en: Understanding Long Short-Term Memory Networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解长短期记忆网络
- en: In this section, we will first explain how an LSTM cell operates. We will see
    that in addition to the hidden states, a gating mechanism is in place to control
    information flow inside the cell.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先解释LSTM单元是如何工作的。我们将看到，除了隐藏状态外，还存在一个门控机制来控制单元内的信息流动。
- en: Then we will work through a detailed example and see how gates and states help
    at various stages of the example to achieve desired behaviors, finally leading
    to the desired output. Finally, we will compare an LSTM against a standard RNN
    to learn how an LSTM differs from a standard RNN.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过一个详细的例子来演示，看看门控和状态如何在例子中的不同阶段帮助实现期望的行为，最终得到期望的输出。最后，我们将对比LSTM与标准RNN，了解LSTM与标准RNN的区别。
- en: What is an LSTM?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是LSTM？
- en: 'LSTMs can be seen as a more complex and capable family of RNNs. Though LSTMs
    are a complicated beast, the underlying principles of LSTMs are as same as of
    RNNs; they process a sequence of items by working on one input at a time in a
    sequential order. An LSTM is mainly composed of five different components:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM可以看作是RNN家族中更复杂、更强大的成员。尽管LSTM是一个复杂的系统，但LSTM的基本原理与RNN相同；它们通过按顺序处理每次输入的序列项来处理序列。LSTM主要由五个不同的组件组成：
- en: '**Cell state**: This is the internal cell state (that is, memory) of an LSTM
    cell'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元状态**：这是LSTM单元的内部单元状态（即记忆）'
- en: '**Hidden state**: This is the external hidden state exposed to other layers
    and used to calculate predictions'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏状态**：这是暴露给其他层并用于计算预测的外部隐藏状态'
- en: '**Input gate**: This determines how much of the current input is read into
    the cell state'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：它决定当前输入有多少被读取到单元状态中'
- en: '**Forget gate**: This determines how much of the previous cell state is sent
    into the current cell state'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：它决定之前的单元状态有多少被传递到当前单元状态中'
- en: '**Output gate**: This determines how much of the cell state is output into
    the hidden state'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：它决定有多少单元状态被输出到隐藏状态中'
- en: 'We can wrap the RNN to a cell architecture as follows: the cell will output
    some state (with a nonlinear activation function) that is dependent on the previous
    cell state and the current input. However, in RNNs, the cell state is continuously
    updated with every incoming input. This behavior is quite undesirable for storing
    long-term dependencies.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将RNN包装成一个单元架构，如下所示：该单元会输出某些状态（带有非线性激活函数），该状态依赖于之前的单元状态和当前输入。然而，在RNN中，单元状态会随着每一个输入的到来不断更新。这种行为对于存储长期依赖关系来说是非常不理想的。
- en: LSTMs can decide when to add, update, or forget information stored in each neuron
    in the cell state. In other words, LSTMs are equipped with a mechanism to keep
    the cell state unchanged (if warranted for better performance), giving them the
    ability to store long-term dependencies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM可以决定何时添加、更新或忘记存储在每个神经元中的信息。换句话说，LSTM配备了一种机制，可以保持单元状态不变（如果有助于更好的性能），从而使它们能够存储长期依赖关系。
- en: 'This is achieved by introducing a gating mechanism. LSTMs possess gates for
    each operation the cell needs to perform. The gates are continuous (often sigmoid
    functions) between 0 and 1, where 0 means no information flows through the gate
    and 1 means all the information flows through the gate. An LSTM uses one such
    gate for each neuron in the cell. As explained in the introduction, these gates
    control the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过引入门控机制来实现的。LSTM 为单元需要执行的每个操作配备了门控。门控是连续的（通常是 sigmoid 函数），其值介于 0 和 1 之间，其中
    0 表示没有信息流经该门，1 表示所有信息都流经该门。每个 LSTM 单元使用一个这样的门控来控制每个神经元。正如在介绍中所解释的，这些门控控制以下内容：
- en: How much of the current input is written to the cell state (input gate)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前输入写入单元状态的多少（输入门）
- en: How much information is forgotten from the previous cell state (forget gate)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从上一个单元状态中忘记了多少信息（遗忘门）
- en: How much information is output into the final hidden state from the cell state
    (output gate)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从单元状态输出到最终隐藏状态的信息量（输出门）
- en: '*Figure 7.1* illustrates this functionality for a hypothetical scenario. Each
    gate decides how much of various data (for example, the current input, the previous
    hidden state, or the previous cell state) flows into the states (that is, the
    final hidden state or the cell state). The thickness of each line represents how
    much information is flowing from/to that gate (in some hypothetical scenarios).
    For example, in this figure, you can see that the input gate is allowing more
    from the current input than from the previous final hidden state, where the forget
    gate allows more from the previous final hidden state than from the current input:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.1* 说明了一个假设场景中的这一功能。每个门决定了各种数据（例如当前输入、上一个隐藏状态或上一个单元状态）流入状态的多少（即最终的隐藏状态或单元状态）。每条线的粗细表示从/到该门的信息流量（在某些假设场景中）。例如，在此图中，你可以看到输入门允许从当前输入流入的信息比从上一个最终隐藏状态流入的信息更多，而遗忘门则允许从上一个最终隐藏状态流入的信息比从当前输入流入的信息更多：'
- en: '![What is an LSTM?](img/B14070_07_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![什么是 LSTM？](img/B14070_07_01.png)'
- en: 'Figure 7.1: An abstract view of the data flow in an LSTM'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：LSTM 中数据流的抽象视图
- en: LSTMs in more detail
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更详细的 LSTM
- en: Here we will walk through the actual mechanism of LSTMs. We will first briefly
    discuss the overall view of an LSTM cell and then start discussing each of the
    computations crunched within an LSTM cell, along with an example of text generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍 LSTM 的实际机制。我们将首先简要讨论 LSTM 单元的整体视图，然后开始讨论 LSTM 单元中各个计算的细节，并结合一个文本生成的示例。
- en: 'As we discussed earlier, LSTMs have a gating mechanism composed of the following
    three gates:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，LSTM 具有由以下三种门控组成的门控机制：
- en: '**Input gate**: A gate that outputs values between 0 (the current input is
    not written to the cell state) and 1 (the current input is fully written to the
    cell state). Sigmoid activation is used to squash the output to between 0 and
    1.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：一个门，它输出的值介于 0（当前输入不会写入单元状态）和 1（当前输入完全写入单元状态）之间。使用 sigmoid 激活函数将输出压缩到
    0 和 1 之间。'
- en: '**Forget gate**: A sigmoidal gate that outputs values between 0 (the previous
    cell state is fully forgotten for calculating the current cell state) and 1 (the
    previous cell state is fully read in when calculating the current cell state).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：一个 sigmoid 门，它输出的值介于 0（上一个单元状态在计算当前单元状态时完全被遗忘）和 1（上一个单元状态在计算当前单元状态时完全被读取）之间。'
- en: '**Output gate**: A sigmoidal gate that outputs values between 0 (the current
    cell state is fully discarded for calculating the final state) and 1 (the current
    cell state is fully used when calculating the final hidden state).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：一个 sigmoid 门，它输出的值介于 0（当前单元状态在计算最终状态时完全被丢弃）和 1（当前单元状态在计算最终隐藏状态时完全被使用）之间。'
- en: 'This can be shown as in *Figure 7.2*. This is a very high-level diagram, and
    some details have been omitted in order to avoid clutter. We present LSTMs, both
    with loops and without loops, to improve understanding. The figure on the right-hand
    side depicts an LSTM with loops, and the one on the left-hand side shows the same
    LSTM with the loops unfolded so that no loops are present in the model:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过 *图 7.2* 展示。这是一个非常高层次的图示，为了避免杂乱，一些细节被省略了。我们展示了带环路和不带环路的 LSTM，以便于理解。右侧的图显示了一个带环路的
    LSTM，左侧的图则展示了相同的 LSTM，但环路已经展开，以便模型中没有环路：
- en: '![LSTMs in more detail](img/B14070_07_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的 LSTM](img/B14070_07_02.png)'
- en: 'Figure 7.2: An LSTM with recurrent links (that is, loops) (right) and an LSTM
    with recurrent links unfolded (left)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：带有递归链接（即，循环）的LSTM（右）和展开的递归链接的LSTM（左）
- en: Now, to get a better understanding of LSTMs, let’s consider a language modeling
    example. We will discuss the actual update rules and equations side by side with
    the example to ground our understanding of LSTMs better.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了更好地理解LSTM，让我们考虑一个语言建模的例子。我们将并排讨论实际的更新规则和方程式，以便更好地理解LSTM。
- en: 'Let’s consider an example of generating text starting from the following sentence:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个从以下句子开始生成文本的例子：
- en: '*John gave Mary a puppy.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*John 给 Mary 一只小狗。*'
- en: 'The story that we output should be about *John*, *Mary*, and the *puppy*. Let’s
    assume our LSTM outputs two sentences following the given sentence:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输出的故事应该是关于 *John*、*Mary* 和 *puppy* 的。假设我们的LSTM在给定句子后输出两个句子：
- en: '*John gave Mary a puppy. ____________________. _____________________.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*John 给 Mary 一只小狗。____________________. _____________________.*'
- en: 'The following is the output given by our LSTM:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们LSTM输出的结果：
- en: '*John gave Mary a puppy. It barks very loudly. They named it Luna.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*John 给 Mary 一只小狗。它非常大声地叫。它们给它取名为 Luna。*'
- en: 'We are still far from outputting realistic phrases such as these. However,
    LSTMs can learn relationships such as between nouns and pronouns. For example,
    *it* is related to the *puppy*, and *they* to *John* and *Mary*. Then, it should
    learn the relationship between the noun/pronoun and the verb. For example, for
    *it*, the verb should have an *s* at the end. We illustrate these relationships/dependencies
    in *Figure 7.3*. As we can see, both long-term (for example, *Luna --> puppy*)
    and short-term (for example, *It -->barks*) dependencies are present in this phrase.
    The solid arrows depict links between nouns and pronouns and dashed arrows show
    links between nouns/pronouns and verbs:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们离输出像这样的真实短语还远远不够。然而，LSTM可以学习名词和代词之间的关系。例如，*it* 与 *puppy* 相关，*they* 与 *John*
    和 *Mary* 相关。接下来，它应该学习名词/代词和动词之间的关系。例如，对于 *it*，动词的末尾应该加上 *s*。我们在*图7.3*中展示了这些关系/依赖关系。正如我们所看到的，短期（例如，*It
    --> barks*）和长期（例如，*Luna --> puppy*）的依赖关系都存在于这个短语中。实线箭头表示名词与代词之间的联系，虚线箭头表示名词/代词与动词之间的联系：
- en: '![LSTMs in more detail](img/B14070_07_03.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM更详细的介绍](img/B14070_07_03.png)'
- en: 'Figure 7.3: Sentences given and predicted by the LSTM with various relationships
    between words highlighted'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：LSTM给出的句子和预测的句子，其中单词之间的各种关系被高亮显示
- en: Now let’s consider how LSTMs, using their various operations, can model such
    relationships and dependencies to output sensible text, given a starting sentence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑LSTM如何通过其各种操作，建模这些关系和依赖，以便在给定起始句子的情况下输出合理的文本。
- en: 'The input gate (*i*[t]) takes the current input (*x*[t]) and the previous final
    hidden state (*h*[t-1]) as the input and calculates *i*[t], as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门 (*i*[t]) 接收当前输入 (*x*[t]) 和上一个最终隐藏状态 (*h*[t-1]) 作为输入，并计算 *i*[t]，计算方式如下：
- en: '![](img/B14070_07_001.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_001.png)'
- en: 'The input gate *i*[t] can be understood as the calculation performed at the
    hidden layer of a single-hidden-layer standard RNN with the sigmoidal activation.
    Remember that we calculated the hidden state of a standard RNN as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门 *i*[t] 可以理解为在标准的单隐藏层RNN的隐藏层中执行的计算，该RNN使用的是sigmoid激活函数。记住我们是通过以下方式计算标准RNN的隐藏状态的：
- en: '![](img/B14070_07_002.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_002.png)'
- en: Therefore, the calculation of *i*[t] of the LSTM looks quite analogous to the
    calculation of *h*[t] of a standard RNN, except for the change in the activation
    function and the addition of bias.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LSTM的 *i*[t] 计算与标准RNN的 *h*[t] 计算非常相似，唯一的区别在于激活函数的变化和添加了偏置项。
- en: After the calculation, a value of 0 for *i*[t] will mean that no information
    from the current input will flow to the cell state, where a value of 1 means that
    all the information from the current input will flow to the cell state.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 经过计算，*i*[t]的值为0意味着当前输入的任何信息都不会流入单元状态，而值为1则意味着所有当前输入的信息都会流入单元状态。
- en: 'Next, another value (which is called **candidate value**) is calculated as
    follows, which is fed in to calculate the current cell state later. This value
    will be treated as a potential candidate for the final cell state of this time
    step:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算另一个值（称为**候选值**），该值将被用于后续计算当前单元状态。这个值将被视为当前时间步长最终单元状态的潜在候选值：
- en: '![](img/B14070_07_003.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_003.png)'
- en: 'We can visualize these calculations in *Figure 7.4*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图7.4*中可视化这些计算：
- en: '![LSTMs in more detail](img/B14070_07_04.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 详细信息](img/B14070_07_04.png)'
- en: 'Figure 7.4: Calculation of i[t] and ![](img/B14070_07_004.png) (in bold) in
    the context of all the calculations (grayed out) that take place in an LSTM'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：i[t] 和 ![](img/B14070_07_004.png)（加粗）在所有LSTM计算（灰色部分）上下文中的计算
- en: In our example, at the very beginning of the learning, the input gate needs
    to be highly activated, as the model has no prior knowledge of the task. The first
    word that the LSTM outputs is *it*. Also, in order to do so, the LSTM must learn
    that *puppy* is also referred to as *it*. Let’s assume our LSTM has five neurons
    to store the state. We would like the LSTM to store the information that *it*
    refers to *puppy*. Another piece of information we would like the LSTM to learn
    (in a different neuron) is that the present tense verb should have an *s* at the
    end of the verb when the pronoun *it* is used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，在学习的最初阶段，输入门需要高度激活，因为模型对任务没有任何先验知识。LSTM输出的第一个词是*它*。此外，为了做到这一点，LSTM必须学会*puppy*也可以称作*它*。假设我们的LSTM有五个神经元来存储状态。我们希望LSTM存储的信息是*它*指的是*puppy*。我们希望LSTM学习的另一个信息（在不同的神经元中）是，当使用代词*它*时，动词的现在时应加上*'s'*。
- en: 'One more thing the LSTM needs to know is that the *puppy barks loud*. *Figure
    7.5* illustrates how this knowledge might be encoded in the cell state of the
    LSTM. Each circle represents a single neuron (that is, a hidden unit) of the cell
    state:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM还需要知道的一件事是*puppy barks loud*。*图 7.5*展示了这条知识如何可能被编码到LSTM的单元状态中。每个圆圈代表单元状态中的一个神经元（即一个隐藏单元）：
- en: '![LSTMs in more detail](img/B14070_07_05.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 详细信息](img/B14070_07_05.png)'
- en: 'Figure 7.5: The knowledge that should be encoded in the cell state to output
    the first sentence'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：应编码到单元状态中以输出第一个句子的知识
- en: 'With this information, we can output the first new sentence:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们可以输出第一个新的句子：
- en: '*John gave Mary a puppy. It barks very loudly.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一只小狗。它叫得非常大声。*'
- en: 'Next, the forget gate is calculated as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，忘记门的计算如下：
- en: '![](img/B14070_07_005.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_005.png)'
- en: The forget gate does the following. A value of 0 for the forget gate means that
    no information from *c*[t-1] will be passed to calculate *c*[t], and a value of
    1 means that all the information of *c*[t-1] will propagate into the calculation
    of *c*[t]. It may sound counter-intuitive, as switching on the forget gate causes
    the model to remember from the previous step and vice versa. But to respect the
    original naming conventions and design, we’ll continue to use them as they are.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记门的作用如下。忘记门的值为0意味着* c *[t-1]中的信息不会传递到计算 *c* [t]，而值为1则意味着* c *[t-1]的所有信息都会传递到
    *c* [t]的计算中。这可能听起来有些反直觉，因为打开忘记门会让模型记住前一步的内容，反之亦然。但为了尊重原始命名规范和设计，我们将继续使用它们。
- en: 'Now we will see how the forget gate helps in predicting the next sentence:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到忘记门如何帮助预测下一句话：
- en: '*They named it Luna.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们把它命名为Luna。*'
- en: 'Now, as you can see, the new relationship we are looking at is between *John*
    and *Mary* and *they*. Therefore, we no longer need information about *it* and
    how the verb *bark* behaves, as the subjects are *John* and *Mary*. We can use
    the forget gate in combination with the current subject *they* and the corresponding
    verb *named* to replace the information stored in the **Current subject** and
    **Verb for current subject** neurons (see *Figure 7.6*):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们现在关注的新的关系是*约翰*和*玛丽*以及*他们*之间的关系。因此，我们不再需要关于*它*的信息，也不再需要动词*bark*的行为，因为主语是*约翰*和*玛丽*。我们可以结合当前的主语*他们*和相应的动词*命名*来替代存储在**当前主语**和**当前主语动词**神经元中的信息（见*图
    7.6*）：
- en: '![LSTMs in more detail](img/B14070_07_06.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 详细信息](img/B14070_07_06.png)'
- en: 'Figure 7.6: The knowledge in the third neuron from the left (it --> barks)
    is replaced with new information (they --> named)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：第三个神经元（从左数）的知识（it --> barks）被新信息（they --> named）替代
- en: 'In terms of the values of weights, we illustrate this transformation in *Figure
    7.7*. We do not change the state of the neuron maintaining the *it --> puppy*
    relationship, because *puppy* appears as an object in the last sentence. This
    is done by setting weights connecting *it --> puppy* from *c*[t-1] to *c*[t] to
    1\. Then we will replace the neurons maintaining the current subject and verb
    information with a new subject and verb. This is achieved by setting the forget
    weights of *f*[t], for that neuron, to 0\. Then we will set the weights of *i*[t],
    connecting the current subject and verb to the corresponding state neurons, to
    1\. We can think of ![](img/B14070_07_004.png) (the candidate value) as a potential
    candidate for the cell’s memory, as it contains information from the current input
    *x*[t]:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 就权重值而言，我们在*图7.7*中展示了这种转化。我们不会改变保持*it --> puppy*关系的神经元的状态，因为*puppy*在最后一句话中作为一个对象出现。这是通过将连接*it
    --> puppy*的权重从*c*[t-1]到*c*[t]设置为1来完成的。然后我们将保持当前主语和动词信息的神经元替换为新的主语和动词。这是通过将该神经元的*forget*权重*f*[t]设置为0来实现的。接着，我们将连接当前主语和动词到相应状态神经元的*i*[t]权重设置为1。我们可以将![](img/B14070_07_004.png)（候选值）看作是单元记忆的潜在候选者，因为它包含了来自当前输入*x*[t]的信息：
- en: '![LSTMs in more detail](img/B14070_07_07.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的LSTM](img/B14070_07_07.png)'
- en: 'Figure 7.7: How the cell state c[t] is calculated with the previous state c[t-1]
    and the candidate value ![](img/B14070_07_004.png)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：如何使用前一个状态c[t-1]和候选值![](img/B14070_07_004.png)来计算单元状态c[t]
- en: 'The current cell state will be updated as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的单元状态将如下更新：
- en: '![](img/B14070_07_008.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_008.png)'
- en: 'In other words, the current state is a combination of the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当前状态是以下内容的组合：
- en: What information to forget/remember from the previous cell state
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记/记住来自前一个单元状态的信息
- en: What information to add/discard to the current input
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加/丢弃当前输入的信息
- en: 'Next, in *Figure 7.8*, we highlight what we have calculated so far with respect
    to all the calculations that are taking place inside an LSTM:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在*图7.8*中，我们突出显示了到目前为止我们所计算的内容，涉及LSTM内部进行的所有计算：
- en: '![LSTMs in more detail](img/B14070_07_08.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的LSTM](img/B14070_07_08.png)'
- en: 'Figure 7.8: Calculations covered so far, including i[t], f[t], ![](img/B14070_07_004.png),
    and c[t]'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：到目前为止的计算，包括i[t]、f[t]、![](img/B14070_07_004.png) 和 c[t]
- en: 'After learning the full cell state, it would look like *Figure 7.9*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 学习完整的单元状态后，它将像*图7.9*那样：
- en: '![LSTMs in more detail](img/B14070_07_09.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的LSTM](img/B14070_07_09.png)'
- en: 'Figure 7.9: The full cell state will look like this after outputting both the
    sentences'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：输出两句话后，完整的单元状态将如下所示
- en: 'Next, we will look at how the final state of the LSTM cell (*h*[t]) is computed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看LSTM单元的最终状态（*h*[t]）是如何计算的：
- en: '![](img/B14070_07_010.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_010.png)'
- en: '![](img/B14070_07_011.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_011.png)'
- en: 'In our example, we want to output the following sentence:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们希望输出以下句子：
- en: '*They named it Luna.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们给它取名为Luna。*'
- en: 'For this, we do **not** need the second to last neuron to compute this sentence,
    as it contains information about how the puppy barks, whereas this sentence is
    about the name of the puppy. Therefore, we can ignore this neuron (containing
    the *bark -> loud* relationship) during the predictions of the last sentence.
    This is exactly what *o*[t] does; it ignores the unnecessary memory and only retrieves
    the related memory from the cell state when calculating the final output of the
    LSTM cell. Also, in *Figure 7.10*, we illustrate what a full LSTM cell would look
    like at a glance:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个，我们**不需要**倒数第二个神经元来计算这个句子，因为它包含了关于小狗叫声的信息，而这个句子是关于小狗的名字。因此，在预测最后一句话时，我们可以忽略这个神经元（包含*叫声
    -> 大声*关系）。这正是*o*[t]所做的；它忽略了不必要的记忆，并且在计算LSTM单元的最终输出时，只从单元状态中提取相关的记忆。同时，在*图7.10*中，我们展示了完整的LSTM单元的概览：
- en: '![LSTMs in more detail](img/B14070_07_10.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的LSTM](img/B14070_07_10.png)'
- en: 'Figure 7.10: What the full LSTM looks like'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：完整的LSTM单元结构
- en: 'Here, we summarize all the equations relating to the operations taking place
    within an LSTM cell:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总结了与LSTM单元内操作相关的所有方程式：
- en: '![](img/B14070_07_001.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_001.png)'
- en: '![](img/B14070_07_005.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_005.png)'
- en: '![](img/B14070_07_003.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_003.png)'
- en: '![](img/B14070_07_008.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_008.png)'
- en: '![](img/B14070_07_010.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_010.png)'
- en: '![](img/B14070_07_011.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_011.png)'
- en: 'Now in the bigger picture, for a sequential learning problem, we can unroll
    the LSTM cells over time to show how they would link together so that they receive
    the previous state of the cell to compute the next state, as shown in *Figure
    7.11*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从更大的角度来看，对于一个序列学习问题，我们可以将LSTM单元在时间上展开，显示它们如何相互连接，以便接收细胞的前一个状态来计算下一个状态，如*图7.11*所示：
- en: '![LSTMs in more detail](img/B14070_07_11.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的LSTM](img/B14070_07_11.png)'
- en: 'Figure 7.11: How LSTMs would be linked over time'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：LSTM如何在时间上连接
- en: 'However, this is not adequate to do something useful. We typically use machine
    learning models to solve a task formulated as a classification or regression problem.
    As you can see, we still don’t have an output layer to output predictions. But
    if we want to use what the LSTM actually learned, we need a way to extract the
    final output from the LSTM. Therefore, we will fit a `softmax` layer (with weights
    *W*[s] and bias *b*[s]) on top of the LSTM. The final output is obtained using
    the following equation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这还不足以完成一些有用的任务。我们通常使用机器学习模型来解决形式化为分类或回归问题的任务。正如你所看到的，我们仍然没有输出层来输出预测。但是，如果我们想要使用LSTM实际学到的东西，我们需要一种方法来从LSTM中提取最终的输出。因此，我们将在LSTM上方安装一个`softmax`层（带有权重*W*[s]和偏置*b*[s]）。最终输出是通过以下公式获得的：
- en: '![](img/B14070_07_018.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_018.png)'
- en: 'Now the final picture of the LSTM with the softmax layer looks like *Figure
    7.12*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，带有softmax层的LSTM的最终图像看起来像*图7.12*：
- en: '![LSTMs in more detail](img/B14070_07_12.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![更详细的LSTM](img/B14070_07_12.png)'
- en: 'Figure 7.12: LSTMs with a softmax output layer linked over time'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：带有softmax输出层的LSTM在时间上连接
- en: With the softmax head attached to the LSTM, it can now perform a given classification
    task end to end. Now let’s compare and contrast LSTMs and the standard RNN model
    we discussed in the previous chapter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM上附加softmax头后，它现在可以执行给定的分类任务，并且能够端到端地完成。现在，让我们比较和对比LSTM和上一章讨论的标准RNN模型。
- en: How LSTMs differ from standard RNNs
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM与标准RNN的区别
- en: 'Let’s now investigate how LSTMs compare to standard RNNs. An LSTM has a more
    intricate structure compared to a standard RNN. One of the primary differences
    is that an LSTM has two different states: a cell state *c*[t] and a final hidden
    state *h*[t]. However, an RNN only has a single hidden state *h*[t]. The next
    primary difference is that, since an LSTM has three different gates, an LSTM has
    much more control over how the current input and the previous cell state are handled
    when computing the final hidden state *h*[t].'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们研究LSTM与标准RNN的比较。与标准RNN相比，LSTM具有更复杂的结构。一个主要的区别是，LSTM有两个不同的状态：细胞状态*c*[t]和最终隐藏状态*h*[t]。然而，RNN只有一个隐藏状态*h*[t]。下一个主要区别是，由于LSTM有三个不同的门，LSTM对如何在计算最终隐藏状态*h*[t]时处理当前输入和前一个细胞状态具有更大的控制权。
- en: Having the two different states is quite advantageous. With this mechanism,
    we can decouple the model’s short-term and long-term memory. In other words, even
    when the cell state is changing quickly, the final hidden state will still be
    changed more slowly. So, while the cell state is learning both short-term and
    long-term dependencies, the final hidden state can reflect either only the short-term
    dependencies, only the long-term dependencies, or both.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这两个不同的状态是非常有优势的。通过这种机制，我们可以将模型的短期记忆和长期记忆解耦。换句话说，即使细胞状态在快速变化，最终的隐藏状态仍然会更慢地变化。所以，尽管细胞状态在学习短期和长期依赖关系，但最终的隐藏状态可以仅反映短期依赖、仅反映长期依赖，或者同时反映两者。
- en: 'Next, the gating mechanism is composed of three gates: the input, forget, and
    output gates.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，门控机制由三个门组成：输入门、遗忘门和输出门。
- en: It is quite evident that this is a more principled approach (especially compared
    to the standard RNNs) that permits better control over how much the current input
    and the previous cell state contribute to the current cell state. Also, the output
    gate gives better control over how much the cell state contributes to the final
    hidden state.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这是一种更加有原则的方法（特别是与标准RNN相比），它允许更好地控制当前输入和前一个细胞状态在当前细胞状态中的贡献。此外，输出门可以更好地控制细胞状态对最终隐藏状态的贡献。
- en: 'In *Figure 7.13*, we compare schematic diagrams of a standard RNN and an LSTM
    to emphasize the difference in terms of the functionality of the two models:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.13*中，我们比较了标准RNN和LSTM的示意图，以强调这两种模型在功能上的区别：
- en: '![How LSTMs differ from standard RNNs](img/B14070_07_13.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM与标准RNN的区别](img/B14070_07_13.png)'
- en: 'Figure 7.13: A side-by-side comparison of a standard RNN and an LSTM cell'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：标准RNN与LSTM单元的并排比较
- en: In summary, with the design of maintaining two different states, an LSTM can
    learn both short-term and long-term dependencies, which helps solve the problem
    of the vanishing gradient, which we’ll discuss in the following section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，通过设计保持两种不同状态，LSTM可以学习短期和长期的依赖关系，这有助于解决我们将在下一节讨论的梯度消失问题。
- en: How LSTMs solve the vanishing gradient problem
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM如何解决梯度消失问题
- en: As we discussed earlier, even though RNNs are theoretically sound, in practice
    they suffer from a serious drawback. That is, when **Backpropagation Through Time**
    (**BPTT**) is used, the gradient diminishes quickly, which allows us to propagate
    the information of only a few time steps. Consequently, we can only store the
    information of very few time steps, thus possessing only short-term memory. This
    in turn limits the usefulness of RNNs in real-world sequential tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，尽管RNN从理论上是合理的，但在实践中它们存在一个严重缺陷。也就是说，当使用**时间反向传播**（**BPTT**）时，梯度会迅速衰减，这使得我们只能传播几个时间步的信息。因此，我们只能存储非常少的时间步信息，从而只有短期记忆。这反过来限制了RNN在实际序列任务中的使用。
- en: 'Often, useful and interesting sequential tasks (such as stock market predictions
    or language modeling) require the ability to learn and store long-term dependencies.
    Think of the following example for predicting the next word:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有用且有趣的序列任务（例如股票市场预测或语言建模）需要能够学习和存储长期依赖关系。考虑以下预测下一个单词的例子：
- en: '*John is a talented student. He is an A-grade student and plays rugby and cricket.
    All the other students envy ______.*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰是一个有天赋的学生。他是一个A等生，且会打橄榄球和板球。其他所有学生都羡慕______。*'
- en: For us, this is a very easy task. The answer would be *John*. However, for an
    RNN, this is a difficult task. We are trying to predict an answer that lies at
    the very beginning of the text. Also, to solve this task, we need a way to store
    long-term dependencies in the state of the RNN. This is exactly the type of task
    LSTMs are designed to solve.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，这是一个非常简单的任务。答案是*约翰*。然而，对于RNN来说，这是一个困难的任务。我们正在尝试预测一个位于文本开头的答案。而且，为了解决这个任务，我们需要一种方法在RNN的状态中存储长期依赖关系。这正是LSTM设计用来解决的任务。
- en: In *Chapter 6*, *Recurrent Neural Networks*, we discussed how a vanishing/exploding
    gradient can appear without any nonlinear functions present. We will now see that
    it could still happen even with the nonlinear term present. For this, we will
    derive the term ![](img/B14070_07_019.png) for a standard RNN and ![](img/B14070_07_020.png)
    for an LSTM network to understand the differences. This is the crucial term that
    causes the vanishing gradient, as we learned in the previous chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6章*，*递归神经网络*中，我们讨论了在没有任何非线性函数存在的情况下，梯度消失/爆炸是如何出现的。现在我们将看到，即使有非线性项存在，梯度消失问题仍然可能发生。为此，我们将推导出标准RNN的项！[](img/B14070_07_019.png)和LSTM网络的项！[](img/B14070_07_020.png)，以理解它们之间的差异。这是导致梯度消失的关键项，正如我们在上一章所学的那样。
- en: 'Let’s assume the hidden state is calculated as follows for a standard RNN:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假设标准RNN的隐藏状态计算如下：
- en: '![](img/B14070_07_021.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_021.png)'
- en: 'To simplify the calculations, we can ignore the current input related terms
    and focus on the recurrent part, which will give us the following equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化计算，我们可以忽略当前输入相关的项，专注于递归部分，这将给出以下方程：
- en: '![](img/B14070_07_022.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_022.png)'
- en: 'If we calculate ![](img/B14070_07_019.png) for the preceding equations, we
    will get the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算前面方程的![](img/B14070_07_019.png)，我们将得到以下结果：
- en: '![](img/B14070_07_024.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_024.png)'
- en: '![](img/B14070_07_025.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_025.png)'
- en: Now let’s see what happens when ![](img/B14070_07_026.png) or ![](img/B14070_07_027.png)
    (which will happen as learning continues). In both cases, ![](img/B14070_07_019.png)
    will start to approach 0, giving rise to the vanishing gradient. Even when ![](img/B14070_07_029.png),
    where the gradient is maximum (0.25) for sigmoid activation, when multiplied for
    many time steps, the overall gradient becomes quite small. Moreover, the term
    ![](img/B14070_07_030.png) (possibly due to bad initialization) can cause exploding
    or vanishing of the gradients as well. However, compared to the gradient vanishing
    due to ![](img/B14070_07_026.png) or ![](img/B14070_07_027.png), the gradient
    vanishing/explosion caused by the term ![](img/B14070_07_030.png) is relatively
    easy to solve (with careful initialization of weights and gradient clipping).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看当![](img/B14070_07_026.png)或![](img/B14070_07_027.png)（随着学习的进行，这将发生）时会发生什么。在这两种情况下，![](img/B14070_07_019.png)将开始趋近于0，从而产生消失梯度。即使在![](img/B14070_07_029.png)时，对于sigmoid激活函数，梯度在最大值（0.25）下，经过多次时间步长的乘积，整体梯度变得非常小。此外，项![](img/B14070_07_030.png)（可能由于初始化不当）也可能导致梯度爆炸或消失。然而，与由于![](img/B14070_07_026.png)或![](img/B14070_07_027.png)导致的梯度消失相比，项![](img/B14070_07_030.png)所导致的梯度消失/爆炸相对较容易解决（通过仔细初始化权重和梯度裁剪）。
- en: 'Now let’s look at an LSTM cell. More specifically, we’ll look at the cell state,
    given by the following equation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看LSTM单元。更具体地，我们将查看由以下方程给出的单元状态：
- en: '![](img/B14070_07_008.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_008.png)'
- en: 'This is the product of all the forget gate applications happening in the LSTM.
    However, if you calculate ![](img/B14070_07_020.png)in a similar way for LSTMs
    (that is, ignoring the ![](img/B14070_07_036.png) terms and *b*[f], as they are
    non-recurrent), we get the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是LSTM中所有忘记门应用的乘积。然而，如果你以类似的方式计算LSTM中的![](img/B14070_07_020.png)（也就是说，忽略![](img/B14070_07_036.png)项和*
    b *[f]，因为它们是非递归的），我们得到以下结果：
- en: '![](img/B14070_07_037.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_037.png)'
- en: 'In this case, though the gradient will vanish if ![](img/B14070_07_038.png),
    on the other hand, if ![](img/B14070_07_039.png), the derivative will decrease
    much slower than it would in a standard RNN. Therefore, we have one alternative,
    where the gradient will not vanish. Also, as the squashing function is used, the
    gradients will not explode due to ![](img/B14070_07_020.png) being large (which
    is the thing likely to be the cause of a gradient explosion). In addition, when
    ![](img/B14070_07_027.png), we get a maximum gradient close to 1, meaning that
    the gradients will not rapidly decrease as we saw with RNNs (when the gradient
    is at maximum). Finally, there is no term such as ![](img/B14070_07_030.png) in
    the derivation. However, derivations are trickier for ![](img/B14070_07_043.png).
    Let’s see if such terms are present in the derivation of ![](img/B14070_07_043.png).
    If you calculate the derivatives of this, you will get something of the following
    form:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，尽管当![](img/B14070_07_038.png)时梯度会消失，另一方面，如果![](img/B14070_07_039.png)，则导数将比标准RNN中的下降速度慢得多。因此，我们有一个替代方法，在这种方法下梯度不会消失。此外，随着压缩函数的使用，梯度不会由于![](img/B14070_07_020.png)过大而爆炸（这通常是导致梯度爆炸的原因）。此外，当![](img/B14070_07_027.png)时，我们获得一个接近1的最大梯度，这意味着梯度不会像我们在RNN中看到的那样迅速减小（当梯度处于最大值时）。最后，推导中没有![](img/B14070_07_030.png)这样的项。然而，对于![](img/B14070_07_043.png)的推导更加棘手。让我们看看在![](img/B14070_07_043.png)的推导中是否存在这样的项。如果你计算这个的导数，你将得到以下形式的结果：
- en: '![](img/B14070_07_045.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_045.png)'
- en: 'Once you solve this, you will get something of this form:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你解决了这个问题，你将得到以下形式的结果：
- en: '![](img/B14070_07_046.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_046.png)'
- en: 'We do not care about the content within ![](img/B14070_07_047.png) or ![](img/B14070_07_048.png),
    because no matter the value, it will be bounded by (0,1) or (-1,1). If we further
    reduce the notation by replacing the ![](img/B14070_07_047.png), ![](img/B14070_07_050.png),
    ![](img/B14070_07_051.png) and ![](img/B14070_07_052.png) terms with a common
    notation such as ![](img/B14070_07_053.png), we get something of this form:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不关心![](img/B14070_07_047.png)或![](img/B14070_07_048.png)中的内容，因为无论其值如何，它都将被限制在(0,1)或(-1,1)之间。如果我们通过将![](img/B14070_07_047.png)、![](img/B14070_07_050.png)、![](img/B14070_07_051.png)和![](img/B14070_07_052.png)项替换为公共符号，如![](img/B14070_07_053.png)，我们得到以下形式：
- en: '![](img/B14070_07_054.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_054.png)'
- en: 'Alternatively, we get the following (assuming that the outside ![](img/B14070_07_055.png)
    gets absorbed by each ![](img/B14070_07_055.png) term present within the square
    brackets):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们得到以下结果（假设外部![](img/B14070_07_055.png)被每个![](img/B14070_07_055.png)项吸收，这些项存在于方括号内）：
- en: '![](img/B14070_07_057.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_057.png)'
- en: 'This will give the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下结果：
- en: '![](img/B14070_07_058.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_058.png)'
- en: This means that though the term ![](img/B14070_07_020.png) is safe from any
    ![](img/B14070_07_030.png) terms, ![](img/B14070_07_019.png) is not. Therefore,
    we must be careful when initializing the weights of the LSTM and we should use
    gradient clipping as well.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，尽管术语![](img/B14070_07_020.png)安全地避免了任何![](img/B14070_07_030.png)术语，但![](img/B14070_07_019.png)却不是。因此，我们在初始化LSTM的权重时必须小心，并且应该使用梯度裁剪。
- en: '**Note**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: However, *h*[t] of LSTMs being unsafe from vanishing gradient is not as crucial
    as it is for RNNs, because *c*[t] still can store the long-term dependencies without
    being affected by vanishing gradient, and *h*[t] can retrieve the long-term dependencies
    from *c*[t], if required to.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*h*[t]对于LSTM来说并不像RNN那样由于梯度消失而不安全，因为*c*[t]仍然可以存储长期依赖性，而不受梯度消失的影响，并且*h*[t]如果需要的话可以从*c*[t]中检索长期依赖性。
- en: Improving LSTMs
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进LSTM
- en: 'Having a model backed up by solid foundations does not always guarantee pragmatic
    success when used in the real world. Natural language is quite complex. Sometimes
    seasoned writers struggle to produce quality content. So we can’t expect LSTMs
    to magically output meaningful, well-written content all of a sudden. Having a
    sophisticated design—allowing for better modeling of long-term dependencies in
    the data—does help, but we need more techniques during inference to produce better
    text. Therefore, numerous extensions have been developed to help LSTMs perform
    better at the prediction stage. Here we will discuss several such improvements:
    greedy sampling, beam search, using word vectors instead of a one-hot-encoded
    representation of words, and using bidirectional LSTMs. It is important to note
    that these optimization techniques are not specific to LSTMs; rather, any sequential
    model can benefit from them.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个基于坚实基础的模型并不总能在实际应用中保证切实的成功。自然语言非常复杂。有时经验丰富的作家也难以创作出高质量的内容。因此，我们不能指望LSTM突然间就能神奇地输出有意义、写得很好的内容。拥有一个复杂的设计——使得能够更好地建模数据中的长期依赖性——确实有帮助，但我们仍然需要在推理过程中使用更多的技术来生成更好的文本。因此，已经开发出了许多扩展，以帮助LSTM在预测阶段表现得更好。这里我们将讨论几种改进方法：贪心采样、束搜索、使用词向量代替词的独热编码表示、以及使用双向LSTM。需要注意的是，这些优化技术并非专门针对LSTM的；任何序列模型都可以从中受益。
- en: Greedy sampling
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贪心采样
- en: If we try to always predict the word with the highest probability, the LSTM
    will tend to produce very monotonic results. For example, due to the frequent
    occurrence of stop words (e.g. *the*), it may repeat them many times before switching
    to another word.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们总是尝试预测概率最高的单词，LSTM往往会产生非常单调的结果。例如，由于停用词（例如*the*）的频繁出现，它可能会在切换到另一个单词之前重复这些停用词很多次。
- en: One way to get around this is to use **greedy sampling**, where we pick the
    predicted best *n* and sample from that set. This helps to break the monotonic
    nature of the predictions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用**贪心采样**，即我们选择预测出的最佳*n*并从该集合中进行采样。这有助于打破预测的单调性。
- en: 'Let’s consider the first sentence of the previous example:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑前一个例子中的第一句话：
- en: '*John gave Mary a puppy.*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给玛丽一只小狗。*'
- en: 'Say, we start with the first word and want to predict the next four words:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从第一个单词开始，并希望预测接下来的四个单词：
- en: '*John ____ ____ _ _____.*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰 ____ ____ _ _____。*'
- en: 'If we attempt to choose samples deterministically, the LSTM might output something
    like the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试以确定性方式选择样本，LSTM可能会输出如下内容：
- en: '*John gave Mary gave John.*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给玛丽给约翰。*'
- en: 'However, by sampling the next word from a subset of words in the vocabulary
    (most highly probable ones), the LSTM is forced to vary the prediction and might
    output the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过从词汇表中的子集（最有可能的词）中采样下一个单词，LSTM被迫变化预测，可能会输出以下内容：
- en: '*John gave Mary a puppy.*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给玛丽一只小狗。*'
- en: 'Alternatively, it might give the following output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，它可能会给出以下输出：
- en: '*John gave puppy a puppy.*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给小狗了一只小狗。*'
- en: However, even though greedy sampling helps to add more flavor/diversity to the
    generated text, this method does not guarantee that the output will always be
    realistic, especially when outputting longer sequences of text. Now we will see
    a better search technique that actually looks ahead several steps before predictions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管贪心采样有助于为生成的文本增加更多的风味/多样性，但这种方法并不能保证输出的内容始终是现实的，尤其是在输出较长的文本序列时。现在，我们将看到一种更好的搜索技术，它实际上会在做出预测之前向前看几个步骤。
- en: Beam search
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 束搜索
- en: '**Beam search** is a way of helping with the quality of the predictions produced
    by the LSTM. In this, the predictions are found by solving a search problem. Particularly,
    we predict several steps ahead for multiple candidates at each step. This gives
    rise to a tree-like structure with candidate sequences of words (*Figure 7.14*).
    The crucial idea of beam search is to produce the *b* outputs (that is, ![](img/B14070_07_062.png))
    at once instead of a single output *y*[t]. Here, *b* is known as the **length**
    of the beam, and the *b* outputs produced are known as the **beam**. More technically,
    we pick the beam that has the highest joint probability ![](img/B14070_07_063.png)
    instead of picking the highest probable ![](img/B14070_07_064.png). We are looking
    farther into the future before making a prediction, which usually leads to better
    results.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**束搜索**是一种帮助提高LSTM生成的预测质量的方法。在这个过程中，预测是通过解决一个搜索问题来找到的。特别地，我们在每一步为多个候选词预测多个步骤。这就产生了一个树状结构，其中包含单词的候选序列（*图7.14*）。束搜索的关键思想是一次生成*b*个输出（即
    ![](img/B14070_07_062.png)），而不是生成单一的输出*y*[t]。这里，*b* 被称为束的**长度**，生成的*b*个输出被称为**束**。更技术上来说，我们选择具有最高联合概率
    ![](img/B14070_07_063.png) 的束，而不是选择具有最高概率的 ![](img/B14070_07_064.png)。我们在做出预测之前向未来预测得更远，这通常会导致更好的结果。'
- en: 'Let’s understand beam search through the previous example:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过前面的示例来理解束搜索：
- en: '*John gave Mary a puppy.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*John gave Mary a puppy.*'
- en: 'Say, we are predicting word by word and initially we have the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们逐词预测，最初我们有以下内容：
- en: '*John ____ ____ _ _____.*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*John ____ ____ _ _____.*'
- en: Let’s assume hypothetically that our LSTM produces the example sentence using
    beam search. Then the probabilities for each word might look like what we see
    in *Figure 7.14*. Let’s assume beam length *b* = *2*, and we will consider the
    *n* = *3* best candidates at each stage of the search.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的LSTM通过束搜索生成了示例句子。那么每个单词的概率可能如下所示，如*图7.14*所示。假设束长*b* = *2*，我们将在搜索的每个阶段考虑*n*
    = *3*个最佳候选词。
- en: 'The search tree would look like the following figure:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索树看起来如下图所示：
- en: '![Beam search](img/B14070_07_14.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![Beam search](img/B14070_07_14.png)'
- en: 'Figure 7.14: The search space of beam search for a b=2 and n=3'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14：束搜索的搜索空间，b=2，n=3
- en: 'We start with the word *John* and get the probabilities for all the words in
    the vocabulary. In our example, as *n* = *3*, we pick the best three candidates
    for the next level of the tree: *gave*, *Mary*, and *puppy*. (Note that these
    might not be the candidates found by an actual LSTM and are only used as an example.)
    Then from these selected candidates, the next level of the tree is grown. And
    from that, we will pick the best three candidates, and the search will repeat
    until we reach a depth of *b* in the tree.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从单词*John*开始，并获取词汇表中所有单词的概率。在我们的示例中，由于*n* = *3*，我们为树的下一层选择最佳的三个候选词：*gave*、*Mary*
    和 *puppy*。（请注意，这些可能不是实际LSTM找到的候选词，仅用于示例。）然后从这些选定的候选词中，树的下一层会继续扩展。接着，我们将从中选出最好的三个候选词，搜索会重复，直到我们达到树的深度*b*。
- en: The path that gives the highest joint probability (that is, ![](img/B14070_07_065.png))
    is highlighted with heavier arrows. Also, this is a better prediction mechanism,
    as it would return a higher probability, or a reward, for a phrase such as *John
    gave Mary* than *John Mary John* or *John John gave*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 给出最高联合概率的路径（即 ![](img/B14070_07_065.png)）用较粗的箭头突出显示。此外，这是一种更好的预测机制，因为它会为像*John
    gave Mary*这样的短语返回更高的概率或奖励，而不是*John Mary John*或*John John gave*。
- en: Note that the outputs produced by both greedy sampling and beam search are identical
    in our example, which is a simple sentence containing five words. However, this
    is not the case when we scale this to output a small paragraph. Then the results
    produced by beam search will be much more realistic and meaningful than the ones
    produced by greedy sampling.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们的示例中，通过贪婪采样和束搜索生成的输出是相同的，这是一个包含五个单词的简单句子。然而，当我们将其扩展到输出一小段文章时，情况就不一样了。那时，束搜索生成的结果将比贪婪采样生成的结果更具现实性和意义。
- en: Using word vectors
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词向量
- en: 'Another popular way of improving the performance of LSTMs is to use word vectors
    instead of using one-hot-encoded vectors as the input to the LSTM. Let’s understand
    the value of this method through an example. Let’s assume that we want to generate
    text starting from some random word. In our case, it would be the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提高LSTM性能的另一种流行方法是使用词向量，而不是使用独热编码向量作为LSTM的输入。我们通过一个例子来理解这种方法的价值。假设我们想要从某个随机词开始生成文本。在我们的案例中，它将是以下内容：
- en: '*John ____ ____ _ _____.*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰____ ____ _ _____.*'
- en: 'We have already trained our LSTM on the following sentences:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在以下句子上训练过我们的LSTM：
- en: '*John gave Mary a puppy. Mary has sent Bob a kitten.*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一只小狗。玛丽给鲍勃送了一只小猫。*'
- en: 'Let’s also assume that we have the word vectors positioned as shown in *Figure
    7.15*. Remember that semantically similar words will have vectors placed close
    to each other:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有如*图7.15*所示的位置的词向量。记住，语义相似的词会有相近的词向量：
- en: '![Using word vectors](img/B14070_07_15.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![使用词向量](img/B14070_07_15.png)'
- en: 'Figure 7.15: Assumed word vectors’ topology in two-dimensional space'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：假定词向量在二维空间中的拓扑
- en: 'The word embeddings of these words, in their numerical form, might look like
    the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词的词嵌入，在其数字形式下，可能看起来像如下：
- en: '*kitten:* [0.5, 0.3, 0.2]'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*小猫:* [0.5, 0.3, 0.2]'
- en: '*puppy:* [0.49, 0.31, 0.25]'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*小狗:* [0.49, 0.31, 0.25]'
- en: '*gave:* [0.1, 0.8, 0.9]'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*给:* [0.1, 0.8, 0.9]'
- en: 'It can be seen that *distance(kitten, puppy) < distance(kitten, gave)*. However,
    if we use one-hot encoding, they would be as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，*distance(小猫, 小狗) < distance(小猫, 给)*。然而，如果我们使用独热编码，它们将变成如下：
- en: '*kitten:* [ 1, 0, 0, …]'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*小猫:* [ 1, 0, 0, …]'
- en: '*puppy:* [0, 1, 0, …]'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*小狗:* [0, 1, 0, …]'
- en: '*gave:* [0, 0, 1, …]'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*给:* [0, 0, 1, …]'
- en: Then, *distance(kitten, puppy) = distance(kitten, gave)*. As we can already
    see, one-hot-encoded vectors do not capture the proper relationship between words
    and see all the words are equally distanced from each other. However, word vectors
    are capable of capturing such relationships and are more suitable to represent
    text for machine learning models.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*distance(小猫, 小狗) = distance(小猫, 给)*。正如我们已经看到的，独热编码向量不能捕捉词与词之间的适当关系，它们将所有词视为相等的距离。然而，词向量能够捕捉这些关系，更适合用于机器学习模型中的文本表示。
- en: 'Using word vectors, the LSTM will learn to exploit relationships between words
    better. For example, with word vectors, LSTM will learn the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词向量，LSTM将更好地学习词与词之间的关系。例如，使用词向量时，LSTM将学到以下内容：
- en: '*John gave Mary a kitten.*'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一只小猫。*'
- en: 'This is quite close to the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这与以下内容非常接近：
- en: '*John gave Mary a puppy.*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一只小狗。*'
- en: 'Also, it is quite different from the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它与以下内容有很大的不同：
- en: '*John gave Mary a gave.*'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一个给。*'
- en: However, this would not be the case if one-hot-encoded vectors are used.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果使用独热编码向量，情况就不一样了。
- en: Bidirectional LSTMs (BiLSTMs)
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向LSTM（BiLSTM）
- en: 'Making LSTMs bidirectional is another way of improving the quality of the predictions
    of an LSTM. By this we mean training the LSTM with text read in both directions:
    from the beginning to the end and the end to the beginning. So far during the
    training of the LSTM, we would create a dataset as follows.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使LSTM变为双向LSTM是提高LSTM预测质量的另一种方法。这里的意思是用从开始到结束和从结束到开始的文本来训练LSTM。到目前为止，在训练LSTM时，我们将创建如下的数据集：
- en: 'Consider the following two sentences:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两个句子：
- en: '*John gave Mary a _____. It barks very loudly.*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一个_____. 它叫得非常大声。*'
- en: At this stage, there is data missing in one of the sentences that we would want
    our LSTM to fill sensibly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，有一个句子中缺失了数据，我们希望LSTM能合理地填充这个缺失部分。
- en: 'If we read from the beginning up to the missing word, it would be as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从句子的开头读到缺失单词，它将是如下：
- en: '*John gave Mary a _____.*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一个_____.*'
- en: 'This does not provide enough information about the context of the missing word
    to fill the word properly. However, if we read in both directions, it would be
    the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这并没有提供足够的信息来确定缺失单词的上下文。然而，如果我们从两个方向阅读，它将变成以下内容：
- en: '*John gave Mary a _____.*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰给了玛丽一个_____.*'
- en: '*_____. It barks very loudly.*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*_____. 它叫得非常大声。*'
- en: If we created data with both these pieces, it is adequate to predict that the
    missing word should be something like *dog* or *puppy*. Therefore, certain problems
    can benefit significantly from reading data from both sides. BiLSTMs also help
    in multilingual problems as different languages can have very different sentence
    structures.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们同时创建了这两部分数据，那么可以预测缺失的单词应该是像*dog*或*puppy*这样的词。因此，某些问题可以从双向读取数据中显著受益。BiLSTM还帮助解决多语言问题，因为不同语言可能有非常不同的句子结构。
- en: Another application of BiLSTMs is neural machine translation, where we translate
    a sentence of a source language to a target language. As there is no specific
    alignment between the translation of one language to another, having access to
    both sides of a given token in the source language can greatly help to understand
    the context better, thus producing better translations. As an example, consider
    a translation task of translating Filipino to English. In Filipino, sentences
    are usually written having *verb-object-subject* in that order, whereas in English,
    it is *subject-verb-object*. In this translation task, it will be extremely helpful
    to read sentences both forward and backward to make a good translation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: BiLSTM的另一个应用是神经机器翻译，其中我们将源语言的句子翻译成目标语言。由于不同语言之间没有具体的一对一对齐关系，能够访问源语言中给定词汇的前后信息可以极大地帮助更好地理解上下文，从而生成更好的翻译。例如，考虑将菲律宾语翻译成英语。在菲律宾语中，句子的顺序通常是*动词-宾语-主语*，而在英语中，则是*主语-动词-宾语*。在这个翻译任务中，前后双向阅读句子将极大地帮助生成良好的翻译。
- en: A BiLSTM is essentially two separate LSTM networks. One network learns data
    from the beginning to the end, and the other network learns data from the end
    to the beginning. In *Figure 7.16*, we illustrate the architecture of a BiLSTM
    network.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: BiLSTM本质上是两个独立的LSTM网络。一个网络从头到尾学习数据，另一个网络从尾到头学习数据。在*图7.16*中，我们展示了BiLSTM网络的架构。
- en: 'Training occurs in two phases. First, the solid-colored network is trained
    with data created by reading the text from the beginning to the end. This network
    represents the normal training procedure used for standard LSTMs. Secondly, the
    dashed network is trained with data generated by reading the text in the reversed
    direction. Then, at the inference phase, we use both the solid and dashed states’
    information (by concatenating both states and creating a vector) to predict the
    missing word:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分为两个阶段。首先，实线网络使用从头到尾读取文本生成的数据进行训练。这个网络代表了标准LSTM的常规训练过程。其次，虚线网络使用从后向前读取文本生成的数据进行训练。然后，在推理阶段，我们通过连接实线和虚线的状态信息（并生成一个向量）来预测缺失的单词：
- en: '![Bidirectional LSTMs (BiLSTM)](img/B14070_07_16.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![双向LSTM（BiLSTM）](img/B14070_07_16.png)'
- en: 'Figure 7.16: A schematic diagram of a BiLSTM'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：双向LSTM的示意图
- en: In this section, we discussed several different ways to improve the performance
    of LSTM models. This involved employing better prediction strategies to introduce
    structural changes such as word vectors and BiLSTMs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了几种不同的方法来提高LSTM模型的性能。这包括采用更好的预测策略，引入结构性变化，如词向量和双向LSTM（BiLSTM）。
- en: Other variants of LSTMs
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM的其他变体
- en: 'Though we will mainly focus on the standard LSTM architecture, many variants
    have emerged that either simplify the complex architecture found in standard LSTMs,
    produce better performance, or both. We will look at two variants that introduce
    structural modifications to the cell architecture of LSTMs: peephole connections
    and GRUs.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将主要关注标准LSTM架构，但许多变体已经出现，它们要么简化了标准LSTM中的复杂架构，要么提高了性能，或者两者兼有。我们将探讨两种引入结构性修改的LSTM变体：窥视连接（peephole
    connections）和GRU。
- en: Peephole connections
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窥视连接
- en: '**Peephole connections** allow gates to see not only the current input and
    the previous final hidden state, but also the previous cell state. This increases
    the number of weights in the LSTM cell. Having such connections has been shown
    to produce better results. The equations would look like these:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**窥视连接**允许门不仅查看当前输入和先前的最终隐藏状态，还可以查看先前的细胞状态。这增加了LSTM单元中的权重数量。已经证明，拥有这种连接可以产生更好的结果。方程式将如下所示：'
- en: '![](img/B14070_07_066.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_066.png)'
- en: '![](img/B14070_07_003.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_003.png)'
- en: '![](img/B14070_07_068.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_068.png)'
- en: '![](img/B14070_07_008.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_008.png)'
- en: '![](img/B14070_07_070.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_070.png)'
- en: '![](img/B14070_07_011.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_011.png)'
- en: Let’s briefly look at how this helps the LSTM perform better. So far, the gates
    see the current input and final hidden state but not the cell state. However,
    in this configuration, if the output gate is close to zero, even when the cell
    state contains information crucial to better performance, the final hidden state
    will be close to zero. Thus, the gates will not take the hidden state into consideration
    during calculation. Including the cell state directly in the gate calculation
    equation allows more control over the cell state, and it can perform well even
    in situations where the output gate is close to zero.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要看看这如何帮助LSTM表现得更好。到目前为止，门控机制只能看到当前输入和最终隐藏状态，但看不到单元状态。然而，在这种配置下，如果输出门接近零，即使单元状态包含对更好性能至关重要的信息，最终的隐藏状态也会接近零。因此，门控机制在计算时不会考虑隐藏状态。直接将单元状态包括在门控计算方程中，可以对单元状态进行更多控制，即使在输出门接近零的情况下，它也能表现良好。
- en: 'We illustrate the architecture of the LSTM with peephole connections in *Figure
    7.17*. We have grayed all the existing connections in a standard LSTM and the
    newly added connections are shown in black:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图7.17*中展示了具有窥视连接的LSTM架构。我们已将标准LSTM中所有现有的连接设为灰色，新增的连接则用黑色表示：
- en: '![Peephole connections](img/B14070_07_17.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![Peephole connections](img/B14070_07_17.png)'
- en: 'Figure 7.17: An LSTM with peephole connections (the peephole connections are
    shown in black while the other connections are grayed out)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：具有窥视连接的LSTM（窥视连接用黑色表示，其他连接用灰色表示）
- en: Gated Recurrent Units
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控递归单元
- en: '**GRUs** can be seen as a simplification of the standard LSTM architecture.
    As we have seen already, an LSTM has three different gates and two different states.
    This alone requires a large number of parameters even for a small state size.
    Therefore, scientists have investigated ways to reduce the number of parameters.
    GRUs are a result of one such endeavor.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRU**可以看作是标准LSTM架构的简化版。正如我们之前所见，LSTM有三个不同的门和两个不同的状态。仅这一点就需要大量的参数，即使对于一个较小的状态尺寸来说也是如此。因此，科学家们研究了减少参数数量的方法。GRU是其中一个成果。'
- en: There are several main differences in GRUs compared to LSTMs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: GRU与LSTM相比，有几个主要的区别。
- en: First, GRUs combine two states, the cell state and the final hidden state, into
    a single hidden state *h*[t]. Now, as a side effect of this simple modification
    of not having two different states, we can get rid of the output gate. Remember,
    the output gate was merely deciding how much of the cell state is read into the
    final hidden state. This operation greatly reduces the number of parameters in
    the cell.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，GRU将两个状态，即单元状态和最终隐藏状态，合并成一个单一的隐藏状态*h*[t]。现在，由于这个简单的修改没有两个不同的状态，我们可以去除输出门。记住，输出门仅仅是决定有多少单元状态被读取到最终隐藏状态中。这个操作大大减少了单元中的参数数量。
- en: 'Next, GRUs introduce a reset gate that, when it’s close to 1, takes the full
    previous state information in when computing the current state. Also, when the
    reset gate is close to 0, it ignores the previous state when computing the current
    state:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，GRU引入了一个重置门，当它接近1时，在计算当前状态时会完全采纳前一个状态的信息。而当重置门接近0时，它会忽略前一个状态，只关注当前状态的计算：
- en: '![](img/B14070_07_072.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_072.png)'
- en: '![](img/B14070_07_073.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_073.png)'
- en: 'Then, GRUs combine the input and forget gates into one *update gate*. The standard
    LSTM has two gates known as the input and forget gates. The input gate decides
    how much of the current input is read into the cell state, and the forget gate
    determines how much of the previous cell state is read into the current cell state.
    Mathematically, this can be shown as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，GRU将输入门和遗忘门合并成一个*更新门*。标准的LSTM有两个门，分别是输入门和遗忘门。输入门决定当前输入有多少被读入到单元状态中，而遗忘门决定前一个单元状态有多少被读入到当前单元状态中。数学上，这可以表示如下：
- en: '![](img/B14070_07_001.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_001.png)'
- en: '![](img/B14070_07_005.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_005.png)'
- en: 'GRUs combine these two operations into a single gate known as the update gate.
    If the update gate is 0, then the full state information of the previous cell
    state is pushed into the current cell state, where none of the current input is
    read into the state. If the update gate is 1, then all of the current input is
    read into the current cell state and none of the previous cell state is propagated
    into the current cell state. In other words, the input gate *i*[t] becomes inverse
    of the forget gate, that is, ![](img/B14070_07_076.png):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: GRU将这两种操作合并成一个单一的门控操作，称为更新门。如果更新门为0，则将前一单元状态的全部状态信息传递到当前单元状态，此时不会将当前输入读入状态。如果更新门为1，则所有当前输入都会读入当前单元状态，且前一单元状态不会传递到当前单元状态。换句话说，输入门*i*[t]变成了遗忘门的反向，即！[](img/B14070_07_076.png)：
- en: '![](img/B14070_07_077.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_077.png)'
- en: '![](img/B14070_07_078.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_078.png)'
- en: 'Now let’s bring all the equations into one place. The GRU computations would
    look like this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将所有的公式整理到一起。GRU的计算过程如下所示：
- en: '![](img/B14070_07_072.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_072.png)'
- en: '![](img/B14070_07_073.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_073.png)'
- en: '![](img/B14070_07_077.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_077.png)'
- en: '![](img/B14070_07_078.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_07_078.png)'
- en: 'This is much more compact than LSTMs. In *Figure 7.18*, we can visualize a
    GRU cell (left) and an LSTM cell (right) side by side:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这比LSTM更加简洁。在*图7.18*中，我们可以将GRU单元（左）和LSTM单元（右）并排展示：
- en: '![Gated Recurrent Units](img/B14070_07_18.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![门控递归单元](img/B14070_07_18.png)'
- en: 'Figure 7.18: A side-by-side comparison of a GRU (left) and the standard LSTM
    (right)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18：GRU（左）和标准LSTM（右）的并排比较
- en: 'In this section, we learned two variants of the LSTM: LSTMs with peepholes
    and GRUs. GRUs have become a popular choice over LSTMs, due to their simplicity
    and on-par performance with more complex LSTMs.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了LSTM的两种变体：带窥视孔的LSTM和GRU。由于其简洁性以及与更复杂的LSTM相当的性能，GRU已经成为比LSTM更受欢迎的选择。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about LSTM networks. First, we discussed what an
    LSTM is and its high-level architecture. We also delved into the detailed computations
    that take place in an LSTM and discussed the computations through an example.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了LSTM网络。首先，我们讨论了LSTM是什么及其高层次的架构。我们还深入探讨了LSTM中的详细计算，并通过一个例子讨论了这些计算。
- en: 'We saw that an LSTM is composed mainly of five different things:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，LSTM主要由五个不同的部分组成：
- en: '**Cell state**: The internal cell state of an LSTM cell'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元状态**：LSTM单元的内部单元状态'
- en: '**Hidden state**: The external hidden state used to calculate predictions'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏状态**：用于计算预测的外部隐藏状态'
- en: '**Input gate**: This determines how much of the current input is read into
    the cell state'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：决定多少当前输入被读取到单元状态中'
- en: '**Forget gate**: This determines how much of the previous cell state is sent
    into the current cell state'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：决定多少前一单元状态被发送到当前单元状态'
- en: '**Output gate**: This determines how much of the cell state is output into
    the hidden state'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：决定多少单元状态被输出到隐藏状态中'
- en: Having such a complex structure allows LSTMs to capture both short-term and
    long-term dependencies quite well.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有如此复杂的结构，使得LSTM能够很好地捕捉短期和长期依赖。
- en: We compared LSTMs to vanilla RNNs and saw that LSTMs are actually capable of
    learning long-term dependencies as an inherent part of their structure, whereas
    RNNs can fail to learn long-term dependencies. Afterward, we discussed how LSTMs
    solve the vanishing gradient with its complex structure.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LSTM与普通RNN进行了比较，发现LSTM实际上能够学习长期依赖，这是其结构的固有部分，而RNN则可能无法学习长期依赖。之后，我们讨论了LSTM如何通过其复杂的结构解决消失梯度问题。
- en: Then we discussed several extensions that improve the performance of LSTMs.
    First, a very simple technique we called greedy sampling, in which, instead of
    always outputting the best candidate, we randomly sample a prediction from a set
    of best candidates. We saw that this improves the diversity of the generated text.
    After that, we looked at a more complex search technique called beam search. With
    this, instead of making a prediction for a single time step into the future, we
    predict several time steps into the future and pick the candidates that produce
    the best joint probability. Another improvement involved seeing how word vectors
    can help improve the quality of the predictions of an LSTM. Using word vectors,
    LSTMs can learn more effectively to replace semantically similar words during
    prediction (for example, instead of outputting *dog*, LSTM might output *cat*),
    leading to more realism and correctness of the generated text. The final extension
    we considered was BiLSTMs or bidirectional LSTMs. A popular application of BiLSTMs
    is filling missing words in a phrase. BiLSTMs read the text in both directions,
    from the beginning to the end and the end to the beginning. This gives more context
    as we are looking at both the past and future before predicting.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们讨论了几种改进 LSTM 性能的扩展。首先是一个非常简单的技术，叫做贪婪采样，在这种方法中，我们并非总是输出最佳候选，而是从一组最佳候选中随机采样一个预测。我们看到这提高了生成文本的多样性。之后，我们看了一个更复杂的搜索技术，叫做束搜索。使用束搜索时，我们不是仅预测单个时间步的未来，而是预测多个时间步的未来，并选择产生最佳联合概率的候选。另一个改进是观察词向量如何帮助提升
    LSTM 的预测质量。通过使用词向量，LSTM 能更有效地学习在预测时替换语义相似的词（例如，LSTM 可能会输出 *cat* 代替 *dog*），从而使生成的文本更加真实和准确。最后，我们考虑的扩展是双向
    LSTM（BiLSTM）。BiLSTM 的一个流行应用是填补短语中的缺失词。BiLSTM 会从两个方向读取文本：从前往后和从后往前。这提供了更多的上下文信息，因为我们在做出预测前，既看到了过去的内容，也看到了未来的内容。
- en: 'Finally, we discussed two variants of vanilla LSTMs: peephole connections and
    GRUs. Vanilla LSTMs, when calculating the gates, only look at the current input
    and the hidden state. With peephole connections, we make the gate computations
    dependent on all: the current input, and the hidden and cell states.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了普通 LSTM 的两种变体：窥视孔连接和 GRU。普通 LSTM 在计算门时，只查看当前输入和隐藏状态。而使用窥视孔连接时，门的计算依赖于所有内容：当前输入、隐藏状态和细胞状态。
- en: GRUs are a much more elegant variant of vanilla LSTMs that simplify LSTMs without
    compromising on performance. GRUs have only two gates and a single state, whereas
    vanilla LSTMs have three gates and two states.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 是一种比普通 LSTM 更加优雅的变体，它简化了 LSTM，同时没有牺牲性能。GRU 只有两个门和一个状态，而普通的 LSTM 有三个门和两个状态。
- en: In the next chapter, we will see all these different architectures in action
    with implementations of each of them and see how well they perform in text generation
    tasks.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到这些不同的架构在实际应用中的表现，展示每种架构的实现，并观察它们在文本生成任务中的表现如何。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起学习，和超过 1000 名成员一起进步，访问链接：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
