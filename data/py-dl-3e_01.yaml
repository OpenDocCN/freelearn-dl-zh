- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Machine Learning – an Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习——简介
- en: '**Machine learning** (**ML**) techniques are being applied in a variety of
    fields, and data scientists are being sought after in many different industries.
    With ML, we identify the processes through which we gain knowledge that is not
    readily apparent from data to make decisions. Applications of ML techniques may
    vary greatly and are found in disciplines as diverse as medicine, finance, and
    advertising.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）技术正在广泛应用于各个领域，数据科学家在许多不同的行业中需求量很大。通过机器学习，我们识别出从数据中获得不易察觉的知识以做出决策的过程。机器学习技术的应用领域各不相同，涵盖了医学、金融、广告等多个学科。'
- en: In this chapter, we’ll present different ML approaches, techniques, and some
    of their applications to real-world problems, and we’ll also introduce one of
    the major open source packages available in Python for ML, PyTorch. This will
    lay the foundation for later chapters in which we’ll focus on a particular type
    of ML approach using **neural networks** (**NNs**). In particular, we will focus
    on **deep learning** (**DL**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍不同的机器学习方法、技术及其在实际问题中的一些应用，同时也将介绍Python中一个主要的开源机器学习包——PyTorch。这为后续章节奠定了基础，在这些章节中，我们将专注于使用**神经网络**（**NNs**）的某种类型的机器学习方法，特别是我们将重点讲解**深度学习**（**DL**）。
- en: DL makes use of more advanced NNs than those used previously. This is not only
    a result of recent developments in the theory but also advancements in computer
    hardware. This chapter will summarize what ML is and what it can do, preparing
    you to better understand how DL differentiates itself from popular traditional
    ML techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）采用了比以往更先进的神经网络（NN）。这不仅是近年来理论发展的结果，也是计算机硬件进步的体现。本章将总结机器学习（ML）的基本概念及其应用，帮助你更好地理解深度学习（DL）如何区别于传统的流行机器学习技术。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主要内容：
- en: Introduction to ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Different ML approaches
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的机器学习方法
- en: Neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Introduction to PyTorch
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch简介
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python and PyTorch. If you
    don’t have an environment set up with these tools, fret not – the example is available
    as a Jupyter notebook on Google Colab. You can find the code examples in the book’s
    GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter01](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter01).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python和PyTorch实现示例。如果你还没有配置好相关环境，也不必担心——示例代码已作为Jupyter笔记本提供在Google
    Colab上。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter01](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter01)。
- en: Introduction to ML
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: ML is often associated with terms such as **big data** and **artificial intelligence**
    (**AI**). However, both are quite different from ML. To understand what ML is
    and why it’s useful, it’s important to understand what big data is and how ML
    applies to it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）通常与**大数据**和**人工智能**（**AI**）等术语相关联。然而，这两者与机器学习是完全不同的。要理解机器学习是什么以及它为何有用，首先需要了解大数据是什么，以及机器学习如何应用于大数据。
- en: Big data is a term used to describe huge datasets that are created as the result
    of large increases in data that is gathered and stored. For example, this may
    be through cameras, sensors, or internet social sites.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据是一个用来描述由大量数据积累和存储所产生的大型数据集的术语。例如，这些数据可能来源于摄像头、传感器或社交网络网站。
- en: How much data do we create daily?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每天创造多少数据？
- en: It’s estimated that Google alone processes over 20 petabytes of information
    per day, and this number is only going to increase. A few years ago, Forbes estimated
    that every day, 2.5 quintillion bytes of data are created and that 90% of all
    the data in the world has been created in the last two years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 据估计，仅谷歌每天处理的信息量就超过20PB，而且这个数字还在不断增加。几年前，福布斯估计，每天创造的数据量为2.5万亿字节，而全球90%的数据是在过去两年内生成的。
- en: ([https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/](https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/))
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/](https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/))
- en: Humans alone are unable to grasp, let alone analyze, such huge amounts of data,
    and ML techniques are used to make sense of these very large datasets. ML is the
    tool that’s used for large-scale data processing. It is well suited to complex
    datasets that have huge numbers of variables and features. One of the strengths
    of many ML techniques, and DL in particular, is that they perform best when used
    on large datasets, thus improving their analytic and predictive power. In other
    words, ML techniques, and DL NNs in particular, learn best when they can access
    large datasets where they can discover patterns and regularities hidden in the
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人类无法独立掌握，更不用说分析如此庞大的数据量，机器学习技术就是用来理解这些非常大的数据集的工具。机器学习是用于大规模数据处理的工具。它非常适合具有大量变量和特征的复杂数据集。许多机器学习技术，尤其是深度学习的优势之一是，它们在大数据集上使用时表现最佳，从而提高了它们的分析和预测能力。换句话说，机器学习技术，特别是深度学习神经网络，最擅长在能够访问大量数据集的情况下进行学习，在这些数据集中它们能够发现隐藏的模式和规律。
- en: On the other hand, ML’s predictive ability can be successfully adapted to AI
    systems. ML can be thought of as the brain of an AI system. AI can be defined
    (though this definition may not be unique) as a system that can interact with
    its environment. Also, AI machines are endowed with sensors that enable them to
    know the environment they are in and tools with which they can relate to the environment.
    Therefore, ML is the brain that allows the machine to analyze the data ingested
    through its sensors to formulate an appropriate answer. A simple example is Siri
    on an iPhone. Siri hears the command through its microphone and outputs an answer
    through its speakers or its display, but to do so, it needs to understand what
    it’s being told. Similarly, driverless cars will be equipped with cameras, GPS
    systems, sonars, and LiDAR, but all this information needs to be processed to
    provide a correct answer. This may include whether to accelerate, brake, or turn.
    ML is the information-processing method that leads to the answer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，机器学习的预测能力可以成功地应用到人工智能（AI）系统中。机器学习可以被认为是人工智能系统的大脑。人工智能可以定义为（尽管这种定义可能不唯一）一种能够与其环境进行交互的系统。此外，AI机器配备了传感器，使它们能够了解所处的环境，并配有工具来与环境进行交互。因此，机器学习就是让机器分析通过传感器获取的数据并得出合适答案的大脑。一个简单的例子是iPhone上的Siri。Siri通过麦克风接收命令，并通过扬声器或显示屏输出答案，但为了做到这一点，它需要理解收到的命令。类似地，自动驾驶汽车将配备摄像头、GPS系统、声纳和激光雷达，但所有这些信息都需要经过处理才能提供正确的答案。这可能包括是否加速、刹车或转弯。机器学习是这种信息处理方法，能够得出最终答案。
- en: We’ve explained what ML is, but what about DL? For now, let’s just say that
    DL is a subfield of ML. DL methods share some special common features. The most
    popular representatives of such methods are deep NNs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释了什么是机器学习（ML），那深度学习（DL）呢？目前，我们暂且说深度学习是机器学习的一个子领域。深度学习方法具有一些特殊的共同特征。最著名的代表方法就是深度神经网络（NN）。
- en: Different ML approaches
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的机器学习方法
- en: 'As we have seen, the term ML is used in a very general way and refers to the
    general techniques that are used to extrapolate patterns from large sets, or it
    is the ability to make predictions on new data based on what is learned by analyzing
    available known data. ML techniques can roughly be divided into two core classes,
    while one more class is often added. Here are the classes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，机器学习（ML）这一术语被广泛使用，它指的是从大量数据集中推断模式的通用技术，或者是基于分析已知数据来对新数据进行预测的能力。机器学习技术大致可以分为两大核心类别，此外通常还会添加一个类别。以下是这些类别：
- en: Supervised learning
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Let’s take a closer look.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来仔细看看。
- en: Supervised learning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning algorithms are a class of ML algorithms that use previously
    labeled data to learn its features, so they can classify similar but unlabeled
    data. Let’s use an example to understand this concept better.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法是一类机器学习算法，它们使用之前标注过的数据来学习其特征，以便能够对类似但未标注的数据进行分类。让我们通过一个例子来更好地理解这个概念。
- en: Let’s assume that a user receives many emails every day, some of which are important
    business emails and some of which are unsolicited junk emails, also known as **spam**.
    A supervised machine algorithm will be presented with a large body of emails that
    have already been labeled by a teacher as spam or not spam (this is called **training
    data**). For each sample, the machine will try to predict whether the email is
    spam or not, and it will compare the prediction with the original target label.
    If the prediction differs from the target, the machine will adjust its internal
    parameters in such a way that the next time it encounters this sample, it will
    classify it correctly. Conversely, if the prediction is correct, the parameters
    will stay the same. The more training data we feed to the algorithm, the better
    it becomes (this rule has caveats, as we’ll see next).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个用户每天会收到许多电子邮件，其中一些是重要的商务邮件，而另一些则是未经请求的垃圾邮件，也就是**垃圾邮件**。监督式机器学习算法会接收到大量已经由“教师”标注为垃圾邮件或非垃圾邮件的电子邮件（这叫做**训练数据**）。对于每个样本，机器将尝试预测该邮件是否是垃圾邮件，并将预测结果与原始目标标签进行比较。如果预测结果与目标不符，机器将调整其内部参数，以便下次遇到该样本时，能够正确分类。相反，如果预测正确，参数将保持不变。我们给算法提供的训练数据越多，它的表现就会越好（但这一规则也有例外，稍后我们会讨论）。
- en: 'In the example we used, the emails had only two classes (spam or not spam),
    but the same principles apply to tasks with arbitrary numbers of classes (or categories).
    For example, Gmail, the free email service by Google, allows the user to select
    up to five categories, which are labeled as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用的例子中，电子邮件只有两个类别（垃圾邮件或非垃圾邮件），但是相同的原则适用于具有任意类别（或分类）的任务。例如，Google提供的免费电子邮件服务Gmail允许用户选择最多五个类别，分类标签如下：
- en: '**Primary**: Includes person-to-person conversations'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主要**：包括人与人之间的对话'
- en: '**Promotions**: Includes marketing emails, offers, and discounts'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促销**：包括营销邮件、优惠和折扣'
- en: '**Social**: Includes messages from social networks and media-sharing sites'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交**：包括来自社交网络和媒体共享网站的消息'
- en: '**Updates**: Includes bills, bank statements, and receipts'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新**：包括账单、银行对账单和收据'
- en: '**Forums**: Includes messages from online groups and mailing lists'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**论坛**：包括来自在线小组和邮件列表的消息'
- en: To summarize, the ML task, which maps a set of input values to a finite number
    of classes, is called **classification**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，机器学习任务是将一组输入值映射到有限数量的类别，这个任务叫做**分类**。
- en: In some cases, the outcome may not necessarily be discrete, and we may not have
    a finite number of classes to classify our data into. For example, we may try
    to predict the life expectancy of a group of people based on their predetermined
    health parameters. In this case, the outcome is a numerical value, and we don’t
    talk about classification but rather **regression**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，结果可能不一定是离散的，我们可能没有有限数量的类别来对数据进行分类。例如，我们可能尝试根据预定的健康参数预测一组人的预期寿命。在这种情况下，结果是一个数值，我们不再谈论分类，而是谈论**回归**。
- en: 'One way to think of supervised learning is to imagine we are building a function,
    *f*, defined over a dataset, which comprises information organized by **features**.
    In the case of email classification, the features can be specific words that may
    appear more frequently than others in spam emails. The use of explicit sex-related
    words will most likely identify a spam email rather than a business/work email.
    On the contrary, words such as *meeting*, *business*, or *presentation* are more
    likely to describe a work email. If we have access to metadata, we may also use
    the sender’s information as a feature. Each email will then have an associated
    set of features, and each feature will have a value (in this case, how many times
    the specific word is present in the email’s body). The ML algorithm will then
    seek to map those values to a discrete range that represents the set of classes,
    or a real value in the case of regression. The definition of the *f* function
    is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一种看待监督学习的方法是想象我们正在构建一个函数，*f*，该函数定义在一个数据集上，数据集由**特征**组织的信息组成。在电子邮件分类的例子中，特征可以是一些在垃圾邮件中出现频率高于其他单词的特定词汇。显式的性别相关词汇最有可能识别出垃圾邮件，而不是商务/工作邮件。相反，像*meeting*、*business*或*presentation*这样的词更有可能描述工作邮件。如果我们可以访问元数据，我们也可以使用发件人的信息作为特征。每封电子邮件将具有一组相关的特征，每个特征将有一个值（在这种情况下，就是特定单词在电子邮件正文中出现的次数）。然后，机器学习算法将尝试将这些值映射到一个离散的范围，该范围代表一组类别，或者在回归的情况下，映射到一个实数值。*f*函数的定义如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mo>:</mo><mtext>space</mtext><mtext>of</mtext><mtext>features</mtext><mo>→</mo><mtext>classes</mtext><mtext>=</mtext><mtext>(discrete</mtext><mtext>values</mtext><mtext>or</mtext><mtext>real</mtext><mtext>values)</mtext></mrow></mrow></math>](img/1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mo>:</mo><mtext>特征</mtext><mo>→</mo><mtext>类别</mtext><mtext>=</mtext><mtext>(离散</mtext><mtext>值</mtext><mtext>或</mtext><mtext>实数</mtext><mtext>值)</mtext></mrow></mrow></math>](img/1.png)'
- en: In later chapters, we’ll see several examples of either classification or regression
    problems. One such problem we’ll discuss is classifying handwritten digits of
    the **Modified National Institute of Standards and Technology** (**MNIST**) database
    ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)). When
    given a set of images representing 0 to 9, the ML algorithm will try to classify
    each image in one of the 10 classes, wherein each class corresponds to one of
    the 10 digits. Each image is 28×28 (= 784) pixels in size. If we think of each
    pixel as one feature, then the algorithm will use a 784-dimensional feature space
    to classify the digits.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们将看到一些分类或回归问题的示例。我们将讨论的一个问题是对**修改后的国家标准与技术研究所**（**MNIST**）数据库中的手写数字进行分类（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）。当给定一个表示
    0 到 9 的图像集合时，机器学习算法将尝试将每个图像分类到 10 个类别之一，每个类别对应 10 个数字中的一个。每个图像的大小为 28×28 (= 784)
    像素。如果我们把每个像素看作一个特征，那么算法将使用 784 维的特征空间来分类这些数字。
- en: 'The following figure depicts the handwritten digits from the MNIST dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了来自 MNIST 数据集的手写数字：
- en: '![Figure 1.1 – An example of handwritten digits from the MNIST dataset](img/B19627_01_1.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 来自 MNIST 数据集的手写数字示例](img/B19627_01_1.jpg)'
- en: Figure 1.1 – An example of handwritten digits from the MNIST dataset
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 来自 MNIST 数据集的手写数字示例
- en: In the next sections, we’ll talk about some of the most popular classical supervised
    algorithms. The following is by no means an exhaustive list or a thorough description
    of each ML method. We recommend referring to the book *Python Machine Learning*,
    by Sebastian Raschka ([https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750](https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750)).
    It’s a simple review meant to provide you with a flavor of the different ML techniques
    in Python.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论一些最受欢迎的经典监督学习算法。以下内容绝非详尽无遗，也不是每种机器学习方法的深入描述。我们建议参考 Sebastian Raschka
    所著的《*Python 机器学习*》一书（[https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750](https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750)）。这是一本简单的回顾，旨在为你提供不同机器学习技术的概览，特别是在
    Python 中的应用。
- en: Linear and logistic regression
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归与逻辑回归
- en: A **regression** algorithm is a type of supervised algorithm that uses features
    of the input data to predict a numeric value, such as the cost of a house, given
    certain features, such as size, age, number of bathrooms, number of floors, and
    location. Regression analysis tries to find the value of the parameters for the
    function that best fits an input dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**算法是一种监督学习算法，它利用输入数据的特征来预测一个数值，例如在给定房屋的大小、年龄、浴室数量、楼层数和位置等特征的情况下，预测房屋的价格。回归分析试图找到使输入数据集最符合的函数参数值。'
- en: In a **linear regression** algorithm, the goal is to minimize a **cost function**
    by finding appropriate parameters for the function over the input data that best
    approximates the target values. A cost function is a function of the error – that
    is, how far we are from getting a correct result. A popular cost function is the
    **mean squared error** (**MSE**), where we take the square of the difference between
    the expected value and the predicted result. The sum of all the input examples
    gives us the error of the algorithm and represents the cost function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在**线性回归**算法中，目标是通过在输入数据上找到适当的参数，使得函数最小化**代价函数**，从而最好地逼近目标值。代价函数是误差的函数——即，我们离正确结果有多远。一个常见的代价函数是**均方误差**（**MSE**），它通过取预期值与预测结果之间差值的平方来计算。所有输入示例的和给出了算法的误差，并代表了代价函数。
- en: Say we have a 100-square-meter house that was built 25 years ago with three
    bathrooms and two floors. Let’s also assume that the city is divided into 10 different
    neighborhoods, which we’ll denote with integers from 1 to 10, and say this house
    is located in the area denoted by 7\. We can parameterize this house with a five-dimensional
    vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>100,25,3</mml:mn><mml:mo>,</mml:mo><mml:mn>2,7</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/2.png).
    Say that we also know that this house has an estimated value of $100,000 (in today’s
    world, this would be enough for just a tiny shack near the North Pole, but let’s
    pretend). What we want is to create a function, *f*, such that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/3.png).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一栋100平方米的房子，它建于25年前，有三个浴室和两层楼。我们还假设这座城市分为10个不同的社区，我们用1到10的整数来表示这些社区，假设这座房子位于编号为7的区域。我们可以用一个五维向量来表示这栋房子，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>100,25,3</mml:mn><mml:mo>,</mml:mo><mml:mn>2,7</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/2.png)。假设我们还知道这栋房子的估计价值为$100,000（在今天的世界里，这可能只够在北极附近买一座小屋，但我们假设如此）。我们想要创建一个函数，*f*，使得
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/3.png)。
- en: A note of encouragement
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励的话
- en: Don’t worry If you don’t fully understand some of the terms in this section.
    We’ll discuss vectors, cost functions, linear regression, and gradient descent
    in more detail in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047). We will also
    see that training NNs and linear/logistic regressions have a lot in common. For
    now, you can think of a vector as an array. We’ll denote vectors with boldface
    font – for example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi></mml:math>](img/4.png).
    We’ll denote the vector elements with italic font and subscript – for example,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/5.png).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有完全理解本节中的某些术语，不用担心。我们将在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中更详细地讨论向量、成本函数、线性回归和梯度下降。我们还将看到，训练神经网络（NN）和线性/逻辑回归有很多相似之处。现在，你可以将向量看作一个数组。我们将用粗体字表示向量——例如，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/4.png)。我们将用斜体字和下标表示向量的元素——例如，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/5.png)。
- en: 'In linear regression, this means finding a vector of weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/6.png),
    such that the dot product of the vectors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/7.png),
    would be ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>100</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>25</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>7</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/8.png)
    or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi> </mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi> </mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/9.png).
    If we had 1,000 houses, we could repeat the same process for every house, and
    ideally, we would like to find a single vector, **w**, that can predict the correct
    value that is close enough for every house. The most common way to train a linear
    regression model can be seen in the following pseudocode block:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，这意味着找到一个权重向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/6.png)，使得向量的点积，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/7.png)，将是
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>100</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>25</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>7</mml:mn><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/8.png)
    或者 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi> </mml:mi><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi> </mml:mi></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100000</mml:mn></mml:math>](img/9.png)。如果我们有1000栋房子，我们可以对每栋房子重复相同的过程，理想情况下，我们希望找到一个单一的向量，**w**，能够预测每栋房子足够接近的正确值。训练线性回归模型的最常见方法可以在以下伪代码块中看到：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, we iterate over the training data to compute the cost function, MSE.
    Once we know the value of MSE, we’ll use the **gradient descent** algorithm to
    update the weights of the vector, **w**. To do this, we’ll calculate the derivatives
    of the cost function concerning each weight, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/14.png).
    In this way, we’ll know how the cost function changes (increase or decrease) concerning
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/15.png).
    Then we’ll update that weight’s value accordingly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们遍历训练数据来计算代价函数 MSE。一旦我们知道了 MSE 的值，就会使用 **梯度下降** 算法来更新向量的权重，**w**。为此，我们将计算代价函数关于每个权重的导数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/14.png)。通过这种方式，我们就能知道代价函数如何相对于
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/15.png)
    发生变化（增加或减少）。然后，我们将相应地更新该权重的值。
- en: 'Previously, we demonstrated how to solve a regression problem with linear regression.
    Now, let’s take a classification task: trying to determine whether a house is
    overvalued or undervalued. In this case, the target data would be categorical
    [1, 0] – 1 for overvalued and 0 for undervalued. The price of the house will be
    an input parameter instead of the target value as before. To solve the task, we’ll
    use logistic regression. This is similar to linear regression but with one difference:
    in linear regression, the output is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:math>](img/16.png). However, here, the output
    will be a special logistic function ([https://en.wikipedia.org/wiki/Logistic_function](https://en.wikipedia.org/wiki/Logistic_function)),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/17.png).
    This will squash the value of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:math>](img/18.png) in the (0:1) interval. You
    can think of the logistic function as a probability, and the closer the result
    is to 1, the more chance there is that the house is overvalued, and vice versa.
    Training is the same as with linear regression, but the output of the function
    is in the (0:1) interval, and the labels are either 0 or 1.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们演示了如何使用线性回归解决回归问题。现在，让我们来处理一个分类任务：尝试判断一座房子是被高估还是低估。在这种情况下，目标数据将是分类的[1,
    0]——1代表高估，0代表低估。房子的价格将作为输入参数，而不再是目标值。为了解决这个问题，我们将使用逻辑回归。这与线性回归类似，但有一个区别：在线性回归中，输出是
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:math>](img/16.png)。然而，在这里，输出将是一个特殊的逻辑函数
    ([https://en.wikipedia.org/wiki/Logistic_function](https://en.wikipedia.org/wiki/Logistic_function))，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/17.png)。这将把
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:math>](img/18.png)
    的值压缩到 (0:1) 区间。你可以将逻辑函数看作是一个概率，结果越接近1，房子被高估的可能性越大，反之亦然。训练过程与线性回归相同，但函数的输出在 (0:1)
    区间内，标签要么是0，要么是1。
- en: Logistic regression is not a classification algorithm, but we can turn it into
    one. We just have to introduce a rule that determines the class based on the logistic
    function’s output. For example, we can say that a house is overvalued if the value
    of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced><mml:mo><</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/19.png)
    and undervalued otherwise.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归不是一种分类算法，但我们可以将其转化为分类算法。我们只需引入一条规则，根据逻辑函数的输出确定类别。例如，如果值为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced><mml:mo><</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/19.png)
    时，我们可以认为一所房子被高估，否则它被低估。
- en: Multivariate regression
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 多元回归
- en: The regression examples in this section have a single numerical output. A regression
    analysis can have more than one output. We’ll refer to such analysis as **multivariate
    regression**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的回归示例具有单一的数值输出。回归分析可以有多个输出。我们将这种分析称为**多元回归**。
- en: Support vector machines
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: A **support vector machine** (**SVM**) is a supervised ML algorithm that’s mainly
    used for classification. It is the most popular member of the kernel method class
    of algorithms. An SVM tries to find a hyperplane, which separates the samples
    in the dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种监督式机器学习算法，主要用于分类。它是核方法算法类中最流行的成员。SVM试图找到一个超平面，将数据集中的样本分隔开。'
- en: Hyperplanes
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面
- en: A hyperplane is a plane in a high-dimensional space. For example, a hyperplane
    in a one-dimensional space is a point, and in a two-dimensional space, it would
    just be a line. In three-dimensional space, the hyperplane would be a plane, and
    we can’t visualize the hyperplane in four-dimensional space, but we know that
    it exists.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面是一个高维空间中的平面。例如，单维空间中的超平面是一个点，二维空间中的超平面是直线。在三维空间中，超平面是一个平面，而在四维空间中我们无法直观地想象超平面，但我们知道它的存在。
- en: We can think of classification as the process of trying to find a hyperplane
    that will separate different groups of data points. Once we have defined our features,
    every sample (in our case, an email) in the dataset can be thought of as a point
    in the multidimensional space of features. One dimension of that space represents
    all the possible values of one feature. The coordinates of a point (a sample)
    are the specific values of each feature for that sample. The ML algorithm task
    will be to draw a hyperplane to separate points with different classes. In our
    case, the hyperplane would separate spam from non-spam emails.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将分类看作是寻找一个超平面的过程，该超平面能够分隔不同的点集。一旦我们定义了特征，数据集中的每个样本（在我们的例子中是一个电子邮件）都可以看作是特征多维空间中的一个点。该空间的一维代表一个特征的所有可能值。一个点（一个样本）的坐标是该样本每个特征的具体值。机器学习算法的任务是绘制一个超平面，将不同类别的点分开。在我们的例子中，超平面将垃圾邮件与非垃圾邮件分开。
- en: 'In the following diagram, at the top and bottom, we can see two classes of
    points (red and blue) that are in a two-dimensional feature space (the *x* and
    *y* axes). If both the *x* and *y* values of a point are below 5, then the point
    is blue. In all other cases, the point is red. In this case, the classes are **linearly
    separable**, meaning we can separate them with a hyperplane. Conversely, the classes
    in the image on the right are linearly inseparable:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，顶部和底部展示了两类点（红色和蓝色），这些点位于二维特征空间中（*x* 和 *y* 轴）。如果一个点的 *x* 和 *y* 值都低于 5，则该点为蓝色。其他情况下，该点为红色。在这种情况下，类别是**线性可分**的，这意味着我们可以用一个超平面将它们分开。相反，右图中的类别是线性不可分的：
- en: '![Figure 1.2 – A linearly separable set of points (left) and a linearly inseparable
    set (right)](img/B19627_01_2.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 一组线性可分的点集（左）和一组线性不可分的点集（右）](img/B19627_01_2.jpg)'
- en: Figure 1.2 – A linearly separable set of points (left) and a linearly inseparable
    set (right)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 一组线性可分的点集（左）和一组线性不可分的点集（右）
- en: 'The SVM tries to find a hyperplane that maximizes the distance between itself
    and the points. In other words, from all possible hyperplanes that can separate
    the samples, the SVM finds the one that has the maximum distance from all points.
    In addition, SVMs can deal with data that is not linearly separable. There are
    two methods for this: introducing soft margins or using the **kernel trick**.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）尝试找到一个超平面，使得它与各个点之间的距离最大。换句话说，在所有可以分开样本的超平面中，SVM找到的是那个与所有点的距离最大的超平面。此外，SVM还可以处理非线性可分的数据。有两种方法可以实现这一点：引入软间隔或使用**核技巧**。
- en: Soft margins work by allowing a few misclassified elements while retaining the
    most predictive ability of the algorithm. In practice, it’s better not to overfit
    the ML model, and we could do so by relaxing some of the SVM hypotheses.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 软间隔通过允许少数被误分类的元素，同时保持算法的最强预测能力来工作。在实际应用中，最好避免对机器学习模型过拟合，我们可以通过放宽一些支持向量机（SVM）的假设来做到这一点。
- en: 'The kernel trick solves the same problem differently. Imagine that we have
    a two-dimensional feature space, but the classes are linearly inseparable. The
    kernel trick uses a kernel function that transforms the data by adding more dimensions
    to it. In our case, after the transformation, the data will be three-dimensional.
    The linearly inseparable classes in the two-dimensional space will become linearly
    separable in the three dimensions and our problem is solved:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 核技巧通过不同的方式解决相同的问题。假设我们有一个二维特征空间，但类别是线性不可分的。核技巧使用一个核函数，通过为数据添加更多的维度来对其进行转换。在我们的例子中，经过转换后，数据将变成三维的。二维空间中线性不可分的类别在三维空间中将变得线性可分，我们的问题得以解决：
- en: "![Figure 1.3 – A non-linearl\uFEFFy separable set before the kernel was applied\
    \ (left) and the same dataset after the kernel has been applied, and the data\
    \ can be linearly separated (right)](img/B19627_01_3.jpg)"
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 在应用核技巧之前的非线性可分数据集（左），以及应用核技巧后的同一数据集，数据变得线性可分（右）](img/B19627_01_3.jpg)'
- en: Figure 1.3 – A non-linearly separable set before the kernel was applied (left)
    and the same dataset after the kernel has been applied, and the data can be linearly
    separated (right)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 在应用核技巧之前的非线性可分数据集（左），以及应用核技巧后的同一数据集，数据变得线性可分（右）
- en: Lets move to the last one in our list, decision trees.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看列表中的最后一个，决策树。
- en: Decision trees
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: Another popular supervised algorithm is the decision tree, which creates a classifier
    in the form of a tree. It is composed of decision nodes, where tests on specific
    attributes are performed, and leaf nodes, which indicate the value of the target
    attribute. To classify a new sample, we start at the root of the tree and navigate
    down the nodes until we reach a leaf.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的监督算法是决策树，它创建一个树形的分类器。决策树由决策节点组成，在这些节点上进行特征测试，以及叶节点，表示目标属性的值。为了分类一个新样本，我们从树的根节点开始，沿着节点向下导航，直到到达叶节点。
- en: A classic application of this algorithm is the Iris flower dataset ([http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)),
    which contains data from 50 samples of three types of irises
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的一个经典应用是鸢尾花数据集（[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)），其中包含来自三种鸢尾花的50个样本的数据。
- en: '(**Iris Setosa**, **Iris Virginica**, and **Iris Versicolor**). Ronald Fisher,
    who created the dataset, measured four different features of these flowers:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (**鸢尾花 Setosa**, **鸢尾花 Virginica**, 和 **鸢尾花 Versicolor**)。创建该数据集的罗纳德·费舍尔（Ronald
    Fisher）测量了这三种花的四个不同特征：
- en: The length of their sepals
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们萼片的长度
- en: The width of their sepals
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们萼片的宽度
- en: The length of their petals
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们花瓣的长度
- en: The width of their petals
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们花瓣的宽度
- en: 'Based on the different combinations of these features, it’s possible to create
    a decision tree to decide which species each flower belongs to. In the following
    diagram, we have defined a decision tree that will correctly classify almost all
    the flowers using only two of these features, the petal length and width:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些特征的不同组合，可以创建一个决策树来决定每朵花属于哪个物种。在下图中，我们定义了一个决策树，它将仅使用这两个特征——花瓣长度和宽度，正确地分类几乎所有的花：
- en: '![Figure 1.4 – A decision tree for classifying the Iris dataset](img/B19627_01_4.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 用于分类鸢尾花数据集的决策树](img/B19627_01_4.jpg)'
- en: Figure 1.4 – A decision tree for classifying the Iris dataset
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 用于分类鸢尾花数据集的决策树
- en: To classify a new sample, we start at the root note of the tree (petal length).
    If the sample satisfies the condition, we go left to the leaf, representing the
    Iris Setosa class. If not, we go right to a new node (petal width). This process
    continues until we reach a leaf.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要对一个新样本进行分类，我们从树的根节点（花瓣长度）开始。如果样本满足条件，我们就向左走到叶子节点，表示 Iris Setosa 类别。如果不满足条件，我们就向右走到一个新节点（花瓣宽度）。这个过程会一直进行，直到我们到达叶子节点。
- en: In recent years, decision trees have seen two major improvements. The first
    is **random forests**, which is an ensemble method that combines the predictions
    of multiple trees. The second is a class of algorithms called **gradient boosting**,
    which creates multiple sequential decision trees, where each tree tries to improve
    the errors made by the previous tree. Thanks to these improvements, decision trees
    have become very popular when working with certain types of data. For example,
    they are one of the most popular algorithms used in Kaggle competitions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，决策树经历了两项重大改进。第一项是**随机森林**，这是一种集成方法，它结合了多个决策树的预测结果。第二项是一个叫做**梯度提升**的算法类，它通过创建多个连续的决策树，每棵树都试图改进前一棵树所犯的错误。由于这些改进，决策树在处理某些类型的数据时变得非常流行。例如，它们是
    Kaggle 比赛中最常用的算法之一。
- en: Unsupervised learning
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The second class of ML algorithms is unsupervised learning. Here, we don’t label
    the data beforehand; instead, we let the algorithm come to its conclusion. One
    of the advantages of unsupervised learning algorithms over supervised ones is
    that we don’t need labeled data. Producing labels for supervised algorithms can
    be costly and slow. One way to solve this issue is to modify the supervised algorithm
    so that it uses less labeled data; there are different techniques for this. But
    another approach is to use an algorithm, which doesn’t need labels in the first
    place. In this section, we’ll discuss some of these unsupervised algorithms.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类机器学习算法是无监督学习。在这里，我们不事先标注数据；相反，我们让算法自行得出结论。无监督学习算法相对于监督学习算法的一个优势是，我们不需要标签数据。为监督算法生成标签可能既昂贵又缓慢。解决这个问题的一种方法是修改监督算法，使其使用更少的标注数据；对此有不同的技术。但另一种方法是使用一种根本不需要标签的算法。在本节中，我们将讨论一些无监督算法。
- en: Clustering
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: One of the most common, and perhaps simplest, examples of unsupervised learning
    is clustering. This is a technique that attempts to separate the data into subsets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习最常见、也许是最简单的例子之一是聚类。这是一种尝试将数据分成多个子集的技术。
- en: To illustrate this, let’s view the spam-or-not-spam email classification as
    an unsupervised learning problem. In the supervised case, for each email, we had
    a set of features and a label (spam or not spam). Here, we’ll use the same set
    of features, but the emails will not be labeled. Instead, we’ll ask the algorithm,
    when given the set of features, to put each sample in one of two separate groups
    (or clusters). Then, the algorithm will try to combine the samples in such a way
    that the intraclass similarity (which is the similarity between samples in the
    same cluster) is high and the similarity between different clusters is low. Different
    clustering algorithms use different metrics to measure similarity. For some more
    advanced algorithms, you don’t have to specify the number of clusters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们将垃圾邮件与非垃圾邮件的电子邮件分类看作一个无监督学习问题。在监督学习的情况下，对于每封电子邮件，我们都有一组特征和一个标签（垃圾邮件或非垃圾邮件）。在这里，我们将使用相同的特征集，但电子邮件不会被标注。相反，我们会让算法在给定特征集的情况下，将每个样本分配到两个独立的组（或簇）之一。然后，算法会尝试以一种方式组合这些样本，使得同一簇内的相似度（即同一簇内样本的相似性）很高，而不同簇之间的相似度很低。不同的聚类算法使用不同的度量标准来衡量相似度。对于一些更高级的算法，你甚至不需要指定簇的数量。
- en: 'The following graph shows how a set of points can be classified to form three
    subsets:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了如何将一组点分类为三个子集：
- en: '![Figure 1.5 – Clustering a set of points in three subsets](img/B19627_01_5.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 将一组点聚类为三个子集](img/B19627_01_5.jpg)'
- en: Figure 1.5 – Clustering a set of points in three subsets
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 将一组点聚类为三个子集
- en: K-means
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: K-means
- en: 'K-means is a clustering algorithm that groups the elements of a dataset into
    *k* distinct clusters (hence the *k* in the name). Here’s how it works:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是一种聚类算法，它将数据集中的元素分成 *k* 个不同的簇（因此名称中有一个 *k*）。它的工作原理如下：
- en: Choose *k* random points, called **centroids**, from the feature space, which
    will represent the center of each of the *k* clusters.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征空间中选择 *k* 个随机点，称为**质心**，它们将代表每个 *k* 个簇的中心。
- en: Assign each sample of the dataset (that is, each point in the feature space)
    to the cluster with the closest centroid.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集中的每个样本（即特征空间中的每个点）分配给与其最接近的质心所在的簇。
- en: For each cluster, we recomputed new centroids by taking the mean values of all
    the points in the cluster.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个簇，我们通过计算簇内所有点的均值来重新计算新的质心。
- en: With the new centroids, we repeat *Steps 2* and *3* until the stopping criteria
    are met.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的质心，我们重复执行 *步骤 2* 和 *步骤 3*，直到满足停止准则。
- en: The preceding method is sensitive to the initial choice of random centroids,
    and it may be a good idea to repeat it with different initial choices. It’s also
    possible for some centroids to not be close to any of the points in the dataset,
    reducing the number of clusters down from *k*. Finally, it’s worth mentioning
    that if we used k-means with *k=3* on the Iris dataset, we may get different distributions
    of the samples compared to the distribution of the decision tree that we’d introduced.
    Once more, this highlights how important it is to carefully choose and use the
    correct ML method for each problem.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法对初始随机质心的选择非常敏感，可能需要用不同的初始选择重复进行。也有可能某些质心离数据集中的任何点都很远，从而减少簇的数量，低于 *k*。最后值得一提的是，如果我们在鸢尾花数据集上使用
    k-means 并设置 *k=3*，我们可能会得到与之前介绍的决策树分布不同的样本分布。再次强调，这突显了为每个问题选择和使用正确的机器学习方法的重要性。
- en: 'Now, let’s discuss a practical example that uses k-means clustering. Let’s
    say a pizza delivery place wants to open four new franchises in a city, and they
    need to choose the locations for the sites. We can solve this problem with k-means:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一个使用 k-means 聚类的实际例子。假设一个披萨配送点想在城市中开设四个新加盟店，并需要选择站点的位置。我们可以通过 k-means
    来解决这个问题：
- en: Find the locations where pizza is ordered most often; these will be our data
    points.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出披萨订单最多的地点，这些地点将作为我们的数据点。
- en: Choose four random points where the sites will be located.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择四个随机点作为新站点的位置。
- en: 'By using k-means clustering, we can identify the four best locations that minimize
    the distance to each delivery place:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 k-means 聚类，我们可以识别出四个最佳位置，最小化与每个配送地点的距离：
- en: '![Figure 1.6 – The distribution of points where pizza is delivered most often
    (left); the round points indicate where the new franchises should be located and
    their corresponding delivery areas (right)](img/B19627_01_6.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 披萨配送最多的地点分布（左）；圆点表示新加盟店应选择的位置及其相应的配送区域（右）](img/B19627_01_6.jpg)'
- en: Figure 1.6 – The distribution of points where pizza is delivered most often
    (left); the round points indicate where the new franchises should be located and
    their corresponding delivery areas (right)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 披萨配送最多的地点分布（左）；圆点表示新加盟店应选择的位置及其相应的配送区域（右）
- en: Self-supervised learning
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自监督学习
- en: 'Self-supervised learning refers to a combination of problems and datasets,
    which allow us to *automatically generate* (that is, without human intervention)
    labeled data from the dataset. Once we have these labels, we can train a supervised
    algorithm to solve our task. To understand this concept better, let’s discuss
    some use cases:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习指的是通过一组合适的问题和数据集，*自动生成*（即无需人工干预）标注数据。一旦我们拥有这些标签，就可以训练一个监督算法来解决我们的任务。为了更好地理解这个概念，让我们讨论一些应用场景：
- en: '**Time series forecasting**: Imagine that we have to predict the future value
    of a time series based on its most recent historical values. Examples of this
    include stock (and nowadays crypto) price prediction and weather forecasting.
    To generate a labeled data sample, let’s take a window with length *k* of the
    historical data that ends at past moment *t*. We’ll take the historical values
    in the range *[t – k; t]* and we’ll use them as input for the supervised algorithm.
    We’ll also take the historical value at moment *t + 1* and we’ll use it as the
    label for the given input sample. We can apply this division to the rest of the
    historical values and generate a labeled training dataset automatically.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列预测**：假设我们需要基于时间序列的最新历史数据来预测其未来的值。此类例子包括股票（以及现在的加密货币）价格预测和天气预测。为了生成一个标注的数据样本，我们将选取一个长度为
    *k* 的历史数据窗口，窗口以过去时刻 *t* 作为终点。我们将使用时间区间 *[t – k; t]* 内的历史值作为监督算法的输入，同时取时刻 *t +
    1* 的历史值作为给定输入样本的标签。我们可以对其余的历史值应用这种划分方式，自动生成标注的训练数据集。'
- en: '**Natural language processing** (**NLP**): Similar to time series, the natural
    text represents a sequence of words (or tokens). We can train an NLP algorithm
    to predict the next word based on the preceding *k* words in a similar manner
    to a time series. However, the natural text does not carry the same strict past/future
    division as time series do. Because of this, we can use the whole context around
    the target word as input – that is, words that come both before and after the
    target word in the consequence, instead of the preceding words only. As we’ll
    see in [*Chapter 6*](B19627_06.xhtml#_idTextAnchor185), this technique is foundational
    to contemporary NLP algorithms.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）：类似于时间序列，自然文本代表了一个单词（或标记）序列。我们可以训练一个NLP算法，基于前面的*k*个单词来预测下一个单词，方式与时间序列类似。然而，自然文本不像时间序列那样有严格的过去/未来划分。正因为如此，我们可以使用目标单词周围的整个上下文作为输入——也就是说，不仅仅是前面的单词，还包括目标单词后面的单词。如我们将在[*第6章*](B19627_06.xhtml#_idTextAnchor185)中看到的，这种技术是当代NLP算法的基础。'
- en: '**Autoencoders**: This is a special type of NN that tries to reproduce its
    input. In other words, the target value (label) of an autoencoder is equal to
    the input data, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/20.png),
    where *i* is the sample index. We can formally say that it tries to learn an identity
    function (a function that repeats its input). Since our labels are just input
    data, the autoencoder is an unsupervised algorithm. You might be wondering what
    the point of an algorithm that tries to predict its input is. The autoencoder
    is split into two parts – an encoder and a decoder. First, the encoder tries to
    compress the input data into a vector with a smaller size than the input itself.
    Next, the decoder tries to reproduce the original input based on this smaller
    internal state vector. By setting this limitation, the autoencoder is forced to
    extract only the most significant features of the input data. The goal of the
    autoencoder is to learn a representation of the data that is more efficient or
    compact than the original representation, while still retaining as much of the
    original information as possible.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**：这是一种特殊类型的神经网络，旨在重建其输入。换句话说，自编码器的目标值（标签）等于输入数据，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/20.png)，其中*i*是样本索引。我们可以正式地说，它试图学习一个恒等函数（即重复其输入的函数）。由于我们的标签只是输入数据，自编码器是一个无监督算法。你可能会想，试图预测其输入的算法有什么意义。自编码器分为两部分——编码器和解码器。首先，编码器尝试将输入数据压缩成比输入本身更小的向量。接着，解码器根据这个较小的内部状态向量重建原始输入。通过设定这个限制，自编码器被迫仅提取输入数据中最重要的特征。自编码器的目标是学习一种比原始表示更高效或更紧凑的数据表示，同时尽可能保留原始信息。'
- en: 'Another interesting application of self-supervised learning is in generative
    models, as opposed to discriminative models. Let’s discuss the difference between
    the two. Given input data, a discriminative model will map it to a certain label
    (in other words, classification or regression). A typical example is the classification
    of MNIST images in 1 of 10-digit classes, where the NN maps input data features
    (pixel intensities) to the digit label. We can also say this in another way: a
    discriminative model gives us the probability of *y* (class), given *x* (input)
    – ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/21.png).
    In the case of MNIST, this is the probability of the digit when given the pixel
    intensities of the image.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习的另一个有趣应用是生成模型，而非判别模型。让我们讨论这两者之间的区别。给定输入数据，判别模型会将其映射到一个特定的标签（换句话说，就是分类或回归）。一个典型的例子是将MNIST图像分类为10个数字类别中的一个，其中神经网络将输入数据特征（像素强度）映射到数字标签。我们也可以用另一种方式来表达：判别模型给出的是在给定*
    x *（输入）时，* y *（类别）的概率 —— ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/21.png)。在MNIST的情况下，这是在给定图像的像素强度时，数字的概率。
- en: 'On the other hand, a generative model learns how classes are distributed. You
    can think of it as the opposite of what the discriminative model does. Instead
    of predicting the class probability, *y*, given certain input features, it tries
    to predict the probability of the input features when given a class, *y* – ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/22.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/23.png).
    For example, a generative model will be able to create an image of a handwritten
    digit when given the digit class. Since we only have 10 classes, it will be able
    to generate just 10 images. However, we’ve only used this example to illustrate
    this concept. In reality, the class could be an arbitrary tensor of values, and
    the model would be able to generate an unlimited number of images with different
    features. If you don’t understand this now, don’t worry; we’ll discuss this topic
    again in [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146). In [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220)
    and [*Chapter 9*](B19627_09.xhtml#_idTextAnchor236), we’ll discuss how transformers
    (a new type of NN) have been used to produce some impressive generative models.
    They have gained popularity both in the research community and the mainstream
    public because of the attractive results they produce. Two of the most popular
    visual models are **Stable Diffusion** ([https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)),
    by Stability AI ([https://stability.ai/](https://stability.ai/)), and **DALL-E**
    ([https://openai.com/dall-e-2/](https://openai.com/dall-e-2/)), by OpenAI, which
    can create photorealistic or artistic images from a natural language description.
    When prompted with the text *Musician frog playing on a guitar*, Stable Diffusion
    produces the following figure:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成模型学习类的分布情况。你可以将它视为与判别模型的相反。它不是在给定某些输入特征的情况下预测类的概率 *y*，而是尝试在给定某个类 *y*
    时预测输入特征的概率 – ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/22.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/23.png)。例如，生成模型能够在给定数字类别时生成手写数字的图像。由于我们只有
    10 个类别，它将仅能生成 10 张图像。然而，我们只是用这个例子来说明这一概念。实际上，类别可以是一个任意的张量值，模型将能够生成无限数量的具有不同特征的图像。如果你现在不理解这一点，别担心；我们将在[*第
    5 章*](B19627_05.xhtml#_idTextAnchor146)中再次讨论这个话题。在[*第 8 章*](B19627_08.xhtml#_idTextAnchor220)和[*第
    9 章*](B19627_09.xhtml#_idTextAnchor236)中，我们将讨论变换器（新型神经网络）如何被用于生成一些令人印象深刻的生成模型。由于它们产生的吸引人结果，变换器在研究界和主流公众中都获得了很大的关注。最受欢迎的两种视觉模型是**稳定扩散**（[https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)），由
    Stability AI（[https://stability.ai/](https://stability.ai/)）开发，以及**DALL-E**（[https://openai.com/dall-e-2/](https://openai.com/dall-e-2/)），由
    OpenAI 开发，它们可以从自然语言描述生成逼真的或艺术性的图像。当提示文本为*弹吉他的音乐青蛙*时，稳定扩散生成以下图像：
- en: '![Figure 1.7 – Stable Diffusion output for the prompt Musician frog playing
    on a guitar](img/B19627_01_7.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 稳定扩散对“弹吉他的音乐青蛙”提示的输出](img/B19627_01_7.jpg)'
- en: Figure 1.7 – Stable Diffusion output for the prompt Musician frog playing on
    a guitar
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 稳定扩散对“弹吉他的音乐青蛙”提示的输出
- en: Another interesting generative model is OpenAI’s **ChatGPT** (GPT stands for
    **Generative Pre-trained Transformer**), which (as its name suggests) acts as
    a smart chatbot. ChatGPT can answer follow-up questions, admit mistakes, challenge
    incorrect premises, and reject inappropriate requests.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的生成模型是 OpenAI 的**ChatGPT**（GPT 代表**生成预训练变换器**），它（正如其名字所示）充当一个智能聊天机器人。ChatGPT
    能够回答后续问题、承认错误、挑战不正确的前提，并拒绝不当的请求。
- en: Reinforcement learning
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The third class of ML techniques is called **reinforcement learning** (**RL**).
    We will illustrate this with one of the most popular applications of RL: teaching
    machines how to play games. The machine (or agent) interacts with the game (or
    environment). The goal of the agent is to win the game. To do this, the agent
    takes actions that can change the environment’s state. The environment reacts
    to the agent’s actions and provides it with reward (or penalty) signals that help
    the agent to decide its next action. Winning the game would provide the biggest
    reward. In formal terms, the goal of the agent is to maximize the total rewards
    it receives throughout the game:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类机器学习技术叫做**强化学习**（**RL**）。我们将通过一个强化学习的最流行应用来说明这一点：教机器如何玩游戏。机器（或智能体）与游戏（或环境）进行互动。智能体的目标是赢得游戏。为此，智能体采取能够改变环境状态的行动。环境对智能体的行动做出反应，并为其提供奖励（或惩罚）信号，帮助智能体决定下一步的行动。赢得游戏将提供最大的奖励。用正式的术语来说，智能体的目标是最大化它在整个游戏过程中获得的总奖励：
- en: "![Figure 1.8 – The interaction between diﬀerent elements of \uFEFFan RL system](img/B19627_01_8.jpg)"
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 强化学习系统中不同元素之间的交互](img/B19627_01_8.jpg)'
- en: Figure 1.8 – The interaction between diﬀerent elements of an RL system
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 强化学习系统中不同元素之间的交互
- en: Let’s imagine a game of chess as an RL problem. Here, the environment would
    include the chessboard, along with the locations of the pieces. The goal of our
    agent is to beat the opponent. The agent will then receive a reward when they
    capture the opponent’s piece, and they will win the biggest reward if they checkmate
    the opponent. Conversely, if the opponent captures a piece or checkmates the agent,
    the reward will be negative. However, as part of their larger strategies, the
    players will have to make moves that neither capture a piece nor checkmate the
    other’s king. The agent won’t receive any reward then. If this was a supervised
    learning problem, we would have to provide a label or a reward for each move.
    This is not the case with RL. In the RL framework, the agent will improvise with
    a trial-and-error approach to decide its next actions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把国际象棋游戏看作一个强化学习（RL）问题。在这里，环境将包括棋盘以及棋子的具体位置。我们的智能体的目标是击败对手。当智能体捕获对方的棋子时，它会获得奖励，如果将对方将死，则会获得最大奖励。相反，如果对方捕获了智能体的棋子或将其将死，奖励将是负数。然而，作为更大策略的一部分，玩家们必须做出既不捕获对方棋子也不将死对方国王的行动。在这种情况下，智能体将不会收到任何奖励。如果这是一个监督学习问题，我们将需要为每一步提供标签或奖励。但在强化学习中并非如此。在强化学习框架中，智能体将通过试错法即兴决定其下一步行动。
- en: Let’s take another example, in which sometimes we have to sacrifice a pawn to
    achieve a more important goal (such as a better position on the chessboard). In
    such situations, our humble agent has to be smart enough to take a short-term
    loss as a long-term gain. In an even more extreme case, imagine we had the bad
    luck of playing against Ding Liren, the current world chess champion. Surely,
    the agent will lose in this case. However, how would we know which moves were
    wrong and led to the agent’s loss? Chess belongs to a class of problems where
    the game should be considered in its entirety to reach a successful solution,
    rather than just looking at the immediate consequences of each action. RL will
    give us the framework that will help the agent to navigate and learn in this complex
    environment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再举一个例子，有时候我们不得不牺牲一个兵，以实现更重要的目标（比如棋盘上的更好位置）。在这种情况下，我们的聪明智能体必须足够聪明，将短期的损失视为长期的收益。在一个更极端的例子中，假设我们很不幸地与现任世界象棋冠军丁立人对弈。毫无疑问，在这种情况下，智能体会输。然而，我们如何知道哪些步伐是错误的，并导致了智能体的失败呢？国际象棋属于一种问题类型，其中需要将整个游戏视为一个整体来寻找成功的解决方案，而不是仅仅看每个行动的即时后果。强化学习将为我们提供框架，帮助智能体在这个复杂的环境中导航并进行学习。
- en: 'An interesting problem arises from this newfound freedom to take action. Imagine
    that the agent has learned one successful chess-playing strategy (or **policy**,
    in RL terms). After some games, the opponent might guess what that policy is and
    manage to beat us. The agent will now face a dilemma with the following decisions:
    either to follow the current policy and risk becoming predictable, or to experiment
    with new moves that will surprise the opponent, but also carry the risk of turning
    out even worse. In general terms, the agent uses a policy that gives them a certain
    reward, but their ultimate goal is to maximize the total reward. A modified policy
    might be more rewarding, and the agent will be ineffective if they don’t try to
    find such a policy. One of the challenges of RL is the trade-off between exploitation
    (following the current policy) and exploration (trying new moves).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新获得的行动自由带来了一个有趣的问题。假设代理已经学会了一种成功的棋类策略（或者用RL术语说，是**策略**）。经过若干场比赛后，对手可能猜到这一策略并设法战胜我们。现在代理将面临一个困境，需要做出以下决策：要么继续遵循当前的策略，冒着变得可预测的风险，要么尝试新的棋步来让对手吃惊，但这也带来了可能更糟糕的风险。一般来说，代理使用一种给他们带来某种奖励的策略，但他们的终极目标是最大化总奖励。修改后的策略可能带来更多的奖励，如果代理不尝试找到这样的策略，他们将无效。RL的一个挑战是如何在利用（遵循当前策略）和探索（尝试新动作）之间进行权衡。
- en: So far, we’ve used only games as examples; however, many problems can fall into
    the RL domain. For example, you can think of autonomous vehicle driving as an
    RL problem. The vehicle can get positive rewards if it stays within its lane and
    observes the traffic rules. It will gain negative rewards if it crashes. Another
    interesting recent application of RL is in managing stock portfolios. The goal
    of the agent would be to maximize the portfolio value. The reward is directly
    derived from the value of the stocks in the portfolio.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了游戏作为例子；然而，许多问题可以归入RL领域。例如，你可以将自动驾驶汽车看作一个RL问题。如果汽车保持在车道内并遵守交通规则，它会获得正奖励；如果发生碰撞，则会获得负奖励。RL的另一个有趣的应用是管理股票投资组合。代理的目标是最大化投资组合的价值，奖励直接来源于投资组合中股票的价值。
- en: On the absence of RL in this edition
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本版中没有RL的内容
- en: The second edition of this book had two chapters on RL. In this edition, we’ll
    omit those chapters, and we’ll discuss transformers and their applications instead.
    On one hand, RL is a promising field, but at present, training RL models is slow,
    and their practical applications are limited. Because of this, RL research is
    mostly concentrated in well-funded commercial companies and academic institutions.
    On the other hand, transformers have represented the next big step in the field
    of ML, in the same way as GPU-trained deep networks sparked interest in the field
    in the 2009-2012 period.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第二版包含了两章关于强化学习（RL）的内容。在这一版中，我们将省略这些章节，而是讨论变换器及其应用。一方面，RL是一个有前景的领域，但目前训练RL模型的速度较慢，且其实际应用有限。因此，RL研究主要集中在资金充足的商业公司和学术机构。另一方面，变换器代表了机器学习领域的下一个重大进步，就像GPU训练的深度网络在2009-2012年间激发了该领域的兴趣一样。
- en: Q-learning
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q学习
- en: 'Q-learning is an off-policy temporal-difference RL algorithm. What a mouthful!
    But fear not; let’s not worry about what all this means, and instead just see
    how the algorithm works. To do this, we’ll use the game of chess we introduced
    in the previous section. As a reminder, the board’s configuration (the locations
    of the pieces) is the current state of the environment. Here, the agents can take
    actions, *a*, by moving pieces, thus changing the state into a new one. We’ll
    represent a game of chess as a graph where the different board configurations
    are the graph’s vertices, and the possible moves from each configuration are the
    edges. To make a move, the agent follows the edge from the current state, *s*,
    to a new state, *s’*. The basic Q-learning algorithm uses a **Q-table** to help
    the agent decide which moves to make. The Q-table contains one row for each board
    configuration, while the columns of the table are all possible actions that the
    agent can take (the moves). A table cell, *q(s, a)*, contains the cumulative expected
    reward, called a **Q-value**. This is the potential total reward that the agent
    will receive for the remainder of the game if they take an action, *a*, from their
    current state, *s*. At the beginning, the Q-table is initialized with an arbitrary
    value. With that knowledge, let’s see how Q-learning works:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是一种脱离策略的时序差分强化学习（RL）算法。听起来有点复杂！但不用担心，我们不必纠结这些术语的具体含义，而是直接看一下这个算法是如何工作的。为此，我们将使用前一节介绍的国际象棋游戏。提醒一下，棋盘的配置（棋子的位置）就是环境的当前状态。在这里，智能体可以通过移动棋子来采取动作，*a*，从而将状态转变为新的状态。我们将国际象棋游戏表示为一个图，其中不同的棋盘配置是图的顶点，而每种配置下可能的棋步则是边。要进行一次移动，智能体从当前状态
    *s* 按照边移动到新状态 *s’*。基本的 Q-learning 算法使用 **Q-表** 来帮助智能体决定要采取哪些行动。Q-表为每个棋盘配置提供一行，而表格的列则是智能体可以采取的所有可能动作（棋步）。表格的一个单元格
    *q(s, a)* 存储的是累积的期望回报，也称为 **Q值**。这是智能体从当前状态 *s* 执行一个动作 *a* 后，在接下来的游戏中可能获得的总回报。起初，Q-表会用一个任意的值进行初始化。掌握了这些知识后，让我们看看
    Q-learning 是如何工作的：
- en: '[PRE1]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: An episode starts with a random initial state and finishes when we reach the
    terminal state. In our case, one episode would be one full game of chess.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一局游戏从一个随机初始状态开始，并在达到终止状态时结束。在我们的案例中，一局游戏就是一场完整的国际象棋比赛。
- en: The question that arises is, how does the agent’s policy determine what will
    be the next action? To do so, the policy has to consider the Q-values of all the
    possible actions from the current state. The higher the Q-value, the more attractive
    the action is. However, the policy will sometimes ignore the Q-table (exploitation
    of the existing knowledge) and choose another random action to find higher potential
    rewards (exploration). In the beginning, the agent will take random actions because
    the Q-table doesn’t contain much information (a trial-and-error approach). As
    time progresses and the Q-table is gradually filled, the agent will become more
    informed in interacting with the environment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生的问题是，智能体的策略如何决定下一步行动？为此，策略必须考虑当前状态下所有可能动作的 Q值。Q值越高，动作的吸引力越大。然而，策略有时会忽略 Q-表（即利用现有知识），并选择另一个随机的动作来寻找更高的潜在回报（探索）。在开始时，智能体会采取随机动作，因为
    Q-表中并没有太多信息（采用试错法）。随着时间的推移，Q-表逐渐填充，智能体在与环境交互时将变得更加智能。
- en: We update *q(s, a)* after each new action by using the **Bellman equation**.
    The Bellman equation is beyond the scope of this introduction, but it’s enough
    to know that the updated value, *q(s, a)*, is based on the newly received reward,
    *r*, as well as the maximum possible Q-value, *q*(s’, a’)*, of the new state,
    *s’*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的动作后，我们都会使用 **贝尔曼方程** 更新 *q(s, a)*。贝尔曼方程超出了本介绍的范围，但知道更新后的值 *q(s, a)* 是基于新获得的回报
    *r* 以及新状态 *s’* 的最大可能 Q值 *q*(s’, a’)* 即可。
- en: 'This example was intended to help you understand the basic workings of Q-learning,
    but you might have noticed an issue with this. We store the combination of all
    possible board configurations and moves in the Q-table. This would make the table
    huge and impossible to fit in today’s computer memory. Fortunately, there is a
    solution for this: we can replace the Q-table with an NN, which will tell the
    agent what the optimal action is in each state. In recent years, this development
    has allowed RL algorithms to achieve superhuman performance on tasks such as the
    games of Go, Dota 2, Doom, and StarCraft.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子旨在帮助你理解Q学习的基本原理，但你可能注意到一个问题。我们将所有可能的棋盘配置和动作的组合存储在Q表中。这将使得表格非常庞大，无法容纳在今天的计算机内存中。幸运的是，这里有一个解决方案：我们可以用神经网络替代Q表，神经网络将告诉智能体在每种状态下最优的动作是什么。近年来，这一发展使得强化学习算法在围棋、Dota
    2、Doom和星际争霸等任务中取得了超越人类的表现。
- en: Components of an ML solution
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习解决方案的组成部分
- en: 'So far, we’ve discussed three major classes of ML algorithms. However, to solve
    an ML problem, we’ll need a system in which the ML algorithm is only part of it.
    The most important aspects of such a system are as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了三大类机器学习算法。然而，要解决一个机器学习问题，我们需要一个系统，其中机器学习算法只是其中的一部分。这样的系统最重要的方面如下：
- en: '**Learner**: This algorithm is used with its learning philosophy. The choice
    of this algorithm is determined by the problem we’re trying to solve since different
    problems can be better suited for certain ML algorithms.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习者**：该算法与其学习哲学一起使用。选择该算法是由我们试图解决的问题决定的，因为不同的问题可能更适合某些机器学习算法。'
- en: '**Training data**: This is the raw dataset that we are interested in. This
    can be labeled or unlabeled. It’s important to have enough sample data for the
    learner to understand the structure of the problem.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：这是我们感兴趣的原始数据集。它可以是有标签的或无标签的。拥有足够的样本数据对学习者理解问题结构非常重要。'
- en: '**Representation**: This is how we express the data in terms of the chosen
    features so that we can feed it to the learner. For example, to classify handwritten
    images of digits, we’ll represent the image as a two-dimensional array of values,
    where each cell will contain the color value of one pixel. A good choice of representation
    of the data is important for achieving better results.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表示**：这是我们如何通过选择的特征来表达数据，以便将其输入给学习者。例如，为了分类手写数字图像，我们将图像表示为一个二维数组，其中每个单元格包含一个像素的颜色值。数据表示的良好选择对于获得更好的结果非常重要。'
- en: '**Goal**: This represents the reason to learn from the data for the problem
    at hand. This is strictly related to the target and helps us define how and what
    the learner should use and what representation to use. For example, the goal may
    be to clean our mailboxes of unwanted emails, and this goal defines what the target
    of our learner is. In this case, it is the detection of spam emails.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：这代表了我们从数据中学习的原因。它与目标密切相关，帮助我们定义学习者应该使用什么以及如何使用什么表示。例如，目标可能是清理我们的邮箱中的垃圾邮件，这个目标定义了我们学习者的目标。在这种情况下，它是垃圾邮件的检测。'
- en: '**Target**: This represents what is being learned as well as the final output.
    The target can be a classification of unlabeled data, a representation of input
    data according to hidden patterns or characteristics, a simulator for future predictions,
    or a response to an outside stimulus or strategy (in the case of RL).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标值**：这代表了正在学习的内容以及最终的输出。目标值可以是对无标签数据的分类，是根据隐藏的模式或特征对输入数据的表示，是未来预测的模拟器，或是对外部刺激或策略的回应（在强化学习中）。'
- en: 'It can never be emphasized enough: any ML algorithm can only achieve an approximation
    of the target and not a perfect numerical description. ML algorithms are not exact
    mathematical solutions to problems – they are just approximations. In the *Supervised
    learning section*, we defined learning as a function from the space of features
    (the input) into a range of classes. Later, we’ll see how certain ML algorithms,
    such as NNs, can approximate any function to any degree, in theory. This is called
    the universal approximation theorem ([https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)),
    but it does not imply that we can get a precise solution to our problem. In addition,
    solutions to the problem can be better achieved by better understanding the training
    data.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点永远不能被过分强调：任何机器学习算法只能接近目标，而不是完美的数值描述。机器学习算法不是问题的精确数学解答——它们只是近似值。在*监督学习部分*中，我们将学习定义为从特征空间（输入）到一系列类别的函数。之后，我们将看到某些机器学习算法，如神经网络，理论上可以近似任何函数到任意精度。这被称为通用逼近定理（[https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)），但这并不意味着我们能为问题找到精确的解决方案。此外，通过更好地理解训练数据，可以更好地解决问题。
- en: 'Typically, a problem that can be solved with classic ML techniques may require
    a thorough understanding and processing of the training data before deployment.
    The steps to solve an ML problem are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，经典机器学习技术能够解决的问题可能需要在部署之前对训练数据进行深入理解和处理。解决机器学习问题的步骤如下：
- en: '**Data collection**: This involves gathering as much data as possible. In the
    case of supervised learning, this also includes correct labeling.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：这包括尽可能多地收集数据。在监督学习的情况下，这还包括正确的标签。'
- en: '**Data processing**: This involves cleaning the data, such as removing redundant
    or highly correlated features, or even filling in missing data, and understanding
    the features that define the training data.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：这包括清理数据，如去除冗余或高度相关的特征，甚至填补缺失数据，并理解定义训练数据的特征。'
- en: '**Creation of the test case**: Usually, the data can be divided into three
    sets:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试用例的创建**：通常，数据可以分为三个集合：'
- en: '**Training set**: We use this set to train the ML algorithm. In most cases,
    we’ll train the algorithm by iterating the whole training set more than once.
    We’ll refer to the number of full training set iterations as **epochs**.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：我们使用该集合来训练机器学习算法。在大多数情况下，我们将通过多次迭代整个训练集来训练算法。我们将称每次完整训练集迭代的次数为**训练轮次**。'
- en: '**Validation set**: We use this set to evaluate the accuracy of the algorithm
    with unknown data during training. We’ll train the algorithm for some time on
    the training set and then we’ll use the validation set to check its performance.
    If we are not satisfied with the result, we can tune the hyperparameters of the
    algorithm and repeat the process. The validation set can also help us determine
    when to stop the training. We’ll learn more about this later in this section.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：我们使用该集合来评估算法在训练过程中对未知数据的准确性。我们会在训练集上训练算法一段时间，然后使用验证集来检查其性能。如果结果不令人满意，我们可以调整算法的超参数并重复该过程。验证集还可以帮助我们确定何时停止训练。我们将在本节后面进一步学习这一点。'
- en: '**Test set**: When we finish tuning the algorithm with the training or validation
    cycle, we’ll use the test set only once for a final evaluation. The test set is
    similar to the validation set in the sense that the algorithm hasn’t used it during
    training. However, when we strive to improve the algorithm on the validation data,
    we may inadvertently introduce bias, which can skew the results in favor of the
    validation set and not reflect the actual performance. Because we use the test
    only once, this will provide a more objective measurement of the algorithm.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：当我们完成训练或验证周期并调优算法后，我们只会使用测试集进行最终评估一次。测试集与验证集类似，因为算法在训练过程中没有使用它。然而，当我们努力在验证数据上改善算法时，可能会无意中引入偏差，从而导致结果偏向验证集，不能真实反映实际性能。由于我们只使用一次测试集，这将提供对算法更客观的评估。'
- en: Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: One of the reasons for the success of DL algorithms is that they usually require
    less data processing than classic methods. For a classic algorithm, you would
    have to apply different data processing and extract different features for each
    problem. With DL, you can apply the same data processing pipeline for most tasks.
    With DL, you can be more productive, and you don’t need as much domain knowledge
    for the task at hand compared to the classic ML algorithms.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）算法成功的原因之一是它们通常需要比经典方法更少的数据处理。对于经典算法，你需要对每个问题应用不同的数据处理并提取不同的特征。而对于深度学习，你可以对大多数任务应用相同的数据处理流程。通过深度学习，你可以提高生产力，并且与经典的机器学习算法相比，你不需要太多的领域知识来完成当前任务。
- en: There are many valid reasons to create testing and validation datasets. As mentioned
    previously, ML techniques can only produce an approximation of the desired result.
    Often, we can only include a finite and limited number of variables, and there
    may be many variables that are outside of our control. If we only used a single
    dataset, our model may end up memorizing the data and producing an extremely high
    accuracy value on the data it has memorized. However, this result may not be reproducible
    on other similar but unknown datasets. One of the key goals of ML algorithms is
    their ability to generalize. This is why we create both a validation set used
    for tuning our model selection during training and a final test set only used
    at the end of the process to confirm the validity of the selected algorithm.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 创建测试和验证数据集有很多合理的理由。如前所述，机器学习（ML）技术只能产生所需结果的近似值。通常，我们只能包括有限的、有限数量的变量，且可能有许多变量超出我们的控制范围。如果我们只使用一个数据集，我们的模型可能最终会记住数据，并在它记住的数据上产生极高的准确度。然而，这个结果可能无法在其他相似但未知的数据集上复现。机器学习算法的一个关键目标是它们的泛化能力。这就是为什么我们既要创建用于训练过程中调整模型选择的验证集，又要在过程结束时仅用于确认所选算法有效性的最终测试集。
- en: To understand the importance of selecting valid features and to avoid memorizing
    the data, which is also referred to as **overfitting** in the literature (we’ll
    use this term from now on), let’s use a joke taken from an XKND comic as an example
    ([http://xkcd.com/1122):](http://xkcd.com/1122):)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解选择有效特征的重要性，并避免记住数据（在文献中也称为**过拟合**，我们从现在开始使用这个术语），让我们用一则来自XKCD漫画的笑话作为例子 ([http://xkcd.com/1122):](http://xkcd.com/1122):)
- en: “Up until 1996, no democratic US presidential candidate who was an incumbent
    and with no combat experience had ever beaten anyone whose first name was worth
    more in Scrabble.”
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: “直到1996年，任何没有作战经验且为现任总统的美国民主党总统候选人都从未击败过任何名字在《拼字游戏》中得分更高的人。”
- en: It’s apparent that such a rule is meaningless, but it underscores the importance
    of selecting valid features, and how the question, “How much is a name worth in
    Scrabble?” can bear any relevance while selecting a US president. Also, this example
    doesn’t have any predictive power over unknown data. We’ll call this overfitting,
    which refers to making predictions that fit the data at hand perfectly but don’t
    generalize to larger datasets. Overfitting is the process of trying to make sense
    of what we’ll call noise (information that does not have any real meaning) and
    trying to fit the model to small perturbations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这样的规则是没有意义的，但它强调了选择有效特征的重要性，以及问题“一个名字在《拼字游戏》中的得分值有多高？”在选择美国总统时可能有任何相关性。此外，这个例子对未知数据没有任何预测能力。我们将其称为过拟合（overfitting），即指做出适应手头数据完美的预测，但这些预测不能推广到更大的数据集。过拟合是试图理解我们称之为噪声（没有实际意义的信息），并试图将模型拟合到小的扰动中的过程。
- en: 'To explain this further, let’s try to use ML to predict the trajectory of a
    ball thrown from the ground up into the air (not perpendicularly) until it reaches
    the ground again. Physics teaches us that the trajectory is shaped like a parabola.
    We also expect that a good ML algorithm observing thousands of such throws would
    come up with a parabola as a solution. However, if we were to zoom into the ball
    and observe the smallest fluctuations in the air due to turbulence, we might notice
    that the ball does not hold a steady trajectory but may be subject to small perturbations,
    which in this case is the noise. An ML algorithm that tries to model these small
    perturbations would fail to see the big picture and produce a result that is not
    satisfactory. In other words, overfitting is the process that makes the ML algorithm
    see the trees but forget about the forest:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解释这一点，让我们尝试使用机器学习（ML）预测一个从地面抛向空中的球的轨迹（不是垂直方向），直到它再次落地。物理学教导我们，轨迹的形状像一个抛物线。我们也期望一个好的机器学习算法，观察到成千上万次这样的抛掷后，能够得出一个抛物线作为解决方案。然而，如果我们放大观察球体，注意到由于空气湍流引起的最小波动，我们可能会发现球体并没有保持稳定的轨迹，而是会受到小的扰动影响，这些扰动在这种情况下就是噪声。试图对这些小扰动进行建模的机器学习算法会忽略大局，从而得出不令人满意的结果。换句话说，过拟合是让机器学习算法只看到树木却忘记森林的过程：
- en: '![Figure 1.9 – A good prediction model (left) and a bad (overﬁtted) prediction
    model, with the trajectory of a ball thrown from the ground (right)](img/B19627_01_9.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 一个好的预测模型（左）和一个差的（过拟合的）预测模型，展示了一个从地面抛出的球的轨迹（右）](img/B19627_01_9.jpg)'
- en: Figure 1.9 – A good prediction model (left) and a bad (overﬁtted) prediction
    model, with the trajectory of a ball thrown from the ground (right)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 一个好的预测模型（左）和一个差的（过拟合的）预测模型，展示了一个从地面抛出的球的轨迹（右）
- en: This is why we separate the training data from the validation and test data;
    if the algorithm’s accuracy over the test data was not similar to the training
    data accuracy, that would be a good indication that the model overfits. We need
    to make sure that we don’t make the opposite error either – that is, underfitting
    the model. In practice, though, if we aim to make our prediction model as accurate
    as possible on our training data, underfitting is much less of a risk, and care
    is taken to avoid overfitting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们要将训练数据与验证数据和测试数据分开的原因；如果算法在测试数据上的准确度与训练数据上的准确度不相似，那将是一个很好的迹象，表明模型存在过拟合问题。我们需要确保不犯相反的错误——即模型欠拟合。然而，在实际操作中，如果我们旨在使我们的预测模型在训练数据上尽可能准确，欠拟合的风险要小得多，而且我们会特别注意避免过拟合。
- en: 'The following figure depicts underfitting:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了欠拟合：
- en: '![Figure 1.10 – Underﬁtting can be a problem as well](img/B19627_01_10.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10 – 欠拟合也可能是一个问题](img/B19627_01_10.jpg)'
- en: Figure 1.10 – Underﬁtting can be a problem as well
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 欠拟合也可能是一个问题
- en: Neural networks
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: We introduced some of the popular classical ML algorithms in the previous sections.
    In this section, we’ll talk about NNs, which are the focus of this book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了一些流行的经典机器学习算法。在本章节中，我们将讨论神经网络（NN），这是本书的重点。
- en: The first example of an NN is called the perceptron, and this was invented by
    Frank Rosenblatt in 1957\. The perceptron is a classification algorithm that is
    very similar to logistic regression. Similar to logistic regression, it has a
    vector of weights, **w**, and its output is a function, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/24.png),
    of the dot product, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:math>](img/25.png) (or ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><mrow><msubsup><mo>∑</mo><mi>i</mi><mrow /></msubsup><msub><mi>w</mi><mi>i</mi></msub></mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></math>](img/26.png)),
    of the weights and input.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的第一个例子叫做感知器，这是由 Frank Rosenblatt 在1957年发明的。感知器是一种分类算法，非常类似于逻辑回归。与逻辑回归类似，它有一个权重向量
    **w**，其输出是一个函数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/24.png)，即点积，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi></mml:math>](img/25.png)（或者![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced open="("
    close=")"><mrow><mrow><msubsup><mo>∑</mo><mi>i</mi><mrow /></msubsup><msub><mi>w</mi><mi>i</mi></msub></mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></math>](img/26.png))，表示权重和输入的点积。
- en: The only difference is that *f* is a simple step function – that is, if ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold">x</mi><mo>⋅</mo><mi
    mathvariant="bold">w</mi><mo>></mo><mn>0</mn></mrow></mrow></math>](img/27.png),
    then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/28.png),
    or else
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是 *f* 是一个简单的阶跃函数——也就是说，如果![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="bold">x</mi><mo>⋅</mo><mi mathvariant="bold">w</mi><mo>></mo><mn>0</mn></mrow></mrow></math>](img/27.png)，则![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/28.png)，否则
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/29.png),
    wherein we apply a similar logistic regression rule over the output of the logistic
    function. The perceptron is an example of a simple one-layer neural feedforward
    network:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/29.png)，其中我们对逻辑函数的输出应用类似的逻辑回归规则。感知器是一个简单的单层前馈神经网络示例：'
- en: '![Figure 1.11 – A simple perceptron with three inputs and one output](img/B19627_01_11.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.11 – 一个简单的感知器，具有三个输入和一个输出](img/B19627_01_11.jpg)'
- en: Figure 1.11 – A simple perceptron with three inputs and one output
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – 一个简单的感知器，具有三个输入和一个输出
- en: The perceptron was very promising, but it was soon discovered that it has serious
    limitations as it only works for linearly separable classes. In 1969, Marvin Minsky
    and Seymour Paper demonstrated that it could not learn even a simple logical function
    such as XOR. This led to a significant decline in the interest in perceptrons.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器非常有前景，但很快就被发现它有严重的局限性，因为它只适用于线性可分的类别。在1969年，Marvin Minsky 和 Seymour Paper
    证明了它甚至无法学习像 XOR 这样的简单逻辑函数。这导致了对感知器的兴趣大幅下降。
- en: However, other NNs can solve this problem. A classic multilayer perceptron has
    multiple interconnected perceptrons, such as units that are organized in different
    sequential layers (input layer, one or more hidden layers, and an output layer).
    Each unit of a layer is connected to all units of the next layer. First, the information
    is presented to the input layer, then we use it to compute the output (or activation),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/30.png),
    for each unit of the first hidden layer. We propagate forward, with the output
    as input for the next layers in the network (hence feedforward), and so on until
    we reach the output. The most common way to train NNs is by using gradient descent
    in combination with backpropagation. We’ll discuss this in detail in [*Chapter
    2*](B19627_02.xhtml#_idTextAnchor047).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他神经网络可以解决这个问题。经典的多层感知器（MLP）包含多个相互连接的感知器，例如组织在不同顺序层中的单元（输入层、一个或多个隐藏层以及输出层）。每一层的单元都与下一层的所有单元相连接。首先，信息呈现给输入层，然后我们用它计算输出（或激活值），![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/30.png)，用于第一个隐藏层的每个单元。我们向前传播，输出作为下一层输入（因此叫做前向传播），以此类推，直到达到输出层。训练神经网络最常见的方法是结合使用梯度下降和反向传播。我们将在[*第二章*](B19627_02.xhtml#_idTextAnchor047)中详细讨论这个过程。
- en: 'The following diagram depicts an NN with one hidden layer:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示描绘了一个具有隐藏层的神经网络：
- en: "![Figure 1.12 – \uFEFFAn NN with one hidden layer](img/B19627_01_12.jpg)"
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – 一个具有隐藏层的神经网络](img/B19627_01_12.jpg)'
- en: Figure 1.12 – An NN with one hidden layer
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – 一个具有隐藏层的神经网络
- en: Think of the hidden layers as an abstract representation of the input data.
    This is the way the NN understands the features of the data with its internal
    logic. However, NNs are non-interpretable models. This means that if we observe
    the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/31.png)
    activations of the hidden layer, we wouldn’t be able to understand them. For us,
    they are just a vector of numerical values. We need the output layer to bridge
    the gap between the network’s representation and the actual data we’re interested
    in. You can think of this as a translator; we use it to understand the network’s
    logic, and at the same time, we can convert it into the actual target values that
    we are interested in.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将隐藏层视为输入数据的抽象表示。这是神经网络用其内部逻辑理解数据特征的方式。然而，神经网络是不可解释的模型。这意味着，如果我们观察隐藏层的![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/31.png)激活值，我们是无法理解它们的。对我们来说，它们只是一个数值向量。我们需要输出层来弥合网络表示与我们实际关注的数据之间的差距。你可以把它当作一个翻译器；我们用它来理解网络的逻辑，同时也能将其转换为我们关注的实际目标值。
- en: The universal approximation theorem tells us that a feedforward network with
    one hidden layer can represent any function. It’s good to know that there are
    no theoretical limits on networks with one hidden layer, but in practice, we can
    achieve limited success with such architectures. In [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    we’ll discuss how to achieve better performance with deep NNs, and their advantages
    over the shallow ones. For now, let’s apply our knowledge by solving a simple
    classification task with an NN.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近定理告诉我们，一个具有一个隐藏层的前馈网络可以表示任何函数。值得注意的是，理论上，具有一个隐藏层的网络没有限制，但在实际应用中，使用这种架构的成功是有限的。在[*第三章*](B19627_03.xhtml#_idTextAnchor079)中，我们将讨论如何通过深度神经网络来实现更好的性能，并与浅层神经网络相比的优势。目前，让我们通过解决一个简单的分类任务来应用我们的知识。
- en: Introducing PyTorch
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入 PyTorch
- en: In this section, we’ll introduce PyTorch – an open source Python DL framework
    that was developed primarily by Facebook that has been gaining momentum recently.
    It provides **graphics processing unit** (**GPU**) accelerated multidimensional
    array (or tensor) operations, and computational graphs, which we can use to build
    NNs. Throughout this book, we’ll use PyTorch and Keras, and we’ll talk about these
    libraries and compare them in detail in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍PyTorch——一个由Facebook开发的开源Python深度学习框架，近年来获得了广泛关注。它提供**图形处理单元**（**GPU**）加速的多维数组（或张量）运算和计算图，我们可以用它来构建神经网络。在本书中，我们将使用PyTorch和Keras，并将在[*第3章*](B19627_03.xhtml#_idTextAnchor079)中详细讨论和比较这两个库。
- en: 'Let’s create a simple NN that will classify the Iris flower dataset. The steps
    are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个简单的神经网络来分类鸢尾花数据集。步骤如下：
- en: 'Start by loading the dataset:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载数据集：
- en: '[PRE2]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code is boilerplate code that downloads the Iris dataset’s CSV
    file and then loads it into a `dataset`. Then, we shuffle the DataFrame’s rows
    and split the code into NumPy arrays, `train_input`/`train_target` (flower properties/flower
    class), for the training data and `test_input`/`test_target` for the test data.
    We’ll use 120 samples for training and 30 for testing. If you are not familiar
    with pandas, think of this as an advanced version of NumPy.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码是下载鸢尾花数据集CSV文件并将其加载到`dataset`中的模板代码。然后，我们将DataFrame的行进行洗牌，并将代码拆分为NumPy数组，`train_input`/`train_target`（花卉特征/花卉类别）用于训练数据，`test_input`/`test_target`用于测试数据。我们将使用120个样本进行训练，30个样本用于测试。如果你不熟悉pandas，可以把它当作NumPy的高级版本。
- en: 'Next, define our first NN. We’ll use a feedforward network with one hidden
    layer with five units, a **ReLU** activation function (this is just another type
    of activation, defined simply as *f(x) = max(0, x)*), and an output layer with
    three units. The output layer has three units, and each unit corresponds to one
    of the three classes of Iris flowers. The following is the PyTorch definition
    of the network:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义我们的第一个神经网络。我们将使用一个前馈网络，包含一个具有五个单元的隐藏层，一个**ReLU**激活函数（这只是另一种激活函数，简单定义为*f(x)
    = max(0, x)*），以及一个具有三个单元的输出层。输出层有三个单元，每个单元对应一种鸢尾花的类别。以下是PyTorch定义的网络：
- en: '[PRE3]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ll use `Iris Setosa = [1, 0, 0]`, `Iris Versicolor = [0, 1, 0]`, and `Iris
    Virginica = [0, 0, 1]`), and one element of the array will be the target for one
    unit of the output layer. When the network classifies a new sample, we’ll determine
    the class by taking the unit with the highest activation value. `torch.manual_seed(1234)`
    enables us to use the same random data seed every time for the reproducibility
    of results.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用`Iris Setosa = [1, 0, 0]`，`Iris Versicolor = [0, 1, 0]`，和`Iris Virginica
    = [0, 0, 1]`，其中数组中的每个元素将是输出层单元的目标。当网络对新样本进行分类时，我们通过选择激活值最高的单元来确定类别。`torch.manual_seed(1234)`使我们每次都能使用相同的随机数据种子，从而保证结果的可重复性。
- en: 'Choose the `loss` function:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`loss`函数：
- en: '[PRE4]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With the `criterion` variable, we define the loss function as **cross-entropy
    loss**. The loss function will measure how different the output of the network
    is compared to the target data.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`criterion`变量，我们将损失函数定义为**交叉熵损失**。损失函数将衡量网络输出与目标数据之间的差异。
- en: 'Define the **stochastic gradient descent** (**SGD**) optimizer (a variation
    of the gradient descent algorithm) with a **learning rate** of 0.1 and a **momentum**
    of 0.9 (we’ll discuss SGD and its parameters in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)):'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义**随机梯度下降**（**SGD**）优化器（梯度下降算法的一种变体），学习率为0.1，动量为0.9（我们将在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中讨论SGD及其参数）：
- en: '[PRE5]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Train the network:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE6]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We’ll run the training for 50 epochs, which means that we’ll iterate 50 times
    over the training dataset:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将训练50个周期，也就是对训练数据集迭代50次：
- en: Create the `torch` variables from the NumPy array – that is, `train_input` and
    `train_target`.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从NumPy数组创建`torch`变量——即`train_input`和`train_target`。
- en: Zero the gradients of the optimizer to prevent accumulation from the previous
    iterations. We feed the training data to the NN, `net(inputs)`, and we compute
    the loss function’s `criterion` (`out`, `targets`) between the network output
    and the target data.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器的梯度归零，以防止上一次迭代的梯度累积。我们将训练数据输入到神经网络中，`net(inputs)`，并计算损失函数的`criterion`（`out`，`targets`），即网络输出和目标数据之间的差异。
- en: Propagate the `loss` value back through the network. We’re doing this so that
    we can calculate how each network weight affects the loss function.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`loss`值反向传播通过网络。我们这么做是为了计算每个网络权重如何影响损失函数。
- en: The optimizer updates the weights of the network in a way that will reduce the
    future loss function’s values.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器以一种能够减少未来损失函数值的方式更新网络的权重。
- en: 'When we run the training, the output will be as follows:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们运行训练时，输出将如下所示：
- en: '[PRE7]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following graph shows how the loss function decreases with each epoch.
    This shows how the network gradually learns the training data:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了损失函数随着每个训练周期的减少。这展示了网络是如何逐渐学习训练数据的：
- en: '![Figure 1.13 – The loss function decreases with each epoch](img/B19627_01_13.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图1.13 – 损失函数随着每个训练周期的减少](img/B19627_01_13.jpg)'
- en: Figure 1.13 – The loss function decreases with each epoch
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13 – 损失函数随着每个训练周期的减少
- en: 'Let’s see what the final accuracy of our model is:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看模型的最终准确率是多少：
- en: '[PRE8]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We do this by feeding the test set to the network and computing the error manually.
    The output is as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过将测试集输入网络并手动计算误差来实现这一点。输出如下：
- en: '[PRE9]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We were able to classify all 30 test samples correctly.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们成功地正确分类了所有30个测试样本。
- en: We must also try different hyperparameters of the network and see how the accuracy
    and loss functions work. You could try changing the number of units in the hidden
    layer, the number of epochs we train in the network, as well as the learning rate.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须尝试不同的网络超参数，并观察准确率和损失函数的变化。你可以尝试改变隐藏层中的单元数、训练周期数以及学习率。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered what ML is and why it’s so important. We talked about the
    main classes of ML techniques and some of the most popular classic ML algorithms.
    We also introduced a particular type of ML algorithm, called NNs, which is the
    basis for DL. Then, we looked at a coding example where we used a popular ML library
    to solve a particular classification problem.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了什么是机器学习（ML）以及它为什么如此重要。我们讨论了机器学习技术的主要类别以及一些最受欢迎的经典机器学习算法。我们还介绍了一种特定类型的机器学习算法，称为神经网络（NNs），它是深度学习（DL）的基础。然后，我们看了一个编码示例，展示了如何使用流行的机器学习库来解决一个特定的分类问题。
- en: In the next chapter, we’ll cover NNs in more detail and explore their theoretical
    justifications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更详细地讨论神经网络（NNs）并探讨它们的理论依据。
