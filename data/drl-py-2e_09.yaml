- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Deep Q Network and Its Variants
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 网络及其变体
- en: In this chapter, let's get started with one of the most popular **Deep Reinforcement
    Learning** (**DRL**) algorithms called **Deep Q Network** (**DQN**). Understanding
    DQN is very important as many of the state-of-the-art DRL algorithms are based
    on DQN. The DQN algorithm was first proposed by researchers at Google's DeepMind
    in 2013 in the paper *Playing Atari with Deep Reinforcement Learning.* They described
    the DQN architecture and explained why it was so effective at playing Atari games
    with human-level accuracy. We begin the chapter by learning what exactly a deep
    Q network is, and how it is used in reinforcement learning. Next, we will deep
    dive into the algorithm of DQN. Then we will learn how to implement DQN to play Atari
    games.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从一种非常流行的**深度强化学习**（**DRL**）算法——**深度 Q 网络**（**DQN**）开始。理解 DQN 非常重要，因为许多最先进的
    DRL 算法都是基于 DQN 的。DQN 算法首次由 Google 的 DeepMind 研究人员于 2013 年在论文《*通过深度强化学习玩 Atari
    游戏*》中提出。他们描述了 DQN 架构，并解释了为什么它能以接近人类水平的准确度玩 Atari 游戏。本章首先将介绍深度 Q 网络到底是什么，以及它如何在强化学习中使用。接下来，我们将深入探讨
    DQN 的算法。然后，我们将学习如何实现 DQN 来玩 Atari 游戏。
- en: After learning about DQN, we will cover several variants of DQN, such as double
    DQN, DQN with prioritized experience replay, dueling DQN, and the deep recurrent
    Q network in detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了 DQN 后，我们将详细讲解几种 DQN 的变体，如双重 DQN、带优先经验重放的 DQN、对战 DQN 和深度递归 Q 网络。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is DQN?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 DQN？
- en: The DQN algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN 算法
- en: Playing Atari games with DQN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DQN 玩 Atari 游戏
- en: Double DQN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双重 DQN
- en: DQN with prioritized experience replay
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带优先经验重放的 DQN
- en: The dueling DQN
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对战 DQN
- en: The deep recurrent Q network
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度递归 Q 网络
- en: What is DQN?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 DQN？
- en: 'The objective of reinforcement learning is to find the optimal policy, that
    is, the policy that gives us the maximum return (the sum of rewards of the episode).
    In order to compute the policy, first we compute the Q function. Once we have
    the Q function, then we extract the policy by selecting an action in each state
    that has the maximum Q value. For instance, let''s suppose we have two states
    **A** and **B** and our action space consists of two actions; let the actions
    be *up* and *down*. So, in order to find which action to perform in state **A**
    and **B**, first we compute the Q value of all state-action pairs, as *Table 9.1*
    shows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到最优策略，即给我们带来最大回报（本回合奖励总和）的策略。为了计算策略，首先我们需要计算 Q 函数。一旦我们得到了 Q 函数，就可以通过在每个状态中选择具有最大
    Q 值的动作来提取策略。例如，假设我们有两个状态 **A** 和 **B**，并且我们的动作空间由两个动作组成；假设这两个动作是 *上* 和 *下*。因此，为了找出在状态
    **A** 和 **B** 中应该执行哪个动作，我们首先计算所有状态-动作对的 Q 值，如 *表 9.1* 所示：
- en: '![](img/B15558_09_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_01.png)'
- en: 'Table 9.1: Q-value of state-action pairs'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1：状态-动作对的 Q 值
- en: Once we have the Q value of all state-action pairs, then we select the action
    in each state that has the maximum Q value. So, we select the action *up* in state
    **A** and *down* in state **B** as they have the maximum Q value. We improve the
    Q function on every iteration and once we have the optimal Q function, then we
    can extract the optimal policy from it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了所有状态-动作对的 Q 值，我们将在每个状态中选择具有最大 Q 值的动作。因此，我们在状态 **A** 中选择动作 *上*，在状态 **B**
    中选择动作 *下*，因为它们具有最大 Q 值。我们在每次迭代中改进 Q 函数，一旦获得了最优 Q 函数，就可以从中提取最优策略。
- en: 'Now, let''s revisit our grid world environment, as shown in *Figure 9.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新审视我们的网格世界环境，如 *图 9.1* 所示：
- en: '![](img/B15558_09_02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_02.png)'
- en: 'Figure 9.1: Grid world environment'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：网格世界环境
- en: We learned that in the grid world environment, the goal of our agent is to reach
    state **I** from state **A** without visiting the shaded states, and in each state,
    the agent has to perform one of the four actions—*up*, *down*, *left*, *right*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在网格世界环境中，智能体的目标是从状态 **A** 到达状态 **I**，并且不能经过阴影状态，在每个状态中，智能体必须执行四个动作之一——*上*、*下*、*左*、*右*。
- en: To compute the policy, first we compute the Q values of all state-action pairs.
    Here, the number of states is 9 (**A** to **I**) and we have 4 actions in our
    action space, so our Q table will consist of 9 x 4 = 36 rows containing the Q
    values of all possible state-action pairs. Once we obtain the Q values, then we
    extract the policy by selecting the action in each state that has the maximum
    Q value. But is it a good approach to compute the Q value exhaustively for all
    state-action pairs? Let's explore this in more detail.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算策略，首先我们计算所有状态-动作对的 Q 值。这里，状态的数量是 9（**A** 到 **I**），我们的动作空间中有 4 个动作，因此我们的 Q
    表将包含 9 x 4 = 36 行，包含所有可能的状态-动作对的 Q 值。一旦我们获得 Q 值，我们就可以通过选择每个状态下具有最大 Q 值的动作来提取策略。但是，全面计算所有状态-动作对的
    Q 值是否是一种好的方法呢？让我们更详细地探讨一下这个问题。
- en: Let's suppose we have an environment where we have 1,000 states and 50 possible
    actions in each state. In this case, our Q table will consist of 1,000 x 50 =
    50,000 rows containing the Q values of all possible state-action pairs. In cases
    like this, where our environment consists of a large number of states and actions,
    it will be very expensive to compute the Q values of all possible state-action
    pairs in an exhaustive fashion.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个环境，其中有 1,000 个状态，每个状态下有 50 个可能的动作。在这种情况下，我们的 Q 表将包含 1,000 x 50 = 50,000
    行，包含所有可能的状态-动作对的 Q 值。在这种环境中，状态和动作数量非常大时，全面计算所有可能的状态-动作对的 Q 值将非常昂贵。
- en: Instead of computing Q values in this way, can we approximate them using any
    function approximator, such as a neural network? Yes! We can parameterize our
    Q function by a parameter ![](img/B15558_09_001.png) and compute the Q value where
    the parameter ![](img/B15558_09_002.png) is just the parameter of our neural network.
    So, we just feed the state of the environment to a neural network and it will
    return the Q value of all possible actions in that state. Once we obtain the Q
    values, then we can select the best action as the one that has the maximum Q value.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不采用这种方式计算 Q 值，我们能否使用任何函数逼近器来近似它们，例如神经网络？当然可以！我们可以用参数 ![](img/B15558_09_001.png)
    来对 Q 函数进行参数化，并计算 Q 值，其中参数 ![](img/B15558_09_002.png) 只是我们神经网络的参数。因此，我们只需将环境的状态输入到神经网络中，它就会返回该状态下所有可能动作的
    Q 值。一旦我们获得 Q 值，就可以选择 Q 值最大的动作作为最优动作。
- en: 'For example, let''s consider our grid world environment. As *Figure 9.2* shows,
    we just feed state **D** as an input to the network and it returns the Q value
    of all actions in state **D**, which are *up*, *down*, *left*, and *right*, as
    output. Then, we select the action that has the maximum Q value. Since action
    *right* has a maximum Q value, we select action *right* in the state **D**:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们考虑我们的网格世界环境。如*图 9.2*所示，我们只需将状态**D**作为输入传递给网络，它就会返回状态**D**下所有动作的 Q 值，分别是*向上*、*向下*、*向左*和*向右*。然后，我们选择具有最大
    Q 值的动作。由于动作*向右*的 Q 值最大，我们在状态**D**下选择动作*向右*：
- en: '![](img/B15558_09_03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_03.png)'
- en: 'Figure 9.2: Deep Q network'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：深度 Q 网络
- en: Since we are using a neural network to approximate the Q value, the neural network
    is called the Q network, and if we use a deep neural network to approximate the
    Q value, then the deep neural network is called a **deep Q network** (**DQN**).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用神经网络来近似 Q 值，因此神经网络被称为 Q 网络，如果我们使用深度神经网络来近似 Q 值，那么这个深度神经网络就被称为**深度 Q 网络**（**DQN**）。
- en: We can denote our Q function by ![](img/B15558_09_003.png), where the parameter
    ![](img/B15558_09_004.png) in subscript indicates that our Q function is parameterized
    by ![](img/B15558_09_004.png), and ![](img/B15558_09_006.png) is just the parameter
    of our neural network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 ![](img/B15558_09_003.png) 来表示我们的 Q 函数，其中下标中的参数 ![](img/B15558_09_004.png)
    表示我们的 Q 函数由 ![](img/B15558_09_004.png) 参数化，且 ![](img/B15558_09_006.png) 只是我们神经网络的参数。
- en: We initialize the network parameter ![](img/B15558_09_004.png) with random values
    and approximate the Q function (Q values), but since we initialized ![](img/B15558_09_008.png)
    with random values, the approximated Q function will not be optimal. So, we train
    the network for several iterations by finding the optimal parameter ![](img/B15558_09_002.png).
    Once we find the optimal ![](img/B15558_09_004.png), we will have the optimal
    Q function. Then we can extract the optimal policy from the optimal Q function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将网络参数 ![](img/B15558_09_004.png) 初始化为随机值，并近似 Q 函数（Q 值），但是由于我们将 ![](img/B15558_09_008.png)
    初始化为随机值，近似的 Q 函数将不是最优的。因此，我们通过多次迭代训练网络，找到最优的参数 ![](img/B15558_09_002.png)。一旦找到最优的
    ![](img/B15558_09_004.png)，我们就得到了最优的 Q 函数。然后我们可以从最优的 Q 函数中提取最优策略。
- en: Okay, but how can we train our network? What about the training data and the
    loss function? Is it a classification or regression task? Now that we have a basic
    understanding of how DQN works, in the next section, we will get into the details
    and address all these questions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但我们怎么训练我们的网络呢？训练数据和损失函数是什么？是分类任务还是回归任务？现在我们基本理解了 DQN 的工作原理，接下来的部分，我们将深入细节并解答所有这些问题。
- en: Understanding DQN
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 DQN
- en: In this section, we will understand how exactly DQN works. We learned that we
    use DQN to approximate the Q value of all the actions in the given input state.
    The Q value is just a continuous number, so we are essentially using our DQN to
    perform a regression task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解 DQN 是如何工作的。我们知道，使用 DQN 来近似给定输入状态下所有动作的 Q 值。Q 值只是一个连续的数字，因此我们本质上是使用
    DQN 执行一个回归任务。
- en: Okay, what about the training data? We use a buffer called a replay buffer to
    collect the agent's experience and, based on this experience, we train our network.
    Let's explore the replay buffer in detail.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么训练数据呢？我们使用一个叫做重放缓冲区的缓冲区来收集智能体的经验，并根据这些经验来训练我们的网络。让我们详细探讨重放缓冲区。
- en: Replay buffer
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重放缓冲区
- en: 'We know that the agent makes a transition from a state *s* to the next state
    ![](img/B15558_09_011.png) by performing some action *a*, and then receives a
    reward *r*. We can save this transition information ![](img/B15558_09_012.png)
    in a buffer called a replay buffer or experience replay. The replay buffer is
    usually denoted by ![](img/B15558_09_013.png). This transition information is
    basically the agent''s experience. We store the agent''s experience over several
    episodes in the replay buffer. The key idea of using the replay buffer to store
    the agent''s experience is that we can train our DQN with experience (transition)
    sampled from the buffer. A replay buffer is shown here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，智能体通过执行某个动作 *a* 从状态 *s* 转移到下一个状态 ![](img/B15558_09_011.png)，然后获得奖励 *r*。我们可以将这个转换信息
    ![](img/B15558_09_012.png) 保存到一个叫做重放缓冲区或经验回放的缓冲区中。重放缓冲区通常用 ![](img/B15558_09_013.png)
    表示。这个转换信息基本上就是智能体的经验。我们将在多个回合中将智能体的经验存储在重放缓冲区中。使用重放缓冲区存储智能体经验的关键思想是，我们可以通过从缓冲区中采样经验（转换）来训练我们的
    DQN。一个重放缓冲区如下所示：
- en: '![](img/B15558_09_04.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_04.png)'
- en: 'Figure 9.3: Replay buffer'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：重放缓冲区
- en: 'The following steps help us to understand how we store the transition information
    in the replay buffer ![](img/B15558_09_014.png):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤帮助我们理解如何将转换信息保存在重放缓冲区中 ![](img/B15558_09_014.png)：
- en: Initialize the replay buffer ![](img/B15558_09_015.png).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_015.png)。
- en: For each episode perform *step 3*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一轮执行 *步骤 3*。
- en: 'For each step in the episode:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步：
- en: Make a transition, that is, perform an action *a* in the state *s*, move to
    the next state ![](img/B15558_09_016.png), and receive the reward *r*.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一次转换，即在状态 *s* 下执行一个动作 *a*，移动到下一个状态 ![](img/B15558_09_016.png)，并获得奖励 *r*。
- en: Store the transition information ![](img/B15558_09_017.png) in the replay buffer
    ![](img/B15558_09_018.png).
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转换信息 ![](img/B15558_09_017.png) 存储到重放缓冲区 ![](img/B15558_09_018.png) 中。
- en: 'As explained in the preceding steps, we collect the agent''s transition information
    over many episodes and save it in the replay buffer. To understand this clearly,
    let''s consider our favorite grid world environment. Let''s suppose we have the
    following two episodes/trajectories:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面步骤所解释的，我们在多轮中收集智能体的转换信息，并将其保存在重放缓冲区中。为了更清楚地理解这一点，假设我们有以下两个回合/轨迹：
- en: '**Episode 1**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**回合 1**：'
- en: '![](img/B15558_09_05.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_05.png)'
- en: 'Figure 9.4: Trajectory 1'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：轨迹 1
- en: '**Episode 2**:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**回合 2**：'
- en: '![](img/B15558_09_06.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_06.png)'
- en: 'Figure 9.5: Trajectory 2'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：轨迹 2
- en: 'Now, this information will be stored in the replay buffer, as *Figure 9.6*
    shows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些信息将存储在重放缓冲区中，正如 *图 9.6* 所示：
- en: '![](img/B15558_09_07.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_07.png)'
- en: 'Figure 9.6: Replay buffer'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：重放缓冲区
- en: As *Figure 9.6* shows, we store the transition information by stacking it sequentially
    one after another. We train the network by sampling a minibatch of transitions
    from the replay buffer. Wait! There is a small issue here. Since we are stacking
    up the agent's experience (transition) one after another sequentially, the agent's
    experience will be highly correlated. For example, as shown in the preceding figure,
    transitions will be correlated with the rows above and below. If we train our
    network with this correlated experience then our neural network will easily overfit.
    So, to combat this, we sample a random minibatch of transitions from the replay
    buffer and train the network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.6*所示，我们通过将过渡信息按顺序堆叠来存储它。我们通过从重放缓冲区中采样一个小批量的过渡来训练网络。等等！这里有一个小问题。由于我们将智能体的经验（过渡）一个接一个地按顺序堆叠，智能体的经验将高度相关。例如，如前图所示，过渡将与上下行的行相关。如果我们用这些相关的经验来训练网络，那么我们的神经网络将很容易出现过拟合。因此，为了应对这一问题，我们从重放缓冲区中随机采样一个小批量的过渡来训练网络。
- en: Note that the replay buffer is of limited size, that is, a replay buffer will
    store only a fixed amount of the agent's experience. So, when the buffer is full
    we replace the old experience with new experience. A replay buffer is usually
    implemented as a queue structure (first in first out) rather than a list. So,
    if the buffer is full when new experience comes in, we remove the old experience
    and add the new experience into the buffer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，重放缓冲区的大小是有限的，也就是说，重放缓冲区只会存储固定量的智能体经验。因此，当缓冲区满时，我们会用新经验替换旧经验。重放缓冲区通常实现为队列结构（先进先出）而不是列表。因此，如果缓冲区已满，当新经验进入时，我们会移除旧经验并将新经验添加到缓冲区中。
- en: We have learned that we train our network by randomly sampling a minibatch of
    experience from the buffer. But how exactly does the training happen? How does
    our network learn to approximate the optimal Q function using this minibatch of
    samples? This is exactly what we discuss in the next section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，我们通过从缓冲区随机采样一个小批量的经验来训练网络。那么，训练到底是如何进行的呢？我们的网络是如何利用这个小批量的样本来近似最优Q函数的呢？这正是我们在下一节中讨论的内容。
- en: Loss function
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'We learned that in DQN, our goal is to predict the Q value, which is just a
    continuous value. Thus, in DQN we basically perform a regression task. We generally
    use the **mean squared error** (**MSE**) as the loss function for the regression
    task. MSE can be defined as the average squared difference between the target
    value and the predicted value, as shown here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，在DQN中，我们的目标是预测Q值，这只是一个连续值。因此，在DQN中，我们基本上执行的是一个回归任务。我们通常使用**均方误差**（**MSE**）作为回归任务的损失函数。MSE可以定义为目标值与预测值之间的平均平方差，如下所示：
- en: '![](img/B15558_09_019.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_019.png)'
- en: Where *y* is the target value, ![](img/B15558_09_020.png) is the predicted value,
    and *K* is the number of training samples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*是目标值，！[](img/B15558_09_020.png)是预测值，*K*是训练样本的数量。
- en: 'Now, let''s learn how to use MSE in the DQN and train the network. We can train
    our network by minimizing the MSE between the target Q value and predicted Q value.
    First, how can we obtain the target Q value? Our target Q value should be the
    optimal Q value so that we can train our network by minimizing the error between
    the optimal Q value and predicted Q value. But how can we compute the optimal
    Q value? This is where the Bellman equation helps us. In *Chapter 3*, *The Bellman
    Equation and Dynamic Programming*, we learned that the optimal Q value can be
    obtained using the Bellman optimality equation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何在DQN中使用MSE并训练网络。我们可以通过最小化目标Q值和预测Q值之间的MSE来训练网络。首先，我们如何获得目标Q值呢？我们的目标Q值应该是最优Q值，这样我们就可以通过最小化最优Q值和预测Q值之间的误差来训练网络。但是我们如何计算最优Q值呢？这就是贝尔曼方程帮助我们的地方。在*第3章*，*贝尔曼方程与动态规划*中，我们了解到最优Q值可以通过贝尔曼最优性方程来获得：
- en: '![](img/B15558_09_021.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_021.png)'
- en: 'where ![](img/B15558_09_022.png) represents the immediate reward *r* that we
    obtain while performing an action *a* in state *s* and moving to the next state
    ![](img/B15558_09_016.png), so we can just denote ![](img/B15558_09_024.png) by
    *r*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/B15558_09_022.png)表示在执行动作*a*时，我们在状态*s*下获得的即时奖励*r*，并移动到下一个状态！[](img/B15558_09_016.png)，因此我们可以将！[](img/B15558_09_024.png)表示为*r*：
- en: '![](img/B15558_09_025.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_025.png)'
- en: In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer and
    taking the average value; we will learn more about this in a while.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，我们可以去除期望值。我们将通过从重放缓冲区随机采样 *K* 个转移，并取平均值来近似期望值；稍后我们将学习更多内容。
- en: 'Thus, according to the Bellman optimality equation, the optimal Q value is
    just the sum of the reward and the discounted maximum Q value of the next state-action
    pair, that is:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据 Bellman 最优方程，最优 Q 值就是奖励与下一个状态-动作对的折扣最大 Q 值之和，即：
- en: '![](img/B15558_09_026.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_026.png)'
- en: 'So, we can define our loss as the difference between the target value (the
    optimal Q value) and the predicted value (the Q value predicted by the DQN) and
    express the loss function *L* as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将损失定义为目标值（最优 Q 值）与预测值（DQN 预测的 Q 值）之间的差异，并将损失函数 *L* 表示为：
- en: '![](img/B15558_09_027.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_027.png)'
- en: 'Substituting equation (1) in the preceding equation, we can write:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 (1) 代入上述方程，我们可以写出：
- en: '![](img/B15558_09_028.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_028.png)'
- en: We know that we compute the predicted Q value using the network parameterized
    by ![](img/B15558_09_029.png). How can we compute the target value? That is, we
    learned that the target value is the sum of the reward and the discounted maximum
    Q value of the next state-action pair. How do we compute the Q value of the next
    state-action pair?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们使用由 ![](img/B15558_09_029.png) 参数化的网络来计算预测的 Q 值。那么我们如何计算目标值呢？也就是说，我们已经学到，目标值是奖励与下一个状态-动作对的折扣最大
    Q 值之和。我们如何计算下一个状态-动作对的 Q 值呢？
- en: '![](img/B15558_09_19.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_19.png)'
- en: 'Similar to the predicted Q value, we can compute the Q value of the next state-action
    pair in the target using the same DQN parameterized by ![](img/B15558_09_001.png).
    So, we can rewrite our loss function as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于预测的 Q 值，我们可以使用相同的 DQN 参数化来计算目标中下一个状态-动作对的 Q 值！[](img/B15558_09_001.png)。因此，我们可以将损失函数重写为：
- en: '![](img/B15558_09_031.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_031.png)'
- en: As shown, both the target value and the predicted Q value are parameterized
    by ![](img/B15558_09_006.png).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，目标值和预测的 Q 值都由 ![](img/B15558_09_006.png) 参数化。
- en: 'Instead of computing the loss as just the difference between the target Q value
    and the predicted Q value, we use MSE as our loss function. We learned that we
    store the agent''s experience in a buffer called a replay buffer. So, we randomly
    sample a minibatch of *K* number of transitions ![](img/B15558_09_012.png) from
    the replay buffer and train the network by minimizing the MSE, as shown here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再仅仅计算目标 Q 值和预测 Q 值之间的差异来计算损失，而是使用均方误差（MSE）作为我们的损失函数。我们已经学到，我们将智能体的经验存储在一个叫做重放缓冲区的缓冲区中。因此，我们从重放缓冲区随机采样一个
    *K* 数量的转移小批量！[](img/B15558_09_012.png)，并通过最小化 MSE 来训练网络，如下所示：
- en: '![](img/B15558_09_08.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_08.png)'
- en: 'Figure 9.7: Loss function of DQN'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：DQN 的损失函数
- en: 'Thus, our loss function can be expressed as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的损失函数可以表示为：
- en: '![](img/B15558_09_034.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_034.png)'
- en: 'For simplicity of notation, we can denote the target value by *y* and rewrite
    the preceding equation as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号表示，我们可以将目标值表示为 *y*，并将上述方程重写为：
- en: '![](img/B15558_09_035.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_035.png)'
- en: 'Where ![](img/B15558_09_036.png). We have learned that the target value is
    just the sum of the reward and the discounted maximum Q value of the next state-action
    pair. But what if the next state ![](img/B15558_03_034.png) is a terminal state?
    If the next state ![](img/B15558_03_034.png) is terminal then we cannot compute
    the Q value as we don''t take any action in the terminal state, so in that case,
    the target value will be just the reward, as shown here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_09_036.png)。我们已经学到，目标值就是奖励与下一个状态-动作对的折扣最大 Q 值之和。但是如果下一个状态
    ![](img/B15558_03_034.png) 是终止状态呢？如果下一个状态 ![](img/B15558_03_034.png) 是终止状态，则我们无法计算
    Q 值，因为我们在终止状态中不进行任何动作，因此在这种情况下，目标值将只是奖励，如下所示：
- en: '![](img/B15558_09_039.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_039.png)'
- en: 'Thus, our loss function is given as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的损失函数表示为：
- en: '![](img/B15558_09_035.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_035.png)'
- en: 'We train our network by minimizing the loss function. We can minimize the loss
    function by finding the optimal parameter ![](img/B15558_09_002.png). So, we use
    gradient descent to find the optimal parameter ![](img/B15558_09_042.png). We
    compute the gradient of our loss function ![](img/B15558_09_043.png) and update
    our network parameter ![](img/B15558_09_044.png) as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最小化损失函数来训练我们的网络。我们可以通过寻找最优参数 ![](img/B15558_09_002.png) 来最小化损失函数。因此，我们使用梯度下降法来寻找最优参数
    ![](img/B15558_09_042.png)。我们计算损失函数的梯度 ![](img/B15558_09_043.png)，并按如下方式更新我们的网络参数
    ![](img/B15558_09_044.png)：
- en: '![](img/B15558_09_045.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_045.png)'
- en: Target network
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标网络
- en: 'In the last section, we learned that we train the network by minimizing the
    loss function, which is the MSE between the target value and the predicted value,
    as shown here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学到了通过最小化损失函数来训练网络，该损失函数是目标值与预测值之间的均方误差（MSE），如下所示：
- en: '![](img/B15558_09_034.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_034.png)'
- en: 'However, there is a small issue with our loss function. We have learned that
    the target value is just the sum of the reward and the discounted maximum Q value
    of the next state-action pair. We compute this Q value of the next state-action
    pair in the target and predicted Q values using the same network parameterized
    by ![](img/B15558_09_001.png), as shown here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的损失函数存在一个小问题。我们已经了解到，目标值只是奖励和下一个状态-动作对的折扣最大Q值之和。我们通过相同的网络参数来计算下一个状态-动作对的Q值，这些参数由
    ![](img/B15558_09_001.png) 表示，如下所示：
- en: '![](img/B15558_09_20.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_20.png)'
- en: The problem is since the target and predicted value depend on the same parameter
    ![](img/B15558_09_048.png), this will cause instability in the MSE and the network
    learns poorly. It also causes a lot of divergence during training.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，由于目标值和预测值都依赖于相同的参数 ![](img/B15558_09_048.png)，这将导致MSE的不稳定，并且网络学习效果较差。这还会导致训练过程中出现大量的发散。
- en: Let's understand this with a simple example. We will take arbitrary numbers
    to make it easier to understand. We know that we try to minimize the difference
    between the target value and the predicted value. So, on every iteration, we compute the
    gradient of loss and update our network parameter ![](img/B15558_09_001.png) so
    that we can make our predicted value the same as the target value.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来理解这一点。我们将使用任意数字来使其更容易理解。我们知道我们试图最小化目标值与预测值之间的差异。因此，在每次迭代中，我们计算损失的梯度并更新我们的网络参数
    ![](img/B15558_09_001.png)，以使预测值与目标值相同。
- en: Let's suppose in iteration 1, the target value is 13 and the predicted value
    is 11\. So, we update our parameter ![](img/B15558_09_002.png) to match the predicted
    value to the target value, which is 13\. But in the next iteration, the target
    value changes to 15 and the predicted value becomes 13 since we updated our network
    parameter ![](img/B15558_09_002.png). So, again we update our parameter ![](img/B15558_09_002.png)
    to match the predicted value to the target value, which is now 15\. But in the
    next iteration, the target value changes to 17 and the predicted value becomes
    15 since we updated our network parameter ![](img/B15558_09_053.png).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在第一次迭代中，目标值为13，预测值为11。那么我们更新参数 ![](img/B15558_09_002.png)，使预测值与目标值13匹配。但是在下一次迭代中，目标值变为15，而预测值变为13，因为我们更新了网络参数
    ![](img/B15558_09_002.png)。因此，我们再次更新参数 ![](img/B15558_09_002.png)，使预测值与目标值15匹配。但是在下一次迭代中，目标值变为17，而预测值变为15，因为我们更新了网络参数
    ![](img/B15558_09_053.png)。
- en: 'As *Table 9.2* shows, on every iteration, the predicted value tries to be the
    same as the target value, which keeps on changing:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*表 9.2*所示，在每次迭代中，预测值都会尽量与目标值相同，而目标值则不断变化：
- en: '![](img/B15558_09_09.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_09.png)'
- en: 'Table 9.2: Target and predicted value'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2：目标值和预测值
- en: This is because the predicted and target values both depend on the same parameter
    ![](img/B15558_09_054.png). If we update ![](img/B15558_09_054.png), then both
    the target and predicted values change. Thus, the predicted value keeps on trying
    to be the same as the target value, but the target value keeps on changing due
    to the update on the network parameter ![](img/B15558_09_056.png).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为预测值和目标值都依赖于相同的参数 ![](img/B15558_09_054.png)。如果我们更新 ![](img/B15558_09_054.png)，那么目标值和预测值都会发生变化。因此，预测值不断试图与目标值相同，但由于网络参数
    ![](img/B15558_09_056.png) 的更新，目标值也在不断变化。
- en: How can we avoid this? Can we freeze the target value for a while and compute
    only the predicted value so that our predicted value matches the target value?
    Yes! To do this, we introduce another neural network called a target network for
    computing the Q value of the next state-action pair in the target. The parameter
    of the target network is represented by ![](img/B15558_09_057.png). So, our main
    deep Q network, which is used for predicting Q values, learns the optimal parameter
    ![](img/B15558_09_054.png) using gradient descent. The target network is frozen
    for a while and then the target network parameter ![](img/B15558_09_059.png) is
    updated by just copying the main deep Q network parameter ![](img/B15558_09_054.png).
    Freezing the target network for a while and then updating its parameter ![](img/B15558_09_061.png)
    with the main network parameter ![](img/B15558_09_054.png) stabilizes the training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何避免这种情况？我们能否冻结目标值一段时间，只计算预测值，使得我们的预测值与目标值匹配？可以！为此，我们引入了另一个神经网络，称为目标网络，用于计算下一个状态-动作对的Q值。目标网络的参数用
    ![](img/B15558_09_057.png) 表示。因此，我们的主深度Q网络用于预测Q值，并通过梯度下降学习最佳参数 ![](img/B15558_09_054.png)。目标网络会冻结一段时间，然后通过复制主深度Q网络参数
    ![](img/B15558_09_054.png) 来更新目标网络参数 ![](img/B15558_09_059.png)。冻结目标网络一段时间后，再通过主网络参数
    ![](img/B15558_09_054.png) 更新目标网络参数 ![](img/B15558_09_061.png)，有助于稳定训练过程。
- en: 'So, now our loss function can be rewritten as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们的损失函数可以重写为：
- en: '![](img/B15558_09_063.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_063.png)'
- en: 'Thus, the Q value of the next state-action pair in the target is computed by
    the target network parameterized by ![](img/B15558_09_064.png), and the predicted
    Q value is computed by our main network parameterized by ![](img/B15558_09_065.png):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下一个状态-动作对的Q值由目标网络通过参数 ![](img/B15558_09_064.png) 计算，预测的Q值由我们的主网络通过参数 ![](img/B15558_09_065.png)
    计算：
- en: '![](img/B15558_09_21.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_21.png)'
- en: 'For notation simplicity, we can represent our target value by *y* and rewrite
    the preceding equation as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，我们可以用 *y* 来表示我们的目标值，并将前面的方程重写为：
- en: '![](img/B15558_09_035.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_035.png)'
- en: Where ![](img/B15558_09_067.png).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_09_067.png)。
- en: We have learned about several concepts concerning DQN, including the experience
    replay, the loss function, and the target network. In the next section, we will
    put all these concepts together and see how DQN works.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了与DQN相关的几个概念，包括经验回放、损失函数和目标网络。在下一节中，我们将把这些概念结合起来，看看DQN是如何工作的。
- en: Putting it all together
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容放在一起
- en: First, we initialize the main network parameter ![](img/B15558_09_054.png) with
    random values. We learned that the target network parameter is just a copy of
    the main network. So, we initialize the target network parameter ![](img/B15558_09_069.png)
    by just copying the main network parameter ![](img/B15558_09_054.png). We also
    initialize the replay buffer ![](img/B15558_09_071.png).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用随机值初始化主网络参数 ![](img/B15558_09_054.png)。我们了解到目标网络参数只是主网络的副本。因此，我们通过复制主网络参数
    ![](img/B15558_09_054.png) 来初始化目标网络参数 ![](img/B15558_09_069.png)。我们还初始化了回放缓冲区
    ![](img/B15558_09_071.png)。
- en: 'Now, for each step in the episode, we feed the state of the environment to
    our network and it outputs the Q values of all possible actions in that state.
    Then, we select the action that has the maximum Q value:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在每一集的每一步中，我们将环境的状态输入到网络中，它输出该状态下所有可能动作的Q值。然后，我们选择具有最大Q值的动作：
- en: '![](img/B15558_09_072.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_072.png)'
- en: If we only select the action that has the highest Q value, then we will not
    explore any new actions. So, to avoid this, we select actions using the epsilon-greedy
    policy. With the epsilon-greedy policy, we select a random action with probability
    epsilon and with probability 1-epsilon, we select the best action that has the
    maximum Q value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只选择具有最高Q值的动作，那么我们将不会探索任何新的动作。因此，为了避免这种情况，我们使用epsilon-greedy策略选择动作。在epsilon-greedy策略下，我们以概率epsilon选择一个随机动作，以概率1-epsilon选择具有最大Q值的最佳动作。
- en: Note that, since we initialized our network parameter ![](img/B15558_09_056.png)
    with random values, the action we select by taking the maximum Q value will not
    be the optimal action. But that's okay, we simply perform the selected action,
    move to the next state, and obtain the reward. If the action is good then we will
    receive a positive reward, and if it is bad then the reward will be negative.
    We store all this transition information ![](img/B15558_09_074.png) in the replay
    buffer ![](img/B15558_09_075.png).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们用随机值初始化了网络参数 ![](img/B15558_09_056.png)，因此通过选择最大Q值来选择的动作并不是最优动作。但这没关系，我们只需执行选定的动作，进入下一个状态并获得奖励。如果动作是好的，我们将获得正奖励；如果动作不好，奖励则为负。我们将所有这些过渡信息
    ![](img/B15558_09_074.png) 存储在重放缓冲区 ![](img/B15558_09_075.png) 中。
- en: 'Next, we randomly sample a minibatch of *K* transitions from the replay buffer
    and compute the loss. We have learned that our loss function is computed as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们随机从重放缓冲区中采样一个*K*的过渡小批量并计算损失。我们已经了解到，损失函数是这样计算的：
- en: '![](img/B15558_09_035.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_035.png)'
- en: Where *y*[i] is the target value, that is, ![](img/B15558_09_067.png).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y*[i] 是目标值，即 ![](img/B15558_09_067.png)。
- en: 'In the initial iterations, the loss will be very high since our network parameter
    ![](img/B15558_09_008.png) is just random values. To minimize the loss, we compute
    the gradients of the loss and update our network parameter ![](img/B15558_09_054.png)
    as:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在初期迭代中，由于我们的网络参数 ![](img/B15558_09_008.png)只是随机值，所以损失会非常高。为了最小化损失，我们计算损失的梯度并按以下方式更新网络参数
    ![](img/B15558_09_054.png)：
- en: '![](img/B15558_09_045.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_045.png)'
- en: We don't update the target network parameter ![](img/B15558_09_059.png) in every
    time step. We freeze the target network parameter ![](img/B15558_09_059.png) for
    several time steps and then we copy the main network parameter ![](img/B15558_09_054.png)
    to the target network parameter ![](img/B15558_09_084.png).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是在每一个时间步骤都更新目标网络参数 ![](img/B15558_09_059.png)。我们将目标网络参数 ![](img/B15558_09_059.png)冻结若干个时间步骤，然后将主网络参数
    ![](img/B15558_09_054.png)复制到目标网络参数 ![](img/B15558_09_084.png)。
- en: We keep repeating the preceding steps for several episodes to approximate the
    optimal Q value. Once we have the optimal Q value, we extract the optimal policy
    from them. To give us a more detailed understanding, the algorithm of DQN is given
    in the next section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会继续重复前面的步骤，进行若干回合，以逼近最优Q值。一旦我们得到了最优Q值，就可以从中提取最优策略。为了让我们更详细地理解，DQN算法将在下一节给出。
- en: The DQN algorithm
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DQN算法
- en: 'The DQN algorithm is given in the following steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: DQN算法的步骤如下：
- en: Initialize the main network parameter ![](img/B15558_09_056.png) with random
    values
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化主网络参数 ![](img/B15558_09_056.png)
- en: Initialize the target network parameter ![](img/B15558_09_086.png) by copying
    the main network parameter ![](img/B15558_09_087.png)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_087.png) 初始化目标网络参数 ![](img/B15558_09_086.png)
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_088.png)
- en: For *N* number of episodes, perform *step 5*
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*N*个回合，执行第5步
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T*-1:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步，即对于 *t* = 0, . . ., *T*-1：
- en: Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select random action *a* and with probability
    1-epsilon, select the action ![](img/B15558_09_072.png)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察状态 *s* 并使用ε-贪婪策略选择一个动作，即以概率epsilon选择随机动作 *a*，以概率1-epsilon选择动作 ![](img/B15558_09_072.png)
- en: Perform the selected action and move to the next state ![](img/B15558_03_021.png)
    and obtain the reward *r*
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选定的动作并进入下一个状态 ![](img/B15558_03_021.png)，获得奖励 *r*
- en: Store the transition information in the replay buffer ![](img/B15558_09_075.png)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将过渡信息存储在重放缓冲区中 ![](img/B15558_09_075.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_092.png)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机从重放缓冲区中采样一个*K*的过渡小批量 ![](img/B15558_09_092.png)
- en: Compute the target value, that is, ![](img/B15558_09_067.png)
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标值，即 ![](img/B15558_09_067.png)
- en: Compute the loss, ![](img/B15558_09_035.png)
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失，![](img/B15558_09_035.png)
- en: 'Compute the gradients of the loss and update the main network parameter ![](img/B15558_09_054.png)
    using gradient descent: ![](img/B15558_09_096.png)'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度并使用梯度下降法更新主网络参数 ![](img/B15558_09_054.png)：![](img/B15558_09_096.png)
- en: Freeze the target network parameter ![](img/B15558_09_084.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结目标网络参数 ![](img/B15558_09_084.png) 若干时间步，然后通过复制主网络参数 ![](img/B15558_09_098.png)
    来更新目标网络。
- en: Now that we have understood how DQN works, in this next section, we will learn
    how to implement it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 DQN 的工作原理，在接下来的部分，我们将学习如何实现它。
- en: Playing Atari games using DQN
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DQN 玩 Atari 游戏
- en: The Atari 2600 is a popular video game console from a game company called Atari.
    The Atari game console provides several popular games, such as Pong, Space Invaders,
    Ms. Pac-Man, Breakout, Centipede, and many more. In this section, we will learn
    how to build a DQN for playing the Atari games. First, let's explore the architecture
    of the DQN for playing the Atari games.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 2600 是一款由游戏公司 Atari 推出的流行视频游戏主机。Atari 游戏主机提供了多个受欢迎的游戏，如 Pong、Space Invaders、Ms.
    Pac-Man、Breakout、Centipede 等等。在本节中，我们将学习如何构建一个 DQN 来玩 Atari 游戏。首先，让我们探讨一下用于玩 Atari
    游戏的 DQN 架构。
- en: Architecture of the DQN
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN 的架构
- en: In the Atari environment, the image of the game screen is the state of the environment.
    So, we just feed the image of the game screen as input to the DQN and it returns
    the Q values of all the actions in the state. Since we are dealing with images,
    instead of using a vanilla deep neural network for approximating the Q value,
    we can use a **convolutional neural network (CNN)** since it is very effective
    for handling images.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Atari 环境中，游戏画面的图像就是环境的状态。因此，我们只需将游戏画面的图像作为输入传递给 DQN，它会返回该状态下所有动作的 Q 值。由于我们处理的是图像，因此，我们可以使用**卷积神经网络（CNN）**来近似
    Q 值，因为它在处理图像时非常有效。
- en: Thus, now our DQN is a CNN. We feed the image of the game screen (the game state)
    as input to the CNN, and it outputs the Q values of all the actions in the state.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的 DQN 是一个 CNN。我们将游戏画面（游戏状态）的图像作为输入传递给 CNN，CNN 输出该状态下所有动作的 Q 值。
- en: 'As *Figure 9.8* shows, given the image of the game screen, the convolutional
    layers extract features from the image and produce a feature map. Next, we flatten
    the feature map and feed the flattened feature map as input to the feedforward
    network. The feedforward network takes this flattened feature map as input and
    returns the Q values of all the actions in the state:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 9.8*所示，给定游戏画面的图像，卷积层从图像中提取特征并生成特征图。接下来，我们将特征图展平，并将展平后的特征图作为输入传递给前馈网络。前馈网络将这个展平后的特征图作为输入，并返回该状态下所有动作的
    Q 值：
- en: '![](img/B15558_09_10.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_10.png)'
- en: 'Figure 9.8: Architecture of a DQN'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：DQN 的架构
- en: Note that we don't perform a pooling operation. A pooling operation is useful
    when we perform tasks such as object detection, image classification, and so on
    where we don't consider the position of the object in the image and we just want
    to know whether the desired object is present in the image. For example, if we
    want to identify whether there is a dog in an image, we only look for whether
    a dog is present in the image and we don't check the position of the dog in the
    image. Thus, in this case, a pooling operation is used to identify whether there
    is a dog in the image irrespective of the position of the dog.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有执行池化操作。池化操作通常在进行目标检测、图像分类等任务时使用，在这些任务中，我们不考虑图像中物体的位置，只需要知道目标物体是否存在于图像中。例如，如果我们想要识别图像中是否有一只狗，我们只看图像中是否有狗，而不检查狗的位置。因此，在这种情况下，池化操作用来识别图像中是否有狗，而不管狗的位置。
- en: But in our setting, a pooling operation should not be performed because to understand
    the current game state, the position is very important. For example, in the Pong,
    we just don't want to classify if there is a ball on the game screen. We want
    to know the position of the ball so that we can make a better action. Thus, we
    don't include the pooling operation in our DQN architecture.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们的设置中，应该避免执行池化操作，因为要理解当前的游戏状态，位置非常重要。例如，在 Pong 游戏中，我们不仅仅想知道游戏画面上是否有球。我们希望知道球的位置，以便做出更好的操作。因此，我们没有在
    DQN 架构中包含池化操作。
- en: Now that we have understood the architecture of the DQN to play Atari games,
    in the next section, we will start implementing it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了用于玩 Atari 游戏的 DQN 架构，接下来的部分我们将开始实现它。
- en: Getting hands-on with the DQN
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动手实践 DQN
- en: 'Let''s implement the DQN to play the Ms Pacman game. First, let''s import the
    necessary libraries:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个 DQN 来玩 Ms Pacman 游戏。首先，导入必要的库：
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let''s create the Ms Pacman game environment using Gym:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Gym创建Ms Pacman游戏环境：
- en: '[PRE1]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the state size:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 设置状态大小：
- en: '[PRE2]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Get the number of actions:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 获取动作数量：
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess the game screen
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理游戏屏幕
- en: We learned that we feed the game state (an image of the game screen) as input
    to the DQN, which is a CNN, and it outputs the Q values of all the actions in
    the state. However, directly feeding the raw game screen image is not efficient,
    since the raw game screen size will be 210 x 160 x 3\. This will be computationally
    expensive.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们将游戏状态（游戏屏幕的图像）作为输入传入DQN，它是一个CNN，输出该状态下所有动作的Q值。然而，直接输入原始的游戏屏幕图像效率不高，因为原始游戏屏幕的大小为210
    x 160 x 3，这将非常耗费计算资源。
- en: To avoid this, we preprocess the game screen and then feed the preprocessed
    game screen to the DQN. First, we crop and resize the game screen image, convert
    the image to grayscale, normalize it, and then reshape the image to 88 x 80 x
    1\. Next, we feed this preprocessed game screen image as input to the CNN, which
    returns the Q values.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们对游戏屏幕进行预处理，然后将预处理后的游戏屏幕输入到DQN。首先，我们裁剪并调整游戏屏幕图像的大小，将图像转换为灰度图，进行归一化处理，然后将图像调整为88
    x 80 x 1。接着，我们将该预处理后的游戏屏幕图像作为输入，传入CNN，它将返回Q值。
- en: 'Now, let''s define a function called `preprocess_state`, which takes the game
    state (image of the game screen) as an input and returns the preprocessed game
    state:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个名为`preprocess_state`的函数，它接收游戏状态（游戏屏幕图像）作为输入，并返回预处理后的游戏状态：
- en: '[PRE4]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Crop and resize the image:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪并调整图像大小：
- en: '[PRE5]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Convert the image to grayscale:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像转换为灰度图：
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Improve the image contrast:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提高图像对比度：
- en: '[PRE7]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Normalize the image:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像进行归一化处理：
- en: '[PRE8]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Reshape and return the image:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重新调整图像并返回：
- en: '[PRE9]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Defining the DQN class
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义DQN类
- en: 'Let''s define the class called DQN where we will implement the DQN algorithm.
    For a clear understanding, let''s look into the code line by line. You can also
    access the complete code from the GitHub repository of the book:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为DQN的类，在这里我们将实现DQN算法。为了更清晰地理解，我们逐行查看代码。你也可以从本书的GitHub仓库中获取完整代码：
- en: '[PRE10]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Defining the init method
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义init方法
- en: First, let's define the `init` method
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义`init`方法
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the state size:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态大小：
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the action size:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 定义动作大小：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the replay buffer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 定义重放缓冲区：
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the discount factor:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 定义折扣因子：
- en: '[PRE15]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the epsilon value:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 定义ε值：
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the update rate at which we want to update the target network:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们希望更新目标网络的更新率：
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the main network:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 定义主网络：
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define the target network:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标网络：
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Copy the weights of the main network to the target network:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 将主网络的权重复制到目标网络：
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Building the DQN
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建DQN
- en: 'Now, let''s build the DQN. We have learned that to play Atari games, we use
    a CNN as the DQN, which takes the image of the game screen as an input and returns
    the Q values. We define the DQN with three convolutional layers. The convolutional
    layers extract the features from the image and output the feature maps, and then
    we flatten the feature map obtained by the convolutional layers and feed the flattened
    feature maps to the feedforward network (the fully connected layer), which returns
    the Q values:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建DQN。我们已经了解到，玩Atari游戏时，使用CNN作为DQN，将游戏屏幕的图像作为输入并返回Q值。我们定义了一个包含三层卷积层的DQN。卷积层从图像中提取特征并输出特征图，然后我们将卷积层获得的特征图展平，并将展平后的特征图输入到前馈网络（即全连接层），该网络返回Q值：
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the first convolutional layer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一个卷积层：
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the second convolutional layer:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二个卷积层：
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the third convolutional layer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第三个卷积层：
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Flatten the feature maps obtained as a result of the third convolutional layer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 展平通过第三个卷积层获得的特征图：
- en: '[PRE25]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Feed the flattened maps to the fully connected layer:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 将展平的特征图输入到全连接层：
- en: '[PRE26]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Compile the model with loss as MSE:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）编译模型：
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Return the model:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 返回模型：
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Storing the transition
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存储转换
- en: 'We have learned that we train the DQN by randomly sampling a minibatch of transitions
    from the replay buffer. So, we define a function called `store_transition`, which
    stores the transition information in the replay buffer:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，通过从重放缓冲区随机采样一个小批量的转换数据来训练DQN。因此，我们定义一个名为`store_transition`的函数，用于将转换信息存储在重放缓冲区：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Defining the epsilon-greedy policy
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义ε-贪婪策略
- en: 'We learned that in DQN, to take care of exploration-exploitation trade-off,
    we select action using the epsilon-greedy policy. So, now we define the function
    called `epsilon_greedy` for selecting an action using the epsilon-greedy policy:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在 DQN 中，为了平衡探索与利用之间的权衡，我们使用 epsilon-greedy 策略来选择动作。所以，现在我们定义一个名为 `epsilon_greedy`
    的函数，通过 epsilon-greedy 策略来选择动作：
- en: '[PRE30]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Define the training
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义训练过程
- en: 'Now let''s define a function called `train` for the training network:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个名为 `train` 的函数来进行网络训练：
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Sample a minibatch of transitions from the replay buffer:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从回放缓冲区中采样一个小批量转移：
- en: '[PRE32]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Compute the target value using the target network:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标网络计算目标值：
- en: '[PRE33]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Compute the predicted value using the main network and store the predicted
    value in the `Q_values`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主网络计算预测值，并将预测值存储在 `Q_values` 中：
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Update the target value:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 更新目标值：
- en: '[PRE35]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Train the main network:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 训练主网络：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Updating the target network
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新目标网络
- en: 'Now, let''s define the function called `update_target_network` for updating
    the target network weights by copying from the main network:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个名为 `update_target_network` 的函数，通过从主网络复制来更新目标网络的权重：
- en: '[PRE37]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Training the DQN
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 DQN
- en: 'Now, let''s train the network. First, let''s set the number of episodes we
    want to train the network for:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练网络。首先，设置我们希望训练网络的回合数：
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the number of time steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 定义时间步数：
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the batch size:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 定义批次大小：
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Set the number of past game screens we want to consider:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 设置我们希望考虑的过去游戏画面数量：
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Instantiate the DQN class:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化 DQN 类：
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Set done to `False`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 将 done 设置为 `False`：
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Initialize the `time_step`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `time_step`：
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For each episode:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Set `Return` to `0`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `Return` 设置为 `0`：
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Preprocess the game screen:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理游戏画面：
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For each step in the episode:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 每个回合的每一步：
- en: '[PRE48]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Render the environment:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE49]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Update the time step:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 更新时间步：
- en: '[PRE50]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Update the target network:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 更新目标网络：
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Select the action:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作：
- en: '[PRE52]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Perform the selected action:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 执行动作：
- en: '[PRE53]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Preprocess the next state:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理下一个状态：
- en: '[PRE54]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Store the transition information:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 存储转移信息：
- en: '[PRE55]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Update the current state to the next state:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将当前状态更新为下一个状态：
- en: '[PRE56]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Update the return value:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 更新返回值：
- en: '[PRE57]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'If the episode is done, then print the return:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回合结束，则打印返回值：
- en: '[PRE58]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'If the number of transitions in the replay buffer is greater than the batch
    size, then train the network:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回放缓冲区中的转移数量大于批次大小，则训练网络：
- en: '[PRE59]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'By rendering the environment, we can also observe how the agent learns to play
    the game over a series of episodes:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过渲染环境，我们还可以观察到智能体如何通过一系列回合来学习玩游戏：
- en: '![](img/B15558_09_11.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_11.png)'
- en: 'Figure 9.9: DQN agent learning to play'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：DQN 智能体学习玩游戏
- en: Now that we have learned how DQNs work and how to build a DQN to play Atari
    games, in the next section, we will learn an interesting variant of DQN called
    the double DQN.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了 DQN 如何工作，以及如何构建一个 DQN 来玩 Atari 游戏，在下一节中，我们将学习一种 DQN 的有趣变体，称为双重 DQN。
- en: The double DQN
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重 DQN
- en: 'We have learned that in DQN, the target value is computed as:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，在 DQN 中，目标值是这样计算的：
- en: '![](img/B15558_09_099.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_099.png)'
- en: 'One of the problems with a DQN is that it tends to overestimate the Q value
    of the next state-action pair in the target:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 的一个问题是它往往会高估目标中下一个状态-动作对的 Q 值：
- en: '![](img/B15558_09_22.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_22.png)'
- en: This overestimation is due to the presence of the max operator. Let's see how
    this overestimation happens with an example. Suppose we are in a state ![](img/B15558_09_100.png)
    and we have three actions *a*[1], *a*[2], and *a*[3]. Assume *a*[3] is the optimal
    action in the state ![](img/B15558_09_101.png). When we estimate the Q values
    of all the actions in state ![](img/B15558_03_004.png), the estimated Q value
    will have some noise and differ from the actual value. Say, due to the noise,
    action *a*[2] will get a higher Q value than the optimal action *a*[3].
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高估是由于最大值操作符的存在。让我们通过一个例子来看看这种高估是如何发生的。假设我们处于状态 ![](img/B15558_09_100.png)，并且我们有三个动作
    *a*[1]、*a*[2] 和 *a*[3]。假设 *a*[3] 是状态 ![](img/B15558_09_101.png) 下的最优动作。当我们估计状态
    ![](img/B15558_03_004.png) 中所有动作的 Q 值时，估计的 Q 值会有一些噪声，并与实际值不同。比如，由于噪声，动作 *a*[2]
    的 Q 值会比最优动作 *a*[3] 的 Q 值高。
- en: 'We know that the target value is computed as:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道目标值是这样计算的：
- en: '![](img/B15558_09_103.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_103.png)'
- en: 'Now, if we select the best action as the one that has the maximum value then
    we will end up selecting the action *a*[2] instead of optimal action *a*[3], as
    shown here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们选择最大值对应的动作作为最佳动作，那么我们最终会选择动作 *a*[2] 而不是最优动作 *a*[3]，如图所示：
- en: '![](img/B15558_09_104.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_104.png)'
- en: 'So, how can we get rid of this overestimation? We can get rid of this overestimation
    by just modifying our target value computation as:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何消除这种过高的估计呢？我们可以通过仅修改目标值计算来消除这种过高估计，如下所示：
- en: '![](img/B15558_09_105.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_105.png)'
- en: As we can observe, now we have two Q functions in our target value computation.
    One Q function parameterized by the main network parameter ![](img/B15558_09_106.png)
    is used for action selection, and the other Q function parameterized by the target
    network parameter ![](img/B15558_09_107.png) is used for Q value computation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以观察到的，现在我们在目标值计算中有两个 Q 函数。一个由主网络参数 ![](img/B15558_09_106.png) 参数化的 Q 函数用于选择动作，另一个由目标网络参数
    ![](img/B15558_09_107.png) 参数化的 Q 函数用于计算 Q 值。
- en: 'Let''s understand the preceding equation by breaking it down into two steps:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将前面的公式分解为两个步骤来理解它：
- en: '**Action selection**: First, we compute the Q values of all the next state-action
    pairs using the main network parameterized by ![](img/B15558_09_087.png), and
    then we select action ![](img/B15558_09_109.png), which has the maximum Q value:![](img/B15558_09_23.png)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作选择**：首先，我们使用由主网络参数 ![](img/B15558_09_087.png) 参数化的主网络计算所有下一状态-动作对的 Q 值，然后选择具有最大
    Q 值的动作 ![](img/B15558_09_109.png)：![](img/B15558_09_23.png)'
- en: '**Q value computation**: Once we have selected action ![](img/B15558_09_110.png),
    then we compute the Q value using the target network parameterized by ![](img/B15558_09_111.png)
    for the selected action ![](img/B15558_09_112.png):![](img/B15558_09_24.png)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q 值计算**：一旦我们选择了动作 ![](img/B15558_09_110.png)，我们就使用由 ![](img/B15558_09_111.png)
    参数化的目标网络计算该动作的 Q 值 ![](img/B15558_09_112.png)：![](img/B15558_09_24.png)'
- en: 'Let''s understand this with an example. Let''s suppose state ![](img/B15558_03_021.png)
    is *E*, then we can write:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来理解。假设状态 ![](img/B15558_03_021.png) 是 *E*，那么我们可以写成：
- en: '![](img/B15558_09_114.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_114.png)'
- en: 'First, we compute the Q values of all actions in state *E* using the main network
    parameterized by ![](img/B15558_09_054.png), and then we select the action that
    has the maximum Q value. Let''s suppose the action that has the maximum Q value
    is *right*:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用由主网络参数 ![](img/B15558_09_054.png) 参数化的主网络计算状态 *E* 中所有动作的 Q 值，然后选择具有最大
    Q 值的动作。假设具有最大 Q 值的动作是 *右*：
- en: '![](img/B15558_09_25.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_25.png)'
- en: 'Now, we can compute the Q value using the target network parameterized by ![](img/B15558_09_061.png)
    with the action selected by the main network, which is *right*. Thus, we can write:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用目标网络参数 ![](img/B15558_09_061.png) 和主网络选择的动作（即 *右*）来计算 Q 值。因此，我们可以写成：
- en: '![](img/B15558_09_117.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_117.png)'
- en: 'Still not clear? The difference between how we compute the target value in
    DQN and double DQN is shown here:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 还不清楚吗？我们在 DQN 和双 DQN 中计算目标值的区别如下所示：
- en: '![](img/B15558_09_12.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_12.png)'
- en: 'Figure 9.10: Difference between a DQN and double DQN'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：DQN 和双 DQN 之间的区别
- en: 'Thus, we learned that in a double DQN, we compute the target value using two
    Q functions. One Q function parameterized by the main network parameter ![](img/B15558_09_118.png)
    used for selecting the action that has the maximum Q value, and the other Q function
    parameterized by target network parameter ![](img/B15558_09_059.png) computes
    the Q value using the action selected by the main network:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们了解到，在双 DQN 中，我们使用两个 Q 函数计算目标值。一个由主网络参数 ![](img/B15558_09_118.png) 参数化的
    Q 函数用于选择具有最大 Q 值的动作，另一个由目标网络参数 ![](img/B15558_09_059.png) 参数化的 Q 函数则使用主网络选择的动作来计算
    Q 值：
- en: '![](img/B15558_09_105.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_105.png)'
- en: Apart from target value computation, double DQN works exactly the same as DQN.
    To give us more clarity, the algorithm of double DQN is given in the next section.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 除了目标值计算外，双 DQN 的工作方式与 DQN 完全相同。为了更清楚地说明，双 DQN 的算法将在下一节给出。
- en: The double DQN algorithm
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双 DQN 算法
- en: 'The algorithm of double DQN is shown here. As we can see, except the target
    value computation (the bold step), the rest of the steps are exactly the same
    as in the DQN:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 双 DQN 的算法如下所示。正如我们所看到的，除了目标值计算（加粗步骤）外，其他步骤与 DQN 完全相同：
- en: Initialize the main network parameter ![](img/B15558_09_087.png) with random
    values
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主网络参数 ![](img/B15558_09_087.png) 为随机值
- en: Initialize the target network parameter ![](img/B15558_09_086.png) by copying
    the main network parameter ![](img/B15558_09_123.png)
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_123.png)，初始化目标网络参数 ![](img/B15558_09_086.png)
- en: Initialize the replay buffer ![](img/B15558_09_124.png)
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_124.png)
- en: For *N* number of episodes repeat *step 5*
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*N*个回合，重复*步骤5*
- en: 'For each step in the episode, that is, for *t* = 0, . . . , *T*-1:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每一步，即对于*t* = 0, . . . , *T*-1：
- en: 'Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select a random action *a* with probability
    1-epsilon; select the action: ![](img/B15558_09_072.png)'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察状态*s*并使用epsilon-贪婪策略选择一个动作，即以epsilon的概率选择一个随机动作*a*，以1-epsilon的概率选择该动作：![](img/B15558_09_072.png)
- en: Perform the selected action, move to the next state ![](img/B15558_09_126.png),
    and obtain the reward *r*
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作，进入下一个状态 ![](img/B15558_09_126.png)，并获得奖励*r*
- en: Store the transition information in the replay buffer ![](img/B15558_09_075.png)
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将过渡信息存储在重放缓冲区中 ![](img/B15558_09_075.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_128.png)
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区随机抽取一个包含*K*个过渡的小批量 ![](img/B15558_09_128.png)
- en: '**Compute the target value, that is,** ![](img/B15558_09_129.png)'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算目标值，即** ![](img/B15558_09_129.png)'
- en: Compute the loss, ![](img/B15558_09_035.png)
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失， ![](img/B15558_09_035.png)
- en: 'Compute the gradients of the loss and update the main network parameter ![](img/B15558_09_054.png)
    using gradient descent: ![](img/B15558_09_132.png)'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度，并使用梯度下降法更新主网络参数 ![](img/B15558_09_054.png)：![](img/B15558_09_132.png)
- en: Freeze the target network parameter ![](img/B15558_09_133.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_087.png)
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结目标网络参数 ![](img/B15558_09_133.png)几次时间步，然后通过简单地复制主网络参数 ![](img/B15558_09_087.png)来更新它
- en: Now that we have learned how the double DQN works, in the next section, we will
    learn about an interesting variant of DQN called DQN with prioritized experience
    replay.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了双重DQN的工作原理，在下一节中，我们将学习一种名为优先经验重放的DQN的有趣变体。
- en: DQN with prioritized experience replay
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用优先经验重放的DQN
- en: We learned that in DQN, we randomly sample a minibatch of *K* transitions from
    the replay buffer and train the network. Instead of doing this, can we assign
    some priority to each transition in the replay buffer and sample the transitions
    that had high priority for learning?
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在DQN中，我们从重放缓冲区随机抽取一个包含*K*个过渡的小批量并训练网络。那么，是否可以为重放缓冲区中的每个过渡分配一些优先级，并选择那些具有高优先级的过渡来进行学习呢？
- en: Yes, but first, why do we need to assign priority for the transition, and how
    can we decide which transition should be given more priority than the others?
    Let's explore this more in detail.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，但是首先，为什么我们需要为过渡分配优先级？我们如何决定哪些过渡应该比其他过渡具有更高的优先级呢？让我们更详细地探讨这个问题。
- en: 'The TD error ![](img/B15558_09_135.png) is the difference between the target
    value and the predicted value, as shown here:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: TD误差 ![](img/B15558_09_135.png)是目标值与预测值之间的差异，如下所示：
- en: '![](img/B15558_09_136.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_136.png)'
- en: A transition that has a high TD error implies that the transition is not correct,
    and so we need to learn more about that transition to minimize the error. A transition
    that has a low TD error implies that the transition is already good. We can always
    learn more from our mistakes rather than only focusing on what we are already
    good at, right? Similarly, we can learn more from the transitions that have a
    high TD error than those that have a low TD error. Thus, we can assign more priority
    to the transitions that have a high TD error and less priority to transitions
    that have a low TD error.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 具有较高TD误差的过渡意味着该过渡不正确，因此我们需要更多地学习该过渡以最小化误差。具有较低TD误差的过渡意味着该过渡已经很好。我们总是可以从错误中学到更多，而不仅仅是专注于我们已经擅长的内容，对吧？同样，我们可以从具有较高TD误差的过渡中学到更多，而不是从那些具有较低TD误差的过渡中。因此，我们可以给具有较高TD误差的过渡分配更高的优先级，而给具有较低TD误差的过渡分配较低的优先级。
- en: 'We know that the transition information consists of ![](img/B15558_09_137.png),
    and along with this information, we also add priority *p* and store the transition
    with the priority in our replay buffer as ![](img/B15558_09_138.png). The following
    figure shows the replay buffer containing transitions along with the priority:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，过渡信息由 ![](img/B15558_09_137.png) 组成，并且在这些信息的基础上，我们还添加了优先级*p*，并将带有优先级的过渡存储在我们的重放缓冲区中，格式为
    ![](img/B15558_09_138.png)。下图显示了包含过渡及其优先级的重放缓冲区：
- en: '![](img/B15558_09_13.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_13.png)'
- en: 'Figure 9.11: Prioritized replay buffer'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：优先级重放缓冲区
- en: In the next section, we will learn how to prioritize our transitions using the
    TD error based on two different types of prioritization methods.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用基于TD误差的两种不同优先级方法对过渡进行优先排序。
- en: Types of prioritization
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优先级排序类型
- en: 'We can prioritize our transition using the following two methods:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下两种方法对过渡进行优先级排序：
- en: Proportional prioritization
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成比例优先级排序
- en: Rank-based prioritization
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于排名的优先级排序
- en: Proportional prioritization
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成比例优先级排序
- en: 'We learned that the transition can be prioritized using the TD error, so the
    priority *p* of the transition *i* will be just its TD error:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，可以使用TD误差来对过渡进行优先级排序，因此过渡*i*的优先级*p*将仅为其TD误差：
- en: '![](img/B15558_09_139.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_139.png)'
- en: 'Note that we take the absolute value of the TD error as a priority to keep
    the priority positive. Okay, what about a transition that has a TD error of zero?
    Say we have a transition *i* and its TD error is 0, then the priority of the transition
    *i* will just be 0:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们取TD误差的绝对值作为优先级，以保持优先级为正。那么，TD误差为零的过渡如何处理呢？假设我们有一个过渡*i*，其TD误差为0，那么过渡*i*的优先级将为0：
- en: '![](img/B15558_09_140.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_140.png)'
- en: 'But setting the priority of the transition to zero is not desirable, and if
    we set the priority of a transition to zero then that particular transition will
    not be used in our training at all. So, to avoid this issue, we will add a small
    value called epsilon to our TD error. So, even if the TD error is zero, we will
    still have a small priority due to the epsilon. To be more precise, adding an
    epsilon to the TD error guarantees that there will be no transition with zero
    priority. Thus, our priority can be modified as:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，将过渡的优先级设置为零是不理想的，因为如果将某个过渡的优先级设置为零，那么该过渡将完全不参与训练。为了避免这个问题，我们将向TD误差中添加一个小值，称为epsilon。因此，即使TD误差为零，由于epsilon的存在，我们仍然会有一个小的优先级。更准确地说，向TD误差中添加epsilon可以保证没有过渡的优先级为零。因此，我们的优先级可以修改为：
- en: '![](img/B15558_09_141.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_141.png)'
- en: 'Instead of having the priority as a raw number, we can convert it into a probability
    so that we will have priorities ranging from 0 to 1\. We can convert the priority
    to a probability as shown here:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将优先级转换为概率，以便优先级范围从0到1。我们可以通过如下方式将优先级转换为概率：
- en: '![](img/B15558_09_142.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_142.png)'
- en: The preceding equation calculates the probability *P* of the transition *i*.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式计算了过渡*i*的概率*P*。
- en: 'Can we also control the amount of prioritization? That is, instead of sampling
    only the prioritized transition, can we also take a random transition? Yes! To
    do this, we introduce a new parameter called ![](img/B15558_09_143.png) and rewrite
    our equation as follows. When the value of ![](img/B15558_07_025.png) is high,
    say 1, then we sample only the transitions that have high priority and when the
    value of ![](img/B15558_07_025.png) is low, say 0, then we sample a random transition:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否也可以控制优先级排序的程度呢？也就是说，除了仅采样优先级较高的过渡外，我们还能否采样随机的过渡？可以！为了做到这一点，我们引入了一个新的参数，称为
    ![](img/B15558_09_143.png)，并将我们的公式改写为如下。当 ![](img/B15558_07_025.png) 的值较高时，比如为1，我们就只采样优先级高的过渡；而当
    ![](img/B15558_07_025.png) 的值较低时，比如为0，我们就采样随机的过渡：
- en: '![](img/B15558_09_146.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_146.png)'
- en: Thus, we have learned how to assign priority to a transition using the proportional
    prioritization method. In the next section, we will learn another prioritization
    method called rank-based prioritization.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学习了如何使用成比例优先级排序方法来分配过渡的优先级。在下一节中，我们将学习另一种优先级排序方法，称为基于排名的优先级排序。
- en: Rank-based prioritization
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于排名的优先级排序
- en: Rank-based prioritization is the simplest type of prioritization. Here, we assign
    priority based on the rank of a transition. What is the rank of a transition?
    The rank of a transition *i* can be defined as the location of the transition
    in the replay buffer where the transitions are sorted from high TD error to low
    TD error.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 基于排名的优先级排序是最简单的优先级排序类型。在这种方法中，我们根据过渡的排名来分配优先级。那么，什么是过渡的排名呢？过渡*i*的排名可以定义为在回放缓冲区中，按TD误差从高到低排序的过渡位置。
- en: 'Thus, we can define the priority of the transition *i* using rank as:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用排名来定义过渡*i*的优先级，如下所示：
- en: '![](img/B15558_09_147.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_147.png)'
- en: 'Just as we learned in the previous section, we convert the priority into probability:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中所学，我们将优先级转换为概率：
- en: '![](img/B15558_09_142.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_142.png)'
- en: 'Similar to what we learned in the previous section, we can add a parameter
     to control the amount of prioritization and express our final equation as:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在上一节中学到的内容，我们可以添加一个参数α来控制优先级排序的程度，并将最终公式表示为：
- en: '![](img/B15558_09_146.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_146.png)'
- en: Correcting the bias
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修正偏差
- en: We have learned how to prioritize the transitions using two methods—proportional
    prioritization and rank-based prioritization. But the problem with these methods
    is that we will be highly biased towards the samples that have high priority.
    That is, when we give more importance to samples that have a high TD error, it
    essentially means that we are learning only from a subset of samples that have
    a high TD error.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何通过两种方法来优先考虑转移——比例优先级和基于排名的优先级。但这些方法的问题在于，我们会对具有高优先级的样本产生很大的偏差。也就是说，当我们更重视具有高TD误差的样本时，这实际上意味着我们只从具有高TD误差的样本子集进行学习。
- en: 'Okay, what''s the issue with this? It will lead to the problem of overfitting,
    and our agent will be highly biased to those transitions that have a high TD error.
    To combat this, we use importance weights *w*. The importance weights help us
    to reduce the weights of transitions that have occurred many times. The importance
    weight *w* of the transition *i* can be expressed as:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那这个问题是什么呢？它会导致过拟合问题，我们的智能体会对那些具有高TD误差的转移产生强烈的偏向。为了应对这个问题，我们使用了重要性权重*w*。重要性权重帮助我们减少多次发生的转移的权重。转移*i*的权重*w*可以表示为：
- en: '![](img/B15558_09_150.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_150.png)'
- en: In the preceding expression, *N* denotes the length of our replay buffer and
    *P*(*i*) denotes the probability of the transition *i*. Okay, what's that parameter
    ![](img/B15558_09_151.png)? It controls the importance weight. We start off with
    small values of ![](img/B15558_09_152.png), from 0.4 and anneal it toward 1.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，*N*表示我们的回放缓冲区的长度，*P*(*i*)表示转移*i*的概率。那么，那个参数！[](img/B15558_09_151.png)是什么呢？它控制着重要性权重。我们从较小的值开始！[](img/B15558_09_152.png)，从0.4开始，并将其逐步调整到1。
- en: Thus, in this section, we have learned how to prioritize transitions in DQN
    with prioritized experience replay. In the next section, we will learn about another
    interesting variant of DQN called dueling DQN.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这一部分中，我们学习了如何通过优先经验回放在DQN中优先考虑转移。在下一部分中，我们将学习DQN的另一个有趣变种——对战DQN。
- en: The dueling DQN
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对战DQN
- en: 'Before going ahead, let''s learn about one of the most important functions
    in reinforcement learning, called the advantage function. The advantage function
    is defined as the difference between the Q function and the value function, and
    it is expressed as:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们学习一下强化学习中最重要的函数之一——优势函数。优势函数被定义为Q函数和值函数之间的差异，其表达式为：
- en: '![](img/B15558_09_153.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_153.png)'
- en: 'Okay, but what''s the use of an advantage function? What does it signify? First,
    let''s recall the Q function and the value function:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但是优势函数有什么用呢？它意味着什么？首先，我们回顾一下Q函数和值函数：
- en: '**Q function**: The Q function gives the expected return an agent would obtain
    starting from state *s*, performing action *a*, and following the policy ![](img/B15558_03_008.png).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q函数**：Q函数给出了智能体从状态*s*开始，执行动作*a*并遵循策略后，所能获得的期望回报！[](img/B15558_03_008.png)。'
- en: '**Value function**: The value function gives the expected return an agent would obtain
    starting from state *s* and following the policy ![](img/B15558_04_032.png).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值函数**：值函数给出的是一个智能体从状态*s*开始并遵循策略后，所能获得的期望回报！[](img/B15558_04_032.png)。'
- en: Now if we think intuitively, what's the difference between the Q function and
    the value function? The Q function gives us the value of a state-action pair,
    while the value function gives the value of a state irrespective of the action.
    Now, the difference between the Q function and the value function tells us how
    good the action *a* is compared to the average actions in state *s*.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们直观地思考，Q函数和值函数之间有什么区别？Q函数给出了状态-动作对的价值，而值函数则给出了一个状态的价值，不考虑动作。现在，Q函数和值函数之间的区别告诉我们，在状态*s*下，动作*a*相比于平均动作有多好。
- en: Thus, the advantage function tells us that in state *s*, how good the action
    *a* is compared to the average actions. Now that we have understood what the advantage
    function is, let's see why and how we can make use of it in the DQN.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优势函数告诉我们，在状态*s*下，动作*a*相比于平均动作有多好。现在我们已经理解了什么是优势函数，接下来让我们看看为什么以及如何在DQN中使用它。
- en: Understanding the dueling DQN
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解对战DQN
- en: 'We have learned that in a DQN, we feed the state as input and our network computes
    the Q value for all actions in that state. Instead of computing the Q values in
    this way, can we compute the Q values using the advantage function? We have learned
    that the advantage function is given as:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，在DQN中，我们将状态作为输入，网络计算该状态下所有动作的Q值。我们能否不以这种方式计算Q值，而是通过优势函数来计算Q值呢？我们已经了解到，优势函数是通过以下方式给出的：
- en: '![](img/B15558_09_153.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_153.png)'
- en: 'We can rewrite the preceding equation as:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的方程式改写为：
- en: '![](img/B15558_09_157.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_157.png)'
- en: As we can see from the preceding equation, we can compute the Q value just by
    adding the value function and the advantage function together. Wait! Why do we
    have to do this? What's wrong with computing the Q value directly?
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程式中我们可以看出，我们只需要将价值函数和优势函数加在一起，就可以计算Q值。等等！为什么我们要这么做？直接计算Q值不行吗？
- en: 'Let''s suppose we are in some state *s* and we have 20 possible actions to
    perform in this state. Computing the Q values of all these 20 actions in state
    *s* is not going to be useful because most of the actions will not have any effect
    on the state, and also most of the actions will have a similar Q value. What do
    we mean by that? Let''s understand this with the grid world environment shown
    in *Figure 9.12*:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于某个状态*s*，并且在这个状态下有20个可能的动作可以执行。计算这20个动作在状态*s*下的Q值是没有用的，因为大多数动作不会对状态产生任何影响，而且大多数动作的Q值也非常相似。这是什么意思呢？让我们通过图
    *9.12* 中展示的网格世界环境来理解这一点：
- en: '![](img/B15558_09_14.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_14.png)'
- en: 'Figure 9.12: Grid world environment'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：网格世界环境
- en: As we can see, the agent is in state **A**. In this case, what is the use of
    computing the Q value of the action *up* in state **A**? Moving *up* will have
    no effect in state **A**, and it's not going to take the agent anywhere. Similarly,
    think of an environment where our action space is huge, say 100\. In this case,
    most of the actions will not have any effect in the given state. Also, when the
    action space is large, most of the actions will have a similar Q value.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，智能体处于状态**A**。在这种情况下，计算在状态**A**下动作*向上*的Q值有什么用呢？在状态**A**中，向*上*移动不会产生任何效果，而且也不会让智能体到达任何地方。类似地，假设我们的动作空间非常大，比如说有100个动作。在这种情况下，大多数动作在给定的状态下不会产生任何效果。而且，当动作空间很大时，大多数动作的Q值会非常相似。
- en: Now, let's talk about the value of a state. Note that not all the states are
    important for an agent. There could be a state that always gives a bad reward
    no matter what action we perform. In that case, it is not useful to compute the
    Q value of all possible actions in the state if we know that the state is always
    going to give us a bad reward.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论状态的价值。请注意，并不是所有状态对智能体来说都是重要的。可能会有一个状态，无论我们执行什么动作，总是给出一个很差的奖励。在这种情况下，如果我们知道该状态总是会给出一个很差的奖励，那么计算该状态下所有可能动作的Q值是没有意义的。
- en: Thus, to solve this we can compute the Q function as the sum of the value function
    and the advantage function. That is, with the value function, we can understand
    whether a state is valuable or not without computing the values of all actions
    in the state. And with the advantage function, we can understand whether an action
    is really good or it just gives us the same value as all the other actions.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了解决这个问题，我们可以将Q函数计算为价值函数和优势函数的和。也就是说，通过价值函数，我们可以理解一个状态是否有价值，而无需计算该状态下所有动作的值。通过优势函数，我们可以理解一个动作是否真的很好，或者它只是给我们带来了与其他所有动作相同的价值。
- en: Now that we have a basic idea of dueling DQN, let's explore the architecture
    of dueling DQN in the next section.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对对抗DQN有了基本的了解，让我们在下一节中深入探讨对抗DQN的架构。
- en: The architecture of a dueling DQN
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗DQN的架构
- en: 'We have learned that in a dueling DQN, the Q values can be computed as:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，在对抗DQN中，Q值可以计算为：
- en: '![](img/B15558_09_157.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_157.png)'
- en: 'How can we design our neural network to emit Q values in this way? We can break
    the final layer of our network into two streams. The first stream computes the
    value function and the second stream computes the advantage function. Given any
    state as input, the value stream gives the value of a state, while the advantage
    stream gives the advantage of all possible actions in the given state. For instance,
    as *Figure 9.13* shows, we feed the game state (game screen) as an input to the
    network. The value stream computes the value of a state while the advantage stream
    computes the advantage values of all actions in the state:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何设计神经网络以这种方式输出 Q 值呢？我们可以将网络的最终层分成两个流。第一个流计算值函数，第二个流计算优势函数。给定任何状态作为输入，值流输出一个状态的值，而优势流输出给定状态下所有可能动作的优势。例如，正如*图
    9.13*所示，我们将游戏状态（游戏画面）作为输入传入网络。值流计算状态的值，而优势流计算该状态下所有动作的优势值：
- en: '![](img/B15558_09_15.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_15.png)'
- en: 'Figure 9.13: Architecture of a dueling DQN'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：对抗 DQN 的架构
- en: 'We learned that we compute the Q value by adding the state value and the advantage
    value together, so we combine the value stream and the advantage stream using
    another layer called the aggregate layer, and compute the Q value as *Figure 9.14*
    shows. Thus, the value stream computes the state value, the advantage stream computes
    the advantage value, and the aggregate layer sums these streams and computes the
    Q value:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们通过将状态值和优势值相加来计算 Q 值，因此我们将值流和优势流通过另一个层——聚合层结合，并按照*图 9.14*所示计算 Q 值。这样，值流计算状态值，优势流计算优势值，聚合层将这两个流相加并计算
    Q 值：
- en: '![](img/B15558_09_16.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_16.png)'
- en: 'Figure 9.14: Architecture of a dueling DQN including an aggregate layer'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：包含聚合层的对抗 DQN 架构
- en: But there is a small issue here. Just summing the state value and advantage
    value in the aggregate layer and computing the Q value leads us to a problem of
    identifiability.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里有一个小问题。仅仅在聚合层中将状态值和优势值相加并计算 Q 值会导致可识别性问题。
- en: 'So, to combat this problem, we make the advantage function to have zero advantage
    for the selected action. We can achieve this by subtracting the average advantage
    value, that is, the average advantage of all actions in the action space, as shown
    here:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为了应对这个问题，我们让优势函数对于所选动作的优势为零。我们可以通过减去平均优势值来实现这一点，即所有动作在动作空间中的平均优势值，如下所示：
- en: '![](img/B15558_09_159.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_159.png)'
- en: Where ![](img/B15558_09_160.png) denotes the length of the action space.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_09_160.png) 表示动作空间的长度。
- en: 'Thus, we can write our final equation for computing the Q value with parameters
    as:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写出最终的计算 Q 值的方程式，带有参数如下：
- en: '![](img/B15558_09_161.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_161.png)'
- en: 'In the preceding equation, ![](img/B15558_09_054.png) is the parameter of the
    convolutional network, ![](img/B15558_06_016.png) is the parameter of the value
    stream, and ![](img/B15558_09_143.png) is the parameter of the advantage stream.
    After computing the Q value, we can select the action:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在上式中，![](img/B15558_09_054.png) 是卷积网络的参数，![](img/B15558_06_016.png) 是值流的参数，![](img/B15558_09_143.png)
    是优势流的参数。计算出 Q 值后，我们可以选择动作：
- en: '![](img/B15558_09_165.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_165.png)'
- en: Thus, the only difference between a dueling DQN and DQN is that in a dueling
    DQN, instead of computing the Q values directly, we compute them by combining
    the state value and the advantage value.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对抗 DQN 和 DQN 之间唯一的区别是，在对抗 DQN 中，我们不是直接计算 Q 值，而是通过将状态值和优势值结合来计算 Q 值。
- en: In the next section, we will explore another variant of DQN called deep recurrent
    Q network.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索 DQN 的另一个变种，称为深度递归 Q 网络。
- en: The deep recurrent Q network
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度递归 Q 网络
- en: '**The deep recurrent Q network** (**DRQN**) is just the same as a DQN but with
    recurrent layers. But what''s the use of recurrent layers in DQN? To answer this
    question, first, let''s understand the problem called **Partially Observable Markov
    Decision Process** (**POMDP**).'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度递归 Q 网络**（**DRQN**）与 DQN 完全相同，只是增加了递归层。那么，DQN 中递归层的作用是什么呢？为了回答这个问题，我们首先需要了解一个叫做**部分可观察马尔可夫决策过程**（**POMDP**）的问题。'
- en: An environment is called a POMDP when we have a limited set of information available
    about the environment. So far, in the previous chapters, we have seen a fully
    observable MDP where we know all possible actions and states—although we might
    be unaware of transition and reward probabilities, we had complete knowledge of
    the environment. For example, in the frozen lake environment, we had complete
    knowledge of all the states and actions of the environment.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对环境的可用信息有限时，这个环境被称为 POMDP。在之前的章节中，我们看到的是一个完全可观察的 MDP，我们知道所有可能的动作和状态——虽然我们可能不知道转移和奖励的概率，但我们对环境有完全的了解。例如，在冰冻湖环境中，我们对所有状态和动作都有完整的知识。
- en: But most real-world environments are only partially observable; we cannot see
    all the states. For instance, consider an agent learning to walk in a real-world
    environment. In this case, the agent will not have complete knowledge of the environment
    (the real world); it will have no information outside its view.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 但大多数现实世界的环境只能部分观察到；我们不能看到所有状态。例如，假设一个智能体在现实世界中学习走路。在这种情况下，智能体对环境（即真实世界）没有完全的了解；它无法获得超出视野的信息。
- en: Thus, in a POMDP, states provide only partial information, but keeping the information
    about past states in the memory will help the agent to understand more about the
    nature of the environment and find the optimal policy. Thus, in POMDP, we need
    to retain information about past states in order to take the optimal action.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 POMDP 中，状态只提供部分信息，但将过去状态的信息保存在记忆中将帮助智能体更好地理解环境的性质，并找到最优策略。因此，在 POMDP 中，我们需要保留过去状态的信息，以便采取最优行动。
- en: So, can we take advantage of recurrent neural networks to understand and retain
    information about the past states as long as it is required? Yes, the **Long Short-Term
    Memory Recurrent Neural Network** (**LSTM RNN**) is very useful for retaining,
    forgetting, and updating the information as required. So, we can use the LSTM
    layer in the DQN to retain information about the past states as long as it is
    required. Retaining information about the past states helps when we have the problem
    of POMDP.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能否利用循环神经网络来理解并保留过去状态的信息，只要它是需要的呢？是的，**长短期记忆循环神经网络**（**LSTM RNN**）对于保留、遗忘和根据需要更新信息非常有用。因此，我们可以在
    DQN 中使用 LSTM 层来保留过去状态的信息，只要它是需要的。保留过去状态的信息在 POMDP 问题中非常有帮助。
- en: Now that we have a basic understanding of why we need DRQN and how it solves
    the problem of POMDP, in the next section, we will look into the architecture
    of DRQN.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们基本了解了为什么需要 DRQN 以及它如何解决 POMDP 问题，在下一节中，我们将深入研究 DRQN 的架构。
- en: The architecture of a DRQN
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DRQN 的架构
- en: '*Figure 9.15* shows the architecture of a DRQN. As we can see, it is similar
    to the DQN architecture except that it has an LSTM layer:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.15* 展示了 DRQN 的架构。正如我们所见，它与 DQN 架构类似，唯一的区别是它有一个 LSTM 层：'
- en: '![](img/B15558_09_17.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_17.png)'
- en: 'Figure 9.15: Architecture of a DRQN'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：DRQN 的架构
- en: We pass the game screen as an input to the convolutional layer. The convolutional
    layer convolves the image and produces a feature map. The resulting feature map
    is then passed to the LSTM layer. The LSTM layer has memory to hold information.
    So, it retains information about important previous game states and updates its
    memory over time steps as required. Then, we feed the hidden state from the LSTM
    layer to the fully connected layer, which outputs the Q value.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将游戏屏幕作为输入传递给卷积层。卷积层对图像进行卷积并生成特征图。生成的特征图随后传递给 LSTM 层。LSTM 层具有记忆功能，可以保存信息。因此，它会保留关于重要过去游戏状态的信息，并根据需要在时间步长上更新其记忆。然后，我们将
    LSTM 层的隐藏状态输入到全连接层，后者输出 Q 值。
- en: '*Figure 9.16* helps us to understand how exactly DRQN works. Let''s suppose
    we need to compute the Q value for the state *s*[t] and the action *a*[t]. Unlike
    DQN, we don''t just compute the Q value as Q(*s*[t], *a*[t]) directly. As we can
    see, along with the current state *s*[t] we also use the hidden state *h*[t] to
    compute the Q value. The reason for using the hidden state is that it holds information
    about the past game states in memory.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.16* 帮助我们理解 DRQN 是如何工作的。假设我们需要计算状态 *s*[t] 和动作 *a*[t] 的 Q 值。与 DQN 不同，我们不仅仅是直接计算
    Q(*s*[t], *a*[t])。如我们所见，除了当前状态 *s*[t]，我们还使用隐藏状态 *h*[t] 来计算 Q 值。之所以使用隐藏状态，是因为它保存了过去游戏状态的信息。'
- en: 'Since we are using the LSTM cells, the hidden state *h*[t] will consist of
    information about the past game states in the memory as long as it is required:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了 LSTM 单元，隐藏状态 *h*[t] 将包含有关过去游戏状态的记忆信息，只要它是需要的：
- en: '![](img/B15558_09_18.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_09_18.png)'
- en: 'Figure 9.16: Architecture of DRQN'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：DRQN 架构
- en: Except for this change, DRQN works just like DQN. Wait. What about the replay
    buffer? In DQN, we learned that we store the transition information in the replay
    buffer and train our network by sampling a minibatch of experience. We also learned
    that the transition information is placed sequentially in the replay buffer one
    after another, so to avoid the correlated experience, we randomly sample a minibatch
    of experience from the replay buffer and train the network.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一变化外，DRQN 的工作方式与 DQN 相同。等等，那回放缓冲区呢？在 DQN 中，我们了解到我们将转移信息存储在回放缓冲区中，并通过从回放缓冲区随机采样一小批次经验来训练网络。我们还了解到，转移信息会在回放缓冲区中按顺序排列，为了避免相关经验的影响，我们随机从回放缓冲区中采样一小批次经验来训练网络。
- en: But in the case of a DRQN, we need sequential information so that our network
    can retain information from past game states. Thus we need sequential information
    but also we don't want to overfit the network by training with correlated experience.
    How can we achieve this?
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 但在 DRQN 的情况下，我们需要序列信息，以便我们的网络能够保留过去游戏状态的信息。因此，我们需要序列信息，同时又不希望通过训练相关经验而使网络过拟合。我们该如何实现这一点？
- en: To achieve this, in a DRQN, we randomly sample a minibatch of episodes rather
    than a random minibatch of transitions. That is, we know that in the episode,
    we will have transition information that follows sequentially, so we take a random
    minibatch of episodes and in each episode we will have the transition information
    that is placed sequentially. So, in this way, we can accommodate both randomization
    and also the transition information that follows one another. This is called **bootstrapped
    sequential updates**.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，在 DRQN 中，我们随机采样一小批次的 episode，而不是随机采样一小批次的转移。也就是说，我们知道在一个 episode 中，会有按顺序排列的转移信息，因此我们采样一个随机的
    episode 小批次，在每个 episode 中，转移信息会按顺序排列。通过这种方式，我们既能实现随机化，又能保留按顺序排列的转移信息。这被称为 **引导式序列更新**。
- en: After sampling the minibatch of episodes randomly, then we can train the DRQN
    just like we trained the DQN network by minimizing the MSE loss. To learn more,
    you can refer to the DRQN paper given in the *Further reading* section.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机采样 episode 小批次之后，我们可以像训练 DQN 网络一样，通过最小化 MSE 损失来训练 DRQN。如果想深入了解，可以参考 *进一步阅读*
    部分中提供的 DRQN 论文。
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by learning what deep Q networks are and how they are
    used to approximate the Q value. We learned that in a DQN, we use a buffer called
    the replay buffer to store the agent's experience. Then, we randomly sample a
    minibatch of experience from the replay buffer and train the network by minimizing
    the MSE. Moving on, we looked at the algorithm of DQN in more detail, and then
    we learned how to implement DQN to play Atari games.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从学习什么是深度 Q 网络以及它们如何用于近似 Q 值开始。本章中，我们了解到，在 DQN 中，我们使用一个叫做回放缓冲区的缓存来存储智能体的经验。然后，我们从回放缓冲区随机采样一小批次经验，并通过最小化
    MSE 来训练网络。接下来，我们更详细地了解了 DQN 的算法，随后学习了如何实现 DQN 来玩 Atari 游戏。
- en: Following this, we learned that the DQN overestimates the target value due to
    the max operator. So, we used double DQN, where we have two Q functions in our
    target value computation. One Q function parameterized by the main network parameter
    ![](img/B15558_09_042.png) is used for action selection, and the other Q function
    parameterized by the target network parameter ![](img/B15558_09_059.png) is used
    for Q value computation.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们了解到 DQN 因为使用了 max 操作符而高估了目标值。因此，我们使用了双重 DQN，在目标值计算中使用了两个 Q 函数。一个 Q 函数由主网络参数
    ![](img/B15558_09_042.png) 参数化，用于选择动作，另一个 Q 函数由目标网络参数 ![](img/B15558_09_059.png)
    参数化，用于 Q 值计算。
- en: Going ahead, we learned about the DQN with prioritized experience replay, where
    the transition is prioritized based on the TD error. We explored two different
    types of prioritization methods called proportional prioritization and rank-based
    prioritization.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了带有优先经验回放的 DQN，其中转移根据 TD 错误进行优先级排序。我们探讨了两种不同的优先级排序方法，分别是按比例优先级排序和基于排名的优先级排序。
- en: Next, we learned about another interesting variant of DQN called dueling DQN.
    In dueling DQN, instead of computing Q values directly, we compute them using
    two streams called the value stream and the advantage stream.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了DQN的另一个有趣变体——对抗DQN。在对抗DQN中，我们不是直接计算Q值，而是使用两个流——值流和优势流——来计算它们。
- en: At the end of the chapter, we learned about DRQN and how they solve the problem
    of partially observable Markov decision processes.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们了解了DRQN以及它们如何解决部分可观测马尔可夫决策过程的问题。
- en: In the next chapter, we will learn about another popular algorithm called policy
    gradient.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习另一个流行的算法——策略梯度。
- en: Questions
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our understanding of DQN and its variants by answering the
    following questions:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们对DQN及其变体的理解：
- en: Why do we need a DQN?
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要DQN？
- en: What is the replay buffer?
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是重放缓冲区？
- en: Why do we need the target network?
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要目标网络？
- en: How does a double DQN differ from a DQN?
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双重DQN与DQN有什么不同？
- en: Why do we have to prioritize the transitions?
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们必须优先考虑转换？
- en: What is the advantage function?
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是优势函数？
- en: Why do we need LSTM layers in a DRQN?
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在DRQN中需要LSTM层？
- en: Further reading
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, we can refer to the following papers:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下论文：
- en: '**Playing Atari with Deep Reinforcement Learning** by *Volodymyr Mnih, et al.*,
    [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf)'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过深度强化学习玩Atari**，*Volodymyr Mnih 等人*，[https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf)'
- en: '**Deep Reinforcement Learning with Double Q-learning** by *Hado van Hasselt,
    Arthur Guez, David Silver*, [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用双重Q学习的深度强化学习**，*Hado van Hasselt, Arthur Guez, David Silver*，[https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
- en: '**Prioritized Experience Replay** by *Tom Schaul, John Quan, Ioannis Antonoglou
    and David Silver*, [https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先经验重放**，*Tom Schaul, John Quan, Ioannis Antonoglou 和 David Silver*，[https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)'
- en: '**Dueling Network Architectures for Deep Reinforcement Learning** by *Ziyu
    Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas*,
    [https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度强化学习中的对抗网络架构**，*Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt,
    Marc Lanctot, Nando de Freitas*，[https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)'
- en: '**Deep Recurrent Q-Learning for Partially Observable MDPs** by *Matthew Hausknecht
    and Peter Stone*, [https://arxiv.org/pdf/1507.06527.pdf](https://arxiv.org/pdf/1507.06527.pdf)'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度递归Q学习用于部分可观测的马尔可夫决策过程**，*Matthew Hausknecht和Peter Stone*，[https://arxiv.org/pdf/1507.06527.pdf](https://arxiv.org/pdf/1507.06527.pdf)'
