- en: Chapter 8. Translating and Explaining with Encoding – decoding Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 使用编码解码网络进行翻译与解释
- en: Encoding-decoding techniques occur when inputs and outputs belong to the same
    space. For example, image segmentation consists of transforming an input image
    into a new image, the segmentation mask; translation consists of transforming
    a character sequence into a new character sequence; and question-answering consists
    of replying to a sequence of words with a new sequence of words.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 编码解码技术在输入和输出属于同一空间时应用。例如，图像分割是将输入图像转换为新图像，即分割掩码；翻译是将字符序列转换为新的字符序列；问答则是以新的字词序列回答输入的字词序列。
- en: 'To address these challenges, encoding-decoding networks are networks composed
    of two symmetric parts: an encoding network and a decoding network. The encoder
    network encodes the input data into a vector, which will be used by the decoder
    network to produce an output, such as a *translation*, an *answer* to the input
    question, an *explanation*, or an *annotation* of an input sentence or an input
    image.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，编码解码网络由两部分对称组成：编码网络和解码网络。编码器网络将输入数据编码成一个向量，解码器网络则利用该向量生成输出，例如*翻译*、*回答*输入问题、*解释*或*输入句子或输入图像的注释*。
- en: An encoder network is usually composed of the first layers of a network of the
    type of the ones presented in the previous chapters, without the last layers for
    dimensionality reduction and classification. Such a truncated network produces
    a multi-dimensional vector, named *features*, that gives an *internal state representation*
    to be used by the decoder to produce the output representation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络通常由前几层组成，这些层属于前面章节中介绍的网络类型，但没有用于降维和分类的最后几层。这样一个截断的网络会生成一个多维向量，称为*特征*，它为解码器提供一个*内部状态表示*，用于生成输出表示。
- en: 'This chapter decomposes into the following key concepts:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分解为以下关键概念：
- en: Sequence-to-sequence networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列网络
- en: Application to machine translation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译应用
- en: Application to chatbots
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人应用
- en: Deconvolutions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反卷积
- en: Application to image segmentation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割应用
- en: Application to image captioning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像标注应用
- en: Refinements in decoding techniques
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码技术的改进
- en: Sequence-to-sequence networks for natural language processing
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的序列到序列网络
- en: Rule-based systems are being replaced by end-to-end neural networks because
    of their increase in performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的系统正在被端到端神经网络所取代，因为后者在性能上有所提升。
- en: An end-to-end neural network means the network directly infers all possible
    rules by example, without knowing the underlying rules, such as syntax and conjugation;
    the words (or the characters) are directly fed into the network as input. The
    same is true for the output format, which can be directly the word indexes themselves.
    The architecture of the network takes care of learning the rules with its coefficients.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端神经网络意味着网络直接通过示例推断所有可能的规则，而无需了解潜在的规则，如语法和词形变化；单词（或字符）直接作为输入喂入网络。输出格式也是如此，输出可以直接是单词索引本身。网络架构通过其系数负责学习这些规则。
- en: 'The architecture of choice for such end-to-end encoding-decoding networks applied
    to **Natural Language Processing** (**NLP**), is the **sequence-to-sequence network**,
    displayed in the following figure:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于**自然语言处理**（**NLP**）的端到端编码解码网络架构是**序列到序列网络**，如以下图所示：
- en: '![Sequence-to-sequence networks for natural language processing](img/00116.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言处理中的序列到序列网络](img/00116.jpeg)'
- en: Word indexes are converted into their continuous multi-dimensional values in
    the embedded space with a lookup table. This conversion, presented in [Chapter
    3](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 3. Encoding
    Word into Vector"), *Encoding Word into Vector* is a crucial step to encode the
    discrete word indexes into a high dimensional space that a neural network can
    process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 单词索引通过查找表转换为其在嵌入空间中的连续多维值。这一转换，详见[第3章](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 3. Encoding Word into Vector")，*将单词编码为向量*，是将离散的单词索引编码到神经网络能够处理的高维空间中的关键步骤。
- en: Then, a first stack of LSTM is run on the input word embeddings, to encode the
    inputs and produce the thought vector. A second stack of LSTM is initiated with
    this vector as an initial internal state, and is expected to produce the next
    word for each word in the target sentence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，首先对输入的单词嵌入执行一堆LSTM操作，用以编码输入并生成思维向量。第二堆LSTM以这个向量作为初始内部状态，并且期望为目标句子中的每个单词生成下一个单词。
- en: 'At the core, is our classical step function for the LSTM cell, with input,
    forget, output, and cell gates:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心部分，是我们经典的LSTM单元步骤函数，包含输入、遗忘、输出和单元门：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A simple closure is better than a class. There are not enough methods and parameters
    to go for a class. Writing classes impose to add lots of `self`. Before all variables,
    an `__init__` method.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的闭包比一个类更好。没有足够的方法和参数去写一个类。编写类要求添加许多`self`，并且每个变量之前都要有一个`__init__`方法。
- en: To reduce computational cost, the full stack of layers is built into a one-step
    function and the recurrency is added to the top of the full stack step function
    that the output of the last layer produces for each timestep. Some other implementations
    have every layer independently recurrent, which is a lot less efficient (more
    than two times slower).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少计算成本，整个层栈被构建成一个一步函数，并且递归性被添加到整个栈步骤函数的顶部，该步骤函数会为每个时间步生成最后一层的输出。其他一些实现让每一层都独立递归，这样效率要低得多（慢于两倍以上）。
- en: 'On top of the *X* input, a mask variable, `m`, stops the recurrency when set
    to zero: hidden and cell states are kept constant when there is no more data (mask
    value is zero). Since the inputs are processed in batches, sentences in each batch
    can have different lengths and, thanks to the mask, all sentences in a batch can
    be processed in parallel with the same number of steps, corresponding to the maximal
    sentence length. The recurrency stops at a different position for each row in
    the batch.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在*X*输入的顶部，使用一个掩码变量`m`，当设置为零时，停止递归：当没有更多数据时，隐藏状态和单元状态保持不变（掩码值为零）。由于输入是批量处理的，因此每个批次中的句子可能有不同的长度，并且借助掩码，所有批次中的句子都可以并行处理，步数与最大句子长度相同。递归会在批次中每行的不同位置停止。
- en: 'The reason for a closure of a class is because the model cannot be applied
    directly to some symbolic input variables as in previous examples: indeed, the
    model is applied to the sequences inside a recurrency loop (with the scan operator).
    For this reason, in many high level deep learning frameworks, each layer is designed
    as a module that exposes a forward/backward method, to be added in various architectures
    (parallel branches and recurrency), as in this example.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类的闭包是因为模型不能像先前示例那样直接应用于某些符号输入变量：实际上，模型是应用于递归循环内的序列（使用扫描操作符）。因此，在许多高级深度学习框架中，每一层都被设计为一个模块，暴露出前向/反向方法，可以添加到各种架构中（并行分支和递归），正如本示例所示。
- en: 'The full stack step function of the encoder/decoder to be placed inside their
    respective recurrency loop can be designed as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器/解码器的完整栈步骤函数，放置在它们各自的递归循环内，可以设计如下：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first part is the conversion of the input to the embedding space. The second
    part is the stack of LSTM layers. For the decoder (when `target_voca_size != 0`),
    a linear layer is added to compute the output.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是将输入转换为嵌入空间。第二部分是LSTM层的堆栈。对于解码器（当`target_voca_size != 0`时），添加了一个线性层来计算输出。
- en: Now that we have our encoder/decoder step function, let's build the full encoder-decoder
    network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的编码器/解码器步骤函数，让我们构建完整的编码器-解码器网络。
- en: 'First, the encoder-decoder network has to encode the input into the internal
    state representation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，编码器-解码器网络必须将输入编码成内部状态表示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To encode the input, the encoding stack step function is run recurrently on
    each word.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编码输入，编码栈步骤函数会在每个单词上递归地运行。
- en: When `outputs_info` is composed of three variables, the scan operator considers
    that the output of the scan operation is composed of three values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当`outputs_info`由三个变量组成时，扫描操作符认为扫描操作的输出由三个值组成。
- en: 'These outputs come from the encoding stack step function and correspond to:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输出来自编码栈步骤函数，并且对应于：
- en: The output of the stack
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 栈的输出
- en: The hidden states of the stack, and
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 栈的隐藏状态，以及
- en: The cell states for the stack, for each step/word of the input sentence
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 栈的单元状态，对于输入句子的每个步骤/单词
- en: In `outputs_info`, `None` indicates to consider that the encoder will produce
    three outputs, but only the last two will be fed back into the step function (`h0
    -> h_` and `C0 -> C_`).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在`outputs_info`中，`None`表示考虑到编码器将产生三个输出，但只有最后两个会被反馈到步骤函数中（`h0 -> h_` 和 `C0 ->
    C_`）。
- en: Given that sequences point to two sequences, the step function for the scan
    operation has to handle four arguments.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序列指向两个序列，scan操作的步骤函数必须处理四个参数。
- en: 'Then, once the input sentence has been encoded into a vector, the encoder-decoder
    network decodes it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦输入句子被编码成向量，编码器-解码器网络将其解码：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The last states `hS[-1]`, `CS[-1]]` of the encoder network are fed as initial
    hidden and cell states of the decoder network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络的最后状态`hS[-1]`和`CS[-1]`将作为解码器网络的初始隐藏状态和细胞状态输入。
- en: Computing the log likelihood on top of the output is the same as in the previous
    chapter on sequences.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出上计算对数似然度与上一章关于序列的内容相同。
- en: 'For evaluation, the last predicted word has to be fed into the input of the
    decoder to predict the next word, which is a bit different from training, where
    input and output sequences are known:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估，最后预测的单词必须输入解码器中，以预测下一个单词，这与训练有所不同，在训练中输入和输出序列是已知的：
- en: '![Sequence-to-sequence networks for natural language processing](img/00117.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言处理的序列到序列网络](img/00117.jpeg)'
- en: 'In this case, `None` in `outputs_info` can be replaced with an initial value,
    `prediction_start`, the `start` token. Since it is not `None` anymore, this initial
    value will be fed into the step function of the decoder, as long as it is with
    `h0` and `C0`. The scan operator considers that there are three previous values
    to feed into the decoder function (and not two as before) at each step. Since
    the `decoderInputs` is removed from the input sequences, the number of arguments
    to the decoder stack step function remains four: the previous predicted output
    value is used in place of the fed input value. That way, the same decoder function
    can be used for both training and prediction:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`outputs_info`中的`None`可以替换为初始值`prediction_start`，即`start`标记。由于它不再是`None`，该初始值将被输入到解码器的步骤函数中，只要它与`h0`和`C0`一起存在。scan操作符认为每个步骤都有三个先前的值输入到解码器函数（而不是像之前那样只有两个）。由于`decoderInputs`已从输入序列中移除，因此传递给解码器堆栈步骤函数的参数数量仍然是四个：先前预测的输出值将取代输入值。这样，同一个解码器函数可以同时用于训练和预测：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The non-sequence parameter, `valid_data.idx_stop`, indicates to the decoder
    step function that it is in prediction mode, meaning the input is not a word index,
    but its previous output (requires finding the max index).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 非序列参数`valid_data.idx_stop`告诉解码器步骤函数，它处于预测模式，这意味着输入不是单词索引，而是其先前的输出（需要找到最大索引）。
- en: Also in prediction mode, one sentence at a time is predicted (batch size is
    `1`). The loop is stopped when the `end` token is produced, thanks to the `theano.scan_module.until`
    output in the decoder stack step function, and does not need to decode further
    words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在预测模式下，一次预测一个句子（批量大小为`1`）。当产生`end`标记时，循环停止，这得益于解码器堆栈步骤函数中的`theano.scan_module.until`输出，之后无需再解码更多单词。
- en: Seq2seq for translation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于翻译的Seq2seq
- en: '**Sequence-to-sequence** (**Seq2seq**) networks have their first application
    in language translation.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列到序列**（**Seq2seq**）网络的第一个应用是语言翻译。'
- en: A translation task has been designed for the conferences of the **Association
    for Computational Linguistics** (**ACL**), with a dataset, WMT16, composed of
    translations of news in different languages. The purpose of this dataset is to
    evaluate new translation systems or techniques. We'll use the German-English dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该翻译任务是为**计算语言学协会**（**ACL**）的会议设计的，数据集WMT16包含了不同语言的新闻翻译。此数据集的目的是评估新的翻译系统或技术。我们将使用德英数据集。
- en: 'First, preprocess the data:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，预处理数据：
- en: '[PRE5]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Train the `Seq2seq` network:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练`Seq2seq`网络：
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: At first glance, you notice the GPU time for one epoch is *445.906425953*, hence
    ten times faster than on the CPU (*4297.15962195*).
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初看之下，你会注意到每个周期的GPU时间是*445.906425953*，因此比CPU快十倍（*4297.15962195*）。
- en: 'Once trained, translate your sentences in English to German, loading the trained
    model :'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，将英语句子翻译成德语，加载已训练的模型：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Seq2seq for chatbots
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于聊天机器人的Seq2seq
- en: A second target application of sequence-to-sequence networks is question-answering,
    or chatbots.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列网络的第二个目标应用是问答系统或聊天机器人。
- en: 'For that purpose, download the Cornell Movie--Dialogs Corpus and preprocess
    it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，下载Cornell电影对话语料库并进行预处理：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This corpus contains a large metadata-rich collection of fictional conversations
    extracted from raw movie scripts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该语料库包含大量富含元数据的虚构对话，数据来自原始电影剧本。
- en: 'Since source and target sentences are in the same language, they use the same
    vocabulary, and the decoding network can use the same word embedding as the encoding
    network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于源语言和目标语言的句子使用相同的词汇表，解码网络可以使用与编码网络相同的词嵌入：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The same commands are true for `chatbot` dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`chatbot`数据集，相同的命令也适用：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Improving efficiency of sequence-to-sequence network
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高序列到序列网络的效率
- en: 'A first interesting point to notice in the chatbot example is the reverse ordered
    input sequence: such a technique has been shown to improve results.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天机器人示例中，第一个值得注意的有趣点是输入序列的反向顺序：这种技术已被证明能改善结果。
- en: 'For translation, it is very common then to use a bidirectional LSTM to compute
    the internal state as seen in [Chapter 5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 5. Analyzing Sentiment with a Bidirectional LSTM"), *Analyzing Sentiment
    with a Bidirectional LSTM*: two LSTMs, one running in the forward order, the other
    in the reverse order, run in parallel on the sequence, and their outputs are concatenated:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于翻译任务，使用双向LSTM来计算内部状态是非常常见的，正如在[第5章](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "第5章：使用双向LSTM分析情感")中所看到的，*使用双向LSTM分析情感*：两个LSTM，一个按正向顺序运行，另一个按反向顺序运行，两个并行处理序列，它们的输出被连接在一起：
- en: '![Improving efficiency of sequence-to-sequence network](img/00118.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![提高序列到序列网络的效率](img/00118.jpeg)'
- en: Such a mechanism captures better information given future and past.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制能够更好地捕捉给定未来和过去的信息。
- en: Another technique is the *attention mechanism* that will be the focus of the
    next chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术是*注意力机制*，这是下一章的重点。
- en: 'Lastly, *refinement techniques* have been developed and tested with two-dimensional
    Grid LSTM, which are not very far from stacked LSTM (the only difference is a
    gating mechanism in the depth/stack direction):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*精细化技术*已经开发并在二维Grid LSTM中进行了测试，这与堆叠LSTM相差不大（唯一的区别是在深度/堆叠方向上的门控机制）：
- en: '![Improving efficiency of sequence-to-sequence network](img/00119.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![提高序列到序列网络的效率](img/00119.jpeg)'
- en: Grid long short-term memory
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Grid 长短期记忆
- en: The principle of refinement is to run the stack in both orders on the input
    sentence as well, sequentially. The idea behind this formulation is to have the
    encoder network revisit or re-encode the sentence, after having encoded it in
    the forward direction, and implicitly capture some time patterns. Also, note that
    the 2D-grid gives more possible interactions for this re-encoding, re-encoding
    the vector at each prediction step, using previously outputted words as an orientation
    for the next predicted word. All this improvement is linked to a bigger computational
    capacity, in **O(n m)** for this re-encoder network (*n* and *m* represent the
    length of input and target sentences), while being of **O(n+m)** for the encoder-decoder
    network.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化的原则是也在输入句子上按两种顺序运行堆栈，顺序进行。这个公式的思想是让编码器网络在正向编码之后重新访问或重新编码句子，并隐式地捕捉一些时间模式。此外，请注意，二维网格提供了更多可能的交互作用来进行这种重新编码，在每个预测步骤重新编码向量，使用之前输出的单词作为下一个预测单词的方向。所有这些改进与更大的计算能力有关，对于这个重新编码器网络，其复杂度为**O(n
    m)**（*n*和*m*分别表示输入和目标句子的长度），而对于编码-解码网络来说，其复杂度为**O(n+m)**。
- en: All these techniques decrease perplexity. When the model is trained, consider
    also using the **beam search algorithm** that will keep track of the top-N possible
    predictions with their probabilities, instead of one, at each time step, to avoid
    the possibility that one bad prediction ranking at first position could lead to
    further erroneous predictions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术都有助于降低困惑度。当模型训练时，还可以考虑使用**束搜索算法**，该算法会在每个时间步跟踪多个预测及其概率，而不是仅跟踪一个，以避免一个错误的预测排名第一时导致后续错误预测。
- en: Deconvolutions for images
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像的反卷积
- en: In the case of images, researchers have been looking for decoding operations
    acting as the inverse of the encoding convolutions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像的情况下，研究人员一直在寻找作为编码卷积逆操作的解码操作。
- en: 'The first application was the analysis and understanding of convolutional networks,
    as seen in [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network*, composed of convolutional layers,
    max-pooling layers and rectified linear units. To better understand the network,
    the idea is to visualize the parts of an image that are most discriminative for
    a given unit of a network: one single neuron in a high level feature map is left
    non-zero and, from that activation, the signal is retro-propagated back to the
    2D input.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个应用是对卷积网络的分析与理解，如在[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "第2章 使用前馈网络分类手写数字")中所示，*使用前馈网络分类手写数字*，它由卷积层、最大池化层和修正线性单元组成。为了更好地理解网络，核心思想是可视化图像中对于网络某个单元最具判别性的部分：在高层特征图中的一个神经元被保持为非零，并且从该激活信号开始，信号会反向传播回二维输入。
- en: 'To reconstruct the signal through the max pooling layers, the idea is to keep
    track of the position of the maxima within each pooling region during the forward
    pass. Such architecture, named **DeConvNet** can be shown as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过最大池化层重建信号，核心思想是在正向传递过程中跟踪每个池化区域内最大值的位置。这种架构被称为**DeConvNet**，可以表示为：
- en: '![Deconvolutions for images](img/00120.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00120.jpeg)'
- en: Visualizing and understanding convolutional networks
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化和理解卷积网络
- en: The signal is retro-propagated to the position that had the maximal value during
    the forward pass.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 信号会被反向传播到在正向传递过程中具有最大值的位置。
- en: 'To reconstruct the signal through the ReLU layers, three methods have been
    proposed:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过ReLU层重建信号，已提出了三种方法：
- en: '*Back-propagation* retro-propagates only to the positions that have been positive'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向传播*仅反向传播到那些在正向传递过程中为正的位置。'
- en: '*Backward DeconvNet* retro-propagates only the positive gradients'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向DeConvNet*仅反向传播正梯度'
- en: '*Guided back-propagation* retro-propagates only to a position that satisfies
    both previous conditions, positive input during forward pass and positive gradient'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*引导反向传播*仅反向传播到满足两个先前条件的位置：在正向传递过程中输入为正，并且梯度为正。'
- en: 'The methods are illustrated in the following figure:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在下图中进行了说明：
- en: '![Deconvolutions for images](img/00121.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00121.jpeg)'
- en: 'The retro-propagation from the first layers gives various sorts of filter:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一层的反向传播给出了各种类型的滤波器：
- en: '![Deconvolutions for images](img/00122.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00122.jpeg)'
- en: 'However, from higher layers in the network, the guided back-propagation gives
    much better results:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从网络的更高层开始，引导反向传播给出了更好的结果：
- en: '![Deconvolutions for images](img/00123.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00123.jpeg)'
- en: 'It is also possible to condition the back-propagation on an input image, that
    will activate more than one neuron, from which the retro-propagation will be applied,
    to get a more precise input visualization:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以将反向传播条件化为输入图像，这样将激活多个神经元，并从中应用反向传播，以获得更精确的输入可视化：
- en: '![Deconvolutions for images](img/00124.jpeg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00124.jpeg)'
- en: 'The back-propagation can also be applied to the original input image rather
    than a blank one, a process that has been named **Inceptionism** by Google research,
    when retro-propagation is used to augment the output probability:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播也可以应用于原始输入图像，而不是空白图像，这一过程被谷歌研究命名为**Inceptionism**，当反向传播用于增强输出概率时：
- en: '![Deconvolutions for images](img/00125.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00125.jpeg)'
- en: 'But the main purpose of deconvolution is for scene segmentation or image semantic
    analysis, where the deconvolution is replaced by a learned upsampling convolution,
    such as in the **SegNet network**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但反卷积的主要目的是用于场景分割或图像语义分析，其中反卷积被学习的上采样卷积所替代，如**SegNet网络**中所示：
- en: '![Deconvolutions for images](img/00126.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00126.jpeg)'
- en: 'SegNet: A deep convolutional encoder-decoder architecture for image segmentation'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SegNet：一种用于图像分割的深度卷积编码器-解码器架构
- en: At every step in the deconvolution process, lower input features are usually
    concatenated to the current features for upsampling.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在反卷积过程中，每一步通常会将较低输入特征与当前特征进行连接，以进行上采样。
- en: 'The **DeepMask network** takes a mixed approach, deconvolutioning only the
    patches containing the objects. For that purpose, it is trained on input patches
    of 224x224 containing the objects (+/- 16 pixels in translation) instead of the
    full image:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepMask网络**采取一种混合方法，仅对包含对象的补丁进行反卷积。为此，它在包含对象的224x224输入补丁（平移误差±16像素）上进行训练，而不是完整的图像：'
- en: '![Deconvolutions for images](img/00127.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00127.jpeg)'
- en: Learning to segment object candidates
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 学习分割对象候选
- en: The convolutions of the encoder (VGG-16) network have a downsampling of factor
    16, leading to a feature map of 14x14.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器（VGG-16）网络的卷积层有一个16倍的下采样因子，导致特征图为14x14。
- en: A joint learning trains two branches, one for segmentation, one for scoring
    if the object is present, centered, and at the right scale in the patch.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个联合学习训练两个分支，一个用于分割，一个用于评分，判断补丁中对象是否存在、是否居中以及是否在正确的尺度上。
- en: 'The branch of interest is the semantic branch that upsamples to a 56x56 segmentation
    map of the object in the patch from the 14x14 feature map. To upsample, is possible
    if:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 相关分支是语义分支，它将14x14特征图中的对象上采样到56x56的分割图。上采样是可能的，如果：
- en: A fully connected layer, meaning that each position in the upsampled map depends
    on all features and has the global picture to predict the value
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个全连接层，意味着上采样图中的每个位置都依赖于所有特征，并且具有全局视野来预测值。
- en: A convolution (or locally connected layer), reducing the number of parameters,
    but also predicting each position score with a partial view
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个卷积（或局部连接层），减少了参数数量，但也通过部分视图预测每个位置的分数。
- en: A mixed approach, consisting of two linear layers with no non-linearity between
    them, in a way to perform a dimensionality reduction, as presented in the preceding
    figure
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种混合方法，由两个线性层组成，二者之间没有非线性，旨在执行降维操作，如前图所示
- en: The output mask is then upsampled back to the original patch dimensions 224x224
    by a simple bilinear upsampling layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出掩膜随后通过一个简单的双线性上采样层被上采样回原始的224x224补丁维度。
- en: To deal with the full input image, fully connected layers can be transformed
    into convolutions with a kernel size equal to the fully connected layer input
    size and the same coefficients, so that the network becomes fully convolutional,
    with stride 16, when applied to the full image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理完整的输入图像，可以将全连接层转换为卷积层，卷积核大小等于全连接层的输入大小，并使用相同的系数，这样网络在应用到完整图像时就变成了完全卷积的网络，步长为16。
- en: 'As sequence-to-sequence networks have been refined with a bidirectional reencoding
    mechanism, the **SharpMask** approach improves the sharpness of the upsampling
    deconvolutional process using the input convolutional features at the equivalent
    scale:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随着序列到序列网络通过双向重新编码机制得到改进，**SharpMask**方法通过在等效尺度上使用输入卷积特征来改善上采样反卷积过程的锐度：
- en: '![Deconvolutions for images](img/00128.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图像反卷积](img/00128.jpeg)'
- en: Learning to refine object segments
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 学习细化对象分割
- en: While the SegNet approach only learns to deconvolve from an up-sampled map produced
    by keeping track of the max pooling indices, the SharpMask approach directly reuses
    the input feature maps, a very usual technique for coarse-to-finegrained approaches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 而SegNet方法仅通过跟踪最大池化索引产生的上采样图来学习反卷积，SharpMask方法直接重用输入特征图，这是一种非常常见的粗到细方法。
- en: Lastly, bear in mind that it is possible to improve the results one step further
    with the application of a **Conditional Random Fields** (**CRF**) post-processing
    step, either for one-dimensional inputs such as texts, or two-dimensional inputs
    such as segmentation images.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住，通过应用**条件随机场**（**CRF**）后处理步骤，可以进一步改善结果，无论是对于一维输入（如文本），还是二维输入（如分割图像）。
- en: Multimodal deep learning
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态深度学习
- en: To open the possible applications further, the encoding-decoding framework can
    be applied with different modalities, such as, for example, for image captioning.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步开放可能的应用，编码-解码框架可以应用于不同的模态，例如，图像描述。
- en: Image captioning consists of describing the content of the image with words.
    The input is an image, naturally encoded into a thought vector with a deep convolutional
    network.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述是用文字描述图像的内容。输入是图像，通常通过深度卷积网络编码成一个思想向量。
- en: 'The text to describe the content of the image can be produced from this internal
    state vector with the same stack of LSTM networks as a decoder, as in Seq2seq
    networks:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 用于描述图像内容的文本可以从这个内部状态向量中生成，解码器采用相同的LSTM网络堆栈，就像Seq2seq网络一样：
- en: '![Multimodal deep learning](img/00129.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![多模态深度学习](img/00129.jpeg)'
- en: Further reading
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following topics for better insights:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下主题以获取更深入的见解：
- en: '*Sequence to Sequence Learning with Neural Networks*, Ilya Sutskever, Oriol
    Vinyals, Quoc V. Le, Dec 2014'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于神经网络的序列到序列学习*，Ilya Sutskever，Oriol Vinyals，Quoc V. Le，2014年12月'
- en: '*Learning Phrase Representations using RNN Encoder–Decoder for Statistical
    Machine Translation*, Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Sept 2014'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用RNN编码器-解码器的短语表示学习用于统计机器翻译*，Kyunghyun Cho，Bart van Merrienboer，Caglar Gulcehre，Dzmitry
    Bahdanau，Fethi Bougares，Holger Schwenk，Yoshua Bengio，2014年9月'
- en: '*Neural Machine Translation by Jointly Learning to Align and Translate*, Dzmitry
    Bahdanau, Kyunghyun Cho, Yoshua Bengio, May 2016'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过联合学习对齐与翻译的神经机器翻译*，Dzmitry Bahdanau，Kyunghyun Cho，Yoshua Bengio，2016年5月'
- en: '*A Neural Conversational Model*, Oriol Vinyals, Quoc Le, July 2015'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经对话模型*，Oriol Vinyals，Quoc Le，2015年7月'
- en: '*Fast and Robust Neural Network Joint Models for Statistical Machine Translation*,
    Jacob Devlin, Rabih Zbib, Zhongqiang Huang,Thomas Lamar, Richard Schwartz, John
    Mkahoul, 2014'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*快速而强大的神经网络联合模型用于统计机器翻译*，Jacob Devlin，Rabih Zbib，Zhongqiang Huang，Thomas Lamar，Richard
    Schwartz，John Mkahoul，2014年'
- en: '*SYSTRAN''s Pure Neural Machine Translation Systems*, Josep Crego, Jungi Kim,
    Guillaume Klein, Anabel Rebollo, Kathy Yang, Jean Senellart, Egor Akhanov, Patrice
    Brunelle, Aurelien Coquard, Yongchao Deng, Satoshi Enoue, Chiyo Geiss, Joshua
    Johanson, Ardas Khalsa, Raoum Khiari, Byeongil Ko, Catherine Kobus, Jean Lorieux,
    Leidiana Martins, Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia
    Segal, Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou,
    Peter Zoldan, 2016'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SYSTRAN的纯神经机器翻译系统*，Josep Crego，Jungi Kim，Guillaume Klein，Anabel Rebollo，Kathy
    Yang，Jean Senellart，Egor Akhanov，Patrice Brunelle，Aurelien Coquard，Yongchao Deng，Satoshi
    Enoue，Chiyo Geiss，Joshua Johanson，Ardas Khalsa，Raoum Khiari，Byeongil Ko，Catherine
    Kobus，Jean Lorieux，Leidiana Martins，Dang-Chuan Nguyen，Alexandra Priori，Thomas
    Riccardi，Natalia Segal，Christophe Servan，Cyril Tiquet，Bo Wang，Jin Yang，Dakun Zhang，Jing
    Zhou，Peter Zoldan，2016年'
- en: '*Blue: a method for automatic evaluatoin of machine translation,* Kishore Papineni,
    Salim Roukos, Todd Ward, and Wei-Jing Zhu, 2002'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Blue：一种自动评估机器翻译的方法*，Kishore Papineni，Salim Roukos，Todd Ward，Wei-Jing Zhu，2002年'
- en: ACL 2016 translation task
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ACL 2016翻译任务
- en: '*Chameleons in imagined conversations: A new approach to understanding coordination
    of linguistic style in dialogs*, Cristian Danescu-NiculescuMizil and Lillian Lee2011
    at: [https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变色龙在假想对话中的应用：一种理解对话中文本风格协调的新方法*，Cristian Danescu-NiculescuMizil 和 Lillian
    Lee，2011，见：[https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)'
- en: '*Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected
    CRFs*, Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan
    L., Yuille 2014'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过深度卷积网络和完全连接的CRFs进行语义图像分割*，Liang-Chieh Chen，George Papandreou，Iasonas Kokkinos，Kevin
    Murphy，Alan L.，Yuille，2014年'
- en: '*SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation*,
    Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, Oct 2016'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SegNet：一种用于图像分割的深度卷积编码器-解码器架构*，Vijay Badrinarayanan，Alex Kendall，Roberto Cipolla，2016年10月'
- en: '*R-FCN: Object Detection via Region-based Fully Convolutional Networks*, Jifeng
    Dai, Yi Li, Kaiming He, Jian Sun2016'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R-FCN：基于区域的全卷积网络进行物体检测*，Jifeng Dai，Yi Li，Kaiming He，Jian Sun，2016年'
- en: '*Learning to segment object candidates*, Pedro O. Pinheiro, Ronan Collobert,
    Piotr Dollar, June 2015'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习分割物体候选框*，Pedro O. Pinheiro，Ronan Collobert，Piotr Dollar，2015年6月'
- en: '*Learning to refine object segments*, Pedro O. Pinheiro, Tsung-Yi Lin, Ronan
    Collobert, Piotr Dollàr, Mar 2016'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习优化物体分割*，Pedro O. Pinheiro，Tsung-Yi Lin，Ronan Collobert，Piotr Dollàr，2016年3月'
- en: '*Visualizing and Understanding Convolutional Networks*, Matthew D Zeiler, Rob
    Fergus, Nov 2013'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化与理解卷积网络*，Matthew D Zeiler，Rob Fergus，2013年11月'
- en: '*Show and tell: A Neural Image Caption Generator*, Oriol Vinyals, Alexander
    Toshev, Samy Bengio, Dumitru Erhan, 2014'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*展示与讲述：神经图像标题生成器*，Oriol Vinyals，Alexander Toshev，Samy Bengio，Dumitru Erhan，2014年'
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As for love, head-to-toe positions provide exciting new possibilities: encoder
    and decoder networks use the same stack of layers but in their opposite directions.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 至于爱情，头到脚的姿势提供了令人兴奋的新可能性：编码器和解码器网络使用相同的层堆栈，但方向相反。
- en: Although it does not provide new modules to deep learning, such a technique
    of *encoding-decoding* is quite important because it enables the training of the
    networks 'end-to-end', that is, directly feeding the inputs and corresponding
    outputs, without specifying any rules or patterns to the networks and without
    decomposing encoding training and decoding training into two separate steps.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它没有为深度学习提供新的模块，但*编码-解码*技术非常重要，因为它使得网络能够进行“端到端”训练，也就是说，直接将输入和相应的输出喂入网络，而不需要为网络指定任何规则或模式，也不需要将编码训练和解码训练拆分成两个独立的步骤。
- en: While image classification was a one-to-one task, and sentiment analysis a many-to-one
    task, encoding-decoding techniques illustrate many-to-many tasks, such as translation
    or image segmentation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图像分类是一个一对一的任务，情感分析是一个多对一的任务，但编码-解码技术展示了多对多的任务，比如翻译或图像分割。
- en: In the next chapter, we'll introduce an *attention mechanism* that provides
    the ability for encoder-decoder architecture to focus on some parts of the input
    in order to produce a more accurate output.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一种*注意力机制*，它赋予编码-解码架构专注于输入的某些部分，以便生成更准确的输出的能力。
