- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: A Primer on TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 入门
- en: TensorFlow is one of the most popular deep learning libraries. In upcoming chapters,
    we will use TensorFlow to build deep reinforcement models. So, in this chapter,
    we will get ourselves familiar with TensorFlow and its functionalities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是最受欢迎的深度学习库之一。在接下来的章节中，我们将使用 TensorFlow 构建深度强化学习模型。因此，在本章中，我们将熟悉
    TensorFlow 及其功能。
- en: We will learn about what computational graphs are and how TensorFlow uses them.
    We will also explore TensorBoard, which is a visualization tool provided by TensorFlow
    used for visualizing models. Going forward, we will understand how to build a
    neural network with TensorFlow to perform handwritten digit classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将了解什么是计算图，以及 TensorFlow 如何使用它们。我们还将探讨 TensorBoard，这是 TensorFlow 提供的一个可视化工具，用于可视化模型。接下来，我们将理解如何使用
    TensorFlow 构建神经网络来执行手写数字分类。
- en: Moving on, we will learn about TensorFlow 2.0, which is the latest version of
    TensorFlow. We will understand how TensorFlow 2.0 differs from its previous versions
    and how it uses Keras as its high-level API.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习 TensorFlow 2.0，它是 TensorFlow 的最新版本。我们将了解 TensorFlow 2.0 与其先前版本的不同之处，以及它如何使用
    Keras 作为其高级 API。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Computational graphs and sessions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图和会话
- en: Variables, constants, and placeholders
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量、常量和占位符
- en: TensorBoard
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard
- en: Handwritten digit classification in TensorFlow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中的手写数字分类
- en: Math operations in TensorFlow
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中的数学运算
- en: TensorFlow 2.0 and Keras
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 和 Keras
- en: What is TensorFlow?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: TensorFlow is an open source software library from Google, which is extensively
    used for numerical computation. It is one of the most used libraries for building
    deep learning models. It is highly scalable and runs on multiple platforms, such
    as Windows, Linux, macOS, and Android. It was originally developed by the researchers
    and engineers of the Google Brain team.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是 Google 的一个开源软件库，广泛用于数值计算。它是构建深度学习模型时使用最广泛的库之一，具有高度的可扩展性，并能在多个平台上运行，例如
    Windows、Linux、macOS 和 Android。最初由 Google Brain 团队的研究人员和工程师开发。
- en: TensorFlow supports execution on everything, including CPUs, GPUs, TPUs, which
    are tensor processing units, and mobile and embedded platforms. Due to its flexible
    architecture and ease of deployment, it has become a popular choice of library
    among many researchers and scientists for building deep learning models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 支持在各种设备上执行，包括 CPU、GPU、TPU（张量处理单元）、移动平台和嵌入式平台。由于其灵活的架构和易于部署，它已成为许多研究人员和科学家构建深度学习模型时的热门选择。
- en: In TensorFlow, every computation is represented by a data flow graph, also known
    as a **computational graph**, where nodes represent operations, such as addition
    or multiplication, and edges represent tensors. Data flow graphs can also be shared
    and executed on many different platforms. TensorFlow provides a visualization
    tool, called TensorBoard, for visualizing data flow graphs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，每个计算都是通过数据流图表示的，也称为 **计算图**，其中节点代表操作（如加法或乘法），边代表张量。数据流图也可以在许多不同的平台上共享和执行。TensorFlow
    提供了一种可视化工具，叫做 TensorBoard，用于可视化数据流图。
- en: TensorFlow 2.0 is the latest version of TensorFlow. In the upcoming chapters,
    we will use TensorFlow 2.0 for building deep reinforcement learning models. However,
    it is important to understand how TensorFlow 1.x works. So, first, we will learn
    to use TensorFlow 1.x and then we will look into TensorFlow 2.0\.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 是 TensorFlow 的最新版本。在接下来的章节中，我们将使用 TensorFlow 2.0 构建深度强化学习模型。然而，理解
    TensorFlow 1.x 的工作原理也很重要。所以，首先，我们将学习如何使用 TensorFlow 1.x，然后再深入了解 TensorFlow 2.0。
- en: 'You can install TensorFlow easily through `pip` by just typing the following
    command in your terminal:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在终端输入以下命令，轻松通过 `pip` 安装 TensorFlow：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can check the successful installation of TensorFlow by running the following
    simple `Hello TensorFlow!` program:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下简单的 `Hello TensorFlow!` 程序来检查 TensorFlow 是否安装成功：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding program should print `Hello TensorFlow!`. If you get any errors,
    then you probably have not installed TensorFlow correctly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的程序应该打印出`Hello TensorFlow!`。如果你遇到任何错误，那么可能是你没有正确安装 TensorFlow。
- en: Understanding computational graphs and sessions
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解计算图和会话
- en: As we have learned, every computation in TensorFlow is represented by a computational
    graph. They consist of several nodes and edges, where nodes are mathematical operations,
    such as addition and multiplication, and edges are tensors. Computational graphs
    are very efficient at optimizing resources and promote distributed computing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所学，TensorFlow 中的每一个计算都由一个计算图表示。计算图由多个节点和边组成，其中节点是数学运算，如加法和乘法，边是张量。计算图在资源优化方面非常高效，并且促进了分布式计算。
- en: A computational graph consists of several TensorFlow operations, arranged in
    a graph of nodes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图由多个 TensorFlow 操作组成，排列成一个节点图。
- en: 'A computational graph helps us to understand the network architecture when
    we work on building a really complex neural network. For instance, let''s consider
    a simple layer, *h* = Relu(*WX* + *b*). Its computational graph would be represented
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图帮助我们理解网络架构，尤其在构建复杂神经网络时。例如，假设我们考虑一个简单的层，*h* = Relu(*WX* + *b*)。其计算图将如下所示：
- en: '![](img/B15558_08_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_01.png)'
- en: 'Figure 8.1: Computational graph'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：计算图
- en: 'There are two types of dependency in the computational graph, called direct
    and indirect dependency. Say we have node `b`, the input of which is dependent
    on the output of node `a`; this type of dependency is called **direct dependency**,
    as shown in the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图中有两种依赖关系，分别称为直接依赖和间接依赖。假设我们有节点`b`，其输入依赖于节点`a`的输出；这种依赖关系称为**直接依赖**，如以下代码所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When node `b` doesn''t depend on node `a` for its input, it is called **indirect
    dependency**, as shown in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点 `b` 的输入不依赖于节点 `a` 时，这称为**间接依赖**，如以下代码所示：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So, if we can understand these dependencies, we can distribute the independent
    computations in the available resources and reduce the computation time. Whenever
    we import TensorFlow, a default graph is created automatically and all of the
    nodes we create are associated with the default graph. We can also create our
    own graphs instead of using the default graph, and this is useful when building
    multiple models that do not depend on one another. A TensorFlow graph can be created
    using `tf.Graph()`, as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够理解这些依赖关系，就能在可用资源中分配独立的计算，减少计算时间。每当我们导入 TensorFlow 时，默认图会自动创建，我们创建的所有节点都会与该默认图关联。我们还可以创建自己的图，而不是使用默认图，这在构建相互独立的多个模型时非常有用。可以使用`tf.Graph()`创建一个
    TensorFlow 图，如下所示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we want to clear the default graph (that is, if we want to clear the previously
    defined variables and operations in the graph), then we can do this using `tf.reset_default_graph()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想清除默认图（即清除图中先前定义的变量和操作），则可以使用`tf.reset_default_graph()`来实现。
- en: Sessions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 会话
- en: As mentioned in the previous section, a computational graph with operations
    on its nodes and tensors to its edges is created, and in order to execute the
    graph, we use a TensorFlow session.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，创建了一个计算图，其节点上包含操作，边缘上连接张量。为了执行该图，我们使用一个 TensorFlow 会话。
- en: 'A TensorFlow session can be created using `tf.Session()`, as shown in the following
    code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`tf.Session()`来创建 TensorFlow 会话，如以下代码所示：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After creating the session, we can execute our graph, using the `sess.run()`
    method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建会话后，我们可以使用`sess.run()`方法来执行我们的计算图。
- en: Every computation in TensorFlow is represented by a computational graph, so
    we need to run a computational graph for everything. That is, in order to compute
    anything on TensorFlow, we need to create a TensorFlow session.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的每一个计算都由一个计算图表示，因此我们需要运行计算图来进行所有计算。也就是说，要在 TensorFlow 中进行任何计算，我们需要创建一个
    TensorFlow 会话。
- en: 'Let''s execute the following code to multiply two numbers:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下代码以乘法运算两个数字：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Instead of printing `9`, the preceding code will print a TensorFlow object,
    `Tensor("Mul:0", shape=(), dtype=int32)`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与打印 `9` 不同，前面的代码将打印一个 TensorFlow 对象，`Tensor("Mul:0", shape=(), dtype=int32)`。
- en: As we discussed earlier, whenever we import TensorFlow, a default computational
    graph is automatically created and all nodes are attached to the graph. Hence,
    when we print `a`, it just returns the TensorFlow object because the value for
    `a` is not computed yet, as the computation graph has not been executed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每当我们导入 TensorFlow 时，默认的计算图会自动创建，所有节点都会附加到该图上。因此，当我们打印 `a` 时，它只会返回 TensorFlow
    对象，因为 `a` 的值尚未计算，计算图尚未执行。
- en: 'In order to execute the graph, we need to initialize and run the TensorFlow
    session, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行计算图，我们需要初始化并运行 TensorFlow 会话，如下所示：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code prints `9`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印出 `9`。
- en: Now that we have learned about sessions, in the next section, we will learn
    about variables, constants, and placeholders.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了会话，接下来的部分将学习变量、常量和占位符。
- en: Variables, constants, and placeholders
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量、常量和占位符
- en: Variables, constants, and placeholders are fundamental elements of TensorFlow.
    However, there is always confusion between these three. Let's look at each element,
    one by one, and learn the difference between them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 变量、常量和占位符是 TensorFlow 的基础元素。然而，这三者之间经常会产生混淆。让我们逐一查看每个元素，并了解它们之间的区别。
- en: Variables
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量
- en: 'Variables are containers used to store values. Variables are used as input
    to several other operations in a computational graph. A variable can be created
    using the `tf.Variable()` function, as shown in the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 变量是用来存储值的容器。变量作为输入用于计算图中的多个其他操作。可以使用 `tf.Variable()` 函数来创建变量，如下列代码所示：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s create a variable called `W`, using `tf.Variable()`, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `tf.Variable()` 创建一个名为 `W` 的变量，如下所示：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see in the preceding code, we create a variable, `W`, by randomly
    drawing values from a normal distribution with a standard deviation of `0.35`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的代码所示，我们通过从标准差为 `0.35` 的正态分布中随机抽取值来创建一个变量 `W`。
- en: What is that `name` parameter in `tf.Variable()`?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable()` 中的 `name` 参数是什么？'
- en: It is used to set the name of the variable in the computational graph. So, in
    the preceding code, Python saves the variable as `W` but in the TensorFlow graph,
    it will be saved as `weights`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它用于设置计算图中变量的名称。所以，在前面的代码中，Python 将变量保存为 `W`，但在 TensorFlow 图中，它将被保存为 `weights`。
- en: After defining a variable, we need to initialize all of the variables in the
    computational graph. That can be done using `tf.global_variables_initializer()`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义变量后，我们需要初始化计算图中的所有变量。可以使用 `tf.global_variables_initializer()` 来完成这一步。
- en: 'Once we create a session, we run the initialization operation, which initializes
    all of the defined variables, and only then can we run the other operations, as
    shown in the following code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了会话，我们运行初始化操作，初始化所有已定义的变量，只有在此之后才能运行其他操作，如下列代码所示：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Constants
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常量
- en: 'Constants, unlike variables, cannot have their values changed. That is, constants
    are immutable. Once they are assigned values, they cannot be changed throughout
    the program. We can create constants using `tf.constant()`, as the following code
    shows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 常量与变量不同，常量的值不能改变。也就是说，常量是不可变的。一旦赋值，它们在整个程序中都无法更改。我们可以使用 `tf.constant()` 创建常量，如下列代码所示：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Placeholders and feed dictionaries
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 占位符和馈送字典
- en: We can think of placeholders as variables, where we only define the type and
    dimension, but do not assign a value. Values for the placeholders will be fed
    at runtime. We feed data to computational graphs using placeholders. Placeholders
    are defined with no values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把占位符看作变量，在其中只定义类型和维度，但不分配值。占位符的值将在运行时输入。我们使用占位符将数据传递给计算图。占位符在定义时没有值。
- en: 'A placeholder can be defined using `tf.placeholder()`. It takes an optional
    argument called `shape`, which denotes the dimensions of the data. If `shape`
    is set to `None`, then we can feed data of any size at runtime. A placeholder
    can be defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符可以通过 `tf.placeholder()` 来定义。它有一个可选参数 `shape`，表示数据的维度。如果 `shape` 设置为 `None`，则可以在运行时输入任何大小的数据。占位符可以这样定义：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To put it in simple terms, we use `tf.Variable` to store data and `tf.placeholder`
    to feed in external data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们使用 `tf.Variable` 来存储数据，而使用 `tf.placeholder` 来输入外部数据。
- en: 'Let''s consider a simple example to better understand placeholders:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解占位符，我们考虑一个简单的例子：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we run the preceding code, then it will return an error because we are trying
    to compute `y`, where `y= x+3` and `x` is a placeholder whose value is not assigned.
    As we have learned, values for the placeholders will be assigned at runtime. We
    assign the values of the placeholder using the `feed_dict` parameter. The `feed_dict`
    parameter is basically a dictionary where the key represents the name of the placeholder,
    and the value represents the value of the placeholder.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行上述代码，它将返回一个错误，因为我们正在尝试计算 `y`，其中 `y = x + 3`，而 `x` 是一个占位符，其值尚未分配。正如我们所学，占位符的值将在运行时分配。我们通过
    `feed_dict` 参数来分配占位符的值。`feed_dict` 参数实际上是一个字典，其中键代表占位符的名称，值代表占位符的值。
- en: 'As you can see in the following code, we set `feed_dict = {x:5}`, which implies
    that the value for the `x` placeholder is `5`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，我们设置了`feed_dict = {x:5}`，这意味着`x`占位符的值为`5`：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code returns `8.0`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回`8.0`。
- en: That's it. In the next section, we will learn about TensorBoard.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。在下一节中，我们将学习 TensorBoard。
- en: Introducing TensorBoard
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 TensorBoard
- en: TensorBoard is TensorFlow's visualization tool, which can be used to visualize
    a computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it becomes confusing when we have to debug the network. So,
    if we can visualize the computational graph in TensorBoard, we can easily understand
    such complex models, debug them, and optimize them. TensorBoard also supports
    sharing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是 TensorFlow 的可视化工具，可用于可视化计算图。它还可以用来绘制各种定量指标以及几个中间计算的结果。当我们训练一个非常深的神经网络时，如果我们必须调试网络，就会变得非常混乱。因此，如果我们能够在
    TensorBoard 中可视化计算图，就可以轻松理解这些复杂的模型、调试它们并优化它们。TensorBoard 还支持共享。
- en: 'As shown in *Figure 8.2*, the TensorBoard panel consists of several tabs—**SCALARS**,
    **IMAGES**, **AUDIO**, **GRAPHS**, **DISTRIBUTIONS**, **HISTOGRAMS**, and **EMBEDDINGS**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 8.2*所示，TensorBoard 面板由多个标签组成——**标量（SCALARS）**、**图像（IMAGES）**、**音频（AUDIO）**、**图形（GRAPHS）**、**分布（DISTRIBUTIONS）**、**直方图（HISTOGRAMS）**和**嵌入（EMBEDDINGS）**：
- en: '![](img/B15558_08_02.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_02.png)'
- en: 'Figure 8.2: TensorBoard'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：TensorBoard
- en: The tabs are pretty self-explanatory. The **SCALARS** tab shows useful information
    about the scalar variables we use in our program. For example, it shows how the
    value of a scalar variable called `loss` changes over several iterations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签是相当直观的。**SCALARS** 标签显示我们程序中使用的标量变量的有用信息。例如，它显示了一个名为 `loss` 的标量变量在多个迭代过程中如何变化。
- en: The **GRAPHS** tab shows the computational graph. The **DISTRIBUTIONS** and
    **HISTOGRAMS** tabs show the distribution of a variable. For example, our model's
    weight distribution and histogram can be seen under these tabs. The **EMBEDDINGS**
    tab is used for visualizing high-dimensional vectors, such as word embeddings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRAPHS** 标签显示计算图。**DISTRIBUTIONS** 和 **HISTOGRAMS** 标签显示一个变量的分布。例如，我们模型的权重分布和直方图可以在这两个标签下看到。**EMBEDDINGS**
    标签用于可视化高维向量，如词嵌入。'
- en: 'Let''s build a basic computational graph and visualize it in TensorBoard. Let''s
    say we have four constants, shown as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个基本的计算图并在 TensorBoard 中可视化它。假设我们有四个常量，如下所示：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s multiply `x` and `y` and `a` and `b` and save them as `prod1` and `prod2`,
    as shown in the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `x` 和 `y` 以及 `a` 和 `b` 相乘，并将结果分别存储为 `prod1` 和 `prod2`，如以下代码所示：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add `prod1` and `prod2` and store them in `sum`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `prod1` 和 `prod2` 相加，并将结果存储在 `sum` 中：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we can visualize all of these operations in TensorBoard. To visualize in
    TensorBoard, we first need to save our event files. That can be done using `tf.summary.FileWriter()`.
    It takes two important parameters, `logdir` and `graph`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在 TensorBoard 中可视化所有这些操作。要在 TensorBoard 中可视化，我们首先需要保存事件文件。这可以通过使用`tf.summary.FileWriter()`来完成。它有两个重要的参数，`logdir`和`graph`。
- en: 'As the name suggests, `logdir` specifies the directory where we want to store
    the graph, and `graph` specifies which graph we want to store:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，`logdir` 指定了我们要存储图形的目录，而 `graph` 指定了我们要存储的图形：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, `./graphs` is the directory where we are storing our
    event file, and `sess.graph` specifies the current graph in our TensorFlow session.
    So, we are storing the current graph of the TensorFlow session in the `graphs`
    directory.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`./graphs` 是我们存储事件文件的目录，`sess.graph` 指定了我们 TensorFlow 会话中的当前图。因此，我们将
    TensorFlow 会话的当前图存储在 `graphs` 目录中。
- en: 'To start TensorBoard, go to your Terminal, locate the working directory, and
    type the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动TensorBoard，请打开终端，定位到工作目录，并输入以下命令：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `logdir` parameter indicates the directory where the event file is stored
    and `port` is the port number. Once you run the preceding command, open your browser
    and type `http://localhost:8000/`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`logdir`参数表示事件文件存储的目录，`port`是端口号。运行上述命令后，打开浏览器并输入`http://localhost:8000/`。'
- en: 'In the TensorBoard panel, under the **GRAPHS** tab, you can see the computational
    graph:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorBoard面板下的**GRAPHS**标签中，你可以看到计算图：
- en: '![](img/B15558_08_03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_03.png)'
- en: 'Figure 8.3: Computational graph'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：计算图
- en: As you may notice, all of the operations we have defined are clearly shown in
    the graph.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们定义的所有操作在图中都有清晰的展示。
- en: Creating a name scope
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建命名范围
- en: Scoping is used to reduce complexity and helps us to better understand a model
    by grouping related nodes together. Having a name scope helps us to group similar
    operations in a graph. This comes in handy when we are building a complex architecture.
    Scoping can be created using `tf.name_scope()`. In the previous example, we performed
    two operations, `Product` and `sum`. We can simply group them into two different
    name scopes as `Product` and `sum`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 范围作用用于减少复杂性，并通过将相关节点分组在一起来帮助我们更好地理解模型。拥有命名空间有助于我们将相似的操作分组在图中。当我们构建复杂架构时，这非常有用。可以使用`tf.name_scope()`来创建范围。在前面的例子中，我们执行了两个操作，`Product`和`sum`。我们可以将它们简单地分组到两个不同的命名范围中，分别为`Product`和`sum`。
- en: 'In the previous section, we saw how `prod1` and `prod2` perform multiplication
    and compute the result. We''ll define a name scope called `Product`, and group
    the `prod1` and `prod2` operations, as shown in the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到`prod1`和`prod2`执行乘法并计算结果。我们将定义一个名为`Product`的命名范围，并将`prod1`和`prod2`操作分组，如下所示：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, define the name scope for `sum`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，定义`sum`的命名范围：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Store the file in the `graphs` directory:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件存储在`graphs`目录中：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Visualize the graph in TensorBoard:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorBoard中可视化图形：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you may notice, now, we have only two nodes, **sum** and **Product**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，现在我们只有两个节点，**sum**和**Product**：
- en: '![](img/B15558_08_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_04.png)'
- en: 'Figure 8.4: A computational graph'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：计算图
- en: 'Once we double-click on the nodes, we can see how the computation is happening.
    As you can see, the **prod1** and **prod2** nodes are grouped under the **Product**
    scope, and their results are sent to the **sum** node, where they will be added.
    You can see how the **prod1** and **prod2** nodes compute their value:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们双击节点，就可以看到计算是如何进行的。如你所见，**prod1**和**prod2**节点被分组在**Product**范围下，它们的结果被传送到**sum**节点，在那里进行相加。你可以看到**prod1**和**prod2**节点是如何计算它们的值的：
- en: '![](img/B15558_08_05.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_05.png)'
- en: 'Figure 8.5: A computational graph in detail'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：详细的计算图
- en: The preceding graph is just a simple example. When we are working on a complex
    project with a lot of operations, name scoping helps us to group similar operations
    together and enables us to understand the computational graph better.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图仅是一个简单的示例。当我们处理一个有许多操作的复杂项目时，命名范围帮助我们将相似的操作分组在一起，并使我们更好地理解计算图。
- en: Now that we have learned about TensorFlow, in the next section, let's see how
    to build handwritten digit classification using TensorFlow.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了TensorFlow，在接下来的章节中，让我们看看如何使用TensorFlow构建手写数字分类。
- en: Handwritten digit classification using TensorFlow
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行手写数字分类
- en: Putting together all the concepts we have learned so far, we will see how we
    can use TensorFlow to build a neural network to recognize handwritten digits.
    If you have been playing around with deep learning of late, then you must have
    come across the MNIST dataset. It has been called the *hello world* of deep learning.
    It consists of 55,000 data points of handwritten digits (0 to 9).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们迄今为止学到的所有概念结合起来，我们将看到如何使用TensorFlow构建一个神经网络来识别手写数字。如果你最近在玩深度学习，那么你一定接触过MNIST数据集。它被称为深度学习的*hello
    world*。它包含了55,000个手写数字数据点（0到9）。
- en: In this section, we will see how we can use our neural network to recognize
    these handwritten digits, and we will get the hang of TensorFlow and TensorBoard.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何使用神经网络识别这些手写数字，并掌握TensorFlow和TensorBoard的使用。
- en: Importing the required libraries
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入所需的库
- en: 'As a first step, let''s import all of the required libraries:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们导入所有所需的库：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Loading the dataset
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'Load the dataset, using the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码加载数据集：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code, `data/mnist` implies the location where we store the
    MNIST dataset, and `one_hot=True` implies that we are one-hot encoding the labels
    (0 to 9).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`data/mnist` 表示我们存储 MNIST 数据集的位置，而 `one_hot=True` 表示我们正在对标签进行 one-hot
    编码（0 到 9）。
- en: 'We will see what we have in our data by executing the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过执行以下代码查看我们数据中的内容：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have `55000` images in the training set, each image is of size `784`, and
    we have `10` labels, which are actually 0 to 9\. Similarly, we have `10000` images
    in the test set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练集中有 `55000` 张图像，每张图像的大小是 `784`，并且我们有 `10` 个标签，实际上是 0 到 9。同样，我们的测试集中有 `10000`
    张图像。
- en: 'Now, we''ll plot an input image to see what it looks like:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将绘制一个输入图像，以查看它的样子：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Thus, our input image looks like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的输入图像如下所示：
- en: '![](img/B15558_08_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_06.png)'
- en: 'Figure 8.6: Image of the digit 7 from the training set'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：训练集中数字 7 的图像
- en: Defining the number of neurons in each layer
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义每一层神经元的数量
- en: We'll build a four-layer neural network with three hidden layers and one output
    layer. As the size of the input image is `784`, we set `num_input` to `784`, and
    since we have 10 handwritten digits (0 to 9), we set `10` neurons in the output
    layer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个四层神经网络，其中包含三层隐藏层和一层输出层。由于输入图像的大小是 `784`，我们将 `num_input` 设置为 `784`，并且因为我们有
    10 个手写数字（0 到 9），我们将输出层设置为 10 个神经元。
- en: 'We define the number of neurons in each layer as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义每一层神经元的数量如下：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Defining placeholders
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义占位符
- en: 'As we have learned, we first need to define the placeholders for `input` and
    `output`. Values for the placeholders will be fed in at runtime through `feed_dict`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所学，我们首先需要为 `input` 和 `output` 定义占位符。占位符的值将在运行时通过 `feed_dict` 提供：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Since we have a four-layer network, we have four weights and four biases. We
    initialize our weights by drawing values from the truncated normal distribution
    with a standard deviation of `0.1`. Remember, the dimensions of the weight matrix
    should be *the* *number of neurons in the previous layer* x *the number of neurons
    in the current layer*. For instance, the dimensions of weight matrix `w3` should
    be *the* *number of neurons in hidden layer 2* x *the number of neurons in hidden
    layer 3*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个四层网络，我们有四个权重和四个偏置。我们通过从截断正态分布中抽取值来初始化权重，标准差为 `0.1`。记住，权重矩阵的维度应该是*前一层神经元的数量*
    x *当前层神经元的数量*。例如，权重矩阵 `w3` 的维度应该是*隐藏层 2 中神经元的数量* x *隐藏层 3 中神经元的数量*。
- en: 'We often define all of the weights in a dictionary, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会将所有权重定义在一个字典中，如下所示：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The shape of the bias should be the number of neurons in the current layer.
    For instance, the dimension of the `b2` bias is the number of neurons in hidden
    layer 2\. We set the bias value as a constant; `0.1` in all of the layers:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置的形状应该是当前层神经元的数量。例如，`b2` 偏置的维度是隐藏层 2 中神经元的数量。我们将偏置值设置为常量；在所有层中都设置为 `0.1`：
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Forward propagation
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'Now we''ll define the forward propagation operation. We''ll use ReLU activations
    in all layers. In the last layers, we''ll apply `sigmoid` activation, as shown
    in the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义前向传播操作。我们将在所有层中使用 ReLU 激活函数。在最后几层，我们将应用 `sigmoid` 激活函数，如下所示的代码所示：
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Computing loss and backpropagation
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算损失和反向传播
- en: 'Next, we''ll define our loss function. We''ll use softmax cross-entropy as
    our loss function. TensorFlow provides the `tf.nn.softmax_cross_entropy_with_logits()`
    function for computing softmax cross-entropy loss. It takes two parameters as
    inputs, `logits` and `labels`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的损失函数。我们将使用 softmax 交叉熵作为我们的损失函数。TensorFlow 提供了 `tf.nn.softmax_cross_entropy_with_logits()`
    函数来计算 softmax 交叉熵损失。它接受两个参数作为输入，`logits` 和 `labels`：
- en: The `logits` parameter specifies the `logits` predicted by our network; for
    example, `y_hat`
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` 参数指定我们网络预测的 `logits`；例如，`y_hat`'
- en: The `labels` parameter specifies the actual labels; for example, true labels,
    `Y`
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` 参数指定实际的标签；例如，真实标签，`Y`'
- en: 'We take the mean of the `loss` function using `tf.reduce_mean()`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.reduce_mean()` 计算 `loss` 函数的平均值：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we need to minimize the loss using backpropagation. Don''t worry! We don''t
    have to calculate the derivatives of all the weights manually. Instead, we can
    use TensorFlow''s `optimizer`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要使用反向传播来最小化损失。别担心！我们不需要手动计算所有权重的导数。相反，我们可以使用 TensorFlow 的 `optimizer`：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Computing accuracy
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算准确率
- en: 'We calculate the accuracy of our model as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下方式计算模型的准确率：
- en: The `y_hat` parameter denotes the predicted probability for each class of our
    model. Since we have `10` classes, we will have `10` probabilities. If the probability
    is high at position `7`, then it means that our network predicts the input image
    as digit `7` with high probability. The `tf.argmax()` function returns the index
    of the largest value. Thus, `tf.argmax(y_hat,1)` gives the index where the probability
    is high. Thus, if the probability is high at index `7`, then it returns `7`.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_hat`参数表示我们模型中每个类别的预测概率。由于我们有`10`个类别，因此我们会有`10`个概率。如果在位置`7`的概率较高，那么意味着我们的网络以较高的概率将输入图像预测为数字`7`。`tf.argmax()`函数返回最大值的索引。因此，`tf.argmax(y_hat,1)`给出概率较高的索引。如果在索引`7`的概率较高，它就返回`7`。'
- en: The `Y` parameter denotes the actual labels, and they are the one-hot encoded
    values. That is, the `Y` parameter consists of zeros everywhere except at the
    position of the actual image, where it has a value of `1`. For instance, if the
    input image is `7`, then `Y` has a value of 0 at all indices except at index `7`,
    where it has a value of `1`. Thus, `tf.argmax(Y,1)` returns `7` because that is
    where we have a high value, `1`.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Y`参数表示实际标签，它们是独热编码的值。也就是说，`Y`参数除了实际图像的位置以外，其他位置都是0，而在实际图像的位置上值为`1`。例如，如果输入图像是`7`，那么`Y`在所有索引位置的值都是0，只有在索引`7`的位置上值为`1`。因此，`tf.argmax(Y,1)`返回`7`，因为在这个位置上我们有一个高值`1`。'
- en: Thus, `tf.argmax(y_hat,1)` gives the predicted digit, and `tf.argmax(Y,1)` gives
    us the actual digit.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`tf.argmax(y_hat,1)`给出预测的数字，而`tf.argmax(Y,1)`给出我们实际的数字。
- en: The `tf.equal(x, y)` function takes `x` and `y` as inputs and returns the truth
    value of *(x == y)* element-wise. Thus, `correct_pred = tf.equal(predicted_digit,actual_digit)`
    consists of `True` where the actual and predicted digits are the same, and `False`
    where the actual and predicted digits are not the same. We convert the Boolean
    values in `correct_pred` into float values using TensorFlow's cast operation,
    `tf.cast(correct_pred, tf.float32)`. After converting them into float values,
    we take the average using `tf.reduce_mean()`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.equal(x, y)`函数将`x`和`y`作为输入，并返回*(x == y)*元素级别的真值。因此，`correct_pred = tf.equal(predicted_digit,
    actual_digit)`在实际数字与预测数字相同的位置为`True`，而在两者不相同的位置为`False`。我们使用TensorFlow的类型转换操作`tf.cast(correct_pred,
    tf.float32)`将`correct_pred`中的布尔值转换为浮动值。在将它们转换为浮动值后，我们使用`tf.reduce_mean()`求平均值。'
- en: 'Thus, `tf.reduce_mean(tf.cast(correct_pred, tf.float32))` gives us the average
    correct predictions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`tf.reduce_mean(tf.cast(correct_pred, tf.float32))`给出我们平均的正确预测：
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Creating a summary
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个汇总
- en: 'We can also visualize how the loss and accuracy of our model changes during
    several iterations in TensorBoard. So, we use `tf.summary()` to get the summary
    of the variable. Since the loss and accuracy are scalar variables, we use `tf.summary.scalar()`,
    as shown in the following code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在TensorBoard中可视化我们模型的损失和准确度在多次迭代过程中的变化。所以，我们使用`tf.summary()`来获取变量的汇总。由于损失和准确度是标量变量，我们使用`tf.summary.scalar()`，如下代码所示：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we merge all of the summaries we use in our graph, using `tf.summary.merge_all()`.
    We do this because when we have many summaries, running and storing them would
    become inefficient, so we run them once in our session instead of running them
    multiple times:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将所有在图中使用的汇总合并，使用`tf.summary.merge_all()`。我们这样做是因为当我们有许多汇总时，运行和存储它们会变得低效，因此我们在会话中只运行一次，而不是多次运行：
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Training the model
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now, it is time to train our model. As we have learned, first, we need to initialize
    all of the variables:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候训练我们的模型了。正如我们所学的，首先我们需要初始化所有变量：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the batch size, number of iterations, and learning rate, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 定义批处理大小、迭代次数和学习率，如下所示：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Start the TensorFlow session:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 启动TensorFlow会话：
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Initialize all the variables:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有变量：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Save the event files:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 保存事件文件：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Train the model for a number of iterations:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型若干次迭代：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Get a batch of data according to the batch size:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 根据批处理大小获取一批数据：
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Train the network:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Print `loss` and `accuracy` for every 100^(th) iteration:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 打印每100^(次)迭代的`loss`和`accuracy`：
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As you may notice from the following output, the loss decreases and the accuracy
    increases over various training iterations:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从下面的输出中可以注意到的那样，损失在不断减少，准确度在不断增加：
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Visualizing graphs in TensorBoard
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在TensorBoard中可视化图表
- en: 'After training, we can visualize our computational graph in TensorBoard, as
    shown in *Figure 8.7*. As you can see, our **Model** takes **input**, **weights**,
    and **biases** as input and returns the output. We compute **Loss** and **Accuracy**
    based on the output of the model. We minimize the loss by calculating **gradients**
    and updating **weights**:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以在 TensorBoard 中可视化我们的计算图，如*图 8.7*所示。如你所见，我们的**Model**接受**input**、**weights**和**biases**作为输入，并返回输出。我们根据模型的输出计算**Loss**和**Accuracy**。我们通过计算**gradients**并更新**weights**来最小化损失：
- en: '![](img/B15558_08_07.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_07.png)'
- en: 'Figure 8.7: Computational graph'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：计算图
- en: 'If we double-click and expand **Model**, we can see that we have three hidden
    layers and one output layer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们双击并展开**Model**，我们可以看到该模型有三层隐藏层和一层输出层：
- en: '![](img/B15558_08_08.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_08.png)'
- en: 'Figure 8.8: Expanding the Model node'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：扩展模型节点
- en: 'Similarly, we can double-click and see every node. For instance, if we open
    **weights**, we can see how the four weights are initialized using truncated normal
    distribution, and how it is updated using the Adam optimizer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以双击并查看每个节点。例如，如果我们打开**weights**，我们可以看到四个权重是如何通过截断正态分布初始化的，以及它是如何通过 Adam
    优化器更新的：
- en: '![](img/B15558_08_09.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_09.png)'
- en: 'Figure 8.9: Expanding the weights node'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：扩展权重节点
- en: As we have learned, the computational graph helps us to understand what is happening
    on each node.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学，计算图帮助我们理解每个节点发生了什么。
- en: 'We can see how the accuracy is being calculated by double-clicking on the **Accuracy**
    node:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过双击**Accuracy**节点查看准确度是如何计算的：
- en: '![](img/B15558_08_10.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_10.png)'
- en: 'Figure 8.10: Expanding the Accuracy node'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：扩展准确度节点
- en: 'Remember that we also stored a summary of our `loss` and `accuracy` variables.
    We can find them under the **SCALARS** tab in TensorBoard. *Figure 8.11* shows
    how the loss decreases over iterations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们还存储了`loss`和`accuracy`变量的摘要。我们可以在 TensorBoard 的**SCALARS**标签下找到它们。*图 8.11*展示了损失如何随着迭代逐步减少：
- en: '![](img/B15558_08_11.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_11.png)'
- en: 'Figure 8.11: Plot of the loss function'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11：损失函数图
- en: '*Figure 8.12* shows how accuracy increases over iterations:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.12*展示了准确度随着迭代次数的增加而提升：'
- en: '![](img/B15558_08_12.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_12.png)'
- en: Figure 8.12 Plot of accuracy
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 准确度图
- en: That's it. In the next section, we will learn about another interesting feature
    in TensorFlow called eager execution.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。在下一节中，我们将学习 TensorFlow 中另一个有趣的特性——急切执行模式。
- en: Introducing eager execution
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入急切执行模式
- en: 'Eager execution in TensorFlow is more Pythonic and allows rapid prototyping.
    Unlike graph mode, where we need to construct a graph every time we want to perform
    any operation, eager execution follows the imperative programming paradigm, where
    any operation can be performed immediately, without having to create a graph,
    just like we do in Python. Hence, with eager execution, we can say goodbye to
    sessions and placeholders. It also makes the debugging process easier with an
    immediate runtime error, unlike graph mode. For instance, in graph mode, to compute
    anything, we run the session. As shown in the following code, to evaluate the
    value of `z`, we have to run the TensorFlow session:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的急切执行模式更加符合 Python 编程风格，允许快速原型开发。与图模式不同，在图模式下，我们每次执行操作时都需要构建一个图，而急切执行模式遵循命令式编程范式，任何操作都可以立即执行，无需构建图，就像在
    Python 中一样。因此，使用急切执行模式，我们可以告别会话和占位符。它还使得调试过程更加简便，因为会立即出现运行时错误，而不是像图模式那样需要先运行会话。例如，在图模式下，为了计算任何内容，我们必须运行会话。如下面的代码所示，要计算`z`的值，我们必须运行
    TensorFlow 会话：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'With eager execution, we don''t need to create a session; we can simply compute
    `z`, just like we do in Python. In order to enable eager execution, just call
    the `tf.enable_eager_execution()` function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用急切执行模式时，我们不需要创建会话；我们可以像在 Python 中一样直接计算`z`。为了启用急切执行模式，只需调用`tf.enable_eager_execution()`函数：
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It will return the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回以下内容：
- en: '[PRE50]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In order to get the output value, we can print the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取输出值，我们可以打印以下内容：
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Although eager execution enables the imperative programming paradigm, in this
    book, we will investigate most of the examples in non-eager mode to better understand
    the algorithms from scratch. In the next section, we will see how to perform math
    operations using TensorFlow.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管急切执行模式实现了命令式编程范式，但在本书中，我们将以非急切模式来研究大多数示例，以便更好地理解从零开始的算法。在下一节中，我们将学习如何使用 TensorFlow
    进行数学运算。
- en: Math operations in TensorFlow
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 中的数学运算
- en: 'Now, we will explore some of the operations in TensorFlow using eager execution
    mode:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用急切执行模式探索 TensorFlow 中的一些操作：
- en: '[PRE52]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Let's start with some basic arithmetic operations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些基本的算术运算开始。
- en: 'Use `tf.add` to add two numbers:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.add` 将两个数字相加：
- en: '[PRE53]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `tf.subtract` function is used for finding the difference between two numbers:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.subtract` 函数用于计算两个数字之间的差值：'
- en: '[PRE54]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `tf.multiply` function is used for multiplying two numbers:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.multiply` 函数用于乘法运算两个数：'
- en: '[PRE55]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Divide two numbers using `tf.divide`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.divide` 除以两个数字：
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The dot product can be computed as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 点积可以按以下方式计算：
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, let''s find the index of the minimum and maximum elements:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来找出最小值和最大值的索引：
- en: '[PRE58]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The index of the minimum value is computed using `tf.argmin()`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值的索引使用 `tf.argmin()` 计算：
- en: '[PRE59]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The index of the maximum value is computed using `tf.argmax()`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值的索引使用 `tf.argmax()` 计算：
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Run the following code to find the squared difference between `x` and `y`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码，找到 `x` 和 `y` 之间的平方差：
- en: '[PRE61]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Let's try typecasting; that is, converting from one data type into another.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试类型转换；即，将一种数据类型转换为另一种。
- en: 'Print the type of `x`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 打印 `x` 的类型：
- en: '[PRE62]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can convert the type of `x`, which is `tf.int32`, into `tf.float32`, using
    `tf.cast`, as shown in the following code:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `tf.cast` 将 `x` 的类型从 `tf.int32` 转换为 `tf.float32`，如以下代码所示：
- en: '[PRE63]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, check the `x` type. It will be `tf.float32`, as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查 `x` 的类型。它将是 `tf.float32`，如下所示：
- en: '[PRE64]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Concatenate the two matrices:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个矩阵连接起来：
- en: '[PRE65]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Concatenate the matrices row-wise:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 按行连接矩阵：
- en: '[PRE66]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Use the following code to concatenate the matrices column-wise:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码按列连接矩阵：
- en: '[PRE67]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Stack the `x` matrix using the `stack` function:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `stack` 函数堆叠 `x` 矩阵：
- en: '[PRE68]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, let''s see how to perform the `reduce_mean` operation:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何执行 `reduce_mean` 操作：
- en: '[PRE69]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Compute the mean value of `x`; that is, (*1.0* + *5.0* + *2.0* + *3.0*) / *4*:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 `x` 的平均值；即，(*1.0* + *5.0* + *2.0* + *3.0*) / *4*：
- en: '[PRE70]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Compute the mean across the row; that is, (*1.0*+*5.0*)/*2,* (*2.0*+*3.0*)/*2*:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 计算行的平均值；即，(*1.0*+*5.0*)/*2,* (*2.0*+*3.0*)/*2*：
- en: '[PRE71]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Compute the mean across the column; that is, (*1.0*+*5.0*)/*2.0,* (*2.0*+*3.0*)/*2.0*:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 计算列的平均值；即，(*1.0*+*5.0*)/*2.0,* (*2.0*+*3.0*)/*2.0*：
- en: '[PRE72]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Draw random values from the probability distributions:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率分布中绘制随机值：
- en: '[PRE73]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Compute the softmax probabilities:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 softmax 概率：
- en: '[PRE74]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now, we'll look at how to compute the gradients.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看看如何计算梯度。
- en: 'Define the `square` function:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 `square` 函数：
- en: '[PRE75]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The gradients can be computed for the preceding `square` function using `tf.GradientTape`,
    as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.GradientTape` 可以计算前面 `square` 函数的梯度，如下所示：
- en: '[PRE76]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: TensorFlow 2.0 and Keras
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 和 Keras
- en: TensorFlow 2.0 has got some really cool features. It sets the eager execution
    mode by default. It provides a simplified workflow and uses Keras as the main
    API for building deep learning models. It is also backward compatible with TensorFlow
    1.x versions.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 具有一些非常酷的特性。默认启用了急切执行模式。它提供了简化的工作流程，并使用 Keras 作为构建深度学习模型的主要 API。它也向后兼容
    TensorFlow 1.x 版本。
- en: 'To install TensorFlow 2.0, open your Terminal and type the following command:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 TensorFlow 2.0，请打开终端并输入以下命令：
- en: '[PRE77]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Since TensorFlow 2.0 uses Keras as a high-level API, we will look at how Keras
    works in the next section.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TensorFlow 2.0 使用 Keras 作为高层 API，我们将在下一节中讨论 Keras 的工作原理。
- en: Bonjour Keras
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bonjour Keras
- en: Keras is another popularly used deep learning library. It was developed by François
    Chollet at Google. It is well known for its fast prototyping, and it makes model
    building simple. It is a high-level library, meaning that it does not perform
    any low-level operations on its own, such as convolution. It uses a backend engine
    for doing that, such as TensorFlow. The Keras API is available in `tf.keras`,
    and TensorFlow 2.0 uses it as the primary API.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是另一个广泛使用的深度学习库。它由 Google 的 François Chollet 开发。以其快速原型设计著称，它使得模型构建变得简单。它是一个高级库，这意味着它不会自己执行任何低级操作，比如卷积。它使用一个后端引擎来完成这些操作，如
    TensorFlow。Keras API 可在 `tf.keras` 中找到，TensorFlow 2.0 使用它作为主要 API。
- en: 'Building a model in Keras involves four important steps:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中构建模型涉及四个重要步骤：
- en: Defining the model
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型
- en: Compiling the model
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型
- en: Fitting the model
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Evaluating the model
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Defining the model
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'The first step is defining the model. Keras provides two different APIs to
    define the model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义模型。Keras 提供了两种不同的 API 来定义模型：
- en: The sequential API
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序 API
- en: The functional API
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数式 API
- en: Defining a sequential model
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义顺序模型
- en: 'In a sequential model, we stack each layer, one above another:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个顺序模型中，我们将每一层堆叠在一起：
- en: '[PRE78]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'First, let''s define our model as a `Sequential()` model, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将模型定义为`Sequential()`模型，如下所示：
- en: '[PRE79]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, define the first layer, as shown in the following code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，定义第一个层，如下所示：
- en: '[PRE80]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: In the preceding code, `Dense` implies a fully connected layer, `input_dim`
    implies the dimension of our input, and `activation` specifies the activation
    function that we use. We can stack up as many layers as we want, one above another.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`Dense`表示一个全连接层，`input_dim`表示输入的维度，`activation`指定了我们使用的激活函数。我们可以堆叠任意数量的层，一个接一个。
- en: 'Define the next layer with the `relu` activation function, as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 定义下一个使用`relu`激活函数的层，如下所示：
- en: '[PRE81]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Define the output layer with the `sigmoid` activation function:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用`sigmoid`激活函数的输出层：
- en: '[PRE82]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The final code block of the sequential model is shown as follows. As you can
    see, the Keras code is much simpler than the TensorFlow code:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模型的最终代码块如下所示。正如你所看到的，Keras代码比TensorFlow代码要简单得多：
- en: '[PRE83]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Defining a functional model
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义一个功能模型
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack, one above another. A functional
    model comes in handy when creating complex models, such as directed acyclic graphs,
    models with multiple input values, multiple output values, and shared layers.
    Now, we will see how to define a functional model in Keras.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 功能模型比顺序模型提供了更多的灵活性。例如，在功能模型中，我们可以轻松地将任何一层连接到另一层，而在顺序模型中，每一层都是按顺序堆叠的。功能模型在创建复杂模型时非常有用，例如有向无环图、具有多个输入值、多个输出值和共享层的模型。现在，我们将看到如何在Keras中定义功能模型。
- en: 'The first step is to define the input dimensions:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义输入维度：
- en: '[PRE84]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, we''ll define our first fully connected layer with `10` neurons and `relu`
    activation, using the `Dense` class, as shown:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`Dense`类定义第一个具有`10`个神经元并使用`relu`激活的全连接层，如下所示：
- en: '[PRE85]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We defined `layer1`, but where is the input to `layer1` coming from? We need
    to specify the input to `layer1` in a bracket notation at the end, as shown:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了`layer1`，但是`layer1`的输入来自哪里？我们需要在末尾使用括号表示法指定`layer1`的输入，如下所示：
- en: '[PRE86]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We define the next layer, `layer2`, with `13` neurons and `relu` activation.
    The input to `layer2` comes from `layer1`, so that is added in the bracket at
    the end, as shown in the following code:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义下一个层`layer2`，具有`13`个神经元和`relu`激活。`layer2`的输入来自`layer1`，因此在末尾的括号中添加，如以下代码所示：
- en: '[PRE87]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, we can define the output layer with the `sigmoid` activation function.
    Input to the output layer comes from `layer2`, so that is added in bracket at
    the end:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`sigmoid`激活函数定义输出层。输出层的输入来自`layer2`，因此这将在末尾的括号中添加：
- en: '[PRE88]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'After defining all of the layers, we define the model using a `Model` class,
    where we need to specify `inputs` and `outputs`, as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完所有层后，我们使用`Model`类定义模型，在这里我们需要指定`inputs`和`outputs`，如以下代码所示：
- en: '[PRE89]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The complete code for the functional model is shown here:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的功能模型代码如下所示：
- en: '[PRE90]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Compiling the model
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'Now that we have defined the model, the next step is to compile it. In this
    phase, we set up how the model should learn. We define three parameters when compiling
    the model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，下一步是编译它。在这个阶段，我们设置模型的学习方式。编译模型时，我们需要定义三个参数：
- en: 'The `optimizer` parameter: This defines the optimization algorithm we want
    to use; for example, the gradient descent.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`参数：这定义了我们想要使用的优化算法；例如，梯度下降法。'
- en: 'The `loss` parameter: This is the objective function that we are trying to
    minimize; for example, the mean squared error or cross-entropy loss.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`参数：这是我们试图最小化的目标函数；例如，均方误差或交叉熵损失。'
- en: 'The `metrics` parameter: This is the metric through which we want to assess
    the model''s performance; for example, `accuracy`. We can also specify more than
    one metric.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics`参数：这是我们用来评估模型性能的度量指标；例如，`accuracy`。我们也可以指定多个度量指标。'
- en: 'Run the following code to compile the model:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码来编译模型：
- en: '[PRE91]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Training the model
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We defined and also compiled the model. Now, we will train the model. Training
    the model can be done using the `fit` function. We specify our features, `x`;
    labels, `y`; the number of `epochs` we want to train the model for; and `batch_size`,
    as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义并编译了模型。现在，我们将训练模型。训练模型可以通过 `fit` 函数完成。我们需要指定特征 `x`、标签 `y`、训练轮数 `epochs`
    和 `batch_size`，如下所示：
- en: '[PRE92]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Evaluating the model
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'After training the model, we will evaluate the model on the test set:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后，我们将在测试集上评估模型：
- en: '[PRE93]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We can also evaluate the model on the same train set, and that will help us
    to understand the training accuracy:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在同一个训练集上评估模型，这将帮助我们理解训练的准确性：
- en: '[PRE94]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: That's it. Let's see how to use TensorFlow for the MNIST digit classification
    task in the next section.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。接下来我们将在下一节中学习如何使用 TensorFlow 进行 MNIST 数字分类任务。
- en: MNIST digit classification using TensorFlow 2.0
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.0 进行 MNIST 数字分类
- en: Now, we will see how we can perform MNIST handwritten digit classification,
    using TensorFlow 2.0\. It requires only a few lines of code compared to TensorFlow
    1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just
    need to add `tf.keras` to the Keras code.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何使用 TensorFlow 2.0 进行 MNIST 手写数字分类。与 TensorFlow 1.x 相比，它只需要几行代码。正如我们所学，TensorFlow
    2.0 使用 Keras 作为其高级 API，我们只需要在 Keras 代码中添加 `tf.keras`。
- en: 'Let''s start by loading the dataset:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始：
- en: '[PRE95]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Create a training set and a test set with the following code:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码创建训练集和测试集：
- en: '[PRE96]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Normalize the train and test sets by dividing the values of `x` by the maximum
    value of `x`; that is, `255.0`:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `x` 的值除以 `x` 的最大值，即 `255.0`，来归一化训练集和测试集：
- en: '[PRE97]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Define the sequential model as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下定义顺序模型：
- en: '[PRE98]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Now, let''s add layers to the model. We use a three-layer network with the
    ReLU function in the hidden layer and softmax in the final layer:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们给模型添加层。我们使用一个三层的网络，隐藏层使用 ReLU 函数，最后一层使用 softmax：
- en: '[PRE99]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Compile the model by running the following line of code:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下代码行来编译模型：
- en: '[PRE100]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Train the model:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE101]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Evaluate the model:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型：
- en: '[PRE102]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: That's it! Writing code with the Keras API is that simple.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！使用 Keras API 编写代码就是这么简单。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off this chapter by understanding TensorFlow and how it uses computational
    graphs. We learned that computation in TensorFlow is represented by a computational
    graph, which consists of several nodes and edges, where nodes are mathematical
    operations, such as addition and multiplication, and edges are tensors.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过了解 TensorFlow 以及它如何使用计算图开始了本章的学习。我们了解到，TensorFlow 中的计算通过计算图表示，计算图由多个节点和边组成，其中节点是数学运算，例如加法和乘法，边则是张量。
- en: Next, we learned that variables are containers used to store values, and they
    are used as input to several other operations in a computational graph. We also
    learned that placeholders are like variables, where we only define the type and
    dimension but do not assign the values, and the values for the placeholders are
    fed at runtime. Moving forward, we learned about TensorBoard, which is TensorFlow's
    visualization tool and can be used to visualize a computational graph. We also
    explored eager execution, which is more Pythonic and allows rapid prototyping.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了变量是用于存储值的容器，它们作为输入用于计算图中的多个其他操作。我们还了解到占位符类似于变量，它们仅定义类型和维度，但不赋值，值将在运行时提供。接下来，我们了解了
    TensorBoard，这是 TensorFlow 的可视化工具，可以用来可视化计算图。我们还探索了急切执行，它更加符合 Python 风格，并支持快速原型开发。
- en: We understood that, unlike graph mode, where we need to construct a graph every
    time to perform any operation, eager execution follows the imperative programming
    paradigm, where any operation can be performed immediately, without having to
    create a graph, just like we do in Python.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解到，与图模式不同，图模式下每次执行操作时都需要构建图，而急切执行（eager execution）遵循命令式编程范式，任何操作都可以立即执行，无需构建图，就像我们在
    Python 中做的那样。
- en: In the next chapter, we begin our **deep reinforcement learning** (**DRL**)
    journey by understanding one of the popular DRL algorithms, called the **Deep
    Q Network** (**DQN**).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过理解一种流行的深度强化学习算法——**深度 Q 网络**（**DQN**），开始我们的**深度强化学习**（**DRL**）之旅。
- en: Questions
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s put our knowledge of TensorFlow to the test by answering the following
    questions:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来检验我们对 TensorFlow 的理解：
- en: What is a TensorFlow session?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow 会话？
- en: Define a placeholder.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个占位符。
- en: What is TensorBoard?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 TensorBoard？
- en: Why is eager execution mode useful?
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么急切执行模式有用？
- en: What are all the steps involved in building a model using Keras?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras 构建模型的所有步骤有哪些？
- en: How does Keras's functional model differ from its sequential model?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras 的函数式模型与其顺序模型有何不同？
- en: Further reading
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: You can learn more about TensorFlow by checking its official documentation at
    [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看其官方文档来了解更多关于 TensorFlow 的信息，链接：[https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)。
