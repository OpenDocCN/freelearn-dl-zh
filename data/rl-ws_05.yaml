- en: 5\. Dynamic Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 动态规划
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you will be introduced to the driving principles of dynamic
    programming. You will be introduced to the classic coin-change problem as an application
    of dynamic programming. Furthermore, you will learn how to implement policy evaluation,
    policy iteration, and value iteration and learn the differences between them.
    By the end of the chapter, you will be able to implement dynamic programming to
    solve problems in **Reinforcement Learning** (**RL**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解动态规划的驱动原理。您将了解经典的零钱兑换问题，并将其作为动态规划的应用。此外，您还将学习如何实现策略评估、策略迭代和价值迭代，并了解它们之间的差异。到本章结束时，您将能够使用**强化学习**（**RL**）中的动态规划来解决问题。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, we were introduced to the OpenAI Gym environment and
    also learned how to implement custom environments, depending on the application.
    You also learned the basics of TensorFlow 2, how to implement a policy using the
    TensorFlow 2 framework, and how to visualize learning using TensorBoard. In this
    chapter, we will see how **Dynamic Programming** (**DP**) works in general, from
    a computer science perspective. Then, we'll go over how and why it is used in
    RL. Next, we will dive deep into classic DP algorithms such as policy evaluation,
    policy iteration, and value iteration and compare them. Lastly, we will implement
    the algorithms in the classic coin-change problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了OpenAI Gym环境，并学习了如何根据应用需求实现自定义环境。您还了解了TensorFlow 2的基础知识，如何使用TensorFlow
    2框架实现策略，以及如何使用TensorBoard可视化学习成果。在本章中，我们将从计算机科学的角度，了解**动态规划**（**DP**）的一般工作原理。接着，我们将讨论它在强化学习中的使用方式及其原因。然后，我们将深入探讨经典的动态规划算法，如策略评估、策略迭代和价值迭代，并进行比较。最后，我们将实现经典零钱兑换问题中的算法。
- en: 'DP is one of the most fundamental and foundational topics in computer science.
    Furthermore, RL algorithms such as **Value Iteration**, **Policy Iteration**,
    and others, as we will see, use the same basic principle: avoid repeated computations
    to save time, which is what DP is all about. The philosophy of DP is not new;
    it is self-evident and commonplace once you learn the ways to solve it. The hard
    part is identifying whether a problem can be solved using DP.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划是计算机科学中最基本和最基础的主题之一。此外，强化学习算法，如**价值迭代**、**策略迭代**等，正如我们将看到的，使用相同的基本原理：避免重复计算以节省时间，这正是动态规划的核心。动态规划的哲学并不新鲜；一旦学会了解决方法，它是显而易见且普遍的。真正困难的部分是识别一个问题是否可以用动态规划来解决。
- en: 'The basic principle can be explained to a child as well. Imagine counting the
    number of candies in a box. If you know there are 100 candies in a box, and the
    shopkeeper offers 5 extra candies, you don''t start counting the candies all over
    again. You use the prior information to add 5 to the original count and say, "I
    have 105 candies." That''s the core of DP: saving intermediate information and
    reusing it, if required, to avoid re-computation. While it sounds simple, as mentioned
    before, the hard part is identifying whether a problem can be solved using DP.
    As we will see later, in the *Identifying Dynamic Programming Problems* section,
    a problem must satisfy a specific prerequisite, such as optimal substructure and
    overlapping subproblems, to be solved using DP, which we will study in the *Identifying
    Dynamic Programming Problems* section. Once a problem qualifies, there are some
    well-known techniques such as top-down memoization, that is, saving intermediate
    states in an unordered fashion, and bottom-up tabulation, which is saving the
    states in an ordered array or matrix.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本原理也可以用简单的方式向孩子解释。想象一下在一个盒子里数糖果的数量。如果你知道盒子里有100颗糖果，而店主又给了你5颗额外的糖果，你就不会重新开始数糖果。你会利用已有的信息，将5颗糖果加到原来的数量上，并说：“我有105颗糖果。”这就是动态规划的核心：保存中间信息并在需要时重新利用，以避免重复计算。虽然听起来简单，但如前所述，真正困难的部分是确定一个问题是否可以用动态规划来解决。正如我们稍后在*识别动态规划问题*一节中所看到的，问题必须满足特定的前提条件，如最优子结构和重叠子问题，才能用动态规划解决，我们将在*识别动态规划问题*一节中详细研究。一旦一个问题符合要求，就有一些著名的技术，比如自顶向下的备忘录法，即以无序的方式保存中间状态，以及自底向上的表格法，即将状态保存在有序的数组或矩阵中。
- en: Combining these techniques can achieve a considerable performance boost over
    solving them using brute force. Furthermore, the difference in time increases
    with an increase in the number of operations. Mathematically speaking, solutions
    solved using DP usually run in O(n2), while those using brute force execute in
    O(2n) time, where the notation "O" (Big-O) can be loosely thought of as the number
    of operations performed. So, for instance, if N=500, which is a reasonably small
    number, a DP algorithm will roughly execute 5002 steps, compared to a brute force
    algorithm, which will use 2500 steps. For reference, there are 280 hydrogen atoms
    in the sun, which is undoubtedly a much smaller number than 2500.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些技巧可以显著提升性能，相比使用暴力算法进行求解。另外，随着操作次数的增加，时间差异也会变得更加明显。从数学角度来说，使用动态规划求解的方案通常在O(n²)时间内运行，而暴力算法则需要O(2ⁿ)时间，其中"O"（大O符号）可以粗略理解为执行的操作次数。所以，举个例子，如果N=500，这是一个相对较小的数字，动态规划算法大约需要执行500²次操作，而暴力算法则需要执行2500次操作。作为参考，太阳中有280个氢原子，这个数字无疑要比2500小得多。
- en: 'The following figure depicts the difference in the number of operations executed
    for both algorithms:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了两种算法执行操作次数的差异：
- en: '![Figure 5.1: Visualizing Big-O values'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：可视化大O值'
- en: '](img/B16182_05_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_01.jpg)'
- en: 'Figure 5.1: Visualizing Big-O values'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：可视化大O值
- en: Let's now move toward studying the approach of solving DP problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始研究求解动态规划问题的方法。
- en: Solving Dynamic Programming Problems
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求解动态规划问题
- en: 'There are two popular ways to solve DP problems: the tabular method and memoization.
    In the tabular method, we build a matrix that stores the intermediate values one
    by one in the lookup table. On the other hand, in the memoization method, we store
    the same values in an unstructured way. Here, unstructured way refers to the fact
    that the lookup table may be filled all at once.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 解决动态规划问题的两种常见方法是：表格法和备忘录法。在表格法中，我们构建一个矩阵，在查找表中逐一存储中间值。另一方面，在备忘录法中，我们以非结构化的方式存储相同的值。这里所说的非结构化方式是指查找表可能一次性填满所有内容。
- en: 'Imagine you''re a baker and are selling cakes to shops. Your job is to sell
    cakes and make the maximum profit out of it. For simplicity, we will assume that
    all other costs are fixed, and the highest price offered for your product is the
    only indicator of profits earned, which is a fair assumption for most business
    cases. So, naturally, you''d wish to sell all your cakes to the shop offering
    the highest price, but there''s a decision to make as there are multiple shops
    that offer different prices on different sizes of cakes. So, you have two choices:
    how much to sell, and which shop to trade with. For this example, we''ll forget
    other variables and assume there are no additional hidden costs. We''ll tackle
    the problem using the tabular method, as well as memoization.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你是一个面包师，正在向商店出售蛋糕。你的工作是出售蛋糕并获得最大利润。为了简化问题，我们假设所有其他成本都是固定的，而你产品的最高价格就是利润的唯一指标，这在大多数商业案例中是合理的假设。所以，自然地，你会希望把所有蛋糕卖给提供最高价格的商店，但你需要做出决定，因为有多个商店提供不同价格和不同大小的蛋糕。因此，你有两个选择：卖多少蛋糕，和选择哪家商店进行交易。为了这个例子，我们将忽略其他变量，假设没有额外的隐藏成本。我们将使用表格法和备忘录法来解决这个问题。
- en: Phrasing the problem formally, you have a cake with weight W, and an array of
    prices that different shops are willing to offer, and you have to find out the
    optimal configuration that yields the highest price (and by the assumptions stated
    previously, highest profit).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正式描述问题时，你有一个重量为W的蛋糕，以及一个各个商店愿意提供的价格数组，你需要找出能够获得最高价格（根据之前的假设，也就是最高利润）的最优配置。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the code examples, which will be listed further in this section, we have
    used profit and price interchangeably. So, for example, if you encounter a variable
    such as `best_profit`, it would also be an indicator of best price and vice-versa.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来本节列出的代码示例中，我们将利润和价格互换使用。所以，例如，如果你遇到一个变量，如`best_profit`，它也可以表示最佳价格，反之亦然。
- en: 'For instance, say W = 5, meaning we have a cake that weighs 5 kilograms and
    the prices, indicated in the following table, are what are offered by restaurants:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，假设W = 5，意味着我们有一个重5千克的蛋糕，以下表格中列出的价格是餐馆所提供的价格：
- en: '![Figure 5.2: Different prices offered for different weights of cakes'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：不同重量蛋糕的不同价格'
- en: '](img/B16182_05_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_02.jpg)'
- en: 'Figure 5.2: Different prices offered for different weights of cakes'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：不同重量蛋糕的不同价格
- en: 'Now consider restaurant A pays $10 for a 1 kg cake, but $40 for a 2 kg cake.
    So, the question is: should I sell a 5 kg cake and partition it into 5 x 1 kg
    slices, which will yield $45, or should I sell the 5 kg cake as a whole to restaurant
    B, which is offering $80\. In this case, the most optimal configuration is to
    partition the cake into a 3 kg part that yields $50 and a 2 kg part that generates
    $40, which yields a total of $90\. The following table indicates various ways
    of partitioning and the corresponding price that we''ll get:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑餐厅A支付10美元购买1千克蛋糕，但支付40美元购买2千克蛋糕。那么问题是：我应该将5千克的蛋糕分割成5个1千克的切片出售，总价为45美元，还是应该将整个5千克的蛋糕作为一个整体卖给餐厅B，后者提供80美元？在这种情况下，最优的配置是将蛋糕分割成3千克的部分，售价50美元，和2千克的部分，售价40美元，总计90美元。以下表格显示了各种分割方式及其对应的价格：
- en: '![Figure 5.3: Different combinations for cake partitioning'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：蛋糕分割的不同组合'
- en: '](img/B16182_05_03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_03.jpg)'
- en: 'Figure 5.3: Different combinations for cake partitioning'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：蛋糕分割的不同组合
- en: Now, from the preceding table, it is quite evident that the best price is provided
    by the combination of 2 kg + 3 kg. But to really understand the limitation of
    the brute force approach, we'll assume that we don't know the best combination
    for yielding the maximum price. We'll try to implement the brute force approach
    in code. In reality, the number of observations for an actual business problem
    may be too large for you to arrive at an answer as quickly as you may have done
    here. The preceding table is just an example to help you understand the limitations
    of the brute force approach.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的表格来看，显然最佳的价格由2千克+3千克的组合提供。但为了真正理解暴力破解法的局限性，我们假设我们不知道哪个组合能够获得最大价格。我们将尝试用代码实现暴力破解法。实际上，对于一个实际的商业问题，观察的数据量可能过大，以至于你无法像在这里一样快速得到答案。前面的表格只是一个例子，帮助你理解暴力破解法的局限性。
- en: 'So, let''s try to solve this problem using brute force. We can rephrase the
    question slightly differently: at every junction, we have a choice – partition
    or not. If we choose to partition the cake into two unequal parts first, the left
    side, for instance, becomes one part of the cake, and the right side can be treated
    as an independent partition. In the next iteration, we''ll only concentrate on
    the right side / the other part. Now, again, we can partition it, and the right
    side becomes a part of the cake that is divided further. This paradigm is also
    called **recursion**.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们尝试使用暴力破解法解决这个问题。我们可以稍微重新表述这个问题：在每个决策点，我们有一个选择——分割或不分割。如果我们首先选择将蛋糕分割成两部分不等的部分，左侧部分可以视为蛋糕的一部分，右侧部分则视为独立分割。在下一次迭代中，我们只集中于右侧部分/其他部分。然后，我们再次可以对右侧部分进行分割，右侧部分成为进一步分割的蛋糕部分。这种模式也称为**递归**。
- en: '![Figure 5.4: Cake partitioned into several pieces'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：蛋糕分割成几块'
- en: '](img/B16182_05_04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_04.jpg)'
- en: 'Figure 5.4: Cake partitioned into several pieces'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：蛋糕分割成几块
- en: 'In the preceding figure, we can see a cake being partitioned into multiple
    pieces. For a cake that weighs 5 kg (and assuming you can partition the cake in
    a manner that the minimum weight of each partition is 1 kg, and thus the partitions
    can only be integral multiples of 1), we are presented with "partition or not"
    a total of 32 times; here''s how:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到蛋糕被分割成多个部分。对于一块5千克重的蛋糕（假设你可以按照每个分割部分至少1千克的重量进行分割，因此每个分割部分的重量只能是1的整数倍），我们会看到"分割或不分割"一共出现了32次；如下所示：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'So, for starters, let''s do this: for each of the 32 possible combinations,
    calculate the total price, and in the end, report the combination with the highest
    amount of price. We''ve defined the price in a list, where the index tells us
    the weight of a slice of cake:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先让我们这样做：对于每一种32种可能的组合，计算总价，最后报告价格最高的组合。我们已经定义了价格列表，其中索引表示切片的重量：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For instance, selling a whole 1 kg cake yields a price of $9; whereas selling
    a 2 kg cake/slice yields a price of $40\. The price on the zeroth index is NA
    because we won''t ever have a cake that weighs 0 kg. Here is pseudo-code formulated
    to implement the preceding scenario:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，出售一整个1千克的蛋糕，售价为9美元；而出售一个2千克的蛋糕/切片，售价为40美元。零索引处的价格为NA，因为我们不可能有0千克重的蛋糕。以下是实现上述情境的伪代码：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding function partition, `cake_size`, will take an integer input:
    the size of the cake. Then, in the `for` loop, we will cut the cake in every possible
    way and calculate the best profit. Given that we are taking a partition/no partition
    decision for every single place, the code runs in O(2n) time. Now let''s call
    the function using the following code. The `if __name__` block will make sure
    that your code runs only when you run the script (and not when you import it):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的`partition`函数，`cake_size`将接受一个整数输入：蛋糕的大小。然后，在`for`循环中，我们会以每种可能的方式切割蛋糕并计算最佳利润。由于我们对每个位置都做出分割/不分割的决策，代码运行的时间复杂度是O(2n)。现在让我们使用以下代码来调用该函数。`if
    __name__`块将确保代码仅在运行脚本时执行（而不是在导入时）：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Upon running it, we can see the best possible profit for a cake of size `5`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，我们可以看到大小为`5`的蛋糕的最佳利润：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding method solves the problem of calculating maximum profit, but
    it has a huge flaw: it is very slow. We are performing unnecessary computations,
    and exploring the entire search tree (all possible combinations). Why is this
    a bad idea? Well, imagine you''re traveling from point A to point C, and it costs
    $10\. Would you ever consider traveling from A to B to D to F and then to C, which
    might cost, say, $150? Of course not, right? The idea is similar: if I know the
    current path is not the most optimal one, why bother exploring that way?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法解决了计算最大利润的问题，但它有一个巨大的缺陷：非常慢。我们正在进行不必要的计算，并且要遍历整个搜索树（所有可能的组合）。为什么这是个坏主意？想象一下你从A点旅行到C点，费用是$10。你会考虑从A到B，再到D，再到F，最后到C，可能需要花费$150吗？当然不会，对吧？这个思路是类似的：如果我知道当前的路径不是最优路径，为什么还要去探索那条路？
- en: 'To solve this problem more efficiently, we will look at two great techniques:
    the tabular method and memoization. Both are based on the same principle: avoid
    unproductive exploration. But each uses a slightly fundamentally different approach
    to solving the problem, as you will see.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更高效地解决这个问题，我们将研究两种优秀的技术：表格法和备忘录法。它们的原理相同：避免无效的探索。但它们使用略有不同的方式来解决问题，正如你将看到的那样。
- en: Let's explore memoization in the following section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将深入研究备忘录法。
- en: Memoization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 备忘录法
- en: The `memoization` method refers to a method in which we save the results of
    the intermediate outputs for further use in a dictionary, also known as memo.
    Hence the name "memoization."
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`备忘录法`是指一种方法，在这种方法中，我们将中间输出的结果保存在一个字典中，供以后使用，也就是所谓的备忘录。因此得名“备忘录法”。'
- en: 'Coming back to our cake partition example, if we modify the `partition` function
    and print the value of `cake_size` and the best solution for the size, there''s
    a new pattern to be found. Using the same code as was used in the brute force
    approach before, we add a `print` statement to display the cake size and the corresponding
    profit:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的蛋糕分割示例，如果我们修改`partition`函数，并打印`cake_size`的值以及该大小的最佳解决方案，就会发现一个新的模式。使用之前暴力方法中相同的代码，我们添加一个`print`语句来显示蛋糕大小及对应的利润：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Call the function using the `main` block:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`main`块调用函数：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then see the output as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们会看到如下输出：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see in the preceding output, there is a pattern here – the best profit
    for a given size remains the same, but we calculate it many times. Especially
    pay attention to the size and the order of the calculations. It calculates the
    profit for size 1, and then 2, and now when it wants to calculate it for size
    3, it does so by starting from scratch again by calculating the answer for 1,
    and then 2, and then finally 3\. This happens repeatedly since it doesn't store
    any intermediate results. An obvious improvement would be to store the profit
    in a memo and then use it later.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的输出所示，这里有一个模式——对于给定大小的最佳利润保持不变，但我们计算了多次。特别需要注意的是计算的大小和顺序。它会先计算大小为1的利润，然后是2，当它要计算大小为3时，它会从头开始计算，首先是1，然后是2，最后是3。这种情况会不断重复，因为它不存储任何中间结果。一个显而易见的改进是将利润存储在一个备忘录中，然后稍后使用它。
- en: 'We add a small modification here: if the `best_profit` for a given `cake_size`
    is already calculated, we just use it right away without calculating it, as shown
    in the following code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做了一个小修改：如果给定`cake_size`的`best_profit`已经计算过，我们就直接使用它，而不再重新计算，代码如下所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s now look at the complete code snippet:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下完整的代码片段：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now if we run this program, we get the following output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们运行这个程序，我们将得到以下输出：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, instead of running the calculations 2n times, we're running it just `n`
    times. That's a vast improvement. And all we had to do was save the result of
    the output in a dictionary, or memo, hence the name **memoization**. In this method,
    we essentially save the intermediate solution in a dictionary to avoid re-computation.
    This method is also called the top-bottom method as we follow natural ordering
    analogous to searching in a binary tree, for instance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不是运行计算2n次，而是只运行`n`次。这是一个巨大的改进。我们所需要做的只是将输出结果保存在字典或备忘录中，因此这种方法叫做**备忘录化**。在这种方法中，我们本质上将中间解保存到字典中，以避免重新计算。这个方法也被称为自顶向下方法，因为我们遵循自然顺序，类似于在二叉树中查找，例如。
- en: Next, we will be looking at the tabular method.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨表格方法。
- en: The Tabular Method
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表格方法
- en: 'Using memoization, we arbitrarily store the intermediate computation. The tabular
    method does almost the same thing, but slightly differently: it goes in a predetermined
    order, which is almost always fixed – from small to large. This means that to
    obtain the most profitable cuts, we will first get the most profitable cut in
    a 1 kg cake, then a 2 kg cake, then 3 kg, and so on. This is usually done using
    a matrix and is called the bottom-up method as we solve the smaller problems first.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用备忘录化方法，我们随意地存储中间计算结果。表格方法几乎做了相同的事情，只是方式稍有不同：它按预定顺序进行，这几乎总是固定的——从小到大。这意味着，为了获得最有利的切割，我们将首先获得
    1 公斤蛋糕的最有利切割，然后是 2 公斤蛋糕，接着是 3 公斤蛋糕，依此类推。通常使用矩阵完成此操作，这被称为自底向上方法，因为我们先解决较小的问题。
- en: 'Consider the following code snippet:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码片段：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code, we are iterating over the sizes first and then cuts.
    A good exercise would be to run the code in an IDE using a debugger to see how
    the `profits` array is updated. First, it would find the most profit in the cake
    of size 1, and then it would find the most profit in the cake of size 2\. But
    here, the second `for` loop would try both the configurations: one cut (two cakes
    of size 1), and no cuts (one cake of size 2) indicated by `profits[i – current_size]`.
    Now, similarly, for every size, it would try to cut the cake in all the possible
    configurations, without recalculating the profits on the smaller part. For instance,
    `profits[i – current_size]` would return the best possible configuration, without
    recalculating it.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先遍历尺寸，然后是切割。一个不错的练习是使用 IDE 和调试器运行代码，查看 `profits` 数组是如何更新的。首先，它会找到大小为
    1 的蛋糕的最大利润，然后找到大小为 2 的蛋糕的最大利润。但是在这里，第二个 `for` 循环会尝试两种配置：一种是切割（两块大小为 1 的蛋糕），另一种是不切割（一个大小为
    2 的蛋糕），由 `profits[i – current_size]` 指定。现在，对于每个尺寸，它都会尝试在所有可能的配置中切割蛋糕，而不会重新计算较小部分的利润。例如，`profits[i
    – current_size]` 会返回最佳配置，而无需重新计算。
- en: 'Exercise 5.01: Memoization in Practice'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.01：实践中的备忘录化
- en: 'In this exercise, we will try to solve a DP problem using the memoization method.
    The problem is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将尝试使用备忘录化方法解决一个动态规划问题。问题如下：
- en: 'Given a number `n`, print the nth Tribonacci number. The Tribonacci sequence
    is similar to the Fibonacci sequence but uses three numbers instead of two. This
    means that the nth Tribonacci number is the sum of the prior three numbers. The
    following is an example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数字 `n`，打印第 n 个三斐波那契数。三斐波那契数列类似于斐波那契数列，但使用三个数字而不是两个。这意味着，第 n 个三斐波那契数是前面三个数字的和。以下是一个示例：
- en: 'Fibonacci sequence 0, 1, 2, 3, 5, 8… is defined as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 斐波那契数列 0, 1, 2, 3, 5, 8……定义如下：
- en: '![Figure 5.5: Fibonacci sequence'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5：斐波那契数列](img/B16182_05_05.jpg)'
- en: '](img/B16182_05_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_05.jpg)'
- en: 'Figure 5.5: Fibonacci sequence'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：斐波那契数列
- en: 'Tribonacci sequence 0, 0, 1, 1, 2, 4, 7…. is defined as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 三斐波那契数列 0, 0, 1, 1, 2, 4, 7……定义如下：
- en: '![Figure 5.6: Tribonacci sequence'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6：三斐波那契数列](img/B16182_05_06.jpg)'
- en: '](img/B16182_05_06.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_06.jpg)'
- en: 'Figure 5.6: Tribonacci sequence'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：三斐波那契数列
- en: 'The generalized formula for the Tribonacci sequence is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 三斐波那契数列的广义公式如下：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following steps will help you complete the exercise:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: 'Now that we know the formula, the first step is to create a simple recursive
    implementation in Python. Use the formulas in the description and convert them
    into a Python function. You can choose to do it in a Jupyter notebook, or just
    a simple `.py` Python file:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们知道了公式，第一步是用 Python 创建一个简单的递归实现。使用描述中的公式并将其转换为 Python 函数。你可以选择在 Jupyter notebook
    中做，或者直接用一个简单的 `.py` Python 文件：
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code, we are recursively calculating the value of the Tribonacci
    number. Furthermore, if the number is less than or equal to 1, we know the answer
    is going to be 0, and for 2 it''s going to be 1, so we add the `if-else` condition
    to take care of the edge cases. To test the preceding code, just call it in the
    `main` block and check the output is as expected:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们递归地计算Tibonacci数的值。此外，如果数字小于或等于1，我们知道答案将是0，2的答案将是1，因此我们添加了`if-else`条件来处理边缘情况。要测试前面的代码，只需在`main`块中调用它，并检查输出是否符合预期：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we''ve learned, this implementation is quite slow and grows exponentially
    with higher values of `n`. Now, using the principle of memoization, store the
    intermediate results so they are not recomputed. Create a dictionary that will
    check whether the answer to that nth tribonacci number is already added to the
    dictionary. If yes, just return that; otherwise, try to compute it:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们所学到的，这个实现非常慢，并且随着`n`的增加，增长速度呈指数级。现在，使用备忘录法，存储中间结果，以便它们不被重新计算。创建一个字典来检查该第`n`个Tibonacci数的答案是否已经添加到字典中。如果是，则直接返回；否则，尝试计算：
- en: '[PRE16]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, using the previous code snippet, calculate the nth Tribonacci number without
    using recursion. Run the code and make sure the output matches the expectation
    by running it in the `main` block:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用前面的代码片段，不使用递归来计算第`n`个Tibonacci数。运行代码并确保输出与预期相符，通过在`main`块中运行它：
- en: '[PRE17]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see in the output, the sum is `7`. We have learned how to convert
    a simple recursive function into memoized DP code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在输出中看到的，和是`7`。我们已经学会了如何将一个简单的递归函数转换为记忆化的动态规划代码。
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3dghMJ1](https://packt.live/3dghMJ1).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3dghMJ1](https://packt.live/3dghMJ1)。
- en: You can also run this example online at [https://packt.live/3fFE7RK](https://packt.live/3fFE7RK).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在线运行此示例，网址为[https://packt.live/3fFE7RK](https://packt.live/3fFE7RK)。
- en: Next, we will try to do the same with the tabular method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试使用表格方法做同样的事情。
- en: 'Exercise 5.02: The Tabular Method in Practice'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习5.02：表格法在实践中的应用
- en: 'In this exercise, we will solve a DP problem using the tabular method. The
    goal of the exercise is to identify the length of the longest common substring
    between two strings. For instance, if the two strings are `BBBABDABAA` and `AAAABDABBAABB`,
    then the longest common substring is `ABDAB`. Other common substrings are `AA`,
    `BB`, and `BA`, and `BAA` but they''re not the longest:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用表格方法解决一个动态规划问题。练习的目标是识别两个字符串之间的最长公共子串的长度。例如，如果两个字符串分别是`BBBABDABAA`和`AAAABDABBAABB`，那么最长的公共子串是`ABDAB`。其他公共子串有`AA`、`BB`和`BA`，以及`BAA`，但它们不是最长的：
- en: 'Import the `numpy` library:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`库：
- en: '[PRE19]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Implement the brute force method to calculate the longest common substring
    of two strings first. Imagine we have two variables, `i` and `j`, that indicate
    the start and end of the substring. Use these pointers to indicate the start and
    end of the substring for both strings. You can use the `==` operator in Python
    to see whether the strings match:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现暴力法，首先计算两个字符串的最长公共子串。假设我们有两个变量`i`和`j`，它们表示子串的开始和结束位置。使用这些指针来指示两个字符串中子串的开始和结束位置。您可以使用Python中的`==`运算符来查看字符串是否匹配：
- en: '[PRE20]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Call the function using the `main` block:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`main`块调用函数：
- en: '[PRE21]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can verify that the output is correct:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以验证输出是否正确：
- en: '[PRE22]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s implement the tabular method. Now that we have a simple solution, we
    can proceed to optimize it. Look at the main loop, which nests four times. Meaning
    the solution runs in `O(N^4)`. It performs the same calculations irrespective
    of whether we have the longest common substring or not. Use the tabular method
    to come up with more solutions:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现表格方法。现在我们有了一个简单的解决方案，我们可以继续优化它。看看主循环，它嵌套了四次。这意味着该解决方案的运行时间是`O(N^4)`。无论我们是否有最长公共子串，解决方案都会执行相同的计算。使用表格方法得出更多的解决方案：
- en: '[PRE23]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The problem has a nice matrix structure inherent to it. Consider the length
    of one string to be the rows and the length of the other string as the columns
    of the matrix. Initialize this matrix with `0`. The values in the matrix at position
    `i, j` will indicate whether the `i`th character in the first string is the same
    as the `j`th character in the second string.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个问题具有天然的矩阵结构。将其中一个字符串的长度视为矩阵的行，另一个字符串的长度视为矩阵的列。将该矩阵初始化为`0`。矩阵中位置`i, j`的值将表示第一个字符串的第`i`个字符是否与第二个字符串的第`j`个字符相同。
- en: Now the longest common substring will have the highest number of ones in a diagonal.
    Use this fact to increment the maximum length of the substring by 1 if there's
    a match at the current position and there's a `1` in the `i-1` and `j-1` positions.
    This will essentially indicate that there are two subsequent matches. Return the
    `max` element in the matrix using `np.max(table)`. We can also look at the diagonally
    increasing sequence until the value reaches `5`.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，最长公共子串将具有在对角线上最多的1个数字。利用这个事实，如果当前位子匹配且`i-1`和`j-1`位置上有`1`，则将最大子串的长度增加1。这将表明有两个连续的匹配。使用`np.max(table)`返回矩阵中的`max`元素。我们也可以查看对角线递增的序列，直到该值达到`5`。
- en: 'Call the function using the `main` block:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`main`模块调用该函数：
- en: '[PRE24]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output will be as follows:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.7: Output for LCS'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.7：LCS输出结果'
- en: '](img/B16182_05_07.jpg)'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_07.jpg)'
- en: 'Figure 5.7: Output for LCS'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：LCS输出结果
- en: As you can see, there is a direct mapping between the rows (the first string)
    and the columns (the second string), so the LCS string would just be the diagonal
    elements counted backward from the LCS length. In the preceding output, you can
    see that the highest element is 5 and hence you know that the length is 5\. The
    LCS string would be the elements going diagonally upward from the element `5`.
    The direction of the string will always be diagonally upward since the columns
    always run from left to right. Note that the solution involves just calculating
    the length of the LCS and not finding the actual LCS.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，行（第一列）和列（第二列）之间存在直接的映射关系，因此LCS（最长公共子序列）字符串只会是从LCS长度开始倒数的对角线元素。在前面的输出中，你可以看到最高的元素是5，因此你知道长度是5。LCS字符串将是从元素`5`开始的对角线元素。字符串的方向总是向上对角线，因为列总是从左到右排列。请注意，解决方案仅仅是计算LCS的长度，而不是找到实际的LCS。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fD79BC](https://packt.live/3fD79BC).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一特定部分的源代码，请参考 [https://packt.live/3fD79BC](https://packt.live/3fD79BC)。
- en: You can also run this example online at https://packt.live/2UYVIfK.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，网址是 [https://packt.live/2UYVIfK](https://packt.live/2UYVIfK)。
- en: Now that we have learned how to solve DP problems, we should next learn how
    to identify them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何解决动态规划问题，接下来我们应该学习如何识别这些问题。
- en: Identifying Dynamic Programming Problems
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别动态规划问题
- en: 'While it is easy to solve a DP problem once you identify how it recurses, it
    is difficult to determine whether a problem can be solved using DP. For instance,
    the traveling salesman problem, where you are given a graph and wish to cover
    all the vertices in the least possible time, is something that can''t be solved
    using DP. Every DP problem must satisfy two prerequisites: it should have an optimal
    substructure and should have overlapping subproblems. We''ll look into exactly
    what they mean and how to solve them in the subsequent section.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一旦识别出问题如何递归，解决动态规划问题就变得很容易，但确定一个问题是否可以通过动态规划来解决却是很困难的。例如，旅行商问题，你被给定一个图，并希望在最短的时间内覆盖所有的顶点，这是一个无法用动态规划解决的问题。每个动态规划问题必须满足两个先决条件：它应该具有最优子结构，并且应该有重叠子问题。我们将在接下来的部分中详细了解这些条件的含义以及如何解决它们。
- en: Optimal Substructures
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最优子结构
- en: 'Recall the best path example we discussed earlier. If you want to go from point
    A to point C through B, and you know that''s the best path, there''s no point
    in exploring others. Rephrasing this: If I want to go from A to D and I know the
    best path from A to C, then the best route from A to D will include the path from
    A to C. This is called the optimal substructure. Essentially, what it means is
    the optimal solution to the problem contains optimal solutions to subproblems.
    Remember how we didn''t care to recalculate the best profit for a cake of size
    `n` once we knew it? Because we know the best profit for the cake of size `n +
    1` will include `n` while considering making a cut and dividing the cake into
    size `n` and `1`. To reiterate, the property of optimal substructure would be
    a requirement if we were to solve the problem using DP.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们之前讨论的最佳路径示例。如果你想从A点通过B点到达C点，并且你知道这是最佳路径，那么就没有必要探索其他路径。换句话说：如果我想从A点到达D点，而我知道从A点到C点的最佳路径，那么从A点到D点的最佳路径一定会包含从A点到C点的路径。这就是所谓的最优子结构。本质上，它意味着问题的最优解包含了子问题的最优解。记得我们在知道一个大小为`n`的蛋糕的最佳利润后，就不再重新计算它吗？因为我们知道，大小为`n+1`的蛋糕的最佳利润会包括`n`，这是在考虑切分蛋糕为大小`n`和`1`时得到的。再重复一遍，如果我们要使用动态规划（DP）解决问题，最优子结构的属性是一个必要条件。
- en: Overlapping Subproblems
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重叠子问题
- en: 'Remember when we were initially designing a brute force solution for the cake
    partition example, and later using memoization. Initially, it required 32 steps
    for the brute force approach to arrive at the solution, while memoization took
    only 5\. This was because the brute force approach performed the same computation
    repeatedly: the optimal solution for size three would call for size two and then
    one. Then, for size 4, it would again call for three, and then two, and then one.
    This recursive re-computation is due to the nature of the problem: the overlapping
    subproblems. This is the reason we could save the answer in a memo and later use
    the same solution without recomputing it. The overlapping subproblem is another
    requirement that a problem must have to be solved using DP.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们最初在设计蛋糕分配问题的暴力解法时，后来又采用了备忘录法。最初，暴力解法需要 32 步才能得到解，而备忘录法只需要 5 步。这是因为暴力解法重复执行相同的计算：对于大小为
    3 的问题，它需要先解决大小为 2 和 1 的问题。然后，对于大小为 4 的问题，它又需要解决大小为 3、2 和 1 的问题。这个递归重计算是由于问题的性质：重叠子问题。这也是我们能够将答案保存在备忘录中，之后使用相同的解而不再重新计算的原因。重叠子问题是使用动态规划（DP）来解决问题的另一个必要条件。
- en: The Coin-Change Problem
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬币换零钱问题
- en: 'The coin-change problem is one of the most commonly asked interview questions
    in software engineering interviews. The statement is simple: given a list of coin
    denominations, and a sum value N, identify the number of unique ways to arrive
    at the sum. For instance, if N = 3 and D, the coin denomination, = {1, 2} the
    answer is 2\. That is, there are two ways to arrive at 3 by having coins of denomination
    1 and 2, which are {1, 1, 1} and {2, 1}:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 硬币换零钱问题是软件工程面试中最常被问到的题目之一。题目很简单：给定一个硬币面额的列表，以及一个总和 N，找出到达该总和的不同方式的数量。例如，如果 N
    = 3 且硬币面额 D = {1, 2}，那么答案是 2。也就是说，有两种方式可以得到 3：{1, 1, 1} 和 {2, 1}：
- en: To solve the problem, you would need to prepare the recursion formula that will
    calculate the number of ways to arrive at a sum. To do this, you might want to
    start with a simple version that solves just a single number and then try to convert
    it to a more general solution.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你需要准备一个递归公式，计算得到一个总和的不同方式数。为此，你可以从一个简单的版本开始，先解决一个数字的情况，再尝试将其转换为更一般的解法。
- en: The end output could be a table as shown in the following figure, which can
    be used to summarize the result. In the following table, the first row represents
    the denominations, and the first column represents the sum. More specifically,
    the first row, 0, 1, 2, 3, 4, 5, represents the sum. And the first column represents
    the available denominations. We initialize the base cases with 1 and not 0 because
    if the denomination is less than the sum, then we just copy the previous combinations
    over.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终输出可能是如下图所示的表格，可用于总结结果。在下表中，第一行表示面额，第一列表示总和。更具体地说，第一行的 0、1、2、3、4、5 表示总和，第一列表示可用的面额。我们将基础情况初始化为
    1 而非 0，因为如果面额小于总和，则我们只是将之前的组合复制过来。
- en: 'The following table represents how to count the number of ways to get to 5
    using coins [1, 2]:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下表表示如何使用硬币 [1, 2] 来计算得到 5 的方法数：
- en: '![Figure 5.8: Counting the number of ways to get to sum 5 using denominations
    of 1, 2'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.8：计算使用面额为 1 和 2 的硬币得到总和 5 的方法数'
- en: '](img/B16182_05_08.jpg)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_08.jpg)'
- en: 'Figure 5.8: Counting the number of ways to get to sum 5 using denominations
    of 1, 2'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.8：计算使用面额为 1 和 2 的硬币得到总和 5 的方法数
- en: So, we can see the number of ways to arrive at the sum of 5 using coins of denominations
    1 and 2 is 3, which is basically 1+1+1+1+1, 2+1+1+1, and 2+2+1\. Remember we're
    looking for only unique ways, meaning, 2+2+1 is the same as 1+2+2\.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，我们可以看到使用面额为 1 和 2 的硬币得到总和 5 的方法数是 3，具体来说就是 1+1+1+1+1、2+1+1+1 和 2+2+1。记住，我们只考虑独特的方式，也就是说，2+2+1
    和 1+2+2 是相同的。
- en: Let's execute an exercise to solve the coin-change problem.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来解决硬币换零钱问题。
- en: 'Exercise 5.03: Solving the Coin-Change Problem'
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.03：解决硬币换零钱问题
- en: 'In this exercise, we will be solving the classic and very popular coin-change
    problem. Our goal is to find the number of permutations, in which the coins can
    be used to arrive at a sum, 5, using the coin denominations of 1, 2, and 3\. The
    following steps will help you complete the exercise:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将解决经典且非常流行的硬币换零钱问题。我们的目标是找到用面额为 1、2 和 3 的硬币组合得到总和 5 的不同排列数。以下步骤将帮助你完成这个练习：
- en: 'Import the `numpy` and `pandas` libraries:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy` 和 `pandas` 库：
- en: '[PRE25]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s now try to identify the overlapping subproblem. As previously, there''s
    one common thing: we have to search for all possible denominations and check whether
    they sum to a certain number. Furthermore, it''s a little more complicated than
    the cake example since we have got two things to iterate on: firstly, the denomination,
    and secondly the total sum (in the cake example, it was only one variable, the
    cake size). So, we need a 2D array, or a matrix.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们尝试识别重叠子问题。如之前所述，有一个共同点：我们必须搜索所有可能的面额，并检查它们是否能加起来得到某个数。此外，这比蛋糕示例稍微复杂一些，因为我们有两个变量需要迭代：首先是面额，其次是总和（在蛋糕示例中，只有一个变量，即蛋糕大小）。因此，我们需要一个二维数组或矩阵。
- en: 'On the columns, we will have the sum we are trying to reach, and on the rows,
    we will consider various denominations available. As we loop over the denominations
    (columns), we will calculate the number of ways to sum up to `n` by first adding
    the number of ways to reach the sum without considering the current denomination,
    and then by considering it. This is analogous to the cake example, where we first
    performed the cut, calculated the profit, and then didn''t perform the cut and
    calculate the profit. The difference, however, is this time the previous best
    configuration would be fetched from the row above, and also, we would add the
    two numbers instead of selecting the maximum out of it since we are interested
    in the total number of ways to reach the sum. For example, the number of ways
    to sum up to 4 using {1, 2} would be first to use {2} and then add the number
    of ways to sum up to 4 – 2 = 2\. We could fetch it from the same row and the index
    would be 2\. We will also initiate the first row with 1s as they are either invalid
    (the number of ways to reach zeros using 1) or valid with one solution:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在列上，我们将展示我们试图达到的和，而在行上，我们将考虑可用的各种面额。当我们遍历面额（列）时，我们将通过首先计算不考虑当前面额时达到某个和的方式数量，然后再加上考虑当前面额的方式数量，来计算合计数。这类似于蛋糕示例，其中我们首先进行切割，计算利润，然后不切割并计算利润。然而，区别在于这次我们会从上方的行中获取之前的最佳配置，并且我们会将这两个数相加，而不是选择其中的最大值，因为我们关心的是到达和的所有可能方式的总数。例如，使用
    {1, 2} 求和为 4 的方式是首先使用 {2}，然后加上求和为 4 - 2 = 2 的方式数量。我们可以从同一行获取这个值，索引为 2。我们还会将第一行初始化为
    1，因为它们要么是无效的（使用 1 到达零的方式数量），要么是有效的，并且只有一个解决方案：
- en: '![Figure 5.9: Initial setup of the algorithm'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.9：算法的初始设置'
- en: '](img/B16182_05_09.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_09.jpg)'
- en: '[PRE26]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will initialize a table with the dimension `len(denomination)` x `(N
    + 1)`. The number of columns is `N + 1` since the index includes zero as well:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化一个尺寸为 `len(denomination)` x `(N + 1)` 的表格。列数是 `N + 1`，因为索引包括零：
- en: '[PRE27]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, in the end, we will print the table:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，最后我们将打印出这个表格：
- en: '[PRE28]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a Python script with the following utility, which pretty prints a table.
    This will be useful for debugging. Pretty printing is essentially used to present
    data in a more legible and comprehensive way. By setting the denominations as
    the index, we will see the output more clearly:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个带有以下实用功能的 Python 脚本，它可以漂亮地打印表格。这对于调试非常有用。漂亮打印本质上是用来以更易读和更全面的方式呈现数据。通过将面额作为索引，我们可以更清晰地查看输出：
- en: '[PRE29]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more details on pretty printing, you can refer to the official documentation
    at the following link: [https://docs.python.org/3/library/pprint.html](https://docs.python.org/3/library/pprint.html).'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欲了解更多关于漂亮打印的细节，您可以参考以下链接的官方文档：[https://docs.python.org/3/library/pprint.html](https://docs.python.org/3/library/pprint.html)。
- en: 'Initialize the script with the following configuration:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下配置初始化脚本：
- en: '[PRE30]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output will be as follows:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE31]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we can see in the entry in the last row and column, the number of ways to
    get a 5 using [1, 2] is 3\. We have now learned about the concept of DP in detail.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在最后一行和列的条目中看到的，使用 [1, 2] 获得 5 的方式有 3 种。我们现在已经详细了解了动态规划（DP）的概念。
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2NeU4lT](https://packt.live/2NeU4lT).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/2NeU4lT](https://packt.live/2NeU4lT)。
- en: You can also run this example online at [https://packt.live/2YUd6DD](https://packt.live/2YUd6DD).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在网上运行这个示例，访问 [https://packt.live/2YUd6DD](https://packt.live/2YUd6DD)。
- en: Next, let's see how it is used to solve problems in RL.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看它是如何用于解决强化学习中的问题的。
- en: Dynamic Programming in RL
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的动态规划
- en: DP plays an important role in RL as the number of choices you have at a given
    time is too large. For instance, whether the robot should take a left or right
    turn given the current state of the environment. To solve such a problem, it's
    infeasible to find the outcome of every state using brute force. We can do that,
    however, using DP, using the methods we learned in the previous section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: DP 在 RL 中扮演着重要角色，因为在给定的时刻你所面临的选择太多。例如，机器人在当前环境状态下应该向左转还是向右转。为了求解此类问题，通过蛮力计算每个状态的结果是不可行的。然而，通过
    DP，我们可以使用前一节中学到的方法来解决这一问题。
- en: We have seen the Bellman equation in previous chapters. Let's reiterate the
    basics and see how the Bellman equation has both of the required properties for
    using DP.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的章节中已经看过贝尔曼方程。让我们重述一下基本内容，看看贝尔曼方程如何具备 DP 所需的两个属性。
- en: Assuming the environment is a finite **Markov Decision Process** (**MDP**),
    let's define the state of the environment by a finite set of states, *S*. This
    indicates the state configuration, for instance, the current position of the robot.
    A finite set of actions, *A*, gives the action space, and a finite set of rewards,
    *R*. Let's denote the discounting rate using ![which is a value between](img/B16182_05_09a.png),
    which is a value between 0 and 1\.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设环境是一个有限的**马尔可夫决策过程**（**MDP**），我们用一个有限的状态集 *S* 来定义环境的状态。这表示状态配置，例如机器人的当前位置。有限的动作集
    *A* 给出了动作空间，有限的奖励集 *R*。我们用 ![表示折扣因子](img/B16182_05_09a.png) 来表示折扣率，这个值介于 0 和 1
    之间。
- en: Given a state, *S*, the algorithm chooses one of the actions in *A* using a
    deterministic policy, ![a](img/B16182_02_31b.png). The policy is nothing but a
    mapping between state *S* and action *A*, for instance, a choice a robot would
    make such as go left or right. And a deterministic policy allows us to choose
    an action in a non-random fashion (as opposed to a stochastic policy, which has
    a significant random component).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个状态 *S*，该算法使用一个确定性策略从 *A* 中选择一个动作，![a](img/B16182_02_31b.png)。该策略仅仅是状态 *S*
    和动作 *A* 之间的映射，例如，机器人可能做出的选择，如向左或向右。确定性策略允许我们以非随机的方式选择动作（与随机策略相对，后者包含显著的随机成分）。
- en: 'To concretize our understanding, let''s take an example of a simple autonomous
    car. To make it simple, we will make some reasonable assumptions here. The action
    space can be defined as {left, right, straight, reverse}. A deterministic policy
    is: if there''s a hole in the ground, take a left or right turn to avoid it. A
    stochastic policy, however, would say: if here''s a hole in the ground, take a
    left turn with 80% probability, which means there''s a small chance that the car
    would purposely enter the hole. While this move might not make sense at the moment,
    we will see later, in the *Chapter 7, Temporal Difference Learning*, that this
    is a rather important thing to do and addresses one of the critical concepts in
    RL: the exploration versus exploitation dilemma.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体化我们的理解，假设一个简单的自动驾驶汽车。为了简化起见，我们将在这里做一些合理的假设。动作空间可以定义为 {左转，右转，直行，倒退}。一个确定性策略是：如果地面上有个坑，向左或右转以避免它。然而，一个随机策略会说：如果地面上有个坑，以
    80% 的概率向左转，这意味着汽车有小概率故意进入坑中。虽然这个动作目前看起来可能没有意义，但我们稍后会看到，在*第7章，时间差学习*中，这实际上是一个非常重要的举措，并且解决了
    RL 中的一个关键概念：探索与利用的困境。
- en: 'Coming back to the original point of using DP in RL, the following is the **simplified**
    Bellman equation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 回到使用 DP 在 RL 中的原始点，下面是**简化版**的贝尔曼方程：
- en: '![Figure 5.10: Simplified Bellman equation'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10：简化贝尔曼方程'
- en: '](img/B16182_05_10.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_10.jpg)'
- en: 'Figure 5.10: Simplified Bellman equation'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：简化贝尔曼方程
- en: 'The only difference with the complete equation is we are not summing over ![b](img/B16182_05_10a.png),
    which is valid in the case that we have a non-deterministic environment. Here
    is the complete Bellman equation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 完整方程与简化方程的唯一区别在于我们没有对 ![b](img/B16182_05_10a.png) 进行求和，这在非确定性环境下是有效的。以下是完整的贝尔曼方程：
- en: '![Figure 5.11: Complete Bellman equation'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：完整的贝尔曼方程'
- en: '](img/B16182_05_11.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_11.jpg)'
- en: 'Figure 5.11: Complete Bellman equation'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：完整的贝尔曼方程
- en: 'In the preceding equation, ![formula ](img/B16182_05_11a.png) is the value
    function, the reward for being in a particular state. We will look more deeply
    into it later. ![formula ](img/B16182_05_11b.png) is the reward of taking action
    `a` and ![formula ](img/B16182_05_11c.png) is the reward of the next state. Two
    things you can observe are the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![公式](img/B16182_05_11a.png)是值函数，表示处于特定状态时的奖励。我们稍后会更深入地探讨它。![公式](img/B16182_05_11b.png)是采取动作`a`的奖励，![公式](img/B16182_05_11c.png)是下一个状态的奖励。你可以观察到以下两点：
- en: The recursive nature between ![formula ](img/B16182_05_11d.png) and ![formula
    ](img/B16182_05_11e.png), meaning ![formula ](img/B16182_05_11f.png) has an optimal
    substructure.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在![公式](img/B16182_05_11d.png)和![公式](img/B16182_05_11e.png)之间的递归关系，意味着![公式](img/B16182_05_11f.png)具有最优子结构。
- en: The computation of ![formula ](img/B16182_05_11g.png) will have to be recomputed
    at some point meaning it has overlapping subproblems. Both conditions of DP are
    qualified so we can use it to speed up our solutions.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算![公式](img/B16182_05_11g.png)将在某些时刻需要重新计算，这意味着它存在重叠子问题。动态规划（DP）的两个条件都符合，因此我们可以利用它来加速解决方案。
- en: As we will see later, the structure of the value function is similar to the
    one we saw before in the coin denomination problem. Instead of saving the number
    of ways to reach the sum, we are going to save the best ![c](img/B16182_05_11h.png),
    that is, the best value of the value function that yields the highest return.
    Next, we will look at policy and value iteration, which are the basic algorithms
    that help us solve RL problems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们稍后将看到的，值函数的结构与我们在硬币面额问题中看到的类似。不同之处在于，我们不再保存到达和为的方式数量，而是保存最佳![c](img/B16182_05_11h.png)，即能够带来最高回报的值函数的最佳值。接下来，我们将探讨策略迭代和价值迭代，它们是帮助我们解决RL问题的基本算法。
- en: Policy and Value Iteration
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略和价值迭代
- en: 'The main idea of solving a RL problem is to search for the best policies (a
    way to make decisions) using value functions. This method works well for simple
    RL problems as we need information on the entire environment: the number of states
    and the action space. We can use this method even in a continuous space, but the
    exact solution is not possible in every case. During the updating process, we
    will have to iterate over all the possible scenarios, and that''s the reason using
    this method becomes infeasible when the state and action space is too high:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 解决强化学习（RL）问题的主要思路是利用值函数寻找最佳策略（决策方式）。这种方法对于简单的RL问题效果很好，因为我们需要了解整个环境的信息：状态的数量和动作空间。我们甚至可以在连续空间中使用此方法，但并不是在所有情况下都能得到精确的解。在更新过程中，我们必须遍历所有可能的场景，这也是当状态和动作空间过大时，使用该方法变得不可行的原因：
- en: 'Policy Iteration: start with a random policy and iteratively converge to the
    best one.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略迭代：从一个随机策略开始，逐步收敛到最佳策略。
- en: 'Value Iteration: state with random values and iteratively update them toward
    convergence.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 价值迭代：使用随机值初始化状态，并逐步更新它们直到收敛。
- en: State-Value Functions
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态值函数
- en: 'The state-value function is an array that represents the reward for being in
    that state. Imagine having four possible states in a particular game: `S1`, `S2`,
    `S3`, and `S4`, with `S4` being the terminal (end) state. The state-value table
    can be represented by an array, as indicated in the following table. Please note
    that the values are simply examples. Every state has a "value," hence state-value
    function. This table can be used to make decisions later on in the game:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值函数是一个数组，表示处于该状态时的奖励。假设在一个特定的游戏中有四个可能的状态：`S1`、`S2`、`S3`和`S4`，其中`S4`是终止状态（结束状态）。状态值表可以通过一个数组表示，如下表所示。请注意，值只是示例。每个状态都有一个“值”，因此称为状态值函数。此表格可以用于游戏中稍后的决策：
- en: '![Figure 5.12: Sample table for the state-value function'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.12：状态值函数的示例表格'
- en: '](img/B16182_05_12.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_12.jpg)'
- en: 'Figure 5.12: Sample table for the state-value function'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：状态值函数的示例表格
- en: For instance, if you're in state `S3`, you have two possible choices, `S4` and
    `S2`; you'd go to `S4` since the value of being in that state is higher than that
    of `S2`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你处于状态`S3`，你有两个可能的选择，`S4`和`S2`；你会选择`S4`，因为在那个状态中的值比`S2`更高。
- en: Action-Value Functions
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作值函数
- en: 'The action-value function is a matrix that represents the reward for every
    state-action pair. This again can be used to select the best action to take in
    a particular state. Unlike the previous state-action table, this time, we have
    rewards associated with every action as well, as depicted in the following table:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 动作-值函数是一个矩阵，表示每个状态-动作对的奖励。这同样可以用来选择在特定状态下应该采取的最佳动作。与之前的状态-动作表不同，这次我们为每个动作也关联了奖励，具体如下表所示：
- en: '![Figure 5.13: Sample table for the action-value function'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13：动作-值函数的示例表格'
- en: '](img/B16182_05_13.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_13.jpg)'
- en: 'Figure 5.13: Sample table for the action-value function'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：动作-值函数的示例表格
- en: Note these are just example values and will be calculated using a specific update
    policy. We will be looking at more specific examples of updating policies in the
    *Policy Improvement* section. The table will be later used in the value iteration
    algorithm so we can update the table iteratively and not wait till the very end.
    More on this is in the *Value Iteration* section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些只是示例值，实际计算时会使用特定的更新策略。我们将在*策略改进*部分查看更新策略的更具体示例。这个表格将稍后用于值迭代算法，因此我们可以迭代地更新表格，而不是等到最后一步。更多内容请参考*值迭代*部分。
- en: 'OpenAI Gym: Taxi-v3 Environment'
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Gym：Taxi-v3 环境
- en: 'We saw what an OpenAI Gym environment is in previous chapters, but we''ll be
    playing a different game this time: Taxi-v3\. In this game, we will teach our
    agent taxi driver to pick up and drop off passengers. The yellow block represents
    the taxi. There are four possible locations that are labeled with different characters:
    R, G, B, and Y for Red, Green, Blue, and Yellow, as you can see in the following
    figure. The agent has to pick up the passenger at a location and drop them off
    at a second location. Moreover, there are walls in the environment depicted by
    a `|`. Whenever there''s a wall, the number of possible actions is limited as
    the taxi is not allowed to pass through a wall. This makes the problem interesting
    as the agent has to smartly navigate through the grid while avoiding the walls
    and finding the best possible (shortest) solution:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经了解了什么是 OpenAI Gym 环境，但这次我们将玩一个不同的游戏：Taxi-v3。在这个游戏中，我们将教导我们的代理司机接送乘客。黄色方块代表出租车。环境中有四个可能的地点，分别用不同的字符标记：R、G、B
    和 Y，分别代表红色、绿色、蓝色和黄色，具体如下图所示。代理需要在某个地点接乘客并将其送到另一个地点。此外，环境中有用 `|` 表示的墙壁。每当有墙壁时，可能的动作数量就会受到限制，因为出租车不能穿越墙壁。这使得问题变得有趣，因为代理必须巧妙地在网格中导航，同时避开墙壁，找到最佳的（最短的）解决方案：
- en: '![Figure 5.14: Taxi-v3 environment'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14：Taxi-v3 环境'
- en: '](img/B16182_05_14.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_14.jpg)'
- en: 'Figure 5.14: Taxi-v3 environment'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：Taxi-v3 环境
- en: 'The following is the list of rewards offered for every action:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个动作对应的奖励列表：
- en: '**+20**: On a successful drop-off.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+20**：成功接送时的奖励。'
- en: '**-1**: On every step you take. This is important since we are interested in
    finding the shortest path.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**-1**：每一步都会发生。这一点很重要，因为我们关注的是找到最短的路径。'
- en: '**-10**: On an illegal drop-off or pickup.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**-10**：非法的接送操作。'
- en: '**Policy**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略**'
- en: Every state in the environment is encoded by a number. For instance, the state
    in the previous photo can be represented by `54`. There are 500 such unique states
    in this game. For every such state, we have a corresponding policy (that is, which
    action to perform).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中的每个状态都由一个数字表示。例如，前一张照片中的状态可以用`54`来表示。在这个游戏中有500个这样的独特状态。对于每一个状态，我们都有相应的策略（即，应该执行的动作）。
- en: Let's now try the game ourselves.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们自己尝试一下这个游戏。
- en: 'Initialize the environment and print the possible number of states and the
    action space, which are 500 and 6 currently. In real-world problems, this number
    will be huge (in the billions) and we can''t use discrete agents. But let''s make
    these assumptions for the sake of simplicity and solve it:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化环境并打印可能的状态数和动作空间，当前分别为 500 和 6。在现实问题中，这个数字会非常庞大（可能达到数十亿），我们无法使用离散的代理。但为了简化问题，我们假设这些并进行求解：
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code will print the following output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出以下内容：
- en: '![Figure 5.15: Initiating the Taxi-v3 environment'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15：启动 Taxi-v3 环境'
- en: '](img/B16182_05_15.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_15.jpg)'
- en: 'Figure 5.15: Initiating the Taxi-v3 environment'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：启动 Taxi-v3 环境
- en: 'As you can see, the grid represents the current (initial) state of the environment.
    The yellow box represents the taxi. The six possible choices are: left, right,
    up, down, pickup, and drop. Let''s go ahead and see how we can control the taxi.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网格表示当前（初始）状态的环境。黄色框代表出租车。六个可能的选择是：左、右、上、下、接客和放客。现在，让我们看看如何控制出租车。
- en: 'Using the following code, we will randomly step through the environment and
    look at the output. The `env.step` function is used to go from one state to another.
    The argument it accepts is one of the valid actions in its action space. On stepping,
    it returns a few values as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们将随机地在环境中执行步骤并查看输出。`env.step`函数用于从一个状态转移到另一个状态。它接受的参数是其动作空间中的有效动作之一。在执行一步后，它会返回几个值，如下所示：
- en: '`new_state`: The new state (an integer denoting the next state)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_state`：新的状态（一个表示下一个状态的整数）'
- en: '`reward`: The reward obtained from transitioning to the next state'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`：从转移到下一个状态中获得的奖励'
- en: '`done`: If the environment needs to be reset (meaning you''ve reached a terminal
    state)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done`：如果环境需要重置（意味着你已经到达了终止状态）'
- en: '`info`: Debug info that indicates transition probabilities'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`：表示转移概率的调试信息'
- en: 'Since we''re using a deterministic environment, we will always have transition
    probabilities that are `1.0`. There are other environments that have non-1 transition
    probability that indicate if you take a certain decision; for instance, if you
    take a right turn, the environment will take a right turn with said probability,
    meaning there''s a chance that you will stay in the same place even after taking
    a specific action. The agent is not allowed to learn this information as it interacts
    with the environment as, otherwise, it would be unfair if the agent knows the
    environment information:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是确定性环境，因此转移概率始终为`1.0`。还有其他环境具有非1的转移概率，表示如果你做出某个决策；例如，如果你右转，环境将以相应的概率右转，这意味着即使做出特定的行动后，你也有可能停留在原地。代理在与环境互动时不能学习这些信息，否则如果代理知道环境信息，将会是不公平的：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Using this code, we will take random (but valid) steps in the environment and
    stop when we''ve reached the terminal state. If we execute the code, we will see
    the following output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，我们将在环境中随机（但有效）地执行步骤，并在到达终止状态时停止。如果执行代码，我们将看到以下输出：
- en: '![Figure 5.16: Randomly stepping through the environment'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16：随机遍历环境'
- en: '](img/B16182_05_16.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_16.jpg)'
- en: 'Figure 5.16: Randomly stepping through the environment'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：随机遍历环境
- en: 'Looking at the output, we can see the new state that is stepped through after
    taking an action and the reward received for taking the action; done will indicate
    that we''ve arrived at a terminal stage; and some environment information such
    as transition probabilities. Next, we will look at our first RL algorithm: policy
    iteration.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看输出，我们可以看到在执行某个动作后，所经历的新状态以及执行该动作所获得的奖励；`done`会指示我们已经到达了终止阶段；还有一些环境信息，例如转移概率。接下来，我们将查看我们的第一个强化学习算法：策略迭代。
- en: Policy Iteration
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略迭代
- en: 'As the name suggests, in policy iteration, we iterate over multiple policies
    and then optimize them. The policy iteration algorithm works in two steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，在策略迭代中，我们会遍历多个策略，然后进行优化。策略迭代算法分为两步：
- en: Policy evaluation
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略评估
- en: Policy improvement
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略改进
- en: Policy evaluation calculates the value function for the current policy, which
    is initialized randomly. We then use the Bellman optimality equation to update
    the values for every single state. Then, once we have a new value function, we
    update the policy to maximize the rewards and update the policy, which is also
    called policy improvement. Now if the policy is updated (that is, even if a single
    decision in the policy is changed), this newer policy is guaranteed to be better
    than the older once. If the policy doesn't update, it means that the current policy
    is already the most optimal one (otherwise, it would have updated and found a
    better one).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 策略评估计算当前策略的值函数，初始时是随机的。然后，我们使用贝尔曼最优性方程更新每个状态的值。接着，一旦我们得到了新的值函数，就更新策略以最大化奖励并进行策略改进。现在，如果策略发生了更新（即使策略中的一个决策发生了变化），这个更新后的策略保证比旧的策略更好。如果策略没有更新，则意味着当前的策略已经是最优的（否则它会被更新并找到更好的策略）。
- en: 'The following are the steps in which the policy iteration algorithm works:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是策略迭代算法的工作步骤：
- en: Start with a random policy.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个随机策略开始。
- en: Compute the value function for all the states.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有状态的值函数。
- en: Update the policy to choose the action that maximizes the rewards (Policy Improvement).
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略，选择能够最大化奖励的行动（策略改进）。
- en: Stop when the policy doesn't change. This indicates the optimal policy has been
    obtained.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当策略不再变化时停止。这表明已经获得了最优策略。
- en: 'Let''s take a dry run through the algorithm manually and see how it is updated,
    using a simple example:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动通过算法进行一次干运行，看看它是如何更新的，使用一个简单的例子：
- en: 'Start with a random policy. The following table lists the possible actions
    for an agent to take in a given position in the Taxi-v3 environment:![Figure 5.17:
    Possible actions for an agent'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个随机策略开始。下表列出了代理在 Taxi-v3 环境中给定位置可以采取的可能行动：![图 5.17：代理的可能行动
- en: '](img/B16182_05_17.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_17.jpg)'
- en: 'Figure 5.17: Possible actions for an agent'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.17：代理的可能行动
- en: In the preceding figure, the table is the environment and the boxes represent
    the choices. The arrows indicate the action to take if the agent were in that
    position.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的图中，表格表示环境，框表示选择。箭头表示如果代理处于该位置，应采取的行动。
- en: 'Calculate the value function for all the unique states. The following table
    lists the sample state values for each state of the agent. The values are initiated
    with zeros (some variations of the algorithm also use small random values close
    to 0):![Figure 5.18: Reward values for each state'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有唯一状态的值函数。下表列出了每个状态的样本状态值。值初始化为零（某些算法的变体也使用接近零的小随机值）：![图 5.18：每个状态的奖励值
- en: '](img/B16182_05_18.jpg)'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_18.jpg)'
- en: 'Figure 5.18: Reward values for each state'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.18：每个状态的奖励值
- en: 'To understand the update rule visually, let''s use an extremely simple example:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了直观地理解更新规则，我们使用一个极其简单的例子：
- en: '![Figure 5.19: Sample policy to understand the update rule'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.19：理解更新规则的示例策略'
- en: '](img/B16182_05_19.jpg)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_19.jpg)'
- en: 'Figure 5.19: Sample policy to understand the update rule'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.19：理解更新规则的示例策略
- en: 'Starting from the blue position, the policy will end in the green (terminal)
    position after the first `policy_evaluation` step. The values will be updated
    the following way (one diagram for every iteration):'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从蓝色位置开始，经过第一步`policy_evaluation`后，策略将到达绿色（终止）位置。值将按照以下方式更新（每次迭代都有一个图示）：
- en: '![Figure 5.20: Reward multiplying at every step'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.20：每步奖励的乘法'
- en: '](img/B16182_05_20.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_20.jpg)'
- en: 'Figure 5.20: Reward multiplying at every step'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.20：每步奖励的乘法
- en: At every step, the reward is multiplied by gamma (`0.9` in this case). Also,
    in this example, we already started out with an optimal policy, so the updated
    policy will look exactly the same as the current one.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每一步，奖励都会乘以gamma（在此示例中为`0.9`）。此外，在这个例子中，我们已经从最优策略开始，因此更新后的策略将与当前策略完全相同。
- en: 'Update the policy. Let''s look at the update rule with a small example. Consider
    the following as the current value function and the corresponding policy:![Figure
    5.21: The sample value function and the corresponding policy.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略。让我们通过一个小例子来看看更新规则。假设以下是当前的值函数及其对应的策略：![图 5.21：样本值函数及其对应的策略。
- en: '](img/B16182_05_21.jpg)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_21.jpg)'
- en: 'Figure 5.21: The sample value function and the corresponding policy.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.21：样本值函数及其对应的策略。
- en: As you can see in the preceding figure, the left table indicates the values,
    and the right table indicates the policy (decision).
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前图所示，左侧的表格表示值，右侧的表格表示策略（决策）。
- en: 'Once we perform an update, imagine the value function changes to the following:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们执行更新，假设值函数变为如下所示：
- en: '![Figure 5.22: Updated values of the sample value function'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.22：样本值函数的更新值'
- en: '](img/B16182_05_22.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_22.jpg)'
- en: 'Figure 5.22: Updated values of the sample value function'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.22：样本值函数的更新值
- en: 'Now, the policy, in every cell, will update so that the action will take the
    agent to the state that yields the highest reward and thus the corresponding policy
    will look something like the following:'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，策略将在每个单元格中更新，使得行动会带领代理到达能够提供最高奖励的状态，因此对应的策略将类似于以下内容：
- en: '![Figure 5.23: Corresponding policy to the updated value function'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.23：更新值函数对应的策略'
- en: '](img/B16182_05_23.jpg)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_23.jpg)'
- en: 'Figure 5.23: Corresponding policy to the updated value function'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.23：更新值函数对应的策略
- en: Repeat steps 1-3 until the policy no longer changes.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1-3，直到策略不再变化。
- en: We will train the algorithm to iteratively approximate the true value function
    and do that in episodes, which will give us the most optimal policy. One episode
    is a series of actions until the agent reaches the terminal state. This can be
    the goal (drop-off, for instance, in the Taxi-v3 environment) state or it can
    be a number that defines the maximum number of steps the agent can take to avoid
    infinite loops.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将训练算法，通过回合迭代地逼近真实的价值函数，并且每个回合都给我们提供最优策略。一个回合是智能体执行一系列动作直到达到终止状态。这可以是目标状态（例如，在Taxi-v3环境中的乘客下车状态），也可以是定义智能体可以采取的最大步数的数字，以避免无限循环。
- en: 'Let''s use the following code to initialize the environment and the value function
    table. We will save the value function in the variable `V`. Furthermore, following
    the first step in the algorithm, we will start out with a random policy using
    the `env.action_space.sample()` method, which will return a random action every
    time it''s called:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用以下代码初始化环境和价值函数表。我们将把价值函数保存在变量`V`中。此外，根据算法的第一步，我们将使用`env.action_space.sample()`方法从一个随机策略开始，这个方法每次调用时都会返回一个随机动作：
- en: '[PRE34]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, in the next section, we will define the variables and initialize them:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在下一节中，我们将定义并初始化变量：
- en: '[PRE35]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now comes the main loop, which will perform the iteration:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在进入主循环，它将执行迭代：
- en: '[PRE36]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have the basic setup ready, we will first do the policy evaluation
    step using the following code:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了基本设置，我们将首先使用以下代码进行策略评估步骤：
- en: '[PRE37]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the following code, we will loop through the states and update ![c](img/B16182_05_23a.png):'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将循环遍历状态并更新![c](img/B16182_05_23a.png)：
- en: '[PRE38]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we will use the Bellman optimality equation to update ![b](img/B16182_05_23b.png):'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用贝尔曼最优方程更新![b](img/B16182_05_23b.png)：
- en: '[PRE39]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Once we do the policy evaluation step, we will perform policy improvement with
    the following code:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成策略评估步骤，我们将使用以下代码进行策略改进：
- en: '[PRE40]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s start by defining all the required variables:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义所有必需的变量：
- en: '[PRE41]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, here, we will calculate the future reward by taking this action. Note
    that we''re using a simplified equation because we don''t have non-one transition
    probabilities:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在这里，我们将通过采取这个动作来计算未来的奖励。请注意，我们使用的是简化的方程，因为我们没有非一的转移概率：
- en: '[PRE42]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Once the optimal policy is learned, we will test it on a fresh environment.
    Now that both the parts are ready. Let''s call them using the `main` block of
    code:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦最优策略被学习，我们将在新的环境中对其进行测试。现在，两个部分都已准备好。让我们通过`main`代码块调用它们：
- en: '[PRE43]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we will add a `play` function that will test the policy on a fresh environment:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个`play`函数，用于在新的环境中测试策略：
- en: '[PRE44]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, let''s define `max_steps`. This is essentially the maximum number of
    steps the agent is allowed to take. If it doesn''t reach a solution in this time,
    then we call it an episode and proceed:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义`max_steps`。这基本上是智能体允许采取的最大步数。如果在此时间内没有找到解决方案，我们将其称为一个回合并继续：
- en: '[PRE45]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here, we will take the action that we saved in the policy earlier:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将采取之前保存在策略中的动作：
- en: '[PRE46]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'After running the main block, we see the following output:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行主代码块后，我们看到如下输出：
- en: '![Figure 5.24: The agent drops the passenger in the correct location'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.24：智能体将乘客送到正确的位置'
- en: '](img/B16182_05_24.jpg)'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_24.jpg)'
- en: 'Figure 5.24: The agent drops the passenger in the correct location'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：智能体将乘客送到正确的位置
- en: As you can see, the agent drops the passenger in the right location. Note that
    the output is truncated for presentation purposes.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，智能体将乘客送到正确的位置。请注意，输出已被截断以便于展示。
- en: Value Iteration
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 价值迭代
- en: 'As you saw in the previous section, we arrived at the optimal solution after
    a few iterations, but policy iteration has one disadvantage: we get to improve
    the policy only once after multiple iterations of evaluation.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在前一节看到的，我们在几次迭代后得到了最优解，但策略迭代有一个缺点：我们只能在多次评估迭代后改进一次策略。
- en: 'The simplified Bellman equation can be updated in the following way. Note that
    this is similar to the policy evaluation step, but the only addition is taking
    the max value of the value function over all the possible actions:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的贝尔曼方程可以通过以下方式更新。请注意，这与策略评估步骤相似，唯一的不同是采取所有可能动作的价值函数的最大值：
- en: '![Figure 5.25: Updated Bellman equation'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.25：更新的贝尔曼方程'
- en: '](img/B16182_05_25.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_25.jpg)'
- en: 'Figure 5.25: Updated Bellman equation'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25：更新的贝尔曼方程
- en: 'The equation can be comprehended as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程可以理解为如下：
- en: '"*For a given state, take all the possible actions and then store the one with
    the highest V[s] value*."'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '"*对于给定的状态，采取所有可能的动作，然后存储具有最高 V[s] 值的那个*。"'
- en: It's as simple as that. Using this technique, we can combine both evaluation
    and improvement in a single step as you will see now.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。使用这种技术，我们可以将评估和改进结合在一个步骤中，正如你现在将看到的那样。
- en: 'We will start off as usual by defining the important variables, such as `gamma`,
    `state_size`, and `policy`, and the value function dictionary:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像往常一样，先定义一些重要的变量，比如 `gamma`、`state_size` 和 `policy`，以及值函数字典：
- en: '[PRE47]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'And using the equation defined before, we will take the same loop and make
    the change in the ![formula ](img/B16182_05_25a.png) calculation part. We are
    now using the updated Bellman equation, which was defined earlier:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前定义的公式，我们将采用相同的循环，并在 ![公式](img/B16182_05_25a.png) 计算部分做出更改。现在，我们使用的是之前定义的更新后的
    Bellman 方程：
- en: '[PRE48]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Thus, we have successfully implemented the policy iteration and value iteration
    for the Taxi-v3 environment.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功实现了 Taxi-v3 环境中的策略迭代和值迭代。
- en: In the next activity, we will be using the very popular FrozenLake-v0 environment
    for policy and value iteration. Before we begin, let's quickly explore the basics
    of the environment.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将使用非常流行的 FrozenLake-v0 环境来进行策略迭代和值迭代。在我们开始之前，让我们快速了解一下该环境的基本情况。
- en: The FrozenLake-v0 Environment
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FrozenLake-v0 环境
- en: 'The environment is based on a scenario in which there is a frozen lake, except
    for some parts where the ice has melted. Suppose that a group of friends is playing
    frisbee near the lake and one of them made a wild throw that landed the frisbee
    right in the middle of the lake. The goal is to navigate across the lake and get
    the frisbee back. Now, the fact that has to be considered here is that the ice
    is slippery, and you cannot always move in the intended direction. The surface
    is described using a grid as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 该环境基于一个场景，场景中有一个冰冻湖，除了部分地方冰面已经融化。假设一群朋友在湖边玩飞盘，其中一个人投了一个远离的飞盘，飞盘正好落在湖中央。目标是穿越湖面并取回飞盘。现在，必须考虑的事实是，冰面非常滑，你不能总是按照预期的方向移动。这个表面用以下网格描述：
- en: '[PRE49]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note that the episode ends when one of the players reaches the goal or falls
    in the hole. The player is rewarded with a 1 or 0 respectively.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当其中一名玩家到达目标或掉进洞里时，回合结束。玩家分别会获得 1 或 0 的奖励。
- en: Now, in the Gym environment, the agent is supposed to control the movement of
    the player accordingly. As you know, some tiles in the grid can be stepped upon
    and some may land you directly into the hole where the ice has melted. Hence,
    the movement of the player is highly unpredictable and is partially dependent
    on the direction that the agent has chosen.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 Gym 环境中，代理应该相应地控制玩家的移动。正如你所知，网格中的某些方格可以踩上去，而有些方格可能会把你直接带到冰面融化的洞里。因此，玩家的移动非常不可预测，部分取决于代理选择的方向。
- en: Note
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on the FrozenLake-v0 environment, please refer to the
    following link: [https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 FrozenLake-v0 环境的信息，请参见以下链接：[https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)
- en: Let's now implement the policy and value iteration techniques to solve the problem
    and retrieve the frisbee.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现策略迭代和值迭代技术来解决问题并取回飞盘。
- en: 'Activity 5.01: Implementing Policy and Value Iteration on the FrozenLake-v0
    Environment'
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.01：在 FrozenLake-v0 环境中实现策略迭代和值迭代
- en: 'In this activity, we will solve FrozenLake-v0 using policy and value iteration.
    The goal of the activity is to define a safe path through the frozen lake and
    retrieve the frisbee. The episode ends when the goal is achieved or when the agent
    falls into the hole. The following steps will help you complete the activity:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将通过策略迭代和值迭代来解决 FrozenLake-v0。该活动的目标是定义穿越冰冻湖的安全路径并取回飞盘。当目标达成或代理掉进洞里时，回合结束。以下步骤将帮助你完成此活动：
- en: 'Import the required libraries: `numpy` and `gym`.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：`numpy` 和 `gym`。
- en: Initialize the environment and reset the current one. Set `is_slippery=False`
    in the initializer. Show the size of the action space and the number of possible
    states.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化环境并重置当前环境。在初始化器中设置 `is_slippery=False`。显示动作空间的大小和可能的状态数量。
- en: Perform policy evaluation iterations until the smallest change is less than
    `smallest_change`.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行策略评估迭代，直到最小的变化小于 `smallest_change`。
- en: Perform policy improvement using the Bellman optimality equation.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝尔曼最优性方程进行策略改进。
- en: Find the most optimal policy for the FrozenLake-v0 environment using policy
    iteration.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略迭代找到 FrozenLake-v0 环境的最优策略。
- en: Perform a test pass on the FrozenLake-v0 environment.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 FrozenLake-v0 环境上执行测试。
- en: Take steps through the FrozenLake-v0 environment randomly.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机通过 FrozenLake-v0 环境进行步进。
- en: Perform value iteration to find the most optimal policy for the FrozenLake-v0
    environment. Note that the aim here is to make sure the reward value for each
    action should be one (or close to one) to ensure maximum rewards.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行值迭代，以找到 FrozenLake-v0 环境的最优策略。请注意，这里目标是确保每个行动的奖励值为 1（或接近 1），以确保最大奖励。
- en: 'The output should be similar to the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于以下内容：
- en: '![Figure 5.26: Expected output average score (1.0)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.26：期望输出平均分（1.0）'
- en: '](img/B16182_05_26.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_05_26.jpg)'
- en: 'Figure 5.26: Expected output average score (1.0)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26：期望输出平均分（1.0）
- en: Note
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 711.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 711 页找到。
- en: Thus, with this activity, we have successfully implemented the policy and value
    iteration methods in the FrozenLake-v0 environment.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过这项活动，我们成功地在 FrozenLake-v0 环境中实现了策略迭代和值迭代方法。
- en: With this, we have reached the end of the chapter, and you can now confidently
    implement the techniques learned in this chapter for various environments and
    scenarios.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了本章内容，你现在可以自信地将本章所学的技术应用于各种环境和场景中。
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the two most commonly used techniques to solve
    DP problems. The first method, memoization, also called the top-bottom method
    uses a dictionary (or HashMap-like structure) to store intermediate results in
    a natural (unordered) manner. While the second method, the tabular method, also
    called the bottom-up method, sequentially solves problems from small to large
    and usually saves the result in a matrix-like structure.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了解决动态规划（DP）问题的两种最常用技术。第一种方法是备忘录法，也叫做自顶向下法，它使用字典（或类似 HashMap 的结构）以自然（无序）的方式存储中间结果。第二种方法是表格法，也叫自底向上的方法，它按顺序从小到大解决问题，并通常将结果保存在类似矩阵的结构中。
- en: 'Next, we also looked at how to use DP to solve RL problems using policy and
    value iteration, and how we overcome the disadvantage of policy iteration by using
    the modified Bellman equation. We implemented policy and value iteration in two
    very popular environments: Taxi-v3 and FrozenLake-v0.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还探讨了如何使用动态规划（DP）通过策略和值迭代解决强化学习（RL）问题，以及如何通过使用修改后的贝尔曼方程克服策略迭代的缺点。我们在两个非常流行的环境中实现了策略迭代和值迭代：Taxi-v3
    和 FrozenLake-v0。
- en: In the next chapter, we will be studying Monte Carlo methods, which are used
    to simulate real-world scenarios and are some of the most widely used tools in
    domains such as finance, mechanics, and trading.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习蒙特卡罗方法，它用于模拟现实世界的场景，并且是金融、机械学和交易等领域中最广泛使用的工具之一。
