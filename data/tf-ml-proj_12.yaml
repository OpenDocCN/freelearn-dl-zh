- en: Object Detection at a Large Scale with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行大规模目标检测
- en: The recent breakthroughs in the field of **Artificial Intelligence** (**AI**)
    have brought deep learning to the forefront. Today, even more organizations are
    employing deep learning technologies for analyzing their data, which is often
    voluminous in nature. Hence, it's imperative that deep learning frameworks such
    as TensorFlow can be combined with big data platforms and pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）领域的最新突破使深度学习成为焦点。如今，越来越多的组织正在采用深度学习技术分析其数据，而这些数据通常是庞大的。因此，将深度学习框架如TensorFlow与大数据平台和管道结合变得至关重要。'
- en: The 2017 Facebook paper regarding training ImageNet in one hour using 256 GPUs
    spread over 32 servers ([https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf))
    and a recent paper by Hong Kong Baptist University where they train ImageNet in
    four minutes using 2,048 GPUs ([https://arxiv.org/pdf/1807.11205.pdf](https://arxiv.org/pdf/1807.11205.pdf))
    prove that distributed AI can be a viable solution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年Facebook的论文讨论了如何使用256个GPU在32台服务器上训练ImageNet，仅需一小时([https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf))，以及香港浸会大学最近的论文，他们使用2,048个GPU在四分钟内训练ImageNet([https://arxiv.org/pdf/1807.11205.pdf](https://arxiv.org/pdf/1807.11205.pdf))，这些研究证明了分布式AI是一个可行的解决方案。
- en: The main idea behind distributed AI is that the task can be divided into different
    processing clusters. A large number of frameworks have been proposed for distributed
    AI. We can use either distributed TensorFlow or TensorFlowOnSpark, two popular
    choices for distributed AI. Both have their own sets of pros and cons, as we'll
    learn in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式AI的主要思想是将任务划分到不同的处理集群中。已经提出了大量的框架用于分布式AI。我们可以使用分布式TensorFlow或TensorFlowOnSpark，这两种都是流行的分布式AI选择。我们将在本章中了解它们各自的优缺点。
- en: Applying computationally expensive deep learning applications at a large scale
    can be an enormous challenge. Using TensorFlowOnSpark, we can distribute these
    computationally expensive processes in the cluster, enabling us to perform computations
    at a larger scale.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模应用计算密集型深度学习时，可能面临巨大的挑战。使用TensorFlowOnSpark，我们可以在集群中分布这些计算密集型过程，使我们能够在更大规模上进行计算。
- en: 'In this chapter, we''ll explore Yahoo''s TensorFlowOnSpark framework for distributed
    deep learning on Spark clusters. Then, we''ll apply TensorFlowOnSpark on a large
    scale dataset of images and train the network to detect objects. In this chapter,
    we''ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探索Yahoo的TensorFlowOnSpark框架，用于在Spark集群上进行分布式深度学习。然后，我们将在一个大规模图像数据集上应用TensorFlowOnSpark，并训练网络以检测物体。在这一章中，我们将涵盖以下主题：
- en: The need for distributed AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分布式AI的需求
- en: An introduction to the Apache Spark platform for big data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据平台Apache Spark的介绍
- en: TensorFlowOnSpark – a Python framework to run TensorFlow on Spark clusters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlowOnSpark – 一种在Spark集群上运行TensorFlow的Python框架
- en: Performing object detection using TensorFlowOnSpark and the Sparkdl API
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlowOnSpark和Sparkdl API执行目标检测
- en: 'For big data, Spark is the de facto choice, so we''ll start with an introduction
    to Spark. Then, we''ll explore the two popular choices: distributed TensorFlow,
    and TensorFlowOnSpark.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大数据，Spark是事实上的首选，因此我们将从Spark的介绍开始。然后，我们将探索两种流行的选择：分布式TensorFlow和TensorFlowOnSpark。
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Projects/tree/master/Chapter12](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Projects/tree/master/Chapter12).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Projects/tree/master/Chapter12](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Projects/tree/master/Chapter12)找到。
- en: Introducing Apache Spark
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Apache Spark
- en: If you have worked in big data, there is a high probability that you already
    know what Apache Spark is, and you can skip this section. But if you don't, don't
    worry—we'll go through the basics.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经从事过大数据工作，可能已经知道Apache Spark是什么，可以跳过这一部分。但如果你不知道，别担心——我们会介绍基本概念。
- en: Spark is a powerful, fast, and scalable real-time data analytics engine for
    large scale data processing. It's an open source framework that was developed
    initially by the UC Berkeley AMPLab around the year 2009\. Around 2013, AMPLab
    contributed Spark to the Apache Software Foundation, with Apache Spark Community
    releasing Spark 1.0 in 2014.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个强大、快速、可扩展的实时数据分析引擎，用于大规模数据处理。它是一个开源框架，最初由加利福尼亚大学伯克利分校的 AMPLab 开发，约在
    2009 年。到 2013 年，AMPLab 将 Spark 贡献给了 Apache 软件基金会，Apache Spark 社区在 2014 年发布了 Spark
    1.0。
- en: The community continues to make regular releases and brings new features into
    the project. At the time of writing this book, we have the Apache Spark 2.4.0
    release and active community on GitHub. It's a real-time data analytics engine
    that allows you to distribute programs across a cluster of machines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 社区继续定期发布新版本并为项目带来新特性。在写本书时，我们有 Apache Spark 2.4.0 版本以及活跃的 GitHub 社区。它是一个实时数据分析引擎，允许你将程序分布式执行到多台机器上。
- en: 'The beauty of Spark lays in the fact that it''s **scalable**: it runs on top
    of a cluster manager, allowing you to use the scripts written in Python (Java
    or Scala, too) with minimal change. Spark is made up of many components. At the
    heart, we have the Spark core, which distributes the processing of data and the
    mapping and reducing of large datasets. There are several libraries that run on
    top of it. Here are some of the important components of the Spark API:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的美妙之处在于它是 **可扩展**的：它运行在集群管理器之上，允许你在最小修改的情况下使用用 Python（也可以是 Java 或 Scala）编写的脚本。Spark
    由多个组件构成。核心部分是 Spark 核心，它负责分发数据处理以及大数据集的映射和归约。上面运行着一些库。以下是 Spark API 中的一些重要组件：
- en: '**Resilient Distributed Dataset (RDD)**:RDD is the base element of the Spark
    API. It''s a fault-tolerant collection of elements that can be operated on in
    parallel, which means that the elements in RDD can be accessed and operated upon
    by the workers in the cluster at the same time.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集（RDD）**：RDD 是 Spark API 的基本元素。它是一个容错的元素集合，可以并行操作，这意味着 RDD 中的元素可以被集群中的工作节点同时访问和操作。'
- en: '**Transformations and actions**: On the Spark RDD, we can perform two types
    of operations, transformations and actions. Transformations take RDDs as their
    argument and return another RDD. Actions take an RDD as an argument and return
    the local results. All transformations in Spark are lazy, which means that the
    results are not computed right away. Instead, they are computed only when an action
    requires a result to be returned.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换和操作**：在 Spark 的 RDD 上，我们可以执行两种类型的操作，转换和操作。转换以 RDD 作为参数，并返回另一个 RDD。操作以 RDD
    作为参数，并返回本地结果。Spark 中的所有转换都是懒加载的，这意味着结果不会立即计算，而是只有当操作需要返回结果时，才会进行计算。'
- en: '**DataFrames**: These are very similar to pandas DataFrames. Like pandas, we
    can read from various file formats in the DataFrame (JSON, Parquet, Hive, and
    so on) and perform an operation on the entire DataFrame with single command functions.
    They are distributed across the cluster. Spark uses an engine called Catalyst
    to optimize their usage.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据框（DataFrames）**：这些与 pandas 的数据框非常相似。像 pandas 一样，我们可以从多种文件格式（如 JSON、Parquet、Hive
    等）中读取数据，并使用单个命令对整个数据框执行操作。它们在集群中分布式运行。Spark 使用一个叫做 Catalyst 的引擎来优化它们的使用。'
- en: 'Spark uses a master/worker architecture. It has a master node/process and many
    worker nodes/processes. The driver, SparkContext, is the heart of Spark Application.
    It''s the main entry point and the master of the Spark application. It sets up
    the internal services and establishes a connection with the Spark execution environment.
    The following diagram shows Spark''s architecture:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 使用主/从架构。它有一个主节点/进程和多个工作节点/进程。驱动程序 SparkContext 是 Spark 应用程序的核心。它是 Spark
    应用程序的主要入口点和主控，它设置内部服务并与 Spark 执行环境建立连接。下图展示了 Spark 的架构：
- en: '![](img/81a07ebd-ff60-49e1-b2ed-6af95112f289.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81a07ebd-ff60-49e1-b2ed-6af95112f289.png)'
- en: 'So far, we have provided an introduction to Apache Spark. It''s a big and vast
    subject, and we would recommend readers to refer to the Apache documentation for
    more information: [https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了 Apache Spark。这是一个庞大且广泛的话题，我们建议读者参考 Apache 文档获取更多信息：[https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html)。
- en: Understanding distributed TensorFlow
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布式 TensorFlow
- en: TensorFlow also supports distributed computing, allowing us to partition a graph
    and compute it on different processes. Distributed TensorFlow works like a client-server
    model, or to be more specific, a master-workers model. In TensorFlow, we first
    create a cluster of workers, with one being the master-worker. The master coordinates
    the distribution of tasks to different workers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow还支持分布式计算，允许我们将图拆分并在不同进程上计算。分布式TensorFlow工作方式类似于客户端-服务器模型，或者更具体地说，是主节点-工作节点模型。在TensorFlow中，我们首先创建一个工作节点集群，其中一个节点是主节点。主节点负责协调任务分配到不同的工作节点。
- en: 'The first thing to do when you have to work with many machines (or processors)
    is to define their name and job type, that is, make a cluster of machines (or
    processors). Each machine in the cluster is assigned a unique address (for example,
    `worker0.example.com:2222`), and they have a specific job, such as `type: master`
    (parameter server), or worker. Later, the TensorFlow server assigns a specific
    task to each worker. To create a cluster, we first need to define cluster specification.
    This is a dictionary that maps worker processes and jobs. The following code creates
    a cluster with the job name `work` and two worker processes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '当你需要在多台机器（或处理器）上工作时，首先要做的事情是定义它们的名称和工作类型，也就是构建一个机器（或处理器）集群。集群中的每台机器都会被分配一个唯一地址（例如，`worker0.example.com:2222`），并且它们会有一个特定的工作类型，比如`type:
    master`（参数服务器），或者是工作节点。稍后，TensorFlow服务器会将特定的任务分配给每个工作节点。为了创建集群，我们首先需要定义集群规格。这是一个字典，用于映射工作进程和任务类型。以下代码创建了一个名为`work`的集群，并有两个工作进程：'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we can start the process by using the `Server` class and specifying the
    task and task index. The following code will start the `worker` job on `worker1`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`Server`类并指定任务和任务索引来启动进程。以下代码将在`worker1`上启动`worker`任务：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We''ll need to define a `Server` class for each worker in the cluster. This
    will start all of the workers, making us ready to distribute. To place TensorFlow
    operations on a particular task, we''ll use `tf.device` to specify which tasks
    run on a particular worker. Consider the following code, which distributes the
    task between two workers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为集群中的每个工作节点定义一个`Server`类。这将启动所有工作节点，使我们准备好进行分发。为了将TensorFlow操作分配到特定的任务上，我们将使用`tf.device`来指定哪些任务在哪个工作节点上运行。考虑以下代码，它将在两个工作节点之间分配任务：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code creates two workers on the same machine. In this case, the
    work is divided between the two workers via the `tf.device` function. The variables
    are created on the respective workers; TensorFlow inserts the appropriate data
    transfers between the jobs/workers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在同一台机器上创建了两个工作节点。在这种情况下，工作被通过`tf.device`函数在两个工作节点之间分配。变量在各自的工作节点上创建；TensorFlow在任务/工作节点之间插入适当的数据传输。
- en: 'This is done by creating a `GrpcServer`, which is created with the target, `grpc://localhost:2222`.
    This server knows how to talk to the tasks in the same job via `GrpcChannels`.
    In the following screenshot, you can see the output of the previous code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过创建一个`GrpcServer`来完成的，它通过目标`grpc://localhost:2222`来创建。这个服务器知道如何通过`GrpcChannels`与同一任务中的其他任务进行通信。在下面的截图中，你可以看到前述代码的输出：
- en: '![](img/f35bd435-a9c4-48ed-82b2-197f09604d2e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f35bd435-a9c4-48ed-82b2-197f09604d2e.png)'
- en: The code for this chapter is located in the repository under the `Chapter12/distributed.py` directory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于仓库中的`Chapter12/distributed.py`目录下。
- en: This looked easy, right? But what if we want to extend this to our deep learning
    pipeline?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很简单，对吧？但是如果我们想将其扩展到我们的深度学习流水线中呢？
- en: Deep learning through distributed TensorFlow
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过分布式TensorFlow进行深度学习
- en: At the heart of any deep learning algorithm is the stochastic gradient descent
    optimizer. This is what makes the model learn and, at the same time, makes learning
    computationally expensive. Distributing the computation to different nodes on
    the cluster should reduce the training time. TensorFlow allows us to split the
    computational graph, describes the model to different nodes in the cluster, and
    finally merges the result.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 任何深度学习算法的核心是随机梯度下降优化器。这使得模型能够学习，同时也让学习过程计算开销大。将计算分发到集群中的不同节点应该能减少训练时间。TensorFlow允许我们拆分计算图，将模型描述到集群中的不同节点，最后合并结果。
- en: 'This is achieved in TensorFlow with the help of master nodes, worker nodes,
    and parameter nodes. The actual computation is done by the worker nodes; the computed
    parameters are kept by the parameter nodes and shared with worker nodes. The master
    node is responsible for coordinating the workload among different worker nodes.
    There are two popular approaches that are employed for distributed computing:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点通过主节点、工作节点和参数节点在 TensorFlow 中实现。实际的计算由工作节点执行；计算出的参数由参数节点保存，并与工作节点共享。主节点负责在不同工作节点之间协调工作负载。分布式计算中有两种常用的方法：
- en: '**Synchronous approach**: In this case, the mini-batches are divided among
    the workers. Each worker has a replica of the model and calculates the gradients
    separately for the mini-batches allocated to it. Later, the gradients are combined
    at the master and updates are applied to the parameters at the same time.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步方法**：在这种方法中，工作节点之间分配了小批量数据。每个工作节点都有一个模型副本，并分别计算分配给它的小批量数据的梯度。稍后，梯度在主节点处合并，并同时应用于参数更新。'
- en: '**Asynchronous approach**: Here, the updates to the model parameters are applied
    asynchronously.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步方法**：在这种方法中，模型参数的更新是异步应用的。'
- en: 'These two approaches are shown in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法在下图中展示：
- en: '![](img/2342b586-d056-42f5-8109-ef177e503ca4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2342b586-d056-42f5-8109-ef177e503ca4.png)'
- en: 'Now, let''s look at how we can incorporate distributed TensorFlow in a deep
    learning pipeline. The following code is based upon the following Medium post, [https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3](https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在深度学习管道中集成分布式 TensorFlow。以下代码基于 Medium 上的文章，[https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3](https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3)：
- en: 'Import the necessary modules. Here, we are importing only the necessary ones
    to demonstrate the changes needed to convert existing deep learning code to distributed
    TensorFlow code:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块。在这里，我们仅导入了必要的模块，以演示将现有深度学习代码转换为分布式 TensorFlow 代码所需的更改：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define the cluster. We''ll create it with one master at the address and two
    workers. In our case, the machine we want to make master has an IP address assigned
    to it, that is, `192.168.1.3`, and we specify port `2222`. You can modify them
    with the addresses of your machines:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义集群。我们将其创建为一个主节点，地址为 `192.168.1.3`，并且两个工作节点。我们希望将主节点分配到的机器有一个分配给它的 IP 地址，即
    `192.168.1.3`，并且我们指定端口为 `2222`。你可以根据你机器的地址修改这些设置：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The same code executes on each machine, so we need to parse the command-line
    arguments:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相同的代码会在每台机器上执行，因此我们需要解析命令行参数：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create the TensorFlow server for each worker and the master so that the nodes
    in the cluster can communicate:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个工作节点和主节点创建 TensorFlow 服务器，以便集群中的节点能够进行通信：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Ensure that the variables are allocated on the same worker device. TensorFlow''s `tf.train.replica_device_setter()` function
    helps us to automatically assign devices to `Operation` objects as they are constructed.
    At the same time, we want the parameter server to wait until the server shuts
    down. This is achieved by using the `server.join()` method at the parameter server:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保变量分配在相同的工作设备上。TensorFlow 的 `tf.train.replica_device_setter()` 函数帮助我们在构造 `Operation`
    对象时自动分配设备。同时，我们希望参数服务器在服务器关闭之前等待。这是通过在参数服务器上使用 `server.join()` 方法实现的：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can access this script from GitHub or from the `Chapter12/tensorflow_distributed_dl.py` directory.
    Remember that the same script needs to be executed on each machine in the cluster,
    but with different command-line arguments.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 GitHub 或 `Chapter12/tensorflow_distributed_dl.py` 目录访问此脚本。请记住，相同的脚本需要在集群中的每台机器上执行，但命令行参数不同。
- en: 'The same script now needs to be executed on the parameter server and the four
    workers:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的脚本现在需要在参数服务器和四个工作节点上执行：
- en: 'Use the following code to execute the script on the parameter server (`192.168.1.3:2222`):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码在参数服务器（`192.168.1.3:2222`）上执行脚本：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use the following code to execute the script on worker 0 (`192.168.1.4:2222`):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在 `worker 0`（`192.168.1.4:2222`）上执行脚本：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Use the following code to execute the script on `worker 1` (`192.168.1.5:2222`):'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在 `worker 1`（`192.168.1.5:2222`）上执行脚本：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Use the following code to execute the script on `worker 2` (`192.168.1.6:2222`):'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在 `worker 2`（`192.168.1.6:2222`）上执行脚本：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use the following code to execute the script on `worker 3` (`192.168.1.6:2222`):'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在 `worker 3`（`192.168.1.6:2222`）上执行脚本：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The major disadvantage of distributed TensorFlow is that we need to specify
    the IP addresses and ports of all of the nodes in the cluster at startup. This
    puts a limitation on the scalability of distributed TensorFlow. In the next section,
    you will learn about TensorFlowOnSpark, an API built by Yahoo. It provides a simplified
    API to run deep learning models on the distributed Spark platform.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式TensorFlow的主要缺点是我们需要在启动时指定集群中所有节点的IP地址和端口。这限制了分布式TensorFlow的可扩展性。在下一部分中，您将了解由Yahoo构建的TensorFlowOnSpark
    API。它提供了一个简化的API，用于在分布式Spark平台上运行深度学习模型。
- en: To find out more about distributed TensorFlow, we suggest that you read the
    paper *TensorFlow:* *Large Scale Machine Learning on Heterogeneous Distributed
    Systems* by Google REsearch teamNIPS, 2012 ([http://download.tensorflow.org/paper/whitepaper2015.pdf](http://download.tensorflow.org/paper/whitepaper2015.pdf)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于分布式TensorFlow的内容，建议您阅读Google REsearch团队的论文《*TensorFlow:* *Large Scale
    Machine Learning on Heterogeneous Distributed Systems*》（2012年NIPS）（[http://download.tensorflow.org/paper/whitepaper2015.pdf](http://download.tensorflow.org/paper/whitepaper2015.pdf)）。
- en: Learning about TensorFlowOnSpark
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解TensorFlowOnSpark
- en: In the year 2016, Yahoo open sourced TensorFlowOnSpark, a Python framework for
    performing TensorFlow-based distributed deep learning on Spark clusters. Since
    then, it has undergone a lot of developmental changes and is one of the most active
    repositories regarding the distributed deep learning framework.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，Yahoo开源了TensorFlowOnSpark，这是一个用于在Spark集群上执行基于TensorFlow的分布式深度学习的Python框架。从那时起，它经历了很多开发变化，是分布式深度学习框架中最活跃的开源项目之一。
- en: The **TensorFlowOnSpark** (**TFoS**) framework allows you to run distributed
    TensorFlow applications from within Spark programs. It runs on the existing Spark
    and Hadoop clusters. It can use existing Spark libraries such as SparkSQL or MLlib
    (the Spark machine learning library).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlowOnSpark**（**TFoS**）框架允许您在Spark程序中运行分布式TensorFlow应用。它运行在现有的Spark和Hadoop集群上。它可以使用现有的Spark库，如SparkSQL或MLlib（Spark的机器学习库）。'
- en: TFoS is automatic, so we do not need to define the nodes as PS nodes, nor do
    we need to upload the same code to all of the nodes in the cluster. By just performing
    a few modifications, we can run our existing TensorFlow code. It allows us to
    scale up the existing TensorFlow apps with minimal changes. It supports all of
    the existing TensorFlow functionality such as synchronous/asynchronous training,
    data parallelism, and TensorBoard. Basically, it's a PySpark wrapper for the TensorFlow
    code. It launches distributed TensorFlow clusters using Spark executors. To support
    TensorFlow data ingestion, it adds `feed_dict` and `queue_runner`, allowing direct
    HDFS access from TensorFlow.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TFoS是自动化的，因此我们无需将节点定义为PS节点，也无需将相同的代码上传到集群中的所有节点。只需进行少量修改，我们就可以运行现有的TensorFlow代码。它使我们能够以最小的改动扩展现有的TensorFlow应用。它支持所有现有的TensorFlow功能，如同步/异步训练、数据并行和TensorBoard。基本上，它是TensorFlow代码的PySpark封装。它通过Spark执行器启动分布式TensorFlow集群。为了支持TensorFlow的数据摄取，它添加了`feed_dict`和`queue_runner`，允许直接从TensorFlow访问HDFS。
- en: Understanding the architecture of TensorFlowOnSpark
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TensorFlowOnSpark的架构
- en: 'The following diagram depicts the architecture of TFoS. We can see that TFoS
    does not involve Spark drivers in tensor communication, giving the same scalability
    as standalone TensorFlow clusters:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了TFoS的架构。我们可以看到，TFoS在张量通信中不涉及Spark驱动程序，提供与独立TensorFlow集群相同的可扩展性：
- en: '![](img/cb60c0c1-2ed6-45ef-aaf7-d1f78d082151.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb60c0c1-2ed6-45ef-aaf7-d1f78d082151.png)'
- en: 'TFoS provides two input modes to take in data for training and inference:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TFoS提供了两种输入模式，用于训练和推理时获取数据：
- en: '**Spark RDD**: Spark RDD data is fed to each Spark executor. The executor,
    in turn, feeds the data to the TensorFlow graph using `feed_dict`. However, in
    this mode, TensorFlow worker failures stay hidden from Spark.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark RDD**：Spark RDD数据被传递到每个Spark执行器。执行器将数据通过`feed_dict`传递给TensorFlow图。然而，在这种模式下，TensorFlow工作节点的失败对Spark是隐藏的。'
- en: '**TensorFlow QueueRunners**: Here, the TensorFlow worker runs in the foreground.
    TFoS takes advantage of the TensorFlow file readers and QueueRunners to read data
    directly from HDFS files. TensorFlow worker failures are retired as Spark Tasks,
    and it restores them from the checkpoint.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow QueueRunners**：在这里，TensorFlow工作节点在前台运行。TFoS利用TensorFlow的文件读取器和QueueRunners，直接从HDFS文件读取数据。TensorFlow工作节点的失败会被视为Spark任务，并通过检查点进行恢复。'
- en: Deep delving inside the TFoS API
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨TFoS API
- en: 'The use of TFoS can be divided into three basic steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TFoS可以分为三个基本步骤：
- en: 'Launch the TensorFlow cluster. We can launch the cluster using `TFCluster.run`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动TensorFlow集群。我们可以使用`TFCluster.run`来启动集群：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Feed the data into the TensorFlow app. The data is given for both training
    and inference. To train, we use the `train` method:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据输入TensorFlow应用程序。数据用于训练和推理。为了训练，我们使用`train`方法：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We perform the inference with the help of `cluster.inference(dataRDD)`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`cluster.inference(dataRDD)`来执行推理。
- en: Finally, shut down the TensorFlow cluster with `cluster.shutdown()`.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过`cluster.shutdown()`关闭TensorFlow集群。
- en: We can modify any TensorFlow program to work with TFoS. In the following section,
    we'll look at how we can train a model to recognize handwritten digits using TFoS.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改任何TensorFlow程序以与TFoS一起使用。在接下来的部分中，我们将介绍如何使用TFoS训练一个模型来识别手写数字。
- en: Handwritten digits using TFoS
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TFoS进行手写数字识别
- en: In this section, we'll look at how to convert our TensorFlow code to run on
    TFoS. To do this, first, we need to build an EC2 cluster on Amazon AWS. One of
    the easy ways to do this is to use Flintrock, a CLI tool for launching Apache
    Spark clusters from your local machine.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何将TensorFlow代码转换为在TFoS上运行。为此，首先，我们需要在Amazon AWS上构建一个EC2集群。一个简单的方法是使用Flintrock，这是一个从本地机器启动Apache
    Spark集群的CLI工具。
- en: 'The following are the prerequisites that you''ll need to complete this section:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成本节所需的先决条件：
- en: Hadoop
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop
- en: PySpark
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark
- en: Flintrock
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flintrock
- en: Python
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: TensorFlow
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlowOnSpark
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlowOnSpark
- en: 'Now, let''s see how we can do this. We''re using the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    The following code is taken from the TensorFlowOnSpark GitHub. The repository
    contains the links to documentation and more examples ([https://github.com/yahoo/TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark)):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何实现这一点。我们使用的是MNIST数据集（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）。以下代码来自TensorFlowOnSpark的GitHub仓库。该仓库包含文档链接和更多示例（[https://github.com/yahoo/TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark)）：
- en: 'Define the model architecture and training in the `main(argv, ctx)` function,
    where the `argv` parameter contains the arguments supplied at the command line,
    and `ctx` contains the node metadata such as `job` and `task_idx`. The `cnn_model_fn`
    model function is the CNN model that''s defined as a function:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`main(argv, ctx)`函数中定义模型架构和训练，其中`argv`参数包含命令行传递的参数，而`ctx`包含节点元数据，如`job`和`task_idx`。`cnn_model_fn`模型函数是定义的CNN模型：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the `if  __name__=="__main__"` block, add the following imports:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`if __name__=="__main__"`块中，添加以下导入：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Launch the Spark Driver and initiate the TensorFlowOnSpark cluster:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark Driver并初始化TensorFlowOnSpark集群：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Parse the arguments:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析参数：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Use `TFCluster.run` to manage the cluster:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TFCluster.run`来管理集群：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once the training is over, shut down the cluster:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，关闭集群：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The complete code is available in the GitHub repository in the `Chapter12/mnist_TFoS.py` directory.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在GitHub仓库的`Chapter12/mnist_TFoS.py`目录中找到。
- en: 'To execute the code on the EC2 cluster, you''ll need to submit it to Spark
    cluster using `spark-submit`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要在EC2集群上执行代码，您需要使用`spark-submit`将其提交到Spark集群：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The model learned in 6.6 minutes on the EC2 cluster with two workers:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在EC2集群上用两台工作节点训练了6.6分钟：
- en: '![](img/57532b58-b342-4bf2-9abf-24125871d75f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57532b58-b342-4bf2-9abf-24125871d75f.png)'
- en: We can use TensorBoard to visualize the model architecture. Once we run the
    code successfully, the event file is created and it can be viewed on the TensorBoard.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用TensorBoard来可视化模型架构。一旦代码成功运行，事件文件会被创建，并且可以在TensorBoard中查看。
- en: 'When we visualize loss, we can see that the loss decreases as the network learns:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们可视化损失时，可以看到随着网络学习，损失逐渐减少：
- en: '![](img/a95a92b5-e3a7-4608-8ceb-14f0b745b946.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a95a92b5-e3a7-4608-8ceb-14f0b745b946.png)'
- en: The model provides 75% accuracy on the test data set on only 1,000 steps, with
    a very basic CNN model. We can further optimize the result by using a better model
    architecture and tuning hyperparameters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在测试数据集上只用了1,000步就获得了75%的准确率，且使用了一个非常基础的CNN模型。我们可以通过使用更好的模型架构和调整超参数进一步优化结果。
- en: Object detection using TensorFlowOnSpark and Sparkdl
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlowOnSpark和Sparkdl进行物体检测
- en: Apache Spark has a higher level API Sparkdl for scalable deep learning in Python.
    In this section, we'll use the Sparkdl API. In this section, you will learn how
    to build a model over the pre-trained Inception v3 model to detect cars and buses.
    This technique of using a pre-trained model is called **transfer learning**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了一个高级 API Sparkdl，用于在 Python 中进行可扩展的深度学习。在本节中，我们将使用 Sparkdl API。在本节中，您将学习如何在预训练的
    Inception v3 模型基础上构建一个模型，用于检测汽车和公交车。使用预训练模型的技术称为 **迁移学习**。
- en: Transfer learning
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Learning in humans is a continuous process—whatever we learn today is built
    upon the learning we have had in the past. For example, if you know how to drive
    a bicycle, you can extend the same knowledge to drive a motorcycle, or drive a
    car. The driving rule remains the same—the only thing that changes is the control
    panel and actuators. However, in deep learning, we often start afresh. Is it possible
    to use the knowledge the model has gained in solving a problem in one domain,
    to solve the problem in another related domain?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的学习是一个持续的过程——我们今天学到的知识是建立在过去学习的基础上的。例如，如果你会骑自行车，你可以将相同的知识扩展到骑摩托车或开汽车。驾驶规则是一样的——唯一不同的是控制面板和执行器。然而，在深度学习中，我们通常是从头开始。是否可以利用模型在一个领域解决问题时获得的知识，来解决另一个相关领域中的问题呢？
- en: Yes, it's indeed possible, and it's called transfer learning. Though a lot of
    research is still going on in the field, a great deal of success has been achieved
    in applying transfer learning in the area of computer vision. This is due to the
    fact that for computer vision tasks **Convolutional Neural Networks** (**CNNs**)
    are preferred since they are good in extracting features from the image (features
    such as lines, circles, and squares, at lower layers, and higher abstract features
    such as ears and nose at the higher layers). Hence, the features extracted by
    convolutional layers while learning one type of image dataset can be reused in
    other similar domain images. This can help in reducing the training time.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实可以，这就是迁移学习。虽然该领域仍在进行大量研究，但在计算机视觉领域，迁移学习的应用已取得了巨大的成功。这是因为对于计算机视觉任务，**卷积神经网络**（**CNNs**）被优先使用，因为它们擅长从图像中提取特征（例如，较低层次的特征如线条、圆圈和方块，以及较高层次的抽象特征如耳朵和鼻子）。因此，在学习一种类型的图像数据集时，卷积层提取的特征可以在其他相似领域的图像中重复使用。这有助于减少训练时间。
- en: In this section, we'll use Inception v3 ([https://arxiv.org/pdf/1512.00567v1.pdf](https://arxiv.org/pdf/1512.00567v1.pdf)),
    a state-of-the-art CNN trained on the ImageNet dataset. ImageNet ([http://image-net.org/](http://image-net.org/))
    contains over 14 million labelled high-resolution hand-annotated images that have
    been classified into 22,000 categories. Inception v3 was trained on a subset of
    it consisting of about 1.3 million images with 1,000 categories.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Inception v3（[https://arxiv.org/pdf/1512.00567v1.pdf](https://arxiv.org/pdf/1512.00567v1.pdf)），一个经过训练的最先进的
    CNN，使用的是 ImageNet 数据集。ImageNet（[http://image-net.org/](http://image-net.org/)）包含超过
    1400 万张标注的高分辨率手工标注图像，这些图像被分类为 22,000 个类别。Inception v3 是在其子集上训练的，包含大约 130 万张图像和
    1,000 个类别。
- en: 'In the transfer learning approach, you keep the feature extractor CNN layers
    but replace the classifier layers with a new classifier. This new classifier is
    then trained on the new images. Two approaches are generally followed: either
    we only train the new classifier or we fine-tune the entire network. In the first
    case, we extract the features from our new dataset, called **bottleneck features**,
    by feeding the new dataset into CNN layers. The extracted bottleneck features
    are then used to train the final classifier. In the second case, we train the
    entire network, the original CNN, along with the new classifier on the training
    dataset.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习方法中，您保留特征提取的 CNN 层，但将分类器层替换为新的分类器。然后在新的图像上训练这个新的分类器。一般有两种方法：要么只训练新的分类器，要么微调整个网络。在第一种情况下，我们通过将新数据集输入
    CNN 层来提取 **瓶颈特征**。然后将提取的瓶颈特征用于训练最终的分类器。在第二种情况下，我们在训练数据集上训练整个网络，原始的 CNN 以及新的分类器。
- en: Understanding the Sparkdl interface
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Sparkdl 接口
- en: 'To access Spark functionality in the deep learning pipeline, we need to use
    a Spark driver program. From Spark 2.0.0, we have a single point entry using `SparkSession`.
    The simplest way to do this is by using `builder`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问深度学习管道中的 Spark 功能，我们需要使用 Spark 驱动程序。自 Spark 2.0.0 起，我们有一个单一的入口点，使用 `SparkSession`。最简单的方法是使用
    `builder`：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This can allow us to get an existing session or create a new session. At the
    time of instantiation, we can use the `.config()`, `.master()`, and `.appName()`
    methods to set configuration options, set the Spark master, and set the application
    name.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以获取现有的会话或创建一个新会话。在实例化时，我们可以使用 `.config()`、`.master()` 和 `.appName()` 方法来设置配置选项、设置
    Spark 主节点和设置应用程序名称。
- en: To read and manipulate images, Sparkdl provides the `ImageSchema` class. Out
    of its many methods, we'll be using the `readImages` method to read the directory
    of images. It returns a Spark DataFrame with a single column – `image`, of images.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取和处理图片，Sparkdl 提供了 `ImageSchema` 类。在其众多方法中，我们将使用 `readImages` 方法读取图片目录。它返回一个包含单个列
    `image` 的 Spark DataFrame，其中存储了图片。
- en: We can add or remove column/rows from the Spark DataFrames using transformations.
    The example code in this section uses the `withColumn` transformation to add a
    column named `label` and assign label classes to our dataset. Just like with a
    pandas Dataframe, we can view the rows of the Spark DataFrame with the help of
    the `show()` method. The Spark DataFrames can also be split or combined together.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用转换来添加或删除 Spark DataFrame 中的列/行。本节中的示例代码使用 `withColumn` 转换添加了一个名为 `label`
    的列，并为我们的数据集分配了标签类。就像使用 pandas DataFrame 一样，我们可以通过 `show()` 方法查看 Spark DataFrame
    的行。Spark DataFrame 还可以被拆分或组合在一起。
- en: The Sparkdl API has methods to enable fast transfer learning. It provides the
    `DeepImageFeaturizer` class, which automatically peels the classifier layer from
    the pre-trained model and uses the features (bottleneck features) from the pre-trained
    CNN layers as an input to the new classifier.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkdl API 提供了方法来实现快速的迁移学习。它提供了 `DeepImageFeaturizer` 类，自动剥离预训练模型中的分类器层，并使用预训练
    CNN 层的特征（瓶颈特征）作为新分类器的输入。
- en: One advantage of working with Sparkdl is that we can access all of the Spark
    APIs—even its machine learning API MLlib from the same `SparkSession` instance.
    Using MLlib, we can easily combine multiple algorithms into a single a pipeline. The
    Spark machine learning API MLlib also provides support for various classification
    and regression methods.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Sparkdl 的一个优势是我们可以通过相同的 `SparkSession` 实例访问所有 Spark API，甚至包括其机器学习 API MLlib。通过
    MLlib，我们可以轻松地将多个算法结合成一个管道。Spark 机器学习 API MLlib 还提供对各种分类和回归方法的支持。
- en: Building an object detection model
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个物体检测模型
- en: 'We''ll now make some code by using TFoS and Sparkdl. The dataset consists of
    images of buses and cars that have been curated from a Google image search. The
    aim is to train a model so that it can differentiate between cars and buses. The following is
    a list of prerequisites that you will need for this code to work:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 TFoS 和 Sparkdl 编写一些代码。数据集由从 Google 图片搜索中整理出来的公交车和汽车图片组成。目标是训练一个模型，使其能够区分汽车和公交车。以下是您需要的先决条件，以确保此代码能够运行：
- en: PySpark
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark
- en: Python
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: TensorFlow
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlowOnSpark
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlowOnSpark
- en: Pillow
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pillow
- en: Keras
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras
- en: TensorFrames
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFrames
- en: Wrapt
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wrapt
- en: pandas
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: FindSpark
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FindSpark
- en: py4j
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: py4j
- en: 'First, let''s explore our dataset. Inception v3 was trained on ImageNet data
    with 1,000 categories. These included images of various vehicles as well. We have
    49 images for buses and 41 images of cars. Here, you can see the sample images
    from the dataset:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们查看一下我们的数据集。Inception v3 在包含 1,000 类别的 ImageNet 数据上进行了训练，其中也包含了各种车辆的图片。我们有
    49 张公交车的图片和 41 张汽车的图片。这里，您可以看到数据集中的一些示例图片：
- en: '![](img/d1e926c3-d5ac-4d48-834b-a05f458bcda5.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1e926c3-d5ac-4d48-834b-a05f458bcda5.png)'
- en: '![](img/f7acc5e6-ee73-440b-b004-434ac416c2a2.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7acc5e6-ee73-440b-b004-434ac416c2a2.png)'
- en: 'Now, let''s build the code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建代码：
- en: 'This time, we''ll not be using `spark-submit`. Instead, we''ll run the code
    like any standard Python code. Therefore, we''ll define the location of spark
    driver and the Spark deep learning package in the code itself and create a Spark
    session using PySpark''s `SparkSession` builder. One thing to keep in mind here
    is the memory allocated to the heap: Spark executor and Spark driver. The values
    should be based on your machine''s specifications:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次，我们不使用 `spark-submit`。相反，我们像运行任何标准的 Python 代码一样运行代码。因此，我们将在代码中定义 Spark 驱动程序的位置和
    Spark 深度学习包，并使用 PySpark 的 `SparkSession` 构建器创建一个 Spark 会话。这里需要记住的一点是分配给堆的内存：Spark
    执行器和 Spark 驱动程序。这个值应该基于您机器的规格：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The images are loaded in the Spark DataFrame using PySpark''s `ImageSchema` class.
    The bus and cars images are loaded in different Spark DataFrames:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图片通过 PySpark 的 `ImageSchema` 类加载到 Spark DataFrame 中。公交车和汽车的图片分别加载到不同的 Spark
    DataFrame 中：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can see the top five rows of the Spark DataFrame here:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在这里看到Spark DataFrame的前五行：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出结果如下：
- en: '![](img/70221402-30db-4f84-8719-ac9fb0414f7a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70221402-30db-4f84-8719-ac9fb0414f7a.png)'
- en: 'We split the dataset into the training-test datasets, with a ratio of 60% training
    and 40% test. Remember that these values are random and you can vary them accordingly:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集划分为训练集和测试集，比例为60%的训练集和40%的测试集。请记住，这些值是随机的，你可以根据需要进行调整：
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The training dataset for buses and cars is combined. The same is done for the
    test dataset:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 公交车和汽车的训练数据集已合并。测试数据集也进行了相同的处理：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We use the Sparkdl API to get the pre-trained Inception v3 model and on top
    of the CNN layers of Inception, we add a logistic regressor. Now, we''ll train
    the model on our dataset:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Sparkdl API获取预训练的Inception v3模型，并在Inception的CNN层上添加了一个逻辑回归器。现在，我们将在我们的数据集上训练这个模型：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s see how the trained model fairs on the test dataset. Let''s use a perfect
    confusion matrix:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看训练好的模型在测试数据集上的表现。我们使用完美的混淆矩阵来进行评估：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出结果如下：
- en: '![](img/9ebcccea-cfec-43ab-910f-ec6742d7112a.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ebcccea-cfec-43ab-910f-ec6742d7112a.png)'
- en: 'For the test dataset, the model gives 100% accuracy:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于测试数据集，模型的准确率达到了100%：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our model is giving such a good performance because the Inception v3 model that
    we have used as the base model for transfer learning has already been trained
    on a lot of vehicle images. A word of caution, however—100% accuracy doesn't mean
    it's the best model, just that it does well on the present test images.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型表现如此出色，是因为我们作为迁移学习基础模型使用的Inception v3模型已经在大量的车辆图像上进行了训练。然而，需要提醒的是，100%的准确率并不意味着这是最好的模型，只是说明它在当前测试图像上表现良好。
- en: 'Developed by DataBricks, Sparkdl is part of the Deep Learning Pipelines. They
    provide high-level APIs for scalable deep learning in Python with Apache Spark.
    You can learn more about its features and how to use it here: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkdl是由DataBricks开发的，属于深度学习管道的一部分。它提供了用于Python中基于Apache Spark的可扩展深度学习的高级API。你可以在这里了解更多关于它的功能及如何使用：[https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Deep learning models provide better performance when the training dataset is
    large (big data). Training models for big data is computationally expensive. This
    problem can be handled using the divide and conquer approach: we divide the extensive
    computation part to many machines in a cluster, in other words, distributed AI.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在训练数据集较大时（大数据）能提供更好的性能。训练大数据模型在计算上开销很大。这个问题可以通过分治法来解决：我们将大量的计算任务分配给集群中的多台机器，换句话说，就是分布式人工智能。
- en: One way of achieving this is by using Google's distributed TensorFlow, the API
    that helps in distributing the model training among different worker machines
    in the cluster. You need to specify the address of each worker machine and the
    parameter server. This makes the task of scaling the model difficult and cumbersome.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一种方法是使用Google的分布式TensorFlow，这是一个帮助将模型训练分配到集群中不同工作机器的API。你需要指定每台工作机器和参数服务器的地址。这使得模型扩展变得困难且繁琐。
- en: This problem can be solved by using the TensorFlowOnSpark API. By making minimal
    changes to the preexisting TensorFlow code, we can make it run on the cluster.
    The Spark framework handles the distribution among executor machines and the master,
    shielding the user from the details and giving better scalability.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过使用TensorFlowOnSpark API来解决。通过对已有的TensorFlow代码进行最小的修改，我们可以使其在集群上运行。Spark框架处理执行器机器和主节点之间的分配，避免了用户关注细节，同时提供了更好的可扩展性。
- en: In this chapter, the TensorFlowOnSpark API was used to train a model to recognize
    handwritten digits. This solved the problem of scalability, but we still had to
    process data so that it's available in the right format for training. Unless you
    are well-versed with the Spark infrastructure, especially Hadoop, this can be
    a difficult task.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，使用了TensorFlowOnSpark API来训练一个模型，以识别手写数字。这解决了可扩展性的问题，但我们仍然需要处理数据，以确保它以正确的格式提供给训练。除非你对Spark基础设施，尤其是Hadoop，十分熟悉，否则这可能是一个困难的任务。
- en: To ease the difficulty, we can make use of another API, Sparkdl, which provides
    the complete deep learning pipeline on Spark for training using Spark DataFrames.
    Finally, this chapter used the Sparkdl API for object detection. A model was built
    over the pre-trained Inception v3 model to classify images of buses and cars.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低难度，我们可以利用另一个API，Sparkdl，它提供了基于Spark的完整深度学习训练管道，使用Spark DataFrames进行训练。最后，本章使用了Sparkdl
    API进行目标检测。一个模型在预训练的Inception v3模型基础上构建，用于分类公共汽车和汽车的图像。
- en: In the next chapter, you will learn how to generate book scripts using RNN.
    Who knows—it may win the Booker Prize!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用RNN生成书籍脚本。谁知道呢——它也许会赢得布克奖！
