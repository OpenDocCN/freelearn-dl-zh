- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Univariate Time Series Forecasting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单变量时间序列预测
- en: In this chapter, we’ll develop deep learning models to tackle univariate time
    series forecasting problems. We’ll touch on several aspects of time series preprocessing,
    such as preparing a time series for supervised learning and dealing with conditions
    such as trend or seasonality.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开发深度学习模型来解决单变量时间序列预测问题。我们将涉及时间序列预处理的几个方面，例如为监督学习准备时间序列以及处理趋势或季节性等情况。
- en: We’ll cover different types of models, including simple baselines such as the
    naïve or historical mean method. We’ll provide a brief background on a popular
    forecasting technique, **autoregressive integrated moving average** (**ARIMA**).
    Then, we’ll explain how to create a forecasting model using different types of
    deep learning methods. These include feedforward neural networks, **long short-term
    memory** (**LSTM**), **gated recurrent units** (**GRU**), Stacked **LSTM**, and
    **convolutional neural networks** (**CNNs**). You will also learn how to deal
    with common problems that arise in time series modeling; for example, how to deal
    with trend using first differences, and how to stabilize the variance using a
    logarithm transformation. By the end of this chapter, you will be able to solve
    a univariate time series forecasting problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖不同类型的模型，包括简单的基准模型，如天真预测或历史均值法。我们还将简要介绍一种流行的预测技术——**自回归积分滑动平均**（**ARIMA**）。接下来，我们将解释如何使用不同类型的深度学习方法创建预测模型。这些方法包括前馈神经网络、**长短期记忆**（**LSTM**）、**门控循环单元**（**GRU**）、堆叠**LSTM**和**卷积神经网络**（**CNNs**）。你还将学习如何解决时间序列建模中常见的问题，例如如何使用一阶差分处理趋势，如何通过对数变换稳定方差。通过本章的学习，你将能够解决单变量时间序列预测问题。
- en: 'This chapter will guide you through the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将引导你完成以下方案：
- en: Building simple forecasting models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建简单的预测模型
- en: Univariate forecasting with ARIMA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ARIMA 进行单变量预测
- en: Preparing a time series for supervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为监督学习准备时间序列
- en: Univariate forecasting with a feedforward neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前馈神经网络进行单变量预测
- en: Univariate forecasting with an LSTM
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LSTM 进行单变量预测
- en: Univariate forecasting with a GRU
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GRU 进行单变量预测
- en: Univariate forecasting with a Stacked LSTM
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用堆叠 LSTM 进行单变量预测
- en: Combining an LSTM with multiple fully connected layers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 LSTM 与多个全连接层结合
- en: Univariate forecasting with a CNN
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行单变量预测
- en: Handling trend – taking first differences
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理趋势 – 采用一阶差分
- en: Handling seasonality – seasonal dummies and a Fourier series
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理季节性 – 季节虚拟变量和傅里叶级数
- en: Handling seasonality – seasonal differencing
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理季节性 – 季节差分
- en: Handling seasonality – seasonal decomposition
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理季节性 – 季节分解
- en: Handling non-constant variance – log transformation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理非恒定方差 – 对数变换
- en: Technical requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before diving into univariate time series forecasting problems, we need to
    ensure that we have the appropriate software and libraries installed on our system.
    Here, we’ll go over the main technical requirements for implementing the procedures
    described in this chapter:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨单变量时间序列预测问题之前，我们需要确保系统中已安装适当的软件和库。这里，我们将讨论实现本章中描述的程序的主要技术要求：
- en: We will primarily need Python 3.9 or a later version, `pip` or Anaconda, PyTorch,
    and CUDA (optional). You can check the *Installing PyTorch* recipe from the previous
    chapter for more information on these.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们主要需要 Python 3.9 或更高版本，`pip` 或 Anaconda，PyTorch 和 CUDA（可选）。你可以参考上一章的 *安装 PyTorch*
    方案，了解有关这些工具的更多信息。
- en: 'NumPy (1.26.3) and pandas (2.1.4): Both these `Python` libraries provide several
    methods for data manipulation and analysis.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy (1.26.3) 和 pandas (2.1.4)：这两个 `Python` 库提供了多种数据处理和分析方法。
- en: '`statsmodels` (0.14.1): This library implements several statistical methods,
    including a few useful time series analysis techniques.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsmodels` (0.14.1)：该库实现了几种统计方法，包括一些有用的时间序列分析技术。'
- en: '`scikit-learn` (1.4.0): `scikit-learn` is a popular `Python` library for statistical
    learning. It contains several methods to solve different tasks, such as classification,
    regression, and clustering.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn` (1.4.0)：`scikit-learn` 是一个流行的 `Python` 库，用于统计学习。它包含了几种解决不同任务的方法，例如分类、回归和聚类。'
- en: '`sktime` (0.26.0): A Python library that provides a framework for tackling
    several problems involving time series.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sktime` (0.26.0)：一个 Python 库，提供了一个框架来处理涉及时间序列的多个问题。'
- en: 'You can install these libraries using `pip`, Python’s package manager. For
    example, to install `scikit-learn`, you would run the following code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `pip`，Python 的包管理器，来安装这些库。例如，要安装 `scikit-learn`，你可以运行以下代码：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub URL 中找到：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。
- en: Building simple forecasting models
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建简单的预测模型
- en: 'Before diving into more complex methods, let’s get started with some simple
    forecasting models: the naive, seasonal naive, and mean models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨更复杂的方法之前，让我们从一些简单的预测模型开始：朴素模型、季节性朴素模型和均值模型。
- en: Getting ready
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this chapter, we focus on forecasting problems involving univariate time
    series. Let’s start by loading one of the datasets we explored in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于涉及单变量时间序列的预测问题。让我们从加载我们在[*第1章*](B21145_01.xhtml#_idTextAnchor019)中探索的一个数据集开始：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, `series` is a `pandas Series` object that contains the
    univariate time series.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`series` 是一个 `pandas Series` 对象，包含单变量时间序列。
- en: How to do it…
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We can now forecast our time series using the three following methods:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下三种方法来预测我们的时间序列：
- en: '`Python`, it could be implemented as simply as the following:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Python` 中可以像下面这样简单实现：'
- en: '[PRE2]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`m`, in `Python`, this can be done as follows:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m`，在 `Python` 中，可以这样实现：'
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Python` as follows:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Python`如下：'
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These three methods are useful baselines to benchmark the performance of other,
    more complex, forecasting solutions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法是有用的基准，可以用来评估其他更复杂预测解决方案的表现。
- en: How it works…
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Each of these simple models makes an assumption about the time series data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的模型对时间序列数据做了一些假设：
- en: The naive model assumes that the series is random and that each observation
    is independent of the previous ones
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素模型假设序列是随机的，每个观测值与前一个观测值是独立的。
- en: The seasonal naive model adds a little complexity by recognizing patterns at
    fixed intervals or “seasons”
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 季节性朴素模型通过识别固定间隔或“季节”的模式，增加了一些复杂性。
- en: The mean model assumes that the series oscillates around a constant mean, and
    future values will regress to this mean
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值模型假设序列围绕一个常数均值波动，未来的值将回归到该均值。
- en: There’s more…
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'While these simple models might seem overly basic, they serve two critical
    purposes:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些简单模型看起来过于基础，但它们有两个关键作用：
- en: '**Baselines**: Simple models such as these are often used as baselines for
    more sophisticated models. If a complex model cannot outperform these simple methods,
    it suggests that the complex model might be flawed or that the time series data
    does not contain predictable patterns.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准**：像这样的简单模型通常用作更复杂模型的基准。如果复杂模型无法超过这些简单方法的表现，那么就说明复杂模型可能存在问题，或者时间序列数据没有可预测的模式。'
- en: '**Understanding data**: These models can also help us understand our data.
    If time series data can be well forecasted by a naive or mean model, it suggests
    that the data may be random or fluctuate around a constant mean.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解数据**：这些模型还可以帮助我们理解数据。如果时间序列数据能够通过朴素模型或均值模型进行良好的预测，那么这表明数据可能是随机的，或围绕一个常数均值波动。'
- en: Implementing simple forecasting models such as naive, seasonal naive, and historical
    mean models can be quite straightforward, but it may be beneficial to leverage
    existing libraries that provide off-the-shelf implementations of these models.
    These libraries not only simplify the implementation but also often come with
    additional features such as built-in model validation, optimization, and other
    utilities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实现简单的预测模型，如朴素模型、季节性朴素模型和历史均值模型，可能非常简单，但利用现有的库来提供这些模型的现成实现可能会更有益。这些库不仅简化了实现过程，而且通常还提供了额外的功能，如内置模型验证、优化和其他实用工具。
- en: 'Here are two examples of libraries that provide these models:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个提供这些模型的库的示例：
- en: 'GluonTS: GluonTS is a `Python` library focused on probabilistic models for
    time series. Among other models, there is an implementation of the seasonal naive
    model, which can be found at the following link: [https://ts.gluon.ai/dev/api/gluonts/gluonts.model.seasonal_naive.html](https://ts.gluon.ai/dev/api/gluonts/gluonts.model.seasonal_naive.html).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GluonTS：GluonTS 是一个专注于时间序列概率模型的 `Python` 库。除了其他模型外，它还实现了季节性朴素模型，可以在以下链接找到：[https://ts.gluon.ai/dev/api/gluonts/gluonts.model.seasonal_naive.html](https://ts.gluon.ai/dev/api/gluonts/gluonts.model.seasonal_naive.html)。
- en: '`sktime`: This library provides a framework to develop different types of models
    with time series data. This includes a `NaiveForecaster``()` method, which implements
    several baselines. You can read more about this method at the following URL: [https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sktime`：这个库提供了一个框架，用于开发基于时间序列数据的不同类型模型。它包括一个 `NaiveForecaster` 方法，实现了几种基准模型。你可以在以下网址阅读更多关于此方法的信息：[https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html)。'
- en: 'PyTorch Forecasting: This library focuses on developing state-of-the-art time
    series forecasting models with neural networks for both real-world cases and research.
    PyTorch Forecasting provides a baseline model class, which uses the last known
    target value as the prediction. This class can be found at the following link:
    [https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.baseline.Baseline.html](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.baseline.Baseline.html).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Forecasting：这个库专注于使用神经网络开发最先进的时间序列预测模型，适用于实际应用和研究。PyTorch Forecasting
    提供了一个基准模型类，使用最后已知的目标值作为预测。这个类可以在以下链接找到：[https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.baseline.Baseline.html](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.baseline.Baseline.html)。
- en: The preceding libraries can be a great starting point when you are working on
    a forecasting task. They not only provide implementations of simple forecasting
    models but also contain many other sophisticated models and utilities that can
    help streamline the process of developing and validating time series forecasting
    models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述库在进行预测任务时可以作为一个很好的起点。它们不仅提供了简单预测模型的实现，还包含了许多其他复杂的模型和工具，可以帮助简化开发和验证时间序列预测模型的过程。
- en: In the next recipes, we will see how these assumptions can be relaxed or extended
    to build more complex models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的教程中，我们将看到如何放宽或扩展这些假设，以构建更复杂的模型。
- en: Univariate forecasting with ARIMA
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ARIMA 进行单变量预测
- en: 'ARIMA is a univariate time series forecasting method based on two components:
    an autoregression part and a moving average part. In autoregression, a **lag**
    refers to a previous point or points in the time series data that are used to
    predict future values. For instance, if we’re using a lag of one, we’d use the
    value observed in the previous time step to model a given observation. The moving
    average part uses past errors to model the future observations of the time series.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ARIMA 是一种基于两个组成部分的单变量时间序列预测方法：自回归部分和移动平均部分。在自回归中，**滞后**指的是时间序列数据中用于预测未来值的先前数据点。例如，如果我们使用一个滞后为
    1，那么我们将使用前一个时间步的值来建模当前观测值。移动平均部分则使用过去的误差来建模时间序列的未来观测值。
- en: Getting ready
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To work with the ARIMA model, you’ll need to install the `statsmodels` Python
    package if it’s not already installed. You can install it using `pip`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 ARIMA 模型，如果尚未安装 `statsmodels` Python 包，则需要安装它。你可以使用 `pip` 安装：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For this recipe, we’ll use the same dataset as in the previous recipe.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将使用与前一个教程相同的数据集。
- en: How to do it…
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In Python, you can use the ARIMA model from the `statsmodels` library. Here’s
    a basic example of how to fit an ARIMA model:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，你可以使用 `statsmodels` 库中的 ARIMA 模型。以下是如何拟合 ARIMA 模型的一个基本示例：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How it works…
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'ARIMA models explain a time series based on its past values. They combine aspects
    of **autoregressive** (**AR**) models, **integrated** (**I**) models, and **moving
    average** (**MA**) models:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ARIMA 模型基于时间序列的过去值来解释时间序列。它们结合了 **自回归**（**AR**）模型、**积分**（**I**）模型和 **移动平均**（**MA**）模型的特点：
- en: The AR part involves a regression where the next value of the time series is
    modeled based on the previous `p` lags.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AR 部分涉及回归，其中时间序列的下一个值是基于前 `p` 个滞后值来建模的。
- en: ARIMA is defined for stationary data, so it may be necessary to preprocess the
    data before modeling. This is done by the I part, which represents the number
    of differencing operations (`d`) required to make the series stationary.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARIMA 是为平稳数据定义的，因此可能需要在建模前对数据进行预处理。这是通过 I 部分完成的，I 部分表示使序列平稳所需的差分操作次数（`d`）。
- en: The MA component is another regression where the next value of the series is
    modeled based on the past `q` errors.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MA 组件是另一种回归模型，其中系列的下一个值是基于过去 `q` 个误差来建模的。
- en: The order of these operations is represented as a tuple (`p, d, q`). The best
    combination depends on the input data. In this example, we used a (`1, 1, 1`)
    order as an example.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作的顺序表示为一个元组（`p, d, q`）。最佳组合取决于输入数据。在这个例子中，我们使用了（`1, 1, 1`）作为示例。
- en: A prediction is made for the next six observations into the future using the
    `model_fit.predict()` function. The start and end indices for the prediction are
    set to `0` and `5`, respectively. The `typ='levels'` parameter is used to return
    the predicted values directly rather than differenced values.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `model_fit.predict()` 函数对未来六个观察值进行预测。预测的起始和结束索引分别设置为 `0` 和 `5`。`typ='levels'`
    参数用于直接返回预测值，而不是差分后的值。
- en: There’s more…
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Determining the ARIMA model’s correct order (`p, d, q`) can be challenging.
    This often involves checking the `2` measures the correlation between a time series
    and its values in two time periods in the past. On the other hand, the PACF measures
    autocorrelation while controlling for previous lags. This means a PACF at lag
    `2` measures the correlation between a series and its values two time periods
    ago but with the linear dependence of the one time period lag removed. You can
    learn more about this at the following URL: [https://otexts.com/fpp3/acf.html](https://otexts.com/fpp3/acf.html).
    By examining the ACF and PACF plots, we can better understand the underlying patterns
    of a time series and thus make more accurate predictions.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 确定 ARIMA 模型的正确顺序（`p, d, q`）可能具有挑战性。这通常涉及检查 `2` 测量时间序列与其在过去两个时间周期的值之间的相关性。另一方面，PACF
    测量自相关，同时控制前一个滞后。这意味着，PACF 在滞后 `2` 时，测量的是系列与其两个时间周期前的值之间的相关性，但去除了一个时间周期滞后的线性依赖。你可以通过以下网址了解更多：[https://otexts.com/fpp3/acf.html](https://otexts.com/fpp3/acf.html)。通过检查
    ACF 和 PACF 图，我们可以更好地理解时间序列的潜在模式，从而做出更准确的预测。
- en: Also, the ARIMA model assumes that the time series is stationary, which might
    not always be true. Thus, it may be necessary to use transformations such as differencing
    or the log to make the time series stationary before fitting the ARIMA model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ARIMA 模型假设时间序列是平稳的，但这并不总是正确的。因此，可能需要使用如差分或对数等转换方法来使时间序列平稳，从而适应 ARIMA 模型。
- en: The **seasonal ARIMA** model is commonly used for non-stationary time series
    with a seasonal component. This model adds a set of parameters to model the seasonal
    components of the time series specifically.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**季节性 ARIMA** 模型通常用于具有季节性成分的非平稳时间序列。该模型添加了一组参数，专门用来建模时间序列的季节性成分。'
- en: 'Note that there are automated ways to tune the parameters of ARIMA. One popular
    approach is to use the `pmdarima` library’s `auto_arima()` function. Another useful
    implementation is the one available in the `statsforecast` package. You can check
    it out at the following URL: [https://nixtlaverse.nixtla.io/statsforecast/index.html](https://nixtlaverse.nixtla.io/statsforecast/index.html).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，调节 ARIMA 参数有自动化的方法。一种常见的方法是使用 `pmdarima` 库中的 `auto_arima()` 函数。另一个有用的实现是
    `statsforecast` 包中提供的实现。你可以通过以下网址了解更多：[https://nixtlaverse.nixtla.io/statsforecast/index.html](https://nixtlaverse.nixtla.io/statsforecast/index.html)。
- en: Besides ARIMA, you can also explore exponential smoothing methods, which is
    another popular classical approach to forecasting. The implementation of exponential
    smoothing approaches is also available in `statsmodels` or `statsforecast`, for
    example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 ARIMA，你还可以探索指数平滑方法，这是另一种流行的经典预测方法。指数平滑方法的实现也可以在 `statsmodels` 或 `statsforecast`
    中找到，例如。
- en: Preparing a time series for supervised learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为监督学习准备时间序列
- en: In this recipe, we turn our attention to machine learning approaches to forecasting.
    We start by describing the process of transforming a time series from a sequence
    of values into a format suitable for supervised learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇教程中，我们将重点介绍机器学习方法用于预测。我们首先描述将时间序列从一系列值转换为适合监督学习格式的过程。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Supervised learning involves a dataset with explanatory variables (input) and
    a target variable (output). A time series comprises a sequence of values with
    an associated timestamp. Therefore, we need to restructure the time series for
    supervised learning. A common approach to do this is using a sliding window. Each
    value of the series is based on the recent past values before it (also called
    lags).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习涉及一个包含解释变量（输入）和目标变量（输出）数据集。时间序列由一系列带有时间戳的值组成。因此，我们需要重构时间序列，以适应监督学习。常见的做法是使用滑动窗口。序列中的每个值基于其之前的最近值（也称为滞后值）。
- en: 'To prepare for this section, you need to have your time series data available
    in a `pandas` DataFrame and have the `pandas` and NumPy libraries installed. If
    not, you can install them using `pip`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备这一部分时，您需要确保时间序列数据可用，并以`pandas` DataFrame的形式存在，同时安装了`pandas`和NumPy库。如果没有，您可以使用`pip`进行安装：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We also load the univariate time series into the Python session:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将单变量时间序列加载到Python会话中：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How to do it…
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'The following Python function takes a univariate time series and the window
    size as input and returns the input (`X`) and output (`y`) for a supervised learning
    problem:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python函数接受单变量时间序列和窗口大小作为输入，并返回监督学习问题的输入（`X`）和输出（`y`）：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `series_to_supervised``()` function is the heart of this script, which
    takes in four arguments: the time series data, the number of lag observations
    (`n_in`), the number of observations as output (`n_out`), and whether to drop
    rows with `NaN` values (`dropnan`):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`series_to_supervised`函数是此脚本的核心，它接受四个参数：时间序列数据、滞后观测值的数量（`n_in`）、作为输出的观测值数量（`n_out`），以及是否删除包含`NaN`值的行（`dropnan`）：'
- en: The function begins by checking the data type and preparing an empty list for
    columns (`cols`) and their names (`names`). It then creates the input sequence
    (`t-n, ..., t-1`) by shifting the DataFrame and appending these columns to `cols`,
    and the corresponding column names to `names`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数首先检查数据类型，并为列（`cols`）及其名称（`names`）准备一个空列表。然后，它通过平移DataFrame并将这些列附加到`cols`中，以及将相应的列名附加到`names`中，创建输入序列（`t-n,
    ..., t-1`）。
- en: The function continues to create the forecast sequence `t, t+1 ..., t+n` similarly,
    again appending these to `cols` and `names`. Then, it aggregates all columns into
    a new DataFrame (`agg`), assigns the column `names`, and optionally drops rows
    with `NaN` values.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数继续以相似的方式创建预测序列`t, t+1 ..., t+n`，并将其附加到`cols`和`names`中。然后，它将所有列聚合到一个新的DataFrame（`agg`）中，分配列名，并可选择删除包含`NaN`值的行。
- en: The script then loads a time series dataset about solar radiation (`time_series_solar.csv`)
    into a DataFrame (`df`), extracts the `Incoming Solar` column into a NumPy array
    (`values`), and transforms this array into a supervised learning dataset with
    three lag observations using the `series_to_supervised``()` function.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，脚本加载有关太阳辐射的时间序列数据集（`time_series_solar.csv`）到一个DataFrame（`df`）中，提取`Incoming
    Solar`列到一个NumPy数组（`values`），并使用`series_to_supervised`函数将该数组转换为具有三个滞后观测值的监督学习数据集。
- en: Finally, it prints the transformed data, which consists of sequences of lagged
    observations as input and the corresponding future observations as output. This
    format is ready for any supervised learning algorithm.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它会打印转换后的数据，其中包含作为输入的滞后观测序列，以及作为输出的相应未来观测值。该格式已准备好用于任何监督学习算法。
- en: How it works…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In supervised learning, the goal is to train a model to learn the relationship
    between input variables and a target variable. Nevertheless, this type of structure
    is not immediately available when dealing with time series data. The data is typically
    a sequence of observations (for example, temperature and stock prices) made over
    time. Thus, we must transform this time series data into a suitable format for
    supervised learning. This is what the `series_to_supervised``()` function does.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，目标是训练一个模型，以学习输入变量与目标变量之间的关系。然而，在处理时间序列数据时，这种结构并不直接可用。数据通常是随时间变化的观测序列（例如，温度和股价）。因此，我们必须将时间序列数据转换为适合监督学习的格式。这正是`series_to_supervised`函数的作用。
- en: The transformation process involves creating lagged versions of the original
    time series data using a sliding window approach. This is done by shifting the
    time series data by a certain number of steps (denoted by `n_in` in the code)
    to create the input features. These lagged observations serve as the explanatory
    variables (input), with the idea that past values influence future ones in many
    real-world time series.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 转化过程涉及使用滑动窗口方法创建原始时间序列数据的滞后版本。这是通过将时间序列数据按一定步数（在代码中由`n_in`表示）进行平移来创建输入特征。这些滞后观察值作为解释变量（输入），其思想是过去的值会影响许多现实世界时间序列的未来值。
- en: The target variable (output) is created by shifting the time series in the opposite
    direction by a number of steps (forecasting horizon) denoted by `n_out`. This
    means that for each input sequence, we have the corresponding future values that
    the model should predict.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量（输出）通过将时间序列按相反方向平移一定步数（预测跨度），由`n_out`表示。这意味着，对于每个输入序列，我们有相应的未来值，模型需要预测这些值。
- en: 'For example, suppose we were to prepare a univariate time series for a simple
    forecasting task using a sliding window of size `3`. In that case, we might transform
    the series `[1, 2, 3, 4, 5, 6]` into the following supervised learning dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们准备使用大小为`3`的滑动窗口来为一个简单的预测任务准备单变量时间序列。在这种情况下，我们可以将序列`[1, 2, 3, 4, 5, 6]`转化为以下监督学习数据集：
- en: '| **input (t-3,** **t-2, t-1)** | **output (t)** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **输入 (t-3,** **t-2, t-1)** | **输出 (t)** |'
- en: '| --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1, 2, 3 | 4 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 1, 2, 3 | 4 |'
- en: '| 2, 3, 4 | 5 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2, 3, 4 | 5 |'
- en: '| 3, 4, 5 | 6 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 3, 4, 5 | 6 |'
- en: 'Table 3.1: Example of transforming a time series into a supervised learning
    dataset'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1：将时间序列转化为监督学习数据集的示例
- en: The `series_to_supervised``()` function takes as input a sequence of observations,
    `n_in`, which specifies the number of lag observations as input, `n_out`, which
    specifies the number of observations as output, and a boolean argument `dropnan`
    to remove rows with `NaN` values. It returns a DataFrame suitable for supervised
    learning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`series_to_supervised()`函数接收一系列观察值作为输入，`n_in`指定作为输入的滞后观察值数量，`n_out`指定作为输出的观察值数量，还有一个布尔参数`dropnan`用于去除包含`NaN`值的行。它返回一个适用于监督学习的DataFrame。'
- en: The function works by iterating over the input data a specified number of times,
    each time shifting the data and appending it to a list (`cols`). The list is then
    concatenated into a DataFrame and the columns are renamed appropriately. If `dropnan=True`,
    any rows with missing values are dropped.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过在输入数据上迭代指定次数，每次平移数据并将其附加到列表（`cols`）中。然后将列表连接成一个DataFrame，并适当地重命名列。如果`dropnan=True`，则删除任何包含缺失值的行。
- en: There’s more…
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: The window size, which represents how many past time steps we should use to
    predict future ones, depends on the specific problem and the nature of the time
    series. A too-small window might not capture important patterns, while a too-large
    one might include irrelevant information. Testing different window sizes and comparing
    model performance is a common way to select an appropriate window size.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口大小决定了我们应该使用多少过去的时间步来预测未来的时间步，这取决于具体问题和时间序列的性质。过小的窗口可能无法捕捉到重要的模式，而过大的窗口可能会包含不相关的信息。测试不同的窗口大小并比较模型性能是选择合适窗口大小的常见方法。
- en: Univariate forecasting with a feedforward neural network
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用前馈神经网络进行单变量预测
- en: This recipe walks you through the process of building a feedforward neural network
    for forecasting with univariate time series.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将引导你通过使用单变量时间序列构建前馈神经网络进行预测的过程。
- en: Getting ready
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Having transformed the time series data into an appropriate format for supervised
    learning, we are now ready to employ it for training a feedforward neural network.
    We strategically decided to resample the dataset, transitioning from hourly to
    daily data. This optimization significantly accelerates our training processes:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在将时间序列数据转化为适用于监督学习的格式后，我们现在准备使用它来训练一个前馈神经网络。我们策略性地决定对数据集进行重采样，从每小时数据转为每日数据。这一优化显著加速了我们的训练过程：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How to do it…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here are the steps for building and evaluting a feedforward neural network
    using PyTorch:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用PyTorch构建和评估前馈神经网络的步骤：
- en: 'We begin by splitting the data into training and testing and normalizing them.
    It’s important to note that the scaler should be fitted on the training set and
    used to transform both the training and test sets:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将数据拆分为训练集和测试集并进行归一化。需要注意的是，Scaler 应该在训练集上进行拟合，并用于转换训练集和测试集：
- en: '[PRE11]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we create a simple feedforward neural network with one hidden layer using
    PyTorch. `input_dim` represents the number of lags, which is often referred to
    as the lookback window. `hidden_dim` is the number of hidden units in the hidden
    layer of the neural network. Finally, `output_dim` is the forecasting horizon,
    which is set to `1` in the following example. We use a `ReLU``()` activation function,
    which we described in the *Training a feedforward neural network* recipe from
    the previous chapter:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 PyTorch 创建一个简单的前馈神经网络，包含一个隐藏层。`input_dim` 代表滞后的数量，通常称为回溯窗口。`hidden_dim`
    是神经网络隐藏层中的隐藏单元数量。最后，`output_dim` 是预测范围，在以下示例中设置为 `1`。我们使用 `ReLU` 激活函数，这是我们在上一章的
    *训练前馈神经网络* 章节中描述的：
- en: '[PRE12]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we define the `loss` function and the `optimizer` and train the model:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义`loss`函数和`optimizer`，并训练模型：
- en: '[PRE13]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we evaluate the model on the test set:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在测试集上评估模型：
- en: '[PRE14]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works…
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: This script starts by dividing the data into training and testing sets. `MinMaxScaler`
    is used to scale the features to between `-1` and `1`. It’s important to note
    that we fit the scaler only on the training set to avoid data leakage.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本首先将数据分为训练集和测试集。`MinMaxScaler` 用于将特征缩放到 `-1` 和 `1` 之间。需要注意的是，我们只在训练集上拟合 scaler，以避免数据泄漏。
- en: Next, we define a simple feedforward neural network model with one hidden layer.
    The `FeedForwardNN` class extends `nn.Module`, which is the base class for all
    neural network modules in PyTorch. The class constructor defines the layers of
    the network, and the `forward` method specifies how forward propagation is done.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个简单的前馈神经网络模型，包含一个隐藏层。`FeedForwardNN`类继承自`nn.Module`，这是 PyTorch 中所有神经网络模块的基类。类构造函数定义了网络的层，`forward`方法指定了前向传播的过程。
- en: The model is then trained using the mean squared error loss function and the
    `Adam` optimizer. The model parameters are updated over multiple epochs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型使用均方误差损失函数和 `Adam` 优化器进行训练。模型参数在多个迭代周期中不断更新。
- en: Finally, the model is evaluated on the testing set, and the loss of this unseen
    data measures how well the model generalizes beyond the training data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型在测试集上进行评估，未见数据的损失衡量了模型在训练数据之外的泛化能力。
- en: There’s more…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'This is a simple example of how a feedforward neural network can be used for
    time series forecasting. There are several ways you can improve this model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的例子，展示了如何使用前馈神经网络进行时间序列预测。你可以通过以下几种方式改进此模型：
- en: You can experiment with different network architectures, for example, by adding
    more layers or changing the number of neurons in the hidden layer. You can also
    try different activation functions, optimizers, and learning rates.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以尝试不同的网络架构，例如，通过增加更多层或改变隐藏层中神经元的数量。你还可以尝试不同的激活函数、优化器和学习率。
- en: It might be beneficial to use a more sophisticated method for preparing the
    training and testing sets; for example, using a rolling-window validation strategy.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更复杂的方法准备训练集和测试集可能会更有益；例如，使用滚动窗口验证策略。
- en: Another improvement can be using early stopping to prevent overfitting. We’ll
    learn about this technique in the next chapter.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个改进可以是使用早停法来防止过拟合。我们将在下一章学习这一技术。
- en: Last but not least, advanced models such as **recurrent neural networks** (**RNNs**)
    and LSTM networks are specifically designed for sequence data and can give better
    results for time series forecasting.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，**递归神经网络**（**RNNs**）和 LSTM 网络是专门为序列数据设计的，能够为时间序列预测提供更好的结果。
- en: Univariate forecasting with an LSTM
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LSTM 进行单变量预测
- en: This recipe walks you through the process of building an LSTM neural network
    for forecasting with univariate time series.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将引导你完成构建 LSTM 神经网络以进行单变量时间序列预测的过程。
- en: Getting ready
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: As we saw in [*Chapter 2*](B21145_02.xhtml#_idTextAnchor140), LSTM networks,
    a variant of RNNs, have gained substantial attention for their performance on
    time series and sequence data. LSTM networks are particularly suited for this
    task because they can effectively capture long-term temporal dependencies in the
    input data due to their inherent memory cells.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第2章*](B21145_02.xhtml#_idTextAnchor140)中看到的那样，LSTM网络作为RNN的一种变体，因其在时间序列和序列数据上的表现而受到广泛关注。LSTM网络特别适合这个任务，因为它们能够有效地捕捉输入数据中的长期时间依赖性，得益于其内在的记忆单元。
- en: This section will extend our univariate time series forecasting to LSTM networks
    using PyTorch. So, we continue with the objects created in the previous recipe
    (*Univariate forecasting with a feedforward* *neural network*).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用PyTorch扩展我们的一元时间序列预测到LSTM网络。因此，我们继续使用前一个食谱中创建的对象（*使用前馈神经网络进行一元预测*）。
- en: How to do it…
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We will use the same train and test sets from the previous section. For an
    LSTM, we must reshape the input data to 3D. As we explored in the previous chapter,
    the three dimensions of the input tensor to LSTMs represent the following aspects:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前一部分中的相同训练集和测试集。对于LSTM，我们必须将输入数据重塑为3D格式。正如我们在前一章中探讨的那样，LSTM输入张量的三个维度分别表示以下内容：
- en: '**Samples**: One sub-sequence (for example, the past five lags) is one sample.
    A batch is a set of samples.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本**：一个子序列（例如，过去五个滞后值）就是一个样本。一个批次是一组样本。'
- en: '**Time steps**: The window size; how many observations we use from the past
    at each point.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间步长**：窗口大小；每个时间点使用多少过去的观测值。'
- en: '**Features**: The number of variables used in the model. Univariate time series
    always contain a single feature.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：模型中使用的变量数量。一元时间序列始终只有一个特征。'
- en: 'The following code transforms the input explanatory variables into a 3D format:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将输入的解释性变量转换为3D格式：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding lines of code, `X_train.shape[0]` and `X_test.shape[0]` represent
    the number of samples (that is, the number of sequences), and `X_train.shape[1]`
    and `X_test.shape[1]` represent the number of time steps (the window size). The
    last dimension in the reshape operation, which is set to `1`, represents the number
    of features. We only have one feature in our univariate time series, so we set
    it to `1`. If we had a multivariate time series, this value would correspond to
    the number of variables in the data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码行中，`X_train.shape[0]`和`X_test.shape[0]`表示样本的数量（即序列的数量），而`X_train.shape[1]`和`X_test.shape[1]`表示时间步长的数量（窗口大小）。重塑操作中的最后一个维度，设置为`1`，表示特征的数量。我们的一元时间序列只有一个特征，所以设置为`1`。如果我们有一个多元时间序列，这个值将对应于数据中的变量数量。
- en: The `view()` function in PyTorch is used to reshape a `tensor` object. It’s
    equivalent to the `reshape()` function in NumPy and allows us to restructure our
    data to match the input shape that our LSTM model requires. Reshaping the data
    this way ensures that the LSTM model receives the data in the expected format.
    This is crucial for its ability to model the temporal dependencies in our time
    series data effectively.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的`view()`函数用于重塑`tensor`对象。它等同于NumPy中的`reshape()`函数，允许我们重新组织数据以匹配LSTM模型所需的输入形状。以这种方式重塑数据可以确保LSTM模型接收到期望格式的数据。这对于LSTM有效建模时间序列数据中的时间依赖性至关重要。
- en: 'Then, we define the LSTM model:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义LSTM模型：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that, for the LSTM, the `input_dim` input dimension is `1`, which is the
    number of variables in the time series. This aspect is different from the `input_dim`
    argument we passed to the feedforward neural network in the previous recipe. In
    that case, this parameter was set to `3`, which denoted the number of lags or
    features.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于LSTM，`input_dim`的输入维度是`1`，表示时间序列中的变量数量。这一点与我们在前一个食谱中传递给前馈神经网络的`input_dim`参数不同。在那个例子中，`input_dim`设置为`3`，表示滞后数或特征数。
- en: 'We now proceed to train the model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续训练模型：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we evaluate the model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估模型：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In the first step, we reshape our training and testing sets to match the input
    shape that LSTM expects, i.e., `batch_size`, `sequence_length`, and `number_of_features`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们将训练集和测试集重塑为LSTM所期望的输入形状，即`batch_size`、`sequence_length`和`number_of_features`。
- en: The `LSTM` class inherits from `nn.Module`, which means it is a custom neural
    network in PyTorch. The `LSTM` model has an `LSTM` layer with a specified number
    of hidden dimensions and layers, followed by a fully connected (linear) layer
    that outputs the final prediction.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM` 类继承自 `nn.Module`，这意味着它是 PyTorch 中的自定义神经网络。`LSTM` 模型具有指定数量的隐藏维度和层数的 `LSTM`
    层，然后是一个全连接（线性）层，用于输出最终预测。'
- en: The `forward``()` function defines the forward pass of the `LSTM` model. We
    first initialize the hidden states (`h0`) and cell states (`c0`) of `LSTM` with
    zeros. Then, we pass the input data and initial states into the `LSTM` layer,
    which returns the `LSTM` outputs and the final hidden and cell states. Note that
    we only use the final time-step output of the `LSTM` to pass into the fully connected
    layer to produce the output.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()` 函数定义了 `LSTM` 模型的前向传播。我们首先用零初始化 `LSTM` 的隐藏状态 (`h0`) 和细胞状态 (`c0`)。然后，将输入数据和初始状态传入
    `LSTM` 层，它会返回 `LSTM` 的输出以及最终的隐藏和细胞状态。请注意，我们只使用 `LSTM` 的最终时间步输出传入全连接层以生成输出。'
- en: We then instantiate the model, define the `loss``()` function as the `Adam`
    optimizer for training the network.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化模型，将 `loss()` 函数定义为用于训练网络的 `Adam` 优化器。
- en: During training, we first set the model into training mode, reset the gradients,
    perform the forward pass, calculate the loss, perform back-propagation via `loss.backward`,
    and then perform a single optimization step.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们首先将模型设置为训练模式，重置梯度，执行前向传播，计算损失，通过 `loss.backward` 执行反向传播，然后执行单一优化步骤。
- en: Finally, we evaluate the model on the test data and print the test loss. Note
    that we did not do any hyperparameter tuning, which is a very important step when
    training neural networks. We’ll learn about this process in the next chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在测试数据上评估模型并打印测试损失。请注意，我们没有进行超参数调整，这在训练神经网络时是非常重要的步骤。我们将在下一章学习这个过程。
- en: There’s more…
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这还不止一点……
- en: LSTM models are especially effective for time series forecasting due to their
    ability to capture long-term dependencies. However, their performance can significantly
    depend on the choice of hyperparameters. Hence, it may be useful to perform hyperparameter
    tuning to find the optimal configuration. Some important hyperparameters to consider
    are the number of hidden dimensions, the number of LSTM layers, and the learning
    rate.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`LSTM` 模型特别适用于时间序列预测，因为它们能够捕捉长期依赖关系。然而，它们的性能可能会极大地依赖于超参数的选择。因此，进行超参数调优以找到最佳配置可能是有用的。一些重要的超参数包括隐藏维度的数量、`LSTM`
    层的数量和学习率。'
- en: It’s also important to remember that LSTMs, like all deep learning models, may
    be prone to overfitting if the model complexity is too high. Techniques such as
    dropout, early stopping, or regularization (`L1`, `L2`) can be used to prevent
    overfitting.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 还要记住，像所有深度学习模型一样，如果模型复杂度过高，`LSTM` 也可能会过拟合。可以使用诸如 dropout、早停止或正则化（`L1`、`L2`）等技术来防止过拟合。
- en: Furthermore, advanced variants of LSTMs such as bidirectional LSTMs, or other
    types of RNNs such as GRUs can also be used to improve performance possibly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以使用高级变体的 `LSTM`，如双向 `LSTM`，或其他类型的 `RNN`，如 `GRU`，来可能提升性能。
- en: Lastly, while LSTMs are powerful, they may not always be the best choice due
    to their computational and memory requirements, especially for very large datasets
    or complex models. In these cases, simpler models or other types of neural networks
    may be more suitable.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管 `LSTM` 强大，但由于计算和内存需求，尤其是对于非常大的数据集或复杂模型，它们并不总是最佳选择。在这些情况下，可能更适合使用简单的模型或其他类型的神经网络。
- en: Univariate forecasting with a GRU
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `GRU` 进行单变量预测
- en: This recipe walks you through the process of building a GRU neural network for
    forecasting with univariate time series.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将引导你完成构建用于单变量时间序列预测的 `GRU` 神经网络的过程。
- en: Getting ready
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Now that we have seen how LSTMs can be used for univariate time series forecasting,
    let’s now shift our attention to another type of RNN architecture known as GRU.
    GRUs, like LSTMs, are designed to capture long-term dependencies in sequence data
    effectively but do so with a slightly different and less complex internal structure.
    This often makes them faster to train.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何使用 `LSTM` 进行单变量时间序列预测，让我们现在把注意力转向另一种称为 `GRU` 的 `RNN` 架构。`GRU` 像 `LSTM`
    一样，被设计用来有效地捕捉序列数据中的长期依赖关系，但其内部结构略有不同，更少复杂。这通常使得它们训练速度更快。
- en: 'For this section, we will use the same training and testing sets as in the
    previous sections. Again, the input data should be reshaped into a `3D` tensor
    with dimensions representing observations, time steps, and features respectively:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分，我们将使用与前几部分相同的训练和测试数据集。同样，输入数据应重新调整为一个 `3D` 张量，维度分别表示观测值、时间步和特征：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How to do it…
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Let’s start constructing a GRU network with the help of the following steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤开始构建 GRU 网络：
- en: 'We start by constructing a GRU network in PyTorch:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先在 PyTorch 中构建 GRU 网络：
- en: '[PRE20]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Like before, we define our `loss` function and `optimizer`:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同之前一样，我们定义了我们的 `loss` 函数和 `optimizer`：
- en: '[PRE21]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We train our model:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练我们的模型：
- en: '[PRE22]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we evaluate our model:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们评估我们的模型：
- en: '[PRE23]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works…
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Similar to the LSTM, the GRU also requires 3D input data. We begin by reshaping
    our input data accordingly. Next, we define our GRU model. This model contains
    a GRU layer and a linear layer. The initial hidden state for the GRU is defined
    and initialized with zeros.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 LSTM，GRU 也需要 3D 输入数据。我们首先相应地调整输入数据的形状。接下来，我们定义我们的 GRU 模型。这个模型包含一个 GRU 层和一个线性层。GRU
    的初始隐藏状态被定义并初始化为零。
- en: We then define our `loss``()` function and optimizer and train our model. The
    model’s output from the last time step is used for predictions. Finally, we evaluate
    our model on the test set and print the test loss.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义我们的 `loss` 函数和优化器，训练模型。模型在最后一个时间步的输出用于预测。最后，我们在测试集上评估我们的模型，并打印测试损失。
- en: There’s more…
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'There are many ways to improve this model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以改进这个模型：
- en: Experimenting with different GRU architectures or varying the number of GRU
    layers may yield better results
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的 GRU 架构或调整 GRU 层数可能会得到更好的结果
- en: Using a different loss function or optimizer could also potentially improve
    model performance
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的损失函数或优化器也可能改善模型性能
- en: Implementing early stopping or other regularization techniques can help prevent
    overfitting
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现提前停止或其他正则化技术有助于防止过拟合
- en: Applying more sophisticated data preparation techniques, such as sequence padding
    or truncation, can better equip the model to handle sequences of varying lengths
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用更复杂的数据准备技术，如序列填充或截断，可以更好地帮助模型处理不同长度的序列
- en: More advanced models, such as the sequence-to-sequence model or the transformer,
    may provide better results for more complex time series forecasting tasks
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更先进的模型，如序列到序列模型或 Transformer，可能会为更复杂的时间序列预测任务提供更好的结果
- en: Univariate forecasting with a Stacked LSTM
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用堆叠 LSTM 进行单变量预测
- en: This recipe walks you through the process of building an LSTM neural network
    with multiple layers for forecasting with univariate time series.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方将引导你完成使用多层 LSTM 神经网络进行单变量时间序列预测的过程。
- en: Getting ready
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: For complex time series prediction problems, one LSTM layer may not be sufficient.
    In this case, we can use a stacked LSTM, which is essentially multiple layers
    of LSTM stacked one on top of the other. This can provide a higher level of input
    abstraction and may lead to improved prediction performance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的时间序列预测问题，单一的 LSTM 层可能不足以处理。在这种情况下，我们可以使用堆叠 LSTM，它本质上是多个 LSTM 层堆叠在一起。这可以提供更高层次的输入抽象，并可能提升预测性能。
- en: 'We will continue to use the same reshaped train and test sets from the previous
    recipe:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用前一个配方中相同的重塑训练和测试数据集：
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We also use the LSTM neural network defined in the *Univariate forecasting
    with an* *LSTM* recipe:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了在*单变量预测与* *LSTM* 配方中定义的 LSTM 神经网络：
- en: '[PRE25]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’ll use these elements to train a stacked LSTM model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些元素来训练一个堆叠 LSTM 模型。
- en: How to do it…
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'To construct a stacked LSTM in PyTorch, we need to call the `LSTM` class with
    the input `num_layers=2`, like so:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 PyTorch 中构建堆叠 LSTM，我们需要调用 `LSTM` 类，并输入 `num_layers=2`，如下所示：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The rest of the training process is quite similar to what we did in the preceding
    recipes. We define our loss function and `optimizer`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的训练过程与我们在前面配方中做的相似。我们定义了损失函数和 `optimizer`：
- en: '[PRE27]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We train the model:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练模型：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we evaluate our model:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估我们的模型：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works…
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The setup for the stacked LSTM model is similar to the single-layer LSTM model.
    The major difference lies in the LSTM layer, where we specify that we want more
    than one LSTM layer. This is accomplished by setting `num_layers` to `2` or more.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠 LSTM 模型的设置类似于单层 LSTM 模型。主要的区别在于 LSTM 层，我们指定要使用多个 LSTM 层。通过将 `num_layers`
    设置为 `2` 或更多层来实现这一点。
- en: The forward pass for the stacked LSTM is identical to that of the single-layer
    LSTM. We initialize the hidden state `h0` and cell state `c0` with zeros, pass
    the input and the initial states into the LSTM layers, and then use the output
    from the final time step for our predictions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠LSTM的前向传递与单层LSTM相同。我们用零初始化隐藏状态`h0`和细胞状态`c0`，将输入和初始状态传入LSTM层，然后使用最后时间步的输出进行预测。
- en: The test set loss is again closely aligned with previous results. Several factors
    could contribute to this observation. It could be a result of limited data or
    the fact that the expressiveness of the data may not benefit from the complexity
    of our model. Additionally, we have not conducted any hyperparameter optimization,
    which could potentially enhance the model’s performance. In subsequent sections,
    we will delve deeper into these aspects, exploring potential solutions and strategies
    for further improvement.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的损失与之前的结果紧密对齐。多个因素可能导致了这一观察结果。可能是数据有限，或者数据的表达能力未能从我们模型的复杂性中受益。此外，我们没有进行任何超参数优化，这可能会进一步提高模型的性能。在随后的部分中，我们将更深入地探讨这些方面，探索潜在的解决方案和进一步改进的策略。
- en: Combining an LSTM with multiple fully connected layers
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LSTM与多个全连接层结合
- en: Sometimes, it may be valuable to combine different types of neural networks
    in a single model. In this recipe, you’ll learn how to combine an LSTM module
    with a fully connected layer that is the basis of feedforward neural networks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将不同类型的神经网络组合成一个模型可能是有价值的。在这个食谱中，你将学习如何将一个LSTM模块与全连接层结合，而全连接层是前馈神经网络的基础。
- en: Getting ready
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this section, we’ll use a hybrid model that combines an LSTM layer with multiple
    fully connected (also known as dense) layers. This allows us to further abstract
    features from the sequence, and then learn complex mappings to the output space.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个混合模型，它将LSTM层与多个全连接（也称为密集）层结合。这使我们能够从序列中进一步抽象特征，然后学习到输出空间的复杂映射。
- en: We continue using the reshaped train and test sets from the previous sections.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用前几节中重塑过的训练和测试集。
- en: How to do it…
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'To construct this hybrid model in PyTorch, we add two fully connected layers
    after the LSTM layer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在PyTorch中构建这个混合模型，我们在LSTM层后添加了两个全连接层：
- en: '[PRE30]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We define our loss function and `optimizer`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义我们的损失函数和`optimizer`：
- en: '[PRE31]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We train and evaluate our model similarly to the previous recipes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像之前的食谱一样训练和评估我们的模型。
- en: How it works…
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The setup for the hybrid LSTM model involves an `LSTM` layer followed by two
    fully connected layers. After passing through the `LSTM` layer, the output of
    the final time step is processed by the fully connected layers. Using the `ReLU``()`
    activation function between these layers introduces non-linearities, allowing
    our model to capture more complex relationships in the data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 混合LSTM模型的设置包括一个`LSTM`层，后跟两个全连接层。通过`LSTM`层后，最后时间步的输出被全连接层处理。在这些层之间使用`ReLU`()激活函数引入了非线性，使得我们的模型能够捕捉数据中的更复杂关系。
- en: Note that the output from an `LSTM` layer is a tensor of shape (`batch_size,
    seq_length, hidden_dim)`. This is because `LSTM`, by default, outputs the hidden
    states for each time step in the sequence for each item in the batch.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`LSTM`层的输出是一个形状为`(batch_size, seq_length, hidden_dim)`的张量。这是因为`LSTM`默认输出序列中每个时间步的隐藏状态，且每个批次项都如此。
- en: In this specific model, we’re interested only in the last time step’s hidden
    state to feed into the fully connected layers. We achieve this with `out[:, -1,
    :]`, effectively selecting the last time step’s hidden state for each sequence
    in the batch. The result is a tensor of shape `(``batch_size, hidden_dim)`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定模型中，我们只关注最后一个时间步的隐藏状态，以输入到全连接层。我们通过`out[:, -1, :]`来实现这一点，有效地选择了批次中每个序列的最后一个时间步的隐藏状态。结果是一个形状为`(batch_size,
    hidden_dim)`的张量。
- en: The reshaped output is then passed through the first fully connected (linear)
    layer with the `self.fc1(out[:, -1, :])` function call. This layer has 50 neurons,
    so the output shape changes to `(``batch_size, 50)`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑后的输出通过`self.fc1(out[:, -1, :])`函数调用传递到第一个全连接（线性）层。该层有50个神经元，因此输出的形状变化为`(batch_size,
    50)`。
- en: After applying the `ReLU``()` activation function, this output is then passed
    to the second fully connected layer `self.fc2(out)`, which has a size equal to
    `output_dim`, reducing the tensor to the shape `(batch_size, output_dim)`. This
    is the final output of the model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用`ReLU`激活函数后，输出会传递到第二个全连接层`self.fc2(out)`，其大小等于`output_dim`，将张量的形状缩小为`(batch_size,
    output_dim)`。这是模型的最终输出。
- en: Remember that the hidden dimension (`hidden_dim`) is a hyperparameter of the
    LSTM and can be chosen freely. The number of neurons in the first fully connected
    layer (`50`, in this case) is also a hyperparameter and can be modified to suit
    the specific task better.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，隐藏维度（`hidden_dim`）是LSTM的超参数，可以自由选择。第一个全连接层中的神经元数量（在本例中为`50`）也是一个超参数，可以根据特定任务进行调整。
- en: There’s more…
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'When working with hybrid models, consider the following tips:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用混合模型时，请考虑以下建议：
- en: Vary the number of fully connected layers and their sizes to explore different
    model complexities.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变全连接层的数量及其大小，以探索不同的模型复杂度。
- en: Different activation functions in the fully connected layers may lead to varied
    performance.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层中使用不同的激活函数可能会导致不同的性能表现。
- en: As the complexity of the model increases, so does the computational cost. Be
    sure to balance complexity and computational efficiency.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着模型复杂度的增加，计算成本也会增加。请务必平衡复杂度和计算效率。
- en: Univariate forecasting with a CNN
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN进行单变量预测
- en: Now, we turn our attention to convolutional neural networks that have also shown
    promising results with time series data. Let’s learn how these methods can be
    used for univariate time series forecasting.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将注意力转向卷积神经网络，这些网络在时间序列数据中也表现出了有前景的结果。让我们学习如何将这些方法用于单变量时间序列预测。
- en: Getting ready
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: CNNs are commonly used in problems involving images, but they can also be applied
    to time series forecasting tasks. By treating time series data as a “sequence
    image,” CNNs can extract local features and dependencies from the data. To implement
    this, we’ll need to prepare our time series data similarly to how we did for LSTM
    models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常用于处理图像相关问题，但它们也可以应用于时间序列预测任务。通过将时间序列数据视为“序列图像”，CNN可以从数据中提取局部特征和依赖关系。为了实现这一点，我们需要像处理LSTM模型一样准备时间序列数据。
- en: How to do it…
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Let’s define a simple CNN model in PyTorch. For this example, we will use a
    single convolutional layer followed by a fully connected layer:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在PyTorch中定义一个简单的CNN模型。在这个示例中，我们将使用一个卷积层，后面跟着一个全连接层：
- en: '[PRE32]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We train and evaluate our model similarly to the previous sections.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像之前的章节一样训练和评估我们的模型。
- en: How it works…
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The CNN model is based on convolutional layers. These are designed to extract
    local features directly from the input data. These features are then passed to
    one or more fully connected layers that model the future values of the time series.
    The training stage of this type of neural network is similar to others, such as
    the LSTM.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: CNN模型基于卷积层，这些层用于直接从输入数据中提取局部特征。这些特征随后被传递到一个或多个全连接层，用来建模时间序列的未来值。这种类型的神经网络的训练阶段与其他模型类似，例如LSTM。
- en: 'Let’s go through our neural network architecture. It has the following features:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的神经网络架构。它具有以下特点：
- en: An input layer, which accepts time series data of shape `(batch_size, sequence_length,
    number_of_features)`. For univariate time series forecasting, `number_of_features`
    is `1`.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入层，接受形状为`(batch_size, sequence_length, number_of_features)`的时间序列数据。对于单变量时间序列预测，`number_of_features`是`1`。
- en: A convolutional layer with `64` filters and a kernel size of `3`, defined in
    PyTorch as `self.conv1 = nn.Conv1d(in_channels=1,` `out_channels=64, kernel_size=3)`.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有`64`个滤波器和`3`大小卷积核的卷积层，在PyTorch中定义为`self.conv1 = nn.Conv1d(in_channels=1,
    out_channels=64, kernel_size=3)`。
- en: A fully connected (or linear) layer that maps the output of the convolutional
    layer to our prediction
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个全连接（或线性）层，将卷积层的输出映射到我们的预测值。
- en: 'Let’s see how these layers transform the data:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些层如何转换数据：
- en: '`(batch_size,` `sequence_length, 1)`.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, sequence_length, 1)`。'
- en: '`Conv1d`: A `1D` convolution is performed over the time series data. The kernel
    slides over the sequence, computing the dot product of the weights and the input.
    After this convolution operation, the shape of our data is `(batch_size, out_channels,
    sequence_length-kernel_size+1)`, or in this case, `(batch_size,` `64, sequence_length-3+1)`.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conv1d`：对时间序列数据进行 `1D` 卷积。卷积核在序列上滑动，计算权重与输入的点积。经过这次卷积操作后，我们数据的形状为 `(batch_size,
    out_channels, sequence_length-kernel_size+1)`，或者在这个例子中是 `(batch_size, 64, sequence_length-3+1)`。'
- en: '`(batch_size, remaining_dims)`. `remaining_dims` is calculated by multiplying
    the remaining dimensions of the tensor (`64` and `sequence_length-2` in our case).
    The resulting shape would be `(batch_size, 64 * (sequence_length-2))`. We can
    achieve this by using the `view``()` function in PyTorch as follows: `x =` `x.view(x.size(0),
    -1)`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, remaining_dims)`。`remaining_dims` 是通过乘以张量的剩余维度（在我们例子中是 `64` 和
    `sequence_length-2`）来计算的。最终的形状将是 `(batch_size, 64 * (sequence_length-2))`。我们可以通过使用
    PyTorch 中的 `view()` 函数实现，如下所示：`x = x.view(x.size(0), -1)`。'
- en: Now, `x` is ready to be fed into the fully connected layer, `self.fc = nn.Linear(64
    * (sequence_length-2), output_dim)`, where `output_dim` is the dimensionality
    of the output space, `1` for univariate time series prediction. The output of
    this layer is of shape `(batch_size, output_dim)`, or `(batch_size, 1)`, and these
    are our final predictions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`x` 已经准备好输入到全连接层 `self.fc = nn.Linear(64 * (sequence_length-2), output_dim)`，其中
    `output_dim` 是输出空间的维度，对于单变量时间序列预测来说是 `1`。该层的输出形状为 `(batch_size, output_dim)`，即
    `(batch_size, 1)`，这些就是我们的最终预测结果。
- en: This way, we can see how the tensor shapes are handled and transformed as they
    pass through each network layer. Understanding this process is crucial for troubleshooting
    and designing your own architectures.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以看到张量形状如何在通过每一层网络时得到处理和转换。理解这一过程对于故障排除和设计自己的架构至关重要。
- en: There’s more…
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'CNNs can be extended in several ways:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 可以通过多种方式进行扩展：
- en: Multiple convolutional layers can be stacked to form a deeper network
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以堆叠多个卷积层来构建更深的网络
- en: Pooling layers can be added after convolutional layers to reduce dimensionality
    and computational cost
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层后可以添加池化层，以减少维度和计算成本
- en: Dropout or other regularization techniques can be applied to prevent overfitting
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以应用 Dropout 或其他正则化技术来防止过拟合
- en: The model could be extended to a ConvLSTM, which combines the strengths of CNNs
    and LSTMs for handling spatial and temporal dependencies
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以扩展为 ConvLSTM，结合 CNN 和 LSTM 在处理空间和时间依赖性方面的优势
- en: Handling trend – taking first differences
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理趋势——计算第一次差分
- en: In [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019), we learned about different
    time series patterns such as trend or seasonality. This recipe describes the process
    of dealing with trend in time series before training a deep neural network.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 1 章*](B21145_01.xhtml#_idTextAnchor019) 中，我们学习了不同的时间序列模式，如趋势或季节性。这个步骤描述了在训练深度神经网络之前处理时间序列趋势的过程。
- en: Getting ready
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: As we learned in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019), trend is the
    long-term change in the time series. When the average value of the time series
    changes, this means that the data is not stationary. Non-stationary time series
    are more difficult to model, so it’s important to transform the data into a stationary
    series.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [*第 1 章*](B21145_01.xhtml#_idTextAnchor019) 中学到的，趋势是时间序列中的长期变化。当时间序列的平均值发生变化时，这意味着数据不是平稳的。非平稳时间序列更难建模，因此将数据转换为平稳序列非常重要。
- en: Trend is usually removed from the time series by taking the first differences
    until the data becomes stationary.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 趋势通常通过计算第一次差分来从时间序列中去除，直到数据变得平稳。
- en: 'First, let’s start by splitting the time series into training and testing sets:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从将时间序列分成训练集和测试集开始：
- en: '[PRE33]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We leave the last 20% of observations for testing.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最后 20% 的观察数据留作测试。
- en: How to do it…
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'There are two ways we can compute the difference between consecutive observations
    using `pandas`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `pandas` 通过两种方式计算连续观察值之间的差异：
- en: 'Let’s begin with the standard approach using the `diff()` method:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从使用 `diff()` 方法的标准方法开始：
- en: '[PRE34]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `periods` argument details the number of steps used to compute the differences.
    In this case, `periods=1` means that we compute the difference between consecutive
    observations, also known as first differences. As an example, setting the number
    of periods to `7` would compute the difference between each observation and the
    observation captured 7 time steps before it. In the case of a daily time series,
    this can be an effective way of removing seasonality. But more on that later.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`periods`参数指明了用于计算差分的步数。在本例中，`periods=1`意味着我们计算连续观测值之间的差分，也就是第一差分。例如，将周期数设置为`7`将计算每个观测值与前7个时间步骤的观测值之间的差值。对于每日时间序列来说，这是一种有效的去除季节性的方法。不过，稍后会详细介绍这一点。'
- en: 'Another way to difference a time series is using the `shift()` method:'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一种对时间序列进行差分的方法是使用`shift()`方法：
- en: '[PRE35]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We created a second time series that is shifted by the desired number of periods
    (in this case, `1`). Then, we subtract this series from the original one to get
    a differenced series.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个移位了所需周期数（在本例中为`1`）的第二个时间序列。然后，我们将这个序列从原始序列中减去，得到一个差分后的序列。
- en: 'Differencing stabilizes the level of the series. Still, we can normalize the
    data into a common value range:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 差分能够稳定序列的水平。不过，我们仍然可以将数据标准化为一个统一的值范围：
- en: '[PRE36]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we transform the time series for supervised learning using the `series_to_supervised``()`
    function as in the previous recipes:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们像前面的步骤一样，使用`series_to_supervised()`函数将时间序列转换为监督学习格式：
- en: '[PRE37]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The model training phase will remain the same as in the previous recipes:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练阶段将与之前的步骤相同：
- en: '[PRE38]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'But our job is not done yet. The preceding neural network is trained on differenced
    data. So, the predictions are also differenced:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但我们的工作还没有完成。前面提到的神经网络是基于差分数据训练的。因此，预测结果也是差分过的：
- en: '[PRE39]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We next need to revert the data transformation processes to get the forecasts
    in the time series' original scale
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要恢复数据转换过程，以便在时间序列的原始尺度上获取预测结果。
- en: 'First, we denormalize the time series:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们对时间序列进行反标准化：
- en: '[PRE40]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, we revert the differencing operation by adding back the shifted time
    series:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过加回移位后的时间序列来恢复差分操作：
- en: '[PRE41]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the preceding code, we skip the first three values as they were used during
    the transformation process by the `series_to_supervised()` function.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们跳过了前三个值，因为它们在通过`series_to_supervised()`函数进行转换过程中已经被使用。
- en: How it works…
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其工作原理……
- en: Differencing works by stabilizing the level of the time series, thus making
    it stationary. Instead of modeling the actual values of the series, the neural
    network models the series of changes; how the time series changes from one time
    step to another. The raw forecasts that come out of the neural network represent
    the predicted changes. We need to revert the differencing process to get the forecasts
    at their original scale.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 差分通过稳定时间序列的水平使其变为平稳。神经网络不直接建模序列的实际值，而是建模变化序列；即时间序列如何从一个时间步骤变化到另一个时间步骤。神经网络输出的原始预测结果代表了预测的变化。我们需要恢复差分过程，才能获得原始尺度上的预测结果。
- en: There’s more…
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: You can also deal with trend by including the time information in the input
    data. An explanatory variable that denotes the step at which each observation
    is collected. For example, the first observation has a value of `1`, and the second
    one has a value of `2`. This approach is effective if the trend is deterministic
    and we do not expect it to change. Differencing provides a more general way of
    dealing with trends.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过在输入数据中加入时间信息来处理趋势。一个表示每个观测值收集时步骤的解释变量。例如，第一个观测值的值为`1`，第二个观测值的值为`2`。这种方法在趋势是确定性且我们不期望其发生变化时是有效的。差分提供了一种更通用的处理趋势的方法。
- en: Handling seasonality – seasonal dummies and Fourier series
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理季节性——季节虚拟变量和傅里叶级数
- en: In this recipe, we’ll describe how to deal with seasonality in time series using
    seasonal dummy variables and a Fourier series.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇中，我们将描述如何使用季节性虚拟变量和傅里叶级数处理时间序列中的季节性。
- en: Getting ready
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Seasonality represents repeatable patterns that recur over a given period, such
    as every year. Seasonality is an important piece of time series, and it is important
    to capture it. The consensus in the literature is that neural networks cannot
    capture seasonal effects optimally. The best way to model seasonality is by feature
    engineering or data transformation. One way to handle seasonality is to add extra
    information that captures the periodicity of patterns. This can be done with seasonal
    dummies or a Fourier series.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性代表了在给定周期内重复出现的模式，例如每年一次。季节性是时间序列中的一个重要组成部分，捕捉季节性非常重要。文献中的共识是，神经网络无法最优地捕捉季节性效应。建模季节性最好的方法是通过特征工程或数据转换。处理季节性的一种方式是添加额外的信息，捕捉模式的周期性。这可以通过季节性虚拟变量或傅里叶级数来完成。
- en: 'We start by preparing the data using the `series_to_supervised``()` function:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过使用`series_to_supervised()`函数准备数据：
- en: '[PRE42]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this recipe, we’ll skip the trend removal part for simplicity and focus on
    modeling seasonality. So, the `train_df` and `test_df` objects contain the lagged
    values of the training and testing sets.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，为了简单起见，我们将跳过趋势移除部分，专注于建模季节性。因此，`train_df`和`test_df`对象包含训练集和测试集的滞后值。
- en: How to do it…
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: Both seasonal dummy variables and a Fourier series can be added to the input
    data as additional explanatory variables. Let’s start by exploring seasonal dummies.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性虚拟变量和傅里叶级数都可以作为额外的解释变量添加到输入数据中。让我们首先探索季节性虚拟变量。
- en: Seasonal dummies
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 季节性虚拟变量
- en: Seasonal dummies are binary variables that describe the period of each observation.
    For example, whether a given value is collected on a Monday.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性虚拟变量是描述每个观测周期的二进制变量。例如，给定的值是否在周一收集。
- en: 'To build seasonal dummies, we first get the period information of each point.
    This can be done with the `DateTimeFeatures` class from `sktime` as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建季节性虚拟变量，我们首先获取每个观测点的周期信息。这可以通过以下方式使用`sktime`的`DateTimeFeatures`类来完成：
- en: '[PRE43]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The main argument for `DateTimeFeatures` is `ts_freq`, which we set to `D`.
    This means that we’re telling this method that our data is in a daily granularity.
    Then, we use the training set to fit a `DateTimeFeatures` object by passing it
    the observations of the first lags of this data (`train_df.iloc[:, -1]`). This
    results in a `pandas` DataFrame that contains the information detailed in the
    following table:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`DateTimeFeatures`的主要参数是`ts_freq`，我们将其设置为`D`。这意味着我们告诉此方法，我们的数据是按日粒度进行处理的。然后，我们使用训练集拟合`DateTimeFeatures`对象，传递该数据的前几期观察值（`train_df.iloc[:,
    -1]`）。这会生成一个包含以下表格中详细信息的`pandas` DataFrame：'
- en: '![Table 3.2: Information about the period of each observation](img/B21145_03_001.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![表3.2：每个观测周期的周期信息](img/B21145_03_001.jpg)'
- en: 'Table 3.2: Information about the period of each observation'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：每个观测周期的周期信息
- en: 'For simplicity, we’ll continue this recipe by using the information about the
    day of the week and month of the year. We get these columns with the following
    code:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，我们将继续使用包含星期几和年份月份信息的列。我们可以通过以下代码获取这些列：
- en: '[PRE44]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we convert this data into binary variables using a one-hot encoding approach
    from `sklearn` (`OneHotEncoder`):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`sklearn`的一个热编码方法（`OneHotEncoder`）将这些数据转换为二进制变量：
- en: '[PRE45]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This leads to a set of seasonal dummy variables that are shown in the following
    table:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这会得到一组季节性虚拟变量，如下表所示：
- en: '![Table 3.3: Information about the period of each observation as binary variables](img/B21145_03_002.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![表3.3：每个观测周期的二进制变量信息](img/B21145_03_002.jpg)'
- en: 'Table 3.3: Information about the period of each observation as binary variables'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3：每个观测周期的二进制变量信息
- en: 'We repeat this process using the test set:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用测试集重复此过程：
- en: '[PRE46]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note that we fit `DateTimeFeatures` and `OneHotEncoder` on the training data
    (using the `fit_transform()` method). With the test set, we can use the `transform()`
    method from the respective object.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在训练数据上使用`DateTimeFeatures`和`OneHotEncoder`进行拟合（使用`fit_transform()`方法）。对于测试集，我们可以使用相应对象的`transform()`方法。
- en: A Fourier series
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 傅里叶级数
- en: A Fourier series is made up of deterministic sine and cosine waves. The oscillations
    of these waves enable seasonality to be modeled as a repeating pattern.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶级数由确定性的正弦波和余弦波组成。这些波的振荡使得季节性能够建模为一种重复的模式。
- en: 'We can compute Fourier-based features using `sktime` as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式使用`sktime`计算基于傅里叶的特征：
- en: '[PRE47]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We use the `FourierFeatures` transformer to extract Fourier features. There
    are two main parameters to this operator:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`FourierFeatures`转换器提取傅里叶特征。该操作器有两个主要参数：
- en: '`sp_list`: The periodicity of the data. In this example, we set this parameter
    to `365.25`, which captures yearly variations.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_list`：数据的周期性。在这个例子中，我们将该参数设置为`365.25`，它捕捉了年度变化。'
- en: '`fourier_terms_list`: The number of Fourier waves for each sine and cosine
    function. We set this parameter to `2`, which means we compute `2` sine series
    plus `2` cosine series.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fourier_terms_list`：每个正弦和余弦函数的傅里叶波数。我们将此参数设置为`2`，即计算`2`个正弦序列加上`2`个余弦序列。'
- en: Modeling
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模
- en: 'After extracting seasonal dummies and the Fourier series, we add the extra
    variables to the datasets:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了季节性虚拟变量和傅里叶级数后，我们将额外的变量添加到数据集中：
- en: '[PRE48]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The `np.hstack``()` function is used to merge multiple arrays horizontally (column-wise).
    In this case, we merge the seasonal dummies and the Fourier series with the lagged
    features computed using the `series_to_supervised``()` function.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.hstack()`函数用于水平合并多个数组（按列合并）。在这种情况下，我们将季节性虚拟变量和傅里叶级数与使用`series_to_supervised()`函数计算的滞后特征合并。'
- en: 'Finally, we feed this data to a neural network as we did in previous recipes:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这些数据输入神经网络，就像我们在之前的食谱中所做的那样：
- en: '[PRE49]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: When utilizing seasonal dummies or a Fourier series, there is no need to perform
    any additional transformations after the inference step. In the previous code,
    we reversed the normalization process to obtain the forecasts in their original
    scale.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用季节性虚拟变量或傅里叶级数时，推断步骤后无需执行任何额外的转换。在之前的代码中，我们逆转了标准化过程，以便在其原始尺度上获得预测结果。
- en: How it works…
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Seasonal dummies and the Fourier series are variables that capture the recurrence
    of seasonal patterns. These work as explanatory variables that are added to the
    input data. The cyclical nature of the Fourier series is shown in the following
    figure:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性虚拟变量和傅里叶级数是捕捉季节性模式重复的变量。它们作为解释变量，添加到输入数据中。傅里叶级数的周期性特征如下图所示：
- en: '![Figure 3.1: Fourier deterministic series that capture seasonality](img/B21145_03_004.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：捕捉季节性的傅里叶确定性序列](img/B21145_03_004.jpg)'
- en: 'Figure 3.1: Fourier deterministic series that capture seasonality'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：捕捉季节性的傅里叶确定性序列
- en: Note that this process is independent of the neural network used for training.
    In this recipe, we resorted to a TCN but we could have picked any learning algorithm
    for multiple regression.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一过程与用于训练的神经网络无关。在这个食谱中，我们使用了TCN，但我们也可以选择任何用于多重回归的学习算法。
- en: There’s more…
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'An alternative to the Fourier series or seasonal dummies is repeating basis
    functions. Instead of using trigonometric series, seasonality is modeled using
    radial basis functions. These are implemented in the `sklego` `Python` package.
    You can check out the documentation at the following link: [https://scikit-lego.netlify.app/api/preprocessing.html#sklego.preprocessing.RepeatingBasisFunction](https://scikit-lego.netlify.app/api/preprocessing.html#sklego.preprocessing.RepeatingBasisFunction).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶级数或季节性虚拟变量的替代方法是重复基函数。与使用三角级数不同，季节性通过径向基函数来建模。这些在`sklego` `Python`包中实现。你可以查看以下链接的文档：[https://scikit-lego.netlify.app/api/preprocessing.html#sklego.preprocessing.RepeatingBasisFunction](https://scikit-lego.netlify.app/api/preprocessing.html#sklego.preprocessing.RepeatingBasisFunction)。
- en: 'Sometimes, a time series can exhibit seasonality at multiple periods. For example,
    the example daily time series can show repeating patterns not only every month
    but also every year. In this recipe, we computed seasonal dummies that provide
    information about different periods, namely the month and day of the week. But
    you can also do this with a Fourier series by passing multiple periods. Here’s
    how you could capture weekly and yearly seasonality with a Fourier series:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，时间序列可能会在多个周期内表现出季节性。例如，示例中的日度时间序列不仅每月会有重复的模式，还可能每年重复。在本例中，我们计算了季节性虚拟变量，这些变量提供了有关不同周期的信息，即月份和星期几。但你也可以通过传递多个周期来使用傅里叶级数。以下是如何使用傅里叶级数捕捉每周和每年的季节性：
- en: '[PRE50]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The preceding code would compute `four` Fourier series for each period (`two`
    sine and `two` cosine waves for each).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将为每个周期计算`四`个傅里叶级数（每个周期`两个`正弦和`两个`余弦波）。
- en: Another important recurrent phenomenon in time series is holidays, some of which
    move year after year (for example, Easter). A common way to model these events
    is by using binary dummy variables.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在时间序列中常见的重要现象是节假日，其中一些节假日是每年变动的（例如复活节）。一种常见的建模这些事件的方法是使用二元虚拟变量。
- en: Handling seasonality – seasonal differencing
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理季节性 – 季节性差分
- en: In this recipe, we show how differencing can be used to model seasonal patterns
    in time series.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，我们展示了如何使用差分来建模时间序列中的季节性模式。
- en: Getting ready
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We’ve learned to use first differences to remove the trend from time series.
    Differencing can also work for seasonality. But, instead of taking the difference
    between consecutive observations, for each point, you subtract the value of the
    previous observation from the same season. For example, suppose you’re modeling
    monthly data. You perform seasonal differencing by subtracting the value of February
    of the previous year from the value of February of the current year.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了使用第一次差分来去除时间序列中的趋势。差分也可以用于季节性。但是，不是取连续观察值之间的差异，而是对每个点，从同一季节中减去前一年的相应观测值。例如，假设你正在建模月度数据。你通过从当前年2月的值中减去上一年2月的值来执行季节性差分。
- en: 'The process is similar to what we did with first differences to remove the
    trend. Let’s start by loading the data:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程与我们通过第一次差分去除趋势时做的相似。我们首先加载数据：
- en: '[PRE51]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In this recipe, we’ll use seasonal differencing to remove yearly seasonality.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，我们将使用季节性差分来去除每年的季节性。
- en: How to do it…
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We resort to the `shift``()` method to apply the differencing operation:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`shift()`方法来应用差分操作：
- en: '[PRE52]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'After differencing the series, we transformed it for supervised learning using
    `series_to_supervised`. Then, we can train a neural network with the differenced
    data:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 差分序列后，我们使用`series_to_supervised`将其转化为监督学习格式。然后，我们可以用差分后的数据训练神经网络：
- en: '[PRE53]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In this case, we need to revert the differencing operation to get the forecasts
    in the original scale of the time series. We do that as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要反转差分操作，以便获得原始时间序列尺度上的预测值。我们按以下方式操作：
- en: '[PRE54]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Essentially, we add back the shifted test series to the denormalized predictions.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们将位移后的测试序列添加回去，与去标准化后的预测值结合。
- en: How it works…
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Seasonal differencing removes periodic variations, thus stabilizing the level
    of the series and making it stationary.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性差分去除了周期性变化，从而稳定了序列的水平，使其平稳。
- en: Seasonal differencing is particularly effective when the seasonal patterns change
    in magnitude and periodicity. In such cases, it’s usually a better approach than
    seasonal dummies or a Fourier series.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 当季节性模式在幅度和周期性上发生变化时，季节性差分特别有效。在这种情况下，季节性差分通常比季节性虚拟变量或傅里叶级数更有效。
- en: Handling seasonality – seasonal decomposition
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理季节性 – 季节性分解
- en: This recipe describes yet another approach to modeling seasonality, this time
    using a time series decomposition approach.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法描述了另一种建模季节性的方法，这次使用时间序列分解方法。
- en: Getting ready
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We learned about time series decomposition methods in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019).
    Decomposition methods aim at extracting the individual parts that make up a time
    series.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第1章*](B21145_01.xhtml#_idTextAnchor019)中学习了时间序列分解方法。分解方法旨在提取组成时间序列的各个部分。
- en: We can use this approach to deal with seasonality. The idea is to separate the
    seasonal component from the rest (trend plus residuals). We can use a deep neural
    network to model the seasonally adjusted series. Then, we use a simple model to
    forecast the seasonal component.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这种方法来处理季节性。其思想是将季节性成分与其余部分（趋势加残差）分开。我们可以使用深度神经网络来建模季节调整后的序列。然后，使用简单模型来预测季节成分。
- en: Again, we’ll start with the daily solar radiation time series. This time, we
    won’t split training and testing to show how the forecasts are obtained in practice.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将从日常太阳辐射时间序列开始。这一次，我们不拆分训练和测试数据，以展示预测是如何在实践中获得的。
- en: How to do it…
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We start by decomposing the time series using STL. We learned about this method
    in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019):'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用STL对时间序列进行分解。在[*第1章*](B21145_01.xhtml#_idTextAnchor019)中我们学习了这种方法：
- en: '[PRE55]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The seasonally adjusted series and the seasonal component are shown in the
    following figure:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 季节调整后的序列和季节成分如下图所示：
- en: '![Figure 3.2: Seasonal part and the remaining seasonally adjusted series](img/B21145_03_005.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2：季节部分和剩余的季节调整后序列](img/B21145_03_005.jpg)'
- en: 'Figure 3.2: Seasonal part and the remaining seasonally adjusted series'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：季节性部分和剩余的季节性调整序列
- en: 'Then, we use an LSTM to model the seasonally adjusted series. We’ll use a process
    similar to what we did before in previous recipes:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用LSTM模型对季节性调整后的序列进行建模。我们将采用类似于之前在其他配方中的方法：
- en: '[PRE56]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preceding code trains the LSTM on the seasonally adjusted series. Now,
    we use it to forecast the next 14 days of data:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在季节性调整后的序列上训练了LSTM模型。现在，我们用它来预测接下来14天的数据：
- en: '[PRE57]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This is what we see in the preceding code:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在前面的代码中看到的：
- en: We get the latest `three` lags from the time series and structure it as the
    input data
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取时间序列中的最新`三个`滞后值，并将其结构化为输入数据
- en: We use the model to predict the next value of the series
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用模型预测序列的下一个值
- en: Then we denormalize the forecast using the `scaler` object
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`scaler`对象对预测结果进行去归一化处理
- en: 'Now, we need to forecast the seasonal component. This is usually done with
    a seasonal naive method. In this recipe, we’ll use the implementation available
    in the `sktime` package:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要预测季节性成分。这通常通过季节性简单方法来完成。在本节中，我们将使用`sktime`包中的实现：
- en: '[PRE58]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `NaiveForecaster` object fits with the seasonal component. The idea of this
    method is to predict future observations using the previous known value from the
    same season.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`NaiveForecaster`对象与季节性成分拟合。该方法的思路是使用来自同一季节的已知前一个值来预测未来的观测值。'
- en: 'Finally, we get the final forecast by adding the two predictions:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过加上这两个预测结果得到最终的预测值：
- en: '[PRE59]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This addition reverts the decomposition process carried out before, and we get
    the forecast in the original series scale.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这个加法操作恢复了之前执行的分解过程，我们得到了原始序列尺度下的预测值。
- en: How it works…
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Modeling seasonality with a decomposition approach involves removing the seasonal
    part and modeling the seasonally adjusted time series with a neural network. Another
    simpler model is used to forecast the future of the seasonal part.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分解方法建模季节性，涉及去除季节性部分，并使用神经网络对季节性调整后的时间序列进行建模。另一个更简单的模型用于预测季节性部分的未来。
- en: This process is different than when using seasonal dummies, a Fourier series,
    or seasonal differencing. Seasonal dummies or a Fourier series work as extra input
    variables for the neural network to model. In the case of decomposition or differencing,
    the time series is transformed before modeling. This means that we need to revert
    these transformations after making the predictions with the neural network. With
    decomposition, this means adding the forecasts of the seasonal part. Differencing
    is also reverted by adding back the previous values from the same season.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程不同于使用季节性虚拟变量、傅里叶级数或季节差分的情况。季节性虚拟变量或傅里叶级数作为额外的输入变量，用于神经网络建模。而在分解或差分的情况下，时间序列会在建模之前进行变换。这意味着在使用神经网络进行预测后，我们需要恢复这些变换。对于分解来说，这意味着要加上季节性部分的预测值。差分也是通过加回来自同一季节的前一个值来恢复的。
- en: Handling non-constant variance – log transformation
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理非恒定方差 – 对数变换
- en: We’ve learned how to deal with changes in the level of the time series that
    occur due to either trend or seasonal patterns. In this recipe, we’ll deal with
    changes in the variance of time series.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何处理由于趋势或季节性模式导致的时间序列水平变化。在本节中，我们将处理时间序列方差的变化。
- en: Getting ready
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We’ve learned in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019) that some time
    series are heteroscedastic, which means that the variance changes over time. Non-constant
    variance is problematic as it makes the learning process more difficult.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第1章*](B21145_01.xhtml#_idTextAnchor019)中已经学到，一些时间序列是异方差的，这意味着方差随时间变化。非恒定方差是一个问题，因为它使得学习过程更加困难。
- en: 'Let’s start by splitting the solar radiation time series into training and
    testing sets:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始将太阳辐射时间序列分割为训练集和测试集：
- en: '[PRE60]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Again, we leave the last 20% of observations for testing.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将最后的20%的观测值留作测试。
- en: How to do it…
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: We’ll show how to stabilize the variance of a time series using the logarithm
    transformation and a Box-Cox power transformation.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何使用对数变换和Box-Cox幂变换来稳定时间序列的方差。
- en: Log transformation
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数变换
- en: 'In [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019), we defined the `LogTransformation`
    class that applies the logarithm to a time series:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B21145_01.xhtml#_idTextAnchor019)中，我们定义了应用对数变换的`LogTransformation`类：
- en: '[PRE61]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'You can apply the transformation as follows:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照如下方式应用变换：
- en: '[PRE62]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The `train_log` and `test_log` objects are the transformed datasets with a stabilized
    variance.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_log`和`test_log`对象是具有稳定方差的转换后的数据集。'
- en: Box-Cox transformation
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Box-Cox转换
- en: 'The logarithm is often an effective approach to stabilize the variance, and
    is a particular instance of the Box-Cox method. You can apply this method using
    the `boxcox``()` function from `scipy`:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 对数通常是稳定方差的有效方法，它是Box-Cox方法的一个特例。你可以使用`scipy`中的`boxcox()`函数应用这种方法：
- en: '[PRE63]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The Box-Cox method relies on a `lambda` parameter (`bc_lambda`), which we estimate
    using the training set. Then, we use it to transform the test set as well:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Box-Cox方法依赖于`lambda`参数（`bc_lambda`），我们使用训练集来估算该参数。然后，我们也用它来转换测试集：
- en: '[PRE64]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: After transforming the data using either the logarithm or the Box-Cox transformation,
    we train a neural network.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用对数或Box-Cox转换后，我们将训练一个神经网络。
- en: Modeling
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模
- en: 'The training process is identical to what we did in the previous recipes. We’ll
    continue the recipe using the transformed series with the logarithm (but the process
    would be the same for the Box-Cox case):'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程与我们在之前的配方中所做的完全相同。我们将继续使用经过对数转换的系列进行该配方（但Box-Cox情况下的过程也是相同的）：
- en: '[PRE65]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'After training, we run the model on the test set. The predictions need to be
    reverted to the original scale of the time series. This is done with the following
    code:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完毕后，我们会在测试集上运行模型。预测结果需要恢复到时间序列的原始尺度。这可以通过以下代码完成：
- en: '[PRE66]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'After denormalizing the predictions, we also use the `inverse_transform``()`
    method to revert the log transformation. With the Box-Cox transformation, this
    process could be done as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在将预测结果去归一化后，我们还使用`inverse_transform()`方法来恢复对数转换。对于Box-Cox转换，以下是可以执行的步骤：
- en: '[PRE67]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In the preceding code, we pass the transformed predictions and the `bc_lambda`
    transformation parameter to get the forecasts in the original scale.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们传入了转换后的预测结果和`bc_lambda`转换参数，以获取原始尺度下的预测值。
- en: How it works…
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The process carried out in this recipe attempts to mitigate the problem of non-constant
    variance. Both the logarithm transformation and the Box-Cox method can be used
    to stabilize the variance. These methods also bring the data closer to a `Normal`
    distribution. This type of transformation benefits the training of neural networks
    as it helps avoid saturation areas in the optimization process.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中进行的处理旨在缓解方差不稳定的问题。对数转换和Box-Cox方法都可以用来稳定方差。这些方法还可以将数据拉近`正态`分布。这种转换有助于神经网络的训练，因为它有助于避免优化过程中的饱和区域。
- en: The transformation methods work directly on the input data, so they are agnostic
    to the learning algorithm. The models work with transformed data, which means
    that the forecasts need to be transformed back to the original scale of the time
    series.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 转换方法直接作用于输入数据，因此它们与学习算法无关。模型使用转换后的数据，这意味着预测结果需要转换回时间序列的原始尺度。
