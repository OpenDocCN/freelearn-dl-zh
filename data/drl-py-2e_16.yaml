- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Deep Reinforcement Learning with Stable Baselines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Stable Baselines进行深度强化学习
- en: So far, we have learned various deep **reinforcement learning** (**RL**) algorithms.
    Wouldn't it be nice if we had a library to easily implement a deep RL algorithm?
    Yes! There are various libraries available to easily build a deep RL algorithm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了各种深度**强化学习**（**RL**）算法。如果我们有一个库，可以轻松实现深度RL算法，那该多好呢？是的！目前有多个库可以轻松构建深度RL算法。
- en: One such popular deep RL library is OpenAI Baselines. OpenAI Baselines provides
    an efficient implementation of many deep RL algorithms, which makes them easier
    to use. However, OpenAI Baselines does not provide good documentation. So, we
    will look at the fork of OpenAI Baselines called **Stable Baselines**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的深度强化学习库是OpenAI Baselines。OpenAI Baselines提供了许多深度强化学习算法的高效实现，使得它们更容易使用。然而，OpenAI
    Baselines并没有提供很好的文档。因此，我们将关注OpenAI Baselines的一个分支——**Stable Baselines**。
- en: Stable Baselines is an improved implementation of OpenAI Baselines. Stable Baselines
    is easier to use and it also includes state-of-the-art deep RL algorithms along
    with several useful features. We can use Stable Baselines for quickly prototyping
    the RL model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Baselines是OpenAI Baselines的改进版。Stable Baselines更易于使用，它还包括最先进的深度RL算法和一些有用的功能。我们可以使用Stable
    Baselines快速原型化RL模型。
- en: Let's start off the chapter by installing Stable Baselines, and then we will
    learn how to create our first agent using the library. Next, we will learn about
    vectorized environments. Going further, we will learn to implement several deep
    RL algorithms using Stable Baselines along with exploring various functionalities
    of baselines.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装Stable Baselines开始本章内容，然后我们将学习如何使用该库创建第一个代理。接下来，我们将学习向量化环境。然后，我们将学习如何使用Stable
    Baselines实现几种深度强化学习算法，并探索基准库的各种功能。
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习以下内容：
- en: Installing Stable Baselines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Stable Baselines
- en: Creating our first agent with Stable Baselines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Stable Baselines创建我们的第一个代理
- en: Multiprocessing with vectorized environments
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用向量化环境进行多进程处理
- en: Playing Atari games with DQN and its variants
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DQN及其变种玩Atari游戏
- en: Lunar lander using A2C
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用A2C进行月球着陆任务
- en: Swinging up a pendulum using DDPG
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DDPG摆动起摆锤
- en: Training an agent to walk using TRPO
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TRPO训练代理行走
- en: Implementing GAIL
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现GAIL
- en: Let's begin the chapter by installing Stable Baselines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装Stable Baselines开始本章内容。
- en: Installing Stable Baselines
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Stable Baselines
- en: 'First, let''s install the dependencies:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装所需的依赖项：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Several deep RL algorithms require MPI to run, so, let''s install MPI:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些深度RL算法需要MPI才能运行，因此，让我们安装MPI：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can install Stable Baselines through `pip`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过`pip`安装Stable Baselines：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that currently, Stable Baselines works only with TensorFlow version 1.x.
    So, make sure you are running the Stable Baselines experiment with TensorFlow
    1.x.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前Stable Baselines仅支持TensorFlow 1.x版本。因此，请确保你在使用TensorFlow 1.x版本运行Stable
    Baselines实验。
- en: Now that we have installed Stable Baselines, let's see how to create our first
    agent using it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Stable Baselines，让我们看看如何使用它创建第一个代理。
- en: Creating our first agent with Stable Baselines
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Stable Baselines创建我们的第一个代理
- en: Now, let's build our first deep RL algorithm using Stable Baselines. Let's create
    a simple agent using a **Deep Q Network** (**DQN**) for the mountain car climbing
    task. We know that in the mountain car climbing task, a car is placed between
    two mountains and the goal of the agent is to drive up the mountain on the right.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Stable Baselines构建我们的第一个深度RL算法。我们将使用**深度Q网络**（**DQN**）创建一个简单的代理，用于山地汽车爬坡任务。我们知道，在山地汽车爬坡任务中，汽车被放置在两座山之间，代理的目标是驾车爬上右边的山。
- en: 'First, let''s import `gym` and `DQN` from `stable_baselines`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从`stable_baselines`导入`gym`和`DQN`：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a mountain car environment:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个山地汽车环境：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s instantiate our agent. As we can observe in the following code,
    we are passing `MlpPolicy`, which implies that our network is a multilayer perceptron:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实例化我们的代理。正如我们在下面的代码中看到的，我们传递了`MlpPolicy`，这意味着我们的网络是一个多层感知机：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let''s train the agent by specifying the number of time steps we want
    to train:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过指定训练的时间步数来训练代理：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That's it. Building a DQN agent and training it is that simple.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。构建一个DQN代理并训练它就这么简单。
- en: Evaluating the trained agent
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估训练后的代理
- en: 'We can also evaluate the trained agent by looking at the mean rewards using
    `evaluate_policy`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用`evaluate_policy`来评估训练后的代理，查看平均奖励：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the following code, `agent` is the trained agent, `agent.get_env()` gets
    the environment we trained our agent in, and `n_eval_episodes` represents the
    number of episodes we need to evaluate our agent:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，`agent`是训练好的代理，`agent.get_env()`获取我们训练代理的环境，`n_eval_episodes`表示我们需要评估代理的集数：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Storing and loading the trained agent
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储和加载训练好的代理
- en: With Stable Baselines, we can also save and load our trained agent to and from
    disk.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Stable Baselines，我们还可以将训练好的代理保存到磁盘并从磁盘加载。
- en: 'We can save the agent as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下保存代理：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After saving, we can load the agent as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 保存之后，我们可以如下加载代理：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Viewing the trained agent
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看训练好的代理
- en: After training, we can also have a look at how our trained agent performs in
    the environment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们还可以看看我们的训练代理在环境中的表现。
- en: 'Initialize the state:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化状态：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For 5,000 steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于5000步：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Predict the action to perform in the given state using our trained agent:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们训练好的代理预测在给定状态下执行的动作：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Perform the predicted action:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 执行预测的动作：
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Update `state` to the current state:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将`state`更新为当前状态：
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Render the environment:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE16]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can see how our trained agent performs in the environment:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到训练好的代理在环境中的表现：
- en: '![](img/B15558_16_01.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_01.png)'
- en: 'Figure 16.1: Agent learning to climb mountain'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：代理学习爬山
- en: Putting it all together
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容结合起来
- en: 'Now, let''s look at the final code combining everything we learned so far:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下结合我们迄今为止学到的所有内容的最终代码：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we have a basic idea of how to use Stable Baselines, let's explore
    it in detail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经对如何使用Stable Baselines有了基本的了解，接下来我们将详细探讨它。
- en: Vectorized environments
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化环境
- en: One of the very interesting and useful features of Stable Baselines is that
    we can train our agent in multiple independent environments either in separate
    processes (using **SubprocVecEnv**) or in the same process (using **DummyVecEnv**).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Baselines的一个非常有趣且有用的功能是，我们可以在多个独立环境中训练我们的代理，既可以在单独的进程中（使用**SubprocVecEnv**），也可以在同一个进程中（使用**DummyVecEnv**）。
- en: For example, say we are training our agent in a cart pole balancing environment
    – instead of training our agent only in a single cart pole balancing environment,
    we can train our agent in the multiple cart pole balancing environments.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在一个平衡小车环境中训练我们的代理——我们可以不只在一个小车平衡环境中训练，而是在多个小车平衡环境中训练我们的代理。
- en: We generally train our agent in a single environment per step but now we can
    train our agent in multiple environments per step. This helps our agent to learn
    more quickly. Now, our state, action, reward, and done will be in the form of
    a vector since we are training our agent in multiple environments. So, we call
    this a vectorized environment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常每一步训练代理时只使用一个环境，但现在我们可以每一步在多个环境中训练代理。这有助于代理更快地学习。现在，我们的状态、动作、奖励和结束状态将以向量的形式呈现，因为我们正在多个环境中训练代理。因此，我们称之为向量化环境。
- en: 'There are two types of vectorized environment offered by Stable Baselines:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Baselines提供了两种类型的向量化环境：
- en: SubprocVecEnv
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SubprocVecEnv
- en: DummyVecEnv
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DummyVecEnv
- en: SubprocVecEnv
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SubprocVecEnv
- en: In the subproc vectorized environment, we run each environment in a **separate**
    process (taking advantage of multiprocessing). Now, let's see how to create the
    subproc vectorized environment.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在子进程向量化环境中，我们在**单独**的进程中运行每个环境（利用多进程）。现在，让我们看看如何创建子进程向量化环境。
- en: 'First, let''s import `SubprocVecEnv`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入`SubprocVecEnv`：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we create a function called `make_env` for initializing our environment:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为`make_env`的函数，用于初始化我们的环境：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we can create the subproc vectorized environment as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以如下创建子进程向量化环境：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: DummyVecEnv
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DummyVecEnv
- en: In the dummy vectorized environment, we run each environment in sequence on
    the current Python process. It does not support multiprocessing. Now, let's see
    how to create the dummy vectorized environment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟向量化环境中，我们在当前的Python进程中按顺序运行每个环境。它不支持多进程。现在，让我们看看如何创建虚拟向量化环境。
- en: 'First, let''s import `DummyVecEnv`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入`DummyVecEnv`：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we can create the dummy vectorized environment as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以如下创建虚拟向量化环境：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have learned to train the agent in multiple independent environments
    using vectorized environments, in the next section, we will see how to integrate
    custom environments into Stable Baselines.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经学会了如何在多个独立环境中使用向量化环境训练代理，接下来的部分，我们将学习如何将自定义环境集成到Stable Baselines中。
- en: Integrating custom environments
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成自定义环境
- en: We can also use Stable Baselines to train an agent in our own environment. While
    creating our own environment, we need to make sure that our custom environment
    follows the Gym interface. That is, our environment should include methods such
    as `step`, `reset`, `render`, and so on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Stable Baselines在自定义环境中训练智能体。在创建自定义环境时，我们需要确保我们的环境遵循Gym接口。也就是说，我们的环境应该包含如`step`、`reset`、`render`等方法。
- en: 'Suppose the name of our custom environment is `CustomEnv`. First, we instantiate
    our custom environment as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们自定义环境的名称是`CustomEnv`。首先，我们按如下方式实例化自定义环境：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we can train our agent in the custom environment as usual:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以像往常一样在自定义环境中训练我们的智能体：
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: That's it. In the next section, let's learn how to play Atari games using a
    DQN and its variants.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。在下一部分，让我们学习如何使用DQN及其变体来玩Atari游戏。
- en: Playing Atari games with a DQN and its variants
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DQN及其变体玩Atari游戏
- en: 'Now, let''s learn how to create a DQN to play Atari games with Stable Baselines.
    First, let''s import the necessary modules:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何创建一个DQN来玩Atari游戏，使用Stable Baselines。首先，让我们导入必要的模块：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Since we are dealing with Atari games, we can use a convolutional neural network
    instead of a vanilla neural network. So, we use `CnnPolicy`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是Atari游戏，我们可以使用卷积神经网络（CNN）而不是普通的神经网络。所以，我们使用`CnnPolicy`：
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We learned that we preprocess the game screen before feeding it to the agent.
    With Stable Baselines, we don''t have to preprocess manually; instead, we can
    make use of the `make_atari` module, which takes care of preprocessing the game
    screen:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在将游戏画面输入智能体之前，我们需要对其进行预处理。使用Stable Baselines时，我们不需要手动预处理；相反，我们可以使用`make_atari`模块，它会负责预处理游戏画面：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let''s create an Atari game environment. Let''s create the Ice Hockey
    game environment:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个Atari游戏环境。我们先创建冰球游戏环境：
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Instantiate the agent:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化智能体：
- en: '[PRE29]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Train the agent:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 训练智能体：
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After training the agent, we can have a look at how our trained agent performs
    in the environment:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完智能体之后，我们可以查看训练后的智能体在环境中的表现：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code displays how our trained agent plays the ice hockey game:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了我们的训练智能体如何玩冰球游戏：
- en: '![](img/B15558_16_02.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_02.png)'
- en: 'Figure 16.2: Agent playing the Ice Hockey game'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：智能体正在玩冰球游戏
- en: Implementing DQN variants
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现DQN变体
- en: We just learned how to implement DQN using Stable Baselines. Now, let's see
    how to implement the variants of DQN, such as double DQN, DQN with prioritized
    experience replay, and dueling DQN. Implementing DQN variants is very simple with
    Baselines.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了如何使用Stable Baselines实现DQN。现在，让我们看看如何实现DQN的变体，如双重DQN、带优先经验回放的DQN和对抗DQN。在Baselines中实现DQN变体非常简单。
- en: 'First, we define our keyword arguments as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的关键字参数如下：
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, while instantiating our agent, we just need to pass the keyword arguments:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在实例化智能体时，我们只需要传递关键字参数：
- en: '[PRE33]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we can train the agent as usual:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像往常一样训练智能体：
- en: '[PRE34]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That's it! Now we have the dueling double DQN with prioritized experience replay.
    In the next section, we will learn how to play the lunar lander game using the
    **Advantage Actor-Critic Algorithm** (**A2C**).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在我们已经有了带优先经验回放的对抗双重DQN。接下来的部分，我们将学习如何使用**优势演员-评论家算法**（**A2C**）玩月球着陆游戏。
- en: Lunar lander using A2C
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用A2C进行月球着陆
- en: 'Let''s learn how to implement A2C with Stable Baselines for the lunar landing
    task. In the lunar lander environment, our agent drives the space vehicle, and
    the goal of the agent is to land correctly on the landing pad. If our agent (lander)
    lands away from the landing pad, then it loses the reward, and the episode will
    get terminated if the agent crashes or comes to rest. The action space of the
    environment includes four discrete actions, which are: do nothing, fire left orientation
    engine, fire main engine, and fire right orientation engine. Now, let''s see how
    to train the agent using A2C to correctly land on the landing pad.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何使用Stable Baselines实现A2C来处理月球着陆任务。在月球着陆环境中，我们的智能体驾驶太空飞行器，目标是准确地在着陆平台上着陆。如果我们的智能体（着陆器）偏离着陆平台，那么它将失去奖励，且如果智能体坠毁或停下，回合将会终止。环境的动作空间包括四个离散动作：什么也不做、启动左侧定向引擎、启动主引擎以及启动右侧定向引擎。现在，让我们看看如何使用A2C训练智能体，以便正确地在着陆平台上着陆。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE35]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create the lunar lander environment using Gym:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gym创建月球着陆环境：
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s use the dummy vectorized environment. We learned that in the dummy vectorized
    environment, we run each environment in the same process:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用虚拟向量化环境。我们知道，在虚拟向量化环境中，我们在同一个进程中运行每个环境：
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create the agent:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 创建智能体：
- en: '[PRE38]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Train the agent:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练智能体：
- en: '[PRE39]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After training, we can evaluate our agent by looking at the mean rewards:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结束后，我们可以通过查看平均奖励来评估我们的智能体：
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also have a look at how our trained agent performs in the environment:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看看训练后的智能体在环境中的表现：
- en: '[PRE41]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The preceding code will show how well our trained agent lands on the landing
    pad:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将展示我们训练后的智能体如何成功降落在着陆平台上：
- en: '![](img/B15558_16_03.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_03.png)'
- en: 'Figure 16.3: Agent playing the lunar lander game'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3：智能体玩月球着陆者游戏
- en: Creating a custom network
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建自定义网络
- en: In the previous section, we learned how to create an A2C using Stable Baselines.
    Instead of using the default network, can we customize the network architecture?
    Yes! With Stable Baselines, we can also use our own custom architecture. Let's
    see how to do that.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用 Stable Baselines 创建 A2C。我们可以定制网络架构吗？当然可以！通过 Stable Baselines，我们还可以使用自定义的网络架构。让我们来看一下如何做到这一点。
- en: 'First, let''s import the feedforward policy (feedforward network):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入前馈策略（前馈网络）：
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can instantiate the agent with the custom policy as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式使用自定义策略来实例化智能体：
- en: '[PRE44]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can train the agent as usual:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像往常一样训练智能体：
- en: '[PRE45]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: That's it. Similarly, we can create our own custom network. In the next section,
    let's learn how to perform the *inverted pendulum swing-up* task using the **Deep
    Deterministic Policy Gradient (DDPG)** algorithm.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。类似地，我们可以创建自己的自定义网络。在下一节中，让我们学习如何使用**深度确定性策略梯度（DDPG）**算法执行*倒立摆摆动*任务。
- en: Swinging up a pendulum using DDPG
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DDPG 摆动倒立摆
- en: 'Let''s learn how to implement the DDPG for the inverted pendulum swing-up task
    using Stable Baselines. First, let''s import the necessary libraries:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何使用 Stable Baselines 实现倒立摆摆动任务的 DDPG。首先，让我们导入必要的库：
- en: '[PRE46]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create the pendulum environment using Gym:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Gym 创建倒立摆环境：
- en: '[PRE47]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Get the number of actions:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 获取动作的数量：
- en: '[PRE48]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We know that in DDPG, instead of selecting the action directly, we add some
    noise using the Ornstein-Uhlenbeck process to ensure exploration. So, we create
    the action noise as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在 DDPG 中，我们不是直接选择动作，而是使用 Ornstein-Uhlenbeck 过程添加一些噪声，以确保探索。因此，我们创建动作噪声如下：
- en: '[PRE49]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Instantiate the agent:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化智能体：
- en: '[PRE50]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Train the agent:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 训练智能体：
- en: '[PRE51]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: After training the agent, we can also look at how our trained agent swings up
    the pendulum by rendering the environment. Can we also look at the computational
    graph of DDPG? Yes! In the next section, we will learn how to do that.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完智能体后，我们还可以通过渲染环境来查看训练后的智能体如何摆动倒立摆。我们也可以查看 DDPG 的计算图吗？是的！在下一节中，我们将学习如何做到这一点。
- en: Viewing the computational graph in TensorBoard
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorBoard 中查看计算图
- en: 'With Stable Baselines, it is easier to view the computational graph of our
    model in TensorBoard. In order to that, we just need to pass the directory where
    we will store our log files while instantiating the agent, as shown here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Stable Baselines，我们可以更轻松地在 TensorBoard 中查看模型的计算图。为了做到这一点，我们只需要在实例化智能体时传递将存储日志文件的目录，如下所示：
- en: '[PRE52]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, we can train the agent:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以训练智能体：
- en: '[PRE53]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'After training, open the terminal and type the following command to run TensorBoard:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结束后，打开终端并输入以下命令来运行 TensorBoard：
- en: '[PRE54]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As we can observe, we can now see the computational graph of the DDPG model
    (agent):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们现在可以查看 DDPG 模型（智能体）的计算图：
- en: '![](img/B15558_16_04.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_04.png)'
- en: 'Figure 16.4: Computational graph of DDPG'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4：DDPG 的计算图
- en: From *Figure 16.4*, we can understand how the DDPG computational graph is generated
    just as we learned in *Chapter 12*, *Learning DDPG, TD3, and SAC*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 16.4*中，我们可以理解如何生成 DDPG 的计算图，就像我们在*第 12 章*中学习的那样，*学习 DDPG、TD3 和 SAC*。
- en: 'Now, let''s expand and look into the model node for more clarity:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们展开并深入查看模型节点，以便更清晰地理解：
- en: '![](img/B15558_16_05.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_05.png)'
- en: 'Figure 16.5: Computational graph of DDPG'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5：DDPG 的计算图
- en: As we can observe from *Figure 16.5*, our model includes the policy (actor)
    and Q (critic) network.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如从*图 16.5*所示，我们的模型包括策略（演员）和 Q（评论员）网络。
- en: Now that we have learned how to use Stable Baselines to implement DDPG for the
    inverted pendulum swing-up task, in the next section we will learn how to implement
    TRPO using Stable Baselines.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何使用 Stable Baselines 实现 DDPG 来完成倒立摆摆动任务，接下来我们将学习如何使用 Stable Baselines
    实现 TRPO。
- en: Training an agent to walk using TRPO
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TRPO 训练一个走路的智能体
- en: In this section, let's learn how to train the agent to walk using **Trust Region
    Policy Optimization** (**TRPO**).Let's use the MuJoCo environment for training
    the agent. **MuJoCo** stands for **Multi-Joint dynamics with Contact** and is
    one of the most popular simulators used for training agents to perform continuous
    control tasks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 **信任区域策略优化** (**TRPO**) 训练智能体走路。我们将使用 MuJoCo 环境来训练智能体。**MuJoCo**
    代表 **带接触的多关节动力学**，是用于训练智能体执行连续控制任务的最流行的模拟器之一。
- en: Note that MuJoCo is a proprietary physics engine, so we need to acquire a license
    to use it. Also, MuJoCo offers a free 30-day trial period. Installing MuJoCo requires
    a specific set of steps. So, in the next section, we will see how to install the
    MuJoCo environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，MuJoCo 是一个专有物理引擎，因此我们需要获得许可证才能使用它。此外，MuJoCo 提供 30 天的免费试用期。安装 MuJoCo 需要一系列特定的步骤。接下来，我们将看到如何安装
    MuJoCo 环境。
- en: Installing the MuJoCo environment
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 MuJoCo 环境
- en: 'First, in your home directory, create a new hidden folder called `.mujoco`.
    Next, go to the MuJoCo website ([https://www.roboti.us/](https://www.roboti.us/))
    and download MuJoCo according to your operating system. As shown in *Figure 16.6*,
    MuJoCo provides support for Windows, Linux, and macOS:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在您的主目录中创建一个新的隐藏文件夹，名为 `.mujoco`。接下来，访问 MuJoCo 网站 ([https://www.roboti.us/](https://www.roboti.us/))，并根据您的操作系统下载
    MuJoCo。如 *图 16.6* 所示，MuJoCo 支持 Windows、Linux 和 macOS：
- en: '![](img/B15558_16_06.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_06.png)'
- en: 'Figure 16.6: Different MuJoCo versions'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.6：不同的 MuJoCo 版本
- en: If you are using Linux, then you can download the zip file named `mujoco200
    linux`. After downloading the zip file, unzip the file and rename it to `mujoco200`.
    Now, copy the `mujoco200` folder and place the folder inside the `.mujoco` folder
    in your home directory.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是 Linux，则可以下载名为 `mujoco200 linux` 的压缩文件。下载压缩文件后，解压文件并将其重命名为 `mujoco200`。现在，将
    `mujoco200` 文件夹复制并将该文件夹放置在主目录中的 `.mujoco` 文件夹内。
- en: 'As *Figure 16.7* shows, now in our home directory, we have a `.mujoco` folder,
    and inside the `.mujoco` folder, we have a `mujoco200` folder:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 16.7* 所示，现在在我们的主目录中，我们有一个 `.mujoco` 文件夹，且在 `.mujoco` 文件夹内有一个 `mujoco200`
    文件夹：
- en: '![](img/B15558_16_07.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_07.png)'
- en: 'Figure 16.7: Installing MuJoCo'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.7：安装 MuJoCo
- en: 'Now, we need to obtain a trial license. First, go to [https://www.roboti.us/license.html](https://www.roboti.us/license.html)
    and register for the trial license, as *Figure 16.8* shows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要获取试用许可证。首先，访问 [https://www.roboti.us/license.html](https://www.roboti.us/license.html)
    并注册试用许可证，如 *图 16.8* 所示：
- en: '![](img/B15558_16_08.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_08.png)'
- en: 'Figure 16.8: Registering for the trial license'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8：注册试用许可证
- en: To register, we also need the computer id. As *Figure 16.8* shows, to the right
    of the **Computer id** field, we have the name of different platforms. Now, just
    click on your operating system and you will obtain the relevant executable `getid`
    file. For instance, if you are using Linux, then you will obtain a file named
    `getid_linux`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注册时，我们还需要计算机 ID。如 *图 16.8* 所示，在 **计算机 ID** 字段的右侧，我们有不同平台的名称。现在，只需点击您的操作系统，您就会获得相应的可执行
    `getid` 文件。例如，如果您使用的是 Linux，则会获得名为 `getid_linux` 的文件。
- en: 'After downloading the `getid_linux` file, run the following command on your
    terminal:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 `getid_linux` 文件后，在终端中运行以下命令：
- en: '[PRE55]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Then, run the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行以下命令：
- en: '[PRE56]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The preceding command will display your computer id. After getting the computer
    id, fill in the form and register to obtain a license. Once you click on the **Submit**
    button, you will get an email from Roboti LLC Licensing.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将显示您的计算机 ID。获取计算机 ID 后，填写表格并注册以获取许可。点击 **提交** 按钮后，您将收到 Roboti LLC Licensing
    发送的电子邮件。
- en: 'From the email, download the file named `mjkey.txt`. Next, place the `mjkey.txt`
    file in the `.mujoco` folder. As *Figure 16.9* shows, now our `.mujoco` hidden
    folder contains the `mjkey.txt` file and a folder named `mujoco200`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从电子邮件中下载名为 `mjkey.txt` 的文件。接下来，将 `mjkey.txt` 文件放入 `.mujoco` 文件夹中。如 *图 16.9*
    所示，现在我们的 `.mujoco` 隐藏文件夹中包含 `mjkey.txt` 文件和一个名为 `mujoco200` 的文件夹：
- en: '![](img/B15558_16_09.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_09.png)'
- en: 'Figure 16.9: Installing MuJoCo'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9：安装 MuJoCo
- en: 'Next, open your terminal and run the following command to edit the `bashrc`
    file:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开终端并运行以下命令来编辑 `bashrc` 文件：
- en: '[PRE57]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Copy the following line to the `bashrc` file and make sure to replace the username
    text with your own username:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下行复制到 `bashrc` 文件中，并确保将用户名文本替换为您自己的用户名：
- en: '[PRE58]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, save the file and exit the nano editor. Now, run the following command
    on your terminal:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，保存文件并退出nano编辑器。现在，在终端运行以下命令：
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Well done! We are almost there. Now, clone the MuJoCo GitHub repository:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们快完成了。现在，克隆MuJoCo的GitHub仓库：
- en: '[PRE60]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Navigate to the `mujoco-py` folder:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 进入`mujoco-py`文件夹：
- en: '[PRE61]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Update the packages:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 更新软件包：
- en: '[PRE62]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Install the dependencies:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 安装依赖项：
- en: '[PRE63]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, install MuJoCo:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，安装MuJoCo：
- en: '[PRE64]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To test the successful installation of MuJoCo, let''s run a Humanoid agent
    by taking a random action in the environment. So, create the following Python
    file named `mujoco_test.py` with the following code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试MuJoCo是否安装成功，让我们通过在环境中采取随机行动来运行一个Humanoid智能体。所以，创建一个名为`mujoco_test.py`的Python文件，内容如下：
- en: '[PRE65]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, open the terminal and run the Python file:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开终端并运行Python文件：
- en: '[PRE66]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The preceding code will render the `Humanoid` environment as *Figure 16.10
    shows*:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将渲染`Humanoid`环境，如*图16.10所示*：
- en: '![](img/B15558_16_10.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_10.png)'
- en: 'Figure 16.10: Humanoid environment'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：Humanoid环境
- en: Now that we have successfully installed MuJoCo, let's start implementing TRPO
    to train our agent to walk in the next section.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功安装了MuJoCo，在下一节中，我们将开始实现TRPO训练我们的智能体走路。
- en: Implementing TRPO
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现TRPO
- en: 'Import the necessary libraries:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE67]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create a vectorized `Humanoid` environment using `DummyVecEnv`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DummyVecEnv`创建一个向量化的`Humanoid`环境：
- en: '[PRE68]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Normalize the states (observations):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对状态（观测值）进行归一化：
- en: '[PRE69]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Instantiate the agent:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化智能体：
- en: '[PRE70]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Train the agent:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 训练智能体：
- en: '[PRE71]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'After training the agent, we can see how our trained agent learned to walk
    by rendering the environment:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完智能体后，我们可以通过渲染环境来看我们的训练智能体是如何学会走路的：
- en: '[PRE72]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Save the whole code used in this section in a Python file called `trpo.py`
    and then open the terminal and run the file:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 将本节中使用的整个代码保存到一个名为`trpo.py`的Python文件中，然后打开终端并运行该文件：
- en: '[PRE73]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We can see how our trained agent learned to walk in *Figure 16.11*:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图16.11*中看到我们的训练智能体是如何学会走路的：
- en: '![](img/B15558_16_11.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_11.png)'
- en: 'Figure 16.11: Agent learning to walk using TRPO'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11：使用TRPO训练智能体走路
- en: Always use the terminal to run the program that uses the MuJoCo environment.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 始终使用终端运行使用MuJoCo环境的程序。
- en: That's it. In the next section, we will learn how to record our trained agent's
    actions as a video.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。在下一节中，我们将学习如何将我们训练过的智能体的动作录制成视频。
- en: Recording the video
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 录制视频
- en: In the previous section, we trained our agent to learn to walk using TRPO. Can
    we also record a video of our trained agent? Yes! With Stable Baselines, we can
    easily record a video of our agent using the `VecVideoRecorder` module.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用TRPO训练了我们的智能体学会走路。我们也可以录制训练好的智能体的视频吗？可以！通过Stable Baselines，我们可以轻松地使用`VecVideoRecorder`模块录制智能体的视频。
- en: 'Note that to record the video, we need the `ffmpeg` package installed in our
    machine. If it is not installed, then install it using the following set of commands:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要录制视频，我们需要在机器上安装`ffmpeg`包。如果没有安装，可以使用以下命令集进行安装：
- en: '[PRE74]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, let''s import the `VecVideoRecorder` module:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入`VecVideoRecorder`模块：
- en: '[PRE75]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a function called `record_video` for recording the video:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为`record_video`的函数来录制视频：
- en: '[PRE76]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create the environment:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 创建环境：
- en: '[PRE77]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Instantiate the video recorder:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化视频录制器：
- en: '[PRE78]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Select actions in the environment using our trained agent where the number
    of time steps is set to the video length:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境中选择动作时，我们的训练智能体会将时间步数设置为视频长度：
- en: '[PRE79]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'That''s it! Now, let''s call our `record_video` function. Note that we are
    passing the environment name, our trained agent, the length of the video, and
    the name of our video file:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在，让我们调用我们的`record_video`函数。请注意，我们传递了环境名称、我们训练的智能体、视频时长和视频文件的名称：
- en: '[PRE80]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now, we will have a new file called `Humanoid_walk_TRPO-step-0-to-step-500.mp4`
    in the `videos` folder:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在`videos`文件夹中生成一个名为`Humanoid_walk_TRPO-step-0-to-step-500.mp4`的新文件：
- en: '![](img/B15558_16_12.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_12.png)'
- en: 'Figure 16.12: Recorded video'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12：录制的视频
- en: In this way, we can record our trained agent's action. In the next section,
    we will learn how to implement PPO using Stable Baselines.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以录制训练智能体的动作。在下一节中，我们将学习如何使用Stable Baselines实现PPO。
- en: Training a cheetah bot to run using PPO
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PPO训练猎豹机器人跑步
- en: 'In this section, let''s learn how to train the 2D cheetah bot to run using
    **Proximal Policy Optimization** (**PPO**). First, import the necessary libraries:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用**近端策略优化**（**PPO**）训练2D猎豹机器人跑步。首先，导入必要的库：
- en: '[PRE81]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Create a vectorized environment using `DummyVecEnv`:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DummyVecEnv`创建一个向量化环境：
- en: '[PRE82]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Normalize the state:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对状态进行归一化：
- en: '[PRE83]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Instantiate the agent:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化代理：
- en: '[PRE84]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Train the agent:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代理：
- en: '[PRE85]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'After training, we can see how our trained cheetah bot learned to run by rendering
    the environment:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以通过渲染环境看到我们训练的猎豹机器人是如何学会奔跑的：
- en: '[PRE86]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Save the whole code used in this section in a Python file called `ppo.py` and
    then open the terminal and run the file:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 将本节中使用的整个代码保存在名为`ppo.py`的Python文件中，然后打开终端并运行该文件：
- en: '[PRE87]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We can see how our trained cheetah bot learned to run, as *Figure 16.13* shows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们训练的猎豹机器人是如何学会奔跑的，如*图16.13*所示：
- en: '![](img/B15558_16_13.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_13.png)'
- en: 'Figure 16.13: 2D cheetah bot learning to run'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13：2D猎豹机器人学习奔跑
- en: Making a GIF of a trained agent
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制作一个训练代理的GIF
- en: In the previous section, we learned how to train the cheetah bot to run using
    PPO. Can we also create a GIF file of our trained agent? Yes! Let's see how to
    do that.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用PPO训练猎豹机器人奔跑。我们能否也创建一个训练代理的GIF文件？当然可以！让我们看看如何做到这一点。
- en: 'First, import the necessary libraries:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入必要的库：
- en: '[PRE88]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Initialize the list for storing images:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化用于存储图像的列表：
- en: '[PRE89]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Initialize the state by resetting the environment, where `agent` is the agent
    we trained in the previous section:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态，其中`agent`是我们在前一节中训练的代理：
- en: '[PRE90]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Render the environment and get the image:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境并获取图像：
- en: '[PRE91]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'For every step in the environment, save the image:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对环境中的每一步，保存图像：
- en: '[PRE92]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Create the GIF file as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式创建GIF文件：
- en: '[PRE93]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Now, we will have a new file called `HalfCheetah.gif`, as *Figure 16.14* shows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将获得一个名为`HalfCheetah.gif`的新文件，如*图16.14*所示：
- en: '![](img/B15558_16_14.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_16_14.png)'
- en: 'Figure 16.14: GIF of the trained agent'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14：训练代理的GIF
- en: In this way, we can obtain a GIF of our trained agent. In the next section,
    we will learn how to implement GAIL using Stable Baselines.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以获得训练代理的GIF。在下一节中，我们将学习如何使用Stable Baselines实现GAIL。
- en: Implementing GAIL
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现GAIL
- en: In this section, let's explore how to implement **Generative Adversarial Imitation
    Learning** (**GAIL**) with Stable Baselines. In *Chapter 15*, *Imitation Learning
    and Inverse RL*, we learned that we use the generator to generate the state-action
    pair in a way that the discriminator is not able to distinguish whether the state-action
    pair is generated using the expert policy or the agent policy. We train the generator
    to generate a policy similar to an expert policy using TRPO, while the discriminator
    is a classifier and it is optimized using Adam.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何使用Stable Baselines实现**生成对抗模仿学习**（**GAIL**）。在*第15章*，*模仿学习与逆向强化学习*中，我们了解到，使用生成器以一种方式生成状态-动作对，使得判别器无法区分该状态-动作对是使用专家策略还是代理策略生成的。我们训练生成器使用TRPO生成类似专家策略的策略，而判别器是一个分类器，使用Adam进行优化。
- en: To implement GAIL, we need expert trajectories so that our generator learns
    to mimic the expert trajectory. Okay, so how can we obtain the expert trajectory?
    First, we use the TD3 algorithm to generate expert trajectories and then create
    an expert dataset. Then, using this expert dataset, we train our GAIL agent. Note
    that instead of using TD3, we can also use any other algorithm for generating
    expert trajectories.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现GAIL，我们需要专家轨迹，这样我们的生成器才能学习模仿专家轨迹。那么，如何获得专家轨迹呢？首先，我们使用TD3算法生成专家轨迹，然后创建专家数据集。接着，使用这个专家数据集，我们训练GAIL代理。注意，除了使用TD3，我们还可以使用任何其他算法生成专家轨迹。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE94]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Instantiate the TD3 agent:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化TD3代理：
- en: '[PRE95]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Generate the expert trajectories:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 生成专家轨迹：
- en: '[PRE96]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Create the expert dataset using the expert trajectories:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 使用专家轨迹创建专家数据集：
- en: '[PRE97]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Instantiate the GAIL agent with the expert dataset (expert trajectories):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 使用专家数据集（专家轨迹）实例化GAIL代理：
- en: '[PRE98]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Train the GAIL agent:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAIL代理：
- en: '[PRE99]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: After training, we can also render the environment and see how our trained agent
    performs in the environment. That's it, implementing GAIL using Stable Baselines
    is that simple.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们还可以渲染环境，看看我们训练的代理在环境中的表现。就这样，使用Stable Baselines实现GAIL就是这么简单。
- en: Summary
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding what Stable Baselines is and how to
    install it. Then, we learned to create our first agent with Stable Baselines using
    a DQN. We also learned how to save and load an agent. Next, we learned how to
    create multiple independent environments using vectorization. We also learned
    about two types of vectorized environment called SubprocVecEnv and DummyVecEnv.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们理解了 Stable Baselines 是什么以及如何安装它。然后，我们学习了如何使用 DQN 在 Stable Baselines
    中创建我们的第一个智能体。我们还学习了如何保存和加载智能体。接下来，我们学习了如何通过向量化创建多个独立的环境。我们还学习了两种类型的向量化环境，分别是 SubprocVecEnv
    和 DummyVecEnv。
- en: We learned that in SubprocVecEnv, we run each environment in a different process,
    whereas in DummyVecEnv, we run each environment in the same process.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在 SubprocVecEnv 中，我们将每个环境运行在不同的进程中，而在 DummyVecEnv 中，我们将每个环境运行在同一个进程中。
- en: Later, we learned how to implement a DQN and its variants to play Atari games
    using Stable Baselines. Next, we learned how to implement A2C and also how to
    create a custom policy network. Moving on, we learned how to implement DDPG and
    also how to view the computational graph in TensorBoard.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何使用 Stable Baselines 实现 DQN 及其变体来玩 Atari 游戏。之后，我们学习了如何实现 A2C，以及如何创建自定义策略网络。接着，我们学习了如何实现
    DDPG，并且如何在 TensorBoard 中查看计算图。
- en: Going further, we learned how to set up the MuJoCo environment and how to train
    an agent to walk using TRPO. We also learned how to record a video of a trained
    agent. Next, we learned how to implement PPO and how to make a GIF of a trained
    agent. At the end of the chapter, we learned how to implement generative adversarial
    imitation learning using Stable Baselines.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，我们学习了如何设置 MuJoCo 环境，以及如何使用 TRPO 训练智能体行走。我们还学习了如何录制一个训练好的智能体的视频。接下来，我们学习了如何实现
    PPO，并且如何制作训练好的智能体的 GIF 动图。在本章的最后，我们学习了如何使用 Stable Baselines 实现生成对抗模仿学习。
- en: Questions
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s put our knowledge of Stable Baselines to the test. Try answering the
    following questions:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检验一下我们对 Stable Baselines 的理解。试着回答以下问题：
- en: What is Stable Baselines?
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 Stable Baselines？
- en: How do you store and load a trained agent?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何存储和加载一个训练好的智能体？
- en: What is a vectorized environment?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是向量化环境？
- en: What is the difference between the SubprocVecEnv and DummyVecEnv environments?
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SubprocVecEnv 和 DummyVecEnv 环境有什么区别？
- en: How do you visualize a computational graph in TensorBoard?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在 TensorBoard 中可视化计算图？
- en: How do you record a video of a trained agent?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何录制一个训练好的智能体的视频？
- en: Further reading
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, check the following resource:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请查看以下资源：
- en: Check out the Stable Baselines documentation, available at [https://stable-baselines.readthedocs.io/en/master/index.html](https://stable-baselines.readthedocs.io/en/master/index.html)
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 Stable Baselines 文档，地址为 [https://stable-baselines.readthedocs.io/en/master/index.html](https://stable-baselines.readthedocs.io/en/master/index.html)
