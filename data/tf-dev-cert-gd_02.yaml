- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Introduction to TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 简介
- en: Before the era of TensorFlow, the landscape of deep learning was markedly different.
    Data professionals had fewer comprehensive tools to aid in the development, training,
    and deployment of neural networks. This posed challenges in experimenting with
    various architectures and tuning model settings to solve complex tasks, as data
    experts often had to construct their models from scratch. The process was time-consuming,
    with some experts spending days or even weeks developing effective models. Another
    bottleneck was the difficulty in deploying trained models, which made the practical
    application of neural networks challenging during those early days.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 时代之前，深度学习的格局与今天截然不同。数据专业人士没有那么多全面的工具来帮助开发、训练和部署神经网络。这给实验不同的架构以及调整模型设置以解决复杂任务带来了挑战，因为数据专家通常需要从零开始构建自己的模型。这一过程非常耗时，一些专家可能花费数天甚至数周的时间来开发有效的模型。另一个瓶颈是部署训练好的模型的难度，这使得神经网络的实际应用在早期阶段非常具有挑战性。
- en: But today, everything has changed; with TensorFlow, you can do lots of amazing
    things. In this chapter, we will begin by examining the TensorFlow ecosystem and
    discussing, at a high level, the various components relevant to building state-of-the-art
    applications with TensorFlow. We will begin our journey by setting up our workspace
    to meet the requirements of the exam and our upcoming experiments. We will also
    learn what TensorFlow is all about, understand the concept of tensors, explore
    basic data representation and operations in TensorFlow, and build our first model
    using this powerful tool. We will conclude this chapter by looking at how to debug
    and solve error messages in TensorFlow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但如今，一切都发生了变化；有了 TensorFlow，你可以做许多令人惊叹的事情。在本章中，我们将从审视 TensorFlow 生态系统开始，从高层次讨论与使用
    TensorFlow 构建最先进应用相关的各个组件。我们将通过设置工作环境来开始这段旅程，确保满足考试和即将进行的实验的要求。我们还将了解 TensorFlow
    的基本概念，理解张量的概念，探索 TensorFlow 中的基本数据表示和操作，并使用这个强大的工具构建我们的第一个模型。我们将通过学习如何调试和解决 TensorFlow
    中的错误信息来结束本章。
- en: By the end of this chapter, you will understand the basics of TensorFlow, including
    what tensors are and how to perform basic data operations with them. You will
    be equipped to confidently build your first model with TensorFlow and debug and
    solve any error messages that might arise in the process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解 TensorFlow 的基础知识，包括什么是张量以及如何使用它们执行基本的数据操作。你将具备信心，能够使用 TensorFlow
    构建你的第一个模型，并调试和解决在此过程中可能出现的任何错误信息。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is TensorFlow?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: Setting up our environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置我们的环境
- en: Data representation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据表示
- en: Hello World in TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中的 Hello World
- en: Debugging and solving error messages
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试和解决错误信息
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using `python >= 3.8.0` along with the following packages, which
    can be installed using the `pip` `install` command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `python >= 3.8.0`，并配合以下包，它们可以通过 `pip install` 命令安装：
- en: '`tensorflow>=2.7.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow>=2.7.0`'
- en: '`tensorflow-datasets==4.4.0`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow-datasets==4.4.0`'
- en: '`pillow==8.4.0`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pillow==8.4.0`'
- en: '`pandas==1.3.4`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas==1.3.4`'
- en: '`numpy==1.21.4`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy==1.21.4`'
- en: '`scipy==1.7.3`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scipy==1.7.3`'
- en: 'The code bundle for this book is available at the following GitHub link: [https://github.com/PacktPublishing/TensorFlow-Developer-Certificate](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate).
    Also, solutions to all exercises can be found in the GitHub repo itself. If you
    are new to Google Colab, here is a great resource to get you started quickly:
    [https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL](https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包可在以下 GitHub 链接中找到：[https://github.com/PacktPublishing/TensorFlow-Developer-Certificate](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate)。此外，所有习题的解决方案也可以在
    GitHub 仓库中找到。如果你是 Google Colab 的新手，这里有一个很棒的资源，可以帮助你快速入门：[https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL](https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL)。
- en: What is TensorFlow?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: In the last chapter, we examined the different types of applications we could
    build with our knowledge of **machine learning** (**ML**), from chatbots to facial
    recognition systems and from house price prediction to detecting fraud in the
    banking industry – these are some of the exciting applications we can build using
    deep learning frameworks such as TensorFlow. The question we would logically ask
    is what exactly is TensorFlow? And why should we learn it at all?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了可以利用我们对**机器学习**（**ML**）的知识构建的不同类型的应用，从聊天机器人到人脸识别系统，从房价预测到银行业的欺诈检测——这些都是我们可以使用深度学习框架（如TensorFlow）构建的一些令人兴奋的应用。我们逻辑上会问，TensorFlow究竟是什么？我们为什么要学习它呢？
- en: '**TensorFlow** is an open source end-to-end framework for building deep learning
    applications. It was developed by a team of data professionals at Google in 2011
    and made openly available in 2015\. TensorFlow is a flexible, scalable solution
    that enables us to build models with ease using the Keras API. It allows us to
    access a large array of pretrained deep learning models, thus making it the framework
    of choice for many data professionals in the industry and academia. Currently,
    TensorFlow is used at powerhouses such as Google, DeepMind, Airbnb, Intel, and
    so many more companies.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow**是一个开源的端到端框架，用于构建深度学习应用。它由Google的一组数据专业人员于2011年开发，并在2015年公开发布。TensorFlow是一个灵活且可扩展的解决方案，使我们能够通过Keras
    API轻松构建模型。它允许我们访问大量的预训练深度学习模型，这使得它成为行业和学术界许多数据专业人员首选的框架。目前，TensorFlow被Google、DeepMind、Airbnb、Intel等众多大公司使用。'
- en: Today, with TensorFlow, you can easily train a deep learning model on a single
    PC, using a cloud service such as AWS, or using distributed training with a cluster
    of computers. Model building is just a part of what data professionals do; what
    about visualizing, deploying, and monitoring our models? TensorFlow has a wide
    range of tools to cater to these processes, such as TensorBoard, TensorFlow lite,
    TensorFlow.js, TensorFlow Hub, and **TensorFlow Extended** (**TFX**). These tools
    enable data professionals to build and deploy scalable, low-latency, ML-powered
    applications across various domains – on the web, on mobile, and on edge devices.
    To support TensorFlow developers, there is comprehensive documentation and a large
    community of developers who report bugs and contribute to the further development
    and improvement of this framework.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，使用TensorFlow，你可以轻松地在单台PC上训练深度学习模型，或者通过AWS等云服务，或者使用集群计算进行分布式训练。构建模型只是数据专业人员工作的一部分，那么可视化、部署和监控模型呢？TensorFlow提供了广泛的工具来满足这些需求，例如TensorBoard、TensorFlow
    lite、TensorFlow.js、TensorFlow Hub，以及**TensorFlow Extended**（**TFX**）。这些工具使得数据专业人员能够构建和部署可扩展、低延迟、由机器学习驱动的应用程序，涵盖多个领域——无论是在Web、移动端，还是边缘设备上。为了支持TensorFlow开发者，TensorFlow提供了全面的文档支持，并且有一个庞大的开发者社区，他们报告bug并为这个框架的进一步发展和改进做出贡献。
- en: Another central feature of the TensorFlow ecosystem is its access to a diverse
    array of datasets, cutting across different ML problem types such as image data,
    text data, and time-series data. These datasets are available via TensorFlow Datasets,
    and they are a great way to master the use of TensorFlow in solving real-world
    problems. In subsequent chapters, we will be exploring how to build models to
    solve computer vision, natural language processing, and time-series forecasting
    problems using a range of datasets available within the TensorFlow ecosystem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow生态系统的另一个核心特性是其能够访问各种各样的数据集，这些数据集跨越了不同的机器学习问题类型，如图像数据、文本数据和时间序列数据。这些数据集通过TensorFlow
    Datasets提供，是掌握如何使用TensorFlow解决实际问题的绝佳途径。在接下来的章节中，我们将探索如何使用TensorFlow生态系统内提供的各种数据集构建模型，以解决计算机视觉、自然语言处理和时间序列预测问题。
- en: 'We have explored some indispensable tools in the TensorFlow ecosystem. It is
    always a good idea to take a tour of these features (and the new ones that will
    be added) on the official website: [https://www.tensorflow.org/](https://www.tensorflow.org/).
    However, in the exam, you will not be quizzed on this. The idea here is to get
    familiar with what is available in the ecosystem. The exam focuses on modeling
    with TensorFlow so we will only use tools in the ecosystem such as TensorFlow
    Datasets, the Keras API, and TensorFlow Hub to meet this objective.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了一些在 TensorFlow 生态系统中不可或缺的工具。浏览这些功能（以及将来会添加的新功能）并了解它们总是一个好主意，可以访问官方网页：[https://www.tensorflow.org/](https://www.tensorflow.org/)。不过，在考试中不会涉及这些内容。这里的目的是让你熟悉生态系统中已有的工具。考试的重点是使用
    TensorFlow 进行建模，因此我们将只使用生态系统中的工具，例如 TensorFlow 数据集、Keras API 和 TensorFlow Hub
    来实现这一目标。
- en: Setting up our environment
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置我们的工作环境
- en: 'Before we examine data representations in TensorFlow, let’s set up our work
    environment. We will begin by importing TensorFlow and checking the version:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究 TensorFlow 中的数据表示之前，先让我们设置工作环境。我们将从导入 TensorFlow 并检查其版本开始：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we run this block of code, we get the following output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，得到如下输出：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Hurray! We have successfully imported TensorFlow. Next, let us import NumPy
    and a couple of data types, as we will be using them shortly in this chapter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 好极了！我们已经成功导入了 TensorFlow。接下来，让我们导入 NumPy 和几个数据类型，因为我们将在本章中很快用到它们：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have successfully completed all our import steps without errors. We will
    now look at data representations in TensorFlow as our working environment is fully
    set up.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经顺利完成了所有导入步骤，没有遇到任何错误。接下来，我们将探讨 TensorFlow 中的数据表示方法，因为我们的工作环境已经完全设置好。
- en: Data representation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据表示
- en: In our quest to solve complex tasks using ML, we come across diverse types of
    raw data. Our primary role involves transforming this raw data (which could be
    text, images, audio, or video) into numerical representations. These representations
    allow our ML models to easily digest and learn the underlying patterns in the
    data efficiently. To achieve this, this is where TensorFlow and its fundamental
    data structure, tensors, come into play. While numerical data is commonly used
    in training models, our models are also adept at efficiently handling binary and
    categorical data. For such data types, we apply techniques such as one-hot encoding
    to transform them into a model-friendly format.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们利用机器学习解决复杂任务的过程中，我们会遇到各种类型的原始数据。我们的主要任务是将这些原始数据（可能是文本、图像、音频或视频）转化为数值表示。这些表示使得我们的机器学习模型能够轻松地消化数据并高效地学习其中的潜在模式。为此，TensorFlow
    及其基础数据结构——张量，发挥了重要作用。虽然数值数据通常用于训练模型，但我们的模型同样能够高效处理二进制和分类数据。对于这类数据，我们采用诸如独热编码（one-hot
    encoding）等技术将其转换为适合模型使用的格式。
- en: '**Tensors** are multi-dimensional arrays designed for numerical data representation;
    although they share some similarities with NumPy arrays, they possess certain
    unique features that give them an advantage in deep learning tasks. One of these
    key advantages is their ability to utilize hardware acceleration from GPUs and
    TPUs to significantly speed up computational operations, which is especially useful
    when working with input data such as images, text, and videos, as we will see
    in later chapters of this book.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量**是为数值数据表示设计的多维数组；尽管它们与 NumPy 数组有一些相似之处，但它们具有一些独特的特点，使得它们在深度学习任务中具有优势。这些关键优势之一是它们能够利用
    GPU 和 TPU 的硬件加速，显著提高计算操作的速度，这在处理图像、文本和视频等输入数据时尤其有用，正如我们将在本书后面的章节中看到的那样。'
- en: Let us take a quick look at a real-world example. Let’s say we are building
    an automobile recognition system, as illustrated in *Figure 2**.1*. We would begin
    with collecting images of cars of various sizes, shapes, and colors. To train
    our model to recognize these different automobiles, we would transform each image
    into input tensors that encapsulate the height, width, and color channels. When
    we train the model on these input tensors, it learns patterns based on the pixel
    value representations of the cars in our training set. Once the model completes
    the training, we can use the trained model to identify cars of different shapes,
    colors, and sizes. If we now feed the trained model with the image of a car, it
    returns an output tensor that can be decoded into a human-readable format to enable
    us to identify the type of car that it is.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一个实际应用示例。假设我们正在构建一个汽车识别系统，如 *图 2.1* 所示。我们将从收集各种尺寸、形状和颜色的汽车图像开始。为了训练我们的模型识别这些不同的汽车，我们会将每张图像转换为输入张量，包含高度、宽度和颜色通道。当我们用这些输入张量训练模型时，它会根据训练集中汽车的像素值表示学习模式。训练完成后，我们可以使用训练好的模型识别不同形状、颜色和尺寸的汽车。如果我们现在将一张汽车的图像输入已训练的模型，它会返回一个输出张量，经过解码后转换成可供人类理解的格式，从而帮助我们识别出这是什么类型的汽车。
- en: '![Figure 2.1 – Data representation in TensorFlow](img/B18118_02_001.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – TensorFlow 中的数据表示](img/B18118_02_001.jpg)'
- en: Figure 2.1 – Data representation in TensorFlow
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – TensorFlow 中的数据表示
- en: Now that we get the intuition, let’s examine and drill down into more details
    about tensors. We will start by learning a few ways to generate tensors next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了直觉，接下来让我们进一步探讨张量的更多细节。我们将从学习几种生成张量的方法开始。
- en: Creating tensors
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建张量
- en: 'There are a couple of ways we can generate tensors in TensorFlow. However,
    we will focus on creating tensor objects using `tf.constant`, `tf.Variable`, and
    `tf.range`. Recall that we have already imported TensorFlow, NumPy, and data types
    in the section on setting up our working environment. Next, let us run the following
    code to generate our first tensor using `tf.constant`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们可以通过几种方式生成张量。然而，我们将重点介绍使用 `tf.constant`、`tf.Variable` 和 `tf.range`
    创建张量对象。回想一下，我们已经在设置工作环境的部分导入了 TensorFlow、NumPy 和数据类型。接下来，让我们运行以下代码，使用 `tf.constant`
    生成我们的第一个张量：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When we run this code, we generate our first tensor. If all goes well, we will
    get an output that looks like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，我们生成了第一个张量。如果一切顺利，输出应该如下所示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Excellent! Don’t worry, we will discuss the output and form a clearer picture
    as we proceed. But for now, let us generate a similar tensor object using the
    `tf.Variable` function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！别担心，随着我们继续深入，我们会讨论输出并形成更清晰的认识。但现在，让我们使用 `tf.Variable` 函数生成一个类似的张量对象：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `a_variable` variable returns the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`a_variable` 变量返回以下输出：'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Although the input in both cases is the same, `tf.constant` and `tf.Variable`
    are different. Tensors generated using `tf.constant` cannot be changed, whereas
    `tf.Variable` tensors can be reassigned in the future. We will touch more on this
    shortly as we go further in our exploration of tensors. In the meantime, let us
    look at another way of generating tensors using `tf.range`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然两种情况的输入是相同的，但 `tf.constant` 和 `tf.Variable` 是不同的。使用 `tf.constant` 生成的张量是不可变的，而
    `tf.Variable` 生成的张量可以在未来重新赋值。随着我们进一步探索张量，我们会更详细地讨论这一点。与此同时，让我们看看另一种使用 `tf.range`
    生成张量的方法：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`a_range` returns the following output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`a_range` 返回以下输出：'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Great! From the output, if we visually compare all three methods used for generating
    tensors, we can easily conclude that the output of `a_constant` and `a_range`
    is the same and is slightly different from the output of `a_variable`. This difference
    becomes clearer when performing tensor operations. To see this in action, let’s
    begin exploring tensor operations, starting with tensor rank.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！从输出结果来看，如果我们直观地比较三种生成张量的方法，我们可以很容易得出结论：`a_constant` 和 `a_range` 的输出相同，但与
    `a_variable` 的输出略有不同。当执行张量操作时，这种差异变得更加明显。为了展示这一点，让我们从张量秩开始，继续探索张量操作。
- en: Tensor rank
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量秩
- en: If you are not from a mathematical background, relax. We will cover everything
    together and we won’t be discussing rocket science here – that’s a promise. The
    rank of a tensor identifies the number of dimensions of the tensor. A tensor with
    a rank of `0` is called a scalar, as it has no dimensions. A vector is a rank
    `1` tensor as it has only one dimension, while a matrix of a two-dimension tensor
    has a rank of `2`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是数学背景的，不要担心。我们会一起覆盖所有内容，我们这里不会讨论火箭科学——这是一个承诺。张量的秩标识张量的维度数。秩为`0`的张量称为标量，因为它没有维度。向量是秩为`1`的张量，它只有一个维度，而一个二维张量的矩阵秩为`2`。
- en: '![Figure 2.2 – Tensor rank](img/B18118_02_002.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 张量秩](img/B18118_02_002.jpg)'
- en: Figure 2.2 – Tensor rank
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 张量秩
- en: 'We have practiced how to generate tensors using three different functions.
    For context, we can safely define a scalar as a quantity that has only magnitude
    but no direction. Examples of scalar quantities are time, mass, energy, and speed;
    these quantities have a single numeric value, for example, `1`, `23.4`, or `50`.
    Let us return to our notebook and generate a scalar using the `tf.constant` function:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经练习了如何使用三种不同的函数来生成张量。为了说明，我们可以安全地定义标量为只有大小没有方向的量。标量量的例子有时间、质量、能量和速度；这些量有一个单一的数值，例如`1`、`23.4`或`50`。让我们回到笔记本，使用`tf.constant`函数生成一个标量：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We start by creating a scalar, which is a single value that returns the following
    output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个标量，它是一个单一的值，返回如下输出：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: From the returned output, we can see that the shape has no value since the output
    is a scalar quantity with a single numeric output. If we try out a value of `4`,
    the `numpy` output will be `4`, while other output properties will remain the
    same since `4` is still a scalar quantity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从返回的输出可以看出，形状没有值，因为输出是一个标量量，只有一个数值。如果我们尝试使用`4`，`numpy`的输出将是`4`，而其他输出属性将保持不变，因为`4`仍然是一个标量量。
- en: 'Now that we have seen what a scalar (rank `0` tensor) is, let us go a step
    further by looking at a vector. For context, a vector quantity has both magnitude
    and direction. Examples of vectors are acceleration, velocity, and force. Let
    us jump back into our notebook and try to generate a vector of four numbers. For
    a change, this time, we will use floats since we can generate tensors with floats.
    Also, if you noticed, the default data type returned has been `int32` for integers,
    which we have previously used to generate tensors:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了标量（秩为`0`的张量），让我们更进一步，来看一下向量。为了更好理解，向量是既有大小又有方向的量。向量的例子有加速度、速度和力。让我们回到笔记本，尝试生成一个包含四个数字的向量。为了改变一下，这次我们将使用浮点数，因为我们可以使用浮点数生成张量。另外，如果你注意到的话，返回的默认数据类型是`int32`，这是我们之前用来生成整数张量的数据类型：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'From our result, we see the data type returned is `float32` with a shape of
    `4`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的结果中可以看出，返回的数据类型是`float32`，形状为`4`：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, let us generate a matrix. A matrix is an array of numbers listed in rows
    and columns. Let us try out a matrix in our notebook:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们生成一个矩阵。矩阵是一个按行和列排列的数字数组。让我们在笔记本中尝试一个矩阵：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding matrix is a 2 x 2 matrix, which we can infer by inspecting the
    `shape` output. We see that the data type is also `int32`. Let us generate a higher-dimensional
    tensor:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的矩阵是一个 2 x 2 矩阵，我们可以通过检查`shape`输出推断出这一点。我们还看到数据类型是`int32`。让我们生成一个更高维度的张量：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is a 2 x 3 x 2 tensor, with a data type of `int32`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个 2 x 3 x 2 的张量，数据类型为`int32`：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You should play around with some tensors. Try making some tensors with `tf.Variable`
    and see whether you can reproduce our results so far. Next, let us see how we
    can interpret the properties of a tensor.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该试着操作一些张量。尝试用`tf.Variable`创建一些张量，看看你是否能重现到目前为止的结果。接下来，让我们看看如何解释张量的属性。
- en: Properties of tensors
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量的属性
- en: Now that we have established an understanding of scalars, vectors, and tensors,
    let us explore how to interpret tensor outputs in detail. Previously, we examined
    tensors with a piecemeal approach. Here, we will learn how to identify the key
    properties of a tensor – its rank, shape, and data type – from its printed representation.
    When we print a tensor, it displays the variable name, shape, and data type. Thus
    far, we utilized default arguments when creating tensors. Let us make some adjustments
    to see how this changes the output.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了对标量、向量和张量的理解，让我们详细探讨如何解释张量输出。之前，我们以零碎的方式检查了张量。现在，我们将学习如何从张量的打印表示中识别其关键属性——秩、形状和数据类型。当我们打印张量时，它会显示变量名、形状和数据类型。到目前为止，我们在创建张量时使用的是默认参数。让我们进行一些调整，看看这如何改变输出。
- en: 'We will use `tf.``V``ariable` to generate a scalar tensor, selecting `float16`
    as the data type and naming it `TDC`. (If you are wondering what **TDC** means,
    it is the **TensorFlow Developer Certificate**.) Next, we will run the code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`tf.``V``ariable`来生成一个标量张量，选择`float16`作为数据类型，并将其命名为`TDC`。（如果你想知道**TDC**是什么意思，那是**TensorFlow开发者证书**的缩写。）接下来，我们将运行代码：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When we examine the output, we can see the name of the tensor is now `TDC:
    0`, and the shape of the tensor is `0` since the tensor is of rank `0`. The data
    type we selected was `float16`. And finally, the tensor has a `numpy` value of
    `1.1` also. This example shows how we can configure properties such as data type
    and name when constructing tensors in TensorFlow.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '当我们检查输出时，我们可以看到张量的名称现在是`TDC: 0`，张量的形状是`0`，因为该张量的秩为`0`。我们选择的数据类型是`float16`。最后，张量还具有`numpy`值`1.1`。这个例子展示了我们如何在构建TensorFlow张量时配置诸如数据类型和名称等属性。'
- en: 'Next, let us look at a vector and see what information we can learn from its
    properties:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一个向量，看看我们能从它的属性中学到什么信息：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, again, we included arguments and the name of the tensor and we changed
    the default data type. From the output, we can see the result is similar to what
    we got with the scalar quantity:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们再次包含了参数和张量的名称，并且改变了默认的数据类型。从输出中我们可以看到，结果与我们得到的标量量类似：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, the tensor has the name `''Vector:0`, the shape has a value of `4` (which
    corresponds to the count of the number of entries), and the tensor has a data
    type of `float16`. To have some fun, you can experiment with different configurations
    and see the impact the changes you make have on the returned output; this is an
    excellent way to learn and understand how things work. When we print the result
    of a tensor output, we can see the different properties of the tensor, like when
    we examined the scalar and vector quantities. However, by leveraging TensorFlow
    functions, we can gain more information about a tensor. Let us start by using
    the `tf.rank()` function to inspect the rank of a scalar, vector, and matrix:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，张量的名称是`'Vector:0`，形状的值为`4`（这对应于条目数量），并且张量的数据类型为`float16`。为了有点乐趣，你可以尝试不同的配置，看看你所做的更改对返回输出的影响；这是一个非常好的学习和理解事物是如何运作的方式。当我们打印张量输出的结果时，我们可以看到张量的不同属性，就像我们检查标量和向量量时那样。然而，通过利用TensorFlow函数，我们可以获得更多关于张量的信息。让我们从使用`tf.rank()`函数来检查标量、向量和矩阵的秩开始：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We run the preceding code to generate a scalar, vector, and matrix. After this,
    we print the rank of each of them using the `tf.rank` function. Here is the output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行前面的代码来生成标量、向量和矩阵。之后，我们使用`tf.rank`函数打印出它们的秩。以下是输出结果：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The returned output is a tensor object that displays the rank of the tensors
    along with the shape and the data type of the tensor. To access the rank of the
    tensor as a numeric value, we have to use `.numpy()` on the returned tensor to
    retrieve the actual rank of the tensor:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的输出是一个张量对象，它显示了张量的秩以及张量的形状和数据类型。要获取张量的秩作为数值，我们必须在返回的张量上使用`.numpy()`来检索张量的实际秩：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'However, an easier way to directly obtain the rank of a tensor without the
    need for reevaluating is by using `ndim`. Let’s see this next:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接获得张量秩的一个更简单方法是使用`ndim`，无需重新评估。让我们接下来看看这个方法：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we run the code, we get the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，得到以下输出：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, let us proceed by printing out the data type of all three quantities
    using the `dtype` argument to generate the data type for each of our tensors:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过使用`dtype`参数打印出所有三个量的数据类型，以生成每个张量的数据类型：
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When we run the code, we get the following output.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，得到以下输出。
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'From the preceding output, we can see the data types. Next, let us look at
    the shape of our tensors:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的输出中，我们可以看到数据类型。接下来，我们来看看张量的形状：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When we run the code, we get the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，得到以下输出：
- en: '[PRE27]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'From the results, we can see that the scalar has no shape value while the vector
    has a shape value of one unit, and our matrix has a shape value of two units.
    Next, let us compute the number of elements in each of our tensors:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，标量没有形状值，而向量的形状值为 1 单位，矩阵的形状值为 2 单位。接下来，我们来计算每个张量中的元素数量：
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'When we run the code, we get the following output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，得到以下输出：
- en: '[PRE29]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can see that the scalar has only 1 count since it is a single unit; our vector
    and matrix both have `4` in them and hence they have 4 numeric values in each
    of them. Now, we can confidently use different ways to investigate the properties
    of tensors. Let us proceed to implement basic operations with tensors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到标量只有 1 个计数，因为它是一个单一的单位；而我们的向量和矩阵中都有 `4`，因此每个都有 4 个数值。现在，我们可以自信地使用不同的方法来检查张量的属性。接下来，我们继续实现张量的基本操作。
- en: Basic tensor operations
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本的张量操作
- en: We now know that TensorFlow is a powerful tool for deep learning. One big hurdle
    with learning TensorFlow is understanding what tensor operations are and why we
    need them. We have established that tensors are fundamental data structures in
    TensorFlow and they can be used to store, manipulate, and analyze data in ML models.
    On the other hand, tensor operations are mathematical operations that can be applied
    to tensors in order to manipulate, decode, or analyze data. These operations range
    from simple operations such as element-wise operations to more complex computations
    performed within the layers of a neural network. Let us look at some tensor operations.
    We will start with changing data types. Then, we will look at indexing and aggregating
    tensors. Finally, we will carry out element-wise operations on tensors, reshaping
    tensors, and matrix multiplication.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道 TensorFlow 是一个强大的深度学习工具。学习 TensorFlow 的一大障碍是理解什么是张量操作以及为什么需要它们。我们已经确定，张量是
    TensorFlow 中的基本数据结构，可以用来存储、操作和分析机器学习模型中的数据。另一方面，张量操作是可以应用于张量的数学运算，用来操控、解码或分析数据。这些操作从简单的元素级操作到神经网络各层中执行的更复杂计算不等。让我们来看看一些张量操作。我们将从更改数据类型开始。然后，我们将学习索引和聚合张量。最后，我们将进行张量的元素级操作、张量重塑和矩阵乘法。
- en: Changing data types
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更改数据类型
- en: 'Let’s say we have a tensor and we want to change the data type from `int32`
    to `float32`, perhaps to accommodate some operation that would require the decimal
    numbers. Fortunately, in TensorFlow, there is a way around this problem. Remember
    that we identified that the default data type for integers is `int32` and for
    decimal numbers, it is `float32`. Let us return to Google Colab and see how we
    can get this done in TensorFlow:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个张量，我们想将其数据类型从 `int32` 更改为 `float32`，可能是为了支持某些需要小数的操作。幸运的是，在 TensorFlow
    中，有办法解决这个问题。记住，我们已经确定整数的默认数据类型是 `int32`，而小数的默认数据类型是 `float32`。让我们返回 Google Colab，看看如何在
    TensorFlow 中实现这一点：
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We generated a vector of integers, which produces the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了一个整数向量，输出如下：
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can see that the data type is `int32`. Let us proceed with a data type operation,
    changing the data type to `float32`. We use the `tf.cast()` function and we set
    the data type argument to `float32`. Let us implement this in our notebook:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据类型是 `int32`。让我们继续进行数据类型操作，将数据类型更改为 `float32`。我们使用 `tf.cast()` 函数，并将数据类型参数设置为
    `float32`。让我们在笔记本中实现这一点：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The operation returns a data type of `float32`. We can also see the `numpy`
    array is now an array of decimal numbers and not integers anymore:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 操作返回的数据类型为 `float32`。我们还可以看到 `numpy` 数组现在是一个小数数组，不再是整数数组：
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You can try it out with `int16` or `float64` and see how it goes. When you are
    done, let us move on with indexing in TensorFlow.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试 `int16` 或 `float64`，看看效果如何。当你完成后，我们继续进行 TensorFlow 中的索引操作。
- en: Indexing
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 索引
- en: 'Let’s start by creating a 2 x 2 matrix, which we will use to walk through our
    indexing operation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个 2 x 2 的矩阵开始，接下来我们将用它来演示索引操作：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is the returned output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是返回的输出：
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'What if we want to extract some information from the matrix? Let’s say we want
    to extract `[1,2]`. How do we go about this? Not to worry: we can apply indexing
    to get the desired information. Let us get it done in our notebook:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想从矩阵中提取一些信息呢？假设我们想提取`[1,2]`。我们该如何做呢？别担心：我们可以应用索引来获取所需的信息。让我们在我们的笔记本中实现它：
- en: '[PRE36]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is the returned output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是返回的输出：
- en: '[PRE37]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'What if we want to extract value `2` from the matrix? Let us see how we can
    get it done:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想从矩阵中提取值`2`怎么办？让我们看看我们该如何做到：
- en: '[PRE38]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is the returned output:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是返回的输出：
- en: '[PRE39]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, we have successfully extracted the value we wanted using indexing. To extract
    all the values in the matrix shown in *Figure 2**.3*, we can use indexing to extract
    the desired element in the 2 x 2 matrix.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经成功地使用索引提取了我们想要的值。为了提取*图 2.3*中显示的矩阵中的所有值，我们可以使用索引来提取2 x 2矩阵中的所需元素。
- en: '![Figure 2.3 – Matrix indexing](img/B18118_02_003.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 矩阵索引](img/B18118_02_003.jpg)'
- en: Figure 2.3 – Matrix indexing
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 矩阵索引
- en: 'Next, let us look at another example of indexing – this time, using the `tf.slice()`
    function to extract information from a tensor:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看另一个索引的例子——这次，使用`tf.slice()`函数从张量中提取信息：
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We generate a tensor, `c`. Then, we use the `tf.slice` function to slice the
    vector, starting at index `2` with a size or count of `4`. When we run the code,
    we get the following result:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成一个张量`c`。然后，使用`tf.slice`函数从索引`2`开始切割向量，切割的大小或数量是`4`。当我们运行代码时，得到以下结果：
- en: '[PRE41]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can see that the result contains values from index `2`, and we take 4 elements
    in the vector to generate our slice. Next, let us look at how to expand the dimension
    of a matrix.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到结果包含来自索引`2`的值，并且我们从向量中提取了4个元素来生成切片。接下来，让我们看看如何扩展一个矩阵的维度。
- en: Important note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember, in Python, we start counting from 0, not 1.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在Python中，我们的计数是从0开始的，而不是从1开始。
- en: Expanding a matrix
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展矩阵
- en: We already now know how to check the dimension of the matrix using `ndim`. So,
    let us see how we can expand the dimension of this matrix. We continue using our
    `a` matrix, which is a 2 x 2 matrix, as shown in *Figure 2**.4*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经知道如何使用`ndim`检查矩阵的维度。那么，让我们看看如何扩展这个矩阵的维度。我们继续使用我们的`a`矩阵，它是一个2 x 2的矩阵，如*图
    2.4*所示。
- en: '![Figure 2.4 – A 2x2 matrix](img/B18118_02_004.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 一个 2x2 矩阵](img/B18118_02_004.jpg)'
- en: Figure 2.4 – A 2x2 matrix
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 一个 2x2 矩阵
- en: 'We can use the following code to expand the dimension:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来扩展维度：
- en: '[PRE42]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We use the `expand_dims()` function, and the code expands the dimensions of
    the `a` tensor along the `0` axis. This is useful when you want to add a new dimension
    to the tensor – for example, when you want to convert a 2D tensor into a 3D tensor
    (a technique that will be applied in [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146),
    *Image Classification with Convolutional Neural Networks*, where we will work
    on an interesting classic image dataset):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`expand_dims()`函数，代码将`a`张量的维度沿着`0`轴进行扩展。当你想为张量添加一个新维度时，这非常有用——例如，当你想将一个二维张量转换为三维张量时（这种技术将在[*第7章*](B18118_07.xhtml#_idTextAnchor146)，*卷积神经网络的图像分类*中应用，我们将处理一个有趣的经典图像数据集）：
- en: '[PRE43]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you take a look at the shape of our output tensor, we can now see it has
    an extra dimension of `1` at the `0` axis. Let us proceed by examining the shape
    of the tensor when we expand across different axes, so we can understand how this
    works better:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看我们的输出张量的形状，现在可以看到它在`0`轴上有一个额外的维度`1`。接下来，让我们通过检查在不同轴上扩展张量的形状来更好地理解这一过程：
- en: '[PRE44]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When we run the code to see how the dimension has expanded across the `0`,
    `1`, and `-1` axes, we get the following results:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码查看维度如何在`0`、`1`和`-1`轴上扩展时，我们得到以下结果：
- en: '[PRE45]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the first line of code, the dimensions of `a` are expanded by 1 on the `0`
    axis. This means that the dimensions of `a` will now be 1 x 2 x 2, adding an extra
    dimension at the beginning of the tensor. The second line of code is expanding
    the dimensions of `a` by 1 on the `1` axis. This means that the dimensions of
    `a` will now be 2 x 1 x 2; here, we are adding an extra dimension in the second
    position of the tensor. The third line of code is expanding the dimensions of
    `a` by 1 on the `-1` axis. This means that the dimensions of `a` will now be 2
    x 2 x 1, thereby adding an extra dimension at the end of our tensor. We have now
    explained how to expand the dimension of a matrix. Next, let us look at tensor
    aggregation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行代码中，`a`的维度在`0`轴上扩展了1。这意味着`a`的维度将变为1 x 2 x 2，在张量的开头添加了一个额外的维度。第二行代码是在`1`轴上将`a`的维度扩展了1。这意味着`a`的维度将变为2
    x 1 x 2；这里，我们在张量的第二个位置添加了一个额外的维度。第三行代码是在`-1`轴上将`a`的维度扩展了1。这意味着`a`的维度将变为2 x 2 x
    1，从而在张量的末尾添加了一个额外的维度。我们已经解释了如何扩展矩阵的维度。接下来，让我们看看张量聚合。
- en: Tensor aggregation
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量聚合
- en: 'Let us continue our journey by understanding how to aggregate tensors. We start
    by generating some random numbers by importing the `random` library. Then, we
    generate a range from 1 to 100 in which we generate 50 random numbers. We will
    now use these random numbers to generate a tensor:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前行，了解如何聚合张量。我们首先通过导入`random`库生成一些随机数。然后，我们生成一个从1到100的范围，并在该范围内生成50个随机数。接下来，我们将使用这些随机数来生成一个张量：
- en: '[PRE46]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'When we print `a`, we get the following numbers:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印`a`时，得到以下数字：
- en: '[PRE47]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let’s say we want to find the smallest number in our tensor. It may be difficult
    to manually read through all the numbers and tell me in 5 seconds which is the
    smallest. What if our range of values was up to a thousand or a million? Manually
    checking would take up all our time. Thankfully, in TensorFlow, we can find not
    just the minimum in one strike but we can also find the maximum value, the sum
    of all values, the mean, and much more. Let us do this together in the Colab notebook:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想找出张量中的最小值。手动浏览所有数字，5秒钟内告诉我最小值是什么，可能会有些困难。如果我们的值的范围达到千或百万，手动检查将占用我们所有的时间。幸运的是，在TensorFlow中，我们不仅可以一次找到最小值，还可以找到最大值、所有值的和、均值等更多信息。让我们一起在Colab笔记本中做这个：
- en: '[PRE48]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We use these functions to extract the details we require in one click, which
    generates the following result:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些函数可以一键提取所需的细节，生成如下结果：
- en: '[PRE49]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now that we have used TensorFlow to extract some important details, we know
    that the smallest value in our vector is 1, the largest value is 99, the sum of
    our vector is 2,273, and the mean value is 45\. Not bad, right? What if we want
    to find the position that holds the minimum and maximum value in a vector? How
    do we go about this?
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用TensorFlow提取了一些重要的细节，知道了我们向量中的最小值是1，最大值是99，向量的和是2273，均值是45。不错吧？如果我们想找出向量中最小值和最大值所在的位置，该怎么办呢？
- en: '[PRE50]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We use the `tf.argmin` and `tf.argmax` functions to generate the index of the
    lowest value and the index of the highest value, respectively. The output is as
    follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`tf.argmin`和`tf.argmax`函数分别生成最小值的索引和最大值的索引。输出结果如下：
- en: '[PRE51]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'From the result of the `print` statement, we can tell that the lowest value
    is at index `14` and the highest value is at index `44`. If we manually inspect
    the array, we will see that this is true. Also, we can pass the index position
    into the array to get the lowest and highest value:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从`print`语句的结果中，我们可以看出最小值位于索引`14`，最大值位于索引`44`。如果我们手动检查数组，就会发现这是正确的。此外，我们还可以将索引位置传入数组，获取最小值和最大值：
- en: '[PRE52]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If we run the code, we get the following result:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行代码，得到如下结果：
- en: '[PRE53]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: There are a few other functions you can try out. The TensorFlow documentation
    gives us a lot to try out and have fun with. Next, let us explore how to transpose
    and reshape tensors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的函数可以尝试。TensorFlow文档给了我们很多可以尝试和探索的内容。接下来，让我们看看如何转置和重塑张量。
- en: Transposing and reshaping tensors
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转置和重塑张量
- en: 'Let us look at how to transpose and reshape a matrix. First, let’s generate
    a 3 x 4 matrix:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何转置和重塑一个矩阵。首先，我们生成一个3 x 4的矩阵：
- en: '[PRE54]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'When we run the code, we get this result:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，得到如下结果：
- en: '[PRE55]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can reshape the matrix by using `tf.reshape`. Since the matrix has 12 values
    in it, we can use 2 x 2 x 3\. If we multiply the values, we get a total of 12:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tf.reshape`函数重新调整矩阵的形状。由于矩阵中有12个值，我们可以将其调整为2 x 2 x 3的形状。如果我们相乘这些值，我们将得到12：
- en: '[PRE56]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'When we run the code, we get the following output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们得到以下输出：
- en: '[PRE57]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We can also reshape the matrix by changing the `shape` argument in the `tf.reshape`
    function to a 4 x 3 matrix or a 1 x 2 x 6 matrix. You can also try out a few other
    possibilities with regard to reshaping this matrix. Next, let us look at how to
    transpose this matrix using `tf.transpose()`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过改变`tf.reshape`函数中的`shape`参数来重新调整矩阵的形状为4 x 3矩阵或1 x 2 x 6矩阵。你也可以尝试一些其他的形状调整可能性。接下来，让我们看一下如何使用`tf.transpose()`转置这个矩阵：
- en: '[PRE58]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'When we run the code, we get the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们得到以下输出：
- en: '[PRE59]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: From the output, we can see that transposing flips the axes. We now have a 4
    x 3 matrix rather than our initial 3 x 4 matrices. Next, let us look at element-wise
    matrix operations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到转置操作会翻转轴。现在，我们得到了一个4 x 3的矩阵，而不是最初的3 x 4矩阵。接下来，让我们看一下逐元素矩阵操作。
- en: Element-wise operations
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐元素操作
- en: 'Let’s start by creating a simple vector in Colab:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在Colab中创建一个简单的向量开始：
- en: '[PRE60]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let us display our output so we can see what happens when we perform element-wise
    operations on the vector:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示我们的输出，以便看到当我们对向量执行逐元素操作时会发生什么：
- en: '[PRE61]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This is our initial output. Now, let us try out a few element-wise operations
    and see what happens next:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的初始输出。现在，让我们尝试一些逐元素操作，看看接下来会发生什么：
- en: '[PRE62]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can see the results for the addition, subtraction, multiplication, and division
    operations. These operations are carried out on each element in our vector:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到加法、减法、乘法和除法操作的结果。这些操作是在我们向量中的每个元素上执行的：
- en: '[PRE63]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Next, let us look at matrix multiplication.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下矩阵乘法。
- en: Matrix multiplication
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'Let us look at matrix multiplication and see how it works in TensorFlow. We
    return to our notebook in Colab and generate matrix `a`, which is a 3 x 2 matrix,
    and matrix `b`, which is a 2 x 3 matrix. We will use these for our matrix operations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下矩阵乘法，并了解它在TensorFlow中的工作原理。我们返回到Colab中的笔记本，生成矩阵`a`，它是一个3 x 2的矩阵，以及矩阵`b`，它是一个2
    x 3的矩阵。我们将使用这些矩阵进行矩阵操作：
- en: '[PRE64]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, let us multiply matrix `a` and `b` and see what our result will look like
    in TensorFlow by using `tf.matmul` in our notebook:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`tf.matmul`在我们的笔记本中乘以矩阵`a`和矩阵`b`，看看TensorFlow中的结果会是什么样子：
- en: '[PRE65]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We use the `tf.matmul` function for matrix multiplication in TensorFlow. Here,
    we see the output of this operation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在TensorFlow中使用`tf.matmul`函数进行矩阵乘法。在这里，我们可以看到这个操作的输出：
- en: '[PRE66]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Great! Now, what if we want to multiply matrix `a` by itself? What will our
    result look like? If we tried this out, we will get an error because the shape
    of the matrix does not conform to the rule of matrix multiplication. The rule
    requires that matrix `a` should be made up of *i* rows x *m* columns, and matrix
    `b` should be made up of *m* rows x *n* columns, where the value of *m* must be
    the same in both matrices. The new matrix will have a shape of *i* x *n*, as shown
    in *Figure 2**.5*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！现在，如果我们想要将矩阵`a`与自身相乘，结果会是什么样子呢？如果我们尝试这个操作，我们会得到一个错误，因为矩阵的形状不符合矩阵乘法的规则。该规则要求矩阵`a`应该由*i*行
    x *m*列组成，矩阵`b`应该由*m*行 x *n*列组成，并且矩阵中的*m*值在两个矩阵中必须相同。新的矩阵将具有*i* x *n*的形状，如*图2.5*所示。
- en: '![Figure 2.5 – Matrix multiplication](img/B18118_02_005.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – 矩阵乘法](img/B18118_02_005.jpg)'
- en: Figure 2.5 – Matrix multiplication
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 矩阵乘法
- en: 'Now we can see why we cannot multiply matrix `a` by itself, because the number
    of rows in the first matrix must be equal to the number of columns in the second
    matrix. However, we can fulfill the requirement of the matrix multiplication rule
    by either transposing or reshaping matric `a` if we want to multiply `a` by itself.
    Let us try this out:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到为什么不能将矩阵`a`与自身相乘，因为第一个矩阵的行数必须等于第二个矩阵的列数。然而，如果我们希望将`a`与自身相乘，我们可以通过转置或重新调整矩阵`a`的形状来满足矩阵乘法的要求。让我们来试一下：
- en: '[PRE67]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'When we transpose matrix `a`, we swap the rows and columns of the matrix based
    on the `perm` parameter, which we set to `[1,0]`. When we execute the `matmul`
    function using `a` and the transpose of `a`, we get a new matrix that complies
    with the rule of matrix multiplication:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们转置矩阵`a`时，我们根据`perm`参数交换矩阵的行和列，这里我们将其设置为`[1,0]`。当我们使用`a`和`a`的转置执行`matmul`函数时，我们得到一个符合矩阵乘法规则的新矩阵：
- en: '[PRE68]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: How about we try out matrix multiplication using `reshape`? Give this a shot
    and compare your result with our working Colab notebook results. We have looked
    at a whole lot of operations already. How about we build our first model? Let
    us do that next.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来尝试使用`reshape`进行矩阵乘法如何？试试这个，并将你的结果与我们的 Colab 笔记本中的结果进行对比。我们已经看了很多操作了。接下来我们来构建第一个模型吧。
- en: Hello World in TensorFlow
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的Hello World
- en: We have covered a lot of basic operations in TensorFlow. Now, let’s build our
    first model in TensorFlow. For this example, let us say you are part of a research
    team studying the correlation between the number of hours a student studied in
    a term and their final grade. Of course, this is a theoretical scenario and there
    are a lot more factors that come into play when it comes to how well a student
    will perform. However, in this case, we will take only one attribute as the determinant
    of success – hours of study. After a term of study, we successfully collated the
    hours of study of students and their corresponding grades, as shown in *Table
    2.1*.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了很多 TensorFlow 的基本操作。现在，让我们在 TensorFlow 中构建第一个模型。假设你是一个研究小组的一员，正在研究学生在一个学期内的学习小时数与他们的期末成绩之间的相关性。当然，这只是一个理论场景，实际上，影响学生表现的因素有很多。然而，在这个例子中，我们只考虑一个决定成功的属性——学习小时数。在一个学期的学习之后，我们成功地整理了学生的学习小时数及其相应的成绩，如*表
    2.1*所示。
- en: '| Hours of Study | 20 | 23 | 25 | 28 | 30 | 37 | 40 | 43 | 46 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 学习小时数 | 20 | 23 | 25 | 28 | 30 | 37 | 40 | 43 | 46 |'
- en: '| Test Score | 45 | 51 | 55 | 61 | 65 | 79 | 85 | 91 | 97 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 测试成绩 | 45 | 51 | 55 | 61 | 65 | 79 | 85 | 91 | 97 |'
- en: Table 2.1 – Students’ performance table
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 – 学生表现表
- en: 'Now, we want to build a model to predict how well a student will perform in
    the future based on the hours of study they put in. Ready? Let’s do this together
    now:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要构建一个模型，预测学生未来的表现，基于他们的学习小时数。准备好了吗？现在让我们一起做吧：
- en: 'Let’s build this together by opening the accompanying notebook called `hello
    world`. First, we import TensorFlow. Remember in [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014),
    *Introduction to Machine Learning*, we talked about features and labels. Here,
    we just have one feature – hours of study – and our label or target variable is
    the test score. Using the powerful Keras API, in a few lines of code, we will
    build and train a model to get predictions. Let’s get started:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过打开名为`hello world`的配套笔记本一起构建这个。首先，我们导入 TensorFlow。在[*第1章*](B18118_01.xhtml#_idTextAnchor014)《机器学习简介》中，我们讨论了特征和标签。在这里，我们只有一个特征——学习小时数——而我们的标签或目标变量是测试成绩。通过强大的
    Keras API，只需几行代码，我们就能构建并训练一个模型来进行预测。让我们开始吧：
- en: '[PRE69]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We start by importing TensorFlow and the Keras API; don’t worry about all the
    terms, we will unpack everything in detail in [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065),
    *Linear Regression with TensorFlow*. The goal here is to show you how we build
    a basic model. Don’t worry so much about the technicalities; running the code
    and seeing how it works is the goal here. After importing the necessary libraries,
    we continue with our tradition of printing our TensorFlow version. The code runs
    fine.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 TensorFlow 和 Keras API；不用担心所有的术语，我们将在[*第3章*](B18118_03.xhtml#_idTextAnchor065)《TensorFlow
    的线性回归》中详细展开。这里的目标是展示如何构建一个基础模型。别太担心技术细节；运行代码并看看它是如何工作的才是最重要的。导入所需的库之后，我们继续我们的传统，打印出我们的
    TensorFlow 版本。代码运行顺利。
- en: 'Next, we proceed to import `numpy` for carrying out mathematical operations
    and `matplotlib` for visualizing our data:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们导入`numpy`用于执行数学运算，导入`matplotlib`用于数据可视化：
- en: '[PRE74]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We run the code and we get no errors, so we are good to proceed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行代码并且没有出现错误，所以可以继续进行。
- en: 'We set up a list of `X` and `y` values representing our hours of study and
    test scores, respectively:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了`X`和`y`的值列表，分别代表学习小时数和测试成绩：
- en: '[PRE77]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'To get a good sense of data distribution, we use `matplotlib` to visualize
    our data:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地了解数据分布，我们使用`matplotlib`来可视化数据：
- en: '[PRE81]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The code block plots a graph of `X` (hours of study) against `y` (test score)
    and displays the title (`Exam Performance graph`) of our plot. We use the `show()`
    function to display the graph, as shown in *Figure 2**.6*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块绘制了`X`（学习小时数）与`y`（测试成绩）之间的图形，并显示了我们图表的标题（`考试表现图`）。我们使用`show()`函数来显示图表，如*图
    2.6*所示。
- en: '![Figure 2.6 – Students’ performance plot](img/B18118_02_006.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 学生表现图](img/B18118_02_006.jpg)'
- en: Figure 2.6 – Students’ performance plot
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 学生表现图
- en: From the plot, we can see the data shows a linear relationship. This assumption
    is not so bad considering we would logically expect a student who works harder
    to score better marks.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们可以看到数据表现出线性关系。考虑到我们可以逻辑地预期一个学习更努力的学生能获得更好的分数，这个假设并不算坏。
- en: 'Without getting into a debate about whether this theory holds, let us use the
    Keras API to build a simple model:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不对这个理论进行辩论的前提下，让我们使用 Keras API 来构建一个简单的模型：
- en: '[PRE86]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: We build a one-layer model, which we call `study_model`, and we convert our
    list of `X` and `y` values into a NumPy array.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个单层模型，命名为`study_model`，并将我们的`X`和`y`值列表转换为 NumPy 数组。
- en: 'Next, we fit our model and run it for 2,500 epochs:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们拟合模型并运行 2,500 个迭代周期：
- en: '[PRE92]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'When we run the model, it should take less than 5 minutes. We can see that
    the loss drops rapidly initially and gradually flattens out at around 2,000, epochs
    as shown in *Figure 2**.8*:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行模型时，它应该在 5 分钟以内完成。我们可以看到，损失值最初快速下降，并在大约 2,000 个迭代周期后逐渐趋于平稳，如*图 2.8*所示：
- en: '![Figure 2.7 – Model loss plot](img/B18118_02_007.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 模型损失图](img/B18118_02_007.jpg)'
- en: Figure 2.7 – Model loss plot
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 模型损失图
- en: 'And just like that, we have trained a model that can be used to determine how
    a student will perform at the end of a term. This is a very basic task and feels
    like using a hammer on a fly. However, let us try our model out:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们训练了一个可以用来预测学生在学期结束时表现的模型。这是一个非常基础的任务，感觉就像用锤子打苍蝇。不过，让我们来试试看我们的模型：
- en: '[PRE94]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'If we run this code, we generate the result for a student who studied for 38
    hours. Remember our model was not trained on this value. So, let us see what our
    model thinks this student will score:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这段代码，我们将生成一个学习了 38 小时的学生的结果。记住，我们的模型并没有在这个值上进行训练。那么，让我们看看我们的模型预测这个学生的分数：
- en: '[PRE95]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Our model predicted that this student would score 81.07 marks. Good result,
    but how do we know whether our model was right or wrong? If you look at *Figure
    2**.6*, you may guess that our predicted result should be around this score, but
    you may also have figured out that we used *2x + 5 = y* to generate our `y` values.
    If we input `X=38`, we get *2(38) + 5= 81*. Our model did an excellent job of
    getting the correct score with a minute error of .07; however, we had to train
    it for a very long time to achieve this result for a very simple task. In the
    coming chapters, we will learn how to train a much better model using techniques
    such as normalization and, of course, with a larger dataset, where we will work
    with a training set and a validation set and make predictions on a test set. The
    goal here was to get a feel of what is to come, so try out a few numbers to see
    how the model will perform. Do not go above 47, as you will get a score above
    100.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型预测这个学生的分数是 81.07 分。结果不错，但我们怎么知道模型是对的还是错的呢？如果你看看*图 2.6*，你可能会猜测我们的预测结果应该接近这个分数，但你也许已经猜到我们是用*2x
    + 5 = y*来生成我们的`y`值。如果我们输入`X=38`，我们会得到*2(38) + 5 = 81*。我们的模型在预测正确分数时表现得相当好，误差仅为
    0.07；然而，我们不得不花费很长时间来训练它，以便在这样一个简单的任务上取得这个结果。在接下来的章节中，我们将学习如何使用像归一化等技术，以及通过更大的数据集来训练一个更好的模型，我们会使用训练集、验证集并在测试集上进行预测。这里的目标是让你对接下来要学习的内容有一个初步的了解，因此，尝试一些不同的数字，看看模型的表现。不要超过
    47，因为你将得到一个超过 100 的分数。
- en: Now that we have built our first model, let us look at how to debug and solve
    error messages. This is something you will encounter many times if you decide
    to pursue a career in this space.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了第一个模型，让我们来看一下如何调试和解决错误信息。如果你决定从事这个领域的工作，这是你将会遇到的许多问题之一。
- en: Debugging and solving error messages
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试和解决错误信息
- en: 'As you go through the exercises or walk through the code in this book, in any
    other resource, or in your own personal projects, you will quickly realize how
    often code breaks, and mastering how to resolve these errors will help you to
    move quickly through your learning process or when building projects. First, when
    you get an error, it is important to check what the error message is. Next is
    to understand the meaning of the error message. Let us look at some errors that
    a few students stumbled upon when implementing basic operations in TensorFlow.
    Let’s run the following code to generate a new vector:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在本书中完成练习或走过代码，或者在任何其他资源中，甚至是你自己的个人项目中，你会迅速意识到代码出错的频率，掌握如何解决这些错误将帮助你快速通过学习过程或构建项目时的难关。首先，当你遇到错误时，重要的是要检查错误信息是什么。接下来是理解错误信息的含义。让我们来看一些学生在实现
    TensorFlow 基本操作时遇到的错误。让我们运行以下代码来生成一个新的向量：
- en: '[PRE96]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Running this code will throw the error shown in the following screenshot:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会抛出如下截图所示的错误：
- en: '![Figure 2.8 – Example of an error](img/B18118_02_008.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 错误示例](img/B18118_02_008.jpg)'
- en: Figure 2.8 – Example of an error
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 错误示例
- en: From the error message, we can see that there is no attribute called `variable`
    in TensorFlow. This draws our attention to where the error is coming from and
    we immediately notice that we wrote `variable` instead of `Variable` with a capital
    *V*, as stipulated in the documentation. However, if we are not able to debug
    this ourselves, we can click on the **SEARCH STACK OVERFLOW** button, as this
    is a good place to find solutions to everyday coding problems we might encounter.
    The odds are someone else has faced the same problem and a solution can be found
    on Stack Overflow.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从错误信息中，我们可以看到在 TensorFlow 中没有名为 `variable` 的属性。这引起了我们对错误来源的注意，并且我们立刻发现我们写成了
    `variable`，而应该使用文档中要求的大写字母 *V* 来写 `Variable`。但是，如果我们自己无法调试这个问题，我们可以点击 **搜索 STACK
    OVERFLOW** 按钮，因为这是一个很好的地方，可以找到我们在日常编码中可能遇到的问题的解决方案。很可能别人也遇到过相同的问题，解决方案可以在 Stack
    Overflow 上找到。
- en: 'Let us click on the link and see what we can find on Stack Overflow:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们点击链接，看看在 Stack Overflow 上能找到什么：
- en: '![Figure 2.9 – Stack Overflow solution for AttributeError](img/B18118_02_009.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – Stack Overflow 上解决 AttributeError 的方案](img/B18118_02_009.jpg)'
- en: Figure 2.9 – Stack Overflow solution for AttributeError
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – Stack Overflow 上解决 AttributeError 的方案
- en: Hurray! On Stack Overflow, we see the solution to the problem and a link to
    the documentation for more details. Remember, it is best to first look at the
    error message and see whether you can resolve it yourself before heading to Stack
    Overflow. If you put this into practice, as well as dedicate time to reading the
    documentation, you will get better and better at debugging issues and make fewer
    mistakes, but you will still need Stack Overflow or the documentation. It comes
    with the terrain. Before we draw the curtains on this chapter, let us summarize
    quickly what we learned.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 万岁！在 Stack Overflow 上，我们看到了问题的解决方案，并附有文档链接，供您查看更多详情。请记住，最好先查看错误消息，看看自己是否能解决问题，然后再去
    Stack Overflow。如果你能做到这一点，并投入时间阅读文档，你会越来越擅长调试问题，并减少错误，但你仍然会需要 Stack Overflow 或文档。调试是必经之路。在我们结束这一章之前，让我们快速总结一下我们学到的内容。
- en: Summary
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the TensorFlow ecosystem at a high level. We looked
    at some of the key components that make TensorFlow the platform of choice for
    building deep learning applications and solutions for many ML engineers, researchers,
    and enthusiasts. Next, we discussed what tensors are and how they are useful in
    our models. After this, we looked at a few ways of creating tensors. We explored
    various tensor properties and we saw how to implement some basic tensor operations
    with TensorFlow. We built a simple model and used it to make predictions. Finally,
    we looked at how to debug and solve error messages in TensorFlow and ML at large.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们高层次地讨论了 TensorFlow 生态系统。我们探讨了一些关键组件，这些组件使 TensorFlow 成为许多 ML 工程师、研究人员和爱好者构建深度学习应用和解决方案的首选平台。接下来，我们讨论了张量是什么，以及它们如何在我们的模型中发挥作用。之后，我们学习了几种创建张量的方法，探讨了张量的各种属性，并看到了如何使用
    TensorFlow 实现一些基本的张量操作。我们构建了一个简单的模型并用它进行预测。最后，我们讨论了如何调试和解决 TensorFlow 以及更广泛的机器学习中的错误信息。
- en: In the next chapter, we will look at regression modeling in a hands-on manner.
    We will learn how to extend our simple model to solve a regression problem for
    a company’s HR department. Also, what you have learned about debugging could prove
    useful in the next chapter – see you there.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过实践来探讨回归模型的构建。我们将学习如何扩展我们的简单模型，以解决公司人力资源部门的回归问题。此外，您在调试过程中学到的内容在下一章也可能会派上用场——我们下一章见。
- en: Questions
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let’s test what we have learned in this chapter:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下在本章中学到的内容：
- en: What is TensorFlow?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: What are tensors?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是张量？
- en: Generate a matrix using `tf.``V``ariable` with the `tf.float64` data type and
    name the variable.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.``V``ariable`生成一个矩阵，数据类型为`tf.float64`，并为变量命名。
- en: Generate 15 random numbers between 1 and 20 and extract the lowest number, the
    highest number, the mean, and the index with the lowest and highest numbers.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成 15 个 1 到 20 之间的随机数，并提取出最小值、最大值、均值以及最小和最大值的索引。
- en: Generate a 4 x 3 matrix, and multiply the matrix by its transpose.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个 4 x 3 的矩阵，并将该矩阵与其转置相乘。
- en: Further reading
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, you can check out the following resources:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 若想了解更多内容，您可以查看以下资源：
- en: 'Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*. [S.l.]: Packt Publishing.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amr, T., 2020\. *使用 scikit-learn 和科学 Python 工具包的动手机器学习*. [S.l.]: Packt 出版社。'
- en: '*TensorFlow* *guide*: [https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TensorFlow* *指南*: [https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide)'
