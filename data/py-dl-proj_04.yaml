- en: Building an NLP Pipeline for Building Chatbots
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为构建聊天机器人建立NLP管道。
- en: Our project has expanded once again, thanks to the good work that we've been
    doing. We started off working for a restaurant chain, helping them to classify
    handwritten digits for use in a text notification system, used to alert their
    waiting guests that their table was ready. Based on this success, and when the
    owners realized that their customers were actually responding to the texts, we
    were asked to contribute a deep learning solution using **Natural Language Processing**
    (**NLP**) to accurately classify text into a meaningful sentiment category that
    would give the owners an indication as to their satisfaction with the dining experience.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的项目再次扩展了，这要归功于我们所做的出色工作。最初，我们为一家餐饮连锁店工作，帮助他们对手写数字进行分类，用于一个文本通知系统，提醒等待的客人他们的餐桌已经准备好。基于这一成功，当店主意识到顾客实际上在回应这些短信时，我们被要求贡献一个深度学习解决方案，利用**自然语言处理**（**NLP**）来准确地将文本分类为有意义的情感类别，以便为店主提供顾客在用餐体验中的满意度反馈。
- en: Do you know what happens to deep learning engineers that do good work? They
    get asked to do more!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道做得好深度学习工程师会发生什么吗？他们会被要求做更多的事！
- en: This project for the next business use case is pretty cool. What we're being
    asked to do is to create a natural language pipeline that would power a chatbot
    for open domain question answering. The (hypothetical) restaurant chain has a
    website with their menu, history, location, hours, and other information, and
    they would like to add the ability for a website visitor to ask a question in
    a query box, and have our deep learning NLP chatbot find the relevant information
    and present that back. They think that getting the right information back to the
    website visitor quickly would help drive in-store visits and improve the general
    customer experience.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个下一步的商业用例项目非常酷。我们被要求做的是创建一个自然语言处理管道，为开放域问答提供支持。这个（假设的）餐饮连锁店有一个网站，上面有菜单、历史、地点、营业时间和其他信息，他们希望添加一个功能，使网站访问者可以在查询框中提问，然后由我们的深度学习NLP聊天机器人找到相关信息并反馈给他们。他们认为，将正确的信息快速传达给网站访问者，有助于推动到店访问并改善整体客户体验。
- en: '**Named Entity Recognition** (**NER**) is the approach we will be using, which
    will give us the power we need to quickly classify the input text, which we can
    then match to the relevant content for a response. It''s a great way to take advantage
    of a large corpus of unstructured data that changes without using hardcoded heuristics.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）是我们将使用的方法，它将赋予我们快速分类输入文本的能力，然后我们可以将其与相关内容匹配以回应。这是一种很好的方法，可以利用大量不断变化的非结构化数据，而无需使用硬编码的启发式方法。'
- en: In this chapter, we will learn about the building blocks of the NLP model, including
    pre-processing, tokenizing, and tagging parts of speech. We will use this understanding
    to build a system able to read an unstructured piece of text, in order to formulate
    an answer for a specific question. We will also describe how to include this deep
    learning component in a classic NLP pipeline to retrieve information, in order
    to provide an open-domain question answering system that doesn't require a structured
    knowledge base.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习NLP模型的构建模块，包括预处理、分词和词性标注。我们将利用这些理解来构建一个系统，能够读取非结构化的文本，以便为特定问题制定回答。我们还将描述如何将这个深度学习组件纳入经典的NLP管道中，以便检索信息，从而提供一个无需结构化知识库的开放域问答系统。
- en: 'In this chapter, we will do the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将做以下几件事：
- en: Build a basic FAQ-based chatbot using statistical modeling in a framework, capable
    of detecting intents and entities for answering open-domain questions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用统计建模框架构建一个基于常见问题的聊天机器人，能够检测意图和实体，以回答开放域问题。
- en: Learn to generate dense representations of sentences
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何生成句子的密集表示。
- en: Build a document reader for extracting answers from unstructured text
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个文档阅读器，从非结构化文本中提取答案。
- en: Learn how to integrate deep learning models into a classic NLP pipeline
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何将深度学习模型集成到经典的NLP管道中。
- en: '**Define the goal**: To build a chatbot that understands the context (intent)
    and can also extract the entities. To do this, we need an NLP pipeline that can
    perform intent classification, along with NER extraction to then provide an accurate
    response.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义目标**：构建一个能够理解上下文（意图）并能够提取实体的聊天机器人。为了做到这一点，我们需要一个能够执行意图分类，并结合NER提取的NLP管道，以便提供准确的回应。'
- en: '**Skills learned**: You will learn how to build an open-domain question answering
    system using a classic NLP pipeline, with a document reader component that uses
    deep learning techniques to generate sentence representations.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习的技能**：你将学习如何使用经典的自然语言处理管道构建一个开放领域的问答系统，文档阅读器组件使用深度学习技术来生成句子表示。'
- en: Let's get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Basics of NLP pipelines
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理管道基础
- en: Textual data is a very large source of information, and properly handling it
    is crucial to success. So, to handle this textual data, we need to follow some
    basic text processing steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据是一个非常庞大的信息来源，正确处理它对于成功至关重要。因此，为了处理这些文本数据，我们需要遵循一些基本的文本处理步骤。
- en: Most of the processing steps covered in this section are commonly used in NLP
    and involve combining a number of steps into one executable flow. This is what
    we refer to as the NLP pipeline. This flow can be a combination of tokenization,
    stemming, word frequency, parts of speech tagging, and many more elements.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中覆盖的大部分处理步骤在自然语言处理（NLP）中是常用的，并涉及将多个步骤组合成一个可执行流程。这就是我们所说的 NLP 管道。这个流程可以是分词、词干提取、词频统计、词性标注等多个元素的组合。
- en: 'Let''s look into the details on how to implement the steps in the NLP pipeline
    and, specifically, what each stage of processing does. We will use the **Natural
    Language Toolkit** (**NLTK**) package—an NLP toolkit written in Python, which
    you can install with the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解如何实现 NLP 管道中的各个步骤，特别是每个处理阶段的功能。我们将使用 **自然语言工具包**（**NLTK**）——一个用 Python
    编写的 NLP 工具包，你可以通过以下方式安装它：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code for this project is available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter04/Basic%20NLP%20Pipeline.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter04/Basic%20NLP%20Pipeline.ipynb).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的代码可在 [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter04/Basic%20NLP%20Pipeline.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter04/Basic%20NLP%20Pipeline.ipynb)找到。
- en: Tokenization
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization separates a corpus into sentences, words, or tokens. Tokenization
    is needed to make our texts ready for further processing and is the first step
    in creating an NLP pipeline. A token can vary according to the task we are performing
    or the domain in which we are working, so keep an open mind as to what you consider
    as a token!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 分词将语料库分割成句子、单词或词项。分词是让我们的文本准备好进行进一步处理的必要步骤，也是构建 NLP 管道的第一步。一个词项的定义可以根据我们执行的任务或我们所工作的领域而有所不同，所以在定义词项时要保持开放的心态！
- en: '**Know the code**: NLTK is powerful, as much of the hard coding work is already
    done in the library. You can read more about NLTK tokenization at [http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.tokenize_sents](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.tokenize_sents).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**了解代码**：NLTK 非常强大，因为库中已经完成了大量的硬编码工作。你可以在 [http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.tokenize_sents](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.tokenize_sents)
    阅读更多关于 NLTK 分词的信息。'
- en: 'Let''s try to load a corpus and use NLTK tokenizer to first tokenize the raw
    corpus into sentences, and then tokenize each sentence further into words:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试加载一个语料库，并使用 NLTK 分词器首先将原始语料库分割成句子，然后进一步将每个句子分割成单词：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Part-of-Speech tagging
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: 'Some words have multiple meanings, for example, *charge* is a noun, but can
    also be a verb, (*to) charge*. Knowing a **Part-of-Speech** (**POS**) can help
    to disambiguate the meaning. Each token in a sentence has several attributes that
    we can use for our analysis. The POS of a word is one example: nouns are a person,
    place, or thing; verbs are actions or occurrences and adjectives are words that
    describe nouns. Using these attributes, it becomes straightforward to create a
    summary of a piece of text by counting the most common nouns, verbs, and adjectives:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些单词有多重含义，例如，*charge* 是一个名词，但也可以是动词，(*to) charge*。了解**词性**（**POS**）有助于消除歧义。句子中的每个词项都有多个属性，我们可以用来进行分析。词性的例子包括：名词表示人、地点或事物；动词表示动作或发生的事情；形容词是描述名词的词汇。利用这些属性，我们可以轻松创建文本摘要，统计最常见的名词、动词和形容词：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Extracting nouns
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取名词
- en: 'Let''s extract all of the nouns present in the corpus. This is very useful
    practice when you want to extract something specific. We are using `NN`, `NNS`,
    `NNP`, and `NNPS` tags to extract the nouns:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取语料库中所有的名词。这在你需要提取特定内容时非常有用。我们使用 `NN`、`NNS`、`NNP` 和 `NNPS` 标签来提取名词：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Extracting verbs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取动词
- en: 'Let''s extract all of the verbs present in the corpus. In this case, we are
    using `VB`, `VBD`, `VBG`, `VBN`, `VBP`, and `VBZ` as verb tags:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取语料库中所有的动词。在这种情况下，我们使用`VB`、`VBD`、`VBG`、`VBN`、`VBP`和`VBZ`作为动词标签：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s use `spacy` to tokenize a piece of text and access the POS attribute
    for each token. As an example application, we''ll tokenize the previous paragraph
    and count the most common nouns with the following code. We''ll also lemmatize
    the tokens, which gives the root form a word, to help us standardize across forms
    of a word:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`spacy`对一段文本进行分词，并访问每个词语的词性（POS）属性。作为示例应用，我们将对前一段进行分词，并通过以下代码统计最常见的名词。我们还将对这些词语进行词形还原（lemmatization），将词语还原为其根形，以帮助我们在不同形式的词语之间进行标准化：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Following is the output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Dependency parsing
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依存句法分析
- en: 'Dependency parsing is a way to understand the relationships between words in
    a sentence. Dependency relations are a more fine-grained attribute, available
    to help build the model''s understanding of the words through their relationships
    in a sentence:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 依存句法分析是一种理解句子中词语之间关系的方法。依存关系是一种更细粒度的属性，可以帮助建立模型对单词在句子中的关系的理解：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These relationships between words can get complicated, depending on how sentences
    are structured. The result of dependency-parsing a sentence is a tree data structure,
    with the verb as the root, as shown in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词语之间的关系可能会变得复杂，取决于句子的结构。依存句法分析的结果是一个树形数据结构，其中动词是根节点，如下图所示：
- en: '![](img/6669edc6-66ac-4c9d-955d-3a31c46118e2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6669edc6-66ac-4c9d-955d-3a31c46118e2.png)'
- en: The tree structure of the dependency parsing of a sentence, with the verb as
    the root.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 句子的依存句法分析树结构，其中动词是根节点。
- en: NER
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NER
- en: 'Finally, there''s NER. Named entities are the proper nouns of sentences. Computers
    have gotten pretty good at figuring out if they''re in a sentence and at classifying
    what type of entity they are. `spacy` handles NER at the document level, since
    the name of an entity can span several tokens:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有NER。命名实体是句子中的专有名词。计算机已经能够相当准确地判断句子中是否存在这些实体，并对它们进行分类。`spacy`在文档级别处理NER，因为实体的名称可能跨越多个词语：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So, we just saw some of the basic building blocks of the NLP pipeline. These
    pipelines are consistently used in various NLP projects, be it in machine learning
    or in the deep learning space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们刚刚看到了一些NLP管道的基本构建模块。这些管道在各种NLP项目中被一致地使用，无论是在机器学习领域还是在深度学习领域。
- en: Does something look familiar?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么看起来熟悉的吗？
- en: We used a few of these NLP pipeline building blocks in the previous chapter,
    [Chapter 3](4dcd4b65-934b-4a8a-a252-9af7513a4787.xhtml), *Word Representation
    Using word2vec*, to build our word2vec models. This more in-depth explanation
    of the building blocks of the NLP pipeline helps us take the next step in our
    projects, as we look to deploy more and more complex models!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用了其中一些NLP管道构建块，[第3章](4dcd4b65-934b-4a8a-a252-9af7513a4787.xhtml)，*使用word2vec的词表示*，来构建我们的word2vec模型。对NLP管道构建块的更深入解释帮助我们在项目中迈出下一步，因为我们寻求部署越来越复杂的模型！
- en: As with everything in this book on *Python Deep Learning Projects*, we encourage
    you to also try your own combinations of the previous processes for the use cases
    you work on in your data science career. Now, let's implement a chatbot using
    these pipelines!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本书中关于*Python深度学习项目*的其他内容一样，我们鼓励你也尝试将之前的处理流程与数据科学职业中所处理的用例相结合。现在，让我们使用这些管道实现一个聊天机器人！
- en: Building conversational bots
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建对话机器人
- en: In this section, we will learn about some basic statistical modeling approaches
    to build an information retrieval system using **term frequency**-**inverse document
    frequency** (**TF-IDF**), which we can use with the NLP pipelines to build fully
    functional chatbots. Also, later on, we will learn to build a much more advanced
    conversational bot that can extract a specific piece of information, such as location,
    capture time, and so on, using NER.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习一些基本的统计建模方法，以构建一个信息检索系统，使用**词频**-**逆文档频率**（**TF-IDF**），我们可以将其与NLP管道结合使用，构建功能齐全的聊天机器人。此外，稍后我们将学习构建一个更为高级的对话机器人，能够提取特定的信息，比如位置、捕获时间等，使用命名实体识别（NER）。
- en: What is TF-IDF?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是TF-IDF？
- en: TF-IDFs are a way to represent documents as feature vectors. But what are they?
    TF-IDFs can be understood as a modification of the raw **term frequency** (**TF**)
    and **inverse document frequency** (**IDF**). The TF is the count of how often
    a particular word occurs in a given document. The concept behind the TF-IDF is
    to downweight terms proportionally to the number of documents in which they occur.
    Here, the idea is that terms that occur in many different documents are likely
    to be unimportant, or don't contain any useful information for NLP tasks, such
    as document classification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是一种将文档表示为特征向量的方式。那么它们到底是什么呢？TF-IDF 可以理解为 **词频** (**TF**) 和 **逆文档频率**
    (**IDF**) 的修改版。TF 是特定单词在给定文档中出现的次数。TF-IDF 背后的概念是根据一个词汇出现在多少个文档中来减少它的权重。这里的核心思想是，出现在很多不同文档中的词汇很可能不重要，或者对
    NLP 任务（如文档分类）没有什么有用的信息。
- en: Preparing the dataset
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: If we think about building a chatbot with the TF-IDF approach, we first need
    to form a data structure that supports training data with labels. Now, let's take
    an example of a chatbot built to answer questions from users.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑使用 TF-IDF 方法构建一个聊天机器人，我们首先需要构建一个支持带标签训练数据的数据结构。现在，让我们以一个聊天机器人的例子为例，假设它是用来回答用户提问的。
- en: 'In this case, using historical data, we can form a dataset where we have two
    columns, one of which is the question, and the second of which is the answer to
    that question, as shown in the following table:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，通过使用历史数据，我们可以形成一个数据集，其中包含两列，一列是问题，另一列是该问题的答案，如下表所示：
- en: '| **Question** | **Answer** |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **问题** | **答案** |'
- en: '| When does your shop open? | Our shop timings are 9:00 A.M-9:00 P.M on weekdays
    and 11:00 A.M-12:00 midnight on weekends. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 你们店什么时候开门？ | 我们的营业时间是工作日早上9:00至晚上9:00，周末是早上11:00到午夜12:00。 |'
- en: '| What is today''s special? | Today, we have a variety of Italian pasta, with
    special sauce and a lot more other options in the bakery. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 今天的特价是什么？ | 今天我们提供各种意大利面，配上特制酱料，还有更多其他面包店的选项。 |'
- en: '| What is the cost of an Americano? | Americano with a single shot will cost
    $1.4 and the double shot will cost $2.3. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 美式咖啡多少钱？ | 一杯单份美式咖啡的价格是1.4美元，双份是2.3美元。 |'
- en: '| Do you sell ice-creams? | We do have desserts such as ice-cream, brownies,
    and pastries. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 你们卖冰淇淋吗？ | 我们确实有甜点，比如冰淇淋、布朗尼和糕点。 |'
- en: 'Let''s take the previous example, and consider it a sample dataset. It is a
    very small example and, in the original hypothetical scenario, we will have a
    much larger dataset to work with. The typical process will be as follows: the
    user will interact with the bot and write a random query about the store. The
    bot will simply send that query to the NLP engine, using an API, and then it is
    up to the NLP model to decide what to return for a new query (test data). In reference
    to our dataset, all of the questions are the training data and the answers are
    labels. In the event of a new query, the TF-IDF algorithm will match it to one
    of the questions with a confidence score, which tells us that the new question
    asked by the user is close to some specific question from the dataset, and the
    answer against that question is the answer that our bots return.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续考虑前面的例子，把它看作一个样本数据集。它是一个非常小的例子，而在原始的假设场景中，我们会有一个更大的数据集来处理。典型的流程如下：用户将与机器人互动，并输入关于商店的随机查询。机器人会将查询发送给
    NLP 引擎，使用 API，然后由 NLP 模型决定针对新查询（测试数据）返回什么内容。参考我们的数据集，所有问题都是训练数据，而答案是标签。在出现新的查询时，TF-IDF
    算法会将其与数据集中某个问题进行匹配，并给出一个置信度分数，这个分数告诉我们用户提问的新问题与数据集中的某个特定问题相近，针对该问题的答案就是我们的机器人返回的答案。
- en: Let's take the preceding example even further. When the user queries, "Can I
    get an Americano, btw how much it will cost?", we can see that words like *I*,
    *an*, and *it* are the ones that will have a higher occurrence frequency in other
    questions as well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步考虑前面的例子。当用户查询：“我能买一杯美式咖啡吗？顺便问一下，多少钱？”时，我们可以看到像 *I*、*an* 和 *it* 这些词汇，在其他问题中也会有较高的出现频率。
- en: 'Now, if we match our remaining important words, we will see that this question
    is most close to: "What is the cost of an Americano?"So, our bot will respond
    back with the historical answer to this type of question: "Americano with a single
    shot will cost $1.4 and the double shot will cost $2.3."'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们匹配其余的重要词汇，我们会发现这个问题最接近："美式咖啡多少钱？" 所以，我们的机器人会回复这个问题的历史答案：“一杯单份美式咖啡的价格是1.4美元，双份是2.3美元。”
- en: Implementation
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: After creating the data structure in tabular format, as mentioned previously,
    we will be calculating the predicted answer to a question every time a user queries
    our bot. We load all of the question-answer pairs from the dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到的以表格格式创建数据结构之后，我们将在每次用户查询我们的机器人时计算预测的答案。我们从数据集中加载所有问题-答案对。
- en: 'Let''s load our CSV file using `pandas`, and perform some pre-processing on
    the dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`pandas`加载我们的CSV文件，并对数据集进行一些预处理：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The code for this project can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/tfidf_version](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/tfidf_version).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的代码可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/tfidf_version](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/tfidf_version)找到。
- en: Creating the vectorizer
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建向量化器
- en: 'Now, let''s initialize the TF-IDF vectorizer and define a few parameters:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化TF-IDF向量化器并定义一些参数：
- en: '`min_df`: When building the vocabulary, ignore terms that have a document frequency
    strictly lower than the given threshold'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_df`：在构建词汇表时，忽略文档频率低于给定阈值的术语'
- en: '`ngram_range`: Configures our vectorizer to capture *n*-words at a time'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_range`：配置我们的向量化器一次捕捉*n*个单词'
- en: '`norm`: This is used to normalize term vectors using L1 or L2 norms'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm`：用于使用L1或L2范数对术语向量进行归一化'
- en: '`encoding`: Handles Unicode characters'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding`：处理Unicode字符'
- en: 'There are many more parameters that you can look into, configure, and play
    around with:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他参数，您可以查看、配置并进行实验：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we train the model on the questions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在问题上训练模型：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Processing the query
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理查询
- en: 'To process the query, we find out its similarity with other questions. We do
    this by taking a dot product of the training data matrix with a transpose of the
    query data:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理查询，我们需要找到它与其他问题的相似度。我们通过计算训练数据矩阵与查询数据的转置的点积来实现这一点：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we take out the similarity between the query and train data as a list:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查询与训练数据的相似度提取为一个列表：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Rank results
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排名结果
- en: 'We create a sorted dictionary of similarities for a query:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为查询创建了一个相似度排序字典：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, in the sorted dictionary, we check for the index of the most similar
    question, and the response with the value at that index in the answers column.
    If nothing is found, then we can return our default answer:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在排序的字典中，我们检查最相似问题的索引，以及该索引在答案列中的响应值。如果没有找到任何结果，我们可以返回默认答案：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Advanced chatbots using NER
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NER的高级聊天机器人
- en: We just created a very basic chatbot that can understand the user's query and
    then respond to the customer accordingly. But it is not yet capable of understanding
    the context, because it can not extract information such as the product name,
    places, or any other entities.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建了一个非常基本的聊天机器人，它能够理解用户的查询，并根据情况做出回应。但它还无法理解上下文，因为它无法提取诸如产品名称、地点或其他任何实体的信息。
- en: To build a bot that understands the context (intent) and can also extract the
    entities, we need an NLP pipeline that can perform intent classification, along
    with NER extraction, and then provide an accurate response.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个理解上下文（意图）并能够提取实体的机器人，我们需要一个NLP管道，能够执行意图分类和NER提取，并提供准确的响应。
- en: Keep your eyes on the goal! This is the goal of our open-domain question answering
    bot.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记目标！这就是我们开放领域问答机器人所追求的目标。
- en: To do that, we will use an open source project called Rasa NLU ([https://github.com/RasaHQ/rasa_nlu](https://github.com/RasaHQ/rasa_nlu)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用一个名为Rasa NLU的开源项目（[https://github.com/RasaHQ/rasa_nlu](https://github.com/RasaHQ/rasa_nlu)）。
- en: 'Rasa NLU is a **Natural Language Understanding** tool for understanding a text;
    in particular, what is being said in short pieces of text. For example, imagine
    that the system is given a short message like the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Rasa NLU是一个**自然语言理解**工具，用于理解文本，尤其是短文本中所表达的内容。例如，假设系统接收到类似以下的简短消息：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In such a case, the system returns the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，系统返回以下内容：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, by harnessing the power of RASA, we can build a chatbot that can do intent
    classification and NER extraction.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过利用RASA的强大功能，我们可以构建一个能够进行意图分类和NER提取的聊天机器人。
- en: Great, let's do it!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们开始吧！
- en: The code for this project can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/rasa_version](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/rasa_version)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的代码可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/rasa_version](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/rasa_version)找到。
- en: Installing Rasa
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Rasa
- en: 'Let''s install Rasa in our local environment or server using these commands:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令在我们的本地环境或服务器中安装Rasa：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If it fails to install, then you can look into a detailed approach at [https://nlu.rasa.com/installation.html](https://nlu.rasa.com/installation.html).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安装失败，可以通过查看[https://nlu.rasa.com/installation.html](https://nlu.rasa.com/installation.html)中的详细方法来解决。
- en: Rasa uses a variety of NLP pipelines such as `spacy`, `sklearn`, or MITIE. You
    can use any one of them or build your own custom pipelines, which can include
    any deep model, such as CNN with word2vec, which we created in the previous chapter.
    In our case, we will be using `spacy` with `sklearn`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Rasa使用多种NLP管道，例如`spacy`、`sklearn`或MITIE。你可以选择其中任何一个，或者构建自己的自定义管道，其中可以包含任何深度模型，例如我们在前一章节中创建的带有word2vec的CNN。在我们的案例中，我们将使用`spacy`和`sklearn`。
- en: Preparing dataset
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: In our previous project, we created a dataset in a CSV file with two columns
    for question and answer pairs. We need to do this again, but in a different format.
    In this case, we need questions associated with its intent, as shown in the following
    diagram, so we have a query as **hello** with its intent labeled as **greet**.
    Similarly, we will label all of the questions with their respective intents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的项目中，我们创建了一个CSV文件数据集，包含了问题和答案对的两列。我们需要再次执行此操作，但使用不同的格式。在这种情况下，我们需要将问题与其意图相关联，如下图所示，这样我们就有一个标注为**greet**的**hello**问题。同样，我们将为所有问题标注各自的意图。
- en: 'Once we have all of the forms of questions and intents ready, we need to label
    the entities. In this case, as shown in the following diagram, we have a **location** entity
    with a **centre** value, and a **cuisine** entity with the value as **mexican**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了所有问题和意图的形式，就需要标注实体。在这种情况下，如下图所示，我们有一个**location**实体，值为**centre**，以及一个**cuisine**实体，值为**mexican**：
- en: '![](img/859b52cc-ab9e-4a95-be06-7ae12bd2b065.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/859b52cc-ab9e-4a95-be06-7ae12bd2b065.png)'
- en: The figure illustrated the content of the data what we are preparing for the
    chatbot. Lest most is the list of all intents which we need out bot to understand.
    Then we have respective sample utterneces for each intent. And the right most
    part represents the annotation of the specific entity with its label 'location'
    and 'cuisine' in this case.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了我们为聊天机器人准备的数据内容。最主要的是所有意图的列表，我们需要让我们的机器人理解这些意图。然后，我们为每个意图提供相应的示例语句。最右侧部分表示具体实体的标注，实体的标签是“location”和“cuisine”，在这个例子中就是这样。
- en: 'To feed data into Rasa, we need to store this information in a specific JSON
    format, which looks like the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据输入到Rasa中，我们需要将这些信息存储为特定的JSON格式，格式如下所示：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The final version of the JSON should have this structure:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: JSON的最终版本应该具有如下结构：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To make it simple, there is an online tool into which you can feed and annotate
    all of the data, and download the JSON version of it. You can run the editor locally
    by following the instructions from [https://github.com/RasaHQ/rasa-nlu-trainer](https://github.com/RasaHQ/rasa-nlu-trainer) or
    simply use the online version of it from [https://rasahq.github.io/rasa-nlu-trainer/](https://rasahq.github.io/rasa-nlu-trainer/).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，提供了一个在线工具，您可以将所有数据输入并标注，然后下载JSON版本。您可以按照[https://github.com/RasaHQ/rasa-nlu-trainer](https://github.com/RasaHQ/rasa-nlu-trainer)上的说明在本地运行编辑器，或者直接使用其在线版本：[https://rasahq.github.io/rasa-nlu-trainer/](https://rasahq.github.io/rasa-nlu-trainer/)。
- en: Save this JSON file as `restaurant.json` in the current working directory.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将此JSON文件保存为`restaurant.json`，并存放在当前工作目录中。
- en: Training the model
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now we're going to create a configuration file. This configuration file will
    define the pipeline that is to be used in the process of training and building
    of the model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个配置文件。这个配置文件将定义在训练和构建模型过程中使用的管道。
- en: 'Create a file called `config_spacy.yml` in your working directory, and insert
    the following code into it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的工作目录中创建一个名为`config_spacy.yml`的文件，并将以下代码插入其中：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Know the code**: spaCy configuration customization is there for a reason.
    Other data scientists have found some utility in the ability to change values
    here, and it''s good practice to explore this as you get more familiar with this
    technology. There is a huge list of configurations, which you can look into at [https://nlu.rasa.com/config.html](https://nlu.rasa.com/config.html).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**了解代码**：spaCy配置自定义是有原因的。其他数据科学家已经发现能够在这里更改值有一定的用处，随着你对这项技术的熟悉，探索这一点是一个很好的实践。这里有一个配置的庞大列表，你可以在[https://nlu.rasa.com/config.html](https://nlu.rasa.com/config.html)查看。'
- en: 'This configuration states that we will be using English language models, and
    the pipeline running on the backend will be spaCy with scikit-learn. Now, to begin
    the training process, execute the following command:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置表示我们将使用英语语言模型，并且后台运行的管道将是基于spaCy和scikit-learn。现在，为了开始训练过程，执行以下命令：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This takes the configuration file and the training data file as input. The `--path`
    parameter is the location where the trained model will be stored.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将配置文件和训练数据文件作为输入。`--path`参数是存储训练模型的位置。
- en: 'Once the model training process is completed, you''ll see a new folder named
    in the `projects/default/model_YYYYMMDD-HHMMSS` format, with the timestamp when
    the training finished. The complete project structure will look as shown in the
    following screenshot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练过程完成，你将看到一个新文件夹，命名为`projects/default/model_YYYYMMDD-HHMMSS`格式，包含训练完成时的时间戳。完整的项目结构将如以下截图所示：
- en: '![](img/8d457b2a-fa48-40f8-a2b0-c9bbbf791ba1.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d457b2a-fa48-40f8-a2b0-c9bbbf791ba1.png)'
- en: The folder structure after the training process is completed. The model folder
    will contain all the binary files and metadata which was learned during the training
    process.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程完成后的文件夹结构。模型文件夹将包含所有在训练过程中学到的二进制文件和元数据。
- en: Deploying the model
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署模型
- en: 'Now it''s the moment to make your bot go live! While using Rasa, you don''t
    need to write any API services—everything is available in the package itself.
    So, to expose the trained model as a service, you need to execute the following
    command, which takes the path of the stored trained model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是让你的机器人上线的时候了！使用Rasa时，你不需要编写任何API服务——一切都可以在包内完成。所以，为了将训练好的模型暴露为服务，你需要执行以下命令，该命令需要存储的训练模型路径：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If everything goes fine, then a RESTful API will be exposed at port `5000`,
    and you should see this log on the console screen:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，将会在`5000`端口暴露一个RESTful API，你应该能在控制台屏幕上看到以下日志：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To access the API, you can use the following command. We are querying the model,
    making a statement such as "`I am looking for Mexican food`":'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问API，你可以使用以下命令。我们正在查询模型，提出一个语句，比如"`I am looking for Mexican food`"（我在寻找墨西哥菜）：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So here, we can see that model has performed quite accurately with the intent
    classification and the entity extraction process. It is able to classify the intent
    as `restaurant_search` with 75.8% of accuracy, and is also able to detect the `cuisine`
    entity with the value as `mexican`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，我们可以看到模型在意图分类和实体提取过程中表现得相当准确。它能够以75.8%的准确率将意图分类为`restaurant_search`，并且能够检测到`cuisine`实体，值为`mexican`。
- en: Serving chatbots
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务化聊天机器人
- en: 'Up to now, we have seen how to build chatbots using the two methods of TF-IDF
    and Rasa NLU. Let''s expose both of them as APIs. The architecture of this simple
    chatbot framework will look like this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何使用`TF-IDF`和`Rasa NLU`两种方法来构建聊天机器人。接下来，我们将把它们暴露为API。这个简单聊天机器人框架的架构将如下所示：
- en: '![](img/c640950b-e21d-4dfc-9bd8-c4c61eb06454.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c640950b-e21d-4dfc-9bd8-c4c61eb06454.png)'
- en: This chatbot pipeline illustrates that we can have any User Interface (Slack,
    Skype, and so on) integrated with the chatbot_api which we exposed . And under
    the hood we can setup any number of algorithms 'TFIDF' and 'RASA'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个聊天机器人流程说明了我们可以将任何用户界面（如Slack、Skype等）与我们暴露的`chatbot_api`进行集成。在后台，我们可以设置任意数量的算法，如`TFIDF`和`RASA`。
- en: Refer to the Packt repository for this chapter (available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04))
    for the API code and look into the `chatbot_api.py`file. Here, we have implemented
    a common API that can load both versions of bot, and you can now build a whole
    framework on top of this.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章节的 Packt 仓库（可访问 [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04)）获取
    API 代码，并查看 `chatbot_api.py` 文件。在这里，我们实现了一个通用 API，可以加载两种版本的机器人，你现在可以在其基础上构建完整的框架。
- en: 'To execute the serving of the APIs, follow these steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 API 服务时，请按照以下步骤操作：
- en: 'Enter the chapter directory using the following command:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令进入章节目录：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will expose the Rasa module at `localhost:5000`. If you have not trained
    the Rasa engine, then please apply the following command:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使 Rasa 模块暴露在 `localhost:5000`。如果您尚未训练 Rasa 引擎，请使用以下命令：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In a separate console, execute the following command. This will expose an API
    at `localhost:8080`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个单独的控制台中，执行以下命令。这将在 `localhost:8080` 暴露一个 API：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now your chatbot is ready to be accessed via API. Try the following:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您的聊天机器人已准备好通过 API 进行访问。试试以下操作：
- en: 'Call the following API to execute the TFIDF version:'
  id: totrans-151
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用以下 API 执行 TFIDF 版本：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Call the following API to execute the Rasa version:'
  id: totrans-153
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用以下 API 执行 Rasa 版本：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this project, we were asked to create a natural language pipeline that would
    power a chatbot for open domain question answering. A (hypothetical) restaurant
    chain has much text-based data on their website, including their menu, history,
    location, hours, and other information, and they would like to add the ability
    for a website visitor to ask a question in a query box. Our deep learning NLP
    chatbot would then find the relevant information and present that back to the
    visitor.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们被要求创建一个自然语言处理管道，为开放领域问答的聊天机器人提供支持。一个（假设的）餐饮连锁公司在其网站上拥有大量基于文本的数据，包括菜单、历史、位置、营业时间等信息，他们希望为网站访客提供一个查询框，允许他们提问。我们的深度学习
    NLP 聊天机器人将根据这些信息找到相关内容，并返回给访客。
- en: We got started by showing how we could build a simple FAQ chatbot that took
    in random queries, matched that up to predefined questions, and returned a response
    with a confidence score that indicated the similarity between the input question
    and the question in our database. But this was only a stepping stone to our real
    goal, which was to create a chatbot that could capture the intent of the question
    and prepare an appropriate response.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示了如何构建一个简单的 FAQ 聊天机器人，该机器人接收随机查询，将其与预定义问题匹配，并返回一个响应，带有表示输入问题与数据库中问题相似度的置信度评分。但这仅仅是通向我们真正目标的第一步，我们的目标是创建一个能够捕捉问题意图并准备适当响应的聊天机器人。
- en: We explored an NER approach to give us the added power that we needed to quickly
    classify input text, which we could then match to the relevant content for a response.
    This was determined to fit our goal of allowing for open-domain question answering
    and to take advantage of a large corpus of unstructured data that changes without
    using hardcoded heuristics (as in our hypothetical restaurant example).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了一种命名实体识别（NER）方法，赋予我们所需的能力，快速对输入文本进行分类，然后匹配到相关的响应内容。这种方法适合我们的目标，即支持开放领域问答，并且能够利用大量不断变化的非结构化数据，而无需使用硬编码的启发式方法（就像我们假设的餐厅例子中一样）。
- en: We learned to use the building blocks of the NLP model, including pre-processing, tokenizing,
    and tagging POS. We used this understanding to build a system able to read an
    unstructured text in order to comprehend an answer to a specific question.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学会了使用 NLP 模型的基本构建模块，包括预处理、分词和 POS 标注。我们利用这些理解，构建了一个能够读取非结构化文本的系统，以便理解针对特定问题的答案。
- en: 'Specifically, we gained these skills in this project:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们在这个项目中获得了以下技能：
- en: Building a basic FAQ-based chatbot using statistical modeling in a framework
    capable of detecting intents and entities for answering open-domain questions
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用统计建模构建基于 FAQ 的基础聊天机器人框架，能够检测意图和实体以回答开放领域问题。
- en: Generating a dense representation of sentences
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成句子的密集表示
- en: Building a document reader for extracting answers from unstructured text
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个文档读取器，从非结构化文本中提取答案
- en: Learned how to integrate deep learning models into a classic NLP pipeline
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习了如何将深度学习模型集成到经典的 NLP 管道中。
- en: These skills will come very much in handy in your career, as you see similar
    business use cases, and also as conversational user interfaces continue to gain
    in popularity. Well done—let's see what's in store for our next deep learning
    project in Python!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技能在你的职业生涯中将非常有用，因为你将会遇到类似的商业应用场景，同时，随着对话式用户界面日益流行，它们也会变得更加重要。做得好——让我们来看看下一项目
    Python 深度学习项目会带来什么！
