- en: '*Chapter 5*: Style Transfer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 5 章*：风格迁移'
- en: Generative models such as VAE and GAN are great at generating realistic looking
    images. But we understand very little about the latent variables, let alone how
    to control them with regard to image generation. Researchers began to explore
    ways to better represent images aside from pixel distribution. It was found that
    an image could be disentangled into **content** and **style**. Content describes
    the composition in the image such as a tall building in the middle of the image.
    On the other hand, style refers to the fine details, such as the brick or stone
    textures of the wall or the color of the roof. Images showing the same building
    at different times of the day have different hues and brightness and can be seen
    as having the same content but different styles.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型，如VAE和GAN，在生成逼真的图像方面表现出色。但我们对潜在变量知之甚少，更不用说如何控制它们以生成图像了。研究人员开始探索除了像素分布之外的其他图像表示方式。研究发现，图像可以解构为**内容**和**风格**。内容描述图像中的组成部分，比如图像中间的高楼大厦。另一方面，风格指的是细节部分，例如墙壁的砖石纹理或屋顶的颜色。不同时间段拍摄同一建筑的图像会有不同的色调和亮度，可以看作是相同的内容但具有不同的风格。
- en: In this chapter, we will start by implementing some seminal work in **neural
    style transfer** to transfer the artistic style of an image. We will then learn
    to implement **feed-forward neural style transfer**, which is a lot faster in
    terms of speed. Then we will implement **adaptive instance normalization** (**AdaIN**)
    to perform style transfer with arbitrary numbers of styles. AdaIN has been incorporated
    into some state-of-the-art GANs, which are collectively known as **style-based
    GANs**. This includes **MUNIT** for image translation and **StyleGAN**, which
    is famous for generating realistic looking, high-fidelity faces. We will learn
    about their architecture in the final section of the chapter. This wraps up the
    evolution of style-based generative models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先实现一些神经风格迁移的开创性工作，以将一幅图像的艺术风格转移到另一幅图像。接着，我们将学习实现**前馈神经风格迁移**，这种方法在速度上要快得多。然后，我们将实现**自适应实例归一化**（**AdaIN**），以进行具有任意风格数量的风格迁移。AdaIN已被融入一些最先进的生成对抗网络（GANs）中，这些网络统称为**风格基础GANs**。其中包括用于图像转换的**MUNIT**和著名的用于生成逼真、高保真面孔的**StyleGAN**。我们将在本章的最后部分学习它们的架构。至此，本章总结了风格基础生成模型的演变。
- en: By the end of this chapter, you will have learned how to perform artistic neural
    style transfer to convert a photo into painting. You will have a good understanding
    of how style is used in advanced GANs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学会如何执行艺术风格迁移，将一张照片转化为绘画风格。你将对风格在先进的GAN中的应用有一个深入的理解。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Neural style transfer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经风格迁移
- en: Improving style transfer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进风格迁移
- en: Arbitrary style transfer in real time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时任意风格迁移
- en: Introduction to style-based generative models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风格基础生成模型介绍
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The Jupyter notebooks and codes can be found at the following link:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本和代码可以通过以下链接找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter05)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter05)'
- en: 'The notebooks used in the chapter are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的笔记本如下：
- en: '`ch5_neural_style_transfer.ipynb`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch5_neural_style_transfer.ipynb`'
- en: '`ch5_arbitrary_style_transfer.ipynb`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch5_arbitrary_style_transfer.ipynb`'
- en: Neural style transfer
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经风格迁移
- en: 'When **convolutional neural networks** (**CNNs**) outperformed all other algorithms
    in the ImageNet image classification competition, people started to realize the
    potential of it and began exploring it for other computer vision tasks. In the
    *A Neural Algorithm of Artistic Style* paper published in 2015 by Gatys et al.,
    they demonstrated the use of CNNs to transfer the artistic style of one image
    to another, as shown in the following examples:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当**卷积神经网络**（**CNNs**）在ImageNet图像分类竞赛中超过所有其他算法时，人们开始意识到其潜力，并开始探索其在其他计算机视觉任务中的应用。在2015年由Gatys等人发表的论文《A
    Neural Algorithm of Artistic Style》中，他们展示了使用CNN将一幅图像的艺术风格转移到另一幅图像的方法，如下例所示：
- en: '![Figure 5.1 – (A) Content image. (B)-(D) Bottom image is the style image and
    the bigger pictures are stylized images'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – (A) 内容图像。 (B)-(D) 底部图像是风格图像，较大的图片是风格化图像'
- en: '(Source: Gatys et al., 2015, “A Neural Algorithm of Artistic Style” https://arxiv.org/abs/1508.06576)](img/B14538_05_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Gatys等，2015年，《艺术风格的神经算法》 https://arxiv.org/abs/1508.06576）](img/B14538_05_01.jpg)
- en: 'Figure 5.1 – (A) Content image. (B)-(D) Bottom image is the style image and
    the bigger pictures are stylized images (Source: Gatys et al., 2015, “A Neural
    Algorithm of Artistic Style” https://arxiv.org/abs/1508.06576)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – (A) 内容图像。(B)-(D) 底部图像为风格图像，较大的图像为风格化图像（来源：Gatys等，2015年，《艺术风格的神经算法》 https://arxiv.org/abs/1508.06576）
- en: Unlike most deep learning trainings that require tons of training data, neural
    style transfer requires only two images – content and style images. We can use
    pre-trained CNN such as VGG to transfer the style from the style image to the
    content image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数深度学习训练需要大量训练数据不同，神经风格迁移仅需要两张图像——内容图像和风格图像。我们可以使用预训练的CNN（如VGG）将风格从风格图像转移到内容图像。
- en: As shown in the preceding image, (**A**) is the content image and (**B**) –
    (**D**) are the style and stylized images. The results were so impressive that
    they blew people's minds! Some even use the algorithm to create and sell art paintings.
    There are websites and apps that let people upload photos to perform style transfer
    without having to know the underlying theory and coding. Of course, as technical
    folks, we want to implement things by ourselves.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，(**A**)是内容图像，(**B**)至(**D**)是风格图像和风格化图像。结果令人印象深刻，简直让人大开眼界！甚至有人用该算法创作并出售艺术画作。有一些网站和应用程序让人们上传照片进行风格迁移，而无需了解底层理论和编码。
    当然，作为技术人员，我们更希望自己动手实现这些东西。
- en: We will now look into the details in terms of how to implement neural style
    transfer, starting with extracting image features with CNNs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将详细了解如何实现神经风格迁移，从使用CNN提取图像特征开始。
- en: Extracting features with VGG
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用VGG提取特征
- en: Classification CNNs, like VGG, can be divided into two parts. The first part
    is known as **feature extractor** and is made up of mainly convolutional layers.
    The latter part consists of several dense layers that give the scores of classes.
    This is known as **classifier head**. It was found that a CNN pre-trained on ImageNet
    for classification tasks can be used for other tasks as well.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分类卷积神经网络（CNNs），如VGG，可以分为两部分。第一部分被称为**特征提取器**，主要由卷积层组成。后一部分由几个全连接层组成，用于给出类别的得分，这部分被称为**分类器头**。研究发现，经过ImageNet分类任务预训练的CNN也可以用于其他任务。
- en: For example, if you want to create classification CNNs for other datasets that
    have only 10 classes instead of ImageNet's 1,000 classes, then you could keep
    the feature extractor and only swap out the classifier head with a new one. This
    is known as **transfer learning**, where we could transfer or reuse some learned
    knowledge to new networks or applications. Many deep neural networks for computer
    vision tasks include a feature extractor, either reusing weights or training from
    scratch. This includes **object detection** and **pose estimation**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想为另一个只有10个类别的数据集创建分类CNN，而不是ImageNet的1,000个类别，你可以保留特征提取器，仅用一个新的分类器头替换掉原来的。这被称为**迁移学习**，即我们可以将一些已学到的知识转移或重用于新的网络或应用程序。许多用于计算机视觉任务的深度神经网络都包括特征提取器，不论是复用权重还是从头开始训练。这包括**目标检测**和**姿势估计**。
- en: 'In a CNN, as we go deeper toward the output, it increasingly learns representation
    of the content of the image compared to its detailed pixel values. To understand
    this better, we will build a network to reconstruct the image that the layers
    see. The two steps for image reconstruction are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，随着我们向输出层逐步深入，它越来越倾向于学习图像内容的表示，而非其详细的像素值。为了更好地理解这一点，我们将构建一个网络来重建各层所看到的图像。图像重建的两个步骤如下：
- en: Forward pass the image through a CNN to extract the features.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像前向传递通过CNN以提取特征。
- en: With randomly initialized input, we *train the input* so that it recreates the
    features that best match the reference features from *step 1*.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机初始化的输入，我们*训练输入*，使其重建与*步骤1*中参考特征最匹配的特征。
- en: Let me elaborate on *step 2*. In normal network training, the input image is
    fixed and the backpropagated gradients are used to update the network weights.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我详细讲解一下*步骤2*。在正常的网络训练中，输入图像是固定的，并且反向传播的梯度用于更新网络权重。
- en: 'In neural style transfer, all network layers are frozen, and we use the gradients
    to change the input instead. The original paper uses VGG19 and Keras does have
    a pre-trained model that we could use. The feature extractor part of VGG is made
    up of five blocks and there is one downsampling at the end of each block. Every
    block has between two and four convolutional layers and the entire VGG19 has 16
    convolutional layers and 3 dense layers, hence the number 19 in VGG19 stands for
    19 layers with trainable weights. The following table shows different VGG configurations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经风格迁移中，所有网络层都会被冻结，我们则使用梯度来改变输入。原始论文使用的是VGG19，Keras也有一个我们可以使用的预训练模型。VGG的特征提取部分由五个块组成，每个块的末尾都有一次下采样。每个块包含两个到四个卷积层，整个VGG19有16个卷积层和3个全连接层，因此VGG19中的19代表具有可训练权重的19层。以下表格显示了不同的VGG配置：
- en: '![Figure 5.2 – Different configurations of VGG'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 – 不同的VGG配置'
- en: '(Source: K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks For Large-Scale
    Image Recognition” – https://arxiv.org/abs/1409.1556)](img/B14538_05_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks For Large-Scale
    Image Recognition” – https://arxiv.org/abs/1409.1556）](img/B14538_05_02.jpg)
- en: 'Figure 5.2 – Different configurations of VGG (Source: K. Simonyan, A. Zisserman,
    “Very Deep Convolutional Networks For Large-Scale Image Recognition” – https://arxiv.org/abs/1409.1556)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 不同的VGG配置（来源：K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks
    For Large-Scale Image Recognition” – https://arxiv.org/abs/1409.1556）
- en: The Jupyter notebook for this is `ch5_neural_style_transfer.ipynb`, which is
    the complete neural style transfer solution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的Jupyter笔记本是`ch5_neural_style_transfer.ipynb`，它是完整的神经风格迁移解决方案。
- en: 'However, in the following text, I''ll use a simpler code to show content reconstruction,
    which will be expanded to perform style transfer. The following is the code for
    using a pretrained VGG to extract the output layer of `block4_conv2`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在接下来的文本中，我将使用更简单的代码来展示内容重构，随后会扩展以进行风格迁移。以下是使用预训练VGG提取`block4_conv2`输出层的代码：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pre-trained Keras CNN models are grouped into two parts. The bottom part is
    made up of convolutional layers, commonly known as the `include_top=False` when
    instantiating the VGG model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的Keras CNN模型分为两部分。底部部分由卷积层组成，通常在实例化VGG模型时被称为`include_top=False`。
- en: VGG pre-processing
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: VGG预处理
- en: A Keras pre-trained model expects an input image to be in BGR in the range [0,
    255]. Thus, the first step is to reverse the color channel to convert RGB into
    BGR. VGG uses different mean values for different color channels. Inside `preprocess_input()`,
    the pixel values are subtracted by the values of 103.939, 116.779, and 123.68
    for the B, G, and R channels, respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Keras预训练模型期望输入图像为BGR格式，且范围在[0, 255]之间。因此，第一步是反转颜色通道，将RGB转换为BGR。VGG对不同颜色通道使用不同的均值。在`preprocess_input()`函数内部，像素值会分别从B、G、R通道中减去103.939、116.779和123.68的值。
- en: 'The following is the forward pass code where the image is first pre-processed
    before feeding into the model to return the content feature. We then extract the
    content features and use them as our target:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前向传递代码，其中图像首先进行预处理，然后输入到模型中以返回内容特征。我们然后提取内容特征并将其作为我们的目标：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that the image is normalized to [0., 1.] and so we need to restore that
    to [0., 255.] by multiplying it by 255\. We then create a randomly initialized
    input that will also become the stylized image:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，图像已被归一化到[0., 1.]，因此我们需要通过将其乘以255来恢复到[0., 255.]。然后我们创建一个随机初始化的输入，它也将成为风格化后的图像：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we will use backpropagation to reconstruct the image from the content
    features.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用反向传播从内容特征重构图像。
- en: Reconstructing content
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容重构
- en: 'In the training step, we feed an image to the frozen VGG to extract the content
    features and we use L2 loss to measure against the target content features. The
    following is the custom `loss` function to calculate the L2 loss of each feature
    layer:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练步骤中，我们将图像输入到被冻结的VGG中以提取内容特征，并使用L2损失来与目标内容特征进行比较。以下是自定义的`loss`函数，用于计算每个特征层的L2损失：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following training step uses `tf.GradientTape()` to calculate the gradients.
    In normal neural network training, the gradients are applied to the trainable
    variables, that is, the weights of the neural network. However, in neural style
    transfer, the gradients are applied to the image. After that, we clip the image
    value between [0., 1.] as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的训练步骤使用 `tf.GradientTape()` 来计算梯度。在普通的神经网络训练中，梯度会应用到可训练的变量，即神经网络的权重上。然而，在神经风格迁移中，梯度会应用到图像上。之后，我们将图像值裁剪到
    [0., 1.] 之间，如下所示：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We train it for 1,000 steps, and this is what the reconstructed content looks
    like:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了 1,000 步，这就是重建的内容效果：
- en: '![Figure 5.3 – Image reconstructed from content layers'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3 – 从内容层重建的图像'
- en: '(Source: https://www.pexels.com/. (Left): Original content image, (Right):
    Content of ‘block1_1’)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: https://www.pexels.com/. (左)：原始内容图像，(右)：‘block1_1’ 的内容)'
- en: '](img/B14538_05_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_05_03.jpg)'
- en: 'Figure 5.3 – Image reconstructed from content layers (Source: https://www.pexels.com/.
    (Left): Original content image, (Right): Content of ''block1_1'')'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.3 – 从内容层重建的图像 (来源: https://www.pexels.com/. (左)：原始内容图像，(右)：‘block1_1’ 的内容)'
- en: 'We could reconstruct the image almost perfectly with the first few convolutional
    layers similar to *block1_1*, as shown in the image above:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎可以用前几层卷积层（类似于 *block1_1*）重建图像，如上图所示：
- en: '![Figure 5.4 – Image reconstructed from content layers'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4 – 从内容层重建的图像'
- en: '(Left): Content of ‘block4_1’. (Right): Content of ‘block5_1’](img/B14538_05_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (左)：‘block4_1’ 的内容。(右)：‘block5_1’ 的内容](img/B14538_05_04.jpg)
- en: 'Figure 5.4 – Image reconstructed from content layers (Left): Content of ''block4_1''.
    (Right): Content of ''block5_1'''
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 从内容层重建的图像 (左)：‘block4_1’ 的内容。(右)：‘block5_1’ 的内容
- en: As we go deeper into *block4_1*, we start to lose fine details, such as the
    window frames and the words on the building. As we go deeper into *block5_1*,we
    see that all the details are gone and filled with some random noise. If we look
    carefully, the building structure and edges are still intact and in places where
    they should be. Now we have extracted just the content and omitted the style.
    After extracting the content features, the next step is to extract the style features.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入到 *block4_1*，我们开始失去一些细节，如窗框和建筑上的字母。继续深入到 *block5_1* 时，我们会发现所有细节都消失了，取而代之的是一些随机噪声。如果仔细观察，建筑的结构和边缘仍然完好无损，并且位于它们应该在的位置。现在，我们已经只提取了内容，并省略了风格。提取内容特征后，下一步是提取风格特征。
- en: Reconstructing styles with the Gram matrix
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Gram 矩阵重建风格
- en: As we have seen with the style reconstruction, the feature maps, especially
    the first few layers, contain both style and content. So how do we extract the
    style representation from the image? Gats et al. uses the **Gram matrix**, which
    computes the correlations between the different filter responses. Let's say the
    activation of convolutional layer *l* has a shape of (H, W, C), where *H* and
    *W* are the spatial dimensions and *C* is the number of channels, which equals
    the number of filters. Each filter detects different image features; they can
    be horizontal lines, diagonal lines, colors, and so on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在风格重建中所看到的，特征图，尤其是前几层，既包含了风格也包含了内容。那么我们如何从图像中提取风格表示呢？Gatys 等人使用了**Gram 矩阵**，它计算了不同滤波器响应之间的相关性。假设卷积层
    *l* 的激活形状为 (H, W, C)，其中 *H* 和 *W* 是空间维度，*C* 是通道数，也就是滤波器的数量。每个滤波器检测不同的图像特征，它们可以是水平线、对角线、颜色等等。
- en: Humans perceive things as having the same textures when they share some common
    features, such as a color and an edge. For instance, if we feed an image of a
    grass field into a convolutional layer, the filters that detect *vertical lines*
    and *green color* will produce bigger responses in their feature maps. Hence,
    we can use the correlation between feature maps to represent textures in the image.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过共享一些共同的特征（如颜色和边缘）来感知相似的纹理。例如，如果我们将一张草地的图像输入到卷积层，检测*垂直线*和*绿色*的滤波器将在其特征图中产生更大的响应。因此，我们可以利用特征图之间的相关性来表示图像中的纹理。
- en: 'To create a Gram matrix from activations with a shape of (H, W, C), we will
    first reshape it into C number of vectors. Each vector is a flattened feature
    map with a size of H×W. We perform an inner product on these C vectors to get
    a symmetric C×C Gram matrix. The detailed steps for calculating a Gram matrix
    in TensorFlow are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从形状为（H, W, C）的激活中创建Gram矩阵，我们首先将其重塑为C个向量。每个向量是一个大小为H×W的平铺特征图。我们对这C个向量执行内积，得到一个对称的C×C
    Gram矩阵。计算Gram矩阵的详细步骤如下：
- en: Use `tf.squeeze()` to remove the batch dimension (1, H, W, C) to (H, W, C) as
    the batch size is always `1`.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.squeeze()`来移除批次维度（1, H, W, C），变为（H, W, C），因为批次大小始终是`1`。
- en: Transpose the tensor to transform the shape from (H, W, C) to (C, H, W).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转置张量，将形状从（H, W, C）转换为（C, H, W）。
- en: Flatten the final two dimensions to become (C, H×W).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最后两个维度展平，变为（C, H×W）。
- en: Perform the dot product of the features to create a Gram matrix with a shape
    of (C, C).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对特征进行点积，创建一个形状为（C, C）的Gram矩阵。
- en: Normalize by dividing the matrix by the number of points (H×W) in each flattened
    feature map.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将矩阵除以每个平铺特征图中的点数（H×W）来进行归一化。
- en: 'The code to calculate a Gram matrix from a single convolution layer activation
    is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从单个卷积层激活计算Gram矩阵的代码如下：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can use this function to obtain Gram matrices for each VGG layer that we
    designated as a style layer. We then use L2 loss on Gram matrices from the target
    and reference images. The loss function and the rest of the code is identical
    to content reconstruction. The code to create a list of the Gram matrices is as
    follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数来获取每个我们指定为风格层的VGG层的Gram矩阵。然后，我们对目标图像和参考图像的Gram矩阵使用L2损失。损失函数和其他代码与内容重建相同。创建Gram矩阵列表的代码如下：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following images are reconstructed from style features from the different
    VGG layers:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像是从不同VGG层的风格特征重建而来的：
- en: '![ Figure 5.5 – (Top) Style image: Vincent Van Goh’s Starry Night. (Bottom
    Left) Reconstructed style from ‘block1_1’. (Bottom Right) Reconstructed style
    from ‘block3_1’](img/B14538_05_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![ 图 5.5 – （顶部）风格图像：文森特·梵高的《星空》。 （左下）从‘block1_1’重建的风格。 （右下）从‘block3_1’重建的风格](img/B14538_05_05.jpg)'
- en: 'Figure 5.5 – (Top) Style image: Vincent Van Goh''s Starry Night. (Bottom Left)
    Reconstructed style from ''block1_1''. (Bottom Right) Reconstructed style from
    ''block3_1'''
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – （顶部）风格图像：文森特·梵高的《星空》。 （左下）从‘block1_1’重建的风格。 （右下）从‘block3_1’重建的风格。
- en: In the style image reconstructed from *block1_1*, the content information is
    completely gone, showing only high spatial frequency texture details. The higher
    layer, *block3_1*, shows some curly shapes that seem to capture the higher hierarchy
    of the style in the input image. The loss function for the Gram matrix is the
    sum of **squared error** instead of **mean squared error**. Hence, higher hierarchy
    style layers have higher intrinsic weights. This allows the transfer of higher
    style representations, such as brush strokes. If we use mean squared error, low-level
    style features such as texture will be more prominent visually and may appear
    like high frequency noise.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在从*block1_1*重建的风格图像中，内容信息完全消失，仅显示出高空间频率的纹理细节。较高层次的*block3_1*显示出一些弯曲的形状，似乎捕捉了输入图像中风格的更高层次。Gram矩阵的损失函数是**平方误差**的总和，而不是**均方误差**。因此，更高层次的风格层具有更高的固有权重，这允许转移更高层次的风格表示，如笔触。如果我们使用均方误差，低层次的风格特征（如纹理）会在视觉上更加突出，可能会看起来像高频噪声。
- en: Performing neural style transfer
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行神经风格迁移
- en: We can now merge the code from both the content and style reconstruction to
    perform neural style transfer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将内容和风格重建的代码合并，执行神经风格迁移。
- en: 'We first create a model that extracts two blocks of features, one for content
    and the other for style. We use only one layer of `block5_conv1` for the content,
    and five layers, from `block1_conv1` to `block5_conv1`, to capture styles from
    different hierarchies as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个模型，提取两个特征块，一个用于内容，另一个用于风格。我们仅使用`block5_conv1`的一层作为内容层，使用从`block1_conv1`到`block5_conv1`的五层来捕捉来自不同层次的风格，如下所示：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Before the start of the training loop, we extract content and style features
    from respective images to use as the targets. While we can use randomly initialized
    input for content and style reconstruction, it would be faster to train by starting
    from the content image as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环开始之前，我们从各自的图像中提取内容和风格特征，作为目标使用。虽然我们可以使用随机初始化的输入进行内容和风格重建，但从内容图像开始训练会更快，如下所示：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we weigh the content and style loss and add them. The code snippet is
    as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对内容损失和风格损失进行加权并相加。代码片段如下：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following are the two stylized images produced using different weights
    and content layers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用不同权重和内容层次产生的两张风格化图像：
- en: '![Figure 5.6 – Stylized images using neural style transfer'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6 – 使用神经风格迁移的风格化图像'
- en: '](img/B14538_05_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_05_06.jpg)'
- en: Figure 5.6 – Stylized images using neural style transfer
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 使用神经风格迁移的风格化图像
- en: Feel free to change the weights and layers to create the styles that you want.
    I hope you now have a better understanding of content and style representation,
    which will come in handy when we explore advanced generative models. Next, we
    will look at ways to improve the neural style transfer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随时调整权重和层次，创造你想要的风格。我希望你现在对内容和风格的表示有了更好的理解，这将在我们探索高级生成模型时派上用场。接下来，我们将看看如何改进神经风格迁移。
- en: Improving style transfer
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进风格迁移
- en: The research community and industry were excited about neural style transfer
    and wasted no time in putting it to use. Some set up websites to allow users to
    upload photos to perform style transfer, while some used that to create merchandise
    to sell. Then people realized some of the shortcomings of the original neural
    style transfer and worked to improve it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 研究界和工业界对神经风格迁移感到兴奋，并迅速开始应用它。有些人搭建了网站，允许用户上传照片进行风格迁移，而有些则利用该技术创造商品进行销售。随后，人们意识到原始神经风格迁移的一些局限性，并开始改进它。
- en: One of the biggest limitations is that style transfer takes all the style information,
    including the color and brush strokes of the entire style image, and transfers
    it to the whole of the content image. Using the examples that we just did in the
    previous section, the blueish color from the style image was transferred into
    both the building and background. Wouldn't it be nice if we had the choice to
    transfer only the brush stroke but not the color, and just to the preferred regions?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个最大的问题是，风格迁移将风格图像的所有风格信息，包括颜色和笔触，转移到内容图像的整个图像上。使用我们在前一节中做的例子，风格图像中的蓝色调被转移到了建筑物和背景中。如果我们可以选择只转移笔触而不转移颜色，并且仅转移到所需的区域，那该多好呢？
- en: 'The lead author of neural style transfer and his team produced a new algorithm
    to address these issues. The following diagram shows the control the algorithm
    can give and an example of the results:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移的首席作者及其团队提出了一种新的算法来解决这些问题。下图展示了该算法能够提供的控制效果以及结果示例：
- en: '![Figure 5.7 – Different control methods of neural style transfer. (a) Content
    image (b) The sky and ground are stylized using different style images (c) The
    color of the content image is preserved (d) The fine scale and coarse scale are
    stylized using different style images'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.7 – 神经风格迁移的不同控制方法。（a）内容图像（b）天空和地面使用不同的风格图像进行风格化（c）保持内容图像的颜色（d）细尺度和粗尺度使用不同的风格图像进行风格化'
- en: '(Source: L. Gatys, 2017, “Controlling Perceptual Factors in Neural Style Transfer”,
    https://arxiv.org/abs/1611.07865)](img/B14538_05_07.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：L. Gatys, 2017，“控制神经风格迁移中的感知因素”，https://arxiv.org/abs/1611.07865）](img/B14538_05_07.jpg)
- en: 'Figure 5.7 – Different control methods of neural style transfer. (a) Content
    image (b) The sky and ground are stylized using different style images (c) The
    color of the content image is preserved (d) The fine scale and coarse scale are
    stylized using different style images (Source: L. Gatys, 2017, “Controlling Perceptual
    Factors in Neural Style Transfer”, https://arxiv.org/abs/1611.07865)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 神经风格迁移的不同控制方法。（a）内容图像（b）天空和地面使用不同的风格图像进行风格化（c）保持内容图像的颜色（d）细尺度和粗尺度使用不同的风格图像进行风格化（来源：L.
    Gatys, 2017，“控制神经风格迁移中的感知因素”，https://arxiv.org/abs/1611.07865）
- en: 'The controls proposed in this paper are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的控制方法如下：
- en: '**Spatial control**: This controls the spatial location of style transfer in
    both the content and style images. This is done by applying a spatial mask to
    style features before calculating the Gram matrix.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间控制**：这控制了风格迁移在内容图像和风格图像中的空间位置。通过在计算Gram矩阵之前，对风格特征应用空间掩码来实现这一点。'
- en: '**Color control**: This can be used to preserve the color of the content image.
    To do this, we will convert the RGB format into color space such that HCL separates
    the luminance (brightness) from other color channels. We can think of the luminance
    channel as a grayscale image. We then perform style transfer only in the luminance
    channel and then merge it with color channels from the original style image to
    give the final stylized image.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**颜色控制**：这可以用来保持内容图像的颜色。为此，我们将RGB格式转换为色彩空间，使得HCL将亮度（明度）与其他颜色通道分开。我们可以将亮度通道视为灰度图像。然后，我们仅在亮度通道进行风格迁移，并将其与原始风格图像中的颜色通道合并，以得到最终的风格化图像。'
- en: '**Scale control**: This manages the granularity of the brush strokes. The process
    is more involved as it requires multiple runs of style transfers and different
    layers of style features to be chosen in order to compute the Gram matrix.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尺度控制**：这管理笔触的颗粒度。这个过程更为复杂，因为它需要多次执行风格迁移，并选择不同层次的风格特征以计算Gram矩阵。'
- en: These perceptual controls are useful for creating better stylized images that
    suit your requirements. I'll leave it as an exercise for you to implement those
    controls if you desire, because we have more important things to cover.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些感知控制对于创建更适合您需求的风格化图像非常有用。如果您愿意，我将把实现这些控制作为一个练习留给您，因为我们有更重要的内容需要讨论。
- en: 'The following are the two major themes associated with improving style transfer
    that had a big influence on the development of GANs:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与改善风格迁移相关的两个主要主题，它们对生成对抗网络（GAN）的发展产生了重大影响：
- en: Improving speed
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高速度
- en: Improving style variations
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善风格变化
- en: Let's go through some of these developments to lay some foundations for the
    next project that we will implement – performing arbitrary style transfer in real
    time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些进展，为我们接下来的项目奠定一些基础——在实时中执行任意风格迁移。
- en: Faster style transfer with a feed-forward network
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用前馈网络实现更快的风格迁移
- en: 'Neural style transfer is based on optimization that is akin to neural network
    training. It is slow and takes several minutes to run even with the use of a GPU.
    This limited its potential applications on mobile devices. As a result, researchers
    were motivated to develop faster algorithms for style transfer and **feed-forward
    style transfer** was born. The following diagram shows one of the first networks
    that employed such an architecture:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移基于类似于神经网络训练的优化。它较慢，即使使用GPU，也需要几分钟才能完成。这限制了它在移动设备上的潜在应用。因此，研究人员有动力开发更快速的风格迁移算法，**前馈风格迁移**应运而生。下图展示了采用这种架构的第一个网络之一：
- en: '![Figure 5.8 – Block diagram of a feed-forward convolutional neural network
    for style transfer.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8 – 用于风格迁移的前馈卷积神经网络的框图。'
- en: '(Redrawn from: J. Johnson et al., 2016 “Perceptual Losses for Real-Time Style
    Transfer and Super-Resolution” – https://arxiv.org/abs/1603.08155)](img/B14538_05_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (重绘自：J. Johnson 等人，2016年《实时风格迁移和超分辨率的感知损失》 – https://arxiv.org/abs/1603.08155)](img/B14538_05_08.jpg)
- en: 'Figure 5.8 – Block diagram of a feed-forward convolutional neural network for
    style transfer. (Redrawn from: J. Johnson et al., 2016 “Perceptual Losses for
    Real-Time Style Transfer and Super-Resolution” – https://arxiv.org/abs/1603.08155)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 用于风格迁移的前馈卷积神经网络的框图。 (重绘自：J. Johnson 等人，2016年《实时风格迁移和超分辨率的感知损失》 – https://arxiv.org/abs/1603.08155)
- en: 'The architecture is simpler than the how the block diagram looks. There are
    two networks in this architecture:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构比框图看起来更简单。该架构中有两个网络：
- en: A **trainable convolutional network** (normally known as a **style transfer
    network**) to translate an input image into a stylized image. This can be implemented
    as an encoder-decoder-like architecture, like that of U-Net or VAE.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**可训练的卷积网络**（通常称为**风格迁移网络**），用于将输入图像转换为风格化图像。它可以实现为类似编码器-解码器的架构，例如U-Net或VAE。
- en: A **fixed convolutional network**, usually a pretrained VGG, that measures the
    content and style losses.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**固定的卷积网络**，通常是预训练的VGG，用于测量内容和风格损失。
- en: Similar to the original neural style transfer, we first extract the content
    and style targets with VGG. Instead of training the input image, we now train
    a convolutional network to translate a content image into a stylized image. The
    content and style features of the stylized image are extracted by VGG, and losses
    are measured and backpropagated to the trainable convolutional network. We train
    it like a normal feed-forward CNN. During inference, we only need to perform one
    forward pass to translate the input image into a stylized image, which is 1,000
    times faster than before!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始神经风格迁移类似，我们首先使用VGG提取内容和风格目标。不同的是，我们现在训练一个卷积网络，将内容图像转化为风格化图像，而不是训练输入图像。风格化图像的内容和风格特征通过VGG提取，并计算损失，反向传播到可训练的卷积网络。我们像训练普通的前馈CNN一样训练它。在推理阶段，我们只需要执行一次前向传递即可将输入图像转换为风格化图像，这比之前快了1,000倍！
- en: Alright, the speed problem is now solved, but there is still a problem. Such
    a network could only learn one style to transfer. We'll need to train one network
    for each of the styles we want to perform, which is a lot less flexible than the
    original style transfer. Then people started working on that, and as you may have
    guessed, that got solved too! We'll go over that shortly.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，速度问题现在解决了，但仍然存在一个问题。这样的网络只能学习一种风格进行迁移。我们需要为每种想要执行的风格训练一个网络，这比原始的风格迁移要不够灵活。于是人们开始着手解决这个问题，正如你可能猜到的那样，这个问题也得到了解决！我们稍后会讲解。
- en: A different style feature
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的风格特征
- en: The original neural style transfer paper didn't explain why the Gram matrix
    is effective as a style feature. Many subsequent improvements to style transfers,
    such as the feed-forward style transfer, continued using the Gram matrix solely
    as style features. That's changed with the *Demystifying Neural Style Transfer*
    paper published by Y, Li et al. in 2017\. It was found that the style information
    is intrinsically represented by the *distributions of activations* in a CNN. They
    have shown that matching Gram matrices of activations are equivalent to minimizing
    the **maximum mean discrepancy** (**MMD**) of activation distributions. Therefore,
    we can perform style transfer by matching the activation distribution of an image
    to those of the style image.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的神经风格迁移论文没有解释为什么Gram矩阵作为风格特征是有效的。许多后来的风格迁移改进，例如前馈风格迁移，继续仅将Gram矩阵用作风格特征。直到2017年，Y,
    Li等人发表的*《揭秘神经风格迁移》*论文才做出了改变。他们发现风格信息本质上是通过CNN中的*激活分布*来表示的。他们展示了匹配激活的Gram矩阵等同于最小化激活分布的**最大均值差异**（**MMD**）。因此，我们可以通过匹配图像的激活分布与风格图像的激活分布来实现风格迁移。
- en: Therefore, the Gram matrix is not the only way in which to implement style transfer.
    We could use adversarial loss, too. Let's recall that GANs such as pix2pix ([*Chapter
    4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084), *Image-to-Image Translation*)
    could perform style transfer by matching the pixel distribution of a generated
    image with the real (style) images. The difference is that GANs try to minimize
    the discrepancy in pixel distribution, while style transfer does it to the layer
    activation's distributions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Gram矩阵并不是实现风格迁移的唯一方法。我们也可以使用对抗性损失。让我们回想一下，像pix2pix这样的GAN（[ *第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084),
    *图像到图像的转换*）可以通过匹配生成图像与真实（风格）图像的像素分布来执行风格迁移。不同之处在于，GAN试图最小化像素分布的差异，而风格迁移是对层激活的分布进行最小化。
- en: Later, researchers found that we can use just the basic statistics of the mean
    and variance of the activations to represent the styles. In other words, if we
    feed two images that are similar in style into VGG, their layer activations will
    have a similar mean and variance. We can therefore train a network to perform
    style transfer by minimizing the difference in the mean and variance of activations
    between a generated image and a style image. This leads to the development of
    using a normalization layer to control the style.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，研究人员发现我们可以仅通过激活的均值和方差的基本统计量来表示风格。换句话说，如果我们将两张风格相似的图像输入到VGG，它们的层激活将具有相似的均值和方差。因此，我们可以通过最小化生成图像与风格图像之间激活均值和方差的差异来训练一个网络进行风格迁移。这促使了使用归一化层来控制风格的研究发展。
- en: Controlling styles with a normalization layer
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用归一化层控制风格
- en: 'A simple but effective way of controlling the activation statistics is by changing
    the gamma ![](img/Formula_05_001.png) and beta *β* in the normalization layer.
    In other words, we could change the style by using different affine transform
    parameters (gamma and beta). As a reminder, both batch normalization and instance
    normalization share the same equation, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单而有效的控制激活统计量的方法是通过改变归一化层中的gamma ![](img/Formula_05_001.png)和beta *β*。换句话说，我们可以通过使用不同的仿射变换参数（gamma和beta）来改变风格。作为提醒，批归一化和实例归一化共享相同的公式，如下所示：
- en: '![](img/Formula_05_002.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_002.jpg)'
- en: The difference is that batch normalization (*BN*) calculates the mean *µ* and
    standard deviation *σ* across (N, H, W) dimensions, while instance normalization
    (*IN*) calculates only from (H, W).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于批归一化（*BN*）计算的是（N, H, W）维度的均值*µ*和标准差*σ*，而实例归一化（*IN*）仅从（H, W）维度计算。
- en: However, there is only one gamma and beta pair per normalization layer, which
    limits the network to learning only one style. How do we make the network learn
    multiple styles? Well, we could use multiple sets of gammas and betas where each
    set remembers one style. This is exactly what **conditional instance normalization**
    (**CIN**) does.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个归一化层只有一对gamma和beta，这限制了网络只能学习一种风格。我们如何让网络学习多种风格呢？我们可以使用多组gamma和beta，每组记住一种风格。这正是**条件实例归一化**（**CIN**）所做的。
- en: 'It builds upon instance normalization but has multiple sets of gamma and beta
    pairs. Each gamma and beta set is used to train a particular style; in other words,
    they are conditioned on the style images. The equation of conditional instance
    normalization is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于实例归一化，但有多组gamma和beta。每组gamma和beta用于训练特定的风格；换句话说，它们是基于风格图像进行条件化的。条件实例归一化的公式如下：
- en: '![](img/Formula_05_003.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_003.jpg)'
- en: Say we have *S* different style images, then we have *S* gammas and *S* betas
    in normalization layers for each of the styles. In addition to the content image,
    we also feed in the one-hot encoded style label into the style transfer network.
    In practice, gamma and beta are implemented as matrices with a shape of (S×C).
    We retrieve the gamma and beta for that style by performing matrix multiplication
    of a one-hot encoded label (1×S) with matrices (S×C) to get *γ*S and *β*s for
    each (1×C) channel. It is easier to understand when we implement the code. However,
    we will defer implementation to [*Chapter 9*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175),
    *Video Synthesis*, when we use it to perform class condition normalization. We
    are introducing CIN now to prepare ourselves for the upcoming section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有*S*种不同的风格图像，那么我们在每个风格的归一化层中就有*S*个gamma和*S*个beta。除了内容图像外，我们还将one-hot编码的风格标签输入到风格迁移网络中。在实际操作中，gamma和beta被实现为形状为(S×C)的矩阵。我们通过将one-hot编码的标签（1×S）与矩阵（S×C）进行矩阵乘法，来获取每个（1×C）通道的*γ*S和*β*s。通过代码实现时会更容易理解。不过，我们将在[*第9章*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175)，“视频合成”部分详细实现时再介绍。我们现在引入CIN，为接下来的部分做准备。
- en: 'Now, with style encoded into the embedding spaces of gammas and betas, we could
    perform style interpolation by interpolating gammas and betas as shown in the
    following image:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将风格编码到gamma和beta的嵌入空间中，我们可以通过插值gamma和beta来进行风格插值，如下图所示：
- en: '![Figure 5.9 – Combination of artistic styles by interpolating the gammas and
    betas of two different styles'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9 – 通过插值两种不同风格的gamma和beta来组合艺术风格'
- en: '(Source: V. Dumoulin et al., 2017 “A Learned Representation for Artistic Style”
    – https://arxiv.org/abs/1610.07629) ](img/B14538_05_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: V. Dumoulin 等, 2017 “A Learned Representation for Artistic Style” – https://arxiv.org/abs/1610.07629)](img/B14538_05_09.jpg)'
- en: 'Figure 5.9 – Combination of artistic styles by interpolating the gammas and
    betas of two different styles (Source: V. Dumoulin et al., 2017 “A Learned Representation
    for Artistic Style” – https://arxiv.org/abs/1610.07629)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.9 – 通过插值两种不同风格的gamma和beta来组合艺术风格（来源: V. Dumoulin 等, 2017 “A Learned Representation
    for Artistic Style” – https://arxiv.org/abs/1610.07629）'
- en: This is all good, but the network is still limited to the fixed *N* styles that
    are used in training. Next, we will learn and implement an improvement that allows
    any arbitrary styles!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，但网络仍然局限于训练中使用的固定*N*种风格。接下来，我们将学习并实现一个改进，使得任何任意风格都可以进行迁移！
- en: Arbitrary style transfer in real time
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时任意风格迁移
- en: In this section, we will learn how to implement a network that could perform
    arbitrary style transfer in real time. We have already learned how to use a feed-forward
    network for faster inference and that solves the real-time part. We have also
    learned how to use conditional instance normalization to transfer a fixed number
    of styles. Now, we will learn one further normalization technique that allows
    for any arbitrary style, and then we are good to go in terms of implementing the
    code.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何实现一个能够实时进行任意风格迁移的网络。我们已经学习了如何使用前馈网络进行更快的推理，从而解决了实时部分。我们还学习了如何使用条件实例归一化来迁移固定数量的风格。现在，我们将学习另一种归一化技术，它允许任何任意风格，之后我们就可以开始实现代码了。
- en: Implementing adaptive instance normalization
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自适应实例归一化
- en: 'Like CIN, **AdaIN** is also instance normalization, meaning that the mean and
    standard deviation are calculated across (H, W) per image, and per channel, as
    opposed to batch normalization, which calculates across (N, H, W). In CIN, the
    gammas and betas are trainable variables, and they learn the means and variances
    that are needed for different styles. In AdaIN, gammas and betas are replaced
    by standard deviations and means of style features, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 像CIN一样，**AdaIN**也是实例归一化，这意味着均值和标准差是针对每张图像的(H, W)和每个通道计算的，而不是批量归一化，后者是在(N, H,
    W)上计算的。在CIN中，gamma和beta是可训练的变量，它们学习不同风格所需的均值和方差。在AdaIN中，gamma和beta被风格特征的标准差和均值替代，如下所示：
- en: '![](img/Formula_05_004.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_004.jpg)'
- en: AdaIN can still be understood as a form of conditional instance normalization
    where the conditions are the style features rather than the style labels. In both
    training and inference time, we use VGG to extract the style layer outputs and
    use their statistics as the style conditions. This avoids the need to pre-define
    a fixed set of styles. We can now implement AdaIN in TensorFlow. The notebook
    for this is `ch5_arbitrary_style_transfer.ipynb`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: AdaIN仍然可以被理解为一种条件实例归一化形式，其中条件是风格特征，而不是风格标签。在训练和推理时，我们使用VGG提取风格层的输出，并使用它们的统计量作为风格条件。这避免了需要预定义一组固定的风格。我们现在可以在TensorFlow中实现AdaIN。该部分的笔记本是`ch5_arbitrary_style_transfer.ipynb`。
- en: 'We will use TensorFlow''s subclassing to create a custom `AdaIN` layer as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow的子类化来创建一个自定义的`AdaIN`层，如下所示：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is a straightforward implementation of the equation. One thing that deserves
    a bit of explanation is the use of `tf.nn.moments`, which is also used in the
    TensorFlow batch normalization implementation. It calculates the mean and variance
    of the feature maps, where the axes `1`, `2` refer to H, W of the feature maps.
    We also set `keepdims=True` to keep the results in four dimensions with a shape
    of (N, 1, 1, C) as opposed to the default (N, C). The former allows TensorFlow
    to perform broadcast arithmetic with the input tensor that has a shape of (N,
    H, W, C). Here, broadcast refers to repeating one value in bigger dimensions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是方程的一个直接实现。有一点值得解释的是`tf.nn.moments`的使用，它也被用于TensorFlow批量归一化的实现。它计算特征图的均值和方差，其中轴`1`、`2`指的是特征图的H和W。我们还设置了`keepdims=True`，以保持结果为四维，形状为(N,
    1, 1, C)，而不是默认的(N, C)。前者允许TensorFlow对形状为(N, H, W, C)的输入张量进行广播运算。这里的广播指的是在更大的维度中重复一个值。
- en: To be more precise, when we subtract *x* from the calculated mean for a particular
    instance and channel, the single mean value will first be repeated into the shape
    of (H, W) before the subtraction. We will now look at how to incorporate AdaIN
    into style transfer.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，当我们从特定实例和通道的计算均值中减去*x*时，单个均值首先会重复成（H, W）的形状，然后再进行减法操作。接下来我们将看看如何将AdaIN应用到风格迁移中。
- en: Style transfer network architecture
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格迁移网络架构
- en: 'The following diagram shows the architecture of a style transfer network and
    the training pipeline:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了风格迁移网络的架构和训练流程：
- en: '![Figure 5.10 – Overview of style transfer with AdaIN'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10 – 使用AdaIN进行风格迁移概览'
- en: '(Redrawn from: X. Huang, S. Belongie, 2017, “Arbitrary Style Transfer in Real
    Time with Adaptive Instance Normalization” – https://arxiv.org/abs/1703.06868)](img/B14538_05_10.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: （改编自：X. Huang, S. Belongie, 2017年，“使用自适应实例归一化实时进行任意风格迁移” – [https://arxiv.org/abs/1703.06868](https://arxiv.org/abs/1703.06868))](img/B14538_05_10.jpg)
- en: 'Figure 5.10 – Overview of style transfer with AdaIN (Redrawn from: X. Huang,
    S. Belongie, 2017, “Arbitrary Style Transfer in Real Time with Adaptive Instance
    Normalization” – https://arxiv.org/abs/1703.06868)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 使用 AdaIN 进行风格迁移的概述（重绘自：X. Huang, S. Belongie, 2017, “实时自适应实例归一化的任意风格迁移”
    – https://arxiv.org/abs/1703.06868）
- en: The **style transfer network** (**STN**) is an encoder-decoder network where
    the encoder encodes the content and style features with fixed VGG. AdaIN then
    encodes the style features into the statistics of content features and the decoder
    takes these new features to generate the stylized image.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**风格迁移网络**（**STN**）是一个编码器-解码器网络，其中编码器使用固定的 VGG 编码内容和风格特征。然后，AdaIN 将风格特征编码为内容特征的统计信息，解码器则利用这些新的特征生成风格化的图像。'
- en: Building the encoder
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建编码器
- en: 'The following is the code to build the encoder from VGG:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从 VGG 构建编码器的代码：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is similar to neural style transfer, except that we use the last style
    layer, `'block4_conv1'`, as our content layer. Thus, we don't need to define the
    content layer separately. We will now make a small but important improvement to
    the convolutional layer to improve the appearance of generated images.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这与神经风格迁移类似，只不过我们使用最后的风格层，`'block4_conv1'`，作为我们的内容层。因此，我们无需单独定义内容层。接下来，我们将对卷积层做一个小而重要的改进，以提高生成图像的外观。
- en: Reducing block artifacts with reflection padding
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用反射填充减少块状伪影
- en: Normally, when we apply padding to an input tensor in a convolutional layer,
    constant zeros are padded around the tensor. However, the sudden drop in value
    at a border creates high frequency components and results in block artefacts in
    the generated image. One way to reduce these frequency components is by adding
    *total variation loss* as the regularizer in the network training.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们在卷积层对输入张量应用填充时，常数零会围绕张量进行填充。然而，边界处的突变值会产生高频成分，并导致生成图像中的块状伪影。减少这些高频成分的一种方法是将*总变差损失*作为网络训练中的正则化项。
- en: To do that, we first calculate the high frequency components simply by shifting
    the image by one pixel, and then subtract by the original image to create a matrix.
    Total variation loss is the L1 norm or the sum of absolute values. Therefore,
    the training will try to minimize this loss function so as to reduce the high-frequency
    component.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先通过将图像平移一个像素来简单地计算高频成分，然后减去原始图像生成一个矩阵。总变差损失是 L1 范数或绝对值的总和。因此，训练将尽量最小化此损失函数，从而减少高频成分。
- en: There is another alternative, which is to replace the constant zeros in padding
    with reflective values. For example, if we pad an array of [10, 8, 9] with zeros,
    this will give [0, 10, 8, 9, 0]. We can then see a sudden change in values between
    0 and its neighbors.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种替代方法，即用反射值替代填充中的常数零。例如，如果我们用零填充一个 [10, 8, 9] 的数组，这将得到 [0, 10, 8, 9, 0]。然后我们可以看到值在
    0 和其邻近值之间发生突变。
- en: 'If we use reflective padding, the padded array will be [8, 10, 8, 9, 8], which
    provides a smoother transition toward the border. However, Keras Conv2D doesn''t
    support reflective padding, so we will have to create a custom Conv2D using TensorFlow
    subclassing. The following code snippet (the code has been curtailed for brevity;
    please check out GitHub for the entire code) shows how to add reflective padding
    to the input tensor prior to the convolution:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用反射填充，填充后的数组将是 [8, 10, 8, 9, 8]，这将提供一个更平滑的过渡到边界。然而，Keras 的 Conv2D 不支持反射填充，因此我们需要使用
    TensorFlow 子类化来创建一个自定义的 Conv2D。以下代码片段（代码已简化，完整代码请参阅 GitHub）展示了如何在卷积前向输入张量添加反射填充：
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code is taken from [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017),
    *Getting Started with Image Generation Using TensorFlow*, but with an added low-level
    `tf.pad` API to pad the input tensor.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码来自 [*第1章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)，*使用 TensorFlow
    开始图像生成*，但增加了一个低级的 `tf.pad` API 用于填充输入张量。
- en: Building the decoder
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建解码器
- en: 'Although we use 4 VGG layers (`block1_conv1` to `block4_conv1`) in the encoder
    code, only the last layer, `block4_conv1`, from the encoder is used by AdaIN.
    Therefore, the input tensor to the decoder has the same activation as `block4_conv1`.
    The decoder architecture is not too dissimilar to the ones we have implemented
    in earlier chapters. It consists of convolutional and upsampling layers, as shown
    in the following code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在编码器代码中使用了4个VGG层（`block1_conv1`到`block4_conv1`），但只有编码器中的最后一层`block4_conv1`被AdaIN使用。因此，解码器的输入张量具有与`block4_conv1`相同的激活。解码器架构与我们在前几章中实现的相似，包含卷积层和上采样层，如下所示：
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding code uses custom `Conv2D` with reflective padding. All layers
    use the ReLU activation function, except the output layer, which does not have
    any non-linearity activation function. We have now completed AdaIN, the encoder,
    and the decoder, and can move on to the image pre-processing pipeline.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用了自定义的`Conv2D`并且使用了反射填充。所有层都使用ReLU激活函数，除了输出层，它没有任何非线性激活函数。我们现在已经完成了AdaIN、编码器和解码器的构建，可以继续进行图像预处理流水线。
- en: VGG processing
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VGG处理
- en: 'Like the neural style transfer we built earlier, we will need to pre-process
    the image by inverting the color channel to BGR and then subtracting the color
    means. The code is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前构建的神经风格迁移一样，我们需要通过将颜色通道反转为BGR并减去颜色均值来预处理图像。代码如下：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We could do the same in post-processing, that is, adding back the color means
    and reversing the color channel. However, this is something that could be learned
    by the decoder as color means is equivalent to the biases in the output layer.
    We will let the training do the job and all we need to do is to clip the pixels
    to range of [0, 255], as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在后处理阶段做相同的操作，即将颜色均值加回来并反转颜色通道。然而，这可以由解码器学习，因为颜色均值相当于输出层中的偏置。我们将让训练来完成这项任务，我们所需要做的就是将像素裁剪到[0,
    255]的范围内，如下所示：
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We now have all the building blocks ready and all that is left to do is to put
    them together to create the STN and training pipeline.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有的构建模块，剩下的就是将它们组合起来创建STN和训练流水线。
- en: Building the style transfer network
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建风格迁移网络
- en: 'Constructing the **STN** is straightforward and simply involves connecting
    the encoder, AdaIN, and decoder, as shown in the preceding architectural diagram.
    The STN is also the model we will use to perform inference. The code to do this
    is as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 构建**STN**非常简单，只需将编码器、AdaIN和解码器连接起来，如前面的架构图所示。STN也是我们用于执行推理的模型。实现这一功能的代码如下：
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The content and style images are pre-processed and fed into the encoder. The
    last feature layer, that is, `block4_conv1` from both images, goes to `AdaIN()`.
    The stylized feature then goes into the decoder to generate the stylized image
    in RGB.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 内容图像和风格图像经过预处理后输入编码器。最后的特征层，即来自两幅图像的`block4_conv1`，进入`AdaIN()`。风格化后的特征然后进入解码器，生成RGB格式的风格化图像。
- en: Arbitrary style transfer training
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任意风格迁移训练
- en: 'Like neural and feed-forward style transfer, content loss and style loss are
    computed from an activation extracted by the fixed VGG. The content loss is also
    an L2 norm, but the generated stylized image''s content features are now compared
    against AdaIN''s output rather than the features from the content image, as shown
    in the following code. The authors of the paper found that this makes convergence
    faster:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 就像神经和前馈风格迁移一样，内容损失和风格损失是通过固定的VGG提取的激活计算得出的。内容损失也是L2范数，但生成的风格化图像的内容特征现在与AdaIN的输出进行比较，而不是与内容图像的特征进行比较，如下所示。论文的作者发现，这可以加速收敛：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For style loss, the commonly used Gram matrix is replaced with the L2 norm
    of the activation statistics of mean and variance. This produces similar results
    to the Gram matrix but is conceptually cleaner. The following is the style loss
    function equation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于风格损失，常用的Gram矩阵被替换为激活统计量的L2范数（均值和方差）。这产生与Gram矩阵类似的结果，但在概念上更加简洁。以下是风格损失函数的方程：
- en: '![](img/Formula_05_005.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_005.jpg)'
- en: Here, *φ*i denotes a layer in VGG-19 used to compute the style loss.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*φ*i表示用于计算风格损失的VGG-19中的某一层。
- en: 'We use `tf.nn.moments` as in the AdaIN layer to calculate the statistics and
    the L2 norm between the features from stylized and style images. Each style layer
    carries the same weight, and hence we average the content layer losses as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像AdaIN层一样使用`tf.nn.moments`来计算风格化图像和风格图像的特征统计量以及L2范数。每个风格层的权重相同，因此我们平均内容层的损失，如下所示：
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The final step is to write the training step, as shown here:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是编写训练步骤，如下所示：
- en: '[PRE19]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Instead of tweaking weights to both the content and style, we fix the content
    weight to be `1` and adjust just the style weight. In this example, we set the
    content weight to `1` and the style weight to `1e-4`. In *Figure 5.10*, it may
    look like there are three networks to train but two of them are fixed VGG, so
    the only trainable network is the decoder. Therefore, we only track and apply
    gradients to the decoder.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再调整内容和风格的权重，而是将内容权重固定为 `1`，仅调整风格权重。在这个例子中，我们将内容权重设置为 `1`，将风格权重设置为 `1e-4`。在
    *图 5.10* 中，可能看起来有三个网络需要训练，但其中两个是固定的 VGG 网络，因此唯一需要训练的网络是解码器。因此，我们只跟踪并应用解码器的梯度。
- en: Tips
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The preceding training step can be replaced by Keras' `train_on_batch()` function
    (see [*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060), *Generative
    Adversarial Network*), which uses fewer code lines. I'll leave this to you as
    an additional exercise.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的训练步骤可以通过 Keras 的 `train_on_batch()` 函数替代（参见 [*第 3 章*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)，*生成对抗网络*），这样可以减少代码行数。我将把这个作为附加练习留给你。
- en: 'In this example, we''ll use faces as content images, and `cyclegan/vangogh2photo`
    for the styles. Although Van Gogh''s paintings are of one artistic style, from
    the style transfer perspective, each style image is a unique style. The `vangoh2photo`
    dataset contains 400 style images, meaning we are training the network with 400
    different styles! The following diagram shows examples of images produced by our
    network:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用人脸作为内容图像，使用 `cyclegan/vangogh2photo` 作为风格图像。尽管梵高的画作属于一种艺术风格，但从风格迁移的角度来看，每个风格图像都是一个独特的风格。`vangogh2photo`
    数据集包含了 400 张风格图像，这意味着我们正在用 400 种不同的风格训练网络！以下图示展示了我们网络生成的图像示例：
- en: '![Figure 5.11 – Arbitrary style transfer. (Left) Style image (Middle) Content
    image (Right) Stylized image'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11 – 任意风格迁移。（左）风格图像（中）内容图像（右）风格化图像'
- en: '](img/B14538_05_11.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_05_11.jpg)'
- en: Figure 5.11 – Arbitrary style transfer. (Left) Style image (Middle) Content
    image (Right) Stylized image
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 任意风格迁移。（左）风格图像（中）内容图像（右）风格化图像
- en: The images in the preceding diagram shows the style transfers in inference time
    using style images that were not previously seen by the network. Each style transfer
    happens only with a single forward pass, which is a lot faster than the iterative
    optimization of the original neural style transfer algorithm. Having understood
    various techniques to perform style transfer, we are now in a good position to
    learn how to design GANs in style (pun intended).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图中的图像展示了在推理时使用风格图像进行的风格迁移，这些风格图像是网络之前没有见过的。每次风格迁移只需进行一次前向传播，这比原始神经风格迁移算法的迭代优化要快得多。在理解了各种风格迁移技术后，我们现在可以很好地学习如何设计具有风格的
    GAN（双关语）。
- en: Introduction to style-based GANs
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风格基础 GAN 简介
- en: The innovations in style transfer made their way into influencing the development
    of GANs. Although GANs at that time could generate realistic images, they were
    generated by using random latent variables, where we had little understanding
    in terms of what they represented. Even though multimodal GANs could create variations
    in generated images, we did not know how to control the latent variables to achieve
    the outcome that we wanted.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 风格迁移的创新对 GAN 的发展产生了影响。尽管当时的 GAN 能够生成逼真的图像，但它们是通过使用随机潜在变量生成的，我们对这些潜在变量代表的内容几乎没有理解。即使多模态
    GAN 能够创建生成图像的变化，但我们不知道如何控制潜在变量以达到我们想要的结果。
- en: In an ideal world, we would love to have some knobs to independently control
    the features we would like to generate, as in the face manipulation exercise in
    [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Variational Autoencoder*.
    This is known as **disentangled representation**, which is a relatively new idea
    in deep learning. The idea of disentangled representation is to separate an image
    into independent representation. For example, a face has two eyes, a nose, and
    a mouth, with each of them being a representation of a face. As we have learned
    in style transfer, an image can be disentangled into content and style. So researchers
    brought that idea into GANs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，我们希望能够拥有一些控制器，独立地控制我们希望生成的特征，就像在[*第2章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)中关于面部操控的练习一样，变分自编码器（Variational
    Autoencoder）。这被称为**解耦表示**，这是深度学习中的一个相对较新的概念。解耦表示的思想是将一张图像分解为独立的表示。例如，一张面部图像包含两只眼睛、一只鼻子和一张嘴，每个部分都是面部的一个表示。正如我们在风格迁移中学到的，一张图像可以解耦为内容和风格。因此，研究人员将这一思想引入了生成对抗网络（GAN）。
- en: In the next section, we will look at a style-based GAN known as **MUNIT**. As
    we are limited by the number of pages in the book, we won't be writing the detailed
    code, but will go over the overall architecture to understand how style is used
    in these models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨一种基于风格的生成对抗网络（GAN），即**MUNIT**。由于书本的篇幅限制，我们不会编写详细的代码，而是会概述整体架构，以理解这些模型中风格是如何应用的。
- en: Multimodal Unsupervised Image-to-Image Translation (MUNIT)
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态无监督图像到图像翻译（MUNIT）
- en: MUNIT is an image-to-image translation model similar to BicycleGAN ([*Chapter
    4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084), *Image-to-Image Translation*).
    Both can generate multimodal images with continuous distributions, but BicycleGAN
    needs to have paired data while MUNIT does not. BicycleGAN generates multimodal
    images by using two models that relate the target image to latent variables. It
    is not very clear how these models work, nor how to control the latent variable
    to change the output. MUNIT's approach is conceptually a lot different, but also
    a lot simpler to understand. It assumes that the source and target images share
    the same content space, but with different styles.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: MUNIT是一种图像到图像翻译模型，类似于BicycleGAN（[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)，*图像到图像翻译*）。这两者都可以生成具有连续分布的多模态图像，但BicycleGAN需要配对数据，而MUNIT则不需要。BicycleGAN通过使用两个模型，将目标图像与潜在变量相关联，生成多模态图像。然而，如何控制潜在变量来改变输出，及这些模型如何工作，并不非常清晰。而MUNIT的方式在概念上有很大的不同，但也更易于理解。它假设源图像和目标图像共享相同的内容空间，但风格不同。
- en: 'The following diagram shows the principal idea behind MUNIT:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了MUNIT背后的主要思想：
- en: '![Figure 5.12 – Illustration of the MUNIT method.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.12 – MUNIT方法的示意图。'
- en: '(Redrawn from: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image
    Translation” – https://arxiv.org/abs/1804.04732)](img/B14538_05_12.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: （改绘自：X. Huang等人，2018年，“多模态无监督图像到图像翻译” – https://arxiv.org/abs/1804.04732）](img/B14538_05_12.jpg)
- en: 'Figure 5.12 – Illustration of the MUNIT method.(Redrawn from: X. Huang et al.,
    2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – MUNIT方法的示意图。（改绘自：X. Huang等人，2018年，“多模态无监督图像到图像翻译” – https://arxiv.org/abs/1804.04732）
- en: Say we have two images, **X**1 and **X**2\. Each of them can be represented
    as a content code and style code pair (**C**1, **S**1) and (**C**2, **S**2), respectively.
    It is assumed that both **C**1 and **C**2 are in a shared content space, **C**.
    In other words, the contents may not be exactly the same but are similar. The
    styles are in their respective domain-specific style spaces. Therefore, image
    translation from **X**1 and **X**2 can be formulated as generating image with
    content code from **X**1 and style code from **X**2, or, in other words, from
    code (**C**1, **S**2).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两张图像，**X**1 和 **X**2。每张图像都可以表示为内容编码和风格编码的对（**C**1, **S**1）和（**C**2, **S**2）。假设**C**1和**C**2都处于共享内容空间**C**中。换句话说，内容可能不完全相同，但相似。风格则存在于各自的特定领域风格空间中。因此，从**X**1和**X**2的图像翻译可以表述为生成一个图像，内容编码来自**X**1，风格编码来自**X**2，或者换句话说，来自编码对（**C**1,
    **S**2）。
- en: Previously in style transfer, we viewed styles as artistic styles with different
    brush strokes, colors, and textures. Now, we expand the meaning of style to beyond
    artistic painting. For example, tigers and lions are just cats with different
    styles of whiskers, skin, fur, and shapes. Next, let's look at the MUNIT model
    architecture.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在以前的风格迁移中，我们将风格视为具有不同笔触、颜色和纹理的艺术风格。现在，我们将风格的意义扩展到艺术画作之外。例如，老虎和狮子只是具有不同胡须、皮肤、毛发和形态风格的猫。接下来，让我们来看一下MUNIT模型架构。
- en: Understanding the architecture
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解架构
- en: 'The MUNIT architecture is shown in the following diagram:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: MUNIT架构如下面的图所示：
- en: '![Figure 5.13 – MUNIT model overview'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.13 – MUNIT模型概览'
- en: '(Redrawn from: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image
    Translation” – https://arxiv.org/abs/1804.04732)](img/B14538_05_13.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: (重绘自：X. Huang 等人, 2018年，“多模态无监督图像到图像的转换” – https://arxiv.org/abs/1804.04732)](img/B14538_05_13.jpg)
- en: 'Figure 5.13 – MUNIT model overview (Redrawn from: X. Huang et al., 2018, “Multimodal
    Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – MUNIT模型概览（重绘自：X. Huang 等人, 2018年，“多模态无监督图像到图像的转换” – https://arxiv.org/abs/1804.04732）
- en: There are two autoencoders, one in each domain. The autoencoder encodes the
    image into its style and content codes, and then the decoder decodes them back
    into the original image. This is trained using adversarial loss, in other words,
    the model is made up of an autoencoder but is trained like a GAN.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个自动编码器，一个位于每个域中。自动编码器将图像编码为风格和内容编码，然后解码器将其解码回原始图像。这是通过对抗损失进行训练的，换句话说，模型由一个自动编码器组成，但其训练方式类似于GAN。
- en: 'In the preceding diagram, the image reconstruction process is shown on the
    left. On the right is the cross-domain translation. As mentioned earlier, to translate
    from **X**1 to **X**2, we first encode the images into their respective content
    and style codes, and then we do two things with it as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，图像重建过程显示在左侧。右侧是跨域转换。如前所述，要将**X**1转换为**X**2，我们首先将图像编码为各自的内容和风格编码，然后我们对其进行以下两项操作：
- en: We generate a fake image in style domain 2 with (**C**1, **S**2). This is also
    trained using GANs.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在风格域2中生成一个假的图像，风格为（**C**1, **S**2）。这个过程也是通过GANs进行训练的。
- en: We encode the fake image into content and style code. If the translation works
    well, then it should be similar to (**C**1, **S**2).
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将假图像编码为内容和风格编码。如果转换效果良好，那么它应该与（**C**1, **S**2）相似。
- en: Well, if this is sounding very familiar to you, that is because this is the
    *cycle consistency constraint* from CycleGAN. Except, here the cycle consistency
    is not applied to the image, but to the content and style codes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果这听起来很熟悉，那是因为这就是*循环一致性约束*，它来自CycleGAN。只不过，在这里循环一致性并不是应用于图像，而是应用于内容和风格编码。
- en: Looking into autoencoder design
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索自动编码器设计
- en: 'Finally, let''s look at the detailed architecture of the autoencoder, as shown
    in the following diagram:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一下自动编码器的详细架构，如下图所示：
- en: '![Figure 5.14 – MUNIT model overview'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.14 – MUNIT模型概览'
- en: '(Source: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation”
    – https://arxiv.org/abs/1804.04732)](img/B14538_05_14.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：X. Huang 等人, 2018年，“多模态无监督图像到图像的转换” – https://arxiv.org/abs/1804.04732)](img/B14538_05_14.jpg)
- en: 'Figure 5.14 – MUNIT model overview (Source: X. Huang et al., 2018, “Multimodal
    Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – MUNIT模型概览（来源：X. Huang 等人, 2018年，“多模态无监督图像到图像的转换” – https://arxiv.org/abs/1804.04732）
- en: Unlike other style transfer models, MUNIT doesn't use VGG as an encoder. It
    uses two separate encoders, one for content and another for style. The content
    encoder consists of several residual blocks with instance normalization and downsampling.
    This is quite similar to VGG's style feature.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他风格迁移模型不同，MUNIT不使用VGG作为编码器。它使用两个独立的编码器，一个用于内容，另一个用于风格。内容编码器由若干残差块组成，并具有实例归一化和下采样功能。这与VGG的风格特征非常相似。
- en: 'The style encoder is different from the content encoder in two aspects:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 风格编码器与内容编码器在两个方面不同：
- en: Firstly, there is no normalization. As we have learned, normalizing activations
    to zero means removing the style information.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，这里没有归一化。正如我们所学，归一化激活值为零意味着去除了风格信息。
- en: Secondly, the residual blocks are replaced with fully connected layers. This
    is because style is seen as spatially invariant and therefore we don't need convolutional
    layers to provide the spatial information.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，残差块被完全连接层所替代。这是因为风格被视为空间不变的，因此我们不需要卷积层来提供空间信息。
- en: 'This is to say that the style code only contains information about the eye
    color and doesn''t need to know where the eyes are as it is the responsibility
    of the content code. The style code is a low-dimensional vector and usually has
    the size of 8, which is in contrast to high-dimensional latent variables in GAN
    and VAE, and styles features in style transfer. The reason for a small style code
    size is so that we have a fewer number of knobs to control the styles, which make
    things more manageable. The following diagram shows how the content and style
    code feed into the decoder:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，风格编码只包含眼睛颜色的信息，而无需了解眼睛的位置，因为这是内容编码的责任。风格编码是一个低维向量，通常大小为 8，这与 GAN 和 VAE
    中的高维潜在变量以及风格迁移中的样式特征不同。风格编码大小较小的原因是为了减少控制风格的“旋钮”数量，从而使控制变得更加易于管理。下图展示了内容编码和风格编码如何输入到解码器中：
- en: '![Figure 5.15 – AdaIN layers within the decoder'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15 – 解码器中的 AdaIN 层'
- en: '](img/B14538_05_15.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_05_15.jpg)'
- en: Figure 5.15 – AdaIN layers within the decoder
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 解码器中的 AdaIN 层
- en: 'The generator within the decoder is made up of a group of residual blocks.
    Only residual blocks within the first group have AdaIN as the normalization layer.
    The equation for AdaIN, where *z* is the activation from the previous convolutional
    layer, is shown here:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器中的生成器由一组残差块组成。只有第一组中的残差块使用 AdaIN 作为归一化层。AdaIN 的公式如下，其中 *z* 是前一个卷积层的激活输出：
- en: '![](img/Formula_05_006.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_006.jpg)'
- en: In the arbitrary feed-forward neural style transfer, we use the mean and standard
    deviation from a single style layer as gamma and beta in AdaIN. In MUNIT, the
    gamma and beta are generated from the style code with a **multilayer perceptron**
    (**MLP**).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在任意前馈神经风格迁移中，我们使用单一风格层的均值和标准差作为 AdaIN 中的 gamma 和 beta。在 MUNIT 中，gamma 和 beta
    是通过 **多层感知机** (**MLP**) 从风格编码生成的。
- en: Translating animal images
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动物图像翻译
- en: 'The following screenshot shows samples of *1-to-many* image translations by
    MUNIT. We can generate a variety of output images by using different style codes:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 MUNIT 进行的 *1 对多* 图像翻译样本。通过使用不同的风格编码，我们可以生成多种不同的输出图像：
- en: '![Figure 5.16 – Animal image translation by MUNIT'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16 – MUNIT 动物图像翻译'
- en: '(Source: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation”
    – https://arxiv.org/abs/1804.04732)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：X. Huang 等，2018年，“Multimodal Unsupervised Image-to-Image Translation” –
    https://arxiv.org/abs/1804.04732）
- en: '](img/B14538_05_16.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_05_16.jpg)'
- en: 'Figure 5.16 – Animal image translation by MUNIT (Source: X. Huang et al., 2018,
    “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – MUNIT 动物图像翻译（来源：X. Huang 等，2018年，“Multimodal Unsupervised Image-to-Image
    Translation” – https://arxiv.org/abs/1804.04732）
- en: At the time of writing, MUNIT is still the state-of-the-art model for multimodal
    image-to-image translation, according to [https://paperswithcode.com/task/multimodal-unsupervised-image-to-image](https://paperswithcode.com/task/multimodal-unsupervised-image-to-image).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，MUNIT 仍然是多模态图像到图像翻译领域的最先进模型，详见 [https://paperswithcode.com/task/multimodal-unsupervised-image-to-image](https://paperswithcode.com/task/multimodal-unsupervised-image-to-image)。
- en: If you are interested in the code implementation, you can refer to the official
    implementation by NVIDIA at [https://github.com/NVlabs/MUNIT](https://github.com/NVlabs/MUNIT).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对代码实现感兴趣，可以参考 NVIDIA 提供的官方实现，地址为 [https://github.com/NVlabs/MUNIT](https://github.com/NVlabs/MUNIT)。
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the evolution of styled-based generative models.
    It all started with neural style transfer, where we learned that the image can
    be disentangled into content and style. The original algorithm was slowed and
    the iterative optimization process in inference time replaced with a feed-forward
    style transfer that could perform style transfer in real time.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了基于样式的生成模型的演变。一切始于神经风格迁移，我们了解到图像可以分解为内容和风格。原始算法较慢，在推理时，迭代优化过程被实时样式迁移的前馈过程所取代，能够实现实时的风格迁移。
- en: We then learned that the Gram matrix is not the only method for representing
    style, and that we could use the layers' statistics instead. As a result, normalization
    layers have been explored to control the style of an image, which eventually led
    to the creation of AdaIN. By combing a feed-forward network and AdaIN, we implemented
    arbitrary style transfer in real time.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后了解到，Gram矩阵并不是唯一表示风格的方法，我们也可以使用层的统计信息。因此，已经研究了归一化层来控制图像的风格，这最终促成了AdaIN的诞生。通过结合前馈网络和AdaIN，我们实现了实时的任意风格迁移。
- en: With the success in style transfer, AdaIN found its way into GANs. We went over
    the MUNIT architecture in detail in terms of how AdaIN was used for multimodal
    image generation. There is a style-based GAN that you should be familiar with,
    and it is called StyleGAN. It was made famous for its ability to generate ultra-realistic,
    high-fidelity face images. The implementation of StyleGAN requires pre-requisite
    knowledge of progressive GANs. Therefore, we will defer the detailed implementation
    to [*Chapter 7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136), *High Fidelity
    Face Generation*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 随着风格迁移的成功，AdaIN也被应用于生成对抗网络（GANs）。我们详细介绍了MUNIT架构，重点讲解了AdaIN如何用于多模态图像生成。有一个基于风格的GAN你应该熟悉，它叫做StyleGAN。StyleGAN因其生成超逼真、高保真度人脸图像的能力而闻名。StyleGAN的实现需要对渐进式GAN有所了解。因此，我们将在[*第7章*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)中详细讨论，*高保真度人脸生成*。
- en: At this point, GANs are moving away from the black box method, which uses only
    random noise as input, and toward the disentangled representation approach, which
    better exploits data properties. In the next chapter, we will look at how to use
    specific GAN techniques in drawing paintings.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，GANs正逐渐远离仅使用随机噪声作为输入的黑箱方法，转向更好地利用数据属性的解耦表示方法。在下一章，我们将探讨如何在绘画创作中使用特定的GAN技术。
