- en: Generative Models for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的生成模型
- en: '**Machine learning** (**ML**) and **Artificial Intelligence** (**AI**) have
    touched almost all fields related to man. Agriculture, music, health, defense—you
    won''t find a single field where AI hasn''t left its mark. The enormous success
    of AI/ML, besides the presence of computational powers, also depends on the generation
    of a significant amount of data. The majority of the data generated is unlabeled,
    and hence understanding the inherent distribution of the data is an important
    ML task. It''s here that generative models come into the picture.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）和**人工智能**（**AI**）几乎触及了所有与人类相关的领域。农业、音乐、健康、国防——你找不到一个没有 AI 印记的领域。AI/ML
    的巨大成功，除了计算能力的存在，还依赖于大量数据的生成。大多数生成的数据是未标注的，因此理解数据的内在分布是一个重要的机器学习任务。正是在这一点上，生成模型发挥了作用。'
- en: In the past few years, deep generative models have shown great success in understanding
    data distribution and have been used in a variety of applications. Two of the
    most popular generative models are **Variational Autoencoders** (**VAEs**) and
    **Generative Adversarial Networks** (**GANs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，深度生成模型在理解数据分布方面取得了巨大成功，并已应用于各种领域。最受欢迎的两种生成模型是 **变分自编码器**（**VAEs**）和
    **生成对抗网络**（**GANs**）。
- en: 'In this chapter, we''ll learn about both VAEs and GANs and use them to generate
    images. After reading this chapter, you''ll have covered the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习 VAEs 和 GANs，并使用它们生成图像。阅读完本章后，你将掌握以下内容：
- en: Knowing the difference between generative networks and discriminative networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解生成网络与判别网络之间的区别
- en: Learning about VAEs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 VAEs
- en: Understanding the intuitive functioning of GANs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 GANs 的直观功能
- en: Implementing a vanilla GAN and using it to generate handwritten digits
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个基本的 GAN，并用它生成手写数字
- en: Knowing the most popular variation of GAN, the Deep Convolutional GAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 GAN 最受欢迎的变种——深度卷积 GAN
- en: Implementing the Deep Convolutional GAN in TensorFlow and using it to generate
    faces
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中实现深度卷积 GAN，并用它生成人脸
- en: Knowing further modifications and applications of GANs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 GANs 的进一步修改和应用
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Generative models are an exciting new branch of deep learning models that learn
    through unsupervised learning. The main idea is to generate new samples having
    the same distribution as the given training data; for example, a network trained
    on handwritten digits can create new digits that aren't in the dataset but are
    similar to them. Formally, we can say that if the training data follows the distribution
    *P*[data](*x*), then the goal of generative models is to estimate the probability
    density function *P*[model](*x*), which is similar to *P*[data](*x*).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是深度学习模型中的一个令人兴奋的新分支，它通过无监督学习进行学习。其主要思想是生成具有与给定训练数据相同分布的新样本；例如，一个在手写数字上训练的网络可以生成新的数字，这些数字不在数据集中，但与其相似。从形式上讲，我们可以说，如果训练数据遵循分布
    *P*[data](*x*)，那么生成模型的目标是估计概率密度函数 *P*[model](*x*)，该函数与 *P*[data](*x*) 相似。
- en: 'Generative models can be classified into two types:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以分为两种类型：
- en: '**Explicit generative models**: Here, the probability density function *P*[model](*x*)
    is explicitly defined and solved. The density function may be tractable as in
    the case of PixelRNN/CNN, or an approximation of the density function as in the
    case of VAE.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显式生成模型**：在这些模型中，概率密度函数 *P*[model](*x*) 被显式定义并求解。密度函数可能是可处理的，像 PixelRNN/CNN
    这种情况，或者是密度函数的近似，像 VAE 这种情况。'
- en: '**Implicit generative models**: In these, the network learns to generate a
    sample from *P*[model](*x*) without explicitly defining it. GANs are an example
    of this type of generative model.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐式生成模型**：在这些模型中，网络学习从 *P*[model](*x*) 中生成一个样本，而无需显式地定义它。GANs 就是这种类型的生成模型的一个例子。'
- en: In this chapter, we'll explore VAE, an explicit generative model, and GAN, an
    implicit generative model. Generative models can be instrumental in generating
    realistic samples, and they can be used to perform super-resolution, colorization,
    and so on. With time series data, we can even use them for simulation and planning.
    And last but not least, they can also help us in understanding the latent representation
    of data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 VAE（一种显式生成模型）和 GAN（一种隐式生成模型）。生成模型可以有效地生成逼真的样本，并可用于执行超分辨率、上色等任务。对于时间序列数据，我们甚至可以用它们进行模拟和规划。最后，生成模型还可以帮助我们理解数据的潜在表示。
- en: Generating images using VAEs
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 VAEs 生成图像
- en: 'From [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep Learning
    for IOT*, you should be familiar with autoencoders and their functions. VAEs are
    a type of autoencoder; here, we retain the (trained) **Decoder** part, which can
    be used by feeding random latent features **z** to generate data similar to the
    training data. Now, if you remember, in autoencoders, the **Encoder** results
    in the generation of low-dimensional features, **z**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml)《物联网深度学习》中，你应该对自编码器及其功能有所了解。VAE是一种自编码器；在这里，我们保留了（训练过的）**解码器**部分，可以通过输入随机的潜在特征**z**来生成类似于训练数据的样本。现在，如果你还记得，在自编码器中，**编码器**的作用是生成低维特征，**z**：
- en: '![](img/dd786420-2201-4898-975f-21491a2ed0b9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd786420-2201-4898-975f-21491a2ed0b9.png)'
- en: The architecture of autoencoders
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的架构
- en: 'The VAEs are concerned with finding the likelihood function *p*(*x*) from the
    latent features *z*:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: VAE关注的是从潜在特征**z**中找到似然函数 *p*(*x*)：
- en: '![](img/74b8ddce-feed-42ca-93ee-c4dcb085498b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74b8ddce-feed-42ca-93ee-c4dcb085498b.png)'
- en: 'This is an intractable density function, and it isn''t possible to directly
    optimize it; instead, we obtain a lower bound by using a simple Gaussian prior
    *p*(*z*) and making both **Encoder** and **Decoder** networks probabilistic:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个难以处理的密度函数，无法直接优化；相反，我们通过使用简单的高斯先验 *p*(*z*) 并使**编码器**和**解码器**网络具有概率性质来获得下界：
- en: '![](img/7cd997c5-acba-4732-9b5b-bb83ca6d0bab.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cd997c5-acba-4732-9b5b-bb83ca6d0bab.png)'
- en: Architecture of a VAE
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的架构
- en: 'This allows us to define a tractable lower bound on the log likelihood, given
    by the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够定义一个可处理的对数似然的下界，如下所示：
- en: '![](img/18eabc97-25f9-481b-926c-b58f879c28d2.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18eabc97-25f9-481b-926c-b58f879c28d2.png)'
- en: 'In the preceding, *θ* represents the decoder network parameters and *φ* the
    encoder network parameters. The network is trained by maximizing this lower bound:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的公式中，*θ*表示解码器网络的参数，*φ*表示编码器网络的参数。通过最大化这个下界，网络得以训练：
- en: '![](img/70d81705-2637-40ce-803f-b8c5ac9882b8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70d81705-2637-40ce-803f-b8c5ac9882b8.png)'
- en: The first term in the lower bound is responsible for the reconstruction of the
    input data, and the second term for making the approximate posterior distribution
    close to prior. Once trained, the encoder network works as a recognition or inference
    network, and the decoder network acts as the generator.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下界中的第一个项负责输入数据的重构，第二个项则用于使近似后验分布接近先验分布。训练完成后，编码器网络作为识别或推理网络，而解码器网络则作为生成器。
- en: You can refer to the detailed derivation in the paper titled *Auto-Encoding
    Variational Bayes* by Diederik P Kingma and Max Welling, presented at ICLR 2014
    ([https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考Diederik P Kingma和Max Welling在2014年ICLR会议上发表的论文《Auto-Encoding Variational
    Bayes》（[https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114))中的详细推导。
- en: VAEs in TensorFlow
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的VAE
- en: 'Let''s now see VAE in action. In this example code, we''ll be using the standard
    MNIST dataset and train a VAE to generate handwritten digits. Since the MNIST
    dataset is simple, the encoder and decoder network will consist of only fully
    connected layers; this will allow us to concentrate on the VAE architecture. If
    you plan to generate complex images (such as CIFAR-10), you''ll need to modify
    the encoder and decoder network to convolution and deconvolution networks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下VAE的实际应用。在这个示例代码中，我们将使用标准的MNIST数据集，并训练一个VAE来生成手写数字。由于MNIST数据集较为简单，编码器和解码器网络将仅由全连接层组成；这将帮助我们集中精力于VAE架构。如果你计划生成复杂的图像（例如CIFAR-10），你需要将编码器和解码器网络修改为卷积和反卷积网络：
- en: 'The first step as in all previous cases is to import all of the necessary modules.
    Here, we''ll use the TensorFlow higher API, `tf.contrib`, to make the fully connected
    layers. Note that this saves us from the hassle of declaring weights and biases
    for each layer independently:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与之前的所有情况一样，第一步是导入所有必要的模块。在这里，我们将使用TensorFlow的高级API，`tf.contrib`，来构建全连接层。注意，这样我们就避免了单独声明每一层的权重和偏置的麻烦：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We read the data. The MNIST dataset is available in TensorFlow tutorials, so
    we''ll take it directly from there:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取数据。MNIST数据集可以在TensorFlow教程中找到，所以我们直接从那里获取：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We define the `VariationalAutoencoder` class; this class is the core code.
    It contains methods for defining the encoder and decoder network. The encoder
    generates the mean and variance of the latent feature *z* as `z_mu` and `z_sigma`
    respectively. Using these, a sample `Z` is taken. The latent feature *z* is then
    passed to the decoder network to generate `x_hat`. The network minimizes the sum
    of the reconstruction loss and latent loss using the Adam optimizer. The class
    also defines methods for reconstruction, generation, transformation (to latent
    space), and training a single step:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了`VariationalAutoencoder`类；这个类是核心代码。它包含用于定义编码器和解码器网络的方法。编码器生成潜在特征*z*的均值和方差，分别称为`z_mu`和`z_sigma`。利用这些，取一个样本`Z`。然后，潜在特征*z*被传递给解码器网络生成`x_hat`。网络通过Adam优化器最小化重建损失和潜在损失的总和。该类还定义了重建、生成、转换（到潜在空间）和单步训练的方法：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With all ingredients in place, let''s train our VAE. We do this with the help
    of the `train` function:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有成分准备好之后，我们开始训练我们的VAE。我们通过`train`函数来完成这项任务：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the following screenshot, you can see the reconstructed digits (left) and
    generated handwritten digits (right) for a VAE with the latent space of size 10:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下截图中，您可以看到潜在空间大小为10的VAE的重建数字（左）和生成的手写数字（右）：
- en: '![](img/786be619-47b5-41ef-a066-a0167120a6e3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/786be619-47b5-41ef-a066-a0167120a6e3.png)'
- en: 'As discussed earlier, the encoder network reduces the dimensions of the input
    space. To make it clearer, we reduce the dimension of latent space to 2\. In the
    following, you can see that each label is separated in the two-dimensional z-space:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如之前讨论的，编码器网络将输入空间的维度降低。为了更清楚地说明这一点，我们将潜在空间的维度降为2。以下是二维z空间中每个标签的分离情况：
- en: '![](img/76b1c36f-0c79-4c05-ac28-458193f9e3ba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76b1c36f-0c79-4c05-ac28-458193f9e3ba.png)'
- en: 'The reconstructed and generated digits from a VAE with a latent space of the
    dimension 2 are as follows:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自潜在空间维度为2的VAE的重建和生成数字如下所示：
- en: '![](img/412c3c17-5bea-4042-b19b-205bdd11b800.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/412c3c17-5bea-4042-b19b-205bdd11b800.png)'
- en: 'The interesting thing to note from the preceding screenshot (right) is how
    changing the values of the two-dimensional *z* results in different strokes and
    different numbers. The complete code is on GitHub in `Chapter 07`, in the file
    named `VariationalAutoEncoders_MNIST.ipynb`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图（右）可以看到有趣的一点是，改变二维*z*的值会导致不同的笔画和不同的数字。完整的代码可以在GitHub上的`Chapter 07`文件夹中的`VariationalAutoEncoders_MNIST.ipynb`文件里找到：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The layers (contrib) is a higher level package included in TensorFlow. It provides operations for
    building neural network layers, regularizers, summaries, and so on. In the preceding code,
    we used the `tf.contrib.layers.fully_connected()` operation , defined in [tensorflow/contrib/layers/python/layers/layers.py](https://www.github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/layers/python/layers/layers.py),
    which adds a fully connected layer. By default, it creates weights representing
    a fully connected interconnection matrix, initialized by default using the Xavier
    initialization. It also creates biases initialized to zero. It provides an option
    for choosing normalization and activation function as well.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`contrib`层是TensorFlow中包含的一个更高级的包。它提供了构建神经网络层、正则化器、总结等操作。在前面的代码中，我们使用了`tf.contrib.layers.fully_connected()`操作，定义在[tensorflow/contrib/layers/python/layers/layers.py](https://www.github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/layers/python/layers/layers.py)中，它添加了一个全连接层。默认情况下，它创建代表全连接互连矩阵的权重，默认使用Xavier初始化。同时，它也创建了初始化为零的偏置。它还提供了选择归一化和激活函数的选项。'
- en: GANs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs
- en: GANs are implicit generative networks. During a session at Quora, Yann LeCun,
    Director of AI Research at Facebook and Professor at NYU, described GANs as *the
    most interesting idea in the last 10 years in ML*. At present, lots of research
    is happening in GANs. Major AI/ML conferences conducted in the last few years
    have reported a majority of papers related to GANs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是隐式生成网络。在Quora的一个会议中，Facebook人工智能研究总监兼纽约大学教授Yann LeCun将GANs描述为*过去10年中机器学习领域最有趣的想法*。目前，关于GAN的研究非常活跃。过去几年的主要AI/ML会议上，报道了大多数与GAN相关的论文。
- en: 'GANs were proposed by Ian J. Goodfellow and Yoshua Bengio in the paper *Generative
    Adversarial Networks* in the year 2014 ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)).
    They''re inspired by the two-player game scenario. Like the two players of the
    game, in GANs, two networks—one called the **discriminative network** and the
    other the **generative network**—compete with each other. The generative network
    tries to generate data similar to the input data, and the discriminator network
    has to identify whether the data it''s seeing is real or fake (that is, generated
    by a generator). Every time the discriminator finds a difference between the distribution
    of true input and fake data, the generator adjusts its weights to reduce the difference.
    To summarize, the discriminative network tries to learn the boundary between counterfeit
    and real data, and the generative network tries to learn the distribution of training
    data. As the training ends, the generator learns to produce images exactly like
    the input data distribution, and the discriminator can no longer differentiate
    the two. The general architecture of a GAN is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: GAN由Ian J. Goodfellow和Yoshua Bengio在2014年的论文《生成对抗网络》([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661))中提出。它们的灵感来自于两人对抗的游戏场景。就像游戏中的两个玩家一样，GAN中有两个网络——一个称为**判别网络**，另一个称为**生成网络**——彼此对抗。生成网络试图生成与输入数据相似的数据，而判别网络则必须识别它所看到的数据是来自真实数据还是伪造数据（即由生成器生成）。每次判别器发现真实输入和伪造数据之间的分布差异时，生成器都会调整其权重以减少这种差异。总结来说，判别网络试图学习伪造数据和真实数据之间的边界，而生成网络则试图学习训练数据的分布。随着训练的结束，生成器学会生成与输入数据分布完全相似的图像，判别器则无法再区分二者。GAN的总体架构如下：
- en: '![](img/ea57cdf9-8c57-41a7-ba6d-0b89d7ca7c72.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea57cdf9-8c57-41a7-ba6d-0b89d7ca7c72.png)'
- en: Architecture of GANs
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的架构
- en: 'Let''s now delve deep into how GANs learn. Both the discriminator and generator
    take turns to learn. The learning can be divided into two steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨GAN是如何学习的。判别器和生成器轮流进行学习。学习过程可以分为两个步骤：
- en: 'Here the **Discriminator**, *D*(*x*), learns. The **Generator**, *G*(*z*),
    is used to generate **Fake Images** from random noise **z** (which follows some
    **Prior** distribution *P*(*z*)). The **Fake Images** from the **Generator** and
    the **Real Images** from the training dataset are both fed to the **Discriminator**
    and it performs supervised learning trying to separate fake from real. If *P*[data](*x*)
    is the training dataset distribution, then the **Discriminator Network** tries
    to maximize its objective so that *D*(*x*) is close to 1 when the input data is
    real, and close to 0 when the input data is fake. This can be achieved by performing
    the gradient ascent on the following objective function:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，**判别器**，*D*(*x*)，进行学习。**生成器**，*G*(*z*)，用于从随机噪声**z**（它遵循某种**先验**分布*P*(*z*)）生成**假图像**。**生成器**生成的**假图像**和来自训练数据集的**真实图像**都被输入到**判别器**，然后判别器执行监督学习，试图将假图像与真实图像区分开。如果*P*[data](*x*)是训练数据集的分布，那么**判别器网络**试图最大化其目标，使得当输入数据为真实数据时，*D*(*x*)接近1，当输入数据为假数据时，接近0。这可以通过对以下目标函数执行梯度上升来实现：
- en: '![](img/e5e9e9e9-9f1c-49c5-82dd-b419df342f52.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e9e9e9-9f1c-49c5-82dd-b419df342f52.png)'
- en: 'In the next step, the **Generator Network** learns. Its goal is to fool the
    **Discriminator Network** into thinking that generated *G*(*z*) is real, that
    is, force *D*(*G*(*z*)) close to 1\. To achieve this, the **Generator Network**
    minimizes the objective:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步，**生成器网络**进行学习。它的目标是欺骗**判别器网络**，让它认为生成的*G*(*z*)是真实的，也就是说，迫使*D*(*G*(*z*))接近1。为了实现这一目标，**生成器网络**最小化以下目标：
- en: '![](img/8310f3e2-5c6d-4d72-9e72-0f2539a3d99d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8310f3e2-5c6d-4d72-9e72-0f2539a3d99d.png)'
- en: 'The two steps are repeated sequentially. Once the training ends, the discriminator
    is no longer able to discriminate between real and fake data and the generator
    becomes a pro at creating data very similar to the training data. Well, it''s
    easier said than done: as you experiment with GANs, you''ll find that the training
    isn''t very stable. It''s an open research issue, and many variants of GAN have
    been proposed to rectify the problem.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤会依次重复进行。一旦训练结束，判别器将无法再区分真实数据和伪造数据，生成器也变得非常擅长生成与训练数据非常相似的数据。嗯，说起来容易做起来难：当你尝试使用GAN时，你会发现训练并不是很稳定。这是一个开放的研究问题，已经提出了许多GAN的变种来解决这一问题。
- en: Implementing a vanilla GAN in TensorFlow
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现一个基础的GAN
- en: 'In this section, we''ll write a TensorFlow code to implement a GAN, as we learned
    in the previous section. We''ll use simple MLP networks for both the discriminator
    and generator. And for simplicity, we''ll use the MNIST dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将编写一个TensorFlow代码来实现一个GAN，正如我们在上一节所学的那样。我们将为判别器和生成器使用简单的MLP网络。为了简便起见，我们将使用MNIST数据集：
- en: 'As always, the first step is to add all of the necessary modules. Since we''ll
    need to access and train the generator and discriminator parameters alternatively,
    we''ll define our weights and biases in the present code for clarity. It''s always
    better to initialize weights using the Xavier initialization and biases to all
    zeros. So, we also import from TensorFlow a method to perform Xavier initialization,
    from `tensorflow.contrib.layers import xavier_initializer`:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，第一步是添加所有必要的模块。由于我们需要交替访问和训练生成器和判别器的参数，我们将为了清晰起见在当前代码中定义我们的权重和偏置。使用Xavier初始化权重并将偏置初始化为零总是更好的做法。因此，我们还从TensorFlow导入执行Xavier初始化的方法：`from
    tensorflow.contrib.layers import xavier_initializer`：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s read the data and define hyperparameters:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们读取数据并定义超参数：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We define the training parameters for both generator and discriminator. We
    also define the placeholders for input `X` and latent `Z`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义生成器和判别器的训练参数，并为输入`X`和潜在变量`Z`定义占位符：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we have the placeholders and weights in place, we define functions
    for generating random noise from `Z`. Here, we''re using a uniform distribution
    to generate noise; people have also experimented with using Gaussian noise—to
    do so, you just change the random function from `uniform` to `normal`:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然我们已经设置了占位符和权重，我们定义一个函数来生成来自`Z`的随机噪声。在这里，我们使用均匀分布来生成噪声；有些人也尝试过使用高斯噪声——要做到这一点，你只需要将随机函数从`uniform`改为`normal`：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We construct the discriminator and generator networks:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建判别器和生成器网络：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''ll also need a helper function to plot the handwritten digits generated.
    The following function plots 25 samples generated in a grid of 5×5:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要一个辅助函数来绘制生成的手写数字。以下函数会将生成的25个样本以5×5的网格展示：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we define the TensorFlow operations to generate a sample from the generator
    and a prediction from the discriminator for both fake and real input data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义TensorFlow操作来生成生成器的样本，并从判别器获取对假输入和真实输入数据的预测：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we define cross-entropy losses for the generator and discriminator network,
    and alternatively, minimize them, keeping the other weight parameters frozen:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为生成器和判别器网络定义交叉熵损失，并交替最小化它们，同时保持其他权重参数冻结：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, let''s perform the training within a TensorFlow session:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们在TensorFlow会话中执行训练：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the following screenshot, you can see how the loss for both the generative
    and discriminatives network varies:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下屏幕截图中，你可以看到生成和判别网络的损失是如何变化的：
- en: '![](img/46606566-7484-44ab-88c4-36848eeae909.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46606566-7484-44ab-88c4-36848eeae909.png)'
- en: Loss for both generative and discriminatives network
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 生成和判别网络的损失
- en: 'Let''s also see the handwritten digits generated at different epochs:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们还看看在不同的训练轮次中生成的手写数字：
- en: '![](img/877d7611-dfc9-411e-87e9-9b024bde1493.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/877d7611-dfc9-411e-87e9-9b024bde1493.png)'
- en: Handwritten digits
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字
- en: 'While the handwritten digits are good enough, we can see that a lot of improvements
    can be made. Some approaches used by researchers to stabilize the performance
    are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管手写数字已经足够好，但我们可以看到仍有很多改进的空间。研究人员用来稳定性能的一些方法如下：
- en: Normalize the input images from (0,1) to (-1,1). And, instead of the sigmoid
    as the activation function for the final output of the generator, use the tangent
    hyperbolic activation function.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入图像从(0,1)标准化到(-1,1)。而且，生成器最终输出的激活函数不再使用sigmoid，而是使用双曲正切激活函数。
- en: Instead of minimizing the generator loss minimum `log 1-D`, we can maximize
    the loss maximum `log D`; this can be achieved in TensorFlow by simply flipping
    the labels while training the generator, for example (convert real into fake and
    fake into real).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过最大化损失`log D`来替代最小化生成器损失`log 1-D`；这可以通过在训练生成器时反转标签来实现，例如（将真实标签转为假标签，假标签转为真实标签）。
- en: Another approach is to store previously generated images and train the discriminator
    by choosing randomly from them. (Yes, you guessed right—it's similar to the experience
    replay buffer we learned in [Chapter 6](01e534ff-b0a2-4b5e-bc9a-fd65c527ac7d.xhtml),
    *Reinforcement Learning for IoT*.)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是存储以前生成的图像，并通过从中随机选择来训练判别器。（没错，你猜对了——这类似于我们在[第 6 章](01e534ff-b0a2-4b5e-bc9a-fd65c527ac7d.xhtml)《物联网的强化学习》中学到的经验回放缓冲区。）
- en: People have also experimented with updating the generator or discriminator only
    if their loss is above a certain threshold.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人们还尝试过只有当生成器或判别器的损失超过某个阈值时才更新它们。
- en: Instead of the ReLU activation function for the hidden layers of the discriminator
    and generator, use Leaky ReLU.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在判别器和生成器的隐藏层中，使用 Leaky ReLU 激活函数，而不是 ReLU。
- en: Deep Convolutional GANs
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络
- en: 'In 2016, Alec Radford *et al.* proposed a variation of the GAN called the **Deep
    Convolutional GAN** (**DCGAN**). (The link to the full paper is: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).)
    They replaced the MLP layers with convolutional layers. They also added batch
    normalization in both the generator and discriminator networks. We''ll implement
    DCGAN here on a celebrity images dataset. You can download the ZIP file, `img_align_celeba.zip`,
    from [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
    We make use of the `loader_celebA.py` file we made in [Chapter 2](4351f888-1bf0-4945-a8a6-ddd71bd464dd.xhtml),
    *Data Access and Distributed Processing for IoT,* to unzip and read the images:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2016 年，Alec Radford *等人* 提出了 GAN 的一种变体，叫做 **深度卷积生成对抗网络** (**DCGAN**)。 （完整论文链接：[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)。）他们将
    MLP 层替换为卷积层，并在生成器和判别器网络中都加入了批量归一化。我们将在这里使用名人图像数据集实现 DCGAN。你可以从 [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
    下载 ZIP 文件 `img_align_celeba.zip`。我们将利用我们在[第 2 章](4351f888-1bf0-4945-a8a6-ddd71bd464dd.xhtml)《物联网的数据访问与分布式处理》中创建的
    `loader_celebA.py` 文件来解压并读取图像：
- en: 'We''ll import statements for all of the modules we''ll be requiring:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将导入所有需要的模块的语句：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We use `loader_celebA.py` to unzip `img_align_celeba.zip`. Since the number
    of images is very high, we use the `get_batches` function defined in this file
    to generate batches for training the network:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `loader_celebA.py` 解压 `img_align_celeba.zip`。由于图像数量非常庞大，我们使用该文件中定义的 `get_batches`
    函数来生成用于训练网络的批次：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the following, you can see the dataset images:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，您可以看到数据集的图像：
- en: '![](img/39ae09d7-88db-49d3-8484-11960687886b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39ae09d7-88db-49d3-8484-11960687886b.png)'
- en: 'We define the discriminator network. It consists of three convolutional layers
    with `64`, `128`, and `256` filters respectively, each of size 5×5\. The first
    two layers use a stride of `2` and the third convolutional layer uses a stride
    of `1`. All three convolutional layers use `leakyReLU` as the activation function. Each
    convolutional layer is also followed by a batch normalization layer. The result
    of the third convolutional layer is flattened and passed to the last fully connected
    (dense) layer with the sigmoid activation function:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了判别器网络。它由三个卷积层组成，分别使用 `64`、`128` 和 `256` 个 5×5 大小的滤波器。前两个层使用 `2` 的步幅，第三个卷积层使用
    `1` 的步幅。所有三个卷积层都使用 `leakyReLU` 作为激活函数。每个卷积层后面都跟有一个批量归一化层。第三个卷积层的结果会被拉平，并传递到最后一个全连接（密集）层，该层使用
    sigmoid 激活函数：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The generator network is the reverse of the discriminator; the input to the
    generator is first fed to a dense layer with 2×2×512 units. The output of the
    dense layer is reshaped so that we can feed it to the convolution stack. We use
    the `tf.layers.conv2d_transpose()` method to get the transposed convolution output.
    The generator has three transposed convolutional layers. All of the layers except
    the last convolutional layer have `leakyReLU` as the activation function. The
    last transposed convolution layer uses the tangent hyperbolic activation function
    so that output lies in the range (`-1` to `1`):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器网络是判别器的反向；生成器的输入首先会传递给一个包含 2×2×512 单元的全连接层。全连接层的输出会被重塑，以便我们将其传递给卷积堆栈。我们使用
    `tf.layers.conv2d_transpose()` 方法来获取转置卷积的输出。生成器有三个转置卷积层。除了最后一个卷积层外，所有层都使用 `leakyReLU`
    作为激活函数。最后一个转置卷积层使用双曲正切激活函数，以确保输出位于 (`-1` 到 `1`) 的范围内：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We define functions to calculate the model loss; it defines both the generator
    and discriminator loss and returns them:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了计算模型损失的函数，该函数定义了生成器和判别器的损失，并返回它们：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We next need to define optimizers to make the discriminator and generator learn
    sequentially. To achieve this, we make use of `tf.trainable_variables()` to get
    a list of all training variables, and then first optimize only the discriminator
    training variables, and then the generator training variables:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义优化器，以便判别器和生成器能够顺序学习。为此，我们利用`tf.trainable_variables()`获取所有训练变量的列表，然后首先优化仅判别器的训练变量，再优化生成器的训练变量：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we have all of the necessary ingredients to train the DCGAN. It''s always
    good to keep an eye how the generator has learned, so we define a helper function
    to display the images generated by the generator network as it learns:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经具备了训练DCGAN所需的所有要素。为了更好地观察生成器的学习过程，我们定义了一个辅助函数，来显示生成器网络在学习过程中生成的图像：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, comes the training part. Here, we use the `ops` defined previously
    to train the DCGAN, and the images are fed to the network in batches:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，进入训练部分。在这里，我们使用之前定义的`ops`来训练DCGAN，并将图像按批次输入到网络中：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s now define the parameters of our data and train it:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来定义数据的参数并进行训练：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After each batch, you can see that the generator output is improving:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每处理一个批次，你可以看到生成器的输出在不断改善：
- en: '![](img/4e8528a7-4430-4938-bc2c-93cddef80907.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e8528a7-4430-4938-bc2c-93cddef80907.png)'
- en: DCGAN generator output as learning progresses
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN生成器输出随着学习的进展而改善
- en: Variants of GAN and its cool applications
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN的变体及其酷炫应用
- en: In the last few years, a large number of variants of GANs have been proposed.
    You can access the complete list of different variants of GAN from the GAN Zoo
    GitHub: [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo).
    In this section, we'll list some of the more popular and successful variants.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，已经提出了大量的GAN变体。你可以通过[https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)访问完整的GAN变体列表。在这一部分，我们将列出一些更受欢迎和成功的变体。
- en: Cycle GAN
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cycle GAN
- en: 'At the beginning of the 2018, the Berkeley AI research lab published a paper
    entitled *Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
    Networks *(arXiv link: [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)).
    This paper is special not only because it proposed a new architecture, CycleGAN,
    with improved stability, but also because they demonstrated that such an architecture
    can be used for complex image transformations. The following diagram shows the
    architecture of a cycle GAN; the two sections highlight the **Generator** and
    **Discriminators** playing a role in calculating the two adversarial losses:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年初，伯克利AI研究实验室发表了一篇名为《*使用循环一致对抗网络进行未配对图像到图像的转换*》的论文（arXiv链接：[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)）。这篇论文的特别之处不仅在于它提出了一种新的架构——CycleGAN，并且具有更好的稳定性，还因为它展示了这种架构可以用于复杂的图像转换。以下图示展示了CycleGAN的架构，两个部分分别突出显示了在计算两个对抗损失时起作用的**生成器**和**判别器**：
- en: '![](img/d5036aa6-77cf-41c1-8828-11436977198e.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5036aa6-77cf-41c1-8828-11436977198e.png)'
- en: The architecture of CycleGAN
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN的架构
- en: 'The CycleGAN consists of two GANs. They are trained on two different datasets, *x*∼*P*[data](*x*)
    and *y*∼*P*[data](y). The generator is trained to perform the mappings, namely, *G[A]:
    x→y* and *G[B]: y→x* respectively. Each discriminator is trained so that it can
    differentiate between the image *x* and transformed image *G[B](y)*, hence resulting
    in the adversary loss functions for the two transformations, defined as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'CycleGAN由两个GAN组成。它们分别在两个不同的数据集上进行训练，*x*∼*P*[data](*x*)和*y*∼*P*[data](y)。生成器被训练来执行映射，即*G[A]:
    x→y*和*G[B]: y→x*。每个判别器的训练目标是能够区分图像*x*和变换后的图像*G[B](y)*，从而得出两个变换的对抗损失函数，定义如下：'
- en: '![](img/6f374cc4-833a-4d25-b3df-960cdac65753.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f374cc4-833a-4d25-b3df-960cdac65753.png)'
- en: 'And, the second is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个步骤如下：
- en: '![](img/f1ddf7ab-4912-476c-a8e0-71f929e061f7.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1ddf7ab-4912-476c-a8e0-71f929e061f7.png)'
- en: 'The generators of the two GANs are connected to each other in a cyclic fashion,
    so that if the output of one is fed to another and the corresponding output fed
    back to the first one, we get the same data. Let''s make it clearer with an example;
    let''s say the **Generator A** (**G[A]**) is fed an image *x*, so the output is
    a transformation G[A](*x*). This transformed image now is fed to **Generator B**
    (**G[B]**) *G[B](G[A](x))≈x* and the result should be the initial image *x*. Similarly,
    we shall have G[A](G[B](*y*)≈*y*. This is made possible by introducing a cyclic
    loss term:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 GAN 的生成器以循环方式连接在一起，这样如果一个生成器的输出被输入到另一个生成器中，并且将对应的输出反馈到第一个生成器，我们就会得到相同的数据。我们通过一个例子来解释这个过程；假设**生成器
    A**（**G[A]**）输入一张图像 *x*，那么输出就是转换后的图像 G[A](*x*)。这个转换后的图像现在输入到**生成器 B**（**G[B]**），*G[B](G[A](x))≈x*，结果应该是原始图像
    *x*。类似地，我们也会有 G[A](G[B](*y*)≈*y*。这一过程通过引入循环损失项得以实现：
- en: '![](img/bbc7c74a-a887-4084-82d7-bb139d4ce839.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbc7c74a-a887-4084-82d7-bb139d4ce839.png)'
- en: 'Hence, the net objective function is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络的目标函数如下：
- en: '![](img/ca90af46-a700-49ca-beb6-5b27653e1502.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca90af46-a700-49ca-beb6-5b27653e1502.png)'
- en: 'Here, *λ* controls the relative importance of the two objectives. They also
    retained previous images in an experience buffer to train the discriminator. In
    the following screenshot, you can see some of the results obtained from the CycleGANs
    as reported in the paper:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*λ* 控制着两个目标之间的相对重要性。它们还在经验缓冲区中保留了先前的图像，用于训练判别器。在下面的截图中，你可以看到论文中报告的从 CycleGANs
    获得的一些结果：
- en: '![](img/63324eaf-fd29-46d0-a7cc-02b9c1f9e7fb.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63324eaf-fd29-46d0-a7cc-02b9c1f9e7fb.png)'
- en: Results of CycleGAN (taken from the original paper)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的结果（来自原始论文）
- en: 'The authors showed that CycleGANs can be used for the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了 CycleGANs 可以用于以下几种情况：
- en: '**Image transformation**: Such as changing horses to zebra and vice versa'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像转换**：例如将马变成斑马，反之亦然。'
- en: '**Enhancing the resolution**: The CycleGAN, when trained by a dataset consisting
    of low-resolution and super-resolution images, could perform super-resolution
    when given with low-resolution images'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分辨率增强**：当 CycleGAN 用包含低分辨率和超分辨率图像的数据集进行训练时，能够在输入低分辨率图像时执行超分辨率。'
- en: '**Style transfer**: Given an image, it can be transformed into different painting
    styles'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风格迁移**：给定一张图像，可以将其转换为不同的绘画风格'
- en: Applications of GANs
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs 的应用
- en: 'GANs are indeed interesting networks; besides the applications you''ve seen,
    GANs have been explored in many other exciting applications. In the following,
    we list a few:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 确实是很有趣的网络；除了你已经看到的应用，GANs 还在许多其他令人兴奋的应用中得到了探索。接下来，我们列举了一些：
- en: '**Music generation**: MIDINet, a convolutional GAN, has been demonstrated to
    generate melodies. You can refer to the paper here: [https://arxiv.org/pdf/1703.10847.pdf](https://arxiv.org/pdf/1703.10847.pdf).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音乐生成**：MIDINet 是一种卷积 GAN，已被证明可以生成旋律。你可以参考这篇论文：[https://arxiv.org/pdf/1703.10847.pdf](https://arxiv.org/pdf/1703.10847.pdf)。'
- en: '**Medical anomaly detection**: AnoGAN is a DCGAN shown by Thomas Schlegl et
    al*.* to learn a manifold of normal anatomical variability. They were able to
    train the network to label anomalies on optical coherence tomography images of
    the retina. If the work interests you, you can see the related paper on arXiv
    at [https://arxiv.org/pdf/1703.05921.pdf](https://arxiv.org/pdf/1703.05921.pdf).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学异常检测**：AnoGAN 是一种由 Thomas Schlegl 等人展示的 DCGAN，用于学习正常解剖变异性的流形。他们成功地训练了网络，能够标记视网膜光学相干断层扫描图像中的异常。如果你对这项工作感兴趣，可以在
    arXiv 上阅读相关论文：[https://arxiv.org/pdf/1703.05921.pdf](https://arxiv.org/pdf/1703.05921.pdf)。'
- en: '**Vector arithmetic on faces using GANs**: In the joint research paper by Indico
    Research and Facebook, they demonstrated that it''s possible to use GANs and perform
    image arithmetic. For example, *Man with glasses*—*Man without glasses* + *Woman
    without glasses* = *Woman with glasses*. It''s an interesting paper and you can
    read more about it on Arxiv ([https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 GANs 进行人脸向量运算**：在 Indico Research 和 Facebook 共同研究的论文中，他们展示了使用 GANs 进行图像运算的可能性。例如，*戴眼镜的男人*—*不戴眼镜的男人*
    + *不戴眼镜的女人* = *戴眼镜的女人*。这是一篇有趣的论文，你可以在 Arxiv 上阅读更多内容：[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)。'
- en: '**Text to image synthesis**: GANs have been demonstrated to generate images of
    birds and flowers from human-written textual descriptions. The model uses DCGAN
    along with a hybrid character level convolutional recurrent network. The details
    of the work are given in the paper, *Generative Adversarial Text to Image Synthesis*.
    The link to the paper is [https://arxiv.org/pdf/1605.05396.pdf](https://arxiv.org/pdf/1605.05396.pdf).[ ](https://arxiv.org/pdf/1605.05396.pdf)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本到图像合成**：生成对抗网络（GAN）已被证明可以根据人类书写的文本描述生成鸟类和花卉的图像。该模型使用了DCGAN，并结合了混合字符级卷积递归网络。该工作的详细信息可以在论文《生成对抗文本到图像合成》中找到。论文的链接是[https://arxiv.org/pdf/1605.05396.pdf](https://arxiv.org/pdf/1605.05396.pdf)。'
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This was an interesting chapter, and I hope you enjoyed reading it as much as
    I enjoyed writing it. It's at present the hot topic of research. This chapter
    introduced generative models and their classification, namely implicit generative
    models and explicit generative models. The first generative model that was covered
    is VAEs; they're an explicit generative model and try to estimate the lower bound
    on the density function. The VAEs were implemented in TensorFlow and were used
    to generate handwritten digits.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的章节，希望你在阅读时能像我写这篇文章时一样享受其中。目前这是研究的热点话题。本章介绍了生成模型及其分类，即隐式生成模型和显式生成模型。首先介绍的生成模型是变分自编码器（VAE）；它们是显式生成模型，旨在估计密度函数的下界。我们在TensorFlow中实现了VAE，并用其生成了手写数字。
- en: 'This chapter then moved on to a more popular explicit generative model: GANs.
    The GAN architecture, especially how the discriminator network and generative
    network compete with each other, was explained. We implemented a GAN using TensorFlow
    for generating handwritten digits. This chapter then moved on to the more successful
    variation of GAN: the DCGAN. We implemented a DCGAN to generate celebrity images.
    This chapter also covered the architecture details of CycleGAN, a recently proposed
    GAN, and some of its cool applications.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章随后转向了一个更为流行的显式生成模型：生成对抗网络（GAN）。详细解释了GAN架构，特别是判别器网络和生成器网络如何相互竞争。我们使用TensorFlow实现了一个GAN，用于生成手写数字。本章还介绍了GAN的成功变种：DCGAN，并实现了一个DCGAN，用于生成名人图像。本章还介绍了最近提出的GAN变种——CycleGAN的架构细节，以及它的一些酷应用。
- en: With this chapter, we mark the end of part one of this book. Till now, we concentrated
    on different ML and DL models, which we'll require to understand our data and
    use it for prediction/classification, and other tasks. From the next chapter onward,
    we'll be talking more about the data itself and how we can process the data in
    the present IoT-driven environment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本章标志着本书第一部分的结束。到目前为止，我们集中讨论了不同的机器学习（ML）和深度学习（DL）模型，这些模型是我们理解数据并用于预测/分类以及其他任务所需的。从下一章开始，我们将更多地讨论数据本身，以及在当前物联网驱动的环境中，我们如何处理这些数据。
- en: In the next chapter, we'll move toward distributed processing, a necessity when
    dealing with a large amount of data, and explore two platforms that offer distributed
    processing.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨分布式处理，这是处理大量数据时的必需技术，并介绍两种提供分布式处理的平台。
