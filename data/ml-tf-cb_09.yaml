- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are the primary modern approach for
    modeling data that is sequential in nature. The word "recurrent" in the name of
    the architecture class refers to the fact that the output of the current step
    becomes the input to the next one (and potentially further ones as well). At each
    element in the sequence, the model considers both the current input and what it
    "remembers" about the preceding elements.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）是建模顺序数据的现代主要方法。架构类名中的“循环”一词指的是当前步骤的输出成为下一步的输入（可能还会作为后续步骤的输入）。在序列中的每个元素上，模型都会同时考虑当前的输入和它对前面元素的“记忆”。'
- en: '**Natural Language Processing** (**NLP**) tasks are one of the primary areas
    of application for RNNs: if you are reading through this very sentence, you are
    picking up the context of each word from the words that came before it. NLP models
    based on RNNs can build on this approach to achieve generative tasks, such as
    novel text creation, as well as predictive ones such as sentiment classification
    or machine translation.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）任务是RNN的主要应用领域之一：如果你正在阅读这一句，你就是通过前面出现的词来理解每个词的上下文。基于RNN的NLP模型可以利用这种方式实现生成任务，如新文本创作，也可以完成预测任务，如情感分类或机器翻译。'
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Text generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: Sentiment classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分类
- en: Time series – stock price prediction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列 – 股票价格预测
- en: Open-domain question answering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放领域问答
- en: 'The first topic we''ll tackle is text generation: it demonstrates quite easily
    how we can use an RNN to generate novel content, and can therefore serve as a
    gentle introduction to RNNs.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要处理的第一个主题是文本生成：它很容易演示我们如何使用RNN生成新内容，因此可以作为一个温和的RNN入门。
- en: Text generation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成
- en: One of the best-known applications used to demonstrate the strength of RNNs
    is generating novel text (we will return to this application later, in the chapter
    on Transformer architectures).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 演示RNN强大功能的最著名应用之一是生成新文本（我们将在稍后的Transformer架构章节中回到这个应用）。
- en: 'In this recipe, we will use a **Long Short-Term Memory** (**LSTM**) architecture—a
    popular variant of RNNs—to build a text generation model. The name LSTM comes
    from the motivation for their development: "vanilla" RNNs struggled with long
    dependencies (known as the vanishing gradient problem) and the architectural solution
    of LSTM solved that. LSTM models achieve that by maintaining a cell state, as
    well as a "carry" to ensure that the signal (in the form of a gradient) is not
    lost as the sequence is processed. At each time step, the LSTM model considers
    the current word, the carry, and the cell state jointly.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将使用**长短期记忆**（**LSTM**）架构——一种流行的RNN变体——来构建文本生成模型。LSTM这个名字来源于它们开发的动机：传统的RNN在处理长依赖关系时会遇到困难（即所谓的梯度消失问题），而LSTM通过其架构解决了这一问题。LSTM模型通过维持一个单元状态和一个“携带”信息来确保信号（以梯度的形式）在序列处理时不会丢失。在每个时间步，LSTM模型会联合考虑当前的单词、携带的信息和单元状态。
- en: The topic itself is not that trivial, but for practical purposes, full comprehension
    of the structural design is not essential. It suffices to keep in mind that an
    LSTM cell allows past information to be reinjected at a later point in time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题本身并不简单，但出于实际应用考虑，完全理解结构设计并非必需。只需记住，LSTM单元允许将过去的信息在稍后的时间点重新注入。
- en: We will train our model on the NYT comment and headlines dataset ([https://www.kaggle.com/aashita/nyt-comments](https://www.kaggle.com/aashita/nyt-comments))
    and will use it to generate new headlines. We chose this dataset for its moderate
    size (the recipe should be reproducible without access to a powerful workstation)
    and availability (Kaggle is freely accessible, unlike some data sources accessible
    only via paywall).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用纽约时报评论和标题数据集（[https://www.kaggle.com/aashita/nyt-comments](https://www.kaggle.com/aashita/nyt-comments)）来训练我们的模型，并利用它生成新的标题。我们选择这个数据集是因为它的中等规模（该方法不需要强大的工作站即可复现）和易获取性（Kaggle是免费开放的，而一些数据源只能通过付费墙访问）。
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: As usual, first we import the necessary packages.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，首先我们导入必要的包。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We want to make sure our results are reproducible – due to the nature of the
    interdependencies within the Python deep learning universe, we need to initialize
    multiple random mechanisms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望确保我们的结果是可复现的——由于Python深度学习生态系统内的相互依赖关系，我们需要初始化多个随机机制。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step involves importing the necessary functionality from Keras itself:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从 Keras 本身导入必要的功能：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, it is typically convenient—if not always in line with what purists
    deem best practice—to customize the level of warnings displayed in the execution
    of our code. It is mainly to deal with ubiquitous warnings around assigning value
    to a subset of a DataFrame: clean demonstration is more important in the current
    context than sticking to the coding standards expected in a production environment:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通常来说，定制化代码执行过程中显示的警告级别是方便的——尽管这未必总是符合纯粹主义者的最佳实践标准——这主要是为了处理围绕给 DataFrame
    子集分配值的普遍警告：在当前环境下，清晰展示代码比遵守生产环境中的编码标准更为重要：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We shall define some functions that will streamline the code later on. First,
    let''s clean the text:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一些函数，以便后续简化代码。首先，让我们清理文本：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s use a wrapper around the built-in TensorFlow tokenizer as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个封装器，围绕内置的 TensorFlow 分词器，操作如下：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A frequently useful step is to wrap up a model-building step inside a function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的步骤是将模型构建步骤封装到函数中：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is some boilerplate for padding the sequences (the utility of
    this will become clearer in the course of the recipe):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对序列进行填充的一些模板代码（在实际应用中，这部分的作用会变得更加清晰）：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we create a function that will be used to generate text from our fitted
    model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个函数，用来从已拟合的模型生成文本：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The next step is to load our dataset (the `break` clause serves as a fast way
    to only pick up articles and not comment datasets):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载我们的数据集（`break` 子句作为快速方法，只选择文章数据集，而不包括评论数据集）：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can inspect the first few elements as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式检查前几个元素：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As is usually the case with real-life text data, we need to clean the input
    text. For simplicity, we perform only the basic preprocessing: punctuation removal
    and conversion of all words to lowercase:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如现实中大多数文本数据的情况一样，我们需要清理输入文本。为了简单起见，我们仅进行基础的预处理：去除标点符号并将所有单词转换为小写：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is what the top 10 rows look like after the cleaning operation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 清理操作后，前 10 行的数据如下所示：
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next step is tokenization. Language models require input data in the form
    of sequences—given a sequence of words (tokens), the generation task boils down
    to predicting the next most likely token in the context. We can utilize the built-in
    tokenizer from the `preprocessing` module of Keras.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是分词。语言模型要求输入数据以序列的形式呈现——给定一个由单词（词元）组成的序列，生成任务的关键在于预测上下文中下一个最可能的词元。我们可以利用
    Keras 的 `preprocessing` 模块中内置的分词器。
- en: 'After cleaning up, we tokenize the input text: this is a process of extracting
    individual tokens (words or terms) from a corpus. We utilize the built-in tokenizer
    to retrieve the tokens and their respective indices. Each document is converted
    into a series of tokens:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后，我们对输入文本进行分词：这是从语料库中提取单独词元（单词或术语）的过程。我们利用内置的分词器来提取词元及其相应的索引。每个文档都会转化为一系列词元：
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The vectors like `[1,708]`, `[1,708, 251]` represent the n-grams generated from
    the input data, where an integer is an index of the token in the overall vocabulary
    generated from the corpus.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 像 `[1,708]`、`[1,708, 251]` 这样的向量表示从输入数据生成的 n-grams，其中整数是该词元在从语料库生成的总体词汇表中的索引。
- en: 'We have transformed our dataset into a format of sequences of tokens—possibly
    of different lengths. There are two choices: go with RaggedTensors (which are
    a slightly more advanced topic in terms of usage) or equalize the lengths to adhere
    to the standard requirement of most RNN models. For the sake of simplicity of
    presentation, we proceed with the latter solution: padding sequences shorter than
    the threshold using the `pad_sequence` function. This step is easily combined
    with formatting the data into predictors and labels:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将数据集转化为由词元序列组成的格式——这些序列的长度可能不同。有两种选择：使用 RaggedTensors（这在用法上稍微复杂一些）或者将序列长度统一，以符合大多数
    RNN 模型的标准要求。为了简化展示，我们选择后者方案：使用 `pad_sequence` 函数填充短于阈值的序列。这一步与将数据格式化为预测值和标签的步骤容易结合：
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We utilize a simple LSTM architecture using the Sequential API:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用简单的 LSTM 架构，使用 Sequential API：
- en: 'Input layer: takes the tokenized sequence'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入层：接收分词后的序列
- en: 'LSTM layer: generates the output using LSTM units – we take 100 as a default
    value for the sake of demonstration, but the parameter (along with several others)
    is customizable'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 层：使用 LSTM 单元生成输出——为了演示，我们默认取 100 作为值，但此参数（以及其他多个参数）是可定制的
- en: 'Dropout layer: we regularize the LSTM output to reduce the risk of overfitting'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dropout 层：我们对 LSTM 输出进行正则化，以减少过拟合的风险
- en: 'Output layer: generates the most likely output token:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层：生成最可能的输出标记：
- en: '[PRE15]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now train our model using the standard Keras syntax:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用标准的 Keras 语法训练我们的模型：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have a fitted model, we can examine its performance: how good are
    the headlines generated by our LSTM based on a seed text? We achieve this by tokenizing
    the seed text, padding the sequence, and passing it into the model to obtain our
    predictions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个拟合的模型，我们可以检查其性能：基于种子文本，我们的 LSTM 生成的标题有多好？我们通过对种子文本进行分词、填充序列，并将其传入模型来获得预测结果：
- en: '[PRE17]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, even with a relatively simple setup (a moderately sized dataset
    and a vanilla model), we can generate text that looks somewhat realistic. Further
    fine-tuning would of course allow for more sophisticated content, which is a topic
    we will cover in *Chapter 10*, *Transformers*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，即使使用相对简单的设置（一个中等大小的数据集和一个基础模型），我们也能生成看起来相当真实的文本。当然，进一步的微调将允许生成更复杂的内容，这将是我们在*第10章*中讨论的内容，*Transformers*。
- en: See also
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'There are multiple excellent resources online for learning about RNNs:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有多个优秀的资源可以学习 RNN：
- en: 'For an excellent introduction – with great examples – see the post by Andrej
    Karpathy: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解一个很好的介绍——并且有很棒的例子——请参阅 Andrej Karpathy 的文章：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- en: A curated list of resources (tutorials, repositories) can be found at [https://github.com/kjw0612/awesome-rnn](https://github.com/kjw0612/awesome-rnn)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在 [https://github.com/kjw0612/awesome-rnn](https://github.com/kjw0612/awesome-rnn)
    找到一个精选资源列表（教程、代码库）
- en: Another great introduction can be found at [https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf](https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个很好的介绍可以在 [https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf](https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf)
    找到
- en: Sentiment classification
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分类
- en: 'A popular task in NLP is sentiment classification: based on the content of
    a text snippet, identify the sentiment expressed therein. Practical applications
    include analysis of reviews, survey responses, social media comments, or healthcare
    materials.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）中的一个常见任务是情感分类：根据文本片段的内容，识别其中表达的情感。实际应用包括评论分析、调查回复、社交媒体评论或健康护理材料。
- en: 'We will train our network on the Sentiment140 dataset introduced in [https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf),
    which contains 1.6 million tweets annotated with three classes: negative, neutral,
    and positive. In order to avoid issues with locale, we standardize the encoding
    (this part is best done from the console level and not inside the notebook). The
    logic is the following: the original dataset contains raw text that—by its very
    nature—can contain non-standard characters (such as emojis, which are obviously
    common in social media communication). We want to convert the text to UTF8—the
    de facto standard for NLP in English. The fastest way to do it is by using a Linux
    command-line functionality:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf)
    中介绍的 Sentiment140 数据集上训练我们的网络，该数据集包含 160 万条带有三类标注的推文：负面、中立和正面。为了避免本地化问题，我们对编码进行标准化（最好从控制台级别进行，而不是在笔记本内进行）。逻辑如下：原始数据集包含原始文本，这些文本——由于其固有性质——可能包含非标准字符（例如，表情符号，这在社交媒体通信中显然很常见）。我们希望将文本转换为
    UTF8——这是英语 NLP 的事实标准。最快的方法是使用 Linux 命令行功能：
- en: '`Iconv` is a standard tool for conversion between encodings'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Iconv` 是一个标准的工具，用于在编码之间进行转换'
- en: The `-f` and `-t` flags denote the input encoding and the target one, respectively
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-f` 和 `-t` 标志分别表示输入编码和目标编码'
- en: '`-o` specifies the output file:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-o` 指定输出文件：'
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We begin by importing the necessary packages as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先按如下方式导入必要的包：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we define the hyperparameters of our model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型的超参数：
- en: 'The embedding dimension is the size of word embedding we will use. In this
    recipe, we will use GloVe: an unsupervised learning algorithm trained on aggregated
    word co-occurrence statistics from a combined corpus of Wikipedia and Gigaword.
    The resulting vectors for (English) words give us an efficient way of representing
    text and are commonly referred to as embeddings.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入维度是我们将使用的词嵌入的大小。在本食谱中，我们将使用 GloVe：一种无监督学习算法，基于维基百科和Gigaword的合并语料库的词共现统计信息进行训练。得到的（英文）单词向量为我们提供了有效的文本表示方式，通常称为嵌入。
- en: '`max_length` and `padding_type` are parameters specifying how we pad the sequences
    (see previous recipe).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`和`padding_type`是指定如何填充序列的参数（见前面的食谱）。'
- en: '`training_size` specifies the size of the target corpus.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training_size`指定了目标语料库的大小。'
- en: '`test_portion` defines the proportion of the data we will use as a holdout.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_portion`定义了我们将作为保留数据使用的数据比例。'
- en: '`dropout_val` and `nof_units` are hyperparameters for the model:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_val`和`nof_units`是模型的超参数：'
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s encapsulate the model creation step into a function. We define a fairly
    simple one for our classification task—an embedding layer, followed by regularization
    and convolution, pooling, and then the RNN layer:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将模型创建步骤封装成一个函数。我们为我们的分类任务定义了一个相当简单的模型——一个嵌入层，后接正则化、卷积、池化，再加上RNN层：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Collect the content of the corpus we will train on:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 收集我们将用于训练的语料库内容：
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Convert to sentence format:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为句子格式：
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Normalize the sentence lengths with padding (see previous section):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用填充规范化句子长度（见前一节）：
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Divide the dataset into training and holdout sets:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和保留集：
- en: '[PRE25]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A crucial step in using RNN-based models for NLP applications is the `embeddings`
    matrix:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于RNN的模型进行NLP应用时，一个关键步骤是`embeddings`矩阵：
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With all the preparations completed, we can set up the model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有准备工作完成后，我们可以设置模型：
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Training is performed in the usual way:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 训练按常规方式进行：
- en: '[PRE28]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can also assess the quality of our model visually:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过可视化来评估模型的质量：
- en: '[PRE29]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](img/B16254_09_01.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_09_01.png)'
- en: 'Figure 9.1: Training versus validation accuracy over epochs'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：训练准确率与验证准确率随训练轮次变化
- en: '![Obraz zawierający zrzut ekranu  Opis wygenerowany automatycznie](img/B16254_09_02.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Obraz zawierający zrzut ekranu  Opis wygenerowany automatycznie](img/B16254_09_02.png)'
- en: 'Figure 9.2: Training versus validation loss over epochs'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：训练损失与验证损失随训练轮次变化
- en: As we can see from both graphs, the model already achieves good performance
    after a limited number of epochs and it stabilizes after that, with only minor
    fluctuations. Potential improvements would involve early stopping, and extending
    the size of the dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从两个图表中看到的，模型在有限的训练轮次后已经取得了不错的表现，并且在此之后稳定下来，只有少量波动。潜在的改进包括早停法，并扩大数据集的规模。
- en: See also
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Readers interested in the applications of RNNs to sentiment classification
    can investigate the following resources:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣了解RNN在情感分类中的应用的读者可以查阅以下资源：
- en: 'TensorFlow documentation tutorial: [https://www.tensorflow.org/tutorials/text/text_classification_rnn](https://www.tensorflow.org/tutorials/text/text_classification_rnn)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 文档教程：[https://www.tensorflow.org/tutorials/text/text_classification_rnn](https://www.tensorflow.org/tutorials/text/text_classification_rnn)
- en: '[https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49](https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49)
    is one of many articles demonstrating the application of RNNs to sentiment detection
    and it contains an extensive list of references'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49](https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49)
    是众多展示将RNN应用于情感检测的文章之一，其中包含了大量的参考文献。'
- en: GloVe documentation can be found at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe 文档可以在 [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    找到
- en: Stock price prediction
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 股票价格预测
- en: Sequential models such as RNNs are naturally well suited to time series prediction—and
    one of the most advertised applications is the prediction of financial quantities,
    especially prices of different financial instruments. In this recipe, we demonstrate
    how to apply LSTM to the problem of time series prediction. We will focus on the
    price of Bitcoin—the most popular cryptocurrency.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 像RNN这样的顺序模型非常适合时间序列预测——其中一个最为宣传的应用是金融数量的预测，尤其是不同金融工具的价格预测。在本食谱中，我们展示了如何将LSTM应用于时间序列预测问题。我们将重点关注比特币的价格——最受欢迎的加密货币。
- en: 'A disclaimer is in order: this is a demonstration example on a popular dataset.
    It is not intended as investment advice of any kind; building a reliable time
    series prediction model applicable in finance is a challenging endeavor, outside
    the scope of this book.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 需要说明的是：这是一个基于流行数据集的演示示例。它并不打算作为任何形式的投资建议；建立一个在金融领域适用的可靠时间序列预测模型是一项具有挑战性的工作，超出了本书的范围。
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We begin by importing the necessary packages:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的包：
- en: '[PRE30]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The general parameters for our task are the future horizon of our prediction
    and the hyperparameter for the network:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们任务的通用参数是预测的未来范围和网络的超参数：
- en: '[PRE31]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As before, we will encapsulate our model creation step in a function. It accepts
    a single parameter, `units`, which is the dimension of the inner cells in LSTM:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将把模型创建步骤封装到一个函数中。它接受一个参数`units`，该参数是LSTM内单元的维度：
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can now proceed to load the data, with the usual formatting of the timestamp.
    For the sake of our demonstration, we will predict the average daily price—hence
    the grouping operation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始加载数据，并采用常见的时间戳格式。为了演示的目的，我们将预测每日平均价格——因此需要进行分组操作：
- en: '[PRE33]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The next step is to split the data into training and test periods:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据分为训练期和测试期：
- en: '[PRE34]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Preprocessing could theoretically be avoided, but it tends to help convergence
    in practice:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上可以避免预处理，但在实践中它有助于加速收敛：
- en: '[PRE35]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Fitting the model is straightforward:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合模型非常简单：
- en: '[PRE36]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'With a fitted model we can generate a prediction over the forecast horizon,
    keeping in mind the need to invert our normalization so that the values are back
    on the original scale:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过拟合模型，我们可以在预测范围内生成一个预测，记得要反转我们的标准化处理，以便将值还原到原始尺度：
- en: '[PRE37]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This is what our forecasted results look like:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们预测结果的样子：
- en: '[PRE38]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/B16254_09_03.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_09_03.png)'
- en: 'Figure 9.3: Actual price and predicted price over time'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：实际价格和预测价格随时间变化
- en: 'Overall, it is clear that even a simple model can generate a reasonable prediction—with
    an important caveat: this approach only works as long as the environment is stationary,
    that is, the nature of the relationship between past and present values remains
    stable over time. Regime changes and sudden interventions might have a dramatic
    impact on the price, if for example a major jurisdiction were to restrict the
    usage of cryptocurrencies (as has been the case over the last decade). Such occurrences
    can be modeled, but they require more elaborate approaches to feature engineering
    and are outside the scope of this chapter.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，很明显，即使是一个简单的模型也能生成合理的预测——但有一个重要的警告：这种方法仅在环境保持稳定的情况下有效，即过去和现在的值之间的关系随着时间的推移保持稳定。如果出现体制变化或突发干预，价格可能会受到显著影响，例如，如果某个主要司法管辖区限制了加密货币的使用（正如过去十年所发生的情况）。这种情况可以建模，但需要更复杂的特征工程方法，并且超出了本章的范围。
- en: Open-domain question answering
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放领域问答
- en: '**Question-answering** (**QA**) systems aim to emulate the human process of
    searching for information online, with machine learning methods employed to improve
    the accuracy of the provided answers. In this recipe, we will demonstrate how
    to use RNNs to predict long and short responses to questions about Wikipedia articles.
    We will use the Google Natural Questions dataset, along with which an excellent
    visualization helpful for understanding the idea behind QA can be found at [https://ai.google.com/research/NaturalQuestions/visualization](https://ai.google.com/research/NaturalQuestions/visualization).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**问答**（**QA**）系统旨在模拟人类在网上搜索信息的过程，并通过机器学习方法提高提供答案的准确性。在这个示例中，我们将演示如何使用RNN预测关于维基百科文章的长短答案。我们将使用Google的自然问题数据集，并且在[https://ai.google.com/research/NaturalQuestions/visualization](https://ai.google.com/research/NaturalQuestions/visualization)上可以找到一个非常好的可视化工具，帮助理解QA背后的思想。'
- en: 'The basic idea can be summarized as follows: for each article-question pair,
    you must predict/select long- and short-form answers to the question drawn *directly
    from the article*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想可以总结如下：对于每一对文章-问题对，你必须预测/选择从*文章中直接提取*的长答案和短答案：
- en: A long answer would be a longer section of text that answers the question—several
    sentences or a paragraph.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长答案是指回答问题的较长一段文字——通常是几句话或一段话。
- en: A short answer might be a sentence or phrase, or even in some cases a simple
    YES/NO. The short answers are always contained within, or a subset of, one of
    the plausible long answers.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简短的回答可能是一个句子或短语，甚至在某些情况下是简单的YES/NO。简短的答案总是包含在或作为某个合理长答案的子集。
- en: A given article can (and very often will) allow for both long *and* short answers,
    depending on the question.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定的文章可以（并且通常会）允许长*和*短答案，具体取决于问题。
- en: 'The recipe presented in this chapter is adapted from code made public by Xing
    Han Lu: [https://www.kaggle.com/xhlulu](https://www.kaggle.com/xhlulu).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所展示的配方改编自Xing Han Lu公开的代码：[https://www.kaggle.com/xhlulu](https://www.kaggle.com/xhlulu)。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'As usual, we start by loading the necessary packages. This time we are using
    the fasttext embeddings for our representation (available from [https://fasttext.cc/](https://fasttext.cc/)).
    Other popular choices include GloVe (used in the sentiment detection section)
    and ELMo ([https://allennlp.org/elmo](https://allennlp.org/elmo)). There is no
    clearly superior one in terms of performance on NLP tasks, so we''ll switch our
    choices as we go to demonstrate the different possibilities:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如往常一样，我们首先加载必要的包。这次我们使用fasttext嵌入来表示（可以从[https://fasttext.cc/](https://fasttext.cc/)获取）。其他流行的选择包括GloVe（在情感分析部分使用）和ELMo（[https://allennlp.org/elmo](https://allennlp.org/elmo)）。在NLP任务的性能方面没有明显的优劣之分，所以我们会根据需要切换选择，以展示不同的可能性：
- en: '[PRE39]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The general settings are as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一般设置如下：
- en: '[PRE40]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our next step is to add some boilerplate code to streamline the code flow later.
    Since the task at hand is a little more involved than in the previous instances
    (or less intuitive), we wrap up more of the preparation work inside the dataset
    building functions. Due to the size of the dataset, we only load a subset of the
    training data and sample the negative-labeled data:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是添加一些样板代码，以便之后简化代码流。由于当前任务比之前的任务稍微复杂一些（或不太直观），我们将更多的准备工作封装在数据集构建函数中。由于数据集的大小，我们仅加载训练数据的一个子集，并从中采样带有负标签的数据：
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'With the next function, we train a Keras tokenizer to encode the text and questions
    into a list of integers (tokenization), then pad them to a fixed length to form
    a single NumPy array for text and another for questions:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下一个函数，我们训练一个Keras tokenizer，将文本和问题编码成整数列表（分词），然后将它们填充到固定长度，形成一个用于文本的单一NumPy数组，另一个用于问题：
- en: '[PRE42]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As usual with RNN-based models for NLP, we need an embeddings matrix:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于RNN的NLP模型一样，我们需要一个嵌入矩阵：
- en: '[PRE43]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next is our model construction step, wrapped up in a function:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的模型构建步骤，用一个函数包装起来：
- en: We build two 2-layer bidirectional LSTMs; one to read the questions, and one
    to read the text
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建了两个2层的双向LSTM，一个用于读取问题，另一个用于读取文本。
- en: We concatenate the output and pass it to a fully connected layer
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将输出连接起来，并将其传递到一个全连接层：
- en: 'We use sigmoid on the output:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在输出上使用sigmoid：
- en: '[PRE44]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'With the toolkit that we''ve defined, we can construct the datasets as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们定义的工具包，我们可以构建数据集，具体如下：
- en: '[PRE45]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This is what the dataset looks like:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据集的样子：
- en: '[PRE46]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_09_04.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_09_04.png)'
- en: '[PRE47]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now construct the model itself:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建模型本身：
- en: '[PRE48]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The fitting is next, and that proceeds in the usual manner:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是拟合，按照通常的方式进行：
- en: '[PRE49]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we can build a test set to have a look at our generated answers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建一个测试集，来查看我们生成的答案：
- en: '[PRE50]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We generate the actual predictions:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成实际的预测：
- en: '[PRE51]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_09_05.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_09_05.png)'
- en: As you can see, LSTM allows us to handle fairly abstract tasks such as answering
    different types of questions. The bulk of the work in this recipe was around formatting
    the data into a suitable input format, and then postprocessing the results—the
    actual modeling occurs in a very similar fashion to that in the preceding chapters.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，LSTM使我们能够处理一些相当抽象的任务，比如回答不同类型的问题。这个配方的核心工作是在数据格式化为合适的输入格式，并在之后对结果进行后处理——实际的建模过程与前几章中的方法非常相似。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have demonstrated the different capabilities of RNNs. They
    can handle diverse tasks with a sequential component (text generation and classification,
    time series prediction, and QA) within a unified framework. In the next chapter,
    we shall introduce transformers: an important architecture class that made it
    possible to reach new state-of-the-art results with NLP problems.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了RNN的不同功能。它们能够在一个统一的框架内处理多种具有顺序性任务（文本生成与分类、时间序列预测以及问答）。在下一章，我们将介绍transformer：这一重要的架构类别使得在自然语言处理问题上取得了新的最先进成果。
