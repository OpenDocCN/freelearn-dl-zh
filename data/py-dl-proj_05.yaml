- en: Sequence-to-Sequence Models for Building Chatbots
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于构建聊天机器人的序列到序列模型
- en: We're learning a lot and doing some valuable work! In the evolution of our hypothetical
    business use case, this chapter builds directly on [Chapter 4](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml),
    *Building an NLP Pipeline for Building Chatbots*, where we created our **Natural
    Language Processing** (**NLP**) pipeline. The skills we learned so far in computational
    linguistics should give us the confidence to expand past the training examples
    in this book and tackle this next project. We're going to build a more advanced
    chatbot for our hypothetical restaurant chain to automate the process of fielding
    call-in orders.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到了很多东西，并且做了一些有价值的工作！在我们假设的商业用例的演进过程中，本章直接建立在 [第4章](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml)
    *构建聊天机器人 NLP 管道* 的基础上，我们在该章中创建了 **自然语言处理**（**NLP**）管道。我们到目前为止在计算语言学中学到的技能应该能够让我们有信心扩展到本书中的训练示例之外，并着手处理下一个项目。我们将为我们假设的餐饮连锁店构建一个更先进的聊天机器人，自动化接听电话订单的过程。
- en: This requirement would mean that we'd have to combine a number of technologies
    that we've learned so far. But for this project, we'll be interested in learning
    how to make a chatbot that is more contextually aware and robust, so that we could
    integrate it into a larger system in this hypothetical. By demonstrating mastery
    on this training example, we'll have the confidence to execute this in a real
    situation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个需求意味着我们需要结合我们迄今为止所学的一些技术。但在这个项目中，我们将学习如何制作一个更加上下文敏感且强大的聊天机器人，以便将其集成到这个假设的更大系统中。通过在这个训练示例中展示我们的掌握程度，我们将有信心在实际情况下执行这个任务。
- en: 'In the previous chapters, we learned about representational learning methods,
    such as word2vec, and how to use them in combination with a type of deep learning
    algorithm called a **Convolutional Neural Network** (**CNN**). But there are few
    constraints while using CNNs to build language models, such as the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了表示学习方法，如 word2vec，并了解了如何将其与一种名为 **卷积神经网络**（**CNN**）的深度学习算法结合使用。但是，使用
    CNN 构建语言模型时存在一些限制，如下所示：
- en: The model will not be able to preserve the state information
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型将无法保持状态信息
- en: The length of sentences needs to be of a fixed size for both input values and
    output values
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子的长度需要在输入值和输出值之间保持固定大小
- en: CNNs are sometimes unable to adequately handle complex sequential contexts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 有时无法充分处理复杂的序列上下文
- en: '**Recurrent Neural Networks** (**RNNs**) do better at modeling information
    in sequence'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）在建模序列信息方面表现更好'
- en: So, to overcome all of these problems, we have an alternative algorithm, which
    is specially designed to handle input data that comes in the form of sequences
    (including sequences of words, or of characters). This class of algorithm is called
    RNN.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了解决这些问题，我们有一个替代算法，它专门设计来处理以序列形式输入的数据（包括单词序列或字符序列）。这种类型的算法被称为 RNN。
- en: 'In this chapter, we will do the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将做以下事情：
- en: Learn about RNN and its various forms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 RNN 及其各种形式
- en: Create a language model implementation using RNN
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RNN 创建一个语言模型实现
- en: Build our intuition on the **Long Short-Term Memory** (**LSTM**) model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 **长短期记忆**（**LSTM**）模型构建我们的直觉
- en: Create an LSTM language model implementation and compare it to the RNN model
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 LSTM 语言模型实现并与 RNN 模型进行比较
- en: Implement an encoder-decoder RNN, based on the LSTM unit, for a simple sequence
    of question-answer tasks
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 LSTM 单元实现一个编码器-解码器 RNN，用于简单的问答任务序列
- en: '**Define the goal**: Build a more robust chatbot with memory to provide more
    contextually correct responses to questions.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义目标**：构建一个具有记忆功能的更强大的聊天机器人，以提供更具上下文相关性的正确回答。'
- en: Let's get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Introducing RNNs
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 RNN
- en: RNN is a deep learning model architecture specifically designed for sequential
    data. The purpose of this type of model is to extract relevant features of words
    and characters of text by using a small window that traverses the corpus.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一种深度学习模型架构，专门为序列数据设计。该模型的目的是通过使用一个小窗口来遍历语料库，从而提取文本中单词和字符的相关特征。
- en: RNN applies a non-linear function to each item in the sequence. This is called
    the RNN *ce**ll* or s*tep* and, in our case, the items are words or characters
    in the sequence. The layer's output in an RNN is derived from the output of the
    RNN cell, which is applied to each element in the sequence. With regard to NLP
    and chatbots that use text data as input, the outputs of the model are successive
    characters or words.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RNN对序列中的每个项应用非线性函数。这被称为RNN *单元*或*步*，在我们的例子中，这些项是序列中的单词或字符。RNN中的输出是通过对序列中每个元素应用RNN单元的输出得到的。关于使用文本数据作为输入的自然语言处理和聊天机器人，模型的输出是连续的字符或单词。
- en: Each RNN cell holds an internal memory that summarizes the history of the sequence
    it has seen so far.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个RNN单元都包含一个内部记忆，用于总结它目前为止看到的序列的历史。
- en: 'This diagram helps us to visualize the RNN model architecture:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该图帮助我们可视化RNN模型架构：
- en: '![](img/9712ff5e-a7f3-4dc4-a097-d20b506f745b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9712ff5e-a7f3-4dc4-a097-d20b506f745b.png)'
- en: Vanilla version of RNN model architecture.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型架构的经典版本。
- en: At the heart of the purpose of an RNN was the idea to introduce a feedback mechanism
    that enables context modeling through the use of fixed-weight feedback structures.
    What this does is build a connection between the features in the current mapping
    to the previous version. Basically, it employs a strategy of using an earlier
    version of a sequence to instruct a later version.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的核心目的在于引入一种反馈机制，通过使用固定权重的反馈结构来实现上下文建模。这样做的目的是建立当前映射到先前版本之间的连接。基本上，它使用序列的早期版本来指导后续版本。
- en: This is quite clever; however, it's not without its challenges. Exploding and
    vanishing gradients make it extremely frustrating to train these types of modes
    in instances where the problem is of a complex time series nature.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常聪明；然而，它也并非没有挑战。梯度爆炸和梯度消失使得在处理复杂时间序列问题时，训练这些类型的模型变得极其令人沮丧。
- en: A great reference to dive into that expertly outlines the vanishing and exploding
    gradient problem, and gives a technical explanation of viable solutions, can be
    found in Sepp's work from 1998 ([https://dl.acm.org/citation.cfm?id=355233](https://dl.acm.org/citation.cfm?id=355233)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个很好的参考资料深入讲解了梯度消失和梯度爆炸问题，并提供了可行解决方案的技术解释，可以参考Sepp 1998年的工作（[https://dl.acm.org/citation.cfm?id=355233](https://dl.acm.org/citation.cfm?id=355233)）。
- en: 'A second problem that was discovered was that RNNs were picking up only one
    of two temporal structures: either the short-term or long-term structures. But
    what was needed for the best model performance was a model that was able to learn
    from both types of features (short-term and long-term) at the same time. The solution
    came in changing the basic RNN cell for a **Gated Recurrent Unite** (**GRU**)
    or LSTM cell.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个发现的问题是，RNN只会捕捉到两种时间结构中的一种：短期结构或长期结构。然而，最佳模型的性能需要能够同时从两种类型的特征（短期和长期）中学习。解决方案是将基础的RNN单元更换为**门控递归单元**（**GRU**）或LSTM单元。
- en: For additional information on the GRU refer to [http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/) or,
    to learn more on the LSTM, refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于GRU的信息，请参考[http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)，或者，若要了解更多关于LSTM的内容，请参考[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: We'll explore the LSTM architecture in detail later in this chapter. Let's gain
    some intuition on the value of LSTM that will help us achieve our goal first.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后续部分详细探讨LSTM架构。让我们先直观地了解LSTM的价值，这将有助于我们实现目标。
- en: RNN architectures
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN架构
- en: We will mostly use the LSTM cell, since it has proven better in most NLP tasks.
    The principle benefit of the LSTM in RNN architectures is that it enables model
    training over long sequences, while retaining memory. To solve the gradient problem,
    LSTMs include more gates that effectively control access to the cell state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要使用LSTM单元，因为它在大多数自然语言处理任务中表现更好。LSTM在RNN架构中的主要优点是，它能够在保持记忆的同时进行长序列的模型训练。为了解决梯度问题，LSTM包括更多的门控机制，有效控制对单元状态的访问。
- en: We've found that Colah's blog post ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))
    is a great place to go to obtain a good understand the working of LSTMs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现Colah的博客文章([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))是一个很好的地方，可以帮助理解LSTM的工作原理。
- en: 'These small LSTM units of RNN can be combined in multiple forms to solve various
    kinds of use-cases. RNNs are quite flexible in terms of combining the different
    input and output patterns, as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些RNN的小型LSTM单元可以以多种形式组合来解决各种类型的使用案例。RNN在结合不同输入和输出模式方面非常灵活，具体如下：
- en: '**Many to one**: The model takes a complete input sequence to make a single
    prediction. This is used in sentiment models.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：该模型将完整的输入序列作为输入，做出单一的预测。这在情感分析模型中使用。'
- en: '**One to many**: This model transforms a single input, such as a numerical
    date, to generate a sequence string such as "day", "month", or "year".'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：该模型将单一的输入（例如一个日期）转化为生成一个序列字符串，如“日”、“月”或“年”。'
- en: '**Many to many**: This is a **sequence-to-sequence** (**seq2seq**) model, which takes
    the entire sequence as input into a second sequence form, as Q/A systems do.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多**：这是一个**序列到序列**（**seq2seq**）模型，它将整个序列作为输入，转换为第二个序列的形式，正如问答系统所做的那样。'
- en: 'This diagram maps out these relationships nicely:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图很好地展示了这些关系：
- en: '![](img/cbaaa3d4-d06a-4f7c-b719-f2a5a35fac9f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbaaa3d4-d06a-4f7c-b719-f2a5a35fac9f.png)'
- en: 'In this chapter, we will focus on the **many to many** relationship, also known
    as seq2seq architecture, to build a question-answer chatbot. The standard RNN
    approach to solving the seq2seq problem involves three primary components:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注**多对多**关系，也称为seq2seq架构，以构建一个问答聊天机器人。解决seq2seq问题的标准RNN方法包括三个主要组成部分：
- en: '**Encoders**: These transform the input sentences into some abstract encoded
    representation'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：这些将输入句子转换为某种抽象的编码表示'
- en: '**Hidden layer**: Encoded sentence transformation representations are manipulated
    here'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：在这里处理编码后的句子转换表示'
- en: '**Decoders**: These output a decoded target sequence'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：这些输出解码后的目标序列'
- en: 'Let''s have a look at the following diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看以下图表：
- en: '![](img/a7479ffd-d841-4e5f-bcce-24455fbaa7c2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7479ffd-d841-4e5f-bcce-24455fbaa7c2.png)'
- en: The illustration of building the encode decoder model which takes input text
    (question) in the encoder, it gets transformed in the intermediate step and gets
    mapped with the decoder which represents the respective text (answer).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构建编码解码模型的示意图，该模型将输入文本（问题）传递到编码器，在中间步骤中进行转换，然后与解码器进行映射，解码器表示相应的文本（答案）。
- en: Let's build our intuition on RNNs by first implementing basic forms of RNN models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过首先实现RNN模型的基本形式，来建立对RNN的直觉。
- en: Implementing basic RNNs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现基本的RNN
- en: In this section, we will implement a language model, using a basic RNN to perform
    sentiment classification. Code files for the model can found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个语言模型，使用基础的RNN进行情感分类。模型的代码文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py)找到。
- en: Importing all of the dependencies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所有依赖项
- en: 'This code imports TensorFlow and key dependencies for our RNN:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码导入了TensorFlow和RNN的关键依赖项：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preparing the dataset
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'The dataset we''ll use in this project is the *Movie Review Data* from Rotten
    Tomatoes ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)).
    It contains 10,662 example review sentences, with approximately half of them positive
    and half negative. The dataset has a vocabulary of around 20,000 words. We will
    use the `sklearn` wrapper to load the dataset from a raw file and then a `separate_dataset()` helper
    functionto clean the dataset and transform it from its raw form to the separate
    list structure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们将使用来自Rotten Tomatoes的*电影评论数据*([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/))。该数据集包含10,662个示例评论句子，约一半是正面评价，一半是负面评价。数据集的词汇量约为20,000个单词。我们将使用`sklearn`包装器从原始文件加载数据集，然后使用`separate_dataset()`辅助函数清理数据集，并将其从原始形式转换为分离的列表结构：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, `trainset` is an object that stores all of the text data and the sentiment
    label data:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`trainset`是一个存储所有文本数据和情感标签数据的对象：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we will transform the labels into the one-hot encoding.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将标签转化为一热编码。
- en: It's important to understand the dimensions of the one-hot encoding vector.
    Since we have `10662` separate sentences, and two sentiments, `negative` and `positive`,
    our one-hot vector size will be of a size of [*10662, 2*].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 理解一热编码向量的维度很重要。由于我们有`10662`个独立的句子，且有两个情感类别，`negative`和`positive`，因此我们的“一热”向量的大小将是[*10662,
    2*]。
- en: 'We will be using a popular `train_test_split()` sklearn wrapper to randomly
    shuffle the data and divide the dataset into two parts: the `training` set and
    the `test` set. Further, with another `build_dataset()` helper function, we will
    create the vocabulary using a word-count-based approach:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个流行的`train_test_split()` sklearn包装器来随机打乱数据，并将数据集分为两个部分：`training`集和`test`集。进一步地，借助另一个`build_dataset()`辅助函数，我们将使用基于词频的方式创建词汇表：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can also try to feed any embedding model in here to make the model more
    accurate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试将任何嵌入模型放入这里，以提高模型的准确性。
- en: 'There are a few important things to remember while preparing the dataset for
    the RNN models. We need to add explicitly special tags in the vocabulary to keep
    track of the start of sentences, extra padding, the ends of sentences, and any
    unknown words. Hence, we have reserved the following positions for special tags
    in our vocabulary dictionary:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在为RNN模型准备数据集时，有一些重要的事项需要记住。我们需要在词汇表中明确添加特殊标签，以跟踪句子的开始、额外的填充、句子的结束以及任何未知的词汇。因此，我们在词汇字典中为特殊标签保留了以下位置：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Hyperparameters
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: 'We will define some of the hyperparameters for our model, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为模型定义一些超参数，如下所示：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Defining a basic RNN cell model
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个基本的RNN单元模型
- en: 'Now we will create the RNN model, which takes a few input parameters, including
    the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建RNN模型，它需要几个输入参数，包括以下内容：
- en: '`size_layer`: The number of units in the RNN cell'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size_layer`：RNN单元中的单元数'
- en: '`num_layers`: The number of hidden layers'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers`：隐藏层的数量'
- en: '`embedded_size`: The size of the embedding'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedded_size`：嵌入的大小'
- en: '`dict_size`: The vocabulary size'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dict_size`：词汇表大小'
- en: '`dimension_output`: The number of classes we need to classify'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dimension_output`：我们需要分类的类别数'
- en: '`learning_rate`: The learning rate of the optimization algorithm'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：优化算法的学习率'
- en: 'The architecture of our RNN model consists of the following parts:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RNN模型架构由以下部分组成：
- en: Two placeholders; one to feed sequence data into the model and the second for
    the output
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个占位符；一个用于将序列数据输入模型，另一个用于输出
- en: A variable to store the embedding lookup from the dictionary
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于存储从词典中查找嵌入的变量
- en: Then, add the RNN layer with multiple basic RNN cells
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，添加包含多个基本RNN单元的RNN层
- en: Create weight and bias variables
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建权重和偏差变量
- en: Compute `logits`
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`logits`
- en: Compute loss
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失
- en: Add the Adam Optimizer
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加Adam优化器
- en: Calculate prediction and accuracy
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测和准确率
- en: 'This model is similar to the CNN model created in the previous chapter, [Chapter
    4](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml), *Building an NLP Pipeline for
    Building Chatbots*, except for the RNN cell part:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型类似于前一章中创建的CNN模型，[第4章](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml)，*构建自然语言处理管道以创建聊天机器人*，除了RNN单元部分：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this model, the data flows from the variables that we created in *Step 1*.
    Then, it moves to the embedding layer defined in *Step 2*, followed by our RNN
    layer, which performs the computation in two hidden layers of RNN cells. Later, `logits`
    are computed by performing a matrix multiplication of the weight, the output from
    the RNN layer, and addition of bias. The last step is that we define the `cost`
    function; we will be using the `softmax_cross_entropy` function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，数据从我们在*步骤1*中创建的变量流动。接着，它进入在*步骤2*中定义的嵌入层，然后是我们的RNN层，它在两个隐藏层的RNN单元中执行计算。之后，`logits`通过进行权重与RNN层输出的矩阵乘法并加上偏差来计算。最后一步是我们定义`cost`函数；我们将使用`softmax_cross_entropy`函数。
- en: 'This is what the complete model looks like after computation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是计算后完整模型的样子：
- en: '![](img/2c8c39ff-aa88-4f69-8940-f677365ff683.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c8c39ff-aa88-4f69-8940-f677365ff683.png)'
- en: TensorBoard graph visualization of the RNN architecture
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard图形可视化RNN架构
- en: 'The following diagram represents the structure of the RNN block from the preceding
    screenshot. In this architecture, we have two RNN cells incorporated in hidden
    layers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下图表示前面截图中的RNN块结构。在这个架构中，我们有两个RNN单元被集成在隐藏层中：
- en: '![](img/7bc55c85-35ba-45fb-a70a-1b9b1b354159.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bc55c85-35ba-45fb-a70a-1b9b1b354159.png)'
- en: TensorBoard visualization of the RNN block containing 2 hidden layers as defined
    in the code
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard可视化RNN块，其中包含代码中定义的2个隐藏层
- en: Training the RNN Model
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RNN模型
- en: 'Now that we have our model architecture defined, let''s train our model. We
    begin with a TensorFlow graph initialization and execute the training steps as
    follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型架构，接下来让我们训练模型。我们从TensorFlow图初始化开始，并按以下步骤执行训练：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'While the RNN model is being trained, we can see the logs of each epoch, shown
    as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练RNN模型时，我们可以看到每个epoch的日志，如下所示：
- en: '![](img/346eb0d5-77eb-4d99-8237-384f178a9ebb.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/346eb0d5-77eb-4d99-8237-384f178a9ebb.png)'
- en: Evaluation of the RNN model
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN模型评估
- en: 'Let''s look at our results. Once the model is trained, we can feed the test
    data that we prepared earlier in this chapter and evaluate the predictions. In
    this case, we will use a few different metrics to evaluate our model: precision,
    recall, and F1-scores.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看结果。一旦模型训练完成，我们就可以输入本章前面准备好的测试数据并评估预测结果。在这种情况下，我们将使用几种不同的指标来评估模型：精度、召回率和F1分数。
- en: To evaluate your model, it is important to choose the right kind of metrics—F1-scores
    are considered more practical compared to the accuracy score.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估您的模型，选择合适的指标非常重要——与准确率分数相比，F1分数被认为更为实用。
- en: 'Some key points to help you understand them in simple terms are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些帮助你简单理解这些概念的关键点：
- en: '**Accuracy**: The count of correct predictions, divided by the count of total
    examples that have been evaluated.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：正确预测的数量除以已评估的总样本数。'
- en: '**Precision**: High precision means you identified nearly all positives appropriately;
    a low precision score means you often incorrectly predicted a positive when there
    was none.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度**：高精度意味着你正确识别了几乎所有的正例；低精度意味着你经常错误地预测出一个正例，而实际上并没有。'
- en: '**Recall**: High recall means you correctly predicted almost all of the real
    positives present in the data; a low score means you frequently missed positives
    that were present.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：高召回率意味着你正确预测了数据中几乎所有的真实正例；低召回率意味着你经常漏掉实际存在的正例。'
- en: '**F1-score**: The balanced harmonic mean of recall and precision, giving both
    metrics equal weight. The higher the F-measure, the better.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**：召回率和精度的平衡调和均值，给予这两个指标相等的权重。F1分数越高，表现越好。'
- en: 'Now we will execute the model by feeding the test data with vocabulary and
    the max length of the text. This will produce the `logits` values which we will
    use to generate the evaluation metrics:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将通过提供包含词汇表和文本最大长度的测试数据来执行模型。这将生成`logits`值，我们将利用这些值来生成评估指标：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/80fe4448-9a2c-4123-8abc-fbedffae80d0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80fe4448-9a2c-4123-8abc-fbedffae80d0.png)'
- en: So here, we can see that our average `f1-score` is 66% while using basic RNN
    cells. Let's see if this can be improved on by using other variations of RNN architectures.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到使用基本RNN单元时，我们的平均`f1-score`是66%。让我们看看是否通过使用其他RNN架构变体能有所改进。
- en: LSTM architecture
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM架构
- en: 'The desire to model sequential data more effectively, without the limitations
    of the gradient problem, led researchers to create the LSTM variant of the previous
    RNN model architecture. LSTM achieves better performance because it incorporates
    gates to control the process of memory in the cell. The following diagram shows
    an LSTM cell:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地建模序列数据，并且克服梯度问题的限制，研究人员创造了LSTM变体，这是在之前的RNN模型架构基础上发展出来的。LSTM由于引入了控制细胞内存过程的门控机制，因此能够实现更好的性能。以下图示展示了一个LSTM单元：
- en: '![](img/19f1c280-e0c8-475d-82e1-c2bd83f4cf7b.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19f1c280-e0c8-475d-82e1-c2bd83f4cf7b.png)'
- en: An LSTM unit (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元（来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs）
- en: 'LSTM consist of three primary elements, labeled as **1**, **2**, and **3**
    in the preceding diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM由三个主要部分组成，在上述图示中标记为**1**、**2**和**3**：
- en: '**The forget gate f(t)**: This gate provides the ability, in the LSTM cell
    architecture, to forget information that is not needed. The sigmoid activation
    accepts the inputs *X(t)* and **h(t-1)**, and effectively decides to remove pieces
    of old output information by passing a *0*. The output of this gate is *f(t)*c(t-1)*.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**遗忘门f(t)**：该门控机制在LSTM单元架构中提供了忘记不需要信息的能力。Sigmoid激活函数接受输入*X(t)*和**h(t-1)**，并有效地决定通过传递*0*来移除旧的输出信息。该门控的输出是*f(t)*c(t-1)*。'
- en: Information from the new input, *X(t),* that is determined to be retained needs
    to be stored in the next step in the cell state. A sigmoid activation is used
    in this process to update or ignore parts of the new information. Next, a vector
    of all possible values for the new input is created by a **tanh** activation function.
    The new cell state is the product of these two values, then this new memory is
    added to the old memory, **c(t-1)**, to give **c(t)**.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从新输入的 *X(t)* 中，需要保留的信息将在下一步中存储在单元状态中。此过程中使用了一个 sigmoid 激活函数来更新或忽略新信息的部分。接下来，通过
    **tanh** 激活函数创建一个包含新输入所有可能值的向量。新单元状态是这两个值的乘积，然后将这个新记忆添加到旧记忆 **c(t-1)** 中，得出 **c(t)**。
- en: The last process of the LSTM cell is to determine the final output. A sigmoid
    layer decides which parts of the cell state to output. We then put the cell state
    through a **tanh** activation to generate all of the possible values, and multiply
    it by the output of the sigmoid gate, to produce desired outputs according to
    a non-linear function.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 单元的最后一个过程是确定最终输出。一个 sigmoid 层决定输出单元状态的哪些部分。然后，我们将单元状态通过 **tanh** 激活生成所有可能的值，并将其与
    sigmoid 门的输出相乘，以根据非线性函数产生所需的输出。
- en: These three steps in the LSTM cell process produce a significant result, that
    being that the model can be trained to learn which information to retain in long-term
    memory and which information to forget. Genius!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元过程中的这三步产生了显著的效果，即模型可以被训练去学习哪些信息需要保存在长期记忆中，哪些信息需要被遗忘。真是天才！
- en: Implementing an LSTM model
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 LSTM 模型
- en: The process that we performed previously, to build the basic RNN model, will
    remain the same, except for the model definition part. So, let's implement this
    and check the performance of the new model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前执行的构建基本 RNN 模型的过程将保持不变，唯一的区别是模型定义部分。所以，让我们实现这个并检查新模型的性能。
- en: The code for the model can be viewed at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的代码可以在 [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py)
    查看。
- en: Defining our LSTM model
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义我们的 LSTM 模型
- en: 'Again, most of the code will remain same—the only the major change will be
    to use `tf.nn.rnn_cell.LSTMCell()`, instead of `tf.nn.rnn_cell.BasicRNNCell()`*.*
    While initializing the LSTM cell, we are using an orthogonal initializer that
    will generate a random orthogonal matrix, which is an effective way of combating
    exploding and vanishing gradients:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，大部分代码将保持不变——唯一的主要变化是使用 `tf.nn.rnn_cell.LSTMCell()`，而不是 `tf.nn.rnn_cell.BasicRNNCell()`。在初始化
    LSTM 单元时，我们使用了一个正交初始化器，它会生成一个随机的正交矩阵，这是对抗梯度爆炸和消失的有效方法：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So this is what the architecture of the LSTM model looks like—almost the same,
    compared to the previous basic model, except with the addition of the LSTM cells
    in the **RNN Block**:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是 LSTM 模型的架构——与之前的基本模型几乎相同，唯一的不同是增加了 LSTM 单元在**RNN 块**中的位置：
- en: '![](img/cd5802d3-ee3e-4755-928e-5d33950de711.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd5802d3-ee3e-4755-928e-5d33950de711.png)'
- en: Training the LSTM model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 LSTM 模型
- en: 'Now that we''ve established our LSTM intuition and built the model, let''s
    train it as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经建立了 LSTM 直觉并构建了模型，让我们按照以下方式训练它：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'While the LSTM model is being trained, we can see the logs of each epoch as
    shown in the following screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 模型训练时，我们可以看到每个 epoch 的日志，如下截图所示：
- en: '![](img/9893e58c-b1e5-4d51-8706-2977a0d6a6d7.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9893e58c-b1e5-4d51-8706-2977a0d6a6d7.png)'
- en: 'Following is the output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You will notice that, even after using the same configurations of the model,
    the training time required for the LSTM-based model will be greater than the RNN
    model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，即使使用相同的模型配置，基于 LSTM 的模型所需的训练时间仍然比 RNN 模型要长。
- en: Evaluation of the LSTM model
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 模型的评估
- en: 'Now, let''s again compute the metrics and compare the performance:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次计算指标并比较性能：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The computed outputs are shown as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的输出如下所示：
- en: '![](img/de3e11c1-3789-421c-9a7b-4f7ac7e52d61.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de3e11c1-3789-421c-9a7b-4f7ac7e52d61.png)'
- en: So, we can clearly see the boost in the performance of the model! Now, with
    the LSTM, the `f1-score` is bumped to 72% whereas, in our previous basic RNN model,
    it was 66%, which is quite a good improvement of 7%.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以清晰地看到模型性能的提升！现在，使用 LSTM 后，`f1-score` 提高到了 72%，而在我们之前的基本 RNN 模型中，它为 66%，这意味着提高了
    7%，是一个相当不错的进步。
- en: Sequence-to-sequence models
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到序列模型
- en: In this section, we'll implement a seq2seq model (an encoder-decoder RNN), based
    on the LSTM unit, for a simple sequence-to-sequence question-answer task. This model
    can be trained to map an input sequence (questions) to an output sequence (answers),
    which are not necessarily of the same length as each other.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个基于LSTM单元的seq2seq模型（编码器-解码器RNN），用于一个简单的序列到序列的问答任务。这个模型可以训练将输入序列（问题）映射到输出序列（答案），这些答案的长度不一定与问题相同。
- en: This type of seq2seq model has shown impressive performance in various other
    tasks such as speech recognition, machine translation, question answering, **Neural
    Machine Translation** (**NMT**), and image caption generation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的seq2seq模型在其他许多任务中表现出色，如语音识别、机器翻译、问答、**神经机器翻译**（**NMT**）和图像描述生成。
- en: 'The following diagram helps us visualize our seq2seq model:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图帮助我们可视化我们的seq2seq模型：
- en: '![](img/aa874827-debc-49b6-82a0-42aa8ad6390a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa874827-debc-49b6-82a0-42aa8ad6390a.png)'
- en: The illustration of the sequence to sequence (seq2seq) model. Each rectangle
    box is the RNN cell in which blue ones are the encoders and Red been the Decoders.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列（seq2seq）模型的示意图。每个矩形框表示一个RNN单元，其中蓝色的是编码器，红色的是解码器。
- en: In the encoder-decoder structure, one RNN (blue) **encodes** the input sequence.
    The encoder emits the context **C**, usually as a simple function of its final
    hidden state. And the second RNN (red) **decoder** calculates the target values
    and generates the output sequence. One essential step is to let the encoder and
    decoder communicate. In the simplest approach, you use the last hidden state of
    the encoder to initialize the decoder. Other approaches let the decoder attend
    to different parts of the encoded input at different timesteps in the decoding
    process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器结构中，一个RNN（蓝色）**编码**输入序列。编码器输出上下文**C**，通常是其最终隐藏状态的简单函数。第二个RNN（红色）**解码器**计算目标值并生成输出序列。一个关键步骤是让编码器和解码器进行通信。在最简单的方法中，您使用编码器的最后一个隐藏状态来初始化解码器。其他方法则让解码器在解码过程中在不同的时间步访问编码输入的不同部分。
- en: So, let's get started with data preparation, model building, training, tuning,
    and evaluating our seq2seq model, and see how it performs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始进行数据准备、模型构建、训练、调优和评估我们的seq2seq模型，看看它的表现如何。
- en: The model file can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py)找到。
- en: Data preparation
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Here, we will build our question-answering system. For the project, we need
    a dataset with question and answer pairs, as shown in the following screenshot.
    Both of the columns contain sequences of words, which is what we need to feed
    into our seq2seq model. Also, note that our sentences can be of dynamic length:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将构建我们的问答系统。对于该项目，我们需要一个包含问题和答案对的数据集，如下图所示。两列数据都包含词序列，这正是我们需要输入到seq2seq模型中的内容。此外，请注意我们的句子可以具有动态长度：
- en: '![](img/953818c9-cf3d-48c3-8466-4a296dd7315b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/953818c9-cf3d-48c3-8466-4a296dd7315b.png)'
- en: The dataset which we prepared with set of questions and answers
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备的包含问题和答案的数据集
- en: 'Let''s load them and perform the same data processing using `build_dataset()`*.*
    In the end, we will have a dictionary with words as keys, where the associated
    values are the counts of the word in the respective corpus. Also, we have four
    extras values that we talked about before in this chapter:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载它们并使用`build_dataset()`执行相同的数据处理。最终，我们将得到一个以单词为键的字典，相关值是该单词在相应语料中的出现次数。此外，我们还会得到之前在本章中提到的四个额外的值：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Defining a seq2seq model
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个seq2seq模型
- en: 'In this section, we will outline the TensorFlow seq2seq model definition. We
    employed an embedding layer to go from integer representation to the vector representation
    of the input. This seq2seq model has four major components: the embedding layer,
    encoders, decoders, and cost/optimizers.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述TensorFlow seq2seq模型的定义。我们采用了一个嵌入层，将整数表示转化为输入的向量表示。这个seq2seq模型有四个主要组成部分：嵌入层、编码器、解码器和成本/优化器。
- en: 'You can see the model in graphical form in the following diagram:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图表中查看模型的图形表示：
- en: '![](img/c6268a0b-a8f8-4242-aede-907665dbf9b7.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6268a0b-a8f8-4242-aede-907665dbf9b7.png)'
- en: The TensorBoard visualization of the seq2seq model. This graph shows the connection
    between the encode and the decoder with other relevent components like the optimizer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq 模型的 TensorBoard 可视化。该图显示了编码器与解码器之间的连接，以及其他相关组件如优化器。
- en: 'The following is a formal outline of the TensorFlow seq2seq model definition:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 TensorFlow seq2seq 模型定义的正式大纲：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hyperparameters
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Now that we have our model definition ready, we will define the hyperparameters.
    We will keep most of the configurations the same as in the previous one:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好模型定义，我们将定义超参数。我们将保持大部分配置与之前相同：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the seq2seq model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 seq2seq 模型
- en: 'Now, let''s train the model. We will need some helper functions for the padding
    of the sentence and to calculate the accuracy of the model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来训练模型。我们将需要一些辅助函数来填充句子并计算模型的准确率：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We initialize our model and iterate the session for the defined number of epochs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化模型并迭代会话，训练指定的 epoch 次数：
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Evaluation of the seq2seq model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 seq2seq 模型。
- en: So, after running the training process for few hours on a GPU, you can see that
    the accuracy has reached a value of `1.0`, and loss has significantly reduced
    to `0.00045`. Let's see how the model performs when we ask some generic questions.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在 GPU 上运行训练过程几个小时后，你可以看到准确率已达到`1.0`，并且损失显著降低至`0.00045`。让我们看看当我们提出一些通用问题时，模型表现如何。
- en: 'To make predictions, we will create a `predict()` function that will take the
    raw text of any size as input and return the response to the question that we
    asked. We did a quick fix to handle the **Out Of Vocab** (**OOV**) words by replacing
    them with the `PAD`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，我们将创建一个 `predict()` 函数，它将接受任意大小的原始文本作为输入，并返回我们提出问题的答案。我们对 **Out Of Vocab**（**OOV**）词汇进行了快速修复，通过将其替换为
    `PAD` 来处理：
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When the model was trained for the first 50 epochs, we had the following result:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型经过前 50 次 epoch 训练后，我们得到了以下结果：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When the model was trained for 1,136 epochs:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型训练了 1,136 个 epoch 后：
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Well! That's impressive, right? Now your model is not just able to understand
    the context, but can also generate answers word by word.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这很令人印象深刻，对吧？现在你的模型不仅能理解上下文，还能逐词生成回答。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered basic RNN cells, LSTM cells, and the seq2seq model
    in building a language model that can be used for multiple NLP tasks. We implemented
    a chatbot, from scratch, to answer questions by generating a sequence of words
    from the provided dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了基本的 RNN 单元、LSTM 单元，以及 seq2seq 模型，构建了一个可以用于多种 NLP 任务的语言模型。我们从头开始实现了一个聊天机器人，通过从提供的数据集中生成词语序列来回答问题。
- en: 'The experience in this exercise demonstrates the value of LSTM as an often
    necessary component of the RNN. With the LSTM, we were able to see the following
    improvements over past CNN models:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这次练习的经验展示了 LSTM 作为 RNN 的一个常见必要组件的价值。有了 LSTM，我们能够看到以下相较于过去的 CNN 模型的改进：
- en: The LSTM was able to preserve state information
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 能够保持状态信息。
- en: The length of sentences for both inputs and outputs could be variable and different
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出的句子长度可能是可变且不同的。
- en: The LSTM was able to adequately handle complex context
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 能够有效地处理复杂的上下文。
- en: 'Specifically, in this chapter, we did the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们做了以下工作：
- en: Gained an intuition about the RNN and its primary forms
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得了对 RNN 及其主要形式的直觉理解。
- en: Implemented a language model using RNN
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现了一个基于 RNN 的语言模型。
- en: Learned about the LSTM model
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习了 LSTM 模型。
- en: Implemented the LSTM language model and compared it to the RNN
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现了 LSTM 语言模型并与 RNN 进行了比较。
- en: Implemented an encoder-decoder RNN based on the LSTM unit for a simple sequence-to-sequence
    question-answer task
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现了一个基于 LSTM 单元的编码器-解码器 RNN，用于一个简单的序列到序列的问答任务。
- en: With the right training data, it would be possible to use this model to achieve
    the goal of the hypothetical client (the restaurant chain) of building a robust
    chatbot (in combination with other computational linguistic technologies that
    we've explored) that could automate the over-the-phone food ordering process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有了正确的训练数据，就有可能使用这个模型实现假设客户（餐饮连锁店）的目标，即构建一个强大的聊天机器人（结合我们探索的其他计算语言学技术），可以自动化电话订餐过程。
- en: Well done!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！
