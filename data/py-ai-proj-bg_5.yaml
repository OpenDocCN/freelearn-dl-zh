- en: Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: In this chapter, we'll cover some of the basics of deep learning. Deep learning
    refers to neural networks with lots of layers. It's kind of a buzzword, but the
    technology behind it is real and quite sophisticated.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些深度学习的基础知识。深度学习是指拥有多层的神经网络。这虽然是一个流行词，但其背后的技术是真实且相当复杂的。
- en: 'The term has been rising in popularity along with machine learning and artificial
    intelligence, as shown in this Google trend chart:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语的流行程度正在上升，与机器学习和人工智能一起，如下图所示：
- en: '![](img/00161.jpeg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00161.jpeg)'
- en: As stated by some of the inventors of deep learning methods, the primary advantage
    of deep learning is that adding more data and more computing power often produces
    more accurate results, without the significant effort required for engineering.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如一些深度学习方法的发明者所言，深度学习的主要优势在于，添加更多数据和更多计算能力通常能产生更准确的结果，而无需进行大量的工程工作。
- en: 'In this chapter, we are going to be looking at the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将探讨以下内容：
- en: Deep learning methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习方法
- en: Identifying handwritten mathematical symbols with CNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 识别手写数学符号
- en: Revisiting the bird species identifier to use images
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视鸟类物种识别器以使用图像
- en: Deep learning methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法
- en: Deep learning refers to several methods which may be used in a particular application.
    These methods include convolutional layers and pooling. Simpler and faster activation
    functions, such as ReLU, return the neuron's weighted sum if it's positive and
    zero if negative. Regularization techniques, such as dropout, randomly ignore
    weights during the weight update base to prevent overfitting. GPUs are used for
    faster training with the order that is 50 times faster. This is because they're
    optimized for matrix calculations that are used extensively in neural networks
    and memory units for applications such as speech recognition.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是指在特定应用中可以使用的几种方法。这些方法包括卷积层和池化层。更简单、更快速的激活函数，例如 ReLU，当神经元的加权和为正时返回其加权和，为负时返回零。正则化技术，如丢弃法（dropout），在权重更新过程中随机忽略部分权重，以防止过拟合。GPU
    被用于加速训练，其速度比传统方法快50倍。这是因为 GPU 针对矩阵计算进行了优化，而矩阵计算在神经网络和语音识别等应用中被广泛使用。
- en: Several factors have contributed to deep learning's dramatic growth in the last
    five years. Large public datasets, such as ImageNet, that holds millions of labeled
    images covering a thousand categories and Mozilla's Common Voice Project, that
    contain speech samples are now available. Such datasets have satisfied the basic
    requirement for deep learning-lot of training data. GPUs have transitioned to
    deep learning and clusters while also focusing on gaming. This helps make large-scale
    deep learning possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 过去五年，多个因素推动了深度学习的快速发展。大量公共数据集，如包含数百万张标注图像的 ImageNet 和包含语音样本的 Mozilla Common
    Voice Project，现已可用。这些数据集满足了深度学习的基本要求——大量的训练数据。GPU 已经转向深度学习和集群，同时也专注于游戏领域。这有助于实现大规模的深度学习。
- en: Advanced software frameworks that were released open source and are undergoing
    rapid improvement are also available to everyone. These include TensorFlow, Keras,
    Torch, and Caffe. Deep architectures that achieve state-of-the-art results, such
    as Inception-v3 are being used for the ImageNet dataset. This network actually
    has an approximate of 24 million parameters, and a large community of researchers
    and software engineers quickly translating research prototypes into open source
    software that anyone can download, evaluate, and extend.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的软件框架已经开源并且在快速改进，人人都可以使用。这些框架包括 TensorFlow、Keras、Torch 和 Caffe。像 Inception-v3
    这样的深度架构在 ImageNet 数据集上取得了最先进的成果。这个网络大约有 2400 万个参数，并且有一个庞大的研究人员和软件工程师社区，他们迅速将研究原型转化为开源软件，任何人都可以下载、评估和扩展。
- en: Convolutions and pooling
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积和池化
- en: 'This sections takes a closer look at two fundamental deep learning technologies,
    namely, convolution and pooling. Throughout this section, we will be using images
    to understand these concepts. Nevertheless, what we''ll be studying can also be
    applied to other data, such as, audio signals. Let''s take a look at the following
    photo and begin by zooming in to observe the pixels:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将深入探讨两项基本的深度学习技术，即卷积和池化。在这一部分中，我们将使用图像来理解这些概念。尽管如此，我们所学习的内容同样可以应用于其他数据，如音频信号。让我们来看一下以下的照片，并开始通过放大观察像素：
- en: '![](img/00162.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00162.jpeg)'
- en: 'Convolutions occur per channel. An input image would generally consist of three
    channels; red, green, and blue. The next step would be to separate these three
    colors. The following diagram depicts this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是按通道进行的。输入图像通常由三个通道组成：红色、绿色和蓝色。接下来的步骤是将这三种颜色分离开。下图展示了这一过程：
- en: '![](img/00163.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00163.jpeg)'
- en: 'A convolution is a kernel. In this image, we apply a 3 x 3 kernel. Every kernel
    contains a number of weights. The kernel slides around the image and computes
    the weighted sum of the pixels on the kernel, each multiplied by their corresponding
    kernel weights:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一个核。在这张图中，我们应用了一个3 x 3的卷积核。每个卷积核包含一定数量的权重。卷积核在图像上滑动，并计算卷积核上像素的加权和，每个像素与其对应的卷积核权重相乘：
- en: '![](img/00164.gif)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00164.gif)'
- en: 'A bias term is also added. A single number, the weighted sum, is produced for
    each position that the kernel slides over. The kernel''s weights start off with
    any random value and change during the training phase. The following diagram shows
    three examples of kernels with different weights:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 还添加了一个偏置项。每个卷积核滑动到的位置会产生一个单一的数字，即加权和。卷积核的权重最初是随机值，并在训练阶段发生变化。下图展示了三种不同权重的卷积核示例：
- en: '![](img/00165.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpeg)'
- en: 'You can see how the image transforms differently depending on the weights.
    The rightmost image highlights the edges, which is often useful for identifying
    objects. The stride helps us understand how the kernel slides across the image.
    The following diagram is an example of a 1 x 1 stride:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到图像如何根据权重不同而有所不同。最右边的图像强调了边缘，这通常对识别物体很有帮助。步幅帮助我们理解卷积核如何在图像上滑动。下图是一个1 x 1步幅的例子：
- en: '![](img/00166.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.jpeg)'
- en: 'The kernel moves by one pixel to the right and then down. Throughout this process,
    the center of the kernel will hit every pixel of the image whilst overlapping
    the other kernels. It is also observed that some pixels are missed by the center
    of the kernel. The following image depicts a 2 x 2 stride:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核向右移动一个像素，然后向下移动。在这个过程中，卷积核的中心会接触到图像的每个像素，并与其他卷积核发生重叠。也可以观察到，有些像素被卷积核的中心遗漏。下图展示了一个2
    x 2步幅的例子：
- en: '![](img/00167.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00167.jpeg)'
- en: 'In certain cases, it is observed that no overlapping takes place. To prove
    this, the following diagram contains a 3 x 3 stride:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，观察到没有发生重叠。为了证明这一点，以下图包含了一个3 x 3步幅的例子：
- en: '![](img/00168.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00168.jpeg)'
- en: In such cases, no overlap takes place because the kernel is the same size as
    the stride.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于卷积核与步幅的大小相同，因此不会发生重叠。
- en: 'However, the borders of the image need to be handled differently. To affect
    this, we can use padding. This helps avoid extending the kernel across the border.
    Padding consists of extra pixels, which are always zero. They don''t contribute
    to the weighted sum. The padding allows the kernel''s weights to cover every region
    of the image while still letting the kernels assume the stride is 1\. The kernel
    produces one output for every region it covers. Hence, if we have a stride that
    is greater than 1, we''ll have fewer outputs than there were original pixels.
    In other words, the convolution helped reduce the image''s dimensions. The formula
    shown here tells us the dimensions of the output of a convolution:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图像的边缘需要不同的处理方式。为了处理这一点，我们可以使用填充。填充有助于防止卷积核跨越图像边界。填充由额外的像素组成，这些像素的值始终为零，不参与加权和的计算。填充使得卷积核的权重能够覆盖图像的每个区域，同时仍然假设卷积核的步幅为1。卷积核在覆盖的每个区域上会生成一个输出。因此，如果步幅大于1，我们将得到的输出会比原始像素少。换句话说，卷积有助于减少图像的尺寸。下面的公式展示了卷积输出的尺寸：
- en: '![](img/00169.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00169.jpeg)'
- en: 'It is a general practice to use square images. Kernels and strides are used
    for simplicity. This helps us focus on only one dimension, which will be the same
    for the width and height. In the following diagram, a 3 x 3 kernel with a (3,
    3) stride is depicted:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的做法是使用方形图像。为了简化，卷积核和步幅通常是方形的。这有助于我们仅关注一个维度，并且宽度和高度是相同的。下图展示了一个3 x 3卷积核与(3,
    3)步幅的例子：
- en: '![](img/00170.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00170.jpeg)'
- en: The preceding calculation gives the result of 85 width and 85 height. The image's
    width and height have effectively been reduced by a factor of three from the original
    256\. Rather than use a large stride, we shall let the convolution hit every pixel
    by using a stride of 1\. This will help us attain a more practical result. We
    also need to make sure that there is sufficient padding. However, it is beneficial
    to reduce the image dimensions as we move through the network. This helps the
    network train faster as there will be fewer parameters. Fewer parameters imply
    a smaller chance of over-fitting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述计算结果为85宽度和85高度。图像的宽度和高度有效地从原始256缩小了三倍。我们不会使用较大的步幅，而是选择使用步幅为1的卷积，以便卷积能够遍历每个像素。这将帮助我们得到更实际的结果。我们还需要确保有足够的填充。然而，在网络中移动时减少图像尺寸是有益的，因为这样可以让网络更快地训练，减少参数数量。较少的参数意味着更小的过拟合风险。
- en: 'We often use max or average pooling between convolution dimensionality instead
    of varying the stride length. Pooling looks at a region, which, let us assume, is
    2 x 2, and keeps only the largest or average value. The following image depicts
    a 2 x 2 matrix that depicts pooling:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常在卷积维度之间使用最大池化或平均池化，而不是改变步幅长度。池化操作查看一个区域，假设它是2 x 2，并且只保留最大值或平均值。以下图像展示了一个2
    x 2矩阵，表示池化操作：
- en: '![](img/00171.gif)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00171.gif)'
- en: A pooling region always has the same-sized stride as the pool size. This helps
    avoid overlapping.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 池化区域的步幅大小始终与池的大小相同。这有助于避免重叠。
- en: Pooling doesn't use any weights, which means there is nothing to train.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作不使用任何权重，这意味着没有需要训练的参数。
- en: 'Here''s a relatively shallow **convolutional neural networks** (**CNNs**) representation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对较浅的**卷积神经网络**（**CNN**）表示：
- en: '![](img/00172.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00172.jpeg)'
- en: Source: cs231.github.io, MIT License
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：cs231.github.io，MIT许可证
- en: We observe that the input image is subjected to various convolutions and pooling
    layers with ReLU activations between them before finally arriving at a traditionally
    fully connected network. The fully connected network, though not depicted in the
    diagram, is ultimately predicting the class. In this example, as in most CNNs,
    we will have multiple convolutions at each layer. Here, we will observe 10, which
    are depicted as rows. Each of these 10 convolutions have their own kernels in
    each column so that different convolutions can be learned at each resolution.
    The fully connected layers on the right will determine which convolutions best
    identify the car or the truck, and so forth.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，输入图像经过多个卷积和池化层，并且每层之间有ReLU激活，最终到达一个传统的全连接网络。尽管图中没有展示全连接网络，但它最终会预测类别。在这个例子中，与大多数CNN一样，我们会在每个层中进行多次卷积。这里我们观察到的是10个卷积，它们以行的形式展示。每个卷积都有自己的核在每一列中，这样可以在每个分辨率上学习不同的卷积。右侧的全连接层将决定哪些卷积最能识别汽车、卡车等。
- en: Identifying handwritten mathematical symbols with CNNs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN识别手写数学符号
- en: This sections deals with building a CNN to identify handwritten mathematical
    symbols. We're going to use the `HASYv2` dataset. This contains 168,000 images
    from 369 different classes where each represents a different symbol. This dataset
    is a more complex analog compared to the popular MNIST dataset, which contains
    handwritten numbers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容涉及构建一个卷积神经网络（CNN）来识别手写数学符号。我们将使用`HASYv2`数据集。该数据集包含来自369个不同类别的168,000张图像，每个类别代表一个不同的符号。与流行的MNIST数据集（包含手写数字）相比，这个数据集是一个更为复杂的类比。
- en: 'The following diagram depicts the kind of images that are available in this
    dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了该数据集中可用的图像类型：
- en: '![](img/00173.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00173.jpeg)'
- en: 'And here, we can see a graph showing how many symbols have different numbers
    of images:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到一张图表，展示了有多少符号拥有不同数量的图像：
- en: '![](img/00174.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00174.jpeg)'
- en: 'It is observed that many symbols have few images and there are a few that have
    lots of images. The code to import any image is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到许多符号的图像较少，而有些符号则有大量图像。导入任何图像的代码如下：
- en: '![](img/00175.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpeg)'
- en: 'We begin by importing the `Image` class from the `IPython` library. This allows
    us to show images inside Jupyter Notebook. Here''s one image from the dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从`IPython`库导入`Image`类，这样我们就可以在Jupyter Notebook中显示图像。以下是数据集中的一张图像：
- en: '![](img/00176.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00176.jpeg)'
- en: 'This is an image of the alphabet **A**. Each image is 30 x 30 pixels. This
    image is in the RGB format even though it doesn''t really need to be RGB. The
    different channels are predominately black and white or grayscale. We''re going
    to use these three channels. We then proceed to import CSV, which allows us to
    load the dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张字母**A**的图像。每张图像的尺寸为30 x 30像素。尽管它实际上不需要是RGB格式，这张图像还是采用了RGB格式。不同的通道主要是黑白或灰度的。我们将使用这三个通道。接着，我们导入CSV文件，这样就可以加载数据集了：
- en: '![](img/00177.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpeg)'
- en: 'This CSV file states all the different filenames and the class names. We import
    the image class from `pil`, which allows us to load the image. We import `preprocessing.image`, which
    then allows us to convert the images into `numpy` arrays. Let''s us then go through
    the data file, taking a closer look at every filename and loading it, while recording
    which class it belongs to:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该CSV文件列出了所有不同的文件名和类名。我们从`pil`库导入图像类，它允许我们加载图像。我们还导入了`preprocessing.image`，它使我们能够将图像转换为`numpy`数组。然后，我们会遍历数据文件，逐一查看每个文件名并加载它，同时记录它属于哪个类：
- en: '![](img/00178.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00178.jpeg)'
- en: The immediate next step would be to save the images and the classes and use
    the CSV reader. We need to set a counter to make sure we skip the first row, which
    is the header of the CSV file. Only after this, we proceed to open the image,
    which is in the first column of each row. This is converted into an array. The
    achieved result will have dimensions of 30 x 30 x 3, which is interpreted as 30
    width, 30 height, and 3 channels (RGB).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是保存图像和类，并使用CSV读取器。我们需要设置一个计数器，确保跳过CSV文件的第一行（表头）。在这之后，我们继续打开图像，它位于每行的第一列。然后将其转换为数组。最终结果的维度将为30
    x 30 x 3，即30宽度、30高度和3个通道（RGB）。
- en: 'These three channels will have numbers between 0 and 255\. These are typical
    pixel values, which are not good for a neural network. We need values that lie
    between 0 and 1 or -1 and 1\. To do this, we divide each pixel value by 255\.
    To make things easier, we''re going to collect the filename, the class name, and
    the image matrix and put them into our images list. We will also make a note of
    the name of the class. The following snippet will make us understand the concept
    to a greater depth:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个通道的值将在0到255之间。这些是典型的像素值，但对于神经网络来说并不好。我们需要将值转换到0到1或-1到1之间。为此，我们将每个像素值除以255。为了简化操作，我们将收集文件名、类名和图像矩阵，并将它们放入我们的图像列表中。我们还会记录类的名称。以下代码片段将帮助我们更深入地理解这一概念：
- en: '![](img/00179.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)'
- en: The file is named `hasy-data/v2-00000.png`. `A` is the name of the class followed
    by the array. The array has dimensions 30 x 30 x 3\. The innermost and last dimension,
    is 3\. Each 1.0 depicts the color white. We understand this because we divided
    everything by 255 as mentioned earlier.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件名为`hasy-data/v2-00000.png`。`A`是类的名称，后面跟着数组。该数组的维度为30 x 30 x 3。最内层和最后一个维度是3。每个1.0表示白色。这是因为我们像前面提到的那样将所有值除以255。
- en: 'We have 168,000 images in the `HASYv2` dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`HASYv2`数据集中有168,000张图像：
- en: '![](img/00180.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00180.jpeg)'
- en: 'We then proceed to shuffle and then split the data on an 80% train, 20% test
    basis. As seen in the following codeblock, we first shuffle, then proceed to split
    the image:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续对数据进行打乱，并按照80%的训练集和20%的测试集比例进行拆分。如下代码块所示，我们首先进行打乱，然后再拆分图像：
- en: '![](img/00181.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpeg)'
- en: 'Because we use these tuples with three different values, we''re going to need
    to ultimately collect all that into a matrix:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用了包含三个不同值的元组，所以最终我们需要将它们收集成一个矩阵：
- en: '![](img/00182.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00182.jpeg)'
- en: We need to collect the images as well as the labels. To collect the images,
    we go through each row and take each third element. This element is the image
    matrix. We stick it all together into a `numpy` array. The same is done for the
    train and test datasets.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要收集图像和标签。为了收集图像，我们遍历每一行并提取每个第三个元素。这个元素就是图像矩阵。我们将它们全部合并成一个`numpy`数组。同样的操作适用于训练集和测试集。
- en: For the outputs, we need to go and pick out the second value. These are still
    strings, such as `a` and `=`. We need to convert the second value into one-hot
    encoding before it can be used for a neural network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出，我们需要选择第二个值。这些值仍然是字符串，如`a`和`=`。我们需要将第二个值转换为独热编码，以便它能用于神经网络。
- en: 'We proceed to use scikit-learn''s preprocessing label encoder and one-hot encoder:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着使用scikit-learn的预处理标签编码器和独热编码器：
- en: '![](img/00183.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00183.jpeg)'
- en: 'We''re going to make a `LabelEncoder` object and we''re going to both fit and
    transform on the classes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个`LabelEncoder`对象，并且对类进行拟合和转换：
- en: '![](img/00184.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpeg)'
- en: The `fit` function learns which classes exist. It learns that there are 369
    different class names. The `tranform` function turns them into integers. This
    is done by sorting the classes and giving each class an integer ID. `integer_encoded`
    helps to reproduce the list of classes as integer IDs. The one-hot encoder takes
    these integers and fits on them; this too learns how many different integers are
    represented. Just as `LabelEncoder` learned about the class names, `onehot_encoder`
    is going to learn that there are 369 different integers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`函数学习现存的类别。它学会了有369个不同的类别名称。`transform`函数将这些类别转化为整数。这是通过对类别进行排序并为每个类别分配一个整数ID来实现的。`integer_encoded`帮助我们以整数ID的形式重现类别列表。一热编码器（one-hot
    encoder）将这些整数进行拟合，并学习有多少个不同的整数。就像`LabelEncoder`学习类别名称一样，`onehot_encoder`将学习到有369个不同的整数。'
- en: The code then moves to `LabelEncoder` which transforms `train_output` into integers.
    These integers are then transformed into one-hot encoding. The one-hot encoding
    returns a 369-dimension with the first dimension of 369 values and a vector of
    369 values. All values are zeros except for a single 1\. The position of this
    1 depends on which class it is. `test_output` undergoes the same process. When
    the training data for input and output is ready, we proceed to build a neural
    network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 代码接着使用了`LabelEncoder`，它将`train_output`转化为整数。这些整数随后被转化为一热编码（one-hot encoding）。一热编码返回一个369维的向量，第一维有369个值，向量中所有值都是零，除了一个值是1。这个1的位置取决于该类别是哪一类。`test_output`也经过同样的处理。当输入和输出的训练数据准备好后，我们继续构建神经网络。
- en: 'To do this, we are going to use `Sequential` again:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将再次使用`Sequential`：
- en: '![](img/00185.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.jpeg)'
- en: Sequential is a feed-forward network. Even though there are convolutions that
    still feed forward and are not recurrent, there are no cycles. Dense layers are
    used at the end of the network. We also use `Dropout` to try to prevent overfitting.
    When we switch from convolutions to dense layers, we need to use the `flatten`
    command, since convolutions are two-dimensional and dense layers are not. We also
    need to use `Conv2D` and `MaxPooling2D`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Sequential是一个前馈网络。尽管有卷积层，但它们仍然是前馈的，并且没有循环结构。网络的最后使用了全连接层（dense layers）。我们还使用了`Dropout`来尽量避免过拟合。当我们从卷积层切换到全连接层时，我们需要使用`flatten`命令，因为卷积层是二维的，而全连接层不是。我们还需要使用`Conv2D`和`MaxPooling2D`。
- en: 'The following code block is our network design:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块是我们的网络设计：
- en: '![](img/00186.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00186.jpeg)'
- en: This is modeled after MNIST design, which handles handwritten numbers. We start
    by making a sequential model. We need to add a convolution layer that has 32 different
    convolutions. The kernel size will be 3 x 3 and the activation will be ReLU. Since
    this is the first layer, we need to mention the input shape. If you recall, the
    dimensions were 30 x 30 x 3.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型借鉴了MNIST设计，它处理手写数字。我们首先创建一个顺序模型。我们需要添加一个具有32个不同卷积层的卷积层。卷积核大小为3 x 3，激活函数为ReLU。由于这是第一层，我们需要指定输入形状。如果你还记得，输入的维度是30
    x 30 x 3。
- en: We use the kernel size of 3 x 3 and the stride as 1 as it is the default value.
    Having the stride as 1 will require padding. This is going to produce a 30 x 30
    x 32 shape because there are 32 convolutions. The 30 x 30 dimensions remain constant.
    WE now observe that we haven't really reduced dimensions just by doing this convolution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用3 x 3的卷积核大小，并将步幅设置为1，因为这是默认值。步幅为1时需要进行填充。这样会产生一个30 x 30 x 32的形状，因为有32个卷积层。30
    x 30的维度保持不变。我们现在可以观察到，仅通过进行卷积操作，我们并没有真正地减少维度。
- en: '`MaxPooling` is used to reduce the dimensions by half. This is possible because
    it has a 2 x 2 pool size. We then follow with another convolution layer, which
    is another dimensionality reduction.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxPooling`用于将维度缩减一半。这是可能的，因为它的池化大小为2 x 2。然后，我们进行另一个卷积层，这是另一次维度缩减。'
- en: After all the convolutions have taken place, we flatten everything. This converts
    a two-dimensional representation into a one-dimensional representation. This is
    then fed into a dense layer with more than 1,000 neurons.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有卷积操作完成后，我们将所有内容展平。这将二维表示转化为一维表示。然后将其输入到一个拥有超过1000个神经元的全连接层。
- en: This dense layer will then have a `tanh` activation. This is then fed into another
    dense layer of neurons. This time around, there are 369 of them for the class
    outputs. This is the `onehot_encoding` output. We're not going to do any particular
    activation except for softmax. So, the original values will be rescaled to be
    between 0 and 1\. This means that the sum of all the values across the 369 different
    neurons is 1.0\. Softmax basically turns the output into a probability.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个密集层将会有一个`tanh`激活函数。然后，这个结果会被送入另一个神经元的密集层。这个密集层有369个神经元，用于类别输出。这是`onehot_encoding`输出。除了softmax激活函数外，我们不会使用其他激活函数。因此，原始值将被重新缩放到0和1之间。这意味着所有369个神经元的输出值之和为1.0。Softmax基本上将输出转化为概率。
- en: 'Proceeding to compiling `categorical _crossentropy` again helps us predict
    one of multiple classes. You would want to do this on the `adam` optimizer and
    observe it''s accuracy. Here''s the model''s summary:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重新编译`categorical _crossentropy`有助于我们预测多个类别中的一个。你希望在`adam`优化器上执行此操作，并观察其准确性。以下是模型的总结：
- en: '![](img/00187.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00187.jpeg)'
- en: It is observed that the convolution layer doesn't change the dimensions, but
    the pooling does. It reduces it by half because of the odd dimension size, that
    is, 15\. The next layer is at 13 output, which also gets reduced by half. The
    `conv2d_1 (Conv2D)` parameters are used for learning the convolutions. The `dense_1
    (Dense)` parameters are used for learning the weights connected to the prior layer.
    In a similiar fashion, the `dense_2 (Dense)` parameters are for the weights for
    the prior layer. Ultimately, we have about 1.6 million parameters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到卷积层不会改变维度，但池化会改变维度。由于奇数维度大小，即15，它会将尺寸减半。下一个层的输出是13，同样会被减半。`conv2d_1 (Conv2D)`的参数用于学习卷积。`dense_1
    (Dense)`的参数用于学习与前一层连接的权重。类似地，`dense_2 (Dense)`的参数用于学习前一层的权重。最终，我们有大约160万个参数。
- en: 'We''re going to visualize the performance''s accuracy and validation''s accuracy
    with TensorBoard. We''re going to save all the results into a directory called
    `mnist-style` because that''s the style of the network we built earlier. The following
    is a callback:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorBoard可视化性能的准确性和验证准确性。我们将把所有结果保存到名为`mnist-style`的目录中，因为这就是我们之前构建的网络的风格。以下是一个回调：
- en: '![](img/00188.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00188.jpeg)'
- en: Keras supports callbacks of various types. The callback is used in the `fit`
    method, so after every epoch, it calls the callback. It passes information to
    the callback, such as the validation loss and the training loss. We use 10 epochs
    and a batch size of 32, with a 0.2, 20%, validation split.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Keras支持各种类型的回调。回调函数在`fit`方法中使用，因此每完成一个周期后，它会调用回调函数。回调函数会接收信息，比如验证损失和训练损失。我们使用10个周期和32的批量大小，且验证分割为0.2（20%）。
- en: 'Here''s the result of the training:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练的结果：
- en: '![](img/00189.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00189.jpeg)'
- en: Now, there are a lot of choices, but ultimately we need to check them. We got
    about 76% validation accuracy, and when we test this out on the test set, we get
    the same 76% accuracy. Now, there were a lot of decisions in this design, including
    how many convolution layers to have and what size they should be, what kernel
    should be used or what size of kernel, what kind of stride, what the activation
    was for the convolutions, where the max pooling showed up, if it ever did, what
    the pooling size was, how many dense layers we have, when do they appear, what
    is the activation, and so on and so forth. A lot of decisions. It's quite difficult
    to know how to choose these different designs. These are actually called **hyperparameters**.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有很多选择，但最终我们需要检查它们。我们得到了大约76%的验证准确率，当我们在测试集上进行测试时，得到了相同的76%准确率。现在，设计过程中有很多决策，包括卷积层的数量、每个卷积层的大小、使用什么类型的核，核的大小、步幅、卷积的激活函数、最大池化是否出现、池化的大小、密集层的数量及出现时机、激活函数等等。很多决策。选择这些不同设计的方式相当困难。这些实际上叫做**超参数**。
- en: 'The weights that can be learned during the fit procedure are just called parameters,
    but the decisions you have to make about how to design the network and the activation
    functions and so forth we call hyperparameters, because they can''t be learned
    by the network. In order to try different parameters, we can just do some loops:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合过程中可以学习的权重叫做参数，而关于如何设计网络、激活函数等的决策，我们称之为超参数，因为它们不能通过网络学习。为了尝试不同的参数，我们可以通过一些循环来实现：
- en: '![](img/00190.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00190.jpeg)'
- en: We will time how long it takes to train each of these. We will collect the results,
    which would be the accuracy numbers. Then, we will try a convolution 2D, which
    will have one or two such layers. We're going to try a dense layer with 128 neurons.
    We will try a dropout as `for dropout in [0.0, 0.25, 0.50, 0.7`, which will be
    either yes or no, and means 0-25%, 50%, 75%. So, for each of these combinations,
    we make a model depending on how many convolutions we're going to have, with convolution
    layers either one or two. We're going to add a convolution layer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计时每个训练过程所需的时间。我们将收集结果，也就是准确率数字。然后，我们将尝试一个 2D 卷积层，可能会有一两个这样的层。我们将尝试一个包含 128
    个神经元的密集层。我们将尝试一个 dropout，如`for dropout in [0.0, 0.25, 0.50, 0.7]`，这意味着会有 0-25%、50%、75%
    的可能性进行丢弃。所以，对于这些组合，我们根据卷积层的数量来创建模型，卷积层可能是一个或两个。我们将添加一个卷积层。
- en: 'If it''s the first layer, we need to put in the input shape, otherwise we''ll
    just add the layer. Then, after adding the convolution layer, we''re going to
    do the same with max pooling. Then, we''re going to flatten and add a dense layer
    of whatever size that comes from `for dense_size in [128, 256, 512, 1024, 2048]:
    loop`. It will always be `tanh`, though.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '如果是第一层，我们需要输入输入形状，否则我们只需添加该层。然后，在添加卷积层后，我们将对最大池化做相同的操作。接着，我们将展平并添加一个密集层，其大小由`for
    dense_size in [128, 256, 512, 1024, 2048]: loop`决定。不过，它总是使用`tanh`激活函数。'
- en: If `Dropout` is used, we're going to add a dropout layer. Calling this dropout
    means, say it's 50%, that every time it goes to update the weights after each
    batch, there's a 50% chance for each weight that it won't be updated, but we put
    this between the two dense layers to kind of protect it from overfitting. The
    last layer will always be the number of classes because it has to be, and we'll
    use softmax. It gets compiled in the same way.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用了`Dropout`，我们将添加一个 dropout 层。这个 dropout 的意思是，假设它是 50%，每次在每个批次更新权重时，每个权重有
    50% 的概率不会被更新，但我们将这个层放在两个密集层之间，以防止过拟合。最后一层将始终是类别数量，因为必须如此，并且我们将使用 softmax。编译方法是相同的。
- en: 'Set up a different log directory for TensorBoard so that we can distinguish
    the different configurations. Start the timer and run fit. Do the evaluation and
    get the score, stop the timer, and print the results. So, here it is running on
    all of these different configurations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为 TensorBoard 设置一个不同的日志目录，以便我们能够区分不同的配置。启动计时器并运行 fit。进行评估并获取分数，停止计时器，并打印结果。所以，下面是它在所有这些不同配置下的运行情况：
- en: '![](img/00191.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00191.jpeg)'
- en: 0.74 is the actual test set accuracy. So, you can see that there are a lot of
    different numbers for accuracy. They go down to low point sevens up to the high
    point sevens, and the time differs depending on how many parameters there are
    in the network. We can visualize these results because we are using the callback
    function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 0.74 是实际测试集的准确率。所以，你可以看到准确率有很多不同的数字。从低到高准确率大约在 0.7 到 0.8 之间，并且训练时间因网络中的参数数量而不同。我们可以可视化这些结果，因为我们正在使用回调函数。
- en: 'Here''s the accuracy and loss, which are from the training set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自训练集的准确率和损失：
- en: '![](img/00192.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00192.jpeg)'
- en: 'And here''s the validation accuracy and validation loss:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是验证准确率和验证损失：
- en: '![](img/00193.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00193.jpeg)'
- en: 'Zoom out a bit so that we can see the configurations on the side, and then
    we can turn them all off. Turn `mnist-style` back on. This was the first one we
    tried:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微缩小视图，这样我们就可以看到旁边的配置，然后我们可以将它们全部关闭。再打开`mnist-style`。这是我们尝试的第一个配置：
- en: '![](img/00194.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.jpeg)'
- en: You can see that the accuracy goes up and the loss goes down. That's pretty
    normal. Validation accuracy goes up and loss goes down, and it mostly stays consistent.
    What we don't want to see is validation loss skyrocketing after a while, even
    though the accuracy is going way up. That's pretty much by-definition overfitting.
    It's learning the training examples really well, but it's getting much worse on
    the examples it didn't see. We really don't want that to happen. So, let's compare
    a few things. First, we'll compare different dropouts. Let's go to `conv2d_1`-`dense_128`
    but with different dropouts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，准确率在上升，损失在下降。这是非常正常的。验证准确率上升，损失下降，并且大体保持一致。我们不希望看到的是，尽管准确率在大幅上升，但验证损失在一段时间后急剧飙升。这几乎可以定义为过拟合。模型在训练集上的表现非常好，但在未见过的样本上表现大幅下滑。我们真的不希望发生这种情况。所以，让我们比较一些东西。首先，我们将比较不同的
    dropout。我们来看`conv2d_1`-`dense_128`，但是使用不同的 dropout。
- en: 'As far as loss goes:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 至于损失函数：
- en: '![](img/00195.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00195.jpeg)'
- en: We can see that with a very low dropout, such as 0 or 0.25, the loss is minimized.
    That's because if you want to really learn that training set, don't refuse to
    update weights. Instead, update all of them all the time. With that same run,
    by looking at the dark blue line, we can see that it definitely overfit after
    just two epochs because the validation loss, the examples it did not see, started
    to get much worse. So, that's where the overfitting started. It's pretty clear
    that dropout reduces overfitting. Look at the 0.75 dropout. That's where the validation
    loss just got better and better, which means lower and lower.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当丢弃率非常低时，比如0或0.25，损失值被最小化了。这是因为，如果你真的想学习训练集，就不要拒绝更新权重。相反，要一直更新所有权重。在相同的实验中，通过观察深蓝色的线，我们可以看到，经过两次训练后，模型确实出现了过拟合，因为验证损失（即未见过的样本）开始变得更差了。所以，过拟合从这里开始。很明显，丢弃率能减少过拟合。看看0.75的丢弃率，验证损失在此时变得越来越低，这意味着损失越来越小。
- en: 'It doesn''t make it the most accurate, though, because we can see that the
    accuracy is not necessarily the best for our training set or the validation set:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并没有使其成为最准确的模型，因为我们可以看到，无论是训练集还是验证集，准确率并不一定是最好的。
- en: '![](img/00196.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00196.jpeg)'
- en: Actually, about 0.5 seems pretty good for a validation set. Now, let's just
    make sure it's the same for other layers. Again, with no dropouts (0.0), we get
    the lowest training loss but the highest validation loss. Likewise, we get a 0.75
    dropout for the lowest validation loss but not necessarily the best training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大约0.5的丢弃率对于验证集来说效果相当好。现在，让我们确保其他层也符合这个规律。再次地，当没有丢弃率（0.0）时，我们得到最低的训练损失，但最高的验证损失。同样，0.75的丢弃率会带来最低的验证损失，但不一定是最好的训练表现。
- en: 'Now, let''s compare how many dense layers they have. We''re just going to stick
    with dropout 0.5, so we''ll use `conv2d_1`. So, we have one convolution layer,
    `dense_*`, and a dropout of 0.50:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们比较一下它们有多少个密集层。我们将保持丢弃率为0.5，所以我们将使用`conv2d_1`。因此，我们有一个卷积层，`dense_*`，并且丢弃率为0.50：
- en: '![](img/00197.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00197.jpeg)'
- en: 'So the choice here is, does the dense layer have 128, 256, 512, 1,024, or 2,048?
    In the previous graph, we can see that there are some clear cases of overfitting.
    Pretty much anything that''s not the 128 starts to suffer from overfitting. So,
    a dense layer of 128 is probably the best choice. Now, let''s compare one convolution
    layer to two convolution layers:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的选择是，密集层应该有128、256、512、1024还是2048个神经元？在前面的图中，我们可以看到一些明显的过拟合现象。几乎除了128，其他的密集层数都会开始遭遇过拟合。因此，128个神经元的密集层可能是最好的选择。现在，我们来比较一下一个卷积层和两个卷积层的表现：
- en: '![](img/00198.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00198.jpeg)'
- en: 'Not a big difference, actually. For validation, we get two convolution layers
    and receive the lowest loss, which is usually the same as the highest accuracy.
    This means that we''ve narrowed down. This is called model selection, which is
    all about figuring out what the best model is, as well as the best hyperparameters.
    We''ve narrowed this down to the two-dimensional convolution, two layers of that,
    128 dense in the first dense layer, and 50% dropout. Given that, let''s retrain
    on all the data so that we have the best trained model we could possibly have:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上并没有太大的差别。为了验证，我们使用了两个卷积层，并获得了最低的损失值，这通常也意味着最高的准确率。这表明我们已经缩小了范围。这叫做模型选择，主要是找出最佳模型以及最佳超参数。我们已经将其缩小到二维卷积，两个卷积层，第一层密集层有128个神经元，并使用50%的丢弃率。基于这些条件，接下来让我们在所有数据上重新训练，以确保我们拥有最优的训练模型：
- en: '![](img/00199.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00199.jpeg)'
- en: We get our two convolution layers, we do our dense 128 dropout 0.5, and in this
    case we take all the data we have, the entire dataset trained and tested, and
    stick it all together. Now, we can't really evaluate this model because we just
    lost our testing set, so what we're going to do instead is use this model to predict
    other images. Actually, we're going to save the model after it's fit and we're
    going to show how to load in a minute. If you're going to load this in another
    file, you're also going to want to know what those labels were called because
    all we know is the one-hot encoding. From the one-hot encoding, we can get back
    the integer number, but still that's not the same as the actual name of the symbol.
    So, we have to save the classes from `LabelEncoder` and we're just going to use
    a `numpy` file to save that.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了两个卷积层，做了128个全连接层和0.5的dropout，在这个案例中，我们将我们所有的数据、整个数据集的训练和测试结果都结合在一起。现在，我们不能真正评估这个模型，因为我们丢失了测试集，所以我们接下来要做的就是使用这个模型来预测其他图像。实际上，我们将在拟合模型后保存它，并展示如何加载它。假如你要在另一个文件中加载它，你还需要知道那些标签的名称，因为我们所知道的只是one-hot编码。通过one-hot编码，我们可以得到整数，但那仍然不同于符号的实际名称。所以，我们必须保存`LabelEncoder`中的类别，我们将使用一个`numpy`文件来保存它。
- en: 'Let''s train the model:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型：
- en: '![](img/00200.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.jpeg)'
- en: 'This could actually be all in a separate file. You can load everything again:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其实这些内容可以放在一个单独的文件中。你可以再次加载所有内容：
- en: '![](img/00201.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00201.jpeg)'
- en: Import `keras.models` and you can use the `load _model` feature. The model file
    there actually saves the structure as well as the weights. That's all you need
    to do to recover the network. You can print the summary again. For `LabelEncoder`,
    we need to call the constructor again and give it the classes that we saved ahead
    of time.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`keras.models`，然后你可以使用`load_model`功能。那里保存的模型文件实际上保存了结构和权重。恢复网络时，你只需要做这些。你可以再次打印出总结信息。对于`LabelEncoder`，我们需要重新调用构造函数，并传入我们提前保存的类别。
- en: 'Now, we can make a function called predict takes an image. We do a little bit
    of preprocessing to turn the image into an array, we divide it by 255, and we
    predict. If you have a whole set of images, you won''t need to do this reshape,
    but since we just have one, we can put it in an array that has a single row. We
    will get the prediction out of this, and using `LabelEncoder`, we can reverse
    the prediction to the actual name of the class, the name of the symbol, and which
    prediction? Well, it''s one-hot encoding, so you can figure out the position of
    the highest number. This takes all the neuron outputs, the 369, figures out what
    the largest confidence number is, and says that''s the one that was predicted.
    Therefore, one-hot encoding would tell you this particular symbol, and then we
    can print it:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个名为`predict`的函数来接受图像。我们做一点预处理，将图像转换为数组，除以255，然后进行预测。如果你有一整套图像，就不需要进行这种reshape，但由于我们只有一张，我们可以将它放入一个只有一行的数组中。我们将从中获取预测结果，并使用`LabelEncoder`将预测结果反转为实际的类别名称、符号名称，那么哪个是预测呢？它是one-hot编码，因此你可以找出最大数字的位置。这会处理所有神经元输出，即369个结果，找出最大置信度的数字，然后认为这是预测的结果。因此，one-hot编码会告诉你这个特定的符号，然后我们可以打印它：
- en: '![](img/00202.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00202.jpeg)'
- en: 'Here''s how we can use that function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用该函数的示例：
- en: '![](img/00203.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00203.jpeg)'
- en: We're actually using the training images for this purpose instead of making
    new ones, but you get the idea. You take an image that says that's an `A`, and
    I'm 87% confident about it. For pi prediction, we're 58% confident and for alpha
    prediction, we're 88% confident. Next, we'll look at the bird species example
    we used previously, and instead of using all of the attributes that humans created,
    we're going to use the images themselves.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们正在使用训练图像来进行此操作，而不是制作新的图像，但你明白了这个思路。你拿到一张图像，假设它是一个`A`，而我对此的置信度是87%。对于π的预测，我们的置信度是58%，而对于alpha的预测，置信度是88%。接下来，我们将看看之前使用过的鸟类物种示例，而不是使用人类创建的所有属性，我们将直接使用图像本身。
- en: Revisiting the bird species identifier to use images
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新回顾使用图像的鸟类物种识别器
- en: In this section, we're going to revisit the bird species identifier from before.
    This time, we're going to update it to use neural networks and deep learning.
    Can you recall the birds dataset? It has 200 different species of birds across
    12,000 images. Unlike last time, we won't be using the human-labeled attributes,
    and instead we'll use the actual images without any pre-processing. In our first
    attempt, we're going to build a custom convolutional neural network, just like
    we did for the mathematical symbols classifier.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新审视之前的鸟类物种识别器。这一次，我们将更新它以使用神经网络和深度学习。你还记得鸟类数据集吗？它包含了200种不同的鸟类，共有12000张图像。与上次不同，我们这次不会使用人工标注的属性，而是直接使用原始图像，且不进行任何预处理。在第一次尝试中，我们将构建一个自定义的卷积神经网络，就像我们为数学符号分类器所做的那样。
- en: 'Let''s go to the code. We will start with the typical imports:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下代码。我们将从典型的导入开始：
- en: '![](img/00204.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00204.jpeg)'
- en: 'We''ll make some convenience variables, the rows and columns of the image,
    the width and height, and the number of channels, RGB, though every bird image
    will be equal. Even though they''re not all necessarily the same size, we''re
    going to resize them to this size so that they''re all consistent:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会创建一些便捷变量，包括图像的行数和列数，宽度和高度，以及通道数（RGB），尽管每张鸟类图像将是相同的。尽管它们的大小不一定相同，我们将把它们调整为这个尺寸，以便它们保持一致：
- en: '![](img/00205.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00205.jpeg)'
- en: 'Now, this project introduces an interesting feature on Keras called an **image
    data generator**:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个项目介绍了一个Keras中的有趣功能，叫做**图像数据生成器**：
- en: '![](img/00206.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00206.jpeg)'
- en: The data generator can produce new images from the existing training set and
    these new images can have various differences; for example, they can be rotated,
    they can be flipped  horizontally or vertically, and so forth. Then, we can generate
    more examples than we actually started with. This is a great thing to do when
    you have a small number of training examples. We have, in our case, about 6,000
    training sets. That's relatively small in deep learning, so we want to be able
    to generate more; the data generator will just keep generating them as long as
    we keep asking for them. For the training images, we want to also generate versions
    with the horizontal flip. We don't want to do a vertical flip because I don't
    expect any bird images to be upside down. We also want to support rotations of
    up to 45 degrees, and we want to rescale all the pixel values to divide by 255\.
    Actually, `ImageDataGenerator` just calls the constructor, so nothing's actually
    happened yet. What you want to do next is use `flow_from_directory`, so that your
    images can be organized into directories or subdirectories.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成器可以从现有的训练集生成新的图像，这些新图像可能会有各种差异；例如，它们可以旋转，可以水平或垂直翻转，等等。然后，我们就可以生成比最初更多的示例。当你的训练示例较少时，这是一个很好的方法。在我们的案例中，我们大约有6000个训练集。这个数量在深度学习中相对较小，因此我们希望能够生成更多；数据生成器会持续生成，直到我们要求它停止。对于训练图像，我们还希望生成水平翻转的版本。我们不想进行垂直翻转，因为我不期望任何鸟类图像会被倒置。我们还希望支持最多45度的旋转，并且希望将所有像素值重新缩放为除以255。实际上，`ImageDataGenerator`只是调用构造函数，因此实际上还没有发生任何事情。接下来你需要做的是使用`flow_from_directory`，这样你的图像就可以组织到目录或子目录中。
- en: We have a `train` directory, and inside that there's going to be a folder for
    each bird class. So, there's 200 different folders inside train and inside those
    folders are the images for that particular bird. We want all the images to be
    resized to 256 x 256 and we can indicate that instead of using binary, we want
    to use categorical classes, meaning that we will have lots of different classes
    (200, in this case). We're going to use the data generator for the test set too,
    just because `flow_from_directory` is a convenient function. We don't want to
    do any flips, though, or rotations. We just want to use the testing set as is
    so we can compare it with other people. The other really convenient thing about
    `flow_from _directory` is that it's automatically going to produce a `numpy` matrix
    with the image data, and it's also going to give the class values in one-hot encoding.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个`train`目录，里面会有每个鸟类类别的文件夹。所以，在train中有200个不同的文件夹，而每个文件夹里包含了该鸟类的图像。我们希望所有图像都被调整为256
    x 256的尺寸，并且我们可以指定不使用二进制，而是使用类别类，这意味着我们将有许多不同的类别（在这个例子中是200个）。我们也会使用数据生成器来处理测试集，因为`flow_from_directory`是一个非常方便的函数。不过，我们不希望做任何翻转或旋转。我们只希望直接使用测试集，以便能够与其他人的结果进行比较。`flow_from_directory`的另一个非常方便的特点是，它会自动生成一个包含图像数据的`numpy`矩阵，同时也会提供类别值的独热编码。
- en: So, what was several steps before is now being done all at once.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，之前需要几个步骤的事情，现在可以一次完成。
- en: 'Now, I don''t really need to do a reset, but since these are technically iterators,
    if you''re constantly fixing the model and trying to retrain, then you might want
    to do a reset so that you get all the same images in the same order. In any event,
    it''s an iterator, so you can call next, reset, and so forth:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我其实不需要进行重置，但由于这些技术上是迭代器，如果你一直在修正模型并尝试重新训练，那么你可能想要重置一下，这样你可以确保每次生成相同顺序的图像。无论如何，它是一个迭代器，所以你可以调用`next`、`reset`等：
- en: '![](img/00207.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00207.jpeg)'
- en: 'Now, we will build a sequential model, which is going to be a convolutional
    model. We have a convolution kernel of 3 x 3, 64 of this. We also have a `relu`
    and another convolution built by `relu`, which we can do a max pooling with, and
    just from experimentation, I discovered that this works relatively well: 3 x 3
    followed by 3 x 3, each 64\. By having a pretty dramatic max point of 4 x 4, so
    we repeat this process and then we flatten. We have a dropout of 50% just to reduce
    overfitting, a dense of 400 neurons, another dropout, and then 200 for the output
    because there are 200 different classes, and because it''s categorical one-hot
    encoding, we want to use softmax so that only one of those 200 has the highest
    value. We also want to ensure that they all add up to 1.0.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将构建一个顺序模型，这是一个卷积模型。我们有一个3 x 3的卷积核，数量为64。我们还使用了一个`relu`激活函数和另一个由`relu`激活函数构建的卷积层，接着可以进行最大池化。通过实验，我发现这种结构效果相对较好：3
    x 3之后是3 x 3，每层有64个卷积核。通过使用比较剧烈的4 x 4最大池化，我们重复这个过程，然后进行展平。我们设置50%的Dropout以减少过拟合，接着是一个400个神经元的全连接层，再加一个Dropout，最后输出层有200个神经元，因为有200个不同的类别，并且由于是独热编码，我们希望使用softmax激活函数，这样只有那200个类别中一个类别的值会是最大的。同时，我们也要确保它们的总和为1.0。
- en: 'Here''s the summary of the model. Ultimately, we have about 5 million parameters:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型的概述。最终，我们有大约500万个参数：
- en: '![](img/00208.jpeg)![](img/00209.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00208.jpeg)![](img/00209.jpeg)'
- en: The different variations I did that had far more parameters, such as, say, 100
    million performed worse because there were just too many parameters. There's either
    too many parameters, meaning it's really hard to train it to learn anything because
    obviously all the parameters start random, so it's really hard to make those parameters
    trend toward the right values, or there are so few that it's not going to learn
    anything either. There's kind of a balance that you have to find, and 5 million,
    I think, is somewhere near that balance.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我做过的不同变体，比如说100百万个参数的情况，效果更差，因为参数实在是太多了。要么是参数太多，这意味着训练它学习任何东西都非常困难，因为显然所有参数一开始都是随机的，所以很难让这些参数趋向正确的值，要么是参数太少，这样也学不到任何东西。你需要找到一个平衡点，我认为500万差不多是在这个平衡点附近。
- en: 'Now, if you use a generator, you don''t have all the data for the training
    prepared ahead of time; it''s going to produce those images as it goes:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你使用生成器，你就不需要提前准备好所有训练数据；它会随着训练的进行动态生成这些图像：
- en: '![](img/00210.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00210.jpeg)'
- en: That makes it actually quite memory-efficient. You don't have to load the whole
    dataset ahead of time. It'll just make it as needed, but you have to call `fit
    _generator` instead of just using fit. What you give instead of the train input
    and train output is the generator. The generator knows how to produce the image
    matrices and it knows how to produce one-hot encoding. So, again, that's extremely
    convenient when you have images. There's other kinds of generators, too. Look
    at the Keras documentation for these. `steps_per_epoch` shows how many images
    to produce per epoch, or how many batches to produce. The generator, by default,
    produces batches of 32 images. Regarding the number of epochs, and if you want
    to do some statistics on TensorBoard, you can set up a callback and verbose 2
    so that we can see some output here.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上相当节省内存。你不需要提前加载整个数据集。它只会按需加载，但你必须调用`fit_generator`，而不是仅仅使用fit。你提供的不是训练输入和输出，而是生成器。生成器知道如何生成图像矩阵，也知道如何生成一热编码（one-hot
    encoding）。所以，当你有图像数据时，这非常方便。还有其他类型的生成器，可以查阅Keras文档了解更多。`steps_per_epoch`显示每个训练周期需要生成多少张图片，或者说需要生成多少批次。默认情况下，生成器每次生成32张图片。关于训练周期数量，如果你想在TensorBoard上查看一些统计信息，可以设置一个回调函数，并设置verbose为2，这样我们就可以看到一些输出了。
- en: 'There are 10 epochs:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有10个训练周期（epochs）：
- en: '![](img/00211.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00211.jpeg)'
- en: We can see that the training accuracy is on the images that is training on.
    It's not very accurate for what the accuracy is going to be on the test set, so
    we do this separately. The test images are also in a generator. You don't just
    evaluate—you use `evaluate_generator` and you say, *how many images do you want
    to evaluate?* We'll just do 1,000, and we'll get 22% accuracy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，训练准确度是在训练图片上进行的。它并不是很准确，无法预测测试集上的准确度，所以我们需要分开处理。测试图片也在生成器中。你不只是进行评估——你需要使用`evaluate_generator`，然后你会说，*你想评估多少张图片？*
    我们只评估1,000张，结果得到了22%的准确率。
- en: That's not so bad. Random guessing would yield 0.5%, so 22% is pretty good,
    and that's just from a handcrafted model starting from scratch that had to learn
    everything from those bird images. The reason I'm saying things like this is because
    the next thing we're going to do is extend a pre-trained model to get a good boost
    in accuracy.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这还算不错。随机猜测的准确率大约是0.5%，所以22%是相当不错的，而这仅仅是从一个从零开始的手工制作的模型，学习了所有的鸟类图片后得到的结果。我之所以这么说，是因为接下来我们要做的就是扩展一个预训练模型，以获得更高的准确度提升。
- en: 'This model was built by hand, but it would be even better to extend something
    such as `Inceptionv3`, which is shown here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是手工构建的，但要是扩展像`Inceptionv3`这样的模型会更好，如图所示：
- en: '![](img/00212.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00212.jpeg)'
- en: It's quite deep; it has a lot of convolutional layers and, like most CNNs, it
    ends with a fully-connected layer or perhaps multiple fully connected layers.
    The `Inceptionv3` model was designed for ImageNet. Well, it's the dataset, and
    there's competitions associated with it where there are millions of images and
    1,000 different classes, such as insects, houses, cars, and so on. The `Inceptionv3`
    model is state-of-the-art, or it was at one point. It was ImageNet's competition
    to combat other databases. We're going to use most of this network all the way
    up until the fully-connected layers. We don't want the final fully-connected or
    dense layers because those are designed for ImageNet. Specifically, there are
    1,000 outputs and that's not good for us. We don't need to recognize the ImageNet
    images. We do need to recognize our bird images however, and there's only 200
    different classes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型相当深，它有很多卷积层，和大多数CNN一样，它以一个全连接层结束，或者可能是多个全连接层。`Inceptionv3`模型是为ImageNet设计的。嗯，这是一个数据集，并且它有一些相关的竞赛，里面有数百万张图片，包含1,000个不同类别，比如昆虫、房屋、汽车等等。`Inceptionv3`模型是最先进的，或者至少曾经是。它是ImageNet的竞赛模型，用来与其他数据库竞争。我们将使用这个网络的大部分部分，一直到全连接层。我们不需要最后的全连接或密集层，因为这些是为ImageNet设计的。具体来说，ImageNet有1,000个输出，而这对我们来说并不合适。我们不需要识别ImageNet中的图像，但我们确实需要识别我们的鸟类图像，而且只有200个不同的类别。
- en: 'So, we just chop off the front of that and replace it with our own fully-connected
    layer, or multiple layers. We''re going to use all the convolutions that it learned,
    and all of the kernels that it learned based on those ImageNet images. Let''s
    go to the code. To do this, import `Inceptionv3` from Keras''s applications. There''s
    other models that you can choose from that Keras has available as well:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们只是去掉前面的部分，并用我们自己的全连接层或多个层替换它。我们将使用它学习到的所有卷积和卷积核，这些都是基于ImageNet图像学习得到的。接下来，我们来看看代码。为了实现这一点，从Keras的applications模块导入`Inceptionv3`。Keras还提供了其他可以选择的模型：
- en: '![](img/00213.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00213.jpeg)'
- en: We're going to use the data generator just like we did previously.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像之前一样使用数据生成器。
- en: 'This is where it starts to become different:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这时开始有所不同：
- en: '![](img/00214.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00214.jpeg)'
- en: First, load the `InceptionV3` model using the ImageNet weights. `include_top
    =False` means to drop off the dense fully connected layers at the top. That's
    what they call the top. That's where it finally produces 1,000 different outputs.
    We don't want that. We want just the convolutions. This would be called the `base_model`.
    Call `x`, which is the output of the base model, add a `GlobalAveragePooling`,
    which means that it's computing the average across the whole convolution, and
    then put in some dense layers, with 1,024 dense neurons and another layer of 200\.
    Of course, the 200 is because we have 200 different bird species, and the 1,024
    is just to learn how the convolutions can match the bird species and then produce
    a model with those layers. The input of the model is the input of `Inceptionv3`
    and the output is `out_layer = Dense(200, activation='softmax')(x)`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载`InceptionV3`模型，并使用ImageNet的权重。`include_top=False`意味着去掉顶部的密集全连接层。这就是所谓的顶部，它最终输出1000个不同的结果。我们不需要这个，我们只需要卷积层。这将被称为`base_model`。调用`x`，这是基础模型的输出，添加一个`GlobalAveragePooling`，它会计算整个卷积的平均值，然后加入一些全连接层，包含1024个神经元和另一个200个神经元的层。当然，200个神经元是因为我们有200个不同的鸟类物种，1024个神经元则是为了学习卷积如何匹配鸟类物种，然后生成包含这些层的模型。模型的输入是`Inceptionv3`的输入，输出是`out_layer
    = Dense(200, activation='softmax')(x)`。
- en: At this point, you can call regular model functions such as compile, but before
    we compile, we want to mark all of the base model layers and all of the convolutions
    as not trainable.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以调用常规的模型函数，比如`compile`，但在编译之前，我们希望将所有基础模型的层和所有卷积层标记为不可训练。
- en: 'We''re going to perform two steps here. When we attached our new two dense
    layers, the 1,024 dense and the 200 dense, those have random weights, so they''re
    pretty much useless so far. The convolutions have been learned on ImageNet, so
    they''re good. We don''t want to change the convolutions below all those kernels
    by training on our bird images until we get that new pair of dense layers in the
    right order. So, we''re first going to mark those layers from the inception model
    as not trainable; just keep those numbers as they are—we''re only going to train
    our two new layers:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行两个步骤。在我们添加了新的两层全连接层，即1024和200个神经元时，这些层的权重是随机的，所以它们现在几乎没有用。卷积层已经在ImageNet上进行了学习，所以它们已经很好了。我们不希望在训练我们的鸟类图片时更改这些卷积层，直到我们将那对新的全连接层排列正确。所以，我们首先将Inception模型中的那些层标记为不可训练；只保留那些卷积层的权重不变——我们只训练我们新添加的那两层：
- en: '![](img/00215.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00215.jpeg)'
- en: That happens next on the fit generator, just like before.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的操作是对生成器进行拟合，就像之前一样。
- en: 'We will do 100 epochs to start off:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始训练100个epoch：
- en: '![](img/00216.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00216.jpeg)'
- en: 'And we''ll do an evaluation:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行评估：
- en: '![](img/00217.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00217.jpeg)'
- en: So, now, we're up to 44% accuracy. So just by using the inception v3 weights
    and structure or ImageNet but replacing the top two layers with our own fully-connected
    network, we get a 20% boost from what we had with our own custom convolutional
    neural network. But we can do even better.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的准确率达到了44%。通过使用Inception v3的权重和结构，或者ImageNet，但将顶部的两层替换为我们自己的全连接网络，我们比之前自定义卷积神经网络的结果提高了20%。但我们可以做得更好。
- en: 'We can use what we just got so that the model has now trained the top two layers
    and marked everything as trainable:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们刚刚得到的模型，这样模型就能训练顶部的两层并将所有层标记为可训练：
- en: '![](img/00218.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00218.jpeg)'
- en: 'So, now that the top two layers are kind of massaged into a form that is reasonable,
    with 44% accuracy, we''re going to let the entire network update all of our bird
    images. We''re going to do it very slowly using stochastic gradient descent with
    a very slow learning rate and a high momentum. Going through 100 epochs, we now
    have 64%:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在顶层的两层已经调整成合理的形式，准确率为44%，我们将让整个网络更新我们所有的鸟类图像。我们将非常慢地使用随机梯度下降法（SGD），采用非常低的学习率和高动量。经过100轮训练后，我们的准确率达到了64%：
- en: '![](img/00219.jpeg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00219.jpeg)'
- en: So, we basically did a 20% boost each time. With the custom CNN, we got 22%
    accuracy just by starting from scratch. Now, of course, this is not as big of
    a network as the inception model, but it kind of shows what happens if you just
    start from scratch. Then, we started with inception, all the kernels, but then
    added our own random 2 layers on top, with random weights, trained those weights
    but did not change the kernels, and we got 44% accuracy. Finally, we went through
    and updated all the weights, kernels, and the top layer, and we got 64% accuracy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们基本上每次都提升了20%。使用自定义的CNN，我们从头开始就获得了22%的准确率。当然，这个网络没有`Inception`模型那么大，但它展示了从头开始时会发生什么。然后，我们从`Inception`开始，使用所有的卷积核，但在其上加了我们自己的两个随机层，随机权重，训练这些权重但不改变卷积核，最终得到了44%的准确率。最后，我们更新了所有的权重、卷积核和顶层，并获得了64%的准确率。
- en: 'So, this is far far better than what random guessing would be, which is 0.5%,
    and it''s been an increasing gain in accuracy each time we''ve improved the model.
    You can save the result and then you can load it into a separate file, perhaps
    by loading the model:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这比随机猜测要好得多，随机猜测的准确率是0.5%，而且每次我们改进模型时，准确率都有所提高。你可以保存结果，然后将其加载到一个单独的文件中，也许通过加载模型来实现：
- en: '![](img/00220.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00220.jpeg)'
- en: 'You also want to know what the class names are if you want to print the name
    of the bird to the user:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将鸟的名字打印给用户，你还需要知道类别名称：
- en: '![](img/00221.jpeg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00221.jpeg)'
- en: 'In this case, we can just list the subdirectories in a sorted form because
    that''s going to match the one -hot encoding, and we can define a function called
    `predict` where you give it a filename with an image in it and it loads that image.
    Make sure it resizes it and converts it into an array, divides it by 255, and
    then runs the predictor. All this was done for us before with the image generator:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以按排序的形式列出子目录，因为这将与独热编码匹配，我们可以定义一个名为`predict`的函数，给它一个包含图像的文件名，它会加载该图像。确保它会调整图像大小并转换为数组，将其除以255，然后运行预测器。所有这些之前都通过图像生成器为我们做过了：
- en: '![](img/00222.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00222.jpeg)'
- en: 'But now, because we''re doing this one at a time, we''re just going to do it
    by hand instead. Run the prediction, find out what the best score was, the position,
    and retrieve the class name and then print it, plus the confidence. There''s a
    couple of examples of just birds that I found on the internet:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在，由于我们是一次处理一个，我们将改为手动操作。运行预测，找出最佳得分和位置，获取类别名称并打印出来，再加上置信度。这里有几个例子是我在互联网上找到的鸟类：
- en: '![](img/00223.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00223.jpeg)'
- en: I can't guarantee that these were not part of the training set, but who knows.
    In the case of the hummingbird, they got it right. The house wren was also predicted
    correctly. However, the goose was not predicted correctly. This is an example
    of letting the user type in filenames. So if you have your own images that are
    relatively close to photography type images, you should consider using a pre-trained
    model like `InceptionV3` to get a major gain in accuracy.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我不能保证这些图片不是训练集的一部分，但谁知道呢。在蜂鸟的情况下，它预测正确了。房屋鹪鹩也被正确预测了。然而，大雁并没有被正确预测。这是一个允许用户输入文件名的例子。如果你有自己的图片，且这些图片与摄影类型的图像相似，你应该考虑使用像`InceptionV3`这样的预训练模型，以获得更大的准确率提升。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we discussed deep learning and CNNs. We practiced with convolutional
    neural networks and deep learning with two projects. First, we built a system
    that can read handwritten mathematical symbols and then revisited the bird species
    identifier form and changed the implementation to use a deep convolutional neural
    network that is significantly more accurate. This concludes the Python AI projects
    for beginners.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了深度学习和卷积神经网络（CNN）。我们通过两个项目练习了卷积神经网络和深度学习。首先，我们构建了一个可以读取手写数学符号的系统，然后重新审视了鸟类物种识别器，并将实现方式更改为使用深度卷积神经网络，这样的准确性显著提高。这也标志着Python人工智能初学者项目的结束。
