- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Machine Learning and Deep Learning Deep Dive
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和深度学习深入分析
- en: In the age of AI implementation, the current period of AI we find ourselves
    in, we must understand the pros and cons of both **machine learning** (**ML**)
    and **deep learning** (**DL**) in order to best navigate when to use either technology.
    Some other terms you might have come across with respect to AI/ML tools are **applied
    AI** or **deep tech**. As we’ve mentioned a few times over the course of this
    book, the underlying tech that will, for the most part, power AI products will
    be ML or DL. That’s because expert- or rule-based systems are slowly being powered
    by ML or not evolving at all. So, let’s dive a bit further into these technologies
    and understand how they differ.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI应用的时代，我们所处的AI现阶段，我们必须理解机器学习（**ML**）和深度学习（**DL**）的优缺点，以便最好地决定何时使用这两种技术。你可能还遇到过一些与AI/ML工具相关的其他术语，如**应用AI**或**深度技术**。正如我们在本书中多次提到的，未来大多数AI产品背后的核心技术将是ML或DL。这是因为基于专家或规则的系统正在慢慢被ML所取代，或者根本没有发展。所以，让我们更深入地探讨这些技术，并了解它们之间的区别。
- en: In this chapter, we will explore the relationship between ML and DL and the
    way in which they bring their own sets of expectations, explanations, and elucidations
    to builders and users alike. Whether you work with products that incorporate ML
    models that have been around since the 50s or use cutting-edge models that have
    sprung into use recently, you’ll want to understand the implications either way.
    Incorporating ML or DL into your product will have different repercussions. Most
    of the time when you see an AI label on a product, it’s built using ML or DL,
    so we want to make sure you come out of this chapter with a firm understanding
    of how these areas differ and what this difference will tangibly mean for your
    future products.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨机器学习（ML）和深度学习（DL）之间的关系，以及它们如何为构建者和用户带来各自的期望、解释和阐述。无论你是使用自50年代以来就存在的ML模型的产品，还是使用最近才投入使用的尖端模型，你都会想理解它们的影响。将ML或DL纳入你的产品将会产生不同的影响。大多数时候，当你看到一个产品上标有AI标签时，它是通过ML或DL构建的，因此我们希望确保你在本章结束时，能够清楚了解这些领域的区别，以及这些区别对你未来产品的实际意义。
- en: 'In [*Chapter 1*](B18935_01.xhtml#_idTextAnchor012), we discussed how we’ve
    grappled with the idea of using machines since the 50s, but we wanted to expand
    on the history of ML and DL **artificial neural networks** (**ANNs**) to give
    you a sense of how long these models have been around. In this chapter, we will
    cover the following topics to get more familiar with the nuances related to ML
    and DL:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B18935_01.xhtml#_idTextAnchor012)中，我们讨论了自50年代以来我们如何与使用机器的理念进行斗争，但我们希望扩展一下机器学习（ML）和深度学习（DL）**人工神经网络**（**ANNs**）的历史，让你了解这些模型已经存在多久。在本章中，我们将涵盖以下主题，帮助你更熟悉与ML和DL相关的细微差别：
- en: The old – exploring ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旧的——探索机器学习（ML）
- en: The new – exploring DL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的——探索深度学习（DL）
- en: Emerging technologies – ancillary and related tech
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新兴技术——辅助和相关技术
- en: Explainability – optimizing for ethics, caveats, and responsibility
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性——优化伦理、警告和责任
- en: Accuracy – optimizing for success
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性——优化成功
- en: The old – exploring ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旧的——探索机器学习（ML）
- en: ML models attempt to create some representation of reality in order to help
    us make some sort of data-driven decision. Essentially, we use mathematics to
    represent some phenomenon that’s happening in the real world. ML essentially takes
    mathematics and statistics to predict or classify some future state. The paths
    diverge in one of two ways. The first group lies with the emergence of models
    that continue to progress through statistical models and the second group lies
    with the emergence of models that try to mimic our own natural neural intelligence.
    Colloquially, these are referred to as traditional ML and DL models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型试图创建某种现实的表示，以帮助我们做出某种数据驱动的决策。实质上，我们使用数学来表示现实世界中发生的一些现象。机器学习本质上运用数学和统计学来预测或分类某种未来状态。路径的分歧有两种方式。第一组涉及通过统计模型不断发展的模型，第二组则是试图模仿我们自身自然神经智能的模型。通俗地说，这两类模型分别被称为传统机器学习（ML）和深度学习（DL）模型。
- en: You can think of all the models we covered in the *Model types – from linear
    regression to neural networks* section of [*Chapter 2*](B18935_02.xhtml#_idTextAnchor067)
    as ML models, but we didn’t cover ANNs in great depth. We’ll discuss those further
    in the *Types of neural networks* section later on in this chapter. In this section,
    we will take a look at the traditional statistical ML models in order to understand
    both the historical relevance and prevalence of ML models. To recap the flow of
    ML, it’s essentially a process of retrieving data, preparing that data through
    data processing, wrangling and feature engineering, running that data through
    a model and evaluating that model for performance, and tuning it as needed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把我们在[*第2章*](B18935_02.xhtml#_idTextAnchor067)“模型类型——从线性回归到神经网络”一节中讨论的所有模型都看作是机器学习（ML）模型，但我们并没有深入探讨人工神经网络（ANNs）。我们将在本章后面的“神经网络类型”一节中进一步讨论这些模型。在本节中，我们将着眼于传统的统计学机器学习模型，以便理解这些模型的历史意义和普遍性。回顾一下机器学习的流程，本质上是一个检索数据、通过数据处理、数据整理和特征工程准备数据、将数据输入模型并评估模型性能，然后根据需要对其进行调优的过程。
- en: Some of the most reliable and prevalent models used in ML have been around for
    ages. **Linear regression** models have been around since the late 1800s and were
    popularized through the work of Karl Pearson and Sir Francis Galton, two British
    mathematicians. Their contributions gave way to one of the most popular ML algorithms
    used today, although unfortunately, both were prominent eugenicists. Karl Pearson
    is also credited with inventing **principle component analysis** (**PCA**), an
    unsupervised learning method that reduces dimensions in a dataset, in 1901.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最可靠且普遍使用的机器学习模型已经存在了很长时间。**线性回归**模型自19世纪末期就开始使用，并通过两位英国数学家卡尔·皮尔逊和弗朗西斯·高尔顿的工作而得到普及。他们的贡献为今天最流行的机器学习算法之一奠定了基础，尽管不幸的是，两位数学家都是著名的优生学家。卡尔·皮尔逊还被认为是在1901年发明了**主成分分析**（**PCA**），这是一种无监督学习方法，用于减少数据集的维度。
- en: A popular ML method, **naive Bayes classifiers**, came onto the scene in the
    1960s but they’re based on the work of an English statistician named Thomas Bayes’
    and his theorem of conditional probabilities, which is from the 1700s. The logistic
    function was introduced by Belgian mathematician Pierre Francois Velhulst in the
    mid-1800s, and **logistic regression** models were popularized by a British statistician
    named David Cox in 1958.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的机器学习方法——**朴素贝叶斯分类器**，出现在20世纪60年代，但它基于18世纪英国统计学家托马斯·贝叶斯及其条件概率定理的研究。逻辑函数由比利时数学家皮埃尔·弗朗索瓦·维尔霍斯特在19世纪中叶引入，**逻辑回归**模型则是由英国统计学家大卫·考克斯在1958年推广的。
- en: '**Support vector machines** (**SVMs**) were introduced in 1963 by Soviet mathematicians
    Vladimir Vapnik and Alexey Chervonenis from the Institute of Control Sciences
    at the Russian Academy of Sciences. The first decision tree analytical algorithm
    was also invented in 1963 by American statisticians James N. Morgan and John A.
    Sonquist from the University of Michigan and it was used in their **automatic
    interaction detection** (**AID**) program, but even that was derived from a *Porphyrian
    tree*, a classification tree-based diagram that was created by the eponymous Greek
    philosopher in the 3rd century BCE. Random forests, made up of an ensemble of
    multiple decision trees, were invented by an American statistician, Leo Breiman,
    from the University of California in 2001.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）由苏联数学家弗拉基米尔·瓦普尼克和阿列克谢·切尔沃涅尼斯于1963年在俄罗斯科学院控制科学研究所提出。第一个决策树分析算法也在1963年由美国统计学家詹姆斯·N·摩根和约翰·A·桑奎斯特发明，他们来自密歇根大学，并应用于他们的**自动交互检测**（**AID**）程序，但即便如此，这一方法也源于*波尔菲里树*，这是一种由古希腊哲学家波尔菲里（Porphyry）在公元前三世纪创制的基于分类树的图表。随机森林是由多个决策树组成的集成方法，由加利福尼亚大学的美国统计学家利奥·布雷曼于2001年发明。'
- en: One of the simplest supervised learning models for classification and regression,
    the **KNN algorithm**, emerged from a technical analysis report that was done
    by statisticians Evelyn Fix and Joseph Lawson Hodges Jr. on behalf of the US Armed
    Forces in collaboration with Berkeley University in 1951\. K-means clustering,
    a method of unsupervised ML clustering, was first proposed by a mathematician
    at UCLA named James MacQueen in 1967.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**KNN算法**是最简单的监督学习模型之一，用于分类和回归。它源于1951年由统计学家Evelyn Fix和Joseph Lawson Hodges
    Jr.代表美国军方与伯克利大学合作完成的技术分析报告。K-means聚类是无监督机器学习聚类的一种方法，最早由UCLA的数学家James MacQueen于1967年提出。'
- en: As you can see, many of the algorithms that are used most commonly in ML models
    today have their roots quite far in our modern history. Their simplicity and elegance
    add to their relevance today. Most of the models we’ve covered in this section
    were covered in [*Chapter 2*](B18935_02.xhtml#_idTextAnchor067), with the exception
    of DL ANNs. In the following section, we will focus on DL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，今天在机器学习（ML）模型中最常用的许多算法，其根源可以追溯到我们现代历史的较早时期。它们的简洁性和优雅性使得它们今天仍然具有重要的相关性。本节中我们所讨论的大多数模型，除了深度学习的ANNs之外，都是在[*第二章*](B18935_02.xhtml#_idTextAnchor067)中提到过的。在接下来的章节中，我们将重点讨论深度学习。
- en: The new – exploring DL
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的——探索深度学习
- en: Part of our intention with separating ML and DL conceptually in this book is
    really to create associations in the reader’s mind. For most technical folks in
    the field, there are specific models and algorithms that come up when you see
    “ML” versus “DL” as a descriptor on a product. Quick reminder here that DL is
    a subset of ML. If you ever get confused by the two terms, just remember that
    DL is a form of ML that’s grown and evolved to form its own ecosystem. Our aim
    is to demystify that ecosystem as much as possible so that you can confidently
    understand the dynamics at play with DL products as a product manager.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中将机器学习（ML）和深度学习（DL）概念上区分开来的部分目的是帮助读者在脑海中形成联想。对于大多数领域的技术人员来说，当你看到“ML”与“DL”作为产品的描述时，会联想到一些特定的模型和算法。在这里简单提醒一下，深度学习（DL）是机器学习（ML）的一个子集。如果你对这两个术语感到困惑，只需记住，深度学习是机器学习的一种形式，它已经发展并演变成了自己的生态系统。我们的目标是尽可能地揭开这个生态系统的神秘面纱，让你作为产品经理能够自信地理解深度学习产品背后的动态。
- en: The foundational idea of DL is centered around our own biological neural networks
    and DL uses what’s often the umbrella term of ANNs to solve complex problems.
    As we will see in the next section, much of the ecosystem that’s been formed in
    DL has been inspired by our own brains, where the “original’’ neural networks
    are found. This inspiration comes not just from the function of the human brain,
    particularly the idea of learning through examples, but also from its structure
    as well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的基础思想围绕着我们自己的生物神经网络展开，DL使用的是通常被称为人工神经网络（ANNs）的概念来解决复杂问题。正如我们将在下一节中看到的，DL所形成的生态系统在很大程度上受到了我们大脑的启发，大脑就是“原始”神经网络的来源。这一灵感不仅来源于人脑的功能，尤其是通过示例学习的思想，还来源于其结构。
- en: 'Because this isn’t an overly technical book meant for DL engineers, we will
    refrain from going into the terms and mathematics associated with DL. A basic
    understanding of an ANN would be helpful, however. As we go through this section,
    keep in mind that a neural network is composed of artificial neurons or nodes
    and that these nodes are stacked next to one another in layers. Typically, there
    are three types of layers:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书并非专门为深度学习工程师编写的技术性书籍，我们将避免深入探讨与深度学习相关的术语和数学内容。然而，基本了解人工神经网络（ANN）会有所帮助。在这一节中，请牢记神经网络是由人工神经元或节点组成的，这些节点按层堆叠在一起。通常，有三种类型的层：
- en: The input layer
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: The hidden layer(s)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: The output layer
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: While we will go over the various types of ANNs, there are some basics to how
    these DL algorithms work. Think in terms of layers and nodes. Essentially, data
    is passed through each node of each layer and the basic idea is that there are
    weights and biases that are passed from each node and layer. The ANNs work through
    the data they’re training on in order to best arrive at patterns that will help
    them solve the problem at hand. An ANN that has at least three layers, which means
    an input, output, and a minimum of one hidden layer, is “deep” enough to be classed
    as a DL algorithm. That settles the layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将讨论各种类型的人工神经网络（ANNs），但有一些基本概念是这些深度学习算法工作的基础。可以把它们看作是层和节点的组合。基本的思路是，数据通过每一层的每一个节点传递，而这些节点和层之间会传递权重和偏差。人工神经网络通过其训练数据来识别模式，从而帮助它们解决当前的问题。一个至少有三层的人工神经网络（即输入层、输出层和至少一个隐藏层）就被称为“深度”，因此可以归类为深度学习算法。至此，层的部分已经解决。
- en: What about the nodes? If you recall, one of the simplest models we covered in
    prior chapters is the linear regression model. You can think of each node as its
    own mini-linear regression model because this is the calculation that’s happening
    within each node of an ANN. Each node has its data, a weight for that data, and
    a bias or parameter that it’s measuring against to arrive at an output. The summation
    of all these nodes making these calculations at scale gives you a sense of how
    an ANN works. If you can imagine a large scale of hundreds of layers, each with
    many nodes within each layer, you can start to imagine why it can be hard to understand
    why an ANN arrives at certain conclusions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，节点呢？如果你记得，我们在前几章中讲过的最简单的模型之一就是线性回归模型。你可以把每个节点看作是它自己的迷你线性回归模型，因为这正是每个人工神经网络（ANN）节点内发生的计算。每个节点都有自己的数据、一个数据的权重和一个它用来得出输出的偏差或参数。所有这些节点在大规模进行这些计算时的总和，让你能够理解人工神经网络是如何工作的。如果你能想象成千上万层、每层内有许多节点的庞大规模，你就能开始理解为什么人工神经网络得出某些结论时，可能很难理解其原因。
- en: DL is often referred to as a black-box technology and this starts to get to
    the heart of why that is. Depending on our math skills, we humans can explain
    why a certain error rate or loss function is present in a simple linear regression
    model. We can conceptualize the ways a model, which is being fitted to a curve,
    can be wrong. We can also appreciate the challenge when presented with real-world
    data, which doesn’t lay out a perfect curve, for a model. But if we increase that
    scale and try to conceptualize potentially billions of nodes each representing
    a linear regression model, our brains will start to hurt.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常被称为“黑箱”技术，而这也正好触及了为什么它被这样称呼的核心。根据我们的数学技能，我们人类可以解释为什么在一个简单的线性回归模型中会存在某个错误率或损失函数。我们能够概念化模型如何在拟合曲线时出错。当面对真实世界的数据时，这些数据并没有呈现出完美的曲线，这时我们也能理解其挑战。但如果我们扩大规模，尝试想象可能有数十亿个节点，每个节点代表一个线性回归模型，那么我们的头脑就开始感到痛苦了。
- en: Though DL is often discussed as a bleeding-edge technological advancement, as
    we saw in the prior section, this journey also started long ago.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习（DL）常被讨论为前沿技术突破，但正如我们在前一节中看到的，这个旅程早在很久以前就已开始。
- en: Invisible influences
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 看不见的影响
- en: It’s important to understand the underlying relationships that have influenced
    ML and DL as well as the history associated with both. This is a foundational
    part of the storytelling but it’s also helpful to better understand how this technology
    relates to the world around us. For many, understanding AI/ML concepts can be
    mystifying and unless you come from a tech or computer science background, the
    topics themselves can seem intimidating. Many will, at best, only acquire a rudimentary
    understanding of what this tech is and how it’s come about.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 理解影响机器学习（ML）和深度学习（DL）的潜在关系以及与之相关的历史非常重要。这是故事叙述的基础部分，但它也有助于更好地理解这项技术如何与我们周围的世界相联系。对于许多人来说，理解人工智能/机器学习概念可能会让人困惑，除非你来自技术或计算机科学背景，否则这些话题本身可能会显得令人生畏。即使是最好的情况，许多人也只能获得这项技术的基本理解，以及它是如何发展的。
- en: We want to empower anyone interested in exploring this underlying tech that
    will shape so many products and internal systems in the future by making a deeper
    understanding more accessible. Already, there’s favoritism going on. Most of the
    folks that intimately understand ML and DL already come from a computer science
    background whether it’s through formal education or through boot camps and other
    technical training programs. That means that for the most part, the folks that
    have pursued study and entrepreneurship in this field have traditionally been
    predominantly white and predominantly male.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过让更深入的理解变得更加易于接触，来赋能那些有兴趣探索这项将在未来塑造许多产品和内部系统的基础技术的人。目前，已有偏见存在。大多数深入理解机器学习和深度学习的人，都是来自计算机科学背景的，无论是通过正规教育，还是通过训练营和其他技术培训项目。这意味着，在很大程度上，那些在这一领域进行研究和创业的人，传统上主要是白人且以男性为主。
- en: Apart from the demographics, the level of investment in these technologies,
    from an academic perspective, has gone up. Let’s get into some of the numbers.
    Stanford University’s AI index states that AI investment at the graduate level
    among the world’s top universities has increased by 41.7%. That number jumps to
    102.9% at the undergraduate level. An extra 48% of recipients of AI PhDs have
    left academia in the past decade in pursuit of the private sector’s big bucks
    in the last 10 years. 10 years ago, only 14.2% of computer science PhDs were AI
    related. Now, that number is above 23%. The United States, in particular, is holding
    onto the talent it educates and attracts. Foreign students that come to the US
    to pursue an AI PhD stay at a rate of 81.8%.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了人口统计因素外，从学术角度来看，对这些技术的投资水平也在上升。我们来看一些数据。斯坦福大学的人工智能指数显示，世界顶尖大学的研究生阶段人工智能投资增长了41.7%。这个数字在本科阶段跳升至102.9%。过去十年中，额外的48%的人工智能博士获得者已离开学术界，转向私营部门追逐丰厚的薪水。10年前，只有14.2%的计算机科学博士与人工智能相关。现在，这一数字已超过23%。特别是美国，正在留住其培养和吸引的人才。来美国攻读人工智能博士学位的外国学生，留下的比例为81.8%。
- en: The picture this paints is one of a world that’s in great need of talent and
    skill in AI/ML. This high demand for the AI/ML skill set, particularly a demographically
    diverse AI skill set, is making it hard for those that have the hard skills in
    this field to stay in academia and the private sector handsomely rewards those
    that have these skills. In the start-up circuits, many VCs and investors are able
    to confidently solidify their investments when they know a company has somebody
    with an AI PhD, on staff, whether or not their product needs this heavy expertise.
    Placing a premium on human resources with these sought-after skills is likely
    not going to go away anytime soon.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅画面描绘的是一个世界，它迫切需要人工智能/机器学习领域的人才和技能。对人工智能/机器学习技能集的高需求，特别是一个人口多样化的人工智能技能集，正在使得那些在这一领域拥有硬技能的人很难留在学术界，而私营部门对拥有这些技能的人给予丰厚的回报。在初创公司中，许多风险投资公司和投资者能够自信地巩固他们的投资，因为他们知道某公司有一位拥有人工智能博士学位的员工，不论该公司的产品是否需要这种高深的专业知识。对具备这些抢手技能的人力资源的溢价，可能在短期内不会消失。
- en: We dream of a world where people from many competencies and backgrounds come
    into the field of AI because diversity is urgently needed and the opportunity
    that’s ahead of us is too great for the gatekeeping that’s been going on to prevail.
    It’s not just important that the builders of AI understand the underlying tech
    and what makes its application of it so powerful. It’s equally important for the
    business stakeholders that harness the capabilities of this tech to also understand
    the options and capabilities that lie before them. At the end of the day, nothing
    is so complicated that it can’t be easily explained.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们梦想着一个来自不同背景和能力的人们进入人工智能领域的世界，因为多样性迫切需要，而且我们面前的机会太大，以至于目前的门槛阻碍无法继续存在下去。人工智能的建设者了解其基础技术以及应用这些技术的强大力量非常重要。对于那些利用这一技术能力的商业利益相关者来说，了解眼前的选项和能力同样重要。归根结底，没有什么是如此复杂，以至于无法轻松解释。
- en: A brief history of DL
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的简史
- en: 'In 1943, Warren S. McCulloch and Walter Pitts published a paper, *A logical
    calculus of the ideas immanent in nervous activity*, which made a link between
    mathematics and neurology by creating a computer model based on the neural networks
    inherent in our own brains based on a combination of algorithms to create a “threshold”
    to mimic how we pass information from our own biological network of neurons. Then,
    in 1958, Frank Rosenblatt published a paper that would be widely considered the
    ancestor of neural nets, called *The Perceptron: A perceiving and recognizing
    automaton*. This was, for all intents and purposes, the first, simplest, and oldest
    ANN.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1943年，沃伦·S·麦卡洛克和沃尔特·皮茨发表了一篇论文《神经活动中固有思想的逻辑演算》，通过创建一个基于我们大脑内神经网络的计算机模型，利用一系列算法结合“阈值”来模拟我们如何在生物神经网络中传递信息，建立了数学和神经学之间的联系。然后，1958年，弗兰克·罗森布拉特发表了一篇论文，广泛被认为是神经网络的祖先，名为《感知器：一个感知和识别的自动机》。就所有意图和目的而言，这是第一个、最简单、最古老的人工神经网络。
- en: In the 1960s, developments toward backpropagation, or the idea that a model
    learns from layers of past mistakes as it trains its way through a dataset, made
    significant strides toward what would eventually make up the neural network. The
    most significant part of the development that was happening at this time was coupling
    the idea of inspiring mathematical models with the way the brain works based on
    networks of neurons and backpropagation because this created the foundation of
    ANNs, which learned through past iterations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在1960年代，向反向传播的进展，或者说模型在训练数据集的过程中从过去的错误中学习的想法，朝着最终构成神经网络的方向迈出了重要步伐。当时发展的最重要部分是将启发式数学模型的思想与大脑基于神经元网络和反向传播的工作方式相结合，因为这为人工神经网络（ANN）奠定了基础，后者通过过去的迭代进行学习。
- en: It’s important to note here that many ANNs work in a “feedforward” motion in
    that they go through the input, hidden layers, and output layers sequentially
    and in one direction only, from input to output. The idea of backpropagation essentially
    allows for the ANNs to learn bi-directionally so that they’re able to minimize
    the error in each node, resulting in a better performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意的是，许多人工神经网络是以“前馈”的方式工作的，即它们依次经过输入层、隐藏层和输出层，且只能单向传播，从输入到输出。反向传播的思想基本上使得人工神经网络能够双向学习，从而能够在每个节点最小化误差，最终提高性能。
- en: It wasn’t until 1986 when David Rumelhart, Geoffrey Hinton, and Ronald Williams
    published a famous paper, *Learning representations by back-propagating errors*,
    that people fully began to appreciate the role backpropagation plays in the success
    of DL. The idea that you could backpropagate through time, allowing neural networks
    to assign the appropriate weights as well as train a neural network with hidden
    layers, was revolutionary at the time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 直到1986年，David Rumelhart、Geoffrey Hinton和Ronald Williams发表了著名论文《*通过反向传播误差进行学习表示*》，人们才完全开始理解反向传播在深度学习成功中的作用。你可以通过反向传播跨越时间，这让神经网络能够分配适当的权重，并且训练具有隐藏层的神经网络，这在当时是革命性的。
- en: 'After each development, there was much excitement for ML and the power of neural
    networks but between the mid-60s and the 80s, there was one significant issue:
    a lack of data as well as funding. If you’ve heard the term “AI winter,” this
    is what it’s referring to. Developments were made on the modeling side but we
    didn’t have significant ways to apply the models that were being developed without
    the ability and willingness of research groups to get their hands on enough data
    to feed those models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每次发展后，机器学习和神经网络的潜力都引起了很多兴奋，但在60年代中期到80年代之间，有一个重大问题：缺乏数据和资金。如果你听过“人工智能寒冬”这个术语，这正是它所指的。尽管在建模方面取得了进展，但没有足够的数据来为正在开发的模型提供支持，也缺乏研究团队愿意收集这些数据的动力，这导致了模型的应用面临巨大挑战。
- en: Then, in 1997, Sepp Hochreiter and Jürgen Schmidhuber published their groundbreaking
    work titled *Long Short-Term Memory*, which effectively allowed DL to "solve complex,
    artificial long-time lag tasks that had never been solved by previous recurrent
    network algorithms." The reason why this development was so important was it allowed
    the idea of sequences to remain relevant for DL problems. Because neural networks
    involve hidden layers, it’s difficult for the notion of time to remain relevant,
    which makes a number of problems hard to solve. For instance, a traditional recurrent
    neural network might not be able to autocomplete a sentence in the way that a
    **Long short-term memory (LSTM)** can because it doesn’t understand the time sequence
    involved in the completion of a sentence.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在1997年，Sepp Hochreiter和Jürgen Schmidhuber发表了他们开创性的工作《*长短时记忆网络*》，该论文有效地使得深度学习能够“解决以往递归网络算法无法解决的复杂人工长时间滞后任务”。这一发展的重要性在于它让序列的概念在深度学习问题中依然具有相关性。由于神经网络涉及隐藏层，时间的概念难以保持相关，这使得许多问题难以解决。例如，传统的递归神经网络可能无法像**长短时记忆网络（LSTM）**那样完成句子的自动补全，因为它无法理解完成句子所涉及的时间序列。
- en: Today, most DL models require a ton of supervised datasets, meaning the neural
    networks that power DL need lots of examples to understand whether something is,
    for example, a dog or a horse. If you think about it a bit though, this doesn’t
    actually relate that closely to how our brains work. A small child that’s just
    emerging and learning about the world might need to be reminded once or twice
    about the difference between a dog and a horse, but you likely aren’t reminding
    them of that difference thousands or millions of times.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，大多数深度学习（DL）模型需要大量的监督数据集，这意味着驱动DL的神经网络需要大量的示例来理解某物是否是，比如说，一只狗或一匹马。然而，如果你稍微思考一下，这与我们大脑的工作方式并没有那么紧密的关系。一个刚刚开始学习世界的小孩可能需要一两次提醒关于狗和马的区别，但你大概不会反复提醒他们这一区别成千上万次。
- en: In that sense, DL is evolving towards requiring fewer and fewer examples to
    learn. If you recall, in previous chapters, we went over supervised and unsupervised
    learning techniques, this becomes significant in the case of DL. Sure, these days
    we’re able to gather massive amounts of data for DL models to learn from but the
    models themselves are evolving to improve without needing much data toward the
    ultimate goal of unsupervised DL that can be trained with small amounts of data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，深度学习正在朝着需要越来越少的示例来学习的方向发展。如果你记得之前的章节，我们介绍了监督学习和无监督学习技术，这在深度学习中变得尤为重要。没错，现如今我们能够收集大量数据供深度学习模型学习，但这些模型本身正在进化，在不需要大量数据的情况下提升，最终目标是无监督深度学习，它能够通过少量数据进行训练。
- en: So far, we’ve covered some of the histories and influences shaping the field
    of ML and DL more specifically. While we haven’t gone into many of the technical
    concepts too heavily, this gives us a good foundation with which to understand
    how ML and DL have developed over time and why they’ve risen to prominence. In
    the following section, we will get more hands-on and get into the specific algorithms
    and neural networks that are used most heavily in DL.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了一些塑造机器学习（ML）和深度学习（DL）领域的历史和影响。虽然我们并没有过多深入一些技术概念，但这为我们理解机器学习和深度学习是如何随着时间的发展以及为什么它们崛起为主流提供了良好的基础。在接下来的部分，我们将更为实际，深入探讨在深度学习中最常用的具体算法和神经网络。
- en: Types of neural networks
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的类型
- en: We’d like to now turn your attention toward some of the most popular kinds of
    neural networks used in DL today. Some of these will sound familiar based on the
    previous section, but it will help to familiarize yourself with some of these
    concepts especially if you plan on working as a product manager for a DL product.
    Even if you aren’t currently working in this capacity, you’ll want to take a look
    through these in case your career does veer toward DL products in the future.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想将你的注意力转向今天在深度学习中最受欢迎的一些神经网络。基于上一部分的内容，其中一些可能听起来很熟悉，但熟悉这些概念将对你特别有帮助，尤其是如果你计划作为深度学习产品的产品经理工作。即便你现在不在这一岗位，也值得了解一下这些内容，以防你的职业未来转向深度学习产品。
- en: 'The following is a list of some of the most used ANNs in DL:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些在深度学习中最常用的人工神经网络（ANN）列表：
- en: '**Multilayer** **perceptrons** (**MLPs**)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层** **感知器**（**MLPs**）'
- en: '**Radial basis function** **networks** (**RBFNs**)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**径向基函数** **网络**（**RBFNs**）'
- en: '**Convolutional neural** **networks** (**CNNs**)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经** **网络**（**CNNs**）'
- en: '**Recurrent neural** **networks** (**RNNs**)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归神经** **网络**（**RNNs**）'
- en: '**Long short-term memory** **networks** (**LSTMs**)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆** **网络**（**LSTMs**）'
- en: '**Generative adversarial** **networks** (**GANs**)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗** **网络**（**GANs**）'
- en: '**Self-organizing** **maps** (**SOMs**)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自组织** **映射**（**SOMs**）'
- en: '**Deep belief** **networks** (**DBNs**)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度置信** **网络**（**DBNs**）'
- en: In the following section, we will touch on these various neural networks to
    give you an idea of what they are best suited for. As we did in the previous chapter
    with ML algorithms, we will describe some of the most popular use cases of each
    type of ANN so that we can understand, at least in a general sense, what some
    of the core competencies of each ANN are so that you can start to keep those ideas
    in mind should you pursue the creation of your own DL products in the future.
    If your aim is to specialize exclusively in supporting or building DL products
    of your own, this will be a great summary overview of each ANN.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论这些不同的神经网络，帮助你了解它们最适合什么样的任务。就像在上一章中讨论机器学习算法一样，我们将描述每种人工神经网络最常见的一些应用场景，以便我们至少大致了解每种ANN的核心竞争力，这样在未来你如果想创建自己的深度学习产品时，可以将这些概念纳入考虑。如果你的目标是专门支持或构建自己的深度学习产品，这将是每种人工神经网络的一个很好的概览总结。
- en: MLPs
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPs
- en: After David Rumelhart, Geoffrey Hinton, and Ronald Williams’s paper *Learning
    representations by back-propagating errors* came out in 1986, MLPs were popularized
    because in that paper they used backpropagation to train an MLP. Unlike RNNs,
    MLPs are another form of feedforward neural network that uses backpropagation
    to optimize the weights. For this reason, you can think of MLPs as some of the
    most basic forms of ANNs because they were among the first to appear and today
    they’re still used often to deal with the high compute power that’s needed by
    some of the newer ANNs out there. Their accessibility and reliability are still
    useful today, which is why we wanted to start this list with MLPs to give us a
    good foundation for conceptualizing the rest of the DL algorithms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在1986年，大卫·鲁梅哈特、杰弗里·辛顿和罗纳德·威廉姆斯的论文《通过反向传播错误来学习表示》发布后，MLPs得到了广泛推广，因为在那篇论文中，他们使用反向传播来训练MLP。与RNNs不同，MLPs是另一种前馈神经网络，使用反向传播来优化权重。因此，你可以将MLPs视为一些最基础的人工神经网络形式，因为它们是最早出现的，并且今天仍然常用于处理一些更新的人工神经网络所需的高计算能力。它们的易用性和可靠性至今仍然很有价值，这也是我们希望从MLPs开始列出这些算法的原因，它为我们构建其他深度学习算法提供了一个良好的基础。
- en: The way they learn is the algorithm will send data forward through the input
    and middle layers to the output layer. Then, based on the results in the output
    layer, it will then calculate the error to assess how off it was at predicting
    values. This is where backpropagation comes in because it will get a sense of
    how wrong it was in order to then backpropagate the rate of error. It will then
    optimize itself to minimize that error by adjusting the weights in the network
    and will effectively update itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的学习方式是，算法将数据从输入层通过中间层传递到输出层。然后，根据输出层的结果，它会计算误差，评估预测值的偏差程度。这时，反向传播起作用了，它会评估预测有多么错误，从而反向传播误差率。接着，它会通过调整网络中的权重来优化自己，从而有效地更新自身。
- en: The idea is you would pass these steps through the model multiple times until
    you were satisfied with the performance. Remember the distinction between supervised
    and unsupervised learning in [*Chapter 1*](B18935_01.xhtml#_idTextAnchor012)?
    Because MLPs use backpropagation to minimize their error rate by adjusting weights,
    MLPs are a supervised DL algorithm because they know based on our label data exactly
    how off they were from being right. These algorithms are also heavily used in
    ensembles with other ANNs as a final polishing stage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，你会将这些步骤多次通过模型，直到对其性能满意为止。还记得在[*第1章*](B18935_01.xhtml#_idTextAnchor012)中提到的监督学习与无监督学习的区别吗？因为MLPs通过反向传播来最小化错误率，调整权重，所以MLPs是一种监督式深度学习算法，因为它们知道根据我们的标签数据，自己与正确答案的差距到底有多远。这些算法也常常与其他人工神经网络一起使用，作为最终的优化阶段。
- en: RBFNs
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RBFNs
- en: 'RBFNs came on the scene in 1988 with D.S. Broomhead and David Lowe’s paper
    *Multivariable Functional Interpolation and Adaptive Networks*. RBFNs differ from
    most other ANNs we will cover in this chapter in that they only have three layers.
    While most ANNs, including the MLPs we discussed in the preceding section, will
    have an input and output layer with several hidden layers in between, RBFNs only
    have one hidden layer. Another key difference is rather than having the input
    layer be a computational layer, the input layer only passes data to the hidden
    layer in RBFNs, so this ANN is incredibly fast. These DL algorithms are feedforward
    models, so they are computationally only really passing through two layers: the
    hidden layer and the output layer.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RBFNs（径向基函数网络）出现在1988年，源自D.S. Broomhead和David Lowe的论文《*多变量函数插值与自适应网络*》。RBFNs与本章将讨论的其他大多数ANN（人工神经网络）不同，它们只有三层。大多数ANN，包括我们在前一节讨论的MLP（多层感知器），通常具有输入层和输出层，中间有几个隐藏层，而RBFNs只有一层隐藏层。另一个关键区别是，RBFNs的输入层并不是计算层，而仅仅是将数据传递到隐藏层，因此这种ANN非常快速。这些深度学习算法是前馈模型，因此它们实际上只经过两层计算：隐藏层和输出层。
- en: It would be helpful to think of these networks as similar to the KNN algorithm
    we discussed in the previous chapter, which aims to predict data points based
    on the data points around the value they’re trying to predict. The reason for
    this is RBFNs look to approximate values based on the distance, radius, or Euclidean
    distance between points and they will cluster or group data in circles or spheres
    to better make sense of a complex multivariable dataset similar to how a K-means
    clustering algorithm from [*Chapter 1*](B18935_01.xhtml#_idTextAnchor012) would.
    This is a highly versatile algorithm that can be used with both classification
    and regression problems in both supervised and unsupervised ways.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些网络类比为我们在上一章讨论的KNN（K近邻）算法可能会有帮助，KNN算法旨在根据数据点周围的其他数据点来预测它们的值。之所以这样类比，是因为RBFNs通过距离、半径或欧几里得距离来逼近值，并且它们会将数据聚集或分组为圆形或球形，以更好地理解复杂的多变量数据集，这类似于[*第1章*](B18935_01.xhtml#_idTextAnchor012)中的K-means聚类算法的工作方式。这是一个非常灵活的算法，可以用于分类和回归问题，并且可以在监督和无监督的情况下使用。
- en: SOMs
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SOMs
- en: SOMs were introduced in the 1980s by Tuevo Hohonen and are another example of
    unsupervised competitive learning ANNs in which the algorithm takes a multivariable
    dataset and reduces it into a two-dimensional “map.” Each node will compete with
    the others to decide whether it’s the one that should be activated, so it’s essentially
    just a massive competition, which is how it self-organizes. Structurally though,
    SOMs are very different from most ANNs. There’s really just one layer or node
    outside of the input layer, which is called the Kohonen layer. The nodes themselves
    are also not connected the way they are in more traditional ANNs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs（自组织映射）由Tuevo Hohonen在1980年代提出，是另一种无监督竞争学习的ANN示例，其中算法将多变量数据集压缩成一个二维“地图”。每个节点将与其他节点竞争，以决定是否应该激活它，因此这本质上是一场大规模的竞争，通过这种方式它实现了自组织。不过，从结构上来看，SOMs与大多数ANN非常不同。除了输入层之外，实际上只有一层或一个节点，称为Kohonen层。节点本身也不像传统的ANN那样连接。
- en: The training of a SOM mirrors our own brain’s ability to self-organize and map
    inputs. When we sense certain inputs, our brain organizes those inputs into certain
    areas that are apt for what we’re seeing, hearing, feeling, smelling, or tasting.
    The SOM will similarly cluster data points into certain groupings. The way that
    happens is through a learning/training process where the algorithm sends out the
    data through the input layer and weights, randomly selecting input data points
    to test against the nodes until a node is chosen based on the distance between
    it and the data point, which then updates the weight of the node. This process
    is repeated until the training set is complete and the optimal nodes have been
    selected.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SOM的训练过程类似于我们大脑自我组织和映射输入的能力。当我们感知到某些输入时，大脑会将这些输入组织到特定区域，这些区域适合我们所看到、听到、感觉到、闻到或尝到的东西。SOM也会将数据点聚类到特定的分组中。其过程通过学习/训练进行，算法将数据通过输入层和权重发送，随机选择输入数据点进行测试，直到根据节点与数据点之间的距离选择一个节点，然后更新该节点的权重。这个过程会反复进行，直到训练集完成，最优节点被选中。
- en: SOMs will also be in the same class of clustering algorithms such as K-means,
    or the RBFNs we touched on in the preceding section, in that they are useful for
    finding relationships and groupings in datasets that are unlabeled or undiscovered.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs 也属于同类聚类算法，例如 K-means 或我们在前面部分提到的 RBFN，因为它们在寻找数据集中未标记或未发现的关系和分组时非常有用。
- en: CNNs
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNNs
- en: CNNs, sometimes referred to as ConvNets, have multiple layers that are used
    largely for supervised learning use cases in which they detect objects, process
    images, and detect anomalies in medical and satellite images. The way this ANN
    works is through a feedforward, so it starts from the input layer and makes its
    way through the hidden layers to the ultimate output layer to categorize images.
    This type of ANN is characterized as categorical, so its ultimate goal is looking
    to put images into buckets of categories. Then, once they are categorized, it
    looks to group images by the similarities they share so that it can ultimately
    perform the object recognition that’s used to detect faces, animals, plants, or
    signs on the street. CNNs can be used for things such as facial recognition, object
    identification, and self-driving cars or what’s commonly referred to as computer
    vision applications of AI.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs，有时被称为 ConvNets，具有多个层，主要用于监督学习应用场景，在这些场景中，它们检测物体、处理图像，并检测医学和卫星图像中的异常。这个人工神经网络（ANN）的工作方式是通过前馈神经网络，因此它从输入层开始，经过隐藏层，最终到达输出层来对图像进行分类。这种类型的
    ANN 被称为分类型 ANN，因此它的最终目标是将图像归类到不同的类别中。然后，一旦图像被分类，它会根据共享的相似性对图像进行分组，从而最终执行物体识别，这种技术用于检测人脸、动物、植物或街头的标志。CNNs
    可以用于面部识别、物体识别、自动驾驶汽车或通常所说的 AI 计算机视觉应用。
- en: 'The four important layers in CNNs are as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 中的四个重要层如下：
- en: The convolution layer
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: The **rectified linear unit** (**ReLU**) layer
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）层'
- en: The pooling layer
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: The fully connected layer
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: 'The convolution layer turns an image into a matrix of pixel values that are
    0s and 1s and then further reduces that matrix into a smaller matrix that’s a
    derivative from the first. The ReLU layer effectively pares down the dimensions
    of the image that you pass to the CNN. Even color images are passed through a
    grayscale when they’re originally assigned 0s and 1s. So, in the ReLU stage, the
    CNN actually gets rid of black pixels from the image so that it can reduce the
    image further and make it computationally easier for the model to process it.
    There’s another layer that reduces the dimensions of the image in another way:
    the pooling layer.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层将图像转换为一个由 0 和 1 组成的像素值矩阵，然后将该矩阵进一步简化为一个从第一个矩阵衍生出的更小矩阵。ReLU 层有效地减少了你传递给 CNN
    的图像的维度。即使是彩色图像，在最初分配为 0 和 1 时也会被转换为灰度图像。因此，在 ReLU 阶段，CNN 实际上会去除图像中的黑色像素，以便进一步缩小图像，使其对模型处理来说计算更简便。还有一个层通过另一种方式减少图像的维度：池化层。
- en: While the ReLU layer pares down the gradient in the image itself, the pooling
    layer pares down the features of the image, so if we pass the CNN an image of
    a cat, the pooling layer is where we will see various features such as the ears,
    eyes, nose, and whiskers identified. You can think of the convolution, ReLU, and
    pooling layers as operations that take segments of each image you feed your model
    and concurrently fire the outputs of those prior steps as inputs into the fully
    connected layer, which is what actually passes through the neural network itself
    to classify the image. In essence, the convolution, ReLU, and pooling layers prepare
    the image to pass through the neural network to arrive at a conclusion.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ReLU 层减少图像本身的梯度时，池化层减少图像的特征，因此，如果我们将一张猫的图像传递给 CNN，池化层将识别出耳朵、眼睛、鼻子和胡须等特征。你可以将卷积、ReLU
    和池化层看作是对输入到模型的每个图像的部分进行操作，并同时将这些步骤的输出作为输入传递给全连接层，后者实际上将图像传递通过神经网络进行分类。实质上，卷积、ReLU
    和池化层的作用是为图像通过神经网络准备，从而得出结论。
- en: RNNs
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNNs
- en: There are several operations that feedforward neural networks weren’t able to
    do very well, including working with sequential data that is rooted in time, operations
    that needed to contextualize multiple inputs (not just the current input), and
    operations that require memorization from previous inputs. For these reasons,
    the main draw of RNNs is the internal memory they possess that allows them to
    perform and remember the kind of robust operations required of conversational
    AIs such as Apple’s Siri. RNNs do well with sequential data and place a premium
    on the context in order to excel at working with time-series data, DNA and genomics
    data, speech recognition, and speech-to-text functions. In contrast to the preceding
    CNN example, which works with a feedforward function, RNNs work in loops.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些操作是前馈神经网络无法很好完成的，包括处理基于时间的序列数据、需要对多个输入进行上下文处理（不仅仅是当前输入）的操作，以及需要从之前的输入中进行记忆的操作。正因为如此，RNN
    的主要优势在于其内部记忆，使其能够执行并记住对会话型 AI（如苹果的 Siri）要求的强大操作。RNN 在处理序列数据方面表现优秀，并且非常注重上下文，以便在处理时间序列数据、DNA
    和基因组数据、语音识别以及语音转文本等任务时发挥出色。与之前处理前馈功能的 CNN 示例相比，RNN 在循环中工作。
- en: Rather than the motion going from the input layer, through the hidden layers,
    and ultimately to the output layer, the RNN cycles through a loop back and forth
    and this is how it retains its short-term memory. This means the data passes through
    the input layer, then loops through the hidden layers, before it ultimately passes
    to the output layer. It’s important to note that RNNs only have short-term memory,
    which is why there was a need for an LSTM network. More on that in the next section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与其说运动从输入层经过隐藏层，最终到达输出层，不如说 RNN 在循环中来回传递，这就是它保持短期记忆的方式。这意味着数据首先通过输入层，然后在隐藏层中循环，最后传递到输出层。需要注意的是，RNN
    只有短期记忆，这也是为什么需要 LSTM 网络的原因。更多内容将在下一节中介绍。
- en: In essence, the RNN actually has two inputs. The first is the initial data that
    makes its way through the neural network and the second is actually the information
    and context it’s acquired along the way. This is the framework with which it also
    effectively alters its own weights to current and previous inputs, so it’s course-correcting
    as it goes through its loops. This process of retroactively adjusting weights
    to minimize its error rate is known as backpropagation, which you’ll recall from
    the previous section (*A brief history of DL*) as this was a major advancement
    that has helped DL become so successful.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，RNN 实际上有两个输入。第一个是初始数据，它通过神经网络传递；第二个实际上是它在此过程中获取的信息和上下文。这就是它通过该框架有效地调整当前和先前输入的权重，从而在经过循环时进行修正的方式。这一过程即反向传播，它通过回顾上一节中的内容（*深度学习简史*），你会记得这是深度学习取得成功的一个重要进展。
- en: It’s helpful to imagine that an RNN is actually a collection of neural networks
    that are continuously retrained and optimized for accuracy through backpropagation,
    which is why it’s also considered a supervised learning algorithm. Because it’s
    such a robust and powerful DL algorithm, we can see RNNs used for anything from
    captioning and understanding images to predicting time-series problems to natural
    language processing and machine translation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 RNN 想象成一个由神经网络组成的集合，这些神经网络通过反向传播不断被重新训练和优化，以提高准确性，这也是它被认为是一种监督学习算法的原因。由于它是如此强大和有效的深度学习算法，我们可以看到
    RNN 被广泛应用于从图像字幕生成和理解，到时间序列预测，再到自然语言处理和机器翻译等各个领域。
- en: LSTMs
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM
- en: LSTMs are basically RNNs with more memory power. Often, the way they manifest
    is through networks of the LSTM because what they do is actually connect layers
    of RNNs, which is what allows them to retain inputs over lags or longer periods
    of time. Much like a computer, LSTMs can write, read, or delete data from the
    memory it possesses. Because of this, it has the ability to learn about what data
    it needs to hold onto over time. Just as RNNs continuously adjust their weights
    and optimize for performance, LSTMs do the same thing by assigning levels of importance
    for what data to store or delete through its own weights.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 本质上是具有更多记忆能力的 RNN。通常，它们的表现方式是通过 LSTM 网络，因为它们实际上将 RNN 层连接起来，这使得它们能够在较长时间的延迟或更长时间段内保持输入。就像计算机一样，LSTM
    可以读写或删除它所拥有的记忆数据。因此，它能够学习哪些数据需要长时间保留。正如 RNN 不断调整其权重并优化性能一样，LSTM 也通过为要存储或删除的数据分配重要性来进行相同的操作，权重值就是它们的依据。
- en: LSTMs mirror our own ability to discard irrelevant or trivial information through
    time through LSTM cells, which have the ability to let the information come in
    as an input, be forgotten or excluded completely, or let it pass to influence
    the output. These categorizations are referred to as gates and they’re what allow
    LSTMs to learn through backpropagation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 模型模拟了我们通过 LSTM 单元随着时间丢弃不相关或琐碎信息的能力，这些单元能够让信息作为输入进入，完全被遗忘或排除，或者通过传递影响输出。这些分类被称为门控，它们使得
    LSTM 能够通过反向传播进行学习。
- en: GANs
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GANs
- en: GANs are our favorite type of ANN because they’re essentially made up of two
    neural networks that are pitted against each other, hence the name, and compete
    toward the goal of generating new data that’s passable for real-world data. Because
    of this generative ability, GANs are used for image, video, and voice generation.
    They were also initially used for unsupervised learning because of their generative
    and self-regulation abilities but they can be used for supervised and reinforcement
    learning as well. The way it works is one of the neural networks is referred to
    as the generator and the other is the discriminator and the two compete as part
    of this generative process.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 是我们最喜欢的人工神经网络类型，因为它们本质上由两个神经网络组成，它们彼此对抗，因此得名，并朝着生成能通过现实世界数据的目标进行竞争。由于这种生成能力，GANs
    被广泛用于图像、视频和语音生成。最初，它们因其生成和自我调节能力被用于无监督学习，但也可以用于有监督学习和强化学习。其工作原理是，其中一个神经网络被称为生成器，另一个被称为判别器，这两个网络在这个生成过程中进行竞争。
- en: 'GANs were first introduced in a breakthrough paper that came out in 2014 by
    Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio, which states that GANs *"simultaneously
    train two models: a generative model G that captures the data distribution, and
    a discriminative model D that estimates the probability that a sample came from
    the training data rather than G. The training procedure for G is to maximize the
    probability of D making* *a mistake,"*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 最早在2014年由 Ian J. Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David
    Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio 发表的一篇突破性论文中提出，文中指出
    GANs *“同时训练两个模型：一个生成模型 G，它捕捉数据分布；一个判别模型 D，它估计一个样本来自训练数据而非 G 的概率。G 的训练过程是最大化 D
    犯错的概率。”*
- en: Citation
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 引用
- en: Goodfellow, I. J., Mirza, M., Xu, B., Ozair, S., Courville, A., & Bengio, Y.
    (2014). *Generative Adversarial Networks*. *arXiv*. [https://doi.org/10.48550/arXiv.1406.2661](https://doi.org/10.48550/arXiv.1406.2661)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Goodfellow, I. J., Mirza, M., Xu, B., Ozair, S., Courville, A., & Bengio, Y.
    (2014). *生成对抗网络*。*arXiv*。[https://doi.org/10.48550/arXiv.1406.2661](https://doi.org/10.48550/arXiv.1406.2661)
- en: We can think of discriminative and generative models as two sides of the same
    coin. Discriminative models look at the features a type of image might have, for
    example, looking for associations between all the images of dogs it’s currently
    learning from. Generative models start from the category itself and expand out
    into potential features a category in that image might possess. If we take the
    example of a category such as space kittens, the generative model might look at
    the example data it’s fed and deduce that if it creates an image, it should create
    something that involves space and kittens. The discriminative model then takes
    the image the generative model creates and confirms, based on its own learning,
    that any images in the space kittens category must contain both kittens and space
    as features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把判别模型和生成模型看作同一枚硬币的两面。判别模型关注图像可能具有的特征，例如，寻找它当前正在学习的所有狗图像之间的关联。生成模型则从类别本身出发，扩展到该类别图像可能具有的潜在特征。如果我们以“太空小猫”这个类别为例，生成模型可能会查看它所接受的示例数据，并推断出，如果它创建一张图像，它应该涉及到太空和小猫。然后，判别模型会接收生成模型创建的图像，并根据其自身的学习确认，任何属于太空小猫类别的图像必须包含小猫和太空这两个特征。
- en: Another way to explain this is that the generative model maps the label to potential
    features and the discriminative model maps features to the label. What’s most
    interesting to us about GANs is they effectively pass or fail their own version
    of the Turing test. How do you know whether you passed? If the GAN correctly identifies
    a generated image as a falsified image, it’s passed (or failed?) its own test.
    It really depends on how you look at passing or failing for that matter. If it
    incorrectly labeled a falsified/generated image as a “real” image, it means the
    generative model is pretty strong because its own discriminator wasn’t able to
    discriminate properly. Then again, because it’s a double-sided coin, it means
    that the discriminator needs to be strengthened to be more discerning. This one
    is very meta.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解释方式是，生成模型将标签映射到潜在特征，而判别模型则将特征映射到标签。我们对 GAN 最感兴趣的是，它们实际上能够通过或未能通过自己版本的图灵测试。如何判断是否通过了测试？如果
    GAN 正确地将生成的图像识别为伪造图像，那么它通过了（或者说失败了？）自己的测试。这完全取决于你怎么看待通过与失败。如果它错误地将伪造/生成的图像标记为“真实”图像，那就意味着生成模型非常强大，因为它的判别器没有正确地进行区分。然而，因为这是一个双面硬币，这也意味着判别器需要加强，以便更加敏锐。这一点非常具有自我反思性。
- en: The steps a GAN takes to run through its process first begin with a generator
    neural network that takes in data and returns an image, which is then fed to the
    discriminator along with other images from a real-world dataset. Then, the discriminator
    produces outputs that are numbered between 0 and 1, which it assigns as probabilities
    for each of the images it is discriminating, with 0 representing a fake and 1
    representing an authentic real-world image. GANs also use backpropagation, so
    every time the discriminator makes a wrong call, the GAN learns from previous
    mistakes to correct its weights and optimize itself for accuracy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 运行过程的步骤首先从生成器神经网络开始，该网络接收数据并返回一张图像，然后将该图像与来自真实世界数据集的其他图像一起输入给判别器。接着，判别器会输出一个介于
    0 到 1 之间的数值，该数值表示它对每个被判别图像的概率，0 代表伪造图像，1 代表真实的世界图像。GAN 还使用反向传播，因此每当判别器做出错误判断时，GAN
    就会从之前的错误中学习，调整权重并优化自身的准确性。
- en: DBNs
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBN
- en: DBNs also have multiple layers, including multiple hidden layers, but the nodes
    in one layer aren’t connected to each other, though they are connected to nodes
    in other layers. There are relations between the layers themselves, but not between
    the nodes within. DBNs are unsupervised learning layers of what are called **Restricted
    Boltzmann Machines** (**RBMs**), which are themselves another form of ANN. These
    layers of RBMs are chained together to form a DBN. Because of this chain, as data
    passes through the input layer of each RBM, the DBN learns and obtains features
    from the prior layer. The more layers of RBMs you add, the greater the improvement
    and training of the DBN overall. Also, every RBM is taught individually and the
    DBN training isn’t done until all the DBNs have been trained.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: DBN 也有多个层次，包括多个隐藏层，但同一层的节点之间并没有相互连接，尽管它们与其他层的节点是连接的。层与层之间是有关系的，但层内的节点之间没有关系。DBN
    是所谓**限制玻尔兹曼机**（**RBM**）的无监督学习层，RBM 本身是另一种形式的人工神经网络。这些 RBM 层被串联在一起形成一个 DBN。正因为有这个链条，当数据通过每个
    RBM 的输入层时，DBN 会从前一层学习并获得特征。你添加的 RBM 层越多，整个 DBN 的改善和训练效果也越好。此外，每个 RBM 都是单独训练的，直到所有
    DBN 都训练完毕，整个 DBN 的训练才算完成。
- en: DBNs are referred to as generative ANNs because each of the RBMs learns and
    obtains potential values for your data points based on probability. Because of
    this generative ability that they have, they can be used for things such as image
    recognition, capturing motion data, or recognizing speech. They are also computationally
    energy-efficient because each cluster of RBMs operates independently. Rather than
    data passing through all the layers in concert as with feedforward ANNs, data
    stays local to each cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: DBN 被称为生成式人工神经网络（ANN），因为每个 RBM 都基于概率学习并获得数据点的潜在值。正因为它们具备这种生成能力，它们可以用于图像识别、捕捉运动数据或识别语音等任务。它们在计算上也非常节能，因为每个
    RBM 集群都是独立操作的。与前馈型 ANN 中数据必须穿越所有层不同，数据仅在每个集群内部局部流动。
- en: As a product manager, you won’t need to have in-depth knowledge of each neural
    network because if you’re building a product with DL components, you’ve got internal
    experts that can help determine which neural networks to use. But it does help
    to know what some of the most common types of neural networks out there are so
    that you aren’t left in the dark about those determinations. In the next section,
    we will see how DL neural networks overlap with other emerging technologies for
    better context on the ability and influence of DL.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作为产品经理，你不需要深入了解每一个神经网络，因为如果你在构建一个包含深度学习（DL）组件的产品，你会有内部专家帮助你确定使用哪些神经网络。但了解一些常见的神经网络类型对你是有帮助的，这样你就不会对这些决策感到茫然。在接下来的部分中，我们将看到深度学习神经网络如何与其他新兴技术交叉，以便更好地理解深度学习的能力和影响。
- en: Emerging technologies – ancillary and related tech
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新兴技术——辅助技术和相关技术
- en: ML and DL have been used heavily in applications related to natural language
    processing (**natural language generation** (**NLG**), as well as **natural language
    understanding** (**NLU**)), speech recognition, chatbots, virtual agents and assistants,
    decision management, process automation, text analytics, biometrics, cybersecurity,
    content creation, image and emotion recognition, and marketing automation. It’s
    important to remember, particularly from a product manager’s perspective, that
    AI will increasingly work its way into more of how we live our lives and do our
    work. This is doubly true if you work in an innovative capacity as a product manager
    where you’re involved with the ideation and creation of new use cases and MVPs
    for future products.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）和深度学习（DL）在与自然语言处理（**自然语言生成**（**NLG**）以及**自然语言理解**（**NLU**））、语音识别、聊天机器人、虚拟代理和助手、决策管理、过程自动化、文本分析、生物识别、网络安全、内容创作、图像和情感识别以及营销自动化相关的应用中得到了广泛应用。特别是从产品经理的角度来看，记住AI将越来越多地融入到我们生活和工作方式中是非常重要的。如果你作为产品经理从事创新工作，参与未来产品的新用例和最小可行产品（MVP）的构思与创建，这一点尤为真实。
- en: Over the passage of time, we’ll see AI continue to augment our workforce both
    through the process of internal automation as well as through the adoption of
    AI-based no-code/low-code external products and applications that will boost job
    functions, skills, and abilities across the board. AR, VR, and the metaverse also
    offer us new emerging fields where ML will learn more about our world, help us
    learn about ourselves, and also help us build new worlds altogether. We will also
    continue to see ML employed through AI-powered devices such as self-driving planes,
    trains, and automobiles, as well as biometrics, nanotechnologies, and IoT devices
    that share streams of data about our bodies and appliances that we can increasingly
    use to optimize our security, health and energy usage.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们将看到人工智能继续通过内部自动化的过程以及通过采用基于AI的无代码/低代码外部产品和应用程序来增强我们的劳动力，这些产品和应用程序将提升各个岗位的工作职能、技能和能力。增强现实（AR）、虚拟现实（VR）和元宇宙也为我们提供了新的新兴领域，在这些领域中，机器学习将学习更多关于我们世界的信息，帮助我们了解自己，也帮助我们建立全新的世界。我们还将继续看到机器学习通过AI驱动的设备得到应用，例如自动驾驶的飞机、火车和汽车，以及生物识别技术、纳米技术和物联网设备，这些设备能够共享关于我们身体和家电的数据流，我们可以利用这些数据来优化我们的安全、健康和能源使用。
- en: There are, of course, other forms of AI beyond ML and DL, as well as ancillary
    emerging technologies that are often used in concert with the tech we’ve covered
    in this chapter. For instance, with all the innovation and fame that’s accompanied
    the Boston Dynamics dog Spot, we were surprised to find out recently that they
    don’t actually use ML to train these little guys. But even Spot will soon see
    AI updates to its operating system to help it with things such as object recognition
    and semantic contextualization of those objects.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，除了机器学习（ML）和深度学习（DL）之外，还有其他形式的人工智能（AI），以及一些常常与我们在本章中讨论的技术一起使用的新兴辅助技术。例如，尽管波士顿动力公司推出的狗狗Spot伴随着创新和声誉，但我们最近惊讶地发现，它们实际上并没有使用机器学习来训练这些小家伙。但即便如此，Spot也将很快看到操作系统的AI更新，帮助它进行物体识别以及对这些物体的语义上下文化处理。
- en: AI in general, and DL specifically, might be getting an update of its own soon
    through quantum computing since IBM made its aspirations more concrete by publicly
    announcing a “road map” for the development of its quantum computers, including
    the ambitious goal of building one containing 1,000 qubits by 2023\. IBM’s current
    largest quantum computer contains 65 qubits.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，人工智能（AI），尤其是深度学习（DL），可能很快就会通过量子计算得到更新，因为IBM通过公开宣布其量子计算机发展的“路线图”，使其雄心壮志更为具体，其中包括到2023年建造一台包含1000个量子比特（qubits）的量子计算机的宏伟目标。目前，IBM最大的一台量子计算机包含65个量子比特。
- en: Quantum computing can massively help us deal with the ongoing issue of storing
    and retrieving data, particularly big data, in cost-effective ways. Because so
    many DL projects can take weeks to train and require access to big data, ancillary
    developments in quantum computing can prove groundbreaking in the area of DL to
    the point where the algorithms both require fewer data to train on and can also
    handle more data and compute power more quickly. This could also allow us greater
    opportunities for making sense of how the models come to certain conclusions and
    assist with perhaps the greatest hurdle of DL – explainability.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算可以在以成本效益的方式处理数据存储和检索方面，特别是大数据方面，极大地帮助我们。因为许多深度学习项目可能需要数周的时间来训练，并且需要访问大量数据，量子计算的辅助发展有可能在深度学习领域带来突破，甚至使得算法在训练时需要的数据更少，同时也能更快速地处理更多的数据和计算能力。这还可能为我们提供更多机会，帮助我们理解模型如何得出某些结论，并帮助解决深度学习中可能最大的难题——可解释性。
- en: Explainability – optimizing for ethics, caveats, and responsibility
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性 – 优化伦理、警告和责任
- en: 'Ethics and responsibility play a foundational role in dealing with your customers’
    data and behavior and because most of you will build products that help assist
    humans to make decisions, eventually someone is going to ask you how your product
    arrives at conclusions. Critical thinking is one of the foundational cornerstones
    of human reasoning and if your product is rooted in DL, your answer won’t be able
    to truly satisfy anyone’s skepticism. Our heartfelt advice is this: don’t create
    a product that will harm people, get you sued, or pose a risk to your business.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理和责任在处理客户数据和行为时起着基础性作用，而且因为你们中的大多数人将会构建帮助人类做决策的产品，最终总会有人问你，你的产品是如何得出结论的。批判性思维是人类推理的基石之一，如果你的产品是基于深度学习（DL）的，那么你的回答将无法真正让任何人的怀疑得到满足。我们由衷的建议是：不要创建一个可能会伤害他人、让你被起诉或对你的商业造成风险的产品。
- en: 'If you’re leveraging ML or DL in a capacity that has even the potential to
    cause harm to others, if there’s a clear bias that affects underrepresented or
    minority groups (in terms of race, gender, or culture), go back to the ideation
    phase. This is true whether that’s immediate or downstream harm. This is a general
    risk all of ML poses to us collectively: the notion that we’re coding our societal
    biases into AI without taking the necessary precautions to make sure the data
    we feed our algorithms is truly unbiased.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在以某种方式使用机器学习（ML）或深度学习（DL），并且这种使用方式有可能对他人造成伤害，或者如果存在明显的偏见，影响到少数群体（无论是种族、性别还是文化方面），那么就应该回到构思阶段。这无论是直接伤害还是下游伤害，都应该引起重视。这是机器学习带给我们集体的一个普遍风险：即我们将社会偏见编码进人工智能中，而没有采取必要的预防措施，确保我们输入给算法的数据是真正无偏的。
- en: The engineers themselves that build these ANNs are unable to look under the
    hood and truly explain how ANNs make decisions. As we’ve seen with the, albeit
    layman, preceding explanations of DL algorithms, ANN structures are built on existing
    ML algorithms and scaled, so it’s almost impossible for anyone to truly explain
    how these networks come to certain conclusions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这些人工神经网络（ANN）的工程师们无法真正理解其内部机制，也无法真正解释ANN是如何做出决策的。正如我们之前看到的，尽管是外行的简化解释，ANN结构是建立在现有的机器学习（ML）算法基础上的，并进行了扩展，因此几乎不可能有任何人能够真正解释这些网络是如何得出某些结论的。
- en: Again, this is why DL algorithms are often referred to as a black box because
    they resist a truly in-depth analysis of the underpinnings of the logic that makes
    them work. DL has a natural opacity to it because of the nature and complexity
    of ANNs. Remember that ANNs effectively just make slight adjustments to the weights
    that affect each neuron within its layers. They are basically elaborate pattern
    finders using math and statistics to make optimizations to their weighting system.
    They do that hundreds of times for each data point over multiple iterations of
    training. We simply don’t have the mental capacity or language to explain this.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这就是为什么深度学习算法常被称为黑箱的原因，因为它们抵制对其工作原理的深入分析。由于人工神经网络的性质和复杂性，深度学习本身具有天然的不透明性。请记住，人工神经网络实际上只是对影响每个神经元的权重做出微小的调整。它们基本上是利用数学和统计学进行优化的复杂模式发现器。它们在每个数据点上进行数百次优化，经过多次训练迭代。我们根本没有足够的思维能力或语言来解释这一过程。
- en: You also don’t have to be a DL engineer to truly understand how your product
    affects others. If you, as a product manager, are not able to fully articulate
    how DL is leveraged in your product and, at the very least, can’t prove that the
    outputs of your DL product aren’t causing harm to others, then it probably isn’t
    a product you want to go all in for.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你也不需要成为深度学习工程师才能真正理解你的产品如何影响他人。如果你作为产品经理，无法充分阐述深度学习是如何在你的产品中应用的，至少不能证明你的深度学习产品输出不会对他人造成伤害，那么这可能不是你想全力投入的产品。
- en: DL is still very much in the research phase and many product managers are hesitant
    to incorporate it because of the issue of explainability we discussed earlier
    in the chapter. So we urge product managers to use caution when looking to wet
    their feet with DL. We have plenty of examples of ML causing harm to people even
    when it involves basic linear regression models. Moving forward without a sense
    of stewardship of and responsibility for our peers and customers with something
    as complicated and full of potential as DL is a recipe for adding more chaos and
    harm to the world.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习仍然处于研究阶段，许多产品经理由于我们在本章前面讨论过的可解释性问题而对其应用感到犹豫。因此，我们建议产品经理在尝试深度学习时要小心谨慎。我们有很多机器学习（ML）造成伤害的例子，即使它们仅仅涉及基本的线性回归模型。没有对同行和客户的责任心，贸然推进如此复杂且充满潜力的深度学习技术，只会给世界带来更多混乱和伤害。
- en: Do we always have to be so cautious? Not necessarily. If DL applications get
    really good at saving lives by detecting cancer or they work better when applied
    to robotics, who are we to stand in the way of progress? Be critical about when
    to be concerned with your use of DL. If your system works effectively and is better
    because of DL and there isn’t some problem or concern springing from the opacity
    of the ANNs, then all is right with the world.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否总是需要如此谨慎？不一定。如果深度学习应用在通过检测癌症拯救生命方面做得非常好，或者在机器人技术中应用效果更佳，我们有什么理由阻碍进步呢？在使用深度学习时，要批判性地考虑何时需要关注。如果你的系统运行有效，且因为深度学习而变得更好，并且没有因人工神经网络的不透明性产生任何问题或担忧，那么世界就是美好的。
- en: Accuracy – optimizing for success
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确性 – 优化成功的关键
- en: When it comes to DL, we can only truly grapple with its performance. Even from
    a performance perspective, a lot of DL projects fail to give the results their
    own engineers are hoping for, so it’s important to manage expectations. This is
    doubly true if you’re managing the expectations of your leadership team as well.
    If you’re a product manager or entrepreneur and you’re thinking of incorporating
    DL, do so in the spirit of science and curiosity. Remain open about your expectations.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习来说，我们只能真正衡量它的性能。即使从性能的角度来看，许多深度学习项目也未能提供工程师们期望的结果，因此管理预期变得尤为重要。如果你是产品经理或企业家，考虑将深度学习纳入你的产品，要以科学和好奇心的精神进行。保持对预期的开放态度。
- en: But make sure you’re setting your team up for success. A big part of your ANN’s
    performance also lies in the data preparation you take before you start training
    your models. Passing your data through an ANN is the last step in your pipeline.
    If you don’t have good validation or if the quality of your data is poor, you’re
    not going to see positive results. Then, once you feel confident that you have
    enough data and that it’s clean enough to pass through an ANN, start experimenting.
    If you’re looking for optimal performance, you’ll want to try a few different
    models, or a selection of models together, and compare the performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但请确保你为团队的成功奠定了基础。你ANN性能的一个重要部分也体现在你开始训练模型之前的数据准备上。将数据通过ANN处理是你管道中的最后一步。如果你没有好的验证，或者数据质量差，那么你是无法看到积极的结果的。然后，一旦你确信你拥有足够的且清洁的数据，可以通过ANN处理，就开始尝试实验吧。如果你寻求最佳性能，你将需要尝试几种不同的模型，或者将几种模型组合在一起，并比较它们的表现。
- en: The time it takes to fine-tune a DL model is also aggressively long. If you’re
    used to other forms of ML, it might shock you to experience training a model over
    the course of days or weeks. This is largely because the amount of data you need
    to train ANNs is vast; most of the time you need at least 10,000 data points,
    and all this data is passed through multiple layers of nodes and processed by
    your ANN. Your ANN is also, most of the time, going to be an ensemble of several
    types of ANNs we mentioned previously. The chain then becomes quite long.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 微调深度学习模型所需的时间也相当漫长。如果你习惯了其他形式的机器学习（ML），那么经历几天甚至几周的模型训练可能会让你大吃一惊。这主要是因为训练人工神经网络所需的数据量庞大；大多数情况下，你至少需要10,000个数据点，所有这些数据都需要通过多个节点层并由ANN处理。你的ANN大多数情况下也会是我们之前提到的几种类型ANN的集合体。这个链条因此变得相当长。
- en: The nature of understanding ANNs is inherently mysterious because of the complexity
    of the layers of artificial neurons. We cannot see deterministic qualities. Even
    when you do everything “right” and you get a good performance, you don’t really
    know why. You just know that it works. The same goes when something does go wrong
    or when you see poor performance. You once again don’t really know why. Perhaps
    the fault lies with the ANN or with the method you’re using or something has changed
    in the environment. The process of getting back to better performance is also
    iterative. And then it’s back to the drawing board.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 理解人工神经网络（ANN）的本质本身就很神秘，因为人工神经元层次的复杂性。我们看不见确定性的特性。即使你做了一切“正确”的事，得到一个良好的表现，你也不真正知道为什么。你只知道它有效。相同的情况也会出现在某些事情出错或者你看到表现不佳时。你再次不知道为什么。也许是ANN本身出了问题，或者是你使用的方法有问题，亦或是环境发生了变化。回到更好表现的过程也是一个迭代的过程。然后，一切又回到原点。
- en: Remember that these are emerging tech algorithms. It may take us all some time
    to adjust to new technologies and truly understand the power they have. Or don't!
    Part of the disillusionment that’s happened with DL actually lies in the tempering
    of expectations. Some DL algorithms can make amazing things happen and can show
    immensely promising performance but others can so easily fall flat. It’s not a
    magic bullet. It’s just a powerful tool that needs to be used in the proper way
    by people that have the knowledge, wisdom, and experience to do so. Considering
    most of the ANNs we went over together are from the 80s, 90s, and early 2000s,
    that’s not much time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这些是新兴的技术算法。我们大家可能需要一些时间去适应新技术，并真正理解它们的力量。或者也不必！深度学习的部分失望其实在于预期的调整。有些深度学习算法能够让惊人的事情发生，并展现出极具前景的表现，但其他一些算法却可能轻易地失败。这不是一剂神奇的灵药。它只是一个强大的工具，需要由拥有知识、智慧和经验的人以正确的方式使用。考虑到我们一起讨论的大部分ANN都来自80年代、90年代和2000年代初，那其实并没有多少时间。
- en: So tread carefully here if you’re building, managing, or ideating on DL products.
    When in doubt, there are other more explainable models to choose from, which we’ve
    covered in [*Chapter 1*](B18935_01.xhtml#_idTextAnchor012). It’s better to be
    safe than sorry. If you’ve got lots of time, patience, excitement, and curiosity
    along with a safe, recreational idea for applying DL, then you’re probably in
    a good position to explore that passion and create something the world could use
    in good faith.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你在构建、管理或构思深度学习（DL）产品时，一定要小心谨慎。如果有疑虑，还有其他更易于解释的模型可供选择，我们已经在[*第1章*](B18935_01.xhtml#_idTextAnchor012)中讨论过。宁愿安全一些，也不要事后后悔。如果你有充足的时间、耐心、兴奋和好奇心，并且有一个安全、娱乐性质的深度学习应用想法，那么你可能已经处于一个很好的位置，去探索这个激情，并创造出一些世界能够善用的东西。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We got the chance to go deep into DL in this chapter and understand some of
    the major social and historical influences that impact this subsection of ML.
    We also got the chance to look at some of the specific ANNs that are most commonly
    used in products powered by DL in order to get more familiar with the actual models
    we might come across as we build with DL. We ended the chapter with a look at
    some of the other emerging technologies that collaborate with DL, as well as getting
    further into some of the concepts that impact DL most: explainability and accuracy.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们有机会深入了解深度学习（DL），并理解一些影响这一机器学习子领域的主要社会和历史因素。我们还得以了解一些在深度学习驱动的产品中最常用的人工神经网络（ANN），以便更加熟悉我们在构建深度学习时可能会遇到的实际模型。最后，我们通过探讨一些与深度学习协作的新兴技术，并深入讨论深度学习中最重要的概念：可解释性和准确性，结束了本章内容。
- en: DL ANNs are super powerful and exhibit great performance, but if you need to
    explain them, you will run into more issues than you would if you stick to more
    traditional ML models. We’ve now spent the first three chapters of the book getting
    familiar with the more technical side of AI product management. Now that we’ve
    got that foundation covered, we can spend some time contextualizing all this tech.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的人工神经网络（ANN）非常强大，表现出色，但如果你需要解释它们，你将遇到比使用更传统的机器学习模型时更多的问题。我们已经用书中的前三章时间来熟悉人工智能产品管理的更技术性方面。现在我们有了这个基础，我们可以花时间将所有这些技术进行情境化。
- en: In the next chapter, we will explore some of the major areas of AI products
    we see on the market, as well as examples of the ethics and success factors that
    contribute most to commercialization.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索市场上我们看到的一些主要AI产品领域，并且讨论一些对商业化贡献最大的伦理问题和成功因素。
- en: References
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Some Methods for Classification and Analysis of Multivariate Observations:
    [https://books.google.com/](https://books.google.com/)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些多元观察分类与分析方法：[https://books.google.com/](https://books.google.com/)
- en: 'K-Nearest Neighbors Algorithm: Classification and Regression Star [https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/](https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-最近邻算法：分类与回归明星 [https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/](https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/)
- en: Random Forests [https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林 [https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
- en: Decision Trees [https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf](https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树 [https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf](https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf)
- en: 'Support Vector Machine: The most popular machine learning algorithm [https://cml.rhul.ac.uk/svm.html](https://cml.rhul.ac.uk/svm.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机：最流行的机器学习算法 [https://cml.rhul.ac.uk/svm.html](https://cml.rhul.ac.uk/svm.html)
- en: Logistic Regression [https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X)](https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X))
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归 [https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X)](https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X))
- en: Logistic Regression History [https://holypython.com/log-reg/logistic-regression-history/](https://holypython.com/log-reg/logistic-regression-history/)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归历史 [https://holypython.com/log-reg/logistic-regression-history/](https://holypython.com/log-reg/logistic-regression-history/)
- en: Bayes Classifier [https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s](https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯分类器 [https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s](https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s)
- en: Principal Component Analysis [https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data](https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析 [https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data](https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data)
- en: 'Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics
    Instructors [https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537](https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加尔顿、皮尔逊与豌豆：统计学教师的线性回归简史 [https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537](https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537)
- en: IBM promises 1000-qubit quantum computer—a milestone—by 2023 [https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023](https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM承诺到2023年推出1000量子比特量子计算机——这一里程碑 [https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023](https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023)
- en: Boston Dynamics says AI advances for Spot the robo-dog are coming [https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/](https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波士顿动力公司表示，Spot机器狗的人工智能进展即将到来 [https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/](https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/)
- en: Convolutional Neural Network Tutorial [https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network](https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络教程 [https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network](https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network)
- en: Generative Adversarial Networks [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络 [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
- en: The Self-Organizing Map [https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自组织映射 [https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf)
- en: Multivariable Functional Interpolation and Adaptive Networks [https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量函数插值与自适应网络 [https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf)
- en: Long Short-Term Memory [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆 [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)
- en: Learning Representations by Back Propagating Errors [https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过反向传播误差学习表示 [https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)
- en: The numerical solution of variational problems [https://www.sciencedirect.com/science/article/pii/0022247X62900045](https://www.sciencedirect.com/science/article/pii/0022247X62900045)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分问题的数值解法 [https://www.sciencedirect.com/science/article/pii/0022247X62900045](https://www.sciencedirect.com/science/article/pii/0022247X62900045)
- en: 'The Perceptron: A Perceiving and Recognizing Automaton [https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机：一种感知与识别的自动机 [https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)
- en: A Logical Calculus of the Ideas Immanent in Nervous Activity [https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经活动中固有思想的逻辑演算 [https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)
- en: AI Education [https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4](https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4.pdf)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能教育 [https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4](https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4.pdf)
