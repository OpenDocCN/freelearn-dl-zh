- en: 'Chapter 4:'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章：
- en: Reusable Models and Scalable Data Pipelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重用模型与可扩展数据管道
- en: In this chapter, you will learn different ways of using scalable data ingestion
    pipelines with pre-made model elements in TensorFlow Enterprise's high-level API's.
    These options provide the flexibility to suit different requirements or styles
    for building, training, and deploying models. Armed with this knowledge, you will
    be able to make informed choices and understand trade-offs among different model
    development approaches. The three major approaches are TensorFlow Hub, the TensorFlow
    Estimators API, and the TensorFlow Keras API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用 TensorFlow Enterprise 高级 API 中的预制模型元素，构建可扩展的数据摄取管道。这些选项提供了灵活性，适应不同的需求或风格，以便构建、训练和部署模型。掌握这些知识后，你将能够做出明智的选择，并理解不同模型开发方法之间的取舍。三种主要的方法是
    TensorFlow Hub、TensorFlow Estimators API 和 TensorFlow Keras API。
- en: TensorFlow Hub is a library of open source machine learning models. TensorFlow
    Estimators and `tf.keras` APIs are wrappers that can be regarded as high-level
    elements that can be configured and reused as building blocks in a model. In terms
    of the amount of coding required, TensorFlow Hub models require the least amount
    of extra coding, while Estimator and Keras APIs are building blocks at a lower
    level, and therefore more coding is involved when using either Estimator or Keras
    APIs. But in any case, all three approaches really made TensorFlow much easier
    to learn and use. We are going to spend the next few sections of this chapter
    learning how these approaches work with scalable data ingestion pipelines from
    cloud storage.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub 是一个开源机器学习模型库。TensorFlow Estimators 和 `tf.keras` API 是包装器，可以视为可以配置和重用的高级元素，作为模型的构建块。从所需的代码量来看，TensorFlow
    Hub 模型需要最少的额外代码，而 Estimator 和 Keras API 属于较低级的构建块，因此使用 Estimator 或 Keras API 时需要更多的编码。但是无论如何，这三种方法都使
    TensorFlow 更加易于学习和使用。接下来几节我们将学习这些方法如何与云存储中的可扩展数据摄取管道一起使用。
- en: 'With the help of an example, we will learn to use *TensorFlow datasets* and
    *TensorFlow I/O* as a means to ingest large amounts of data without reading it
    into the JupyterLab runtime memory. We will cover the following topics in this
    chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 借助一个示例，我们将学习如何使用 *TensorFlow 数据集* 和 *TensorFlow I/O* 来摄取大量数据，而无需将其读取到 JupyterLab
    的运行时内存中。本章将涵盖以下内容：
- en: Using TensorFlow Hub
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Hub
- en: Applying models from TensorFlow Hub
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用 TensorFlow Hub 中的模型
- en: Leveraging TensorFlow Keras API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 TensorFlow Keras API
- en: Working with TensorFlow Estimators
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Estimators
- en: Using TensorFlow Hub
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Hub
- en: Of these three approaches (**TensorFlow Hub, the Estimators API, and the Keras
    API**), TensorFlow Hub stands out from the other two. It is a library for open
    source machine learning models. The main purpose of TensorFlow Hub is to enable
    model reusability through transfer learning. Transfer learning is a very practical
    and convenient technique in deep learning modeling development. The hypothesis
    is that as a well-designed model (peer reviewed and made famous by publications)
    learned patterns in features during the training process, the model learned to
    generalize these patterns, and such generalization can be applied to new data.
    Therefore, we do not need to retrain the model again when we have new training
    data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三种方法中（**TensorFlow Hub、Estimators API 和 Keras API**），TensorFlow Hub 相较于其他两者脱颖而出。它是一个开源机器学习模型库。TensorFlow
    Hub 的主要目的是通过迁移学习实现模型的重用。迁移学习是深度学习建模开发中非常实用且便捷的技术。其假设是，经过精心设计的模型（通过同行评审并由出版物推广）在训练过程中学习到了特征中的模式，这些模式可以被泛化，并应用于新的数据。因此，当我们有新的训练数据时，不需要重新训练模型。
- en: Let's take human vision as an example. The content of what we see can be decomposed
    from simple to sophisticated patterns in the order of lines, edges, shapes, layers,
    and finally a pattern. As it turns out, this is how a computer vision model recognizes
    human faces. If we imagine a multilayer perceptron model, in the beginning, the
    layers learn patterns in lines, then shapes, and as we go into deep layers, we
    see that what's learned is the facial patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以人类视觉为例，我们所看到的内容可以从简单到复杂的模式分解，顺序为线条、边缘、形状、层次，最终形成一个模式。事实证明，这也是计算机视觉模型识别人脸的方式。如果我们想象一个多层感知器模型，最开始，层次学习的是线条模式，然后是形状，随着深入到更深层次，我们可以看到学习到的模式是面部特征。
- en: Since the hierarchy of the same pattern can be used to classify other images,
    we can reuse the architecture of the model (from a repository such as TensorFlow
    Hub) and append a final classification layer for our own purposes. We will utilize
    this approach of transfer learning in this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于相同模式的层级结构可以用于分类其他图像，我们可以重用模型的架构（例如来自TensorFlow Hub的模型），并为我们自己的目的添加一个最终的分类层。在本章中，我们将利用这种迁移学习的方法。
- en: Applying models from TensorFlow Hub
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用来自TensorFlow Hub的模型
- en: 'TensorFlow Hub contains many reusable models. For example, in image classification
    tasks, there are pretrained models such as Inception V3, ResNet of different versions,
    as well as feature vectors available. In this chapter, we will take a look at
    how to load and use a ResNet feature vector model for image classification of
    our own images. The images are five types of flowers: daisy, dandelion, roses,
    sunflowers, and tulips. We will use the `tf.keras` API to get these images for
    our use:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub包含了许多可重用的模型。例如，在图像分类任务中，提供了诸如Inception V3、不同版本的ResNet等预训练模型，以及可用的特征向量。在本章中，我们将学习如何加载和使用ResNet特征向量模型来进行我们自己图像的分类。这些图像包括五种花卉：雏菊、蒲公英、玫瑰、向日葵和郁金香。我们将使用`tf.keras`
    API来获取这些图像：
- en: 'You may use Google Cloud AI Platform''s JupyterLab environment for this work.
    Once you are in the AI Platform''s JupyterLab environment, you may start by importing
    the necessary modules and download the images:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用Google Cloud AI平台的JupyterLab环境进行这项工作。一旦进入AI平台的JupyterLab环境，你可以通过导入必要的模块并下载图像来开始：
- en: '[PRE0]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may find the flower images in the runtime instance of your JupyterLab at
    `/home/jupyter/.keras/datasets/flower_photos`.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在你的JupyterLab运行时实例中找到这些花卉图像，路径为`/home/jupyter/.keras/datasets/flower_photos`。
- en: 'In a new cell, we may run the following command to take a look at the directory
    structure for our data:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个新的单元格中，我们可以运行以下命令来查看数据的目录结构：
- en: '[PRE1]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding command will return the following structure:'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的命令将返回以下结构：
- en: '[PRE2]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Inside each folder, you will find colored images in `.jpg` format with different
    widths and heights. Before using any pre-built model, it is important to find
    out about the required data shape at the entry point of the model.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个文件夹中，你会找到不同宽度和高度的彩色`.jpg`格式图像。在使用任何预构建模型之前，了解模型输入端所要求的数据形状非常重要。
- en: We are going to use the ResNet V2 feature vector that was pretrained with imagenet
    as our base model. The URL to this model is [https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4](https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用在imagenet上预训练的ResNet V2特征向量作为我们的基础模型。该模型的URL是[https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4](https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4)。
- en: 'The documentation there indicates the expected height and width of the image
    at the entry point to be `224`. Let''s go ahead and specify these parameters as
    well as the batch size for training:'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该文档指出，图像在模型入口点处的期望高度和宽度为`224`。让我们继续指定这些参数以及训练的批量大小：
- en: '[PRE3]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have understood the dimension of the image expected as model input,
    we are ready to deal with the next step, which is about how to ingest our training
    images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了作为模型输入所期望的图像维度，接下来我们将处理下一步，即如何获取我们的训练图像。
- en: Creating a generator to feed image data at scale
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个生成器以大规模地提供图像数据
- en: A convenient method to ingest data into the model is by a generator. A Pythonic
    generator is an iterator that goes through the data directory and passes batches
    of data to the model. When a generator is used to cycle through our training data,
    we do not have to load the entire image collection at one time and worry about
    memory constraints in our compute node. Rather, we send a batch of images at one
    time. Therefore, the use of the Python generator is more efficient for the compute
    node's memory than passing all the data as a huge NumPy array.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方便的将数据输入模型的方法是使用生成器。Python生成器是一个迭代器，它会遍历数据目录并将数据批次传递给模型。当我们使用生成器遍历训练数据时，无需一次性加载整个图像集合，也不必担心计算节点的内存限制。相反，我们一次发送一批图像。因此，使用Python生成器比将所有数据作为一个巨大的NumPy数组传递要更加高效。
- en: 'TensorFlow provides APIs and workflows to create such generators specific for
    the TensorFlow model''s consumption. At a high level, it follows this process:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了API和工作流，用于创建专门为TensorFlow模型消费设计的生成器。从高层次来看，它遵循以下过程：
- en: It creates an object with the `ImageDataGenerator` function.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它通过`ImageDataGenerator`函数创建了一个对象。
- en: It uses this object to invoke the `flow_from_directory` function to create a
    TensorFlow generator.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用这个对象调用`flow_from_directory`函数来创建一个TensorFlow生成器。
- en: As a result, this generator knows the directory where the training data is stored.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，这个生成器知道训练数据存储的目录。
- en: 'When working with images, there are a few parameters about the data that we
    need to specify for the generator. The input images'' color values are preferred
    to be between `0` and `1`. Therefore, we have to normalize our image by dividing
    it by a rescale factor with a value of `255`, which is the maximum pixel value
    in an RGB image of `.jpg` format. We may also hold on to 20% of the data for validation.
    This is known as the validation split factor. We also need to specify the standard
    image size that conforms to ResNet, an interpolation method that converts images
    of any size into this size, and the amount of data in a batch (batch size). The
    necessary steps are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理图像时，我们需要为生成器指定一些关于数据的参数。输入图像的颜色值应在`0`和`1`之间。因此，我们必须通过将图像除以一个重新缩放因子`255`来对图像进行归一化，这是`.jpg`格式RGB图像中的最大像素值。我们还可以保留20%的数据作为验证集。这被称为验证拆分因子。我们还需要指定符合ResNet标准的图像大小，选择一种插值方法将任何大小的图像转换为该大小，并指定批次中的数据量（批次大小）。必要的步骤如下：
- en: 'Organize these factors as tuples. These factors are specified as input keywords
    for either `ImageDataGenerator` or `flow_from_directory`. We may pass these parameters
    and their values as tuples to these functions. The tuple will be unpacked when
    the function is executed. These parameters are held in these dictionaries:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些因素组织成元组。这些因素作为输入关键字指定给`ImageDataGenerator`或`flow_from_directory`。我们可以将这些参数及其值作为元组传递给这些函数。在执行函数时，元组将被解包。这些参数存储在这些字典中：
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As seen in the preceding lines of code, `datagen_kwargs` goes to `ImageDataGenerator`,
    while `dataflow_kwargs` goes to `flow_from_directory`.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前述代码行所示，`datagen_kwargs`传递给`ImageDataGenerator`，而`dataflow_kwargs`传递给`flow_from_directory`。
- en: 'Pass the tuples to `ImageGenerator`. These tuples encapsulate all these factors.
    Now we will pass these tuples into the generator, as shown in the following code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些元组传递给`ImageGenerator`。这些元组封装了所有这些因素。现在，我们将把这些元组传递给生成器，如下代码所示：
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And you will see the output with number of images and classes in the cross
    validation data:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到交叉验证数据中图像和类别数量的输出：
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For training data, if you wish, you may consider the option for data augmentation.
    If so, then we may set these parameters in `ImageDataGenerator`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练数据，如果你愿意，可以考虑使用数据增强选项。如果是这样，我们可以在`ImageDataGenerator`中设置这些参数：
- en: '[PRE7]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These parameters help transform original images into different orientations.
    This is a typical technique to add more training data to improve accuracy.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些参数有助于将原始图像转换为不同的方向。这是一种典型的技术，用于增加更多训练数据以提高准确性。
- en: 'For now, let''s not bother with that, so we set `do_data_augmentation = False`,
    as shown in the following code. You may set it to `True` if you wish. Suggested
    augmentation parameters are provided:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，我们暂时不需要处理这个问题，因此我们将`do_data_augmentation = False`，如下代码所示。如果你愿意，也可以将其设置为`True`。这里提供了建议的增强参数：
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Upon executing the preceding code, you will receive the following output:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行上述代码后，你将看到以下输出：
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our generators for validation data and training data correctly identified the
    directory and were able to identify the number of classes.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的验证数据和训练数据生成器正确地识别了目录，并能够识别类别数量。
- en: 'As with all classification tasks, labels are converted to integer indices.
    The generator maps labels with `train_generator.class_indices`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和所有分类任务一样，标签会转换为整数索引。生成器使用`train_generator.class_indices`来映射标签：
- en: '[PRE10]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can easily map indices back to labels by creating a reverse lookup, also
    in the form of a dictionary. This can be done by reversing the key-value pairs
    as we iterated `through labels_idx`, where the key is the index and the values
    are flower types:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个反向查找，轻松地将索引映射回标签，反向查找的形式也是一个字典。这可以通过反转`labels_idx`中的键值对来完成，其中键是索引，值是花卉类型：
- en: '[PRE11]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this section, we learned how to implement `ImageGenerator` for both training
    and validation data. We leveraged optional input parameters to rescale and normalize
    our images. We also learned to retrieve the ground truth label mapping so that
    we may decode the model prediction.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何实现用于训练和验证数据的`ImageGenerator`。我们利用可选的输入参数对图像进行重新缩放和归一化。我们还学习了如何获取真实标签映射，以便解码模型的预测结果。
- en: Next, we will learn to implement transfer learning by reusing ResNet feature
    vectors for our own image classification task.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习通过重用 ResNet 特征向量来实现迁移学习，以完成我们自己的图像分类任务。
- en: Reusing pretrained ResNet feature vectors
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重用预训练的 ResNet 特征向量
- en: 'Now we are ready to construct the model. We will use the `tf.keras.sequential`
    API. It consists of three layers—input, ResNet, and a dense layer—as the classification
    output. We also have the choice between fine-tuning and retraining the ResNet
    (this requires longer training time). The code for defining the model architecture
    is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建模型。我们将使用 `tf.keras.sequential` API。它由三层组成——输入层、ResNet 层和一个全连接层——作为分类输出。我们还可以选择对
    ResNet 进行微调或重新训练（这需要更长的训练时间）。定义模型架构的代码如下：
- en: 'We''ll begin by defining the parameters, as shown in the following lines of
    code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先定义参数，如下所示：
- en: '[PRE12]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we will construct the model with the help of the following code:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码来构建模型：
- en: '[PRE13]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let''s build the model with the following line of code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码行来构建模型：
- en: '[PRE14]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ResNet requires RGB layers to be separated as a third dimension. Therefore,
    we need to add an input layer that takes on `input_shape` of `[224, 224, 3]`.
    Also, since we have five types of flowers, this is a multiclass classification.
    We need a dense layer with softmax activation for outputting probability for each
    label.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ResNet 需要将 RGB 层作为第三维分开。因此，我们需要添加一个输入层，接受形状为 `[224, 224, 3]` 的 `input_shape`。此外，由于我们有五种花卉类型，这是一个多类分类问题。我们需要一个带有
    softmax 激活的全连接层来为每个标签输出概率。
- en: 'We may confirm the model architecture with the following line of code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码行来确认模型架构：
- en: '[PRE15]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Upon executing the preceding line of code, you will see the sequence of the
    three layers and their expected output shape:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行前面的代码行后，您将看到三个层的顺序及其预期的输出形状：
- en: '[PRE16]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This shows the model is very simple in its structure. It consists of the ResNet
    feature vector layer that we downloaded from TensorFlow Hub, followed by a classification
    head with five nodes (there are five flower classes in our image collection).
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明模型的结构非常简单。它由我们从 TensorFlow Hub 下载的 ResNet 特征向量层组成，后面是一个包含五个节点的分类头（我们图像集合中有五种花卉类别）。
- en: Compiling the model
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译模型
- en: Now that we have wrapped the ResNet feature vector with proper input and output
    layers, we are ready to set up the training workflow. To begin, we need to compile
    the model, where we specify the optimizer (in this case, we select `loss` function.
    The optimizer leverages the gradient decent algorithm to continuously seek weights
    and biases to minimize the `loss` function. Since this is a multiclass classification
    problem, it needs to be categorical cross-entropy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经用适当的输入和输出层封装了 ResNet 特征向量，我们准备好设置训练工作流了。首先，我们需要编译模型，在其中指定优化器（在此案例中，我们选择
    `loss` 函数）。优化器使用梯度下降算法不断寻找权重和偏置，以最小化 `loss` 函数。由于这是一个多类分类问题，因此需要使用分类交叉熵。
- en: 'For a deeper discussion, see *TensorFlow 2.0 Quick Start Guide*, by *Tony Holroyd*,
    published by *Packt Publishing*. You can refer to [*Chapter 4*](B16070_04_Final_JM_ePub.xhtml#_idTextAnchor101)
    *Supervised Machine Learning Using TensorFlow 2*, and the section entitled *Logistic
    regression*, concerning loss functions and optimizers. This is how we define an
    optimizer:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更深入的讨论，请参阅 *TensorFlow 2.0 快速入门指南*，作者 *Tony Holroyd*，由 *Packt Publishing*
    出版。您可以参考 [*第 4 章*](B16070_04_Final_JM_ePub.xhtml#_idTextAnchor101) *使用 TensorFlow
    2 的监督机器学习*，以及名为 *逻辑回归* 的部分，讨论有关损失函数和优化器的内容。这就是我们定义优化器的方法：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And since we want to output probability for each class, we set `from_logits
    = True`, We also would like the model not to become overconfident, so we set `label_smoothing
    = 0.1` as a regularization to penalize extremely high probability. We may define
    a `loss` function as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望为每个类别输出概率，因此我们设置 `from_logits = True`。同时，我们希望模型不要变得过于自信，因此我们将 `label_smoothing
    = 0.1` 作为正则化项，以惩罚极高的概率。我们可以按如下方式定义 `loss` 函数：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We need to configure the model for training. This is accomplished by defining
    the `loss` function and optimizer as part of the model''s training process, as
    the training process needs to know what the `loss` function is to optimize for,
    and what optimizer to use. To compile the model with the optimizer and `loss`
    function specified, execute the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要配置模型以进行训练。这是通过在模型的训练过程中定义 `loss` 函数和优化器来完成的，因为训练过程需要知道 `loss` 函数用于优化的目标是什么，以及使用什么优化器。要编译模型并指定优化器和
    `loss` 函数，请执行以下代码：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The outcome is a model architecture that is ready to be used for training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个准备好用于训练的模型架构。
- en: Training the model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'For model training, we will use the `tf.keras.fit` function. We are only going
    to train for five epochs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型训练，我们将使用`tf.keras.fit`函数。我们只会训练五个周期：
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And the training result should be similar to this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结果应类似于此：
- en: '[PRE30]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: At each epoch, the `loss` function value and accuracy on training data is provided.
    Since we have cross-validation data provided, the model is also tested with a
    validation dataset at the end of each training epoch. The `loss` function and
    accuracy measurement are provided at each epoch by the Fit API. This is the standard
    output for each training run.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期中，`loss`函数值和训练数据的准确度都会显示出来。由于我们提供了交叉验证数据，模型在每个训练周期结束时也会使用验证数据集进行测试。`loss`函数和准确度测量通过Fit
    API在每个周期提供。这是每次训练运行的标准输出。
- en: It is also worth mentioning that when the preceding code is executed in AI Notebook
    using the Nvidia Tesla T4 Graphics Processing Unit (GPU) and a basic driver node
    of 4 CPUs at 15 GB RAM, the total training time is just a little over 2 minutes,
    whereas if this training process was executed in the same driver node without
    a GPU, it could take more than 30 minutes to complete the training process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，当在AI Notebook中使用Nvidia Tesla T4图形处理单元（GPU）和基本驱动节点（4个CPU，15GB内存）执行上述代码时，总训练时间仅略超过2分钟，而如果在没有GPU的相同驱动节点中执行此训练过程，则可能需要超过30分钟才能完成训练。
- en: GPUs are well suited for deep learning model training because it can process
    multiple computations in parallel. A GPU achieves parallel processing through
    a large number of cores. This translates to large memory bandwidth and faster
    gradient computation of all trainable parameters in the deep learning architecture
    than otherwise would be the case in a CPU.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: GPU非常适合深度学习模型的训练，因为它可以并行处理多个计算。GPU通过大量核心实现并行处理。这意味着它拥有大带宽的内存和比CPU更快的梯度计算能力，可以处理深度学习架构中的所有可训练参数。
- en: Scoring with test images
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用测试图像进行评分
- en: 'Now we may test the model using test (holdout) images. In this example, I uploaded
    five flower images, and we need to convert them all to the shape of `[224, 224]`
    and normalize pixel values to `[0, 1]`. As common practice, test images are stored
    separately from training and cross-validation images. Therefore, it is typical
    to have a different file path to test images:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用测试（保留）图像来测试模型。在这个例子中，我上传了五张花卉图像，我们需要将它们全部转换为`[224, 224]`的形状，并将像素值归一化到`[0,
    1]`。按照惯例，测试图像会与训练和交叉验证图像分开存储。因此，通常会有一个不同的文件路径来存储测试图像：
- en: 'We are going to download some test images for these types of flowers. The images
    are partitioned into training, validation, and test images at the following link:
    [https://dataverse.harvard.edu/api/access/datafile/4159750](https://dataverse.harvard.edu/api/access/datafile/4159750)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为这些花卉类型下载一些测试图像。图像已经在以下链接中划分为训练、验证和测试图像：[https://dataverse.harvard.edu/api/access/datafile/4159750](https://dataverse.harvard.edu/api/access/datafile/4159750)
- en: 'So, in the next cell, you may use `wget` to download it to your notebook:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，在下一个单元格中，你可以使用`wget`将其下载到你的笔记本中：
- en: '[PRE40]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, unzip it:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，解压它：
- en: '[PRE41]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create a data generator instance for test data. Since our `train_datagen` already
    knows how to do that, we may reuse this object. Make sure you specify the `working_dir`
    directory as a file path to where our test images reside:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据生成器实例用于测试数据。由于我们的`train_datagen`已经知道如何实现这一点，我们可以重用该对象。确保你指定`working_dir`目录作为测试图像所在文件路径：
- en: '[PRE42]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s take a note of the label index:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们记录下标签的索引：
- en: '[PRE43]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output indicates the relative position of each label in the array of probability:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果表示每个标签在概率数组中的相对位置：
- en: '[PRE44]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s also define a helper function that plots the images:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了一个辅助函数来绘制图像：
- en: '[PRE45]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, let''s take a look at the test images and their corresponding labels (ground
    truth):'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们来看一下测试图像及其对应的标签（真实标签）：
- en: '[PRE46]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output for the test images is shown as follows. In the first three rows,
    one-hot encoding is `1` at the first position. This corresponds to `daisy` according
    to `test_generator.class_indices`, whereas for the last two rows, `1` is at the
    last position, indicating the last two images are of `tulips`:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试图像的输出结果如下所示。在前三行中，one-hot编码在第一位置为`1`，根据`test_generator.class_indices`，这对应于`雏菊`，而在最后两行中，`1`位于最后位置，表示最后两张图像是`郁金香`：
- en: '[PRE47]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'And we may plot these images:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制这些图像：
- en: '[PRE48]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Figure 4.1 – Test image examples; the first three are daisies, and the last
    two are tulips'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.1 – 测试图像示例；前三个是雏菊，最后两个是郁金香'
- en: '](img/Figure_4.1.jpg)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_4.1.jpg)'
- en: Figure 4.1 – Test image examples; the first three are daisies, and the last
    two are tulips
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.1 – 测试图像示例；前三个是雏菊，最后两个是郁金香
- en: 'For the model to make predictions on these images, execute the following code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要让模型对这些图像进行预测，请执行以下代码：
- en: '[PRE49]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output of the prediction is as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测的输出结果如下：
- en: '[PRE50]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This output is a NumPy array of probabilities for each class in each image.
    Each row corresponds to an image, and consists of five probabilities for each
    label. The first three rows have the highest probability in the first position,
    and the last two rows have the highest probability in the last position. This
    means the model predicted that the first three images would be daisies, and the
    last two images tulips, according to the mapping provided by `test_generator.class_indices`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出是每张图像每个类别的概率的 NumPy 数组。每一行对应一张图像，包含五个类别的概率。前三行的最大概率出现在第一列，后两行的最大概率出现在最后一列。这意味着根据`test_generator.class_indices`提供的映射，模型预测前三张图像为雏菊，最后两张图像为郁金香。
- en: It would also be helpful if we could output these results in a more readable
    format such as a CSV file, with filenames of the test images and their respective
    predictions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能将这些结果输出为更易读的格式，比如 CSV 文件，其中包含测试图像的文件名及其相应的预测结果，那就更有帮助了。
- en: 'Let''s map the probability magnitude with respect to the position and define
    a label reference:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将概率大小与位置关联，并定义一个标签参考：
- en: '[PRE51]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Write a helper function to map position with respect to the actual label:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个辅助函数，将位置映射到实际标签：
- en: '[PRE52]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now we can map the position of each observation''s highest probability:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以映射每个观测值的最高概率位置：
- en: '[PRE53]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'And we can take a look at `predicted_idx`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以查看 `predicted_idx`：
- en: '[PRE54]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This means that in the first three images, the maximum probability occurs at
    position `0`, which corresponds to `daisy` according to `test_generator.class_indices`.
    By the same token, the last two images have the maximum probability occurring
    at position `4`, which corresponds to `tulips`.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着在前三张图像中，最大概率出现在位置 `0`，根据 `test_generator.class_indices` 该位置对应 `daisy`（雏菊）。同样，后两张图像的最大概率出现在位置
    `4`，该位置对应 `tulips`（郁金香）。
- en: 'Then, apply the helper function to each row of the prediction output, and insert
    test image filenames (`test_generator.filenames`) alongside the prediction in
    a nicely formatted pandas DataFrame:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将辅助函数应用于预测输出的每一行，并将测试图像的文件名（`test_generator.filenames`）与预测结果一起插入到格式良好的 pandas
    DataFrame 中：
- en: '[PRE55]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The results should look similar to the following diagram. Now you may save
    the pandas DataFrame to any format of your choice, such as a CSV file or a pickle:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该类似于以下图示。现在，您可以将 pandas DataFrame 保存为任何您选择的格式，例如 CSV 文件或 pickle 文件：
- en: '![Figure 4.2 – Prediction output in a DataFrame format'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 以 DataFrame 格式显示的预测输出'
- en: '](img/Figure_4.2.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.2.jpg)'
- en: Figure 4.2 – Prediction output in a DataFrame format
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 以 DataFrame 格式显示的预测输出
- en: This completes the demonstration of using a pretrained model from TensorFlow
    Hub, applying it to our own data, retraining the model, and making the prediction.
    We also saw how to leverage generators to ingest training data in batches to the
    model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了使用来自 TensorFlow Hub 的预训练模型的演示，将其应用到我们自己的数据，重新训练模型并进行预测。我们还展示了如何利用生成器批量地将训练数据输入模型。
- en: TensorFlow Hub sits at the highest level of model reusability. There, you will
    find many open source models already built for consumption via a technique known
    as transfer learning. In this chapter, we built a regression model using the `tf.keras`
    API. Building a model this way (custom) is actually not a straightforward task.
    Often, you will spend a lot of time experimenting with different model parameters
    and architectures. If your need falls into classification or regression problems
    that are compatible with pre-built open source models, then TensorFlow Hub is
    the one-stop shop for finding the classification or regression model for your
    data. However, for these pre-built models, you still need to investigate the data
    structure required for the input layer and provide a final output layer for your
    purpose. However, reusing these pre-built models in TensorFlow Hub will save time
    in building and debugging your own model architecture.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub 位于模型重用性的最高层次。在那里，你可以找到许多已经构建好的开源模型，可以通过一种称为迁移学习的技术进行使用。在本章中，我们使用
    `tf.keras` API 构建了一个回归模型。以这种方式（自定义）构建模型实际上并不是一件简单的事。通常，你需要花费大量时间实验不同的模型参数和架构。如果你的需求涉及分类或回归问题，并且与预构建的开源模型兼容，那么
    TensorFlow Hub 是寻找适合你数据的分类或回归模型的一站式商店。然而，对于这些预构建的模型，你仍然需要调查输入层所需的数据结构，并为你的目的提供最终的输出层。然而，重用
    TensorFlow Hub 中的这些预构建模型将节省你在构建和调试自己模型架构上的时间。
- en: In the next section, we are going to see the TensorFlow Keras API, which is
    the newest high-level API that provides many reusable models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到 TensorFlow Keras API，它是最新的高级 API，提供了许多可重用的模型。
- en: Leveraging the TensorFlow Keras API
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 TensorFlow Keras API
- en: Keras is a deep learning API that wraps around machine learning libraries such
    as TensorFlow, Theano, and Microsoft Cognitive Toolkit (also known as CNTK). Its
    popularity as a standalone API stems from the succinct style of the model construction
    process. As of 2018, TensorFlow added Keras as a high-level API moving forward,
    and it is now known as `tf.keras`. Starting with the TensorFlow 2.0 distribution
    released in 2019, `tf.keras` has become the official high-level API.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个深度学习 API，封装了 TensorFlow、Theano 和微软认知工具包（也称为 CNTK）等机器学习库。作为一个独立的 API，Keras
    之所以受欢迎，源于它简洁的模型构建过程。从 2018 年起，TensorFlow 将 Keras 作为未来的高级 API 添加进来，现在被称为 `tf.keras`。自
    2019 年发布 TensorFlow 2.0 版本以来，`tf.keras` 已成为官方的高级 API。
- en: '`tf.keras` excels at modeling sophisticated deep learning architecture that
    contains `tf.keras` dense layer to build a regression model with tabular data
    from BigQuery.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras` 擅长建模复杂的深度学习架构，包含 `tf.keras` 密集层，用于构建一个回归模型，处理来自 BigQuery 的表格数据。'
- en: Data acquisition
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据获取
- en: 'We are going to use a publicly available dataset from Google Cloud as the working
    data for this example:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Google Cloud 上的一个公开数据集作为本示例的工作数据：
- en: 'This is our table of interest:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们感兴趣的表格：
- en: '[PRE56]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You may find it in BigQuery:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在 BigQuery 中找到它：
- en: '![Figure 4.3 – BigQuery portal and table selection'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.3 – BigQuery 门户和表格选择'
- en: '](img/Figure_4.3.jpg)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_4.3.jpg)'
- en: Figure 4.3 – BigQuery portal and table selection
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.3 – BigQuery 门户和表格选择
- en: 'Let''s take a look at the data by running the following query:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过运行以下查询来查看数据：
- en: '[PRE57]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The preceding query helps us to retrieve 1,000 random rows of the table: `data.covid19_geotab_mobility_impact.us_border_volumes`.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的查询帮助我们检索了表格 `data.covid19_geotab_mobility_impact.us_border_volumes` 中的 1,000
    行随机数据。
- en: 'And this is the output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.4 – Table columns in us_border_volumes'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – us_border_volumes 表格的列'
- en: '](img/Figure_4.4.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.4.jpg)'
- en: Figure 4.4 – Table columns in us_border_volumes
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – us_border_volumes 表格的列
- en: Solving a data science problem with the us_border_volumes table
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 us_border_volumes 表格解决数据科学问题
- en: The output consists of rows that are randomly selected from the table being
    queried. Your output will consist of different values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 输出由从查询的表格中随机选出的行组成。你的输出将包含不同的值。
- en: In the `us_border_volumes` table, each record represents a truck's entry or
    exit at a USA border point. The attributes in each record are `trip_direction`,
    `day_type`, `day_of_week`, `date`, `avg_crossing_duration`, `percent_of_normal_volume`,
    `avg_crossing_duration_truck`, and `percent_of_nortal_volume_truck`. We would
    like to build a model that predicts how long it would take for a truck to cross
    a border, given these features.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在`us_border_volumes`表中，每条记录表示一辆卡车在美国边境口岸的进出。每条记录中的属性有`trip_direction`、`day_type`、`day_of_week`、`date`、`avg_crossing_duration`、`percent_of_normal_volume`、`avg_crossing_duration_truck`和`percent_of_nortal_volume_truck`。我们希望构建一个模型，根据这些特征预测卡车通过边境所需的时间。
- en: Selecting features and a target for model training
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择特征和目标进行模型训练
- en: Here is an example problem that we will use to demonstrate how to leverage a
    TensorFlow I/O pipeline to provide training data to a model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例问题，我们将使用它来演示如何利用TensorFlow I/O管道为模型提供训练数据。
- en: Let's set this problem up as a regression problem with this data. We are going
    to build a regression model to predict the average time it takes for a truck to
    cross the border (`avg_crossing_duration_truck`). Other columns (except `date`)
    are the features.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个问题设置为一个回归问题，使用这些数据。我们将构建一个回归模型来预测卡车通过边境的平均时间（`avg_crossing_duration_truck`）。其他列（除了`date`）为特征。
- en: Streaming training data
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式训练数据
- en: For the rest of this example, we are going to use the Google AI Platform JupyterLab
    notebook with TensorFlow Enterprise 2.1 distribution. You may reuse the project
    ID from [*Chapter 2*](B16070_02_Final_JM_ePub.xhtml#_idTextAnchor061), *Running
    TensorFlow Enterprise in Google AI Platform*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的例子中，我们将使用Google AI平台的JupyterLab笔记本，配备TensorFlow Enterprise 2.1发行版。你可以重新使用[*第二章*](B16070_02_Final_JM_ePub.xhtml#_idTextAnchor061)中的项目ID，*在Google
    AI平台上运行TensorFlow Enterprise*。
- en: 'Having identified the data source, we are going to build a streaming workflow
    to feed training data to the model. This is different from reading the table as
    a pandas DataFrame in the Python runtime. We want to stream the training data
    by batches rather than using up all the memory allocated for the Python runtime.
    Therefore, we are going to use TensorFlow I/O for streaming the training data
    from BigQuery:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 确定数据源后，我们将构建一个流式工作流程，将训练数据传输到模型中。这不同于在Python运行时将表读取为pandas DataFrame。我们希望通过批次流式传输训练数据，而不是耗尽为Python运行时分配的所有内存。因此，我们将使用TensorFlow
    I/O从BigQuery流式传输训练数据：
- en: 'We will start with the following code to import the necessary libraries and
    set up environmental variables:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从以下代码开始，导入必要的库并设置环境变量：
- en: '[PRE58]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Create a session to read from BigQuery:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个会话来读取BigQuery：
- en: '[PRE59]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We have just selected fields of interest from the table in BigQuery. Now that
    the table is read as a dataset, we need to designate each column as either features
    or target. Let''s use this helper function:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们刚刚从BigQuery中的表中选择了感兴趣的字段。现在，表已经作为数据集被读取，我们需要将每一列指定为特征或目标。让我们使用这个辅助函数：
- en: '[PRE60]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now we apply this function to each row of the training dataset. This is essentially
    a transformation of the dataset, because we are applying a function that splits
    a dataset into a tuple of two dictionaries—features and target:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将这个函数应用于训练数据集的每一行。这本质上是对数据集的一次转换，因为我们应用了一个函数，它将数据集拆分为两个字典的元组——特征和目标：
- en: '[PRE61]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'And now we will shuffle the dataset and batch it:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将对数据集进行洗牌并分批处理：
- en: '[PRE62]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In this section, we identified a table from BigQuery, identified the feature
    and target columns, convert the table to a TensorFlow dataset, shuffled it, and
    batched it. This is a common technique that is preferred when you are not sure
    if data size presents a problem in terms of memory usage.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们从BigQuery中识别了一个表，确定了特征和目标列，将表转换为TensorFlow数据集，对其进行了洗牌并分批。这是当你不确定数据量是否会导致内存使用问题时常用的技术。
- en: For the next section, we are going to look at the `tf.keras` API and how to
    use it to build and train a model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将查看`tf.keras` API以及如何使用它来构建和训练模型。
- en: Input to a model
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型的输入
- en: 'So far, we have taken care of specifying features and the target in the training
    dataset. Now, we need to specify each feature as either categorical or numeric.
    This requires us to set up TensorFlow''s `feature_columns` object. The `feature_columns`
    object is the input to the model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经处理了在训练数据集中指定特征和目标的问题。现在，我们需要将每个特征指定为类别型或数值型。这需要我们设置TensorFlow的`feature_columns`对象。`feature_columns`对象是模型的输入：
- en: 'For each categorical column, we need to keep track of the possible categories.
    This is done through a helper function:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个类别列，我们需要跟踪可能的类别。通过以下辅助函数来实现：
- en: '[PRE63]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we can create the `feature_columns` object (which is really a Python
    list) with the following code snippet:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码片段创建`feature_columns`对象（实际上是一个Python列表）：
- en: '[PRE64]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Notice that the target column is not in the `feature_columns`.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，目标列不在`feature_columns`中。
- en: 'Now all we have to do is create a layer that creates the input to the model.
    The first layer is the feature columns'' input to the model, which is a multilayer
    perceptron as defined by a series of reusable `Dense` layers:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们只需创建一个层来作为模型的输入。第一个层是特征列输入模型的层，这是一个多层感知机，由一系列可重用的`Dense`层定义：
- en: '[PRE65]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In this section, we created a flow to ingest our dataset into the model's feature
    layer. During this process, for categorical columns, we have to one-hot encode
    it as these columns are not numeric. We then build a model architecture with the
    `tf.keras` API.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了一个流程，将数据集导入到模型的特征层。在此过程中，对于类别列，我们必须进行独热编码，因为这些列不是数值型的。接着，我们使用`tf.keras`
    API构建了一个模型架构。
- en: Next, we are going to compile this model and launch the training process.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译这个模型并启动训练过程。
- en: Model training
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Before the model can be used, we need to compile it. Since this is a regression
    model, we may specify `loss` function, and for training metrics, we will track
    MSE as well as **mean-absolute-error** (**MAE**):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型可用之前，我们需要对其进行编译。由于这是一个回归模型，我们可以指定`loss`函数，对于训练指标，我们将跟踪MSE以及**平均绝对误差**（**MAE**）：
- en: 'Compile the model with the proper `loss` function and metrics used in the regression
    task:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适当的`loss`函数和回归任务中使用的指标来编译模型：
- en: '[PRE66]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Train the model:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE67]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Once the model is trained, we may create a sample test dataset with two observations.
    The test data has to be in a dictionary format:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以创建一个包含两个观测值的样本测试数据集。测试数据必须采用字典格式：
- en: '[PRE68]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To score this test sample, execute the following code:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了对这个测试样本进行评分，执行以下代码：
- en: '[PRE69]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE70]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This indicates the predicted average number of waiting minutes for a truck to
    cross the border (`avg_crossing_duration_truck`).
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表示预测的卡车过境平均等待时间（`avg_crossing_duration_truck`）。
- en: We just learned how to reuse the `tf.keras` dense layer and the sequential API,
    and integrate it with a data input pipeline driven by the use of datasets for
    streaming, and `feature_column` objects for feature encoding.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了如何重用`tf.keras`的全连接层和顺序API，并将其与一个由数据集流驱动的数据输入管道进行整合，使用`feature_column`对象进行特征编码。
- en: '`tf.keras` is a high-level API that provides another set of reusable elements
    specifically for deep learning problems. If your solution requires deep learning
    techniques, then `tf.keras` is the recommended starting point.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras`是一个高级API，提供了一组专门用于深度学习问题的可重用元素。如果你的解决方案需要深度学习技术，那么`tf.keras`是推荐的起点。'
- en: In the next section, we are going to take a look at another high-level API known
    as TensorFlow Estimators. Before the `tf.keras` API became a first-class citizen
    in TensorFlow and in the 1.x TensorFlow distribution, TensorFlow Estimators was
    the only high-level API available.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将查看另一个高级API——TensorFlow Estimators。在`tf.keras` API成为TensorFlow的核心API之前，以及在1.x版本的TensorFlow中，TensorFlow
    Estimators是唯一可用的高级API。
- en: So, in the next section, we will take a look at how it works.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在接下来的部分，我们将查看它是如何工作的。
- en: Working with TensorFlow Estimators
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow Estimators
- en: TensorFlow estimators are also reusable components. The Estimators are higher-level
    APIs that enable users to build, train, and deploy machine learning models. It
    has several pre-made models that can save users from the hassle of creating computational
    graphs or sessions. This makes it easier for users to try different model architectures
    quickly with limited code changes. The Estimators are not specifically dedicated
    to deep learning models in the same way as `tf.keras`. Therefore, you will not
    find a lot of pre-made deep learning models available. If you need to work with
    deep learning frameworks, then the `tf.keras` API is the right choice to get started.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow估算器也是可复用的组件。估算器是更高层的API，允许用户构建、训练和部署机器学习模型。它有几个预制的模型，可以节省用户创建计算图或会话的麻烦。这使得用户能够在有限的代码修改下快速尝试不同的模型架构。估算器并不像`tf.keras`那样专门面向深度学习模型。因此，你不会找到很多预制的深度学习模型。如果你需要使用深度学习框架，`tf.keras`
    API是入门的正确选择。
- en: 'For this example, we are going to set up the same regression problem and build
    a regression model. The source of data is the same one we used in streaming training
    data, which is available through Google Cloud''s BigQuery:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将设置相同的回归问题并构建一个回归模型。数据来源是我们在流式训练数据中使用的相同数据，通过Google Cloud的BigQuery提供：
- en: '[PRE71]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This is the same BigQuery table (*Figure 4.4*) that we used for the `tf.keras`
    section. See *Figure 4.4* for some randomly extracted rows of this table.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在`tf.keras`部分使用的同一个BigQuery表（*图 4.4*）。请参阅*图 4.4*，其中展示了从该表随机提取的几行数据。
- en: Just as we did in the previous section using the `tf.keras` API, here we want
    to build a linear regression model using TensorFlow Estimators to predict the
    average time it takes for a truck to cross the border (`avg_crossing_duration_truck`).
    Other columns (except `date`) are the features.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在上一节中使用`tf.keras` API一样，这里我们希望使用TensorFlow Estimators构建一个线性回归模型，来预测卡车过境的平均时间（`avg_crossing_duration_truck`）。其他列（除`date`外）是特征。
- en: The pattern of using the TensorFlow Estimators API to build and train a model
    is as follows.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow Estimators API构建和训练模型的模式如下。
- en: Create an `estimator` object by invoking an estimator (that is, for a pre-made
    estimator such as a linear regressor) and specify the `feature_columns` object,
    so the model knows what data types to expect in the feature data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用估算器（即对于预制的估算器，如线性回归器）并指定`feature_columns`对象来创建`estimator`对象，这样模型就知道在特征数据中应该期待哪些数据类型。
- en: 'Use the `estimator` object to `call .train()` and pass an input function to
    it. This input function is responsible for parsing training data and the label.
    Since we are setting up a regression problem, let''s use the pre-made linear regression
    estimator as an example. This is the common pattern for training:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`estimator`对象调用`.train()`并传递一个输入函数给它。这个输入函数负责解析训练数据和标签。由于我们正在设置一个回归问题，接下来以预制的线性回归估算器为例。这是训练过程的常见模式：
- en: '[PRE74]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'From the preceding code, the following is observed:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，可以观察到以下内容：
- en: First, an instance of a linear regressor, `linear_est`, is created with a `feature_columns`
    object. This object provides an annotation regarding each feature (numeric or
    categorical data types). `model_dir` is the specified directory to save the model
    by checkpoints.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，创建一个线性回归器实例`linear_est`，并使用`feature_columns`对象。这个对象提供关于每个特征的注释（数字或类别数据类型）。`model_dir`是保存模型检查点的指定目录。
- en: Next in the code is `linear_est.train(input_fn)`. This instance invokes the
    `train()` method to start the training process. The `train()` method takes on
    a function, `input_fn`. This is responsible for streaming, formatting, and sending
    the training data by batches into the model. We will take a look at how to construct
    `input_fn`. In other words, TensorFlow Estimators separates data annotation from
    the data ingestion process for training workflows.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的代码是`linear_est.train(input_fn)`。这个实例调用`train()`方法来启动训练过程。`train()`方法接收一个函数`input_fn`，它负责将训练数据按批次流式传送并格式化到模型中。接下来我们会了解如何构建`input_fn`。换句话说，TensorFlow
    Estimators将数据注解与训练工作流中的数据摄取过程分开。
- en: Data pipeline for TensorFlow Estimators
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Estimators的数据管道
- en: Like `tf.keras`, TensorFlow Estimators can leverage the streaming data pipeline
    when running in the TensorFlow Enterprise environment, such as the Google Cloud
    AI Platform. In this section, as an example, we are going to see how to stream
    training data (from a table in BigQuery) into a TensorFlow Estimator model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 像`tf.keras`一样，TensorFlow Estimators在TensorFlow企业环境中运行时，可以利用流式数据管道，例如在Google
    Cloud AI平台上。在本节中，作为示例，我们将看到如何将训练数据（来自BigQuery中的一个表）流式传输到TensorFlow Estimator模型中。
- en: Below are the steps to building a BigQuery data pipeline for TensorFlow Estimator's
    consumption.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为TensorFlow Estimator构建BigQuery数据管道的步骤。
- en: 'As usual, we start with the `import` operations for the requisite libraries:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，我们从所需库的`import`操作开始：
- en: '[PRE76]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now we specify a few parameters for our table of interest in BigQuery. Make
    sure you specify your own `PROJECT_ID`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们为BigQuery中的目标表指定一些参数。确保指定你自己的`PROJECT_ID`：
- en: '[PRE77]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we will specify the input function for the training process. This input
    function will handle the read operation, data annotation, transformation, and
    separation of the target from features by means of the `transform_row` function.
    These are exactly the identical operations seen in the previously described `tf.keras`
    example in the *Leveraging TensorFlow Keras API* section. The only difference
    is that we now wrap all of the code as follows:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将指定训练过程的输入函数。这个输入函数将通过`transform_row`函数处理读取操作、数据注解、转换，以及将目标与特征分离。这些操作与之前在*利用TensorFlow
    Keras API*部分中描述的`tf.keras`示例完全相同。唯一的区别是我们现在将所有代码封装如下：
- en: '[PRE78]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We are still inside `input_fn`. Continue on with `input_fn`:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们仍然在`input_fn`内部。继续处理`input_fn`：
- en: We also reorganized how we specify features and the target in our data with
    the `transform_row` function inside `input_fn`.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还重新组织了如何在数据中指定特征和目标，通过`input_fn`内部的`transform_row`函数。
- en: '[PRE79]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This concludes the entire `input_fn`. At this point, `input_fn` returns a dataset
    read from `us_border_volumes`.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是整个`input_fn`。此时，`input_fn`返回从`us_border_volumes`读取的数据集。
- en: 'Just as we discussed in the `tf.keras` example in the *Leveraging TensorFlow
    Keras API* section, we also need to build a `feature_columns` object for feature
    annotation. We can reuse the same code:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们在*利用TensorFlow Keras API*部分中讨论的`tf.keras`示例一样，我们也需要构建一个`feature_columns`对象来进行特征注解。我们可以重用相同的代码：
- en: '[PRE80]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now, let''s set up a directory to save the model checkpoints:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置一个目录来保存模型的检查点：
- en: '[PRE81]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Use the following command to make the directory:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建目录：
- en: '[PRE82]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Launch the training process:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动训练过程：
- en: '[PRE83]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: This completes the model training process.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了模型训练过程。
- en: Since the `estimator` model expects input to be a function, in order to use
    the `estimator` for scoring, I have to pass into it a function that takes the
    test data, formats it, and feeds it to the model just like the training data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`estimator`模型期望输入为一个函数，为了使用`estimator`进行评分，我必须传入一个函数，该函数接收测试数据，格式化它，并像训练数据一样将其输入到模型中。
- en: 'The input function for training basically did two things:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的输入函数基本上做了两件事：
- en: It queried the table and got the dataset representation of the table.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它查询了表格并获取了表格的数据集表示。
- en: It transformed the data by separating the target from the features.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过将目标与特征分离来转换数据。
- en: 'In terms of the scoring situation here, we do not need to worry about this.
    We just need to get a dataset representation of the test data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这里的评分情况，我们不需要担心。我们只需要获取测试数据的数据集表示：
- en: 'We can reuse the same test data as shown in the *Model training* section:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以重用*模型训练*部分中展示的相同测试数据：
- en: '[PRE84]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Create a helper function to convert `test_samples` to a dataset with the following
    code:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个辅助函数，通过以下代码将`test_samples`转换为数据集：
- en: '[PRE85]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The next step involves scoring the test data with the following lines of code:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用以下代码行对测试数据进行评分：
- en: '[PRE86]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Finally, let''s print the prediction as shown in the following code:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们打印预测结果，如下所示：
- en: '[PRE87]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: In the preceding code, we iterate through the model output, which is a dictionary.
    To refer to the values associated with the model output, we need to access it
    by the key named `prediction`. For readability, we convert it to a list and print
    it out as a string. It shows the first truck is predicted to cross the border
    in `23.87` minutes, while the second truck is predicted to cross the border in
    `13.62` minutes.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们遍历了模型的输出，该输出是一个字典。为了引用与模型输出相关的值，我们需要通过名为 `prediction` 的键来访问它。为了提高可读性，我们将其转换为列表并以字符串的形式打印出来。它显示了第一辆卡车预计在
    `23.87` 分钟内通过边境，而第二辆卡车预计在 `13.62` 分钟内通过边境。
- en: TensorFlow Estimators was the only high-level API before `tf.keras` became an
    official part of TensorFlow distribution. While it contains many pre-made modules,
    such as linear regressors and different flavors of classifiers, it lacks the support
    for some of the commonplace deep learning modules, including CNN, LSTM, and GRU.
    But if your need can be addressed with non-deep learning regressors or classifiers,
    then TensorFlow Estimators is a good starting point. It also integrates with the
    data ingestion pipeline.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `tf.keras` 成为 TensorFlow 发布的正式部分之前，TensorFlow Estimators 是唯一的高层 API。尽管它包含了许多预构建的模块，如线性回归器和不同版本的分类器，但它缺乏对一些常见深度学习模块的支持，包括
    CNN、LSTM 和 GRU。但是，如果你的需求可以通过非深度学习回归器或分类器来解决，那么 TensorFlow Estimators 是一个不错的起点。它也可以与数据摄取管道进行集成。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you have seen how the three major sources of reusable model
    elements can integrate with the scalable data pipeline. Through TensorFlow datasets
    and TensorFlow I/O APIs, training data is streamed into the model training process.
    This enables models to be trained without having to deal with the compute node's
    memory.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经看到了三大可重用模型元素源如何与可扩展的数据管道集成。通过 TensorFlow 数据集和 TensorFlow I/O API，训练数据被流式传输到模型训练过程中。这使得模型能够在无需处理计算节点内存的情况下进行训练。
- en: TensorFlow Hub sits at the highest level of model reusability. There, you will
    find many open source models already built for consumption via a technique known
    as transfer learning. In this chapter, we built a regression model using the `tf.keras`
    API. Building a model this way (custom) is actually not a straightforward task.
    Often, you will spend a lot of time experimenting with different model parameters
    and architectures. If your need can be addressed by means of pre-built open source
    models, then TensorFlow Hub is the place. However, for these pre-built models,
    you still need to investigate the data structure required for the input layer,
    and provide a final output layer for your purpose. However, reusing these pre-built
    models in TensorFlow Hub will save time in building and debugging your own model
    architecture.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub 位于模型可重用性的最高层级。在这里，你将找到许多已经构建好的开源模型，这些模型可以通过一种称为迁移学习的技术进行使用。在本章中，我们使用
    `tf.keras` API 构建了一个回归模型。以这种方式（自定义）构建模型实际上并不是一项简单的任务。通常，你需要花费大量时间来尝试不同的模型参数和架构。如果你的需求可以通过预构建的开源模型来满足，那么
    TensorFlow Hub 就是你理想的选择。然而，对于这些预构建的模型，你仍然需要调查输入层所需的数据结构，并为你的目的提供最终的输出层。尽管如此，在
    TensorFlow Hub 中重用这些预构建的模型将节省构建和调试自己模型架构的时间。
- en: '`tf.keras` is a high-level API that provides another set of reusable elements
    specifically for deep learning problems. If your solution requires deep learning
    techniques, then `tf.keras` is the recommended starting point. With the help of
    an example, we have seen how a multilayer perceptron can be built quickly with
    the `tf.keras` API and integrated with the TensorFlow I/O module that streams
    training data.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras` 是一个高层次的 API，专门为深度学习问题提供一组可重用的元素。如果你的解决方案需要深度学习技术，那么 `tf.keras` 是推荐的起点。在一个示例的帮助下，我们已经看到了如何使用
    `tf.keras` API 快速构建一个多层感知机，并与 TensorFlow I/O 模块结合，后者负责流式传输训练数据。'
- en: In the next chapter, we are going to take up what we learned about `tf.keras`
    and TensorFlow Hub here, and leverage Google Cloud AI Platform to run our model
    training routine as a cloud training job.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探讨在这里学到的关于 `tf.keras` 和 TensorFlow Hub 的内容，并利用 Google Cloud AI Platform
    将我们的模型训练流程作为云端训练任务来运行。
