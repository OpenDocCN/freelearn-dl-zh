- en: '*Chapter 6*: Generative Models and Adversarial Attacks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：生成模型与对抗攻击'
- en: Being able to differentiate between two or more classes is certainly impressive,
    and a healthy sign that deep neural networks do, in fact, learn.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 能够区分两个或更多类别无疑是令人印象深刻的，且是深度神经网络确实在学习的健康信号。
- en: But if traditional classification is impressive, then producing new content
    is staggering! That definitely requires a superior understanding of the domain.
    So, are there neural networks capable of such a feat? You bet there are!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果传统的分类任务令人印象深刻，那么生成新内容则令人叹为观止！这绝对需要对领域有更高的理解。那么，有没有神经网络能够做到这一点呢？当然有！
- en: 'In this chapter, we''ll study one of the most captivating and promising types
    of neural networks: **Generative Adversarial Networks** (**GANs**). As the term
    implies, these networks are actually a system comprised of two sub-networks: the
    generator and the discriminator. The job of the generator is to produce images
    so good that they *could* come from the original distribution (but actually don''t;
    they''re generated from scratch), thereby fooling the discriminator, whose task
    is to discern between real and fake images.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究神经网络中最迷人且最有前景的一种类型：**生成对抗网络**（**GANs**）。顾名思义，这些网络实际上是由两个子网络组成的系统：生成器和判别器。生成器的任务是生成足够优秀的图像，使它们*看起来*像是来自原始分布（但实际上并非如此；它们是从零开始生成的），从而欺骗判别器，而判别器的任务是分辨真假图像。
- en: '**GANs** are the tip of the spear in areas such as semi-supervised learning
    and image-to-image translation, both topics that we will cover in this chapter.
    As a complement, the final recipe in this chapter teaches us how to perform an
    adversarial attack on a network using the **Fast Gradient Signed Method** (**FGSM**).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**GANs** 在半监督学习和图像到图像的翻译等领域处于尖端位置，这两个主题我们将在本章中讨论。作为补充，本章最后的食谱将教我们如何使用**快速梯度符号方法**（**FGSM**）对网络进行对抗攻击。'
- en: 'The recipes that we will cover in this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涉及的食谱如下：
- en: Implementing a deep convolutional GAN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个深度卷积 GAN
- en: Using a DCGAN for semi-supervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DCGAN 进行半监督学习
- en: Translating images with Pix2Pix
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pix2Pix 进行图像翻译
- en: Translating unpaired images with CycleGAN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CycleGAN 翻译未配对的图像
- en: Implementing an adversarial attack using the Fast Gradient Signed Method
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用快速梯度符号方法实现对抗攻击
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'GANs are great, but also extremely taxing in terms of computing power. Therefore,
    a GPU is a must-have in order to work on these recipes (and even then, most will
    run for several hours). In the *Getting ready* section, you''ll find the preparations
    that are necessary, if any, for each recipe. The code for this chapter is available
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 很棒，但在计算能力方面非常消耗资源。因此，GPU 是必不可少的，才能在这些食谱上进行操作（即使如此，大多数情况仍需运行几个小时）。在*准备工作*部分，你会发现每个食谱所需的准备工作（如果有的话）。本章的代码可以在这里找到：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6)。
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/35Z8IYn](https://bit.ly/35Z8IYn).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看《代码实践》视频：[https://bit.ly/35Z8IYn](https://bit.ly/35Z8IYn)。
- en: Implementing a deep convolutional GAN
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个深度卷积 GAN
- en: A `seed`, which is just a vector of Gaussian noise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `seed`，它只是一个高斯噪声的向量。
- en: In this recipe, we'll implement a `EMNIST`, a dataset that extends the well-known
    `MNIST` dataset with uppercase and lowercase handwritten letters on top of the
    digits from 0 to 9\.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将实现一个 `EMNIST` 数据集，它是在原有的 `MNIST` 数据集的基础上，加入了大写和小写的手写字母，并涵盖了从 0 到 9
    的数字。
- en: Let's begin!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll need to install `tensorflow-datasets` to access `EMNIST` more easily.
    Also, in order to display a nice progress bar during the training of our GAN,
    we'll use `tqdm`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装 `tensorflow-datasets` 来更方便地访问 `EMNIST`。另外，为了在训练 GAN 时显示漂亮的进度条，我们将使用 `tqdm`。
- en: 'Both dependencies can be installed as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个依赖项可以按如下方式安装：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are good to go!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始了！
- en: How to do it…
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Perform the following steps to implement a DCGAN on `EMNIST`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来在 `EMNIST` 上实现 DCGAN：
- en: 'Import the necessary dependencies:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的依赖项：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define an alias for the `AUTOTUNE` setting, which we''ll use later to determine
    the number of parallel calls when processing the images in the dataset:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `AUTOTUNE` 设置的别名，我们将在后续处理中使用它来确定处理数据集时的并行调用数量：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define a `DCGAN()` class to encapsulate our implementation. The constructor
    creates the discriminator, generator, loss function, and the respective optimizers
    for both sub-networks:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `DCGAN()` 类来封装我们的实现。构造函数创建判别器、生成器、损失函数以及两个子网络各自的优化器：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define a static method to create the generator network. It reconstructs a 28x28x1
    image from an input tensor of 100 elements. Notice the use of transposed convolutions
    (`Conv2DTranspose`) to expand the output volumes as we go deeper into the network.
    Also, notice the activation is `''tanh''`, which means the outputs will be in
    the range [-1, 1]:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个静态方法来创建生成器网络。它从一个 100 元素的输入张量重建一个 28x28x1 的图像。注意，使用了转置卷积（`Conv2DTranspose`）来扩展输出体积，随着网络的深入，卷积层数量也增多。同时，注意激活函数为
    `'tanh'`，这意味着输出将处于 [-1, 1] 的范围内：
- en: '[PRE4]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Add the first transposed convolution block, with 128 filters:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加第一个转置卷积块，具有 128 个滤波器：
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create the second transposed convolution block, with 64 filters:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建第二个转置卷积块，具有 64 个滤波器：
- en: '[PRE6]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Add the last transposed convolution block, with only one filter, corresponding
    to the output of the network:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加最后一个转置卷积块，只有一个滤波器，对应于网络的输出：
- en: '[PRE7]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define a static method to create the discriminator. This architecture is a
    regular CNN:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个静态方法来创建判别器。该架构是一个常规的 CNN：
- en: '[PRE8]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define a method to calculate the discriminator''s loss, which is the sum of
    the real and fake losses:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算判别器的损失，它是实际损失和假损失的总和：
- en: '[PRE9]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define a method to calculate the generator''s loss:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算生成器的损失：
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define a method to perform a single training step. We''ll start by generating
    a vector of random Gaussian noise:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来执行单次训练步骤。我们将从生成一个随机高斯噪声向量开始：
- en: '[PRE11]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, pass the random noise to the generator to produce fake images:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将随机噪声传递给生成器以生成假图像：
- en: '[PRE12]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Pass the real and fake images to the discriminator and compute the losses of
    both sub-networks:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将真实图像和假图像传递给判别器，并计算两个子网络的损失：
- en: '[PRE13]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Compute the gradients:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度：
- en: '[PRE14]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, apply the gradients using the respective optimizers:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用各自的优化器应用梯度：
- en: '[PRE15]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, define a method to train the whole architecture. Every 10 epochs,
    we will plot the images the generator produces in order to visually assess their
    quality:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，定义一个方法来训练整个架构。每训练 10 个周期，我们将绘制生成器生成的图像，以便直观地评估它们的质量：
- en: '[PRE16]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define a function to produce new images, and then save a 4x4 mosaic of them
    to disk:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来生成新图像，然后将它们的 4x4 马赛克保存到磁盘：
- en: '[PRE17]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define a function to scale the images that come from the `EMNIST` dataset to
    the [-1, 1] interval:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来将来自 `EMNIST` 数据集的图像缩放到 [-1, 1] 区间：
- en: '[PRE18]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load the `EMNIST` dataset using `tfds`. We''ll only use the `''train''` split,
    which contains more than 600,000 images. We will also make sure to scale each
    image to the `''tanh''` range:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tfds` 加载 `EMNIST` 数据集。我们只使用 `'train'` 数据集，其中包含超过 60 万张图像。我们还会确保将每张图像缩放到
    `'tanh'` 范围内：
- en: '[PRE19]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a test seed that will be used throughout the training of the DCGAN to
    generate images:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个测试种子，在整个 DCGAN 训练过程中用于生成图像：
- en: '[PRE20]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, instantiate and train a `DCGAN()` instance for 200 epochs:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，实例化并训练一个 `DCGAN()` 实例，训练 200 个周期：
- en: '[PRE21]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first image generated by the GAN will look similar to this, just a collection
    of shapeless blobs:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由 GAN 生成的第一张图像将类似于这个，只是一些没有形状的斑点：
- en: '![Figure 6.1 – Images generated at epoch 0'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1 – 在第 0 个周期生成的图像'
- en: '](img/B14768_06_001.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_06_001.jpg)'
- en: Figure 6.1 – Images generated at epoch 0
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 在第 0 个周期生成的图像
- en: 'At the end of the training process, the results are much better:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程结束时，结果要好得多：
- en: '![Figure 6.2 – Images generated at epoch 200'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 在第 200 个周期生成的图像'
- en: '](img/B14768_06_002.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_06_002.jpg)'
- en: Figure 6.2 – Images generated at epoch 200
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 在第 200 个周期生成的图像
- en: In *Figure 6.2*, we can distinguish familiar letters and numbers, including
    *A*, *d*, *9*, *X*, and *B*. However, in the first row, we notice a couple of
    ambiguous forms, which is a sign that the generator has room for improvement.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 6.2* 中，我们可以辨认出熟悉的字母和数字，包括 *A*、*d*、*9*、*X* 和 *B*。然而，在第一行中，我们注意到几个模糊的形状，这表明生成器还有改进的空间。
- en: Let's see how it all works in the next section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中看看它是如何工作的。
- en: How it works…
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we learned that GANs work in tandem and, unlike autoencoders,
    they work against each other (hence the *adversarial* in the name) instead of
    cooperating. When our focus is on the generator, the discriminator is just a tool
    to train the latter, as is the case in this recipe. This means that after training,
    the discriminator is tossed out.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们学到 GAN 是协同工作的，不像自编码器那样相互配合，它们是相互对抗的（因此名字中有 *对抗* 二字）。当我们专注于生成器时，判别器只是一个训练生成器的工具，正如本例中的情况一样。这意味着训练后，判别器会被丢弃。
- en: Our generator is actually a decoder that takes random Gaussian vectors of 100
    elements and produces 28x28x1 images that are then passed to the discriminator,
    a regular CNN, which has to guess whether they are real or fake.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的生成器实际上是一个解码器，它接收一个包含 100 个元素的随机高斯向量，并生成 28x28x1 的图像，接着这些图像被传递给判别器，一个常规的 CNN，判别器需要判断它们是真实的还是伪造的。
- en: Because our goal is to create the best generator possible, the classification
    problem the discriminator tries to solve has nothing to do with the actual classes
    in `EMNIST`. For this reason, we don't explicitly label the images as real or
    fake beforehand, but in the `discriminator_loss()` method, where we know that
    all images in `real` come from `EMNIST`, and therefore we compute the loss against
    a tensor of ones (`tf.ones_like(real)`) and, analogously, all images in `fake`
    are synthetic, and we compute the loss against a tensor of zeros (`tf.zeros_like(fake)`).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的目标是创造最好的生成器，所以判别器尝试解决的分类问题与 `EMNIST` 中的实际类别无关。因此，我们不会事先明确标记图像为真实或伪造，但在
    `discriminator_loss()` 方法中，我们知道所有来自 `real` 的图像都来自 `EMNIST`，因此我们对一个全为 1 的张量（`tf.ones_like(real)`）计算损失，类似地，所有来自
    `fake` 的图像是合成的，我们对一个全为 0 的张量（`tf.zeros_like(fake)`）计算损失。
- en: The generator, on the other hand, takes into consideration the feedback received
    from the discriminator when computing its loss to improve its outputs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成器在计算其损失时会考虑来自判别器的反馈，以改进其输出。
- en: It must be noted that the goal here is to achieve an equilibrium, instead of
    minimizing the loss. Therefore, visual inspection is crucial, and the reason why
    we save the images the generator produces every 10 epochs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意的是，这里的目标是实现平衡，而不是最小化损失。因此，视觉检查至关重要，这也是我们每隔 10 个周期保存生成器输出的图像的原因。
- en: In the end, we went from random, shapeless blobs at epoch 0 to recognizable
    digits and letters at epoch 200, although the network can be improved further.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们从第 0 个周期的随机、无形的块，到了第 200 个周期时，生成了可识别的数字和字母，尽管网络仍然可以进一步改进。
- en: See also
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can read more about `EMNIST` here: [https://arxiv.org/abs/1702.05373v1](https://arxiv.org/abs/1702.05373v1).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于 `EMNIST` 的内容：[https://arxiv.org/abs/1702.05373v1](https://arxiv.org/abs/1702.05373v1)。
- en: Using a DCGAN for semi-supervised learning
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DCGAN 进行半监督学习
- en: Data is the most important part of developing any deep learning model. However,
    good data is often scarce and expensive to acquire. The good news is that GANs
    can lend us a hand in these situations by artificially producing novel training
    examples, in a process known as **semi-supervised learning**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是开发任何深度学习模型中最重要的部分。然而，好的数据通常稀缺且获取成本高。好消息是，GAN 可以在这些情况下提供帮助，通过人工生成新颖的训练示例，这个过程被称为
    **半监督学习**。
- en: In this recipe, we'll develop a special DCGAN architecture to train a classifier
    on a very small subset of `Fashion-MNIST` and still achieve a decent performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将开发一个特殊的 DCGAN 架构，在 `Fashion-MNIST` 的一个非常小的子集上训练分类器，并仍然达到不错的性能。
- en: Let's begin, shall we?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧，怎么样？
- en: Getting ready
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We won''t require anything extra to access `Fashion-MNIST` because it comes
    bundled with TensorFlow. In order to display a nice-looking progress bar, let''s
    install `tqdm`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要额外的东西来访问 `Fashion-MNIST`，因为它与 TensorFlow 一起捆绑提供。为了显示一个好看的进度条，让我们安装 `tqdm`：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let's now move on to the next section to start the recipe's implementation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一部分，开始实现这个配方。
- en: How to do it…
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Perform the following steps to complete the recipe:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成配方：
- en: 'Let''s start by importing the required packages:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始导入所需的包：
- en: '[PRE23]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the `pick_supervised_subset()` function to pick a subset of the data.
    This will allow us to simulate a situation of scarce data, a perfect fit for semi-supervised
    learning:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `pick_supervised_subset()` 函数来选择数据的子集。这将帮助我们模拟数据稀缺的情况，非常适合半监督学习。
- en: '[PRE24]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, define a function to select a random sample of data for classification.
    This means that we''ll use the labels from the original dataset:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个函数来选择一个随机数据样本用于分类。这意味着我们将使用原始数据集中的标签：
- en: '[PRE25]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define the `pick_samples_for_discrimination()` function in order to select
    a random sample for discrimination. The main difference with the last function
    is that the labels here are all 1, indicating that all images are real, which
    clearly indicates that this sample is intended for the discriminator:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `pick_samples_for_discrimination()` 函数以选择一个随机样本用于判别。与上一个函数的主要区别在于这里的标签都是
    1，表示所有的图像都是真实的，这清楚地表明该样本是为判别器准备的：
- en: '[PRE26]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Implement the `generate_fake_samples()` function to produce a batch of latent
    points or, put another way, a sample of random noise vectors that the generator
    will use to generate fake images:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 `generate_fake_samples()` 函数来生成一批潜在点，换句话说，就是一组随机噪声向量，生成器将利用这些向量生成假图像：
- en: '[PRE27]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create the `generate_fake_samples()` function to generate fake data using the
    generator:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `generate_fake_samples()` 函数，用生成器生成假数据：
- en: '[PRE28]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We are ready to define our semi-supervised DCGAN, which we''ll encapsulate
    in the `SSGAN()` class defined here. We''ll start with the constructor:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好定义我们的半监督式 DCGAN，接下来将它封装在此处定义的 `SSGAN()` 类中。我们将从构造函数开始：
- en: '[PRE29]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After storing the arguments as members, let''s instantiate the discriminators:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将参数作为成员存储后，让我们实例化判别器：
- en: '[PRE30]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, compile both the classifier and discriminator models:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，编译分类器和判别器模型：
- en: '[PRE31]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create the generator:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建生成器：
- en: '[PRE32]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create the GAN and compile it:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 GAN 并进行编译：
- en: '[PRE33]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the private `_create_discriminators()` method to create the discriminators.
    The inner `custom_activation()` function is used to activate the outputs of the
    classifier model and generate a value between 0 and 1 that will be used to discern
    whether the image is real or fake:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义私有的 `_create_discriminators()` 方法来创建判别器。内部的 `custom_activation()` 函数用于激活分类器模型的输出，生成一个介于
    0 和 1 之间的值，用于判断图像是真实的还是假的：
- en: '[PRE34]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the classifier architecture, which is just a regular softmax-activated
    CNN:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义分类器架构，它只是一个常规的 softmax 激活的 CNN：
- en: '[PRE35]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The discriminator shares weights with the classifier, but instead of softmax
    activating the outputs, it uses the `custom_activation()` function defined previously:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判别器与分类器共享权重，但不同的是，它不再使用 softmax 激活输出，而是使用之前定义的 `custom_activation()` 函数：
- en: '[PRE36]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Return both the classifier and the discriminator:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回分类器和判别器：
- en: '[PRE37]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create the private `_create_generator()` method to implement the generator
    architecture, which is just a decoder, as explained in the first recipe in this
    chapter:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建私有的 `_create_generator()` 方法来实现生成器架构，实际上它只是一个解码器，正如本章第一节中所解释的那样：
- en: '[PRE38]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the private `_create_gan()` method to create the GAN itself, which is
    just the connection between the generator and the discriminator:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义私有的 `_create_gan()` 方法来创建 GAN 本身，实际上它只是生成器和判别器之间的连接：
- en: '[PRE39]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, define `train()`, a function to train the whole system. We''ll start
    by selecting the subset of `Fashion-MNIST` that we''ll train on, and then we''ll
    define the number of batches and training steps required to fit the architecture:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，定义 `train()` 函数来训练整个系统。我们将从选择将要训练的 `Fashion-MNIST` 子集开始，然后定义所需的批次和训练步骤数量来适配架构：
- en: '[PRE40]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Pick samples for classification, and use these to fit the classifier:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择用于分类的样本，并使用这些样本来训练分类器：
- en: '[PRE41]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Pick real samples for discrimination, and use these to fit the discriminator:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择真实样本进行判别，并使用这些样本来训练判别器：
- en: '[PRE42]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Use the generator to produce fake data, and use this to fit the discriminator:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成器生成假数据，并用这些数据来训练判别器：
- en: '[PRE43]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Generate latent points, and use these to train the GAN:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成潜在点，并利用这些点训练 GAN：
- en: '[PRE44]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Load `Fashion-MNIST` and normalize both the training and test sets:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `Fashion-MNIST` 数据集并对训练集和测试集进行归一化处理：
- en: '[PRE45]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Instantiate an `SSCGAN()` and train it for 30 epochs:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 `SSCGAN()` 并训练 30 个 epoch：
- en: '[PRE46]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Report the accuracy of the classifier on both the training and test sets:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告分类器在训练集和测试集上的准确率：
- en: '[PRE47]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: After the training finishes, both the training and test accuracy should be around
    83%, which is pretty satisfying if we consider we only used 1,000 examples out
    of 50,000!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，训练集和测试集的准确率应该都在 83% 左右，如果考虑到我们只使用了 50,000 个样本中的 1,000 个，这个结果是相当令人满意的！
- en: How it works…
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理…
- en: 'In this recipe, we implemented an architecture quite similar to the one implemented
    in the *Implementing a deep convolutional GAN* recipe that opened this chapter.
    The main difference resides in the fact that we have two discriminators: the first
    one is actually a classifier, which is trained on the small subset of labeled
    data at our disposal. The other is a regular discriminator, whose sole job is
    to not be fooled by the generator.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们实现了一个与本章开头的 *实现深度卷积 GAN* 食谱中实现的架构非常相似。主要的区别在于我们有两个判别器：第一个实际上是一个分类器，训练时使用我们手头的少量标记数据的子集；另一个是常规判别器，其唯一任务是不要被生成器欺骗。
- en: How does the classifier achieve such a respectable performance with so little
    data? The answer is shared weights. Both the classifier and the discriminator
    share the same feature extraction layers, differing only in the final output layer,
    which is activated with a plain old softmax function in the case of the classifier,
    and with a `Lambda()` layer that wraps our `custom_activation()` function in the
    case of the discriminator.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器如何在如此少的数据下取得如此出色的性能？答案是共享权重。分类器和判别器共享相同的特征提取层，唯一的区别在于最终的输出层，分类器使用普通的 softmax
    函数进行激活，而判别器则使用一个 `Lambda()` 层包裹我们的 `custom_activation()` 函数进行激活。
- en: This means that these shared weights get updated each time the classifier trains
    on a batch of labeled data, and also when the discriminator trains on both real
    and fake images. In the end, we circumvent the data scarcity problem with the
    aid of the generator.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每次分类器在一批标记数据上训练时，这些共享权重都会被更新，同时判别器在真实和假图像上训练时也会更新。最终，我们借助生成器解决了数据稀缺问题。
- en: Pretty impressive, right?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 很厉害吧？
- en: See also
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can consolidate your understanding of the semi-supervised training approach
    used in this recipe by reading the paper where it was first proposed: [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过阅读最初提出这种方法的论文来巩固对本食谱中使用的半监督训练方法的理解：[https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498)。
- en: Translating images with Pix2Pix
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pix2Pix 翻译图像
- en: One of the most interesting applications of GANs is image-to-image translation,
    which, as the name suggests, consists of translating the content from one image
    domain to another (for instance, sketches to photos, black and white images to
    RGB, and Google Maps to satellite views, among others).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 最有趣的应用之一是图像到图像的翻译，顾名思义，它包括将一个图像领域的内容翻译到另一个领域（例如，素描到照片，黑白图像到 RGB，Google Maps
    到卫星视图等）。
- en: In this recipe, we'll implement a fairly complex conditional adversarial network
    known as Pix2Pix. We'll focus solely on the practical aspects of the solution,
    but if you want to get familiar with the literature, check out the *See also*
    section at the end of the recipe.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将实现一个相当复杂的条件对抗网络，称为 Pix2Pix。我们将专注于解决方案的实际应用，如果你想了解更多文献，可以查看食谱末尾的 *参见*
    部分。
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We''ll use the `cityscapes` dataset, which is available here: https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/cityscapes.tar.gz.
    Download it and decompress it in a location of your choosing. For the purposes
    of this tutorial, we will assume that it''s placed in the `~/.keras/datasets`
    directory, under the name `cityscapes`. To display a progress bar during training,
    install `tqdm`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `cityscapes` 数据集，它可以在此处找到：https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/cityscapes.tar.gz。下载并解压到你选择的位置。为了本教程的目的，我们假设它被放置在
    `~/.keras/datasets` 目录下，命名为 `cityscapes`。为了在训练过程中显示进度条，安装 `tqdm`：
- en: '[PRE48]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'By the end of this recipe, we''ll learn to generate the image on the left from
    the right one using Pix2Pix:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，我们将学会如何使用 Pix2Pix 从右侧的图像生成左侧的图像：
- en: '![Figure 6.3 – We will use the segmented images on the right to produce real-world
    images like the one on the left](img/B14768_06_003.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 我们将使用右侧的分割图像生成像左侧那样的真实世界图像](img/B14768_06_003.jpg)'
- en: Figure 6.3 – We will use the segmented images on the right to produce real-world
    images like the one on the left
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 我们将使用右侧的分割图像生成像左侧那样的真实世界图像
- en: Let's get started!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: After completing these steps, you'll have implemented Pix2Pix from scratch!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你将从头实现 Pix2Pix！
- en: 'Import the dependencies:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入依赖项：
- en: '[PRE49]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define constants for TensorFlow''s autotuning and resizing options, as well
    as the dimensions. We will resize all the images in the dataset:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 TensorFlow 的自动调优和调整大小选项的常量，以及图像尺寸。我们将调整数据集中的所有图像：
- en: '[PRE50]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Each image in the dataset is comprised of both the input and target, so after
    processing it, we need to split them into separate images. The `load_image()`
    function does this:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中的每张图像由输入和目标组成，因此在处理完图像后，我们需要将它们拆分成单独的图像。`load_image()` 函数实现了这一点：
- en: '[PRE51]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s create the `resize()` function to resize both the input and target images:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建 `resize()` 函数来调整输入图像和目标图像的大小：
- en: '[PRE52]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, implement the `random_crop()` function to perform random cropping on the
    images:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，实施 `random_crop()` 函数，对图像进行随机裁剪：
- en: '[PRE53]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, code up the `normalize()` function to normalize the images to the range
    [-1, 1]:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，编写 `normalize()` 函数，将图像归一化到 [-1, 1] 范围内：
- en: '[PRE54]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define the `random_jitter()` function, which performs random jittering on the
    input images (notice that it uses the functions defined in *Step 4* and *Step
    5*):'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `random_jitter()` 函数，对输入图像进行随机抖动（注意它使用了 *第4步* 和 *第5步* 中定义的函数）：
- en: '[PRE55]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Create the `load_training_image()` function to load and augment the training
    images:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `load_training_image()` 函数，用于加载和增强训练图像：
- en: '[PRE56]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s now implement the `load_test_image()` function, which, as its name indicates,
    will be used to load test images:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现 `load_test_image()` 函数，顾名思义，它将用于加载测试图像：
- en: '[PRE57]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, let''s proceed to create the `generate_and_save_images()` function to
    store synthetic images created by the generator model. The resulting images will
    be a concatenation of `input`, `target`, and `prediction`:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续创建 `generate_and_save_images()` 函数，来存储生成器模型生成的合成图像。结果图像将是 `input`、`target`
    和 `prediction` 的拼接：
- en: '[PRE58]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, define the `Pix2Pix()` class, which encapsulates this architecture implementation.
    Start with the constructor:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义 `Pix2Pix()` 类，封装此架构的实现。首先是构造函数：
- en: '[PRE59]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The constructor implemented in *Step 11* defines the loss function to be used
    (**binary cross-entropy**), the lambda value (used in *Step 18*), and instantiates
    the generator and the discriminator, as well as their respective optimizers. Our
    generator is a modified **U-Net**, which is a U-shaped network comprising downsampling
    and upsampling blocks. Let''s create a static method to produce a downsample block:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第11步* 中实现的构造函数定义了要使用的损失函数（**二元交叉熵**）、lambda 值（用于 *第18步*），并实例化了生成器和判别器及其各自的优化器。我们的生成器是一个修改过的
    **U-Net**，它是一个 U 形网络，由下采样和上采样块组成。现在，让我们创建一个静态方法来生成下采样块：'
- en: '[PRE60]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'A downsample block is a convolution, optionally batch normalized, and activated
    with `LeakyReLU()`. Let''s now implement a static method to create upsampling
    blocks:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下采样块是一个卷积块，可选地进行批归一化，并激活 `LeakyReLU()`。现在，让我们实现一个静态方法来创建上采样块：
- en: '[PRE61]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'An upsampling block is a transposed convolution, optionally followed by dropout
    and with `ReLU()` activated. Let''s now use these two convenience methods to implement
    the U-Net generator:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上采样块是一个转置卷积，后面可选地跟随 dropout，并激活 `ReLU()`。现在，让我们使用这两个便捷方法来实现 U-Net 生成器：
- en: '[PRE62]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'After defining the downsampling stack, let''s do the same with the upsampling
    layers:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义了下采样堆栈后，让我们对上采样层做同样的事情：
- en: '[PRE63]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Thread the input through the down and up stacks, and also add skip connections
    to prevent the depth of the network from impeding its learning:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入通过下采样和上采样堆栈，同时添加跳跃连接，以防止网络的深度妨碍其学习：
- en: '[PRE64]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output layers are a transposed convolution with `''tanh''` activated:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层是一个转置卷积，激活函数为 `'tanh'`：
- en: '[PRE65]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define a method to compute the generator loss, as the authors of Pix2Pix recommend.
    Notice the use of the `self._lambda` constant:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算生成器的损失，正如 Pix2Pix 的作者所推荐的那样。注意 `self._lambda` 常量的使用：
- en: '[PRE66]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The discriminator, defined in this step, receives two images; the input and
    the target:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本步骤中定义的判别器接收两张图像；输入图像和目标图像：
- en: '[PRE67]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Notice that the last couple of layers are convolutions, instead of `Dense()`
    layers. This is because the discriminator works on patches of images at a time,
    and tells whether each patch is real or fake:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，最后几层是卷积层，而不是 `Dense()` 层。这是因为判别器一次处理的是图像的一个小块，并判断每个小块是真实的还是假的：
- en: '[PRE68]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Define the discriminator loss:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义判别器的损失：
- en: '[PRE69]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Define a function to perform a single train step, named `train_step()`, consisting
    of taking the input image, passing through the generator, and then using the discriminator
    on the input image paired with the original target image, and then on the input
    imaged paired with the fake image output from the generator:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个执行单个训练步骤的函数，命名为`train_step()`，该函数包括：将输入图像传入生成器，然后使用判别器对输入图像与原始目标图像配对，再对输入图像与生成器输出的假图像配对进行处理：
- en: '[PRE70]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, the losses are computed, along with the gradients:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，计算损失值以及梯度：
- en: '[PRE71]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Use the gradients to update the models through the respective optimizers:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度通过相应的优化器更新模型：
- en: '[PRE72]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Implement `fit()`, a method to train the whole architecture. For each epoch,
    we''ll save to disk the images generated to visually assess the performance of
    the model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`fit()`方法来训练整个架构。对于每一轮，我们将生成的图像保存到磁盘，以便通过视觉方式评估模型的性能：
- en: '[PRE73]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Assemble the path to the training and test splits of the dataset:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组装训练集和测试集数据的路径：
- en: '[PRE74]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Define the training and test datasets:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练集和测试集数据：
- en: '[PRE75]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Instantiate `Pix2Pix()` and fit it over 150 epochs:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`Pix2Pix()`并训练150轮：
- en: '[PRE76]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Here''s a generated image at epoch 1:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是第 1 轮生成的图像：
- en: '![Figure 6.4 – At first, the generator only produces noise'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – 最初，生成器只会产生噪声'
- en: '](img/B14768_06_004.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_06_004.jpg)'
- en: Figure 6.4 – At first, the generator only produces noise
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 最初，生成器只会产生噪声
- en: 'And here''s one at epoch 150:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第150轮的结果：
- en: '![Figure 6.5 – At the end of its training run, the generator is capable of
    producing reasonable results'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 在训练结束时，生成器能够产生合理的结果'
- en: '](img/B14768_06_005.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_06_005.jpg)'
- en: Figure 6.5 – At the end of its training run, the generator is capable of producing
    reasonable results
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 在训练结束时，生成器能够产生合理的结果
- en: When the training ends, our Pix2Pix architecture can translate segmented images
    to real scenes, as demonstrated in *Figure 6.5*, where the first image is the
    input, the second is the target, and the rightmost is the generated one.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练结束时，我们的Pix2Pix架构能够将分割后的图像转换为真实场景，如*图 6.5*所示，其中第一张是输入图像，第二张是目标图像，最右边的是生成的图像。
- en: Let's connect the dots in the next section.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将在下一部分连接这些点。
- en: How it works…
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we implemented an architecture which was a bit hard, but was
    based, but based on the same ideas as all GANs. The main difference is that this
    time, the discriminator works on patches, instead of whole images. More specifically,
    the discriminator looks at patches of the original and fake images at a time and
    decides whether those patches belong to real or synthetized images.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们实现了一个稍微复杂的架构，但它基于与所有GAN相同的思路。主要的区别是，这次判别器工作在图像块上，而不是整个图像。更具体地说，判别器一次查看原始图像和假图像的图像块，并判断这些图像块是否属于真实图像或合成图像。
- en: Because image-to-image translation is a form of image segmentation, our generator
    is a modified U-Net, a groundbreaking type of CNN first used for biomedical image
    segmentation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像到图像的转换是一种图像分割形式，我们的生成器是一个经过修改的U-Net，U-Net是一种首次用于生物医学图像分割的突破性CNN架构。
- en: Because Pix2Pix is such a complex and deep network, the training process takes
    several hours to complete, but in the end, we obtained very good results translating
    the content of segmented city landscapes to real-looking predictions. Impressive!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Pix2Pix是一个如此复杂且深度的网络，训练过程需要几个小时才能完成，但最终，我们在将分割后的城市景观内容转换为真实感预测方面取得了非常好的结果。令人印象深刻！
- en: If you want to take a look at other produced images, as well as a graphical
    representation of the generator and discriminator, consult the official repository
    at [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看其他生成的图像以及生成器和判别器的图形表示，请查阅官方仓库：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3)。
- en: See also
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'I recommend you read the original paper by Phillip Isola, Jun-Yan Zhu, Tinghui
    Zhou, and Alexei A. Efros, the authors of **Pix2Pix**, here: [https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004).
    We used a U-Net as the generator, which you can read more about here: [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你阅读**Pix2Pix**的原始论文，作者为Phillip Isola、Jun-Yan Zhu、Tinghui Zhou和Alexei A. Efros，论文链接在此：[https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004)。我们使用了U-Net作为生成器，你可以在这里了解更多：[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)。
- en: Translating unpaired images with CycleGAN
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CycleGAN翻译未配对的图像
- en: In the *Translating images with Pix2Pix* recipe, we discovered how to transfer
    images from one domain to another. However, in the end, it's supervised learning
    that requires a pairing of input and target images in order for Pix2Pix to learn
    the correct mapping. Wouldn't it be great if we could bypass this pairing condition,
    and let the network figure out on its own how to translate the characteristics
    from one domain to another, while preserving image consistency?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用Pix2Pix翻译图像*的配方中，我们探索了如何将图像从一个领域转移到另一个领域。然而，最终这仍然是监督学习，需要输入和目标图像的配对，以便Pix2Pix学习正确的映射。如果我们能够绕过这个配对条件，让网络自己找出如何将一个领域的特征翻译到另一个领域，同时保持图像的一致性，那该多好？
- en: Well, that's what **CycleGAN** does, and in this recipe, we'll implement one
    from scratch to convert pictures of Yosemite National Park taken during the summer
    into their winter counterparts!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这正是**CycleGAN**的作用，在这个配方中，我们将从头开始实现一个，将夏季拍摄的优胜美地国家公园的照片转换为冬季版本！
- en: Let's get started.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧。
- en: Getting ready
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll use `OpenCV`, `tqdm`, and `tensorflow-datasets` in this recipe.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用`OpenCV`、`tqdm`和`tensorflow-datasets`。
- en: 'Install these simultaneously with `pip`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`同时安装这些：
- en: '[PRE77]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Through the TensorFlow datasets, we'll access the `cyclegan/summer2winter_yosemite`
    dataset.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过TensorFlow数据集，我们将访问`cyclegan/summer2winter_yosemite`数据集。
- en: 'Here are some sample images of this dataset:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该数据集的一些示例图像：
- en: '![Figure 6.6 – Left: Yosemite during summer; right: Yosemite during winter'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – 左：夏季的优胜美地；右：冬季的优胜美地'
- en: '](img/B14768_06_006.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_06_006.jpg)'
- en: 'Figure 6.6 – Left: Yosemite during summer; right: Yosemite during winter'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 左：夏季的优胜美地；右：冬季的优胜美地
- en: Tip
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The implementation of CycleGAN is very similar to Pix2Pix. Therefore, we won't
    explain most of it in detail. Instead, I encourage you to complete the *Translating
    images with Pix2Pix* recipe before tackling this one.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN的实现与Pix2Pix非常相似。因此，我们不会详细解释其中的大部分内容。相反，我建议你先完成*使用Pix2Pix翻译图像*的配方，然后再来挑战这个。
- en: How to do it…
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Perform the following steps to complete the recipe:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来完成这个配方：
- en: 'Import the necessary dependencies:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的依赖项：
- en: '[PRE78]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`tf.data.experimental.AUTOTUNE`的别名：
- en: '[PRE79]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Define a function to perform the random cropping of an image:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行图像的随机裁剪：
- en: '[PRE80]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Define a function to normalize images to the range [-1, 1]:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将图像归一化到[-1, 1]的范围：
- en: '[PRE81]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Define a function to perform random jittering on an image:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，执行图像的随机抖动：
- en: '[PRE82]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Define a function to preprocess and augment training images:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来预处理并增强训练图像：
- en: '[PRE83]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Define a function to preprocess test images:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来预处理测试图像：
- en: '[PRE84]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Define a function to generate and save images using the generator model. The
    resulting images will be a concatenation of the input and the prediction:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，使用生成器模型生成并保存图像。生成的图像将是输入图像与预测结果的拼接：
- en: '[PRE85]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Define a custom instance normalization layer, starting with the constructor:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个自定义实例归一化层，从构造函数开始：
- en: '[PRE86]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, define the `build()` method, which creates the inner components of the
    `InstanceNormalization()` class:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义`build()`方法，它创建`InstanceNormalization()`类的内部组件：
- en: '[PRE87]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Create the `call()` method, which implements the logic to instance-normalize
    the input tensor, `x`:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`call()`方法，该方法实现实例归一化输入张量`x`的逻辑：
- en: '[PRE88]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Define a class to encapsulate the CycleGAN implementation. Start with the constructor:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个类来封装CycleGAN的实现。首先是构造函数：
- en: '[PRE89]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The main difference with Pix2Pix is that we have two generators (`gen_g` and
    `gen_f`) and two discriminators (`dis_x` and `dis_y`). `gen_g` learns how to transform
    image X to image Y, and `gen_f` learns how to transform image Y to image Y. Analogously,
    `dis_x` learns to differentiate between the real image X and the one generated
    by `gen_f`, while `dis_y` learns to differentiate between the real image Y and
    the one generated by `gen_g`.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与Pix2Pix的主要区别在于我们有两个生成器（`gen_g`和`gen_f`）和两个鉴别器（`dis_x`和`dis_y`）。`gen_g`学习如何将图像X转换为图像Y，而`gen_f`学习如何将图像Y转换为图像X。类似地，`dis_x`学习区分真实的图像X和`gen_f`生成的图像，而`dis_y`学习区分真实的图像Y和`gen_g`生成的图像。
- en: 'Now, let''s create a static method to produce downsampling blocks (this is
    the same as in the last recipe, only this time we use instance instead of batch
    normalization):'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个静态方法来生成下采样块（这与上一个示例相同，只是这次我们使用实例化而不是批处理归一化）：
- en: '[PRE90]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Now, define a static method to produce upsampling blocks (this is the same
    as in the last recipe, only this time we use instance instead of batch normalization):'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个静态方法来生成上采样块（这与上一个示例相同，只是这次我们使用实例化而不是批处理归一化）：
- en: '[PRE91]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Define a method to build the generator. Start by creating the downsampling
    layers:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来构建生成器。首先创建下采样层：
- en: '[PRE92]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Now, create the upsampling layers:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建上采样层：
- en: '[PRE93]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Thread the input through the downsampling and upsampling layers. Add skip connections
    to avoid the vanishing gradient problem:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入通过下采样和上采样层。添加跳跃连接以避免梯度消失问题：
- en: '[PRE94]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output layers are a `''tanh''` activated transposed convolution:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层是一个激活函数为`'tanh'`的转置卷积层：
- en: '[PRE95]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Define a method to calculate the generator loss:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算生成器损失：
- en: '[PRE96]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Define a method to create the discriminator:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来创建鉴别器：
- en: '[PRE97]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Add the last couple of layers, which are convolutional:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加最后几层卷积层：
- en: '[PRE98]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Define a method to compute the discriminator loss:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算鉴别器损失：
- en: '[PRE99]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Define a method to compute the loss between the real and cycled images. This
    loss is in charge of quantifying the cycle consistency, which says that if you
    translate an image X to Y, and then Y to X, the result should be X, or close to
    X:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算真实图像和循环图像之间的损失。这个损失用于量化循环一致性，即如果你将图像X翻译为Y，然后再将Y翻译为X，结果应该是X，或者接近X：
- en: '[PRE100]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Define a method to compute the identity loss. This loss establishes that if
    you pass image Y through `gen_g`, we should obtain the real image Y or something
    close to it (the same applies to `gen_f`):'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来计算身份损失。这个损失确保如果你将图像Y通过`gen_g`传递，我们应该得到真实的图像Y或接近它（`gen_f`也同样适用）：
- en: '[PRE101]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Define a method to perform a single training step. It receives images X and
    Y from different domains. Then, it uses `gen_g` to translate X to Y, and `gen_f`
    to translate Y to X:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来执行单步训练。它接收来自不同领域的图像X和Y。然后，使用`gen_g`将X转换为Y，并使用`gen_f`将Y转换为X：
- en: '[PRE102]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Now, pass X through `gen_f` and Y through `gen_y` to later compute the identity
    loss:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将X通过`gen_f`传递，将Y通过`gen_y`传递，以便稍后计算身份损失：
- en: '[PRE103]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Pass real X and fake X to `dis_x`, and real Y, along with generated Y, to `dis_y`:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将真实的X和伪造的X传递给`dis_x`，将真实的Y以及生成的Y传递给`dis_y`：
- en: '[PRE104]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Compute the generators'' losses:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算生成器的损失：
- en: '[PRE105]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Compute the cycle loss:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算循环损失：
- en: '[PRE106]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Compute the identity loss and the total generator G loss:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算身份损失和总生成器G的损失：
- en: '[PRE107]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Repeat for generator F:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对生成器F重复此过程：
- en: '[PRE108]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Compute the discriminators'' losses:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算鉴别器的损失：
- en: '[PRE109]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Compute the gradients for the generators:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算生成器的梯度：
- en: '[PRE110]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Compute the gradients for the discriminators:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算鉴别器的梯度：
- en: '[PRE111]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Apply the gradients to each generator using the respective optimizer:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相应的优化器将梯度应用到每个生成器：
- en: '[PRE112]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Apply the gradients to each discriminator using the respective optimizer:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相应的优化器将梯度应用到每个鉴别器：
- en: '[PRE113]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Define a method to fit the whole architecture. It will save to disk the images
    produced by generator G after each epoch:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来拟合整个架构。它将在每个epoch之后将生成器G生成的图像保存到磁盘：
- en: '[PRE114]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Load the dataset:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE115]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Unpack the training and test splits:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解包训练和测试集：
- en: '[PRE116]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Define the data processing pipelines for the training spit:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练集的数据处理管道：
- en: '[PRE117]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Define the data processing pipelines for the test split:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义测试集的数据处理管道：
- en: '[PRE118]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Create an instance of `CycleGAN()` and train it for 40 epochs:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`CycleGAN()`实例并训练40个epoch：
- en: '[PRE119]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'At epoch 1, we''ll notice that the network hasn''t learned much:'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第1个epoch时，我们会注意到网络尚未学到很多东西：
- en: '![Figure 6.7 – Left: original image during summer; right: translated image
    (winter)](img/B14768_06_007.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 左：夏季的原始图像；右：翻译后的图像（冬季）](img/B14768_06_007.jpg)'
- en: 'Figure 6.7 – Left: original image during summer; right: translated image (winter)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 左：夏季的原始图像；右：翻译后的图像（冬季）
- en: 'However, at epoch 40, the results are more promising:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在第 40 个周期时，结果更加令人鼓舞：
- en: '![Figure 6.8 – Left: original image during summer; right: translated image
    (winter)](img/B14768_06_008.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 左：夏季的原始图像；右：翻译后的图像（冬季）](img/B14768_06_008.jpg)'
- en: 'Figure 6.8 – Left: original image during summer; right: translated image (winter)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 左：夏季的原始图像；右：翻译后的图像（冬季）
- en: As we can see in the preceding image, our `CycleGAN()` added a little more white
    to certain parts of the trail and the trees to make the translated image seem
    like it was taken during winter. Of course, training for more epochs can potentially
    lead to better results, which I encourage you to do to solidify your understanding
    of CycleGANs!
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们的 `CycleGAN()` 在某些区域（如小道和树木）添加了更多的白色，使得翻译后的图像看起来像是冬季拍摄的。当然，训练更多的周期可能会带来更好的结果，我鼓励你这么做，以加深你对
    CycleGAN 的理解！
- en: How it works…
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we learned that CycleGANs work in a very similar fashion to
    Pix2Pix. However, the biggest advantage is that a CycleGAN doesn't require a dataset
    of paired images to achieve its goal. Instead, it relies on two sets of generators
    and discriminators, which, in fact, create a learning cycle, hence the name.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们了解到，CycleGAN 的工作方式与 Pix2Pix 非常相似。然而，最大优势是 CycleGAN 不需要配对图像数据集就能实现目标。相反，它依赖于两组生成器和判别器，实际上，这些生成器和判别器形成了一个学习循环，因此得名。
- en: 'In particular, CycleGANs work as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，CycleGAN 的工作方式如下：
- en: A generator G must learn a mapping from an image X to an image Y.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器 G 必须学习从图像 X 到图像 Y 的映射。
- en: A generator F must learn a mapping from an image Y to an image X.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器 F 必须学习从图像 Y 到图像 X 的映射。
- en: A discriminator D(X) must distinguish the real image X from the fake one generated
    by G.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器 D(X) 必须区分真实图像 X 和由 G 生成的假图像。
- en: A discriminator D(Y) must distinguish the real image Y from the fake one generated
    by F.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器 D(Y) 必须区分真实图像 Y 和由 F 生成的假图像。
- en: 'There are two conditions that ensure that the translation preserves the meaning
    in both domains (very much like when we want to preserve the meaning of our words
    when we translate from English to Spanish, and vice versa):'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个条件确保翻译在两个领域中保持含义（就像我们从英语翻译成西班牙语时，希望保留词语的含义，反之亦然）：
- en: 'Cycle consistency: Going from X to Y and then from Y to X should produce the
    original X or something very similar to X. The same applies to Y.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环一致性：从 X 到 Y，再从 Y 到 X 应该产生原始的 X 或与 X 非常相似的东西。Y 也是如此。
- en: 'Identity consistency: Passing X to G should produce the same X or something
    very similar to X. The same applies to Y.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份一致性：将 X 输入 G 应该产生相同的 X 或与 X 非常相似的东西。Y 也是如此。
- en: Using these four components, CycleGAN tries to preserve the cycle and identity
    consistency in the translation, which generates very satisfying results without
    the need for supervised, paired data.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这四个组件，CycleGAN 试图在翻译中保持循环和身份一致性，从而在无需监督、配对数据的情况下生成非常令人满意的结果。
- en: See also
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can read the original paper on CycleGANs here: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593).
    Also, here is a very interesting thread to understand the difference between instance
    and batch normalization: [https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation](https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读关于 CycleGAN 的原始论文：[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)。此外，这里有一个非常有趣的讨论，帮助你理解实例归一化与批归一化之间的区别：[https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation](https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation)。
- en: Implementing an adversarial attack using the Fast Gradient Signed Method
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用快速梯度符号方法实现对抗性攻击
- en: We often think of highly accurate deep neural networks as robust models, but
    the **Fast Gradient Signed Method** (**FGSM**), proposed by no other than the
    father of GANs himself, Ian Goodfellow, showed otherwise. In this recipe, we'll
    perform an FGSM attack on a pre-trained model to see how, by introducing seemingly
    imperceptible changes, we can completely fool a network.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常认为高度准确的深度神经网络是强大的模型，但由GAN之父伊恩·古德费洛（Ian Goodfellow）提出的**快速梯度符号方法**（**FGSM**）却证明了相反的观点。在这个示例中，我们将对一个预训练模型执行FGSM攻击，看看如何通过引入看似无法察觉的变化，完全欺骗一个网络。
- en: Getting ready
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let's install `OpenCV` with `pip`.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用`pip`安装`OpenCV`。
- en: 'We''ll use it to save the perturbed images using the FGSM method:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用它来使用FGSM方法保存扰动后的图像：
- en: '[PRE120]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Let's begin.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: How to do it
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'After completing the following steps, you''ll have successfully performed an
    adversarial attack:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下步骤后，您将成功执行一次对抗性攻击：
- en: 'Import the dependencies:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入依赖项：
- en: '[PRE121]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Define a function to preprocess an image, which entails resizing it and applying
    the same treatment as the pre-trained network we''ll use (in this case, `NASNetMobile`):'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来预处理图像，这包括调整图像大小并应用与我们将要使用的预训练网络相同的处理（在这个例子中是`NASNetMobile`）：
- en: '[PRE122]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Define a function to get the human-readable image from a set of probabilities:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来根据一组概率获取人类可读的图像：
- en: '[PRE123]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Define a function to save an image. This will use the pre-trained model to
    get the proper label and will utilize it as part of the filename of the image,
    which also contains the prediction confidence percentage. Prior to storing the
    image on disk, it ensures that it''s in the expected [0, 255] range, as well as
    in BGR space, which is the one used by OpenCV:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来保存图像。这个函数将使用预训练模型来获取正确的标签，并将其作为图像文件名的一部分，文件名中还包含预测的置信度百分比。在将图像存储到磁盘之前，它会确保图像在预期的[0,
    255]范围内，并且处于BGR空间中，这是OpenCV使用的颜色空间：
- en: '[PRE124]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Define a function to create the adversarial pattern that will be used later
    on to perform the actual FGSM attack:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来创建对抗性模式，该模式将在后续用于执行实际的FGSM攻击：
- en: '[PRE125]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'The pattern is pretty simple: It consists of a tensor with the sign of the
    gradient in each element. More specifically, `signed_gradient` will contain a
    `-1` for gradient values below `0`, `1` for values above `0`, and `0` if the gradient
    is, well, `0`.'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个模式非常简单：它由一个张量组成，其中每个元素表示梯度的符号。更具体地说，`signed_gradient`将包含一个`-1`表示梯度值小于`0`，`1`表示梯度值大于`0`，而当梯度为`0`时，则是`0`。
- en: 'Instantiate the pre-trained `NASNetMobile()` model and freeze its weights:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化预训练的`NASNetMobile()`模型并冻结其权重：
- en: '[PRE126]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Load the test image and pass it through the network:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载测试图像并通过网络传递：
- en: '[PRE127]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'One-hot encode the ground truth label of the original image, and use it to
    generate the adversarial pattern:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对原始图像的地面真值标签进行独热编码，并用它生成对抗性模式：
- en: '[PRE128]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Perform a series of adversarial attacks using increasing, yet small, values
    of `epsilon`, which will be applied in the direction of the gradient, leveraging
    the pattern present in `disturbances`:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一系列对抗性攻击，使用逐渐增大但仍然较小的`epsilon`值，这些值将在梯度方向上应用，利用`disturbances`中的模式：
- en: '[PRE129]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'For epsilon = 0 (no attack), the image looks like this, and the label is `pug`
    with an 80% confidence:'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于epsilon = 0（无攻击），图像如下，标签为`pug`，置信度为80%：
- en: '![Figure 6.9 – Original image. Label: pug (80.23% confidence)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9 – 原始图像。标签：pug（80.23% 置信度）](img/B14768_06_009.jpg)'
- en: '](img/B14768_06_009.jpg)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_06_009.jpg)'
- en: 'Figure 6.9 – Original image. Label: pug (80.23% confidence)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 原始图像。标签：pug（80.23% 置信度）
- en: 'When epsilon = 0.005 (a very small perturbation), the label changes to `Brabancon_griffon`,
    with a 43.03% confidence:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 当epsilon = 0.005（一个非常小的扰动）时，标签变为`Brabancon_griffon`，置信度为43.03%：
- en: '![Figure 6.10 – Epsilon = 0.005 applied in the gradient direction. Label: Brabancon_gritton
    (43.03% ](img/B14768_06_010.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 在梯度方向上应用epsilon = 0.005。标签：Brabancon_griffon（43.03% 置信度）](img/B14768_06_010.jpg)'
- en: 'Figure 6.10 – Epsilon = 0.005 applied in the gradient direction. Label: Brabancon_gritton
    (43.03% confidence)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 在梯度方向上应用epsilon = 0.005。标签：Brabancon_griffon（43.03% 置信度）
- en: As can be seen from the preceding image, an imperceptible variation in the pixel
    values produced a drastically different response from the network. However, the
    situation worsens the more we increment the magnitude of epsilon. For a complete
    list of results, refer to [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，像素值的微小变化会导致网络产生截然不同的响应。然而，随着ε（epsilon）值增大的情况变得更糟。有关完整的结果列表，请参阅[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5)。
- en: How it works…
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we implemented a fairly simple attack based on the FGSM proposed
    by Ian Goodfellow, which simply consists of determining the direction (sign) of
    the gradient at each location and using that information to create an adversarial
    pattern. The underlying principle is that this technique maximizes the loss at
    each pixel value.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们实现了一个基于Ian Goodfellow提出的FGSM（快速梯度符号法）的简单攻击方法，主要通过确定每个位置的梯度方向（符号）并利用该信息创建对抗性图案。其基本原理是该技术在每个像素值上最大化损失。
- en: Next, we use this pattern to either add or subtract a small perturbation to
    each pixel in the image that gets passed to the network.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用此图案来对图像中的每个像素进行加减微小的扰动，然后将其传递给网络。
- en: Although these changes are often imperceptible to the human eye, they have the
    power to completely confuse a network, resulting in nonsensical predictions, as
    demonstrated in the last step of this recipe.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些变化通常人眼难以察觉，但它们能够完全扰乱网络，导致荒谬的预测，正如在本食谱的最后一步所展示的那样。
- en: See also
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Fortunately, many defenses against this type of attack (and more sophisticated
    ones) have emerged. You can read a pretty interesting survey of adversarial attacks
    and defenses here: [https://arxiv.org/abs/1810.00069](https://arxiv.org/abs/1810.00069).'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，针对这种类型攻击（以及更复杂攻击）的防御措施已经出现。你可以阅读一篇相当有趣的对抗攻击与防御的综述，内容在这里：[https://arxiv.org/abs/1810.00069](https://arxiv.org/abs/1810.00069)。
