- en: Neural Network and Artificial Intelligence Concepts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与人工智能概念
- en: From the scientific and philosophical studies conducted over the centuries,
    special mechanisms have been identified that are the basis of human intelligence.
    Taking inspiration from their operations, it was possible to create machines that
    imitate part of these mechanisms. The problem is that they have not yet succeeded
    in imitating and integrating all of them, so the **Artificial Intelligence** (**AI**)
    systems we have are largely incomplete.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几百年来的科学与哲学研究，已经识别出一些特殊的机制，它们构成人类智能的基础。受其运作的启发，人类成功创造出了部分模仿这些机制的机器。问题在于它们尚未能成功地模仿并整合所有这些机制，因此我们现有的**人工智能**（**AI**）系统在很大程度上是不完整的。
- en: A decisive step in the improvement of such machines came from the use of so-called
    **Artificial Neural Networks** (**ANNs**) that, starting from the mechanisms regulating
    natural neural networks, plan to simulate human thinking. Software can now imitate
    the mechanisms needed to win a chess match or to translate text into a different
    language in accordance with its grammatical rules.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机器改进的决定性一步来自所谓的**人工神经网络**（**ANNs**）的使用，这些网络从调节自然神经网络的机制出发，计划模拟人类思维。现在，软件可以模仿赢得国际象棋比赛或将文本翻译成另一种语言的机制，并遵循其语法规则。
- en: 'This chapter introduces the basic theoretical concepts of ANN and AI. Fundamental
    understanding of the following is expected:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 ANN 和 AI 的基本理论概念。预期您对以下内容有基础理解：
- en: Basic high school mathematics; differential calculus and functions such as *sigmoid*
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础的高中数学；微分学和如 *sigmoid* 等函数
- en: R programming and usage of R libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 编程及 R 库的使用
- en: We will go through the basics of neural networks and try out one model using
    R. This chapter is a foundation for neural networks and all the subsequent chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍神经网络的基础，并使用 R 尝试一个模型。本章是神经网络的基础，所有后续章节都将基于此。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: ANN concepts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANN 概念
- en: Neurons, perceptron, and multilayered neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元、感知器和多层神经网络
- en: Bias, weights, activation functions, and hidden layers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差、权重、激活函数和隐藏层
- en: Forward and backpropagation methods
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播和反向传播方法
- en: Brief overview of **Graphics Processing Unit** (**GPU**)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图形处理单元**（**GPU**）的简要概述'
- en: At the end of the chapter, you will be able to recognize the different neural
    network algorithms and tools which R provides to handle them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够识别 R 提供的不同神经网络算法和工具，以便处理它们。
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The brain is the most important organ of the human body. It is the central processing
    unit for all the functions performed by us. Weighing only 1.5 kilos, it has around
    86 billion neurons. A neuron is defined as a cell transmitting nerve impulses
    or electrochemical signals. The brain is a complex network of neurons which process
    information through a system of several interconnected neurons. It has always
    been challenging to understand the brain functions; however, due to advancements
    in computing technologies, we can now program neural networks artificially.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑是人体最重要的器官，是我们进行所有功能处理的中央处理单元。它仅重1.5公斤，却有约860亿个神经元。神经元被定义为传递神经冲动或电化学信号的细胞。大脑是一个复杂的神经网络，通过一系列相互连接的神经元处理信息。理解大脑功能一直是一个挑战；然而，随着计算技术的进步，我们现在可以人工编程神经网络。
- en: The discipline of ANN arose from the thought of mimicking the functioning of
    the same human brain that was trying to solve the problem. The drawbacks of conventional
    approaches and their successive applications have been overcome within well-defined
    technical environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）的学科源自模仿人类大脑功能的思想，这正是试图解决问题的地方。传统方法及其随后的应用在明确的技术环境中得到了克服。
- en: AI or machine intelligence is a field of study that aims to give cognitive powers
    to computers to program them to learn and solve problems. Its objective is to
    simulate computers with human intelligence. AI cannot imitate human intelligence
    completely; computers can only be programmed to do some aspects of the human brain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能或机器智能是一个研究领域，旨在赋予计算机认知能力，使其能够进行学习和解决问题的编程。其目标是模拟具有类似人类智能的计算机。人工智能无法完全模仿人类智能；计算机只能被编程去执行人类大脑的某些方面。
- en: Machine learning is a branch of AI which helps computers to program themselves
    based on the input data. Machine learning gives AI the ability to do data-based
    problem solving. ANNs are an example of machine learning algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是人工智能（AI）的一个分支，帮助计算机根据输入数据自我编程。机器学习赋予人工智能基于数据进行问题解决的能力。人工神经网络（ANNs）是机器学习算法的一个例子。
- en: '**Deep learning** (**DL**) is complex set of neural networks with more layers
    of processing, which develop high levels of abstraction. They are typically used
    for complex tasks, such as image recognition, image classification, and hand writing
    identification.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）是一种复杂的神经网络集合，具有更多的处理层，能够发展出更高水平的抽象。它们通常用于复杂的任务，如图像识别、图像分类和手写识别。'
- en: 'Most of the audience think that neural networks are difficult to learn and
    use it as a black box. This book intends to open the black box and help one learn
    the internals with implementation in R. With the working knowledge, we can see
    many use cases where neural networks can be made tremendously useful seen in the
    following image:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数观众认为神经网络难以学习，并把它当作一个“黑箱”来使用。本书的目的是打开这个“黑箱”，帮助读者通过R语言的实现来学习神经网络的内部工作原理。通过工作知识，我们可以看到神经网络在以下图示中展示的许多应用场景，能够大大提升其价值：
- en: '**![](img/00005.jpeg)**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/00005.jpeg)**'
- en: Inspiration for neural networks
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的灵感来源
- en: 'Neural networks are inspired by the way the human brain works. A human brain
    can process huge amounts of information using data sent by human senses (especially
    vision). The processing is done by neurons, which work on electrical signals passing
    through them and applying flip-flop logic, like opening and closing of the gates
    for signal to transmit through. The following images shows the structure of a
    neuron:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的灵感来自于人类大脑的工作方式。人类大脑通过人类感官（特别是视觉）传递的数据来处理大量信息。神经元在处理过程中，通过电信号在它们之间传递，并应用翻转逻辑，就像信号传输时门的开闭。下图展示了神经元的结构：
- en: '![](img/00006.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '**![](img/00006.jpeg)**'
- en: 'The major components of each neuron are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元的主要组成部分是：
- en: '**Dendrites**: Entry points in each neuron which take input from other neurons
    in the network in form of electrical impulses'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树突**：每个神经元的输入点，接收来自网络中其他神经元的电信号输入。'
- en: '**Cell Body**: It generates inferences from the dendrite inputs and decides
    what action to take'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细胞体**：它根据树突输入生成推论，并决定采取什么行动。'
- en: '**Axon terminals**: They transmit outputs in form of electrical impulses to
    next neuron'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突末端**：它们以电信号的形式将输出传输到下一个神经元。'
- en: Each neuron processes signals only if it exceeds a certain threshold. Neurons
    either fire or do not fire; it is either *0* or *1*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元只有在信号超过某一特定阈值时才会处理信号。神经元要么触发，要么不触发；要么是*0*，要么是*1*。
- en: 'AI has been a domain for sci-fi movies and fiction books. ANNs within AI have
    been around since the 1950s, but we have made them more dominant in the past 10
    years due to advances in computing architecture and performance. There have been
    major advancements in computer processing, leading to:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能曾是科幻电影和小说书籍中的一个领域。人工神经网络（ANNs）自20世纪50年代以来就已经存在，但由于计算架构和性能的进步，过去10年它们变得更加主流。计算机处理能力的重大进展导致了：
- en: Massive parallelism
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模并行处理
- en: Distributed representation and computation
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式表示与计算
- en: Learning and generalization ability
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习和泛化能力
- en: Fault tolerance
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错性
- en: Low energy consumption
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低能耗
- en: In the domain of numerical computations and symbol manipulation, solving problems
    on-top of centralized architecture, modern day computers have surpassed humans
    to a greater extent. Where they actually lag behind with such an organizing structure
    is in the domains of pattern recognition, noise reduction, and optimizing. A toddler
    can recognize his/her mom in a huge crowd, but a computer with a centralized architecture
    wouldn’t be able to do the same.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值计算和符号操作领域，在集中的架构上解决问题时，现代计算机已经在很大程度上超越了人类。它们在模式识别、噪声降低和优化领域仍有所滞后。例如，一个幼儿能够在人群中认出自己的妈妈，但使用集中式架构的计算机却做不到这一点。
- en: This is where the biological neural network of the brain has been outperforming
    machines, and hence the inspiration to develop an alternative loosely held, decentralized
    architecture mimicking the brain.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大脑的生物神经网络超越机器的地方，因此激发了开发一种松散的、去中心化的结构，模拟大脑的灵感。
- en: ANNs are massively parallel computing systems consisting of an extremely large
    number of simple processors with many interconnections.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是一个大规模并行计算系统，由大量简单的处理器和许多互连组成。
- en: One of the leading global news agencies, Guardian, used big data in digitizing
    the archives by uploading the snapshots of all the archives they had had. However,
    for a user to copy the content and use it elsewhere is the limitation here. To
    overcome that, one can use an ANN for text pattern recognition to convert the
    images to text file and then to any format according to the needs of the end-users.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 全球领先的新闻机构之一《卫报》利用大数据对档案进行数字化，通过上传他们所有档案的快照。然而，用户复制内容并在其他地方使用是这里的限制。为了解决这个问题，可以使用人工神经网络（ANN）进行文本模式识别，将图像转换为文本文件，再根据最终用户的需求转换成任何格式。
- en: How do neural networks work?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络是如何工作的？
- en: 'Similar to the biological neuron structure, ANNs define the neuron as a central
    processing unit, which performs a mathematical operation to generate one output
    from a set of inputs. The output of a neuron is a function of the weighted sum
    of the inputs plus the bias. Each neuron performs a very simple operation that
    involves activating if the total amount of signal received exceeds an activation
    threshold, as shown in the following figure:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于生物神经元结构，人工神经网络将神经元定义为一个中央处理单元，它执行数学运算，将一组输入生成一个输出。神经元的输出是输入的加权和加上偏置的函数。每个神经元执行一个非常简单的操作，当接收到的总信号超过激活阈值时就会被激活，如下图所示：
- en: '![](img/00007.gif)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00007.gif)'
- en: 'The function of the entire neural network is simply the computation of the
    outputs of all the neurons, which is an entirely deterministic calculation. Essentially,
    ANN is a set of mathematical function approximations. We would now be introducing
    new terminology associated with ANNs:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 整个神经网络的功能就是计算所有神经元的输出，这完全是一个确定性的计算过程。从本质上讲，ANN 是一组数学函数的逼近。我们现在将介绍与 ANN 相关的新术语：
- en: Input layer
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layer
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Output layer
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: Weights
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重
- en: Bias
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置
- en: Activation functions
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Layered approach
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层方法
- en: 'Any neural network processing a framework has the following architecture:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 任何神经网络处理框架都有以下结构：
- en: '![](img/00008.gif)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.gif)'
- en: There is a set of inputs, a processor, and a set of outputs. This layered approach
    is also followed in neural networks. The inputs form the **input layer**, the
    **middle layer(s)** which performs the processing is called the **hidden layer(s)**,
    and the **output(s)** forms the output layer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一组输入，一个处理器和一组输出。这种分层方法也被神经网络所采用。输入层由输入组成，执行处理的 **中间层** 被称为 **隐藏层**，而 **输出层**
    形成输出层。
- en: Our neural network architectures are also based on the same principle. The hidden
    layer has the magic to convert the input to the desired output. The understanding
    of the hidden layer requires knowledge of weights, bias, and activation functions,
    which is our next topic of discussion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络架构也基于相同的原理。隐藏层有魔力，它将输入转换为所需的输出。理解隐藏层需要了解权重、偏置和激活函数，这是我们接下来讨论的话题。
- en: Weights and biases
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重和偏置
- en: Weights in an ANN are the most important factor in converting an input to impact
    the output. This is similar to slope in linear regression, where a weight is multiplied
    to the input to add up to form the output. Weights are numerical parameters which
    determine how strongly each of the neurons affects the other.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络（ANN）中，权重是将输入转化为影响输出的最重要因素。这类似于线性回归中的斜率，其中权重被乘以输入，以加和形成输出。权重是数值参数，决定了每个神经元对其他神经元的影响强度。
- en: For a typical neuron, if the inputs are *x[1]*, *x[2]*, and *x[3]*, then the
    synaptic weights to be applied to them are denoted as *w[1]*, *w[2]*, and *w[3]*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个典型的神经元，如果输入为 *x[1]*、*x[2]* 和 *x[3]*，那么应用于它们的突触权重分别表示为 *w[1]*、*w[2]* 和 *w[3]*。
- en: Output is
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为
- en: '![](img/00009.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00009.jpeg)'
- en: where *i* is *1* to the number of inputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i* 是从 *1* 到输入数量的编号。
- en: Simply, this is a matrix multiplication to arrive at the weighted sum.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这是一个矩阵乘法过程，用来得到加权和。
- en: Bias is like the intercept added in a linear equation. It is an additional parameter
    which is used to adjust the output along with the weighted sum of the inputs to
    the neuron.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置就像线性方程中加上的截距。它是一个额外的参数，用于调整输出，同时与神经元输入的加权和一起作用。
- en: 'The processing done by a neuron is thus denoted as :'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元所执行的处理可以表示为：
- en: '![](img/00010.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00010.jpeg)'
- en: 'A function is applied on this output and is called an **activation function**.
    The input of the next layer is the output of the neurons in the previous layer,
    as shown in the following image:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对此输出应用一个函数，称为**激活函数**。下一层的输入是前一层神经元的输出，如下图所示：
- en: '![](img/00011.gif)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.gif)'
- en: Training neural networks
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: Training is the act of presenting the network with some sample data and modifying
    the weights to better approximate the desired function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是通过向网络提供一些样本数据，并修改权重以更好地逼近期望的功能。
- en: 'There are two main types of training: supervised learning and unsupervised
    learning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 训练主要有两种类型：监督学习和无监督学习。
- en: Supervised learning
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: We supply the neural network with inputs and the desired outputs. Response of
    the network to the inputs is measured. The weights are modified to reduce the
    difference between the actual and desired outputs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向神经网络提供输入和期望的输出。网络对输入的响应被测量，权重被修改，以减少实际输出和期望输出之间的差异。
- en: Unsupervised learning
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: We only supply inputs. The neural network adjusts its own weights, so that similar
    inputs cause similar outputs. The network identifies the patterns and differences
    in the inputs without any external assistance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只提供输入。神经网络调整自身的权重，使得相似的输入产生相似的输出。网络识别输入中的模式和差异，而不依赖外部帮助。
- en: Epoch
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练轮次（Epoch）
- en: One iteration or pass through the process of providing the network with an input
    and updating the network's weights is called an **epoch**. It is a full run of
    feed-forward and backpropagation for update of weights. It is also one full read
    through of the entire dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一次迭代或遍历网络过程，即给网络提供输入并更新网络权重的过程，称为**训练轮次（epoch）**。它是前馈和反向传播更新权重的完整过程，也是对整个数据集的完整读取。
- en: Typically, many epochs, in the order of tens of thousands at times, are required
    to train the neural network efficiently. We will see more about epochs in the
    forthcoming chapters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，需要进行多次训练轮次，有时达到数万次，才能高效地训练神经网络。我们将在后续章节中了解更多关于训练轮次的内容。
- en: Activation functions
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'The abstraction of the processing of neural networks is mainly achieved through
    the activation functions. An activation function is a mathematical function which
    converts the input to an output, and adds the magic of neural network processing.
    Without activation functions, the working of neural networks will be like linear
    functions. A linear function is one where the output is directly proportional
    to input, for example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络处理的抽象主要通过激活函数来实现。激活函数是一个数学函数，它将输入转化为输出，并增加神经网络处理的魔力。没有激活函数，神经网络的工作就像线性函数。线性函数的输出与输入成正比，例如：
- en: '![](img/00012.jpeg)![](img/00013.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)![](img/00013.jpeg)'
- en: A linear function is a polynomial of one degree. Simply, it is a straight line
    without any curves.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数是一次多项式。简单来说，它是一条没有任何曲线的直线。
- en: 'However, most of the problems the neural networks try to solve are nonlinear
    and complex in nature. To achieve the nonlinearity, the activation functions are
    used. Nonlinear functions are high degree polynomial functions, for example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数神经网络试图解决的问题本质上是非线性和复杂的。为了实现非线性，使用了激活函数。非线性函数是高次多项式函数，例如：
- en: '![](img/00014.jpeg)![](img/00015.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)![](img/00015.jpeg)'
- en: The graph of a nonlinear function is curved and adds the complexity factor.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性函数的图形是曲线的，增加了复杂性因素。
- en: Activation functions give the nonlinearity property to neural networks and make
    them true universal function approximators.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数赋予神经网络非线性特性，使其成为真正的通用函数逼近器。
- en: Different activation functions
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的激活函数
- en: There are many activation functions available for a neural network to use. We
    shall see a few of them here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有许多可用的激活函数。我们将在这里介绍其中一些。
- en: Linear function
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性函数
- en: 'The simplest activation function, one that is commonly used for the output
    layer activation function in neural network problems, is the linear activation
    function represented by the following formula:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的激活函数，是在神经网络问题中常用于输出层的激活函数，它由以下公式表示：
- en: '![](img/00016.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: 'The output is same as the input and the function is defined in the range (*-infinity,
    +infinity*). In the following figure, a linear activation function is shown:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与输入相同，函数定义在范围（*-infinity, +infinity*）内。下图展示了一个线性激活函数：
- en: '![](img/00017.gif)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.gif)'
- en: Unit step activation function
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单位阶跃激活函数
- en: 'A unit step activation function is a much-used feature in neural networks.
    The output assumes value *0* for negative argument and *1* for positive argument.
    The function is as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 单位阶跃激活函数是神经网络中使用频繁的特性。对于负数参数，输出值为*0*，对于正数参数，输出值为*1*。该函数如下所示：
- en: '![](img/00018.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00018.jpeg)'
- en: 'The range is between *(0,1)* and the output is binary in nature. These types
    of activation functions are useful for binary schemes. When we want to classify
    an input model in one of two groups, we can use a binary compiler with a unit
    step activation function. A unit step activation function is shown in the following
    figure:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 范围在*(0,1)*之间，输出是二元的。这些类型的激活函数在二进制方案中非常有用。当我们想要将输入模型分类到两个组中的一个时，可以使用带有单位阶跃激活函数的二进制编译器。单位阶跃激活函数如下图所示：
- en: '![](img/00019.gif)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.gif)'
- en: Sigmoid
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The *sigmoid* function is a mathematical function that produces a sigmoidal
    curve; a characteristic curve for its *S* shape. This is the earliest and often
    used activation function. This squashes the input to any value between *0* and
    *1*, and makes the model logistic in nature. This function refers to a special
    case of logistic function defined by the following formula:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*sigmoid*函数是一个数学函数，产生一个sigmoidal曲线；其特征曲线呈*S*形状。这是最早且常用的激活函数。它将输入压缩到*0*和*1*之间的任何值，使模型在本质上具有逻辑性。该函数是由以下公式定义的逻辑函数的特例：'
- en: '![](img/00020.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00020.jpeg)'
- en: 'In the following figure is shown a sigmoid curve with an *S* shape:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个具有*S*形状的sigmoid曲线：
- en: '![](img/00021.gif)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.gif)'
- en: Hyperbolic tangent
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: 'Another very popular and widely used activation feature is the *tanh* function.
    If you look at the figure that follows, you can notice that it looks very similar
    to *sigmoid*; in fact, it is a scaled *sigmoid* function. This is a nonlinear
    function, defined in the range of values *(-1, 1)*, so you need not worry about
    activations blowing up. One thing to clarify is that the gradient is stronger
    for *tanh* than *sigmoid* (the derivatives are more steep). Deciding between *sigmoid*
    and *tanh* will depend on your gradient strength requirement. Like the *sigmoid*,
    *tanh* also has the missing slope problem. The function is defined by the following
    formula:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行并广泛使用的激活特性是*tanh*函数。如果你看接下来的图形，你会发现它看起来与*sigmoid*非常相似；事实上，它是一个缩放过的*sigmoid*函数。这是一个非线性函数，定义在*(-1,
    1)*的范围内，因此你不必担心激活值爆炸的问题。需要澄清的一点是，*tanh*的梯度比*sigmoid*更强（其导数更陡峭）。在*sigmoid*和*tanh*之间做决定将取决于你的梯度强度需求。像*sigmoid*一样，*tanh*也有缺失斜率的问题。该函数由以下公式定义：
- en: '![](img/00022.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: 'In the following figure is shown a hyberbolic tangent activation function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个双曲正切激活函数：
- en: '![](img/00023.gif)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.gif)'
- en: This looks very similar to *sigmoid*; in fact, it is a scaled *sigmoid* function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来与*sigmoid*非常相似；事实上，它是一个缩放过的*sigmoid*函数。
- en: Rectified Linear Unit
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: '**Rectified Linear Unit** (**ReLU**) is the most used activation function since
    2015\. It is a simple condition and has advantages over the other functions. The
    function is defined by the following formula:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）是自2015年以来使用最广泛的激活函数。它是一个简单的条件，并且相较于其他函数有优势。该函数由以下公式定义：'
- en: '![](img/00024.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: 'In the following figure is shown a ReLU activation function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个ReLU激活函数：
- en: '![](img/00025.gif)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.gif)'
- en: The range of output is between *0* and infinity. ReLU finds applications in
    computer vision and speech recognition using deep neural nets. There are various
    other activation functions as well, but we have covered the most important ones
    here.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的范围在*0*到无穷大之间。ReLU在计算机视觉和语音识别中使用深度神经网络时找到了应用。还有许多其他的激活函数，但我们在这里涵盖了最重要的几个。
- en: Which activation functions to use?
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应该使用哪些激活函数？
- en: 'Given that neural networks are to support nonlinearity and more complexity,
    the activation function to be used has to be robust enough to have the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络需要支持非线性和更复杂的特性，所使用的激活函数必须足够强大，满足以下要求：
- en: It should be differential; we will see why we need differentiation in backpropagation.
    It should not cause gradients to vanish.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该是可微分的；我们将在反向传播中看到为什么需要微分。它不应导致梯度消失。
- en: It should be simple and fast in processing.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该是简单且快速的处理。
- en: It should not be zero centered.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不应是零中心的。
- en: 'The *sigmoid* is the most used activation function, but it suffers from the
    following setbacks:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*sigmoid*是最常用的激活函数，但它有以下几个问题：'
- en: Since it uses logistic model, the computations are time consuming and complex
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用的是逻辑模型，计算过程非常耗时且复杂。
- en: It cause gradients to vanish and no signals pass through the neurons at some
    point of time
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会导致梯度消失，某个时刻神经元之间没有信号传递。
- en: It is slow in convergence
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它收敛较慢。
- en: It is not zero centered
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不是零中心的。
- en: These drawbacks are solved by ReLU. ReLU is simple and is faster to process.
    It does not have the vanishing gradient problem and has shown vast improvements
    compared to the *sigmoid* and *tanh* functions. ReLU is the most preferred activation
    function for neural networks and DL problems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点通过ReLU得到了改善。ReLU简单且处理速度更快。它没有消失梯度问题，并且相比于*sigmoid*和*tanh*函数，取得了显著的改进。ReLU是神经网络和深度学习问题中最常用的激活函数。
- en: ReLU is used for hidden layers, while the output layer can use a `softmax` function
    for logistic problems and a linear function of regression problems.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU用于隐藏层，而输出层可以使用`softmax`函数来解决逻辑回归问题，线性函数用于回归问题。
- en: Perceptron and multilayer architectures
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器和多层架构
- en: A perceptron is a single neuron that classifies a set of inputs into one of
    two categories (usually *1* or *-1*). If the inputs are in the form of a grid,
    a perceptron can be used to recognize visual images of shapes. The perceptron
    usually uses a step function, which returns *1* if the weighted sum of the inputs
    exceeds a threshold, and *0* otherwise.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一个单一神经元，用于将一组输入分类为两类之一（通常是*1*或*-1*）。如果输入是以网格形式出现，感知器可以用来识别形状的视觉图像。感知器通常使用阶跃函数，如果输入的加权和超过某个阈值，返回*1*，否则返回*0*。
- en: When layers of perceptron are combined together, they form a multilayer architecture,
    and this gives the required complexity of the neural network processing. **Multi-Layer
    Perceptrons** (**MLPs**) are the most widely used architecture for neural networks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个感知器层组合在一起时，它们形成一个多层架构，这提供了神经网络处理所需的复杂性。**多层感知器**（**MLPs**）是最广泛使用的神经网络架构。
- en: Forward and backpropagation
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播和反向传播
- en: The processing from input layer to hidden layer(s) and then to the output layer
    is called **forward propagation**. The *sum(input*weights)+bias* is applied at
    each layer and then the activation function value is propagated to the next layer.
    The next layer can be another hidden layer or the output layer. The construction
    of neural networks uses large number of hidden layers to give rise to **Deep Neural
    Network** (**DNN**).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入层到隐藏层（或多个隐藏层）再到输出层的处理过程称为**前向传播**。每一层都应用*sum(input*weights)+bias*，然后激活函数的值被传递到下一层。下一层可以是另一个隐藏层，也可以是输出层。神经网络的构建使用大量的隐藏层，从而形成**深度神经网络**（**DNN**）。
- en: Once the output is arrived at, at the last layer (the output layer), we compute
    the error (the predicted output minus the original output). This error is required
    to correct the weights and biases used in forward propagation. Here is where the
    derivative function is used. The amount of weight that has to be changed is determined
    by **gradient descent**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在最后一层（输出层）得到了输出，我们计算误差（预测输出减去原始输出）。这个误差用于修正前向传播中使用的权重和偏差。这里需要使用导数函数。需要改变的权重量由**梯度下降**决定。
- en: The backpropagation process uses the partial derivative of each neuron's activation
    function to identify the slope (or gradient) in the direction of each of the incoming
    weights. The gradient suggests how steeply the error will be reduced or increased
    for a change in the weight. The backpropagation keeps changing the weights until
    there is greatest reduction in errors by an amount known as the **learning rate**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播过程使用每个神经元激活函数的偏导数来识别每个输入权重的梯度（或坡度）。梯度指示权重变化时，误差将如何剧烈地增加或减少。反向传播会不断地调整权重，直到误差减少到最大程度，减小量由**学习率**决定。
- en: Learning rate is a scalar parameter, analogous to step size in numerical integration,
    used to set the rate of adjustments to reduce the errors faster. Learning rate
    is used in backpropagation during adjustment of weights and bias.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是一个标量参数，类似于数值积分中的步长，用于设置调整速率以加快误差的减少。学习率在反向传播过程中用于调整权重和偏差。
- en: 'More the learning rate, the faster the algorithm will reduce the errors and
    faster will be the training process:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率越高，算法减少误差的速度越快，训练过程也越快：
- en: '![](img/00026.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: Step-by-step illustration of a neuralnet and an activation function
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络和激活函数的逐步说明
- en: 'We shall take a step-by-step approach to understand the forward and reverse
    pass with a single hidden layer. The input layer has one neuron and the output
    will solve a binary classification problem (predict 0 or 1). In the following
    figure is shown a forward and reverse pass with a single hidden layer:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过逐步的方式，理解带有单隐藏层的前向和反向传播。输入层有一个神经元，输出将解决一个二元分类问题（预测0或1）。下图展示了带有单隐藏层的前向和反向传播：
- en: '![](img/00027.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: 'Next, let us analyze in detail, step by step, all the operations to be done
    for network training:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们详细逐步分析网络训练过程中需要执行的所有操作：
- en: Take the input as a matrix.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入作为矩阵处理。
- en: Initialize the weights and biases with random values. This is one time and we
    will keep updating these with the error propagation process.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重和偏差为随机值。这是一次性操作，我们将通过误差传播过程不断更新这些值。
- en: Repeat the steps 4 to 9 for each training pattern (presented in random order),
    until the error is minimized.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个训练模式重复步骤4到9（以随机顺序呈现），直到误差最小化。
- en: Apply the inputs to the network.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入应用到网络中。
- en: Calculate the output for every neuron from the input layer, through the hidden
    layer(s), to the output layer.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从输入层到输出层，每个神经元的输出，包括隐藏层（如果有）。
- en: 'Calculate the error at the outputs: actual minus predicted.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出的误差：实际值减去预测值。
- en: Use the output error to compute error signals for previous layers. The partial
    derivative of the activation function is used to compute the error signals.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用输出误差计算前面层的误差信号。激活函数的偏导数用于计算误差信号。
- en: Use the error signals to compute weight adjustments.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用误差信号计算权重调整。
- en: Apply the weight adjustments.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用权重调整。
- en: Steps 4 and 5 are forward propagation and steps 6 through 9 are backpropagation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4和5是前向传播，步骤6到9是反向传播。
- en: The learning rate is the amount that weights are updated is controlled by a
    configuration parameter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是控制权重更新量的配置参数。
- en: The complete pass back and forth is called a **training cycle** or **epoch**.
    The updated weights and biases are used in the next cycle. We keep recursively
    training until the error is very minimal.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的来回传递过程称为**训练周期**或**时代**。更新后的权重和偏差将在下一个周期中使用。我们会不断进行递归训练，直到误差极小。
- en: We shall cover more about the forward and backpropagation in detail throughout
    this book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将进一步详细讲解前向传播和反向传播。
- en: Feed-forward and feedback networks
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈网络与反馈网络
- en: 'The flow of the signals in neural networks can be either in only one direction
    or in recurrence. In the first case, we call the neural network architecture feed-forward,
    since the input signals are fed into the input layer, then, after being processed,
    they are forwarded to the next layer, just as shown in the following figure. MLPs
    and radial basis functions are also good examples of feed-forward networks. In
    the following figure is shown an MLPs architecture:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中信号的流动可以是单向的，也可以是循环的。在第一种情况下，我们称神经网络架构为前馈网络，因为输入信号被输入到输入层后，经过处理后会传递到下一层，就像下图所示。MLP（多层感知机）和径向基函数也是前馈网络的典型例子。下图展示了一个MLP架构：
- en: '![](img/00028.gif)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.gif)'
- en: 'When the neural network has some kind of internal recurrence, meaning that
    the signals are fed back to a neuron or layer that has already received and processed
    that signal, the network is of the type feedback, as shown in the following image:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络存在某种内部反馈时，意味着信号被反馈到已经接收和处理过该信号的神经元或层中，那么网络就是反馈型网络，如下图所示：
- en: '![](img/00029.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.jpeg)'
- en: The special reason to add recurrence in a network is the production of a dynamic
    behavior, particularly when the network addresses problems involving time series
    or pattern recognition, that require an internal memory to reinforce the learning
    process. However, such networks are particularly difficult to train, eventually
    failing to learn. Most of the feedback networks are single layer, such as the
    **Elman** and **Hopfield** networks, but it is possible to build a recurrent multilayer
    network, such as echo and recurrent MLP networks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中添加循环的特别原因是产生动态行为，尤其是在网络处理涉及时间序列或模式识别的任务时，这些任务需要一个内部记忆来加强学习过程。然而，这类网络尤其难以训练，最终可能会无法学习。大多数反馈型网络是单层的，如**Elman**网络和**Hopfield**网络，但也可以构建递归的多层网络，如回声网络和递归MLP网络。
- en: Gradient descent
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is an iterative approach for error correction in any learning
    model. For neural networks during backpropagation, the process of iterating the
    update of weights and biases with the error times derivative of the activation
    function is the gradient descent approach. The steepest descent step size is replaced
    by a similar size from the previous step. Gradient is basically defined as the
    slope of the curve and is the derivative of the activation function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是任何学习模型中用于错误校正的迭代方法。对于神经网络在反向传播期间，通过误差乘以激活函数的导数来迭代更新权重和偏置的过程是梯度下降的方法。最陡下降步长被上一步的相似大小替换。梯度基本上定义为曲线的斜率，是激活函数的导数：
- en: '![](img/00030.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.jpeg)'
- en: The objective of deriving gradient descent at each step is to find the global
    cost minimum, where the error is the lowest. And this is where the model has a
    good fit for the data and predictions are more accurate.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步推导梯度下降的目标是找到全局成本最低点，其中误差最小。这是模型对数据拟合良好且预测更准确的地方。
- en: Gradient descent can be performed either for the full batch or stochastic. In
    full batch gradient descent, the gradient is computed for the full training dataset,
    whereas **Stochastic Gradient Descent** (**SGD**) takes a single sample and performs
    gradient calculation. It can also take mini-batches and perform the calculations.
    One advantage of SGD is faster computation of gradients.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降可以在完整批次或随机中执行。在完整批次梯度下降中，为整个训练数据集计算梯度，而**随机梯度下降** (**SGD**) 则使用单个样本进行梯度计算。它还可以采用小批量并执行计算。SGD的一个优点是更快地计算梯度。
- en: Taxonomy of neural networks
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的分类
- en: 'The basic foundation for ANNs is the same, but various neural network models
    have been designed during its evolution. The following are a few of the ANN models:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的基本基础是相同的，但在其演变过程中设计了各种神经网络模型。以下是一些人工神经网络模型：
- en: '**Adaptive Linear Element** (**ADALINE**), is a simple perceptron which can
    solve only linear problems. Each neuron takes the weighted linear sum of the inputs
    and passes it to a bi-polar function, which either produces a *+1* or *-1* depending
    on the sum. The function checks the sum of the inputs passed and if the net is
    *>= 0*, it is *+1*, else it is *-1*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应线性元素** (**ADALINE**)，是一个简单的感知器，只能解决线性问题。每个神经元都接收输入的加权线性和，并将其传递给双极函数，该函数根据总和产生*+1*或*-1*。该函数检查传递的输入总和，如果净值*>=
    0*，则为*+1*，否则为*-1*。'
- en: '**Multiple ADALINEs** (**MADALINE**), is a multilayer network of ADALINE units.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个ADALINE** (**MADALINE**)，是ADALINE单元的多层网络。'
- en: Perceptrons are single layer neural networks (single neuron or unit), where
    the input is multidimensional (vector) and the output is a function on the weight
    sum of the inputs.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器是单层神经网络（单个神经元或单元），其中输入是多维的（向量），输出是输入权重和的函数。
- en: Radial basis function network is an ANN where a radial basis function is used
    as an activation function. The network output is a linear combination of radial
    basis functions of the inputs and some neuron parameters.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 径向基函数网络是一种人工神经网络，其中径向基函数用作激活函数。网络输出是输入和一些神经元参数的径向基函数的线性组合。
- en: 'Feed-forward is the simplest form of neural networks. The data is processed
    across layers without any loops are cycles. We will study the following feed-
    forward networks in this book:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈是神经网络中最简单的形式。数据在各层之间处理，没有任何循环或循环。我们将在本书中研究以下前馈网络：
- en: Autoencoder
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器
- en: Probabilistic
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率的
- en: Time delay
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间延迟
- en: Covolutional
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积的
- en: '**Recurrent Neural Networks** (**RNNs**), unlike feed-forward networks, propagate
    data forward and also backwards from later processing stages to earlier stages.
    The following are the types of RNNs; we shall study them in our later chapters:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归神经网络** (**RNNs**)，与前馈网络不同，将数据前向和后向传播，从后续处理阶段向早期阶段传播。以下是RNN的类型；我们将在后续章节中学习它们：'
- en: Hopfield networks
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络
- en: Boltzmann machine
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波尔兹曼机器
- en: '**Self Organizing Maps** (**SOMs**)'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自组织映射** (**SOMs**)'
- en: '**Bidirectional Associative Memory** (**BAM**)'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向联想记忆** (**BAM**)'
- en: '**Long Short Term Memory** (**LSTM**)'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆** (**LSTM**)'
- en: 'The following images depict **(a) Recurrent neural network** and **(b) Forward
    neural network**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片描述了**(a) 循环神经网络**和**(b) 前向神经网络**：
- en: '![](img/00031.gif)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.gif)'
- en: Simple example using R neural net library - neuralnet()
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用R神经网络库进行简单示例 - neuralnet()
- en: 'Consider a simple dataset of a square of numbers, which will be used to train
    a `neuralnet` function in R and then test the accuracy of the built neural network:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的数据集，包含数字的平方，我们将使用它来训练R中的 `neuralnet` 函数，然后测试构建的神经网络的准确性：
- en: '| **INPUT** | **OUTPUT** |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **输出** |'
- en: '| `0` | `0` |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `0` |'
- en: '| `1` | `1` |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `1` |'
- en: '| `2` | `4` |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `4` |'
- en: '| `3` | `9` |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `3` | `9` |'
- en: '| `4` | `16` |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| `4` | `16` |'
- en: '| `5` | `25` |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| `5` | `25` |'
- en: '| `6` | `36` |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `6` | `36` |'
- en: '| `7` | `49` |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `7` | `49` |'
- en: '| `8` | `64` |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `8` | `64` |'
- en: '| `9` | `81` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `9` | `81` |'
- en: '| `10` | `100` |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `10` | `100` |'
- en: 'Our objective is to set up the weights and bias so that the model can do what
    is being done here. The output needs to be modeled on a function of input and
    the function can be used in future to determine the output based on an input:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是设置权重和偏置，以便模型能够完成这里所做的工作。输出需要基于输入的函数进行建模，这个函数未来可以用于根据输入确定输出：
- en: '[PRE0]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let us go through the code line-by-line
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们逐行查看代码
- en: 'To understand all the steps in the code just proposed, we will look at them
    in detail. Do not worry if a few steps seem unclear at this time, you will be
    able to look into it in the following examples. First, the code snippet will be
    shown, and the explanation will follow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解上面所提到的所有代码步骤，我们将逐一详细讲解。如果某些步骤目前看起来不清楚，不用担心，您可以在接下来的示例中深入了解。首先将显示代码片段，然后给出相应的解释：
- en: '[PRE1]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The line in R includes the library `neuralnet()` in our program. `neuralnet()`
    is part of **Comprehensive R Archive Network** (**CRAN**), which contains numerous
    R libraries for various applications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 该行代码在我们的程序中包含了 `neuralnet()` 库。`neuralnet()` 是 **综合R档案网络**（**CRAN**）的一部分，CRAN
    包含了许多用于各种应用的 R 库。
- en: '[PRE2]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This reads the CSV file with separator `,`(comma), and header is the first line
    in the file. `names()` would display the header of the file.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '| 该代码读取以 `,`（逗号）为分隔符的CSV文件，文件的第一行为表头。使用 `names()` 可以显示文件的表头。|'
- en: '[PRE3]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The training of the output with respect to the input happens here. The `neuralnet()`
    library is passed the output and input column names (`ouput~input`), the dataset
    to be used, the number of neurons in the hidden layer, and the stopping criteria
    (`threshold`).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '| 输出与输入的训练过程发生在此。`neuralnet()` 函数接收输出和输入的列名（`output~input`），数据集，隐藏层中的神经元数目，以及停止标准（`threshold`）。
    |'
- en: 'A brief description of the `neuralnet` package, extracted from the official
    documentation, is shown in the following table:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet` 包的简要描述，摘自官方文档，见下表：'
- en: '| **neuralnet-package**: |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **neuralnet包**： |'
- en: '| **Description**: |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **描述**： |'
- en: '| Training of neural networks using the backpropagation, resilient backpropagation
    with (Riedmiller, 1994) or without weight backtracking (Riedmiller, 1993), or
    the modified globally convergent version by Anastasiadis et al. (2005). The package
    allows flexible settings through custom-choice of error and activation function.
    Furthermore, the calculation of generalized weights (Intrator O & Intrator N,
    1993) is implemented. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 使用反向传播训练神经网络，带有（Riedmiller，1994）或不带权重回溯的弹性反向传播（Riedmiller，1993），或由Anastasiadis等人（2005）修改的全局收敛版本。该包允许通过自定义选择误差和激活函数来进行灵活设置。此外，还实现了广义权重的计算（Intrator
    O 和 Intrator N，1993）。 |'
- en: '| **Details**: |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **详情**： |'
- en: '| Package: `neuralnet`Type: PackageVersion: 1.33Date: 2016-08-05License: GPL
    (>=2) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 包：`neuralnet` 类型：包 版本：1.33 日期：2016-08-05 许可：GPL (>=2) |'
- en: '| **Authors**: |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **作者**： |'
- en: '| Stefan Fritsch, Frauke Guenther (email: `guenther@leibniz-bips.de`)Maintainer:
    Frauke Guenther (email: `guenther@leibniz-bips.de`) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Stefan Fritsch, Frauke Guenther (电子邮件：`guenther@leibniz-bips.de`) 维护者：Frauke
    Guenther (电子邮件：`guenther@leibniz-bips.de`) |'
- en: '| **Usage**: |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **用法**： |'
- en: '| `neuralnet(formula, data, hidden = 1, threshold = 0.01, stepmax = 1e+05,
    rep = 1, startweights = NULL, learningrate.limit = NULL, learningrate.factor =
    list(minus = 0.5, plus = 1.2), learningrate=NULL, lifesign = "none", lifesign.step
    = 1000, algorithm = "rprop+", err.fct = "sse", act.fct = "logistic", linear.output
    = TRUE, exclude = NULL,` `constant.weights = NULL, likelihood = FALSE)` |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `neuralnet(formula, data, hidden = 1, threshold = 0.01, stepmax = 1e+05,
    rep = 1, startweights = NULL, learningrate.limit = NULL, learningrate.factor =
    list(minus = 0.5, plus = 1.2), learningrate=NULL, lifesign = "none", lifesign.step
    = 1000, algorithm = "rprop+", err.fct = "sse", act.fct = "logistic", linear.output
    = TRUE, exclude = NULL,` `constant.weights = NULL, likelihood = FALSE)` |'
- en: '| **Meaning of the arguments**: |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **参数的含义**： |'
- en: '| `formula`: A symbolic description of the model to be fitted.`data`: A dataframe
    containing the variables specified in formula.`hidden`: A vector of integers specifying
    the number of hidden neurons (vertices) in each layer.`threshold`: A numeric value
    specifying the threshold for the partial derivatives of the error function as
    stopping criteria.`stepmax`: The maximum steps for the training of the neural
    network. Reaching this maximum leads to a stop of the neural network''s training
    process.`rep`: The number of repetitions for the neural network''s training.`startweights`:
    A vector containing starting values for the weights. The weights will not be randomly
    initialized.`learningrate.limit`: A vector or a list containing the lowest and
    highest limit for the learning rate. Used only for `RPROP` and `GRPROP`.`learningrate.factor`:
    A vector or a list containing the multiplication factors for the upper and lower
    learning rate, used only for `RPROP` and `GRPROP`.`learningrate`: A numeric value
    specifying the learning rate used by traditional backpropagation. Used only for
    traditional backpropagation.`lifesign`: A string specifying how much the function
    will print during the calculation of the neural network-`''none''`, `''minimal''`,
    or `''full''`.`lifesign.step`: An integer specifying the step size to print the
    minimal threshold in full lifesign mode.`algorithm`: A string containing the algorithm
    type to calculate the neural network.`err.fct`: A differentiable function that
    is used for the calculation of the error.`act.fct`: A differentiable function
    that is used for smoothing the result of the cross product of the covariate or
    neurons and the weights.`linear.output`: Logical. If `act.fct` should not be applied
    to the output neurons set linear output to `TRUE`, otherwise to `FALSE`.`exclude`:
    A vector or a matrix specifying the weights that are excluded from the calculation.`constant.weights`:
    A vector specifying the values of the weights that are excluded from the training
    process and treated as fix.`likelihood`: Logical. If the error function is equal
    to the negative log-likelihood function, the information criteria AIC and BIC
    will be calculated. Furthermore the usage of confidence. interval is meaningful.
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| `formula`: 模型的符号描述。`data`: 一个包含公式中指定变量的数据框。`hidden`: 一个整数向量，指定每一层中的隐藏神经元（节点）数量。`threshold`:
    一个数值，指定误差函数的偏导数的阈值作为停止标准。`stepmax`: 神经网络训练的最大步数。达到此最大值会导致神经网络训练过程停止。`rep`: 神经网络训练的重复次数。`startweights`:
    一个包含初始权重值的向量。权重将不会随机初始化。`learningrate.limit`: 一个向量或列表，包含学习率的最低和最高限制。仅用于`RPROP`和`GRPROP`。`learningrate.factor`:
    一个向量或列表，包含学习率上限和下限的乘法因子，仅用于`RPROP`和`GRPROP`。`learningrate`: 一个数值，指定传统反向传播算法使用的学习率。仅用于传统反向传播。`lifesign`:
    一个字符串，指定神经网络计算过程中输出的详细程度- `''none''`、`''minimal''` 或 `''full''`。`lifesign.step`:
    一个整数，指定在完整的生命周期模式下，打印最小阈值的步长。`algorithm`: 一个字符串，包含用于计算神经网络的算法类型。`err.fct`: 一个可微函数，用于计算误差。`act.fct`:
    一个可微函数，用于平滑协变量或神经元与权重的叉积结果。`linear.output`: 逻辑值。如果输出神经元不应用 `act.fct`，则将线性输出设置为
    `TRUE`，否则设置为 `FALSE`。`exclude`: 一个向量或矩阵，指定从计算中排除的权重。`constant.weights`: 一个向量，指定在训练过程中被排除并视为固定的权重值。`likelihood`:
    逻辑值。如果误差函数等于负对数似然函数，则会计算信息标准 AIC 和 BIC。此外，置信区间的使用也是有意义的。|'
- en: 'After giving a brief glimpse into the package documentation, let''s review
    the remaining lines of the proposed code sample:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要浏览了包的文档后，我们来看一下剩余的代码示例：
- en: '[PRE4]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This command prints the model that has just been generated, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令输出刚生成的模型，如下所示：
- en: '[PRE5]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s go back to the code analysis:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到代码分析：
- en: '[PRE6]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This preceding command plots the neural network for us, as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令为我们绘制了神经网络，如下所示：
- en: '![](img/00032.gif)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.gif)'
- en: '[PRE7]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This preceding code prints the final output, comparing the output predicted
    and actual as:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码输出最终结果，将预测输出与实际输出进行比较，如下所示：
- en: '[PRE8]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Implementation using nnet() library
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 nnet() 库的实现
- en: 'To improve our practice with the `nnet` library, we look at another example.
    This time we will use the data collected at a restaurant through customer interviews.
    The customers were asked to give a score to the following aspects: service, ambience,
    and food. They were also asked whether they would leave the tip on the basis of
    these scores. In this case, the number of inputs is `2` and the output is a categorical
    value (`Tip=1` and `No-tip=0`).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们对`nnet`库的使用，我们来看另一个例子。这一次，我们将使用通过顾客访谈收集的数据。顾客被要求对以下方面进行评分：服务、环境和食物。顾客还被问到是否会根据这些评分给小费。在这个案例中，输入的数量是`2`，输出是一个分类值（`Tip=1`和`No-tip=0`）。
- en: 'The input file to be used is shown in the following table:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用的输入文件如下面的表格所示：
- en: '| **No** | **CustomerWillTip** | **Service** | **Ambience** | **Food** | **TipOrNo**
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| **编号** | **顾客是否给小费** | **服务** | **环境** | **食物** | **是否给小费** |'
- en: '| `1` | `1` | `4` | `4` | `5` | `Tip` |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `1` | `4` | `4` | `5` | `Tip` |'
- en: '| `2` | `1` | `6` | `4` | `4` | `Tip` |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `1` | `6` | `4` | `4` | `Tip` |'
- en: '| `3` | `1` | `5` | `2` | `4` | `Tip` |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `3` | `1` | `5` | `2` | `4` | `Tip` |'
- en: '| `4` | `1` | `6` | `5` | `5` | `Tip` |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `4` | `1` | `6` | `5` | `5` | `Tip` |'
- en: '| `5` | `1` | `6` | `3` | `4` | `Tip` |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `5` | `1` | `6` | `3` | `4` | `Tip` |'
- en: '| `6` | `1` | `3` | `4` | `5` | `Tip` |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `6` | `1` | `3` | `4` | `5` | `Tip` |'
- en: '| `7` | `1` | `5` | `5` | `5` | `Tip` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `7` | `1` | `5` | `5` | `5` | `Tip` |'
- en: '| `8` | `1` | `5` | `4` | `4` | `Tip` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `8` | `1` | `5` | `4` | `4` | `Tip` |'
- en: '| `9` | `1` | `7` | `6` | `4` | `Tip` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `9` | `1` | `7` | `6` | `4` | `Tip` |'
- en: '| `10` | `1` | `7` | `6` | `4` | `Tip` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `10` | `1` | `7` | `6` | `4` | `Tip` |'
- en: '| `11` | `1` | `6` | `7` | `2` | `Tip` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `11` | `1` | `6` | `7` | `2` | `Tip` |'
- en: '| `12` | `1` | `5` | `6` | `4` | `Tip` |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `12` | `1` | `5` | `6` | `4` | `Tip` |'
- en: '| `13` | `1` | `7` | `3` | `3` | `Tip` |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `13` | `1` | `7` | `3` | `3` | `Tip` |'
- en: '| `14` | `1` | `5` | `1` | `4` | `Tip` |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `14` | `1` | `5` | `1` | `4` | `Tip` |'
- en: '| `15` | `1` | `7` | `5` | `5` | `Tip` |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `15` | `1` | `7` | `5` | `5` | `Tip` |'
- en: '| `16` | `0` | `3` | `1` | `3` | `No-tip` |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `16` | `0` | `3` | `1` | `3` | `No-tip` |'
- en: '| `17` | `0` | `4` | `6` | `2` | `No-tip` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `17` | `0` | `4` | `6` | `2` | `No-tip` |'
- en: '| `18` | `0` | `2` | `5` | `2` | `No-tip` |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `18` | `0` | `2` | `5` | `2` | `No-tip` |'
- en: '| `19` | `0` | `5` | `2` | `4` | `No-tip` |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `19` | `0` | `5` | `2` | `4` | `No-tip` |'
- en: '| `20` | `0` | `4` | `1` | `3` | `No-tip` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `20` | `0` | `4` | `1` | `3` | `No-tip` |'
- en: '| `21` | `0` | `3` | `3` | `4` | `No-tip` |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `21` | `0` | `3` | `3` | `4` | `No-tip` |'
- en: '| `22` | `0` | `3` | `4` | `5` | `No-tip` |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `22` | `0` | `3` | `4` | `5` | `No-tip` |'
- en: '| `23` | `0` | `3` | `6` | `3` | `No-tip` |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `23` | `0` | `3` | `6` | `3` | `No-tip` |'
- en: '| `24` | `0` | `4` | `4` | `2` | `No-tip` |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| `24` | `0` | `4` | `4` | `2` | `No-tip` |'
- en: '| `25` | `0` | `6` | `3` | `6` | `No-tip` |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `25` | `0` | `6` | `3` | `6` | `No-tip` |'
- en: '| `26` | `0` | `3` | `6` | `3` | `No-tip` |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `26` | `0` | `3` | `6` | `3` | `No-tip` |'
- en: '| `27` | `0` | `4` | `3` | `2` | `No-tip` |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `27` | `0` | `4` | `3` | `2` | `No-tip` |'
- en: '| `28` | `0` | `3` | `5` | `2` | `No-tip` |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `28` | `0` | `3` | `5` | `2` | `No-tip` |'
- en: '| `29` | `0` | `5` | `5` | `3` | `No-tip` |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `29` | `0` | `5` | `5` | `3` | `No-tip` |'
- en: '| `30` | `0` | `1` | `3` | `2` | `No-tip` |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| `30` | `0` | `1` | `3` | `2` | `No-tip` |'
- en: 'This is a classification problem with three inputs and one categorical output.
    We will address the problem with the following code:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分类问题，包含三个输入和一个分类输出。我们将通过以下代码来解决这个问题：
- en: '[PRE9]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let us go through the code line-by-line
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们逐行分析代码。
- en: To understand all the steps in the code just proposed, we will look at them
    in detail. First, the code snippet will be shown, and the explanation will follow.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解刚才提出的代码中的所有步骤，我们将详细分析它们。首先会展示代码片段，然后进行解释。
- en: '[PRE10]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This includes the libraries `NeuralNetTools` and `nnet()` for our program.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括我们的程序所需的库`NeuralNetTools`和`nnet()`。
- en: '[PRE11]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This sets the working directory and reads the input CSV file.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这将设置工作目录并读取输入的CSV文件。
- en: '[PRE12]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This calls the `nnet()` function with the arguments passed. The output is as
    follows. `nnet()` processes the forward and backpropagation until convergence:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这调用了`nnet()`函数，并传入了参数。输出如下。`nnet()`会处理前向传播和反向传播直到收敛：
- en: '[PRE13]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A brief description of the `nnet` package, extracted from the official documentation,
    is shown in the following table:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`nnet`包的简要描述，摘自官方文档，如下表所示：'
- en: '| **nnet-package**: Feed-forward neural networks and multinomial log-linear
    models |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| **nnet-package**：前馈神经网络和多项式对数线性模型'
- en: '| **Description**: |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| **描述**：'
- en: '| Software for feed-forward neural networks with a single hidden layer, and
    for multinomial log-linear models. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 用于前馈神经网络（单隐层）和多项式对数线性模型的软件。'
- en: '| **Details**: |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| **详情**：'
- en: '| Package: `nnet` Type: Package'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '| 包名：`nnet` 类型：包'
- en: 'Version: 7.3-12'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 版本：7.3-12
- en: 'Date: 2016-02-02'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2016-02-02
- en: 'License: GPL-2 &#124; GPL-3 |'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证：GPL-2 &#124; GPL-3
- en: '| **Author(s)**: |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| **作者**：'
- en: '| *Brian Ripley* *William Venables* |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| *Brian Ripley* *William Venables* |'
- en: '| **Usage**: |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| **使用方法**: |'
- en: '| `nnet(formula, data, weights,subset, na.action, contrasts = NULL)` |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| `nnet(formula, data, weights, subset, na.action, contrasts = NULL)` |'
- en: '| **Meaning of the arguments**: |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| **参数含义**: |'
- en: '| `Formula`: A formula of the form class *~ x1 + x2 + ...* `data`: Dataframe
    from which variables specified in formula are preferentially to be taken'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '| `Formula`: 形如 class *~ x1 + x2 + ...* 的公式 `data`: 从数据框中提取优先使用公式中指定的变量'
- en: '`weights`: (Case) weights for each example; if missing, defaults to *1*'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`weights`: （案例）每个示例的权重；如果缺失，则默认为 *1*'
- en: '`subset`: An index vector specifying the cases to be used in the training sample'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`subset`: 一个索引向量，指定训练样本中要使用的案例'
- en: '`na.action`: A function to specify the action to be taken if NAs are found'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`na.action`: 一个函数，用于指定在发现缺失值（NA）时要采取的措施'
- en: '`contrasts:` A list of contrasts to be used for some or all of the factors
    appearing as variables in the model formula |'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`contrasts:` 用于某些或所有出现在模型公式中的变量的对比列表 |'
- en: 'After giving a brief glimpse into the package documentation, let''s review
    the remaining lines of the proposed in the following code sample:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要介绍包文档后，接下来让我们查看以下代码示例中剩余的行：
- en: '[PRE14]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This command prints the details of the `net()` as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令打印出`net()`的详细信息，如下所示：
- en: '[PRE15]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To plot the `model`, use the following command:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制`model`，使用以下命令：
- en: '[PRE16]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The plot of the `model` is as follows; there are five nodes in the single hidden
    layer:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`的绘图如下；单隐藏层中有五个节点：'
- en: '![](img/00033.jpeg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
- en: 'Using `NeuralNetTools`, it''s possible to obtain the relative importance of
    input variables in neural networks using `garson` algorithm:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`NeuralNetTools`，可以通过`garson`算法获取神经网络中输入变量的相对重要性：
- en: '[PRE17]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This command prints the various input parameters and their importance to the
    output prediction, as shown in the following figure:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令打印出各种输入参数及其对输出预测的重要性，如下图所示：
- en: '**![](img/00034.jpeg)**'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/00034.jpeg)**'
- en: From the chart obtained from the application of the Garson algorithm, it is
    possible to note that, in the decision to give the tip, the service received by
    the customers has the greater influence.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用Garson算法获得的图表中，可以注意到，在决定给小费时，顾客所获得的服务对这一决定的影响最大。
- en: We have seen two neural network libraries in R and used them in simple examples.
    We would deep dive with several practical use cases throughout this book.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到过R中的两个神经网络库，并在简单示例中使用了它们。接下来，我们将深入探讨本书中的几个实际用例。
- en: Deep learning
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'DL forms an advanced neural network with numerous hidden layers. DL is a vast
    subject and is an important concept for building AI. It is used in various applications,
    such as:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）形成了一个具有众多隐藏层的高级神经网络。深度学习是一个庞大的学科，是构建人工智能的重要概念。它被广泛应用于各种领域，例如：
- en: Image recognition
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别
- en: Computer vision
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Handwriting detection
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写识别
- en: Text classification
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Multiclass classification
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类
- en: Regression problems, and more
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归问题等
- en: We would see more about DL with R in the future chapters.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的章节中详细了解使用R进行深度学习。
- en: Pros and cons of neural networks
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的优缺点
- en: Neural networks form the basis of DL, and applications are enormous for DL,
    ranging from voice recognition to cancer detection. The pros and cons of neural
    networks are described in this section. The pros outweigh the cons and give neural
    networks as the preferred modeling technique for data science, machine learning,
    and predictions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是深度学习的基础，深度学习的应用非常广泛，从语音识别到癌症检测。神经网络的优缺点在本节中进行了描述。优点超过缺点，因此神经网络被视为数据科学、机器学习和预测建模的首选技术。
- en: Pros
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势
- en: 'The following are some of the advantages of neural networks:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是神经网络的一些优势：
- en: Neural networks are flexible and can be used for both regression and classification
    problems. Any data which can be made numeric can be used in the model, as neural
    network is a mathematical model with approximation functions.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络具有灵活性，可以用于回归和分类问题。任何可以转化为数值的数据都可以用于模型，因为神经网络是一个具有近似函数的数学模型。
- en: Neural networks are good to model with nonlinear data with large number of inputs;
    for example, images. It is reliable in an approach of tasks involving many features.
    It works by splitting the problem of classification into a layered network of
    simpler elements.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络适合建模具有大量输入的非线性数据，例如图像。它在处理涉及大量特征的任务时非常可靠。它通过将分类问题拆分成简单元素的层次网络来工作。
- en: Once trained, the predictions are pretty fast.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦训练完成，预测速度相当快。
- en: Neural networks can be trained with any number of inputs and layers.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以用任意数量的输入和层进行训练。
- en: Neural networks work best with more data points.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在有更多数据点时效果最佳。
- en: Cons
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺点
- en: 'Let us take a look at some of the cons of neural networks:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看神经网络的一些缺点：
- en: Neural networks are black boxes, meaning we cannot know how much each independent
    variable is influencing the dependent variables.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是黑箱模型，意味着我们无法知道每个独立变量对因变量的影响程度。
- en: It is computationally very expensive and time consuming to train with traditional
    CPUs.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统的CPU进行训练在计算上非常昂贵且耗时。
- en: Neural networks depend a lot on training data. This leads to the problem of
    over-fitting and generalization. The mode relies more on the training data and
    may be tuned to the data.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在很大程度上依赖于训练数据。这会导致过拟合和泛化问题。模型过于依赖训练数据，可能会被调整得过于符合数据。
- en: Best practices in neural network implementations
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络实现中的最佳实践
- en: 'The following are some best practices that will help in the implementation
    of neural network:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最佳实践，有助于神经网络的实现：
- en: Neural networks are best implemented when there is good training data
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在有良好训练数据的情况下实现最佳。
- en: More the hidden layers in an MLP, the better the accuracy of the model for predictions
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多层感知机（MLP）中，隐藏层越多，模型的预测准确度越高。
- en: It is best to have five nodes in the hidden layer
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层最好有五个节点。
- en: ReLU and **Sum of Square of Errors** (**SSE**) are respectively best techniques
    for activation function and error deduction
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU 和 **误差平方和** (**SSE**) 分别是最佳的激活函数和误差调整技术。
- en: Quick note on GPU processing
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速说明GPU处理
- en: The increase in processing capabilities has been a tremendous booster for usage
    of neural networks in day-to-day problems. GPU is a specialized processor designed
    to perform graphical operations (for example, gaming, 3D animation, and so on).
    They perform mathematically intensive tasks and are additional to the CPU. The
    CPU performs the operational tasks of the computer, while the GPU is used to perform
    heavy workload processing.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 处理能力的提升为神经网络在日常问题中的应用提供了强大支持。GPU是专门设计来执行图形操作的处理器（例如，游戏、3D动画等）。它们执行数学密集型任务，且与CPU配合使用。CPU负责计算机的操作任务，而GPU则用于处理繁重的工作负载。
- en: The neural network architecture needs heavy mathematical computational capabilities
    and GPU is the preferred candidate here. The vectorized dot matrix product between
    the weights and inputs at every neuron can be run in parallel through GPUs. The
    advancements in GPUs is popularizing neural networks. The applications of DL in
    image processing, computer vision, bioinformatics, and weather modeling are benefiting
    through GPUs.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构需要强大的数学计算能力，GPU是首选的硬件。在每个神经元的权重和输入之间的向量化点积可以通过GPU并行计算。GPU的进步正在推动神经网络的普及。深度学习在图像处理、计算机视觉、生物信息学和天气建模等领域的应用都受益于GPU。
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw an overview of ANNs. Neural networks implementation
    is simple, but the internals are pretty complex. We can summarize neural network
    as a universal mathematical function approximation. Any set of inputs which produce
    outputs can be made a black box mathematical function through a neural network,
    and the applications are enormous in the recent years.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了人工神经网络（ANN）的概述。神经网络的实现相对简单，但其内部机制非常复杂。我们可以将神经网络总结为一种通用的数学函数逼近方法。任何一组输入输出的映射都可以通过神经网络转化为一个黑箱数学函数，近年来，神经网络的应用也变得非常广泛。
- en: 'We saw the following in this chapter:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们讨论了以下内容：
- en: Neural network is a machine learning technique and is data-driven
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是一种机器学习技术，并且是数据驱动的。
- en: AI, machine learning, and neural networks are different paradigms of making
    machines work like humans
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能、机器学习和神经网络是让机器像人类一样工作的不同范式。
- en: Neural networks can be used for both supervised and unsupervised machine learning
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以用于监督学习和无监督学习。
- en: Weights, biases, and activation functions are important concepts in neural networks
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重、偏置和激活函数是神经网络中的重要概念。
- en: Neural networks are nonlinear and non-parametric
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是非线性和非参数化的。
- en: Neural networks are very fast in prediction and are most accurate in comparison
    with other machine learning models
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在预测时非常快速，并且相较于其他机器学习模型最为准确。
- en: There are input, hidden, and output layers in any neural network architecture
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何神经网络架构中都包含输入层、隐藏层和输出层。
- en: 'Neural networks are based on building MLP, and we understood the basis for
    neural networks: weights, bias, activation functions, feed-forward, and backpropagation
    processing'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络基于构建多层感知机（MLP），我们理解了神经网络的基础：权重、偏差、激活函数、前向传播和反向传播处理
- en: Forward and backpropagation are techniques to derive a neural network model
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播和反向传播是推导神经网络模型的技术
- en: Neural networks can be implemented through many programming languages, namely
    Python, R, MATLAB, C, and Java, among others. The focus of this book will be building
    applications using R. DNN and AI systems are evolving on the basis of neural networks.
    In the forthcoming chapter, we will drill through different types of neural networks
    and their various applications.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以通过多种编程语言实现，例如 Python、R、MATLAB、C 和 Java 等。本书的重点将是使用 R 构建应用程序。深度神经网络（DNN）和人工智能（AI）系统正基于神经网络不断发展。在接下来的章节中，我们将深入探讨不同类型的神经网络及其各种应用。
