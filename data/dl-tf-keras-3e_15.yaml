- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Tensor Processing Unit
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量处理单元
- en: This chapter introduces the **Tensor Processing Unit** (**TPU**), a special
    chip developed at Google for ultra-fast execution of neural network mathematical
    operations. As with **Graphics Processing Units** (**GPUs**), the idea here is
    to have a special processor focusing only on very fast matrix operations, with
    no support for all the other operations normally supported by **Central Processing
    Units** (**CPUs**). However, the additional improvement with TPUs is to remove
    from the chip any hardware support for graphics operations normally present in
    GPUs (rasterization, texture mapping, frame buffer operations, and so on). Think
    of a TPU as a special purpose co-processor specialized for deep learning, being
    focused on matrix or tensor operations. In this chapter, we will compare CPUs
    and GPUs with the four generations of TPUs and with Edge TPUs. All these accelerators
    are available as of April 2022\. The chapter will include code examples of using
    TPUs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**张量处理单元**（**TPU**），这是谷歌开发的一种专用芯片，旨在超快速执行神经网络的数学运算。与**图形处理单元**（**GPU**）一样，这里的理念是拥有一个专门的处理器，只关注极其快速的矩阵运算，而不支持**中央处理单元**（**CPU**）通常支持的其他运算。然而，TPU的额外改进在于，从芯片中去除了通常存在于GPU中的图形操作硬件支持（如光栅化、纹理映射、帧缓冲操作等）。可以把TPU看作是一个专门针对深度学习的协处理器，专注于矩阵或张量运算。本章将对比CPU和GPU与四代TPU及边缘TPU的性能。这些加速器在2022年4月已经推出。本章将包括使用TPU的代码示例。
- en: 'In this chapter, you will learn the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将让你学习以下内容：
- en: C/G/T processing units
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C/G/T处理单元
- en: Four generations of TPUs and Edge TPUs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四代TPU和边缘TPU
- en: TPU performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPU性能
- en: How to use TPUs with Colab
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在Colab中使用TPU
- en: So with that, let’s begin.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧。
- en: C/G/T processing units
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: C/G/T处理单元
- en: In this section we discuss CPUs, GPUs, and TPUs. Before discussing TPUs, it
    will be useful for us to review CPUs and GPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论CPU、GPU和TPU。在讨论TPU之前，回顾一下CPU和GPU是很有帮助的。
- en: CPUs and GPUs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU和GPU
- en: 'You are probably somewhat familiar with the concept of a CPU, a general-purpose
    chip sitting in each computer, tablet, and smartphone. CPUs are in charge of all
    of the computations: from logical controls, to arithmetic, to register operations,
    to operations with memory, and many others. CPUs are subject to the well-known
    Moore’s law [1], which states that the number of transistors in a dense integrated
    circuit doubles about every two years.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对CPU的概念略有了解，它是每台计算机、平板电脑和智能手机中的通用芯片。CPU负责所有计算：从逻辑控制到算术、从寄存器操作到内存操作等。CPU遵循著名的摩尔定律[1]，即密集型集成电路中的晶体管数量大约每两年就会翻一番。
- en: Many people believe that we are currently in an era where this trend cannot
    be sustained for long, and indeed it has already declined during the past decade.
    Therefore, we need some additional technology if we want to support the demand
    for faster and faster computation to process the ever-growing amount of data that
    is available out there.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人认为我们现在正处于一个无法长期维持这一趋势的时代，事实上，它在过去十年中已经出现了衰退。因此，如果我们要支持日益增长的数据处理需求，我们需要一些额外的技术来支持越来越快的计算。
- en: One improvement came from GPUs, special-purpose chips that are perfect for fast
    graphics operations such as matrix multiplication, rasterization, frame-buffer
    manipulation, texture mapping, and many others. In addition to computer graphics
    where matrix multiplications are applied to pixels of images, GPUs turned out
    to be a great match for deep learning. This is a funny story of serendipity (serendipity
    is the occurrence and development of events by chance in a happy or beneficial
    way) – a great example of technology created for one goal and then being met with
    staggering success in a domain completely unrelated to the one it was originally
    envisioned for.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个改进来源于GPU，GPU是专用芯片，完美适合于快速的图形操作，如矩阵乘法、光栅化、帧缓冲操作、纹理映射等。除了计算机图形学中矩阵乘法应用于图像像素外，GPU也非常适合深度学习。这是一个偶然的幸运故事（幸运是指事件通过偶然的方式以愉快或有益的方式发生和发展）——这是一个技术为了一个目标创造出来，却在与最初设想目标无关的领域取得巨大成功的典型例子。
- en: TPUs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU
- en: One problem encountered in using GPUs for deep learning is that these chips
    are made for graphics and gaming, not only for fast matrix computations. This
    would of course be the case, given that the G in GPU stands for Graphics! GPUs
    led to unbelievable improvements in deep learning but, in the case of tensor operations
    for neural networks, large parts of the chip are not used at all. For deep learning,
    there is no need for rasterization, no need for frame-buffer manipulation, and
    no need for texture mapping. The only thing that is necessary is a very efficient
    way to compute matrix and tensor operations. It should be no surprise that GPUs
    are not necessarily the ideal solution for deep learning, since CPUs and GPUs
    were designed long before deep learning became successful.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU进行深度学习时遇到的一个问题是，这些芯片是为图形和游戏设计的，而不仅仅是为了快速的矩阵计算。考虑到GPU中的G代表图形，这种情况显然是可以理解的！GPU极大地推动了深度学习的发展，但在神经网络的张量运算中，芯片的大部分部分根本没有被使用。对于深度学习来说，实际上不需要光栅化、不需要帧缓冲区操作，也不需要纹理映射。唯一需要的，是一种非常高效的矩阵和张量运算方式。因此，GPU不一定是深度学习的理想解决方案也就不足为奇了，因为CPU和GPU的设计早于深度学习的成功。
- en: 'Before going into technical details, let’s first discuss the fascinating genesis
    of Tensor Processing Unit version 1, or TPU v1\. In 2013, Jeff Dean, the Chief
    of the Brain Division at Google, estimated (see *Figure 15.1*) that if all the
    people owning a mobile phone were talking in calls for only 3 minutes more per
    day, then Google would have needed two or three times more servers to process
    this data. This would have been an unaffordable case of success-disaster, i.e.,
    where great success has led to problems that cannot be properly managed. It was
    clear that neither CPUs nor GPUs were a suitable solution. So, Google decided
    that they needed something completely new – something that would allow a 10x growth
    in performance with no significant cost increase. That’s how TPU v1 was born!
    What is impressive is that it took only 15 months from initial design to production.
    You can find more details about this story in Jouppi et al., 2014 [3], where a
    detailed report about different inference workloads seen at Google in 2013 is
    also reported:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入技术细节之前，我们先来讨论一下张量处理单元（Tensor Processing Unit，简称TPU）版本1的迷人起源。2013年，Google大脑部门的负责人Jeff
    Dean估算（见*图15.1*），如果所有拥有手机的人每天多通话3分钟，那么Google就需要两到三倍数量的服务器来处理这些数据。这将是一个无法承受的成功灾难案例，也就是说，巨大的成功导致了无法妥善管理的问题。显然，CPU和GPU都不是合适的解决方案。因此，Google决定需要一种全新的技术——一种能够在没有显著成本增加的情况下实现10倍性能增长的技术。这就是TPU
    v1诞生的背景！令人印象深刻的是，从最初设计到生产只花费了15个月的时间。你可以在Jouppi等人2014年的报告中找到更多关于这个故事的细节[3]，该报告还详细描述了2013年Google遇到的不同推理工作负载：
- en: '![Table  Description automatically generated](img/B18331_15_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_15_01.png)'
- en: 'Figure 15.1: Different inference workloads seen at Google in 2013 (source [3])'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：2013年Google看到的不同推理工作负载（来源 [3]）
- en: 'Let’s talk a bit about the technical details. A TPU v1 is a special device
    (or an **Application-Specific Integrated Circuit**, or **ASIC** for short) designed
    for super-efficient tensor operations. TPUs follow the philosophy *less is more.*
    This philosophy has an important consequence: TPUs do not have all the graphic
    components that are needed for GPUs. Because of this, they are both very efficient
    from an energy consumption perspective, and frequently much faster than GPUs.
    So far, there have been four generations of TPUs. Let’s review them.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈一些技术细节。TPU v1是一种特殊的设备（或**特定应用集成电路**，简称**ASIC**），专为超高效的张量运算设计。TPU遵循“*少即是多*”的理念。这个理念有一个重要的后果：TPU没有GPU所需的所有图形组件。因此，从能效角度来看，TPU非常高效，并且在很多情况下比GPU快得多。迄今为止，TPU已经有四代产品。我们来回顾一下它们。
- en: Four generations of TPUs, plus Edge TPU
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 四代TPU，以及Edge TPU
- en: 'As discussed, TPUs are domain-specific processors expressly optimized for matrix
    operations. Now, you might remember that the basic operation of matrix multiplication
    is a dot product between a line from one matrix and a column from the other matrix.
    For instance, given a matrix multiplication ![](img/B18331_15_001.png), computing
    *Y*[*i*, 0] is:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，TPU是专门针对矩阵运算进行优化的领域专用处理器。现在，你可能还记得矩阵乘法的基本操作是一个矩阵的行和另一个矩阵的列之间的点积。例如，给定一个矩阵乘法
    ![](img/B18331_15_001.png)，计算 *Y*[*i*, 0] 的方法是：
- en: '![](img/B18331_15_002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_15_002.png)'
- en: The sequential implementation of this operation is time-consuming for large
    matrices. A brute-force computation has a time complexity of *O*(*n*³) for *n*
    x *n* matrices so it’s not feasible for running large computations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作的顺序实现对于大矩阵来说是耗时的。蛮力计算的时间复杂度为 *O*(*n*³)，对于 *n* x *n* 矩阵来说，因此它不适用于进行大规模计算。
- en: First generation TPU
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一代TPU
- en: The first generation TPU (TPU v1) was announced in May 2016 at Google I/O. TPU
    v1 [1] supports matrix multiplication using 8-bit arithmetic. TPU v1 is specialized
    for deep learning inference but it does not work for training. For training there
    is a need to perform floating-point operations, as discussed in the following
    paragraphs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代TPU（TPU v1）于2016年5月在Google I/O大会上发布。TPU v1 [1] 支持使用8位算术进行矩阵乘法。TPU v1 专门用于深度学习推理，但不适用于训练。训练需要执行浮点运算，正如下面段落中所讨论的那样。
- en: 'A key function of TPU is the “systolic” matrix multiplication. Let’s see what
    this means. Remember that the core of deep learning is a core product ![](img/B18331_15_001.png),
    where, for instance, the basic operation to compute *Y*[*i*, 0] is:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TPU的一个关键功能是“脉动”矩阵乘法。让我们来看看这是什么意思。请记住，深度学习的核心是一个核心产品 ![](img/B18331_15_001.png)，例如，计算
    *Y*[*i*, 0] 的基本操作是：
- en: '![](img/B18331_15_004.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_15_004.png)'
- en: “Systolic” matrix multiplication allows multiple *Y*[*i*, *j*] values to be
    computed in parallel. Data flows in a coordinated manner and, indeed, in medicine
    the term “systolic” refers to heart contractions and how blood flows rhythmically
    in our veins. Here systolic refers to the data flow that pulses inside the TPU.
    It can be proven that a systolic multiplication algorithm is less expensive than
    the brute-force one [2]. A TPU v1 has a **Matrix Multiply Unit** (**MMU**) running
    systolic multiplications on 256 x 256 cores so that 65,536 multiplications can
    be computed in parallel in one single shot. In addition, a TPU v1 sits in a rack
    and it is not directly accessible. Instead, a CPU acts as the host controlling
    data transfer and sending commands to the TPU for performing tensor multiplications,
    for computing convolutions, and for applying activation functions. The CPU ![](img/B18331_15_005.png)
    TPU v1 communication happens via a standard PCIe 3.0 bus. From this perspective,
    a TPU v1 is closer in spirit to a **Floating-Point Unit** (**FPU**) coprocessor
    than it is to a GPU. However, a TPU v1 has the ability to run whole inference
    models to reduce dependence on the host CPU. *Figure 15.2* represents TPU v1,
    as shown in [3]. As you see in the figure, the processing unit is connected via
    a PCI port, and it fetches weights via a standard DDR4 DRAM chip. Multiplication
    happens within the MMU with systolic processing. Activation functions are then
    applied to the results. The MMU and the unified buffer for activations take up
    a lot of space. There is an area where the activation functions are computed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: “脉动”矩阵乘法允许多个 *Y*[*i*, *j*] 值并行计算。数据以协调的方式流动，实际上，在医学中，“脉动”一词指的是心脏收缩以及血液如何在我们的静脉中有节奏地流动。在这里，脉动指的是数据在TPU内部的脉冲流动。可以证明，脉动乘法算法比蛮力算法更节省成本
    [2]。TPU v1 拥有 **矩阵乘法单元**（**MMU**），在 256 x 256 核心上运行脉动乘法，这样就可以在一次操作中并行计算 65,536
    次乘法。此外，TPU v1 放置在机架中，不能直接访问。相反，CPU 作为主机，控制数据传输并向 TPU 发送指令，以执行张量乘法、计算卷积和应用激活函数。CPU
    ![](img/B18331_15_005.png) TPU v1 之间的通信通过标准 PCIe 3.0 总线进行。从这个角度来看，TPU v1 在本质上更接近
    **浮点运算单元**（**FPU**）协处理器，而不是 GPU。然而，TPU v1 具备运行完整推理模型的能力，从而减少对主机 CPU 的依赖。*图 15.2*
    展示了 TPU v1，如 [3] 中所示。如图所示，处理单元通过 PCI 端口连接，并通过标准 DDR4 DRAM 芯片获取权重。乘法操作在 MMU 内进行脉动处理。然后将激活函数应用于结果。MMU
    和用于激活的统一缓冲区占据了大量空间。还有一个区域专门计算激活函数。
- en: '![](img/B18331_15_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_15_02.png)'
- en: 'Figure 15.2: TPU v1 design schema (source [3])'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2：TPU v1 设计架构（来源 [3]）
- en: 'TPU v1s are manufactured on a 28 nm process node with a die size of ≤ 331 mm2,
    a clock speed of 700 MHz, 28 MiB of on-chip memory, 4 MiB of 32-bit accumulators,
    and a 256 x 256 systolic array of 8-bit multipliers. For this reason, we can get
    700 MHz*65,536 (multipliers) ![](img/B18331_09_004.png) 92 Tera operations/sec.
    This is an amazing performance for matrix multiplications; *Figure 15.3* shows
    the TPU circuit board and flow of data for the systolic matrix multiplication
    performed by the MMU. In addition, TPU v1 has an 8 GiB of dual-channel 2133 MHz
    DDR3 SDRAM offering 34 GB/s of bandwidth. The external memory is standard, and
    it is used to store and fetch the weights used during the inference. Notice also
    that TPU v1 has a thermal design power of 28–40 watts, which is certainly low
    consumption compared to GPUs and CPUs. Moreover, TPU v1s are normally mounted
    in a PCI slot used for SATA disks so they do not require any modification in the
    host server [3]. Up to four cards can be mounted on each server. *Figure 15.3*
    shows a TPU v1 card and the process of systolic computation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v1采用28纳米工艺节点制造，芯片面积为≤331 mm²，时钟频率为700 MHz，配备28 MiB的片上内存、4 MiB的32位累加器，并且拥有256
    x 256的8位乘法器心脏阵列。因此，我们可以得到700 MHz*65,536（乘法器）![](img/B18331_09_004.png) 92万亿次操作/秒。对于矩阵乘法来说，这是一个惊人的性能；*图15.3*展示了TPU电路板及MMU执行心脏阵列矩阵乘法时的数据流。此外，TPU
    v1还配备8 GiB的双通道2133 MHz DDR3 SDRAM，带宽为34 GB/s。外部内存是标准配置，主要用于存储和提取推理过程中使用的权重。另请注意，TPU
    v1的热设计功耗为28–40瓦，这与GPU和CPU相比，消耗非常低。此外，TPU v1通常安装在用于SATA硬盘的PCI插槽中，因此不需要对主机服务器进行任何修改[3]。每台服务器最多可以安装四张卡。*图15.3*展示了TPU
    v1卡以及心脏阵列计算过程：
- en: '![Diagram, schematic  Description automatically generated](img/B18331_15_03.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图示，示意图  描述自动生成](img/B18331_15_03.png)'
- en: 'Figure 15.3: On the left you can see a TPU v1 board, and on the right an example
    of how the data is processed during the systolic computation'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：左侧为TPU v1电路板，右侧为心脏阵列计算过程中数据处理的示例
- en: If you want to have a look at TPU performance compared to GPUs and CPUs, you
    can refer to [3] and see (in a log-log scale graph) that the performance is two
    orders of magnitude higher than a Tesla K80 GPU. The graph shows a “rooftop” performance,
    which is growing until the point where it reaches the peak and then it is constant.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看TPU与GPU和CPU的性能比较，可以参考[3]，并通过对数-对数坐标图查看，TPU的性能比Tesla K80 GPU高出两个数量级。该图展示了一个“屋顶”性能，性能不断增长，直到达到峰值后保持不变。
- en: 'The higher the roof, the better performance is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 屋顶越高，性能越好：
- en: '![Chart, line chart  Description automatically generated](img/B18331_15_04.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  描述自动生成](img/B18331_15_04.png)'
- en: 'Figure 15.4: TPU v1 peak performance can be up to 3x higher than a Tesla K80'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：TPU v1的峰值性能可达到Tesla K80的3倍
- en: Second generation TPU
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二代TPU
- en: The second generation TPUs (TPU2s) were announced in 2017\. In this case, the
    memory bandwidth is increased to 600 GB/s and performance reaches 45 TFLOPS. Four
    TPU2s are arranged in a module with 180 TFLOPS of performance. Then 64 modules
    are grouped into a pod with 11.5 PFLOPS of performance. TPU2s adopt floating-point
    arithmetic and therefore they are suitable for both training and inference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第二代TPU（TPU2）于2017年发布。在这种情况下，内存带宽提高到600 GB/s，性能达到45 TFLOPS。四个TPU2被排列在一个模块中，性能为180
    TFLOPS。然后，将64个模块组合成一个POD，性能达到11.5 PFLOPS。TPU2采用浮点运算，因此既适用于训练也适用于推理。
- en: A TPU2 has an MNU for matrix multiplications of 128*128 cores and a **Vector
    Processing Unit** (**VPU**) for all other tasks such as applying activations etc.
    The VPU handles float32 and int32 computations. The MXU on the other hand operates
    in a mixed precision 16–32 bit floating-point format.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TPU2拥有一个128*128核心的MNU用于矩阵乘法，并且配备**向量处理单元**（**VPU**）处理其他任务，如应用激活函数等。VPU处理float32和int32运算。另一方面，MXU在16-32位混合精度浮点格式下运行。
- en: Each TPU v2 chip has two cores, and up to four chips are mounted on each board.
    In TPU v2, Google adopted a new floating-point model called bfloat16 The idea
    is to sacrifice some resolution but still be very good for deep learning. This
    reduction in resolution allows us to improve the performance of the TPU2s, which
    are more power-efficient than the v1s. Indeed, It can be proven that a smaller
    mantissa helps reduce the physical silicon area and multiplier power. Therefore,
    the bfloat16 uses the same standard IEEE 754 single-precision floating-point format,
    but it truncates the mantissa field from 23 bits to just 7 bits.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个TPU v2芯片有两个核心，每块板上最多可以安装四个芯片。在TPU v2中，Google采用了一种新的浮点模型，叫做bfloat16，目的是牺牲一些精度，但仍然非常适合深度学习。这种精度的降低使得我们能够提升TPU2的性能，而TPU2比TPU
    v1更加节能。事实上，可以证明，较小的尾数有助于减少物理硅面积和乘法器功耗。因此，bfloat16采用与IEEE 754单精度浮点格式相同的标准，但将尾数字段从23位截断为7位。
- en: 'Preserving the exponent bits allows the format to keep the same range as the
    32-bit single precision. This allows for relatively simpler conversion between
    the two data types:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 保留指数位使得该格式能够保持与32位单精度浮点数相同的范围。这使得两种数据类型之间的转换相对简单：
- en: '![](img/B18331_15_05.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_15_05.png)'
- en: 'Figure 15.5: Cloud TPU v2 and Cloud TPU v3'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：Cloud TPU v2 和 Cloud TPU v3
- en: Google offers access to these TPU v2 and TPU v3 via **Google Compute Engine**
    (**GCE**), and via **Google Kubernetes Engine** (**GKE**). Plus, it is possible
    to use them for free via Colab.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Google通过**Google Compute Engine** (**GCE**) 和 **Google Kubernetes Engine** (**GKE**)
    提供这些TPU v2和TPU v3的访问权限。此外，还可以通过Colab免费使用它们。
- en: Third generation TPU
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三代TPU
- en: 'The third generation TPUs (TPU3) were announced in 2018 [4]. TPU3s are 2x faster
    than TPU2 and they are grouped in 4x larger pods. In total, this is an 8x performance
    increase. Cloud TPU v3 Pods can deliver more than 100 petaflops of computing power.
    On the other hand, Cloud TPU v2 Pods released in alpha in 2018 can achieve 11.5
    petaflops – another impressive improvement. As of 2019, both TPU2 and TPU3 are
    in production at different prices:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第三代TPU（TPU3）于2018年发布[4]。TPU3的速度是TPU2的两倍，并且它们被组合成4倍更大的集群。总体来说，这意味着性能提升了8倍。Cloud
    TPU v3 Pods可以提供超过100 petaflops的计算能力。另一方面，2018年以alpha版本发布的Cloud TPU v2 Pods可以达到11.5
    petaflops——这是另一个令人印象深刻的进步。到2019年，TPU2和TPU3已投入生产，并有不同的定价：
- en: '![](img/B18331_15_06.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_15_06.png)'
- en: 'Figure 15.6: Google announced TPU v2 and v3 Pods in beta at the Google I/O
    2019'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6：Google在2019年Google I/O上宣布了TPU v2和v3 Pods的beta版
- en: A TPU v3 board has four TPU chips, eight cores, and liquid cooling. Google has
    adopted ultra-high-speed interconnect hardware derived from supercomputer technology
    for connecting thousands of TPUs with very low latency.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一块TPU v3板有四个TPU芯片、八个核心，并且使用液冷系统。Google采用了源自超级计算机技术的超高速互联硬件，将成千上万个TPU连接起来，具有非常低的延迟。
- en: Each time a parameter is updated on a single TPU, all the others are informed
    via a reduce-all algorithm typically adopted for parallel computation. So, you
    can think about a TPU v3 as one of the fastest supercomputers available today
    for matrix and tensor operations, with thousands of TPUs inside it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在单个TPU上更新参数时，所有其他TPU都会通过一种通常用于并行计算的reduce-all算法得到通知。所以，你可以把TPU v3看作是当今最快的超计算机之一，专门用于矩阵和张量运算，内部包含成千上万个TPU。
- en: Fourth generation TPUs
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四代TPU
- en: 'Google’s fourth generation TPU ASIC has more than double the matrix multiplication
    TFLOPs of TPU v3, a considerable boost in memory bandwidth, and more advances
    in interconnect technology. Each TPU v4 chip provides more than 2x the compute
    power of a TPU v3 chip – up to 275 peak TFLOPS. Each TPU v4 Pod delivers 1.1 exaflops/s
    of peak performance. Google claims that TPU v4 Pods are used extensively to develop
    research breakthroughs such as MUM and LaMDA, and improve core products such as
    Search, Assistant, and Translate (see [https://blog.google/technology/developers/io21-helpful-google/](https://blog.google/technology/developers/io21-helpful-google/)).
    As of April 2022, TPU v4s are only available in preview (*Figure 15.7*):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Google的第四代TPU ASIC的矩阵乘法TFLOPs是TPU v3的两倍多，内存带宽有了显著提升，并且在互联技术方面有更多进展。每个TPU v4芯片提供的计算能力是TPU
    v3芯片的2倍以上——最高可达275峰值TFLOPS。每个TPU v4 Pod的峰值性能可达1.1 exaflops/s。Google声称，TPU v4 Pods广泛用于开发如MUM和LaMDA等研究突破，并改进搜索、助手和翻译等核心产品（见[https://blog.google/technology/developers/io21-helpful-google/](https://blog.google/technology/developers/io21-helpful-google/)）。截至2022年4月，TPU
    v4仅在预览版中提供（*图15.7*）：
- en: '![](img/B18331_15_07.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_15_07.png)'
- en: 'Figure 15.7: A TPU v4 chip and a portion of a TPU v4 Pod – source: https://twitter.com/google/status/1394785686683783170'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：TPU v4 芯片及部分 TPU v4 Pod – 来源：[https://twitter.com/google/status/1394785686683783170](https://twitter.com/google/status/1394785686683783170)
- en: In this section, we have introduced four generations of TPUs. Before concluding,
    I wanted to mention that it is possible to save money by using preemptible Cloud
    TPUs for fault-tolerant machine learning workloads. These workloads include but
    are not limited to long training runs with checkpointing or batch prediction on
    large datasets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了四代 TPU。在结束之前，我想提到，使用可抢占的云 TPU 进行容错机器学习工作负载可以节省成本。这些工作负载包括但不限于带检查点的长时间训练任务或大数据集上的批量预测。
- en: Edge TPU
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Edge TPU
- en: 'In addition to the three generations of TPUs already discussed, in 2018 Google
    announced a special generation of TPUs running on the edge. This TPU is particularly
    appropriate for the **Internet of Things** (**IoT**) and for supporting TensorFlow
    Lite on mobile and IoT. An individual Edge TPU can perform 4 trillion (fixed-point)
    operations per second (4 TOPS), using only 2 watts of power. An Edge TPU is designed
    for small, low-power devices and is ideal for on-device ML, with it being fast
    and power-efficient. Edge TPUs support the TensorFlow Lite development framework
    (see *Figure 15.8*). At the end of 2019, Google announced the Pixel 4 smartphone
    containing an Edge TPU called the Pixel Neural Core:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了已经讨论的三代 TPU 外，Google 在 2018 年宣布了一种专门用于边缘计算的 TPU。该 TPU 特别适用于**物联网**（**IoT**）以及支持移动设备和物联网中的
    TensorFlow Lite。单个 Edge TPU 每秒可以执行 4 万亿（定点）操作（4 TOPS），仅消耗 2 瓦功率。Edge TPU 专为小型、低功耗设备设计，非常适合在设备端进行机器学习，且既快速又节能。Edge
    TPU 支持 TensorFlow Lite 开发框架（参见*图 15.8*）。2019 年底，Google 发布了 Pixel 4 智能手机，内置了一款名为
    Pixel Neural Core 的 Edge TPU：
- en: '![A picture containing logo  Description automatically generated](img/B18331_15_08.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing logo  Description automatically generated](img/B18331_15_08.png)'
- en: 'Figure 15.8: Two Edge TPUs on one penny – source: https://coral.ai/docs/edgetpu/faq/#what-is-the-edge-tpu'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8：一分硬币上的两个 Edge TPU – 来源：[https://coral.ai/docs/edgetpu/faq/#what-is-the-edge-tpu](https://coral.ai/docs/edgetpu/faq/#what-is-the-edge-tpu)
- en: With this we conclude the introduction to TPU v1, v2, v3, v4, and Edge TPU.
    In the next section we will briefly discuss performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，我们结束了 TPU v1、v2、v3、v4 及 Edge TPU 的介绍。在接下来的章节中，我们将简要讨论性能。
- en: TPU performance
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TPU 性能
- en: 'Discussing performance is always difficult because it is important to first
    define the metrics that we are going to measure, and the set of workloads that
    we are going to use as benchmarks. For instance, Google reported an impressive
    linear scaling for TPU v2 used with ResNet-50 [4] (see *Figure 15.9* and *Figure
    15.10*):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论性能总是很困难，因为首先需要定义我们要衡量的指标，以及我们将用作基准的工作负载集。例如，Google 在使用 ResNet-50 [4] 时报告了
    TPU v2 的令人印象深刻的线性扩展性（参见*图 15.9* 和 *图 15.10*）：
- en: '![Chart, line chart  Description automatically generated](img/B18331_15_09.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, line chart  Description automatically generated](img/B18331_15_09.png)'
- en: 'Figure 15.9: Linear scalability in the number of TPUs v2 when increasing the
    number of images'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9：增加图像数量时，TPU v2 的线性扩展性
- en: 'In addition, you can find online a comparison of ResNet-50 [4] where a full
    Cloud TPU v2 Pod is >200x faster than a V100 NVIDIA Tesla GPU for ResNet-50 training:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以在网上找到 ResNet-50 [4] 的比较，其中完整的 Cloud TPU v2 Pod 比 V100 NVIDIA Tesla GPU
    在 ResNet-50 训练中快 >200 倍：
- en: '![A picture containing timeline  Description automatically generated](img/B18331_15_10.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing timeline  Description automatically generated](img/B18331_15_10.png)'
- en: 'Figure 15.10: A full Cloud TPU v2 Pod is >200x faster than a V100 NVIDIA Tesla
    GPU for training a ResNet-50 model'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：完整的 Cloud TPU v2 Pod 比 V100 NVIDIA Tesla GPU 在训练 ResNet-50 模型时快 >200
    倍
- en: 'According to Google, TPU v4 givse top-line results for MLPerf1.0 [5] when compared
    with Nvidia A100 GPUs (see *Figure 15.11*). Indeed, these accelerators are designed
    by keeping in mind the latest large models encompassing billions and sometimes
    trillions of parameters (think about GPT-3, T5, and the Switch Transformer):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Google 的说法，TPU v4 在与 Nvidia A100 GPU 相比时，给出了 MLPerf1.0 [5] 的顶级结果（参见*图 15.11*）。事实上，这些加速器的设计考虑了最新的大型模型，涉及数十亿甚至数万亿的参数（比如
    GPT-3、T5 和 Switch Transformer）：
- en: '![Chart, bar chart  Description automatically generated](img/B18331_15_11.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, bar chart  Description automatically generated](img/B18331_15_11.png)'
- en: 'Figure 15.11: MLPerf 1.0 TPU v4 Pod performance – source: https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：MLPerf 1.0 TPU v4 Pod 性能 – 来源：[https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4](https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4)
- en: How to use TPUs with Colab
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 Colab 中使用 TPU
- en: 'In this section, we show how to use TPUs with Colab. Just point your browser
    to [https://colab.research.google.com/](https://colab.research.google.com/) and
    change the runtime from the **Runtime** menu as shown in *Figure 15.12*. First,
    you’ll need to enable TPUs for the notebook, then navigate to **Edit**→**Notebook
    settings** and select **TPU** from the **Hardware accelerator** drop-down box:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何在Colab中使用TPU。只需将浏览器指向 [https://colab.research.google.com/](https://colab.research.google.com/)
    并从**Runtime**菜单中更改运行时，如*图15.12*所示。首先，你需要为笔记本启用TPU，然后导航到**编辑**→**笔记本设置**并从**硬件加速器**下拉框中选择**TPU**：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B18331_15_12.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，聊天或文本消息 描述自动生成](img/B18331_15_12.png)'
- en: 'Figure 15.12: Setting TPU as the hardware accelerator'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12：将TPU设置为硬件加速器
- en: Checking whether TPUs are available
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查TPU是否可用
- en: 'First of all, let’s check if there is a TPU available, by using this simple
    code fragment that returns the IP address assigned to the TPU. Communication between
    the CPU and TPU happens via **gRPC** (**gRPC Remote Procedure Call**), which is
    a modern, open-source, high-performance **Remote Procedure Call** (**RPC**) framework
    that can run in any environment:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过使用这个简单的代码片段来检查是否有可用的TPU，它会返回分配给TPU的IP地址。CPU与TPU之间的通信是通过**gRPC**（**gRPC远程过程调用**）进行的，gRPC是一个现代的、开源的高性能**远程过程调用**（**RPC**）框架，可以在任何环境中运行：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see something like the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到如下内容：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We’ve confirmed that a TPU is available!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认TPU是可用的！
- en: Keras MNIST TPU end-to-end training
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras MNIST TPU端到端训练
- en: 'Referring to the notebook available on Google Research Colab (see [https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7)),
    we can check how TPUs or GPUs are detected with this code snippet, which uses
    either TPUs or GPUs as a fallback:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参考Google Research Colab上提供的笔记本（参见 [https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7)），我们可以查看如何使用这个代码片段检测TPU或GPU，它会使用TPU或GPU作为回退：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the strategy `tf.distribute.TPUStrategy(tpu)` is the only change you
    need in code for synchronous training on TPUs and TPU Pods. Then, to run TF2 programs
    on TPUs, you can either use `.compile` or `.fit` APIs in `tf.keras` with `TPUStrategy`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`tf.distribute.TPUStrategy(tpu)`是你在TPU和TPU集群上进行同步训练时，代码中唯一需要更改的部分。然后，要在TPU上运行TF2程序，你可以使用`tf.keras`中的`.compile`或`.fit`API与`TPUStrategy`一起使用。
- en: If you want you can write your own customized training loop by calling `strategy.run`
    directly (see [https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你可以通过直接调用`strategy.run`编写你自己的自定义训练循环（参见 [https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)）。
- en: Using pretrained TPU models
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的TPU模型
- en: 'Google offers a collection of models pretrained with TPUs available in the
    GitHub `tensorflow/tpu` repository ([https://github.com/tensorflow/tpu](https://github.com/tensorflow/tpu)).
    Models include image recognition, object detection, low-resource models, machine
    translation and language models, speech recognition, and image generation. Whenever
    it is possible, my suggestion is to start with a pretrained model [6], and then
    fine-tune it or apply some form of transfer learning. As of April 2022, the following
    models are available:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Google在GitHub的`tensorflow/tpu`仓库中提供了一些预训练的TPU模型（[https://github.com/tensorflow/tpu](https://github.com/tensorflow/tpu)）。这些模型包括图像识别、物体检测、低资源模型、机器翻译和语言模型、语音识别以及图像生成。我的建议是，在可能的情况下，先从一个预训练模型[6]开始，然后进行微调或应用某种形式的迁移学习。截至2022年4月，以下模型可用：
- en: '| **Image Recognition, Segmentation, and More** | **Machine Translation and
    Language Models** | **Speech Recognition** | **Image Generation** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **图像识别、分割等** | **机器翻译和语言模型** | **语音识别** | **图像生成** |'
- en: '| **Image Recognition**AmoebaNet-DResNet-50/101/152/2000Inception v2/v3/v4**Object
    Detection**RetinaNetMask R-CNN**Image Segmentation**Mask R-CNNDeepLabRetinaNet**Low-Resource
    Models**MnasNetMobileNetSqueezeNet | **Machine Translation**(transformer based)**Sentiment
    Analysis**(transformer based)**Question Answer****BERT** | **ASR****Transformer**
    | **Image Transformer****DCGAN****GAN** |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **图像识别**AmoebaNet-DResNet-50/101/152/2000Inception v2/v3/v4**物体检测**RetinaNetMask
    R-CNN**图像分割**Mask R-CNNDeepLabRetinaNet**低资源模型**MnasNetMobileNetSqueezeNet | **机器翻译**（基于
    transformer）**情感分析**（基于 transformer）**问答**BERT | **语音识别**Transformer | **图像 Transformer**DCGANGAN
    |'
- en: 'Table 15.1: State-of-the-art collection of models pretrained with TPUs available
    on GitHub'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15.1：在 GitHub 上可用的使用 TPU 预训练的最先进的模型集合
- en: 'The best way to play with the repository is to clone it on the Google Cloud
    console and use the environment available at [https://github.com/tensorflow/tpu/blob/master/README.md](https://github.com/tensorflow/tpu/blob/master/README.md).
    You should be able to browse what is shown in *Figure 15.13*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 玩转这个代码库的最佳方式是通过 Google Cloud 控制台克隆它，并使用在 [https://github.com/tensorflow/tpu/blob/master/README.md](https://github.com/tensorflow/tpu/blob/master/README.md)
    中提供的环境。你应该能够浏览 *图 15.13* 中展示的内容：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18331_15_13.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B18331_15_13.png)'
- en: 'Figure 15.13: Cloud TPUs'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.13：云 TPU
- en: 'If you click the button **OPEN IN GOOGLE CLOUD SHELL**, then the system will
    clone the Git repository into your cloud shell and then open the shell (see *Figure
    15.14*):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击 **OPEN IN GOOGLE CLOUD SHELL** 按钮，系统会将 Git 仓库克隆到你的云 Shell 中，然后打开 Shell（见
    *图 15.14*）：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_15_14.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 描述自动生成](img/B18331_15_14.png)'
- en: 'Figure 15.14: Google Cloud Shell with the TPU Git repository cloned on your
    behalf'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14：Google Cloud Shell 与为你克隆的 TPU Git 仓库
- en: 'From there, you can play with a nice Google Cloud TPU demo for training a ResNet-50
    on MNIST with a TPU flock – a Compute Engine VM and Cloud TPU pair (see *Figure
    15.15*):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里，你可以通过一个不错的 Google Cloud TPU 演示，使用 TPU 群体——一个计算引擎虚拟机和云 TPU 配对，在 MNIST 上训练
    ResNet-50（见 *图 15.15*）：
- en: '![Text  Description automatically generated](img/B18331_15_15.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![文本 描述自动生成](img/B18331_15_15.png)'
- en: 'Figure 15.15: Google Cloud TPU demo for training a ResNet-50 on MNIST with
    a TPU flock'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.15：Google Cloud TPU 演示，使用 TPU 群体在 MNIST 上训练 ResNet-50
- en: I will leave this training demo for you if you are interested in looking it
    up.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣查看，我将为你保留这个训练演示。
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: TPUs are very special ASIC chips developed at Google for executing neural network
    mathematical operations in an ultra-fast manner. The core of the computation is
    a systolic multiplier that computes multiple dot products (row * column) in parallel,
    thus accelerating the computation of basic deep learning operations. Think of
    a TPU as a special-purpose co-processor for deep learning that is focused on matrix
    or tensor operations. Google has announced four generations of TPUs so far, plus
    an additional Edge TPU for IoT. Cloud TPU v1 is a PCI-based specialized co-processor,
    with 92 teraops and inference only. Cloud TPU v2 achieves 180 teraflops and it
    supports training and inference. Cloud TPU v2 Pods released in alpha in 2018 can
    achieve 11.5 petaflops. Cloud TPU v3 achieves 420 teraflops with both training
    and inference support. Cloud TPU v3 Pods can deliver more than 100 petaflops of
    computing power. Each TPU v4 chip provides more than 2x the compute power of a
    TPU v3 chip – up to 275 peak TFLOPS. Each TPU v4 Pod delivers 1.1 exaflops/s of
    peak performance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: TPU 是 Google 为执行神经网络数学运算而开发的非常特殊的 ASIC 芯片，以超快的方式进行运算。计算的核心是一个 systolic multiplier，它并行计算多个点积（行
    * 列），从而加速基础深度学习操作的计算。可以将 TPU 看作是一个专门用于深度学习的协处理器，专注于矩阵或张量操作。到目前为止，Google 已经发布了四代
    TPU，并且还推出了一个针对物联网的 Edge TPU。Cloud TPU v1 是一个基于 PCI 的专用协处理器，提供 92 teraops，只支持推理。Cloud
    TPU v2 实现了 180 teraflops，支持训练和推理。2018 年发布的 Cloud TPU v2 Pods（测试版）可达到 11.5 petaflops。Cloud
    TPU v3 实现了 420 teraflops，支持训练和推理。Cloud TPU v3 Pods 可以提供超过 100 petaflops 的计算能力。每个
    TPU v4 芯片提供超过 TPU v3 芯片 2 倍的计算能力——最高可达 275 peak TFLOPS。每个 TPU v4 Pod 可提供 1.1 exaflops/s
    的峰值性能。
- en: That’s a world-class supercomputer for tensor operations!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一台世界级的张量操作超级计算机！
- en: In the next chapter, we will see some other useful deep learning libraries.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一些其他有用的深度学习库。
- en: References
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Moore’s law: [https://en.wikipedia.org/wiki/Moore%27s_law](https://en.wikipedia.org/wiki/Moore%27s_law)'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摩尔定律：[https://en.wikipedia.org/wiki/Moore%27s_law](https://en.wikipedia.org/wiki/Moore%27s_law)
- en: Milovanović, I. Ž. et al. (May 2010). *Forty-three ways of systolic matrix multiplication*.
    Article in International Journal of Computer Mathematics 87(6):1264–1276.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Milovanović, I. Ž. 等人（2010年5月）。*43种同步矩阵乘法方法*。刊登于《国际计算数学杂志》87(6):1264–1276。
- en: Jouppi, N. P. et al. (June 2014). *In-Datacenter Performance Analysis of a Tensor
    Processing Unit*. 44th International Symposium on Computer Architecture (ISCA).
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jouppi, N. P. 等人（2014年6月）。*数据中心内的张量处理单元性能分析*。第44届国际计算机架构研讨会（ISCA）。
- en: 'Google TPU v2 performance: [https://storage.googleapis.com/nexttpu/index.xhtml](https://storage.googleapis.com/nexttpu/index.xhtml)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google TPU v2 性能：[https://storage.googleapis.com/nexttpu/index.xhtml](https://storage.googleapis.com/nexttpu/index.xhtml)
- en: 'MLPerf site: [https://mlperf.org/](https://mlperf.org/)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLPerf 网站：[https://mlperf.org/](https://mlperf.org/)
- en: 'A collection of models pretrained with TPU: [https://cloud.google.com/tpu](https://cloud.google.com/tpu)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些使用 TPU 预训练的模型：[https://cloud.google.com/tpu](https://cloud.google.com/tpu)
- en: Join our book’s Discord space
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 社区
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过 2000 名成员一起学习，访问链接：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
