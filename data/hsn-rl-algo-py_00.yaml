- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: '**Reinforcement learning** (**RL**) is a popular and promising branch of artificial
    intelligence that involves making smarter models and agents that can automatically
    determine ideal behavior based on changing requirements. *Reinforcement Learning
    Algorithms with Python* will help you master RL algorithms and understand their
    implementation as you build self-learning agents.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是人工智能的一个热门且前景广阔的分支，涉及到构建更智能的模型和智能体，使其能够根据不断变化的需求自动确定理想行为。《用Python实现强化学习算法》将帮助你掌握RL算法，并理解它们的实现，帮助你构建自学习的智能体。'
- en: Starting with an introduction to the tools, libraries, and setup needed to work
    in the RL environment, this book covers the building blocks of RL and delves into
    value-based methods such as the application of Q-learning and SARSA algorithms.
    You'll learn how to use a combination of Q-learning and neural networks to solve
    complex problems. Furthermore, you'll study policy gradient methods, TRPO, and
    PPO, to improve performance and stability, before moving on to the DDPG and TD3
    deterministic algorithms. This book also covers how imitation learning techniques
    work and how Dagger can teach an agent to fly. You'll discover evolutionary strategies
    and black-box optimization techniques. Finally, you'll get to grips with exploration
    approaches such as UCB and UCB1 and develop a meta-algorithm called ESBAS.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书从介绍在RL环境中工作所需的工具、库和设置开始，涵盖了RL的基本构建模块，并深入探讨了基于值的方法，如Q-learning和SARSA算法的应用。你将学习如何将Q-learning与神经网络结合使用，解决复杂问题。此外，你将研究策略梯度方法、TRPO和PPO，以提高性能和稳定性，然后再深入到DDPG和TD3的确定性算法。本书还将介绍模仿学习技术的原理，以及如何通过Dagger教一个智能体飞行。你将了解进化策略和黑箱优化技术。最后，你将掌握如UCB和UCB1等探索方法，并开发出一种名为ESBAS的元算法。
- en: By the end of the book, you'll have worked with key RL algorithms to overcome
    challenges in real-world applications, and you'll be part of the RL research community.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本书结尾时，你将通过与关键的强化学习（RL）算法的实践，克服现实世界应用中的挑战，并且你将成为RL研究社区的一员。
- en: Who this book is for
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合的读者
- en: If you are an AI researcher, deep learning user, or anyone who wants to learn
    RL from scratch, this book is for you. You'll also find this RL book useful if
    you want to learn about the advancements in the field. Working knowledge of Python
    is necessary.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是AI研究人员、深度学习用户，或任何希望从零开始学习RL的人，本书非常适合你。如果你希望了解该领域的最新进展，这本RL书也会对你有所帮助。需要具备一定的Python基础。
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概述
- en: '[Chapter 1](0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml), *The Landscape of
    Reinforcement Learning*, gives you an insight into RL. It describes the problems
    that RL is good at solving and the applications where RL algorithms are already
    adopted. It also introduces the tools, the libraries, and the setup needed for
    the completion of the projects in the following chapters.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml)，*强化学习的全景*，为你提供了关于RL的深刻洞察。它描述了RL擅长解决的问题以及RL算法已经应用的领域。还介绍了完成后续章节项目所需的工具、库和设置。'
- en: '[Chapter 2](716e2db3-37c0-4f2c-8397-81c0c812c80e.xhtml),* Implementing RL Cycle
    and OpenAI Gym*, describes the main cycle of the RL algorithms, the toolkit used
    to develop the algorithms, and the different types of environments. You will be
    able to develop a random agent using the OpenAI Gym interface to play CartPole
    using random actions. You will also learn how to use the OpenAI Gym interface
    to run other environments.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](716e2db3-37c0-4f2c-8397-81c0c812c80e.xhtml)，*实现RL循环与OpenAI Gym*，描述了RL算法的主要循环、用于开发算法的工具包，以及不同类型的环境。你将能够通过OpenAI
    Gym接口开发一个随机智能体，使用随机动作玩CartPole。你还将学习如何使用OpenAI Gym接口运行其他环境。'
- en: '[Chapter 3](f2414b11-976a-4410-92d8-89ee54745d99.xhtml), *Solving Problems
    with Dynamic Programming*, introduces to you the core ideas, terminology, and
    approaches of RL. You will learn about the main blocks of RL and develop a general
    idea about how RL algorithms can be created to solve a problem. You will also
    learn the differences between model-based and model-free algorithms and the categorization
    of reinforcement learning algorithms. Dynamic programming will be used to solve
    the game FrozenLake.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](f2414b11-976a-4410-92d8-89ee54745d99.xhtml)，*通过动态规划解决问题*，向你介绍RL的核心思想、术语和方法。你将学习RL的主要模块，并对如何构建RL算法来解决问题有一个大致的了解。你还将了解基于模型和无模型算法之间的区别，以及强化学习算法的分类。动态规划将被用来解决FrozenLake游戏。'
- en: '[Chapter 4](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml), *Q-Learning and SARSA
    Applications*, talks about value-based methods, in particular Q-learning and SARSA,
    two algorithms that differ from dynamic programming and scale well on large problems.
    To become confident with these algorithms, you will apply them to the FrozenLake
    game and study the differences from dynamic programming.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml)，*Q学习与SARSA应用（Q-Learning and
    SARSA Applications）*，讨论了基于价值的方法，特别是Q学习和SARSA，这两种算法与动态规划不同，并且在大规模问题上具有良好的扩展性。为了深入掌握这些算法，你将把它们应用于FrozenLake游戏，并研究它们与动态规划的差异。'
- en: '[Chapter 5](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml), *Deep Q-Networks*,
    describes how neural networks and **convolutional neural networks** (**CNNs**)
    in particular are applied to Q-learning. You''ll learn why the combination of
    Q-learning and neural networks produces incredible results and how its use can
    open the door to a much larger variety of problems. Furthermore, you''ll apply
    the DQN to an Atari game using the OpenAI Gym interface.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml)，*深度Q网络（Deep Q-Networks）*，描述了神经网络，特别是**卷积神经网络（CNNs）**，如何应用于Q学习。你将学习为什么Q学习和神经网络的结合能产生惊人的结果，并且它的使用能够解决更广泛的问题。此外，你还将利用OpenAI
    Gym接口将DQN应用于Atari游戏。'
- en: '[Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml), *Learning Stochastic
    and PG Optimization*, introduces a new family of model-free algorithms: policy
    gradient methods. You will learn the differences between policy gradient and value-based
    methods, and you''ll learn about their strengths and weaknesses. Then you will
    implement the REINFORCE and Actor-Critic algorithms to solve a new game called
    LunarLander.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml)，*学习随机优化与策略梯度优化（Learning Stochastic
    and PG Optimization）*，介绍了一类新的无模型算法：策略梯度方法。你将学习策略梯度方法和基于价值的方法之间的区别，并了解它们的优缺点。接着，你将实现REINFORCE和Actor-Critic算法，解决一款名为LunarLander的新游戏。'
- en: '[Chapter 7](4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml), *TRPO and PPO Implementation*,
    proposes a modification of policy gradient methods using new mechanisms to control
    the improvement of the policy. These mechanisms are used to improve the stability
    and convergence of the policy gradient algorithms. In particular you''ll learn
    and implement two main policy gradient methods that use these techniques, namely
    TRPO and PPO. You will implement them on RoboSchool, an environment with a continuous
    action space.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml)，*TRPO和PPO实现（TRPO and PPO
    Implementation）*，提出了一种修改策略梯度方法的新机制，用以控制策略的改进。这些机制被用来提高策略梯度算法的稳定性和收敛性。特别是，你将学习并实现两种主要的策略梯度方法，这些方法运用了这些新技术，分别是TRPO和PPO。你将通过在RoboSchool环境中实现它们，探索具有连续动作空间的环境。'
- en: '[Chapter 8](d902d278-a1f5-438a-9ddd-6d9d665f2fa2.xhtml), *DDPG and TD3 Applications*,
    introduces a new category of algorithms called deterministic policy algorithms
    that combine both policy gradient and Q-learning. You will learn about the underlying
    concepts and implement DDPG and TD3, two deep deterministic algorithms, on a new
    environment.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](d902d278-a1f5-438a-9ddd-6d9d665f2fa2.xhtml)，*DDPG和TD3应用（DDPG and TD3
    Applications）*，介绍了一类新的算法——确定性策略算法，这些算法结合了策略梯度和Q学习。你将了解其背后的概念，并在一个新的环境中实现DDPG和TD3这两种深度确定性算法。'
- en: '[Chapter 9](7e8448cf-7b74-4f07-99bd-da8f98f4505c.xhtml), *Model-Based RL*, illustrates
    RL algorithms that learn the model of the environment to plan future actions,
    or, to learn a policy. You will be taught how they work, their strengths, and
    why they are preferred in many situations. To master them, you will implement
    a model-based algorithm on Roboschool.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](7e8448cf-7b74-4f07-99bd-da8f98f4505c.xhtml)，*基于模型的强化学习（Model-Based RL）*，介绍了学习环境模型以规划未来动作或学习策略的强化学习算法。你将学习它们的工作原理、优点，以及为何它们在许多情况下更受青睐。为了掌握它们，你将实现一个基于模型的算法，并在Roboschool环境中进行实验。'
- en: '[Chapter 10](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml), *Imitation Learning
    with the DAgger Algorithm*, explains how imitation learning works and how it can
    be applied and adapted to a problem. You will learn about the most well-known
    imitation learning algorithm, DAgger. To become confident with it, you will implement
    it to speed up the learning process of an agent on FlappyBird.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第10章](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml)，*通过DAgger算法进行模仿学习（Imitation
    Learning with the DAgger Algorithm）*，解释了模仿学习如何工作，以及如何将其应用和调整到具体问题上。你将了解最著名的模仿学习算法——DAgger。为了深入理解它，你将通过在FlappyBird中实施该算法来加速智能体的学习过程。'
- en: '[Chapter 11](dab022a7-3243-4e45-9f91-39a82df3a248.xhtml), *Understanding Black-Box
    Optimization Algorithms*, explores evolutionary algorithms, a class of black-box
    optimization algorithms that don''t rely on backpropagation. These algorithms
    are gaining interest because of their fast training and easy parallelization across
    hundreds or thousands of cores. This chapter provides a theoretical and practical
    background of these algorithms by focusing particularly on the Evolution Strategy
    algorithm, a type of evolutionary algorithm.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[第11章](dab022a7-3243-4e45-9f91-39a82df3a248.xhtml)，*理解黑箱优化算法*，探讨了进化算法，这是一类不依赖反向传播的黑箱优化算法。由于其快速的训练速度和易于在数百或数千个核心上并行化，这些算法正在受到越来越多的关注。本章通过特别关注进化策略算法这一进化算法的类型，提供了这些算法的理论和实践背景。'
- en: '[Chapter 12](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml), *Developing ESBAS
    Algorithm*, introduces the important exploration-exploitation dilemma, which is
    specific to RL. The dilemma is demonstrated using the multi-armed bandit problem
    and is solved using approaches such as UCB and UCB1\. Then, you will learn about
    the problem of algorithm selection and develop a meta-algorithm called ESBAS.
    This algorithm uses UCB1 to select the most appropriate RL algorithm for each
    situation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml)，*开发ESBAS算法*，介绍了强化学习中特有的重要探索-利用困境。通过多臂老虎机问题演示了这一困境，并使用如UCB和UCB1等方法解决。接着，你将了解算法选择问题，并开发一种名为ESBAS的元算法。该算法使用UCB1来为每种情况选择最合适的强化学习算法。'
- en: '[Chapter 13](719f001d-db10-47a4-98c5-fe95666f7c32.xhtml), *Practical Implementations
    to Resolve RL Challenges*, takes a look at the major challenges in this field
    and explains some practices and methods to overcome them. You will also learn
    about some of the challenges of applying RL to real-world problems, future developments
    of deep RL, and their social impact in the world.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](719f001d-db10-47a4-98c5-fe95666f7c32.xhtml)，*解决强化学习挑战的实践实现*，探讨了该领域的主要挑战，并解释了克服这些挑战的一些实践和方法。你还将了解将强化学习应用于现实问题的挑战、深度强化学习的未来发展，以及其对世界的社会影响。'
- en: To get the most out of this book
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要充分利用本书
- en: Working knowledge of Python is necessary. Knowledge of RL and the various tools
    used for it will also be beneficial.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 需要具备一定的Python工作知识。了解强化学习及其相关工具也将是有益的。
- en: Download the example code files
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from your account at [www.packt.com](http://www.packt.com).
    If you purchased this book elsewhere, you can visit [www.packtpub.com/support](https://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您的帐户在[www.packt.com](http://www.packt.com)下载本书的示例代码文件。如果您在其他地方购买了此书，您可以访问[www.packtpub.com/support](https://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。
- en: 'You can download the code files by following these steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下步骤下载代码文件：
- en: Log in or register at [www.packt.com](http://www.packt.com).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录或注册[www.packt.com](http://www.packt.com)。
- en: Select the Support tab.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择“支持”标签。
- en: Click on Code Downloads.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“代码下载”。
- en: Enter the name of the book in the Search box and follow the onscreen instructions.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入书名，并按照屏幕上的指示操作。
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件下载完成，请确保使用最新版本的工具解压或提取文件夹：
- en: WinRAR/7-Zip for Windows
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WinRAR/7-Zip for Windows
- en: Zipeg/iZip/UnRarX for Mac
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipeg/iZip/UnRarX for Mac
- en: 7-Zip/PeaZip for Linux
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7-Zip/PeaZip for Linux
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python). In
    case there's an update to the code, it will be updated on the existing GitHub
    repository.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包也托管在GitHub上，链接为[https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)。如果代码有更新，它将被更新到现有的GitHub仓库中。
- en: We also have other code bundles from our rich catalog of books and videos available
    at **[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**.
    Check them out!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了其他来自丰富书籍和视频目录的代码包，您可以在**[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**查看。快去看看吧！
- en: Download the color images
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图片
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)[.](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一份包含书中所有截图/图表的彩色图像的 PDF 文件。您可以在此下载：[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)
- en: Conventions used
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了若干文本约定。
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    Here is an example: "In this book, we use Python 3.7, but all versions above 3.5
    should work. We also assume that you''ve already installed `numpy` and `matplotlib`."'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`CodeInText`：表示文本中的代码词汇、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟网址、用户输入和 Twitter 账号。以下是一个示例：“在本书中，我们使用的是
    Python 3.7，但 3.5 以上的所有版本应该都可以使用。我们还假设您已经安装了`numpy`和`matplotlib`。”'
- en: 'A block of code is set as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一段代码块的格式如下：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有命令行输入或输出如下所示：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For example, words in menus or dialog boxes appear in the text like this. Here
    is an example: "In **reinforcement learning** (**RL**), the algorithm is called
    the agent, and it learns from the data provided by an environment."'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要词汇或屏幕上看到的词汇。例如，菜单或对话框中的词汇在文中通常是这样呈现的。以下是一个示例：“在**强化学习**（**RL**）中，算法被称为代理，它通过环境提供的数据进行学习。”'
- en: Warnings or important notes appear like this.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要提示如下所示。
- en: Tips and tricks appear like this.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士和技巧如下所示。
- en: Get in touch
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保持联系
- en: Feedback from our readers is always welcome.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常欢迎读者的反馈。
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email us at `customercare@packtpub.com`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：如果您对本书的任何方面有疑问，请在邮件主题中提及书名，并通过`customercare@packtpub.com`联系我们。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](https://www.packtpub.com/support/errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误表**：尽管我们已经尽力确保内容的准确性，但错误仍然可能发生。如果您在本书中发现了错误，恳请您向我们报告。请访问[www.packtpub.com/support/errata](https://www.packtpub.com/support/errata)，选择您的书籍，点击“勘误提交表格”链接，并填写相关细节。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packt.com` with a link
    to the material.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果您在互联网上发现任何我们作品的非法复制品，恳请您提供该位置地址或网站名称。请通过`copyright@packt.com`与我们联系，并附上相关资料的链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com/).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个主题上拥有专业知识，并且有兴趣撰写或参与书籍的编写，请访问[authors.packtpub.com](http://authors.packtpub.com/)。'
- en: Reviews
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 书评
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请留下评论。当您阅读并使用完本书后，不妨在购买平台上留下评论。潜在读者可以通过您的公正评价做出购买决策，我们 Packt 可以了解您的意见，我们的作者也能看到您对他们书籍的反馈。谢谢！
- en: For more information about Packt, please visit [packt.com](http://www.packt.com/).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于 Packt 的信息，请访问[packt.com](http://www.packt.com/)。
