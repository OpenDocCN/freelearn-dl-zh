- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: The Cross-Entropy Method
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵方法
- en: 'In the last chapter, you learned about PyTorch. In this chapter, we will wrap
    up Part 1 of this book and you will become familiar with one of the reinforcement
    learning (RL) methods: cross-entropy.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章节，您已经了解了 PyTorch。在本章中，我们将结束本书的第一部分，您将熟悉其中一种强化学习方法：交叉熵。
- en: Despite the fact that it is much less famous than other tools in the RL practitioner’s
    toolbox, such as deep Q-network (DQN) or advantage actor-critic (A2C), the cross-entropy
    method has its own strengths. Firstly, the cross-entropy method is really simple,
    which makes it an easy method to follow. For example, its implementation on PyTorch
    is less than 100 lines of code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与 RL 从业者工具箱中其他工具（如深度 Q 网络（DQN）或优势演员-评论家（A2C））相比，交叉熵方法的知名度要低得多，但它也有其自身的优势。首先，交叉熵方法非常简单，这使得它成为一种易于遵循的方法。例如，在
    PyTorch 上的实现不到 100 行代码。
- en: Secondly, the method has good convergence. In simple environments that don’t
    require you to learn complex, multistep policies and that have short episodes
    with frequent rewards, the cross-entropy method usually works very well. Of course,
    lots of practical problems don’t fall into this category, but sometimes they do.
    In such cases, the cross-entropy method (on its own or as part of a larger system)
    can be the perfect fit.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，该方法具有良好的收敛性。在不需要学习复杂、多步策略且具有频繁奖励的简单环境中，交叉熵方法通常表现得非常出色。当然，许多实际问题不属于这一类，但有时候会出现。在这种情况下，交叉熵方法（单独使用或作为更大系统的一部分）可能是完美的选择。
- en: 'In this chapter, we will cover:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: The practical side of the cross-entropy method
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵方法的实际应用
- en: How the cross-entropy method works in two environments in Gym (the familiar
    CartPole and the grid world of FrozenLake)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Gym 中两个环境（熟悉的 CartPole 和 FrozenLake 的网格世界）中交叉熵方法的工作原理
- en: The theoretical background of the cross-entropy method. This section is optional
    and requires a bit of knowledge of probability and statistics, but if you want
    to understand why the method works, then you can delve into it.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵方法的理论背景。本节内容是可选的，需要一些概率和统计知识，但如果您想要理解该方法的工作原理，那么您可以深入研究一下。
- en: The taxonomy of RL methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RL 方法的分类
- en: The cross-entropy method falls into the model-free, policy-based, and on-policy
    categories of methods. These notions are new, so let’s spend some time exploring
    them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵方法属于无模型、基于策略以及在线策略方法的范畴。这些概念很新，所以让我们花点时间来探索它们。
- en: 'All the methods in RL can be classified into various groups:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RL 中的所有方法都可以分为不同的组：
- en: Model-free or model-based
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模型或有模型
- en: Value-based or policy-based
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于价值或基于策略
- en: On-policy or off-policy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在策略或离策略
- en: There are other ways that you can taxonomize RL methods, but, for now, we are
    interested in the above three. Let’s define them, as the specifics of your problem
    can influence your choice of a particular method.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法可以对 RL 方法进行分类，但是目前我们对上述三种感兴趣。让我们定义它们，因为您的具体问题的特性可能会影响您选择特定方法。
- en: The term model-free means that the method doesn’t build a model of the environment
    or reward; it just directly connects observations to actions (or values that are
    related to actions). In other words, the agent takes current observations and
    does some computations on them, and the result is the action that it should take.
    In contrast, model-based methods try to predict what the next observation and/or
    reward will be. Based on this prediction, the agent tries to choose the best possible
    action to take, very often making such predictions multiple times to look more
    and more steps into the future.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“无模型”意味着该方法不会建立环境或奖励的模型；它只是直接将观察结果连接到行动（或与行动相关的值）。换句话说，代理器接受当前观察结果并对其进行一些计算，其结果就是它应该采取的行动。相比之下，模型基方法试图预测接下来的观察结果和/或奖励。基于这一预测，代理器试图选择最佳的可能行动，往往多次进行这样的预测，以查看未来更多步骤。
- en: Both classes of methods have strong and weak sides, but usually pure model-based
    methods are used in deterministic environments, such as board games with strict
    rules. On the other hand, model-free methods are usually easier to train because
    it’s hard to build good models of complex environments with rich observations.
    All of the methods described in this book are from the model-free category, as
    those methods have been the most active area of research for the past few years.
    Only recently have researchers started to mix the benefits from both worlds (for
    example, in Chapter [20](ch024.xhtml#x1-36400020), we’ll take a look at the AlphaGo
    Zero and MuZero methods, which use a model-based approach to board games and Atari).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这两类方法各有优缺点，但通常纯粹的基于模型的方法用于确定性环境，如有严格规则的棋盘游戏。另一方面，基于模型的方法通常更难训练，因为很难构建具有丰富观察的复杂环境的良好模型。本书中描述的所有方法都属于无模型类别，因为这些方法在过去几年里一直是研究的最活跃领域。直到最近，研究人员才开始结合两者的优点（例如，在第[20](ch024.xhtml#x1-36400020)章中，我们将介绍AlphaGo
    Zero和MuZero方法，这些方法将基于模型的方法应用于棋盘游戏和Atari游戏）。
- en: Looking at this from another angle, policy-based methods directly approximate
    the policy of the agent, that is, what actions the agent should carry out at every
    step. The policy is usually represented by a probability distribution over the
    available actions. Alternatively, the method could be value-based. In this case,
    instead of the probability of actions, the agent calculates the value of every
    possible action and chooses the action with the best value. These families of
    methods are equally popular, and we will discuss value-based methods in the next
    part of the book. Policy methods will be the topic of Part 3.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，基于策略的方法直接近似智能体的策略，即智能体在每一步应该采取什么动作。策略通常通过一个可用动作的概率分布表示。或者，这种方法也可以是基于价值的。在这种情况下，智能体计算每个可能动作的价值，并选择具有最佳价值的动作，而不是选择动作的概率。这两类方法同样受欢迎，我们将在本书的下一部分讨论基于价值的方法。基于策略的方法将在第3部分中讨论。
- en: The third important classification of methods is on-policy versus off-policy.
    We will discuss this distinction in more depth in Parts 2 and 3 of the book, but,
    for now, it is enough to explain off-policy as the ability of the method to learn
    from historical data (obtained by a previous version of the agent, recorded by
    human demonstration, or just seen by the same agent several episodes ago). On
    the other hand, on-policy methods require fresh data for training, generated from
    the policy we’re currently updating. They cannot be trained on old historical
    data because the result of the training will be wrong. This makes such methods
    much less data-efficient (you need much more communication with the environment),
    but in some cases, this is not a problem (for example, if our environment is very
    lightweight and fast, so we can quickly interact with it).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 方法的第三个重要分类是在线策略与离线策略的区别。我们将在本书的第2部分和第3部分深入讨论这一区别，但目前，解释离线策略足以理解它是指方法能够从历史数据中学习（这些数据可能来自智能体的先前版本、由人类演示录制，或仅仅是同一智能体在几次交互之前观察到的数据）。另一方面，在线策略方法需要最新的数据进行训练，这些数据来自我们当前正在更新的策略。它们不能基于旧的历史数据进行训练，因为训练结果将会错误。这使得这类方法的数据效率较低（你需要更多的与环境交互），但在某些情况下，这不是问题（例如，如果我们的环境非常轻量且快速，那么我们可以迅速与其交互）。
- en: 'So, our cross-entropy method is model-free, policy-based, and on-policy, which
    means the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的交叉熵方法是无模型、基于策略且是在线策略，这意味着以下几点：
- en: It doesn’t build a model of the environment; it just says to the agent what
    to do at every step
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它并不构建环境的模型；它只是告诉智能体在每一步该做什么。
- en: It approximates the policy of the agent
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它近似智能体的策略
- en: It requires fresh data obtained from the environment
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要从环境中获取的新数据
- en: The cross-entropy method in practice
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵方法的实际应用
- en: 'The explanation of the cross-entropy method can be split into two unequal parts:
    practical and theoretical. The practical part is intuitive in nature, while the
    theoretical explanation of why the cross-entropy method works and what happens,
    is more sophisticated.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵方法的解释可以分为两部分：实际部分和理论部分。实际部分是直观的，而交叉熵方法为何有效以及其原理的理论解释则更加复杂。
- en: 'You may remember that the central and trickiest thing in RL is the agent, which
    tries to accumulate as much total reward as possible by communicating with the
    environment. In practice, we follow a common machine learning (ML) approach and
    replace all of the complications of the agent with some kind of nonlinear trainable
    function, which maps the agent’s input (observations from the environment) to
    some output. The details of the output that this function produces may depend
    on a particular method or a family of methods (such as value-based or policy-based
    methods), as described in the previous section. As our cross-entropy method is
    policy-based, our nonlinear function (neural network (NN)) produces the policy,
    which basically says for every observation which action the agent should take.
    In research papers, policy is denoted as π(a|s), where a are actions and s is
    the current state. This is illustrated in the following diagram:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，强化学习中最核心且最棘手的部分是智能体，它试图通过与环境的交互尽可能地积累总奖励。实际上，我们遵循一种常见的机器学习（ML）方法，用某种非线性可训练函数替代智能体的所有复杂性，该函数将智能体的输入（来自环境的观察）映射到某些输出。这种函数所产生的输出的细节可能依赖于特定的方法或方法族（例如基于值的方法或基于策略的方法），正如前一节所描述的那样。由于我们的交叉熵方法是基于策略的，因此我们的非线性函数（神经网络（NN））生成策略，该策略基本上决定了对于每个观察，智能体应该采取哪个动作。在研究论文中，策略表示为
    π(a|s)，其中 a 是动作，s 是当前状态。以下图所示：
- en: '![SaTmrpalienabalcteion EfOPAnubocvnaslticeiirt∼rcooivynnoπamn(tπae (ai(nN|oatNsn|s)))s
    ](img/B22150_04_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![SaTmrpalienabalcteion EfOPAnubocvnaslticeiirt∼rcooivynnoπamn(tπae (ai(nN|oatNsn|s)))s
    ](img/B22150_04_01.png)'
- en: 'Figure 4.1: A high-level approach to policy-based RL'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：基于策略的强化学习的高级方法
- en: In practice, the policy is usually represented as a probability distribution
    over actions, which makes it very similar to a classification problem, with the
    number of classes being equal to the number of actions we can carry out.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，策略通常表示为一个动作的概率分布，这使得它非常类似于分类问题，其中类别的数量等于我们可以执行的动作数量。
- en: 'This abstraction makes our agent very simple: it needs to pass an observation
    from the environment to the NN, get a probability distribution over actions, and
    perform random sampling using the probability distribution to get an action to
    carry out. This random sampling adds randomness to our agent, which is a good
    thing because at the beginning of the training, when our weights are random, the
    agent behaves randomly. As soon as the agent gets an action to issue, it fires
    the action to the environment and obtains the next observation and reward for
    the last action. Then the loop continues, as shown in Figure [4.1](#x1-76002r1).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象使得我们的智能体变得非常简单：它只需要将来自环境的观察传递给神经网络，得到一个动作的概率分布，并使用概率分布进行随机抽样，得到一个需要执行的动作。这种随机抽样为我们的智能体增加了随机性，这是件好事，因为在训练开始时，当我们的权重是随机的，智能体的行为也是随机的。一旦智能体获得了一个需要执行的动作，它就将该动作发送给环境，并获得上一个动作的下一个观察和奖励。然后，循环继续，如图[4.1](#x1-76002r1)所示。
- en: 'During the agent’s lifetime, its experience is presented as episodes. Every
    episode is a sequence of observations that the agent has got from the environment,
    actions it has issued, and rewards for these actions. Imagine that our agent has
    played several such episodes. For every episode, we can calculate the total reward
    that the agent has claimed. It can be discounted or not discounted; for simplicity,
    let’s assume a discount factor of γ = 1, which just means an undiscounted sum
    of all local rewards for every episode. This total reward shows how good this
    episode was for the agent. It is illustrated in Figure [4.2](#x1-76004r2), which
    contains four episodes (note that different episodes have different values for
    o[i], a[i], and r[i]):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体的生命周期中，它的经验呈现为若干个回合（episodes）。每个回合是智能体从环境中获得的一系列观察、它所执行的动作以及这些动作的奖励。假设我们的智能体已经经历了若干个这样的回合。对于每个回合，我们可以计算智能体所获得的总奖励。它可以是折扣奖励，也可以是不折扣奖励；为了简单起见，假设折扣因子
    γ = 1，这意味着每个回合的所有局部奖励的未折扣总和。这个总奖励显示了该回合对智能体来说有多好。它在图[4.2](#x1-76004r2)中有所说明，其中包含了四个回合（注意，不同的回合有不同的
    o[i]、a[i] 和 r[i] 的值）：
- en: '![Episode 1: o1,a1,r1 o2,a2,r2 o3,a3,r3 o4,a4,rR4= r1 + r2 + r3 + r4 Episode
    2: o1,a1,r1 o2,a2,r2 o3,a3,r3 R = r1 + r2 + r3 Episode 3: o1,a1,r1 o2,a2,r2 R
    = r1 + r2 Episode 4: o1,a1,r1 o2,a2,r2 o3,a3,r3 R = r1 + r2 + r3 ](img/B22150_04_02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![第1集：o1,a1,r1 o2,a2,r2 o3,a3,r3 o4,a4,rR4= r1 + r2 + r3 + r4 第2集：o1,a1,r1
    o2,a2,r2 o3,a3,r3 R = r1 + r2 + r3 第3集：o1,a1,r1 o2,a2,r2 R = r1 + r2 第4集：o1,a1,r1
    o2,a2,r2 o3,a3,r3 R = r1 + r2 + r3 ](img/B22150_04_02.png)'
- en: 'Figure 4.2: Example episodes with their observations, actions, and rewards'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：示例回合及其观察、动作和奖励
- en: 'Every cell represents the agent’s step in the episode. Due to randomness in
    the environment and the way that the agent selects actions to take, some episodes
    will be better than others. The core of the cross-entropy method is to throw away
    bad episodes and train on better ones. So, the steps of the method are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格代表代理在回合中的一步。由于环境中的随机性以及代理选择采取行动的方式，一些回合会比其他回合更好。交叉熵方法的核心是丢弃不好的回合，并在更好的回合上进行训练。所以，该方法的步骤如下：
- en: Play N episodes using our current model and environment.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前模型和环境播放 N 个回合。
- en: Calculate the total reward for every episode and decide on a reward boundary.
    Usually, we use a percentile of all rewards, such as the 50th or 70th.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个回合的总奖励并确定奖励边界。通常，我们使用所有奖励的百分位数，如第 50 或第 70 百分位数。
- en: Throw away all episodes with a reward below the boundary.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃所有奖励低于边界的回合。
- en: Train on the remaining ”elite” episodes (with rewards higher than the boundary)
    using observations as the input and issued actions as the desired output.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用观察作为输入，发出的动作作为期望输出，在剩余的“精英”回合（奖励高于边界）上进行训练。
- en: Repeat from step 1 until we become satisfied with the result.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤 1 重复，直到对结果感到满意为止。
- en: So, that’s the cross-entropy method’s description. With the preceding procedure,
    our NN learns how to repeat actions, which leads to a larger reward, constantly
    moving the boundary higher and higher. Despite the simplicity of this method,
    it works well in basic environments, it’s easy to implement, and it’s quite robust
    against changing hyperparameters, which makes it an ideal baseline method to try.
    Let’s now apply it to our CartPole environment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是交叉熵方法的描述。通过上述过程，我们的神经网络学会如何重复动作，从而获得更大的奖励，不断提高边界。尽管该方法非常简单，但在基础环境中效果很好，易于实现，并且对超参数变化具有很强的鲁棒性，这使得它成为一个理想的基准方法。现在，让我们将其应用于我们的
    CartPole 环境。
- en: The cross-entropy method on CartPole
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵方法在 CartPole 上的应用
- en: 'The whole code for this example is in Chapter04/01_cartpole.py. Here, I will
    show only the most important parts. Our model’s core is a one-hidden-layer NN,
    with rectified linear unit (ReLU) and 128 hidden neurons (which is absolutely
    arbitrary; you can try to increase or decrease this constant – we’ve left this
    as an exercise for you). Other hyperparameters are also set almost randomly and
    aren’t tuned, as the method is robust and converges very quickly. We define constants
    at the top of the file:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的完整代码在 Chapter04/01_cartpole.py 中。这里，我只展示最重要的部分。我们模型的核心是一个单隐藏层的神经网络，使用了修正线性单元（ReLU）和
    128 个隐藏神经元（这个数字完全是随意的；你可以尝试增加或减少这个常数——我们将这个作为一个练习留给你）。其他超参数也几乎是随机设置的，并且没有调优，因为该方法具有很强的鲁棒性，且收敛速度非常快。我们在文件顶部定义常数：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As shown in the preceding code, the constants include the count of neurons in
    the hidden layer, the count of episodes we play on every iteration (16), and the
    percentile of each episode’s total rewards that we use for elite episode filtering.
    We will take the 70th percentile, which means that we will keep the top 30% of
    episodes sorted by reward.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，常数包括隐藏层中神经元的数量、每次迭代中我们播放的回合数（16），以及我们用于精英回合筛选的每个回合总奖励的百分位数。我们将采用第 70
    百分位数，这意味着我们将保留奖励排序前 30% 的回合。
- en: 'There is nothing special about our NN; it takes a single observation from the
    environment as an input vector and outputs a number for every action we can perform:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络没有什么特别之处；它从环境中获取单个观察作为输入向量，并为我们可以执行的每个动作输出一个数字：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output from the NN is a probability distribution over actions, so a straightforward
    way to proceed would be to include softmax nonlinearity after the last layer.
    However, in the code, we don’t apply softmax to increase the numerical stability
    of the training process. Rather than calculating softmax (which uses exponentiation)
    and then calculating cross-entropy loss (which uses a logarithm of probabilities),
    we will use the nn.CrossEntropyLoss PyTorch class later, which combines softmax
    and cross-entropy into a single, more numerically stable expression. CrossEntropyLoss
    requires raw, unnormalized values from the NN (also called logits). The downside
    of this is that we need to remember to apply softmax every time we need to get
    probabilities from our NN’s output.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出是一个动作的概率分布，因此直接的方法是在最后一层之后加入 softmax 非线性激活函数。然而，在代码中，我们并没有应用 softmax，以提高训练过程的数值稳定性。与其计算
    softmax（使用指数运算）后再计算交叉熵损失（使用概率的对数），我们将稍后使用 nn.CrossEntropyLoss PyTorch 类，它将 softmax
    和交叉熵合并为一个更加数值稳定的表达式。CrossEntropyLoss 需要神经网络的原始未归一化值（也叫 logits）。这样做的缺点是我们每次需要从神经网络的输出中获取概率时，都需要记得应用
    softmax。
- en: 'Next, we will define two helper dataclasses:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义两个辅助的 dataclass：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The purpose of these dataclasses is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 dataclass 的目的如下：
- en: 'EpisodeStep: This will be used to represent one single step that our agent
    made in the episode, and it stores the observation from the environment and what
    action the agent performed. We will use episode steps from elite episodes as training
    data.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EpisodeStep：这个类用于表示代理在 episode 中的单一步骤，它存储了来自环境的观察值以及代理执行的动作。我们将使用精英 episode
    的步骤作为训练数据。
- en: 'Episode: This is a single episode stored as a total undiscounted reward and
    a collection of EpisodeStep.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Episode：这是一个单一的 episode，存储为总的未折扣奖励和一组 EpisodeStep。
- en: 'Let’s look at a function that generates batches with episodes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个生成包含 episode 的批次的函数：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding function accepts the environment (the Env class instance from
    the Gym library), our NN, and the count of episodes it should generate on every
    iteration. The batch variable will be used to accumulate our batch (which is a
    list of Episode instances). We also declare a reward counter for the current episode
    and its list of steps (the EpisodeStep objects). Then we reset our environment
    to obtain the first observation and create a softmax layer, which will be used
    to convert the NN’s output to a probability distribution of actions. That’s our
    preparations complete, so we are ready to start the environment loop:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数接受环境（来自 Gym 库的 Env 类实例）、我们的神经网络以及它在每次迭代时应该生成的 episode 数量。batch 变量将用于累积我们的批次（这是一个
    Episode 实例的列表）。我们还声明了当前 episode 的奖励计数器和它的步骤列表（EpisodeStep 对象）。然后，我们重置环境以获取第一个观察值，并创建一个
    softmax 层，这将用于将神经网络的输出转换为动作的概率分布。准备工作就绪，我们可以开始环境循环：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At every iteration, we convert our current observation to a PyTorch tensor
    and pass it to the NN to obtain action probabilities. There are several things
    to note here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代时，我们将当前观察值转换为 PyTorch 张量，并将其传递给神经网络以获得动作的概率。这里有几点需要注意：
- en: All nn.Module instances in PyTorch expect a batch of data items, and the same
    is true for our NN, so we convert our observation (which is a vector of four numbers
    in CartPole) into a tensor of size 1 × 4 (to achieve this, we call the unsqueeze(0)
    function on our tensor, which adds an extra dimension at the zero position of
    the shape).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的所有 nn.Module 实例都期望一批数据项，我们的神经网络也不例外，因此我们将观察值（在 CartPole 中是一个包含四个数字的向量）转换成大小为
    1 × 4 的张量（为此，我们在张量上调用 unsqueeze(0) 函数，这样会在形状的零维位置添加一个额外的维度）。
- en: As we haven’t used nonlinearity at the output of our NN, it outputs raw action
    scores, which we need to feed through the softmax function.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们在神经网络的输出中没有使用非线性激活函数，它会输出原始的动作评分，我们需要将这些评分通过 softmax 函数处理。
- en: Both our NN and the softmax layer return tensors that track gradients, so we
    need to unpack this by accessing the tensor.data field and then converting the
    tensor into a NumPy array. This array will have the same two-dimensional structure
    as the input, with the batch dimension on axis 0, so we need to get the first
    batch element to obtain a one-dimensional vector of action probabilities.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的神经网络和 softmax 层都返回跟踪梯度的张量，因此我们需要通过访问张量的 data 字段来解包它，然后将张量转换为 NumPy 数组。这个数组将具有与输入相同的二维结构，批次维度在轴
    0 上，因此我们需要获取第一个批次元素以获得一个一维的动作概率向量。
- en: 'Now that we have the probability distribution of actions, we can use it to
    obtain the actual action for the current step:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了动作的概率分布，可以利用它来获得当前步骤的实际动作：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we sample the distribution using NumPy’s function random.choice(). After
    this, we will pass this action to the environment to get our next observation,
    our reward, the indication of the episode ending, and the truncation flag. The
    last value returned by the step() function is extra information from the environment
    and is discarded.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 NumPy 的函数 random.choice() 来采样分布。然后，我们将这个动作传递给环境，以获取下一个观察结果、奖励、回合结束的指示以及截断标志。step()
    函数返回的最后一个值是来自环境的额外信息，将被丢弃。
- en: 'The reward is added to the current episode’s total reward, and our list of
    episode steps is also extended with an (observation, action) pair:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励被加入到当前回合的总奖励中，我们的回合步骤列表也会扩展，包含（观察，动作）对：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that we save the observation that was used to choose the action, but not
    the observation returned by the environment as a result of the action. These are
    the tiny, but important, details that you need to keep in mind.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们保存的是用于选择动作的观察结果，而不是由环境根据动作返回的观察结果。这些小细节，虽然微小，但非常重要，你需要记住。
- en: 'The continuation of the code handles the situation when the current episode
    is over (in the case of CartPole, the episode ends when the stick has fallen down,
    despite our efforts, or when the time limit of the environment has been reached):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的后续部分处理当前回合结束时的情况（在 CartPole 问题中，回合在杆子掉下时结束，无论我们是否努力，或者当环境的时间限制到达时结束）：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We append the finalized episode to the batch, saving the total reward (as the
    episode has been completed and we have accumulated all the rewards) and steps
    we have taken. Then we reset our total reward accumulator and clean the list of
    steps. After that, we reset our environment to start over.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将完成的回合追加到批次中，保存总奖励（因为回合已经结束，并且我们已经累积了所有奖励）以及我们采取的步骤。然后，我们重置总奖励累加器并清空步骤列表。之后，我们重置环境重新开始。
- en: 'If our batch has reached the desired count of episodes, we return it to the
    caller for processing using yield. Our function is a generator, so every time
    the yield operator is executed, the control is transferred to the outer iteration
    loop and then continues after the yield line. If you are not familiar with Python’s
    generator functions, refer to the Python documentation: [https://wiki.python.org/moin/Generators](https://wiki.python.org/moin/Generators).
    After processing, we will clean up the batch.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的批次已达到期望的回合数，我们将使用 yield 将其返回给调用者进行处理。我们的函数是生成器，因此每次执行 yield 操作符时，控制权将转交给外部迭代循环，然后在
    yield 语句后继续执行。如果你不熟悉 Python 的生成器函数，可以参考 Python 文档：[https://wiki.python.org/moin/Generators](https://wiki.python.org/moin/Generators)。处理完后，我们会清理批次。
- en: 'The last, but very important, step in our loop is to assign an observation
    obtained from the environment to our current observation variable:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们循环中的最后一步，也是非常重要的一步，是将从环境中获得的观察结果赋值给当前的观察变量：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After that, everything repeats infinitely – we pass the observation to the NN,
    sample the action to perform, ask the environment to process the action, and remember
    the result of this processing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，一切将无限重复——我们将观察结果传递给神经网络（NN），从中采样执行的动作，请求环境处理该动作，并记住该处理结果。
- en: One very important fact to understand in this function’s logic is that the training
    of our NN and the generation of our episodes are performed at the same time. They
    are not completely in parallel, but every time our loop accumulates enough episodes
    (16), it passes control to this function caller, which is supposed to train the
    NN using gradient descent. So, when yield is returned, the NN will have different,
    slightly better (we hope) behavior. As you should remember from the beginning
    of the chapter, the cross-entropy method is from the on-policy class, so using
    fresh training data is important for the method to work properly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的一个非常重要的事实是，在这个函数的逻辑中，我们的神经网络（NN）训练和回合生成是同时进行的。它们并非完全并行，但每当我们的循环累积了足够的回合（16），它会将控制权传递给此函数的调用者，调用者应该使用梯度下降来训练神经网络。因此，当
    yield 被返回时，神经网络将表现出不同的、略微更好的（我们希望是这样）行为。正如你从章节开始时应该记得的那样，交叉熵方法属于基于策略（on-policy）类，因此使用新鲜的训练数据对于方法的正常运行至关重要。
- en: 'Since training and data gathering happen in the same thread, proper synchronization
    isn’t necessary. However, you should be aware of the frequent jumps between training
    the NN and using it. Okay; now we need to define yet another function, and then
    we will be ready to switch to the training loop:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练和数据收集发生在同一线程中，因此不需要额外的同步。然而，您应该注意到训练神经网络和使用神经网络之间的频繁切换。好了；现在我们需要定义另一个函数，然后就可以准备切换到训练循环了：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This function is at the core of the cross-entropy method – from the given batch
    of episodes and percentile value, it calculates a boundary reward, which is used
    to filter elite episodes to train on. To obtain the boundary reward, we will use
    NumPy’s percentile function, which, from the list of values and the desired percentile,
    calculates the percentile’s value. Then, we will calculate the mean reward, which
    is used only for monitoring.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是交叉熵方法的核心——它从给定的回合批次和百分位值中计算一个奖励边界，这个边界用于过滤精英回合进行训练。为了获取奖励边界，我们将使用NumPy的`percentile`函数，它根据数值列表和所需的百分位，计算出该百分位的值。然后，我们将计算平均奖励，仅用于监控。
- en: 'Next, we will filter off our episodes:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将过滤掉我们的回合：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For every episode in the batch, we will check that the episode has a higher
    total reward than our boundary and, if it has, we will populate lists of observations
    and actions that we will train on.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批次中的每一个回合，我们会检查该回合的总奖励是否高于我们的奖励边界，如果是，我们将填充观察值和动作的列表，这些将用于训练。
- en: 'The following is the final step of the function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该函数的最后步骤：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we will convert our observations and actions from elite episodes into
    tensors, and return a tuple of four: observations, actions, the boundary of reward,
    and the mean reward. The last two values are not used in the training; we will
    write them into TensorBoard to check the performance of our agent.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将把精英回合的观察值和动作转换成张量，并返回一个包含四个元素的元组：观察值、动作、奖励边界和平均奖励。最后两个值不用于训练；我们将它们写入TensorBoard，以便检查智能体的表现。
- en: 'Now, the final chunk of code that glues everything together, and mostly consists
    of the training loop, is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，整合所有内容的最终代码块，主要由训练循环组成，如下所示：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the beginning, we create all the required objects: the environment, our
    NN, the objective function, the optimizer, and the summary writer for TensorBoard.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，我们创建所有需要的对象：环境、我们的神经网络、目标函数、优化器，以及TensorBoard的摘要写入器。
- en: 'In the training loop, we iterate our batches (a list of Episode objects):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环中，我们迭代处理批次（即Episode对象的列表）：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We perform filtering of the elite episodes using the filter_batch function.
    The result is tensors of observations and taken actions, the reward boundary used
    for filtering, and the mean reward. After that, we zero the gradients of our NN
    and pass observations to the NN, obtaining its action scores. These scores are
    passed to the objective function, which will calculate the cross-entropy between
    the NN output and the actions that the agent took. The idea of this is to reinforce
    our NN to carry out those elite actions that have led to good rewards. Then, we
    calculate gradients on the loss and ask the optimizer to adjust our NN.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`filter_batch`函数对精英回合进行过滤。结果是观察值和采取的动作的张量、用于过滤的奖励边界，以及平均奖励。之后，我们将神经网络（NN）的梯度归零，并将观察值传递给神经网络，获取其动作分数。这些分数会传递给目标函数，计算神经网络输出与智能体采取的动作之间的交叉熵。这样做的目的是强化我们的神经网络，执行那些已经导致良好奖励的精英动作。接着，我们计算损失的梯度，并请求优化器调整神经网络。
- en: 'The rest of the loop is mostly the monitoring of progress:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 循环的其余部分主要是进度监控：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: On the console, we show the iteration number, the loss, the mean reward of the
    batch, and the reward boundary. We also write the same values to TensorBoard to
    get a nice chart of the agent’s learning performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台上，我们显示迭代次数、损失、批次的平均奖励以及奖励边界。我们还将相同的值写入TensorBoard，以便获得智能体学习表现的漂亮图表。
- en: 'The last check in the loop is the comparison of the mean rewards of our batch
    episodes:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 循环中的最后一个检查是比较批次回合的平均奖励：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When the mean reward becomes greater than 475, we stop our training. Why 475?
    In Gym, the CartPole-v1 environment is considered to be solved when the mean reward
    for the last 100 episodes is greater than 475\. However, our method converges
    so quickly that 100 episodes are usually what we need. The properly trained agent
    can balance the stick for an infinitely long period of time (obtaining any amount
    of score), but the length of an episode in CartPole-v1 is limited to 500 steps
    (if you look in [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/\_\_init\_\_.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/)(gymnasium/envs/__init__.py)
    where all the environments are registered, v1 of Cartpole has max_episode_steps=500).
    With all this in mind, we will stop training after the mean reward in the batch
    is greater than 475, which is a good indication that our agent knows how to balance
    the stick like a pro.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当平均奖励超过 475 时，我们停止训练。为什么是 475 呢？在 Gym 中，当过去 100 次训练的平均奖励超过 475 时，CartPole-v1
    环境被认为已解决。然而，我们的方法收敛得非常快，通常 100 次训练就足够了。经过适当训练的智能体能够将杆子保持平衡无限长时间（获得任意数量的分数），但在
    CartPole-v1 中，一次训练的长度被限制为 500 步（如果你查看 [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/__init__.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/)(gymnasium/envs/__init__.py)
    文件，所有环境都在此注册，CartPole v1 的 `max_episode_steps` 为 500）。考虑到这些因素，当批次的平均奖励超过 475 时，我们就会停止训练，这也是智能体学会像专业人士一样平衡杆子的良好指示。
- en: That’s it. So, let’s start our first RL training!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。那么，让我们开始第一次强化学习训练吧！
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It usually doesn’t take the agent more than 50 batches to solve the problem.
    My experiments show something from 30 to 60 episodes, which is a really good learning
    performance (remember, we need to play only 16 episodes for every batch). TensorBoard
    shows that our agent is consistently making progress, pushing the upper boundary
    at almost every batch (there are some periods of rolling down, but most of the
    time it improves):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，智能体解决问题的训练批次不会超过 50 次。我的实验显示，通常需要 30 到 60 次训练，这是一种非常好的学习表现（记住，我们每个批次只需要训练
    16 次）。TensorBoard 显示我们的智能体持续在进步，几乎每个批次都会推动上限（虽然有时会出现下降，但大多数时候它是在提升）：
- en: '![PIC](img/B22150_04_03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_04_03.png)'
- en: 'Figure 4.3: Mean reward (left) and loss (right) during the training![PIC](img/B22150_04_04.png)
    Figure 4.4: The reward boundary during the training'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：训练过程中平均奖励（左）和损失（右）![PIC](img/B22150_04_04.png) 图 4.4：训练过程中奖励的边界
- en: 'To monitor the training process, you can tweak the environment creation by
    setting the rendering mode in the CartPole environment and adding a RecordVideo
    wrapper:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控训练过程，你可以通过在 CartPole 环境中设置渲染模式并添加 RecordVideo 包装器来调整环境创建：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'During the training, it will create a video directory with a bunch of MP4 movies
    inside, allowing you to compare the progress of agent training:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，它将创建一个视频目录，其中包含一堆 MP4 电影，供你比较智能体训练的进展：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The MP4 movies might look like the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: MP4 电影可能如下所示：
- en: '![PIC](img/file16.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file16.png)'
- en: 'Figure 4.5: CartPole episode movie'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：CartPole 训练电影
- en: Let’s now pause for a bit and think about what’s just happened. Our NN has learned
    how to play the environment purely from observations and rewards, without any
    interpretation of observed values. The environment could easily not be a cart
    with a stick; it could be, say, a warehouse model with product quantities as an
    observation and money earned as the reward. Our implementation doesn’t depend
    on environment-related details. This is the beauty of the RL model and, in the
    next section, we will look at how exactly the same method can be applied to a
    different environment from the Gym collection.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们暂停一下，思考一下刚刚发生了什么。我们的神经网络仅通过观察和奖励学习如何玩这个环境，而没有对观察到的值进行任何解释。这个环境可以不是一个带杆的小车，它可以是一个仓库模型，观察值是产品数量，奖励是赚取的金钱。我们的实现并不依赖于环境相关的细节。这就是强化学习模型的魅力，接下来的部分，我们将看看如何将完全相同的方法应用于
    Gym 集合中的不同环境。
- en: The cross-entropy method on FrozenLake
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 FrozenLake 上使用交叉熵方法
- en: 'The next environment that we will try to solve using the cross-entropy method
    is FrozenLake. Its world is from the so-called grid world category, when your
    agent lives in a grid of size 4 × 4 and can move in four directions: up, down,
    left, and right. The agent always starts at the top left, and its goal is to reach
    the bottom-right cell of the grid. There are holes in the fixed cells of the grid
    and if you get into those holes, the episode ends and your reward is zero. If
    the agent reaches the destination cell, then it obtains a reward of 1.0 and the
    episode ends.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试使用交叉熵方法解决的环境是 FrozenLake。它的世界属于所谓的网格世界类别，在这个世界中，代理生活在一个 4 × 4 的网格中，可以朝四个方向移动：上、下、左、右。代理始终从左上角开始，目标是到达网格的右下角单元格。网格中的固定单元格中有洞，如果代理掉入这些洞中，情节结束，奖励为零。如果代理到达目标单元格，则获得
    1.0 的奖励，情节也结束。
- en: To make life more complicated, the world is slippery (it’s a frozen lake after
    all), so the agent’s actions do not always turn out as expected – there is a 33%
    chance that it will slip to the right or to the left. If you want the agent to
    move left, for example, there is a 33% probability that it will, indeed, move
    left, a 33% chance that it will end up in the cell above, and a 33% chance that
    it will end up in the cell below. As you will see at the end of the section, this
    makes progress difficult.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让事情变得更复杂，世界是滑溜的（毕竟它是一个冰冻的湖泊），因此代理的动作并不总是按预期进行——有 33% 的机会它会向右或向左滑动。例如，如果你希望代理向左移动，那么有
    33% 的概率它确实会向左移动，33% 的概率它会移到上方的单元格，另有 33% 的概率它会移到下方的单元格。正如你在本节最后所看到的，这使得进展变得困难。
- en: '![PIC](img/file17.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file17.png)'
- en: 'Figure 4.6: The FrozenLake environment rendered in human mode'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：在人工模式下渲染的 FrozenLake 环境
- en: 'Let’s look at how this environment is represented in the Gym API:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个环境在 Gym API 中是如何表示的：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our observation space is discrete, which means that it’s just a number from
    0 to 15 inclusive. Obviously, this number is our current position in the grid.
    The action space is also discrete, but it can be from zero to three. Although
    the action space is similar to CartPole, the observation space is represented
    in a different way. To minimize the required changes in our implementation, we
    can apply the traditional one-hot encoding of discrete inputs, which means that
    the input to our network will have 16 float numbers and zero everywhere, except
    the index that we will encode (representing our current position on the grid).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的观察空间是离散的，这意味着它只是一个从 0 到 15 的数字（包括 0 和 15）。显然，这个数字是我们在网格中的当前位置。动作空间也是离散的，但它的值可以从零到三。虽然动作空间与
    CartPole 类似，但观察空间的表示方式不同。为了尽量减少我们实现中的所需更改，我们可以应用传统的离散输入的 one-hot 编码，这意味着输入到我们网络的数据将包含
    16 个浮动数，其他位置为零，只有表示我们在网格中的当前位置的索引处为 1。
- en: 'As this transformation only affects the observation of the environment, it
    could be implemented as an ObservationWrapper, as we discussed in Chapter [2](ch006.xhtml#x1-380002).
    Let’s call it DiscreteOneHotWrapper:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种转换仅影响环境的观察，因此可以将其实现为一个 ObservationWrapper，正如我们在第 [2](ch006.xhtml#x1-380002)
    章中讨论的那样。我们将其称为 DiscreteOneHotWrapper：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With that wrapper applied to the environment, both the observation space and
    action space are 100% compatible with our CartPole solution (source code Chapter04/02_frozenlake_naive.py).
    However, by launching it, we can see that our training process doesn’t improve
    the score over time:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在对环境应用了该包装器后，观察空间和动作空间与我们的 CartPole 解决方案（源代码 Chapter04/02_frozenlake_naive.py）100%
    兼容。然而，通过启动它，我们可以看到我们的训练过程并没有随着时间的推移提高分数：
- en: '![PIC](img/B22150_04_07.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_04_07.png)'
- en: 'Figure 4.7: Mean reward (left) and loss (right) on the FrozenLake environment'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：FrozenLake 环境中的平均奖励（左）和损失（右）
- en: '![PIC](img/B22150_04_08.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_04_08.png)'
- en: 'Figure 4.8: The reward boundary during the training (boring 0.0 all the time)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：训练过程中奖励边界（一直是无聊的 0.0）
- en: 'To understand what’s going on, we need to look deeper at the reward structure
    of both environments. In CartPole, every step of the environment gives us the
    reward 1.0, until the moment that the pole falls. So, the longer our agent balanced
    the pole, the more reward it obtained. Due to randomness in our agent’s behavior,
    different episodes were of different lengths, which gave us a pretty normal distribution
    of the episodes’ rewards. After choosing a reward boundary, we rejected less successful
    episodes and learned how to repeat better ones (by training on successful episodes’
    data). This is shown in the following diagram:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解发生了什么，我们需要深入研究两个环境的奖励结构。在CartPole中，环境的每一步都会给我们1.0的奖励，直到杆子倒下为止。因此，我们的代理平衡杆子的时间越长，获得的奖励就越多。由于代理行为的随机性，不同的回合有不同的长度，从而给我们带来了一个比较正常的回合奖励分布。选择奖励边界后，我们会拒绝不太成功的回合，并学习如何重复更好的回合（通过在成功回合的数据上训练）。这一点可以通过下面的图示看到：
- en: '![PIC](img/file21.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file21.png)'
- en: 'Figure 4.9: Distribution of the reward in the CartPole environment'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：CartPole环境中的奖励分布
- en: In the FrozenLake environment, episodes and their rewards look different. We
    get the reward of 1.0 only when we reach the goal, and this reward says nothing
    about how good each episode was. Was it quick and efficient, or did we make four
    rounds on the lake before we randomly stepped into the final cell? We don’t know;
    it’s just a 1.0 reward and that’s it. The distribution of rewards for our episodes
    is also problematic. There are only two kinds of episodes possible, with zero
    reward (failed) and one reward (successful), and failed episodes will obviously
    dominate at the beginning of the training, when the agent acts randomly. So, our
    percentile selection of elite episodes is totally wrong and gives us bad examples
    to train on. This is the reason for our training failure.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在FrozenLake环境中，回合和奖励的情况有所不同。我们只有在到达目标时才能获得1.0的奖励，而这个奖励并不能说明每个回合的好坏。这个回合是快速有效的，还是我们在湖上绕了四圈后才随机进入最终的格子？我们并不知道；它只是一个1.0的奖励，仅此而已。我们的回合奖励分布也存在问题。只有两种可能的回合，一种是零奖励（失败），另一种是奖励为1.0（成功），而失败的回合显然会在训练开始时占据主导地位，因为这时代理的行为是随机的。因此，我们选择精英回合的百分位数是完全错误的，这会给我们提供不良的训练样本。这就是我们训练失败的原因。
- en: '![PIC](img/file22.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file22.png)'
- en: 'Figure 4.10: Reward distribution of the FrozenLake environment'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：FrozenLake环境的奖励分布
- en: 'This example shows us the limitations of the cross-entropy method:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了交叉熵方法的局限性：
- en: For training, our episodes have to be finite (in general, they could be infinite)
    and, preferably, short
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练来说，我们的过程必须是有限的（通常它们可以是无限的），并且最好是短暂的。
- en: The total reward for the episodes should have enough variability to separate
    good episodes from bad ones
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回合的总奖励应当有足够的变异性，以便将好的回合与不好的回合区分开来。
- en: It is beneficial to have an intermediate reward during the episode instead of
    having the reward at the end of the episode
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个过程中有中间奖励是有益的，而不是只在过程结束时获得奖励。
- en: 'Later in the book, you will become familiar with other methods that address
    these limitations. For now, if you are curious about how FrozenLake can be solved
    using the cross-entropy method, here is a list of tweaks of the code that you
    need to make (the full example is in Chapter04/03_frozenlake_tweaked.py):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后续章节中，你将了解其他解决这些局限性的方法。目前，如果你对如何使用交叉熵方法解决FrozenLake问题感兴趣，以下是你需要对代码进行的调整（完整示例见Chapter04/03_frozenlake_tweaked.py）：
- en: 'Larger batches of played episodes: In CartPole, it was sufficient to have 16
    episodes on every iteration, but FrozenLake requires at least 100 just to get
    some successful episodes.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的回合批量：在CartPole中，每次迭代有16个回合就足够了，但FrozenLake至少需要100个回合才能得到一些成功的回合。
- en: 'Discount factor applied to the reward: To make the total reward for an episode
    depend on its length, and to add variety in episodes, we can use a discounted
    total reward with the discount factor γ = 0.9 or 0.95\. In this case, the reward
    for short episodes will be higher than the reward for long ones. This increases
    variability in reward distribution, which helps to avoid situations like the one
    shown in Figure [4.10](#x1-78034r10).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励的折扣因子：为了让一个回合的总奖励依赖于其长度，并且增加回合的多样性，我们可以使用折扣总奖励，折扣因子γ = 0.9或0.95。在这种情况下，短回合的奖励会高于长回合的奖励。这增加了奖励分布的变异性，有助于避免像图[4.10](#x1-78034r10)中所示的情况。
- en: 'Keeping elite episodes for a longer time: In the CartPole training, we sampled
    episodes from the environment, trained on the best ones, and threw them away.
    In FrozenLake, a successful episode is a much rarer animal, so we need to keep
    them for several iterations to train on them.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长时间保存精英回合：在 CartPole 训练中，我们从环境中采样回合，训练最好的回合，然后丢弃它们。而在 FrozenLake 中，成功回合是非常稀有的，因此我们需要将其保留多个迭代以进行训练。
- en: 'Decreasing the learning rate: This will give our NN time to average more training
    samples, as a smaller learning rate decreases the effect of new data on the model.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低学习率：这将给我们的神经网络更多时间来平均更多的训练样本，因为较小的学习率会减小新数据对模型的影响。
- en: 'Much longer training time: Due to the sparsity of successful episodes and the
    random outcome of our actions, it’s much harder for our NN to get an idea of the
    best behavior to perform in any particular situation. To reach 50% successful
    episodes, about 5,000 training iterations are required.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更长的训练时间：由于成功回合的稀疏性以及我们行动的随机性，我们的神经网络（NN）更难理解在任何特定情况下应该执行的最佳行为。为了达到 50% 的成功回合，约需要
    5,000 次训练迭代。
- en: 'To incorporate all these into our code, we need to change the filter_batch
    function to calculate the discounted reward and return elite episodes for us to
    keep:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些内容融入到我们的代码中，我们需要修改 filter_batch 函数来计算折扣奖励并返回精英回合以供我们保存：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, in the training loop, we will store previous elite episodes to pass them
    to the preceding function on the next training iteration:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练循环中，我们将存储先前的精英回合，并在下次训练迭代中将其传递给前面的函数：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The rest of the code is the same, except that the learning rate decreased 10
    times and the BATCH_SIZE was set to 100\. After a period of patient waiting (the
    new version takes about 50 minutes to finish 10,000 iterations), you can see that
    the training of the model stopped improving at around 55% of solved episodes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码保持不变，除了学习率降低了 10 倍，BATCH_SIZE 设置为 100。经过一段耐心等待（新版本大约需要 50 分钟来完成 10,000
    次迭代），你可以看到模型的训练在约 55% 已解决的回合后停止了提升：
- en: '![PIC](img/B22150_04_11.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_04_11.png)'
- en: 'Figure 4.11: Mean reward (left) and loss (right) of the tweaked version'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：调整版本的平均奖励（左）和损失（右）
- en: '![PIC](img/B22150_04_12.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_04_12.png)'
- en: 'Figure 4.12: The reward boundary of the tweaked version'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：调整版本的奖励边界
- en: There are ways to address this (by applying entropy loss regularization, for
    example), but those techniques will be discussed in upcoming chapters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有方法可以解决这个问题（例如，通过应用熵损失正则化），但这些技术将在接下来的章节中讨论。
- en: The final point to note here is the effect of slipperiness in the FrozenLake
    environment. Each of our actions, with 33% probability, is replaced with the 90^∘
    rotated action (the up action, for instance, will succeed with a 0.33 probability,
    and there will be a 0.33 chance that it will be replaced with the left action
    and a 0.33 chance with the right action).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最后需要注意的是 FrozenLake 环境中的滑溜效应。我们的每个行动有 33% 的概率被替换为 90^∘ 旋转后的行动（例如，向上行动会以 0.33
    的概率成功，而有 0.33 的概率它会被替换为向左行动或向右行动）。
- en: 'The nonslippery version is in Chapter04/04_frozenlake_nonslippery.py, and the
    only difference is in the environment creation:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 无滑溜版本的代码在 Chapter04/04_frozenlake_nonslippery.py 中，唯一的不同是在环境创建时：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The effect is dramatic! The nonslippery version of the environment can be solved
    in 120-140 batch iterations, which is 100 times faster than the noisy environment:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 效果显著！无滑溜版本的环境可以在 120-140 个批次迭代内解决，比噪声环境快了 100 倍：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This is also evident from the following graphs:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在以下图表中也很明显：
- en: '![PIC](img/B22150_04_13.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_04_13.png)'
- en: 'Figure 4.13: Mean reward (left) and loss (right) of the nonslippery version'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13：无滑溜版本的平均奖励（左）和损失（右）
- en: '![PIC](img/B22150_04_14.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_04_14.png)'
- en: 'Figure 4.14: The reward boundary of the nonslippery version'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14：无滑溜版本的奖励边界
- en: The theoretical background of the cross-entropy method
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵方法的理论背景
- en: This section is optional and is included for readers who want to understand
    why the method works. If you wish, you can refer to the original paper by Kroese,
    titled Cross-entropy method, [[Kro+11](#)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本节为可选内容，供希望了解该方法为何有效的读者。如果你愿意，可以参考 Kroese 原文论文，标题为《交叉熵方法》，[[Kro+11](#)]。
- en: 'The basis of the cross-entropy method lies in the importance sampling theorem,
    which states this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵方法的基础在于重要性采样定理，定理内容如下：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq4.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq5.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq4.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq5.png)'
- en: 'In our RL case, H(x) is a reward value obtained by some policy x, and p(x)
    is a distribution of all possible policies. We don’t want to maximize our reward
    by searching all possible policies; instead, we want to find a way to approximate
    p(x)H(x) by q(x), iteratively minimizing the distance between them. The distance
    between two probability distributions is calculated by Kullback-Leibler (KL) divergence,
    which is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的强化学习（RL）案例中，H(x) 是某个策略 x 所获得的奖励值，p(x) 是所有可能策略的分布。我们并不想通过搜索所有可能的策略来最大化我们的奖励；相反，我们想通过
    q(x) 来近似 p(x)H(x)，并迭代地最小化它们之间的距离。两个概率分布之间的距离通过 Kullback-Leibler (KL) 散度来计算，公式如下：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq6.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq7.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq6.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq7.png)'
- en: The first term in KL is called entropy and it doesn’t depend on p[2](x), so
    it could be omitted during the minimization. The second term is called cross-entropy,
    which is a very common optimization objective in deep learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: KL 中的第一个项称为熵，它与 p[2](x) 无关，因此在最小化过程中可以省略。第二项称为交叉熵，这是深度学习中非常常见的优化目标。
- en: 'Combining both formulas, we can get an iterative algorithm, which starts with
    q[0](x) = p(x) and on every step improves. This is an approximation of p(x)H(x)
    with an update:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这两个公式，我们可以得到一个迭代算法，起始时 q[0](x) = p(x)，并在每一步进行改进。这是 p(x)H(x) 的近似，并伴随着更新：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq78.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq78.png)'
- en: 'This is a generic cross-entropy method that can be significantly simplified
    in our RL case. We replace our H(x) with an indicator function, which is 1 when
    the reward for the episode is above the threshold and 0 when the reward is below.
    Our policy update will look like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种通用的交叉熵方法，在我们的 RL 案例中可以大大简化。我们将 H(x) 替换为一个指示函数，当回合的奖励超过阈值时其值为 1，当奖励低于阈值时其值为
    0。我们的策略更新将如下所示：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq79.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq79.png)'
- en: 'Strictly speaking, the preceding formula misses the normalization term, but
    it still works in practice without it. So, the method is quite clear: we sample
    episodes using our current policy (starting with some random initial policy) and
    minimize the negative log likelihood of the most successful samples and our policy.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，前面的公式缺少归一化项，但在实践中没有它仍然能起作用。所以，方法非常明确：我们使用当前策略（从一些随机初始策略开始）采样回合，并最小化最成功样本和我们的策略的负对数似然。
- en: If you are interested, refer to the book written by Reuven Rubinstein and Dirk
    P. Kroese [[RK04](#)] that is dedicated to this method. A shorter description
    can be found in the Cross-entropy method paper ([[Kro+11](#)]).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，可以参考 Reuven Rubinstein 和 Dirk P. Kroese 编写的书 [[RK04](#)]，专门讨论这种方法。简短的描述可以在《交叉熵方法》论文中找到
    ([[Kro+11](#)])。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you became familiar with the cross-entropy method, which is
    simple but quite powerful, despite its limitations. We applied it to a CartPole
    environment (with huge success) and to FrozenLake (with much more modest success).
    In addition, we discussed the taxonomy of RL methods, which will be referenced
    many times during the rest of the book, as different approaches to RL problems
    have different properties, which influences their applicability.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经了解了交叉熵方法，尽管它有一些局限性，但它简单且非常强大。我们将其应用于一个 CartPole 环境（取得了巨大的成功）和 FrozenLake（取得了相对较小的成功）。此外，我们还讨论了
    RL 方法的分类，接下来的书中会多次引用这一分类，因为不同的 RL 问题方法具有不同的特性，这会影响它们的适用性。
- en: This chapter ends the introductory part of the book. In the next part, we will
    switch to a more systematic study of RL methods and discuss the value-based family
    of methods. In upcoming chapters, we will explore more complex, but more powerful,
    tools of deep RL.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了本书的导言部分。在下一部分，我们将转向更加系统地学习 RL 方法，并讨论基于值的算法。在接下来的章节中，我们将探索更复杂但更强大的深度强化学习工具。
- en: Join our community on Discord
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在 Discord 上的社区
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、深度学习专家以及作者本人一起阅读本书。提出问题，为其他读者提供解决方案，通过问我任何问题环节与作者交流，还有更多内容。扫描二维码或访问链接加入社区。[https://packt.link/rl](https://packt.link/rl)
- en: '![PIC](img/file1.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
- en: Part 2
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分
- en: Value-based methods
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于价值的方法
