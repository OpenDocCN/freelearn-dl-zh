- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Solving Regression Problems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求解回归问题
- en: In the previous chapters, we learned how to set up and run MXNet, work with
    Gluon and DataLoaders, and visualize datasets for regression, classification,
    image, and text problems. We also discussed the different learning methodologies
    (supervised learning, unsupervised learning, and reinforcement learning). In this
    chapter, we are going to focus on supervised learning, where the expected outputs
    are known for at least some examples. Depending on the given type of these outputs,
    supervised learning can be decomposed into regression and classification. Regression
    outputs are numbers from a continuous distribution (such as predicting the stock
    price of a public company), whereas classification outputs are defined from a
    known set (for example, identifying whether an image corresponds to a mouse, a
    cat, or a dog).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何设置和运行 MXNet，如何使用 Gluon 和 DataLoaders，并且如何可视化回归、分类、图像和文本问题的数据集。我们还讨论了不同的学习方法（监督学习、无监督学习和强化学习）。在本章中，我们将专注于监督学习，其中至少对于某些示例，期望的输出是已知的。根据这些输出的类型，监督学习可以细分为回归和分类。回归输出是来自连续分布的数字（例如，预测一家上市公司的股票价格），而分类输出是从已知集合中定义的（例如，识别图像是鼠标、猫还是狗）。
- en: Classification problems can be seen as a subset of regression problems, and
    therefore, in this chapter, we will start working with the latter ones. We will
    learn why these problems are suitable for deep learning models with an overview
    of the equations that define these problems. We will learn how to create suitable
    models and how to train them, emphasizing the choice of hyperparameters. We will
    end each section by evaluating the models according to our data, as expected in
    supervised learning, and we will see the different evaluation criteria for regression
    problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题可以看作是回归问题的一个子集，因此，在本章中，我们将首先处理后者。我们将学习为什么这些问题适合深度学习模型，并概述定义这些问题的方程式。我们将学习如何创建合适的模型，并学习如何训练它们，重点介绍超参数的选择。我们将通过根据数据评估模型来结束每一节，这正如在监督学习中所期望的那样，我们将看到回归问题的不同评估标准。
- en: 'We will cover the following recipes in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下食谱：
- en: Understanding the math of regression models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解回归模型的数学
- en: Defining loss functions and evaluation metrics for regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义回归的损失函数和评估指标
- en: Training regression models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练回归模型
- en: Evaluating regression models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    are some of the additional requirements needed for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了《*前言*》中指定的技术要求外，本章还需要以下一些附加要求：
- en: Ensure that you have completed *Recipe, Installing MXNet, Gluon, GluonCV and
    GluonNLP* from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016) *Up and Running*
    *with MXNet*.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您已完成来自[*第1章*](B16591_01.xhtml#_idTextAnchor016)的*食谱，安装 MXNet、Gluon、GluonCV
    和 GluonNLP*，*与 MXNet 一起启动并运行*。
- en: 'Ensure that you have completed *Recipe 1, Toy dataset for regression – load,
    manage, and visualize a house sales dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您已经完成了来自[*第2章*](B16591_02.xhtml#_idTextAnchor029)的*食谱 1，回归的玩具数据集——加载、管理和可视化房屋销售数据集*，工作与
    MXNet 和可视化数据集：Gluon *和 DataLoader*。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub URL 找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03)。
- en: 'Furthermore, you can access each recipe directly from Google Colab, for example,
    for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以直接从 Google Colab 访问每个食谱，例如，本章的第一个食谱：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb)。
- en: Understanding the math of regression models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解回归模型的数学
- en: As we saw in the previous chapter, **regression** problems are a type of **supervised
    learning** problem whose output is a number from a continuous distribution, such
    as the price of a house or the predicted value of a company stock price.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章看到的，**回归**问题是一种**监督学习**问题，其输出是来自连续分布的一个数字，比如房价或公司股票价格的预测值。
- en: The simplest model we can use for a regression problem is a **linear regression**
    model. However, these models are extremely powerful for simple problems, as their
    parameters can be trained and are very fast and explainable, given the small number
    of parameters involved. As we will see, this number of parameters is completely
    dependent on the number of features we use.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来解决回归问题的最简单模型是**线性回归**模型。然而，这些模型对于简单问题非常强大，因为它们的参数可以训练，并且在涉及的参数数量较少的情况下，速度很快且易于解释。正如我们将看到的，参数的数量完全取决于我们使用的特征数量。
- en: Another interesting property of linear regression models is that they can be
    represented by neural networks, and as neural networks will be the basis for most
    models that we will be using throughout the book, this is the linear regression
    model based on neural networks that we will be using.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型的另一个有趣属性是，它们可以通过神经网络来表示，而神经网络将是我们在本书中使用的大多数模型的基础，因此我们将使用这种基于神经网络的线性回归模型。
- en: The simplest neural network model is known as the **Perceptron**, and this is
    the building block we will be working on not only in this recipe, but throughout
    the whole chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的神经网络模型被称为**感知机**，这是我们不仅在本食谱中，而是在整个章节中要研究的基础模块。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Before jumping in to understand our model, let me mention that for the math
    part of this recipe, we will encounter a little bit of matrix operations and linear
    algebra, but it will not be hard at all.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入理解我们的模型之前，我想提一下，对于本食谱中的数学部分，我们将遇到一些矩阵运算和线性代数，但这绝对不会很难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will work through the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将通过以下步骤进行操作：
- en: Modeling a biological neuron mathematically
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数学建模生物神经元
- en: Defining a regression model
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义回归模型
- en: Describing basic activation functions
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述基本激活函数
- en: Defining the features
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义特征
- en: Initializing the model
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型
- en: Evaluating the model
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Modeling a biological neuron mathematically
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学建模生物神经元
- en: The Perceptron was first introduced by American psychologist Frank Rosenblatt
    in 1958 at Cornell Aeronautical Laboratory, and it was an initial attempt at replicating
    how information is processed by the neurons in our brain.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机最早由美国心理学家Frank Rosenblatt于1958年在康奈尔航空实验室提出，它是初步尝试复制我们大脑中神经元处理信息的方式。
- en: Rosenblatt analyzed a biological neuron and developed a mathematical model that
    behaved similarly. To make a comparison between these architectures, we will start
    with a very simple model of a neuron.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt分析了生物神经元并开发了一个行为相似的数学模型。为了比较这些架构，我们将从一个非常简单的神经元模型开始。
- en: '![Figure 3.1 – Biological neuron](img/B16591_03_1.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 生物神经元](img/B16591_03_1.jpg)'
- en: Figure 3.1 – Biological neuron
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 生物神经元
- en: 'As we can see in *Figure 3**.1*, the neuron is composed of three main parts:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图 3.1*中看到的，神经元由三个主要部分组成：
- en: '**Dendrites**: Where the neuron receives inputs from other neurons. Depending
    on how strong the connection is, an input will be increased or decreased in the
    dendrites.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树突**：神经元从其他神经元接收输入的地方。根据连接的强度，输入在树突中会被增强或减弱。'
- en: '**Cell Body** or **Soma**: Contains the nucleus, which is the structure that
    receives all the inputs from the dendrites and processes them. The nucleus might
    trigger an electrical message to be communicated to other neurons.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细胞体**或**胞体**：包含细胞核，这是接收来自树突的所有输入并对其进行处理的结构。细胞核可能会触发电信号并传递给其他神经元。'
- en: '**Axon/Axon Terminals**: This is the output structure, which communicates messages
    with other neurons.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突/轴突末端**：这是输出结构，用于与其他神经元传递信息。'
- en: 'Rosenblatt took the preceding simplified model of the neuron and assigned to
    it certain mathematical properties:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt将之前简化的神经元模型赋予了某些数学属性：
- en: '![Figure 3.2 – Perceptron](img/B16591_03_2.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 感知机](img/B16591_03_2.jpg)'
- en: Figure 3.2 – Perceptron
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 感知机
- en: '**Weights**: This will simulate the behavior of dendrites by multiplying the
    inputs or features with a set of weights (*W* in *Figure 3**.2*, while *j* refers
    to any neuron in the model).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：这将通过将输入或特征与一组权重（*W* 在 *图3.2* 中，而 *j* 代表模型中的任何神经元）相乘来模拟树突的行为。'
- en: '**Sum** and **bias**: The combination of input signals done in the nucleus
    will be modeled as a sum with a bias (*θ* in *Figure 3**.2*) and a processing
    function, called the **activation function**. (We will describe these functions
    in the following step.)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**和** 和 **偏置**：在细胞核中输入信号的组合将被建模为带有偏置的求和（*θ* 在 *图3.2* 中）和一个处理函数，称为 **激活函数**。（我们将在下一步描述这些函数。）'
- en: '**Output**: Either connections to other neurons, or the direct output of the
    whole model (*o* in *Figure 3**.2*).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：要么连接到其他神经元，要么是整个模型的直接输出（*o* 在 *图3.2* 中）。'
- en: Comparing *Figure 3**.1* and *Figure 3**.2*, we can see the similarities between
    the simplified model of the biological neuron and the Perceptron. Furthermore,
    we can also see how all these parts are connected together, from processing the
    input to delivering the output.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 *图3.1* 和 *图3.2*，我们可以看到生物神经元的简化模型与感知器之间的相似之处。此外，我们还可以看到这些部分是如何连接在一起的，从处理输入到输出结果。
- en: Defining a regression model
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义回归模型
- en: Therefore, from a mathematical perspective using matrix multiplication, we can
    write the following equations for the model *y = f(W*⋅*X + b)*, where *W* is the
    weight vector *[W*1*, W*2*, …. W*n*]*, (n is the number of features), *X* is the
    feature vector *[X*1*, X*2*, …. X*n*]*, *b* is the bias term, and *f()* is the
    activation function. For the regression case we will be dealing with, we will
    work with a linear activation function, where the output is equal to the input.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从数学角度来看，使用矩阵乘法，我们可以写出以下模型方程 *y = f(W*⋅*X + b)*，其中 *W* 是权重向量 *[W*1*, W*2*,
    …. W*n*]*，（n是特征的数量），*X* 是特征向量 *[X*1*, X*2*, …. X*n*]*，*b* 是偏置项，*f()* 是激活函数。对于我们将处理的回归情况，我们将使用线性激活函数，其中输出等于输入。
- en: Therefore, in our case, the activation function is the identity function (output
    equal to the input), we have *y = W**⋅**X +* *b*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的例子中，激活函数是恒等函数（输出等于输入），我们有 *y = W**⋅**X +* *b*。
- en: 'We can easily achieve this with MXNet and its NDArray library:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MXNet及其NDArray库轻松实现这一点：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And that’s it! That’s our *y* neural network output in terms of the *X* inputs
    and the *W* and *b* parameters of the model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这就是我们基于 *X* 输入以及模型的 *W* 和 *b* 参数的 *y* 神经网络输出。
- en: Important Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the original paper by Rosenblatt, the expected output was either 0 or 1 (classification
    problem) and to fulfill this requirement, the activation function was defined
    as the step function (0 if the input is smaller than 0, or 1 if the input is larger
    than or equal to 0). This was one of the strongest limitations to Rosenblatt’s
    neuron model, and different activation functions were proposed later that improve
    the behavior of the model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在Rosenblatt的原始论文中，期望的输出是0或1（分类问题），为了满足这一要求，激活函数被定义为阶跃函数（如果输入小于0，则输出0；如果输入大于或等于0，则输出1）。这是Rosenblatt神经元模型的一个最大限制，后来提出了不同的激活函数来改善模型的表现。
- en: In deep learning networks, we do not use a single neurons (Perceptrons) as our
    model. Typically, several layers of Perceptrons are stacked together, and the
    number of layers is also known as the **depth** of the network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习网络中，我们不会仅使用单个神经元（感知器）作为模型。通常，多个感知器层被堆叠在一起，层数也被称为网络的 **深度**。
- en: '![Figure 3.3 – Deep learning network](img/B16591_03_3.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 深度学习网络](img/B16591_03_3.jpg)'
- en: Figure 3.3 – Deep learning network
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 深度学习网络
- en: These networks are quite powerful and have been proven to match or surpass human-level
    performance in several fields, including image recognition in computer vision
    and sentiment analysis in natural language processing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络非常强大，并且已经证明在多个领域匹配或超越了人类水平的表现，包括计算机视觉中的图像识别和自然语言处理中的情感分析。
- en: Describing basic activation functions
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述基本激活函数
- en: The most common activation functions for regression problems are the linear
    activation function and the **ReLU** activation function. Let’s briefly describe
    them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题中最常见的激活函数是线性激活函数和 **ReLU** 激活函数。我们简要描述一下它们。
- en: '![Figure 3.4 – Activation functions for regression](img/B16591_03_4.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 回归的激活函数](img/B16591_03_4.jpg)'
- en: Figure 3.4 – Activation functions for regression
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 回归的激活函数
- en: Linear Activation Function
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: In this function, the output is equal to the input. It is not bounded, and therefore
    it is suitable for unbounded numerical outputs, as is the case with the output
    for the regression problems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数中，输出等于输入。它没有界限，因此适用于无界的数值输出，正如回归问题的输出所示。
- en: ReLU
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU
- en: 'The **Rectified Linear Unit** activation function is very similar to the linear
    activation function: its output is equal to the input, but in this case, only
    when the input is larger than 0; the output is 0 otherwise. This function is suitable
    to only pass positive information to the next layers (sparse activation), and
    also provides better gradient propagation. Therefore its use is very common in
    intermediate layers on deep learning networks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**激活函数非常类似于线性激活函数：其输出等于输入，但在这种情况下，只有当输入大于0时，输出才等于输入；否则输出为0。此函数适用于仅将正信息传递到下一层（稀疏激活），并且还提供更好的梯度传播。因此，它在深度学习网络的中间层中非常常见。'
- en: Important Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: As we will see in the next recipes, training involves computing iteratively
    new gradients and using these computations to update the model parameters. When
    using activation functions such as the sigmoid, the larger the number of layers,
    the smaller the gradient becomes. This problem is known as the vanishing gradient
    problem, and the ReLU activation function behaves better in those scenarios.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在接下来的内容中看到的，训练涉及迭代地计算新的梯度，并使用这些计算来更新模型参数。当使用激活函数（如sigmoid）时，层数越多，梯度变得越小。这个问题被称为梯度消失问题，而ReLU激活函数在这种情况下表现得更好。
- en: Defining the features
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义特征
- en: So far, we have defined our model and its behavior theoretically, but we have
    not used our problem framing or our dataset to define it. In this section, we
    will start working at a more practical level.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从理论上定义了我们的模型及其行为，但我们尚未使用我们的任务框架或数据集来定义它。在本节中，我们将开始以更实际的方式进行工作。
- en: 'The next step in defining our model is to decide which features (inputs) we
    are going to work with. We will continue using the House Sales Dataset we met
    in *Recipe 1, Toy dataset for regression – load, manage, and visualize house sales
    dataset* in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*, Working with MXNet
    and Visualizing Datasets: Gluon and DataLoader*. This dataset contained data for
    21,613 houses, including prices and 19 input features. Although in our model we
    will work with all the input features, in the aforementioned recipe, we saw that
    the three non-correlated features that primarily contributed to the house price
    were as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型的下一步是决定我们将使用哪些特征（输入）。我们将继续使用在[*第2章*](B16591_02.xhtml#_idTextAnchor029)《与MXNet合作并可视化数据集：Gluon和DataLoader》中遇到的房屋销售数据集。这个数据集包含了21,613栋房屋的数据，包括价格和19个输入特征。虽然在我们的模型中我们将使用所有输入特征，但在上述配方中，我们看到对房价贡献最大的三个非相关特征如下：
- en: Square feet of living space
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 居住面积（平方英尺）
- en: Grade
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等级
- en: Number of bathrooms
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卫生间数量
- en: 'For an initial study, we will work with these three features. After selecting
    these features, if we show the first five houses, we will see the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步研究中，我们将使用这三个特征。选择这些特征后，如果我们显示前五个房屋的数据，我们将看到以下内容：
- en: '![Figure 3.5 – Filtered features for house prices](img/B16591_03_5.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 房屋价格的过滤特征](img/B16591_03_5.jpg)'
- en: Figure 3.5 – Filtered features for house prices
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 房屋价格的过滤特征
- en: One of the paths that we did not exploit when analyzing this dataset previously
    is that `grade` is not a continuous feature as the others are; its values come
    from a discrete set of values. This type of feature is called a categorical feature,
    and can be nominal or ordinal. A nominal feature is a class name – for example,
    we could have the architectural style of the house as a feature, and the values
    of that feature could be Victorian, Art Deco, Craftsman, and so on. Ordinal features,
    on the other hand, are class numbers. In our case, `grade` is an ordinal feature
    consisting of numerical values in an order (1 is worst, 13 is best).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前分析这个数据集时，我们没有利用的一条路径是，`grade`不像其他特征那样是连续的；它的值来自一个离散的集合。这种类型的特征被称为分类特征，可以是名义型的或有序型的。名义型特征是类名——例如，我们可以将房屋的建筑风格作为一个特征，该特征的值可以是维多利亚式、艺术装饰风格、工匠风格等。而有序型特征则是类编号。在我们的例子中，`grade`是一个有序特征，由按顺序排列的数值组成（1为最差，13为最好）。
- en: In both cases, categorical features can be represented numerically in different
    ways that will help our models better learn the relationship between that particular
    feature and the output. There are several approaches to dealing with categorical
    features. In this example, we are going to work with one of the simplest approaches,
    a **one-hot** **encoding** scheme.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，分类特征可以通过不同的方式以数字形式表示，这有助于我们的模型更好地学习特征与输出之间的关系。处理分类特征的方法有多种。在这个例子中，我们将使用最简单的方式之一——**独热编码**方案。
- en: Tip
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You will find more information on working with categorical data in the *There’s
    more* section at the end of this recipe.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本食谱末尾的*更多内容*部分找到有关处理分类数据的更多信息。
- en: 'With a one-hot encoding scheme, each of the categories is decomposed into its
    own feature and will be assigned a binary value accordingly. In our case, `grade`
    contains integer values from 1 to 13, and therefore, we add 13 new features to
    our input vector. Each of these new features will have a value of 0 or 1\. For
    example, for a house of grade 1, the feature vector looks as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码方案，每个类别都会被分解为它自己的特征，并相应地分配一个二进制值。在我们的例子中，`grade`包含从1到13的整数值，因此，我们向输入向量中添加了13个新特征。这些新特征的值将为0或1。例如，对于一个1级的房子，特征向量如下所示：
- en: '![Figure 3.6 – One-hot encoding for grade feature](img/B16591_03_6.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 – 等级特征的独热编码](img/B16591_03_6.jpg)'
- en: Figure 3.6 – One-hot encoding for grade feature
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – 等级特征的独热编码
- en: If we take a close look at *Figure 3**.6*, we will see that there is no one-hot
    encoding for the grade column corresponding to the value 2\. This is because no
    actual house in our dataset had that grade, and therefore, it has not been added
    as a feature.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细查看*图3.6*，我们会发现没有对应于值2的独热编码。这是因为我们的数据集中没有实际房子的等级为2，因此没有将其作为特征添加。
- en: Therefore, the final number of features is 14 and we have 1 output, the price
    of the house.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的特征数量为14个，输出为1个，即房子的价格。
- en: Initializing the model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化模型
- en: 'Now that we have defined the input (the features) and output dimensions, we
    can initialize our model. We will explore this in more detail in the next recipes,
    but it is useful to show a glimpse of how this would look:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了输入（特征）和输出的维度，我们可以初始化模型。我们将在下一部分更详细地探讨这个问题，但展示一下它的大致样子还是很有用的：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As expected, the `Weights` vector has 14 components (the number of features)
    and the `Bias` vector has 1 component.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，`Weights`向量有14个分量（即特征数量），而`Bias`向量只有1个分量。
- en: Evaluating the model
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now that our model is initialized, we can use it to estimate the price of the
    first house, which can be seen in *Figure 3**.6* to be around 2.2 million dollars.
    With our current model, the estimated house price is (in $):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已初始化，我们可以用它来估算第一套房子的价格，正如*图3.6*所示，估计价格大约为220万美元。在我们当前的模型下，估算的房价为（单位：美元）：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we have the expected price, we can compute some error metrics. In this case,
    I have chosen the absolute error and the error relative to the actual price. These
    quantities can be easily computed in Python:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有预期的价格，我们可以计算一些误差指标。在本例中，我选择了绝对误差和相对于实际价格的误差。这些量可以很容易地在Python中计算：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The values obtained for the errors are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的误差值如下：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, 2.6k dollars is a very small price for a house of 1,180 sqft,
    even though it only has 1 bathroom and is of an average grade (7). This means
    our error metrics have very large values suggesting a ~99% error rate. This means
    that either we are not evaluating our model properly (in this case, we just used
    1 value, we might have been unlucky) or we only used the initialized parameters
    that did not give us an accurate estimation. We need to improve our model parameters
    using a process called **training** to improve our evaluation metrics. We will
    explore these topics in detail in the next recipes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，2.6千美元对于一套面积为1,180平方英尺的房子来说是一个非常低的价格，即使它只有1个浴室，并且评级为中等（7）。这意味着我们的误差指标值非常大，暗示着大约99%的误差率。这表明，要么我们没有正确评估我们的模型（在这种情况下，我们只使用了一个值，可能只是运气不好），要么我们只使用了初始化的参数，这些参数并没有给出准确的估计。我们需要通过一个称为**训练**的过程来改进我们的模型参数，以提高评估指标。我们将在下一部分详细探讨这些话题。
- en: How it works...
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Regression models can be as complex as the model designer wants. They can have
    as many layers as necessary to model adequately the relationships between the
    input features and the desired output values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型可以复杂到设计者所需的程度。它们可以拥有足够多的层次，以适当地建模输入特征和期望输出值之间的关系。
- en: In this recipe, we described how a biological neuron works inside our brain
    and simplified it to derive a simple mathematical model that we could use for
    our regression problem. In our case, we only used one layer, typically called
    the input layer, and we defined the weights and bias as its parameters.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们描述了大脑中生物神经元的工作原理，并简化它以推导出一个简单的数学模型，供我们用于回归问题。在我们的例子中，我们只使用了一个层，通常称为输入层，并将权重和偏置定义为其参数。
- en: Moreover, we learned how to initialize our model, explored the effect initialization
    has on the weights and bias, and saw how we can use our data to evaluate the model.
    We will develop all these topics further in the next recipes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们学习了如何初始化模型，探索了初始化对权重和偏置的影响，并了解了如何利用我们的数据来评估模型。我们将在接下来的食谱中进一步展开这些话题。
- en: There’s more…
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'In this recipe, we briefly introduced several topics. We started by describing
    Rosenblatt’s Perceptron. If you want to read the original paper, here is the link:
    [https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇食谱中，我们简要介绍了几个话题。我们首先描述了罗森布拉特的感知器。如果你想阅读原始论文，可以通过这个链接访问：[https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)。
- en: 'Although in this recipe and the following ones we are going to work with some
    equations, we will use libraries and code that will allow us to focus on the actual
    outputs and their relationships with the inputs. However, for the interested reader,
    here is a refresher: [https://machinelearningmastery.com/gentle-introduction-linear-algebra/](https://machinelearningmastery.com/gentle-introduction-linear-algebra/).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本食谱及之后的食谱中我们会使用一些公式，但我们将使用库和代码来帮助我们专注于实际输出及其与输入的关系。然而，对于感兴趣的读者，这里有一篇回顾文章：[https://machinelearningmastery.com/gentle-introduction-linear-algebra/](https://machinelearningmastery.com/gentle-introduction-linear-algebra/)。
- en: 'Moreover, we analyzed the input features in more detail, specifically working
    with `grade`, a categorical feature, using one-hot encoding. There are different
    ways to work with categorical data, which are explored at this link: https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们对输入特征进行了更详细的分析，特别是使用独热编码处理了`grade`这一分类特征。处理分类数据有多种方法，相关内容可以在此链接中找到：https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63。
- en: For more information regarding initialization and evaluation, please continue
    reading the recipes that follow in this chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 关于初始化和评估的更多信息，请继续阅读本章中的后续食谱。
- en: Defining loss functions and evaluation metrics for regression
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义回归的损失函数和评估指标
- en: In the previous recipe, we defined our input features, described our model,
    and initialized it. At that point, we passed the features vector of a house to
    predict the price, calculated the output, and compared it against the expected
    output.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一篇食谱中，我们定义了输入特征，描述了我们的模型，并对其进行了初始化。那时，我们传入了一栋房子的特征向量来预测价格，计算了输出，并将其与预期输出进行了比较。
- en: 'At the end of the previous recipe, the comparison of the expected output and
    the actual output of the model intuitively provided us with an idea of how good
    our model was. This is what it means to “evaluate” our model: we assessed the
    model’s performance. However, that evaluation is not complete for several reasons,
    as we did not correctly take into account several factors:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一篇食谱的结尾，我们通过比较模型的预期输出和实际输出，直观地了解了模型的好坏。这就是“评估”模型的含义：我们评估了模型的性能。然而，这一评估并不完全，原因有几个，因为我们没有正确地考虑到一些因素：
- en: We only evaluated the model on one house – what about the others? How can we
    take all houses into account in our evaluation?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只对一栋房子进行了模型评估——那么其他的房子呢？我们如何在评估中考虑到所有房子呢？
- en: Is the difference between values an accurate measurement of model error? What
    other operations make sense?
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值之间的差异是否是衡量模型误差的准确方法？还有哪些操作是有意义的？
- en: In this recipe, we will cover how we can assess (that is, evaluate) the performance
    of our models, and study functions that are suitable for this matter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将讨论如何评估（即“评估”）我们模型的性能，并研究适合此目的的函数。
- en: 'Furthermore, we will introduce a concept that will be very important when we
    optimize (i.e., train) our models: loss functions.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将介绍一个在优化（即训练）模型时非常重要的概念：损失函数。
- en: Getting ready
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before defining some useful functions for evaluating regression models and
    computing their losses, let’s specify two required and three desirable properties
    of the functions that we’ll use to evaluate our model:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义一些用于评估回归模型和计算其损失的有用函数之前，让我们明确一下我们将用来评估模型的函数的两个必需属性和三个期望属性：
- en: '[Required] **Continuous**: Obviously, we would like our evaluation functions
    to not be undefined on some potential error value, which will allow us to use
    these functions on a large set of pairs (expected output, actual model output).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必需的] **连续性**：显然，我们希望我们的评估函数在某些潜在的误差值上不会是未定义的，这样我们就可以在大量的（预期输出，实际模型输出）对上使用这些函数。'
- en: '[Required] **Symmetric**: This is easily explained with an example. Let’s say
    that the price of a house is 2.2 million dollars – we would like our evaluation
    function to evaluate the model in the same way whether the output was 2 million
    dollars or 2.4 million dollars, as both values are the same distance from the
    expected value, just in different directions.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必需的] **对称性**：通过一个例子很容易解释这个问题。假设一栋房子的价格是220万美元——我们希望我们的评估函数能够以相同的方式评估模型，无论输出是200万美元还是240万美元，因为这两个值离预期值的距离是相同的，只是方向不同。'
- en: '[Desirable] **Robust**: Again, this is easier to explain with an example. Taking
    the same example as in the previous point, imagine we have 2 outputs of 2.4 and
    2.8 million dollars. The error is already going to be large when compared with
    the expected output of 2.2 million dollars, so we would not want to make the error
    even larger due to the loss/evaluation function. From a mathematical perspective,
    we don’t want the error to grow exponentially, or it could make the computations
    diverge to **Not a Number (NaN**). With robust functions, large errors do not
    make computations diverge.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[期望的] **稳健性**：同样，这个问题通过一个例子更容易解释。以之前提到的例子为例，假设我们有2.4百万美元和2.8百万美元两个输出值。与预期值220万美元相比，误差已经很大，因此我们不希望由于损失/评估函数的影响使误差变得更大。从数学的角度来看，我们不希望误差呈指数增长，否则可能导致计算发散至**不是一个数字（NaN）**。使用稳健函数时，大误差不会导致计算发散。'
- en: '[Desirable] **Differentiable**: This is the least intuitive of all the properties.
    Typically, we would aim to achieve error rates of as close as possible to zero.
    However, that is a theoretical scenario that only happens when we have enough
    data to describe a problem perfectly, and when the model is large enough that
    it can represent the mapping from the data to the output values. In reality, we
    can never be sure that we are complying with neither of these previous assumptions,
    and therefore, the unrealistic expectation of zero error evolves to become the
    minimum error possible for our data and our model. We can only detect the minimum
    values of a function by calculating its differential function, hence this **differentiable**
    property. A small stroke of luck, though, is that differentiability implies continuity,
    therefore if our function can satisfy property #4, it automatically satisfies
    property #1.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[期望的] **可微性**：这是所有属性中最不直观的一项。通常，我们的目标是将误差率尽可能接近零。然而，这只是一个理论场景，只有当我们拥有足够的数据来完美描述问题，并且模型足够大，能够表示从数据到输出值的映射时，才会发生。在现实中，我们永远无法确保符合这两个假设，因此，零误差的不切实际期望会转化为数据和模型的最小误差。我们只能通过计算函数的微分函数来检测函数的最小值，因此才有了这个**可微性**属性。幸运的是，可微性意味着连续性，因此如果我们的函数满足属性
    #4，它也会自动满足属性 #1。'
- en: '[Desirable] **Simple**: The simpler the function that satisfies all the properties,
    the better, because that way we can understand the results more intuitively and
    it will not be costly, computationally speaking.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[期望的] **简洁性**：满足所有属性的函数越简单越好，因为这样我们可以更直观地理解结果，并且从计算上讲也不会太昂贵。'
- en: Tip
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Not only evaluation functions must satisfy these criteria. As we will see in
    the next recipe, they must also be satisfied by a very important function for
    training, the *loss function*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅评估函数必须满足这些标准。正如我们将在下一个章节看到的，它们还必须被一个对训练非常重要的函数所满足，即*损失函数*。
- en: How to do it...
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s discuss some evaluation and loss functions and analyze their advantages
    and disadvantages. The functions we will describe are the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些评估和损失函数，并分析它们的优缺点。我们将描述的函数如下：
- en: Mean absolute error
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: Mean squared error
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 均方误差
- en: Smooth L1 loss
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平滑 L1 损失
- en: Mean absolute error
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: 'The first function we are going to study is *almost* perfect according to the
    five properties described earlier. The intuitive idea of this function is to use
    the difference between values as an indicator of the distance or error between
    those values. We apply the `abs` function, meaning absolute value, to make it
    symmetrical:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要研究的第一个函数根据之前描述的五个属性*几乎*完美。这个函数的直观想法是使用值之间的差异作为这些值之间距离或误差的指标。我们应用 `abs` 函数，即绝对值，使其变得对称：
- en: MAE =  1 _ n  ∑ j=1 n | y j −  ˆ y  j|
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: MAE =  1  _  n  ∑  j=1  n  | y  j −  ˆ  y  j|
- en: 'When plotted, the function produces the following graph:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当绘制时，该函数生成以下图形：
- en: "![Figure 3.7 – \uFEFFMAE graph](img/B16591_03_7.jpg)"
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – MAE 图](img/B16591_03_7.jpg)'
- en: Figure 3.7 – MAE graph
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – MAE 图
- en: 'If we analyze this function according to the properties defined previously,
    we see that all properties except #4 are fulfilled; unfortunately, the function
    is not differentiable at point 0\. As we will see in the next recipe, this is
    particularly challenging when this function is used as a loss function. However,
    when evaluating our models, this is not required as the evaluation process does
    not need differentiation, and **Mean Absolute Error** (**MAE**) is considered
    a typical regression metric in evaluation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们根据之前定义的属性来分析这个函数，我们会发现，除了第 #4 条外，所有属性都得到了满足；不幸的是，该函数在 0 点不可导。如我们将在下一个配方中看到的那样，当这个函数作为损失函数使用时，这特别具有挑战性。然而，在评估我们的模型时并不需要导数，因为评估过程不需要微分，**平均绝对误差**（**MAE**）被视为评估中的典型回归指标。'
- en: Important Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This function can be used for evaluation and can also be used as a loss function
    (by itself or as a regularization term). In this case, it is common to call it
    L1 loss or term explain how this relates to the rest of the sentence, as it computes
    the L1 distance of the vectors corresponding to the expected output and the actual
    output.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以用于评估，也可以作为损失函数（单独使用或作为正则化项）。在这种情况下，它通常被称为 L1 损失或项，它计算的是与期望输出和实际输出对应的向量的
    L1 距离。
- en: 'Another similar metric is the **Mean Absolute Percentage Error** (**MAPE**).
    In this metric, each output error is normalized by the expected output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个类似的指标是 **平均绝对百分比误差**（**MAPE**）。在该指标中，每个输出误差通过期望输出进行归一化：
- en: MAPE =  100% _ n  ∑ i=1 n | y i −  ˆ y  i _ y i |
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: MAPE =  100%  _  n  ∑  i=1  n  |  y  i −  ˆ  y  i  _  y  i |
- en: Mean squared error
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均方误差
- en: 'To solve the differentiability issue, the **Mean Squared Error** (**MSE**)function
    is very similar to the MAE, but increases the order from 1 to 2 in the difference
    term. The intuitive idea is to use the simplest quadratic differentiable function
    (*x*2):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决可导性问题，**均方误差**（**MSE**）函数与 MAE 非常相似，但在差异项上将阶数从 1 提高到 2。直观的想法是使用最简单的二次可导函数（*x*²）：
- en: MSE =  1 _ n  ∑ i=1 n (Y i −  ˆ Y ) 2
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: MSE =  1  _  n  ∑  i=1  n  (Y  i −  ˆ  Y  )  2
- en: 'When plotted, the function produces the following graph:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当绘制时，该函数生成以下图形：
- en: "![Figure 3.8 – \uFEFFMSE graph](img/B16591_03_8.jpg)"
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – MSE 图](img/B16591_03_8.jpg)'
- en: Figure 3.8 – MSE graph
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – MSE 图
- en: 'If we analyze this function according to the properties defined, we see that
    all properties except #3 are fulfilled. Unfortunately, the function is not as
    robust as the MAE. Large errors grow exponentially, and therefore, this evaluation
    function is much more susceptible to outliers, as one single data point with a
    very large error can cause the squared error for that value to be quite large,
    and therefore it will make a large contribution to the MSE, leading to erroneous
    conclusions.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们根据已定义的属性来分析该函数，我们发现，除了第 #3 条外，所有属性都得到了满足。不幸的是，这个函数的稳健性不如 MAE。较大的误差呈指数级增长，因此该评估函数对异常值更加敏感，因为一个数据点的非常大误差可能导致该值的平方误差非常大，从而对
    MSE 产生很大影响，导致错误的结论。'
- en: Important Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This function can be used for loss functions (by itself or as a regularization
    term). In this case. It is common to call it L2 loss or term (ridge regression),
    as it computes the L2 distance of the vectors corresponding to the expected output
    and the actual output.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以用于损失函数（单独使用或作为正则化项）。在这种情况下，它通常被称为 L2 损失或项（岭回归），因为它计算的是与期望输出和实际输出对应的向量的
    L2 距离。
- en: 'In order to have the same units as the output variable, the MSE can have the
    squared root applied. This evaluation metric is called **Root Mean Squared** **Error**
    (**RMSE**):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与输出变量具有相同的单位，MSE可以应用平方根。这个评估指标叫做**均方根误差**（**RMSE**）：
- en: RMSE = √ _ ∑ i=1 n  ( ˆ y  i − y i) 2 _ n
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE = √ _ ∑ i=1 n  ( ˆ y  i − y i) ² _ n
- en: Smooth mean absolute error/smooth L1 loss
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平滑的平均绝对误差/平滑L1损失
- en: Can’t we have the best of both worlds? Of course we can!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能同时拥有两全其美吗？当然可以！
- en: 'By combining both functions – the MSE for small values of the error and the
    MAE for the large values of the error – we get the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这两个函数——对小误差值使用MSE，对大误差值使用MAE——我们得到了如下结果：
- en: smooth   L 1 (x) = { 0.5 x 2 if |x| < 1  |x| − 0.5 otherwise
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑的L1(x) = { 0.5 x²  如果 |x| < 1    |x| − 0.5  否则
- en: 'When plotted, the function produces the following graph:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当绘制时，该函数产生如下图所示：
- en: '![Figure 3.9 – Smooth mean absolute error graph](img/B16591_03_9.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 平滑的平均绝对误差图](img/B16591_03_9.jpg)'
- en: Figure 3.9 – Smooth mean absolute error graph
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 平滑的平均绝对误差图
- en: If we analyze this function according to the properties defined, we see that
    all properties are fulfilled.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据已定义的属性分析这个函数，我们会看到所有属性都得到了满足。
- en: Important Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This function can be used for loss functions. In this case, it is common to
    call it smooth L1 loss.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以用于损失函数。在这种情况下，通常称之为平滑L1损失。
- en: How it works...
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In the first recipe of this chapter, we designed our first regression model
    based on a simple biological neuron. We initialized its parameters randomly and
    performed our first naive evaluation. The result was not good, and we conjectured
    that this was due to two reasons: our evaluation mechanism was not robust enough
    and our model parameters had not been optimized.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个例子中，我们设计了基于简单生物神经元的第一个回归模型。我们随机初始化了其参数，并进行了第一次简单的评估。结果并不好，我们推测这是由于两个原因：我们的评估机制不够稳健，模型参数没有优化。
- en: 'In this recipe, we explored how to improve on the first reason: evaluation.
    We covered three of the most important evaluation metrics and mentioned their
    relationship with loss functions, which we will explore in detail in the next
    recipe.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们探讨了如何改进第一个原因：评估。我们涵盖了三种最重要的评估指标，并提到了它们与损失函数的关系，我们将在下一个例子中详细探讨这些内容。
- en: Moreover, we discussed which evaluation metrics are better, exploring how the
    MAE is robust but unfortunately not differentiable, and the MSE is differentiable
    but allows outliers to steer the metric (which is not ideal). We concluded the
    recipe by combining the functions to get the best of both worlds.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们讨论了哪些评估指标更好，探讨了MAE是如何稳健的，但不幸的是不可微分，MSE是可微分的，但它允许异常值影响指标（这并不理想）。我们通过结合这两种函数，得到了两者的优点。
- en: There’s more...
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'A very interesting set of evaluation functions that we did not explore in this
    recipe is the coefficient of determination and its extensions. However, this set
    is only used for linear regression modeling. More information can be found at
    the following links:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们没有探索的一个非常有趣的评估函数集是决定系数及其扩展。然而，这个集仅用于线性回归建模。更多信息可以通过以下链接获取：
- en: '**Coefficient of** **determination**[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决定系数**[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)'
- en: '**Statistics By** **Jim** [https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/](https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jim的统计学** [https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/](https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/)'
- en: 'Furthermore, there are generally many different functions that can be used
    for evaluation and loss in regression problems; you can refer to this link for
    further details: [https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html](https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在回归问题中，通常可以使用许多不同的函数进行评估和损失计算；你可以参考这个链接了解更多细节：[https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html](https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html)
- en: Training regression models
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练回归模型
- en: In supervised learning, training is the process of optimizing the parameters
    of a model towards a specific objective. It is typically the most complex and
    the most time-consuming step in solving a deep learning problem statement.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，训练是朝着特定目标优化模型参数的过程。它通常是解决深度学习问题中最复杂、最耗时的步骤。
- en: In this recipe, we will visit the basic concepts involved in training a model.
    We will apply them to solve the regression model we previously defined in this
    chapter, combined with the usage of the functions we discussed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们将访问训练模型所涉及的基本概念。我们将应用这些概念来解决我们在本章中先前定义的回归模型，并结合我们讨论过的函数的使用。
- en: 'We will predict house prices using the dataset seen in *Recipe 1, Toy dataset
    for regression – load, manage, and visualize house sales dataset* from [*Chapter
    2*](B16591_02.xhtml#_idTextAnchor029)*, Working with MXNet and Visualizing Datasets:
    Gluon* *and DataLoader.*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在[*第2章*](B16591_02.xhtml#_idTextAnchor029)*中看到的数据集预测房价：*《使用MXNet与数据可视化：Gluon》和DataLoader*。
- en: Getting ready
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'There are a number of concepts that we should get familiar with to understand
    this recipe. These concepts define how the training will proceed:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个方法，我们需要熟悉一些概念。这些概念定义了训练的进行方式：
- en: '**Loss function**: The training process is an iterative optimization process.
    As the training progresses, the model is expected to perform better against an
    operation that compares the expected output with the actual output of the model.
    That operation is the loss function, also known as the objective, target, or cost
    function, which is being optimized during the training process.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：训练过程是一个迭代优化过程。随着训练的进行，期望模型能够在与模型实际输出进行比较的操作中表现得更好。这个操作就是损失函数，也称为目标函数、成本函数或代价函数，它是在训练过程中被优化的。'
- en: '**Optimizer**: With each iteration of the training process, each parameter
    of the model is updated by a quantity (calculated using the loss function). The
    optimizer is an algorithm that defines how that quantity is calculated. The most
    important hyperparameter of the optimizer is the **learning rate**, which is the
    multiplier applied to the calculated quantity to update the parameters.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：在每次训练迭代过程中，模型的每个参数都会通过一个量（通过损失函数计算）进行更新。优化器是一种定义如何计算该量的算法。优化器最重要的超参数是**学习率**，它是应用于计算量的乘数，用于更新参数。'
- en: '**Dataset split**: Stopping the training when the model can perform at its
    best in the real world is critical to achieving success in deep learning projects.
    One method to accomplish this is to split our dataset into training, validation,
    and test sets.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集划分**：当模型能够在真实世界中达到最佳表现时，停止训练对深度学习项目的成功至关重要。一种实现此目标的方法是将数据集划分为训练集、验证集和测试集。'
- en: '**Epochs**: This is the number of iterations for which the training process
    will run.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮次**：这是训练过程将运行的迭代次数。'
- en: '**Batch size**: The number of training samples analyzed at a time to produce
    an estimation of the gradient.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：每次分析的训练样本数，用于生成梯度估计。'
- en: How to do it...
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'In this recipe, we will create our own training loop and evaluate how each
    hyperparameter influences the training. To achieve this, we will follow these
    steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们将创建自己的训练循环，并评估每个超参数如何影响训练。为此，我们将遵循以下步骤：
- en: Improving the model
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进模型
- en: Defining the loss function and optimizer
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和优化器
- en: Splitting our dataset
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 划分数据集
- en: Analyzing fairness and diversity
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析公平性和多样性
- en: Defining the number of epochs and the batch size
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练轮次和批量大小
- en: Putting everything together for a training loop
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整合所有内容以形成训练循环
- en: Improving the model
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改进模型
- en: To solve this problem, the architecture we explored in the previous recipes
    (Perceptron) will not be enough. We will stack several Perceptrons together and
    connect them through different layers. This architecture is known as a **Multi-Layer
    Perceptron** (**MLP**). We will define a network architecture with three hidden
    layers that are fully connected (dense) and use the **ReLU** activation function
    (introduced in the first recipe of the chapter) with 128, 1,024, and 128 neurons,
    respectively, and an output layer with the corresponding 1 single output. The
    last layer is left without an activation function, also called the linear activation
    function (*y =* *x*).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们在前面的食谱中探讨的架构（感知机）将不够用。我们将多个感知机堆叠在一起，并通过不同的层连接它们。这种架构被称为**多层感知机**（**MLP**）。我们将定义一个包含三个隐藏层的网络架构，所有层都是全连接（密集层），并使用**ReLU**激活函数（在本章的第一篇食谱中介绍）来分别包含128、1,024和128个神经元，最后一层是输出层，只有一个输出。最后一层不使用激活函数，也称为线性激活函数（*y
    =* *x*）。
- en: 'Furthermore, the house sales dataset is a very complex problem, and finding
    solutions that generalize well (i.e., work sufficiently well in the real world)
    isn’t easy. For this purpose, we have included two new advanced features in the
    model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，房屋销售数据集是一个非常复杂的问题，找到在现实世界中具有良好泛化能力的解决方案并不容易。为此，我们在模型中加入了两个新的高级特性：
- en: '**Batch normalization**: With this step in the process, for each mini-batch,
    the input distribution is standardized. This helps with training convergence and
    generalization.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化**：通过该步骤，对于每个小批量，输入分布都会进行标准化。这有助于训练收敛和泛化能力。'
- en: '**Dropout**: This method consists of disabling neurons of the network randomly
    (given a probability). This helps reduce overfitting (this concept will be explained
    in the next recipe) and improve generalization.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：此方法的内容是随机禁用神经网络中的神经元（根据一定的概率）。这有助于减少过拟合（这一概念将在下一个食谱中解释）并改善泛化能力。'
- en: 'Our code is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码如下：
- en: '[PRE5]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'One important thing to note is that the input to our model has also been modified:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的重要点是，我们模型的输入也已被修改：
- en: The numerical inputs have been scaled to produce inputs with zero mean and unit
    variance. This improves the convergence of the training algorithm.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值输入已被缩放，以生成均值为零、方差为单位的输入。这改善了训练算法的收敛性。
- en: 'The categorial input (grade) has been one-hot encoded. We introduced this concept
    in *Recipe 4, Toy dataset for text tasks – load, manage, and visualize enron emails
    dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*, Working with MXNet
    and Visualizing Datasets: Gluon* *and DataLoader*'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别输入（等级）已进行了独热编码。我们在[*第2章*](B16591_02.xhtml#_idTextAnchor029)的*食谱4，文本任务的玩具数据集——加载、管理和可视化恩朗邮件数据集*中介绍了这个概念，来自[**MXNet的工作与数据集可视化：Gluon**和DataLoader]。
- en: 'This increases the number of features to 30\. As the dataset contains ~20k
    rows, this provides around 600k data points. Let’s compare this to the number
    of parameters in our model:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这将特征数量增加到30。由于数据集包含大约20k行数据，这提供了大约600k个数据点。我们可以将其与模型中参数的数量进行比较：
- en: '[PRE6]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The number of trainable parameters in our model is ~270k. The number of data
    points is approximately two times the number of trainable parameters in our model.
    Typically, this is the minimum for a successful model and ideally, we would like
    to work with a dataset size of ~10 times the number of the parameters of the model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中可训练参数的数量约为270k。数据点的数量大约是我们模型中可训练参数的两倍。通常，这是成功模型的最低要求，理想情况下，我们希望使用的数据集大小为模型参数的10倍左右。
- en: Tip
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Even though the comparison between the data points available and the number
    of parameters of the model is a very useful one, different architectures have
    different requirements in terms of data. As usual, experimentation (trial and
    error) is the key to finding the right balance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将数据点数量与模型的参数数量进行比较是一个非常有用的方法，但不同的架构对数据有不同的需求。如同往常一样，实验（试错法）是找到正确平衡的关键。
- en: 'The last important point about our model is the initialization method, as with
    multilayer networks, random initialization might not yield the best results. The
    most common methods nowadays are the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们模型的最后一个重要点是初始化方法，因为在多层网络中，随机初始化可能不会产生最佳结果。现如今最常用的方法包括以下几种：
- en: '**Xavier initialization**: Takes into account the number of inputs and the
    number of outputs when computing the variance'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier初始化**：在计算方差时，考虑了输入数量和输出数量。'
- en: '**MSRA PReLU** or **Kaiming initialization**: The Xavier initialization method
    has some issues with ReLU activation functions, so this method is preferred'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MSRA PReLU** 或 **Kaiming 初始化**：Xavier 初始化方法在 ReLU 激活函数中存在一些问题，因此更倾向于使用此方法。'
- en: 'MXNet provides very simple access to these functionalities, in this case, for
    MSRA PReLU initialization:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 提供了非常简单的方式来访问这些功能，在本例中是 MSRA PReLU 初始化：
- en: '[PRE7]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Important Note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Initialization methods provide values for the weights and bias so that the model
    activation functions are not initialized on saturated (flat) regions. The intuition
    is to have these weights and biases with zero mean and unit variance. For the
    statistical analysis, a link is provided in *There’s more...*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化方法为权重和偏置提供初始值，以避免模型激活函数初始化在饱和（平坦）区域。其直觉是让这些权重和偏置的均值为零，方差为单位。有关统计分析的详细信息，请参见
    *更多内容...*。
- en: Defining the loss function and optimizer
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义损失函数和优化器
- en: As we saw in the previous recipe, the smooth L1 (also known as Huber) loss function
    will work quite well.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的示例中所见，平滑 L1（也称为 Huber）损失函数效果非常好。
- en: 'There are several optimizers that have been proven to work well for **supervised**
    **learning** problems:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种优化器已被证明在 **监督学习** 问题中表现良好：
- en: '`batch_size` parameter when we worked with **DataLoader** in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们处理 **DataLoader** 时使用的 `batch_size` 参数，见 [*第 2 章*](B16591_02.xhtml#_idTextAnchor029)*，与
    MXNet 配合使用并可视化数据集：Gluon* *与 DataLoader*。
- en: '**Momentum/Nesterov accelerated gradient**: Gradient descent can have a problem
    with stability and can start jumping and getting trapped in local minima. One
    method to avoid these issues is to consider the past steps that the algorithm
    has taken, which is achieved with these two optimizers.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动量/ Nesterov 加速梯度**：梯度下降可能会遇到稳定性问题，并可能开始跳跃并陷入局部最小值。避免这些问题的一种方法是考虑算法过去的步骤，这可以通过这两种优化器实现。'
- en: '**Adagrad**/**Adadelta**/**RMSprop**: GD uses the same learning rate for all
    parameters, without taking into account the frequency with which they are updated.
    Adagrad and these other optimizers deal with this issue by adjusting the learning
    rate by parameter. However, Adagrad learning rates decrease over time and may
    yield values close to zero that do not perform any further updates. To solve this
    problem, Adadelta and RMSprop were developed.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adagrad**/**Adadelta**/**RMSprop**：GD 对所有参数使用相同的学习率，而不考虑它们更新的频率。Adagrad 及这些优化器通过调整每个参数的学习率来解决这个问题。然而，Adagrad
    的学习率会随着时间的推移而减小，并可能接近零，导致无法进行进一步更新。为了解决这个问题，开发了 Adadelta 和 RMSprop。'
- en: '**Adam**/**AdaMax**/**Nadam**: These state-of-the-art optimizers combine both
    of the improvements from gradient descent: past-step calculations and adaptive
    learning rates. Adam uses the L2 norm for the exponentially weighted average of
    the gradients, whereas AdaMax uses the infinity norm (max operation). Nadam replaces
    the momentum component in Adam with the Nesterov momentum to accelerate convergence.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam**/**AdaMax**/**Nadam**：这些最先进的优化器结合了梯度下降的两项改进：过去步骤的计算和自适应学习率。Adam 使用
    L2 范数来计算梯度的指数加权平均值，而 AdaMax 使用无穷范数（最大操作）。Nadam 用 Nesterov 动量替代了 Adam 中的动量部分，从而加速收敛。'
- en: 'MXNet and Gluon provides very simple interfaces to define the loss function
    and the optimizer. With the following two lines of code, we are choosing the Huber
    loss function and the Adam optimizer:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 和 Gluon 提供了非常简单的接口来定义损失函数和优化器。通过以下两行代码，我们选择了 Huber 损失函数和 Adam 优化器：
- en: '[PRE8]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Splitting our dataset
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 切分我们的数据集
- en: One of the most important things to consider in all data science projects is
    the performance of a trained model on data outside the dataset we are going to
    work with. In supervised learning, for training and evaluation, we work with data
    knowing the desired (expected) output, so how can we make sure that when using
    our model on new data, without a known output, it is going to perform as expected?
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有数据科学项目中，需要考虑的最重要的事情之一就是在我们将模型应用于新的数据时，已训练模型在未见过的数据上的表现如何。对于监督学习，在训练和评估过程中，我们使用已知的（期望的）输出数据，那么如何确保我们在新数据上使用模型时，它能按预期表现呢？
- en: 'We deal with this issue by splitting our dataset into three parts:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将数据集分成三个部分来解决这个问题：
- en: '**Training set**: The training set is used during training to compute the updates
    of the model parameters.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练集**：训练集在训练过程中用于计算模型参数的更新。'
- en: '**Validation set**: The validation set is used during training to check every
    epoch how the model has improved (or not) with those updates previously calculated.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证集**：验证集在训练过程中用于检查每个周期模型的改进情况（或没有改进），即之前计算的那些更新。'
- en: '**Test set**: Finally, once the training has finished, we can compute its performance
    on *unseen data*, which is the test set, the only part of the dataset that was
    not used to improve the model during training.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试集**：最后，一旦训练完成，我们可以计算模型在*未见数据*上的表现，这就是测试集，是数据集中唯一未在训练中用于提升模型的部分。'
- en: 'Furthermore, in order to have stable training that will allow our model to
    work properly with data outside our dataset, the data split needs to be computed
    taking into account the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了保证稳定的训练，使得我们的模型能正确处理数据集外的数据，数据的划分需要考虑以下因素：
- en: '**Size of the splits**: This depends on the amount of data available and the
    task. Typical percentage splits for training/validation/test data are 60/20/20
    or 80/10/10.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**划分大小**：这取决于可用数据的数量和任务的性质。典型的训练/验证/测试数据划分比例为60/20/20或80/10/10。'
- en: '**Which data points to select for each of the splits**: The key here is to
    have a balanced dataset. For example, in our house prices dataset, we do not want
    to have only houses with two and three bedrooms on the training set, then four-bedroom
    houses in the validation set, and finally houses with five or more bedrooms in
    the test set. Ideally, each set should be an accurate representation of the dataset.
    This is very important for sensitive datasets where fairness and diversity must
    be considered.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择每个划分的数据点**：这里的关键是拥有一个平衡的数据集。例如，在我们的房价数据集中，我们不希望训练集里只有两间和三间卧室的房子，验证集里是四卧房的房子，最后测试集是五间或更多卧室的房子。理想情况下，每个数据集应该准确代表整个数据集。这对于需要考虑公平性和多样性的敏感数据集尤其重要。'
- en: 'We can achieve these splits easily, in this case, using a function from the
    well-known library **scikit-learn**:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地实现这些划分，在这个例子中，使用一个来自著名库**scikit-learn**的函数：
- en: '[PRE9]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code snippet, we do our three-way split for train, validation,
    and test data in two steps:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们将训练集、验证集和测试集分成三个部分，分为两个步骤：
- en: We assign 20% of the dataset to the test set.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将数据集的20%分配给测试集。
- en: The remaining 80% will be split equally between the validation and test sets.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的80%将平分给验证集和测试集。
- en: Analyzing fairness and diversity
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析公平性和多样性
- en: 'Imagine for a moment that we work for a real-estate website, and we manage
    the data science team. There is a compelling feature that will drive traffic to
    our website: when homeowners want to sell a property, they can fill in some data
    about their house and will be able to see a machine learning-optimized estimate
    of the price at which they should put their house up for selling, with data indicating
    that it will be sold within the next 3 months at that price. This feature sounds
    really cool as homeowners can fine-tune the asking price of the house they want
    to sell, and potential buyers will see reasonable prices according to market.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一下我们为一个房地产网站工作，负责数据科学团队。我们有一个非常吸引人的功能来为我们的网站带来流量：当房主想要出售房产时，他们可以填写一些房屋数据，并能看到一个经过机器学习优化的估算价格，告诉他们应该以什么价格将房屋挂牌出售，同时数据还表明房屋将在接下来的三个月内按这个价格售出。这个功能听起来非常酷，因为房主可以微调他们想要出售房屋的要价，而潜在买家也会根据市场看到合理的价格。
- en: However, we suddenly realize we do not have enough data yet for houses with
    two or fewer bathrooms, and we know that feature is highly sensitive for our model.
    Deploying this model for real-life properties would mean that houses with two
    or fewer bathrooms could be valued closer to valuations of houses with more bathrooms
    by our model, simply because that is all the data our model has been able to see.
    This would mean that for the cheapest houses, the most affordable ones for low-income
    families, we will be increasing their prices *unfairly*, which would be a serious
    problem.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们突然意识到，对于拥有两间或更少浴室的房子，数据量还不够，而且我们知道这个特征对我们的模型非常敏感。如果我们将这个模型应用于现实中的房产，意味着我们模型可能会将拥有两间或更少浴室的房子的估价定得接近那些拥有更多浴室的房子，仅仅因为这是模型所能看到的所有数据。这就意味着，对于最便宜的房子，即低收入家庭最负担得起的房子，我们可能会*不公平地*提高它们的价格，这将是一个严重的问题。
- en: 'Our model cannot know better because we have not shown it better. In this scenario,
    what options do we have? These are the ones that might suit a real-world situation:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型不能知道更多，因为我们没有展示给它更多。在这种情况下，我们有哪些选择？以下是可能适合实际情况的几种方案：
- en: Be confident about the robustness of our model and deploy it in production regardless.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我们的模型的鲁棒性充满信心，并无论如何都将其部署到生产环境中。
- en: Convince business leaders not to deploy the model until we have all the data
    we need.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说服业务领导者，在我们拥有所需的所有数据之前不要部署模型。
- en: Deploy the model in production, but only allow sellers to use it for houses
    with three bathrooms or more.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境中，但只允许卖家将其用于至少有三个浴室的房屋。
- en: 'The first option is the least adequate of all, however, it is actually the
    most frequently applied one, due to the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项是所有选项中最不充分的，然而，由于以下原因，它实际上是最常用的：
- en: It is inconvenient to work on a project for several months and delay it right
    before it is expected to launch. Management typically does not expect nor want
    this kind of news and it could put some jobs at risk.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在项目进行了几个月后再延迟发布是非常不方便的。管理层通常不期望也不希望听到这样的消息，这可能会让一些工作岗位面临风险。
- en: However, the most common reason for this to happen is that the error in the
    data goes unnoticed. There is never enough time to verify that the data is accurate/fair/diverse
    enough and the focus shifts to delivering an optimized model as soon as possible.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，发生这种情况最常见的原因是数据中的错误没有被注意到。通常没有足够的时间来验证数据是否准确、公平、多样，因此重点转向尽快交付一个优化过的模型。
- en: The second option is difficult to argue for a similar reason as the first option,
    and the third option might look good on paper, but is actually very dangerous.
    If I found myself in that situation, I would not choose the third option, simply
    because we cannot be sure that the data is diverse and fair across all features,
    so a proper data quality assessment is required. If we found this kind of error
    this late in a project, it is because not enough focus was put on data quality.
    This typically happens with companies that have been recording or storing large
    amounts of data and now want to do some machine learning project with it, instead
    of designing data collection operations with a clear objective. This is one of
    the most common reasons why machine learning projects fail in such companies.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项因为与第一个选项类似的原因而很难辩护，而第三个选项可能在纸面上看起来不错，但实际上是非常危险的。如果我遇到这种情况，我不会选择第三个选项，仅仅因为我们不能确保数据在所有特征上都是多样和公平的，因此需要进行适当的数据质量评估。如果我们在项目这么晚的阶段发现这种错误，那是因为在数据质量上没有给予足够的关注。这通常发生在那些已经记录或存储了大量数据的公司中，而这些公司现在想用这些数据做一些机器学习项目，而不是设计有明确目标的数据收集操作。这是机器学习项目在这些公司中失败的最常见原因之一。
- en: 'Let us take a look at how our dataset looks from the point of view of fairness
    and diversity:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从公平性和多样性的角度来看一下我们的数据集：
- en: 'First, as seen in *Recipe 1, Toy dataset for regression – load, manage and
    visualize house sales dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we will start
    with the price distribution:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正如在[《第二章》](B16591_02.xhtml#_idTextAnchor029)*中的*《配方1，回归模型玩具数据集——加载、管理与可视化房屋销售数据集*，与MXNet和数据集可视化：Gluon和DataLoader*一节中所见，我们将从价格分布开始：
- en: '![Figure 3.10 – Price distribution across the training, validation, and test
    sets](img/B16591_03_10.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – 训练集、验证集和测试集的价格分布](img/B16591_03_10.jpg)'
- en: Figure 3.10 – Price distribution across the training, validation, and test sets
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 训练集、验证集和测试集的价格分布
- en: Although we can see a small dip in houses with prices lower than $500k, the
    price distribution is fairly represented across the three datasets and no manual
    modifications are necessary.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以看到价格低于$500k的房屋出现了小幅下降，但三个数据集中的价格分布在很大程度上是均衡的，且不需要进行手动修改。
- en: 'The living space in square foot looks as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 居住面积（平方英尺）如下所示：
- en: '![Figure 3.11 – Living Sqft plots for the training, validation, and test sets](img/B16591_03_11.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 训练集、验证集和测试集的居住面积平方英尺图](img/B16591_03_11.jpg)'
- en: Figure 3.11 – Living Sqft plots for the training, validation, and test sets
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 训练集、验证集和测试集的居住面积平方英尺图
- en: The largest differences we can see here are due to a very small number of high-valued
    houses. We can even consider these outliers, and if the parameters of our training
    are well chosen, this should not harm our prediction capabilities.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的最大差异是由于少数高价房产。我们甚至可以将这些看作是异常值，如果我们的训练参数选择得当，这不应影响我们的预测能力。
- en: 'The number of bathrooms looks as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 浴室数量如下所示：
- en: '![Figure 3.12 – Number of bathrooms across the training, validation, and test
    sets](img/B16591_03_12.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – 训练集、验证集和测试集中的浴室数量分布](img/B16591_03_12.jpg)'
- en: Figure 3.12 – Number of bathrooms across the training, validation, and test
    sets
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – 训练集、验证集和测试集中的浴室数量分布
- en: This distribution is quite well represented in our validation and test sets.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布在我们的验证集和测试集中得到了很好的体现。
- en: 'Grade looks as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 评分如下所示：
- en: '![Figure 3.13 – Grade for training, validation, and test sets](img/B16591_03_13.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13 – 训练集、验证集和测试集的评分](img/B16591_03_13.jpg)'
- en: Figure 3.13 – Grade for training, validation, and test sets
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 训练集、验证集和测试集的评分
- en: This distribution is also well represented in our validation and test sets.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布在我们的验证集和测试集中也得到了很好的体现。
- en: 'Grouping all the individual analyses, we can conclude that our training, validation,
    and test sets are fairly good representations of our full dataset. We must remember
    though that our dataset has its own limitations (although I have to say, for the
    selected features, it is quite well represented):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有个别分析结果汇总，我们可以得出结论：我们的训练集、验证集和测试集相当好地代表了完整数据集。我们必须记住，我们的数据集有其自身的局限性（尽管我必须说，对于所选特征，它表现得相当不错）：
- en: 'Price: [75k$, 7.7M$]'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '价格: [75k$, 7.7M$]'
- en: 'Living Sqft: [290, 13540]'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 居住面积（平方英尺）：[290, 13540]
- en: Number of Bathrooms [0, 8]
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浴室数量 [0, 8]
- en: 'Grade: [1, 13], but lacking 2 as we pointed out earlier'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '评分: [1, 13]，但之前提到的缺少 2'
- en: Defining the number of epochs and batch size
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义训练轮数和批量大小
- en: '**Number of epochs** refers to the number of iterations the training algorithm
    will run. Depending on the complexity of the problem and the optimizer and hyperparameters
    chosen, this number can vary from very low (say 5-10) to very high (thousands
    of iterations).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练轮数**指的是训练算法运行的迭代次数。根据问题的复杂性以及所选择的优化器和超参数，这个数字可能会从非常低（例如 5-10 次）到非常高（几千次迭代）。'
- en: '**Batch size** refers to the number of training samples analyzed at the same
    time to estimate the error gradient. In *Recipe 3, Toy dataset for image tasks
    – load, manage and visualize iris dataset* in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we introduced
    this concept as a means to optimize memory usage; the smaller the batch size,
    the less memory required. Furthermore, this speeds up computation of the gradient;
    the larger the batch size, the faster computations will run (if memory permits).
    Typical values range from 32 to 2,048 samples.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量大小**指的是在同一时间内分析的训练样本数，以估计误差梯度。在[*第 2 章*](B16591_02.xhtml#_idTextAnchor029)*，与MXNet协作并可视化数据集：Gluon
    和 DataLoader* 中的*Recipe 3，面向图像任务的玩具数据集——加载、管理和可视化鸢尾花数据集*，我们引入了这个概念作为优化内存使用的一种手段；批量大小越小，所需内存越少。此外，这样可以加速梯度的计算；批量大小越大，计算运行越快（如果内存允许）。典型的批量大小范围从
    32 到 2,048 个样本。'
- en: Putting everything together for a training loop
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容结合起来，形成一个训练循环
- en: The training loop is the iterative process that runs the optimizer to calculate/estimate
    the gradient so that on each iteration, the error computed from the loss function
    (the objective of the optimizer) is reduced. As mentioned, each iteration is called
    an epoch. And for each iteration, the full training set is accessed in batches
    to compute the gradient.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环是一个迭代过程，运行优化器以计算/估计梯度，从而在每次迭代中减少通过损失函数（优化器的目标）计算得到的误差。如前所述，每次迭代称为一个“轮次”。对于每次迭代，整个训练集都以批次的方式被访问以计算梯度。
- en: Furthermore, as we will see, it is interesting to compute the loss function
    for the validation set. In our case, we will also compute the loss function for
    the test set, as it will provide us with specific details on how our model will
    perform.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们将看到的，计算验证集的损失函数非常有趣。在我们的案例中，我们还将计算测试集的损失函数，因为它将为我们提供关于模型表现的具体细节。
- en: In order to understand the difference in behavior when modifying the hyperparameters,
    we are going to run the training loop for our house price prediction dataset several
    times, modifying just one hyperparameter per table and keeping the rest of the
    variables constant (unless otherwise noted).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解修改超参数时的行为差异，我们将对我们的房价预测数据集多次运行训练循环，每次仅修改一个超参数，其他变量保持不变（除非另有说明）。
- en: Optimizer and learning rate
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化器和学习率
- en: As discussed earlier, the chosen optimizer for the training loop and the learning
    rate are intimately related, as for some optimizers (such as SGD) the learning
    rate is kept constant, whereas for others (such as Adam) it varies from a given
    starting point.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练循环中选择的优化器和学习率是密切相关的，因为对于某些优化器（如SGD），学习率是恒定的，而对于其他优化器（如Adam），学习率从一个给定的起始点开始变化。
- en: Tip
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The best optimizer depends on several factors, and nothing trumps trial and
    error. I strongly suggest to try a few and see which one fits best. In my experience,
    SGD and Adam are typically the ones that work best, including this problem, the
    prediction of house prices.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的优化器取决于多个因素，没有什么比试错法更有效。我强烈建议尝试几个优化器，看看哪个最适合。在我的经验中，SGD和Adam通常是表现最好的，甚至在这个问题中——预测房价问题上。
- en: 'Let’s analyze how the training loss and validation loss vary for the SGD optimizer
    by varying the **learning rate** (**LR**) while keeping the other parameters constant:
    *Epochs = 100, Batch Size = 128, Loss fn =* *HuberLoss*:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下，当变动**学习率**（**LR**）并保持其他参数不变时，SGD优化器的训练损失和验证损失是如何变化的：*训练轮数 = 100，批次大小
    = 128，损失函数 = HuberLoss*：
- en: '![Figure 3.14 – Loss for the SGD optimizer when varying the learning rate](img/B16591_03_14.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14 – 当变动学习率时，SGD优化器的损失](img/B16591_03_14.jpg)'
- en: Figure 3.14 – Loss for the SGD optimizer when varying the learning rate
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 当变动学习率时，SGD优化器的损失
- en: From *Figure 3**.14*, we can conclude that for the SGD optimizer, an LR value
    of between 10-1 and 1.0 is optimal. Furthermore, we can see that for very large
    values of LR (> 2.0) the algorithm diverges. That’s why, when searching for optimal
    values for the LR, it’s better to start small.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图3.14*中，我们可以得出结论，对于SGD优化器，学习率（LR）值在10^-1到1.0之间是最佳的。此外，我们可以看到，对于非常大的LR值（> 2.0），算法会发散。这就是为什么在寻找最佳学习率值时，最好从小值开始。
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by varying the **learning rate** (**LR**) and keeping the other parameters constant:
    *Epochs = 100, Batch Size = 128, Loss fn =* *HuberLoss*:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下，当变动**学习率**（**LR**）并保持其他参数不变时，Adam优化器的训练损失和验证损失是如何变化的：*训练轮数 = 100，批次大小
    = 128，损失函数 = HuberLoss*：
- en: '![Figure 3.15 – Loss for Adam optimizer when varying the learning rate](img/B16591_03_15.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图3.15 – 当变动学习率时，Adam优化器的损失](img/B16591_03_15.jpg)'
- en: Figure 3.15 – Loss for Adam optimizer when varying the learning rate
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 – 当变动学习率时，Adam优化器的损失
- en: From *Figure 3**.15*, we can conclude that for the Adam optimizer, an LR value
    of between 10-4 and 10-3 is optimal. As Adam calculates gradients differently,
    it is more difficult to make it diverge than SGD.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图3.15*中，我们可以得出结论，对于Adam优化器，学习率（LR）值在10^-4到10^-3之间是最佳的。由于Adam计算梯度的方式不同，它比SGD更不容易发散。
- en: Adam requires smaller values for the learning rate because it *adapts* the value
    as the training process evolves.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Adam需要较小的学习率值，因为它*会随着训练过程的进展*调整学习率。
- en: Batch size
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批次大小
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by varying the batch size, keeping the other parameters constant: *Epochs = 100,
    LR = 10-2, Loss fn =* *HuberLoss*:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下，当变动批次大小时，Adam优化器的训练损失和验证损失是如何变化的，其他参数保持不变：*训练轮数 = 100，学习率 = 10^-2，损失函数
    = HuberLoss*：
- en: '![Figure 3.16 – Loss for the Adam optimizer when varying batch size](img/B16591_03_16.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 当变动批次大小时，Adam优化器的损失](img/B16591_03_16.jpg)'
- en: Figure 3.16 – Loss for the Adam optimizer when varying batch size
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 当变动批次大小时，Adam优化器的损失
- en: From *Figure 3**.16,* we can conclude that for the Adam optimizer, a batch size
    value of between 64 and 1,024 provides the best results.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图3.16*中，我们可以得出结论，对于Adam优化器，批次大小在64到1,024之间提供最佳结果。
- en: Epochs
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练轮数
- en: Another hyperparameter is the number of epochs, meaning the number of times
    the optimizer is going to process the full training set.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个超参数是训练轮数（epochs），即优化器处理完整训练集的次数。
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    varying the epochs, keeping the other parameters constant: *LR = 10-2, Batch Size
    = 128, Loss fn =* *HuberLoss*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析在 Adam 优化器下，当改变训练轮次（epochs）时，训练损失和验证损失的变化，同时保持其他参数不变：*LR = 10-2，Batch Size
    = 128，Loss fn =* **HuberLoss**：
- en: '![Figure 3.17 – Loss for the Adam optimizer when varying the number of epochs](img/B16591_03_17.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.17 – Adam 优化器在改变训练轮次时的损失](img/B16591_03_17.jpg)'
- en: Figure 3.17 – Loss for the Adam optimizer when varying the number of epochs
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – Adam 优化器在改变训练轮次时的损失
- en: From *Figure 3**.17*, we can conclude that around 100-200 epochs is good for
    our problem. With these values, it is very likely the best result will be achieved
    earlier than that.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 3.17* 中，我们可以得出结论，大约 100-200 轮次对于我们的问题是合适的。在这些值下，很可能会在此之前取得最佳结果。
- en: How it works...
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: On our journey toward solving our regression problem, we learned in this recipe
    how to update our model hyperparameters optimally. We understood the role that
    each hyperparameter plays in the training loop and we performed some ablation
    studies for each individual hyperparameter. This helped us understand how our
    training and validation losses behaved when we modified each hyperparameter individually.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解决回归问题的过程中，我们在这篇教程中学会了如何最优地更新模型的超参数。我们理解了每个超参数在训练循环中的作用，并对每个超参数进行了单独的消融研究。这帮助我们理解了在单独修改每个超参数时，训练损失和验证损失的表现。
- en: 'For our current problem and the chosen model, we verified that the best set
    of hyperparameters was as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们当前的问题和选择的模型，我们验证了最佳超参数组合如下：
- en: 'Optimizer: Adam'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器：Adam
- en: 'Learning Rate: 10-2'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：10-2
- en: 'Batch Size: 128'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batch Size：128
- en: 'Number of epochs: 200'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮次：200
- en: At the end of the training loop, these hyperparameters gave us a training loss
    of 0.10 and a validation loss of 0.10.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环结束时，这些超参数给我们带来了 0.10 的训练损失和 0.10 的验证损失。
- en: There’s more...
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'With our model definition, we introduced three new concepts: **batch normalization**,
    **dropout**, and **scaling**. I find the following links useful to understand
    these advanced topics:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型定义中，我们引入了三个新概念：**批量归一化**、**dropout** 和 **scaling**。我认为以下链接对理解这些高级话题非常有用：
- en: '**Introduction to batch** **normalization**: [https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化简介**： [https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)'
- en: '**Batch normalization research** **paper**: [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Batch normalization 研究** **论文**： [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
- en: '**Introduction to** **dropout**: [https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dropout 简介**： [https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)'
- en: '**Dropout research** **paper**: [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout 研究** **论文**： [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
- en: '**Scaling**: [https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scaling**： [https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)'
- en: 'On the subject of initialization, this article explores the Xavier and Kaiming
    methods in detail (includes links to the research papers): [https://pouannes.github.io/blog/initialization/](https://pouannes.github.io/blog/initialization/).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化方面，本文详细探讨了 Xavier 和 Kaiming 方法（包括研究论文的链接）： [https://pouannes.github.io/blog/initialization/](https://pouannes.github.io/blog/initialization/)
- en: In this recipe, we explored in depth how two optimizers, **SGD** and **Adam**,
    behave. These are two of the most important and best performant optimizers; however,
    there are many more, and some could work better for your specific problem.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们深入探讨了两种优化器，**SGD** 和 **Adam**，的表现。这是两个最重要且表现最好的优化器；然而，还有许多其他优化器，有些可能更适合你的特定问题。
- en: 'One excellent resource to learn about which optimizers are implemented in MXNet
    and their characteristics is the official documentation: [https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html](https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一个学习MXNet中实现的优化器及其特性的绝佳资源是官方文档：[https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html](https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html)。
- en: 'To compare the behavior and performance of each optimizer, I personally like
    the visualizations shown in this link (optimizers section): [https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1](https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1).'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较每个优化器的行为和表现，我个人喜欢这个链接中展示的可视化（优化器部分）：[https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1](https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1)。
- en: In this recipe, we worked with optimizers and their hyperparameters. Hyperparameter
    choice is a very complex problem, and it always requires a little bit of trial
    and error with each problem, verifying that the training loop works. A rule of
    thumb when selecting hyperparameters is to read research papers that tackle similar
    problems to yours and start with the hyperparameters proposed in those papers.
    You can then move from that starting point and see what works best for your particular
    case.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们研究了优化器及其超参数。超参数选择是一个非常复杂的问题，通常需要通过每个问题的试验和错误来验证训练循环是否有效。选择超参数时的经验法则是阅读解决与自己问题相似的研究论文，并从那些论文中提出的超参数开始。然后，你可以从这个起点出发，看看什么最适合你的特定情况。
- en: 'Apart from the training loss and the validation loss at the end of the training
    loop, we also provided a third loss value, *best validation loss*, we will explore
    what this value means and how it is calculated in the next recipe. This all maps
    to a question we have not answered properly yet: *when do I stop my training loop?*
    We will address this question in the next recipe.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练过程中的训练损失和验证损失外，我们还提供了第三个损失值，*最佳验证损失*，我们将在下一个食谱中探讨这个值的含义及其计算方法。这一切都与我们尚未正确回答的问题相关：*我应该什么时候停止训练循环？*
    我们将在下一个食谱中解决这个问题。
- en: Evaluating regression models
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归模型评估
- en: 'In the previous recipe, we learned how to choose our training hyperparameters
    to optimize our training. We also verified how those choices affected the training
    and validation losses. In this recipe, we are going to explore how those choices
    affect our actual evaluation in the real world. The observant reader will have
    noticed that we split the dataset into three different sets: training, validation,
    and test. However, during our training, we only used the training set and the
    validation set. In this recipe, we will emulate some real-world behavior of our
    model by running it on the unseen data, the test set.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个食谱中，我们学习了如何选择训练超参数以优化我们的训练。我们还验证了这些选择如何影响训练和验证损失。在本食谱中，我们将探讨这些选择如何影响我们在现实世界中的实际评估。细心的读者会注意到我们将数据集分为三个不同的部分：训练集、验证集和测试集。然而，在训练过程中，我们只使用了训练集和验证集。在本食谱中，我们将通过在未见数据（测试集）上运行模型来模拟一些现实世界的行为。
- en: Getting ready
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'When evaluating a model, we can perform qualitative evaluation and quantitative
    evaluation:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，我们可以执行定性评估和定量评估：
- en: '**Qualitative evaluation** is the selection of one or more random (or not so
    random, depending on what we are looking for) samples and analyzing the result,
    verifying whether it matches our expectations.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定性评估**是选择一个或多个随机（或不那么随机，取决于我们要寻找的东西）样本，并分析结果，验证它是否符合我们的预期。'
- en: '**Quantitative evaluation** deals with computing the outputs for a large number
    of inputs and calculating statistics about them (typically the mean), hence we
    will compute the MAE and MAPE.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定量评估**涉及计算大量输入的输出并对其进行统计分析（通常是均值），因此我们将计算MAE和MAPE。'
- en: Furthermore, we are going to take a look at how training can have a large influence
    on the evaluation.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将看看训练如何对评估产生重大影响。
- en: How to do it...
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Before jumping in to model evaluation, let’s discuss how we can measure our
    model training performance. Therefore, the steps in this recipe are the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始模型评估之前，让我们先讨论如何衡量我们的模型训练表现。因此，本食谱中的步骤如下：
- en: Measuring training performance – overfitting
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 衡量训练表现——过拟合
- en: Qualitative evaluation
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定性评估
- en: Quantitative evaluation
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定量评估
- en: Measuring training performance – overfitting
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量训练表现 – 过拟合
- en: Deep learning networks are quite powerful, surpassing human-level performance
    on a variety of problems. However, when not kept in check, these networks can
    also provide incorrect and unexpected results. One of the most important and frequent
    errors happens when the network, using its full capability, memorizes the samples
    that are being shown (the training set), yielding excellent results for that data.
    However, in this scenario, the network has simply memorized the training samples,
    and when deployed in a real-world use case it is going to perform poorly. This
    type of error is called **overfitting**.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络非常强大，在多种问题上超越了人类水平的表现。然而，如果不加以控制，这些网络也可能产生不正确和意外的结果。最重要且常见的错误之一发生在网络充分发挥其能力时，它会记住正在展示的样本（训练集），从而在这些数据上得到非常好的结果。然而，在这种情况下，网络只是记住了训练样本，而当它在实际的使用场景中部署时，表现将会很差。这种错误被称为**过拟合**。
- en: Thankfully, there is a very successful strategy to deal with overfitting, and
    we have already touched on it. It starts with splitting our full dataset into
    a training set and a validation set, which we did in the previous recipe.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种非常成功的策略可以处理过拟合问题，我们已经提到过它。它的开始是将我们的完整数据集拆分为训练集和验证集，这一点我们在前面的步骤中已经做过了。
- en: 'From a theoretical point of view, training and validation losses typically
    have behaviors similar to that shown in the following graph:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论角度来看，训练和验证损失通常表现出类似以下图形的行为：
- en: '![Figure 3.18 – Losses versus epochs – ideal](img/B16591_03_18.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – 损失与轮数的关系 – 理想](img/B16591_03_18.jpg)'
- en: Figure 3.18 – Losses versus epochs – ideal
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – 损失与轮数的关系 – 理想
- en: In *Figure 3**.18*, we can see how the training and validation losses typically
    evolve (an idealized portrayal). The training loss continues decreasing as the
    training progresses, always optimizing (albeit more slowly as the number of epochs
    increases). The validation loss, however, reaches a point where it does not decrease
    further, but rather increases. At the lowest level of validation loss is where
    the model has reached its peak performance and where we should stop the learning
    process (early).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.18*中，我们可以看到训练和验证损失通常是如何变化的（理想化的表现）。随着训练的进行，训练损失持续下降，始终在优化（尽管随着训练轮数的增加，下降速度变慢）。然而，验证损失达到一个点后不再继续下降，反而开始上升。验证损失最低点是模型达到最佳性能的地方，也是我们应该停止学习过程（早停）的时候。
- en: 'Let’s examine how this kind of behavior looks in the real world. For our problem,
    the training loss and the validation loss evolve as the training progresses, as
    follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这种行为在实际中的表现。对于我们的问题，随着训练的进展，训练损失和验证损失是这样变化的：
- en: '![Figure 3.19 – Losses versus epochs – real](img/B16591_03_19.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图3.19 – 损失与轮数的关系 – 实际](img/B16591_03_19.jpg)'
- en: Figure 3.19 – Losses versus epochs – real
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19 – 损失与轮数的关系 – 实际
- en: As we can see in *Figure 3**.19*, the validation loss is much noisier than in
    the ideal world, and early stopping is much more difficult to achieve successfully.
    A very easy implementation is to save the model every time the validation loss
    decreases. This way, we are always certain that given a number of epochs the training
    is going to be run, the model with the best (lowest) validation loss will be saved.
    This was the method implemented in the previous recipe.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图3.19*中所见，验证损失比理想情况下更为嘈杂，早停也更难成功实现。一个非常简单的实现方法是每当验证损失减少时就保存模型。这样，我们总是可以确保在给定的训练轮数内，具有最佳（最低）验证损失的模型会被保存。这正是前面步骤中实施的方法。
- en: Qualitative evaluation
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定性评估
- en: 'To verify our model is behaving similarly to what we expect (yielding a low
    error when predicting a house price), one easy approach is to run our model for
    a random input from the test set (unseen data). This can easily be done with the
    following code:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的模型是否与预期一致（即在预测房价时产生较低的误差），一种简单的方法是使用测试集中的随机输入（未见过的数据）来运行我们的模型。这可以通过以下代码轻松实现：
- en: '[PRE10]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code snippet yields the following results:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段将产生以下结果：
- en: '[PRE11]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As expected, the error rate is quite reasonable (just 2.45%!).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，错误率相当合理（仅为2.45%！）。
- en: Important Note
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Although I tried to keep the code as reproducible as possible, including setting
    the seeds for all random processes, there might be some sources of randomness.
    This means that your results might be different, but typically the order of magnitude
    of errors will be similar.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我尽量保持代码的可复现性，包括为所有随机过程设置种子，但仍可能存在一些随机性来源。这意味着您的结果可能会有所不同，但通常误差的数量级会相似。
- en: Quantitative evaluation – MAE
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量评估 – MAE
- en: 'Let’s calculate the **MAE** function, as described in the *Defining loss functions
    and evaluation metrics for regression* recipe earlier in this chapter:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算**MAE**函数，如本章早些时候在*定义回归损失函数和评估指标*中所描述的那样：
- en: '[PRE12]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The MAE is $81k. Taking into account that prices varied from $75k to $7.7 million,
    this error seems reasonable. Do not forget that estimating house prices is a hard
    problem!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: MAE为$81k。考虑到价格从$75k到$770万不等，这个误差似乎是合理的。别忘了，估计房价是一个困难的问题！
- en: Quantitative evaluation – MAPE
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量评估 – MAPE
- en: 'The value provided by the MAE is good to get an idea of how small or large
    the errors are in our model’s predictions. However, it does not provide a very
    meaningful merit figure, as the same MAE could have been achieved in different
    ways:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: MAE（平均绝对误差）提供的值对于了解我们模型预测中的误差有多大或多小是有帮助的。然而，它并没有提供一个非常有意义的评价标准，因为相同的MAE值可能是通过不同的方式得到的：
- en: '*Small errors for all houses*: As houses increase in price, the absolute error
    number will be higher, and hence an $80k MAE might be quite good.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于所有房屋的较小误差*：随着房屋价格的增加，绝对误差的数值会更高，因此，$80k的MAE可能是相当不错的。'
- en: '*Very large errors for cheap houses*: In this case, an $80k MAE will mean that
    for the cheapest houses, the error might be 2-3 times, or even worse, the actual
    price of the house. This scenario would be very bad.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*便宜房屋的误差很大*：在这种情况下，$80k的MAE意味着对于最便宜的房屋，误差可能是实际房价的2到3倍，甚至更糟。这种情况是非常糟糕的。'
- en: 'In general, we can add to the MAE another figure, similarly calculated to provide
    a **relative** error rate, instead of relying solely on an **absolute** value.
    For our model, we get the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以在MAE的基础上添加另一个数字，通过类似的计算来提供**相对**误差率，而不仅仅依赖于**绝对**值。对于我们的模型，我们得到如下结果：
- en: '[PRE13]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Looks like our model is not behaving too badly, yielding a MAPE of 16%!
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的模型表现得还不错，得到了16%的MAPE！
- en: Quantitative evaluation – thresholds and percentage
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量评估 – 阈值与百分比
- en: 'Another question we could consider for evaluating our model is the following:
    *For how many houses (in %) did we accurately predict* *the price?*'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以考虑的问题是：*我们准确预测了多少房屋（以百分比表示）的价格？*
- en: 'Let’s assume we consider that we accurately predicted the price of a house
    if the predicted price error is less than 25%. In our case, this gives us the
    following:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们认为当预测价格误差小于25%时，我们就认为价格预测是准确的。在我们的情况下，结果如下：
- en: '[PRE14]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This calculation gives us an 81%, well done!
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这一计算结果为81%，做得不错！
- en: 'Furthermore, we could plot the percentage of houses we correctly predicted
    as a function of the error threshold:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以将我们正确预测的房屋百分比与误差阈值绘制成图：
- en: '![Figure 3.20 – Percentage of correct estimations](img/B16591_03_20.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.20 – 正确估计的百分比](img/B16591_03_20.jpg)'
- en: Figure 3.20 – Percentage of correct estimations
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – 正确估计的百分比
- en: In *Figure 3**.20*, we can see, as expected, that considering an error of 25%
    to deem a prediction accurate, our model yields 80%+ correct predictions.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 3.20*中，我们可以看到，正如预期的那样，认为误差在25%以内即为准确预测，我们的模型能够正确预测超过80%的数据。
- en: How it works...
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we explored how to evaluate our regression model. To properly
    do this, we revisited the decision made previously to split our full dataset into
    a training set, a validation set, and a test set.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了如何评估回归模型。为了正确做到这一点，我们重新审视了之前将完整数据集分割成训练集、验证集和测试集的决策。
- en: During training, we used the training set to calculate the gradients to update
    our model parameters, and the validation set to confirm the real-world behavior.
    Afterward, to evaluate our model performance, we used the test set, which was
    the only remaining set of unseen data.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们使用训练集来计算梯度并更新模型参数，验证集则用于确认模型的实际表现。之后，为了评估我们的模型性能，我们使用了测试集，这是唯一一组未见过的数据。
- en: We discovered the value of describing our model behavior qualitatively by calculating
    the output of random samples, and of quantitatively evaluating our model performance
    by exploring calculations and graphs of MAE and MAPE.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算随机样本的输出，发现了定性描述模型行为的价值，并通过探索MAE和MAPE的计算和图表，定量评估了我们的模型性能。
- en: We ended the recipe by defining what constitutes an accurate prediction by setting
    a threshold and plotting the behavior of the model by modifying the threshold.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义什么是准确预测（设置阈值）并通过调整阈值绘制模型行为，结束了这个过程。
- en: There’s more...
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Deep learning has surpassed human-level performance on multiple tasks. However,
    evaluating models properly is paramount to verify how models will perform when
    deployed in production environments in the real world. I found interesting this
    small list of tasks where human-level performance has been reached by AI: https://venturebeat.com/2017/12/08/6-areas-where-artificial-neural-networks-outperform-humans/.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在多个任务上已经超越了人类水平的表现。然而，正确评估模型对于验证模型在真实生产环境中部署后的表现至关重要。我觉得以下这个关于人工智能在多个任务上达到人类水平表现的小清单非常有趣：
    https://venturebeat.com/2017/12/08/6-areas-where-artificial-neural-networks-outperform-humans/。
- en: 'When evaluation is not performed properly, models may not behave as expected.
    Two of the most significant large-scale problems of this type (at Google in 2015
    and Microsoft in 2016, respectively) are detailed in the following articles:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估没有正确进行时，模型可能不会按预期行为表现。以下文章详细描述了这种类型的两个最重要的大规模问题（分别发生在2015年的谷歌和2016年的微软）：
- en: '**Google Mistakenly Tags Black People as ‘Gorillas,’ Showing Limits of** **Algorithms:**
    https://www.wsj.com/articles/BL-DGB-42522'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌错误地将黑人标记为“猩猩”，展示了** **算法的局限性：** https://www.wsj.com/articles/BL-DGB-42522'
- en: '**Statistics By** **Jim:** [https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/](https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/)'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jim的统计学：** [https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/](https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/)'
- en: 'Unfortunately, although these issues are less and less frequent nowadays, they
    still exist. A database that contains these issues has been published and is updated
    whenever one of these issues is reported: [https://incidentdatabase.ai/.](https://incidentdatabase.ai/)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管这些问题现在变得越来越少见，但它们仍然存在。一个包含这些问题的数据库已经发布，并且每当报告出现这些问题时都会进行更新： [https://incidentdatabase.ai/.](https://incidentdatabase.ai/)
- en: 'To prevent these issues, Google wrote a set of principles to develop Responsible
    AI. I strongly recommend all AI practitioners to abide by them: [https://ai.google/principles/](https://ai.google/principles/).'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这些问题，谷歌制定了一套原则来开发负责任的人工智能。我强烈建议所有AI从业人员遵守这些原则： [https://ai.google/principles/](https://ai.google/principles/)。
- en: 'At this stage, we have completed our journey through a complete regression
    problem: we explored our regression dataset, decided on our evaluation metrics,
    and defined and initialized our model. We understood the best hyperparameter combination
    of optimizer, learning rate, batch size, and epochs, and trained it with early
    stopping. Lastly, we concluded by evaluating our model qualitatively and quantitatively.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经完成了整个回归问题的旅程：我们探索了回归数据集，决定了评估指标，定义并初始化了模型。我们理解了优化器、学习率、批量大小和训练周期的最佳超参数组合，并使用提前停止进行训练。最后，我们通过定性和定量的方式对模型进行了评估。
