- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: '**Natural Language Processing** (**NLP**) offers a much-needed set of tools
    and algorithms for understanding and processing the large volume of unstructured
    data in today’s world. Recently, deep learning has been widely adopted for many
    NLP tasks because of the remarkable performance deep learning algorithms have
    shown in a plethora of challenging tasks, such as image classification, speech
    recognition, and realistic text generation. TensorFlow is one of the most intuitive
    and efficient deep learning frameworks currently in existence that enables such
    amazing feats. This book will enable aspiring deep learning developers to handle
    massive amounts of data using NLP and TensorFlow. This chapter covers the following
    topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）提供了一整套急需的工具和算法，用于理解和处理当今世界大量的非结构化数据。近年来，深度学习因其在许多NLP任务中的卓越表现被广泛采用，尤其是在图像分类、语音识别和真实感文本生成等具有挑战性的任务中。TensorFlow是当前最直观高效的深度学习框架之一，能够实现这些惊人的成果。本书将帮助有志成为深度学习开发者的人，使用NLP和TensorFlow处理海量数据。本章涵盖以下内容：'
- en: What is Natural Language Processing?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是自然语言处理？
- en: Tasks of Natural Language Processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理的任务
- en: The traditional approach to Natural Language Processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理的传统方法
- en: The deep learning approach to Natural Language Processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理的深度学习方法
- en: Introduction to the technical tools
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术工具简介
- en: In this chapter, we will provide an introduction to NLP and to the rest of the
    book. We will answer the question, “What is Natural Language Processing?”. Also,
    we’ll look at some of its most important use cases. We will also consider the
    traditional approaches and the more recent deep learning-based approaches to NLP,
    including a **Fully Connected Neural Network** (**FCNN**). Finally, we will conclude
    with an overview of the rest of the book and the technical tools we will be using.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍NLP及本书的其他内容。我们将回答“什么是自然语言处理？”这一问题。同时，我们也将探讨一些NLP最重要的应用案例。我们还将讨论传统方法和近年来基于深度学习的NLP方法，包括**全连接神经网络**（**FCNN**）。最后，我们将总结本书的其他章节和将要使用的技术工具。
- en: What is Natural Language Processing?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自然语言处理？
- en: According to DOMO ([https://www.domo.com/](https://www.domo.com/)), an analytics
    company, there were 1.7MB for every person on earth every second by 2020, with
    a staggering 4.6 billion active users on the internet. This includes roughly 500,000
    tweets sent and 306 billion emails circulated every day. These figures are only
    going in one direction as this book is being written, and that is up! Of all this
    data, a large fraction is unstructured text and speech as there are billions of
    emails and social media content created and phone calls made every day.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据分析公司DOMO（[https://www.domo.com/](https://www.domo.com/)）的数据，到2020年，全球每人每秒产生1.7MB的数据，互联网活跃用户达到46亿。这些数据包括大约50万条推文和3060亿封邮件每天流通。这些数字在本书写作过程中仅有一个方向，那就是不断增长！在这些数据中，大量是非结构化的文本和语音数据，因为每天都有数十亿封邮件、社交媒体内容和电话被创建和拨打。
- en: These statistics provide a good basis for us to define what NLP is. Simply put,
    the goal of NLP is to make machines understand our spoken and written languages.
    Moreover, NLP is ubiquitous and is already a large part of human life. **Virtual
    Assistants** (**VAs**), such as Google Assistant, Cortana, Alexa, and Apple Siri,
    are largely NLP systems. Numerous NLP tasks take place when one asks a VA, “*Can
    you show me a good Italian restaurant nearby?*” First, the VA needs to convert
    the utterance to text (that is, speech-to-text). Next, it must understand the
    semantics of the request (for example, identify the most important keywords like
    restaurant and Italian) and formulate a structured request (for example, cuisine
    = Italian, rating = 3–5, distance < 10 km). Then, the VA must search for restaurants
    filtering by the location and cuisine, and then, rank the restaurants by the ratings
    received. To calculate an overall rating for a restaurant, a good NLP system may
    look at both the rating and text description provided by each user. Finally, once
    the user is at the restaurant, the VA might assist the user by translating various
    menu items from Italian to English. This example shows that NLP has become an
    integral part of human life.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统计数据为我们定义NLP提供了良好的基础。简而言之，NLP的目标是使机器理解我们口语和书面语言。此外，NLP无处不在，已经成为人类生活的重要组成部分。**虚拟助手**（**VAs**），例如谷歌助手、Cortana、Alexa和苹果Siri，基本上是NLP系统。当你向虚拟助手询问“*Can
    you show me a good Italian restaurant nearby?*”时，涉及了许多NLP任务。首先，虚拟助手需要将语音转化为文本（即语音转文本）。接下来，它必须理解请求的语义（例如，识别最重要的关键词，如餐厅和意大利菜），并形成一个结构化的请求（例如，菜系
    = 意大利，评分 = 3–5，距离 < 10公里）。然后，虚拟助手必须根据地点和菜系过滤餐厅，并按照评分对餐厅进行排序。为了计算餐厅的总体评分，一个好的NLP系统可能会查看每个用户提供的评分和文本描述。最后，一旦用户到达餐厅，虚拟助手可能会帮助用户将菜单中的意大利语条目翻译成英语。这个例子表明，NLP已经成为人类生活的一个不可或缺的部分。
- en: It should be understood that NLP is an extremely challenging field of research
    as words and semantics have a highly complex nonlinear relationship, and it is
    even more difficult to capture this information as a robust numerical representation.
    To make matters worse, each language has its own grammar, syntax, and vocabulary.
    Therefore, processing textual data involves various complex tasks such as text
    parsing (for example, tokenization and stemming), morphological analysis, word
    sense disambiguation, and understanding the underlying grammatical structure of
    a language. For example, in these two sentences, *I went to the bank* and *I walked
    along the river bank*, the word *bank* has two entirely different meanings, due
    to the context it’s used in. To distinguish or (disambiguate) the word *bank*,
    we need to understand the context in which the word is being used. Machine learning
    has become a key enabler for NLP, helping to accomplish the aforementioned tasks
    through machines. Below we discuss some of the important tasks that fall under
    NLP.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 应该理解的是，NLP是一个极具挑战性的研究领域，因为单词和语义之间有着高度复杂的非线性关系，而且要将这些信息捕捉为稳健的数值表示更为困难。更糟糕的是，每种语言都有自己独特的语法、句法和词汇。因此，处理文本数据涉及各种复杂的任务，例如文本解析（例如，分词和词干提取）、形态学分析、词义消歧和理解语言的基础语法结构。例如，在这两句话中，*I
    went to the bank* 和 *I walked along the river bank*，词语*bank*有着完全不同的含义，因为它们使用的上下文不同。为了区分或（消歧）*bank*这个词，我们需要理解它所使用的上下文。机器学习已经成为NLP的一个关键推动力，帮助通过机器完成上述任务。以下是我们讨论的NLP中的一些重要任务：
- en: Tasks of Natural Language Processing
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的任务
- en: 'NLP has a multitude of real-world applications. A good NLP system is one that
    performs many NLP tasks. When you search for today’s weather on Google or use
    Google Translate to find out how to say, “*How are you?*” in French, you rely
    on a subset of such tasks in NLP. We will list some of the most ubiquitous tasks
    here, and this book covers most of these tasks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）在现实世界中有着广泛的应用。一个好的NLP系统是能够执行多种NLP任务的系统。当你在谷歌搜索今天的天气，或者使用谷歌翻译查看“*How
    are you?*”用法语怎么说时，你依赖的正是NLP中的一部分任务。我们将在此列举一些最常见的任务，本书涵盖了大部分这些任务：
- en: '**Tokenization**: Tokenization is the task of separating a text corpus into
    atomic units (for example, words or characters). Although it may seem trivial
    for a language like English, tokenization is an important task. For example, in
    the Japanese language, words are not delimited by spaces or punctuation marks.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：分词是将文本语料库分割成原子单位（例如单词或字符）的任务。虽然对于像英语这样的语言来说，分词可能看起来微不足道，但它仍然是一个重要任务。例如，在日语中，单词之间并没有空格或标点符号作为分隔符。'
- en: '**Word-Sense Disambiguation** (**WSD**): WSD is the task of identifying the
    correct meaning of a word. For example, in the sentences, *The dog barked at the
    mailman* and *Tree bark is sometimes used as a medicine*, the word *bark* has
    two different meanings. WSD is critical for tasks such as question answering.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词义消歧** (**WSD**)：WSD 是识别单词正确意义的任务。例如，在句子 *The dog barked at the mailman*
    和 *Tree bark is sometimes used as a medicine* 中，单词 *bark* 有两个不同的含义。WSD 对于问答等任务至关重要。'
- en: '**Named Entity Recognition** (**NER**): NER attempts to extract entities (for
    example, person, location, and organization) from a given body of text or a text
    corpus. For example, the sentence, *John gave Mary two apples at school on Monday*
    will be transformed to *[John]name gave [Mary]name [two]number apples at [school]organization
    on [Monday]time*. NER is an imperative topic in fields such as information retrieval
    and knowledge representation.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别** (**NER**)：NER 旨在从给定的文本或文本语料库中提取实体（例如，人名、地点、组织等）。例如，句子 *John gave
    Mary two apples at school on Monday* 将被转换为 *[John]name 给了 [Mary]name [two]number
    个苹果，在 [school]organization 上 [Monday]time*。NER 在信息检索和知识表示等领域中是一个重要话题。'
- en: '**Part-of-Speech** (**PoS**) **tagging**: PoS tagging is the task of assigning
    words to their respective parts of speech. It can either be basic tags such as
    noun, verb, adjective, adverb, and preposition, or it can be granular such as
    proper noun, common noun, phrasal verb, verb, and so on. The Penn Treebank project,
    a popular project focusing PoS, defines a comprehensive list of PoS tags at [https://www.ling.upenn.edu/courses/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/ling001/penn_treebank_pos.html).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注** (**PoS**) **标注**：PoS 标注是将单词分配到其相应词性的任务。它可以是基本标签，如名词、动词、形容词、副词和介词，也可以是更细粒度的标签，如专有名词、普通名词、短语动词、动词等。Penn
    Treebank 项目是一个专注于 PoS 的流行项目，它定义了一个全面的 PoS 标签列表，详见 [https://www.ling.upenn.edu/courses/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/ling001/penn_treebank_pos.html)。'
- en: '**Sentence**/**synopsis classification**: Sentence or synopsis (for example,
    movie reviews) classification has many use cases such as spam detection, news
    article classification (for example, political, technology, and sport), and product
    review ratings (that is, positive or negative). This is achieved by training a
    classification model with labeled data (that is, reviews annotated by humans,
    with either a positive or negative label).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子/摘要分类**：句子或摘要（例如，电影评论）分类有许多应用场景，如垃圾邮件检测、新闻文章分类（例如，政治、科技和体育）以及产品评论评级（即正面或负面）。这一任务通过使用标注数据（即由人工标注的评论，带有正面或负面标签）训练分类模型来实现。'
- en: '**Text generation**: In text generation, a learning model (for example, a neural
    network) is trained with text corpora (a large collection of textual documents),
    and it then predicts new text that follows. For example, language modeling can
    output an entirely new science fiction story by using existing science fiction
    stories for training.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：在文本生成中，学习模型（例如神经网络）通过文本语料库（大量文本文件集合）进行训练，然后预测接下来的新文本。例如，语言建模可以通过使用现有的科幻故事进行训练，生成一个全新的科幻故事。'
- en: Recently, OpenAI released a language model known as OpenAI-GPT-2, which can
    generate incredibly realistic text. Furthermore, this task plays a very important
    role in understanding language, which helps a downstream decision-support model
    get off the ground quickly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，OpenAI 发布了一个名为 OpenAI-GPT-2 的语言模型，它能够生成极为真实的文本。此外，这项任务在理解语言中起着非常重要的作用，有助于下游决策支持模型的快速启动。
- en: '**Question Answering** (**QA**): QA techniques possess a high commercial value,
    and such techniques are found at the foundation of chatbots and VA (for example,
    Google Assistant and Apple Siri). Chatbots have been adopted by many companies
    for customer support. Chatbots can be used to answer and resolve straightforward
    customer concerns (for example, changing a customer’s monthly mobile plan), which
    can be solved without human intervention. QA touches upon many other aspects of
    NLP such as information retrieval and knowledge representation. Consequently,
    all this makes developing a QA system very difficult.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答系统** (**QA**)：问答技术具有很高的商业价值，这些技术是聊天机器人和虚拟助手（例如，谷歌助手和苹果Siri）的基础。许多公司已经采用了聊天机器人来提供客户支持。聊天机器人可以用来回答并解决简单的客户问题（例如，修改客户的月度手机套餐），这些问题可以在不需要人工干预的情况下解决。问答技术涉及到自然语言处理的许多其他方面，如信息检索和知识表示。因此，开发一个问答系统是非常困难的。'
- en: '**Machine Translation** (**MT**): MT is the task of transforming a sentence/phrase
    from a source language (for example, German) to a target language (for example,
    English). This is a very challenging task, as different languages have different
    syntactical structures, which means that it is not a one-to-one transformation.
    Furthermore, word-to-word relationships between languages can be one-to-many,
    one-to-one, many-to-one, or many-to-many. This is known as the **word alignment
    problem** in MT literature.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译** (**MT**)：机器翻译是将源语言（例如，德语）的句子/短语转换为目标语言（例如，英语）的任务。这是一个非常具有挑战性的任务，因为不同的语言具有不同的句法结构，这意味着它并不是一种一对一的转换。此外，不同语言之间的词与词之间的关系可能是多对一、一对一、一对多或多对多的。这就是机器翻译文献中的**词对齐问题**。'
- en: 'Finally, to develop a system that can assist a human in day-to-day tasks (for
    example, VA or a chatbot) many of these tasks need to be orchestrated in a seamless
    manner. As we saw in the previous example where the user asks, “*Can you show
    me a good Italian restaurant nearby?*” several different NLP tasks, such as speech-to-text
    conversion, semantic and sentiment analyses, question answering, and machine translation,
    need to be completed. In *Figure 1.1*, we provide a hierarchical taxonomy of different
    NLP tasks categorized into several different types. It is a difficult task to
    attribute an NLP task to a single classification. Therefore, you can see some
    tasks spanning multiple categories. We will split the categories into two main
    types: language-based (light-colored with black text) and problem formulation-based
    (dark-colored with white text). The linguistic breakdown has two categories: syntactic
    (structure-based) and semantic (meaning-based). The problem formulation-based
    breakdown has three categories: preprocessing tasks (tasks that are performed
    on text data before feeding to a model), discriminative tasks (tasks where we
    attempt to assign an input text to one or more categories from a set of predefined
    categories) and generative tasks (tasks where we attempt to generate a new textual
    output). Of course, this is one classification among many. But it will show how
    difficult it is to assign a specific NLP task to a specific category.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了开发一个能够帮助人类处理日常任务的系统（例如，虚拟助手或聊天机器人），许多任务需要以无缝的方式进行协同。如我们在前面的例子中所见，用户问：“*你能给我推荐一家附近的意大利餐厅吗？*”时，多个不同的自然语言处理任务，如语音转文本、语义分析和情感分析、问答和机器翻译等，都需要完成。在*图1.1*中，我们提供了一个分层分类法，将不同的自然语言处理任务分为几种不同的类型。将一个自然语言处理任务归类为某一单一类别是一个困难的任务。因此，您可以看到有些任务跨越了多个类别。我们将这些类别分为两大类：基于语言的（浅色背景，黑色文字）和基于问题的（深色背景，白色文字）。语言学分类有两类：句法（基于结构）和语义（基于意义）。基于问题的分类则有三类：预处理任务（在输入模型之前对文本数据进行处理的任务）、判别性任务（我们试图将输入文本分配到一个或多个预定义类别的任务）和生成性任务（我们试图生成新的文本输出的任务）。当然，这只是其中一种分类方法，但它展示了将一个具体的自然语言处理任务归入特定类别的难度。
- en: '![](img/B14070_01_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_01_01.png)'
- en: 'Figure 1.1: A taxonomy of the popular tasks of NLP categorized under broader
    categories'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：自然语言处理中常见任务的分类，按更广泛的类别进行划分
- en: Having understood the various tasks in NLP, let us now move on to understand
    how we can solve these tasks with the help of machines. We will discuss both the
    traditional method and the deep- learning-based approach.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了自然语言处理的各种任务后，我们接下来将讨论如何借助机器来解决这些任务。我们将讨论传统方法和基于深度学习的方法。
- en: The traditional approach to Natural Language Processing
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的传统方法
- en: The traditional or classical approach to solving NLP is a sequential flow of
    several key steps, and it is a statistical approach. When we take a closer look
    at a traditional NLP learning model, we will be able to see a set of distinct
    tasks taking place, such as preprocessing data by removing unwanted data, feature
    engineering to get good numerical representations of textual data, learning to
    use machine learning algorithms with the aid of training data, and predicting
    outputs for novel, unseen data. Of these, feature engineering was the most time-consuming
    and crucial step for obtaining good performance on a given NLP task.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 传统或经典的NLP解决方法是一个包含几个关键步骤的顺序流程，并且它是一种统计方法。当我们仔细观察传统的NLP学习模型时，会发现一系列明确的任务在进行，比如通过去除不需要的数据来预处理数据，进行特征工程以获得文本数据的良好数值表示，使用训练数据学习机器学习算法，并为新颖的、未见过的数据进行预测。在这些任务中，特征工程是获取良好性能的最耗时和最关键的步骤。
- en: Understanding the traditional approach
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解传统方法
- en: The traditional approach to solving NLP tasks involves a collection of distinct
    subtasks. First, the text corpora need to be preprocessed, focusing on reducing
    the vocabulary and *distractions*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的NLP任务解决方法包括一系列不同的子任务。首先，文本语料库需要经过预处理，重点是减少词汇量和*干扰*。
- en: By *distractions*, I refer to the things that distract the algorithm (for example,
    punctuation marks and stop word removal) from capturing the vital linguistic information
    required for the task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我所说的*干扰*是指那些使算法无法捕捉到任务所需的关键信息的事物（例如，标点符号和停用词的去除）。
- en: Next come several feature engineering steps. The main objective of feature engineering
    is to make learning easier for the algorithms. Often the features are hand-engineered
    and biased toward the human understanding of a language. Feature engineering was
    of the utmost importance for classical NLP algorithms, and consequently, the best-performing
    systems often had the best-engineered features. For example, for a sentiment classification
    task, you can represent a sentence with a parse tree and assign positive, negative,
    or neutral labels to each node/subtree in the tree to classify that sentence as
    positive or negative. Additionally, the feature engineering phase can use external
    resources such as WordNet (a lexical database that can provide insights into how
    different words are related to each other – e.g. synonyms) to develop better features.
    We will soon look at a simple feature engineering technique known as *bag-of-words*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是几个特征工程步骤。特征工程的主要目标是使算法的学习更加轻松。通常，特征是手动设计的，并且倾向于基于人类对语言的理解。特征工程对于经典的NLP算法至关重要，因此，表现最好的系统通常具有最精心设计的特征。例如，对于情感分类任务，你可以用一个语法树来表示一个句子，并为树中的每个节点/子树分配正面、负面或中立标签，从而将句子分类为正面或负面。此外，特征工程阶段还可以使用外部资源，如WordNet（一个词汇数据库，可以提供关于不同单词如何相互关联的见解——例如，同义词），来开发更好的特征。我们很快会看到一种简单的特征工程技术，称为*词袋模型*。
- en: 'Next, the learning algorithm learns to perform well at the given task using
    the obtained features and, optionally, the external resources. For example, for
    a text summarization task, a parallel corpus containing common phrases and succinct
    paraphrases would be a good external resource. Finally, prediction occurs. Prediction
    is straightforward, where you will feed a new input and obtain the predicted label
    by forwarding the input through the learning model. The entire process of the
    traditional approach is depicted in *Figure 1.2*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，学习算法使用获得的特征以及可选的外部资源来在给定任务上表现良好。例如，对于文本摘要任务，一个包含常见短语和简洁释义的平行语料库将是一个很好的外部资源。最后，进行预测。预测过程是直接的，你只需输入新数据，并通过学习模型将输入传递以获得预测标签。传统方法的整个过程如*图1.2*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_02.png](img/B14070_01_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_02.png](img/B14070_01_02.png)'
- en: 'Figure 1.2: The general approach of classical NLP'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：经典NLP的一般方法
- en: Next, let’s discuss a use case where we use NLP to generate football game summaries.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一个使用自然语言处理（NLP）生成足球比赛摘要的用例。
- en: Example – generating football game summaries
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 – 生成足球比赛摘要
- en: To gain an in-depth understanding of the traditional approach to NLP, let’s
    consider a task of automatic text generation from the statistics of a game of
    football. We have several sets of game statistics (for example, the score, penalties,
    and yellow cards) and the corresponding articles generated for that game by a
    journalist, as the training data. Let’s also assume that for a given game, we
    have a mapping from each statistical parameter to the most relevant phrase of
    the summary for that parameter. Our task here is that, given a new game, we need
    to generate a natural-looking summary of the game. Of course, this can be as simple
    as finding the best-matching statistics for the new game from the training data
    and retrieving the corresponding summary. However, there are more sophisticated
    and elegant ways of generating text.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解传统的自然语言处理（NLP）方法，我们考虑一个基于足球比赛统计数据的自动文本生成任务。我们有多个比赛统计数据集（例如，比分、罚球和黄牌）以及由记者为该比赛生成的相应文章，作为训练数据。我们还假设对于一场特定的比赛，我们有一个映射，将每个统计参数与该参数的摘要中最相关的短语对应起来。我们在这里的任务是，给定一场新的比赛，我们需要生成一篇自然流畅的比赛摘要。当然，这可以像从训练数据中找到最匹配的统计数据并检索相应的摘要一样简单。然而，也有更复杂和优雅的文本生成方式。
- en: If we were to incorporate machine learning to generate natural language, a sequence
    of operations, such as preprocessing the text, feature engineering, learning,
    and prediction, is likely to be performed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们结合机器学习来生成自然语言，可能会执行一系列操作，如预处理文本、特征工程、学习和预测。
- en: '**Preprocessing**: The text involves operations, such as tokenization (for
    example, splitting “*I went home*” into “*I*”, “*went*”, “*home*”), stemming (for
    example, converting *listened* to *listen*), and removing punctuation (for example,
    ! and ;), in order to reduce the vocabulary (that is, the features), thus reducing
    the dimensionality of the data. Tokenization might appear trivial for a language
    such as English, as the words are isolated; however, this is not the case for
    certain languages such as Thai, Japanese, and Chinese, as these languages are
    not consistently delimited. Next, it is important to understand that stemming
    is not a trivial operation either. It might appear that stemming is a simple operation
    that relies on a simple set of rules such as removing *ed* from a verb (for example,
    the stemmed result of *listened* is *listen*); however, it requires more than
    a simple rule base to develop a good stemming algorithm, as stemming certain words
    can be tricky (for example, using rule-based stemming, the stemmed result of *argued*
    is *argu*). In addition, the effort required for proper stemming can vary in complexity
    for other languages.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理**：文本涉及的操作包括分词（例如，将“I went home”分割为“I”、“went”、“home”），词干提取（例如，将*listened*转化为*listen*），以及去除标点符号（例如，！和；），目的是减少词汇量（即特征），从而减少数据的维度。对于英语等语言来说，分词可能显得微不足道，因为单词是孤立的；然而，对于泰语、日语和中文等语言来说，情况并非如此，因为这些语言的词语并不是始终被清晰地分隔开来。接下来，需要理解的是，词干提取也并不是一个简单的操作。表面上看，词干提取似乎是一个简单的操作，依赖于一些简单的规则，例如去除动词后的*ed*（例如，*listened*的词干结果是*listen*）；然而，开发一个好的词干提取算法需要的不仅仅是简单的规则库，因为某些词的词干提取是棘手的（例如，使用基于规则的词干提取，*argued*的词干结果是*argu*）。此外，正确进行词干提取所需的工作量在不同语言中可能有很大不同。'
- en: '**Feature engineering** is used to transform raw text data into an appealing
    numerical representation so that a model can be trained on that data, for example,
    converting text into a bag-of-words representation or using n-gram representation,
    which we will discuss later. However, remember that state-of-the-art classical
    models rely on much more sophisticated feature engineering techniques.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**用于将原始文本数据转换为具有吸引力的数字表示，从而可以在这些数据上训练模型，例如，将文本转换为词袋模型（bag-of-words）表示，或者使用n-gram表示法，我们稍后会讨论。然而，请记住，最先进的经典模型依赖于更为复杂的特征工程技术。'
- en: 'The following are some of the feature engineering techniques:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些特征工程技术：
- en: '**Bag-of-words**: This is a feature engineering technique that creates feature
    representations based on the word occurrence frequency. For example, let’s consider
    the following sentences:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型（Bag-of-words）**：这是一种基于词频创建特征表示的特征工程技术。例如，我们考虑以下句子：'
- en: '*Bob went to the market to buy some flowers*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲍勃去市场买了一些花*'
- en: '*Bob bought the flowers to give to Mary*'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲍勃买了花要送给玛丽*'
- en: 'The vocabulary for these two sentences would be:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这两句话的词汇表是：
- en: '[“Bob”, “went”, “to”, “the”, “market”, “buy”, “some”, “flowers”, “bought”,
    “give”, “Mary”]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Bob”, “went”, “to”, “the”, “market”, “buy”, “some”, “flowers”, “bought”,
    “give”, “Mary”]'
- en: 'Next, we will create a feature vector of size *V* (vocabulary size) for each
    sentence, showing how many times each word in the vocabulary appears in the sentence.
    In this example, the feature vectors for the sentences would respectively be as
    follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为每个句子创建一个大小为 *V*（词汇表大小）的特征向量，表示词汇表中每个单词在句子中出现的次数。在这个例子中，句子的特征向量分别如下：
- en: '[1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0]'
- en: '[1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 1]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 1]'
- en: A crucial limitation of the bag-of-words method is that it loses contextual
    information as the order of words is no longer preserved.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: bag-of-words 方法的一个重要限制是它失去了上下文信息，因为单词的顺序不再被保留。
- en: '**n-gram**: This is another feature engineering technique that breaks down
    text into smaller components consisting of *n* letters (or words). For example,
    2-gram would break the text into two-letter (or two-word) entities. For example,
    consider this sentence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**n-gram**：这是一种将文本拆分为更小组件的特征工程技术，这些组件由 *n* 个字母（或单词）组成。例如，2-gram 将文本拆分为两个字母（或两个单词）的实体。考虑下面这个句子：'
- en: '*Bob went to the market to buy some flowers*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bob 去市场买花了*'
- en: 'The letter level n-gram decomposition for this sentence is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该句子的字母级 n-gram 分解如下：
- en: '[“Bo”, “ob”, “b “, “ w”, “we”, “en”, ..., “me”, “e “,” f”, “fl”, “lo”, “ow”,
    “we”, “er”, “rs”]'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Bo”, “ob”, “b “, “ w”, “we”, “en”, ..., “me”, “e “,” f”, “fl”, “lo”, “ow”,
    “we”, “er”, “rs”]'
- en: 'The word-based n-gram decomposition is this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单词的 n-gram 分解如下：
- en: '[“Bob went”, “went to”, “to the”, “the market”, ..., “to buy”, “buy some”,
    “some flowers”]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Bob went”, “went to”, “to the”, “the market”, ..., “to buy”, “buy some”,
    “some flowers”]'
- en: The advantage in this representation (letter level) is that the vocabulary will
    be significantly smaller than if we were to use words as features for large corpora.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方法（字母级）的优点是，词汇表会显著小于我们使用单词作为大语料库特征时的情况。
- en: 'Next, we need to structure our data to be able to feed it into a learning model.
    For example, we will have data tuples of the form (*a statistic, a phrase explaining
    the statistic*) as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对数据进行结构化处理，以便将其输入到学习模型中。例如，我们将使用形如（*统计数据，解释该统计数据的短语*）的数据元组，如下所示：
- en: Total goals = 4, “The game was tied with 2 goals for each team at the end of
    the first half”
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总进球数 = 4，“上半场结束时，两队各打入了 2 个进球，比赛为平局”
- en: Team 1 = Manchester United, “The game was between Manchester United and Barcelona”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 队伍 1 = 曼联，“比赛在曼联和巴塞罗那之间进行”
- en: Team 1 goals = 5, “Manchester United managed to get 5 goals”
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 队伍 1 的进球数 = 5，“曼联成功打入了 5 个进球”
- en: '**The learning process** may comprise three sub-modules: a **Hidden Markov
    Model** (**HMM**), a sentence planner, and a discourse planner. An HMM is a recurrent
    model that can be used to solve time-series problems. For example, generating
    text is a time-series problem as the order of generated words matters. In our
    example, an HMM might learn to model language (i.e. generate meaningful text)
    by training on a corpus of statistics and related phrases. We will train the HMM
    so that it produces a relevant sequence of text, given the statistics as the starting
    input. Once trained, the HMM can be used for inference in a recursive manner,
    where we start with a seed (e.g. a statistic) and predict the first word of the
    description, then use the predicted word to generate the next word, and so on.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习过程**可能包括三个子模块：**隐马尔可夫模型**（**HMM**）、句子规划器和话语规划器。HMM 是一种递归模型，可以用于解决时间序列问题。例如，生成文本是一个时间序列问题，因为生成的单词顺序很重要。在我们的例子中，HMM
    可以通过在统计语料库和相关短语上训练来学习建模语言（即生成有意义的文本）。我们将训练 HMM，使其能够在统计数据作为输入的情况下，生成相关的文本序列。一旦训练完成，HMM
    就可以用于递归推理，我们从一个种子（例如统计数据）开始，预测描述的第一个单词，然后使用预测的单词生成下一个单词，依此类推。'
- en: Next, we can have a sentence planner that corrects any syntactical or grammatical
    errors, that might have been introduced by the model. For example, a sentence
    planner might take in the phrase, *I go house* and output *I go home*. For this,
    it can use a database of rules, which contains the correct way of conveying meanings,
    such as the need for a preposition between a verb and the word *house*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用一个句子规划器来修正模型可能引入的任何语法或语法错误。例如，句子规划器可能会将短语 *I go house* 转换为 *I go home*。为此，它可以使用一个包含正确表达方式的规则数据库，例如在动词和单词
    *house* 之间需要一个介词。
- en: 'Using the HMM and the sentence planner, we will have syntactically grammatically
    correct sentences. Then, we need to collate these phrases in such a way that the
    essay made from the collection of phrases is human readable and flows well. For
    example, consider the three phrases, *Player 10 of the Barcelona team scored a
    goal in the second half, Barcelona played against Manchester United, and Player
    3 from Manchester United got a yellow card in the first half*; having these sentences
    in this order does not make much sense. We like to have them in this order: *Barcelona
    played against Manchester United, Player 3 from Manchester United got a yellow
    card in the first half, and Player 10 of the Barcelona team scored a goal in the
    second half*. To do this, we use a discourse planner; discourse planners can organize
    a set of messages so that the meaning of them is conveyed properly.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HMM和句子规划器，我们将得到语法正确的句子。接下来，我们需要以一种方式将这些短语整理起来，使得由这些短语构成的文章既易于阅读又流畅。例如，考虑以下三句话，*巴塞罗那队的10号球员在下半场进了一个球，巴塞罗那与曼联对阵，曼联的3号球员在上半场领到一张黄牌*；将这些句子按此顺序排列并没有太大意义。我们希望按以下顺序排列它们：*巴塞罗那与曼联对阵，曼联的3号球员在上半场领到一张黄牌，巴塞罗那队的10号球员在下半场进了一个球*。为了做到这一点，我们使用话语规划器；话语规划器可以组织一组信息，使其意义能够正确传达。
- en: 'Now, we can get a set of arbitrary test statistics and obtain an essay explaining
    the statistics by following the preceding workflow, which is depicted in *Figure
    1.3*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获得一组任意的测试统计数据，并通过遵循上述工作流程来获取一篇解释这些统计数据的文章，如*图1.3*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_03.png](img/B14070_01_03.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_03.png](img/B14070_01_03.png)'
- en: 'Figure 1.3: The classical approach to solving a language modeling task'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：解决语言建模任务的经典方法
- en: Here, it is important to note that this is a very high-level explanation that
    only covers the main general-purpose components that are most likely to be included
    in traditional NLP. The details can largely vary according to the particular application
    we are interested in solving. For example, additional application-specific crucial
    components might be needed for certain tasks (a rule base and an alignment model
    in machine translation). However, in this book, we do not stress about such details
    as the main objective here is to discuss more modern ways of Natural Language
    Processing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，需要注意的是，这只是一个非常高层次的解释，仅涵盖了最有可能出现在传统自然语言处理（NLP）中的一些主要通用组件。具体细节在很大程度上会根据我们希望解决的特定应用而有所不同。例如，某些任务可能需要额外的应用特定的关键组件（例如，机器翻译中的规则库和对齐模型）。然而，在本书中，我们不会过多强调这些细节，因为我们的主要目标是讨论更现代的自然语言处理方法。
- en: Drawbacks of the traditional approach
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统方法的缺点
- en: 'Let’s list several key drawbacks of the traditional approach as this would
    lay a good foundation for discussing the motivation for deep learning:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列举出传统方法的几个关键缺点，这将为讨论深度学习的动机奠定良好的基础：
- en: The preprocessing steps used in traditional NLP forces a trade-off of potentially
    useful information embedded in the text (for example, punctuation and tense information)
    in order to make the learning feasible by reducing the vocabulary. Though preprocessing
    is still used in modern deep-learning-based solutions, it is not as crucial for
    them as it is for the traditional NLP workflow due to the large representational
    capacity of deep networks and their ability to optimize high-end hardware like
    GPUs.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统NLP中的预处理步骤需要在文本中嵌入的潜在有用信息（例如，标点符号和时态信息）之间做出权衡，以便通过减少词汇量使学习变得可行。尽管在现代基于深度学习的解决方案中仍然使用预处理，但由于深度网络的大量表示能力以及它们优化高端硬件（如GPU）的能力，这对于它们来说并不像传统NLP工作流中那样至关重要。
- en: Feature engineering is very labor-intensive. In order to design a reliable system,
    good features need to be devised. This process can be very tedious as different
    feature spaces need to be extensively explored and evaluated. Additionally, in
    order to effectively explore robust features, domain expertise is required, which
    can be scarce and expensive for certain NLP tasks.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程是一个非常劳动密集的过程。为了设计一个可靠的系统，需要设计好的特征。这个过程可能非常繁琐，因为不同的特征空间需要进行广泛的探索和评估。此外，为了有效地探索鲁棒特征，还需要领域专长，而对于某些NLP任务来说，这种专长可能稀缺且成本高昂。
- en: Various external resources are needed for it to perform well, and there are
    not many freely available ones. Such external resources often consist of manually
    created information stored in large databases. Creating one for a particular task
    can take several years, depending on the severity of the task (for example, a
    machine translation rule base).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使其表现良好，需要各种外部资源，而且可自由获取的资源并不多。这些外部资源通常由手动创建的信息存储在大型数据库中。为某个特定任务创建这样的资源可能需要数年时间，具体取决于任务的复杂性（例如，机器翻译规则库）。
- en: Now, let’s discuss how deep learning can help to solve NLP problems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下深度学习如何帮助解决NLP问题。
- en: The deep learning approach to Natural Language Processing
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在自然语言处理中的应用
- en: I think it is safe to say that deep learning revolutionized machine learning,
    especially in fields such as computer vision, speech recognition, and of course,
    NLP. Deep models created a wave of paradigm shifts in many of the fields in machine
    learning, as deep models learned rich features from raw data instead of using
    limited human-engineered features. This consequentially caused the pesky and expensive
    feature engineering to be obsolete. With this, deep models made the traditional
    workflow more efficient, as deep models perform feature learning and task learning,
    simultaneously. Moreover, due to the massive number of parameters (that is, weights)
    in a deep model, it can encompass significantly more features than a human could’ve
    engineered. However, deep models are considered a black box due to the poor interpretability
    of the model. For example, understanding the “how” and “what” features learned
    by deep models for a given problem is still an active area of research. But it
    is important to understand that there is a lot more research focusing on “model
    interpretability of deep learning models”.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为可以安全地说，深度学习革新了机器学习，特别是在计算机视觉、语音识别以及当然，NLP（自然语言处理）等领域。深度模型在机器学习的许多领域中引发了一波范式的转变，因为深度模型从原始数据中学习到了丰富的特征，而不是依赖于有限的人为工程特征。这导致了烦人的、昂贵的特征工程变得过时。通过这一点，深度模型使得传统的工作流程更加高效，因为深度模型同时进行特征学习和任务学习。此外，由于深度模型中大量的参数（即权重），它可以涵盖比人工工程特征更多的特征。然而，由于模型的可解释性差，深度模型被视为黑盒。例如，理解深度模型在给定问题中学习到的“如何”和“什么”特征仍然是一个活跃的研究领域。但重要的是要理解，越来越多的研究正在专注于“深度学习模型的可解释性”。
- en: A deep neural network is essentially an artificial neural network that has an
    input layer, many interconnected hidden layers in the middle, and finally, an
    output layer (for example, a classifier or a regressor). As you can see, this
    forms an end-to-end model from raw data to predictions. These hidden layers in
    the middle give the power to deep models as they are responsible for learning
    the good features from raw data, eventually succeeding at the task at hand. Let’s
    now understand the history of deep learning briefly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络本质上是一种人工神经网络，具有输入层、中间许多互联的隐藏层，最后是输出层（例如分类器或回归器）。正如你所看到的，这形成了一个从原始数据到预测的端到端模型。这些中间的隐藏层赋予了深度模型强大的能力，因为它们负责从原始数据中学习良好的特征，最终成功地完成任务。现在，让我们简要了解深度学习的历史。
- en: History of deep learning
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的历史
- en: Let’s briefly discuss the roots of deep learning and how the field evolved to
    be a very promising technique for machine learning. In 1960, Hubel and Weisel
    performed an interesting experiment and discovered that a cat’s visual cortex
    is made of simple and complex cells, and that these cells are organized in a hierarchical
    form. Also, these cells react differently to different stimuli. For example, simple
    cells are activated by variously oriented edges while complex cells are insensitive
    to spatial variations (for example, the orientation of the edge). This kindled
    the motivation for replicating a similar behavior in machines, giving rise to
    the concept of artificial neural networks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论深度学习的起源，以及这个领域是如何发展成机器学习中非常有前景的技术的。1960年，Hubel和Weisel进行了一项有趣的实验，发现猫的视觉皮层由简单细胞和复杂细胞组成，并且这些细胞以层级形式组织。除此之外，这些细胞对不同的刺激反应不同。例如，简单细胞对各种不同方向的边缘有反应，而复杂细胞对空间变化（例如，边缘的方向）不敏感。这激发了人们希望在机器中复制类似行为的动机，从而产生了人工神经网络的概念。
- en: In the years that followed, neural networks gained the attention of many researchers.
    In 1965, a neural network trained by a method known as the **Group Method of Data
    Handling** (**GMDH**) and based on the famous *Perceptron* by Rosenblatt, was
    introduced by Ivakhnenko and others. Later, in 1979, Fukushima introduced the
    *Neocognitron*, which planted the seeds for one of the most famous variants of
    deep models—Convolutional Neural Networks (CNNs). Unlike the perceptrons, which
    always took in a 1D input, a Neocognitron was able to process 2D inputs using
    convolution operations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的几年里，神经网络引起了许多研究人员的关注。1965年，一种通过**数据处理组法**（**GMDH**）训练的神经网络，基于Rosenblatt的著名*感知机*，由Ivakhnenko等人提出。随后，在1979年，福岛提出了*Neocognitron*，为深度模型的最著名变体之一——卷积神经网络（CNNs）播下了种子。与始终接受1D输入的感知机不同，Neocognitron能够通过卷积操作处理2D输入。
- en: Artificial neural networks used to backpropagate the error signal to optimize
    the network parameters by computing the gradients of the weights of a given layer
    with regards to the loss. Then, the weights are updated by pushing them in the
    opposite direction of the gradient, in order to minimize the loss. For a layer
    further away from the output layer (i.e. where the loss is computed), the algorithm
    uses the chain rule to compute gradients. The chain rule used with many layers
    led to a practical problem known as the vanishing gradients problem, strictly
    limiting the potential number of layers (depth) of the neural network. The gradients
    of layers closer to the inputs (i.e. further away from the output layer), being
    very small, cause the model training to stop prematurely, leading to an underfitted
    model. This is known as the **vanishing gradients phenomenon**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络通过反向传播误差信号来优化网络参数，方法是计算给定层权重相对于损失的梯度。然后，通过将权重推向梯度的反方向来更新它们，以最小化损失。对于距离输出层更远的层（即计算损失的地方），算法使用链式法则来计算梯度。使用多层链式法则导致了一个实际问题，称为梯度消失问题，严格限制了神经网络的层数（深度）。距离输入层较近的层（即距离输出层较远的层）的梯度非常小，导致模型训练提前停止，从而导致欠拟合的模型。这就是**梯度消失现象**。
- en: Then, in 2006, it was found that *pretraining* a deep neural network by minimizing
    the *reconstruction error* (obtained by trying to compress the input to a lower
    dimensionality and then reconstructing it back into the original dimensionality)
    for each layer of the network provides a good initial starting point for the weight
    of the neural network; this allows a consistent flow of gradients from the output
    layer to the input layer. This essentially allowed neural network models to have
    more layers without the ill effects of the vanishing gradient. Also, these deeper
    models were able to surpass traditional machine learning models in many tasks,
    mostly in computer vision (for example, test accuracy for the MNIST handwritten
    digit dataset). With this breakthrough, deep learning became the buzzword in the
    machine learning community.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2006年，人们发现通过最小化*重建误差*（通过尝试将输入压缩为更低的维度，然后将其重建回原始维度）对深度神经网络进行*预训练*，能够为网络的每一层提供一个良好的初始起点；这使得从输出层到输入层的梯度能够保持一致流动。这本质上使得神经网络模型能够有更多层，而不会出现梯度消失的负面影响。此外，这些更深的模型能够在许多任务中超越传统的机器学习模型，尤其是在计算机视觉方面（例如，MNIST手写数字数据集的测试准确率）。随着这一突破，深度学习成为了机器学习领域的流行词。
- en: Things started gaining progressive momentum when, in 2012, AlexNet (a deep convolutional
    neural network created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton)
    won the **Large Scale Visual Recognition Challenge** (**LSVRC**) 2012 with an
    error decrease of 10% from the previous best. During this time, advances were
    made in speech recognition, wherein state-of-the-art speech recognition accuracies
    were reported using deep neural networks. Furthermore, people began to realize
    that **Graphical Processing Units** (**GPUs**) enable more parallelism, which
    allows for faster training of larger and deeper networks compared with **Central
    Processing Units** (**CPUs**).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，AlexNet（由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton创建的深度卷积神经网络）赢得了2012年**大规模视觉识别挑战赛**（**LSVRC**），相较于之前的最佳成绩，错误率下降了10%。在此期间，语音识别取得了进展，使用深度神经网络的最新语音识别技术报告了很高的准确率。此外，人们开始意识到**图形处理单元**（**GPU**）能够提供更多的并行计算，从而相比**中央处理单元**（**CPU**）能够更快地训练更大、更深的网络。
- en: Deep models were further improved with better model initialization techniques
    (for example, Xavier initialization), making the time-consuming pretraining redundant.
    Also, better nonlinear activation functions, such as **Rectified Linear Units**
    (**ReLUs**), were introduced, which alleviated the adversities of the vanishing
    gradient in deeper models. Better optimization (or learning) techniques, such
    as the Adam optimizer, automatically tweaked individual learning rates of each
    parameter among the millions of parameters that we have in the neural network
    model, which rewrote the state-of-the-art performance in many different fields
    of machine learning, such as object classification and speech recognition. These
    advancements also allowed neural network models to have large numbers of hidden
    layers. The ability to increase the number of hidden layers (that is, to make
    the neural networks deep) is one of the primary contributors to the significantly
    better performance of neural network models compared with other machine learning
    models. Furthermore, better intermediate regularizers, such as batch normalization
    layers, have improved the performance of deep nets for many tasks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型通过更好的模型初始化技术（例如 Xavier 初始化）得到了进一步的改进，这使得耗时的预训练变得不再必要。同时，引入了更好的非线性激活函数，如**修正线性单元**（**ReLUs**），缓解了深层模型中梯度消失的困境。更好的优化（或学习）技术，如
    Adam 优化器，自动调整神经网络模型中成千上万个参数的个体学习率，这在许多不同的机器学习领域（如目标分类和语音识别）中重新定义了最先进的性能。这些进展还使得神经网络模型能够拥有大量的隐藏层。增加隐藏层的数量（即使神经网络更深）是神经网络模型相比其他机器学习模型显著提高性能的主要因素之一。此外，更好的中间正则化方法，如批量归一化层，也提高了深度网络在许多任务中的表现。
- en: Later, even deeper models such as ResNets, Highway Nets, and Ladder Nets were
    introduced, which had hundreds of layers and billions of parameters. It was possible
    to have such an enormous number of layers with the help of various empirically
    and theoretically inspired techniques. For example, ResNets use shortcut connections
    or skip connections to connect layers that are far apart, which minimizes the
    diminishing of gradients layer to layer, as discussed earlier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，甚至更深的模型，如ResNets、Highway Nets 和 Ladder Nets被引入，这些模型拥有数百层和数十亿个参数。借助各种经验性和理论启发的技术，实现如此巨大的层数成为可能。例如，ResNets使用捷径连接或跳跃连接，将远距离的层连接起来，从而最小化了前面提到的层与层之间梯度的消失问题。
- en: The current state of deep learning and NLP
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习和自然语言处理的现状
- en: Many different deep models have seen the light since their inception in early
    2000\. Even though they share a resemblance, such as all of them using nonlinear
    transformation of the inputs and parameters, the details can vary vastly. For
    example, a **CNN** can learn from two-dimensional data (for example, RGB images)
    as it is, while a multilayer perceptron model requires the input to be unwrapped
    to a one-dimensional vector, causing the loss of important spatial information.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自2000年初以来，许多不同的深度模型问世。尽管它们有相似之处，例如都使用输入和参数的非线性变换，但细节可以有很大差异。例如，**CNN**可以直接从二维数据（例如RGB图像）中学习，而多层感知机模型则要求输入展开为一维向量，导致重要的空间信息丢失。
- en: When processing text, as one of the most intuitive interpretations of text is
    to perceive it as a sequence of characters, the learning model should be able
    to do time-series modeling, thus requiring the *memory* of the past. To understand
    this, think of a language modeling task; the next word for the word *cat* should
    be different from the next word for the word *climbed*. One such popular model
    that encompasses this ability is known as a **Recurrent Neural Network** (**RNN**).
    We will see in *Chapter 6*, *Recurrent Neural Networks*,how exactly RNNs achieve
    this by going through interactive exercises.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理文本时，作为文本的最直观解释之一是将其视为字符序列，因此学习模型应能够进行时间序列建模，从而需要对过去的*记忆*。为了解释这一点，可以考虑语言建模任务；单词*cat*的下一个单词应该与单词*climbed*的下一个单词不同。一个具有这种能力的流行模型被称为**递归神经网络**（**RNN**）。我们将在*第6章*，*递归神经网络*中，通过互动练习了解RNN是如何实现这一目标的。
- en: It should be noted that *memory* is not a trivial operation that is inherent
    to a learning model. Conversely, ways of persisting memory should be carefully
    designed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，*记忆*并不是学习模型固有的简单操作。相反，记忆的持久化方式需要精心设计。
- en: Also, the term *memory* should not be confused with the learned weights of a
    non-sequential deep network that only looks at the current input, where a sequential
    model (for example, an RNN) will look at both the learned weights and the previous
    element of the sequence to predict the next output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，术语*memory*不应与非顺序深度网络的学习权重混淆，后者仅查看当前输入，而顺序模型（例如 RNN）会查看学习的权重以及序列中的前一个元素，以预测下一个输出。
- en: One prominent drawback of RNNs is that they cannot remember more than a few
    (approximately seven) time steps, thus lacking long-term memory. **Long Short-Term
    Memory** (**LSTM**) networks are an extension of RNNs that encapsulate long-term
    memory. Therefore, often LSTMs are preferred over standard RNNs, nowadays. We
    will peek under the hood in *Chapter 7, Understanding Long Short-Term Memory Networks*,
    to understand them better.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的一个突出缺点是它们无法记住超过几个（大约七个）时间步，因此缺乏长期记忆。**长短期记忆**（**LSTM**）网络是 RNN 的扩展，封装了长期记忆。因此，LSTM
    在如今通常优于标准的 RNN。我们将在*第7章，理解长短期记忆网络*中深入了解它们，以便更好地理解它们。
- en: Finally, a model known as a **Transformer** has been introduced by Google fairly
    recently, which has outperformed many of the previous state-of-the-art models
    such as LSTMs on a plethora of NLP tasks. Previously, both recurrent models (e.g.
    LSTMs) and convolutional models (e.g. CNNs) dominated the NLP domain. For example,
    CNNs have been used for sentence classification, machine translation, and sequence-to-sequence
    learning tasks. However, Transformers use an entirely different approach where
    they use neither recurrence nor convolution, but an attention mechanism. The attention
    mechanism allows the model to look at the entire sequence at once, to produce
    a single output. For example, consider the sentence “*The animal didn’t cross
    the road because it was tired*.” While generating intermediate representations
    for the word “*it*,” it would be useful for the model to learn that “*it*” refers
    to the “*animal*”. The attention mechanism allows the Transformer model to learn
    such relationships. This capability cannot be replicated with standard recurrent
    models or convolutional models. We will investigate these models further in *Chapter
    10, Transformers* and *Chapter 11, Image Captioning with Transformers.*
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，Google 最近引入了一种被称为**Transformer**的模型，它在许多自然语言处理任务中超过了许多先前的最先进模型，如 LSTM。此前，递归模型（如
    LSTM）和卷积模型（如 CNN）主导了 NLP 领域。例如，CNN 被用于句子分类、机器翻译和序列到序列的学习任务。然而，Transformer 使用的是完全不同的方法，它既不使用递归也不使用卷积，而是采用了注意力机制。注意力机制使得模型能够一次性查看整个序列，以产生单一的输出。例如，考虑句子“*The
    animal didn’t cross the road because it was tired*。”在生成“*it*”一词的中间表示时，模型会从学习中知道“*it*”指代的是“*animal*”。注意力机制使
    Transformer 模型能够学习这种关系。这个能力是标准的递归模型或卷积模型无法复制的。我们将在*第10章，Transformer*和*第11章，使用
    Transformer 进行图像字幕生成*中进一步探讨这些模型。
- en: 'In summary, we can mainly separate deep networks into three categories: the
    non-sequential models that deal with only a single input at a time for both training
    and prediction (for example, image classification), the sequential models that
    cope with sequences of inputs of arbitrary length (for example, text generation
    where a single word is a single input), and finally, attention-based models that
    look at the sequence at once such as the Transformer, BERT, and XLNet, which are
    pretrained models based on the Transformer architecture. We can categorize non-sequential
    (also called feed-forward) models into deep (approximately less than 20 layers)
    and very deep networks (can be greater than hundreds of layers). The sequential
    models are categorized into short-term memory models (for example, RNNs), which
    can only memorize short-term patterns, and long-term memory models, which can
    memorize longer patterns. In *Figure 1.4*, we outline the discussed taxonomy.
    You don’t have to understand these different deep learning models fully at this
    point, but it illustrates the diversity of the deep learning models:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以将深度网络主要分为三类：非序列模型，这类模型在训练和预测时每次只处理一个输入（例如，图像分类）；序列模型，这类模型处理任意长度的输入序列（例如，文本生成，其中每个单词是一个输入）；最后是基于注意力的模型，它们一次性查看整个序列，例如Transformer、BERT和XLNet，这些是基于Transformer架构的预训练模型。我们可以将非序列模型（也称为前馈模型）进一步分为深度模型（大约少于20层）和非常深的网络（可以超过数百层）。序列模型则分为短期记忆模型（例如RNNs），这些模型只能记住短期模式，以及长期记忆模型，它们能记住更长时间的模式。在*图1.4*中，我们概述了上述分类。你现在不需要完全理解这些不同的深度学习模型，但它们展示了深度学习模型的多样性：
- en: '![Diagram  Description automatically generated with low confidence](img/B14070_01_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图示  自动生成的描述，置信度较低](img/B14070_01_04.png)'
- en: 'Figure 1.4: A general taxonomy of the most commonly used deep learning methods,
    categorized into several classes'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：常用深度学习方法的一般分类，分为几类
- en: Now, let’s take our first steps toward understanding the inner workings of a
    neural network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们迈出第一步，理解神经网络的内部工作原理。
- en: Understanding a simple deep model – a fully connected neural network
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解一个简单的深度模型——一个全连接神经网络
- en: Now, let’s have a closer look at a deep neural network in order to gain a better
    understanding. Although there are numerous different variants of deep models,
    let’s look at one of the earliest models (dating back to 1950–60), known as a
    **fully connected neural network** (**FCNN**), sometimes called a multilayer perceptron.
    *Figure 1.5* depicts a standard three-layered FCNN.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更仔细地看一下深度神经网络，以便更好地理解。虽然深度模型有许多不同的变体，但我们先来看其中最早的模型之一（可以追溯到1950-60年代），即**全连接神经网络**（**FCNN**），有时也叫做多层感知器。*图1.5*展示了一个标准的三层FCNN。
- en: The goal of an FCNN is to map an input (for example, an image or a sentence)
    to a certain label or annotation (for example, the object category for images).
    This is achieved by using an input *x* to compute *h* – a hidden representation
    of *x* – using a transformation such as *![](img/B14070_01_011.png)*; here, *W*
    and *b* are the weights and bias of the FCNN, respectively, and *![](img/B14070_01_021.png)*
    is the sigmoid activation function. Neural networks use non-linear activation
    functions at every layer. Sigmoid activation is one such activation. It is an
    element-wise transformation applied to the output of a layer, where the sigmoidal
    output of *x* is given by, *![](img/B14070_01_031.png)*. Next, a classifier is
    placed on top of the FCNN that gives the ability to leverage the learned features
    in hidden layers to classify inputs. The classifier is a part of the FCNN and
    yet another hidden layer with some weights, *W*[s] and a bias, *b*[s]. Also, we
    can calculate the final output of the FCNN as *![](img/B14070_01_041.png)*. For
    example, a softmax classifier can be used for multi-label classification problems.
    It provides a normalized representation of the scores output by the classifier
    layer. That is, it will produce a valid probability distribution over the classes
    in the classifier layer. The label is considered to be the output node with the
    highest softmax value. Then, with this, we can define a classification loss that
    is calculated as the difference between the predicted output label and the actual
    output label. An example of such a loss function is the mean squared loss. You
    don’t have to worry if you don’t understand the actual intricacies of the loss
    function. We will discuss quite a few of them in later chapters. Next, the neural
    network parameters, *W*, *b*, *W*[s], and *b*[s], are optimized using a standard
    stochastic optimizer (for example, the stochastic gradient descent) to reduce
    the classification loss of all the inputs. *Figure 1.5* depicts the process explained
    in this paragraph for a three-layer FCNN. We will walk through the details on
    how to use such a model for NLP tasks, step by step, in *Chapter 3*, *Word2vec
    – Learning Word Embeddings*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: FCNN的目标是将输入（例如，图像或句子）映射到某个标签或注释（例如，图像的物体类别）。这是通过使用输入 *x* 来计算 *h* —— *x* 的隐藏表示
    —— 来实现的，使用如 *![](img/B14070_01_011.png)* 这样的变换；这里，*W* 和 *b* 分别是FCNN的权重和偏置，*![](img/B14070_01_021.png)*
    是 sigmoid 激活函数。神经网络在每一层使用非线性激活函数。sigmoid 激活就是一种这样的激活函数。它是对一层输出的逐元素变换，其中 *x* 的sigmoid
    输出由 *![](img/B14070_01_031.png)* 给出。接下来，在FCNN的顶部放置一个分类器，它可以利用隐藏层中学习到的特征来对输入进行分类。分类器是FCNN的一部分，实际上是另一个隐藏层，具有一些权重
    *W*[s] 和偏置 *b*[s]。另外，我们可以计算FCNN的最终输出为 *![](img/B14070_01_041.png)*。例如，可以使用softmax分类器来处理多标签分类问题。它提供了分类器层输出分数的归一化表示。也就是说，它将为分类器层中的各个类别生成一个有效的概率分布。标签被视为具有最高softmax值的输出节点。然后，通过此方法，我们可以定义一个分类损失，该损失通过预测的输出标签与实际输出标签之间的差异来计算。一个这样的损失函数的例子是均方损失。如果你不理解损失函数的具体细节也没关系。我们将在后续章节中讨论其中的许多内容。接下来，神经网络的参数
    *W*、*b*、*W*[s] 和 *b*[s] 将通过标准的随机优化器（例如，随机梯度下降）进行优化，以减少所有输入的分类损失。*图1.5* 展示了这一段中解释的过程，适用于三层FCNN。我们将在
    *第3章*《Word2vec——学习词嵌入》中，逐步讲解如何将这样的模型应用于NLP任务。
- en: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_05.png](img/B14070_01_05.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_05.png](img/B14070_01_05.png)'
- en: 'Figure 1.5: An example of a fully connected neural network (FCNN)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：一个完全连接的神经网络（FCNN）示例
- en: Let’s look at an example of how to use a neural network for a sentiment analysis
    task. Consider that we have a dataset where the input is a sentence expressing
    a positive or negative opinion about a movie and a corresponding label saying
    if the sentence is actually positive (**1**) or negative (**0**). Then, we are
    given a test dataset, where we have single-sentence movie reviews, and our task
    is to classify these new sentences as positive or negative.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用神经网络进行情感分析任务的例子。假设我们有一个数据集，其中输入是一句关于电影的正面或负面评价，且对应的标签表示该句子是否真的为正面（**1**）或负面（**0**）。然后，我们得到一个测试数据集，其中包含单句的电影评论，任务是将这些新句子分类为正面或负面。
- en: 'It is possible to use a neural network (which can be deep or shallow, depending
    on the difficulty of the task) for this task by adhering to the following workflow:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过遵循以下工作流程来使用神经网络（可以是深度或浅层的，取决于任务的难度）来完成此任务：
- en: Tokenize the sentence by words.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对句子按单词进行分词。
- en: Convert the sentences into a fixed sized numerical representation (for example,
    Bag-of-Words representation). A fixed sized representation is needed as fully
    connected neural networks require a fixed sized input.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将句子转换为固定大小的数字表示（例如，词袋表示）。需要固定大小的表示，因为全连接神经网络需要固定大小的输入。
- en: Feed the numerical inputs to the neural network, predict the output (positive
    or negative), and compare that with the true target.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数字输入传递给神经网络，预测输出（正面或负面），并与真实目标进行比较。
- en: Optimize the neural network using a desired loss function.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所需的损失函数优化神经网络。
- en: In this section we looked at deep learning in more detail. We looked at the
    history and the current state of NLP. Finally, we looked at a fully connected
    neural network (a type of deep learning model) in more detail.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们更详细地探讨了深度学习。我们回顾了NLP的历史和当前状态。最后，我们更详细地讨论了全连接神经网络（一种深度学习模型）。
- en: Now that we’ve introduced NLP, its tasks, and how approaches to it have evolved
    over the years, let’s take a moment to look the technical tools required for the
    rest of this book.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经介绍了自然语言处理（NLP），它的任务，以及这些方法如何随着时间的推移不断发展，我们不妨稍作停顿，看看本书剩余部分所需的技术工具。
- en: Introduction to the technical tools
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术工具介绍
- en: In this section, you will be introduced to the technical tools that will be
    used in the exercises of the following chapters. First, we will present a brief
    introduction to the main tools provided. Next, we will present a rough guide on
    how to install each tool along with hyperlinks to detailed guides provided by
    the official websites. Additionally, we will share tips on how to make sure that
    the tools were installed properly.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向你介绍在接下来的章节练习中将使用的技术工具。首先，我们将简要介绍所提供的主要工具。接下来，我们将提供如何安装每个工具的粗略指南，并附上官方网站提供的详细指南的超链接。此外，我们还将分享一些确保工具正确安装的提示。
- en: Description of the tools
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具描述
- en: We will use Python as the coding/scripting language. Python is a very versatile,
    easy-to-set-up coding language that is heavily used by the scientific and machine
    learning communities.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python作为编程/脚本语言。Python是一种非常多用途、易于设置的编程语言，广泛应用于科学和机器学习社区。
- en: Additionally, there are numerous scientific libraries built for Python, catering
    to areas ranging from deep learning to probabilistic inference to data visualization.
    TensorFlow is one such library that is well known among the deep learning community,
    providing many basic and advanced operations that are useful for deep learning.
    Next, we will use Jupyter Notebook in all our exercises as it provides a rich
    and interactive environment for coding compared to using Python scripts. We will
    also use pandas, NumPy and scikit-learn — three popular — two popular libraries
    for Python—for various miscellaneous purposes such as data preprocessing. Another
    library we will be using for various text-related operations is NLTK—the Python
    Natural Language Toolkit. Finally, we will use Matplotlib for data visualization.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有许多为Python构建的科学库，涵盖了从深度学习到概率推理再到数据可视化等多个领域。TensorFlow就是其中一个在深度学习社区中广为人知的库，提供了许多基本和高级操作，适用于深度学习。接下来，我们将在所有练习中使用Jupyter
    Notebook，因为它提供了比使用Python脚本更丰富和互动的编程环境。我们还将使用pandas、NumPy和scikit-learn——这三个流行的Python库——进行各种杂项任务，如数据预处理。另一个我们将用于处理文本相关操作的库是NLTK——Python自然语言工具包。最后，我们将使用Matplotlib进行数据可视化。
- en: Installing Anaconda and Python
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Anaconda和Python
- en: Python is hassle-free to install in any of the commonly used operating systems,
    such as Windows, macOS, or Linux. We will use Anaconda to set up Python, as it
    does all the laborious work for setting up Python as well as the essential libraries.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Python在Windows、macOS或Linux等常用操作系统中安装起来非常方便。我们将使用Anaconda来设置Python，因为它会为Python及其必需的库做所有繁琐的设置工作。
- en: 'To install Anaconda, follow these steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Anaconda，请按照以下步骤操作：
- en: Download Anaconda from [https://www.continuum.io/downloads](https://www.continuum.io/downloads)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.continuum.io/downloads](https://www.continuum.io/downloads)下载Anaconda。
- en: Select the appropriate OS and download Python 3.7
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择适合的操作系统并下载Python 3.7。
- en: Install Anaconda by following the instructions at [https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/)上的说明安装Anaconda。
- en: 'To check whether Anaconda was properly installed, open a Terminal window (Command
    Prompt in Windows), and then run the following command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查 Anaconda 是否正确安装，请打开一个终端窗口（Windows 中的命令提示符），然后运行以下命令：
- en: '[PRE0]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If installed properly, the version of the current Anaconda distribution should
    be shown in the Terminal.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安装正确，当前 Anaconda 发行版的版本应该显示在终端中。
- en: Creating a Conda environment
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个 Conda 环境
- en: One of the attractive features of Anaconda is that it allows you to create multiple
    Conda, or virtual, environments. Each Conda environment can have its own environment
    variables and Python libraries. For example, one Conda environment can be created
    to run TensorFlow 1.x, whereas another can run TensorFlow 2.x. This is great because
    it allows you to separate your development environments from any changes taking
    place in the host’s Python installation. Then, you can activate or deactivate
    Conda environments depending on which environment you want to use.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 的一个吸引人的特性之一是它允许你创建多个 Conda 或虚拟环境。每个 Conda 环境可以有自己的环境变量和 Python 库。例如，可以创建一个
    Conda 环境来运行 TensorFlow 1.x，而另一个可以运行 TensorFlow 2.x。这很棒，因为它允许你将开发环境与主机的 Python
    安装中发生的任何更改分开。然后，你可以根据需要激活或取消激活 Conda 环境。
- en: 'To create a Conda environment, follow these instructions:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个 Conda 环境，请按照以下说明操作：
- en: Run Conda and create `-n packt.nlp.2 python=3.7` in the terminal window using
    the command `conda create -n packt.nlp.2 python=3.7`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端窗口中运行 Conda 并创建 `-n packt.nlp.2 python=3.7`，使用命令 `conda create -n packt.nlp.2
    python=3.7`。
- en: Change directory (`cd`) to the project directory.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改目录（`cd`）到项目目录。
- en: Activate the new Conda environment by entering `activate` `packt.nlp.2` in the
    terminal. If successfully activated, you should see `(packt.nlp.2)` appearing
    before the user prompt in the terminal.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `activate packt.nlp.2` 在终端中激活新的 Conda 环境。如果成功激活，你应该在终端的用户提示之前看到 `(packt.nlp.2)`。
- en: Install the required libraries using one of the following options.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下选项之一安装所需的库。
- en: If you **have a GPU**, use `pip install -r requirements-base.txt -r requirements-tf-gpu.txt`
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果**有 GPU**，使用 `pip install -r requirements-base.txt -r requirements-tf-gpu.txt`
- en: If you **do not have a GPU**, use `pip install -r requirements-base.txt -r requirements-tf.txt`
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果**没有 GPU**，使用 `pip install -r requirements-base.txt -r requirements-tf.txt`
- en: Next, we’ll discuss some prerequisites for GPU support for TensorFlow.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论 TensorFlow GPU 支持的一些先决条件。
- en: TensorFlow (GPU) software requirements
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow（GPU）软件要求
- en: If you are using the TensorFlow GPU version, you will need to satisfy certain
    software requirements such as installing CUDA 11.0\. An exhaustive list is available
    at [https://www.tensorflow.org/install/gpu#software_requirements](https://www.tensorflow.org/install/gpu#software_requirements).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 TensorFlow GPU 版本，则需要满足诸如安装 CUDA 11.0 的特定软件要求。详细列表请参见[https://www.tensorflow.org/install/gpu#software_requirements](https://www.tensorflow.org/install/gpu#software_requirements)。
- en: Accessing Jupyter Notebook
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问 Jupyter Notebook
- en: 'After running the `pip install` command, you should have Jupyter Notebook available
    in the Conda environment. To check whether Jupyter Notebook is properly installed
    and can be accessed, follow these steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完 `pip install` 命令后，你应该可以在 Conda 环境中使用 Jupyter Notebook。要检查 Jupyter Notebook
    是否安装正确并且可访问，请按照以下步骤操作：
- en: Open a Terminal window.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个终端窗口。
- en: Activate the `packt.nlp.2` Conda environment if it is not already by running
    `activate packt.nlp.2`
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果还没有激活`packt.nlp.2` Conda 环境，请运行 `activate packt.nlp.2`
- en: 'Run the command: `jupyter notebook`'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行命令：`jupyter notebook`
- en: 'You should be presented with a new browser window that looks like *Figure 1.6*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 应该会打开一个看起来像*图 1.6*的新浏览器窗口：
- en: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_06.png](img/B14070_01_06.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH01\B08681_01_06.png](img/B14070_01_06.png)'
- en: 'Figure 1.6: Jupyter Notebook installed successfully'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.6: 成功安装 Jupyter Notebook'
- en: Verifying the TensorFlow installation
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证 TensorFlow 安装
- en: In this book, we are using TensorFlow 2.7.0\. It is important that you install
    the exact version used in the book as TensorFlow can undergo many changes while
    migrating from one version to the other. TensorFlow should be installed in the
    `packt.nlp.2` Conda environment if everything went well. If you are having trouble
    installing TensorFlow, you can find guides and troubleshooting instructions at
    [https://www.tensorflow.org/install](https://www.tensorflow.org/install).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用的是 TensorFlow 2.7.0。重要的是，你需要安装本书中使用的确切版本，因为 TensorFlow 在从一个版本迁移到另一个版本时可能会发生许多变化。如果一切顺利，TensorFlow
    应该安装在 `packt.nlp.2` Conda 环境中。如果你在安装 TensorFlow 时遇到问题，可以在 [https://www.tensorflow.org/install](https://www.tensorflow.org/install)
    找到安装指南和故障排除说明。
- en: 'To check whether TensorFlow installed properly, follow these steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查 TensorFlow 是否正确安装，按照以下步骤操作：
- en: Open Command Prompt in Windows or Terminal in Linux or macOS.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Windows 中打开命令提示符，或在 Linux 或 macOS 中打开终端。
- en: Activate the `packt.nlp.2` Conda environment.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活 `packt.nlp.2` Conda 环境。
- en: Type `python` to enter the Python prompt. You should now see the Python version
    right below. Make sure that you are using Python 3.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `python` 进入 Python 提示符。你现在应该会看到下面显示的 Python 版本。确保你使用的是 Python 3。
- en: 'Next, enter the following commands:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，输入以下命令：
- en: '[PRE1]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If all went well, you should not have any errors (there might be warnings if
    your computer does not have a dedicated GPU, but you can ignore them) and TensorFlow
    version 2.7.0 should be shown.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该不会遇到任何错误（如果你的计算机没有专用 GPU，可能会有警告，但可以忽略它们），并且应该显示 TensorFlow 版本 2.7.0。
- en: 'Many cloud-based computational platforms are also available, where you can
    set up your own machine with various customization (operating system, GPU card
    type, number of GPU cards, and so on). Many are migrating to such cloud-based
    services due to the following benefits:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于云的计算平台也可用，你可以在这些平台上设置自己的机器，并进行各种定制（操作系统、GPU 卡类型、GPU 卡数量等）。许多人正在迁移到这样的云服务，原因包括以下几个优势：
- en: More customization options
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多定制选项
- en: Less maintenance effort
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的维护工作
- en: No infrastructure requirements
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无基础设施要求
- en: 'Several popular cloud-based computational platforms are as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的基于云的计算平台如下：
- en: '**Google Colab**: [https://colab.research.google.com/](https://colab.research.google.com/)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab**：[https://colab.research.google.com/](https://colab.research.google.com/)'
- en: '**Google Cloud Platform** (**GCP**): [https://cloud.google.com/](https://cloud.google.com/)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌云平台**（**GCP**）：[https://cloud.google.com/](https://cloud.google.com/)'
- en: '**Amazon Web Services** (**AWS**): [https://aws.amazon.com/](https://aws.amazon.com/)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊云服务**（**AWS**）：[https://aws.amazon.com/](https://aws.amazon.com/)'
- en: Google Colab is a great cloud-based platform that allows you to write TensorFlow
    code and execute it on CPU/GPU hardware for free.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 是一个优秀的基于云的平台，允许你编写 TensorFlow 代码并免费在 CPU/GPU 硬件上执行。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we broadly explored NLP to get an impression of the kind of
    tasks involved in building a good NLP-based system. First, we explained why we
    need NLP and then discussed various tasks of NLP to generally understand the objective
    of each task and how difficult it is to succeed at them.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们广泛探索了 NLP，以便了解构建一个好的基于 NLP 的系统所涉及的任务类型。首先，我们解释了为什么需要 NLP，然后讨论了 NLP 的各种任务，以便大致了解每个任务的目标以及成功完成这些任务的难度。
- en: After that, we looked at the classical approach of solving NLP and went into
    the details of the workflow using an example of generating sport summaries for
    football games. We saw that the traditional approach usually involves cumbersome
    and tedious feature engineering. For example, in order to check the correctness
    of a generated phrase, we might need to generate a parse tree for that phrase.
    Then, we discussed the paradigm shift that transpired with deep learning and saw
    how deep learning made the feature engineering step obsolete. We started with
    a bit of time-traveling to go back to the inception of deep learning and artificial
    neural networks and worked our way through to the massive modern networks with
    hundreds of hidden layers. Afterward, we walked through a simple example illustrating
    a deep model—a multilayer perceptron model—to understand the mathematical wizardry
    taking place in such a model (on the surface of course!).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们研究了传统的NLP解决方法，并通过生成足球比赛的运动总结的示例深入了解了工作流程。我们看到传统方法通常涉及繁琐且耗时的特征工程。例如，为了检查生成短语的正确性，我们可能需要为该短语生成一个解析树。接着，我们讨论了深度学习带来的范式转变，并了解了深度学习如何让特征工程的步骤变得过时。我们从时光旅行开始，回到深度学习和人工神经网络的起源，一直到现代具有数百个隐藏层的大型网络。之后，我们通过一个简单的例子展示了深度模型——多层感知机模型——以便理解这种模型中的数学奇迹（当然是从表面理解！）。
- en: With a foundation in both the traditional and modern ways of approaching NLP,
    we then discussed the roadmap to understand the topics we will be covering in
    the book, from learning word embeddings to mighty LSTMs, and to state-of-the-art
    Transformers! Finally, we set up our virtual Conda environment by installing Python,
    scikit-learn, Jupyter Notebook, and TensorFlow.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握传统与现代自然语言处理（NLP）方法的基础上，我们讨论了理解本书所涵盖主题的路线图，从学习词嵌入到强大的LSTM，再到最前沿的Transformer！最后，我们通过安装Python、scikit-learn、Jupyter
    Notebook和TensorFlow，设置了我们的虚拟Conda环境。
- en: In the next chapter, you will learn the basics of TensorFlow. By the end of
    the chapter, you should be comfortable with writing a simple algorithm that can
    take some input, transform the input through a defined function and output the
    result.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习TensorFlow的基础知识。到章末时，你应该能轻松编写一个简单的算法，该算法可以接收输入，通过定义的函数转换输入并输出结果。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的GitHub页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，与超过1000名成员一起学习：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
