- en: Text Representations - Words to Numbers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本表示 - 将单词转换为数字
- en: Computers today cannot act on words or text directly. They need to be represented
    by meaningful number sequences. These long sequences of decimal numbers are called
    vectors, and this step is often referred to as the vectorization of text.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的计算机不能直接对单词或文本进行操作。它们需要通过有意义的数字序列来表示。这些长序列的十进制数字被称为向量，这一步骤通常被称为文本的向量化。
- en: 'So, where are these word vectors used:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些词向量在哪里使用：
- en: In text classification and summarization tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本分类和摘要任务中
- en: During similar word searches, such as synonyms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在类似词的搜索中，例如同义词
- en: In machine translation (for example, when translating text from English to German)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器翻译中（例如，将文本从英语翻译成德语）
- en: When understanding similar texts (for example, Facebook articles)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当理解类似文本时（例如，Facebook文章）
- en: During question and answer sessions, and general tasks (for example, chatbots
    used in appointment scheduling)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在问答会话和一般任务中（例如，用于预约安排的聊天机器人）
- en: 'Very frequently, we see word vectors used in some form of categorization task.
    For instance, using a machine learning or deep learning model for sentiment analysis,
    with the following text vectorization methods:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 非常频繁地，我们看到词向量在某种形式的分类任务中使用。例如，使用机器学习或深度学习模型进行情感分析，以下是一些文本向量化方法：
- en: TF-IDF in sklearn pipelines with logistic regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在sklearn管道中使用逻辑回归的TF-IDF
- en: GLoVe by Stanford, looked up via Gensim
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福大学的GLoVe，通过Gensim查找
- en: fastText by Facebook using pre-trained vectors
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook的fastText使用预训练的向量
- en: We have already seen TF-IDF examples, and will see several more throughout this
    book. This chapter will instead cover the other ways in which you can vectorize
    your text corpus or a part of it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了TF-IDF的示例，并且在这本书的其余部分还将看到更多。本章将介绍其他可以将您的文本语料库或其部分向量的方法。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: How to vectorize a specific dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何向量化特定数据集
- en: How to make document embedding
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何制作文档嵌入
- en: Vectorizing a specific dataset
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化特定数据集
- en: This section focuses almost exclusively on word vectors and how we can leverage
    the Gensim library to perform them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节几乎完全专注于词向量以及我们如何利用Gensim库来执行它们。
- en: 'Some of the questions we want to answer in this section include these:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中想要回答的一些问题包括这些：
- en: How do we use original embedding, such as GLoVe?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用原始嵌入，如GloVe？
- en: 'How do we handle Out of Vocabulary words? (Hint: fastText)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理词汇表外的单词？（提示：fastText）
- en: How do we train our own word2vec vectors on our own corpus?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在我们自己的语料库上训练自己的word2vec向量？
- en: How do we train our own word2vec vectors?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何训练我们自己的word2vec向量？
- en: How do we train our own fastText vectors?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何训练我们自己的fastText向量？
- en: How do we use similar words to compare both of the above?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用相似词来比较上述两者？
- en: 'First, let''s get started with some simple imports, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从一些简单的导入开始，如下所示：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please ensure that your Gensim version is at least 3.4.0\. This is a very popular
    package which is maintained and developed mostly by text processing experts over
    at RaRe Technologies. They use the same library in their own work for enterprise
    B2B consulting. Large parts of Gensim's internal implementations are written in
    Cython for speed. It natively uses multiprocessing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您的Gensim版本至少为3.4.0。这是一个非常流行的包，主要由RaRe Technologies的文本处理专家维护和开发。他们在自己的企业B2B咨询工作中使用相同的库。Gensim的内部实现的大部分是用Cython编写的，以提高速度。它原生支持多进程。
- en: Here, the caveat is that Gensim is known to make breaking API changes, so consider
    double-checking the API when you use the code with their documents or tutorials.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，Gensim因其API的破坏性更改而闻名，因此在使用他们的文档或教程中的代码时，请考虑再次检查API。
- en: 'If you using a Windows machine, watch out for a warning similar to the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Windows机器，请注意以下类似的警告：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s get started by downloading the pre-trained GloVe embedding. While
    we could do this manually, here we will download it using the following Python
    code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始下载预训练的GloVe嵌入。虽然我们可以手动完成这项工作，但在这里我们将使用以下Python代码来下载：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will also reuse the `get_data` API to download any arbitrary files that we
    want to use throughout this section. We have also set up `tqdm` (Arabic for progress),
    which provides us with a progress bar by wrapping our `urlretrieve` iterable in
    it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重用`get_data` API来下载我们想要在本节中使用的任何任意文件。我们还设置了`tqdm`（阿拉伯语意为进度），它通过将我们的`urlretrieve`可迭代对象包装在其中，为我们提供了一个进度条。
- en: 'The following text is from tqdm''s README:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文本来自tqdm的README：
- en: tqdm works on any platform (Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),
    in any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: tqdm在Linux、Windows、Mac、FreeBSD、NetBSD、Solaris/SunOS等任何平台上工作，在任何控制台或GUI中，并且对IPython/Jupyter笔记本也很友好。
- en: tqdm does not require any dependencies (not even curses!), just Python and an
    environment supporting carriage return \r and line feed \n control characters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: tqdm不需要任何依赖（甚至不是curses！），只需要Python和一个支持回车符\r和换行符\n控制字符的环境。
- en: Right, let's finally download the embedding, shall we?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对了，我们终于可以下载嵌入文件了，不是吗？
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding snippet will download a large file with GLoVe word representations
    of 6 billion English words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段将下载一个包含60亿个英语单词的GloVe词表示的大型文件。
- en: 'Let''s quickly unzip the file using the Terminal or command-line syntax in
    Jupyter notebooks. You can also do this manually or by writing code, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速使用终端或Jupyter笔记本中的命令行语法解压文件。您也可以手动或通过编写代码来完成此操作，如下所示：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we have moved all of the `.txt` files back to the `data` directory. The
    thing to note here is in the filename, `glove.6B.50d.txt`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经将所有的`.txt`文件移回到`data`目录。这里需要注意的是文件名，`glove.6B.50d.txt`。
- en: '`6B` stands for the 6 billion words or tokens. `50d` stands for 50 dimensions,
    which means that each word is represented by a sequence of 50 numbers, and in
    this case, that''s 50 float numbers.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`6B`代表60亿个单词或标记。`50d`代表50个维度，这意味着每个单词都由一个由50个数字组成的序列表示，在这种情况下，那就是50个浮点数。'
- en: We'll now deviate a little to give you some context about word representations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将稍微偏离一下，给您一些关于词表示的背景信息。
- en: Word representations
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词表示
- en: The most popular names in word embedding are word2vec by Google (Mikolov) and
    GloVe by Stanford (Pennington, Socher, and Manning). fastText seems to be fairly
    popular for multilingual sub-word embeddings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入中最受欢迎的名字是谷歌的word2vec（Mikolov）和斯坦福大学的GloVe（Pennington、Socher和Manning）。fastText似乎在多语言子词嵌入中相当受欢迎。
- en: We advise that you don't use word2vec or GloVe. Instead, use fastText vectors,
    which are much better and from the same authors. word2vec was introduced by T.
    Mikolov et. al. ([https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en))
    when he was with Google, and it performs well on word similarity and analogy tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您不要使用word2vec或GloVe。相反，使用fastText向量，它们要好得多，并且来自同一作者。word2vec是由T. Mikolov等人（[https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en)）在谷歌工作时引入的，它在单词相似性和类比任务上表现良好。
- en: GloVe was introduced by Pennington, Socher, and Manning from Stanford in 2014
    as a statistical approximation for word embedding. The word vectors are created
    by the matrix factorization of word-word co-occurrence matrices.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe是由斯坦福大学的Pennington、Socher和Manning在2014年引入的，作为一种词嵌入的统计近似。词向量是通过词-词共现矩阵的矩阵分解来创建的。
- en: If picking between the lesser of two evils, we recommend using GloVe over word2vec.
    This is because GloVe outperforms word2vec in most machine learning tasks and
    NLP challenges in academia.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在两个恶之间选择较小的那个，我们推荐使用GloVe而不是word2vec。这是因为GloVe在大多数机器学习任务和学术界的NLP挑战中优于word2vec。
- en: 'Skipping the original word2vec here, we will now look at the following topics:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里跳过原始的word2vec，我们现在将探讨以下主题：
- en: How do we use original embeddings in GLoVe?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用GloVe中的原始嵌入？
- en: 'How do we handle out of vocabulary words? (Hint: fastText)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理词汇表外的单词？（提示：fastText）
- en: How do we train our own word2vec vectors on our own corpus?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在自己的语料库上训练自己的word2vec向量？
- en: How do we use pre-trained embeddings?
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何使用预训练的嵌入？
- en: We just downloaded these.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚下载了这些。
- en: The file formats used by word2vec and GloVe are slightly different from each
    other. We'd like a consistent API to look up any word embedding, and we can do
    this by converting the embedding format. Note that there are minor differences
    in how word embedding is stored.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec和GloVe使用的文件格式略有不同。我们希望有一个一致的API来查找任何词嵌入，我们可以通过转换嵌入格式来实现这一点。请注意，在词嵌入的存储方式上存在一些细微的差异。
- en: This format conversion can be done using Gensim's API called `glove2word2vec`.
    We will use this to convert our GloVe embedding information to the word2vec format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种格式转换可以使用Gensim的API `glove2word2vec`来完成。我们将使用它将我们的GloVe嵌入信息转换为word2vec格式。
- en: 'So, let''s get the imports out of the way and begin by setting up filenames,
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们先处理导入，然后设置文件名，如下所示：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We don''t want to repeat this step if we have already done the conversion once.
    The simplest way to check this is to see if `word2vec_output_file` already exists.
    We run the following conversion only if the file does not exist:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经进行过一次转换，我们不想重复这一步骤。最简单的方法是查看`word2vec_output_file`是否已经存在。只有在文件不存在的情况下，我们才运行以下转换：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding snippet will create a new file in a standard that is compatible
    with the rest of Gensim's API stack.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的片段将在一个与Gensim API堆栈兼容的标准中创建一个新文件。
- en: KeyedVectors API
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KeyedVectors API
- en: We now have to perform the simple task of loading vectors from a file. We do
    this using the `KeyedVectors` API in Gensim. The word we want to look up is the
    key, and the numerical representation of that word is the corresponding value.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须执行一个简单的任务，即从文件中加载向量。我们使用Gensim中的`KeyedVectors` API来完成这项工作。我们想要查找的单词是键，该单词的数值表示是相应的值。
- en: 'Let''s first import the API and set up the target filename as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入API并设置目标文件名，如下所示：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will load the entire text file into our memory, thus including the read
    from disk time. In most running processes, this is a one-off I/O step and is not
    repeated for every new data pass. This becomes our Gensim model, detailed as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把整个文本文件加载到我们的内存中，包括从磁盘读取的时间。在大多数运行过程中，这是一个一次性I/O步骤，并且不会为每次新的数据传递而重复。这将成为我们的Gensim模型，详细说明如下：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A faster SSD should definitely speed this up by an order of magnitude.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 更快的SSD应该可以显著提高速度，提高一个数量级。
- en: 'We can do some word vector arithmetic to compose and show that this representation
    captures semantic meaning as well. For instance, let''s repeat the following famous
    word vector example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行一些单词向量算术运算来组合并展示这种表示不仅捕捉了语义意义。例如，让我们重复以下著名的单词向量示例：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s now perform the mentioned arithmetic operations on the word vectors,
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对单词向量执行所提到的算术运算，如下所示：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We did this using the `most_similar` API. Behind the scenes, Gensim has done
    the following for us:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`most_similar` API完成了这项工作。在幕后，Gensim为我们做了以下操作：
- en: Looked up the vectors for `woman`, `king`, and `man`
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询了`woman`、`king`和`man`的向量
- en: Added `king` and `woman`, and subtracted the vector from `man` to find a resultant
    vector
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加了`king`和`woman`，并从`man`中减去向量以找到结果向量
- en: From the 6 billion tokens in this model, ranked all words by distance and found
    the closest words
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个模型中的60亿个标记中，按距离对所有单词进行排序并找到了最接近的单词
- en: Found the closest word
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到了最接近的单词
- en: 'We also added `topn=1` to tell the API that we are only interested in the closest
    match. The expected output is now just one word, `''queen''`, as shown in the
    following snippet:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了`topn=1`来告诉API我们只对最接近的匹配感兴趣。现在的预期输出只是一个单词，`'queen'`，如下面的片段所示：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Not only did we get the correct word, but also an accompanying decimal number!
    We will ignore that for now, but note that the number represents a notion of how
    close or similar the word is to the resultant vector that the API computed for
    us.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅得到了正确的单词，还得到了一个伴随的十进制数！我们现在暂时忽略这个数字，但请注意，这个数字代表了一个概念，即单词与API为我们计算的结果向量的接近程度或相似程度。
- en: 'Let''s try a few more examples, say social networks, as shown in the following
    snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试几个例子，比如社交网络，如下面的片段所示：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example, we are looking for a social network that is more casual than
    LinkedIn but more focused on learning than Facebook by adding Quora. As you can
    see in the following output, it looks like Twitter fits the bill perfectly:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们正在寻找一个比LinkedIn更随意但比Facebook更专注于学习的社交网络，通过添加Quora来实现。正如您在以下输出中可以看到，Twitter似乎完美地符合这一要求：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We could have equally expected Reddit to fit this.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样可以期待Reddit也能符合这一要求。
- en: 'So, can we use this approach to simply explore similar words in a larger corpus?
    It seems so. Let''s now look up words most similar to `india`, as shown in the
    following snippet. Notice that we are writing India in lowercase; this is because
    the model contains only lowercase words:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能否使用这种方法简单地探索更大语料库中的相似单词？看起来是这样的。现在让我们查找与`india`最相似的单词，如下面的片段所示。请注意，我们用小写写印度；这是因为模型中只包含小写单词：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is worth mentioning that these results might be a little biased because
    GloVe was primarily trained on a large news corpus called Gigaword:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这些结果可能有点偏颇，因为GloVe主要是基于一个名为Gigaword的大型新闻语料库进行训练的：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding result does make sense, keeping in mind that, in the foreign press,
    India is often mentioned because of its troubled relationships with its geographical
    neighbours, including Pakistan and Kashmir. Bangladesh, Nepal, and Sri Lanka are
    neighbouring countries, while Maharashtra is the home of India's business capital,
    Mumbai.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在外国媒体中，印度经常因其与地理邻国（包括巴基斯坦和克什米尔）的紧张关系而被提及，先前的结果确实是有意义的。孟加拉国、尼泊尔和斯里兰卡是邻国，而马哈拉施特拉邦是印度商业之都孟买的所在地。
- en: What is missing in both word2vec and GloVe?
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec 和 GloVe 缺少了什么？
- en: Neither GloVe nor word2vec can handle words they didn't see during training.
    These words are called **Out of Vocabulary** (OOV), in the literature.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 GloVe 还是 word2vec 都无法处理训练过程中没有见过的单词。这些单词在文献中被称为**词汇表外**（OOV）。
- en: 'Evidence of this can be seen if you try to look up nouns that are not frequently
    used, for example an uncommon name. As you can see in the following snippet, the
    model throws a `not in vocabulary` error:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试查找不常使用的名词，例如一个不常见的名字，就可以看到这种证据。如下面的代码片段所示，模型会抛出一个 `not in vocabulary` 错误：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下输出：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This result is also accompanied by an API warning that sometimes states the
    API will change in gensim v4.0.0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果还伴随着一个API警告，有时会声明API将在 gensim v4.0.0 中更改。
- en: How do we handle Out Of Vocabulary words?
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何处理词汇表外的单词？
- en: The authors of word2vec (Mikolov et al.) extended it to create fastText at Facebook.
    It works on character n-grams instead of entire words. Character n-grams are effective
    in languages with specific morphological properties.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 的作者（Mikolov 等人）将其扩展到 Facebook 上的 fastText。它使用字符 n-gram 而不是整个单词。字符 n-gram
    在具有特定形态学特性的语言中非常有效。
- en: We can create our own fastText embeddings, which can handle OOV tokens as well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建自己的 fastText 嵌入，它可以处理 OOV 标记。
- en: Getting the dataset
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据集
- en: 'First, we need to download the subtitles of several TED talks from a public
    dataset. We will train our fastText embeddings on these as well as the word2vec
    embeddings for comparison, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从公共数据集中下载几个 TED 演讲的字幕。我们将使用这些字幕以及 word2vec 嵌入进行对比训练，如下所示：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Python empowers us to access files inside a `.zip` file, which is easy to do
    with the `zipfile` package. Notice it is the `zipfile.zipFile` syntax that enables
    this.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Python 允许我们访问 `.zip` 文件内部的文件，使用 `zipfile` 包很容易做到这一点。注意，是 `zipfile.zipFile` 语法使得这一点成为可能。
- en: We additionally use the `lxml` package to `parse` the XML file inside the ZIP.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 `lxml` 包来解析 ZIP 文件内的 XML 文件。
- en: 'Here, we manually opened the file to find the relevant `content` path and look
    up `text()` from it. In this case, we are interested only in the subtitles and
    not any accompanying metadata, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们手动打开文件以找到相关的 `content` 路径，并从中查找 `text()`。在这种情况下，我们只对字幕感兴趣，而不是任何伴随的元数据，如下所示：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s now preview the first 500 characters of the following `input_text`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们预览一下以下 `input_text` 的前500个字符：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Since we are using subtitles from TED talks, there are some fillers that are
    not useful. These are often words describing sounds in parentheses and the speaker's
    name.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是 TED 演讲中的字幕，有一些填充词是没有用的。这些通常是括号中描述声音的单词和演讲者的名字。
- en: 'Let''s remove these fillers using some regex, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一些正则表达式删除这些填充词，如下所示：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Notice that we created `sentence_strings_ted` using the `.split(''\n'')` syntax
    on our entire corpus. Replace this with a better sentence tokenizer, such as that
    from spaCy or NLTK, as a reader exercise:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用 `.split('\n')` 语法在我们的整个语料库上创建了 `sentence_strings_ted`。作为读者练习，将其替换为更好的句子分词器，例如
    spaCy 或 NLTK 中的分词器：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Notice that each `sentences_ted` is now a list of a lists. Each element of the
    first list is a sentence, and each sentence is a list of tokens (words).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个 `sentences_ted` 现在都是一个列表的列表。第一个列表的每个元素都是一个句子，每个句子是一个标记（单词）的列表。
- en: 'This is the expected structure for training text embeddings using Gensim. We
    will write the following code to disk for easy retrieval later:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 Gensim 训练文本嵌入的预期结构。我们将把以下代码写入磁盘，以便稍后检索：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: I personally prefer JSON serialization over Pickle because it's slightly faster,
    more inter-operable among languages, and, most importantly, human readable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人更喜欢 JSON 序列化而不是 Pickle，因为它稍微快一点，跨语言互操作性更强，最重要的是，它对人类来说是可读的。
- en: Let's now train both fastText and word2vec embedding over this small corpus.
    Although small, the corpus we are using is representative of the data sizes usually
    seen in practice. Large annotated text corpora are extremely rare in the industry.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在这个小语料库上训练 fastText 和 word2vec 嵌入。虽然语料库很小，但我们使用的语料库代表了实践中通常看到的数据大小。在行业中，大型标注文本语料库极其罕见。
- en: Training fastText embedddings
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 fastText 嵌入
- en: 'Setting up imports is actually quite simple in the new Gensim API; just use
    the following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的 Gensim API 中设置导入实际上非常简单；只需使用以下代码：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step is to feed the text and make our text embedding model, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是输入文本并构建我们的文本嵌入模型，如下所示：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You will probably noticed the parameters we pass to make our model. The following
    list explains these parameters, as explained in the Gensim documentation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到我们传递给构建模型的参数。以下列表解释了这些参数，正如 Gensim 文档中所述：
- en: '`min_count (int, optional)`: The model ignores all words with total frequency
    lower than this'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count (int, 可选)`: 模型忽略所有总频率低于此值的单词'
- en: '`size (int, optional)`: This represents the dimensionality of word vectors'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size (int, 可选)`: 这表示单词向量的维度'
- en: '`window (int, optional)`: This represents the maximum distance between the
    current and predicted word within a sentence'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window (int, 可选)`: 这表示句子中当前单词和预测单词之间的最大距离'
- en: '`workers (int, optional)`: Use these many worker threads to train the model
    (this enables faster training with multicore machines; `workers=-1` means using
    one worker for each core available in your machine)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workers (int, 可选)`: 使用这些工作线程来训练模型（这可以在多核机器上实现更快的训练；`workers=-1` 表示使用机器中每个可用的核心的一个工作线程）'
- en: '`sg ({1, 0}, optional)`: This is a training algorithm, `skip-gram if sg=1`
    or CBOW'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sg ({1, 0}, 可选)`: 这是一个训练算法，当 `sg=1` 时为 `skip-gram` 或 CBOW'
- en: The preceding parameters are actually part of a larger list of levers that can
    move around to improve the quality of your text embedding. We encourage you to
    play around with the numbers in addition to exploring the other parameters that
    the Gensim API exposes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的参数实际上是更大列表的一部分，可以通过调整这些参数来改善文本嵌入的质量。我们鼓励您在探索 Gensim API 提供的其他参数的同时，尝试调整这些数字。
- en: 'Let''s now take a quick peek at the words most similar to India in this corpus,
    as ranked by fastText embedding-based similarity, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们快速查看这个语料库中按 fastText 嵌入相似度排名的与印度最相似的单词，如下所示：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we notice that fastText has leveraged the sub-word structure, such as
    `ind`, `ian`, and `dian`, to rank the words. We get both `indians` and `indian`
    in the top 3, which is quite good. This is one of the reasons fastText is effective—even
    for small training text tasks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们注意到 fastText 利用子词结构，例如 `ind`、`ian` 和 `dian` 来对单词进行排序。我们在前 3 名中得到了 `indians`
    和 `indian`，这相当不错。这是 fastText 有效的原因之一——即使是对于小型的训练文本任务。
- en: Let's now repeat the same process using word2vec and look at the words most
    similar to `india` there.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们重复使用 word2vec 进行相同的过程，并查看那里与 `india` 最相似的单词。
- en: Training word2vec embeddings
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 word2vec 嵌入
- en: 'Importing the model is simple, simply use the following command. By now, you
    should have an intuitive feel of how the Gensim model''s API is structured:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 导入模型很简单，只需使用以下命令。到现在为止，您应该对 Gensim 模型 API 的结构有了直观的了解：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we are using an identical configuration for the word2vec model as we did
    for fastText. This helps to reduce bias in the comparison.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用与 fastText 相同的配置来构建 word2vec 模型。这有助于减少比较中的偏差。
- en: 'You are encouraged to compare the best fastText model to the best word2vec
    model with the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励您使用以下方法比较最佳 fastText 模型和最佳 word2vec 模型：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Right, let''s now look at the words most similar to `india`, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对了，现在让我们查看与 `india` 最相似的单词，如下所示：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The words most similar to `india` have no tangible relation to the original
    word. For this particular dataset, and word2vec's training configuration, the
    model has not captured any semantic or syntactic information at all. This is not
    unusual since word2vec is meant to work on large text corpora.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `india` 最相似的单词与原始单词没有实质性的关系。对于这个特定的数据集和 word2vec 的训练配置，模型根本没有捕捉到任何语义或句法信息。这在
    word2vec 旨在处理大型文本语料库的情况下并不罕见。
- en: fastText versus word2vec
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fastText 与 word2vec 的比较
- en: 'According to the following preliminary comparison by Gensim:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下 Gensim 的初步比较：
- en: <q>fastText embeddings are significantly better than word2vec at encoding syntactic
    information. This is expected, since most syntactic analogies are morphology based,
    and the char n-gram approach of fastText takes such information into account.
    The original word2vec model seems to perform better on semantic tasks, since words
    in semantic analogies are unrelated to their char n-grams, and the added information
    from irrelevant char n-grams worsens the embeddings.</q>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <q>fastText在编码句法信息方面显著优于word2vec。这是预料之中的，因为大多数句法类比都是基于形态学的，而fastText的字符n-gram方法考虑了这种信息。原始的word2vec模型在语义任务上似乎表现更好，因为语义类比中的单词与它们的字符n-gram无关，而无关字符n-gram添加的信息反而恶化了嵌入。</q>
- en: 'The source for this is: *word2vec fasttext comparison notebook* ([https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此资料的来源是：*word2vec fasttext比较笔记本* ([https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb))。
- en: In general, we prefer fastText because of its innate ability to handle words
    that it has not seen in training. It is definitely better than word2vec when working
    with small data (as we've shown), and is at least as good as word2vec on larger
    datasets.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们更喜欢fastText，因为它天生具有处理训练中未见过的单词的能力。当处理小数据（如我们所展示的）时，它肯定优于word2vec，并且在大型数据集上至少与word2vec一样好。
- en: fastText is also useful in cases where we are processing text riddled with spelling
    mistakes. For example, it can leverage sub-word similarity to bring `indian` and
    `indain` close in the embedding space.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: fastText在处理充满拼写错误的文本时也非常有用。例如，它可以利用子词相似性在嵌入空间中将`indian`和`indain`拉近。
- en: In most downstream tasks, such as sentiment analysis or text classification,
    we continue to recommend GloVe over word2vec.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数下游任务中，如情感分析或文本分类，我们继续推荐GloVe优于word2vec。
- en: 'The following is our recommended rule of thumb for text embedding applications:
    fastText > GloVe > word2vec.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们为文本嵌入应用推荐的经验法则：fastText > GloVe > word2vec。
- en: Document embedding
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档嵌入
- en: Document embedding is often considered an underrated way of doing things. The
    key idea in document embedding is to compress an entire document, for example
    a patent or customer review, into one single vector. This vector in turn can be
    used for a lot of downstream tasks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 文档嵌入通常被认为是一种被低估的方法。文档嵌入的关键思想是将整个文档，例如专利或客户评论，压缩成一个单一的向量。这个向量随后可以用于许多下游任务。
- en: Empirical results show that document vectors outperform bag-of-words models
    as well as other techniques for text representation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，文档向量优于词袋模型以及其他文本表示技术。
- en: Among the most useful downstream tasks is the ability to cluster text. Text
    clustering has several uses, ranging from data exploration to online classification
    of incoming text in a pipeline.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最有用的下游任务之一是能够聚类文本。文本聚类有几种用途，从数据探索到在线分类管道中传入的文本。
- en: In particular, we are interested in document modeling using doc2vec on a small
    dataset. Unlike sequence models such as RNN, where a word sequence is captured
    in generated sentence vectors, doc2vec sentence vectors are word order independent.
    This word order independence means that we can process a large number of examples
    quickly, but it does mean capturing less of a sentence's inherent meaning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们对在小型数据集上使用doc2vec进行文档建模感兴趣。与捕捉在生成句子向量中的单词序列的序列模型（如RNN）不同，doc2vec句子向量与单词顺序无关。这种单词顺序无关性意味着我们可以快速处理大量示例，但这确实意味着捕捉到句子固有的意义较少。
- en: This section is loosely based on the doc2Vec API Tutorial from the Gensim repository.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节大致基于Gensim存储库中的doc2Vec API教程。
- en: 'Let''s first get the imports out of the way with the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先通过以下代码处理掉导入：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s pull out the talks from the `doc` variable we used earlier, as
    follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照以下方式从我们之前使用的`doc`变量中提取谈话：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To train the Doc2Vec model, each text sample needs a label or unique identifier.
    To do this, write a small function like the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练Doc2Vec模型，每个文本样本都需要一个标签或唯一标识符。为此，可以编写一个如下的小函数：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are a few things happening inside the preceding function; they are as
    follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中发生了一些事情；具体如下：
- en: '**Overloaded if condition**: This reads a test corpora and sets `tokens_only`
    to `True`.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重载if条件**：这个函数读取测试语料库并将`tokens_only`设置为`True`。'
- en: '**Target Label:** This assigns an arbitrary index variable, `i`, as the target
    label.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标标签**：这个函数分配一个任意的索引变量`i`作为目标标签。'
- en: '`**gensim.utils.simple_preprocess**`: This converts a document into a list
    of lowercase tokens, ignoring tokens that are too short or too long, which then
    yields instances of `TaggedDocument`. Since we are yielding instead of returning,
    this entire function is acting as a generator.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**gensim.utils.simple_preprocess**`：这个函数将文档转换为一系列小写标记，忽略过短或过长的标记，然后产生`TaggedDocument`实例。由于我们产生而不是返回，这个整个函数作为一个生成器在运行。'
- en: It is worth mentioning how this changes the function behavior. With a `return`
    in use, when a function is called it would have returned a specific object, such
    as `TaggedDocument` or `None` if the return is not specified. A `generator` function,
    on the other hand, only returns a `generator` object.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这如何改变函数的行为。使用`return`时，当函数被调用时，它会返回一个特定的对象，例如`TaggedDocument`或如果没有指定返回，则返回`None`。另一方面，生成器函数只返回一个`generator`对象。
- en: So, what do you expect the following code line to return?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你期望以下代码行返回什么？
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If you guessed correctly, you''ll know we expect it to return a `generator`
    object, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你猜对了，你就会知道我们期望它返回一个`generator`对象，如下所示：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding object means that we can read the text corpus element by element
    as and when it's needed. This is exceptionally useful if a training corpus is
    larger than your memory size.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的对象意味着我们可以按需逐个读取文本语料库的元素。如果训练语料库的大小超过你的内存大小，这特别有用。
- en: Understand how Python iterators and generators work. They make your code memory
    efficient and easy to read.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Python迭代器和生成器的工作方式。它们使你的代码内存效率更高，更容易阅读。
- en: 'In this particular case, we have a rather small training corpus as an example,
    so let''s read this entire corpus into working memory as a list of `TaggedDocument`
    objects, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，我们有一个相当小的训练语料库作为示例，所以让我们将整个语料库作为一个`TaggedDocument`对象的列表读入工作内存，如下所示：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `list()` statement runs over the entire corpora until the function stops
    yielding. Our variable `ted_talk_docs` should look something like the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`list()`语句遍历整个语料库，直到函数停止产生。我们的变量`ted_talk_docs`应该看起来像以下这样：'
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s quickly take a look at how many cores this machine has. We will use
    the following code to initialize the doc2vec model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下这台机器有多少个核心。我们将使用以下代码初始化doc2vec模型：
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Let's now go and initialize our doc2vec model from Gensim.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们去初始化我们的doc2vec模型，来自Gensim。
- en: Understanding the doc2vec API
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解doc2vec API
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s quickly understand the flags we have used in the preceding code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解前面代码中使用的标志：
- en: '`dm ({1,0}, optional)`: This defines the training algorithm; if `dm=1`, *distributed
    memory* (PV-DM) is used; otherwise, a distributed bag of words (PV-DBOW) is employed'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dm ({1,0}, optional)`：这定义了训练算法；如果`dm=1`，则使用*分布式内存*（PV-DM）；否则，使用分布式词袋（PV-DBOW）。'
- en: '`size (int, optional)`: This is the dimensionality of feature vectors'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size (int, optional)`：这是特征向量的维度'
- en: '`window (int, optional)`: This represents the maximum distance between the
    current and predicted word within a sentence'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window (int, optional)`：这代表句子中当前词和预测词之间的最大距离'
- en: '`negative (int, optional)`: If > `0`, negative sampling will be used (the int
    for negative values specifies how many *noise words* should be drawn, which is
    usually between 5-20); if set to `0`, no negative sampling is used'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative (int, optional)`：如果大于`0`，则使用负采样（负值的int指定应该抽取多少*噪声词*，这通常在5-20之间）；如果设置为`0`，则不使用负采样。'
- en: '`hs ({1,0}, optional)`: If `1`, hierarchical softmax will be used for model
    training, and if set to 0 where the negative is non-zero, negative sampling will
    be used'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hs ({1,0}, optional)`：如果设置为`1`，则用于模型训练的层次softmax；如果设置为0且负采样非零，则使用负采样。'
- en: '`iter (int, optional)`: This represents the number of iterations (epochs) over
    the corpus'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iter (int, optional)`：这代表在语料库上的迭代次数（周期）'
- en: The preceding list has been taken directly from the Gensim documentation. With
    that in mind, we'll now move on and explain some of the new terms introduced here,
    including negative sampling and hierarchical softmax.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的列表直接来自Gensim文档。考虑到这一点，我们现在将解释这里引入的一些新术语，包括负采样和层次softmax。
- en: Negative sampling
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负采样
- en: Negative sampling started out as a hack to speed up training and is now a well-accepted
    practice. The click point here is that in addition to training your model on what
    might be the correct answer, why not show it a few examples of wrong answers?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 负采样最初是一种为了加速训练而采用的技巧，现在已经成为一种被广泛接受的实践。这里的要点是，除了在可能正确的答案上训练你的模型之外，为什么不给它展示一些错误的例子？
- en: In particular, using negative sampling speeds up training by reducing the number
    of model updates required. Instead of updating the model for every single wrong
    word, we pick a small number, usually between 5 and 25, and train the model on
    them. So, we have reduced the number of updates from a few million, which is required
    for training on a large corpus, to a much smaller number. This is a classic software
    programming hack that works in academia too.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，使用负采样通过减少所需的模型更新次数来加速训练。我们不是为每个错误的单词更新模型，而是选择一个较小的数字，通常在5到25之间，并在它们上训练模型。因此，我们将从在大语料库上训练所需的几百万次更新减少到一个更小的数字。这是一个经典的软件编程技巧，在学术界也有效。
- en: Hierarchical softmax
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化软最大化
- en: The denominator term in our usual softmax is calculated using the sum operator
    over a large number of words. This normalization is a very expensive operation
    to do at each update during training.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常的softmax中的分母项是通过在大量单词上的求和操作来计算的。这种归一化操作在训练过程中的每次更新都是一个昂贵的操作。
- en: Instead, we can break this down into a specific sequence of calculations, which
    saves us from having to calculate expensive normalization over all words. This
    means that for each word, we use an approximation of sorts.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以将其分解为一系列特定的计算，这样可以节省我们计算所有单词上的昂贵归一化的时间。这意味着对于每个单词，我们使用某种近似。
- en: In practice, this approximation has worked so well that some systems use this
    in both training and inference time. For training, it can give a speed of up to
    50x (as per Sebastian Ruder, an NLP research blogger). In my own experiments,
    I have seen speed gains of around 15-25x.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这种近似已经非常有效，以至于一些系统在训练和推理时间都使用这种方法。对于训练来说，它可以提供高达50倍的速度（据NLP研究博主Sebastian
    Ruder所说）。在我的实验中，我看到了大约15-25倍的速度提升。
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The API to train a doc2vec model is slightly different. We use the `build_vocab`
    API first to build the vocabulary from a sequence of sentences, as shown in the
    previous snippet. We also pass our memory variable `ted_talk_docs` here, but we
    could have passed our once-only generator stream from the `read_corpora` function
    as well.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练doc2vec模型的API略有不同。我们首先使用`build_vocab` API从一系列句子中构建词汇表，如前一个代码片段所示。我们还将我们的内存变量`ted_talk_docs`传递到这里，但也可以传递从`read_corpora`函数中获得的单次生成器流。
- en: 'Let''s now set up some of the following sample sentences to find out whether
    our model learns something or not:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们设置一些以下样本句子，以找出我们的模型是否学到了什么：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Gensim has an interesting API that allows us to find a similarity value between
    two unseen documents using the model we just updated with our vocabulary, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim有一个有趣的API，允许我们使用我们刚刚用词汇表更新的模型在两个未见文档之间找到相似度值，如下所示：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The preceding output doesn't quite make sense, does it? The sentences we wrote
    should have some reasonable degree of similarity that is definitely not negative.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出并不完全合理，对吧？我们写的句子应该有一些合理的相似度，这绝对不是负面的。
- en: 'A-ha! We forgot to train the model on our corpora. Let''s do that now with
    the following code and then repeat the previous comparisons to see how they have
    changed:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！我们忘记在语料库上训练模型了。现在让我们用以下代码来做这件事，然后重复之前的比较，看看它们是如何变化的：
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: On a machine with BLAS set up, this step should take less than a few seconds.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在BLAS设置好的机器上，这一步应该不到几秒钟。
- en: 'We can actually pull out raw inference vectors for any particular sentence
    based on the following model:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以根据以下模型提取任何特定句子的原始推理向量：
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, the `infer_vector` API expects a list of tokens as an input. This should
    explain why we could have used `read_corpora` with `tokens_only =True` here as
    well.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`infer_vector` API期望一个标记列表作为输入。这应该解释了为什么我们也可以在这里使用`read_corpora`并设置`tokens_only
    = True`。
- en: 'Now that our model is trained, let''s compare the following sentences again:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的模型已经训练好了，让我们再次比较以下句子：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The new preceding output makes sense. The first and third sentences are definitely
    more similar than the first and second. In the spirit of exploring, let''s now
    see how similar the second and third sentences are, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 新的前置输出是有意义的。第一句和第三句确实比第一句和第二句更相似。在探索的精神下，现在让我们看看第二句和第三句的相似度，如下所示：
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Ah, this is better. Our result is now consistent with our expectations. The
    similarity value is more than the first and second sentences, but less than that
    of the first and third, which were also almost identical in intent.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，这样更好。我们的结果现在与我们的预期一致。相似度值大于第一句和第二句，但小于第一句和第三句，它们在意图上几乎相同。
- en: As an anecdotal observation or heuristic, truly similar sentences have a value
    greater than 0.8 on the similarity scale.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种轶事观察或启发式方法，真正相似的句子在相似度量表上的值大于0.8。
- en: We have mentioned how document or text vectors in general are a good way of
    exploring a data corpus. Next, we will do that to explore our corpus in a very
    shallow manner before leaving you with some ideas on how to continue the exploration.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，文档或文本向量通常是一种探索数据语料库的好方法。接下来，我们将以非常浅显的方式探索我们的语料库，然后给你一些继续探索的想法。
- en: Data exploration and model evaluation
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索和模型评估
- en: One simple technique for assessing any vectorization method is to simply use
    the training corpus as the test corpus. Of course, we expect that we will overfit
    our model to the training set, but that's fine.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 评估任何向量化的简单技术是将训练语料库作为测试语料库。当然，我们预计我们的模型会对训练集过度拟合，但这没关系。
- en: 'We can use the training corpus as a test corpus by doing the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式使用训练语料库作为测试语料库：
- en: Learning a new result or *inference* vectors for each document
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每份文档学习新的结果或*推理*向量
- en: Comparing the vector to all examples
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将向量与所有示例进行比较
- en: Ranking the document, sentence, and paragraph vectors according to the similarity
    score
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据相似度分数对文档、句子和段落向量进行排序
- en: 'Let''s do this in code, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下代码来做这件事：
- en: '[PRE46]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We have now figured out where each document placed itself in the rank. So,
    if the highest rank is the document itself, that''s good enough. As we said, we
    might overfit a little on the training corpus, but it''s a good sanity test nonetheless.
    We can find this using the frequency count via `Counter` as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经弄清楚每个文档在排名中的位置。所以，如果最高排名是文档本身，那就足够了。正如我们所说，我们可能在训练语料库上略微过度拟合，但这仍然是一个很好的合理性测试。我们可以通过以下方式使用`Counter`进行频率计数：
- en: '[PRE47]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `Counter` object tells us how many documents found themselves at what ranks.
    So, 2079 documents found themselves first (index 0), but two documents each found
    themselves second (index 1) and sixth (index 5) ranks. There is one document that
    ranked fifth (index 4) and third (index 2) respectively. All in all, this is a
    very good training performance, because 2079 out of 2084 documents ranked themselves
    first.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`Counter`对象告诉我们有多少文档发现自己处于什么排名。所以，2079份文档发现自己排名第一（索引0），但有两份文档分别发现自己排名第二（索引1）和第六（索引5）。有一份文档排名第五（索引4）和第三（索引2）。总的来说，这是一个非常好的训练性能，因为2084份文档中有2079份将自己排名为第一。'
- en: This helps us understand that the vectors did represent information in the document
    in a meaningful manner. If they did not, we would see a lot more rank dispersal.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于我们理解向量确实以有意义的方式在文档中代表了信息。如果它们没有这样做，我们会看到更多的排名分散。
- en: 'Let''s now quickly take a single document and find the most similar document
    to it, the least similar document, and a document that is somewhat in between
    in similarity. Do this with the following code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们快速取一份文档，找到与之最相似、最不相似以及介于两者之间的文档。以下代码可以完成这个任务：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Notice how we are choosing to preview a slice of the entire document for exploration.
    You are free to either do this or use a small text summarization tool to create
    your preview on the fly instead.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何选择预览整个文档的一部分以供探索的。你可以自由选择这样做，或者使用一个小型文本摘要工具来动态创建你的预览。
- en: 'The results are as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter was more than an introduction to the Gensim API. We now know how
    to load pre-trained GloVe vectors, and you can use these vector representations
    instead of TD-IDF in any machine learning model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这章不仅仅是Gensim API的介绍。我们现在知道如何加载预训练的GloVe向量，你可以在任何机器学习模型中使用这些向量表示，而不是TD-IDF。
- en: We looked at why fastText vectors are often better than word2vec vectors on
    a small training corpus, and learned that you can use them with any ML models.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了为什么fastText向量在小型训练语料库上通常比word2vec向量更好，并了解到你可以将它们用于任何ML模型。
- en: We learned how to build doc2vec models. You can now extend this doc2vec approach
    to build sent2vec or paragraph2vec style models as well. Ideally, paragraph2vec
    will change, simply because each document will be a paragraph instead.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何构建doc2vec模型。现在，你可以将这种doc2vec方法扩展到构建sent2vec或paragraph2vec风格的模型。理想情况下，paragraph2vec将会改变，仅仅是因为每个文档将变成一个段落。
- en: In addition, we now know how we can quickly perform sanity checks on our doc2vec
    vectors without using an annotated test corpora. We did this by checking the rank
    dispersal metric.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们现在知道如何在不使用标注测试语料库的情况下快速对doc2vec向量进行合理性检查。我们是通过检查排名分散度指标来做到这一点的。
