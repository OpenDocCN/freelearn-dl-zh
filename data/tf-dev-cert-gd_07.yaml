- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Image Classification with Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络进行图像分类
- en: '**Convolutional neural networks** (**CNNs**) are the go-to algorithms when
    it comes to image classification. In the 1960s, neuroscientists Hubel and Wiesel
    conducted a study on the visual cortex in cats and monkeys. Their work unraveled
    how we visually process information in a hierarchical structure, showing how visual
    systems are organized into a series of layers where each layer is responsible
    for a different aspect of visual processing. This earned them a Nobel Prize, but
    more importantly, it served as the basis upon which CNNs are built. CNNs, by virtue
    of their nature, are well designed to work with data with spatial structures such
    as images.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是进行图像分类时的首选算法。在1960年代，神经科学家Hubel和Wiesel对猫和猴子的视觉皮层进行了研究。他们的工作揭示了我们如何以层次结构处理视觉信息，展示了视觉系统是如何组织成一系列层次的，每一层都负责视觉处理的不同方面。这一发现为他们赢得了诺贝尔奖，但更重要的是，它为CNN的构建奠定了基础。CNN本质上非常适合处理具有空间结构的数据，例如图像。'
- en: However, in the early days, CNNs did not have the limelight due to a number
    of factors, such as insufficient training data, underdeveloped network architecture,
    insufficient computational resources, and the absence of modern techniques such
    as data augmentation and dropout. In the 2012 ImageNet Large Scale Visual Recognition
    Challenge, the **machine learning** (**ML**) community was taken by storm when
    a CNN architecture called AlexNet outperformed all other methods by a large margin.
    Today, ML practitioners apply CNNs to achieve state-of-the-art performance on
    computer vision tasks such as image classification, image segmentation, and object
    detection, among others.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在早期，由于多种因素的影响，例如训练数据不足、网络架构不成熟、计算资源匮乏，以及缺乏现代技术（如数据增强和丢弃法），CNN未能获得广泛关注。在2012年ImageNet大规模视觉识别挑战赛中，一种名为AlexNet的CNN架构震惊了**机器学习**（**ML**）社区，它比其他所有方法都超出了一个较大的优势。今天，机器学习从业者通过应用CNN，在计算机视觉任务（如图像分类、图像分割和目标检测等）中取得了最先进的表现。
- en: In this chapter, we will examine CNNs to see how they do things differently
    from the fully connected neural networks we have used so far. We will start with
    the challenges faced by fully connected networks when working with image data,
    after which we will explore the anatomy of CNNs. We will look at the core building
    blocks of CNN architecture and their overall impact on the performance of the
    network. Next, we will build an image classifier using a CNN architecture with
    the Fashion MNIST dataset, then move on to building a real-world image classifier.
    We will be working with color images of different sizes, and our target objects
    are in different positions within the image.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究CNN，看看它们与我们迄今为止使用的全连接神经网络有什么不同。我们将从全连接网络在处理图像数据时面临的挑战开始，接着探索CNN的结构。我们将研究CNN架构的核心构建模块及其对网络性能的整体影响。接下来，我们将使用Fashion
    MNIST数据集构建一个图像分类器，然后开始构建一个真实世界的图像分类器。我们将处理不同大小的彩色图像，且图像中的目标物体位置不同。
- en: By the end of this chapter, you will have a sound understanding of what CNNs
    are and why they are superior to fully connected networks when it comes to image
    classification tasks. Also, you will be able to effectively build, train, tune,
    and test CNN models on real-world image classification problems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将对卷积神经网络（CNN）有一个清晰的理解，并了解为什么在图像分类任务中，它们比全连接网络更具优势。此外，您还将能够在真实世界的图像分类问题中，构建、训练、调整和测试CNN模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: The anatomy of CNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN的结构解析
- en: Fashion MNIST with CNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN进行Fashion MNIST分类
- en: Real-world images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实世界的图像
- en: Weather data classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气数据分类
- en: Applying hyperparameters to improve the model’s performance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用超参数以提高模型的性能
- en: Evaluating image classifiers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估图像分类器
- en: Challenges of image recognition with fully connected networks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用全连接网络进行图像识别的挑战
- en: In [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with
    Neural Networks*, we applied a **deep neural network** (**DNN**) to the Fashion
    MNIST dataset. We saw how every neuron in the input layer is connected to every
    neuron in the hidden layer and those in the hidden layer are connected to neurons
    in the output layer, hence the name *fully connected*. While this architecture
    can solve many ML problems, they are not well suited for modeling image classification
    tasks, due to the spatial nature of image data. Let’s say you are looking at a
    picture of a face; the positioning and orientation of the features on the face
    enable you to know it is a human face even when you just focus on a specific feature,
    such as the eyes. Instinctively, you know it’s a face by virtue of the spatial
    relationship between the features of the face; however, DNNs do not see this bigger
    picture when looking at images. They process each pixel in the image as independent
    features, without taking the spatial relationships between these features into
    consideration.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B18118_05.xhtml#_idTextAnchor105)《使用神经网络进行图像分类》中，我们将**深度神经网络**（**DNN**）应用于时尚MNIST数据集。我们看到输入层中的每个神经元都与隐藏层中的每个神经元相连，而隐藏层中的神经元又与输出层中的神经元相连，因此称为*全连接*。虽然这种架构可以解决许多机器学习问题，但由于图像数据的空间特性，它并不适合用于图像分类任务。假设你正在看一张人脸的照片；人脸特征的位置和朝向使得即便你只专注于某个特定特征（例如眼睛），你也能知道那是一张人脸。你本能地通过人脸各个特征之间的空间关系知道它是一张人脸；然而，DNN在查看图像时无法看到这种全貌。它们将图像中的每个像素处理为独立的特征，而没有考虑这些特征之间的空间关系。
- en: Another issue with using fully connected architectures is the curse of dimensionality.
    Let’s say we are working with a real-world image of size 150 x 150 with 3 color
    channels, **red, green, and blue** (**RGB**); we will have an input size of 67,500\.
    As all the neurons are connected to neurons in the next layer, if we feed these
    values into a hidden layer with 500 neurons, we will have 67,500 x 500 = 33,750,000
    parameters, and this number of parameters will grow exponentially as we add more
    layers, making it resource intensive to apply this type of network to image classification
    tasks. Another accompanying problem we could stumble upon is overfitting; this
    happens due to the large number of parameters in our network. If we have images
    of larger sizes or we add more neurons to our networks, the number of trainable
    parameters will grow exponentially, and it could become impractical to train such
    a network due to cost and resource requirements. In light of these challenges,
    there is a need for a more sophisticated architecture, and this is where CNNs
    come in with their ability to uncover spatial relationships and hierarchies, ensuring
    features are recognized irrespective of where they are located within an image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全连接架构的另一个问题是维度灾难。假设我们正在处理一张尺寸为150 x 150、具有3个颜色通道的真实世界图像，**红色、绿色和蓝色**（**RGB**）；我们将有一个67,500的输入大小。由于所有神经元都与下一层的神经元相连，如果我们将这些值输入到一个拥有500个神经元的隐藏层中，那么参数数量将为67,500
    x 500 = 33,750,000，并且随着我们增加更多层，这个参数数量将呈指数级增长，使得将这种网络应用于图像分类任务变得资源密集。我们可能还会遇到的另一个问题是过拟合；这是由于网络中大量参数的存在。如果我们处理的是更大尺寸的图像，或者我们为网络增加更多神经元，训练的可训练参数数量将呈指数级增长，而训练这样一个网络可能变得不切实际，因为成本和资源需求过高。考虑到这些挑战，迫切需要一种更为复杂的架构，这就是CNN的优势所在，它能够揭示空间关系和层次结构，确保无论特征位于图像的哪个位置，都能被识别出来。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Spatial relationship** refers to how features within an image are arranged
    in relation to each other in terms of position, distance, and orientation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**空间关系**指的是图像中各个特征在位置、距离和朝向上的相对排列方式。'
- en: Anatomy of CNNs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的结构
- en: 'In the last section, we saw some of the challenges DNNs grappled with when
    dealing with visual recognition tasks. These issues include the lack of spatial
    awareness, high dimensionality, computational inefficiency, and the risk of overfitting.
    How do we overcome these challenges? This is where CNNs come into the picture.
    CNNs by design are uniquely positioned to handle image data. Let''s go through
    *Figure 7**.1* and uncover why and how CNNs stand out:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到DNN在处理视觉识别任务时面临的一些挑战。这些问题包括缺乏空间感知、高维性、计算低效和过拟合的风险。我们如何克服这些挑战呢？这就是CNN登场的地方。CNN天生就特别适合处理图像数据。让我们通过*图7.1*来了解CNN为何及如何脱颖而出：
- en: '![Figure 7.1 – The anatomy of a CNN](img/B18118_07_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – CNN的结构](img/B18118_07_01.jpg)'
- en: Figure 7.1 – The anatomy of a CNN
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – CNN的结构
- en: 'Let’s break down the different layers in the diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解图中的不同层：
- en: '**Convolutional layer – the eyes of the network**: Our journey begins with
    us feeding in images into the convolutional layer; this layer can be viewed as
    the “eyes of our network.” Their job is primarily to extract vital features. Unlike
    DNNs, where each neuron is connected to every neuron in the next layer, CNNs apply
    filters (also known as kernels) to capture local patterns within an image in a
    hierarchical fashion. The output of the interactions between a segment of the
    input image that the filter slides over is called a feature map. As shown in *Figure
    7**.2*, we can see that each feature map highlights specific patterns in the shirt
    that we passed into the network. Images go through CNNs in a hierarchical fashion
    with filters in the earlier layers adept at capturing simple features, while those
    in subsequent layers capture more complex patterns, mimicking the hierarchical
    structure of a human’s visual cortex. Another important property of CNNs is parameter
    sharing – this happens because patterns are only learned once and applied everywhere
    else across an image. This ensures that the visual ability of the model is not
    location-specific. In ML, we refer to this concept as **translation invariance**
    – the network’s ability to detect a shirt regardless of whether it is aligned
    to the right or left or centered within an image.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积层 – 网络的眼睛**：我们的旅程从将图像输入卷积层开始；这个层可以看作是我们网络的“眼睛”。它们的主要工作是提取重要特征。与DNN（深度神经网络）不同，DNN中的每个神经元都与下一层的每个神经元相连，而CNN通过应用滤波器（也叫做卷积核）以分层的方式捕捉图像中的局部模式。滤波器滑过输入图像的一段区域后，所产生的输出称为特征图。如*图
    7**.2*所示，我们可以看到每个特征图突出显示了我们输入到网络中的衬衫特定图案。图像通过CNN按层次结构处理，早期层的滤波器擅长捕捉简单的特征，而后续层的滤波器则捕捉更复杂的模式，模仿人类视觉皮层的层级结构。CNN的另一个重要特点是参数共享——这是因为模式只需要学习一次，然后应用到图像的其他地方。这确保了模型的视觉能力不依赖于特定位置。在机器学习中，我们称这个概念为**平移不变性**——网络能够检测衬衫，无论它是对齐在图像的左边、右边还是居中。'
- en: '![Figure 7.2 – Visualization of features captured by a convolutional layer](img/B18118_07_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 卷积层捕捉的特征可视化](img/B18118_07_02.jpg)'
- en: Figure 7.2 – Visualization of features captured by a convolutional layer
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 卷积层捕捉的特征可视化
- en: '**Pooling layer – the summarizer**: After the convolutional layer comes the
    pooling layer. This layer can be viewed as a summarizer in CNNs as it focuses
    on condensing the overall dimensionality of the feature maps while retaining important
    features, as illustrated in *Figure 7**.3*. By methodically downsampling the feature
    maps, CNNs significantly not only reduce the number of parameters required for
    image processing but also improve the overall computational efficiency of CNNs.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**池化层 – 总结器**：卷积层之后是池化层。这个层可以看作是CNN中的总结器，它专注于压缩特征图的整体维度，同时保留重要特征，如*图 7**.3*所示。通过系统地对特征图进行下采样，CNN不仅显著减少了图像处理所需的参数数量，而且提高了CNN的整体计算效率。'
- en: '![Figure 7.3 – An example of the pooling operation, preserving essential details](img/B18118_07_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 池化操作示例，保留重要细节](img/B18118_07_03.jpg)'
- en: Figure 7.3 – An example of the pooling operation, preserving essential details
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 池化操作示例，保留重要细节
- en: '**Fully connected layer – the decision maker**: Our image traverses a series
    of convolution and pooling layers that extract features and reduce the dimensionality
    of feature maps, eventually reaching the fully connected layer. This layer can
    be viewed as the decision maker. This layer offers high-level reasoning as it
    brings together all the important details collected through the layers and uses
    them to make the final classification verdict. One of the hallmarks of CNNs is
    its end-to-end learning process, which seamlessly integrates feature extraction
    and image classification. This methodological and hierarchical learning approach
    makes CNN a well-suited tool for image recognition and analysis.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全连接层 – 决策者**：我们的图像通过一系列卷积和池化层，这些层提取特征并减少特征图的维度，最终到达全连接层。这个层可以看作是决策者。这个层提供了高级推理，它将通过各层收集的所有重要细节整合在一起，用来做出最终的分类判断。CNN的一个显著特点是其端到端的学习过程，它无缝地整合了特征提取和图像分类。这种有条理且分层的学习方法使得CNN成为图像识别和分析的理想工具。'
- en: We have only scratched the surface of how CNNs work. Let's now drill down into
    the key operations that take place within the different layers, starting with
    convolutions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅触及了卷积神经网络（CNN）如何工作的表面。现在，让我们深入探讨不同层内发生的关键操作，从卷积开始。
- en: Convolutions
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: We now know that convolutional layers apply filters, which slide over patches
    of the input image. A typical CNN applies multiple filters, with each filter learning
    a specific kind of feature by interacting with the input image. By combining the
    detected features, a CNN arrives at a comprehensive understanding of the image
    features and uses this detailed information to classify the input image. Mathematically,
    this convolution process involves the dot product between a patch of the input
    image and the filter (a small matrix), as illustrated in *Figure 7**.4*. This
    process yields an output known as the **activation map** or **feature map**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，卷积层应用过滤器，这些过滤器会滑过输入图像的各个区域。典型的CNN应用多个过滤器，每个过滤器通过与输入图像的交互来学习特定类型的特征。通过组合检测到的特征，CNN能够全面理解图像特征，并利用这些详细信息来对输入图像进行分类。从数学上讲，这一卷积过程涉及输入图像的一块区域与过滤器（一个小矩阵）之间的点积运算，如*图7.4*所示。这个过程生成了一个输出，称为**激活图**或**特征图**。
- en: '![Figure 7.4 – Convolution operation – applying a filter to an input image
    to generate a feature map](img/B18118_07_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 卷积操作 – 应用过滤器到输入图像生成特征图](img/B18118_07_04.jpg)'
- en: Figure 7.4 – Convolution operation – applying a filter to an input image to
    generate a feature map
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 卷积操作 – 应用过滤器到输入图像生成特征图
- en: As the filter slides over the patches of the image, it produces a feature map
    for each dot operation. Feature maps are a representation of the input image in
    which certain visual patterns are enhanced by the filter, as shown in *Figure
    7**.2*. When we stack feature maps from all the filters in the network, we arrive
    at a rich, multi-faceted view of the input image, which gives later layers adequate
    information to learn more complex patterns.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当过滤器滑过图像的各个区域时，它为每个点操作生成一个特征图。特征图是输入图像的一个表示，其中某些视觉模式通过过滤器得到增强，如*图7.2*所示。当我们将网络中所有过滤器的特征图叠加在一起时，我们就能得到输入图像的丰富多维视图，为后续层提供足够的信息来学习更复杂的模式。
- en: '![Figure 7.5  – a (top) and b (bottom): Dot product computation](img/B18118_07_05.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – a（上）和 b（下）：点积计算](img/B18118_07_05.jpg)'
- en: 'Figure 7.5 – a (top) and b (bottom): Dot product computation'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – a（上）和 b（下）：点积计算
- en: In *Figure 7**.5 a*, we see a dot product operation in progress, as a filter
    slides over a section of the input image resulting in a destination pixel value
    of 13\. If we move the filter 1 pixel to the right, as shown in *Figure 7**.5
    b*, we will arrive at the next destination pixel value of 14\. If we continue
    sliding the filter one pixel at a time over the input image, we will achieve the
    complete output shown in *Figure* *7**.5 b*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.5 a*中，我们看到一个点积操作正在进行中，过滤器滑过输入图像的一个区域，得到一个目标像素值13。如果我们将过滤器向右移动1个像素，如*图7.5
    b*所示，我们将得到下一个目标像素值14。如果我们继续每次移动1个像素，滑过输入图像，就会得到*图7.5 b*中显示的完整输出。
- en: We have now seen how convolution operations work; however, there are various
    types of convolutional layers that we can apply in CNNs. For image classification,
    we typically use 2D convolutional layers, while we apply 1D convolution layers
    for audio processing and 3D convolutional layers for video processing. When designing
    our convolutional layer, there are a number of adjustable hyperparameters that
    can impact the performance of our network, such as the number of filters, the
    size of the filters, stride, and padding. It is pertinent to explore how these
    hyperparameters impact our network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了卷积操作的工作原理；然而，我们可以在CNN中应用多种类型的卷积层。对于图像分类，我们通常使用2D卷积层，而对于音频处理则应用1D卷积层，视频处理则使用3D卷积层。在设计卷积层时，有许多可调的超参数会影响网络的性能，比如滤波器的数量、滤波器的大小、步幅和填充。探讨这些超参数如何影响我们的网络是非常重要的。
- en: Let’s begin this exploration by looking at the impact of the number of filters
    in a convolutional layer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过观察卷积层中过滤器数量的影响，来开始这次探索。
- en: Impact of the number of filters
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器数量的影响
- en: By increasing the number of filters within a CNN, we empower it to learn a richer
    and more diverse representation of the input image. The more filters we have,
    the more representation will be learned. However, more filters mean more parameters
    to train, and this could not only increase the computational cost but also slow
    down the training process and increase the risk of overfitting. When deciding
    on the number of filters to apply to your network, it is important to consider
    the type of data in use. If the data has a lot of variability, you may need more
    filters to capture the diversity in your data, whereas with smaller datasets,
    you should be more conservative to reduce the risk of overfitting.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加卷积神经网络（CNN）中滤波器的数量，我们可以使其学习到输入图像更丰富、更多样化的表示。滤波器越多，学习到的表示越多。然而，更多的滤波器意味着更多的参数需要训练，这不仅会增加计算成本，还可能会减慢训练过程并增加过拟合的风险。在决定为网络应用多少滤波器时，重要的是要考虑所使用数据的类型。如果数据具有较大的变异性，可能需要更多的滤波器来捕捉数据的多样性；而对于较小的数据集，则应该更为保守，以减少过拟合的风险。
- en: Impact of the size of the filter
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器大小的影响
- en: 'We now know that filters are small matrices that slide over our input image
    to produce feature maps. The size of the filter we apply to the input image will
    determine the level and type of features that will be extracted from the input
    image. The filter size is the dimension of the filter – that is, the height and
    width of the filter matrix. Typically, you will come across 3x3, 5x5, and 7x7
    filters. Smaller filters will cover a smaller patch of the input image, while
    a larger filter will cover a more extensive section of the input image:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，滤波器是滑过输入图像以生成特征图的小矩阵。我们应用于输入图像的滤波器的大小将决定从输入图像中提取的特征的层次和类型。滤波器大小是指滤波器的维度——即滤波器矩阵的高度和宽度。通常，你会遇到3x3、5x5和7x7滤波器。较小的滤波器会覆盖输入图像的较小区域，而较大的滤波器则会覆盖输入图像的更广泛部分：
- en: '**Granularity of features** – Smaller filters such as 3x3 filters can be applied
    to capture finer and more local details of an image such as edges, textures, and
    corners, while larger filters such as 7x7 filters can learn broader patterns such
    as face shapes or object parts.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征的粒度** – 像3x3滤波器这样的较小滤波器可以用于捕捉图像中更细致、更局部的细节，如边缘、纹理和角落，而像7x7滤波器这样的较大滤波器则可以学习更广泛的模式，如面部形状或物体部件。'
- en: '**Computational efficiency** – Smaller filters cover a smaller receptive field
    of the input image, as illustrated in *Figure 7**.6*, which means they will require
    more operations.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算效率** – 较小的滤波器覆盖输入图像较小的感受野，如*图 7**.6*所示，这意味着它们需要更多的计算操作。'
- en: '![Figure 7.6 – Convolution operation with a 3x3 filter](img/B18118_07_06.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 使用3x3滤波器的卷积操作](img/B18118_07_06.jpg)'
- en: Figure 7.6 – Convolution operation with a 3x3 filter
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 使用3x3滤波器的卷积操作
- en: On the other hand, a larger filter covers a large segment of the input image,
    as shown in *Figure 7**.7*. However, many modern CNN architectures (for example,
    VGG) use 3x3 filters. Stacking these smaller filters together would increase the
    depth of the network and enhance the capabilities of these filters to capture
    more complex patterns with a smaller number of parameters in comparison to using
    a large filter, which makes smaller filters easier to train.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，较大的滤波器覆盖了输入图像的较大部分，如*图 7**.7*所示。然而，许多现代卷积神经网络（例如，VGG）使用的是3x3滤波器。将这些较小的滤波器叠加在一起会增加网络的深度，并增强这些滤波器捕捉更复杂模式的能力，同时相比于使用大滤波器，所需的参数更少，这使得较小的滤波器更容易训练。
- en: '![Figure 7.7 – Convolution operation with a 5x5 filter](img/B18118_07_07.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 使用5x5滤波器的卷积操作](img/B18118_07_07.jpg)'
- en: Figure 7.7 – Convolution operation with a 5x5 filter
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 使用5x5滤波器的卷积操作
- en: '**Parameter count** – A larger filter typically has more weight in comparison
    to a smaller filter; for example, a 5x5 filter will have 25 parameters and a 3x3
    filter will have 9 parameters. Here, we are ignoring the depth for the sake of
    simplicity. Hence, larger filters will contribute to making the model more complex
    in comparison to smaller filters.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数数量** – 较大的滤波器通常比较小的滤波器拥有更多的权重；例如，5x5滤波器会有25个参数，而3x3滤波器会有9个参数。这里为了简便起见，我们忽略了深度。因此，较大的滤波器相对于较小的滤波器，会使模型变得更加复杂。'
- en: Impact of stride
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步幅的影响
- en: Stride is an important hyperparameter in CNNs. It determines the number of pixels
    a filter moves over an input image. We can liken stride to the step we take when
    walking; if we take small steps, it will take us a longer time to reach our destination,
    while larger steps will ensure we reach it much quicker. In *Figure 7**.8*, we
    apply a stride of 1, which means the filter moves over the input image 1 pixel
    at a time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 步幅是卷积神经网络（CNN）中的一个重要超参数。它决定了滤波器在输入图像上移动的像素数。我们可以将步幅类比为我们走路时的步伐；如果步伐小，达到目的地会花费更长时间，而较大的步伐则可以更快到达目的地。在*图7.8*中，我们应用了步幅为1，这意味着滤波器每次在输入图像上移动1个像素。
- en: '![Figure 7.8 – Convolution operation with a stride of 1](img/B18118_07_08.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 步幅为1的卷积操作](img/B18118_07_08.jpg)'
- en: Figure 7.8 – Convolution operation with a stride of 1
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 步幅为1的卷积操作
- en: If we apply a stride of 2, it means the filter will move 2 pixels at a time,
    as illustrated in *Figure 7**.9*. We see that a large stride will lead to a reduced
    spatial dimension of the output feature map. We can see this when we compare the
    output of both figures.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应用步幅为2，意味着滤波器每次移动2个像素，如*图7.9*所示。我们看到，较大的步幅会导致输出特征图的空间维度减小。通过比较这两幅图的输出，我们可以看到这一点。
- en: '![Figure 7.9 – Convolution operation with a stride of 2](img/B18118_07_09.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 步幅为2的卷积操作](img/B18118_07_09.jpg)'
- en: Figure 7.9 – Convolution operation with a stride of 2
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 步幅为2的卷积操作
- en: When we apply a larger stride, it can increase the computational efficiency,
    but it also reduces the spatial resolution of the input image. Hence, we need
    to consider this trade-off when selecting the right stride for our network. Next,
    let's examine the border effect.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用较大的步幅时，它可以提高计算效率，但也会降低输入图像的空间分辨率。因此，在为我们的网络选择合适的步幅时，需要考虑这种权衡。接下来，我们来看看边界效应。
- en: The boundary problem
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边界问题
- en: When a filter slides over the input image performing convolution operations,
    it soon reaches the borders or the edges, where it becomes difficult to perform
    dot product operations due to the absence of pixels outside the image boundaries.
    This results in the output feature map being smaller than the input image as a
    result of loss of information around the edges or borders. This issue is referred
    to as the **edge effect** or the **boundary problem** in ML. In *Figure 7**.10*,
    we can observe that we are unable to perform a dot product operation on the bottom-left
    corner as we cannot center the filter over the highlighted pixel value of 3 without
    some part of the filter falling out of the defined image boundary.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当滤波器在输入图像上滑动并执行卷积操作时，很快就会到达边界或边缘，此时由于图像边界外缺少像素，难以执行点积操作。由于边缘或边界信息的丢失，这导致输出特征图小于输入图像。这个问题在机器学习中被称为**边效应**或**边界问题**。在*图7.10*中，我们可以看到，由于滤波器的一部分会超出定义的图像边界，我们无法将滤波器集中在突出显示的像素值3上，因此无法在左下角执行点积操作。
- en: '![Figure 7.10 – Showing the boundary problem](img/B18118_07_10.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 显示边界问题](img/B18118_07_10.jpg)'
- en: Figure 7.10 – Showing the boundary problem
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 显示边界问题
- en: To fix the boundary issue and preserve the spatial dimension of the output feature
    map, we may want to apply padding. Let's discuss this concept next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决边界问题并保持输出特征图的空间维度，我们可能需要应用填充。接下来我们讨论这个概念。
- en: Impact of padding
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充的影响
- en: '**Padding** is a technique we can apply to our convolution process to prevent
    the boundary effect by adding extra pixels to the edges, as shown in *Figure 7**.11*.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**填充**是一种可以应用于卷积过程中的技术，通过向边缘添加额外的像素来防止边界效应，如*图7.11*所示。'
- en: '![Figure 7.11 – A padded image undergoing convolution operation](img/B18118_07_11.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 进行卷积操作的填充图像](img/B18118_07_11.jpg)'
- en: Figure 7.11 – A padded image undergoing convolution operation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 进行卷积操作的填充图像
- en: 'We can now perform dot product operations on pixels at the edges, hence preserving
    information at the edges. Padding can also be applied to maintain the spatial
    dimension pre- and post-convolution. This could prove useful in deep CNN architecture
    with several convolution layers. Let’s look at the two main types of padding:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在边缘的像素上执行点积操作，从而保留边缘的信息。填充也可以应用于保持卷积前后的空间维度。这在具有多个卷积层的深度卷积神经网络（CNN）架构中可能会非常有用。我们来看一下两种主要的填充类型：
- en: '**Valid Padding (No Padding)**: Here, no padding is applied. This can be useful
    when we want to achieve a reduced spatial dimensionality, especially in deeper
    layers.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效填充（无填充）**：这里没有应用填充。当我们希望减少空间维度，尤其是在较深的层时，这种方法非常有用。'
- en: '**Same Padding**: Here, we set padding to ensure the output feature map and
    the input image dimensions are the same. We use this when maintaining spatial
    dimensionality is of paramount importance.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相同填充**：这里我们设置填充以确保输出特征图和输入图像的尺寸相同。我们在保持空间维度至关重要时使用这种方法。'
- en: Before we move on to examining the pooling layer, let's put together the different
    hyperparameters we have discussed in the convolutional layer and see them in action.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续检查池化层之前，让我们将之前讨论过的卷积层的不同超参数组合起来，并看看它们的实际效果。
- en: Putting it all together
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 综合起来
- en: In *Figure 7**.12*, we have a 7x7 input image and a 3x3 filter. Here, we use
    a stride of 1 and set padding to Valid (no padding).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.12*中，我们有一个7x7的输入图像和一个3x3的滤波器。在这里，我们使用步幅为1并将填充设置为有效（无填充）。
- en: '![Figure 7.12 – Setting the hyperparameters](img/B18118_07_12.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 设置超参数](img/B18118_07_12.jpg)'
- en: Figure 7.12 – Setting the hyperparameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 设置超参数
- en: 'To compute the output feature map of a convolutional operation, we can apply
    the following formula:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算卷积操作的输出特征图，我们可以应用以下公式：
- en: ( W − F + 2P _ S ) + 1
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (W − F + 2P _ S) + 1
- en: 'In this formula, the following applies:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，以下内容适用：
- en: '*W* represents the size of the input image'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W* 代表输入图像的大小'
- en: '*F* stands for the filter size'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F* 代表滤波器大小'
- en: '*S* represents stride'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* 代表步幅'
- en: '*P* stands for padding'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P* 代表填充'
- en: When we input the respective values into the equation, we get a resulting value
    of 5, which means we will have a 5x5 output feature map. If we alter any of the
    values, it will impact the size of the output feature map one way or the other.
    For example, if we increase the stride size, we will have a smaller output feature
    map, while if we set padding to same, this will increase the size of the output.
    We can now move on from the convolution operations and explore pooling next.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将相应的值代入公式时，得到的结果值为5，这意味着我们将得到一个5x5的输出特征图。如果我们更改任何一个值，它都会以某种方式影响输出特征图的大小。例如，如果我们增加步幅大小，输出特征图将变得更小，而如果我们将填充设置为same，则会增加输出的大小。现在，我们可以从卷积操作转到池化操作。
- en: Pooling
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: '**Pooling** is an important operation that takes play in the pooling layer
    of a CNN. It is a technique used to downsample the spatial dimension of individual
    feature maps generated by the convolutional layers. Let''s examine some important
    types of pooling layers. We’ll begin by exploring max pooling, as shown in *Figure
    7**.13*. Here, we see how max pooling operations work. The pooling layer simply
    takes the highest value from each region of the input data.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**池化**是一个重要的操作，发生在卷积神经网络（CNN）的池化层中。它是一种用于下采样卷积层生成的单个特征图空间维度的技术。让我们来看看一些常见的池化层类型。我们将从最大池化开始，如*图
    7.13*所示。在这里，我们可以看到最大池化操作是如何工作的。池化层简单地从输入数据的每个区域提取最大值。'
- en: '![Figure 7.13 – A max pooling operation](img/B18118_07_13.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 最大池化操作](img/B18118_07_13.jpg)'
- en: Figure 7.13 – A max pooling operation
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 最大池化操作
- en: Max pooling enjoys several benefits as it is intuitive and easy to implement.
    It is also efficient since it simply extracts the highest value in a region, and
    it has been applied with good effect across diverse tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化具有多个优点，因为它直观且易于实现。它也很高效，因为它只提取区域中的最大值，并且在各类任务中都取得了良好的效果。
- en: Average pooling, as the name suggests, reduces the data dimensionality by taking
    the average value for a designated region, as illustrated in *Figure 7**.14*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 平均池化，顾名思义，通过对指定区域取平均值来减少数据的维度，如*图 7.14*所示。
- en: '![Figure 7.14 – An average pooling operation](img/B18118_07_14.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 平均池化操作](img/B18118_07_14.jpg)'
- en: Figure 7.14 – An average pooling operation
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 平均池化操作
- en: On the other hand, min pooling extracts the minimum value in a specified region
    of the input data. Pooling reduces the spatial size of the output feature maps
    and this, in turn, reduces the memory requirement for storing intermediate representations.
    Pooling can be beneficial to a network; however, excessive pooling can be counterproductive
    as this could lead to information loss. After the pooling layer, we arrive at
    the fully connected layer, the decision maker of our network.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，最小池化会提取输入数据指定区域中的最小值。池化减少了输出特征图的空间大小，从而减少了存储中间表示所需的内存。池化对网络是有益的；然而，过度池化可能适得其反，因为这可能导致信息丢失。经过池化层后，我们到达了全连接层，这是我们网络的决策者。
- en: The fully connected layer
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接层
- en: The final component of our CNN architecture is the fully connected layer. Unlike
    the convolutional layer, here, every neuron is connected to every neuron in the
    next layer. This layer is responsible for decision-making, such as classifying
    whether our input image is a shirt or a hat. The fully connected layer takes the
    learned features from the earlier layers and maps them to their corresponding
    labels. We have now covered CNNs in theory; let's now proceed by applying them
    to our fashion dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CNN架构的最后一个组成部分是全连接层。与卷积层不同，在这里，每个神经元都与下一层中的每个神经元相连接。这个层负责决策，例如分类我们的输入图像是衬衫还是帽子。全连接层将从早期层学到的特征映射到相应的标签。现在我们已经在理论上覆盖了CNN，接下来我们将其应用到我们的时尚数据集上。
- en: Fashion MNIST 2.0
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fashion MNIST 2.0
- en: 'By now, you are already familiar with this dataset, as we used it in [*Chapter
    5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with Neural Networks*,
    and [*Chapter 6*](B18118_06.xhtml#_idTextAnchor129), *Improving the Model*. Now,
    let''s see how CNNs compare to the simple neural networks we have worked with
    so far. We will continue in the same spirit as before. We start by importing the
    required libraries:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你已经熟悉了这个数据集，因为我们在[*第5章*](B18118_05.xhtml#_idTextAnchor105)，《*神经网络图像分类*》，和[*第6章*](B18118_06.xhtml#_idTextAnchor129)，《*改进模型*》中使用了它。现在，让我们看看CNN与我们迄今为止使用的简单神经网络相比如何。我们将继续保持之前的精神，首先导入所需的库：
- en: 'We will import the requisite libraries for preprocessing, modeling, and visualizing
    our ML model using TensorFlow:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将导入所需的库以进行预处理、建模和使用TensorFlow可视化我们的机器学习模型：
- en: '[PRE0]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will load the Fashion MNIST dataset from TensorFlow Datasets using
    the `load_data()` function. This function returns our training and testing data
    consisting of NumPy arrays. The training data consists of `x_train` and `y_train`,
    and the test data is made up of `x_test` and `y_test`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`load_data()`函数从TensorFlow数据集中加载Fashion MNIST数据集。此函数返回我们的训练和测试数据，这些数据由NumPy数组组成。训练数据包括`x_train`和`y_train`，测试数据由`x_test`和`y_test`组成：
- en: '[PRE3]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can confirm the data size by using the `len` function on our training and
    testing data:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过对训练数据和测试数据使用`len`函数来确认数据的大小：
- en: '[PRE4]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When we run the code, we get the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们得到以下输出：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that we have a training data size of 60,000 images and test data
    of 10,000 images.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的训练数据集有60,000张图像，测试数据集有10,000张图像。
- en: 'In CNNs, unlike the DNNs we used previously, we need to account for the color
    channels of the input images. Currently, our training and testing data has a shape
    of `(batch_size, height, width)` for grayscale images, with a single channel.
    However, CNN models require a 4D input tensor, made up of `batch_size`, `height`,
    `width`, and `channels`. We can fix this data mismatch by simply reshaping our
    data and converting the elements to `float32` values:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CNN中， 与我们之前使用的DNN不同，我们需要考虑输入图像的颜色通道。目前，我们的训练和测试数据是灰度图像，其形状为`(batch_size, height,
    width)`，并且只有一个通道。然而，CNN模型需要一个4D输入张量，由`batch_size`、`height`、`width`和`channels`组成。我们可以通过简单地重塑数据并将元素转换为`float32`值来修复这个数据不匹配问题：
- en: '[PRE6]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This preprocessing step is standard before training an ML model, as most models
    require floating-point input. Since our images are grayscale, there is only one
    color channel, which is why we reshape the data to include a single channel dimension.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预处理步骤在训练机器学习模型之前是标准步骤，因为大多数模型需要浮动点输入。由于我们的图像是灰度图像，因此只有一个颜色通道，这就是我们将数据重塑为包含单一通道维度的原因。
- en: 'The pixel value of our data (training and testing data) ranges from `0` to
    `255`, where `0` represents black and `255` represents white. We normalize our
    data by dividing the pixel values by 255 to bring the pixel values in our data
    to a scale of between `0` and `1`. We do this to enable our model to converge
    faster and perform better:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们数据的像素值（训练数据和测试数据）范围从 `0` 到 `255`，其中 `0` 代表黑色，`255` 代表白色。我们通过将像素值除以 255 来规范化数据，从而将像素值缩放到
    `0` 到 `1` 的区间。这样做的目的是让模型更快收敛，并提高其性能：
- en: '[PRE11]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We use the `to_categorical` function from the `utils` module of `tf.keras`
    to convert our labels (`y_train` and `y_test`) that have an integer value of `0`
    to `9` into one-hot encoded arrays. The `to_categorical` function takes two arguments:
    the labels to be converted, and the number of classes; it returns a one-hot encoded
    array, as shown in *Figure 7**.15*.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `tf.keras` 中 `utils` 模块的 `to_categorical` 函数，将标签（`y_train` 和 `y_test`）中的整数值（`0`
    到 `9`）转换为一维独热编码数组。`to_categorical` 函数接受两个参数：需要转换的标签和类别的数量；它返回一个一维独热编码数组，如 *图 7.15*
    所示。
- en: '![Figure 7.15 – A one-hot encoded array](img/B18118_07_15.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 一维独热编码数组](img/B18118_07_15.jpg)'
- en: Figure 7.15 – A one-hot encoded array
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 一维独热编码数组
- en: 'The one-hot encoded vectors will have a length of `10`, with a number `1` in
    the index that corresponds to the label for a given data point, and `0` in all
    other indices:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一维独热编码向量的长度为 `10`，其中在对应标签的索引位置上为 `1`，其他位置则为 `0`：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Using the Sequential Model API from `tf.keras.model`, we will create a CNN
    architecture:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tf.keras.model` 中的顺序模型 API，我们将创建一个卷积神经网络（CNN）架构：
- en: '[PRE15]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The first layer is a convolution layer composed of 64 filters of size 3x3 to
    process the input images, which have a shape of 28x28 pixels and 1 channel (grayscale).
    ReLU is used as the activation function. The subsequent max pooling layer is a
    2D pooling layer that applies max pooling to downsample the output of the convolution
    layer, reducing the dimensionality of the feature maps. The `flatten` layer takes
    the output of the pooling layer and flattens it into a 1D array, which is then
    processed by the fully connected layer. The output layer contains `softmax` activation
    for multiclass classification and 10 neurons, one for each class.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是一个卷积层，包含 64 个 3x3 的滤波器，用来处理输入图像，输入图像的形状是 28x28 像素，1 个通道（灰度图）。ReLU 被用作激活函数。随后的最大池化层是一个
    2D 池化层，应用最大池化对卷积层的输出进行降采样，减少特征图的维度。`flatten` 层将池化层的输出展平为一维数组，然后由全连接层处理。输出层使用 `softmax`
    激活函数进行多类分类，并包含 10 个神经元，每个类一个神经元。
- en: 'Next, we compile and fit the model on our training data:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在训练数据上编译并训练模型：
- en: '[PRE31]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `compile()` function takes three arguments: loss function (`categorical_crossentropy`,
    since this is a multi-class classification task), optimizer (`adam`), and metrics
    (`accuracy`). After compiling the model, we used the `fit()` function to train
    the model on the training data. We specified the number of epochs as `10` and
    used 20% of the training data for validation purposes.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`compile()` 函数有三个参数：损失函数（`categorical_crossentropy`，因为这是一个多类分类任务）、优化器（`adam`）和度量标准（`accuracy`）。编译模型后，我们使用
    `fit()` 函数在训练数据上训练模型。我们将迭代次数设定为 `10`，并使用 20% 的训练数据进行验证。'
- en: 'In 10 epochs, we arrive at a training accuracy of `0.9785` and a validation
    accuracy of `0.9133`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 10 次迭代后，我们得到了训练准确率为 `0.9785`，验证准确率为 `0.9133`：
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `summary` function is a very useful way to get a high-level overview of
    the model’s architecture and understand the number of parameters and the shape
    of the output tensors:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`summary` 函数是一个非常有用的方式，用来快速概览模型架构，并理解每层的参数数量以及输出张量的形状：'
- en: '[PRE37]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output returns the five layers that make up our current model architecture.
    It also displays the output shape and the number of parameters of each layer.
    The total number of parameters is 1,386,506\. From the output, we see that the
    output shape from the convolution layer is 26x26 as a result of the border effect
    since we did not apply padding. Next, the max pooling layer halves the pixel size
    after which we flatten the data and generate predictions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出返回了组成我们当前模型架构的五个层。它还显示了每个层的输出形状和参数数量。总的参数数量是 1,386,506。通过输出，我们可以看到卷积层的输出形状是
    26x26，这是由于边缘效应造成的，因为我们没有使用填充。接下来，最大池化层将像素大小减半，然后我们将数据展平并生成预测：
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we will use the `evaluate` function to evaluate our model on test
    data. The `evaluate` function returns the loss and the accuracy of the model on
    test data:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用 `evaluate` 函数在测试数据上评估我们的模型。`evaluate` 函数返回模型在测试数据上的损失和准确度：
- en: '[PRE39]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Our model achieved an accuracy of `0.9079` on the test data, surpassing the
    performance of the architectures used in [*Chapter 6*](B18118_06.xhtml#_idTextAnchor129)*,
    Improving the Model*. We can try to further improve the model’s performance by
    adjusting the hyperparameters and applying data augmentation. Let's turn our attention
    to real-world images, where CNNs clearly outshine our previous models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在测试数据上实现了 `0.9079` 的准确率，超过了在 [*第 6 章*](B18118_06.xhtml#_idTextAnchor129)*，改进模型*
    中使用的架构的性能。我们可以通过调整超参数和应用数据增强来进一步提高模型的性能。让我们把注意力转向现实世界的图像，CNNs 显然比我们以前的模型更出色。
- en: Working with real-world images
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理现实世界图像
- en: Real-world images pose a different type of challenge as these images are usually
    colored images with three color channels (red, green, and blue), unlike the grayscale
    images we used from our fashion MNIST dataset. In *Figure 7**.16*, where we see
    an example of real-world images from the weather dataset that we will be modeling
    shortly, you will notice the images are of varying sizes. This introduces another
    layer of complexity that requires additional preprocessing steps such as resizing
    or cropping to ensure all our images are of uniform dimensions before we feed
    them into our neural network.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的图像提出了不同类型的挑战，因为这些图像通常是彩色图像，具有三个色彩通道（红色、绿色和蓝色），不像我们从时尚MNIST数据集中使用的灰度图像。在
    *图 7**.16* 中，我们看到了一些即将建模的来自天气数据集的实际图像示例，您会注意到这些图像的大小各异。这引入了另一层复杂性，需要额外的预处理步骤，如调整大小或裁剪，以确保我们所有的图像在输入神经网络之前具有统一的尺寸。
- en: '![Figure 7.16 – Images from the weather dataset](img/B18118_07_16.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 天气数据集中的图像](img/B18118_07_16.jpg)'
- en: Figure 7.16 – Images from the weather dataset
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 天气数据集中的图像
- en: Another issue we may encounter when working with real-world images is the presence
    of various noise sources. For example, we may have images in our dataset taken
    in conditions with uneven lighting or unintended blurring. Again, we could have
    images with multiple objects or other unintended distractions in the background
    among the images in our real-world dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理现实世界图像时，我们可能会遇到的另一个问题是各种噪声源的存在。例如，我们的数据集中可能有在光线不均匀或意外模糊条件下拍摄的图像。同样，在我们的现实世界数据集中可能会有多个对象或其他意外的背景干扰图像。
- en: To address these issues, we could apply noise reduction techniques such as denoising
    to improve the quality of our data. We could also use object detection techniques
    such as bounding boxes or segmentation to help us identify the target object within
    an image with multiple objects. The good part is TensorFlow is well equipped with
    a comprehensive set of tools tailored to handling these challenges. One important
    tool from TensorFlow is the `tf.image` module, which offers an array of image
    preprocessing functionalities such as resizing various adjustments (for example,
    brightness, contrast, hue, and saturation), application of bounding boxes, cropping,
    flipping, and much more.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这些问题，我们可以应用去噪技术，如去噪，以改善数据的质量。我们还可以使用对象检测技术，如边界框或分割，帮助我们在具有多个对象的图像中识别目标对象。好消息是
    TensorFlow 配备了一套完整的工具集，专门处理这些挑战。来自 TensorFlow 的一个重要工具是 `tf.image` 模块，提供了一系列图像预处理功能，如调整大小、亮度、对比度、色调和饱和度的应用、边界框、裁剪、翻转等等。
- en: However, this module is beyond the scope of this book and the exam itself. But,
    if you wish to learn more about this module, you can visit the TensorFlow documentation
    at [https://www.tensorflow.org/api_docs/python/tf/image](https://www.tensorflow.org/api_docs/python/tf/image).
    Another tool in TensorFlow’s arsenal is `ImageDataGenerator`, which enables us
    to perform data augmentation on the fly, offering us the ability to preprocess
    and perform augmentative actions (such as rotation and flipping images) in real
    time as we feed these images into our training pipeline. Let's proceed to work
    with our real-world image dataset and see `ImageDataGenerator` in action.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个模块超出了本书及考试的范围。但是，如果你希望深入了解这个模块，可以访问 TensorFlow 文档：[https://www.tensorflow.org/api_docs/python/tf/image](https://www.tensorflow.org/api_docs/python/tf/image)。TensorFlow
    的另一个工具是`ImageDataGenerator`，它使我们能够实时执行数据增强操作，提供了在将图像输入训练管道时对其进行预处理和增广（例如旋转和翻转图像）的能力。接下来，我们将使用实际的图像数据集，看看`ImageDataGenerator`如何发挥作用。
- en: Weather dataset classification
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 天气数据集分类
- en: 'In this case study, we will be working as a computer vision consultant for
    an emerging start-up called WeatherBIG. You have been assigned the responsibility
    of developing an image classification system that will be used to identify different
    weather conditions; the dataset for this task can be found on Kaggle using this
    link: [https://www.kaggle.com/datasets/rahul29g/weatherdataset](https://www.kaggle.com/datasets/rahul29g/weatherdataset).
    The dataset has been packaged into three folders made up of a training folder,
    a validation folder, and a testing folder. Each of these folders has subfolders
    with each weather class. Let’s get started with the task:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将作为计算机视觉顾问为一个新兴的初创公司 WeatherBIG 提供支持。你被分配了开发一个图像分类系统的任务，该系统将用于识别不同的天气状况；该任务的数据集可以通过以下链接在
    Kaggle 上找到：[https://www.kaggle.com/datasets/rahul29g/weatherdataset](https://www.kaggle.com/datasets/rahul29g/weatherdataset)。该数据集已被分成三个文件夹，包括训练文件夹、验证文件夹和测试文件夹。每个文件夹下都有各自的天气类别子文件夹。让我们开始这个任务：
- en: 'We start by importing several libraries to build our image classifier:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入几个库来构建我们的图像分类器：
- en: '[PRE41]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We have used several of these libraries in our previous experiments; however,
    let's address the functionalities of a few libraries that we will be using for
    the first time. The `os` module acts as a bridge to our operating system. It gives
    us the ability to read from and write to our filesystem, while `pathlib` offers
    us an intuitive, object-oriented way to streamline our file navigation tasks.
    For image manipulation, we use `PIL`, and we also have the `ImageDataGenerator`
    class from the `tensorflow.keras.preprocessing.image` module for our data preprocessing
    steps, batch generation, and data augmentation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的实验中使用了这些库中的几个；然而，接下来我们将介绍一些第一次使用的库的功能。`os`模块作为我们操作系统的桥梁。它使我们能够读取和写入文件系统，而`pathlib`提供了一种直观的面向对象的方式来简化文件导航任务。对于图像操作，我们使用`PIL`，此外还有来自`tensorflow.keras.preprocessing.image`模块的`ImageDataGenerator`类，用于我们的数据预处理、批次生成和数据增强步骤。
- en: 'You can access/download the dataset for this case study from [https://www.kaggle.com/datasets/rahul29g/weatherdataset](https://www.kaggle.com/datasets/rahul29g/weatherdataset)
    and upload it to Google Drive. Once you do this, you can easily follow along with
    the code in this section. In my case, the data is stored in this root directory:
    `/content/drive/MyDrive/weather dataset`. In your case, your root directory will
    be different, so make sure you change the directory path to match the directory
    where the dataset is stored in your Google Drive: `root_dir = "/``content/drive/MyDrive/weather
    dataset"`.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以从[https://www.kaggle.com/datasets/rahul29g/weatherdataset](https://www.kaggle.com/datasets/rahul29g/weatherdataset)获取/下载此案例研究的数据集，并将其上传到
    Google Drive。完成后，你可以轻松地跟随本节中的代码进行操作。在我的例子中，数据存储在此根目录：`/content/drive/MyDrive/weather
    dataset`。在你的情况下，根目录将会不同，所以请确保将目录路径更改为与数据集存储在 Google Drive 中的目录匹配：`root_dir = "/content/drive/MyDrive/weather
    dataset"`。
- en: 'Next, we apply the `os.walk` function to access the root directory and generate
    information about the content of all the directories and subdirectories:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们应用`os.walk`函数来访问根目录，并生成关于所有目录和子目录内容的信息：
- en: '[PRE51]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Running the code returns a tuple made up of the path of each directory and
    the number of images within each of them, as illustrated in *Figure 7**.17*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码会返回一个元组，包含每个目录的路径以及每个目录中图片的数量，如*图 7.17*所示：
- en: '![Figure 7.17 – A snapshot directory and subdirectories](img/B18118_07_17.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – 快照目录及其子目录](img/B18118_07_17.jpg)'
- en: Figure 7.17 – A snapshot directory and subdirectories
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 快照目录及其子目录
- en: We use this step to get a sense of the contents of each directory and subdirectory.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过这一步来了解每个目录和子目录的内容。
- en: 'We use the `retrieve_labels` function to fetch and display labels and their
    corresponding counts from the training, test, and validation directories. To craft
    this function, we use the `listdir` method from the `os` module and we pass in
    the respective directory paths (`train_dir`, `test_dir`, and `val_dir`):'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`retrieve_labels`函数从训练、测试和验证目录中提取并显示标签及其对应的计数。为了实现这个函数，我们使用`os`模块中的`listdir`方法，并传入相应的目录路径（`train_dir`、`test_dir`和`val_dir`）：
- en: '[PRE55]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We specify the path to the training, test, and validation directories in the
    `train_dir`, `test_dir`, and `val_dir` arguments, respectively:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们分别在`train_dir`、`test_dir`和`val_dir`参数中指定训练、测试和验证目录的路径：
- en: '[PRE71]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'When we run the code, it returns the training data, test data, validation data
    labels, and the number of labels:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它将返回训练数据、测试数据、验证数据标签以及标签的数量：
- en: '[PRE75]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'For our exploration, let''s craft a function called `view_random_images` to
    randomly access and display images from the subdirectories within our dataset.
    The function takes in the main directory that holds the subdirectories housing
    our images and the number of images we want to display. We apply `listdir` to
    access the subdirectories and to introduce randomness in the selection process.
    We use the `shuffle` function from the `random` library for shuffling and selecting
    images randomly. Matplotlib is used to display the specified number of random
    images in our function:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的探索中，创建一个名为`view_random_images`的函数，从数据集中的子目录中随机访问并显示图片。该函数接受包含子目录的主目录以及我们希望显示的图片数量。我们使用`listdir`来访问子目录，并引入随机性来选择图片。我们使用`random`库中的`shuffle`函数进行打乱并随机选择图片。我们利用Matplotlib来显示指定数量的随机图片：
- en: '[PRE76]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Let''s try out the function by setting `num_images` to `4` and examine some
    data in our `train` directory:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过将`num_images`设置为`4`来尝试这个函数，并查看`train`目录中的一些数据：
- en: '[PRE105]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'This returns four randomly selected images, as illustrated here:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回四张随机选择的图片，如下所示：
- en: '![7.18 – Randomly selected images from the weather dataset](img/B18118_07_18.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![7.18 – 从天气数据集中随机选择的图片](img/B18118_07_18.jpg)'
- en: 7.18 – Randomly selected images from the weather dataset
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 7.18 – 从天气数据集中随机选择的图片
- en: From the data displayed, we can see the images come in various sizes (height
    and weight) and we will need to fix this preprocessing issue. We will be using
    the `ImageDataGenerator` class from TensorFlow. Let's discuss this next.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从显示的数据来看，图片的尺寸（高度和宽度）各不相同，我们需要解决这个预处理问题。我们将使用TensorFlow中的`ImageDataGenerator`类。接下来我们将讨论这个问题。
- en: Image data preprocessing
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图片数据预处理
- en: We saw, in *Figure 7**.18*, that our training images are of different sizes.
    Here, we will be resizing and normalizing our data before training. Also, we want
    to develop an efficient method of loading our training data in batches, ensuring
    optimized memory usage with seamless integration with our model’s training process.
    To achieve all of this, we will be utilizing the `ImageDataGenerator` class from
    the `TensorFlow.keras.preprocessing.image` module. In [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186),
    *Handling Overfitting,* we will take our application of `ImageDataGenerator` further
    by using it to enlarge our training dataset by developing variants of our image
    data by rotating, flipping, and zooming. This could help our model become more
    robust and reduce the risk of overfitting.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 7.18*中看到，训练图片的大小各不相同。在这里，我们将在训练前调整并规范化数据。此外，我们还希望开发一种有效的方法来批量加载训练数据，确保优化内存使用并与模型的训练过程无缝对接。为了实现这一目标，我们将使用`TensorFlow.keras.preprocessing.image`模块中的`ImageDataGenerator`类。在[*第
    8 章*](B18118_08.xhtml#_idTextAnchor186)《处理过拟合》中，我们将进一步应用`ImageDataGenerator`，通过旋转、翻转和缩放等方式生成训练数据的变种，扩大我们的训练数据集。这将有助于我们的模型变得更强大，并减少过拟合的风险。
- en: Another useful tool to aid our data preprocessing task is the `flow_from_directory`
    method. We can use this method to build data pipelines. It is especially useful
    when we are working on large-scale, real-world data because of its ability to
    automate reading, resizing, and batching images for model training or inference.
    The `flow_from_directory` method takes three main arguments. The first is the
    directory path that contains our image data. Next, we specify the desired size
    of the images before we feed them into our neural network. Then, we also have
    to specify the batch size to determine the number of images we want to process
    simultaneously. We can tailor the process more by specifying other parameters,
    such as color mode, class mode, and shuffle. Let's now take a look at a typical
    directory structure for a multiclass classification problem, as illustrated in
    *Figure 7**.19*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有助于我们数据预处理任务的有用工具是`flow_from_directory`方法。我们可以使用此方法构建数据管道。当我们处理大规模、实际数据时，它尤其有用，因为它能够自动读取、调整大小并将图像批量化，以便进行模型训练或推理。`flow_from_directory`方法接受三个主要参数。第一个是包含图像数据的目录路径。接下来，我们指定在将图像输入神经网络之前，图像的期望大小。然后，我们还需要指定批量大小，以确定我们希望同时处理的图像数量。我们还可以通过指定其他参数，如颜色模式、类别模式和是否打乱数据，来进一步定制该过程。现在，让我们来看一下一个多分类问题的典型目录结构，如*图7.19*所示。
- en: '![Figure 7.19 – The directory structure for a multiclass classification problem](img/B18118_07_19.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图7.19 – 多分类问题的目录结构](img/B18118_07_19.jpg)'
- en: Figure 7.19 – The directory structure for a multiclass classification problem
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 多分类问题的目录结构
- en: 'When applying the `flow_from_directory` method, it is important that we organize
    our images in a well-structured directory, with subdirectories for each unique
    class label as displayed in *Figure 7**.19*. Here, we have four subdirectories,
    one for each class label in our weather dataset. Once all the images are in the
    appropriate subdirectories, we can apply `flow_from_directory` to set up an iterator.
    This iterator is adjustable so that we can define parameters such as the image
    size and batch size and decide whether we want to shuffle our data or not. Let’s
    apply these new ideas to our current case study:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用`flow_from_directory`方法时，重要的是我们需要将图像组织在一个结构良好的目录中，每个唯一的类别标签都有一个子目录，如*图7.19*所示。在这里，我们有四个子目录，每个子目录对应我们的天气数据集中的一个类别标签。一旦所有图像都放入了适当的子目录，我们就可以应用`flow_from_directory`来设置一个迭代器。这个迭代器是可调的，我们可以定义图像大小、批量大小等参数，并决定是否打乱数据。接下来，我们将这些新想法应用到我们当前的案例研究中：
- en: '[PRE106]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Here, we define three instances of the `ImageDataGenerator` class: one for
    training, one for validation, and one for testing. We apply a rescaling factor
    of 1/255 to the pixel values of images in each instance to normalize our data:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了三个`ImageDataGenerator`类的实例：一个用于训练，一个用于验证，一个用于测试。我们对每个实例中的图像像素值应用了1/255的缩放因子，以便对数据进行归一化：
- en: '[PRE107]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We use `flow_from_directory` to import images from the respective training,
    validation, and testing directories, and the resulting data is stored in our `train_data`,
    `valid_data`, and `test_data` variables. In addition to specifying the directories
    in our `flow_from_directory` method, you will notice we also specified not just
    the target size (224 x244) and batch size (64) but also the type of problem we
    are tackling as `categorical` because we are dealing with a multi-classification
    use case. We have now successfully completed our data preprocessing steps. Let''s
    move on to modeling our data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`flow_from_directory`从各自的训练、验证和测试目录中导入图像，结果数据存储在`train_data`、`valid_data`和`test_data`变量中。除了在`flow_from_directory`方法中指定目录外，您会注意到我们不仅指定了目标大小（224
    x 244）和批量大小（64），还指定了我们正在处理的问题类型为`categorical`，因为我们处理的是一个多分类问题。我们现在已经成功完成了数据预处理步骤。接下来，我们将开始对数据建模：
- en: '[PRE108]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Here, we use a CNN architecture made up of three sets of convolutional and pooling
    layers. In the first convolutional layer, we apply 16 filters with a filter size
    of 3\. Notice the input shape also matches the shape defined in our preprocessing
    step. After the first convolutional layer, we apply max pooling of 2x2\. Next,
    we reach the second convolutional layer, which utilizes 32 filters, each 3x3 in
    size, followed by another 2x2 max pooling layer. The final convolutional layer
    has 64 filters, each 3x3 in size, followed by another max pooling layer, which
    further downsamples the data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一个由三组卷积层和池化层组成的CNN架构。在第一层卷积层中，我们应用了16个3×3的过滤器。请注意，输入形状也与我们在预处理步骤中定义的形状匹配。在第一层卷积层之后，我们应用了2x2的最大池化。接下来，我们进入第二层卷积层，使用32个3×3的过滤器，后面跟着另一个2x2的最大池化层。最后一层卷积层有64个3×3的过滤器，后面跟着另一个最大池化层，进一步对数据进行下采样。
- en: 'Next, we reach the fully connected layers. Here, we first flatten the 3D output
    of the earlier layers into a 1D array. Then, we feed the data into dense layers
    for final classification. We proceed by compiling and fitting our model to our
    data. It’s important to note that in our `compile` step, we use `CategoricalCrossentropy`
    for our `loss` function as we are dealing with a task with multiple classes, and
    we set `metrics` to `accuracy`. The resulting output is a probability distribution
    over the four classes in our dataset, with the class with the highest probability
    being the predicted label:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入全连接层。在这里，我们首先将之前层的3D输出展平为1D数组。然后，我们将数据传递到密集层进行最终分类。接下来，我们编译并拟合我们的模型到数据上。需要注意的是，在我们的`compile`步骤中，我们使用`CategoricalCrossentropy`作为`loss`函数，因为我们正在处理一个多类任务，并将`metrics`设置为`accuracy`。最终输出是一个概率分布，表示我们数据集中的四个类别，具有最高概率的类别即为预测标签：
- en: '[PRE109]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'We train our model for 10 epochs, attaining a training accuracy of around 94%
    on training data and 91% on validation data. We use the `summary` method to obtain
    information about the different layers in the model. This information includes
    the layer-wise overview, output shape, and number of parameters used (trainable
    and non-trainable):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了我们的模型10个周期，达到了在训练数据上的94%训练准确率和在验证数据上的91%准确率。我们使用`summary`方法来获取模型中不同层的信息。这些信息包括每层的概述、输出形状以及使用的参数数量（可训练和不可训练）：
- en: '[PRE110]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'From our model’s summary, we see our architecture has three convolutional (`Conv2D`)
    layers, each accompanied by a pooling (`MaxPooling2D`) layer. Information flows
    from these layers into the fully connected layer, where the final classification
    is carried out. Let''s drill down into each of the layers and unpack the information
    they provide us with. The first convolutional layer is with an output shape of
    `(None, 222, 222, 16)`. Here, `None` means we didn’t hardcode the batch size,
    which gives us the flexibility to use different batch sizes with ease. Next, we
    have `222, 222`, which represents the dimension of the output feature map; we
    lose 2 pixels in height and weight because of the boundary effect if we do not
    apply padding. Finally, `16` represents the number of filters or kernels used,
    which means we will have an output of 16 different feature maps from each of the
    filters. You will also notice this layer has `448` parameters. To calculate the
    number of parameters in the convolutional layers, we use the following formula:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的总结中，我们看到我们的架构包含三层卷积层（`Conv2D`），每一层都配有一个池化层（`MaxPooling2D`）。信息从这些层流入到全连接层，在那里进行最终的分类。让我们深入分析每一层，解读它们提供的信息。第一层卷积层的输出形状为`(None,
    222, 222, 16)`。这里，`None`表示我们没有硬编码批次大小，这使得我们能够灵活地使用不同的批次大小。接下来，`222, 222`表示输出特征图的尺寸；如果不应用填充，我们会因为边界效应而丢失2个像素的高度和宽度。最后，`16`表示使用的过滤器或内核的数量，这意味着每个过滤器将输出16个不同的特征图。你还会注意到这一层有`448`个参数。为了计算卷积层中的参数数量，我们使用以下公式：
- en: '*(Filter width × Filter height × Input channels + 1(for bias)) × Number of
    filters = Total number of parameters in the* *convolutional layer*'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*(过滤器宽度 × 过滤器高度 × 输入通道数 + 1（偏置）) × 过滤器数量 = 卷积层的总参数数量*'
- en: When we key in the values into the formula, we arrive at (3 × 3 × 3 + 1) × 16
    = 448 parameters.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数值代入公式时，我们得到（3 × 3 × 3 + 1）× 16 = 448个参数。
- en: The next layer is the first pooling layer, which is a `MaxPooling2D` layer that
    downsamples the output feature maps from the convolutional layer. Here, we have
    an output shape of `(None, 111, 111, 16)`. From the output, you can see that the
    spatial dimension has been reduced to half, and it is also important to note that
    pooling layers have no parameters, as you will observe with all the pooling layers
    in our model’s summary.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个层是第一个池化层，它是一个`MaxPooling2D`层，用于对卷积层的输出特征图进行降采样。在这里，我们的输出形状为`(None, 111, 111,
    16)`。从输出中可以看到，空间维度已经缩小了一半，同时也要注意，池化层没有参数，正如我们在模型总结中看到的所有池化层一样。
- en: 'Next, we reach the second convolutional layer and notice the depth of our output
    has increased to `32`. This happens because we employed 32 filters in this layer;
    hence, we will have 32 different feature maps returned. Also, we have the spatial
    dimension of the feature maps again reduced by two pixels because of the boundary
    effect. We can easily calculate the number of parameters in this layer as follows:
    (3 × 3 × 16 + 1) × 32 = 4,640 parameters.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入第二个卷积层，注意到我们的输出深度已经增加到了`32`。这是因为我们在该层使用了32个过滤器，因此将返回32个不同的特征图。同时，由于边界效应，我们的特征图的空间维度再次被减少了两个像素。我们可以通过以下方式轻松计算该层的参数数量：（3
    × 3 × 16 + 1）× 32 = 4,640个参数。
- en: 'Next, we reach the second pooling layer, which downsamples the feature maps
    further to `(None, 54, 54, 32)`. The final convolutional layer uses 64 filters,
    so it has an output shape of `(None, 52, 52, 64)` and 18,496 parameters. The final
    pooling layer again reduces the dimension of our data to `(None, 26, 26, 64)`.
    The output of the final pooling layer is fed into the `Flatten` layer, which reshapes
    the data from a 3D tensor into a 1D tensor with a size of 26 x 26 x 64 = 43,264\.
    This is fed into the first `Dense` layer, which has an output shape of `(None,
    1050)`. To calculate the number of parameters in the `Dense` layer, we use this
    formula:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入第二个池化层，它进一步对特征图进行降采样，输出尺寸为`(None, 54, 54, 32)`。最后的卷积层使用64个过滤器，因此输出形状为`(None,
    52, 52, 64)`，有18,496个参数。最后的池化层再次将数据维度降至`(None, 26, 26, 64)`。最后池化层的输出被送入`Flatten`层，后者将数据从3D张量重塑为1D张量，尺寸为26
    x 26 x 64 = 43,264。这个数据接着被送入第一个`Dense`层，其输出形状为`(None, 1050)`。为了计算`Dense`层的参数数量，我们使用以下公式：
- en: '*(Number of input nodes + 1) × Number of* *output nodes*'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '*(输入节点数 + 1) × 输出节点数*'
- en: When we input the values, we get (43,264 + 1) × 1,050 = 45,428,250 parameters.
    The final `Dense` layer is the output layer and it has a shape of `(None, 4)`,
    where `4` represents the number of unique classes in our data that we want to
    predict. This layer has (1,050 + 1) × 4 = 4,204 parameters due to its connections,
    biases, and the number of output neurons.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们输入这些数值时，得到(43,264 + 1) × 1,050 = 45,428,250个参数。最终的`Dense`层是输出层，其形状为`(None,
    4)`，其中`4`表示我们要预测的独特类别数。由于连接、偏置和输出神经元数量，该层有(1,050 + 1) × 4 = 4,204个参数。
- en: 'Next, we evaluate our model using the `evaluate` method:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`evaluate`方法评估我们的模型：
- en: '[PRE111]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: We reach an accuracy of 91% on our test data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试数据上达到了91%的准确率。
- en: 'Let''s compare our CNN architecture with two DNNs:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们的CNN架构与两个DNN进行比较：
- en: '[PRE112]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'We build a DNN called `model_2` made up of 4 `Dense` layers, with `1200`, `600`,
    `300`, and `4` neurons, respectively. Apart from the output layer, which uses
    the `softmax` function for classification, all the other layers use ReLU as their
    activation function. We compile and fit `model_2` in the same way as `model_1`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个名为`model_2`的DNN，由4个`Dense`层组成，分别有`1200`、`600`、`300`和`4`个神经元。除了输出层使用`softmax`函数进行分类外，其他所有层都使用ReLU作为激活函数。我们与`model_1`的方式相同，编译并拟合`model_2`：
- en: '[PRE113]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: After 10 epochs, we reach a validation accuracy of 74.86% and when we examine
    the model’s summary, we see that we have used a total of 181,536,904 parameters,
    which is 4 times the size of our CNN architecture parameters.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10个epoch，我们达到了74.86%的验证准确率，当我们查看模型的总结时，发现我们总共使用了181,536,904个参数，这是我们CNN架构参数的4倍。
- en: 'Next, let''s look at another DNN architecture:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看另一个DNN架构：
- en: '[PRE114]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'We use another set of 4 `Dense` layers, with `1000`, `500`, `500`, and `4`
    neurons, respectively. We fit and compile `model_3` as well for 10 epochs:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用另外一组4个`Dense`层，分别有`1000`、`500`、`500`和`4`个神经元。我们同样为`model_3`进行了10个epoch的拟合和编译：
- en: '[PRE115]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'We reach a validation accuracy of 77.65% after 10 epochs and this model has
    around 151,282,004 parameters; the results are not close to those of our CNN architecture.
    Let''s proceed to compare all three models on test data, which is what we want
    to be judging our models on. To do this, we will write a function to generate
    a DataFrame showing the names, the loss, and the accuracy of the models:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过10个epoch后，我们达到了77.65%的验证准确率，该模型大约有151,282,004个参数；结果与我们的CNN架构相差较大。接下来，让我们在测试数据上比较三种模型，这是我们评估模型的标准。为此，我们将编写一个函数来生成一个DataFrame，显示模型的名称、损失和准确率：
- en: '[PRE116]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'The `evaluate_models()` function takes a list of models, model names, and test
    data as input and returns a DataFrame with the evaluation results for each model
    as percentages:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_models()`函数接受一个模型列表、模型名称和测试数据作为输入，并返回一个包含每个模型评估结果（以百分比形式）的DataFrame：'
- en: '[PRE117]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: When we run the code, it generates the table shown in *Figure 7**.20*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它会生成如*图 7.20*所示的表格。
- en: '![Figure 7.20 – A DataFrame showing the experimental results of all three models](img/B18118_07_20.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.20 – 显示所有三种模型实验结果的DataFrame](img/B18118_07_20.jpg)'
- en: Figure 7.20 – A DataFrame showing the experimental results of all three models
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 显示所有三种模型实验结果的DataFrame
- en: 'From the results, we can clearly see that Model 1 is ahead. You may wish to
    experiment with larger DNNs but you will soon run out of memory. For larger datasets,
    the results may be much worse for DNNs. Next, let''s look at how we fared on our
    training and validation data with Model 1:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以清楚地看到模型1表现最好。你可能希望尝试更大的DNN，但很快会遇到内存不足的问题。对于更大的数据集，DNN的结果可能会大幅下降。接下来，让我们看看模型1在训练和验证数据上的表现：
- en: '[PRE118]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We created a function to plot the training and validation loss and accuracy
    using matplotlib. We pass `history_1` into our function:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个函数，使用matplotlib绘制训练和验证的损失与准确率图。我们将`history_1`传递给这个函数：
- en: '[PRE119]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'This will generate the following output:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 7.21 – Loss and accuracy plot for Model 1](img/B18118_07_21.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.21 – 模型1的损失和准确率图](img/B18118_07_21.jpg)'
- en: Figure 7.21 – Loss and accuracy plot for Model 1
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 – 模型1的损失和准确率图
- en: From the plot, we can see that our training accuracy rises steadily but falls
    below its highest point just before the 10th epoch. Also, our validation data
    experiences a sharp fall in accuracy. Our loss veers off from the fourth epoch.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看到，我们的训练准确率稳步上升，但在接近第10个epoch时，准确率未能达到最高点。同时，我们的验证数据的准确率出现了急剧下降。我们的损失从第4个epoch开始偏离。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw the power of CNNs. We began by examining the challenges
    faced by DNNs for visual recognition tasks. Next, we journeyed through the anatomy
    of CNNs, zooming in on the various moving parts, such as the convolutional, pooling,
    and fully connected layers. Here, we saw the impact and effect of different hyperparameters,
    and we also discussed the boundary effect. Next, we moved on to using all we learned
    to build a real-world weather classifier using two DNNs and a CNN. Our CNN model
    outperformed the DNNs, showcasing the strength of CNNs in handling image-based
    problems. Also, we discussed and applied some TensorFlow functions that streamline
    data preprocessing and modeling when we are working with image data.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们见识了CNN的强大。我们首先探讨了DNN在视觉识别任务中面临的挑战。接着，我们深入了解了CNN的构造，重点讲解了卷积层、池化层和全连接层等各个部分。在这里，我们观察了不同超参数的影响，并讨论了边界效应。接下来，我们利用所学知识，构建了一个实际的天气分类器，使用了两个DNN和一个CNN。我们的CNN模型优于DNN，展示了CNN在处理基于图像的问题中的优势。同时，我们还讨论并应用了一些TensorFlow函数，这些函数可以简化数据预处理和建模过程，特别是在处理图像数据时。
- en: By now you should have a good understanding of the structure and operations
    of CNNs and how to use them to solve real-world image classification problems,
    as well as utilizing various tools in TensorFlow to effectively and efficiently
    preprocess image data for improved model performance. In the next chapter, we
    will address the issue of overfitting in neural networks and explore various techniques
    to overcome this challenge, ensuring that our models generalize well to unseen
    data.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经很好地理解了CNN的结构和操作原理，以及如何使用它们解决实际的图像分类问题，并掌握了利用TensorFlow中各种工具有效、高效地预处理图像数据，从而提高模型性能。在下一章中，我们将讨论神经网络中的过拟合问题，并探索各种技术来克服这一挑战，确保我们的模型能很好地泛化到未见过的数据上。
- en: In the next chapter, we will use some old tricks such as callbacks and hyperparameter
    tuning to see whether we can improve our model’s performance. We will also experiment
    with data augmentation and other new techniques to improve our model’s performance.
    We draw the curtains on our task for now until [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186)*,*
    *Handling Overfitting*.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用一些老技巧，如回调和超参数调整，看看是否能提高模型性能。我们还将实验数据增强和其他新技术，以进一步提高模型的表现。我们将在此任务上暂时画上句号，直到[*第8章*](B18118_08.xhtml#_idTextAnchor186)*，*
    *处理过拟合*。
- en: Questions
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let’s test what we have learned in this chapter:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下本章所学内容：
- en: What are the components of a typical CNN architecture?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个典型的CNN架构有哪些组成部分？
- en: How does a convolutional layer work in a CNN architecture?
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层在CNN架构中是如何工作的？
- en: What is pooling and why is it used in a CNN architecture?
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是池化，为什么在CNN架构中使用池化？
- en: What is the purpose of a fully connected layer in a CNN architecture?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CNN架构中，全连接层的目的是什么？
- en: What is the impact of the padding on a convolution operation?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充对卷积操作有什么影响？
- en: What are the advantages of using TensorFlow image data generators?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow图像数据生成器的优势是什么？
- en: Further reading
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, you can check out the following resources:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多内容，您可以查看以下资源：
- en: Dumoulin, V., & Visin, F. (2016). *A guide to convolution arithmetic for deep*
    *learning*. [http://arxiv.org/abs/1603.07285](http://arxiv.org/abs/1603.07285)
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dumoulin, V., & Visin, F. (2016). *《深度学习的卷积算术指南》*。[http://arxiv.org/abs/1603.07285](http://arxiv.org/abs/1603.07285)
- en: 'Gulli, A., Kapoor, A. and Pal, S., 2019\. *Deep Learning with TensorFlow 2
    and Keras*. Birmingham: Packt Publishing Ltd'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulli, A., Kapoor, A. 和 Pal, S., 2019. *《使用TensorFlow 2和Keras的深度学习》*。伯明翰：Packt
    Publishing Ltd
- en: 'Kapoor, A., Gulli, A. and Pal, S. (2020) *Deep Learning with TensorFlow and
    Keras Third Edition: Build and deploy supervised, unsupervised, deep, and reinforcement
    learning models*. Packt Publishing Ltd'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor, A., Gulli, A. 和 Pal, S. (2020) *《深度学习与TensorFlow和Keras第三版：构建和部署监督、非监督、深度和强化学习模型》*。Packt
    Publishing Ltd
- en: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *ImageNet classification
    with deep convolutional neural networks*. In Advances in neural information processing
    systems (pp. 1097-1105)
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *《使用深度卷积神经网络进行ImageNet分类》*。《神经信息处理系统进展》(第1097-1105页)
- en: Zhang, Y., & Yang, H. (2018). *Food classification with convolutional neural
    networks and multi-class linear discernment analysis*. In 2018 IEEE International
    Conference on Information Reuse and Integration (IRI) (pp. 1-5). IEEE
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang, Y., & Yang, H. (2018). *《使用卷积神经网络和多类线性判别分析进行食物分类》*。2018年IEEE国际信息重用与集成会议（IRI）(第1-5页)。IEEE
- en: Zhang, Z., Ma, H., Fu, H., & Zha, C. (2020). *Scene-Free Multi-Class Weather
    Classification on Single Images*. IEEE Access, 8, 146038-146049\. doi:10.1109
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang, Z., Ma, H., Fu, H., & Zha, C. (2020). *《基于单图像的无场景多类天气分类》*。IEEE Access,
    8, 146038-146049\. doi:10.1109
- en: 'The `tf.image` module: [https://www.tensorflow.org/api_docs/python/tf/image](https://www.tensorflow.org/api_docs/python/tf/image)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.image`模块：[https://www.tensorflow.org/api_docs/python/tf/image](https://www.tensorflow.org/api_docs/python/tf/image)'
