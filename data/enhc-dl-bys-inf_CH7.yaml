- en: Chapter 7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章
- en: Practical Considerations for Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习的实际考虑
- en: Over the last two chapters, [*Chapter 5*](CH5.xhtml#x1-600005), [*Principled
    Approaches for Bayesian* *Deep Learning*](CH5.xhtml#x1-600005) and [*Chapter 6*](CH6.xhtml#x1-820006),
    [*Using the Standard Toolbox for Bayesian* *Deep Learning*](CH6.xhtml#x1-820006),
    we’ve been introduced to a range of methods that facilitate Bayesian inference
    with neural networks. [*Chapter 5*](CH5.xhtml#x1-600005), [*Principled Approaches*
    *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) introduced specially crafted
    Bayesian neural network approximations, while [*Chapter 6*](CH6.xhtml#x1-820006),
    [*Using the Standard Toolbox for* *Bayesian Deep Learning*](CH6.xhtml#x1-820006)
    showed how we can use the standard toolbox of machine learning to add uncertainty
    estimates to our models. These families of methods come with their own advantages
    and disadvantages. In this chapter, we will explore some of these differences
    in practical scenarios in order to help you understand how to select the best
    method for the task at hand.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，[*第五章*](CH5.xhtml#x1-600005)，[*贝叶斯深度学习的原则性方法*](CH5.xhtml#x1-600005)
    和 [*第六章*](CH6.xhtml#x1-820006)，[*使用标准工具箱进行贝叶斯深度学习*](CH6.xhtml#x1-820006)，我们介绍了一系列能够促进神经网络贝叶斯推断的方法。[*第五章*](CH5.xhtml#x1-600005)，[*贝叶斯深度学习的原则性方法*](CH5.xhtml#x1-600005)
    介绍了专门设计的贝叶斯神经网络近似方法，而 [*第六章*](CH6.xhtml#x1-820006)，[*使用标准工具箱进行贝叶斯深度学习*](CH6.xhtml#x1-820006)
    展示了如何使用机器学习的标准工具箱为我们的模型添加不确定性估计。这些方法家族各有其优缺点。在本章中，我们将探讨一些在实际场景中的差异，以帮助您理解如何为当前任务选择最佳方法。
- en: 'We will also look at different sources of uncertainty, which can improve your
    understanding of the data or help you choose a different exception path based
    on the source of uncertainty. For example, if a model is uncertain because the
    input data is inherently noisy, you might want to send the data to a human for
    review. However, if a model is uncertain because the input data has not been seen
    before, it might be helpful to add this data to your model so that it can reduce
    its uncertainty on this type of data. Bayesian deep learning techniques can help
    you to distinguish between these sources of uncertainty. These topics will be
    covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨不同来源的不确定性，这有助于提升您对数据的理解，或根据不确定性的来源帮助您选择不同的异常路径。例如，如果一个模型因输入数据本身的噪声而产生不确定性，您可能需要将数据交给人类进行审查。然而，如果模型因未见过的输入数据而产生不确定性，将该数据添加到模型中可能会有所帮助，这样模型就能减少对这类数据的不确定性。贝叶斯深度学习技术能够帮助您区分这些不确定性的来源。以下部分将详细介绍这些内容：
- en: Balancing uncertainty quality and computational considerations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡不确定性质量和计算考虑
- en: BDL and sources of uncertainty
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDL与不确定性来源
- en: 7.1 Technical requirements
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 技术要求
- en: 'To complete the practical tasks in this chapter, you will need a Python 3.8
    environment with the SciPy and scikit-learn stack and the following additional
    Python packages installed:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的实际任务，您需要一个Python 3.8环境，安装有SciPy和scikit-learn堆栈，并且还需要安装以下额外的Python包：
- en: TensorFlow 2.0
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0
- en: TensorFlow Probability
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow概率
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码都可以在书籍的GitHub仓库中找到：[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)。
- en: 7.2 Balancing uncertainty quality and computational considerations
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 平衡不确定性质量和计算考虑
- en: While Bayesian methods have many benefits, there are also trade-offs to consider
    in terms of memory and computational overheads. These considerations play a critical
    role in selecting the most appropriate methods to use within real-world applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然贝叶斯方法有许多优点，但在内存和计算开销方面也有需要考虑的权衡。这些因素在选择适用于实际应用的最佳方法时起着至关重要的作用。
- en: In this section, we’ll examine the trade-offs between different methods in terms
    of performance and uncertainty quality, and we’ll learn how we can use TensorFlow’s
    profiling tools to measure the computational costs associated with different models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考察不同方法在性能和不确定性质量方面的权衡，并学习如何使用TensorFlow的性能分析工具来衡量不同模型的计算成本。
- en: 7.2.1 Setting up our experiments
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 设置实验环境
- en: 'To evaluate the performance of different models, we’ll need a few different
    datasets. One of these is the California Housing dataset, which is conveniently
    provided by scikit-learn. The others we’ll use are commonly used in papers comparing
    uncertainty models: the Wine Quality dataset and the Concrete Comdivssive Strength
    dataset. Let’s take a look at a breakdown of these datasets:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估不同模型的性能，我们需要一些不同的数据集。其一是加利福尼亚住房数据集，scikit-learn 已经方便地提供了这个数据集。我们将使用的其他数据集是常见于不确定性模型比较论文中的：葡萄酒质量数据集和混凝土抗压强度数据集。让我们来看看这些数据集的详细信息：
- en: '**California Housing**: This dataset comprises a number of features for different
    regions in California derived from the 1990 California census. The dependent variable
    is house value, which is provided as the median value for each block of houses.
    In older papers, you’ll see the Boston Housing dataset used; the California Housing
    dataset is now favored due to ethical issues around the Boston Housing dataset.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加利福尼亚住房**：这个数据集包含了从1990年加利福尼亚人口普查中得出的不同地区的多个特征。因变量是房屋价值，以每个住宅区的中位房价表示。在较早的论文中，您会看到使用波士顿住房数据集；由于波士顿住房数据集存在伦理问题，现在更倾向于使用加利福尼亚住房数据集。'
- en: '**Wine Quality**: The Wine Quality dataset comprises features pertaining to
    the chemical composition of a variety of different wines. The value we’re trying
    to predict is the subjective quality of the wine.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**葡萄酒质量**：葡萄酒质量数据集包含与不同葡萄酒的化学成分相关的特征。我们要预测的值是葡萄酒的主观质量。'
- en: '**Concrete Comdivssive Strength**: The Concrete Comdivssive Strength dataset’s
    features describe the ingredients used for mixing concrete, and each data point
    is a different concrete mixture. The dependent variable is the concrete’s comdivssive
    strength.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混凝土抗压强度**：混凝土抗压强度数据集的特征描述了用于混合混凝土的成分，每个数据点代表不同的混凝土配方。因变量是混凝土的抗压强度。'
- en: The following experiments will use code from the book’s GitHub repository (
    [https://github.com/PacktPublishing/Bayesian-Deep-Learning](https://github.com/PacktPublishing/Bayesian-Deep-Learning)),
    which we’ve seen in various forms in the previous chapters. The example assumes
    that we’re running the code from within this repository.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验将使用本书 GitHub 仓库中的代码（ [https://github.com/PacktPublishing/Bayesian-Deep-Learning](https://github.com/PacktPublishing/Bayesian-Deep-Learning)），我们在前几章中已经见过不同形式的代码示例。示例假定我们从这个仓库内部运行代码。
- en: Importing our dependencies
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导入我们的依赖库
- en: 'As usual, we’ll start by importing our dependencies:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将首先导入我们的依赖库：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we can see that we’re using a number of model classes defined in the repository.
    While these classes each support different architectures of models, they’ll be
    using the default structure, which is defined in `constants.py`. This structure
    comprises a single densely connected hidden layer of 64 units, and a single densely
    connected output layer. The BBB and PBP equivalents will be used and are defined
    as their default architectures in their respective classes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们正在使用仓库中定义的多个模型类。虽然这些类支持不同架构的模型，但它们将使用默认结构，该结构在 `constants.py` 中定义。该结构包含一个64个单元的密集连接隐藏层，以及一个密集连接的输出层。BBB
    和 PBP 等价物将使用并在各自的类中定义为其默认架构。
- en: preparing our data and models
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备数据和模型
- en: 'Now we need to prepare our data and models to run our experiments. Firstly,
    we’ll set up a dictionary that we can iterate over to access data from different
    datasets:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要准备数据和模型以运行实验。首先，我们将设置一个字典，便于我们遍历并访问不同数据集中的数据：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we’ll create another dictionary to allow us to iterate over our different
    BDL models:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建另一个字典，以便我们可以遍历不同的BDL模型：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we’ll create a dictionary to hold our results:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个字典来保存我们的结果：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we see that we’ll be recording two results: the log-likelihood, and the
    mean-squared error. We’re using these metrics as we’re looking at regression problems,
    but for classification problems you may opt to use F-score or accuracy in place
    of the mean-squared error, and expected calibration error in place of (or as well
    as) log-likelihood. We’ll also be storing the model type in the `Method` field,
    and the dataset in the `Dataset` field.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们将记录两个结果：对数似然和均方误差。我们使用这些指标是因为我们在处理回归问题，但对于分类问题，您可以选择使用F分数或准确度替代均方误差，使用预期校准误差替代（或与）对数似然。我们还将在
    `Method` 字段中存储模型类型，在 `Dataset` 字段中存储数据集。
- en: Running our experiments
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行我们的实验
- en: 'Now we’re ready to run our experiments. However, we’re not only interested
    in the model performance, but also the computational considerations of our various
    models. As such, we’ll see calls to `tf.profiler` in the following code. First,
    however, we’ll set a few parameters:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始运行实验了。然而，我们不仅对模型性能感兴趣，还对我们各种模型的计算考虑因素感兴趣。因此，我们将在接下来的代码中看到对`tf.profiler`的调用。首先，我们将设置一些参数：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we’re setting the number of epochs each model will train for, as well
    as the batch size each model will use. We’re also setting `logdir_base`, the location
    that all of our profiling logs will be written to.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置每个模型的训练周期数以及每个模型将使用的批次大小。我们还设置了`logdir_base`，即所有分析日志的存储位置。
- en: 'Now we’re ready to drop in our experiment code. We’ll start by iterating over
    the datasets:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好插入实验代码了。我们将首先遍历数据集：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we see that for each dataset we’re splitting the data, using ![2 3](img/file149.jpg)
    of the data for training and ![1 3](img/file150.jpg) for testing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到对于每个数据集，我们将数据拆分，使用![2 3](img/file149.jpg)的数据进行训练，使用![1 3](img/file150.jpg)的数据进行测试。
- en: 'Next, we iterate over the models:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历模型：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For each model, we instantiate a new log directory to log the training information.
    We then instantiate the model and run `model.fit()`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，我们实例化一个新的日志目录来记录训练信息。然后我们实例化模型并运行`model.fit()`：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the model fits, we stop the profiler and create a new directory to log
    the prediction information, after which we start the profiler again:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型拟合完成，我们停止分析器，并创建一个新目录来记录预测信息，之后我们再次启动分析器：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the profiler running, we run predict, after which we again stop the profiler.
    With our predictions in hand, we can compute our mean squared error and log-likelihood,
    and store these to our `results` dictionary. Finally, we run `tf.keras.backend.clear_session()`
    to clear our TensorFlow graph after each experiment within our `model` loop:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析器运行的情况下，我们进行预测，然后再次停止分析器。手头有了预测后，我们可以计算均方误差和对数似然，并将其存储到`results`字典中。最后，我们在每次实验结束后运行`tf.keras.backend.clear_session()`来清理TensorFlow图：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once we’ve got results for all models and all datasets, we convert our results
    dictionary into a pandas DataFrame:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了所有模型和所有数据集的结果，我们将结果字典转换为pandas DataFrame：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we’re ready to analyze our data!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好分析数据了！
- en: 7.2.2 Analyzing model performance
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 分析模型性能
- en: 'With the data obtained from our experiments, we can plot this to see which
    models performed best on which datasets. To do so, we’ll use the following plotting
    code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 利用从实验中获得的数据，我们可以绘制图表，查看哪些模型在不同的数据集上表现最佳。为此，我们将使用以下绘图代码：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that initially we add a `'NLL'` field to our pandas DataFrame. This gives
    us the negative log-likelihood. This makes things a little less confusing when
    looking at the plots, as lower is better for both mean squared error and negative
    log-likelihood.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最初我们将`'NLL'`字段添加到pandas DataFrame中。这为我们提供了负对数似然。这使得查看图表时不那么混乱，因为对于均方误差和负对数似然，较低的值是更好的。
- en: The code iterates over the datasets and metrics, and creates some nice bar plots
    with the help of the Seaborn plotting library. In addition to this, we use calls
    to `ax.text()` to overlay the metric values on the bar plots, allowing us to see
    the values clearly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代码遍历数据集和度量，借助Seaborn绘图库生成一些漂亮的条形图。此外，我们还使用`ax.text()`调用将度量值叠加到条形图上，以便清晰地看到数值。
- en: Also notice how, for the California Housing data, we’re capping our *y* values
    at 100 for our negative log-likelihood. This is because, for this dataset, our
    negative log-likelihood value is *incredibly* high - making it difficult to view
    this in context with the other values. This is another reason why we’re overlaying
    the metric values, to allow us to compare them easily as one of the values exceeds
    the limit of the plot.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，对于加利福尼亚住房数据，我们将负对数似然中的*y*值限制在100。这是因为，在这个数据集中，我们的负对数似然值*极其*高，使得它与其他值一起显示时变得困难。因此，我们叠加了度量值，以便在某些值超过图表限制时，能够更容易地进行比较。
- en: '![PIC](img/file151.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file151.png)'
- en: 'Figure 7.1: Bar plot of results from LL and MSE experiments'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：LL和MSE实验结果的条形图
- en: It’s worth noting that, for fair comparison, we’ve used the equivalent architecture
    across all models, used the same batch size, and trained for the same number of
    epochs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，为了公平比较，我们在所有模型中使用了等效的架构，使用了相同的批次大小，并进行了相同次数的训练周期。
- en: 'As we see here, there’s no single best method: each model performs differently
    depending on the data, and a model with low mean squared error isn’t guaranteed
    to also have a low negative log-likelihood score. Generally speaking, MC dropout
    exhibits the worst mean squared error scores; however, it also produces the best
    negative log-likelihood observed during our experiments for the Wine Quality dataset,
    for which it achieves a negative log-likelihood of 2.9\. This is due to the fact
    that, while it generally performs worse in terms of error, its uncertainties are
    very high. As such, because it is more uncertain in regions for which it is wrong,
    it produces a more favorable negative log-likelihood score. We can see this if
    we plot the errors against the uncertainties:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，并没有一种单一的最佳方法：每种模型根据数据的不同表现不同，而且具有较低均方误差的模型并不保证也会有较低的负对数似然分数。一般来说，MC dropout表现出最差的均方误差分数；然而，它也产生了在我们实验中观察到的最佳负对数似然分数，对于葡萄酒质量数据集，它达到了2.9的负对数似然。这是因为，尽管它在误差方面通常表现较差，但它的不确定性非常高。因此，由于在其错误的区域表现出更大的不确定性，它会产生更有利的负对数似然分数。如果我们将误差与不确定性绘制出来，就能看到这一点：
- en: '![PIC](img/file152.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file152.png)'
- en: 'Figure 7.2: Scatter plot of errors vs uncertainty estimates'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：误差与不确定性估计的散点图
- en: In *Figure* [*7.2*](#x1-120077r2), we see the BBB, PBP, and ensemble results
    in the plot on the left, while MC dropout’s results are in the plot on the right.
    The reason for this is that MC dropout’s uncertainty estimates are two orders
    of magnitude higher than the uncertainty estimates produced by the other methods,
    thus they can’t be clearly represented on the same axes. These very high-magnitude
    uncertainties are also the reason behind its comparatively low negative log-likelihood
    score. This is quite a surprising example for MC dropout, as it is typically *over-confident*,
    whereas in this case, it’s clearly *under-confident*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图* [*7.2*](#x1-120077r2)中，我们可以看到左侧图中是BBB、PBP和集成方法的结果，而右侧图则是MC dropout的结果。原因在于，MC
    dropout的不确定性估计比其他方法高出两个数量级，因此它们无法在同一坐标轴上清晰地表示。这些极高的不确定性也是其相对较低负对数似然分数的原因。这是MC
    dropout的一个相当令人惊讶的例子，因为它通常是*过于自信的*，而在这种情况下，它显然是*不自信的*。
- en: While MC dropout’s under-confidence may lead to better likelihood scores, these
    metrics need to be considered in context; we typically want a good trade-off between
    likelihood and error. As such, PBP is probably the best choice in the case of
    the Wine Quality data as it has the lowest error, but it also has a reasonable
    likelihood; its negative log-likelihood is not so low as to be suspicious, but
    also low enough to know that the uncertainty estimates will be reasonably consistent
    and principled.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MC dropout的低自信可能会导致更好的似然分数，但这些度量需要在具体背景下进行考虑；我们通常希望在似然与误差之间取得良好的平衡。因此，在葡萄酒质量数据集的情况下，PBP可能是最佳选择，因为它具有最低的误差，同时也有合理的似然；它的负对数似然并不低到让人怀疑，但又足够低，可以知道其不确定性估计将会是合理一致和有原则的。
- en: 'In the case of the other datasets, the choices are a little more straightforward:
    BBB is the clear winner for California Housing, and PBP again proves to be the
    sensible choice on balance in the case of Concrete Comdivssive Strength. This
    is all with the important caveat that none of these networks have been specifically
    optimized for these datasets: this is merely an illustrative example.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他数据集，选择会更直接一些：对于加利福尼亚住房数据集，BBB显然是最优选择，而在混凝土抗压强度数据集的情况下，PBP再次被证明是最为理智的选择。需要注意的是，这些网络并未针对这些数据集进行专门优化：这只是一个说明性示例。
- en: Crucially, it’s going to come down to the specific application and how much
    robust uncertainty estimates matter. For example, in a safety-critical scenario,
    you’ll want to go with a method with the most robust uncertainty estimates, and
    so you may favor under-confidence over a lower error because you want to make
    sure that you’re only going with the model when you’re very confident in its outcome.
    In these cases, you may well go for an under-confident but high likelihood (low
    negative likelihood) method such as MC dropout on the Wine Quality dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，最终决定将取决于具体应用以及强健的不确定性估计有多重要。例如，在安全关键的场景中，你会希望选择具有最强健不确定性估计的方法，因此你可能会偏向于选择低自信而非较低的误差，因为你想确保只有在对模型结果非常有信心时才会使用该模型。在这些情况下，你可能会选择一种不自信但高概率（低负概率）的方法，比如在葡萄酒质量数据集上的MC
    dropout。
- en: In other cases, perhaps uncertainty doesn’t matter at all, in which case you
    may just go for a standard neural network. But in most mission-critical or safety-critical
    applications, you’re going to want to strike a balance and take advantage of the
    additional information provided by model uncertainty estimates while still achieving
    a low error score. However, practically, these performance metrics aren’t the
    only thing we need to consider when developing machine learning systems. We also
    care about the practical implications. In the next section, we’ll see how the
    computational requirements of these models stack up against each other.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，也许不需要考虑不确定性，这时你可能会选择一个标准的神经网络。但在大多数关键任务或安全关键型应用中，你会希望找到一个平衡点，利用模型不确定性估计提供的附加信息，同时保持较低的错误率。然而，实际上，在开发机器学习系统时，性能指标并不是唯一需要考虑的因素。我们还关心实际的应用影响。在下一部分，我们将看看这些模型的计算要求如何相互比较。
- en: 7.2.3 Computational considerations of Bayesian deep learning models
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 贝叶斯深度学习模型的计算考虑
- en: 'For every real-world application of machine learning, there are considerations
    beyond performance: we also need to understand the practical limitations of the
    compute infrastructure. They are usually governed by a few things, but existing
    infrastructure and cost tend to come up time and time again.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习的每一个现实世界应用，除了性能之外，还有其他考虑因素：我们还需要理解计算基础设施的实际限制。它们通常由几个因素决定，但现有的基础设施和成本往往反复出现。
- en: 'Existing infrastructure is often important because unless it’s a totally new
    project, it’s a case of working out how a machine learning model can be integrated,
    and this means either finding or requesting additional computational resources
    on a hardware or software stack. It will come as no surprise that cost is a significant
    factor: every project has a budget, and the expense that the machine learning
    component of the solution brings to the table needs to be balanced against the
    advantages that it provides. The budget will often dictate what machine learning
    solutions are feasible based on the cost of the computational resources required
    to train, deploy, and run them at inference time.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现有基础设施通常很重要，因为除非是一个全新的项目，否则需要解决如何将机器学习模型集成的问题，这意味着要么找到现有的计算资源，要么请求额外的硬件或软件资源。成本是一个显著的因素也不足为奇：每个项目都有预算，机器学习解决方案的开销需要与其带来的优势相平衡。预算往往会决定哪些机器学习解决方案是可行的，这取决于训练、部署和推理时所需计算资源的成本。
- en: 'To get some insight into how the methods here compare in terms of their computational
    requirements, we’ll look at the output of the TensorFlow profiler that we included
    in our experiment code. To do this, we simply need to run TensorBoard from the
    command line, pointing to the logging directory for the particular model we’re
    interested in:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解这些方法在计算要求方面的比较，我们将查看我们实验代码中包含的 TensorFlow 性能分析器的输出。为此，我们只需从命令行运行 TensorBoard，并指向我们感兴趣的特定模型的日志目录：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will start a TensorBoard instance (typically at http://localhost:6006/).
    Copy the URL into your browser, and you’ll be greeted with the TensorBoard GUI.
    TensorBoard provides you with a suite of tools for understanding the performance
    of your TensorFlow models, from the execution time right down to the memory allocation
    of different processes. You can scroll through the available tools via the **Tools**
    selection box in the top left corner of the screen:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一个 TensorBoard 实例（通常在 http://localhost:6006/）。将 URL 复制到浏览器中，你将看到 TensorBoard
    的图形用户界面（GUI）。TensorBoard 为你提供了一套工具，用于理解 TensorFlow 模型的性能，从执行时间到不同进程的内存分配。你可以通过屏幕左上角的
    **Tools** 选择框浏览可用的工具：
- en: '![PIC](img/tensorboard_tool_select.JPG)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/tensorboard_tool_select.JPG)'
- en: 'Figure 7.3: Tool selection box in the TensorBoard GUI'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：TensorBoard 图形用户界面的工具选择框
- en: 'To get a very detailed picture of what’s going on, take a look at the Trace
    Viewer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要更详细地了解发生了什么，请查看追踪查看器（Trace Viewer）：
- en: '![PIC](img/file154.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file154.png)'
- en: 'Figure 7.4: Trace Viewer in the TensorBoard GUI'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：TensorBoard 图形用户界面中的追踪查看器
- en: 'Here, we get an overall picture of the time taken to run our model’s functions,
    as well as a detailed picture of which processes are running under the hood, and
    how long each of these processes take to run. We can even dig deeper by double-clicking
    on a block and looking at its statistics. For example, we can double-click on
    the **train** block:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以获得一个整体视图，展示运行模型函数所需的时间，以及一个详细的视图，展示哪些进程在后台运行，以及每个进程的运行时间。我们甚至可以通过双击一个模块查看其统计信息。例如，我们可以双击
    **train** 模块：
- en: '![PIC](img/file155.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file155.png)'
- en: 'Figure 7.5: Trace Viewer from the TensorBoard GUI, highlighting the train block'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：TensorBoard 图形界面中的追踪查看器，突出显示训练模块
- en: 'This brings up some information at the bottom of our screen. This allows us
    to closely examine the run time of this process. If we click on **Duration**,
    we get a detailed breakdown of the run duration statistics for the process:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在屏幕底部显示一些信息。这使我们能够密切检查此过程的运行时间。如果我们点击 **持续时间**，则会得到此过程的详细运行时统计数据：
- en: '![PIC](img/tensorboard_profiling_trace_train_stats.JPG)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/tensorboard_profiling_trace_train_stats.JPG)'
- en: 'Figure 7.6: Examining the statistics of a block in the TensorBoard Trace Viewer'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：在 TensorBoard 追踪查看器中检查模块的统计数据
- en: 'Here, we see that the process was run 10 times (once per epoch) and that the
    average duration is 144,527,053 ns (nanoseconds). Let’s use our profiler results
    for the Concrete Comdivssion Strength dataset and collect the runtime and memory
    allocation information using TensorBoard. If we do this for each of our models’
    training runs, we obtain the following information:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到该过程运行了 10 次（每个 epoch 运行一次），平均持续时间为 144,527,053 纳秒（ns）。让我们使用混凝土抗压强度数据集的性能分析结果，并通过
    TensorBoard 收集运行时和内存分配信息。如果我们为每个模型的训练过程都进行此操作，就能得到以下信息：
- en: '|  |  | **Profiling data for model training** |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **模型训练的性能分析数据** |  |'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Model** |  | **Peak memory usage (MiB)** | **Duration (ms)** |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **模型** |  | **峰值内存使用量（MiB）** | **持续时间（ms）** |'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **BBB** |  | 0.09 | 4270 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **BBB** |  | 0.09 | 4270 |'
- en: '| **PBP** |  | 0.253 | 10754 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **PBP** |  | 0.253 | 10754 |'
- en: '| **MCDropout** |  | 0.126 | 2198 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **MC Dropout** |  | 0.126 | 2198 |'
- en: '| **Ensemble** |  | 0.215 | 20630 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **集成模型** |  | 0.215 | 20630 |'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  |  |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: 'Figure 7.7: Table of profiling data for model training for Concrete Comdivssive
    Strength dataset'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：混凝土抗压强度数据集模型训练的性能分析数据表
- en: Here, we see that MC dropout is the fastest model to train for this dataset,
    taking half as long as BBB. We also see that the ensemble takes by far the longest
    to train, nearly 10 times as long as MC dropout, despite the fact that the ensemble
    comprises only 5 models. In terms of memory usage, we see that the ensemble again
    performs poorly, but that PBP is the most memory hungry of the models, while BBB
    has the lowest peak memory usage.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到 MC dropout 是该数据集中训练速度最快的模型，训练时间仅为 BBB 的一半。我们还看到，尽管集成模型只包含 5 个模型，但其训练时间远远是最久的，几乎是
    MC dropout 的 10 倍。就内存使用而言，我们看到集成模型的表现较差，而 PBP 是所有模型中内存最消耗的，BBB 则具有最低的峰值内存使用量。
- en: 'But it’s not just training that counts. We also need to factor in the computational
    cost of inference. Looking at our profiling data for our models’ predict functions,
    we see the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非仅仅训练才是关键。我们还需要考虑推理的计算成本。查看我们模型预测函数的性能分析数据，我们看到如下信息：
- en: '|  |  | **Profiling data for model predictions** |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **模型预测的性能分析数据** |  |'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Model** |  | **Peak memory usage (MiB)** | **Duration (ms)** |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **模型** |  | **峰值内存使用量（MiB）** | **持续时间（ms）** |'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **BBB** |  | 0.116 | 849 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **BBB** |  | 0.116 | 849 |'
- en: '| **PBP** |  | 1.27 | 176 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| **PBP** |  | 1.27 | 176 |'
- en: '| **MCDropout** |  | 0.548 | 23 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **MC Dropout** |  | 0.548 | 23 |'
- en: '| **Ensemble** |  | 0.389 | 17 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **集成模型** |  | 0.389 | 17 |'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: 'Figure 7.8: Table of profiling data for model prediction for Concrete Comdivssive
    Strength dataset'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：混凝土抗压强度数据集模型预测的性能分析数据表
- en: Interestingly, here we see that the ensemble is in the lead when it comes to
    model inference speed, and it also comes in second when we look at peak memory
    usage for predictions. In contrast, PBP has by far the highest peak memory usage,
    while BBB takes the longest to run inference.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在模型推理速度方面，集成方法领先，而在预测时的峰值内存使用上也位居第二。相比之下，PBP的峰值内存使用最高，而BBB推理所需的时间最长。
- en: There are various factors contributing to the results we see here. Firstly,
    it’s important to note that none of these models are properly optimized for computational
    performance. For example, we could significantly cut down the training duration
    for our ensemble by training all the ensemble members in parallel, which we don’t
    do here. Similarly, because PBP used a lot of high-level code in its implementation
    (unlike the other methods, which are all built on nicely optimized TensorFlow
    or TensorFlow Probability code), its performance suffers as a result.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有多种因素导致了我们看到的结果。首先，需要注意的是，这些模型都没有针对计算性能进行优化。例如，我们可以通过并行训练所有集成成员来显著缩短集成方法的训练时长，而在这里我们并没有这么做。类似地，由于PBP在实现中使用了大量的高级代码（不同于其他方法，这些方法都是基于优化过的TensorFlow或TensorFlow
    Probability代码），因此其性能受到影响。
- en: Most crucially, we need to ensure that, when selecting the right model for the
    job, we consider the computational implications as well as the typical performance
    metrics. So, with all that in mind, how do we choose the right model?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最关键的是，我们需要确保在选择适合的模型时，既要考虑计算影响，也要考虑典型的性能指标。那么，考虑到这一点，我们如何选择合适的模型呢？
- en: 7.2.4 Choosing the right model
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 选择合适的模型
- en: With our performance metrics and profiling information in hand, we have all
    the data we need to choose the right model for the task. But model selection isn’t
    easy; we see here that all our models have advantages and disadvantages.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的性能指标和分析信息，我们拥有了选择适合任务的模型所需的所有数据。但模型选择并不容易；正如我们在这里看到的，所有模型都有其优缺点。
- en: 'If we start with performance metrics, then we see that BBB has the lowest mean
    squared error, but it also has a very high negative log-likelihood. So, the best
    choice on the basis of performance metrics alone is PBP: it has the lowest negative
    log-likelihood score, and its mean squared error is nowhere near as poor as MC
    dropout’s error, making PBP the best choice, on balance.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从性能指标开始，那么我们看到BBB的均方误差最低，但它的负对数似然值却非常高。所以，仅从性能指标来看，最佳选择是PBP：它的负对数似然得分最低，且均方误差也远远好于MC
    dropout的误差，这使得PBP在综合考虑下成为最佳选择。
- en: 'However, if we look at the computational implications in *Figure* [*7.7*](#x1-121014r7)
    and [*7.8*](#x1-121017r8), we see that PBP is one of the worst choices both in
    terms of memory usage and execution time. The best choice here, on balance, would
    be MC dropout: its prediction time is only a little slower than the prediction
    time of the ensemble, and it has the shortest training duration.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们查看*图* [*7.7*](#x1-121014r7) 和 [*7.8*](#x1-121017r8)中的计算影响，我们会发现PBP在内存使用和执行时间方面都是最差的选择。在这里，综合来看，最好的选择是MC
    dropout：它的预测时间仅比集成方法稍慢，而且训练时长最短。
- en: 'At the end of the day, it’s entirely down to the application: perhaps inference
    doesn’t need to be run in real time, so we can go with our PBP implementation.
    Or perhaps inference time and low error are our key considerations, in which case
    the ensemble is a good choice. As we see here, metrics and computational overheads
    need to be considered in context, and, as with any class of machine learning models,
    there’s no single best choice for all applications. It’s all down to choosing
    the right tool for the job.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，这完全取决于应用：也许推理不需要实时运行，那么我们可以选择PBP实现。或者，也许推理时间和低误差是我们关注的重点，在这种情况下，集成方法是一个不错的选择。正如我们在这里看到的，指标和计算开销需要在具体情况下加以考虑，正如任何一类机器学习模型一样，并没有一个适用于所有应用的最佳选择。一切都取决于选择合适的工具来完成工作。
- en: In this section, we’ve introduced tools for comprehensively understanding model
    performance and demonstrated how important it is to consider a range of factors
    when selecting a model. Fundamentally, performance analysis and profiling are
    as important for helping us to make the right practical choices as they are for
    helping us to uncover opportunities for further improvements. We may not have
    time for further optimizing our code and so may need to be pragmatic and go with
    the best computationally optimized method we have to hand. Alternatively, the
    business case may dictate that we need the model with the best performance, which
    may justify investing time to optimize our code and reduce the computational overheads
    of a given method. In the next section, we’ll take a look at another important
    practical consideration of working with BDL methods as we learn how we can use
    these methods to better understand sources of uncertainty.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了用于全面理解模型性能的工具，并演示了在选择模型时考虑多种因素的重要性。从根本上讲，性能分析和分析配置文件对帮助我们做出正确的实际选择与帮助我们发现进一步改进的机会同样重要。我们可能没有时间进一步优化代码，因此可能需要务实地选择我们手头上最优化的计算方法。或者，业务需求可能决定我们需要选择性能最好的模型，这可能值得花时间优化代码并减少某种方法的计算开销。在下一节中，我们将进一步探讨使用
    BDL 方法时的另一个重要实际考虑因素，学习如何使用这些方法更好地理解不确定性的来源。
- en: 7.3 BDL and sources of uncertainty
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 BDL 与不确定性来源
- en: In this case study, we will look at how we can model aleatoric and epistemic
    uncertainty in a regression problem when we are trying to predict a continuous
    outcome variable. We will use a real-life dataset of diamonds that contains the
    physical attributes of more than 50,000 diamonds as well as their prices. In particular,
    we will look at the relationship between the weight of a diamond (measured as
    its **carat**) and the price paid for the diamond.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将探讨如何在回归问题中建模 aleatoric 和 epistemic 不确定性，目标是预测一个连续的结果变量。我们将使用一个现实世界的钻石数据集，该数据集包含了超过
    50,000 颗钻石的物理属性以及它们的价格。特别地，我们将关注钻石的重量（以 **克拉** 测量）和钻石价格之间的关系。
- en: 'Step 1: Setting up the environment'
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：设置环境
- en: 'To set up the environment, we import several packages. We import `tensorflow`
    and `tensorflow_probability` for building and training vanilla and probabilistic
    neural networks, `tensorflow_datasets` for importing the diamonds data set, `numpy`
    for performing calculations and operations on numerical arrays (such as calculating
    the mean), `pandas` for handling DataFrames, and `matplotlib` for plotting:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置环境，我们导入了几个包。我们导入 `tensorflow` 和 `tensorflow_probability` 用于构建和训练传统的和概率神经网络，导入
    `tensorflow_datasets` 用于导入钻石数据集，导入 `numpy` 用于对数值数组进行计算和操作（如计算均值），导入 `pandas` 用于处理
    DataFrame，导入 `matplotlib` 用于绘图：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, we load the diamonds dataset using the `load` function provided by `tensorflow_datasets`.
    We load the dataset as a `pandas` DataFrame, which is convenient for preparing
    the data for training and inference.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `tensorflow_datasets` 提供的 `load` 函数加载钻石数据集。我们将数据集加载为一个 `pandas` DataFrame，这对于准备训练和推理数据非常方便。
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The dataset contains many different attributes of diamonds, but here we will
    focus on carat and price by selecting the respective columns from the DataFrame:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含了钻石的许多不同属性，但在这里我们将重点关注克拉和价格，通过选择 DataFrame 中相应的列：
- en: '[PRE15]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then divide the dataset into train and test splits. We use 80% of the data
    for training and 20% for testing:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据集分为训练集和测试集。我们使用 80% 的数据进行训练，20% 的数据进行测试：
- en: '[PRE16]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For further processing, we convert the train and test DataFrames to NumPy arrays:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步处理，我们将训练和测试 DataFrame 转换为 NumPy 数组：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We also save the number of training samples into a variable because we will
    need it later during model training:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将训练样本的数量保存到一个变量中，因为我们在模型训练过程中需要它：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we define a plotting function. This function will come in handy during
    the rest of the case study. It allows us to plot the data points as well as the
    fitted model predictions and their standard deviations:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了一个绘图函数。这个函数将在接下来的案例研究中派上用场。它允许我们绘制数据点以及拟合模型的预测值和标准差：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using this function, we can have a first look at the training data by running
    the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，我们可以通过运行以下代码来首次查看训练数据：
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The training data distribution is shown in *Figure* [*7.9*](#x1-124168r9). We
    observe that the relationship between carat and diamond price is non-linear, with
    prices increasing more rapidly at higher carat.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据分布如 *图 7.9* 所示。我们观察到，克拉和钻石价格之间的关系是非线性的，随着克拉数的增加，价格的增长速度也更快。
- en: '![PIC](img/file157.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file157.png)'
- en: 'Figure 7.9: Relationship between the carat of a diamond and its price'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：钻石克拉数与价格之间的关系
- en: 'Step 2: Fitting a model without uncertainty'
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：拟合一个不带不确定性的模型
- en: Having completed the setup, we are ready to fit some regression models to the
    data. We start by fitting a neural network model without quantifying the uncertainty
    in the predictions. This allows us to establish a baseline and to introduce some
    tools (in the form of functions) that will be useful for all of the models in
    this case study.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 完成设置后，我们可以开始为数据拟合回归模型。首先，我们拟合一个神经网络模型，但不对预测中的不确定性进行量化。这使我们能够建立一个基准，并引入一些对本案例研究中所有模型都非常有用的工具（以函数的形式）。
- en: 'It is recommended that you normalize the input features to a neural network
    model. In this example, that means normalizing the weight of the diamonds in carats.
    Normalizing input features will make the model converge faster during training.
    `tensorflow.keras` provides a convenient normalization function that allows us
    to do just that. We can use it as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 建议对神经网络模型的输入特征进行归一化。在本示例中，这意味着对钻石的克拉重量进行归一化。归一化输入特征将使模型在训练过程中收敛得更快。`tensorflow.keras`
    提供了一个方便的归一化函数，可以帮助我们实现这一点。我们可以按如下方式使用它：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will also need a loss function, ideally one that can be used for all the
    models in this case study. A regression model can be posed as *P*(*y*|*x,w*),
    the probability distribution of labels *y* given the inputs *x* and model parameters
    *w*. We can fit such a model to the data by minimizing the negative log-likelihood
    loss −*logP*(*y*|*x*). In `Python` code, this can be written as a function that
    takes as input the true outcome value `y_true` and the predicted outcome distribution
    `y_divd` and returns the negative log-likelihood of the outcome value under the
    predicted outcome distribution, which is implemented in the `log_prob()` method
    provided by the `distributions` module in `tensorflow_probability`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个损失函数，理想情况下，该损失函数可以用于本案例研究中的所有模型。回归模型可以表示为 *P*(*y*|*x,w*)，即给定输入 *x* 和模型参数
    *w* 的标签 *y* 的概率分布。我们可以通过最小化负对数似然损失 −*logP*(*y*|*x*) 来拟合此类模型。用 `Python` 代码表示时，可以编写一个函数，该函数将真实结果值
    `y_true` 和预测结果分布 `y_divd` 作为输入，并返回在预测结果分布下的结果值的负对数似然值，该方法由 `tensorflow_probability`
    中的 `distributions` 模块提供的 `log_prob()` 实现：
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Equipped with these tools, let’s build our first model. We use the normalizer
    function that we just defined to normalize the model inputs. We then stack two
    dense layers on top. The first dense layer consists of 32 nodes. This allows us
    to model the non-linearity observed in the data. The second dense layer consists
    of one node in order to reduce the model prediction to a single value. Importantly,
    we do not use the output produced by this second dense layer as the model output.
    Instead, we use the dense layer output to parameterize the mean of a normal distribution,
    which means that we are modeling the ground truth labels using a normal distribution.
    We also set the variance of the normal distribution to 1\. Parameterizing the
    mean of a distribution while setting the variance to a fixed value implies that
    we are modeling the overall trend of the data without yet quantifying uncertainty
    in the model’s predictions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些工具，我们可以构建第一个模型。我们使用刚才定义的归一化函数来归一化模型的输入。然后，我们在其上堆叠两个全连接层。第一个全连接层包含 32 个节点，这使我们能够建模数据中观察到的非线性关系。第二个全连接层包含一个节点，用于将模型的预测压缩为一个单一的值。重要的是，我们不使用第二个全连接层产生的输出作为模型的最终输出。相反，我们使用该全连接层的输出作为正态分布均值的参数化，这意味着我们正在使用正态分布来建模真实标签。我们还将正态分布的方差设为
    1。参数化分布的均值并将方差设为固定值，意味着我们正在建模数据的总体趋势，但尚未量化模型预测中的不确定性：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we’ve seen in previous case studies, to train the model we use the `compile()`
    and `fit()` functions. During compilation of the model, we specify the `Adam`
    optimizer and the previously defined loss function. For the `fit` function, we
    specify that we want to train the model for 100 epochs on the carat and price
    data:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的案例研究中看到的，训练模型时，我们使用`compile()`和`fit()`函数。在模型编译时，我们指定了`Adam`优化器和之前定义的损失函数。对于`fit`函数，我们指定了希望在克拉和价格数据上训练模型100个周期：
- en: '[PRE24]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can then obtain the model’s predictions on the hold-out test data and visualize
    everything using our `plot_scatter()` function:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过我们的`plot_scatter()`函数获得模型在保留测试数据上的预测，并可视化所有结果：
- en: '[PRE25]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This produces the following chart:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![PIC](img/prediction_model_no_uncertainty.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/prediction_model_no_uncertainty.png)'
- en: 'Figure 7.10: Predictions without uncertainty on diamond test data'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：没有不确定性的钻石测试数据预测
- en: We can see in *Figure* [*7.10*](#x1-125089r10) that the model captures the non-linear
    trend of the data. As a diamond’s weight increases, the model predicts prices
    to increase more rapidly as we add more and more weight.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图* [*7.10*](#x1-125089r10)中看到，模型捕捉到了数据的非线性趋势。随着钻石重量的增加，模型预测的价格也随着重量的增加而迅速上升。
- en: However, there is another obvious trend in the data that the model does not
    capture. We can observe that as weight increases, there is more and more variability
    in the price. At low weight, we only observe a little scatter around the fitted
    line, but the scatter increases at higher weight. We can consider this variability
    as inherent to the problem. That is, even if we had a lot more training data,
    we would still not be able to predict price, especially at high weight, perfectly.
    This sort of variability is aleatoric uncertainty, which we first encountered
    in [*Chapter 4*](CH4.xhtml#x1-490004), [*Introducing Bayesian Deep Learning*](CH4.xhtml#x1-490004),
    and will have a closer look at in the next subsection.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据中还有另一个显而易见的趋势是模型未能捕捉到的。我们可以观察到，随着重量的增加，价格的变异性越来越大。在低重量时，我们仅观察到拟合线周围少量的散布，但在较高重量时，散布增大。我们可以将这种变异性视为问题的固有特性。也就是说，即使我们有更多的训练数据，我们仍然无法完美地预测价格，尤其是在高重量时。这种类型的变异性就是随机不确定性，我们在[*第4章*](CH4.xhtml#x1-490004)中首次遇到过，将在下一小节中仔细探讨。
- en: 'Step 3: Fitting a model with aleatoric uncertainty'
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：拟合带有随机不确定性的模型
- en: 'We can account for aleatoric uncertainty in our model by predicting the standard
    deviation of the normal distribution in addition to predicting its mean. As before,
    we build a model with a normalizer layer and two dense layers. However, this time
    the second dense layer will output two values instead of one. The first output
    value will again be used to parameterize the mean of a normal distribution. But
    the second output value will parameterize the variance of the normal distribution,
    which allows us to quantify the aleatoric uncertainty in the model’s predictions:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过预测正态分布的标准差来考虑模型中的随机不确定性，除了预测其均值之外。和之前一样，我们构建了一个带有标准化层和两个全连接层的模型。然而，这次第二个全连接层将输出两个值，而不是一个。第一个输出值将再次用于参数化正态分布的均值。但第二个输出值将参数化正态分布的方差，从而使我们能够量化模型预测中的随机不确定性：
- en: '[PRE26]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We again compile and fit the model on the weight and price data:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次在重量和价格数据上编译并拟合模型：
- en: '[PRE27]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we can obtain and visualize predictions on the test data. Note that this
    time, we pass `plot_std=True` in order to also plot the standard deviation of
    the predicted output distribution:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获得并可视化测试数据的预测结果。请注意，这次我们传递了`plot_std=True`，以便同时绘制预测输出分布的标准差：
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We have now trained a model that represents the variation inherent to the data.
    The dashed error bars in *Figure* [*7.11*](#x1-126075r11) show the predicted variability
    of price as a function of weight. We can observe that the model is indeed less
    certain about what price to predict at weight above 1 carat, reflecting the larger
    scatter in the data that we observe at the higher weight range.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经训练了一个模型，表示数据固有的变异性。*图* [*7.11*](#x1-126075r11)中的虚线误差条显示了价格作为重量函数的预测变异性。我们可以观察到，模型确实对重量超过1克拉时的价格预测不太确定，这反映了我们在较高重量范围内观察到的数据的较大散布。
- en: '![PIC](img/prediction_model_aleatoric_uncertainty.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/prediction_model_aleatoric_uncertainty.png)'
- en: 'Figure 7.11: Predictions with aleatoric uncertainty on diamond test data'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：包含随机不确定性的钻石测试数据预测
- en: 'Step 4: Fitting a model with epistemic uncertainty'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第四步：拟合带有认知不确定性的模型
- en: In addition to aleatoric uncertainty, we also deal with epistemic uncertainty
    – the uncertainty that stems not from the data, but from our model. Looking back
    at *Figure* [*7.11*](#x1-126075r11), for example, the solid line, which represents
    the mean of our model prediction, appears to capture the trend of the data reasonably
    well. But given that training data is limited, we cannot be 100% certain that
    we found the true mean of the underlying data distribution. Maybe the true mean
    is actually a little bit greater or a little less than what we estimated it to
    be. In this section, we look at how we can model such uncertainty, and we will
    also see that epistemic uncertainty can be reduced by observing more data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 除了偶然性不确定性，我们还需要处理认知不确定性——这种不确定性来源于我们的模型，而不是数据本身。回顾一下[*图7.11*](#x1-126075r11)，例如，实线代表我们模型预测的均值，它似乎能合理地捕捉数据的趋势。然而，由于训练数据有限，我们无法百分之百确定我们找到了数据分布的真实均值。也许真实均值实际上略大于或略小于我们估计的值。在这一部分，我们将研究如何建模这种不确定性，我们还将看到，通过观察更多的数据，认知不确定性是可以减少的。
- en: The trick to modeling epistemic uncertainty is, once again, to represent the
    weights in our neural network by a distribution rather than a point estimate.
    We can achieve this by replacing the dense layers that we used previously with
    DenseVariational layers from `tensorflow_probability`. Under the hood, this will
    implement BBB, which we first learned about in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches for Bayesian Deep Learning*](CH5.xhtml#x1-600005). In
    brief, when using BBB, we learn the posterior distribution over the weights of
    our network using the principle of variational learning. In order to do so, we
    need to define both prior and posterior distribution functions.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 模型认知不确定性的技巧再次是，将我们神经网络中的权重表示为分布，而不是点估计。我们可以通过将之前使用的密集层替换为`tensorflow_probability`中的DenseVariational层来实现这一点。在底层，这将实现我们在[*第5章*](CH5.xhtml#x1-600005)中首次学习到的BBB方法，[*贝叶斯深度学习的原则方法*](CH5.xhtml#x1-600005)。简而言之，使用BBB时，我们通过变分学习原理来学习网络权重的后验分布。为了实现这一点，我们需要定义先验和后验分布函数。
- en: Note that the code example for BBB presented in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled* *Approaches for Bayesian Deep Learning*](CH5.xhtml#x1-600005) made
    use of predefined `tensorflow_probability` modules for 2D convolution and dense
    layers with the reparameterization trick, which implicitly defined prior and posterior
    functions for us. In this example, we will define the prior and posterior functions
    for the dense layer ourselves.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在[*第5章*](CH5.xhtml#x1-600005)中展示的BBB代码示例，[*贝叶斯深度学习的原则方法*](CH5.xhtml#x1-600005)使用了预定义的`tensorflow_probability`模块来处理2D卷积和密集层，并应用重参数化技巧，这样我们就隐式地定义了先验和后验函数。在本示例中，我们将自己定义密集层的先验和后验函数。
- en: 'We start by defining the prior over the dense layer’s weights (both the kernel
    and the bias terms). The prior distribution models the uncertainty in the weights
    before we observe any data. It can be defined using a multivariate normal distribution
    that has a trainable mean and a variance that is fixed at 1:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定义密集层权重的先验开始（包括内核和偏置项）。先验分布描述了我们在观察到任何数据之前，对权重的猜测不确定性。它可以通过一个多元正态分布来定义，其中均值是可训练的，方差固定为1：
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also define the variational posterior. The variational posterior is an approximation
    to the distribution of the dense layer’s weights after we have observed the training
    data. We again use a multivariate normal distribution:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了变分后验。变分后验是我们观察到训练数据后，密集层权重分布的近似值。我们再次使用多元正态分布：
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Equipped with these prior and posterior functions, we can define our model.
    As before, we use the normalizer layer to normalize our inputs and then stack
    two dense layers on top of each other. But this time, the dense layers will represent
    their parameters as distributions rather than point estimates. We achieve this
    by using the DenseVariational layers from `tensorflow_probability` together with
    our prior and posterior functions. The final output layer is a normal distribution
    with its variance set to 1 and with its mean parameterized by the output of the
    preceding DenseVariational layer:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些先验和后验函数，我们就能定义我们的模型。和之前一样，我们使用归一化层对输入进行归一化，然后堆叠两个密集层。但这一次，密集层将把它们的参数表示为分布，而不是点估计。我们通过将`tensorflow_probability`中的DenseVariational层与我们的先验和后验函数结合使用来实现这一点。最终的输出层是一个正态分布，其方差设置为1，均值由前一个DenseVariational层的输出来参数化：
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To observe the effect of the amount of available training data on epistemic
    uncertainty estimates, we first fit our model on a small subset of data before
    fitting it on all available training data. We take the first 500 samples from
    the training dataset:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察可用训练数据量对表征性不确定性估计的影响，我们首先在一个较小的数据子集上拟合模型，然后再用所有可用的训练数据拟合模型。我们取训练数据集中的前500个样本：
- en: '[PRE32]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We build, compile, and fit the model as before:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按之前的方式构建、编译并拟合模型：
- en: '[PRE33]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then obtain and plot predictions on the test data. Note that here, we sample
    from the posterior distribution 10 times, which allows us to observe how much
    the predicted mean varies with every sample iteration. If the predicted mean varies
    a lot, this means that epistemic uncertainty is estimated to be large, while if
    the mean varies only very little, there is only little estimated epistemic uncertainty:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们在测试数据上获得并绘制预测结果。请注意，这里我们从后验分布中抽取了10次样本，这使我们能够观察每次样本迭代时预测均值的变化。如果预测均值变化很大，说明表征性不确定性估计较大；如果均值变化非常小，则表示表征性不确定性较小：
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In *Figure* [*7.12*](#x1-127216r12), we can observe that the predicted mean
    varies over the 10 different samples. Interestingly, variation (and thus epistemic
    uncertainty) seems to be lower at lower weights and increases as weight increases.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图* [*7.12*](#x1-127216r12)中，我们可以观察到，预测均值在10个不同的样本中有所变化。有趣的是，变化（因此表征性不确定性）在较低权重时似乎较低，而随着权重的增加，变化逐渐增大。
- en: '![PIC](img/prediction_model_high_epistemic_uncertainty.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/prediction_model_high_epistemic_uncertainty.png)'
- en: 'Figure 7.12: Predictions with high epistemic uncertainty on diamond test data'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：在钻石测试数据上，具有高表征性不确定性的预测
- en: 'In order to verify that epistemic uncertainty can be reduced by training on
    more data, we train our model on the full training dataset:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证通过更多数据训练可以减少表征性不确定性，我们在完整的训练数据集上训练我们的模型：
- en: '[PRE35]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And then plot the predictions for the full data model:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后绘制完整数据模型的预测结果：
- en: '[PRE36]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As expected, we see in *Figure* [*7.13*](#x1-127259r13) that epistemic uncertainty
    is much lower now and the predicted mean varies very little over the 10 samples
    (to the point where it is hard to see any difference between the 10 red curves):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们在*图* [*7.13*](#x1-127259r13)中看到，表征性不确定性现在要低得多，且预测均值在10个样本中变化很小（以至于很难看到10条红色曲线之间的任何差异）：
- en: '![PIC](img/prediction_model_low_epistemic_uncertainty.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/prediction_model_low_epistemic_uncertainty.png)'
- en: 'Figure 7.13: Predictions with low epistemic uncertainty on diamond test data'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13：在钻石测试数据上，具有低表征性不确定性的预测
- en: 'Step 5: Fitting a model with aleatoric and epistemic uncertainty'
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：拟合具有偶然性和表征性不确定性的模型
- en: 'As a final exercise, we can put all the building blocks together and build
    a neural network that models both aleatoric and epistemic uncertainty. We can
    achieve this by using two DenseVariational layers (which will allow us to model
    epistemic uncertainty) and then stacking a normal distribution layer on top whose
    mean and variance are parameterized by the outputs of the second DenseVariational
    layer (which will allow us to model aleatoric uncertainty):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的练习，我们可以将所有构建模块结合起来，构建一个同时建模偶然性和表征性不确定性的神经网络。我们可以通过使用两个DenseVariational层（这将使我们能够建模表征性不确定性），然后在其上堆叠一个正态分布层，该层的均值和方差由第二个DenseVariational层的输出参数化（这将使我们能够建模偶然性不确定性）：
- en: '[PRE37]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can build and train this model following the same procedure as before. We
    can then again perform inference on the test data for 10 times, which yields the
    predictions shown in *Figure* [*7.14*](#x1-128075r14). Every of the 10 inferences
    now yields a predicted mean and standard deviation. The standard deviation represents
    the estimated aleatoric uncertainty for every inference, and the variation observed
    across the different inferences represents the epistemic uncertainty.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照之前的相同流程构建和训练该模型。然后我们可以再次在测试数据上进行10次推理，这会产生如*图* [*7.14*](#x1-128075r14)所示的预测结果。每一次推理都会产生一个预测均值和标准差。标准差代表每次推理的偶然性不确定性，而在不同推理之间观察到的变化则代表表征性不确定性。
- en: '![PIC](img/prediction_model_aleatoric_and_epistemic_uncertainty.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/prediction_model_aleatoric_and_epistemic_uncertainty.png)'
- en: 'Figure 7.14: Predictions with both epistemic and aleatoric uncertainty on diamond
    test data'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：在钻石测试数据上，具有表征性和偶然性不确定性的预测
- en: '7.3.1 Sources of uncertainty: Image classification case study'
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 不确定性的来源：图像分类案例研究
- en: In the previous case study, we saw how we can model aleatoric and epistemic
    uncertainty in a regression problem. In this section, we’ll look at the MNIST
    digits dataset one more time to model aleatoric and epistemic uncertainty. We
    will also explore how aleatoric uncertainty can be difficult to reduce, whereas
    epistemic uncertainty can be reduced with more data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的案例研究中，我们看到如何在回归问题中建模随机不确定性和认知不确定性。在本节中，我们将再次查看MNIST数字数据集，以建模随机不确定性和认知不确定性。我们还将探讨随机不确定性如何难以减少，而认知不确定性则可以通过更多数据来减少。
- en: 'Let’s start with our data. To make our example more insightful, we will not
    just use the standard MNIST dataset but also use a variant of MNIST named AmbiguousMNIST.
    This dataset contains generated images that are, unsurprisingly, inherently ambiguous.
    Let’s first load the data and then explore the AmbiguousMNIST dataset. We’ll start
    with the necessary imports:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据开始。为了让我们的例子更具启发性，我们不仅使用标准的MNIST数据集，还使用一个名为AmbiguousMNIST的MNIST变体。这个数据集包含生成的图像，显然是固有模糊的。让我们首先加载数据，然后探索AmbiguousMNIST数据集。我们从必要的导入开始：
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can download the AmbiguousMNIST dataset with the `ddu_dirty_mnist` library:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`ddu_dirty_mnist`库下载AmbiguousMNIST数据集：
- en: '[PRE39]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We then concatenate and shuffle the images and labels so that we have a good
    mix of both datasets during training. We also fix the shape of the dataset so
    that it fits the setup of our model:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将图像和标签进行拼接和混洗，以便在训练期间两种数据集能有良好的混合。我们还固定数据集的形状，以使其适应我们模型的设置：
- en: '[PRE40]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*Figure* [*7.15*](#x1-129091r15) gives an example of the AmbiguousMNIST images.
    We can see that the images are in between classes: a 4 can also be interpreted
    as an 9, a 0 can be interpreted as a 6, and vice versa. This means that our model
    will most likely struggle to classify at least a portion of these images correctly
    as they are inherently noisy.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.15*（[*7.15*](#x1-129091r15)）给出了AmbiguousMNIST图像的示例。我们可以看到图像处于两类之间：一个4也可以被解释为9，一个0可以被解释为6，反之亦然。这意味着我们的模型很可能难以正确分类这些图像中的至少一部分，因为它们本质上是噪声。'
- en: '![PIC](img/file158.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file158.png)'
- en: 'Figure 7.15: Examples of images from the AmbiguousMNIST dataset'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：来自AmbiguousMNIST数据集的图像示例
- en: 'Now that we have our train dataset, let’s load our test dataset as well. We
    will just use the standard MNIST test dataset:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练数据集，让我们也加载我们的测试数据集。我们将仅使用标准的MNIST测试数据集：
- en: '[PRE41]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now start to define our model. In this example, we use a small Bayesian
    neural net with **Flipout** layers. These layers sample from the kernel and bias
    posteriors during the forward pass and thus add stochasticity to our model. We
    can use this later on when we want to compute uncertainty values:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始定义我们的模型。在这个例子中，我们使用一个小型的贝叶斯神经网络，带有**Flipout**层。这些层在前向传递过程中从内核和偏置的后验分布中采样，从而为我们的模型增加随机性。我们可以在以后需要计算不确定性值时使用它：
- en: '[PRE42]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We define a block as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个块如下：
- en: '[PRE43]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We compile our model and can start training:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编译我们的模型并开始训练：
- en: '[PRE44]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We are now interested in separating images via epistemic uncertainty and aleatoric
    uncertainty. Epistemic uncertainty should separate our in-distribution images
    from out-of-distribution images, as these images can be seen as unknown unknowns:
    our model has never seen these images before, and should therefore assign high
    epistemic uncertainty (or *knowledge uncertainty*) to them. Although our model
    was trained on the AmbiguousMNIST dataset, the model should still have high aleatoric
    uncertainty when it would see images from this dataset at test time: training
    with these images does not reduce aleatoric uncertainty (or *data uncertainty*)
    as the images are inherently ambiguous.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有兴趣通过认知不确定性和随机不确定性来分离图像。认知不确定性应该将我们的分布内图像与分布外图像区分开，因为这些图像可以被视为未知的未知：我们的模型以前从未见过这些图像，因此应该对它们分配较高的认知不确定性（或*知识不确定性*）。尽管我们的模型是在AmbiguousMNIST数据集上训练的，但在测试时，当它看到这个数据集中的图像时，它仍然应该具有较高的随机不确定性：用这些图像进行训练并不会减少随机不确定性（或*数据不确定性*），因为这些图像本质上是模糊的。
- en: 'We use the FashionMNIST dataset as the out-of-distribution dataset. We use
    the AmbiguousMNIST test set as our ambiguous dataset for testing:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用FashionMNIST数据集作为分布外数据集。我们使用AmbiguousMNIST测试集作为我们用于测试的模糊数据集：
- en: '[PRE45]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s use the stochasticity of our model to create a variety of model predictions.
    We iterate over our test images fifty times:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用模型的随机性来生成多样的模型预测。我们对测试图像进行50次迭代：
- en: '[PRE46]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can then define some functions to compute the different kinds of uncertainty:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以定义一些函数来计算不同类型的不确定性：
- en: '[PRE47]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, we can see how well our model can distinguish between in-distribution,
    ambiguous, and out-of-distribution images. Let’s plot a histogram of the different
    distributions according to the different uncertainty methods:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到我们的模型在区分分布内、模糊和分布外图像方面的表现。让我们根据不同的不确定性方法绘制不同分布的直方图：
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This produces the following output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '![PIC](img/file159.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file159.png)'
- en: 'Figure 7.16: The different types of uncertainty on MNIST'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：MNIST上的不同类型不确定性
- en: What can we observe?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能观察到什么？
- en: Total and data uncertainty are relatively good at distinguishing in-distribution
    data from out-of-distribution and ambiguous data.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体不确定性和数据不确定性在区分分布内数据、分布外数据和模糊数据方面相对有效。
- en: However, data and total uncertainty are not able to separate ambiguous data
    from out-of-distribution data. To do that, we need knowledge uncertainty. We can
    see that knowledge uncertainty clearly separates ambiguous data from out-of-distribution
    data.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，数据不确定性和总体不确定性无法区分模糊数据与分布外数据。要做到这一点，我们需要知识不确定性。我们可以看到，知识不确定性能够清晰地区分模糊数据和分布外数据。
- en: We trained on ambiguous samples as well, but that doesn’t reduce the uncertainty
    of the ambiguous test samples to uncertainty levels similar to the original in-distribution
    data. This shows that data uncertainty cannot easily be reduced. The data is inherently
    ambiguous, no matter how much ambiguous data the model sees.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也对模糊样本进行了训练，但这并没有将模糊测试样本的不确定性降低到与原始分布内数据类似的水平。这表明数据不确定性不能轻易降低。无论模型看到多少模糊数据，数据本身就是模糊的。
- en: We can confirm these observations by looking at the AUROC for the different
    combinations of distributions.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看不同分布组合的AUROC来验证这些观察结果。
- en: 'We can first compute the AUROC score to compute the ability of our model to
    separate in-distribution and ambiguous images from out-of-distribution images:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以首先计算AUROC分数，以评估我们模型区分分布内和模糊图像与分布外图像的能力：
- en: '[PRE49]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We see a confirmation of what we saw in our histograms: knowledge uncertainty
    is far better than the other two types of uncertainty at separating in-distribution
    and ambiguous data from out-of-distribution data.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在直方图中看到的结果得到了确认：知识不确定性在区分分布内和模糊数据与分布外数据方面远胜于另外两种不确定性类型。
- en: '[PRE50]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can see that both total uncertainty and data uncertainty are able to separate
    in-distribution from ambiguous data pretty well. Using data uncertainty gives
    us a small improvement over using total uncertainty. Knowledge uncertainty, however,
    is not able to distinguish between in-distribution data and ambiguous data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，整体不确定性和数据不确定性能够相当好地区分分布内和模糊数据。使用数据不确定性相较于使用总体不确定性有所改善。然而，知识不确定性无法区分分布内数据和模糊数据。
- en: 7.4 Summary
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 总结
- en: 'In this chapter, we’ve taken a look at a number of practical considerations
    of using Bayesian deep learning: exploring trade-offs in model performance and
    learning how we can use Bayesian neural network methods to better understand the
    effects of different uncertainty sources on our data.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了使用贝叶斯深度学习的一些实际考虑因素：探索模型性能的权衡，并了解如何使用贝叶斯神经网络方法更好地理解不同不确定性来源对数据的影响。
- en: In the next chapter, we’ll dig further into applying BDL through a variety of
    case studies, demonstrating the benefits of these methods in a range of practical
    settings.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过各种案例研究进一步探讨应用贝叶斯深度学习，展示这些方法在多种实际环境中的优势。
- en: 7.5 Further reading
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 进一步阅读
- en: '*Practical Considerations for Probabilistic Backpropagation*, Matt Benatan
    *et al.*: In this paper, the authors explore methods to get the most out of PBP,
    demonstrating how different early stopping approaches can be used to improve training,
    exploring the tradeoffs associated with mini-batching, and more'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《概率反向传播的实践考虑》*，Matt Benatan *等*：在这篇论文中，作者探讨了如何最大限度地利用PBP，展示了不同的早停方法如何改善训练，探讨了迷你批次的权衡，等等。'
- en: '*Modeling aleatoric and epistemic uncertainty using TensorFlow and* *TensorFlow
    Probability*, Alexander Molak: In this Jupyter notebook, the author shows how
    to model aleatoric and epistemic uncertainty on regression toy data'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《使用TensorFlow和TensorFlow概率建模阿莱托里克和认知不确定性》*，Alexander Molak：在这个Jupyter笔记本中，作者展示了如何在回归玩具数据上建模阿莱托里克不确定性和认知不确定性。'
- en: '*Weight Uncertainty in Neural Networks*, Charles Blundell *et al.*: In this
    paper, the authors introduce BBB, which we use in the regression case study and
    is one of the key pieces of BDL literature'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络中的权重不确定性*，Charles Blundell *等*：在本文中，作者介绍了BBB，我们在回归案例研究中使用了它，它是贝叶斯深度学习文献中的关键部分。'
- en: '*Deep Deterministic Uncertainty: A Simple Baseline*, Jishnu Mukhoti *et al.*:
    In this work, the authors describe several experiments related to the different
    types of uncertainty and introduce the *AmbiguousMNIST* dataset that we used in
    the last case study'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度确定性不确定性：一个简单的基准*，Jishnu Mukhoti *等*：在这项工作中，作者描述了与不同类型的不确定性相关的几项实验，并介绍了我们在最后一个案例研究中使用的*AmbiguousMNIST*数据集。'
- en: '*Uncertainty Estimation in Deep Learning with application to Spoken* *Language
    Assessment*, Andrey Malinin: This thesis highlights the different sources of uncertainty
    with intuitive examples'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习中的不确定性估计及其在语音语言评估中的应用*，Andrey Malinin：本论文通过直观的示例突出不同来源的不确定性。'
