- en: Variational Autoencoders
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: Autoencoders can be really powerful in finding rich latent spaces. They are
    almost magical, right? What if we told you that **variational autoencoders** (**VAEs**)
    are even more impressive? Well, they are. They have inherited all the nice things
    about traditional autoencoders and added the ability to generate data from a parametric
    distribution.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器在寻找丰富的潜在空间方面非常强大。它们几乎是神奇的，对吧？如果我们告诉你**变分自编码器**（**VAE**）更令人印象深刻呢？嗯，它们确实更强大。它们继承了传统自编码器的所有优点，并增加了从参数化分布中生成数据的能力。
- en: In this chapter, we will introduce the philosophy behind generative models in
    the unsupervised deep learning field and their importance in the production of
    new data. We will present the VAE as a better alternative to a deep autoencoder.
    At the end of this chapter, you will know where VAEs come from and what their
    purpose is. You will be able to see the difference between deep and shallow VAE
    models and you will be able to appreciate the generative property of VAEs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍生成模型在无监督深度学习领域的理念及其在新数据生成中的重要性。我们将展示VAE作为深度自编码器的更好替代方案。在本章结束时，你将了解VAE的来源和目的。你将能够看出深度和浅层VAE模型之间的区别，并且能够欣赏VAE的生成特性。
- en: 'The chapter is organized as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构如下：
- en: Introducing deep generative models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍深度生成模型
- en: Examining the VAE model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检视VAE模型
- en: Comparing deep and shallow VAEs on MNIST
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MNIST上比较深层和浅层VAE
- en: Thinking about the ethical implications of generative models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思考生成模型的伦理影响
- en: Introducing deep generative models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度生成模型
- en: 'Deep learning has very interesting contributions to the general machine learning
    community, particularly when it comes to deep discriminative and generative models.
    We are familiar with what a discriminative model is—for example, a **Multilayer
    Perceptron** (**MLP**) is one. In a discriminative model, we are tasked with guessing,
    predicting, or approximating a desired target, [![](img/2c8c7283-b742-4625-b194-d682a0dae077.png)],
    given input data *![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png)*. In statistical
    theory terms, we are modeling the conditional probability density function, ![](img/4af3a90c-1525-4445-a4ec-ddade5cbafce.png).
    On the other hand, by a generative model, this is what most people mean:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为整个机器学习社区做出了非常有趣的贡献，特别是在深度判别模型和生成模型方面。我们熟悉判别模型的定义——例如，**多层感知机**（**MLP**）就是一个例子。在判别模型中，我们的任务是根据输入数据，猜测、预测或近似所需的目标，[![](img/2c8c7283-b742-4625-b194-d682a0dae077.png)]，*！[](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png)*。用统计学术语来说，我们正在建模条件概率密度函数，！[](img/4af3a90c-1525-4445-a4ec-ddade5cbafce.png)。另一方面，生成模型就是大多数人所指的：
- en: '*A model that can generate data ![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png) that
    follows a particular distribution based on an input or stimulus ![](img/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png).*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个可以生成数据！[](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png)，它遵循特定分布，基于输入或刺激！[](img/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png)。*'
- en: In deep learning, we can build a neural network that can model this generative
    process very well. In statistical terms, the neural model approximates the conditional
    probability density function, ![](img/8c63f596-488c-4e73-a7aa-7e08c2fa3fac.png). While
    there are several generative models today, in this book, we will talk about three
    in particular.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们可以建立一个神经网络，它能够非常好地建模这个生成过程。从统计学的角度来看，神经模型近似条件概率密度函数，！[](img/8c63f596-488c-4e73-a7aa-7e08c2fa3fac.png)。虽然如今有多种生成模型，但在本书中，我们将重点讨论三种。
- en: First, we will talk about VAEs, which are discussed in the next section. Second, [Chapter
    10](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml), *Restricted Boltzmann Machines*,
    will introduce a graphical approach and its properties (Salakhutdinov, R., et
    al. (2007)). The last approach will be covered in [Chapter 14](7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml), *Generative
    Adversarial Networks.* These networks are changing the way we think about model
    robustness and data generation (Goodfellow, I., *et al.* (2014)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论VAE，它将在下一节中详细介绍。其次，[第10章](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml)，《*受限玻尔兹曼机*》将介绍图形方法及其特性（Salakhutdinov,
    R., 等，2007年）。最后的方法将在[第14章](7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml)，《*生成对抗网络*》中介绍。这些网络正在改变我们对模型鲁棒性和数据生成的思考方式（Goodfellow,
    I., *等*，2014年）。
- en: Examining the VAE model
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检视VAE模型
- en: 'The VAE is a particular type of autoencoder (Kingma, D. P., & Welling, M. (2013)).
    It learns specific statistical properties of the dataset derived from a Bayesian
    approach. First, let''s define [![](img/83389b96-e277-4cf3-8778-7c1450d525ac.png)] as
    the prior probability density function of a random latent variable, [![](img/52cbb884-6c5d-4cfc-9d26-f0eb85c61622.png)].
    Then, we can describe a conditional probability density function, [![](img/a04acbe7-4f47-4fd3-9d94-2a97e8e478b8.png)],
    which can be interpreted as a model that can produce data—say, [![](img/7b2a4e2f-4402-4293-b890-0ab54f1f0694.png)].
    It follows that we can approximate the posterior probability density function
    in terms of the conditional and prior distributions, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: VAE是一种特定类型的自编码器（Kingma, D. P., & Welling, M. (2013)）。它通过贝叶斯方法学习数据集的特定统计属性。首先，定义[![](img/83389b96-e277-4cf3-8778-7c1450d525ac.png)]为随机潜变量的先验概率密度函数，[![](img/52cbb884-6c5d-4cfc-9d26-f0eb85c61622.png)]。接下来，我们可以描述条件概率密度函数[![](img/a04acbe7-4f47-4fd3-9d94-2a97e8e478b8.png)]，它可以解释为一个能够生成数据的模型——比如，[![](img/7b2a4e2f-4402-4293-b890-0ab54f1f0694.png)]。由此我们可以用条件分布和先验分布来近似后验概率密度函数，如下所示：
- en: '![](img/ab9cec77-1f80-41d4-96d0-67a90fd91a22.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab9cec77-1f80-41d4-96d0-67a90fd91a22.png)'
- en: It turns out that an exact posterior is intractable, but this problem can be
    solved, approximately, by making a few assumptions and using an interesting idea
    to compute gradients. To begin with, the prior can be assumed to follow an isotropic
    Gaussian distribution, ![](img/8732332a-8166-4cbc-b4cf-fca9b6f5b0c9.png). We can
    also assume that the conditional distribution, ![](img/dc0906a5-b6fc-4f7e-b632-939b6c8585ca.png), can
    be parametrized and modeled using a neural network; that is, given a latent vector
    ![](img/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png), we use a neural network to
    *generate* ![](img/de19cd96-7891-4682-a488-d8f832e9aeee.png). The weights of the
    network, in this case, are denoted as  [![](img/29d99b0a-5807-4f74-8a55-16b1a1745334.png)],
    and the network would be the equivalent of the *decoder* network. The choice of
    the parametric distribution could be Gaussian, for outputs where ![](img/cac94549-d340-466d-b3d0-6e71f6c823f4.png) can
    take on a wide variety of values, or Bernoulli, if the output ![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png) is
    likely to be binary (or Boolean) values. Next, we must go back again to another
    neural network to approximate the posterior, using [![](img/80b72202-e128-44ce-ad05-ace625e90357.png)] with
    separate parameters, [![](img/160f2e57-8c5d-4a98-9a12-faf7e05313f1.png)]. This
    network can be interpreted as the *encoder* network that takes ![](img/913f83a9-0a47-4629-b4ea-7d16124233b3.png) as
    input and generates the latent variable ![](img/f8da3963-6d86-4faf-90ad-0ce4598b25f8.png).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，精确的后验是不可处理的，但这个问题可以通过做一些假设并使用一个有趣的想法来计算梯度，从而近似求解。首先，假设先验服从各向同性高斯分布，![](img/8732332a-8166-4cbc-b4cf-fca9b6f5b0c9.png)。我们还可以假设条件分布[![](img/dc0906a5-b6fc-4f7e-b632-939b6c8585ca.png)]可以用神经网络进行参数化建模；也就是说，给定潜在向量![](img/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png)，我们用神经网络来*生成* ![](img/de19cd96-7891-4682-a488-d8f832e9aeee.png)。在这种情况下，网络的权重记作[![](img/29d99b0a-5807-4f74-8a55-16b1a1745334.png)]，该网络相当于*解码器*网络。选择的参数化分布可以是高斯分布，用于输出![](img/cac94549-d340-466d-b3d0-6e71f6c823f4.png)可能取值范围广泛的情况，或者是伯努利分布，如果输出![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png)可能是二进制（或布尔）值。接下来，我们必须再次使用另一个神经网络来近似后验，通过[![](img/80b72202-e128-44ce-ad05-ace625e90357.png)]与独立的参数[![](img/160f2e57-8c5d-4a98-9a12-faf7e05313f1.png)]。这个网络可以被解释为*编码器*网络，它接受![](img/913f83a9-0a47-4629-b4ea-7d16124233b3.png)作为输入并生成潜在变量![](img/f8da3963-6d86-4faf-90ad-0ce4598b25f8.png)。
- en: 'Under this assumption, we can define a loss function as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个假设下，我们可以定义一个损失函数，如下所示：
- en: '![](img/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png)'
- en: The full derivation can be followed in Kingma, D. P., & Welling, M. (2013).
    However, in short, we can say that in the first term, ![](img/6c8220c0-ab8a-49f3-8984-c004e5782ac5.png) is
    the Kullback–Leibler divergence function, which aims to measure how different
    the distribution of the prior is, [![](img/110e5ad7-4d3b-42f3-bd5a-c048eefa31b3.png)],
    with respect to the distribution of the posterior, [![](img/68e048a8-5c70-4a2a-b51e-9ded415b10bd.png)].
    This happens in the *encoder*, and we want to make sure that the prior and posterior
    of ![](img/f1af68c2-2332-4405-bd6f-16ec408d80c3.png) are a close match. The second
    term is related to the decoder network, which aims to minimize the reconstruction
    loss based on the negative log likelihood of the conditional distribution, ![](img/84214a05-6ba7-43ae-9981-1c3469e9407a.png),
    with respect to the expectation of the posterior, ![](img/4e736730-4af1-4524-b200-f64746e7e823.png).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的推导可以参照 Kingma, D. P., & Welling, M. (2013) 中的内容。然而，简而言之，我们可以说，在第一个项中， ![](img/6c8220c0-ab8a-49f3-8984-c004e5782ac5.png) 是
    Kullback–Leibler 散度函数，它旨在衡量先验分布 [![](img/110e5ad7-4d3b-42f3-bd5a-c048eefa31b3.png)] 与后验分布 [![](img/68e048a8-5c70-4a2a-b51e-9ded415b10bd.png)] 的差异。这发生在
    *编码器* 中，我们希望确保先验和后验的分布 ![](img/f1af68c2-2332-4405-bd6f-16ec408d80c3.png) 相匹配。第二项与解码器网络相关，旨在基于条件分布 ![](img/84214a05-6ba7-43ae-9981-1c3469e9407a.png) 的负对数似然最小化重构损失，同时参考后验的期望 ![](img/4e736730-4af1-4524-b200-f64746e7e823.png)。
- en: One last trick to make the VAE learn through gradient descent is to use an idea
    called **re-parameterization***.* This trick is necessary because it is impossible
    to encode one sample, ![](img/82260ac8-99f8-4fb4-9f7d-509ac5aefc7c.png), to approximate
    an isotropic Gaussian with 0 mean and some variance, and draw a sample ![](img/79de787c-640f-439e-80f7-bbb776fdf0b1.png)
    from that distribution, pause there, and then go on to decode that and calculate
    the gradients, and then go back and make updates. The re-parametrization trick
    is simply a method for generating samples from [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)],
    while at the same time, it allows gradient calculation. If we say that [![](img/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png)],
    we can express the random variable ![](img/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png) in
    terms of an auxiliary variable, ![](img/d20928b8-47fc-495e-8597-0c476e80bb92.png),
    with marginal probability density function, [![](img/800b5609-ccd9-447d-8da9-add870f8b803.png)],
    such that, [![](img/04fa784b-ebe5-4b9c-9b22-2506e12654a9.png)] and [![](img/c07036e2-7876-4882-a7df-c93fcdd8a6b2.png)] is
    a function parameterized by [![](img/dbeee19b-aca5-4a8f-b802-64dcd61ec13e.png)] and
    returns a vector. This allows gradient calculations on parameters [![](img/6dc7faab-b474-45db-87c2-62cc0e4a321b.png)] (generator
    or decoder) and updates on both [![](img/5635e398-101a-4836-9b24-2b94084c779c.png)] and [![](img/456dbd6d-36b0-42c9-8457-3124c7245a53.png)] using
    any available gradient descent method.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使 VAE 通过梯度下降学习的最后一个技巧是使用一个叫做**重参数化**的概念。这一技巧是必要的，因为无法将一个样本 ![](img/82260ac8-99f8-4fb4-9f7d-509ac5aefc7c.png) 编码为一个均值为
    0 且方差为某值的各向同性高斯分布，并从该分布中抽取样本 ![](img/79de787c-640f-439e-80f7-bbb776fdf0b1.png)，停顿在那里，然后继续解码并计算梯度，最后再返回并进行更新。重参数化技巧实际上是一种从 [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)] 生成样本的方法，同时允许进行梯度计算。如果我们说 [![](img/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png)]，我们可以用一个辅助变量 ![](img/d20928b8-47fc-495e-8597-0c476e80bb92.png)
    来表示随机变量 ![](img/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png)，并且其边际概率密度函数为 [![](img/800b5609-ccd9-447d-8da9-add870f8b803.png)]，使得 [![](img/04fa784b-ebe5-4b9c-9b22-2506e12654a9.png)] 和 [![](img/c07036e2-7876-4882-a7df-c93fcdd8a6b2.png)] 是一个由 [![](img/dbeee19b-aca5-4a8f-b802-64dcd61ec13e.png)] 参数化的函数，并返回一个向量。这使得可以对参数 [![](img/6dc7faab-b474-45db-87c2-62cc0e4a321b.png)] （生成器或解码器）进行梯度计算，并使用任何可用的梯度下降方法对 [![](img/5635e398-101a-4836-9b24-2b94084c779c.png)] 和 [![](img/456dbd6d-36b0-42c9-8457-3124c7245a53.png)] 进行更新。
- en: The *tilde* sign (~) in the [![](img/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png)] equation
    can be interpreted as *follows the distribution of*. Thus, the equation can be
    read as ![](img/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png) follows the distribution
    of the posterior, [![](img/a5b1d977-263d-4bda-ba53-00039ad43c93.png)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中 [![](img/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png)] 的 *tilde* 符号（~）可以解释为
    *服从分布*。因此，该方程可以读作 ![](img/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png) 服从后验分布 [![](img/a5b1d977-263d-4bda-ba53-00039ad43c93.png)]。
- en: '*Figure 9.1* depicts the VAE architecture, explicitly showing the pieces involved
    in the *bottlenecks* and how the pieces of the network are interpreted:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.1* 展示了 VAE 的架构，明确显示了在 *瓶颈* 中涉及的各个部分，以及网络各部分的解释：'
- en: '![](img/c6f80791-6557-4162-a6b7-642b28784a67.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6f80791-6557-4162-a6b7-642b28784a67.png)'
- en: Figure 9.1 – VAE architecture
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – VAE 架构
- en: The preceding figure shows that in an ideal VAE, the parameters of the distributions
    are learned precisely and perfectly to achieve exact reconstruction. However,
    this is just an illustration, and in practice, perfect reconstruction can be difficult
    to achieve.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示表明，在理想的 VAE 中，分布的参数会被准确无误地学习，从而实现精确的重构。然而，这仅仅是一个示意图，实际上，完美重构可能很难实现。
- en: '**Bottlenecks **are the latent representations or parameters that are found
    in neural networks that go from layers with large numbers of neural units to layers
    with a decreasing number of neural units. These bottlenecks are known to produce
    interesting feature spaces (Zhang, Y., *et al.* (2014)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**瓶颈**是神经网络中存在的潜在表示或参数，这些网络从具有大量神经单元的层转向具有较少神经单元的层。这些瓶颈已知能产生有趣的特征空间（Zhang,
    Y., *et al.* (2014)）。'
- en: Now, let's prepare to make our first VAE piece by piece. We will begin by describing
    the dataset we will use.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们准备逐步构建第一个 VAE。我们将从描述我们将使用的数据集开始。
- en: The heart disease dataset revisited
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新审视心脏病数据集
- en: 'In [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data*,
    we described in full the properties of a dataset called the **Cleveland Heart
    Disease** dataset. A screenshot of two columns from this dataset is depicted in
    *Figure 9.2*. Here, we will revisit this dataset with the purpose of reducing
    the original 13 dimensions of the data down to only two dimensions. Not only that,
    but we will also try to produce new data from the generator—that is, the decoder:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)中，*数据准备*，我们详细描述了一个叫做**克利夫兰心脏病**数据集的属性。该数据集的两列截图如*图9.2*所示。在这里，我们将重新审视这个数据集，目的是将原始的13个维度数据降至仅2个维度。不仅如此，我们还将尝试从生成器——即解码器——中生成新数据：
- en: '![](img/6e25e8f1-e950-4fd6-a7ec-9ac75ecdd0cb.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e25e8f1-e950-4fd6-a7ec-9ac75ecdd0cb.png)'
- en: Figure 9.2 – The Cleveland Heart Disease dataset samples on two columns
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 克利夫兰心脏病数据集的两列样本
- en: Our attempts to perform dimensionality reduction can be easily justified by
    looking at *Figures 3.8* and *3.9* in [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml),
    *Preparing Data*, and noticing that the data can possibly be processed so as to
    see whether a neural network can cause the data associated with hearts with no
    disease to cluster separately from the rest. Similarly, we can justify the generation
    of new data given that the dataset itself only contains 303 samples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试进行降维的工作可以通过查看[第3章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)中的*图3.8*和*图3.9*轻松证明，并注意到数据可能会被处理，以查看神经网络是否能将没有心脏病的数据与其他数据分开聚类。同样，我们可以证明生成新数据的合理性，因为数据集本身仅包含303个样本。
- en: 'To download the data, we can simply run the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下载数据，我们可以简单地运行以下代码：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, to load the data into a data frame and separate the training data and
    the targets, we can execute the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了将数据加载到数据框中并分离训练数据和目标，我们可以执行以下代码：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next thing we will need to do is to code the re-parametrization trick so
    that we can sample random noise during training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要做的是编写重参数化技巧的代码，以便在训练过程中能够采样随机噪声。
- en: The re-parametrization trick and sampling
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重参数化技巧和采样
- en: Remember that the re-parametrization trick aims to sample from [![](img/800b5609-ccd9-447d-8da9-add870f8b803.png)] instead
    of [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)]. Also, recall the distribution
    [![](img/65ff4f78-8d25-4b17-a877-3775474be6a7.png)]. This will allow us to make
    the learning algorithm learn the parameters of [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)]—that
    is, [![](img/3ab31204-0e3f-44a5-97b3-e0829a9a29c0.png)]—and we simply produce
    a sample from [![](img/57985506-28ec-4d9a-9bdb-499ecfa47d83.png)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，重参数化技巧的目的是从 [![](img/800b5609-ccd9-447d-8da9-add870f8b803.png)] 而不是 [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)] 进行采样。此外，回想一下分布 [![](img/65ff4f78-8d25-4b17-a877-3775474be6a7.png)]。这将使我们能够让学习算法学习 [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)] 的参数——即 [![](img/3ab31204-0e3f-44a5-97b3-e0829a9a29c0.png)]——我们只需要从 [![](img/57985506-28ec-4d9a-9bdb-499ecfa47d83.png)] 中生成一个样本。
- en: 'To achieve this, we can generate the following method:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们可以生成以下方法：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `sampling()` method receives the mean and log variance of ![](img/c8dc8061-6987-435e-a863-5b8b836b864d.png) (which
    are to be learned), and returns a vector that is sampled from this parametrized
    distribution; ![](img/e1c06fa7-5aa1-40cb-8903-2ba1bd71dcac.png) is just random
    noise from a Gaussian (`random_normal`) distribution with 0 mean and unit variance.
    To make this method fully compatible with mini-batch training, the samples are
    generated according to the size of the mini-batch.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`sampling()` 方法接收 ![](img/c8dc8061-6987-435e-a863-5b8b836b864d.png) 的均值和对数方差（这些需要学习），并返回从这个参数化分布中抽取的样本向量；![](img/e1c06fa7-5aa1-40cb-8903-2ba1bd71dcac.png)
    只是来自高斯（`random_normal`）分布的随机噪声，均值为 0，方差为 1。为了使此方法完全兼容小批量训练，样本将根据小批量的大小生成。'
- en: Learning the posterior's parameters in the encoder
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在编码器中学习后验的参数
- en: The posterior distribution, [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)],
    is intractable by itself, but since we are using the re-parametrization trick,
    we can actually perform sampling based on ![](img/18a8f5d8-cb9e-484d-8786-64bc62d3bb16.png).
    We will now make a simple encoder to learn these parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 后验分布，[![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)]，本身是不可处理的，但由于我们使用了重参数化技巧，我们实际上可以基于
    ![](img/18a8f5d8-cb9e-484d-8786-64bc62d3bb16.png) 进行采样。接下来，我们将创建一个简单的编码器来学习这些参数。
- en: 'For numerical stability, we need to scale the input to make it have 0 mean
    and unit variance. For this, we can invoke the methodology learned in [Chapter
    3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证数值稳定性，我们需要将输入缩放为均值为 0，方差为 1。为此，我们可以调用在 [第 3 章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)《数据准备》中学到的方法：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `x_train` matrix contains the scaled training data. The following variables
    will also be useful for designing the encoder of the VAE:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_train` 矩阵包含经过缩放的训练数据。以下变量对于设计 VAE 的编码器也会非常有用：'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These variables are straightforward, except that the batch size is in terms
    of the square root of the number of samples. This is an empirical value found
    to be a good start, but on larger datasets, it is not guaranteed to be the best.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量比较直观，唯一需要注意的是批次大小是以样本数量的平方根为基准的。这是一个通过经验得出的良好起点，但在较大的数据集上，它并不能保证是最优的。
- en: 'Next, we can build the encoder portion, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以按如下方式构建编码器部分：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This approach of encoder utilizes the `Lambda` class, which is part of the `tensorflow.keras.layers`
    collection. This allows us to use the previously defined `sampling()` method (or
    any arbitrary expression, really) as a layer object. *Figure 9.3* illustrates
    the architecture of the full VAE, including the layers of the encoder described
    in the preceding code block:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码器方法利用了 `Lambda` 类，它是 `tensorflow.keras.layers` 库的一部分。这使得我们能够将之前定义的 `sampling()`
    方法（或任何任意表达式）作为一个层对象来使用。*图 9.3* 展示了完整 VAE 的架构，包括前面代码块中描述的编码器层：
- en: '![](img/c5540457-c546-4297-8aa3-3166a1987073.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5540457-c546-4297-8aa3-3166a1987073.png)'
- en: Figure 9.3 – VAE architecture for the Cleveland Heart Disease dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 适用于克利夫兰心脏病数据集的 VAE 架构
- en: The encoder uses batch normalization, followed by **dropout** at the input layers,
    followed by a dense layer that has **Tanh activation** and **dropout**. From the
    **dropout**, two dense layers are in charge of modeling the parameters of the
    distribution of the latent variable, and a sample is drawn from this parametrized
    distribution. The decoder network is discussed next.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器使用批量归一化，接着是输入层的 **丢弃**，然后是一个包含 **Tanh 激活** 和 **丢弃** 的密集层。通过 **丢弃**，两个密集层负责建模潜在变量分布的参数，并从这个参数化分布中抽取样本。接下来将讨论解码器网络。
- en: Modeling the decoder
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模解码器
- en: 'The decoder portion of the VAE is very much standard with respect to what you
    already know about an autoencoder. The decoder takes the latent variable, which
    in the VAE was produced by a parametric distribution, and then it should reconstruct
    the input exactly. The decoder can be specified as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 的解码器部分在你已知的自动编码器中是非常标准的。解码器接受潜在变量，该变量在 VAE 中由参数化分布生成，然后它应该精确地重建输入。解码器可以按如下方式指定：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding code, we simply connect two dense layers—the first contains
    a ReLU activation, while the second has linear activations in order to map back
    to the input space. Finally, we can define the complete VAE in terms of inputs
    and outputs as defined in the encoder and decoder:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们简单地连接了两个密集层——第一个层包含 ReLU 激活，而第二个层则具有线性激活，以便映射回输入空间。最后，我们可以根据编码器和解码器中定义的输入和输出来定义完整的
    VAE：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The VAE model is completed as described here and depicted in *Figure 9.3*. The
    next steps leading to the training of this model include the definition of a loss
    function, which we will discuss next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如此处所述并在*图9.3*中所示，VAE模型已经完成。接下来将进入训练该模型的步骤，其中包括定义损失函数，我们将在后面讨论。
- en: Minimizing the loss
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化损失
- en: 'We explained earlier that the loss function needs to be in terms of the encoder
    and decoder; this is the equation we discussed:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前解释过，损失函数需要以编码器和解码器为基础；这是我们讨论过的方程：
- en: '![](img/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png)'
- en: 'If we want to code this loss, we will need to code it in terms of something
    more practical. Applying all the previous assumptions made on the problem, including
    the re-parametrization trick, allows us to rewrite an approximation of the loss
    in simpler terms, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想编写这个损失函数的代码，我们需要用更实际的方式来表示它。应用之前在问题中做出的所有假设，包括重新参数化技巧，可以让我们将损失的近似值用更简单的方式重新编写，如下所示：
- en: '![](img/8bddf69c-11f1-415d-be2a-c1766f4a885e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bddf69c-11f1-415d-be2a-c1766f4a885e.png)'
- en: This is for all samples of ![](img/71d43944-64da-4574-b03d-25570e0ecc86.png) where [![](img/445b73e4-f1f8-49d6-953f-301b73f27395.png)] and
    [![](img/ce34c93c-41c6-4549-99f3-1c8eb8a3152c.png)]. Furthermore, the decoder
    loss portion can be approximated using any of your favorite reconstruction losses—for
    example, the **mean squared error** (**MSE**) loss or the binary cross-entropy
    loss. It has been proven that minimizing any of these losses will also minimize
    the posterior.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于所有样本的 ![](img/71d43944-64da-4574-b03d-25570e0ecc86.png)，其中 [![](img/445b73e4-f1f8-49d6-953f-301b73f27395.png)] 和
    [![](img/ce34c93c-41c6-4549-99f3-1c8eb8a3152c.png)]。此外，解码器损失部分可以使用你喜欢的任何重建损失来近似——例如**均方误差**（**MSE**）损失或二元交叉熵损失。已证明，最小化这些损失中的任何一个也会最小化后验。
- en: 'We can define the reconstruction loss in terms of the MSE, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将重建损失定义为MSE，如下所示：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Alternatively, we could do so with the binary cross-entropy loss, like so:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以用二元交叉熵损失来表示，如下所示：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'One additional thing we could do, which is optional, is to monitor how important
    the reconstruction loss is in comparison to the KL-divergence loss (the term related
    to the encoder). One typical thing to do is to make the reconstruction loss be
    multiplied by either the latent dimension or by the input dimension. This effectively
    makes the loss larger by that factor. If we do the latter, we can penalize the
    reconstruction loss, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的一件额外的事情（这是可选的），是监控重建损失与KL散度损失（与编码器相关的项）之间的重要性。通常的做法是将重建损失乘以潜在维度或输入维度。这样做实际上会使损失增大。若选择后者，我们可以像下面这样对重建损失进行惩罚：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The KL-divergence loss for the encoder term can now be expressed in terms of
    the mean and variance, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编码器项的KL散度损失可以用均值和方差来表示，如下所示：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Therefore, we can simply add the overall loss to the model, which becomes the
    following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以简单地将整体损失添加到模型中，模型变为如下形式：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this, we are good to go ahead and compile the model and train it, as explained
    next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就可以继续编译模型并进行训练，接下来将做详细说明。
- en: Training a VAE
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练VAE
- en: The finishing touch is to compile the VAE model, which will put all the pieces
    together. During the compilation, we will choose an optimizer (the gradient descent
    method). In this case, we will choose *Adam* (Kingma, D. P., *et al.* (2014)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是编译VAE模型，这将把所有部分结合在一起。在编译过程中，我们需要选择一个优化器（梯度下降法）。在这种情况下，我们选择*Adam*（Kingma,
    D. P.，*et al.*（2014））。
- en: 'Fun fact: the creator of the VAE is the same person who soon after created
    Adam. His name is **Diederik P. Kingma** and he is currently a research scientist
    at Google Brain.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，VAE的创始人是同一个人在不久之后创造了Adam优化器。他的名字是**Diederik P. Kingma**，目前是Google Brain的研究科学家。
- en: 'To compile the model and choose the optimizer, we do the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编译模型并选择优化器，我们需要执行以下操作：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we train with training data for 500 epochs, using a batch size of
    18, like so:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用训练数据进行500次epoch的训练，批大小为18，代码如下：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Notice that we are using the training set as the validation set. This is not
    recommended in most settings, but here, it works because the chance of selecting
    identical mini-batches for training and validation is very low. Furthermore, it
    would usually be considered cheating to do that; however, the latent representation
    used in the reconstruction does not directly come from the input; rather, it comes
    from a distribution similar to the input data. To demonstrate that the training
    and validation sets yield different results, we plot the training progress across
    epochs, as shown in *Figure 9.4*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用训练集作为验证集。在大多数情况下，这样做是不推荐的，但在这里这样做是可行的，因为选择相同的mini-batch进行训练和验证的概率非常低。此外，通常来说，这样做被认为是作弊；然而，用于重构的潜在表示并不直接来自输入，而是来自与输入数据相似的分布。为了展示训练集和验证集会产生不同的结果，我们绘制了跨越各个epoch的训练进度，如*图
    9.4*所示：
- en: '![](img/5bced4db-4f13-4517-8210-33d5982e7a4a.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bced4db-4f13-4517-8210-33d5982e7a4a.png)'
- en: Figure 9.4 – VAE training performance across epochs
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – VAE在各个epoch中的训练表现
- en: The preceding figure not only indicates that the model converges quickly, but
    it also shows that the model does not overfit on the input data. This is a nice
    property to have, usually.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上图不仅表明模型收敛速度很快，而且还显示模型不会对输入数据进行过拟合。通常来说，这是一个很好的特性。
- en: '*Figure 9.4* can be produced with the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.4*可以通过以下代码生成：'
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: However, note that the results may vary due to the unsupervised nature of the
    VAE.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，由于VAE是无监督的，因此结果可能会有所不同。
- en: 'Next, if we take a look at the latent representation that is produced by sampling
    from a random distribution using the parameters that were learned during training,
    we can see what the data looks like. *Figure 9.5* depicts the latent representations
    obtained:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们观察通过从随机分布中采样并利用训练过程中学到的参数生成的潜在表示，我们就能看到数据的样子。*图 9.5*展示了获得的潜在表示：
- en: '![](img/e59a5b95-6a1a-4f10-8cc4-6f568df25c1c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e59a5b95-6a1a-4f10-8cc4-6f568df25c1c.png)'
- en: Figure 9.5 – VAE's sampled latent representation in two dimensions
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – VAE在二维空间中采样的潜在表示
- en: 'The figure clearly suggests that the data corresponding to no indication of
    heart disease is clustered on the left quadrants, while the samples corresponding
    to heart disease are clustered on the right quadrant of the latent space. The
    histogram shown on the top of the figure suggests the presence of two well-defined
    clusters. This is great! Also, recall that the VAE does not know anything about
    labels: we can''t stress this enough! Compare *Figure 9.5* here with *Figure 3.9*
    in [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data*,
    and you will notice that the performance of the VAE is superior to KPCA. Furthermore,
    compare this figure with *Figure 3.8* in [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data,*
    and notice that the performance of the VAE is comparable (if not better) than
    **Linear Discriminant Analysis** (**LDA**), which uses label information to produce
    low-dimensional representations. In other words, LDA cheats a little bit.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图中清晰地显示，未指示心脏病的数据聚集在左侧象限，而对应心脏病的样本聚集在潜在空间的右侧象限。图顶端显示的直方图表明存在两个明确的聚类。这很好！另外，请记住，VAE并不知道任何标签信息：我们无法过分强调这一点！将这里的*图
    9.5*与[第三章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)中的*图 3.9*进行比较，*数据预处理*，你会注意到VAE的表现优于KPCA。此外，将此图与[第三章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)中的*图
    3.8*进行比较，*数据预处理*，并注意到VAE的表现与**线性判别分析**(**LDA**)相当（如果不是更好）。LDA利用标签信息生成低维表示。换句话说，LDA有点“作弊”。
- en: One of the most interesting properties of the VAE is that we can generate data;
    let's go ahead and see how this is done.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: VAE最有趣的特性之一是我们可以生成数据；接下来，让我们看看是如何做到这一点的。
- en: Generating data from the VAE
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从VAE生成数据
- en: Since the VAE learns the parameters of a parametric distribution over the latent
    space, which is sampled to reconstruct the input data back, we can use those parameters
    to draw more samples and reconstruct them. The idea is to generate data for whatever
    purposes we have in mind.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于VAE学习潜在空间上参数化分布的参数，并利用这些参数采样来重构输入数据，因此我们可以利用这些参数生成更多的样本并进行重构。这个想法是根据我们的需求生成数据。
- en: 'Let''s start by encoding the original dataset and see how close the reconstruction is to
    the original. Then, generating data should be straightforward. To encode the input
    data into the latent space and decode it, we do the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从编码原始数据集开始，看看重建的结果与原始数据有多接近。然后，生成数据应该是直观的。为了将输入数据编码到潜在空间并解码，我们可以做如下操作：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Recall that `x_train` is ![](img/b0c5b964-8583-4007-8807-e18c4b9522ef.png) and
    `x_hat` is the reconstruction, ![](img/6476b534-a425-4523-a577-baeeb7b34df3.png).
    Notice that we are using `encdd[0]` as the input for the decoder. The reason for
    this is that the encoder yields a list of three vectors, `[z_mean, z_log_var,
    z]`. Therefore, to use the 0 element in the list is to refer to the mean of the
    distribution corresponding to the samples. In fact, `encdd[0][10]` would yield
    a two-dimensional vector corresponding to the mean parameter of the distribution
    that can produce the 10^(th) sample in the dataset—that is, `x_train[10]`. If
    you think about it, the mean could be the best latent representation that we can
    find since it would be the most likely to reconstruct the input in the decoder.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`x_train`是！[](img/b0c5b964-8583-4007-8807-e18c4b9522ef.png)，而`x_hat`是重建结果，！[](img/6476b534-a425-4523-a577-baeeb7b34df3.png)。注意，我们使用`encdd[0]`作为解码器的输入。之所以这样做，是因为编码器输出的是三个向量的列表，`[z_mean,
    z_log_var, z]`。因此，使用列表中的第0个元素即表示对应样本的分布均值。实际上，`encdd[0][10]`会得到一个二维向量，对应于能够生成数据集中第10个样本的分布均值——也就是`x_train[10]`。如果你仔细想一下，均值可能是我们能够找到的最佳潜在表示，因为它最有可能在解码器中重建输入。
- en: 'With this in mind, we can take a look at how good the reconstruction is by
    running something like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，我们可以通过运行类似的代码来查看重建的效果有多好：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If the output shows scientific notation that is difficult to read, try disabling
    it temporarily like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出显示的是难以阅读的科学计数法，可以尝试暂时禁用它，如下所示：
- en: '`import numpy as np`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`import numpy as np`'
- en: '`np.set_printoptions(suppress=True)`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.set_printoptions(suppress=True)`'
- en: '`print(np.around(scaler.inverse_transform(x_train[0]), decimals=1))`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`print(np.around(scaler.inverse_transform(x_train[0]), decimals=1))`'
- en: '`print(np.around(scaler.inverse_transform(x_hat[0]), decimals=1))`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`print(np.around(scaler.inverse_transform(x_hat[0]), decimals=1))`'
- en: '`np.set_printoptions(suppress=False)`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.set_printoptions(suppress=False)`'
- en: In this example, we are focusing on the first data point in the training set—that
    is, `x_train[0]`—in the top row; its reconstruction is the bottom row. Close examination
    reveals that there are differences between both; however, these differences might
    be relatively small in terms of the MSE.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们专注于训练集中的第一个数据点，也就是`x_train[0]`，位于顶部行；它的重建结果位于底部行。仔细检查可以发现两者之间有差异；然而，这些差异可能在均方误差（MSE）方面相对较小。
- en: Another important aspect to point out here is that the data needs to be scaled
    back to its original input space since it was scaled prior to its use in training
    the model. Fortunately, the `StandardScaler()` class has an `inverse_transform()`
    method that can help in mapping any reconstruction back to the range of values
    of each dimension in input space.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要指出的重要方面是，数据在训练模型之前已被缩放，因此需要将其缩放回原始输入空间。幸运的是，`StandardScaler()`类有一个`inverse_transform()`方法，可以帮助将任何重建数据映射回输入空间中每个维度的值范围。
- en: 'In order to generate more data at will, we can define a method to do so. The
    following method produces random noise, uniformly, in the range `[-2, +2]`, which
    comes from an examination of *Figure 9.5*, which shows the range of the latent
    space to be within such range:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了随意生成更多数据，我们可以定义一个方法来实现。以下方法会产生均匀分布的随机噪声，范围为`[-2, +2]`，这个范围来自于对*图9.5*的分析，该图显示了潜在空间的范围在这个区间内：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This function would need to be adjusted according to the range of values in
    the latent space; also, it can be adjusted by looking at the distribution of the
    data in the latent space. For example, if the latent space seems to be normally
    distributed, then a normal distribution can be used like this: `noise = np.random.normal(0.0,
    1.0, (N,latent_dim))`, assuming 0 mean and unit variance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数需要根据潜在空间中值的范围进行调整；同时，也可以通过观察潜在空间中数据的分布来调整。例如，如果潜在空间似乎是正态分布的，那么可以像这样使用正态分布：`noise
    = np.random.normal(0.0, 1.0, (N,latent_dim))`，假设均值为0，方差为1。
- en: 'We can call the function to generate *fake* data by doing the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式调用函数生成*伪*数据：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This gives the following output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Recall that this data is generated from random noise. You can see how this is
    a major breakthrough in the deep learning community. You can use this data to
    augment your dataset and produce as many samples as you wish. We can look at the
    quality of the generated samples and decide for ourselves whether the quality
    is good enough for our purposes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，这些数据是由随机噪声生成的。你可以看到，这是深度学习领域的一个重大突破。你可以使用这些数据来增强你的数据集，并根据需要生成任意数量的样本。我们可以查看生成样本的质量，自己判断其是否足够满足我们的需求。
- en: Now, granted that since you and I may not be medical doctors specializing in
    heart disease, we might not be qualified to determine with certainty that the
    data generated makes sense; but if we did this correctly, it generally does make
    sense. To make this clear, the next section uses MNIST images to prove that the
    generated samples are good since we can all make a visual assessment of numeral
    images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到你和我可能不是专门从事心脏病研究的医生，我们可能没有资格确切判断生成的数据是否合理；但如果我们正确执行了操作，通常来说，它确实是合理的。为了明确这一点，接下来的部分将使用
    MNIST 图像来证明生成的样本是好的，因为我们都可以对数字图像进行视觉评估。
- en: Comparing a deep and shallow VAE on MNIST
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 MNIST 上的深层和浅层 VAE
- en: 'Comparing shallow and deep models is part of the experimentation process that
    leads to finding the best models. In this comparison over MNIST images, we will
    be implementing the architecture shown in *Figure 9.6* as the shallow model, while
    the deep model architecture is shown in *Figure 9.7*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 比较浅层和深层模型是实验过程的一部分，目的是找到最佳模型。在这个关于 MNIST 图像的比较中，我们将实现 *图 9.6* 中显示的架构作为浅层模型，而深层模型的架构如
    *图 9.7* 所示：
- en: '![](img/066f2ead-8aeb-434c-b6fc-e6b5081790be.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/066f2ead-8aeb-434c-b6fc-e6b5081790be.png)'
- en: Figure 9.6 – VAE shallow architecture over MNIST
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – MNIST 上的 VAE 浅层架构
- en: 'As you can appreciate, both models are substantially different when it comes
    to the number of layers involved in each one. The quality of the reconstruction
    will be different as a consequence:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这两个模型在涉及的层数上有很大不同。由于这一差异，重建的质量也会有所不同：
- en: '![](img/2b345ff5-b67c-4f2d-977d-1dd9834d5b0d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b345ff5-b67c-4f2d-977d-1dd9834d5b0d.png)'
- en: Figure 9.7 – VAE deep architecture over MNIST
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – MNIST 上的 VAE 深度架构
- en: These models will be trained using a small number of epochs for the shallow
    VAE and a much larger number of epochs for the deeper model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型将使用少量的 epoch 来训练浅层 VAE，而更深的模型将使用更多的 epoch。
- en: The code to reproduce the shallow encoder can be easily inferred from the example
    used in the Cleveland Heart Disease dataset; however, the code for the deep VAE
    will be discussed in the sections that follow.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 重现浅层编码器的代码可以从在克利夫兰心脏病数据集中使用的示例中轻松推断出来；然而，深层 VAE 的代码将在接下来的章节中讨论。
- en: Shallow VAE
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浅层 VAE
- en: 'One of the first things that we can use to compare the VAE is its learned representations.
    *Figure 9.8* depicts the latent space projections of the training set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来比较 VAE 的第一件事之一是其学习到的表示。*图 9.8* 显示了训练集的潜在空间投影：
- en: '![](img/5234c729-7ee3-4c7c-82d2-2a97581675a7.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5234c729-7ee3-4c7c-82d2-2a97581675a7.png)'
- en: Figure 9.8 – Shallow VAE latent space projections of the training dataset
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 浅层 VAE 潜在空间投影（训练数据集）
- en: From the preceding figure, we can observe clusters of data points that are spreading
    out from the center coordinates. What we would like to see are well-defined clusters
    that are ideally separated enough so as to facilitate classification, for example.
    In this case, we see a little bit of overlap among certain groups, particularly
    number 4 and number 9, which makes a lot of sense.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以观察到数据点的聚类从中心坐标向外扩展。我们希望看到的是清晰定义的聚类，这些聚类之间的分隔足够清晰，以便于分类。例如，在这种情况下，我们看到某些组之间有一点重叠，特别是
    4 和 9 号组，这其实是有道理的。
- en: 'The next thing to look into is the reconstruction ability of the models. *Figure
    9.9* shows a sample of the input, and *Figure 9.10* shows the corresponding reconstructions
    after the model has been trained:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来需要关注的是模型的重建能力。*图 9.9* 显示了输入的示例，*图 9.10* 显示了模型训练后的相应重建结果：
- en: '![](img/9a20024e-4abf-44aa-ac42-9314e2d61afb.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a20024e-4abf-44aa-ac42-9314e2d61afb.png)'
- en: Figure 9.9 – Sample input to the VAE
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – VAE 的示例输入
- en: 'The expectation for the shallow model is to perform in a manner that is directly
    related to the size of the model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于浅层模型的预期是，其表现将直接与模型的大小相关：
- en: '![](img/9de3f378-6e48-4e41-b4e2-7d575ea4d98f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9de3f378-6e48-4e41-b4e2-7d575ea4d98f.png)'
- en: Figure 9.10 – Shallow VAE reconstructions with respect to the input in Figure
    9.9
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 浅层 VAE 重建与图 9.9 中输入的关系
- en: Clearly, there seem to be some issues with the reconstruction of the number
    2 and number 8, which is confirmed by observing the great deal of overlap shown
    between these two numerals in *Figure 9.8*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，数字 2 和数字 8 的重建似乎存在一些问题，这一点通过观察*图 9.8*中这两个数字之间的重叠可以得到确认。
- en: 'Another thing we can do is to visualize the data generated by the VAE if we
    draw numbers from the range of the latent space. *Figure 9.11* shows the latent
    space as it changes across the two dimensions:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以做的事情是，如果我们从潜在空间的范围中绘制数字，来可视化 VAE 生成的数据。*图 9.11*展示了潜在空间在两个维度上的变化：
- en: '![](img/b394a3ad-b012-4f4e-bd26-5ee5448e1c6a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b394a3ad-b012-4f4e-bd26-5ee5448e1c6a.png)'
- en: Figure 9.11 – Shallow VAE latent space exploration in the range [-4,4] in both
    dimensions
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 浅层 VAE 在范围 [-4,4] 内的潜在空间探索
- en: What we find really interesting in *Figure 9.11* is that we can see how numerals
    are transformed into others progressively as the latent space is traversed. If
    we take the center line going from top to bottom, we can see how we can go from
    the number 0 to the number 6, then to the number 2, then to the number 8, down
    to the number 1\. We could do the same by tracing a diagonal path, or other directions.
    Making this type of visualization also allows us to see some artifacts that were
    not seen in the training dataset that could cause potential problems if we generate
    data without care.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.11*中，我们发现非常有趣的一点是，我们可以看到随着潜在空间的遍历，数字是如何逐步转换成其他数字的。如果我们沿着从上到下的中线，我们可以看到数字如何从
    0 转变为 6，然后是 2，再到 8，最后到 1。我们也可以通过沿着对角线或其他方向进行同样的操作。制作这种类型的可视化还允许我们看到一些在训练数据集中没有看到的伪影，这些伪影可能会在我们生成数据时造成潜在问题。
- en: To see whether the deeper model is any better than this, we will implement it
    in the next section.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看更深层的模型是否比这个更好，我们将在下一节中实现它。
- en: Deep VAE
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深层 VAE
- en: '*Figure 9.7* depicts a deep VAE architecture that can be implemented in parts—first
    the encoder, and then the decoder.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.7*展示了一种可以分部分实现的深层 VAE 架构——首先是编码器，然后是解码器。'
- en: Encoder
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The encoder can be implemented using the functional paradigm, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器可以使用函数式范式实现，如下所示：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, `inpt_dim` corresponds to the 784 dimensions of a 28*28 MNIST image.
    Continuing with the rest, we have the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`inpt_dim`对应于 28*28 的 MNIST 图像的 784 个维度。接下来，继续以下内容：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the encoder model uses dropout layers with a 10% dropout rate. The
    rest of the layers are all things we have seen before, including batch normalization.
    The only new thing here is the `Lambda` function, which is exactly as defined
    earlier in this chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，编码器模型使用了 10% 丢弃率的丢弃层。其他层都是我们之前见过的内容，包括批量归一化。唯一的新内容是`Lambda`函数，正如本章前面所定义的。
- en: Next, we will define the decoder.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义解码器。
- en: Decoder
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder is a few layers shorter than the encoder. This choice of layers
    is simply to show that as long as the number of dense layers is almost equivalent
    in the encoder and decoder, some of the other layers can be omitted as part of
    the experiment to look for performance boosts.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器比编码器少了几层。这种层的选择只是为了展示，只要编码器和解码器中的稠密层数量几乎相等，就可以省略一些其他层，作为实验的一部分，寻找性能提升。
- en: 'This is the design of the decoder:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解码器的设计：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once again, there is nothing new here that we have not seen before. It is all
    layers after more layers. Then, we can put all this together in the model, as
    follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这里没有什么新内容，是层与层之间的叠加。然后，我们可以将所有这些组合在一起形成模型，如下所示：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: That is it! After this, we can compile the model, choose our optimizer, and
    train the model in the exact same way that we did in earlier sections.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在此之后，我们可以编译模型，选择优化器，并像之前的节那样训练模型。
- en: 'If we want to visualize the latent space of the deep VAE in order to compare
    it with *Figure 9.8*, we could look at the space shown in *Figure 9.12*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要可视化深层 VAE 的潜在空间，以便与*图 9.8*进行比较，我们可以查看*图 9.12*中展示的空间：
- en: '![](img/243634d2-d778-484e-8dac-ae717e021a0e.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/243634d2-d778-484e-8dac-ae717e021a0e.png)'
- en: Figure 9.12 – Deep VAE latent space projections of the training dataset
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – 深层 VAE 训练数据集的潜在空间投影
- en: As you can see, the latent space appears to be different in the way the geometry
    of it looks. This is most likely the effect of activation functions delimiting
    the latent space range for specific manifolds. One of the most interesting things
    to observe is the separation of groups of samples even if there still exists some
    overlap–for example, with numerals 9 and 4\. However, the overlaps in this case
    are less severe in comparison to *Figure 9.8*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，潜在空间在几何形态上看起来有所不同。这很可能是激活函数作用的结果，它们限制了特定流形的潜在空间范围。最有趣的现象之一是，即使存在一些重叠——例如，数字
    9 和 4，样本组之间的分离仍然很明显。然而，与*图 9.8*相比，这种重叠程度较轻。
- en: '*Figure 9.13* shows the reconstruction of the same input shown in *Figure 9.9*,
    but now using the deeper VAE:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.13* 展示了与*图 9.9*中的相同输入的重建效果，但这次使用的是更深层的 VAE：'
- en: '![](img/c3c397cf-bc79-42f8-b882-7ffecf4360b5.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3c397cf-bc79-42f8-b882-7ffecf4360b5.png)'
- en: Figure 9.13 – Deep VAE reconstructions with respect to the input in Figure 9.9\.
    Compare to Figure 9.10
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 深层 VAE 重建与图 9.9 中的输入相对应。与图 9.10 进行比较
- en: 'Clearly, the reconstruction is much more better and robust in comparison to
    the shallow VAE. To make things even clearer, we can also explore the generator
    by traversing the latent space by producing random noise in the same range as
    the latent space. This is shown in *Figure 9.14*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，与浅层 VAE 相比，重建效果要好得多且更具鲁棒性。为了更清楚地展示这一点，我们还可以通过在潜在空间中生成与潜在空间相同范围的随机噪声来探索生成器。这在*图
    9.14*中得到了展示：
- en: '![](img/f32052b8-c121-446c-9efe-212cf3ccc400.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f32052b8-c121-446c-9efe-212cf3ccc400.png)'
- en: Figure 9.14 – Deep VAE latent space exploration in the range [-4,4] in both
    dimensions. Compare to Figure 9.11
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 深度 VAE 潜在空间在范围 [-4,4] 内的探索，涵盖两个维度。与图 9.11 进行比较
- en: The latent space of the deeper VAE clearly seems to be richer in diversity and
    more interesting from a visual perspective. If we pick the rightmost line and
    traverse the space from bottom to top, we can see how numeral 1 becomes a 7 and
    then a 4 with smaller and progressive changes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 深层 VAE 的潜在空间显然在多样性上更为丰富，并且从视觉角度来看更加有趣。如果我们选择最右侧的那一条线，并从下到上遍历空间，我们可以看到数字 1 变成
    7，再变成 4，且变化是逐渐且小幅的。
- en: Denoising VAEs
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪 VAE
- en: VAEs are also known to be good in image denoising applications (Im, D. I. J.,
    *et al.* (2017)). This property is achieved by injecting noise as part of the
    learning process. To find out more about this, you can search the web for denoising
    VAEs and you will find resources on that particular topic. We just want you to
    be aware of them and know that they exist if you need them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 在图像去噪应用中也表现得非常出色（Im, D. I. J., *等*（2017））。这一特性是通过在学习过程中注入噪声来实现的。若想了解更多信息，你可以在网上搜索去噪
    VAE，你会找到关于这个话题的资料。我们只是希望你了解它们的存在，并知道它们在你需要时可以使用。
- en: Thinking about the ethical implications of generative models
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考生成模型的伦理影响
- en: 'Generative models are one of the most exciting topics in deep learning nowadays.
    But with great power comes great responsibility. We can use the power of generative
    models for many good things, such as the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是当前深度学习领域最激动人心的话题之一。但强大的能力伴随着巨大的责任。我们可以利用生成模型的力量做许多有益的事情，例如：
- en: Augmenting your dataset to make it more complete
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展你的数据集，使其更加完整
- en: Training your model with unseen data to make it more stable
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用未见过的数据训练模型，使其更加稳定
- en: Finding adversarial examples to re-train your model and make it more robust
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到对抗样本来重新训练你的模型，使其更具鲁棒性
- en: Creating new images of things that look like other things, such as images of
    art or vehicles
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建看起来像其他事物的全新图像，例如艺术作品或车辆的图像
- en: Creating new sequences of sounds that sound like other sounds, such as people
    speaking or birds singing
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建听起来像其他声音的新声音序列，例如人类说话或鸟类鸣叫
- en: Generating new security codes for data encryption
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为数据加密生成新的安全代码
- en: We can go on as our imagination permits. What we must always remember is that
    these generative models, if not modeled properly, can lead to many problems, such
    as bias, causing trustworthiness issues on your models. It can be easy to use
    these models to generate a fake sequence of audio of a person saying something
    that they did not really say, or producing an image of the face of a person doing
    something they did not really do, or with a body that does not belong to the face.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据想象力的限制继续前进。我们必须时刻记住的是，这些生成模型如果没有正确建模，可能会导致许多问题，比如偏见，从而引发模型的可信度问题。使用这些模型生成某人说过的话，但实际上他们并没有说过的话的虚假音频序列，或者生成某人做过的事情但其实并未发生过的面孔图像，甚至是将不属于某人面孔的身体合成进去，这些都非常容易做到。
- en: Some of the most notable wrongdoings include deepfakes. It is not worth our
    time going through the ways of achieving such a thing, but suffice to say, our
    generative models should not be used for malicious purposes. Soon enough, international
    law will be established to punish those who commit crimes through malicious generative
    modeling.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最显著的不当行为包括深度伪造。我们无需浪费时间讨论如何实现这一点，但可以明确的是，我们的生成模型不应被用于恶意目的。不久之后，国际法将会出台，惩罚那些通过恶意生成建模犯罪的人。
- en: 'But until international laws are set and countries adopt new policies, you
    must follow the best practices when developing your models:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 但在国际法律尚未出台、各国尚未制定新政策之前，你在开发模型时必须遵循最佳实践：
- en: 'Test your models for the most common types of bias: historical, societal, algorithmic,
    and so on (Mehrabi, N., *et al.* (2019)).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试你的模型是否存在最常见的偏见类型：历史偏见、社会偏见、算法偏见等（Mehrabi, N., *et al.* (2019)）。
- en: Train your models using reasonable training and test sets.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用合理的训练集和测试集来训练你的模型。
- en: Be mindful of data preprocessing techniques; see [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml),
    *Preparing Data*, for more details.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留意数据预处理技术；更多细节请参见[第3章](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml)，*数据准备*。
- en: Make sure your models produce output that always respects the dignity and worth
    of all human beings.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你的模型产生的输出始终尊重所有人类的尊严和价值。
- en: Have your model architectures validated by a peer.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让同行验证你的模型架构。
- en: 'With this in mind, go on and be as responsible and creative as you can with
    this new tool that you now have at your disposal: VAEs.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，继续以负责任且富有创意的方式使用你现在拥有的新工具：VAE。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This advanced chapter has shown you one of the most interesting and simpler
    models that is able to generate data from a learned distribution using the configuration
    of an autoencoder and by applying variational Bayes principles leading to a VAE.
    We looked at the pieces of the model itself and explained them in terms of input
    data from the Cleveland dataset. Then, we generated data from the learned parametric
    distribution, showing that VAEs can easily be used for this purpose. To prove
    the robustness of VAEs on shallow and deep configurations, we implemented a model
    over the MNIST dataset. The experiment proved that deeper architectures produce
    well-defined regions of data distributions as opposed to fuzzy groups in shallow
    architectures; however, both shallow and deep models are particularly good for
    the task of learning representations.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本高级章节展示了一个非常有趣且相对简单的模型，能够通过配置自编码器并应用变分贝叶斯原理，从已学习的分布中生成数据，最终形成变分自编码器（VAE）。我们分析了该模型的各个部分，并通过克利夫兰数据集的输入数据来解释它们。随后，我们从学习到的参数化分布中生成了数据，证明了VAE可以轻松用于此目的。为了验证VAE在浅层和深层配置中的鲁棒性，我们在MNIST数据集上实现了一个模型。实验证明，深层架构能产生更加明确的数据分布区域，而浅层架构则产生模糊的分布区域；然而，无论是浅层模型还是深层模型，都在学习表示任务中表现出色。
- en: By this point, you should feel confident in identifying the pieces of a VAE
    and being able to tell the main differences between a traditional autoencoder
    and a VAE in terms of its motivation, architecture, and capabilities. You should
    appreciate the generative power of VAEs and feel ready to implement them. After
    reading this chapter, you should be able to code both basic and deep VAE models
    and be able to use them for dimensionality reduction and data visualization, as
    well as to generate data while being mindful of the potential risks. Finally,
    you should now be familiarized with the usage of the `Lambda` functions for general-purpose
    use in TensorFlow and Keras.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经能够自信地识别VAE的各个组成部分，并能够在其动机、架构和能力方面区分传统自编码器和VAE的主要区别。你应该能够欣赏VAE的生成能力，并且准备好实现它们。在阅读完本章之后，你应该能够编写基本的和深度的VAE模型，并能够使用它们进行降维和数据可视化，以及在意识到潜在风险的情况下生成数据。最后，你现在应该已经熟悉了如何在TensorFlow和Keras中使用
    `Lambda` 函数进行通用操作。
- en: If you have liked learning about unsupervised models so far, stay with me and
    continue to [Chapter 10](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml), *Restricted
    Boltzmann Machines,* which will present a unique model that is rooted in what
    is known as graphical models. Graphical models use graph theory mixed with learning
    theory to perform machine learning. An interesting aspect of restricted Boltzmann
    machines is that the algorithm can go forward and backward during learning to
    satisfy connection constraints. Stay tuned!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你到目前为止喜欢学习无监督模型，请继续跟我一起阅读 [第10章](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml)，*限制玻尔兹曼机*，本章将介绍一个独特的模型，它基于图形模型。图形模型将图论与学习理论结合，用于执行机器学习。限制玻尔兹曼机的一个有趣之处是，在学习过程中算法可以前进和后退，以满足连接约束。敬请期待！
- en: Questions and answers
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**How is data generation possible from random noise?**'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如何从随机噪声中生成数据？**'
- en: Since the VAE learns the parameters of a parametric random distribution, we
    can simply use those parameters to sample from such a distribution. Since random
    noise usually follows a normal distribution with certain parameters, we can say
    that we are sampling random noise. The nice thing is that the decoder knows what
    to do with the noise that follows a particular distribution.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于VAE学习的是一个参数化的随机分布的参数，我们可以简单地利用这些参数从该分布中进行采样。由于随机噪声通常遵循具有特定参数的正态分布，因此我们可以说我们正在对随机噪声进行采样。值得一提的是，解码器知道如何处理符合特定分布的噪声。
- en: '**What is the advantage of having a deeper VAE? **'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更深层次的VAE有什么优势？**'
- en: It is hard to say what the advantage is (if there is any) without having the
    data or knowing the application. For the Cleveland Heart Disease dataset, for
    example, a deeper VAE might not be necessary; while for MNIST or CIFAR, a moderately
    large model might be beneficial. It depends.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 很难说出其优势是什么（如果有的话），因为没有数据或不知道应用场景。例如，对于克利夫兰心脏病数据集，深层VAE可能不必要；而对于MNIST或CIFAR，适度较大的模型可能有益。视情况而定。
- en: '**Is there a way to make changes to the loss function?**'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**有没有办法修改损失函数？**'
- en: Of course, you can change the loss function, but be careful to preserve the
    principles on which it is constructed. Let's say that a year from now we found
    a simpler way of minimizing the negative log likelihood function, then we could
    (and should) come back and edit the loss to adopt the new ideas.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以更改损失函数，但要小心保持其构建原理。如果假设一年后我们找到了简化负对数似然函数的方式，那么我们可以（且应该）回过头来编辑损失函数，以采用新的方法。
- en: References
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Kingma, D. P., & Welling, M. (2013). Auto-encoding variational Bayes. *arXiv
    preprint* arXiv:1312.6114.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma, D. P., & Welling, M. (2013)。自编码变分贝叶斯。*arXiv预印本* arXiv:1312.6114。
- en: Salakhutdinov, R., Mnih, A., & Hinton, G. (2007, June). Restricted Boltzmann
    machines for collaborative filtering. In *Proceedings of the 24th International
    Conference on Machine Learning* (pp. 791-798).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salakhutdinov, R., Mnih, A., & Hinton, G. (2007年6月)。用于协同过滤的限制玻尔兹曼机。发表于 *第24届国际机器学习大会论文集*（第791-798页）。
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A. and Bengio, Y. (2014). Generative adversarial nets. In *Advances
    in Neural Information Processing Systems* (pp. 2672-2680).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A. 和 Bengio, Y. (2014)。生成对抗网络。发表于 *神经信息处理系统进展*（第2672-2680页）。
- en: Zhang, Y., Chuangsuwanich, E., & Glass, J. (2014, May). Extracting deep neural
    network bottleneck features using low-rank matrix factorization. In *2014 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*
    (pp. 185-189). IEEE.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang, Y., Chuangsuwanich, E., & Glass, J. (2014, May). 使用低秩矩阵分解提取深度神经网络瓶颈特征。在*2014年IEEE国际会议上的声学、语音和信号处理（ICASSP）*（pp.
    185-189）。IEEE。
- en: 'Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.
    *arXiv preprint* arXiv:1412.6980.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma, D. P., & Ba, J. (2014). Adam：随机优化方法。*arXiv预印本* arXiv:1412.6980。
- en: Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019).
    A survey on bias and fairness in machine learning. *arXiv preprint* arXiv:1908.09635.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019).
    机器学习中的偏见和公平性调查。*arXiv预印本* arXiv:1908.09635。
- en: Im, D. I. J., Ahn, S., Memisevic, R., & Bengio, Y. (2017, February). Denoising
    criterion for variational auto-encoding framework. In *Thirty-First AAAI Conference
    on Artificial Intelligence*.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Im, D. I. J., Ahn, S., Memisevic, R., & Bengio, Y. (2017, February). 变分自动编码框架的去噪标准。在*第31届AAAI人工智能大会*。
