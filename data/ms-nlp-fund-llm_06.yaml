- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: 'Text Classification Reimagined: Delving Deep into Deep Learning Language Models'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新构想文本分类：深入探索深度学习语言模型
- en: In this chapter, we delve into the realm of **deep learning** (**DL**) and its
    application in **natural language processing** (**NLP**), specifically focusing
    on the groundbreaking transformer-based models such as **Bidirectional Encoder
    Representations from Transformers** (**BERT**) and **generative pretrained transformer**
    (**GPT**). We begin by introducing the fundamentals of DL, elucidating its powerful
    capability to learn intricate patterns from large amounts of data, making it the
    cornerstone of state-of-the-art NLP systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨深度学习（**DL**）及其在自然语言处理（**NLP**）中的应用，特别是专注于基于转换器的突破性模型，如**双向编码器表示从转换器**（**BERT**）和**生成预训练转换器**（**GPT**）。我们首先介绍深度学习的基础知识，阐述其从大量数据中学习复杂模式的能力，使其成为最先进
    NLP 系统的基石。
- en: Following this, we delve into transformers, a novel architecture that has revolutionized
    NLP by offering a more effective method of handling sequence data compared to
    traditional **recurrent neural networks** (**RNNs**) and **convolutional neural
    networks** (**CNNs**). We unpack the transformer’s unique characteristics, including
    its attention mechanisms, which allow it to focus on different parts of the input
    sequence to better understand the context.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们深入探讨转换器，这是一种新颖的架构，它通过提供比传统的**循环神经网络**（**RNNs**）和**卷积神经网络**（**CNNs**）更有效的处理序列数据的方法，从而彻底改变了
    NLP。我们剖析了转换器的独特特性，包括其注意力机制，这使得它能够关注输入序列的不同部分，从而更好地理解上下文。
- en: Then, we turn our attention to BERT and GPT, transformer-based language models
    that leverage these strengths to create highly nuanced language representations.
    We provide a detailed breakdown of the BERT architecture, discussing its innovative
    use of bidirectional training to generate contextually rich word embeddings. We
    will demystify the inner workings of BERT and explore its pretraining process,
    which leverages a large corpus of text to learn language semantics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将注意力转向 BERT 和 GPT，这些基于转换器的语言模型利用这些优势来创建高度细腻的语言表示。我们详细分析了 BERT 架构，讨论了其创新性地使用双向训练来生成语境丰富的词嵌入。我们将揭示
    BERT 的内部运作机制，并探索其预训练过程，该过程利用大量文本语料库来学习语言语义。
- en: Finally, we discuss how BERT can be fine-tuned for specific tasks, such as text
    classification. We walk you through the steps, from data preprocessing and model
    configuration to training and evaluation, providing a hands-on understanding of
    how to leverage BERT’s power for text classification.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了如何针对特定任务（如文本分类）微调 BERT。我们将引导你通过数据预处理、模型配置、训练和评估的步骤，提供如何利用 BERT 的力量进行文本分类的动手理解。
- en: This chapter provides a thorough exploration of DL in NLP, moving from foundational
    concepts to practical applications, equipping you with the knowledge to harness
    the capabilities of BERT and transformer models for your text classification tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章全面探讨了自然语言处理（NLP）中的深度学习（DL），从基础概念到实际应用，使你具备利用 BERT 和转换器模型的能力，以应对你的文本分类任务。
- en: 'The following topics are covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Understanding deep learning basics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习基础知识
- en: The architecture of different neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同神经网络的架构
- en: Transformers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器
- en: Language models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型
- en: The challenges of training neural networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络的挑战
- en: BERT
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT
- en: GPT
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT
- en: How to use language models for classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用语言模型进行分类
- en: NLP-ML system design example
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP-ML 系统设计示例
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To successfully navigate through this chapter, certain technical prerequisites
    are necessary, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功阅读本章，以下技术先决条件是必要的：
- en: '**Programming knowledge**: A strong understanding of Python is essential, as
    it’s the primary language used for most DL and NLP libraries.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程知识**：对 Python 的深入了解是必不可少的，因为它是大多数深度学习和 NLP 库的主要语言。'
- en: '**Machine learning fundamentals**: A good grasp of basic ML concepts such as
    training/testing data, overfitting, underfitting, accuracy, precision, recall,
    and F1 score will be valuable.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习基础**：对基本机器学习概念的良好掌握，如训练/测试数据、过拟合、欠拟合、准确率、精确率、召回率和 F1 分数，将非常有价值。'
- en: '**DL basics**: Familiarity with **DL** concepts and architectures, including
    neural networks, backpropagation, activation functions, and loss functions, will
    be essential. Knowledge of RNNs and CNNs would be advantageous but not strictly
    necessary as we will focus more on transformer architectures.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习基础知识**：熟悉**深度学习**的概念和架构，包括神经网络、反向传播、激活函数和损失函数，将是必不可少的。了解RNN和CNN将是有益的，但不是严格必要的，因为我们将会更多地关注transformer架构。'
- en: '**NLP basics**: Some understanding of basic NLP concepts such as tokenization,
    stemming, lemmatization, and word embeddings (such as **Word2Vec** or **GloVe**)
    would be beneficial.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLP基础知识**：对基本NLP概念如分词、词干提取、词形还原和词嵌入（例如**Word2Vec**或**GloVe**）的理解将大有裨益。'
- en: '**Libraries and frameworks**: Experience with libraries such as **TensorFlow**
    and **PyTorch** for building and training neural models is crucial. Familiarity
    with NLP libraries such as **NLTK** or **SpaCy** can also be beneficial. For working
    with BERT specifically, knowledge of the **transformers** library from **Hugging
    Face** would be very helpful.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库和框架**：对于使用**TensorFlow**和**PyTorch**构建和训练神经网络的经验至关重要。熟悉NLP库如**NLTK**或**SpaCy**也可能有益。对于专门使用BERT，了解**Hugging
    Face**的**transformers**库将非常有帮助。'
- en: '**Hardware requirements**: DL models, especially transformer-based models such
    as BERT, are computationally intensive and typically require a modern **graphics
    processing unit** (**GPU)** to train in a reasonable amount of time. Access to
    a high-performance computer or cloud-based solutions with GPU capabilities is
    highly recommended.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件要求**：深度学习模型，尤其是基于transformer的模型如BERT，计算密集，通常需要现代**图形处理单元**（**GPU**）在合理的时间内进行训练。推荐使用具有GPU能力的性能计算机或基于云的解决方案。'
- en: '**Mathematics**: A good understanding of linear algebra, calculus, and probability
    is helpful for understanding the inner workings of these models, but most of the
    chapter can be understood without in-depth mathematical knowledge.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学**：对线性代数、微积分和概率的良好理解有助于理解这些模型的内部工作原理，但大多数章节可以在没有深入数学知识的情况下理解。'
- en: These prerequisites are intended to equip you with the necessary background
    to understand and implement the concepts discussed in the chapter. With these
    in place, you should be well-prepared to delve into the fascinating world of DL
    for text classification using **BERT**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些先决条件旨在为你提供理解和实现章节中讨论的概念所需的必要背景。有了这些，你应该为深入探索使用**BERT**进行文本分类的迷人的深度学习世界做好了充分的准备。
- en: Understanding deep learning basics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习基础知识
- en: In this part, we explain what neural network and deep neural networks are, what
    is the motivation for using them, and the different types (architectures) of deep
    learning models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们解释了神经网络和深度神经网络是什么，使用它们的动机，以及深度学习模型的不同类型（架构）。
- en: What is a neural network?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: Neural networks are a subfield of **artificial intelligence** (**AI**) and ML
    that focuses on algorithms inspired by the structure and function of the brain.
    It is also known as “deep” learning because these neural networks often consist
    of many repetitive layers, creating a deep architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是人工智能（**AI**）和机器学习（ML）的一个子领域，它专注于受大脑结构和功能启发的算法。它也被称为“深度”学习，因为这些神经网络通常由许多重复的层组成，形成了一个深度架构。
- en: These DL models are capable of “learning” from large volumes of complex, high-dimensional,
    and unstructured data. The term “learning” refers to the ability of the model
    to automatically learn and improve from experience without being explicitly programmed
    to do so for any one particular task of the tasks it learns.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些深度学习模型能够从大量复杂、高维和无结构的数据中“学习”。术语“学习”指的是模型能够自动从经验中学习并改进，而无需为任何特定任务（或任务集）明确编程。
- en: DL can be supervised, semi-supervised, or unsupervised. It’s used in numerous
    applications, including NLP, speech recognition, image recognition, and even playing
    games. The models can identify patterns and make data-driven predictions or decisions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以是监督的、半监督的或无监督的。它在众多应用中使用，包括自然语言处理（NLP）、语音识别、图像识别，甚至玩游戏。这些模型可以识别模式并做出基于数据的预测或决策。
- en: One of the critical advantages of DL is its ability to process and model data
    of various types, including text, images, sound, and more. This versatility has
    led to a vast range of applications, from self-driving cars to sophisticated web
    search algorithms and highly responsive speech recognition systems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的一个关键优势是它能够处理和建模各种类型的数据，包括文本、图像、声音等。这种多功能性导致了从自动驾驶汽车到复杂的网络搜索算法以及高度响应的语音识别系统等广泛的应用。
- en: It’s worth noting that DL, despite its high potential, also requires significant
    computational power and large amounts of high-quality data to train effectively,
    which can be a challenge.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管深度学习具有很高的潜力，但它也要求有显著的计算能力和大量高质量的数据来有效地训练，这可能是一个挑战。
- en: In essence, DL is a powerful and transformative technology that is at the forefront
    of many of today’s technological advancements.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，深度学习（DL）是一种强大且变革性的技术，它是许多当今技术进步的前沿。
- en: The motivation for using neural networks
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用神经网络的动机
- en: 'Neural networks are used for a variety of reasons in the field of ML and artificial
    intelligence. Here are some of the key motivations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和人工智能领域，神经网络被用于各种原因。以下是一些关键动机：
- en: '**Nonlinearity**: Neural networks, with their intricate structure and use of
    activation functions, can capture nonlinear relationships in data. Many real-world
    phenomena are nonlinear in nature, and neural networks offer a way to model these
    complexities.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性**：神经网络通过其复杂的结构和激活函数的使用，可以捕捉数据中的非线性关系。许多现实世界现象在本质上是非线性的，神经网络提供了一种建模这些复杂性的方法。'
- en: '**Universal approximation theorem**: This theorem states that a neural network
    with enough hidden units can approximate virtually any function with a high degree
    of accuracy. This makes them highly flexible and adaptable to a wide range of
    tasks.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用逼近定理**：这个定理表明，具有足够隐藏单元的神经网络可以以高精度逼近几乎任何函数。这使得它们非常灵活，能够适应广泛的任务。'
- en: '**Ability to handle high dimensional data**: Neural networks can handle data
    with a large number of features or dimensions effectively, which makes them useful
    for tasks such as image or speech recognition, where data is highly dimensional.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理高维数据的能力**：神经网络可以有效地处理具有大量特征或维度的数据，这使得它们在图像或语音识别等高度维度的任务中非常有用。'
- en: '**Pattern recognition and prediction**: Neural networks excel at identifying
    patterns and trends within large datasets, making them especially useful for prediction
    tasks, such as forecasting sales or predicting stock market trends.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式识别和预测**：神经网络在识别大型数据集中的模式和趋势方面表现出色，这使得它们在预测任务中特别有用，例如预测销售或预测股市趋势。'
- en: '**Parallel processing**: Neural networks’ architecture allows them to perform
    many operations simultaneously, making them highly efficient when implemented
    on modern hardware.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行处理**：神经网络的结构允许它们同时执行许多操作，这使得它们在现代硬件上实现时效率非常高。'
- en: '**Learning from data**: Neural networks can improve their performance as they
    are exposed to more data. This ability to learn from data makes them highly effective
    for tasks where large amounts of data are available.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从数据中学习**：随着神经网络接触到更多的数据，它们可以提高自己的性能。这种从数据中学习的能力使它们在大量数据可用的任务中非常有效。'
- en: '**Robustness**: Neural networks can handle noise in the input data and are
    robust to small variations in the input.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：神经网络可以处理输入数据中的噪声，对输入的小幅变化具有鲁棒性。'
- en: 'Additionally, neural networks are extensively used in NLP tasks due to several
    reasons. Here are some of the primary motivations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于几个原因，神经网络在自然语言处理（NLP）任务中被广泛使用。以下是一些主要动机：
- en: '**Handling sequential data**: Natural language is inherently sequential (words
    follow one another to make coherent sentences). RNNs and their advanced versions,
    such as **long short-term memory** (**LSTM**) and **gated recurrent units** (**GRUs**),
    are types of neural networks that are capable of processing sequential data by
    maintaining a form of internal state or memory about the previous steps in the
    sequence.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理序列数据**：自然语言本质上是序列性的（单词依次排列以形成连贯的句子）。循环神经网络（RNN）及其高级版本，如**长短期记忆**（**LSTM**）和**门控循环单元**（**GRUs**），是能够通过保持关于序列中先前步骤的某种内部状态或记忆来处理序列数据的神经网络类型。'
- en: '**Context understanding**: Neural networks, especially recurrent types, are
    capable of understanding the context in a sentence by taking into account the
    surrounding words or even previous sentences, which is crucial in NLP tasks.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文理解**：神经网络，尤其是循环神经网络，能够通过考虑周围的词语甚至之前的句子来理解句子中的上下文，这在NLP任务中至关重要。'
- en: '**Semantic hashing**: Neural networks, through the use of word embeddings (such
    as Word2Vec and GloVe), can encode words in a way that preserves their semantic
    meaning. Words with similar meanings are placed closer together in the vector
    space, which is highly valuable for many NLP tasks.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义哈希**：神经网络通过使用词嵌入（如Word2Vec和GloVe）以保留其语义意义的方式对词语进行编码。具有相似意义的词语在向量空间中放置得更近，这对于许多NLP任务非常有价值。'
- en: '**End-to-end learning**: Neural networks can learn directly from raw data.
    For example, in image classification, a neural network can learn features from
    the pixel level without needing any manual feature extraction steps. This is a
    significant advantage, as the feature extraction process can be time-consuming
    and require domain expertise.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端学习**：神经网络可以直接从原始数据中学习。例如，在图像分类中，神经网络可以从像素级别学习特征，而不需要任何手动特征提取步骤。这是一个显著的优势，因为特征提取过程可能耗时且需要领域专业知识。'
- en: Similarly, neural networks can learn to perform NLP tasks from raw text data
    without the need for manual feature extraction. This is a big advantage in NLP,
    where creating hand-engineered features can be challenging and time-consuming.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，神经网络可以学习从原始文本数据中执行NLP任务，而不需要手动特征提取。这在NLP中是一个很大的优势，因为创建手工特征可能既困难又耗时。
- en: '**Performance**: Neural networks, especially with the advent of transformer-based
    architectures such as BERT, GPT, and so on., have been shown to achieve state-of-the-art
    results in many NLP tasks, including but not limited to machine translation, text
    summarization, sentiment analysis, and question answering.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：神经网络，特别是随着基于transformer架构的BERT、GPT等的出现，已经在许多NLP任务中实现了最先进的结果，包括但不限于机器翻译、文本摘要、情感分析和问答。'
- en: '**Handling large vocabularies**: Neural networks can effectively handle large
    vocabularies and continuous text streams, which is typical in many **NLP** problems.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理大型词汇**：神经网络可以有效地处理大型词汇和连续的文本流，这在许多NLP问题中很常见。'
- en: '**Learning hierarchical features**: Deep neural networks can learn hierarchical
    representations. In the context of NLP, lower layers often learn to represent
    simple things such as n-grams, whereas higher layers can represent complex concepts
    such as sentiment.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习层次特征**：深度神经网络可以学习层次化的表示。在自然语言处理（NLP）的背景下，较低层通常学习表示简单的事物，例如n-gram，而较高层可以表示复杂的概念，例如情感。'
- en: Despite these advantages, it’s worth noting that neural networks also have their
    challenges, including their “black box” nature, which makes their decision-making
    process difficult to interpret, and their need for large amounts of data and computational
    resources for training. However, the benefits they provide in terms of performance
    and their ability to learn from raw text data and model complex relationships
    make them a go-to choice for many NLP tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些优势，但值得注意的是，神经网络也面临挑战，包括其“黑盒”性质，这使得其决策过程难以解释，以及它们在训练过程中需要大量数据和计算资源。然而，它们在性能方面提供的优势以及从原始文本数据中学习并建模复杂关系的能力，使它们成为许多NLP任务的首选选择。
- en: The basic design of a neural network
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的基本设计
- en: A neural network consists of multiple layers of interconnected nodes, or “neurons,”
    each of which performs a simple computation on the data it receives, passing its
    output to the neurons of the next layer. Each connection between neurons has an
    associated weight that is adjusted during the learning process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由多层相互连接的节点，或称为“神经元”组成，每个神经元对其接收到的数据进行简单的计算，并将输出传递给下一层的神经元。每个神经元之间的连接都有一个相关的权重，该权重在学习过程中进行调整。
- en: 'The architecture of a basic neural network consists of three types of layers,
    as shown in *Figure 6**.1*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基本神经网络的结构由三种类型的层组成，如图*图6.1*所示：
- en: '![Figure 6.1 – Basic architecture of neural networks](img/B18949_06_001.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 神经网络的基本架构](img/B18949_06_001.jpg)'
- en: Figure 6.1 – Basic architecture of neural networks
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 神经网络的基本架构
- en: 'In the following list, we explain each layer of the model in more detail:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们将更详细地解释模型的每一层：
- en: '**Input layer**: This is where the network receives its input. If the network
    is designed to process an image with dimensions of 28x28 pixels, for instance,
    there would be 784 neurons in the input layer, each representing the value of
    one pixel.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：这是网络接收输入的地方。例如，如果网络被设计为处理28x28像素的图像，那么输入层将有784个神经元，每个神经元代表一个像素的值。'
- en: '**Hidden layer(s)**: These are the layers between the input and output layers.
    Each neuron in a hidden layer takes the outputs of the neurons from the previous
    layer, multiplies each of these by the weight of the respective connection, and
    sums these values up. This sum is then passed through an “activation function”
    to introduce nonlinearity into the model, which helps the network learn complex
    patterns. There can be any number of hidden layers in a neural network, and a
    network with many hidden layers is often referred to as a “deep” neural network.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层（s）**：这些层位于输入层和输出层之间。隐藏层中的每个神经元都会接收来自前一层的神经元输出，将这些输出与各自连接的权重相乘，并将这些值相加。这个和随后通过一个“激活函数”传递，以向模型引入非线性，这有助于网络学习复杂的模式。神经网络中可以有任何数量的隐藏层，具有许多隐藏层的网络通常被称为“深度”神经网络。'
- en: '**Output layer**: This is the final layer in the network. The neurons in this
    layer produce the final output of the network. For a classification problem, for
    instance, you might design the network to have one output neuron for each class
    in the problem, with each neuron outputting a value indicating the probability
    that the input belongs to its respective class.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：这是网络中的最后一层。该层的神经元产生网络的最终输出。例如，对于分类问题，你可能设计网络使其具有一个输出神经元对应于问题中的每个类别，每个神经元输出一个值，表示输入属于其相应类别的概率。'
- en: The neurons in the network are interconnected. The weights of these connections,
    which are initially set to random values, represent what the network has learned
    once it has been trained on data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的神经元是相互连接的。这些连接的权重最初设置为随机值，代表网络在训练数据上训练后所学习的内容。
- en: During the training process, an algorithm such as backpropagation is used to
    adjust the weights of the connections in the network in response to the difference
    between the network’s output and the desired output. This process is repeated
    many times, and the network gradually improves its performance on the training
    data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，使用诸如反向传播之类的算法来调整网络中连接的权重，以响应网络输出与期望输出之间的差异。这个过程会重复多次，网络逐渐提高其在训练数据上的性能。
- en: To provide a simple visual idea, imagine three sets of circles (representing
    neurons) arranged in columns (representing layers). The first column is the input
    layer, the last column is the output layer and any columns in between are the
    hidden layers. Then, imagine lines connecting every circle in each column to every
    circle in the next column, representing the weighted connections between neurons.
    That’s a basic visual representation of a neural network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个简单的视觉概念，想象有三组圆圈（代表神经元）排列成列（代表层）。第一列是输入层，最后一列是输出层，任何介于两者之间的列都是隐藏层。然后，想象连接每一列中每个圆圈到下一列中每个圆圈的线条，代表神经元之间的加权连接。这就是神经网络的基本视觉表示。
- en: In the next part, we are going to describe the common terms related to neural
    networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将描述与神经网络相关的常见术语。
- en: Neural network common terms
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络常见术语
- en: In the following subsections, we'll look at some of the most commonly used terms
    in the context of neural networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将探讨神经网络中最常用的术语。
- en: Neuron (or node)
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元（或节点）
- en: This is the basic unit of computation in a neural network; typically, a simple
    computation involves inputs, weights, a bias, and an activation function. A neuron,
    also known as a node or unit, is a fundamental element in a neural network. It
    receives input from some other nodes or from an external source if the neuron
    is in the input layer. The neuron then computes an output based on this input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是神经网络中的基本计算单元；通常，简单的计算涉及输入、权重、偏差和激活函数。神经元，也称为节点或单元，是神经网络的基本元素。它从其他节点或外部源接收输入，如果神经元位于输入层，则从外部源接收。然后，神经元根据这个输入计算输出。
- en: Each input has an associated weight (*w*), which is assigned based on its relative
    importance to other inputs. The neuron applies a weight to the inputs, sums them
    up, and then applies an activation function to the sum plus a bias value (*b*).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入都有一个相关的权重（*w*），这个权重是根据其与其他输入的相对重要性分配的。神经元将权重应用于输入，将它们加起来，然后对总和加上偏置值（*b*）应用激活函数。
- en: 'Here’s a step-by-step breakdown:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是逐步分解：
- en: '**Weighted sum**: Each input (*x*) to the neuron is multiplied by a corresponding
    weight (*w*). These weighted inputs are then summed together with a bias term
    (*b*). The bias term allows for the activation function to be shifted to the left
    or the right, helping the neuron model a wider range of patterns. Mathematically,
    this step can be represented as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加权求和**：每个输入（*x*）到神经元的乘以相应的权重（*w*）。这些加权输入然后与偏置项（*b*）相加。偏置项允许激活函数向左或向右移动，有助于神经元模拟更广泛的模式。从数学上讲，这一步可以表示如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>z</mi><mo>=</mo><mi>w</mi><mn>1</mn><mi
    mathvariant="normal">*</mi><mi>x</mi><mn>1</mn><mo>+</mo><mi>w</mi><mn>2</mn><mi
    mathvariant="normal">*</mi><mi>x</mi><mn>2</mn><mo>+</mo><mo>…</mo><mo>+</mo><mi>w</mi><mi
    mathvariant="normal">n</mi><mi mathvariant="normal">*</mi><mi>x</mi><mi mathvariant="normal">n</mi><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/297.png)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>z</mi><mo>=</mo><mi>w</mi><mn>1</mn><mi
    mathvariant="normal">*</mi><mi>x</mi><mn>1</mn><mo>+</mo><mi>w</mi><mn>2</mn><mi
    mathvariant="normal">*</mi><mi>x</mi><mn>2</mn><mo>+</mo><mo>…</mo><mo>+</mo><mi>w</mi><mi
    mathvariant="normal">n</mi><mi mathvariant="normal">*</mi><mi>x</mi><mi mathvariant="normal">n</mi><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/297.png)'
- en: '**Activation function**: The result of the weighted sum is then passed through
    an activation function. The purpose of the activation function is to introduce
    nonlinearity into the output of a neuron. This nonlinearity allows the network
    to learn from errors and make adjustments, which is essential when it comes to
    performing complex tasks such as language translation or image recognition. Common
    choices for activation functions include the sigmoid function, hyperbolic **tangent**
    (**tanh**), and **rectified linear unit** (**ReLU**), among others.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**激活函数**：加权求和的结果随后通过激活函数。激活函数的目的是将非线性引入神经元的输出。这种非线性使得网络能够从错误中学习并做出调整，这对于执行复杂任务（如语言翻译或图像识别）至关重要。常见的激活函数选择包括
    sigmoid 函数、双曲正切（**tanh**）和修正线性单元（**ReLU**）等。'
- en: The output of the neuron is the result of the activation function. It serves
    as the input to the neurons in the next layer of the network.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经元的输出是激活函数的结果。它作为网络下一层神经元的输入。
- en: 'The weights and bias in the neuron are learnable parameters. In other words,
    their values are learned over time as the neural network is trained on data:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经元中的权重和偏置是可学习的参数。换句话说，它们的值是在神经网络在数据上训练的过程中逐渐学习的：
- en: '**Weights**: The strength or amplitude of the connection between two neurons.
    During the training phase, the neural network learns the correct weights that
    better map inputs to outputs. Weight is used in the neuron, as explained previously.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：两个神经元之间连接的强度或幅度。在训练阶段，神经网络学习正确的权重，以便更好地将输入映射到输出。权重在神经元中，如前所述使用。'
- en: '**Bias**: An additional parameter in the neuron that allows for the activation
    function to be shifted to the left or right, which can be critical for successful
    learning (also used in the neuron).'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏置**：神经元中的一个附加参数，允许激活函数向左或向右移动，这对于成功学习至关重要（也用于神经元）。'
- en: Activation function
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: The function (in each neuron) that determines the output a neuron should produce
    given its input is called an activation function. Common examples include sigmoid,
    ReLU and tanh.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 确定神经元应产生给定其输入的输出的函数（在每个神经元中）称为激活函数。常见的例子包括 sigmoid、ReLU 和 tanh。
- en: 'Here are some of the most common types of activation functions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些最常见的激活函数类型：
- en: '**Sigmoid function**: This is where we’re essentially classifying the input
    as either 0 or 1\. The sigmoid function takes a real-valued input and squashes
    it to range between 0 and 1\. It’s often used in the output layer of a binary
    classification network:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid 函数**：这里我们基本上是将输入分类为 0 或 1。Sigmoid 函数将实值输入压缩到 0 到 1 之间。它通常用于二元分类网络的输出层：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></mrow></math>](img/298.png)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/298.png)'
- en: 'However, it has two major drawbacks: **the vanishing gradients problem** (gradients
    are very small for large positive or negative inputs, which can slow down learning
    during backpropagation) and the **outputs are** **not zero-centered**.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，它有两个主要缺点：**梯度消失问题**（对于大的正或负输入，梯度非常小，这可能会在反向传播期间减慢学习速度）和**输出不是** **零中心**。
- en: '**Hyperbolic tanh function**: The tanh function also takes a real-valued input
    and squashes it to range between -1 and 1\. Unlike the sigmoid function, its output
    is zero-centered because its range is symmetric around the origin:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双曲正切函数**：tanh函数也接受实数值输入并将其压缩到-1和1之间。与sigmoid函数不同，其输出以零为中心，因为其范围在原点周围是对称的：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mo>(</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow><mrow><mo>(</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></mrow></math>](img/299.png)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mo>(</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow><mrow><mo>(</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/299.png)'
- en: It also suffers from the vanishing gradients problem, as does the sigmoid function.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它也受到梯度消失问题的影响，与sigmoid函数一样。
- en: '**ReLU function**: The ReLU function has become very popular in recent years.
    It computes the function as follows:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU函数**：ReLU函数在近年来变得非常流行。其计算方式如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/300.png)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/300.png)'
- en: In other words, the activation is simply the input if the input is positive;
    otherwise, it’s zero.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 换句话说，如果输入是正的，激活就是输入本身；否则，它是零。
- en: It doesn’t activate all the neurons at the same time, meaning that the neurons
    will only be deactivated if the output of the linear transformation is less than
    0\. This makes the network sparse and efficient. However, ReLU units can be fragile
    during training and can “die” (they stop learning completely) if a large gradient
    flows through them.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它不会同时激活所有神经元，这意味着只有当线性变换的输出小于0时，神经元才会被关闭。这使得网络稀疏且高效。然而，ReLU单元在训练期间可能很脆弱，如果通过它们的大梯度流动，它们可能会“死亡”（完全停止学习）。
- en: '**Leaky ReLU**: Leaky ReLU is a variant of ReLU that addresses the “dying ReLU”
    problem. Instead of defining the function as *0* for negative *x*, we define it
    as a small linear component of *x*:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU**：Leaky ReLU是ReLU的一种变体，它解决了“dying ReLU”问题。我们不是将函数定义为负*x*的*0*，而是将其定义为*x*的一个小的线性分量：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0.01</mn><mi>x</mi><mo>,</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/301.png)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0.01</mn><mi>x</mi><mo>,</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/301.png)'
- en: This allows the function to “leak” some information when the input is negative
    and helps to mitigate the dying ReLU problem.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这允许函数在输入为负时“泄露”一些信息，有助于缓解dying ReLU问题。
- en: '**Exponential linear unit (ELU)**: ELU is also a variant of ReLU that modifies
    the function to be a non-zero value for negative *x*, which can help the learning
    process:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数线性单元（ELU）**：ELU也是ReLU的一种变体，它修改了函数，使其对于负*x*具有非零值，这有助于学习过程：'
- en: f(x) = x if x > 0, else
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f(x) = x if x > 0, else
- en: α(exp(x) − 1)
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: α(exp(x) − 1)
- en: Here alpha (*α*) is a constant that defines function smoothness when inputs
    are negative. ELU tends to converge cost to zero faster and produce more accurate
    results. However, it can be slower to compute because of the use of the exponential
    operation.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，alpha (*α*) 是一个常数，它定义了当输入为负时函数的平滑性。ELU倾向于更快地将成本收敛到零并产生更准确的结果。然而，由于使用了指数运算，它可能计算得较慢。
- en: '**Softmax function**: The softmax function is often used in the output layer
    of a classifier where we’re trying to assign the input to one of several classes.
    It gives the probability that any given input belongs to each of the possible
    classes:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax函数**：softmax函数常用于分类器的输出层，其中我们试图将输入分配到几个可能的类别之一。它给出了任何给定输入属于每个可能类别的概率：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow></mrow></math>](img/302.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow></mrow></math>](img/302.png)'
- en: The denominator normalizes the probabilities, so they all sum up to 1 across
    all classes. The softmax function is also used in multinomial logistical regression.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 分母将概率归一化，因此它们在所有类别中加起来等于1。softmax函数也用于多项式逻辑回归。
- en: Each of these activation functions has pros and cons, and the choice of activation
    function can depend on the specific application and context of the problem at
    hand.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数各有优缺点，激活函数的选择可能取决于具体的应用和问题的具体背景。
- en: Layer
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层
- en: A set of neurons that process signals at the same level of abstraction. The
    first layer is the input layer, the last layer is the output layer, and all layers
    in between are called hidden layers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一组在同一抽象级别处理信号的神经元。第一层是输入层，最后一层是输出层，介于两者之间的所有层都称为隐藏层。
- en: Epoch
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Epoch
- en: In the context of training a neural network, an epoch is a term used to denote
    one complete pass through the entire training dataset. During an epoch, the neural
    network’s weights are updated in an attempt to minimize the loss function.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络的背景下，epoch是一个术语，用来表示对整个训练数据集的一次完整遍历。在一个epoch期间，神经网络会更新其权重，试图最小化损失函数。
- en: The number of epochs hyperparameter sets how many times the deep learning algorithm
    processes the entire training dataset. Too many epochs can cause overfitting,
    where the model performs well on training data but poorly on new data. Conversely,
    training for too few epochs may mean the model is underfitting—it could improve
    with further training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数“epoch”的数量决定了深度学习算法处理整个训练数据集的次数。epoch过多可能导致过拟合，即模型在训练数据上表现良好，但在新数据上表现较差。相反，epoch过少可能意味着模型欠拟合——它可能需要进一步训练来改进。
- en: It’s also important to note that the concept of an epoch is more relevant in
    the batch and mini-batch variants of gradient descent. In stochastic gradient
    descent, the model’s weights are updated after seeing each individual example,
    so the concept of an epoch is less straightforward.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，epoch的概念在批量和迷你批量的梯度下降变体中更为相关。在随机梯度下降中，模型在看到每个单独的例子后更新其权重，因此epoch的概念就不那么直接了。
- en: Batch size
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量大小
- en: The number of training instances used in one iteration. Batch size refers to
    the number of training examples used in one iteration.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次迭代中使用的训练实例数量。批量大小指的是在一次迭代中使用的训练示例数量。
- en: 'When you start training a neural network, you have a couple of options for
    how you feed your data into the model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始训练一个神经网络时，你有几种选择来决定如何将数据输入到模型中：
- en: '**Batch gradient descent**: Here, the entire training dataset is used to compute
    the gradient of the loss function for each iteration of the optimizer (as with
    gradient descent). In this case, the batch size is equal to the total number of
    examples in the training dataset.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量梯度下降**：在这里，整个训练数据集被用来计算优化器每个迭代的损失函数的梯度（就像梯度下降一样）。在这种情况下，批量大小等于训练数据集中示例的总数。'
- en: '**Stochastic gradient descent (SGD)**: SGD uses a single example at each iteration
    of the optimizer. Therefore, the batch size for SGD is *1*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降（SGD）**：SGD在每个优化器的迭代中使用单个例子。因此，SGD的批量大小为*1*。'
- en: '**Mini-batch gradient descent**: This is a compromise between batch gradient
    descent and SGD. In mini-batch gradient descent, the batch size is usually between
    10 and 1,000 and is chosen depending on the computational resources you have.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量梯度下降法**：这是批梯度下降法和随机梯度下降法之间的折中方案。在小批量梯度下降法中，批量大小通常在10到1,000之间，具体取决于你拥有的计算资源。'
- en: The batch size can significantly impact the learning process. Larger batch sizes
    result in faster progress in training but don’t always converge as fast. Smaller
    batch sizes update the model frequently but the progress in training is slower.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小可以显著影响学习过程。较大的批量大小在训练中进展更快，但并不总是收敛得那么快。较小的批量大小频繁更新模型，但训练进展较慢。
- en: Moreover, smaller batch sizes have a regularizing effect and can help the model
    generalize better, leading to better performance on unseen data. However, using
    a batch size that is too small can lead to unstable training, less accurate estimates
    of the gradient, and, ultimately, a model with worse performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，较小的批量大小具有正则化作用，可以帮助模型更好地泛化，从而在未见过的数据上获得更好的性能。然而，使用过小的批量大小可能导致训练不稳定，梯度估计不准确，最终导致模型性能更差。
- en: 'Choosing the right batch size is a matter of trial and error and depends on
    the specific problem and the computational resources at hand:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的批量大小是一个试错的过程，并且取决于具体问题和可用的计算资源：
- en: '**Iterations**: The number of batches of data the algorithm has seen (or the
    number of passes it has made on the dataset).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代次数**：算法看到的批量数据数量（或它在数据集上进行的遍历次数）。'
- en: '**Learning rate**: A hyperparameter that controls the speed of convergence
    of the learning algorithm by adjusting the weight update rate based on the loss
    gradient.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：一个超参数，通过根据损失梯度调整权重更新率来控制学习算法的收敛速度。'
- en: '**Loss function (cost function)**: The loss function evaluates the neural network’s
    performance on the dataset. Higher deviations between predictions and actual results
    result in a larger output from the loss function. The goal is to minimize this
    output, which will give the model more accurate predictions.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数（代价函数）**：损失函数评估神经网络在数据集上的性能。预测值与实际结果之间的偏差越大，损失函数的输出就越大。目标是使这个输出最小化，这将使模型做出更准确的预测。'
- en: '**Backpropagation**: The primary algorithm for performing gradient descent
    on neural networks. It calculates the gradient of the loss function at the output
    layer and distributes it back through the layers of the network, updating the
    weights and biases in a way that minimizes the loss.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：在神经网络上执行梯度下降的主要算法。它计算输出层的损失函数梯度，并将其分布回网络的各层，通过更新权重和偏置以最小化损失。'
- en: '**Overfitting**: A situation where a model learns the detail and noise in the
    training data to the extent that it performs poorly on new, unseen data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：一种情况，其中模型学习训练数据中的细节和噪声，以至于它在新的、未见过的数据上的表现较差。'
- en: '**Underfitting**: A situation where a model is too simple to learn the underlying
    structure of the data and, thus, performs poorly on both training and new data.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：一种情况，其中模型过于简单，无法学习数据的潜在结构，因此训练数据和新的数据上的表现都较差。'
- en: '**Regularization**: A technique used to prevent overfitting by adding a penalty
    term to the loss function, which, in turn, constrains the weights of the network.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：一种通过向损失函数添加惩罚项来防止过拟合的技术，这反过来又限制了网络的权重。'
- en: '**Dropout**: A regularization technique where randomly selected neurons are
    ignored during training, which helps to prevent overfitting.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：一种正则化技术，在训练期间忽略随机选择的神经元，这有助于防止过拟合。'
- en: '**CNN**: A type of neural network well-suited to image processing and computer
    vision tasks.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNN**：一种非常适合图像处理和计算机视觉任务的神经网络类型。'
- en: '**RNN**: A type of neural network designed to recognize patterns in sequences
    of data, such as time series or text.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RNN**：一种旨在识别数据序列中模式（如时间序列或文本）的神经网络类型。'
- en: Let’s move on to the architecture of different neural networks next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探讨不同神经网络的架构。
- en: The architecture of different neural networks
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同神经网络的架构
- en: 'Neural networks come in various types, each with a specific architecture suited
    to a different kind of task. The following list contains general descriptions
    of some of the most common types:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有多种类型，每种类型都有适合不同任务的特定架构。以下列表包含了一些最常见类型的通用描述：
- en: '**Feedforward neural network (FNN)**: This is the most straightforward type
    of neural network. Information in this network moves in one direction only, from
    the input layer through any hidden layers to the output layer. There are no cycles
    or loops in the network; it’s a straight, “feedforward” path.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络 (FNN)**: 这是最直接的一种神经网络。在这个网络中，信息仅沿一个方向移动，从输入层通过任何隐藏层到输出层。网络中没有循环或环路；它是一条直线，“前馈”路径。'
- en: '![Figure 6.2 – Feedforward neural network](img/B18949_06_002.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 前馈神经网络](img/B18949_06_002.jpg)'
- en: Figure 6.2 – Feedforward neural network
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 前馈神经网络
- en: '**Multilayer perceptron (MLP)**: An MLP is a type of feedforward network that
    has at least one hidden layer in addition to its input and output layers. The
    layers are fully connected, meaning each neuron in a layer connects with every
    neuron in the next layer. MLPs can model complex patterns and are widely used
    for tasks such as image recognition, classification, speech recognition, and other
    types of machine learning tasks. The MLP is a feedforward network with layers
    of neurons arranged sequentially. Information flows from the input layer through
    hidden layers to the output layer in one direction:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器 (MLP)**: MLP是一种前馈网络，除了其输入和输出层外，至少还有一个隐藏层。层之间是完全连接的，这意味着层中的每个神经元都与下一层的每个神经元相连。MLP可以模拟复杂模式，并被广泛用于图像识别、分类、语音识别和其他类型的机器学习任务。MLP是一种前馈网络，具有按顺序排列的神经元层。信息从输入层通过隐藏层流向输出层，方向单一：'
- en: '![Figure 6.3 – Multilayer perceptron](img/B18949_06_003.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 多层感知器](img/B18949_06_003.jpg)'
- en: Figure 6.3 – Multilayer perceptron
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 多层感知器
- en: '**CNN**: A CNN is particularly well-suited to tasks involving spatial data,
    such as images. Its architecture includes three main types of layers: convolutional
    layers, pooling layers, and fully connected layers. The convolutional layers apply
    a series of filters to the input, which allows the network to automatically and
    adaptively learn spatial hierarchies of features. Pooling layers decrease the
    spatial size of the representation, thereby reducing parameters and computation
    in the network to control overfitting and decrease the computation cost in the
    following layers. Fully connected layers get the output of the pooling layer and
    conduct high-level reasoning on the output.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNN**: CNN特别适合涉及空间数据的任务，如图像。其架构包括三种主要类型的层：卷积层、池化层和全连接层。卷积层对输入应用一系列过滤器，这使得网络能够自动和自适应地学习特征的空间层次结构。池化层减少表示的空间大小，从而减少网络中的参数和计算，以控制过拟合并降低后续层的计算成本。全连接层获取池化层的输出，并在输出上进行高级推理。'
- en: "![Figure 6.4 – \uFEFFConvolutional neural network](img/B18949_06_004.jpg)"
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 卷积神经网络](img/B18949_06_004.jpg)'
- en: Figure 6.4 – Convolutional neural network
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 卷积神经网络
- en: '**Recurrent neural network (RNN)**: Unlike feedforward networks, RNNs have
    connections that form directed cycles. This architecture allows them to use information
    from their previous outputs as inputs, making them ideal for tasks involving sequential
    data, such as time series prediction or NLP. A significant variation of RNNs is
    the LSTM network, which uses special units in addition to standard units. RNN
    units include a "memory cell" that can maintain information in memory for long
    periods of time, a feature that is particularly useful for tasks that require
    learning from long-distance dependencies in the data, such as handwriting or speech
    recognition.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络 (RNN)**: 与前馈网络不同，RNNs具有形成有向循环的连接。这种架构允许它们使用先前输出的信息作为输入，这使得它们非常适合涉及序列数据的任务，例如时间序列预测或自然语言处理。RNNs的一个重要变体是LSTM网络，它除了标准单元外还使用特殊单元。RNN单元包括一个“记忆细胞”，可以在长时间内保持信息在内存中，这对于需要从数据中的长距离依赖关系学习的任务特别有用，例如手写识别或语音识别。'
- en: '![Figure 6.5 – Recurrent neural network](img/B18949_06_005.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 循环神经网络](img/B18949_06_005.jpg)'
- en: Figure 6.5 – Recurrent neural network
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 循环神经网络
- en: '**Autoencoder (AE)**: An AE is a type of neural network used to learn the efficient
    coding of input data. It has a symmetrical architecture and is designed to apply
    backpropagation, setting the target values to be equal to the inputs. Autoencoders
    are typically used for feature extraction, learning representations of data, and
    dimensionality reduction. They’re also used in generative models, noise removal,
    and recommendation systems.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器 (AE)**: AE 是一种用于学习输入数据有效编码的神经网络。它具有对称架构，并设计用于应用反向传播，将目标值设置为等于输入。自动编码器通常用于特征提取、学习数据的表示和降维。它们还用于生成模型、噪声去除和推荐系统。'
- en: '![Figure 6.6 – Autoencoder architecture](img/B18949_06_006.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 自动编码器架构](img/B18949_06_006.jpg)'
- en: Figure 6.6 – Autoencoder architecture
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 自动编码器架构
- en: '**Generative adversarial network (GAN)**: A GAN consists of two parts, a generator
    and a discriminator, which are both neural networks. The generator creates data
    instances that aim to come from the same distribution as the training dataset.
    The discriminator’s goal is to distinguish between instances from the true distribution
    and instances from the generator. The generator and the discriminator are trained
    together, with the goal that the generator produces better instances as training
    progresses, whereas the discriminator becomes better at distinguishing true instances
    from generated ones.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络 (GAN)**: GAN 由两部分组成，一个生成器和一个判别器，它们都是神经网络。生成器创建数据实例，旨在与训练数据集的分布相同。判别器的目标是区分来自真实分布的实例和来自生成器的实例。生成器和判别器一起训练，目标是随着训练的进行，生成器产生更好的实例，而判别器则变得更好地区分真实实例和生成实例。'
- en: '![Figure 6.7 – Generative adversarial network in computer vision](img/B18949_06_007.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 计算机视觉中的生成对抗网络](img/B18949_06_007.jpg)'
- en: Figure 6.7 – Generative adversarial network in computer vision
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 计算机视觉中的生成对抗网络
- en: These are just a few examples of neural network architectures, and many variations
    and combinations exist. The architecture you choose for a task will depend on
    the specific requirements and constraints of your task.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是神经网络架构的几个例子，存在许多变体和组合。你为任务选择架构将取决于任务的具体要求和限制。
- en: The challenges of training neural networks
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练的挑战
- en: 'Training neural networks is a complex task and comes with challenges during
    the training, such as local minima and vanishing/exploding gradients, as well
    as computational costs and interpretability. All challenges are explained in detail
    in the following points:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络是一项复杂的工作，在训练过程中会面临挑战，如局部最小值和梯度消失/爆炸，以及计算成本和可解释性。所有挑战都在以下各点中详细解释：
- en: '**Local minima**: The objective of training a neural network is to find the
    set of weights that minimizes the loss function. This is a high-dimensional optimization
    problem, and there are many points (sets of weights) where the loss function has
    local minima. A suboptimal local minimum is a point where the loss is lower than
    for the nearby points but higher than the global minimum, which is the overall
    lowest possible loss. The training process can get stuck in such suboptimal local
    minima. It’s important to remember that the local minima problem exists even in
    convex loss functions due to the discrete representation that is a part of digital
    computation.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部最小值**: 训练神经网络的目的是找到使损失函数最小化的权重集。这是一个高维优化问题，存在许多点（权重集）的损失函数具有局部最小值。次优局部最小值是损失低于附近点的点，但高于全局最小值，全局最小值是整体可能的最小损失。训练过程可能会陷入这种次优局部最小值。重要的是要记住，即使是在凸损失函数中，由于数字计算中的一部分离散表示，局部最小值问题也存在。'
- en: '**Vanishing/exploding gradients**: This is a difficulty encountered, especially
    when training deep neural networks. The gradients of the loss function may become
    very small (vanish) or very large (explode) in deeper layers of the network during
    the backpropagation process. Vanishing gradients make it hard for the network
    to learn from the data because the weight updates become very small. Exploding
    gradients can cause the training process to fail because weight updates become
    too large, and the loss becomes undefined (e.g., NaN).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失/爆炸**：这是在训练深度神经网络时遇到的一个困难。在反向传播过程中，损失函数的梯度可能在网络的深层中变得非常小（消失）或非常大（爆炸）。梯度消失使得网络难以从数据中学习，因为权重更新变得非常小。梯度爆炸可能导致训练过程失败，因为权重更新变得过大，损失变得未定义（例如，NaN）。'
- en: '**Overfitting**: One of the common problems in training machine learning models
    is when our model is too complex, and we train it too much. In this case, the
    model learns even the noises in the training data and works very well on training
    data but poorly on the unseen test data.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：在训练机器学习模型时，一个常见的问题是我们的模型过于复杂，训练过度。在这种情况下，模型甚至学习了训练数据中的噪声，在训练数据上表现良好，但在未见过的测试数据上表现不佳。'
- en: '**Underfitting**: Conversely, underfitting occurs when the model is too simple
    and can’t capture the underlying structure of the data. Both overfitting and underfitting
    can be mitigated by using proper model complexity, regularization techniques,
    and a sufficient amount of training data.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：相反，当模型过于简单，无法捕捉数据的潜在结构时，就会发生欠拟合。通过使用适当的模型复杂度、正则化技术和足够数量的训练数据，可以减轻过拟合和欠拟合。'
- en: '**Computational resources**: Training neural networks, particularly deep networks,
    requires significant computational resources (CPU/GPU power and memory). They
    also often require a large amount of training data to perform well, which can
    be a problem when such data are not available.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**：训练神经网络，尤其是深度网络，需要大量的计算资源（CPU/GPU功率和内存）。它们通常还需要大量的训练数据才能表现良好，当这些数据不可用时，这可能成为一个问题。'
- en: '**Lack of interpretability**: While not strictly a training issue, the lack
    of interpretability of neural networks is a significant problem. They are often
    referred to as “black boxes” because it is challenging to understand why they
    are making the predictions they do.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏可解释性**：虽然这不是一个严格意义上的训练问题，但神经网络缺乏可解释性是一个重大问题。它们通常被称为“黑盒”，因为很难理解它们为什么会做出这样的预测。'
- en: '**Difficulty in selecting appropriate architecture and hyperparameters**: There
    are many types of neural network architectures to choose from (such as CNN and
    RNN), and each has a set of hyperparameters that need to be tuned (such as learning
    rate, batch size, number of layers, and number of units per layer). Selecting
    the best architecture and tuning these hyperparameters for a given problem can
    be a challenging and time-consuming task.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择适当架构和超参数的困难**：有众多类型的神经网络架构可供选择（例如CNN和RNN），每种架构都有一组需要调整的超参数（例如学习率、批量大小、层数和每层的单元数）。为特定问题选择最佳架构并调整这些超参数可能是一个具有挑战性和耗时的工作。'
- en: '**Data preprocessing**: Neural networks often require the input data to be
    in a specific format. For instance, data might need to be normalized, categorical
    variables might need to be one-hot encoded, and missing values might need to be
    imputed. This preprocessing can be a complex and time-consuming step.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：神经网络通常需要输入数据以特定格式。例如，数据可能需要归一化，分类变量可能需要独热编码，缺失值可能需要填充。这个预处理步骤可能既复杂又耗时。'
- en: These challenges make training neural networks a non-trivial task, often requiring
    a combination of technical expertise, computational resources, and trial and error.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战使得训练神经网络成为一个非平凡的任务，通常需要技术专长、计算资源和试错法的结合。
- en: Language models
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型
- en: A language model is a statistical model in NLP that is designed to learn and
    understand the structure of human language. More specifically, it is a probabilistic
    model that is trained to estimate the likelihood of words when provided with a
    given word scenario. For instance, a language model could be trained to predict
    the next word in a sentence, given the previous words.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是NLP中的一种统计模型，旨在学习和理解人类语言的结构。更具体地说，它是一种概率模型，经过训练可以估计在给定一个单词场景的情况下单词的可能性。例如，语言模型可以被训练来预测句子中的下一个单词，给定前面的单词。
- en: Language models are fundamental to many NLP tasks. They are used in machine
    translation, speech recognition, part-of-speech tagging, and named entity recognition,
    among other things. More recently, they have been used to create conversational
    AI models such as chatbots and personal assistants and to generate human-like
    text.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是许多NLP任务的基础。它们被用于机器翻译、语音识别、词性标注和命名实体识别等任务。最近，它们还被用来创建对话式AI模型，如聊天机器人和个人助手，以及生成类似人类的文本。
- en: Traditional language models were often based on explicitly statistical methods,
    such as n-gram models, which consider only the previous n words when predicting
    the next word, or **hidden Markov** **models** (**HMMs**).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的语言模型通常基于显式的统计方法，例如n-gram模型，这些模型在预测下一个词时只考虑前n个词，或者**隐藏马尔可夫模型**（HMMs）。
- en: More recently, neural networks have become popular for creating language models,
    leading to the rise of neural language models. These models use the power of neural
    networks to consider the context of each word when making predictions, resulting
    in higher accuracy and fluency. Examples of neural language models include RNNs,
    the transformer model, and various transformer-based architectures such as BERT
    and GPT.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，神经网络在创建语言模型方面变得流行，导致了神经语言模型的兴起。这些模型利用神经网络的强大功能来考虑每个词的上下文，从而实现更高的准确性和流畅性。神经语言模型的例子包括RNN、Transformer模型以及各种基于Transformer的架构，如BERT和GPT。
- en: Language models are essential for understanding, generating, and interpreting
    human language in a computational setting, and they play a vital role in many
    applications of NLP.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型在计算环境中理解、生成和解释人类语言是必不可少的，它们在许多自然语言处理（NLP）的应用中扮演着至关重要的角色。
- en: 'Here are several motivations for using language models:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用语言模型的一些动机：
- en: '**Machine translation**: Language models are a crucial component in systems
    that translate text from one language to another. They can assess the fluency
    of translated sentences and help choose between multiple possible translations.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：语言模型是翻译系统中的一个关键组件，它们可以评估翻译句子的流畅性，并帮助在多种可能的翻译中选择。'
- en: '**Speech recognition**: Language models are used in speech recognition systems
    to help distinguish between words and phrases that sound similar. By predicting
    what word is likely to come next in a sentence, they can improve the accuracy
    of transcription.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：语言模型在语音识别系统中被用来帮助区分听起来相似的单词和短语。通过预测句子中可能出现的下一个词，它们可以提高转录的准确性。'
- en: '**Information retrieval**: When you search for something on the internet, language
    models help to determine what documents are relevant to your query. They can understand
    the semantic similarity between your search terms and potential results.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索**：当你在互联网上搜索某物时，语言模型有助于确定哪些文档与你的查询相关。它们可以理解你的搜索词与潜在结果之间的语义相似性。'
- en: '**Text generation**: Language models can generate human-like text, which is
    useful in various applications such as chatbots, writing assistants, and content
    creation tools. For example, a chatbot can use a language model to generate appropriate
    responses to user queries.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：语言模型可以生成类似人类的文本，这在聊天机器人、写作助手和内容创作工具等应用中非常有用。例如，聊天机器人可以使用语言模型来生成对用户查询的适当响应。'
- en: '**Sentiment analysis**: By understanding the structure of language, language
    models can help determine whether the sentiment of a piece of text is positive,
    negative, or neutral. This is useful in areas such as social media monitoring,
    product reviews, and customer feedback.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：通过理解语言结构，语言模型可以帮助判断一段文本的情感是积极、消极还是中性。这在社交媒体监控、产品评论和客户反馈等领域非常有用。'
- en: '**Grammar checking**: Language models can predict what word should come next
    in a sentence, which can help identify grammatical errors or awkward phrasing.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法检查**：语言模型可以预测句子中下一个词应该是什么，这有助于识别语法错误或表达不当。'
- en: '**Named entity recognition**: Language models can help identify named entities
    in text, such as people, organizations, locations, and more. This can be useful
    for tasks such as information extraction and automated summarization.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**：语言模型可以帮助识别文本中的命名实体，如人名、组织、地点等。这对于信息提取和自动摘要等任务非常有用。'
- en: '**Understanding context**: Language models, especially recent models based
    on **DL**, such as transformers, are excellent at understanding the context of
    words and sentences. This capability is vital for many **NLP** tasks, such as
    question answering, summarization, and dialogue systems.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解上下文**：语言模型，尤其是基于**深度学习（DL**）的最近模型，如transformers，在理解单词和句子的上下文方面非常出色。这种能力对于许多**自然语言处理（NLP**）任务至关重要，例如问答、摘要和对话系统。'
- en: 'All these motivations stem from a central theme: language models help machines
    understand and generate human language more effectively, which is crucial for
    many applications in today’s data-driven world.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些动机都源于一个中心主题：语言模型帮助机器更有效地理解和生成人类语言，这在当今数据驱动的世界中对于许多应用至关重要。
- en: In the following section, we introduce the different types of learning and then
    explain how one can use self-supervised learning to train language models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍不同类型的学习，然后解释如何使用自监督学习来训练语言模型。
- en: Semi-supervised learning
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Semi-supervised learning is a type of ML approach that utilizes both labeled
    and unlabeled data for training. It is particularly useful when you have a small
    amount of labeled data and a large amount of unlabeled data. The strategy here
    is to use the labeled data to train an initial model and then use this model to
    predict labels for the unlabeled data. The model is then retrained using the newly
    labeled data, improving its accuracy in the process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习是一种机器学习方法，它利用标记数据和未标记数据来训练。当你只有少量标记数据而大量未标记数据时，这种方法特别有用。这里的策略是使用标记数据来训练一个初始模型，然后使用这个模型来预测未标记数据的标签。然后，使用新标记的数据重新训练模型，从而提高其准确性。
- en: Unsupervised learning
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning, on the other hand, involves training models entirely
    on unlabeled data. The goal here is to find underlying patterns or structures
    in the data. Unsupervised learning includes techniques such as clustering (where
    the aim is to group similar instances together) and dimensionality reduction (where
    the aim is to simplify the data without losing too much information).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，无监督学习涉及完全基于未标记数据进行模型训练。这里的目的是在数据中找到潜在的规律或结构。无监督学习包括诸如聚类（目的是将相似实例分组在一起）和降维（目的是简化数据而不丢失太多信息）等技术。
- en: Using self-supervised learning to train language models
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自监督学习来训练语言模型
- en: Self-supervised learning is a form of unsupervised learning where the data provides
    the supervision. In other words, the model learns to predict certain parts of
    the input data from other parts of the same input data. It does not require explicit
    labels provided by humans, hence the term “self-supervised.”
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是一种无监督学习形式，其中数据提供监督。换句话说，模型学会从同一输入数据的其他部分预测输入数据的一部分。它不需要人类提供的显式标签，因此称为“自监督”。
- en: In the context of language models, self-supervision is typically implemented
    by predicting parts of a sentence when given other parts. For example, given the
    sentence “The cat is on the __,” the model would be trained to predict the missing
    word (“mat,” in this case).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型的背景下，自监督通常通过预测句子的一部分来实现，当给出其他部分时。例如，给定句子“The cat is on the __”，模型将被训练来预测缺失的单词（在这种情况下是“mat”）。
- en: Let’s look at a couple of popular self-supervised learning strategies for training
    language models next.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一些流行的自监督学习策略，用于训练语言模型。
- en: Masked language modeling (MLM)
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩码语言模型（MLM）
- en: This strategy, used in the training of BERT, randomly masks some percentage
    of the input tokens and tasks the model with predicting the masked words based
    on the context provided by the unmasked words. For instance, in the sentence “The
    cat is on the mat,” we could mask “cat,” and the model’s job would be to predict
    this word. Please note that more than one word can also be masked.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略在BERT的训练中使用，随机掩盖输入标记的一部分，并要求模型根据未掩盖的词提供的上下文预测掩盖的单词。例如，在句子“The cat is on
    the mat”中，我们可以掩盖“cat”，而模型的任务就是预测这个单词。请注意，也可以掩盖多个单词。
- en: 'Mathematically, the objective of an MLM is to maximize the following likelihood:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，多标签学习（MLM）的目标是最大化以下似然函数：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/303.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/303.png)'
- en: where *w*_i is a masked word, *w*_{-i} are the non-masked words, and *θ* represents
    the model parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w*_i 是一个掩码词，*w*_{-i} 是非掩码词，而 *θ* 代表模型参数。
- en: Autoregressive language modeling
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自回归语言模型
- en: In autoregressive language modeling, which is used in models such as GPT, the
    model predicts the next word in a sentence given all the preceding words. It’s
    trained to maximize the likelihood of a word given its previous words in the sentence.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在自回归语言模型中，如 GPT 模型所使用的，模型根据句子中所有前面的词预测句子中的下一个词。它被训练以最大化给定句子中先前词的词的概率。
- en: The objective of an autoregressive language model is to maximize
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归语言模型的目标是最大化
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>log</mi><mfenced
    open="(" close=")"><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/304.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>log</mi><mfenced
    open="(" close=")"><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/304.png)'
- en: where *w_*i is the current word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/305.png)
    are the previous words, and *θ* represents the model parameters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w_*i 是当前词，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/305.png)
    是前面的词，而 *θ* 代表模型参数。
- en: These strategies enable language models to obtain a rich understanding of language
    syntax and semantics directly from raw text without the need for explicit labels.
    The models can then be fine-tuned for various tasks such as text classification,
    sentiment analysis, and more, leveraging the language understanding gained from
    the self-supervised pretraining phase.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略使语言模型能够直接从原始文本中获得对语言语法和语义的丰富理解，而无需显式标签。然后，这些模型可以通过各种任务进行微调，例如文本分类、情感分析等，利用从自监督预训练阶段获得的语言理解。
- en: Transfer learning
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Transfer learning is an ML technique where a pretrained model is reused as the
    starting point for a different but related problem. Compared to traditional ML
    approaches, where you start with initializing your model with random weights,
    transfer learning has the advantage of kick-starting the learning process from
    patterns that have been learned from a related task, which can both speed up the
    training process and improve the performance of the model, especially when you
    have limited labeled training data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种机器学习技术，其中预训练模型被重新用作不同但相关问题的起点。与传统机器学习方法相比，传统方法是从随机权重初始化模型开始，迁移学习有从相关任务学习到的模式中启动学习过程的优势，这既可以加快训练过程，也可以提高模型的性能，尤其是在你有有限的标记训练数据时。
- en: In transfer learning, a model is typically trained on a large-scale task, and
    then parts of the model are used as a starting point for another task. The large-scale
    task is often chosen to be broad enough that the learned representations are useful
    for many different tasks. This process works particularly well when the input
    data for both tasks are of the same type and the tasks are related.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，一个模型通常是在一个大规模任务上训练的，然后使用模型的一部分作为另一个任务的起点。这个大规模任务通常选择得足够广泛，以至于学习到的表示对许多不同的任务都有用。当两个任务的输入数据类型相同且任务相关时，这个过程特别有效。
- en: There are several ways to apply transfer learning, and the best approach can
    depend on how much data you have for your task and how similar your task is to
    the original task the model was trained on.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 应用迁移学习有几种方法，最佳方法可能取决于你任务的数据量以及你的任务与模型训练的原任务相似程度。
- en: Feature extraction
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取
- en: The pretrained model acts as a feature extractor. You remove the last layer
    or several layers of the model, leaving the rest of the network intact. Then,
    you pass your data through this truncated model and use the output as input to
    a new, smaller model that is trained for your specific task.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型充当特征提取器。你移除模型的最后一层或几层，保留网络的其他部分。然后，你将数据通过这个截断的模型传递，并使用输出作为训练用于你特定任务的新、更小模型输入。
- en: Fine-tuning
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: You use the pretrained model as a starting point and update all or some of the
    model’s parameters for your new task. In other words, you continue the training
    where it left off, allowing the model to adjust from generic feature extraction
    to features more specific to your task. Often, a lower learning rate is used during
    fine-tuning to avoid overwriting the prelearned features entirely during training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用预训练模型作为起点，并更新模型的所有或部分参数以适应你的新任务。换句话说，你继续从上次停止的地方训练，允许模型从通用的特征提取调整到更具体于你任务的特性。在微调期间，通常使用较低的学习率，以避免在训练过程中完全覆盖预学习的特征。
- en: Transfer learning is a powerful technique that can be used to improve the performance
    of ML models. It is particularly useful for tasks where there are limited labeled
    data available. It is commonly used in DL applications. For instance, it’s almost
    a standard in image classification problems where pretrained models on ImageNet,
    a large-scale annotated image dataset (ResNet, VGG, Inception, and so on), are
    used as the starting point. The features learned by these models are generic for
    image classification and can be fine-tuned on a specific image classification
    task with a smaller amount of data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种强大的技术，可以用来提高机器学习模型的性能。它在有少量标记数据可用的任务中特别有用。它通常在深度学习应用中使用。例如，在图像分类问题中，几乎已经成为标准，即使用在ImageNet（一个大规模标注图像数据集）上预训练的模型（ResNet、VGG、Inception等）作为起点。这些模型学习到的特征对图像分类来说是通用的，并且可以在包含更少数据的具体图像分类任务上进行微调。
- en: 'Here are some examples of how transfer learning can be used:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些迁移学习应用的例子：
- en: A model trained to classify images of cats and dogs can be used to fine-tune
    a model to classify images of other animals, such as birds or fish
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于分类猫和狗图像的模型可以被用来微调一个模型，以分类其他动物的图像，如鸟类或鱼类。
- en: A model trained to translate text from English to Spanish can be used to fine-tune
    a model to translate text from Spanish to French
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于将英语翻译成西班牙语的文本模型可以被用来微调一个模型，以将西班牙语翻译成法语。
- en: A model trained to predict the price of a house can be used to fine-tune a model
    to predict the price of a car
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于预测房价的模型可以被用来微调一个模型，以预测汽车的价格。
- en: Similarly, in natural language processing, large pretrained models, such as
    BERT or GPT, are often used as the starting point for a wide range of tasks. These
    models are pretrained on a large corpus of text and learn a rich representation
    of language that can be fine-tuned for specific tasks such as text classification,
    sentiment analysis, question answering, and more.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在自然语言处理中，大型预训练模型，如BERT或GPT，通常被用作广泛任务（如文本分类、情感分析、问答等）的起点。这些模型在大量文本语料库上预训练，并学习到丰富的语言表示，可以针对特定任务进行微调。
- en: Understanding transformers
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Transformer
- en: Transformers are a type of neural network architecture that was introduced in
    a paper called *Attention is All You Need* by Ashish Vaswani, Noam Shazeer, Niki
    Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia
    Polosukhin (*Advances in neural information processing systems 30* (2017), Harvard).
    They have been very influential in the field of NLP and have formed the basis
    for state-of-the-art models such as BERT and GPT.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种由Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N. Gomez、Łukasz Kaiser和Illia Polosukhin在论文《Attention is All You Need》中提出的神经网络架构（*Advances
    in neural information processing systems 30* (2017), Harvard）。它们在NLP领域产生了深远的影响，并成为BERT和GPT等最先进模型的基础。
- en: The key innovation in transformers is the self-attention mechanism, which allows
    the model to weigh the relevance of each word in the input when producing an output,
    thereby considering the context of each word. This is unlike previous models such
    as RNNs or RNNs, which process the input sequentially and, therefore, have a harder
    time capturing the long-range dependencies between words.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的关键创新是自注意力机制，它允许模型在生成输出时权衡输入中每个词的相关性，从而考虑每个词的上下文。这与之前的模型（如RNN或LSTM）不同，这些模型按顺序处理输入，因此更难捕捉词之间的长距离依赖关系。
- en: Architecture of transformers
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer的架构
- en: 'A transformer is composed of an encoder and a decoder, both of which are made
    up of several identical layers, as shown in *Figure 6**.8*. Each layer in the
    encoder contains two sub-layers: a self-attention mechanism and a position-wise
    fully connected feedforward network. A residual connection is employed around
    each of the two sub-layers, followed by layer normalization:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer由一个编码器和解码器组成，它们都由几个相同的层组成，如图*图6.8*所示。编码器中的每一层包含两个子层：一个自注意力机制和一个位置感知的全连接前馈网络。在每个子层周围都采用了残差连接，然后是层归一化：
- en: '![Figure 6.8 – Self-attention mechanism](img/B18949_06_008.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 自注意力机制](img/B18949_06_008.jpg)'
- en: Figure 6.8 – Self-attention mechanism
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 自注意力机制
- en: Similarly, each layer in the decoder has three sub-layers. The first is a self-attention
    layer, the second is a cross-attention layer that attends to the output of the
    encoder stack, and the third is a position-wise fully connected feedforward network.
    Like the encoder, each of these sub-layers has a residual connection around it,
    followed by layer normalization. Please note that in the figure, just one head
    is being shown, and we can have multiple heads working in parallel (*N* heads).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，解码器中的每一层也有三个子层。第一个是自注意力层，第二个是跨注意力层，它关注编码器堆栈的输出，第三个是位置感知的全连接前馈网络。与编码器一样，这些子层周围都有残差连接，然后是层归一化。请注意，在图中只显示了一个头，我们可以有多个头并行工作（*N*个头）。
- en: Self-attention mechanism
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力机制
- en: The self-attention mechanism, or scaled dot-product attention, calculates the
    relevance of each word in the sequence to the current word being processed. The
    input to the self-attention layer is a sequence of word embeddings, each of which
    is split into a **query** (*Q*), a **key** (*K*), and a **value** (*V*) using
    separately learned linear transformations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制，或缩放点积注意力，计算序列中每个词对当前正在处理的词的相关性。自注意力层的输入是一个词嵌入序列，每个嵌入通过分别学习的线性变换被分割成一个**查询**（*Q*）、一个**键**（*K*）和一个**值**（*V*）。
- en: 'The attention score for each word is then calculated as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词的注意力分数计算如下：
- en: "![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>Q</mi><mi>K</mi><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mi mathvariant=\"normal\">T</mi><mo>/</mo><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mo>(</mo><mi>d</mi><mo>_</mo><mi\
    \ mathvariant=\"normal\">k</mi><mo>)</mo><mo>)</mo><mi>V</mi></mrow></mrow></mrow></math>](img/306.png)"
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: "![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>Q</mi><mi>K</mi><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mi mathvariant=\"normal\">T</mi><mo>/</mo><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mo>(</mo><mi>d</mi><mo>_</mo><mi\
    \ mathvariant=\"normal\">k</mi><mo>)</mo><mo>)</mo><mi>V</mi></mrow></mrow></mrow></math>](img/306.png)"
- en: Where *d_k* is the dimensionality of the queries and keys, which is used to
    scale the dot product to prevent it from growing too large. The softmax operation
    ensures that the attention scores are normalized and sum to 1\. These scores represent
    the weight given to each word’s value when producing the output for the current
    word.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*d_k*是查询和键的维度，用于缩放点积以防止其变得过大。softmax操作确保注意力分数是归一化的并且总和为1。这些分数代表在产生当前单词的输出时，对每个单词值的赋予的权重。
- en: The output of the self-attention layer is a new sequence of vectors, where the
    output for each word is a weighted sum of all the input values, with the weights
    determined by the attention scores.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层的输出是一个新的向量序列，其中每个单词的输出是所有输入值的加权总和，权重由注意力分数决定。
- en: Positional encoding
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: Since the self-attention mechanism does not take into account the position of
    the words in the sequence, the transformer adds a positional encoding to the input
    embeddings at the bottom of the encoder and decoder stacks. This encoding is a
    fixed function of the position and allows the model to learn to use the order
    of the words.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自注意力机制没有考虑序列中单词的位置，因此Transformer在编码器和解码器堆栈的底部添加了位置编码到输入嵌入中。这种编码是位置的固定函数，允许模型学习使用单词的顺序。
- en: In the original transformer paper, positional encoding is a sinusoidal function
    of the position and the dimension, although learned positional encodings have
    also been used effectively.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的Transformer论文中，位置编码是位置和维度的正弦函数，尽管也有效地使用了学习位置编码。
- en: Applications of transformers
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer的应用
- en: Since their introduction, transformers have been used to achieve state-of-the-art
    results on a wide range of NLP tasks, including machine translation, text summarization,
    sentiment analysis, and more. They have also been adapted for other domains, such
    as computer vision and reinforcement learning.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 自从它们被引入以来，Transformer已被用于在包括机器翻译、文本摘要、情感分析等在内的广泛NLP任务上实现最先进的结果。它们也被应用于其他领域，如计算机视觉和强化学习。
- en: The introduction of transformers has led to a shift in the NLP field towards
    pretraining large transformer models on a large corpus of text and then fine-tuning
    them on specific tasks, which is an effective form of transfer learning. This
    approach has been used in models such as BERT, GPT-2, GPT-3, and GPT-4.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的引入导致了NLP领域向在大型文本语料库上预训练大型Transformer模型，然后在特定任务上进行微调的转变，这是一种有效的迁移学习方法。这种方法已被用于BERT、GPT-2、GPT-3和GPT-4等模型。
- en: Learning more about large language models
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多关于大型语言模型
- en: Large language models are a class of ML models that have been trained on a broad
    range of internet text.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是一类在广泛互联网文本上训练的机器学习模型。
- en: The term “large” in “large language models” refers to the number of parameters
    that these models have. For example, GPT-3 has 175 billion parameters. These models
    are trained using self-supervised learning on a large corpus of text, which means
    they predict the next word in a sentence (such as GPT) or a word based on surrounding
    words (such as BERT, which is also trained to predict whether a pair of sentences
    is sequential). Because they are exposed to such a large amount of text, these
    models learn grammar, facts about the world, reasoning abilities, and also biases
    in the data they’re trained on.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: “大型语言模型”中的“大型”一词指的是这些模型所拥有的参数数量。例如，GPT-3有1750亿个参数。这些模型使用自监督学习在大量的文本语料库上进行训练，这意味着它们预测句子中的下一个单词（例如GPT）或基于周围单词的单词（例如BERT，它也被训练来预测一对句子是否连续）。由于它们接触到了如此大量的文本，这些模型学习了语法、关于世界的知识、推理能力，以及它们在训练数据中存在的偏见。
- en: These models are transformer-based, meaning they leverage the transformer architecture,
    which uses self-attention mechanisms to weigh the importance of words in input
    data. This architecture allows these models to process long-range dependencies
    in text, making them very effective for a wide range of NLP tasks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型基于转换器架构，这意味着它们利用了转换器架构，该架构使用自注意力机制来衡量输入数据中单词的重要性。这种架构使得这些模型能够处理文本中的长距离依赖关系，使它们在广泛的自然语言处理任务中非常有效。
- en: Large language models can be fine-tuned on specific tasks to achieve high performance.
    Fine-tuning involves additional training on a smaller, task-specific dataset and
    allows the model to adapt its general language understanding abilities to the
    specifics of the task. This approach has been used to achieve state-of-the-art
    results on many NLP benchmarks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型可以通过在特定任务上进行微调来实现高性能。微调涉及在较小的、特定任务的语料库上进行额外的训练，这使得模型能够将其通用的语言理解能力适应到任务的特定细节。这种方法已被用于在许多自然语言处理基准测试中实现最先进的结果。
- en: While large language models have demonstrated impressive abilities, they also
    raise important challenges. For example, because they’re trained on internet text,
    they can reproduce and amplify biases present in the data. They can also generate
    outputs that are harmful or misleading. Additionally, due to their size, these
    models require significant computational resources to train and deploy, which
    raises issues around cost and environmental impact.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型展示了令人印象深刻的能力，但它们也提出了重要的挑战。例如，由于它们是在互联网文本上训练的，它们可以复制和放大数据中存在的偏见。它们还可以生成有害或误导性的输出。此外，由于它们的规模，这些模型在训练和部署时需要大量的计算资源，这引发了成本和环境影响的问题。
- en: Despite these challenges, large language models represent a significant advance
    in the field of AI and are a powerful tool for a wide range of applications, including
    translation, summarization, content creation, question answering, and more.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，大型语言模型在人工智能领域代表了重大进步，并且是广泛应用于翻译、摘要、内容创作、问答等众多领域的强大工具。
- en: The challenges of training language models
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练语言模型的挑战
- en: 'Training large language models is a complex and resource-intensive task that
    poses several challenges. Here are some of the key issues:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型是一项复杂且资源密集型的工作，面临着诸多挑战。以下是其中一些关键问题：
- en: '**Computational resources**: The training of large language models requires
    substantial computational resources. These models have billions of parameters
    that need to be updated during training, which involves performing a large amount
    of computation over an extensive dataset. This computation is usually carried
    out on high-performance GPUs or **tensor processing units** (**TPUs**), and the
    costs associated can be prohibitive.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**：大型语言模型的训练需要大量的计算资源。这些模型拥有数十亿个参数，需要在训练过程中进行更新，这涉及到在庞大的数据集上执行大量的计算。这种计算通常在高性能GPU或**张量处理单元**（**TPUs**）上执行，相关的成本可能非常昂贵。'
- en: '**Memory limitations**: As the size of the model increases, the amount of memory
    required to store the model parameters, intermediate activations, and gradients
    during training also increases. This can lead to memory issues on even the most
    advanced hardware. Techniques such as model parallelism, gradient checkpointing,
    and offloading can be used to mitigate these issues, but they add complexity to
    the training process.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存限制**：随着模型大小的增加，存储模型参数、训练过程中的中间激活和梯度的内存需求也增加。这可能导致即使在最先进的硬件上也会出现内存问题。可以使用模型并行化、梯度检查点和卸载等技术来减轻这些问题，但它们会增加训练过程的复杂性。'
- en: '**Dataset size and quality**: Large language models are trained on extensive
    text corpora. Finding, cleaning, and structurally organizing such massive datasets
    can be challenging. Moreover, the quality of the dataset directly impacts the
    performance of the model. Since these models learn from the data they’re trained
    on, biases or errors in the data can lead to a biased or error-prone model.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集大小和质量**：大型语言模型是在广泛的文本语料库上训练的。找到、清理和结构化组织如此庞大的数据集可能具有挑战性。此外，数据集的质量直接影响模型的表现。由于这些模型从它们训练的数据中学习，数据中的偏差或错误可能导致偏差或易出错的模型。'
- en: '**Overfitting**: While large models have a high capacity to learn complex patterns,
    they can also be overfitted to the training data, especially when the amount of
    available data is limited compared to the size of the model. Overfitting leads
    to poor generalization of unseen data. Regularization techniques, such as weight
    decay, dropout, and early stopping, can be used to combat overfitting.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：虽然大型模型具有学习复杂模式的高容量，但它们也可能过度拟合训练数据，尤其是在可用数据量与模型大小相比有限的情况下。过拟合会导致未见数据泛化能力差。可以使用正则化技术，如权重衰减、dropout和提前停止，来对抗过拟合。'
- en: '**Training stability**: As models get larger, stably training them becomes
    more difficult. The challenges include managing learning rates and batch sizes
    and dealing with issues such as vanishing or exploding gradients.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练稳定性**：随着模型变大，稳定训练它们变得更加困难。挑战包括管理学习率和批量大小，以及处理梯度消失或爆炸等问题。'
- en: '**Evaluation and fine-tuning**: Evaluating the performance of these models
    can also be challenging due to their size. Moreover, fine-tuning these models
    on a specific task can be tricky, as it can lead to “catastrophic forgetting,”
    where the model forgets the pretraining knowledge.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估和微调**：由于这些模型的大小，评估它们的性能也可能具有挑战性。此外，在特定任务上对这些模型进行微调可能很棘手，因为这可能导致“灾难性遗忘”，即模型忘记了预训练的知识。'
- en: '**Ethical and safety concerns**: Large language models can generate content
    that is harmful or inappropriate. They can also propagate and amplify biases present
    in the training data. These issues necessitate the development of robust methods
    to control the behavior of the model, both during training and at runtime.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理和安全问题**：大型语言模型可以生成有害或不适当的内容。它们还可以传播和放大训练数据中存在的偏见。这些问题需要开发强大的方法来控制模型的行为，无论是在训练期间还是在运行时。'
- en: Despite these challenges, progress continues in the field of large language
    models. Researchers are developing new strategies to mitigate these issues and
    to train large models more effectively and responsibly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，大型语言模型领域仍在继续取得进展。研究人员正在开发新的策略来减轻这些问题，并更有效地、负责任地训练大型模型。
- en: Specific designs of language models
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型的特定设计
- en: Here, we are going to explain two popular architectures of language models,
    BERT and GPT, in detail.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将详细解释两种流行的语言模型架构，BERT和GPT。
- en: BERT
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT
- en: 'BERT, which we mentioned already and will now expand on, is a transformer-based
    ML technique for NLP tasks. It was developed by Google and introduced in a paper
    by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova titled *Bert:
    Pre-training of deep bidirectional transformers for language understanding*, arXiv
    preprint arXiv:1810.04805 (2018).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '我们之前提到的BERT，现在将进一步展开介绍，它是一种基于转换器的NLP任务机器学习技术。由谷歌开发，并在Jacob Devlin、Ming-Wei
    Chang、Kenton Lee和Kristina Toutanova合著的论文《Bert: Pre-training of deep bidirectional
    transformers for language understanding》，arXiv预印本arXiv:1810.04805（2018）中提出。'
- en: BERT is designed to pretrain deep bidirectional representations from the unlabeled
    text by joint conditioning on both left and right contexts in all layers. This
    is in contrast to previous methods, such as GPT and ELMo, which pretrain text
    representations from only the left context or from left and right contexts separately.
    This bi-directionality allows BERT to understand the context and the semantic
    meaning of a word more accurately.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的设计是为了通过在所有层中对左右上下文进行联合条件预训练，从未标记的文本中预训练深度双向表示。这与之前的方法，如GPT和ELMo，不同，它们只从左上下文或分别从左右上下文中预训练文本表示。这种双向性允许BERT更准确地理解上下文和单词的语义意义。
- en: BERT’s design
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BERT的设计
- en: BERT is based on the transformer model architecture, which is shown in *Figure
    6**.8*, originally introduced by Vaswani et al. in the paper *Attention is All
    You Need*. The model architecture consists of stacked self-attention and point-wise
    fully connected layers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: BERT基于Transformer模型架构，如*图6*所示，最初由Vaswani等人发表在论文*Attention is All You Need*中提出。模型架构由堆叠的自注意力层和逐点全连接层组成。
- en: 'BERT comes in two sizes: **BERT Base** and **BERT Large**. BERT Base is composed
    of 12 transformer layers, each with 12 self-attention heads, and a total of 110
    million parameters. BERT Large is much bigger and has 24 transformer layers, each
    with 16 self-attention heads, for a total of 340 million parameters.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: BERT有两种大小：**BERT Base**和**BERT Large**。BERT Base由12个Transformer层组成，每个层有12个自注意力头，总共有1100万个参数。BERT
    Large更大，有24个Transformer层，每个层有16个自注意力头，总共有3400万个参数。
- en: 'BERT’s training process involves two steps: **pretraining** and **fine-tuning**.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的训练过程包括两个步骤：**预训练**和**微调**。
- en: The very first step in training or using a language model is to create or load
    its dictionary. We usually use a tokenizer to achieve this goal.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 训练或使用语言模型的第一步是创建或加载其字典。我们通常使用分词器来实现这一目标。
- en: Tokenizer
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分词器
- en: In order to use the language models efficiently, we need to use a tokenizer
    that converts the input text into a limited number of tokens. Subword tokenization
    algorithms, such as **byte pair encoding** (**BPE**), **unigram language model**
    (**ULM**), and **WordPiece**, split words into smaller subword units. This is
    useful for handling out-of-vocabulary words and allows the model to learn meaningful
    representations for subword parts that often carry semantic meaning.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地使用语言模型，我们需要使用一个分词器，将输入文本转换为有限数量的标记。子词分词算法，如**字节对编码**（**BPE**）、**一元语言模型**（**ULM**）和**WordPiece**，将单词分割成更小的子词单元。这对于处理词汇表外的单词很有用，并允许模型学习对子词部分的有意义表示，这些部分通常携带语义意义。
- en: The BERT tokenizer is a critical component of the BERT model, performing the
    initial preprocessing of text data necessary for input into the model. BERT uses
    WordPiece tokenization, a subword tokenization algorithm that breaks words into
    smaller parts, allowing BERT to handle out-of-vocabulary words, reduce the size
    of the vocabulary, and deal with the richness and diversity of languages.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: BERT分词器是BERT模型的一个关键组件，执行模型输入所需文本数据的初始预处理。BERT使用WordPiece分词，这是一种子词分词算法，将单词分解成更小的部分，允许BERT处理词汇表外的单词，减少词汇表的大小，并处理语言的丰富性和多样性。
- en: 'Here’s a detailed breakdown of how the BERT tokenizer works:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是BERT分词器工作原理的详细分解：
- en: '**Basic tokenization**: First, the BERT tokenizer performs basic tokenization,
    breaking text into individual words by splitting on whitespace and punctuation.
    This is similar to what you might find in other tokenization methods.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基本分词**：首先，BERT分词器执行基本分词，通过在空白和标点符号处分割文本来将文本分解成单个单词。这与其他分词方法中可能找到的方法类似。'
- en: '**WordPiece tokenization**: After basic tokenization, the BERT tokenizer applies
    WordPiece tokenization. This step breaks words into smaller subword units or “WordPieces.”
    If a word isn’t in the BERT vocabulary, the tokenizer will iteratively break the
    word down into smaller sub words until it finds a match in the vocabulary or until
    it has to resort to character-level representation.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**WordPiece分词**：在基本分词之后，BERT分词器应用WordPiece分词。这一步将单词分解成更小的子词单元或“WordPieces”。如果一个单词不在BERT词汇表中，分词器将迭代地将单词分解成更小的子词，直到在词汇表中找到匹配项，或者直到不得不求助于字符级表示。'
- en: 'For example, the word “unhappiness” might be broken down into two WordPieces:
    “un” and “##happiness”. The “##” symbol is used to denote sub-words that are part
    of a larger word and not a whole word on their own.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，单词“unhappiness”可能被分解成两个WordPieces：“un”和“##happiness”。符号“##”用于表示是更大单词的一部分的子词，而不是一个完整的单词。
- en: '**Special tokens addition**: The **BERT** tokenizer then adds special tokens
    necessary for specific **BERT** functionalities. The [**CLS**] token is appended
    at the beginning of each sentence, serving as an aggregate representation for
    classification tasks. The [**SEP**] token is added at the end of each sentence
    to signify sentence boundaries. If two sentences are inputted (for tasks that
    require sentence pairs), they are separated by this [**SEP**] token.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特殊标记添加**：BERT分词器随后添加了必要的特殊标记，以支持特定的**BERT**功能。每个句子的开头都附加了[**CLS**]标记，作为分类任务的聚合表示。每个句子的结尾添加了[**SEP**]标记，以表示句子边界。如果输入了两个句子（对于需要句子对的任务），它们将通过这个[**SEP**]标记分开。'
- en: '**Token to ID conversion**: Finally, each token is mapped to an integer ID
    corresponding to its index in the **BERT** vocabulary. These IDs are what the
    **BERT** model actually uses as input.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标记到ID的转换**：最后，每个标记都被映射到一个整数ID，这个ID对应于它在**BERT**词汇表中的索引。这些ID是**BERT**模型实际使用的输入。'
- en: So, in summary, the BERT tokenizer works by first tokenizing the text into words,
    then further breaking these words down into WordPieces (if necessary), adding
    special tokens, and finally converting these tokens into IDs. This process allows
    the model to understand and generate meaningful representations for a wide variety
    of words and sub-words, contributing to BERT’s powerful performance on various
    NLP tasks.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，BERT分词器的工作原理是首先将文本分词成单词，然后（如果需要）进一步将这些单词分解成WordPieces，添加特殊标记，最后将这些标记转换成ID。这个过程使得模型能够理解和生成对各种单词和子词的有意义的表现，从而有助于BERT在多种NLP任务上表现出强大的性能。
- en: Pretraining
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预训练
- en: 'During pretraining, **BERT** was trained on a large corpus of text (the entire
    English Wikipedia and BooksCorpus are used in the original paper). The model was
    trained to predict masked words in a sentence (masked language model) and to distinguish
    whether two sentences come in order in the text (next sentence prediction), as
    explained here:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，**BERT**在大量的文本语料库上进行了训练（原始论文中使用了整个英文维基百科和BooksCorpus）。模型被训练去预测句子中的掩码单词（掩码语言模型），以及区分文本中两个句子是否按顺序出现（下一句预测），如这里所解释的：
- en: '**Masked language model**: In this task, 15% of the words in a sentence are
    replaced by a [**MASK**] token, and the model is trained to predict the original
    word from the context provided by the non-masked words.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩码语言模型**：在这个任务中，一个句子中的15%的单词被一个[**MASK**]标记所替换，模型被训练去预测由非掩码单词提供的上下文中的原始单词。'
- en: '**Next sentence prediction**: When the model is given a pair of two sentences,
    it is also trained to predict whether sentence *B* is the next sentence following
    sentence *A*.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一句预测**：当模型被给出一对两个句子时，它也被训练去预测句子B是否是句子A之后的下一句。'
- en: Fine-tuning
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调
- en: After pretraining, BERT can be fine-tuned on a specific task with a significantly
    smaller amount of training data. Fine-tuning involves adding an additional output
    layer to BERT and training the entire model end-to-end on the specific task. This
    approach has been shown to achieve state-of-the-art results on a wide range of
    NLP tasks, including question answering, named entity recognition, sentiment analysis,
    and more.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练后，BERT可以在具有显著较少训练数据的具体任务上进行微调。微调涉及向BERT添加一个额外的输出层，并在特定任务上端到端地训练整个模型。这种方法已经在包括问答、命名实体识别、情感分析等多种NLP任务上实现了最先进的结果。
- en: BERT’s design and its pretraining/fine-tuning approach revolutionized the field
    of NLP and have led to a shift toward training large models on a broad range of
    data and then fine-tuning them on specific tasks.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的设计及其预训练/微调方法彻底改变了NLP领域，并导致了在广泛的数据上训练大型模型，然后在特定任务上进行微调的趋势。
- en: How to fine-tune BERT for text classification
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何微调BERT进行文本分类
- en: 'As mentioned, BERT has been pretrained on a large corpus of text data, and
    the learned representations can be fine-tuned for specific tasks, including text
    classification. Here is a step-by-step process on how to fine-tune BERT for text
    classification:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，BERT已经在大量的文本数据集上进行了预训练，学习到的表示可以针对特定任务进行微调，包括文本分类。以下是如何逐步微调BERT进行文本分类的步骤：
- en: '**Preprocessing input data**: BERT requires a specific format for input data.
    The sentences need to be tokenized into sub-words using BERT’s own tokenizer,
    and special tokens such as [CLS] (classification) and [SEP] (separation) need
    to be added. The [CLS] token is added at the beginning of each example and is
    used as the aggregate sequence representation for classification tasks. The [SEP]
    token is added at the end of each sentence to denote sentence boundaries. All
    sequences are then padded to a fixed length to form a uniform input.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理输入数据**：BERT需要特定的输入数据格式。句子需要使用BERT自己的分词器将句子分词成子词，并添加特殊标记，如[CLS]（分类）和[SEP]（分隔）。[CLS]标记添加在每个示例的开头，用作分类任务的聚合序列表示。[SEP]标记添加在每个句子的末尾，以表示句子边界。然后，所有序列都填充到固定长度，以形成一个统一的输入。'
- en: '**Loading the pretrained BERT model**: BERT has several pretrained models,
    and the right one should be chosen based on the task at hand. The models differ
    in terms of the size of the model and the language of the pretraining data. Once
    the pretrained BERT model is loaded, it can be used to create contextualized word
    embeddings for the input data.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载预训练BERT模型**：BERT有多个预训练模型，应根据任务选择正确的模型。这些模型在模型大小和预训练数据的语言方面有所不同。一旦加载预训练BERT模型，就可以用它为输入数据创建上下文化的词嵌入。'
- en: '**Adding a classification layer**: A classification layer, also known as the
    classification head, is added on top of the pretrained BERT model. This layer
    will be trained to make predictions for the text classification task. Usually,
    this layer is a fully connected neural network layer that takes the representation
    corresponding to the [CLS] token as input and outputs the probability distribution
    over the classes.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加分类层**：在预训练BERT模型之上添加一个分类层，也称为分类头。这个层将被训练以对文本分类任务进行预测。通常，这个层是一个全连接神经网络层，它以对应于[CLS]标记的表示作为输入，并输出类别的概率分布。'
- en: '**Fine-tuning the model**: Fine-tuning involves training the model on the specific
    task (in this case, text classification) using the labeled data. This process
    can be done in multiple ways. The more common approach is to update the weights
    of the pretrained BERT model and the newly added classification layer to minimize
    a loss function, typically the cross-entropy loss for classification tasks. It
    is important to use a lower learning rate during fine-tuning, as larger rates
    can destabilize the prelearned weights. Additionally, the number of recommended
    epochs is two to four, so the model learns the task but does not overfit. The
    benefit of this approach is that the model weights will be adjusted to perform
    well on specific tasks. Alternatively, we can freeze BERT layers and just update
    the classifier layer weights.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调模型**：微调涉及使用标记数据在特定任务（在这种情况下，文本分类）上训练模型。这个过程可以通过多种方式完成。更常见的方法是更新预训练BERT模型和新添加的分类层权重，以最小化损失函数，通常是分类任务的交叉熵损失。在微调期间使用较低的学习率很重要，因为较大的学习率可能会使预学习的权重不稳定。此外，建议的epoch数通常是两到四个，这样模型就能学习任务但不会过拟合。这种方法的好处是模型权重将被调整以在特定任务上表现良好。或者，我们可以冻结BERT层，只更新分类层权重。'
- en: '**Evaluating the model**: Once the model has been fine-tuned, it can be evaluated
    on a validation set to assess its performance. This involves calculating metrics
    such as accuracy, precision, recall, and F1 score. During the training and evaluation
    task, similar to other ML and DL models, we can perform hyperparameter tuning.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：一旦模型经过微调，就可以在验证集上评估其性能，以评估其性能。这包括计算准确率、精确率、召回率和F1分数等指标。在训练和评估任务期间，与其他ML和DL模型类似，我们可以执行超参数调整。'
- en: '**Applying the model**: The fine-tuned model can now be used to make predictions
    on new, unseen text data. As with the training data, this new data also need to
    be preprocessed into the format that BERT expects.'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用模型**：经过微调的模型现在可以用于对新、未见过的文本数据进行预测。与训练数据一样，这些新数据也需要预处理成BERT期望的格式。'
- en: Important note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: Note that working with **BERT** requires considerable computational resources,
    as the model has a large number of parameters. A GPU is typically recommended
    for fine-tuning and applying BERT models. There are some models that are lighter
    than BERT with slightly lower performance, such as DistilBERT, that we can use
    in the case of being constrained by the computation or memory resources. Additionally,
    BERT is able to process 512 tokens, which limits the length of our input text.
    If we want to process longer text, Longformer or BigBird are good choices. What
    we explained here works for similar language models such as RoBERTa, XLNet, and
    so on.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与**BERT**一起工作需要相当的计算资源，因为该模型具有大量的参数。通常建议使用GPU进行微调和应用BERT模型。有一些模型比BERT轻量，性能略低，例如DistilBERT，在计算或内存资源受限的情况下可以使用。此外，BERT能够处理512个标记，这限制了我们的输入文本长度。如果我们想处理更长的文本，Longformer或BigBird是不错的选择。这里所解释的内容适用于类似的语言模型，如RoBERTa、XLNet等。
- en: In summary, fine-tuning BERT for text classification involves preprocessing
    the input data, loading the pretrained BERT model, adding a classification layer,
    fine-tuning the model on the labeled data, and then evaluating and applying the
    model.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对BERT进行文本分类的微调涉及预处理输入数据，加载预训练的BERT模型，添加分类层，在标记数据上微调模型，然后评估和应用模型。
- en: We will demonstrate the preceding paradigm of fine-tuning BERT and then apply
    it at the end of this chapter. You will have the opportunity to employ it firsthand
    and adjust it to your needs.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示先前的BERT微调范式，然后在本章末尾应用它。您将有机会亲自使用它并调整以满足您的需求。
- en: GPT-3
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-3
- en: '**GPT-3**, short for **generative** **pretrained transformer 3**, is an autoregressive
    language model developed by OpenAI that uses DL techniques to generate human-like
    text. It is the third version of the GPT series. The GPT versions that followed
    it, GPT-3.5 and GPT-4, will be covered in the next chapter, as we will expand
    on large language models.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3**，即**生成****预训练Transformer 3**，是由OpenAI开发的一种自回归语言模型，它使用深度学习技术生成类似人类的文本。它是GPT系列的第三个版本。GPT系列的后续版本，GPT-3.5和GPT-4，将在下一章中介绍，因为我们将扩展关于大型语言模型的内容。'
- en: Design and architecture of GPT-3
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-3的设计和架构
- en: GPT-3 extends the transformer model architecture used by its predecessors. The
    architecture is based on a transformer model that uses layers of transformer blocks,
    where each block is composed of self-attention and feedforward neural network
    layers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的架构扩展了其前辈使用的transformer模型架构。该架构基于一个使用Transformer块的Transformer模型，其中每个块由自注意力层和前馈神经网络层组成。
- en: GPT-3 is massive compared to the previous versions. It consists of 175 billion
    ML parameters. These parameters are learned during the training phase, where the
    model learns to predict the next word in a sequence of words.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的版本相比，GPT-3规模庞大。它由1750亿个ML参数组成。这些参数在训练阶段学习，模型学习预测一系列单词中的下一个单词。
- en: GPT-3’s transformer model is designed to process sequences of data (in this
    case, sequences of words or tokens in text), making it well-suited for language
    tasks. It processes input data sequentially from left to right and generates predictions
    for the next item in the sequence. This is the difference between BERT and GPT,
    where, in BERT, words from both sides are used to predict masked words, but in
    GPT, just the previous words are used for prediction, which makes it a good choice
    for generative tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的Transformer模型旨在处理数据序列（在这种情况下，文本中的单词或标记序列），使其非常适合语言任务。它从左到右顺序处理输入数据，并为序列中的下一个项目生成预测。这是BERT和GPT之间的区别，在BERT中，使用两侧的单词来预测掩码词，但在GPT中，仅使用前面的单词进行预测，这使得它成为生成任务的不错选择。
- en: Pretraining and fine-tuning
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预训练和微调
- en: 'Similar to BERT and other transformer-based models, GPT-3 also involves a two-step
    process: **pretraining** and **fine-tuning**.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT和其他基于Transformer的模型类似，GPT-3也涉及两个步骤的过程：**预训练**和**微调**。
- en: Pretraining
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预训练
- en: In this phase, GPT-3 is trained on a large corpus of text data. It learns to
    predict the next word in a sentence. However, unlike BERT, which uses a bidirectional
    context for prediction, GPT-3 only uses the left context (i.e., the previous words
    in the sentence).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，GPT-3在大量的文本数据语料库上进行训练。它学习预测句子中的下一个单词。然而，与BERT使用双向上下文进行预测不同，GPT-3仅使用左侧上下文（即句子中的前面的单词）。
- en: Fine-tuning
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调
- en: After the pretraining phase, GPT-3 can be fine-tuned on a specific task using
    a smaller amount of task-specific training data. This could be any NLP task, such
    as text completion, translation, summarization, question answering, and so on.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段之后，GPT-3 可以使用较少的任务特定训练数据在特定任务上进行微调。这可以是任何 NLP 任务，例如文本补全、翻译、摘要、问答等。
- en: Zero-shot, one-shot, and few-shot learning
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零样本、一样本和少样本学习
- en: One of the impressive features of GPT-3 is its capability to perform few-shot
    learning. When given a task and a few examples of that task, GPT-3 can often learn
    to perform the task accurately.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 令人印象深刻的特点之一是其执行少样本学习的能力。当被赋予一个任务和该任务的几个示例时，GPT-3 通常能够准确地学习执行该任务。
- en: In the zero-shot setting, the model is given a task without any prior examples.
    In the one-shot setting, it’s given one example, and in the few-shot setting,
    it’s given a few examples to learn from.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在零样本设置中，模型被赋予一个任务而没有任何先前的示例。在一样本设置中，它被赋予一个示例，在少样本设置中，它被赋予几个示例来学习。
- en: Challenges of using GPT-3
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-3 的挑战
- en: Despite its impressive capabilities, GPT-3 also presents some challenges. Due
    to its large size, it requires substantial computational resources to train. It
    can sometimes generate incorrect or nonsensical responses, and it can reflect
    biases present in the training data. It also struggles with tasks that require
    a deep understanding of the world or common sense reasoning beyond what can be
    learned from text.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-3拥有令人印象深刻的性能，但也存在一些挑战。由于其规模庞大，它需要大量的计算资源来训练。它有时会生成不正确或无意义的响应，并且可能会反映训练数据中存在的偏差。它还难以处理需要深入理解世界或超越从文本中学习到的常识推理的任务。
- en: Reviewing our use case – ML/DL system design for NLP classification in a Jupyter
    Notebook
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾我们的用例 - 在 Jupyter Notebook 中为 NLP 分类设计的 ML/DL 系统设计
- en: In this section, we are going to work on a real-world problem and see how we
    can use an NLP pipeline to solve it. The code for this part is shared as a Google
    Colab notebook at [Ch6_Text_Classification_DL.ipynb](https://colab.research.google.com/drive/1HVD2fvxHup6OsPi2mKxNS_nfCRZ0iGCw?usp=sharing).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理一个现实世界的问题，并看看我们如何可以使用 NLP 管道来解决它。这部分代码以 Google Colab 笔记本的形式共享，网址为
    [Ch6_Text_Classification_DL.ipynb](https://colab.research.google.com/drive/1HVD2fvxHup6OsPi2mKxNS_nfCRZ0iGCw?usp=sharing)。
- en: The business objective
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务目标
- en: In this scenario, we are in the healthcare sector. Our objective is to develop
    a general medical knowledge engine that is very up to date with recent findings
    in the world of healthcare.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们处于医疗保健行业。我们的目标是开发一个与医疗保健领域最新发现非常同步的通用医学知识引擎。
- en: The technical objective
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术目标
- en: 'The CTO derives several technical objectives from the business objective. One
    objective is for the ML team: given the growing collection of conclusions that
    correspond to medical publications, identify the ones that represent advice. This
    will allow us to identify the medical advice that stems from the underlying research.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 技术总监从业务目标中推导出几个技术目标。其中一个目标是针对 ML 团队：鉴于与医学出版物相对应的结论集合不断增长，识别出代表建议的结论。这将使我们能够识别出源于基础研究的医学建议。
- en: The pipeline
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: 'Let’s review the parts of the pipeline, as depicted in *Figure 6**.9*:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下管道的各个部分，如图 *图 6**.9* 所示：
- en: "![Figure 6.9 – The structure of a typical exploration and model pipeline\uFEFF\
    ](img/B18949_06_009.jpg)"
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 典型探索和模型管道的结构](img/B18949_06_009.jpg)'
- en: Figure 6.9 – The structure of a typical exploration and model pipeline
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 典型探索和模型管道的结构
- en: Notice how this design is different from the design we saw in *Figure 5**.2*.
    There, the exploration and evaluation parts leverage the same feature engineering
    technique that is later used by the ML models. Here, with LMs, feature engineering
    is not a part of the preparation for the modeling. The pretrained model, and particularly
    the tokenizer, performs feature engineering, which yields very different and less
    interpretable features than the binary, BoW, or TF-IDF features.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个设计与我们在 *图 5**.2* 中看到的设计有何不同。在那里，探索和评估部分利用了后来由 ML 模型使用的相同特征工程技术。在这里，使用 LMs，特征工程不是建模准备的一部分。预训练模型，特别是分词器，执行特征工程，这产生了与二进制、BoW
    或 TF-IDF 特征非常不同且难以解释的特征。
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Code parts: From “Settings” through “Generating Results of the Traditional
    ML Models.”'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 代码部分：从“设置”到“生成传统 ML 模型的结果”。
- en: These parts are identical in their nature to the analog parts discussed in [*Chapter
    5*](B18949_05_split_000.xhtml#_idTextAnchor130). The only differences relate to
    the differences in the data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分在本质上与第[*第5章*](B18949_05_split_000.xhtml#_idTextAnchor130)中讨论的模拟部分相同。唯一的区别与数据的不同有关。
- en: Deep learning
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习
- en: In this part of the code, we employ a deep learning language model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用了一个深度学习语言模型。
- en: When looking to apply transfer learning via LMs and fine-tuning them per our
    objective and data, there are several stacks to choose from. The ones that stand
    out the most are Google’s TensorFlow, and Meta’s PyTorch. A package called **Transformers**
    was built as a wrapper around these stacks to allow for a simpler implementation
    of the code. In this example, we leverage the simplicity and richness of transformers
    models.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑通过LM应用迁移学习并针对我们的目标和数据进行微调时，有多个堆栈可供选择。最突出的是Google的TensorFlow和Meta的PyTorch。一个名为**Transformers**的包被构建为这些堆栈的包装器，以允许代码的更简单实现。在这个例子中，我们利用了transformers模型的简洁性和丰富性。
- en: 'It is worth highlighting the company that built and supports the Transformers
    package: Hugging Face. Hugging Face took it upon themselves to create an entire
    ecosystem around the collection and sharing of free, open source DL models, which
    includes the many components that accommodate for implementing these models. The
    most actionable tool is the Transformers package, which is a Python package dedicated
    to picking, importing, training, and employing a large and growing set of DL models.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是构建并支持Transformers包的公司：Hugging Face。Hugging Face承担起创建一个围绕免费、开源DL模型收集和共享的整个生态系统，这包括许多适应实现这些模型的组件。最实用的工具是Transformers包，这是一个Python包，致力于选择、导入、训练和部署一个庞大且不断增长的DL模型集合。
- en: The code we are reviewing here provides more than just an example of ML/DL system
    design in the real world; it also showcases Hugging Face’s Transformers.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里审查的代码不仅提供了一个现实世界中的ML/DL系统设计的示例；还展示了Hugging Face的Transformers。
- en: Formatting the data
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据格式化
- en: Here, we set the data up in a format that suits the Transformers library. The
    column names must be very specific.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据设置成适合Transformers库的格式。列名必须非常具体。
- en: Evaluation metric
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: We decided which metric we wished to optimize and plugged it into the training
    process. For this problem of binary classification, we optimized for accuracy
    and evaluated our result in comparison to the dataset’s baseline accuracy, also
    known as the prior.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定要优化的指标，并将其插入到训练过程中。对于这个二元分类问题，我们优化了准确率，并将我们的结果与数据集的基线准确率进行了比较，也称为先验。
- en: Trainer object
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练器对象
- en: 'This is the core object for training the LM in Transformers. It holds a set
    of predefined configurations. Some of the key training configurations are the
    following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于在Transformers中训练LM的核心对象。它包含一组预定义的配置。一些关键的训练配置如下：
- en: 'The neural net’s mathematical learning hyperparameters, such the following:'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的数学学习超参数，例如以下：
- en: The learning rate
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: The gradient decent settings
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降设置
- en: The number of training epochs
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮数
- en: The computation hardware usage
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算硬件使用情况
- en: Logging setting for capturing the progression of the objective metric throughout
    the training process
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中记录目标指标进度的日志设置
- en: Fine-tuning the neural network parameters
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调神经网络参数
- en: The fundamental concept around fine-tuning LMs is transfer learning. Neural
    networks lend themselves so well to transfer learning because one can simply strip
    any number of layers from the end of the structure and replace them with untrained
    layers that would be trained based on the underlying problem. The rest of the
    layers that weren’t removed and aren’t trained continue to operate exactly in
    the same way they did when the LM was originally trained (when it was originally
    built). If we replace the last layer but leave the rest of the original layers,
    then we could view those layers as supervised feature engineering or, conversely,
    as an embedding mechanism. This trait reflects the concept of transfer learning.
    Ideally, the model is expected to lend itself well to our underlying problem so
    that we will choose to keep the vast majority of the original layers, and only
    a small minority would be replaced and trained. In this way, a large DL model
    that took many weeks to be pretrained can be transferred and adapted to a new
    problem in minutes.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 关于微调语言模型的基本概念是迁移学习。神经网络非常适合迁移学习，因为可以从结构的末端简单地剥离任意数量的层，并用未训练的层替换它们，这些层将基于底层问题进行训练。未移除且未训练的其余层将继续以与语言模型最初训练时（当它最初构建时）完全相同的方式运行。如果我们替换最后一层但保留其余的原始层，那么我们可以将这些层视为监督特征工程，或者相反，作为嵌入机制。这种特性反映了迁移学习的概念。理想情况下，模型预计将很好地适应我们的底层问题，因此我们将选择保留绝大多数的原始层，只有一小部分会被替换并训练。这样，一个需要几周时间预训练的大型深度学习模型可以在几分钟内迁移和适应新的问题。
- en: In our code, we set the model up in a way that we dictate exactly which of its
    layers we are looking to fine-tune. It is a design choice for us for this to be
    based on performance and also computation resources. One choice is to fine-tune
    the last layer right before the final output, also known as the classification
    head. The alternative is to fine-tune all the layers. In our code, we explicitly
    call the model’s configuration, which controls which layer is fine-tuned, so the
    code can be changed in any way that suits the design.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，我们以这种方式设置模型，即我们确切地指定我们想要微调其哪些层。这是我们的设计选择，基于性能和计算资源。一个选择是微调紧接最终输出的最后一层，也称为分类头。另一种选择是微调所有层。在我们的代码中，我们明确调用模型的配置，该配置控制哪些层被微调，因此代码可以以任何适合设计的方式更改。
- en: We configure the trainer to log the performance of the training in real time.
    It prints those logs out for us in a table so we can observe and monitor them.
    When the training is complete, we plot the progress of the training and the evaluation.
    This helps us see the relation between the evolution of the training results and
    the evaluation results. Since the evaluation set that the trainer uses can be
    viewed as a held-out set in the context of the trainer, this plot allows us to
    investigate underfitting and overfitting.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练器配置为实时记录训练性能。它将这些日志以表格形式打印出来，以便我们观察和监控。当训练完成后，我们绘制训练和评估的进度。这有助于我们了解训练结果和评估结果之间的关联。由于训练器使用的评估集可以被视为训练器上下文中的保留集，这个图表使我们能够研究欠拟合和过拟合。
- en: Generating the training results – used for design choices
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成训练结果——用于设计选择
- en: We reviewed the results of the training set, along with the logs that the trainer
    printed out. We compared them to the baseline accuracy and observed an increase
    in accuracy. We learned about the quality of our design by iterating over several
    different design choices and comparing them. That process of iterating over many
    sets of design parameters would be automated into code to allow for a systematic
    evaluation of the optimal setting. We didn’t do that in our notebook just to keep
    things simple in the example. Once we believed we had found the optimal setting,
    we could say that the process was finished.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了训练集的结果，以及训练器打印出的日志。我们将它们与基线准确率进行了比较，并观察到准确率的提高。通过迭代多个不同的设计选择并进行比较，我们了解了我们设计的质量。将这个过程自动化为代码，以便对最佳设置进行系统评估。我们只是在笔记本中这样做，以保持示例的简单性。一旦我们相信我们已经找到了最佳设置，我们就可以说这个过程已经完成。
- en: Generating the testing results – used for presenting performance
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成测试结果——用于展示性能
- en: As with the code in [*Chapter 5*](B18949_05_split_000.xhtml#_idTextAnchor130),
    here, too, we finished by reviewing the test results. It is worth noting the difference
    between the evaluation set and the test set. One could suggest that since the
    trainer doesn’t use the evaluation set for training, it could be used as a held-out
    test set, thus saving the need to exclude so many observations from training and
    supplying the model with more labeled data. However, while the trainer didn’t
    use the evaluation set, we did use it to make our design decisions. For instance,
    we observed the plot from the preceding section and judged which number of epochs
    is optimal to achieve optimal fitting. In [*Chapter 5*](B18949_05_split_000.xhtml#_idTextAnchor130),
    an evaluation set was used too, but we didn’t need to explicitly define it; it
    was carried out as a part of the K-fold cross-validation mechanism.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 [*第 5 章*](B18949_05_split_000.xhtml#_idTextAnchor130) 中的代码一样，在这里，我们也以回顾测试结果结束。值得注意的是评估集和测试集之间的差异。有人可能会建议，由于训练者没有使用评估集进行训练，它可以作为一个保留的测试集，从而节省了从训练中排除许多观察结果的需要，并为模型提供更多标记数据。然而，尽管训练者没有使用评估集，我们确实使用了它来做出我们的设计决策。例如，我们观察了前一个部分的图表，并判断哪个数量的周期数是达到最佳拟合的最优选择。在
    [*第 5 章*](B18949_05_split_000.xhtml#_idTextAnchor130) 中也使用了评估集，但我们不需要明确定义它；它是作为
    K 折交叉验证机制的一部分执行的。
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this enlightening chapter, we embarked on a comprehensive exploration of
    DL and its remarkable application to text classification tasks through language
    models. We began with an overview of DL, revealing its profound ability to learn
    complex patterns from vast amounts of data and its indisputable role in advancing
    state-of-the-art NLP systems.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一启发性的章节中，我们全面探索了深度学习及其在文本分类任务中通过语言模型应用的显著应用。我们从深度学习的概述开始，揭示了其从大量数据中学习复杂模式的能力，以及在推进最先进自然语言处理系统中的无可争议的作用。
- en: We then delved into the transformative world of transformer models, which have
    revolutionized NLP by providing an effective alternative to traditional RNNs and
    CNNs for processing sequence data. By unpacking the attention mechanism—a key
    feature in transformers—we highlighted its capacity to focus on different parts
    of the input sequence, hence facilitating a better understanding of context.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入到变换器模型的变革性世界，这些模型通过提供传统 RNN 和 CNN 处理序列数据的有效替代方案，已经彻底改变了自然语言处理。通过解开注意力机制——变换器中的一个关键特性——我们突出了其专注于输入序列不同部分的能力，从而促进了上下文理解的更好。
- en: Our journey continued with an in-depth exploration of the BERT model. We detailed
    its architecture, emphasizing its pioneering use of bidirectional training to
    generate contextually rich word embeddings, and we highlighted its pretraining
    process, which learns language semantics from a large text corpus.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程继续深入探索 BERT 模型。我们详细介绍了其架构，强调其开创性地使用双向训练来生成语境丰富的词嵌入，并突出了其预训练过程，该过程从大量文本语料库中学习语言语义。
- en: However, our exploration did not end there; we also introduced GPT, another
    transformative model that leverages the power of transformers in a slightly different
    way—focusing on generating human-like text. By comparing BERT and GPT, we shed
    light on their distinct strengths and use cases.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的探索并未就此结束；我们还介绍了 GPT，另一个利用变换器能力以略有不同方式变革的模型——专注于生成类似人类的文本。通过比较 BERT 和 GPT，我们阐明了它们的独特优势和用例。
- en: The chapter culminated in a practical guide on how to design and implement a
    text classification model using these advanced models. We walked you through all
    the stages of this process, from data preprocessing and model configuration to
    training, evaluation, and finally, making predictions on unseen data.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以一个关于如何使用这些高级模型设计和实现文本分类模型的实用指南结束。我们引导您经历了这个过程的各个阶段，从数据预处理和模型配置到训练、评估，最后是在未见过的数据上做出预测。
- en: In essence, this chapter provided a well-rounded understanding of DL in NLP,
    transitioning from fundamental principles to hands-on applications. With this
    knowledge, you are now equipped to leverage the capabilities of transformer models,
    BERT, and GPT for your text classification tasks. Whether you are looking to delve
    further into the world of NLP or apply these skills in a practical setting, this
    chapter has equipped you with a firm foundation on which to build.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，本章提供了对自然语言处理中深度学习的全面理解，从基本原理过渡到实际应用。凭借这些知识，您现在可以充分利用变压器模型、BERT和GPT的能力来处理您的文本分类任务。无论您是想进一步深入研究自然语言处理的世界，还是在实际环境中应用这些技能，本章都为您奠定了坚实的基础。
- en: In this chapter, we introduced you to large language models. In the next chapter,
    we dive deeper into these models to learn more about them.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了大型语言模型。在下一章中，我们将更深入地探讨这些模型，以了解更多关于它们的信息。
