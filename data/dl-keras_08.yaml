- en: AI Game Playing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 游戏玩法
- en: In previous chapters, we looked at supervised learning techniques such as regression
    and classification, and unsupervised learning techniques such as GANs, autoencoders
    and generative models. In the case of supervised learning, we train the network
    with the expected input and output and expect it to predict the output given a
    new input. In the case of unsupervised learning, we show the network some input
    and expect it to learn the structure of the data so that it can apply this knowledge
    to a new input.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们研究了监督学习技术，如回归和分类，以及无监督学习技术，如生成对抗网络（GANs）、自编码器和生成模型。在监督学习的情况下，我们使用预期的输入和输出训练网络，并希望它在面对新的输入时能够预测正确的输出。在无监督学习的情况下，我们向网络展示一些输入，期望它学习数据的结构，以便能够将这一知识应用于新的输入。
- en: In this chapter, we will learn about reinforcement learning, or more specifically
    deep reinforcement learning, that is, the application of deep neural networks
    to reinforcement learning. Reinforcement learning has its roots in behavioral
    psychology. An agent is trained by rewarding it for correct behavior and punishing
    it for incorrect behavior. In the context of deep reinforcement learning, a network
    is shown some input and is given a positive or negative reward based on whether
    it produces the correct output from that input. Thus, in reinforcement learning,
    we have sparse and time-delayed labels. Over many iterations, the network learns
    to produce the correct output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习强化学习，或者更具体地说是深度强化学习，即将深度神经网络应用于强化学习。强化学习源于行为心理学。通过奖励正确的行为和惩罚错误的行为来训练代理。在深度强化学习的背景下，网络展示某些输入，并根据其从该输入中产生的输出是否正确给予正面或负面的奖励。因此，在强化学习中，我们有稀疏和时延的标签。经过多次迭代，网络学习产生正确的输出。
- en: 'The pioneer in the deep reinforcement learning space was a small British company
    called DeepMind, which in 2013 published a paper (for more information refer to:
    *Playing Atari with Deep Reinforcement Learning*, by V. Mnih, arXiv:1312.5602,
    2013.) describing how a **convolutional neural network** (**CNN**) could be taught
    to play Atari 2600 video games by showing it screen pixels and giving it a reward
    when the score increases. The same architecture was used to learn seven different
    Atari 2600 games, in six of which the model outperformed all previous approaches,
    and it outperformed a human expert in three.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习领域的先驱是一家名为DeepMind的小型英国公司，该公司于2013年发表了一篇论文（更多信息请参见：*通过深度强化学习玩Atari游戏*，V.
    Mnih，arXiv:1312.5602，2013年），描述了如何通过显示屏幕像素并在分数增加时给予奖励，训练一个**卷积神经网络**（**CNN**）玩Atari
    2600电子游戏。相同的架构被用于学习七种不同的Atari 2600游戏，其中六个游戏的模型超越了所有之前的方法，并且在三个游戏中超越了人类专家。
- en: 'Unlike the learning strategies we learned about previously, where each network
    learns about a single discipline, reinforcement learning seems to be a general
    learning algorithm that can be applied to a variety of environments; it may even
    be the first step to general artificial intelligence. DeepMind has since been
    acquired by Google, and the group has been on the forefront of AI research. A
    subsequent paper (for more information refer to: *Human-Level Control through
    Deep Reinforcement Learning*, by V. Mnih, Nature 518.7540, 2015: 529-533.) was
    featured in the prestigious Nature journal in 2015, where they applied the same
    model to 49 different games.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前学习的策略不同，在这些策略中每个网络都学习单一的学科，强化学习似乎是一种可以应用于各种环境的通用学习算法；它甚至可能是通用人工智能的第一步。DeepMind后来被谷歌收购，并且该团队一直处于AI研究的前沿。随后的一篇论文（更多信息请参见：*通过深度强化学习实现人类级控制*，V.
    Mnih，Nature 518.7540，2015年：529-533。）于2015年在著名的《自然》期刊上发表，他们将相同的模型应用于49种不同的游戏。
- en: In this chapter, we will explore the theoretical framework that underlies deep
    reinforcement learning. We'll then apply this framework to build a network using
    Keras that learns to play a game of catch. We'll briefly look at some ideas that
    can make this network better as well as some promising new areas of research in
    this space.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索支撑深度强化学习的理论框架。然后，我们将应用这一框架使用Keras构建一个网络，学习玩接球游戏。我们还将简要看看可以使这个网络更好的几个想法以及这个领域中的一些有前景的新研究方向。
- en: 'To sum up, we will learn the following core concepts around reinforcement learning
    in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在本章中，我们将学习关于强化学习的以下核心概念：
- en: Q-learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning
- en: Exploration versus exploitation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索与利用
- en: Experience replay
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验回放
- en: Reinforcement learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Our objective is to build a neural network to play the game of catch. Each
    game starts with a ball being dropped from a random position from the top of the
    screen. The objective is to move a paddle at the bottom of the screen using the
    left and right arrow keys to catch the ball by the time it reaches the bottom.
    As games go, this is quite simple. At any point in time, the state of this game
    is given by the *(x, y)* coordinates of the ball and paddle. Most arcade games
    tend to have many more moving parts, so a general solution is to provide the entire
    current game screen image as the state. The following screenshot shows four consecutive
    screenshots of our catch game:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个神经网络来玩接球游戏。每场游戏开始时，球会从屏幕顶部的一个随机位置掉落。目标是在球到达底部之前，使用左右箭头键移动底部的球拍接住球。就游戏而言，这相当简单。在任何时刻，这场游戏的状态由球和球拍的*(x,
    y)*坐标给出。大多数街机游戏通常有更多的运动部件，因此一个通用的解决方案是将整个当前的游戏屏幕图像作为状态。下图展示了我们接球游戏的四个连续屏幕截图：
- en: '![](img/catch-frames-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/catch-frames-1.png)'
- en: Astute readers might note that our problem could be modeled as a classification
    problem, where the input to the network are the game screen images and the output
    is one of three actions--move left, stay, or move right. However, this would require
    us to provide the network with training examples, possibly from recordings of
    games played by experts. An alternative and simpler approach might be to build
    a network and have it play the game repeatedly, giving it feedback based on whether
    it succeeds in catching the ball or not. This approach is also more intuitive
    and is closer to the way humans and animals learn.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 精明的读者可能注意到，我们的问题可以被建模为一个分类问题，其中网络的输入是游戏屏幕图像，输出是三种动作之一——向左移动、保持不动或向右移动。然而，这需要我们提供训练示例，可能来自专家进行的游戏录制。另一种更简单的做法可能是构建一个网络，让它反复玩游戏，并根据它是否成功接住球来给它反馈。这种方法也更直观，且更接近人类和动物学习的方式。
- en: 'The most common way to represent such a problem is through a **markov decision
    process** (**MDP**). Our game is the environment within which the agent is trying
    to learn. The state of the environment at time step *t* is given by *s[t]* (and
    contains the location of the ball and paddle). The agent can perform certain actions
    (such as moving the paddle left or right). These actions can sometimes result
    in a reward *r[t]*, which can be positive or negative (such as an increase or
    decrease in the score). Actions change the environment and can lead to a new state
    *s[t+1]*, where the agent can perform another action *a[t+1]*, and so on. The
    set of states, actions and rewards, together with the rules for transitioning
    from one state to the other, make up a markov decision process. A single game
    is one episode of this process, and is represented by a finite sequence of states,
    actions, and rewards:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表示此类问题最常见的方法是通过**马尔可夫决策过程**（**MDP**）。我们的游戏是智能体试图学习的环境。在时间步* t *时，环境的状态由*s[t]*给出（并包含球和球拍的位置）。智能体可以执行某些动作（例如，向左或向右移动球拍）。这些动作有时会导致奖励*r[t]*，奖励可以是正的或负的（例如，得分增加或减少）。动作改变环境，可能导致新的状态*s[t+1]*，然后智能体可以执行另一个动作*a[t+1]*，依此类推。状态、动作和奖励的集合，以及从一个状态过渡到另一个状态的规则，构成了马尔可夫决策过程。单场游戏是这个过程的一个回合，表示为状态、动作和奖励的有限序列：
- en: '![](img/episode.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/episode.png)'
- en: Since, this is a markov decision process, the probability of state *s[t+1]*
    depends only on current state *s[t]* and action *a[t]*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个马尔可夫决策过程，状态*s[t+1]*的概率仅依赖于当前状态*s[t]*和动作*a[t]*。
- en: Maximizing future rewards
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大化未来奖励
- en: 'As an agent, our objective is to maximize the total reward from each game.
    The total reward can be represented as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个智能体，我们的目标是最大化每场游戏的总奖励。总奖励可以表示如下：
- en: '![](img/total_reward.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/total_reward.png)'
- en: 'In order to maximize the total reward, the agent should try to maximize the
    total reward from any time point *t* in the game. The total reward at time step
    *t* is given by *R[t]* and is represented as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化总奖励，智能体应该尽力从游戏中的任何时刻*t*开始，最大化总奖励。时间步*t*的总奖励由*R[t]*给出，并表示为：
- en: '![](img/reward_at_t.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/reward_at_t.png)'
- en: 'However, it is harder to predict the value of the rewards the further we go
    into the future. In order to take this into consideration, our agent should try
    to maximize the total discounted future reward at time *t* instead. This is done
    by discounting the reward at each future time step by a factor γ over the previous
    time step. If γ is *0*, then our network does not consider future rewards at all,
    and if γ is *1*, then our network is completely deterministic. A good value for
    γ is around *0.9*. Factoring the equation allows us to express the total discounted
    future reward at a given time step recursively as the sum of the current reward
    and the total discounted future reward at the next time step:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预测未来的奖励值越远就越困难。为了考虑这一点，我们的智能体应该尝试最大化时间*t*时的总折扣未来奖励。这是通过在每个未来时间步长中使用折扣因子γ来折扣奖励，从而实现的。如果γ为*0*，则我们的网络根本不考虑未来的奖励；如果γ为*1*，则我们的网络完全是确定性的。一个好的γ值大约是*0.9*。通过因式分解方程，我们可以递归地表达在给定时间步长的总折扣未来奖励，作为当前奖励与下一个时间步长的总折扣未来奖励之和：
- en: '![](img/reward_recursive.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/reward_recursive.png)'
- en: Q-learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习
- en: 'Deep reinforcement learning utilizes a model-free reinforcement learning technique
    called **Q-learning**. Q-learning can be used to find an optimal action for any
    given state in a finite markov decision process. Q-learning tries to maximize
    the value of the Q-function which represents the maximum discounted future reward
    when we perform action *a* in state *s*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习利用一种称为**Q学习**的无模型强化学习技术。Q学习可以用来在有限马尔可夫决策过程中为任何给定状态找到最优动作。Q学习尝试最大化Q函数的值，Q函数表示在状态*s*下执行动作*a*时，获得的最大折扣未来奖励：
- en: '![](img/qfunc.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/qfunc.png)'
- en: 'Once we know the Q-function, the optimal action *a* at a state *s* is the one
    with the highest Q-value. We can then define a policy *Ï€(s)* that gives us the
    optimal action at any state:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了Q函数，状态*s*下的最优动作*a*就是具有最高Q值的动作。然后我们可以定义一个策略*Ïπ(s)*，它可以为我们提供任何状态下的最优动作：
- en: '![](img/qpolicy.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/qpolicy.png)'
- en: 'We can define the Q-function for a transition point (*s[t]*, *a[t]*, *r[t]*, *s[t+1]*)
    in terms of the Q-function at the next point (*s[t+1]*, *a[t+1]*, *r[t+1]*, *s[t+2]*)
    similar to how we did with the total discounted future reward. This equation is
    known as the **Bellman equation**:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过贝尔曼方程的方式定义一个过渡点(*s[t]*, *a[t]*, *r[t]*, *s[t+1]*)的Q函数，类似于我们对总折扣未来奖励的处理。这个方程被称为**贝尔曼方程**：
- en: '![](img/bellman.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bellman.png)'
- en: 'The Q-function can be approximated using the Bellman equation. You can think
    of the Q-function as a lookup table (called a **Q-table**) where the states (denoted
    by *s*) are rows and actions (denoted by *a*) are columns, and the elements (denoted
    by *Q(s, a)*) are the rewards that you get if you are in the state given by the
    row and take the action given by the column. The best action to take at any state
    is the one with the highest reward. We start by randomly initializing the Q-table,
    then carry out random actions and observe the rewards to update the Q-table iteratively
    according to the following algorithm:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Q函数可以通过贝尔曼方程来近似。你可以将Q函数看作一个查找表（称为**Q表**），其中状态（由*s*表示）是行，动作（由*a*表示）是列，元素（由*Q(s,
    a)*表示）是你在给定行的状态下采取给定列的动作所获得的奖励。在任何状态下，最佳的动作是具有最高奖励的那个。我们从随机初始化Q表开始，然后执行随机动作并观察奖励，以便根据以下算法迭代更新Q表：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will realize that the algorithm is basically doing stochastic gradient descent
    on the Bellman equation, backpropagating the reward through the state space (or
    episode) and averaging over many trials (or epochs). Here α is the learning rate
    that determines how much of the difference between the previous Q-value and the
    discounted new maximum Q-value should be incorporated.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现这个算法本质上是在对贝尔曼方程做随机梯度下降，通过状态空间（或回合）反向传播奖励，并在多次试验（或周期）中取平均。在这里，α是学习率，决定了前一个Q值和折扣后的新最大Q值之间的差异应被纳入多少。
- en: The deep Q-network as a Q-function
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络作为Q函数
- en: 'We know that our Q-function is going to be a neural network, the natural question
    is: what kind? For our simple example game, each state is represented by four
    consecutive black and white screen images of size *(80, 80)*, so the total number
    of possible states (and the number of rows of our Q-table) is *2^(80x80x4)*. Fortunately,
    many of these states represent impossible or highly improbable pixel combinations.
    Since convolutional neural networks have local connectivity (that is, each neuron
    is connected to only a local region of its input), it avoids these impossible
    or improbable pixel combinations. In addition, neural networks are generally very
    good at coming up with good features for structured data such as images. Hence
    a CNN can be used to model a Q-function very effectively.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的Q函数将是一个神经网络，接下来自然会问：什么类型的神经网络呢？对于我们的简单示例游戏，每个状态由四张连续的黑白屏幕图像（大小为*(80,
    80)*）表示，因此可能的状态总数（也是Q表的行数）是*2^(80x80x4)*。幸运的是，这些状态中的许多代表了不可能或极不可能的像素组合。由于卷积神经网络具有局部连接性（即每个神经元仅与输入的局部区域相连），它避免了这些不可能或极不可能的像素组合。此外，神经网络通常非常擅长为结构化数据（如图像）提取有效特征。因此，CNN可以非常有效地用于建模Q函数。
- en: 'The DeepMind paper (for more information refer to: *Playing Atari with Deep
    Reinforcement Learning*, by V. Mnih, arXiv:1312.5602, 2013.), also uses three
    layers of convolutions followed by two fully connected layers. Unlike traditional
    CNNs used for image classification or recognition, there are no pooling layers.
    This is because pooling layers makes the network less sensitive to the location
    of specific objects in the image. In case of games this information is likely
    to be required to compute the reward, and thus cannot be discarded.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind论文（更多信息请参考：*Playing Atari with Deep Reinforcement Learning*，V. Mnih，arXiv:1312.5602，2013）也使用了三层卷积层，后面跟着两层全连接层。与传统用于图像分类或识别的CNN不同，该网络没有池化层。这是因为池化层使网络对图像中特定物体位置的敏感性降低，而在游戏中，这些信息可能是计算奖励时所需要的，因此不能被丢弃。
- en: 'The following diagram, shows the structure of the deep Q-network that is used
    for our example. It follows the same structure as the original DeepMind paper
    except for the input and output layer shapes. The shape for each of our inputs
    is *(80, 80, 4)*: four black and white consecutive screenshots of the game console,
    each *80* x *80* pixels in size. Our output shape is (*3*), corresponding to the
    Q-value for each of three possible actions (move left, stay, move right):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了用于我们示例的深度Q网络结构。它遵循原始DeepMind论文中的相同结构，唯一不同的是输入层和输出层的形状。我们每个输入的形状为*(80, 80,
    4)*：四张连续的黑白游戏控制台截图，每张图像大小为*80* x *80* 像素。我们的输出形状为(*3*)，对应三个可能动作的Q值（向左移动、停留、向右移动）：
- en: '![](img/deep-q-cnn.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/deep-q-cnn.png)'
- en: 'Since our output are the three Q-values, this is a regression task, and we
    can optimize this by minimizing the difference of the squared error between the
    current value of *Q(s, a)* and its computed value in terms of the sum of the reward
    and the discounted Q-value *Q(s'', a'')* one step into the future. The current
    value is already known at the beginning of the iteration and the future value
    is computed based on the reward returned by the environment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的输出是三个Q值，这是一个回归任务，我们可以通过最小化*Q(s, a)*当前值与其根据奖励和未来折扣Q值*Q(s', a')*计算值之间的平方误差之差来进行优化。当前值在迭代开始时已知，未来值是基于环境返回的奖励计算出来的：
- en: '![](img/loss_function.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/loss_function.png)'
- en: Balancing exploration with exploitation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡探索与利用
- en: Deep reinforcement learning is an example of online learning, where the training
    and prediction steps are interspersed. Unlike batch learning techniques where
    the best predictor is generated by learning on the entire training data, a predictor
    trained with online learning is continuously improving as it trains on new data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习是在线学习的一个例子，在这种方法中，训练和预测步骤是交替进行的。与批量学习技术不同，批量学习技术通过在整个训练数据上进行学习生成最佳预测器，而在线学习训练的预测器则随着在新数据上的训练不断改进。
- en: Thus in the initial epochs of training, a deep Q-network gives random predictions
    which can give rise to poor Q-learning performance. To alleviate this, we can
    use a simple exploration method such as &epsi;-greedy. In case of &epsi;-greedy
    exploration, the agent chooses the action suggested by the network with probability
    *1-&epsi;* or an action uniformly at random otherwise. That is why this strategy
    is called exploration/exploitation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练的初期阶段，深度Q网络给出随机预测，这可能导致Q学习性能不佳。为了缓解这一问题，我们可以使用简单的探索方法，如&epsi;-贪婪法。在&epsi;-贪婪探索中，智能体以概率*1-&epsi;*选择网络建议的动作，否则选择一个随机动作。
    这就是为什么这个策略被称为探索/利用。
- en: As the number of epochs increases and the Q-function converges, it begins to
    return more consistent Q-values. The value of &epsi; can be attenuated to account
    for this, so as the network begins to return more consistent predictions, the
    agent chooses to exploit the values returned by the network over choosing random
    actions. In case of DeepMind, the value of &epsi; decreases over time from *1*
    to *0.1*, and in our example it decreases from *0.1* to *0.001*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练轮数的增加，Q函数逐渐收敛，开始返回更一致的Q值。&epsi;的值可以逐渐减小以适应这一变化，因此，随着网络开始返回更一致的预测，智能体选择利用网络返回的值，而不是选择随机动作。以DeepMind为例，&epsi;的值随时间从*1*减小到*0.1*，而在我们的示例中，它从*0.1*减小到*0.001*。
- en: Thus, &epsi;-greedy exploration ensures that in the beginning the system balances
    the unreliable predictions made from the Q-network with completely random moves
    to explore the state space, and then settles down to less aggressive exploration
    (and more aggressive exploitation) as the predictions made by the Q-network improve.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，&epsi;-贪婪探索确保在开始时系统平衡Q网络做出的不可靠预测和完全随机的动作来探索状态空间，随后随着Q网络的预测改善，系统逐渐转向较少的激进探索（更多的激进利用）。
- en: Experience replay, or the value of experience
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验回放，或经验的价值
- en: Based on the equations that represent the Q-value for a state action pair *(s[t],
    a[t])* in terms of the current reward *r[t]* and the discounted maximum Q-value
    for the next time step *(s[t+1], a[t+1])*, our strategy would logically be to
    train the network to predict the best next state *s'* given the current state
    *(s, a, r)*. It turns out that this tends to drive the network into a local minimum.
    The reason for this is that consecutive training samples tend to be very similar.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于表示状态动作对*(s[t], a[t])*的Q值方程，该Q值由当前奖励*r[t]*和下一个时间步的折扣最大Q值*(s[t+1], a[t+1])*表示，我们的策略在逻辑上是训练网络预测给定当前状态*(s,
    a, r)*下的最佳下一个状态*s'*。事实证明，这倾向于将网络推向局部最小值。原因是连续的训练样本往往非常相似。
- en: To counter this, during game play, we collect all the previous moves *(s, a,
    r, s')* into a large fixed size queue called the **replay memory**. The replay
    memory represents the experience of the network. When training the network, we
    generate random batches from the replay memory instead of the most recent (batch
    of) transactions. Since the batches are composed of random experience tuples *(s,
    a, r, s')* that are out of order, the network trains better and avoids getting
    stuck in local minima.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一点，在游戏过程中，我们将所有先前的动作*(s, a, r, s')*收集到一个固定大小的队列中，称为**回放记忆**。回放记忆代表网络的经验。在训练网络时，我们从回放记忆中生成随机批次，而不是从最近的（批量）事务中生成。由于这些批次由乱序的经验元组*(s,
    a, r, s')*组成，网络的训练效果更好，并且避免陷入局部最小值。
- en: Experiences could be collected from human gameplay as well instead of (or in
    addition to) from previous moves during game play by the network. Yet another
    approach is to collect experiences by running the network in *observation* mode
    for a while in the beginning, when it generates completely random actions (![](img/B06258_08_13-1.png)* =
    1*) and extracts the reward and next state from the game and collects them into
    its experience replay queue.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 经验可以通过人类的游戏玩法来收集，而不仅仅是（或额外地）通过网络在游戏中的先前动作来收集。另一种方法是在游戏开始时，以*观察*模式运行网络一段时间，在此期间它会生成完全随机的动作（![](img/B06258_08_13-1.png)*
    = 1*），并从游戏中提取奖励和下一个状态，并将其收集到经验回放队列中。
- en: Example - Keras deep Q-network for catch
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - Keras深度Q网络用于接球
- en: 'The objective of our game is to catch a ball released from a random location
    from the top of the screen with a paddle at the bottom of the screen by moving
    the paddle horizontally using the left and right arrow keys. The player wins if
    the paddle can catch the ball and loses if the balls falls off the screen before
    the paddle gets to it. The game has the advantage of being very simple to understand
    and build, and is modeled after the game of catch described by Eder Santana in
    his blog post (for more information refer to: *Keras Plays Catch, a Single File
    Reinforcement Learning Example*, by Eder Santana, 2017.) on deep reinforcement
    learning. We built the original game using Pygame ([https://www.pygame.org/news](https://www.pygame.org/news)),
    a free and open source library for building games. This game allows the player
    to move the paddle using the left and right arrow keys. The game is available
    as `game.py` in the code bundle for this chapter in case you want to get a feel
    for it.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们游戏的目标是通过左右箭头键水平移动屏幕底部的挡板，接住从屏幕顶部随机位置释放的球。当挡板成功接住球时，玩家获胜；如果球在挡板接到之前掉出屏幕，玩家失败。这个游戏的优点是非常简单易懂并且易于构建，它的模型源自
    Eder Santana 在他的博客文章中描述的接球游戏（更多信息请参考：*Keras 玩接球游戏，一个单文件强化学习示例*，作者 Eder Santana，2017。）关于深度强化学习。我们使用
    Pygame（[https://www.pygame.org/news](https://www.pygame.org/news)）这个免费且开源的游戏构建库来构建最初的游戏。该游戏允许玩家使用左右箭头键来移动挡板。你可以在本章的代码包中找到
    `game.py` 文件，以便亲自体验一下这个游戏。
- en: '**Installing Pygame**:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 Pygame**：'
- en: 'Pygame runs on top of Python, and is available for Linux (various flavors),
    macOS, Windows, as well as some phone operating systems such as Android and Nokia.
    The full list of distributions can be found at: [http://www.pygame.org/download.shtml](http://www.pygame.org/download.shtml).
    Pre-built versions are available for 32-bit and 64-bit versions of Linux and Windows
    and 64-bit version of macOS. On these platforms, you can install Pygame with `pip
    install pygame` command.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Pygame 运行在 Python 之上，并且可以在 Linux（各种版本）、macOS、Windows 以及一些手机操作系统（如 Android 和
    Nokia）上使用。完整的发行版列表可以在以下网址找到：[http://www.pygame.org/download.shtml](http://www.pygame.org/download.shtml)。预构建版本适用于
    32 位和 64 位的 Linux 和 Windows，以及 64 位的 macOS。在这些平台上，你可以通过 `pip install pygame` 命令来安装
    Pygame。
- en: If a pre-built version does not exist for your platform, you can also build
    it from source using instructions available at: [http://www.pygame.org/wiki/GettingStarted](http://www.pygame.org/wiki/GettingStarted).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有适用于你的平台的预构建版本，你也可以按照以下网址提供的说明从源代码构建： [http://www.pygame.org/wiki/GettingStarted](http://www.pygame.org/wiki/GettingStarted)。
- en: 'Anaconda users can find pre-built Pygame versions on the conda-forge:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 用户可以在 conda-forge 上找到预构建的 Pygame 版本：
- en: '`conda install binstar`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install binstar`'
- en: '`conda install anaconda-client`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install anaconda-client`'
- en: '`conda install -c https://conda.binstar.org/tlatorre pygame # Linux`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install -c https://conda.binstar.org/tlatorre pygame # Linux`'
- en: '`conda install -c https://conda.binstar.org/quasiben pygame # Mac`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install -c https://conda.binstar.org/quasiben pygame # Mac`'
- en: In order to train our neural network, we need to make some changes to the original
    game so the network can play instead of the human player. We want to wrap the
    game to allow the network to communicate with it via an API instead of the keyboard
    left and right arrow keys. Let us look at the code for this wrapped game.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的神经网络，我们需要对原始游戏进行一些修改，使得网络能够代替人类玩家进行游戏。我们希望将游戏封装起来，允许网络通过 API 与游戏进行交互，而不是通过键盘的左右箭头键。让我们来看看这个封装游戏的代码。
- en: 'As usual, we start with the imports:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们从导入开始：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We define our class. Our constructor can optionally set the wrapped version
    of the game to run in *headless* mode, that is, without needing to display a Pygame
    screen. This is useful where you have to run on a GPU box in the cloud and only
    have access to a text based terminal. You can comment this line out if you are
    running the wrapped game locally where you have access to a graphics terminal.
    Next we call the `pygame.init()` method to initialize all Pygame components. Finally,
    we set a bunch of class level constants:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了我们的类。构造函数可以选择性地将封装版本的游戏设置为 *无头* 模式，即不需要显示 Pygame 屏幕。当你需要在云中的 GPU 主机上运行并且只能访问基于文本的终端时，这非常有用。如果你在本地运行封装的游戏并且能够访问图形终端，可以注释掉这一行。接下来，我们调用
    `pygame.init()` 方法来初始化所有 Pygame 组件。最后，我们设置了一些类级常量：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `reset()` method defines the operations that need to be called at the start
    of each game, such as clearing out the state queue, setting the ball, and paddle
    to their starting positions, initializing the scores, and so on:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset()`方法定义了在每局游戏开始时需要调用的操作，如清空状态队列、设置球和挡板的位置、初始化分数等：'
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the original game, there is a Pygame event queue into which the left and
    right arrow key events raised by the player as he moves the paddle, as well as
    internal events raised by Pygame components are written to. The central part of
    the game code is basically a loop (called the **event loop**), that reads the
    event queue and reacts to it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始游戏中，有一个Pygame事件队列，玩家通过按左右箭头键移动挡板时产生的事件，以及Pygame组件产生的内部事件，都会写入该队列。游戏代码的核心部分基本上是一个循环（称为**事件循环**），它读取事件队列并对此做出反应。
- en: 'In the wrapped version, we have moved the event loop to the caller. The `step()`
    method describes what happens in a single pass in the loop. The method takes an
    integer `0`, `1`, or `2` representing an action (respectively move left, stay,
    and move right), and then it sets variables that control the position of the ball
    and paddle at this time step. The `PADDLE_VELOCITY` variable represents a *speed*
    that moves the paddle that many pixels to the left or right when the move left
    and move right actions are sent. If the ball has dropped past the paddle, it checks
    whether there is a collision. If there is, the paddle *catches* the ball and the
    player (the neural network) wins, otherwise the player loses. The method then
    redraws the screen and appends it to the fixed length `deque` that contains the
    last four frames of the game screen. Finally, it returns the state (given by the
    last four frames), the reward for the current action and a flag that tells the
    caller if the game is over:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在包装版本中，我们将事件循环移到了调用者处。`step()`方法描述了循环中单次执行时发生的事情。该方法接受一个整数`0`、`1`或`2`表示一个动作（分别是向左移动、保持不动和向右移动），然后它设置控制此时球和挡板位置的变量。`PADDLE_VELOCITY`变量表示一个*速度*，当发送向左或向右移动的动作时，挡板会向左或向右移动相应的像素。如果球已经越过挡板，它会检查是否发生碰撞。如果发生碰撞，挡板会*接住*球，玩家（即神经网络）获胜，否则玩家失败。然后，该方法会重新绘制屏幕并将其附加到固定长度的`deque`中，该队列包含游戏画面的最后四帧。最后，它返回状态（由最后四帧组成）、当前动作的奖励以及一个标志，告诉调用者游戏是否结束：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will look at the code to train our network to play the game.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看训练我们的网络以玩游戏的代码。
- en: 'As usual, first we import the libraries and objects that we need. In addition
    to third-party components from Keras and SciPy, we also import the `wrapped_game`
    class we described previously:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，首先我们导入所需的库和对象。除了来自Keras和SciPy的第三方组件外，我们还导入了之前描述的`wrapped_game`类：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We define two convenience functions. The first converts the set of four input
    images to a form suitable for use by the network. The input comes in a set of
    four 800 x 800 images, so the shape of the input is *(4, 800, 800)*. However,
    the network expects its input as a four-dimensional tensor of shape *(batch size,
    80, 80, 4)*. At the very beginning of the game, we don't have four frames, so
    we fake it by stacking the first frame four times. The shape of the output tensor
    returned from this function is *(80, 80, 4)*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个便捷函数。第一个函数将四个输入图像转换为适合网络使用的形式。输入是一组四个800 x 800的图像，因此输入的形状是*(4, 800, 800)*。然而，网络期望其输入是一个形状为*(batch
    size, 80, 80, 4)*的四维张量。在游戏的最初，我们没有四帧画面，所以通过将第一帧堆叠四次来模拟。这个函数返回的输出张量的形状是*(80, 80,
    4)*。
- en: 'The `get_next_batch()` function samples `batch_size` state tuples from the
    experience replay queue, and gets the reward and predicted next state from the
    neural network. It then calculates the value of the Q-function at the next time
    step and returns it:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_next_batch()`函数从经验回放队列中采样`batch_size`个状态元组，并从神经网络获取奖励和预测的下一个状态。然后，它计算下一时间步的Q函数值并返回：'
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We define our network. This is the network that models the Q-function for our
    game. Our network is very similar to the one proposed in the DeepMind paper. The
    only difference is the size of the input and the output. Our input shape is *(80,
    80, 4)* while theirs was *(84, 84, 4)* and our output is *(3)* corresponding to
    the three actions for which the value of the Q-function needs to be computed,
    whereas their was *(18)*, corresponding to the actions possible from Atari.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了我们的网络。这是一个用于建模游戏 Q 函数的网络。我们的网络与 DeepMind 论文中提出的网络非常相似。唯一的区别是输入和输出的大小。我们的输入形状是
    *(80, 80, 4)*，而他们的是 *(84, 84, 4)*，我们的输出是 *(3)*，对应需要计算 Q 函数值的三个动作，而他们的是 *(18)*，对应
    Atari 游戏中可能的动作：
- en: 'There are three convolutional layers and two fully connected (dense) layers.
    All layers, except the last have the ReLU activation unit. Since we are predicting
    values of Q-functions, it is a regression network and the last layer has no activation
    unit:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 网络有三层卷积层和两层全连接（密集）层。除最后一层外，所有层都使用 ReLU 激活单元。由于我们是在预测 Q 函数的值，因此这是一个回归网络，最后一层没有激活单元：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we have described previously, our loss function is the squared difference
    between the current value of *Q(s, a)* and its computed value in terms of the
    sum of the reward and the discounted Q-value *Q(s'', a'')* one step into the future,
    so the mean squared error (MSE) loss function works very well. For the optimizer,
    we choose Adam, a good general-purpose optimizer, instantiated with a low learning
    rate:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的损失函数是当前 *Q(s, a)* 值与根据奖励和折扣后的 Q 值 *Q(s', a')* 计算得到的值之间的平方差，因此均方误差（MSE）损失函数非常有效。对于优化器，我们选择了
    Adam，这是一个通用的优化器，并且以较低的学习率进行了初始化：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We define some constants for our training. The `NUM_ACTIONS` constant defines
    the number of output actions that the network can send to the game. In our case,
    these actions are `0`, `1`, and `2`, corresponding to move left, stay, and move
    right. The `GAMMA` value is the discount factor ![](img/image_01_042.jpg) for
    future rewards. The `INITIAL_EPSILON` and `FINAL_EPSILON` refer to starting and
    ending values for the ![](img/B06258_08_13-1.png) parameter in ![](img/B06258_08_13-1.png)-greedy
    exploration. The `MEMORY_SIZE` is the size of the experience replay queue. The
    `NUM_EPOCHS_OBSERVE` refer to the number of epochs where the network is allowed
    to explore the game by sending it completely random actions and seeing the rewards.
    The `NUM_EPOCHS_TRAIN` variable refers to the number of epochs the network will
    undergo online training. Each epoch corresponds to a single game or episode. The
    total number of games played for a training run is the sum of the `NUM_EPOCHS_OBSERVE`
    and `NUM_EPOCHS_TRAIN` values. The `BATCH_SIZE` is the size of the mini-batch
    that we will use for training:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为训练定义了一些常量。`NUM_ACTIONS` 常量定义了网络可以向游戏发送的输出动作的数量。在我们的例子中，这些动作是 `0`、`1` 和 `2`，分别对应向左移动、停留和向右移动。`GAMMA`
    值是未来奖励的折扣因子 ![](img/image_01_042.jpg)。`INITIAL_EPSILON` 和 `FINAL_EPSILON` 分别指代
    ![](img/B06258_08_13-1.png) 参数在 ![](img/B06258_08_13-1.png)-贪婪探索中的起始值和结束值。`MEMORY_SIZE`
    是经验回放队列的大小。`NUM_EPOCHS_OBSERVE` 指网络在完全随机发送动作并观察奖励的过程中允许探索游戏的轮次。`NUM_EPOCHS_TRAIN`
    变量指网络进行在线训练的轮次。每一轮对应一个单独的游戏或回合。一个训练运行的总游戏数是 `NUM_EPOCHS_OBSERVE` 和 `NUM_EPOCHS_TRAIN`
    的总和。`BATCH_SIZE` 是我们在训练中使用的迷你批次的大小：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We instantiate the game and the experience replay queue. We also open up a
    log file and initialize some variables in preparation for training:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了游戏和经验回放队列。我们还打开了一个日志文件并初始化了一些变量，为训练做准备：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next up, we set up the loop that controls the number of epochs of training.
    As noted previously, each epoch corresponds to a single game, so we reset the
    game state at this point. A game corresponds to a single episode of a ball falling
    from the ceiling and either getting caught by the paddle or being missed. The
    loss is the squared difference between the predicted and actual Q-value for the
    game.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置控制训练轮次的循环。正如之前提到的，每一轮对应一个单独的游戏，因此我们此时会重置游戏状态。一个游戏对应一个小球从天花板掉下，可能被挡板接住或错过的回合。损失是预测值和实际
    Q 值之间的平方差：
- en: 'We start the game off by sending it a dummy action (in our case, a *stay*)
    and get back the initial state tuple for the game:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过发送一个虚拟动作（在我们的例子中是 *停留*）来开始游戏，并获得游戏的初始状态元组：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next block is the main loop of the game. This is the event loop in the
    original game that we moved to the calling code. We save the current state because
    we will need that for our experience replay queue, then decide what action signal
    to send the wrapped game. If we are in observation mode, we will just generate
    a random number corresponding to one of our actions, otherwise we will use ![](img/B06258_08_13-1.png)-greedy
    exploration to either select a random action or use our neural network (which
    we are also training) to predict the action we should send:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个块是游戏的主循环。这是原始游戏中的事件循环，我们将其移到了调用代码中。我们保存当前状态，因为我们将需要它来进行经验回放队列，然后决定向封装的游戏发送什么动作信号。如果我们处于观察模式，我们只会生成一个对应于我们动作的随机数，否则我们将使用![](img/B06258_08_13-1.png)-贪心探索方法，随机选择一个动作或使用我们的神经网络（我们也在训练它）来预测我们应该发送的动作：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once we know our action, we send it to the game by calling `game.step()`, which
    returns the new state, the reward and a Boolean flag indicating the game is over.
    If the reward is positive (indicating that the ball was caught), we increment
    the number of wins, and we store this *(state, action, reward, new state, game
    over)* tuple in our experience replay queue:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦知道我们的动作，我们通过调用`game.step()`发送它到游戏中，`game.step()`返回新的状态、奖励和一个布尔标志，表示游戏是否结束。如果奖励是正数（表示球被接住），我们会增加胜利次数，并将这个*(状态，动作，奖励，新状态，游戏结束)*元组存储在我们的经验回放队列中：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then draw a random mini-batch from our experience replay queue and train
    our network. For each session of training, we compute the loss. The sum of the
    losses for all the trainings in each epoch is the loss for the entire epoch:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从经验回放队列中随机抽取一个小批量进行训练。对于每次训练，我们计算损失。在每个训练周期中，所有训练的损失总和即为该周期的损失：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When the network is relatively untrained, its predictions are not very good,
    so it makes sense to explore the state space more in an effort to reduce the chances
    of getting stuck in a local minima. However, as the network gets more and more
    trained, we reduce the value of ![](img/B06258_08_13-1.png) gradually so the model
    gets to predict more and more of the actions the network sends to the game:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络相对未经训练时，它的预测不太准确，因此有意义的是更多地探索状态空间，尽量减少卡在局部最小值的机会。然而，随着网络越来越多地训练，我们逐渐减少![](img/B06258_08_13-1.png)的值，以便模型能够预测网络向游戏发送的更多动作：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We write out a per epoch log both on console and into a log file for later
    analysis. After 100 epochs of training, we save the current state of the model
    so that we can recover in case we decide to stop training for any reason. We also
    save our final model so that we can use it to play our game later:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在控制台和日志文件中写出每个训练周期的日志，供后续分析。在训练100个周期后，我们保存当前模型的状态，以便在我们决定停止训练时能够恢复。我们还保存了最终模型，以便以后用它来玩游戏：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We trained the game by making it observe 100 games, followed by playing 1,000,
    2,000, and 5,000 games respectively. The last few lines of the log file for the
    5,000 game run are shown next. As you can see, towards the end of the training,
    the network gets quite skilled at playing the game:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过让游戏观察100场比赛，然后分别玩1,000场、2,000场和5,000场比赛来进行训练。以下是5,000场比赛训练的日志文件中的最后几行。如您所见，在训练接近尾声时，网络变得相当擅长玩这个游戏：
- en: '![](img/ss-8-1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-8-1.png)'
- en: 'The plot of loss and win count over epoch, shown in the following graph, also
    tells a similar story. While it does look like the loss could converge further
    with more training, it has gone down from *0.6* to around *0.1* in *5000* epochs
    of training. Similarly, the plot of the number of wins curve upward, showing that
    the network is learning faster as the number of epochs increases:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 损失和胜利计数随训练轮数变化的图表，显示了类似的趋势。虽然看起来随着更多的训练，损失可能会进一步收敛，但它已经从*0.6*下降到大约*0.1*，并且训练了*5000*个周期。同样，胜利次数的图表呈上升趋势，表明随着训练轮数的增加，网络学习得更快：
- en: '![](img/loss_function.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/loss_function.png)'
- en: 'Finally, we evaluate the skill of our trained model by making it play a fixed
    number of games (100 in our case) and seeing how many it can win. Here is the
    code to do this. As previously, we start with our imports:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过让训练好的模型玩固定数量的游戏（在我们的案例中是100场）来评估它的技能，并查看它能赢得多少场。以下是执行此操作的代码。如之前一样，我们从导入开始：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We load up the model we had saved at the end of training and compile it. We
    also instantiate our `wrapped_game`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载在训练结束时保存的模型并进行编译。我们还实例化了我们的`wrapped_game`：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We then loop over 100 games. We instantiate each game by calling its `reset()`
    method, and start it off. Then, for each game, until it is over, we call on the
    model to predict the action with the best Q-function. We report a running total
    of how many games it won.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行 100 场游戏的循环。通过调用每个游戏的 `reset()` 方法来实例化每场游戏，并开始它。然后，对于每场游戏，直到结束，我们调用模型预测具有最佳
    Q 函数的动作。我们报告它赢得的游戏总数。
- en: 'We ran the test with each of our models. The first one that was trained for
    1,000 games won 42 of 100 games, the one trained for 2,000 games won 74 of 100
    games, and the one trained for 5,000 games won 87 of 100 games. This clearly shows
    that the network is improving with training:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用每个模型进行了测试。第一个训练了 1,000 场游戏，赢得了 42 场中的 100 场；第二个训练了 2,000 场游戏，赢得了 74 场中的 100
    场；第三个训练了 5,000 场游戏，赢得了 87 场中的 100 场。这清楚地表明，网络随着训练的进行在不断改进：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you run the evaluation code with the call to run it in headless mode commented
    out, you can watch the network playing the game and it's quite amazing to watch.
    Given that the Q-value predictions start off as random values and that it's mainly
    the sparse reward mechanism that provides the guidance to the network during training,
    it is almost unreasonable that the network learns to play the game this effectively.
    But as with other areas of deep learning, the network does in fact learn to play
    quite well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行评估代码，并取消注释以启用无头模式运行的调用，你可以观看网络进行游戏，观看的过程相当令人惊叹。考虑到 Q 值预测一开始是随机的，并且训练过程中主要是稀疏奖励机制为网络提供指导，网络能够如此高效地学会玩游戏几乎是不可思议的。但和深度学习的其他领域一样，网络确实学会了相当好地玩游戏。
- en: The example presented previously is fairly simple, but it illustrates the process
    by which deep reinforcement learning models work, and hopefully has helped create
    a mental model using which you can approach more complex implementations. One
    implementation you might find interesting is Ben Lau's implementation of FlappyBird
    (for more information refer to: *Using Keras and Deep Q-Network to Play FlappyBird*,
    by Ben Lau, 2016\. and GitHub page: [https://github.com/yanpanlau/Keras-FlappyBird](https://github.com/yanpanlau/Keras-FlappyBird)) using
    Keras. The Keras-RL project ([https://github.com/matthiasplappert/keras-rl](https://github.com/matthiasplappert/keras-rl)),
    a Keras library for deep reinforcement learning, also has some very good examples.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 前面展示的例子相对简单，但它阐明了深度强化学习模型的工作过程，并且希望能帮助你建立一个思维模型，通过这个模型你可以接近更复杂的实现。你可能会对 Ben
    Lau 使用 Keras 实现的 FlappyBird 感兴趣（更多信息请参考：*使用 Keras 和深度 Q 网络来玩 FlappyBird*，Ben Lau，2016年，以及
    GitHub 页面：[https://github.com/yanpanlau/Keras-FlappyBird](https://github.com/yanpanlau/Keras-FlappyBird)）。Keras-RL
    项目（[https://github.com/matthiasplappert/keras-rl](https://github.com/matthiasplappert/keras-rl)），这是一个用于深度强化学习的
    Keras 库，也有一些非常好的例子。
- en: Since the original proposal from DeepMind, there have been other improvements
    suggested, such as double Q-learning (for more information refer to: *Deep Reinforcement
    Learning with Double Q-Learning*, by H. Van Hasselt, A. Guez, and D. Silver, AAAI.
    2016), prioritized experience replay (for more information refer to: *Prioritized
    Experience Replay*, by T. Schaul, arXiv:1511.05952, 2015), and dueling network
    architectures (for more information refer to: *Dueling Network Architectures for
    Deep Reinforcement Learning*, by Z. Wang, arXiv:1511.06581, 2015). Double Q-learning
    uses two networks - the primary network chooses the action and the target network
    chooses the target Q-value for the action. This reduces possible overestimation
    of Q-values by the single network, and allows the network to train quicker and
    better. Prioritized experience replay increases the probability of sampling experience
    tuples with a higher expected learning progress. Dueling network architectures
    decompose the Q-function into state and action components and combine them back
    separately.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 DeepMind 提出的原始方案以来，已经有其他改进方案被提出，例如双 Q 学习（更多信息请参考：*使用双 Q 学习的深度强化学习*，H. Van
    Hasselt，A. Guez 和 D. Silver，AAAI，2016），优先经验回放（更多信息请参考：*优先经验回放*，T. Schaul，arXiv:1511.05952，2015），以及对抗网络架构（更多信息请参考：*用于深度强化学习的对抗网络架构*，Z.
    Wang，arXiv:1511.06581，2015）。双 Q 学习使用两个网络——主网络选择动作，目标网络选择该动作的目标 Q 值。这可以减少单个网络对
    Q 值的过高估计，从而让网络训练得更快、更好。优先经验回放增加了采样具有更高预期学习进展的经验元组的概率。对抗网络架构将 Q 函数分解为状态和动作组件，并将它们单独合并回来。
- en: All of the code discussed in this section, including the base game that can
    be played by a human player, is available in the code bundle accompanying this
    chapter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的所有代码，包括可以由人类玩家玩的基础游戏，都可以在本章附带的代码包中找到。
- en: The road ahead
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前路漫漫
- en: In January 2016, DeepMind announced the release of AlphaGo (for more information
    refer to: *Mastering the Game of Go with Deep Neural Networks and Tree Search*,
    by D. Silver, Nature 529.7587, pp. 484-489, 2016), a neural network to play the
    game of Go. Go is regarded as a very challenging game for AIs to play, mainly
    because at any point in the game, there are an average of approximately *10^(170)*
    possible (for more information refer to: [http://ai-depot.com/LogicGames/Go-Complexity.html](http://ai-depot.com/LogicGames/Go-Complexity.html))
    moves (compared with approximately *10^(50)* for chess). Hence determining the
    best move using brute force methods is computationally infeasible. At the time
    of publication, AlphaGo had already won 5-0 in a 5-game competition against the
    current European Go champion, Fan Hui. This was the first time that any computer
    program had defeated a human player at Go. Subsequently, in March 2016, AlphaGo
    won 4-1 against Lee Sedol, the world's second professional Go player.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年1月，DeepMind宣布发布AlphaGo（更多信息请参见：*利用深度神经网络和树搜索掌握围棋游戏*，D. Silver，Nature 529.7587，页484-489，2016），这是一个用于下围棋的神经网络。围棋被认为是AI难以掌握的一项非常具有挑战性的游戏，主要因为在游戏的任何时刻，可能的着法平均大约有*10^(170)*种（更多信息请参见：[http://ai-depot.com/LogicGames/Go-Complexity.html](http://ai-depot.com/LogicGames/Go-Complexity.html)）（而国际象棋则大约是*10^(50)*种）。因此，使用暴力破解的方法来确定最佳着法在计算上是不可行的。在发布时，AlphaGo已经以5-0的成绩战胜了现任欧洲围棋冠军范辉。这是计算机程序首次在围棋对抗中击败人类玩家。随后，2016年3月，AlphaGo以4-1战胜了世界第二号职业围棋选手李世石。
- en: There were several notable new ideas that went into AlphaGo. First, it was trained
    using a combination of supervised learning from human expert games and reinforcement
    learning by playing one copy of AlphaGo against another. You have seen applications
    of both these ideas in previous chapters.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo的研发过程中有几个值得注意的新想法。首先，它是通过结合人类专家对局的监督学习和通过让一台AlphaGo与另一台AlphaGo对弈的强化学习来进行训练的。你已经在前几章中见过这些思想的应用。
- en: 'Second, AlphaGo was composed of a value network and a policy network. During
    each move, AlphaGo uses Monte Carlo simulation, a process used to predict the
    probability of different outcomes in the future in the presence of random variables, to
    imagine many alternative games starting from the current position. The value network
    is used to reduce the depth of the tree search to estimate win/loss probability
    without having to compute all the way to the end of the game, sort of like an
    intuition about how good the move is. The policy network is used to reduce the
    breadth of the search by guiding the search towards actions that promise the maximum
    immediate reward (or Q-value). For a more detailed description, please refer to
    the blog post: *AlphaGo: Mastering the ancient game of Go with Machine Learning*,
    Google Research Blog, 2016.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，AlphaGo由价值网络和策略网络组成。在每一步棋时，AlphaGo会使用蒙特卡罗模拟，这是一种在存在随机变量的情况下预测未来不同结果概率的过程，用来从当前局面想象出许多替代棋局。价值网络用于减少树搜索的深度，从而估算胜负概率，而无需计算游戏到结束的所有步骤，类似于对棋步优劣的直觉判断。策略网络则通过引导搜索朝向能够获得最大即时奖励（或Q值）的动作，来减少搜索的广度。有关更详细的描述，请参考博客文章：*AlphaGo：通过机器学习掌握古老的围棋游戏*，Google
    Research Blog，2016年。
- en: 'While AlphaGo was a major improvement over the original DeepMind network, it
    was still playing a game where all the players can see all the game pieces, that
    is, they are still games of perfect information. In January, 2017, researchers
    at Carnegie Mellon University announced Libratus (for more information refer to: *AI
    Takes on Top Poker Players*, by T. Revel, New Scientist 223.3109, pp. 8, 2017),
    an AI that plays Poker. Simultaneously, another group comprised of researchers
    from the University of Alberta, Charles University of Prague, and Czech Technical
    University (also from Prague), have proposed the DeepStack architecture (for more
    information refer to: *DeepStack: Expert-Level Artificial Intelligence in No-Limit
    Poker*, by M. Moravaa­k, arXiv:1701.01724, 2017) to do the same thing. Poker is
    a game of imperfect information, since a player cannot see the opponent''s cards.
    So, in addition to learning how to play the game, the Poker playing AI also needs
    to develop an intuition about the opponent''s game play.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然 AlphaGo 相较于原始的 DeepMind 网络有了很大的进步，但它仍然是在一个所有玩家都可以看到所有棋子的游戏中进行的，也就是说，它仍然是一个完全信息博弈。2017年1月，卡内基梅隆大学的研究人员宣布了
    Libratus（一篇相关论文：*AI Takes on Top Poker Players*，作者 T. Revel，New Scientist 223.3109，页码
    8，2017），这是一款可以玩扑克的 AI。与此同时，由阿尔伯塔大学、捷克布拉格的查尔斯大学和捷克技术大学的研究人员组成的另一个团队，提出了 DeepStack
    架构（一篇相关论文：*DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker*，作者
    M. Moravaák，arXiv:1701.01724，2017），用于实现同样的目标。扑克是一种不完全信息博弈，因为玩家无法看到对手的牌。因此，除了学习如何玩游戏之外，扑克
    AI 还需要对对手的游戏玩法发展出直觉。'
- en: 'Rather than use a built-in strategy for its intuition, Libratus has an algorithm
    that computes this strategy by trying to achieve a balance between risk and reward,
    also known as the Nash equilibrium. From January 11, 2017 to January 31, 2017,
    Libratus was pitted against four top human Poker players (for more information
    refer to: *Upping the Ante: Top Poker Pros Face Off vs. Artificial Intelligence*,
    Carnegie Mellon University, January 2017), and beat them resoundingly.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '与其使用内置策略来获得直觉，Libratus 使用一种算法，通过尝试在风险和回报之间取得平衡来计算这一策略，这也被称为纳什均衡。从2017年1月11日到1月31日，Libratus与四名顶级扑克玩家进行了对战（一篇相关论文：*Upping
    the Ante: Top Poker Pros Face Off vs. Artificial Intelligence*，卡内基梅隆大学，2017年1月），并以压倒性优势战胜了他们。'
- en: DeepStack's intuition is trained using reinforcement learning, using examples
    generated from random Poker situations. It has played 33 professional Poker players
    from 17 countries and has a win rating that makes it an *order of magnitude* better
    than a good player rating (for more information refer to: *The Uncanny Intuition
    of Deep Learning to Predict Human Behavior*, by C. E. Perez, Medium corporation,
    Intuition Machine, February 13, 2017).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: DeepStack的直觉通过强化学习进行训练，使用从随机扑克情境中生成的示例。它已与来自17个国家的33位职业扑克玩家进行过对战，且其获胜评分使其比优秀玩家的评分高出*一个数量级*（一篇相关论文：*The
    Uncanny Intuition of Deep Learning to Predict Human Behavior*，作者 C. E. Perez，Medium
    corporation，Intuition Machine，2017年2月13日）。
- en: As you can see, these are very exciting times indeed. Advances that started
    with deep learning networks able to play arcade games have led to networks that
    can effectively read your mind, or at least anticipate (sometimes non-rational)
    human behavior and win at games of bluffing. The possibilities with deep learning
    seem to be just limitless.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这的确是一个令人激动的时代。最初从能够玩街机游戏的深度学习网络开始，进展到现在能够有效读取你的思维，或至少能够预测（有时是非理性的）人类行为并在虚张声势的博弈中获胜的网络。深度学习的可能性似乎几乎是无限的。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned the concepts behind reinforcement learning,
    and how it can be used to build deep learning networks with Keras that learn how
    to play arcade games based on reward feedback. From there, we moved on to briefly
    discuss advances in this field, such as networks that have been taught to play
    harder games such as Go and Poker at a superhuman level. While game playing might
    seem like a frivolous application, these ideas are the first step towards general
    artificial intelligence, where a network learns from experience rather than large
    amounts of training data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了强化学习背后的概念，以及如何使用 Keras 构建深度学习网络，通过奖励反馈学习如何玩街机游戏。接下来，我们简要讨论了该领域的进展，例如已经能够在超人水平上玩围棋和扑克等更复杂游戏的网络。虽然游戏玩耍看起来像是一个轻松的应用，但这些理念是通向通用人工智能的第一步，在这种人工智能中，网络通过经验学习，而不是依赖大量的训练数据。
