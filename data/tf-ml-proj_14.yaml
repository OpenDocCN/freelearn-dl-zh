- en: Playing Pacman Using Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度强化学习玩吃豆人
- en: 'Reinforcement learning refers to a paradigm where an agent learns from environment
    feedback by virtue of receiving observations and rewards in return for actions
    it takes. The following diagram captures the feedback-based learning loop of reinforcement
    learning:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是指一种范式，代理通过从环境反馈中学习，依据其所采取的动作获得观察结果和奖励。以下图展示了强化学习的基于反馈的学习循环：
- en: '![](img/185ca50f-32f0-4e67-9d9b-200eb4c15771.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/185ca50f-32f0-4e67-9d9b-200eb4c15771.png)'
- en: Although mostly applied to learn how to play games, reinforcement learning has
    also been successfully applied in digital advertising, stock trading, self-driving
    cars, and industrial robots.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习主要应用于学习如何玩游戏，但它也已成功应用于数字广告、股票交易、自动驾驶汽车和工业机器人等领域。
- en: 'In this chapter, we will use reinforcement learning to create a PacMan game
    and learn about reinforcement learning in the process. We will cover the following
    topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用强化学习创建一个吃豆人游戏，并在这个过程中学习强化学习。我们将涵盖以下主题：
- en: Reinforcement learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning  versus supervised and unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习与监督学习和无监督学习的区别
- en: Components of reinforcement learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的组件
- en: OpenAI Gym
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: A PacMan game in OpenAI Gym
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym中的吃豆人游戏
- en: 'DQN for deep reinforcement learning:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度强化学习中的DQN：
- en: Q Learning
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: Deep Q Network
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: Applying DQN to a PacMan game
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将DQN应用于吃豆人游戏
- en: Let's get started!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Reinforcement learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is a type of machine learning in which an agent learns
    from the environment. The agent takes actions and, as a result of the actions,
    the environment returns observations and rewards. From the observation and rewards,
    the agent learns the policy and takes further actions, thus continuing the sequence
    of actions, observations, and rewards. In the long run, the agent has to learn
    the policy such that, when it takes actions based on the policy, it does so in
    such a way as to maximize the long-term rewards.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种机器学习方法，代理通过与环境的交互进行学习。代理采取行动，并根据这些行动，环境返回观察结果和奖励。通过这些观察结果和奖励，代理学习策略并采取进一步的行动，从而不断延续行动、观察和奖励的循环。在长期来看，代理必须学习一种策略，使其在依据策略采取行动时，能够最大化长期奖励。
- en: Reinforcement learning versus supervised and unsupervised learning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与监督学习和无监督学习的区别
- en: 'Machine learning solutions can be of three major types: supervised learning,
    unsupervised learning, and reinforcement learning. So how is reinforcement learning
    different from the other two types?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习解决方案可以分为三种主要类型：监督学习、无监督学习和强化学习。那么强化学习与其他两种类型有何不同呢？
- en: '**Supervised learning**: In supervised learning, the agent learns the model
    from a training dataset consisting of features and labels. The two most common
    types of supervised learning problems are regression and classification. Regression
    refers to predicting the future values based on the model, and classification
    refers to predicting the categories of the input values.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监督学习**：在监督学习中，代理从包含特征和标签的训练数据集中学习模型。监督学习的两种最常见问题是回归和分类。回归是指根据模型预测未来的值，分类是指预测输入值的类别。'
- en: '**Unsupervised learning**: In unsupervised learning, the agent learns the model
    from a training dataset consisting of only features. The two most common types
    of unsupervised learning problems are dimensionality reduction and clustering.
    Dimensionality reduction refers to reducing the number of features or dimensions
    in a dataset without altering its natural distribution. Clustering refers to dividing
    the input data into multiple groups, thus producing clusters or segments.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无监督学习**：在无监督学习中，代理从仅包含特征的训练数据集中学习模型。无监督学习的两种最常见问题是降维和聚类。降维是指在不改变数据集自然分布的情况下，减少数据集中的特征或维度数量。聚类是指将输入数据划分为多个组，从而产生聚类或段。'
- en: '**Reinforcement learning**: In reinforcement learning, the agent starts with
    an initial model and then continuously learns the model based on feedback from
    the environment. An RL agent updates the model by applying supervised or unsupervised
    learning techniques on a sequence of actions, observations, and rewards. The agent
    only learns from a reward signal, not from a loss function as in other machine
    learning approaches. The agent receives the feedback after it has already taken
    the action, while, in other ML approaches, the feedback is provided at the time
    of training in terms of loss or error. The data is not i.i.d. (independent and
    identically distributed) because it depends on previous actions taken, while in
    other ML approaches data is i.i.d.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**强化学习**：在强化学习中，代理从初始模型开始，然后根据环境反馈不断学习该模型。强化学习代理通过对一系列动作、观察和奖励应用监督或无监督学习方法来更新模型。代理仅从奖励信号中学习，而不像其他机器学习方法那样从损失函数中学习。代理在采取动作后才会收到反馈，而在其他机器学习方法中，反馈在训练时就会通过损失或错误提供。数据不是独立同分布（i.i.d.），因为它依赖于先前采取的动作，而在其他机器学习方法中，数据是独立同分布的。'
- en: Components of Reinforcement Learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的组成部分
- en: In any RL formalization, we talk in terms of a **state space** and an **action
    space**. Action space is a set of finite numbers of actions that can be taken
    by the agent, represented by *A*. State space is a finite set of states that the
    environment can be in, represented by *S*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何强化学习的形式化中，我们使用**状态空间**和**动作空间**来讨论。动作空间是代理可以采取的有限数量的动作集合，用*A*表示。状态空间是环境可能处于的有限状态集合，用*S*表示。
- en: The goal of the agent is to learn a policy, denoted by ![](img/2d80d9e2-ee59-47b1-90c6-5050457942b1.png).
    A **policy** can be **deterministic** or **stochastic**. A policy basically represents
    the model, using which the agent to  select the best action to take. Thus, the
    policy maps the rewards and observations received from the environment to actions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的目标是学习一个策略，表示为 ![](img/2d80d9e2-ee59-47b1-90c6-5050457942b1.png)。**策略**可以是**确定性的**或**随机的**。策略基本上代表了模型，代理使用该模型来选择最优动作。因此，策略将来自环境的奖励和观察映射到动作。
- en: When an agent follows a policy, it results in a sequence of state, action, reward,
    state, and so on. This sequence is known as a **trajectory** or an **episode**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理遵循某个策略时，会产生一个状态、动作、奖励、状态等的序列。这个序列被称为**轨迹**或**回合**。
- en: 'An important component of reinforcement learning formalizations is the **return**.
    The return is the estimate of the total long-term reward. Generally, the return
    can be represented by the following formula:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习形式化中的一个重要组成部分是**回报**。回报是对总长期奖励的估计。通常，回报可以用以下公式表示：
- en: '![](img/8d82b310-321c-4ff0-82b6-6ca9742f366b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d82b310-321c-4ff0-82b6-6ca9742f366b.png)'
- en: Here ![](img/77a691d7-b626-4fd0-98fc-5acebc7745d2.png) is a discount factor
    with values between (0,1), and ![](img/48e152e1-7cdc-4d4d-9d55-68e496294f47.png) is
    the reward at time step *t*. The discount factor represents how much importance
    should be given to the reward at later time steps. If ![](img/5187e85e-81f4-4c40-81a5-a70743fec3cd.png) is
    0 then only the rewards from the next action are considered, and if it is 1 then
    the future rewards have the same weight as the rewards from the next action.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![](img/77a691d7-b626-4fd0-98fc-5acebc7745d2.png) 是一个折扣因子，值在(0,1)之间，![](img/48e152e1-7cdc-4d4d-9d55-68e496294f47.png) 是时间步长*t*时的奖励。折扣因子表示在未来的时间步长中，奖励的重要性。若 ![](img/5187e85e-81f4-4c40-81a5-a70743fec3cd.png) 为0，则只考虑下一动作的奖励；若为1，则未来的奖励与下一动作的奖励具有相同的权重。
- en: However, since it is difficult to compute the value of the return, hence it
    is estimated with **state-value** or **action-value** functions. We shall talk
    about action-value functions further in the q-learning section in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于计算回报值非常困难，因此它是通过**状态值**或**动作值**函数来估计的。我们将在本章的Q学习部分进一步讨论动作值函数。
- en: For simulating our agent which will play the PacMan game, we shall be using
    the OpenAI Gym. Let's learn about OpenAI Gym now.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟我们将要玩吃豆人游戏的代理，我们将使用OpenAI Gym。现在让我们了解一下OpenAI Gym。
- en: You can follow along with the code in the Jupyter Notebook  `ch-14_Reinforcement_Learning` in
    the code bundle of this book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中的Jupyter Notebook文件`ch-14_Reinforcement_Learning`中跟随代码进行学习。
- en: OpenAI Gym
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: OpenAI Gym is a Python-based toolkit for the development of reinforcement learning
    algorithms. It provides more than 700 open source contributed environments at
    the time of writing this book. Custom environments for OpenAI can also be created.
    OpenAI Gym provides a unified interface for working with reinforcement learning
    environments and takes care of running the simulation, while the user of OpenAI
    can focus on designing and implementing the reinforcement learning algorithms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个基于 Python 的工具包，用于开发强化学习算法。在撰写本书时，它提供了超过 700 个开源贡献的环境。还可以创建自定义的
    OpenAI 环境。OpenAI Gym 提供了一个统一的接口，用于处理强化学习环境的工作，同时 OpenAI 的用户可以专注于设计和实现强化学习算法。
- en: The original research paper on OpenAI Gym is available at the following link: [http://arxiv.org/abs/1606.01540](http://arxiv.org/abs/1606.01540).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 的原始研究论文可以在以下链接找到：[http://arxiv.org/abs/1606.01540](http://arxiv.org/abs/1606.01540)。
- en: 'Let''s take a look at the following steps to learn how to install and explore
    OpenAI Gym:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一看以下步骤，学习如何安装和探索 OpenAI Gym：
- en: 'Install OpenAI Gym using the following command:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装 OpenAI Gym：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If the preceding command does not work, then refer to the following link for
    further help with installation: [https://github.com/openai/gym#installation](https://github.com/openai/gym#installation).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前述命令无法运行，请参考以下链接获取安装的进一步帮助：[https://github.com/openai/gym#installation](https://github.com/openai/gym#installation)。
- en: 'Print the number of available environments in the OpenAI Gym with the following
    code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码打印 OpenAI Gym 中可用的环境数量：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code generates the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成了以下输出：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Print the list of all environments, as shown in the following code:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印所有环境的列表，如下代码所示：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The partial list from the output is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的部分列表如下：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Each environment, represented by the `env` object, has a standardized interface:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个由 `env` 对象表示的环境都有一个标准化的接口：
- en: An `env` object can be created with the `env.make(<game-id-string>)` function
    by passing the `id` string.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `env.make(<game-id-string>)` 函数通过传递 `id` 字符串来创建 `env` 对象。
- en: 'Each `env` object contains the following main functions:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 `env` 对象包含以下主要函数：
- en: 'The `step()` function takes an action object as an argument and returns four
    objects:'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 `step()` 接受一个动作对象作为参数，并返回四个对象：
- en: '`observation`: An object implemented by the environment, representing the observation
    of the environment.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`: 环境实现的对象，表示环境的观察。'
- en: '`reward`: A signed float value indicating the gain (or loss) from the previous
    action.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`: 一个有符号浮点数，表示上一个动作的收益（或损失）。'
- en: '`done`: A Boolean value representing whether or not the scenario is finished.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done`: 一个布尔值，表示场景是否结束。'
- en: '`info`: A Python dictionary object representing the diagnostic information.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`: 一个表示诊断信息的 Python 字典对象。'
- en: The `render()` function creates a visual representation of the environment.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 `render()` 创建环境的视觉表示。
- en: The `reset()` function resets the environment to the original state.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 `reset()` 将环境重置为原始状态。
- en: Each `env` object comes with well-defined actions and observations, represented
    by `action_space` and `observation_space`.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 `env` 对象都带有明确定义的动作和观察，分别由 `action_space` 和 `observation_space` 表示。
- en: Creating a Pacman game in OpenAI Gym
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 OpenAI Gym 中创建 Pacman 游戏
- en: 'In this chapter, we will use the PacMan game as an example, known as **MsPacman-v0**.
    Let''s explore this game a bit further:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将以 **MsPacman-v0** 作为示例，探索这个游戏更深入一些：
- en: 'Create the `env` object with the standard `make` function, as shown in the
    following command:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准的 `make` 函数创建 `env` 对象，如下命令所示：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s print the action space of the game with the following code:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用以下代码打印游戏的动作空间：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code generates the following output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下输出：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Discrete 9` refers to the nine actions, such as up, down, left, and right.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discrete 9` 指的是九种动作，如上、下、左、右。'
- en: 'We can now see the observation space, as shown in the following example:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以看到观察空间，如下例所示：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code generates the following output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成了以下输出：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Thus, the observation space has three color channels and is of size 210 x 160\.
    The observation space gets rendered as in the following screenshot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，观察空间有三个颜色通道，大小为 210 x 160。观察空间的渲染如下截图所示：
- en: '![](img/a5d91e17-360f-4dec-97be-4aad1eb1ad46.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5d91e17-360f-4dec-97be-4aad1eb1ad46.png)'
- en: 'The number of episodes is the number of game plays. We shall set it to one,
    for now, indicating that we just want to play the game once. Since every episode
    is stochastic, in actual production runs you will run over several episodes and
    calculate the average values of the rewards. Let''s run the game for one episode
    while randomly selecting one of the actions during the gameplay with the following
    code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 章节数是游戏次数的数量。我们现在将其设置为 1，表示我们只想玩一次游戏。由于每一轮游戏都是随机的，实际生产运行时，你会运行多轮游戏并计算奖励的平均值。让我们运行一次游戏，同时在游戏过程中随机选择一个动作，代码如下：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then get the following output:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后得到以下输出：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s run, the game `500` times and see what maximum, minimum, and average
    scores we get. This is demonstrated in the following example:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们运行游戏 `500` 次，看看我们得到的最大分数、最小分数和平均分数。这在以下示例中得以演示：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code generates the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Randomly picking an action and applying it is probably not the best strategy.
    There are many algorithms for finding solutions to make the agent learn from playing
    the game and apply the best actions. In this chapter, we shall apply Deep Q Network
    for learning from the game. The reader is encouraged to explore other algorithms.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择一个动作并应用它可能不是最佳策略。为了让智能体通过玩游戏学习并应用最佳动作，有许多算法可以用来找到解决方案。在本章中，我们将应用深度 Q 网络来从游戏中学习。我们鼓励读者探索其他算法。
- en: DQN for deep reinforcement learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 用于深度强化学习
- en: The **Deep Q Networks** (**DQN**) are based on Q-learning. In this section,
    we will explain both of them before we implement the DQN in Keras to play the
    PacMan game.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度 Q 网络**（**DQN**）是基于 Q-learning 的。在本节中，我们将在实现 DQN 来玩 PacMan 游戏之前，先解释这两者。'
- en: '**Q-learning**: In Q-learning, the agent learns the action-value function,
    also known as the Q-function. The *Q* function denoted with *q(s*,*a)* is used
    to estimate the long-term value of taking an action *a* when the agent is in state
    *s*. The *Q* function maps the state-action pairs to the estimates of long-term
    values, as shown in the following equation:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q-learning**：在 Q-learning 中，智能体学习动作值函数，也称为 Q 函数。*Q* 函数表示为 *q(s*,*a)*，用于估计当智能体处于状态
    *s* 时采取动作 *a* 的长期价值。*Q* 函数将状态-动作对映射到长期价值的估计值，如下方程所示：'
- en: '![](img/eab73493-acce-4360-8164-a86d11c95447.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eab73493-acce-4360-8164-a86d11c95447.png)'
- en: 'Thus, under a policy, the *q*-value function can be written as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在策略下，*q* 值函数可以写作如下：
- en: '![](img/28dd49bb-5f0b-43ea-96dc-6a94209695cb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28dd49bb-5f0b-43ea-96dc-6a94209695cb.png)'
- en: 'The *q* function can be recursively written as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*q* 函数可以递归地写作如下：'
- en: '![](img/3041f8dd-a123-4a77-bcf0-0206b13f832a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3041f8dd-a123-4a77-bcf0-0206b13f832a.png)'
- en: 'The expectation can be expanded as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值可以展开如下：
- en: '![](img/c71a670c-3b18-47a3-8821-2c15ac30b20e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c71a670c-3b18-47a3-8821-2c15ac30b20e.png)'
- en: 'An optimal *q* function is the one that returns the maximum value, and an optimal
    policy is the one that applies the optimal *q* function. The optimal *q* function
    can be written as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个最优的 *q* 函数是返回最大值的函数，而一个最优策略是应用最优 *q* 函数的策略。最优 *q* 函数可以写作如下：
- en: '![](img/f4c59cf4-9276-49c5-b00a-950c70652361.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4c59cf4-9276-49c5-b00a-950c70652361.png)'
- en: This equation represents the **Bellman Optimality Equation**. Since directly
    solving this equation is difficult, Q-learning is one of the methods used to approximate
    the value of this function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表示**贝尔曼最优方程**。由于直接求解这个方程很困难，Q-learning 是一种用来逼近该函数值的方法。
- en: Thus, in Q-learning, a model is built that can predict this value, given the
    state and action. Generally, this model is in the form of a table that contains
    all the possible combinations of state *s* and action *a*, and the expected value
    from that combination. However, for situations with a large number of state-action
    combinations, this table becomes cumbersome to maintain. The DQN helps to overcome
    this shortcoming of table-based Q-learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 Q-learning 中，构建了一个模型，该模型能够预测给定状态和动作下的值。通常，这个模型以表格的形式存在，包含了所有可能的状态 *s* 和动作
    *a* 的组合，以及该组合的期望值。然而，对于状态-动作组合数量较大的情况，这个表格就变得难以维护。DQN 有助于克服基于表格的 Q-learning 的这一缺点。
- en: The DQN: In DQN, instead of tables, a neural network model is built that learns
    from the state-action-reward-next state tuples and predicts the approximate q-value
    based on the state and action provided. Since the sequence of states-action-rewards
    is correlated in time, deep learning faces the challenge, since, in deep learning,
    the input samples need to be i.i.d. Thus, in DQN algorithms, **experience replay**
    is used to alleviate that. In the experience replay, the previous actions and
    their results are sampled randomly to train the network.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN：在 DQN 中，神经网络模型被用来从状态-动作-奖励-下一状态元组中学习，并根据提供的状态和动作预测近似的 q 值。由于状态-动作-奖励序列在时间上是相关的，深度学习面临挑战，因为深度学习中的输入样本需要是独立同分布（i.i.d.）。因此，在
    DQN 算法中，**经验回放**被用来缓解这一问题。在经验回放中，之前的动作及其结果被随机采样，用于训练网络。
- en: 'The basic Deep Q-learning algorithm is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的深度 Q 学习算法如下：
- en: Start the play in its initial state
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始状态开始游戏
- en: Select to explore or exploit
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择探索或利用
- en: If you selected exploit, then predict the action with the neural network and
    take the predicted action
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你选择了利用，则通过神经网络预测动作并执行预测的动作
- en: If you selected explore, then randomly select an action
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你选择了探索，则随机选择一个动作
- en: Record the previous state, action, rewards, and next state in the experience
    buffer
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录之前的状态、动作、奖励和下一状态到经验缓存中
- en: Update the `q_values` using `bellman` function
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `bellman` 函数更新 `q_values`
- en: Train the neural network with `states`, `actions`, and `q_values`
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `states`、`actions` 和 `q_values` 训练神经网络
- en: Repeat from *step 2*
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*第 2 步*开始重复
- en: To improve the performance, and implement experience replay, one of the things
    you can do is to randomly select the training data in *step 7*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能并实现经验回放，你可以在*第 7 步*中随机选择训练数据。
- en: Applying DQN to a game
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 DQN 应用到游戏中
- en: So far, we have randomly picked an action and applied it to the game. Now, let's
    apply DQN for selecting actions for playing the PacMan game.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经随机选择了一个动作并将其应用于游戏。现在，让我们应用 DQN 来选择动作，以便玩吃豆人游戏。
- en: 'We define the `q_nn` policy function as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了 `q_nn` 策略函数，如下所示：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we modify the `episode` function to incorporate calculation of `q_values`
    and `train` the neural network on the sampled experience buffer. This is shown
    in the following code:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们修改 `episode` 函数，将 `q_values` 的计算和神经网络训练加入到采样的经验缓存中。代码如下所示：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define an `ex``periment` function that will run for a specific number of episodes;
    each episode runs until the game is lost, namely when `done` is `True`. We use `rewards_max` to
    indicate when to break out of the loop as we do not wish to run the experiment
    forever, as shown in the following code:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `experiment` 函数，该函数将在特定数量的回合中运行；每个回合运行直到游戏失败，即 `done` 为 `True` 时结束。我们使用
    `rewards_max` 来表示何时退出循环，因为我们不希望实验永远运行，代码如下所示：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create a simple MLP network with the following code:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码创建一个简单的 MLP 网络：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code generates the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create an empty list to contain the game memory and define other hyperparameters
    and run the experiment for one episode, as shown in the following example:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空列表来保存游戏记忆，并定义其他超参数，运行一个回合的实验，如下所示：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result we get is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果如下：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: That is definitely an improvement in our case, but, in your case, it might be
    different. In this case, our game has only learned from a limited memory and only
    from game replay in one episode.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，这无疑是一个改进，但在你的案例中可能会有所不同。在这种情况下，我们的游戏只从有限的记忆中学习，并且仅仅通过一次回合中的游戏回放进行学习。
- en: 'Now, run it for `100` episodes, as shown in the following example:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，运行 `100` 回合，如下所示：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We get the following results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Thus we see that, on average, the results did not improve, although we reached
    a high max reward. Tuning the network architecture, features, and hyperparameters
    might produce better results. We would encourage you to modify the code. As an
    example, instead of MLP, you can use the simple one-layer convolutional network,
    as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到，尽管我们达到了高的最大奖励，平均结果并没有得到改善。调整网络架构、特征和超参数可能会产生更好的结果。我们鼓励你修改代码。例如，你可以用简单的单层卷积网络替代
    MLP，如下所示：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code displays the network summary as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了网络摘要，如下所示：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned what reinforcement learning is. Reinforcement learning
    is an advanced technique that you will find is often used to solve complex problems. We
    learned about OpenAI Gym, a framework that provides an environment for simulating
    many popular games in order to implement and practice reinforcement learning algorithms.
    We touched on deep reinforcement learning concepts, and we encourage you to explore
    books (mentioned in the further reading) specifically written about reinforcement
    learning to learn deeply about the theories and concepts.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了什么是强化学习。强化学习是一种高级技术，你会发现它常用于解决复杂问题。我们了解了 OpenAI Gym，这个框架提供了一个环境来模拟许多流行的游戏，以便实现和实践强化学习算法。我们简要介绍了深度强化学习的概念，我们鼓励你去阅读一些专门写关于强化学习的书籍（在深入阅读中提到），以便深入了解其理论和概念。
- en: We learned how to play the PacMan game in OpenAI Gym. We implemented DQN and
    used it to learn to play the PacMan game. We only used an MLP network to keep
    things simple, but, for complex examples, you may end up using complex CNN, RNN,
    or Sequence-to-Sequence models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何在 OpenAI Gym 中玩 PacMan 游戏。我们实现了 DQN，并使用它来学习玩 PacMan 游戏。为了简化，我们只使用了一个
    MLP 网络，但对于复杂的示例，你可能最终会使用复杂的 CNN、RNN 或 Sequence-to-Sequence 模型。
- en: In the next chapter, we shall learn about future opportunities in the fields
    of machine learning and TensorFlow.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习机器学习和 TensorFlow 领域的未来机会。
- en: Further Reading
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: Deep Reinforcement Learning Hands-On by Maxim Lapan, Packt Publications
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maxim Lapan 的《深度强化学习实战》，Packt 出版社
- en: 'Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G.
    Barto'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richard S. Sutton 和 Andrew G. Barto 的《强化学习：导论》
- en: 'Statistical Reinforcement Learning: Modern Machine Learning Approaches by Masashi
    Sugiyama'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masashi Sugiyama 的《统计强化学习：现代机器学习方法》
- en: Algorithms for reinforcement learning by Csaba Szepesvari
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Csaba Szepesvari 的《强化学习算法》
