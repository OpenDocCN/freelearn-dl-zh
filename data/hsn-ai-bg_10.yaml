- en: Deep Learning for Game Playing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏AI的深度学习
- en: Over the past several years, one of the most notable applications of **Artificial** **Intelligence **(**AI**)
    has been in the game-playing space. Especially with the recent success of AlphaGo
    and AlphaGo Zero, game-playing AIs have been a topic of great public interest. In
    this chapter, we'll implement two basic versions of game-playing AIs; one for
    a video game and one for a board game. We'll primarily be utilizing reinforcement
    learning methods as our workhorse. We'll also touch upon the methods behind some
    of the most advanced game-playing AIs in existence at the moment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，**人工智能**（**AI**）最显著的应用之一就是游戏领域，尤其是随着AlphaGo和AlphaGo Zero的成功，游戏类AI成为公众广泛关注的话题。在本章中，我们将实现两种基本的游戏AI版本；一种用于视频游戏，另一种用于棋盘游戏。我们主要会使用强化学习方法作为我们的核心方法。我们还将介绍一些目前存在的最先进的游戏AI背后的方法。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涵盖以下主题：
- en: Game Trees and Fundamental Game Theory
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏树与基本博弈论
- en: Constructing an AI agent to play tic-tac-toe
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个AI代理来玩井字游戏
- en: Constructing ...
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建...
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be utilizing the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下内容：
- en: TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: OpenAI gym
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: It is recommend that you have access to a GPU for training the models in this
    chapter, whether on your machine itself or via a cloud service platform.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议你在训练本章中的模型时，能够使用GPU，无论是在自己的机器上还是通过云服务平台。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: AI has been making a splash lately in the game-playing arena. DeepMind, the
    Google research group that created AlphaGo, have been proponents of utilizing
    reinforcement learning methods for game-playing applications. More and more video
    game companies are using deep learning methods for their AI players. So, where
    are we coming from, and where are we going with AI in the gaming space?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AI最近在游戏领域引起了广泛关注。创造AlphaGo的Google研究团队DeepMind一直提倡在游戏应用中使用强化学习方法。越来越多的视频游戏公司开始使用深度学习方法来开发其AI玩家。那么，我们从哪里开始，AI在游戏领域的发展又将走向何方呢？
- en: Traditionally, game-playing systems have been made up of a combination of hardcoded
    rules that have covered the range of behaviors that the AI is supposed to cover.
    Have you ever played an older adventure, first-person shooter, or strategy game
    where the AI players were clearly operating off a hardcoded strategy? More often
    than not, these AIs used ...
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，游戏AI系统由一系列硬编码规则组成，覆盖了AI应对的各种行为。你是否曾玩过一些老旧的冒险游戏、第一人称射击游戏或策略游戏，其中的AI玩家显然是根据硬编码策略运作的？这些AI通常...
- en: Networks for board games
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 棋盘游戏的网络
- en: 'When we talk about creating algorithms for game-playing, we are really talking
    about creating them for a specific type of game, known as a **finite two person
    zero**-**sum sequential game**. This is really just a fancy way of saying the
    following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论为游戏设计算法时，实际上是在谈论为特定类型的游戏创建算法，这种游戏被称为**有限的二人零和顺序游戏**。这实际上就是在说：
- en: An interactive situation between two independent actors (a game)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个独立角色之间的互动情境（一个游戏）
- en: There are a finite amount of ways in which the two actors can interact with
    each other at any point
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何时刻，两方玩家的互动方式是有限的
- en: The game is zero-sum, meaning that the end state of the game results in a complete
    win for one of the actors
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏是零和的，这意味着游戏的最终状态导致其中一方完全获胜
- en: The game is sequential, meaning that the actors make their moves in sequence,
    one after another
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏是顺序进行的，这意味着角色按顺序一个接一个地进行移动
- en: 'Classic examples of these types of games that we''ll cover in this section
    are Tic Tac Toe, Chess, and the game of Go. Since creating and training an algorithm
    for a board game such as Go would be an immense task, for time and computation
    constraint, we''ll be creating an agent to compete in a much more reasonable game
    with a finite amount of steps: chess. In this section, we''ll introduce some of
    the fundamental concepts behind game-playing AIs, and walk through examples in
    Python for the different strategies.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将介绍一些经典的游戏类型示例，如井字游戏、国际象棋和围棋。由于为围棋这样的棋盘游戏创建和训练一个算法将是一个巨大的任务，出于时间和计算约束，我们将在一个有限步骤且更为合理的游戏中创建一个代理：国际象棋。在本节中，我们将介绍一些游戏AI背后的基本概念，并通过Python示例演示不同的策略。
- en: Understanding game trees
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解游戏树
- en: 'If you''re familiar with formal **game theory**, then you know that there is
    an entire branch of mathematics devoted to understanding and analyzing games.
    In the computer science world, we can analyze a game by simplifying it into a
    decision tree structure called a **game tree**. Game trees are a way of mapping
    out possible moves and states within a board game. Let''s take the simple example
    of a game tree for Tic Tac Toe, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉**博弈论**，你就知道有一整个数学分支致力于理解和分析游戏。在计算机科学的世界里，我们可以通过将游戏简化为一个决策树结构来分析它，这就是**博弈树**。博弈树是一种描绘棋盘游戏中可能走法和状态的方法。让我们以井字游戏的博弈树为例，如下所示：
- en: '![](img/f66fda2e-7c5a-4229-bf6b-eb1e35775fb6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f66fda2e-7c5a-4229-bf6b-eb1e35775fb6.png)'
- en: This tree gives us all of the possible combinations and outcomes for the game
    based on a certain starting point. Moving from one node to another represents
    a **move** in the game, and moves continue ...
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树展示了基于某一特定起始点的所有可能组合和游戏结果。从一个节点到另一个节点的移动表示**一步棋**，棋步持续进行...
- en: AlphaGo and intelligent game-playing AIs
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo和智能游戏AI
- en: While MCTS has been a cornerstone of game-playing AI for a while, it was DeepMind's
    AlphaGo program that really took game-playing AIs into the modern age. AlphaGo
    and its derivatives (AlphaGo Zero and AlphaGo Master) are game-playing AI systems
    that utilize MCTS to play the notoriously difficult ancient Chinese game of Go.
    With 10^(761) possible game combinations, creating a system to play the game became
    something of a milestone in the AI world. It's even the subject of a much talked
    about documentary by the same name.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MCTS已经是游戏AI的基石，但正是DeepMind的AlphaGo程序将游戏AI带入了现代时代。AlphaGo及其衍生版本（AlphaGo Zero和AlphaGo
    Master）是利用MCTS来玩极其复杂的古老中国围棋的游戏AI系统。围棋有10^(761)种可能的棋局组合，创造一个可以玩这款游戏的系统成为了AI领域的一个里程碑。它甚至成为了一部同名的广受关注的纪录片的主题。
- en: AlphaGo uses a combination of MCTS with deep learning methods that made the
    Alpha Go programs truly extraordinary. DeepMind trained deep neural networks,
    such as the ones that we have been learning about throughout this book, to learn
    the state of the game and effectively guide the MCTS in a more intelligent manner.
    This network looks at the current state of the board, along with the previous
    moves that have been made, and decides which move to play next. DeepMind's major
    innovation with AlphaGo was to use deep neural networks to understand the state
    of the game, and then use this understanding to intelligently guide the search
    of the MCTS. The system was architected in a way that AlphaGo would teach itself
    to learn the game, first by watching humans play the game, and secondly by playing
    the game against itself and correcting itself for its prior mistakes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo结合了MCTS和深度学习方法，使得AlphaGo程序变得非凡。DeepMind训练了深度神经网络，类似于我们在本书中学习的那种，来学习游戏状态，并有效地引导MCTS以更智能的方式进行搜索。这个网络会查看棋盘的当前状态，以及之前的棋步，决定接下来该下什么棋。DeepMind在AlphaGo中的主要创新是使用深度神经网络来理解游戏的状态，然后利用这种理解来智能地引导MCTS的搜索。系统的架构方式使得AlphaGo能够自我学习游戏，首先通过观看人类下棋，其次通过与自己对弈并纠正之前的错误。
- en: 'The architecture actually uses two different neural networks; a policy network
    and a value network:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构实际上使用了两种不同的神经网络：策略网络和价值网络：
- en: '**Value network**: Reduces the depth of the MCTS search by approximating a
    value function.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值网络**：通过近似价值函数来减少MCTS搜索的深度。'
- en: '**Policy network**: Reduces the breadth of the MCTS search by simulating future
    actions. The policy network learns from actual human play of the game Go, and
    develops a policy accordingly:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略网络**：通过模拟未来的行动来减少MCTS搜索的广度。策略网络从实际的人类围棋游戏中学习，并据此发展出策略：'
- en: '![](img/2827cca8-d649-45e4-8cf4-20107b4b8e06.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2827cca8-d649-45e4-8cf4-20107b4b8e06.png)'
- en: Let's dive into each of these to understand how the system works.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这两者，以理解系统是如何工作的。
- en: AlphaGo policy network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo策略网络
- en: 'The goal of the policy network is to capture and understand the general actions
    of players on the board in order to aid the MCTS by guiding the algorithm toward
    promising actions during the search process; this reduces the **breadth of the
    search**. Architecturally, the policy network comes in two parts: a supervised
    learning policy network and a reinforcement learning policy network.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 策略网络的目标是捕捉并理解棋盘上玩家的一般行动，以帮助MCTS在搜索过程中引导算法朝向有前景的行动；这减少了**搜索的广度**。在架构上，策略网络分为两部分：一个监督学习策略网络和一个强化学习策略网络。
- en: The first network, the supervised network, is a 13-layer **Convolutional Neural
    Network** (**CNN**). It was trained by observing the moves that humans make while
    playing the game – 30 million, to be exact – and outputs a probability distribution
    for each action given a certain state. We call this type of supervised learning **behavior
    cloning**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个网络，监督网络，是一个13层的**卷积神经网络**（**CNN**）。它通过观察人类在游戏中进行的动作进行训练——准确来说是3000万次——并根据特定状态输出每个动作的概率分布。我们将这种类型的监督学习称为**行为克隆**。
- en: The ...
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: AlphaGo value network
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo价值网络
- en: 'The value network was used to reduce the error in the system''s play by guiding
    MCTS toward certain nodes. It helped reduce the **depth of the search**. The AlphaGo
    value network was trained by playing further games against itself in order to
    optimize the policy that it learned from the policy networks by estimating the
    value function, specifically the action value function. Recall from [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, that action value functions describe the value of taking a certain
    action while in a specific state. It measures the cumulative reward from a pair
    of states and actions; for a given state and action we take, how much will this
    increase our reward? It lets us postulate what would happen by taking a different
    action in the first time step than what they may want the agent to do, and then
    afterward following the policy. The action value function is also often called
    the **Q**-**function**,because of the Q that we use to represent it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 价值网络被用来通过引导MCTS朝向某些节点来减少系统玩法中的误差。它有助于减少**搜索的深度**。AlphaGo价值网络通过与自己进行更多的对局来进行训练，以优化从策略网络学到的策略，方法是通过估算价值函数，特别是行动价值函数。回顾一下[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)，*强化学习*，行动价值函数描述了在特定状态下采取某个行动的价值。它衡量的是从一对状态和行动中得到的累积奖励；对于给定的状态和我们采取的行动，这会增加多少奖励？它让我们假设，如果在第一步采取不同的行动，而不是代理人可能希望做的，然后在之后遵循策略，会发生什么。行动价值函数通常也被称为**Q**-**函数**，因为我们用Q来表示它：
- en: '![](img/31ddcfc7-7f63-4340-a66e-c9434f7e38c9.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31ddcfc7-7f63-4340-a66e-c9434f7e38c9.png)'
- en: The network approximated the value function by utilizing a noisy version of
    the policy from the policy network and regressing the state of the board against
    the result of the game.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过利用来自策略网络的噪声版本的策略并回归棋盘状态与游戏结果，来逼近价值函数。
- en: The network was trained using the reinforce algorithm that we learned about
    in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning*.
    If you recall, Reinforce is a Monte Carlo policy gradient method that uses likelihood
    ratios to estimate the value of a policy at a given point. The Reinforce algorithm
    attempts to maximize the expected reward, so that the entire system has the dual
    goal of playing like a professional human player while attempting to win the game.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络使用我们在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)中学习的强化算法进行训练，*强化学习*。如果你还记得，Reinforce是一种蒙特卡洛策略梯度方法，使用似然比来估计某一点上的策略价值。Reinforce算法试图最大化预期奖励，因此整个系统有两个目标：像专业人类玩家一样玩游戏，同时尝试赢得比赛。
- en: AlphaGo in action
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo在行动中
- en: 'We''ve gone over how AlphaGo helped select actions, so now let''s get back
    to the core of any game-playing system: the game tree. While AlphaGo utilized
    game trees and MCTS, the authors created a variation of it called *asynchronous
    policy* and *value MCTS* (APV-MCTS). Unlike standard MCTS, which we discussed
    previously, APV-MCTS decides which node to expand and evaluate by two different
    metrics:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了AlphaGo如何帮助选择行动，现在让我们回到任何游戏系统的核心：游戏树。虽然AlphaGo利用了游戏树和MCTS，作者们创造了一个变种，称为*异步策略*和*价值MCTS*（APV-MCTS）。与我们之前讨论的标准MCTS不同，APV-MCTS通过两种不同的指标来决定扩展和评估哪个节点：
- en: The outcome of the value network
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值网络的结果
- en: The outcome of the Monte Carlo simulations
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛模拟的结果
- en: The results of these methods are combined with mixing parameters, λ. The algorithm
    then chooses an action according to the probabilities that were obtained during
    the initial supervised learning phase. While it may seem counter intuitive to
    use the probabilities ...
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的结果与混合参数λ相结合。然后，算法根据在初始监督学习阶段获得的概率选择一个动作。虽然使用这些概率似乎有些反直觉……
- en: Networks for video games
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频游戏的网络
- en: 'Thus far, we''ve learned how we can use reinforcement learning methods to play
    board games utilizing UCT and MCTS; now, let''s see what we can do with video
    games. In [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, we saw how we could use reinforcement learning methods to complete
    basic tasks such as the OpenAI cartpole challenge. In this section, we''ll be
    focusing on a more difficult set of games: classic Atari video games, which have
    become standard benchmarks for deep learning tasks.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用强化学习方法来玩桌面游戏，利用UCT和MCTS；现在，让我们看看在视频游戏中我们能做些什么。在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)《强化学习》中，我们看到如何使用强化学习方法来完成诸如OpenAI倒立摆挑战之类的基本任务。在这一节中，我们将重点关注一组更困难的游戏：经典的雅达利视频游戏，这些游戏已成为深度学习任务的标准基准。
- en: You might be thinking – *can't we extend the methods that we used in the cartpole
    environment to Atari games?* While we can, there's a lot more input that we have
    to handle. In Atari environments, and really any video game environment, the inputs
    to the network are individual pixels. Instead of the simple four control variables
    for cartpole, we are now dealing with 100,800 variables (210 * 160 * 3). As such,
    complexity and training times for these networks can be much higher. In this section,
    we'll try to make the network as simple as possible in order to make it easier
    to learn from and train.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想——*我们不能将使用在倒立摆环境中的方法扩展到雅达利游戏中吗？* 其实可以，但我们需要处理的输入信息要多得多。在雅达利环境中，实际上在任何视频游戏环境中，网络的输入都是单个像素。与倒立摆的四个简单控制变量不同，我们现在处理的是100,800个变量（210
    * 160 * 3）。因此，这些网络的复杂性和训练时间可能会更高。在这一节中，我们将尽量简化网络，使其更容易学习和训练。
- en: 'We''ll be using the OpenAI gym environment to simulate the Atari game Space
    Invaders:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用OpenAI Gym环境来模拟雅达利游戏《太空入侵者》：
- en: '![](img/2798b969-f161-453a-a10a-e9d05fee3ab6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2798b969-f161-453a-a10a-e9d05fee3ab6.png)'
- en: For those of you who aren't familiar with Space Invaders, the concept is simple –
    you (the green rocket at the bottom) must destroy a grouping of alien spaceships
    before they destroy you. The spaceships attempt to hit you with missiles, and
    vice versa. Google's DeepMind originally introduced Space Invaders as a benchmark
    task in their paper *Playing Atari with Deep Reinforcement Learning*, which really
    set off the concept of Atari games as benchmarks to beat with these intelligent
    agents.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不熟悉《太空入侵者》的朋友来说，概念很简单——你（屏幕底部的绿色火箭）必须在外星飞船摧毁你之前摧毁它们。外星飞船会用导弹攻击你，你也可以反击。谷歌的DeepMind最早在他们的论文《用深度强化学习玩雅达利》*Playing
    Atari with Deep Reinforcement Learning*中将《太空入侵者》作为基准任务提出，这也真正推动了将雅达利游戏作为基准任务，用智能体击败的概念。
- en: We'll be constructing something called a **Deep Q-network**, which we touched
    upon in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning. *In the next section, we expand upon many of the fundamental Q-learning
    subjects that we covered in that chapter. With that – let's dive in!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个叫做**深度Q网络**的东西，我们在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)《强化学习》中提到过它。在下一节中，我们将进一步探讨在该章节中涵盖的许多基本Q学习内容。话不多说——让我们开始吧！
- en: Constructing a Deep Q–network
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度Q网络
- en: Deep Q-networks were first introduced by DeepMind in their paper *Human-level
    control through deep reinforcement* *learning,* published in the British scientific
    journal *Nature*, and now commonly referred to as the *Nature Paper*. The goal
    of Deep Q-learning was to create an AI agent that could learn a policy from high-dimensional
    inputs such as video games. In our case, we'll want to construct a Deep Q-network
    that can advance through basic tasks and then towards harder tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络最早由DeepMind在他们的论文《通过深度强化学习实现人类级别的控制》*Human-level control through deep reinforcement
    learning*中提出，该论文发表在英国科学期刊《Nature》上，现在通常被称为*Nature论文*。深度Q学习的目标是创建一个能够从高维输入（如视频游戏）中学习策略的AI代理。在我们的案例中，我们希望构建一个能够通过基本任务并向更困难任务推进的深度Q网络。
- en: Deep Q-networks approximate Q-values instead of calculating them individually
    with Q tables, and they do this by using artificial neural networks as a value
    approximator. The input of the network will be a stack of preprocessed frames,
    and the ...
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络通过使用人工神经网络作为值逼近器来逼近Q值，而不是使用Q表单独计算它们。网络的输入将是一堆预处理过的帧，接下来...
- en: Utilizing a target network
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用目标网络
- en: 'Let''s look back at the Q- function optimization process:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下Q-函数优化过程：
- en: '![](img/3f120be8-ba2f-4aa8-b4df-31fe58195ff7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f120be8-ba2f-4aa8-b4df-31fe58195ff7.png)'
- en: You might notice that this function has a unique property in that it's recursive;
    one set of Q-values depend on the value of the other set of Q-values. This becomes
    a problem in training; if we change one set of values, we'll end up changing the
    other set of values. To put it simply, as we get closer to the targeted Q-value,
    that Q-value moves even further away. It is continually moving the finish line
    when you are about to finish a race!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，这个函数有一个独特的属性，那就是它是递归的；一组Q值依赖于另一组Q值的值。这在训练中会成为一个问题；如果我们改变一组值，就会改变另一组值。简单来说，当我们接近目标Q值时，Q值会变得更远。它就像是在你快要完成比赛时不断把终点线往后移！
- en: 'To remedy this, the Q-network creates a copy of itself every 10,000 iterations,
    called a **target network**, which will represent the targeted Q-value. We can
    do this in TensorFlow by creating a target network variable that we''ll first
    initialize with the class, and later run in a TensorFlow session:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Q网络每经过10,000次迭代就会创建一个自身的副本，称为**目标网络**，它将表示目标Q值。我们可以通过在TensorFlow中创建一个目标网络变量来实现这一点，首先用类初始化，然后在TensorFlow会话中运行：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As for the 10,000 iterations, we've already defined that as `self.update_time
    = 10000` when we started building out our DQN class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 至于10,000次迭代，我们在开始构建DQN类时已经定义了`self.update_time = 10000`。
- en: Experience replay buffer
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验回放缓冲区
- en: 'While we touched upon it briefly in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, let''s dive into the technical details of experience replay buffers.
    Experience replay is a biologically inspired tool that stores an agent''s experience
    at each time step process. Specifically, it stores the [state, action, reward,
    next_state] pairs at each time step:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)中简要提到过，*强化学习*，但现在让我们深入探讨一下经验回放缓冲区的技术细节。经验回放是一种受生物启发的工具，它在每个时间步骤的过程中存储智能体的经验。具体来说，它在每个时间步骤存储[状态、动作、奖励、下一个状态]的配对：
- en: '![](img/8358a2c5-8cb6-4bea-a46a-f3dea5f8c237.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8358a2c5-8cb6-4bea-a46a-f3dea5f8c237.png)'
- en: 'Instead of running Q-learning on state-action pairs as they occur, experience
    replay stores these pairs as they are discovered. Experience replay buffers help
    with two things:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在状态-动作对发生时立即运行Q学习，经验回放将这些对在它们被发现时存储起来。经验回放缓冲区有两个作用：
- en: Remember past experiences by storing and randomly sampling them
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过存储并随机抽样来记住过去的经验
- en: Reduce the chance that experiences will ...
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少经验被...的机会
- en: Choosing action
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择动作
- en: Thus far, we've told our network to follow random weights that we've initialized
    for it, without giving it direction on how to decide what actions to take to update
    those weights. In the case of policy methods such as the ones we used in [Chapter
    8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning*, and
    preceding with Tic Tac Toe, Q-learning methods work toward approximating the value
    of the Q-function instead of learning a policy directly. So, what do we do?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经告诉网络遵循我们为其初始化的随机权重，但并没有给它指导，告诉它如何决定采取哪些动作来更新这些权重。在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)中使用的策略方法，比如我们在*强化学习*中的应用，以及之前的井字棋案例，Q-learning方法的目标是通过逼近Q函数的值，而不是直接学习策略。那么，我们该怎么做呢？
- en: Deep Q-networks use a tactic called **exploration** to determine what actions
    to take. If we didn't use a tactic, our network would probably be limited to the
    most basic levels of the game because it wouldn't have any idea what moves would
    allow it to improve!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络使用一种叫做**探索**的策略来决定采取什么动作。如果我们不使用这种策略，网络可能只会局限于游戏的最基本层次，因为它不会知道哪些动作能够帮助它提升！
- en: 'To remedy this, we will utilize a strategy called** ∈-greedy**. This strategy
    works by choosing actions to learn from based on two methods; first, choosing
    methods that give our model the highest reward (maximum Q-value), and second,
    choosing methods at random with the probability ∈. In this strategy, we use a
    basic inequality:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将采用一种叫做 **∈-贪婪** 的策略。这种策略通过基于两种方法选择学习的动作；首先，选择那些为我们的模型提供最高奖励（最大 Q
    值）的方法；其次，以 ∈ 的概率随机选择方法。在这种策略中，我们使用一个基本的不等式：
- en: '![](img/eba46b04-720b-4280-9119-5947fa5b3557.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eba46b04-720b-4280-9119-5947fa5b3557.png)'
- en: 'We set the epsilon variable as 1, and draw random numbers from a distribution.
    If the number doesn''t satisfy the inequality and is less than epsilon, our agent **explores** the
    space, meaning it seeks to figure out more information about where it is in order
    to start choosing state/action pairs so that we can calculate the Q-value. If
    the number is in fact larger than epsilon, we **exploit** the information that
    we already know to choose a Q-value to calculate. The algorithm starts with a
    high ∈ and reduces its value by a fixed amount as training continues. We call
    this process **annealing**. In the original DeepMind paper, the researchers used
    this strategy with an annealing from 1 to 0.1 over the first million frames, and
    then held at 0.1 afterward. Let''s look back at the parameters we initialized
    in the beginning of the section:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 epsilon 变量设置为 1，并从一个分布中抽取随机数。如果这个数字不满足不等式并且小于 epsilon，我们的代理将**探索**这个空间，这意味着它会寻求更多关于它当前所在位置的信息，从而开始选择状态/动作对，以便我们可以计算
    Q 值。如果数字实际上大于 epsilon，我们将**利用**我们已经知道的信息来选择一个 Q 值进行计算。算法以较高的∈开始，并在训练过程中按固定量减少其值。我们称这个过程为**退火**。在原始的
    DeepMind 论文中，研究人员使用了这种策略，在前百万帧内将 epsilon 从 1 退火到 0.1，然后在之后保持 0.1。让我们回顾一下我们在本节开始时初始化的参数：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You'll notice that we used these exact specifications. During the testing phase
    of the network, epsilon will be considerably lower, and hence be biased toward
    an exploitation strategy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们使用了这些确切的规格。在网络的测试阶段，epsilon 将会显著降低，因此更偏向于利用策略。
- en: 'Let''s implement this strategy in Python and take a closer look at the mechanics.
    We''ll define a function, `getAction`, which sits within our `deepQ` class:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Python 中实现这个策略，并仔细研究它的机制。我们将定义一个名为 `getAction` 的函数，它位于我们的 `deepQ` 类中：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We''ll also adjust our epsilon value:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将调整我们的 epsilon 值：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we've defined the bulk of our network, let's move on to training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了大部分网络，接下来我们进入训练阶段。
- en: Training methods
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练方法
- en: 'First, let''s define our training method. We''ll call this function `trainingPipeline`;
    it will take in an action input as well as a *y* input which represents the targeting
    Q-value, which we''ll define here as placeholders, and calculate a Q-value for
    an action based on those action/state pairs:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们的训练方法。我们将这个函数命名为 `trainingPipeline`；它将接受一个动作输入以及一个 *y* 输入，*y* 代表目标
    Q 值，我们将其在这里定义为占位符，并根据这些动作/状态对计算 Q 值：
- en: '![](img/4e5acb70-ef78-414f-a93e-e7c04e723f4c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e5acb70-ef78-414f-a93e-e7c04e723f4c.png)'
- en: We'll use a **Mean Squared Error** (**MSE**) loss function and calculate it,
    utilizing the predicted Q-value minus the actual Q-value. Lastly, you might notice
    that we are using an optimizer here that you might not be familiar with, RMSProp.
    It's an adaptive learning rate optimizer similar to Adam that was proposed by
    Geoffrey Hinton. We won't ...
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**均方误差** (**MSE**) 损失函数并进行计算，利用预测的 Q 值减去实际的 Q 值。最后，你可能会注意到我们使用了一个你可能不太熟悉的优化器，RMSProp。它是一种自适应学习率优化器，类似于
    Adam，由 Geoffrey Hinton 提出。我们将...
- en: Training the network
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'We''ll give our training function a simple name: `train`. First, we''ll feed
    it mini-batches of data from the replay memory:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将给我们的训练函数起一个简单的名字：`train`。首先，我们将从回放记忆中输入小批次的数据：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we''ll calculate the Q-value for each batch:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算每个批次的 Q 值：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let''s bind this all together with out training method. We''ll take the
    `trainStep` variable that we defined and run the training cycle. We''ll feed in
    three variables as input; the targeted Q-value, an action, and a state:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这些内容结合起来，使用我们的训练方法。我们将使用之前定义的 `trainStep` 变量并运行训练周期。我们将输入三个变量作为输入；目标
    Q 值，一个动作和一个状态：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We''ll define a handler function to save network weights and states for us.
    While we didn''t go over the definition of the saver explicitly in this chapter,
    you can find it in the fully assembled code in the GitHub repository:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个处理函数来为我们保存网络权重和状态。虽然我们在本章中没有明确介绍保存器的定义，但你可以在GitHub仓库中的完整代码中找到它：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, let''s define the cycle the appends to experience replay:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们定义一个循环来将经验回放添加到其中：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We've assembled our network, so now let's move on to running it!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经组装好了网络，接下来让我们开始运行它！
- en: Running the network
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行网络
- en: 'Now, let''s get to the moment we''ve been waiting for! Let''s import `gym`,
    NumPy, our deep-q network file, as well as a few handler functions:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入我们一直期待的时刻！让我们导入`gym`、NumPy、我们的深度Q网络文件以及一些处理函数：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''ll define our agent class as `Atari`, and initialize the environment, network,
    and actions with the class:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义我们的代理类为`Atari`，并使用该类初始化环境、网络和动作：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Our Deep Q-network can''t innately ingest the Atari games, so we have to write
    a bit of preprocessing code to handle the video input. We''ll call this function
    `preprocess` and it will take in a single game observation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度Q网络无法直接处理Atari游戏，因此我们需要编写一些预处理代码来处理视频输入。我们将这个函数称为`preprocess`，它将接收一个单一的游戏观察：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We've learned a great deal in this chapter, from how to implement MCTS methods
    to play board games, to creating an advanced network to play an Atari game, and
    even the technology behind the famous AlphaGo system. Let's recap what we have
    learned.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学到了很多东西，从如何实现MCTS方法来玩棋盘游戏，到创建一个先进的网络来玩Atari游戏，甚至了解了著名的AlphaGo系统背后的技术。让我们回顾一下我们所学到的内容。
- en: Reinforcement learning methods have become the main tools to create AIs for
    playing games. Whether we are creating systems for real-life board games, or systems
    for video games, the fundamental concepts of policies, Q-learning, and more that
    we learned about in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, form the basis for these complex AI systems. When we create AIs for
    board games, we rely on the building block of the game tree, and use MCTS to simulate
    various game outcomes from that game tree. For more advanced systems such as AlphaGo
    and chess-playing AIs, we utilize neural networks to help guide MCTS and make
    its simulations more effective.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习方法已成为创建游戏AI的主要工具。无论是为现实生活中的棋盘游戏创建系统，还是为视频游戏创建系统，我们在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)《强化学习》中学到的关于策略、Q学习等基本概念，构成了这些复杂AI系统的基础。当我们为棋盘游戏创建AI时，我们依赖于游戏树的基本构建块，并使用MCTS来模拟从该游戏树中产生的各种游戏结果。对于像AlphaGo和棋类AI这样的更高级系统，我们利用神经网络来帮助指导MCTS，并使其模拟更为有效。
- en: When it comes to video game-playing AIs, we can utilize either policy gradient
    methods or Q-learning methods. In this chapter, we learned about utilizing a variant
    of the latter, deep Q-learning, to play the Atari game Space Invaders. Deep Q-learning
    makes advances on basic Q-learning by using techniques such as target networks
    and experience replay buffers to improve performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频游戏AI方面，我们可以利用策略梯度方法或Q学习方法。在本章中，我们学习了如何利用后者的一种变体——深度Q学习来玩Atari游戏《太空入侵者》。深度Q学习通过使用目标网络和经验回放缓冲区等技术，改进了基本的Q学习，从而提升了性能。
- en: We'll look more into how reinforcement learning methods can create intelligent
    systems in one of our upcoming chapters on deep learning for robotics.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的机器人深度学习章节中，我们将更深入地探讨强化学习方法如何创造智能系统。
