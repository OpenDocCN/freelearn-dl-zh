- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: The TensorFlow Way
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 方式
- en: In *Chapter 1*, *Getting Started with TensorFlow 2.x* we introduced how TensorFlow
    creates tensors and uses variables. In this chapter, we'll introduce how to put
    together all these objects using eager execution, thus dynamically setting up
    a computational graph. From this, we can set up a simple classifier and see how
    well it performs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 1 章*，*TensorFlow 2.x 入门*中，我们介绍了 TensorFlow 如何创建张量并使用变量。在本章中，我们将介绍如何使用急切执行将这些对象组合在一起，从而动态地设置计算图。基于此，我们可以设置一个简单的分类器，并查看其表现如何。
- en: Also, remember that the current and updated code from this book is available
    online on GitHub at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请记住，本书当前和更新后的代码可以在 GitHub 上在线获取，地址为[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)。
- en: 'Over the course of this chapter, we''ll introduce the key components of how
    TensorFlow operates. Then, we''ll tie it together to create a simple classifier
    and evaluate the outcomes. By the end of the chapter, you should have learned
    about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 TensorFlow 操作的关键组件。然后，我们将把它们结合起来，创建一个简单的分类器并评估结果。在本章结束时，你应该已学到以下内容：
- en: Operations using eager execution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用急切执行的操作
- en: Layering nested operations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层嵌套操作
- en: Working with multiple layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多层
- en: Implementing loss functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现损失函数
- en: Implementing backpropagation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: Working with batch and stochastic training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量训练和随机训练
- en: Combining everything together
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容结合在一起
- en: Let's start working our way through more and more complex recipes, demonstrating
    the TensorFlow way of handling and solving data problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始处理越来越复杂的配方，展示 TensorFlow 处理和解决数据问题的方法。
- en: Operations using eager execution
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用急切执行的操作
- en: Thanks to *Chapter 1*, *Getting Started with TensorFlow 2.x* we can already
    create objects such as variables in TensorFlow. Now we will introduce operations
    that act on such objects. In order to do so, we'll return to eager execution with
    a new basic recipe showing how to manipulate matrices. This recipe, and the following
    ones, are still basic ones, but over the course of the chapter, we'll combine
    these basic recipes into more complex ones.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢*第 1 章*，*TensorFlow 2.x 入门*，我们已经能够创建 TensorFlow 中的对象，如变量。现在，我们将介绍针对这些对象的操作。为了做到这一点，我们将返回到急切执行，并通过一个新的基本配方展示如何操作矩阵。本配方及后续配方仍然是基础的，但在本章过程中，我们将把这些基础配方结合起来形成更复杂的配方。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To start, we load TensorFlow and NumPy, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载 TensorFlow 和 NumPy，如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That's all we need to get started; now we can proceed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们开始所需的一切；现在我们可以继续进行。
- en: How to do it...
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this example, we'll use what we have learned so far, and send each number
    in a list to be computed by TensorFlow commands and print the output.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用到目前为止学到的内容，将列表中的每个数字发送到 TensorFlow 命令进行计算并打印输出。
- en: 'First, we declare our tensors and variables. Here, out of all the various ways
    we could feed data into the variable using TensorFlow, we will create a NumPy
    array to feed into our variable and then use it for our operation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们声明我们的张量和变量。在所有可能的将数据传递给变量的方式中，我们将创建一个 NumPy 数组来传递给变量，然后将其用于操作：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出结果如下：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once you get accustomed to working with TensorFlow variables, constants, and
    functions, it will become natural to start from NumPy array data, progress to
    scripting data structures and operations, and test their results as you go.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你习惯了使用 TensorFlow 变量、常量和函数，就会自然而然地从 NumPy 数组数据开始，逐步构建数据结构和操作，并在过程中测试它们的结果。
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Using eager execution, TensorFlow immediately evaluates the operation values,
    instead of manipulating the symbolic handles referred to the nodes of a computational
    graph to be later compiled and executed. You can therefore just iterate through
    the results of the multiplicative operation and print the resulting values using
    the `.NumPy` method, which returns a NumPy object from a TensorFlow tensor.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用急切执行，TensorFlow 会立即评估操作值，而不是操作符号句柄，这些句柄指向计算图中的节点，后者将在之后编译和执行。因此，你可以直接遍历乘法操作的结果，并使用
    `.NumPy` 方法打印出返回的 NumPy 对象，这个方法会从 TensorFlow 张量中返回一个 NumPy 对象。
- en: Layering nested operations
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层嵌套操作
- en: In this recipe, we'll learn how to put multiple operations to work; it is important
    to know how to chain operations together. This will set up layered operations
    to be executed by our network. In this recipe, we will multiply a placeholder
    by two matrices and then perform addition. We will feed in two matrices in the
    form of a three-dimensional NumPy array.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何将多个操作结合起来工作；了解如何将操作串联在一起非常重要。这将为我们的网络设置分层操作进行执行。在这个示例中，我们将用两个矩阵乘以一个占位符，并执行加法操作。我们将输入两个以三维
    NumPy 数组形式表示的矩阵。
- en: This is another easy-peasy recipe to give you ideas about how to code in TensorFlow
    using common constructs such as functions or classes, improving readability and
    code modularity. Even if the final product is a neural network, we're still writing
    a computer program, and we should abide by programming best practices.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个简单的示例，给你一些关于如何使用 TensorFlow 编写代码的灵感，利用函数或类等常见结构，提高代码的可读性和模块化性。即使最终产品是一个神经网络，我们依然在编写计算机程序，并且应该遵循编程最佳实践。
- en: Getting ready
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As usual, we just need to import TensorFlow and NumPy, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们只需要导入 TensorFlow 和 NumPy，代码如下：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We're now ready to move forward with our recipe.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好继续进行我们的示例。
- en: How to do it...
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will feed in two NumPy arrays of size *3* x *5*. We will multiply each matrix
    by a constant of size *5* x *1,* which will result in a matrix of size *3* x *1*.
    We will then multiply this by a *1* x *1* matrix resulting in a *3* x *1* matrix
    again. Finally, we add a *3* x *1* matrix at the end, as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入两个大小为*3* x *5*的 NumPy 数组。我们将每个矩阵与大小为*5* x *1*的常量相乘，得到一个大小为*3* x *1*的矩阵。接着，我们将这个结果与一个*1*
    x *1*的矩阵相乘，最终得到一个*3* x *1*的矩阵。最后，我们在末尾加上一个*3* x *1*的矩阵，代码如下：
- en: 'First, we create the data to feed in and the corresponding placeholder:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建要输入的数据和相应的占位符：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we create the constants that we will use for matrix multiplication and
    addition:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建用于矩阵乘法和加法的常量：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we declare the operations to be eagerly executed. As good practice, we
    create functions that execute the operations we need:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们声明要立即执行的操作。作为好的编程实践，我们创建执行所需操作的函数：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we nest our functions and display the result:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们嵌套我们的函数并显示结果：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using functions (and also classes, as we are going to cover) will help you write
    clearer code. That makes debugging more effective and allows easy maintenance
    and reuse of code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用函数（以及类，我们将进一步介绍）将帮助你编写更清晰的代码。这使得调试更加高效，并且便于代码的维护和重用。
- en: How it works...
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Thanks to eager execution, there''s no longer a need to resort to the "kitchen
    sink" programming style (meaning that you put almost everything in the global
    scope of the program; see [https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming](https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming))
    that was so common when using TensorFlow 1.x. At the moment, you can adopt either
    a functional programming style or an object-oriented one, such as the one we present
    in this brief example, where you can arrange all your operations and computations
    in a more logical and understandable way:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 借助即时执行，我们不再需要使用那种"万事俱备"的编程风格（指的是将几乎所有内容都放在程序的全局作用域中；详情请见[https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming](https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming)）这种风格在使用
    TensorFlow 1.x 时非常常见。目前，你可以选择采用函数式编程风格或面向对象编程风格，就像我们在这个简短示例中展示的那样，你可以将所有操作和计算按逻辑合理的方式排列，使其更易理解：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Classes can help you organize your code and reuse it better than functions,
    thanks to class inheritance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 类比函数更能帮助你组织代码并提高重用性，这得益于类继承。
- en: There's more...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In all the examples in this recipe, we've had to declare the data shape and
    know the outcome shape of the operations before we run the data through the operations.
    This is not always the case. There may be a dimension or two that we do not know
    beforehand or some that can vary during our data processing. To take this into
    account, we designate the dimension or dimensions that can vary (or are unknown)
    as value `None`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们需要在运行数据之前声明数据形状并知道操作的结果形状。并非所有情况下都如此。可能会有一维或二维我们事先不知道的情况，或者某些维度在数据处理过程中会发生变化。为此，我们将可以变化（或未知）的维度标记为`None`。
- en: 'For example, to initialize a variable to have an unknown amount of rows, we
    would write the following line and then we can assign values of arbitrary row
    numbers:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要初始化一个具有未知行数的变量，我们将写下以下行，然后我们可以分配任意行数的值：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is fine for matrix multiplication to have flexible rows because that won't
    affect the arrangement of our operations. This will come in handy in later chapters
    when we are feeding data in multiple batches of varying batch sizes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法具有灵活的行数是可以接受的，因为这不会影响我们操作的安排。当我们将数据馈送给多个大小批次时，这将在后续章节中非常方便。
- en: While the use of *None* as a dimension allows us to use variably-sized dimensions,
    I always recommend that you be as explicit as possible when filling out dimensions.
    If the size of our data is known in advance, then we should explicitly write that
    size as the dimensions. The use of `None` as a dimension is recommended to be
    limited to the batch size of the data (or however many data points we are computing
    on at once).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用`None`作为维度允许我们使用可变大小的维度，但我建议在填写维度时尽可能明确。如果我们的数据大小预先已知，则应将该大小明确写入维度中。建议将`None`用作维度的使用限制在数据的批大小（或者我们同时计算的数据点数）。
- en: Working with multiple layers
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理多层
- en: Now that we have covered multiple operations, we will cover how to connect various
    layers that have data propagating through them. In this recipe, we will introduce
    how to best connect various layers, including custom layers. The data we will
    generate and use will be representative of small random images. It is best to
    understand this type of operation with a simple example and see how we can use
    some built-in layers to perform calculations. The first layer we will explore
    is called a **moving window**. We will perform a small moving window average across
    a 2D image and then the second layer will be a custom operation layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了多个操作，接下来我们将讨论如何连接各层，这些层中有数据通过它们传播。在本示例中，我们将介绍如何最佳连接各层，包括自定义层。我们将生成和使用的数据将代表小随机图像。最好通过一个简单的示例理解这种操作，并看看如何使用一些内置层执行计算。我们将探讨的第一层称为**移动窗口**。我们将在二维图像上执行一个小的移动窗口平均值，然后第二层将是一个自定义操作层。
- en: Moving windows are useful for everything related to time series. Though there
    are layers specialized for sequences, a moving window may prove useful when you
    are analyzing, for instance, MRI scans (neuroimages) or sound spectrograms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 移动窗口对与时间序列相关的所有内容都非常有用。尽管有专门用于序列的层，但在分析MRI扫描（神经影像）或声谱图时，移动窗口可能会很有用。
- en: Moreover, we will see that the computational graph can get large and hard to
    look at. To address this, we will also introduce ways to name operations and create
    scopes for layers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将看到计算图可能变得很大，难以查看。为了解决这个问题，我们还将介绍如何命名操作并为层创建作用域。
- en: Getting ready
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To start, you have to load the usual packages – NumPy and TensorFlow – using
    the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您必须加载常用的包 - NumPy 和 TensorFlow - 使用以下内容：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's now progress to the recipe. This time things are getting more complex
    and interesting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进行配方。这次事情变得更加复杂和有趣。
- en: How to do it...
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We proceed with the recipe as follows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤进行。
- en: 'First, we create our sample 2D image with NumPy. This image will be a *4* x *4* pixel
    image. We will create it in four dimensions; the first and last dimensions will
    have a size of `1` (we keep the batch dimension distinct, so you can experiment
    with changing its size). Note that some TensorFlow image functions will operate
    on four-dimensional images. Those four dimensions are image number, height, width,
    and channel, and to make it work with one channel, we explicitly set the last
    dimension to `1`, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用NumPy创建我们的示例二维图像。这个图像将是一个4x4像素的图像。我们将在四个维度上创建它；第一个和最后一个维度将具有大小为`1`（我们保持批维度不同，以便您可以尝试更改其大小）。请注意，某些TensorFlow图像函数将操作四维图像。这四个维度是图像编号、高度、宽度和通道，为了使它与一个通道的图像兼容，我们将最后一个维度明确设置为`1`，如下所示：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To create a moving window average across our *4* x *4* image, we will use a
    built-in function that will convolute a constant across a window of the shape *2* x *2*.
    The function we will use is `conv2d()`; this function is quite commonly used in
    image processing and in TensorFlow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建跨我们的4x4图像的移动窗口平均值，我们将使用一个内置函数，该函数将在形状为2x2的窗口上卷积一个常数。我们将使用的函数是`conv2d()`；这个函数在图像处理和TensorFlow中经常使用。
- en: 'This function takes a piecewise product of the window and a filter we specify.
    We must also specify a stride for the moving window in both directions. Here,
    we will compute four moving window averages: the upper-left, upper-right, lower-left,
    and lower-right four pixels. We do this by creating a *2* x *2* window and having
    strides of length `2` in each direction. To take the average, we will convolute
    the *2* x *2* window with a constant of `0.25`, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数对我们指定的窗口和滤波器进行分段乘积。我们还必须在两个方向上指定移动窗口的步幅。在这里，我们将计算四个移动窗口的平均值：左上角、右上角、左下角和右下角的四个像素。我们通过创建一个 *2* x *2* 窗口，并且在每个方向上使用长度为 `2` 的步幅来实现这一点。为了取平均值，我们将 *2* x *2* 窗口与常数 `0.25` 做卷积，如下所示：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that we are also naming this layer `Moving_Avg_Window` by using the name argument
    of the function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还通过函数的 name 参数将此层命名为 `Moving_Avg_Window`。
- en: 'To figure out the output size of a convolutional layer, we can use the following
    formula: Output = (*W* – *F* + 2*P*)/*S* + 1), where *W* is the input size, *F* is
    the filter size, *P* is the padding of zeros, and *S* is the stride.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算卷积层的输出大小，我们可以使用以下公式：输出 = (*W* – *F* + 2*P*)/*S* + 1)，其中 *W* 是输入大小，*F* 是滤波器大小，*P* 是零填充，*S* 是步幅。
- en: 'Now, we define a custom layer that will operate on the *2* x *2* output of
    the moving window average. The custom function will first multiply the input by
    another *2* x *2* matrix tensor, and then add `1` to each entry. After this, we
    take the sigmoid of each element and return the *2* x *2* matrix. Since matrix
    multiplication only operates on two-dimensional matrices, we need to drop the
    extra dimensions of our image that are of size `1`. TensorFlow can do this with
    the built-in `squeeze()` function. Here, we define the new layer:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个自定义层，该层将操作移动窗口平均值的 *2* x *2* 输出。自定义函数将首先将输入乘以另一个 *2* x *2* 矩阵张量，然后将每个条目加 `1`。之后，我们对每个元素取sigmoid，并返回 *2* x *2* 矩阵。由于矩阵乘法仅适用于二维矩阵，我们需要去除大小为 `1` 的图像的额外维度。TensorFlow
    可以使用内置的 `squeeze()` 函数来完成这一点。在这里，我们定义了新的层：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we have to arrange the two layers in the network. We will do this by calling
    one layer function after the other, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须安排网络中的两个层。我们将通过依次调用一个层函数来完成这一点，如下所示：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we just feed in the *4* x *4* image into the functions. Finally, we can
    check the result, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需将 *4* x *4* 图像输入函数中。最后，我们可以检查结果，如下所示：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let's now understand more in depth how it works.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更深入地了解它是如何工作的。
- en: How it works...
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The first layer is named `Moving_Avg_Window`. The second is a collection of
    operations called `Custom_Layer`. Data processed by these two layers is first
    collapsed on the left and then expanded on the right. As shown by the example,
    you can wrap all the layers into functions and call them, one after the other,
    in a way that later layers process the outputs of previous ones.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层被命名为 `Moving_Avg_Window`。第二层是称为 `Custom_Layer` 的一组操作。这两个层处理的数据首先在左侧被折叠，然后在右侧被扩展。正如示例所示，您可以将所有层封装到函数中并依次调用它们，以便后续层处理前一层的输出。
- en: Implementing loss functions
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现损失函数
- en: For this recipe, we will cover some of the main loss functions that we can use
    in TensorFlow. Loss functions are a key aspect of machine learning algorithms.
    They measure the distance between the model outputs and the target (truth) values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将介绍在 TensorFlow 中可以使用的一些主要损失函数。损失函数是机器学习算法的关键组成部分。它们衡量模型输出与目标（真实）值之间的距离。
- en: In order to optimize our machine learning algorithms, we will need to evaluate
    the outcomes. Evaluating outcomes in TensorFlow depends on specifying a loss function.
    A loss function tells TensorFlow how good or bad the predictions are compared
    to the desired result. In most cases, we will have a set of data and a target
    on which to train our algorithm. The loss function compares the target to the
    prediction (it measures the distance between the model outputs and the target
    truth values) and provides a numerical quantification between the two.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化我们的机器学习算法，我们需要评估结果。在 TensorFlow 中，评估结果取决于指定的损失函数。损失函数告诉 TensorFlow 模型输出与目标结果的好坏程度。在大多数情况下，我们会有一组数据和一个目标，用于训练我们的算法。损失函数比较目标和预测（它衡量模型输出与目标真实值之间的距离），并提供两者之间的数值量化。
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will first start a computational graph and load `matplotlib`, a Python plotting
    package, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先启动一个计算图，并加载 `matplotlib`，一个 Python 绘图包，如下所示：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that we are ready to plot, let's proceed to the recipe without further ado.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经准备好绘制图表了，让我们毫不拖延地进入配方部分。
- en: How to do it...
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'First, we will talk about loss functions for regression, which means predicting
    a continuous dependent variable. To start, we will create a sequence of our predictions
    and a target as a tensor. We will output the results across 500 x values between
    `-1` and `1`. See the *How it works...* section for a plot of the outputs. Use
    the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论回归的损失函数，这意味着预测一个连续的因变量。首先，我们将创建一个预测序列和目标作为张量。我们将在`-1`和`1`之间的500个x值上输出结果。有关输出的图表，请参见*如何工作...*部分。使用以下代码：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The L2 norm loss is also known as the Euclidean loss function. It is just the
    square of the distance to the target. Here, we will compute the loss function
    as if the target is zero. The L2 norm is a great loss function because it is very
    curved near the target and algorithms can use this fact to converge to the target
    more slowly the closer it gets to zero. We can implement this as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: L2范数损失也称为欧几里得损失函数。它只是与目标的距离的平方。在这里，我们将假设目标为零来计算损失函数。L2范数是一个很好的损失函数，因为它在接近目标时非常曲线，算法可以利用这一事实在接近零时更慢地收敛。我们可以按如下方式实现：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: TensorFlow has a built-in form of the L2 norm, called `tf.nn.l2_loss()`. This
    function is actually half the L2 norm. In other words, it is the same as the previous
    one but divided by 2.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow有一个内置的L2范数形式，叫做`tf.nn.l2_loss()`。这个函数实际上是L2范数的一半。换句话说，它与之前的函数相同，只是除以了2。
- en: 'The L1 norm loss is also known as the **absolute loss function**. Instead of
    squaring the difference, we take the absolute value. The L1 norm is better for
    outliers than the L2 norm because it is not as steep for larger values. One issue
    to be aware of is that the L1 norm is not smooth at the target, and this can result
    in algorithms not converging well. It appears as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数损失也称为**绝对损失函数**。它不对差异进行平方处理，而是取绝对值。L1范数比L2范数更适合处理离群值，因为它对较大值的陡峭度较低。需要注意的问题是，L1范数在目标处不平滑，这可能导致算法无法很好地收敛。它的形式如下所示：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Pseudo-Huber loss is a continuous and smooth approximation to the **Huber loss
    function**. This loss function attempts to take the best of the L1 and L2 norms
    by being convex near the target and less steep for extreme values. The form depends
    on an extra parameter, `delta`, which dictates how steep it will be. We will plot
    two forms, *delta1 = 0.25* and *delta2 = 5*, to show the difference, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 伪-Huber 损失是**Huber 损失函数**的连续且平滑的近似。这个损失函数试图通过在接近目标时采用L1和L2范数的优点，并且对于极端值更加平缓，来结合这两者。它的形式取决于一个额外的参数`delta`，它决定了它的陡峭度。我们将绘制两个形式，*delta1
    = 0.25*和*delta2 = 5*，以显示差异，如下所示：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, we'll move on to loss functions for classification problems. Classification
    loss functions are used to evaluate loss when predicting categorical outcomes.
    Usually, the output of our model for a class category is a real-value number between
    `0` and `1`. Then, we choose a cutoff (0.5 is commonly chosen) and classify the
    outcome as being in that category if the number is above the cutoff. Next, we'll
    consider various loss functions for categorical outputs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论分类问题的损失函数。分类损失函数用于评估在预测分类结果时的损失。通常，我们模型的输出是一个介于`0`和`1`之间的实数值。然后，我们选择一个阈值（通常选择0.5），如果结果大于该阈值，则将结果分类为该类别。接下来，我们将考虑针对分类输出的各种损失函数。
- en: 'To start, we will need to redefine our predictions `(x_vals)` and `target`.
    We will save the outputs and plot them in the next section. Use the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要重新定义我们的预测值`(x_vals)`和`target`。我们将保存输出并在下一部分进行绘制。使用以下代码：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Hinge loss is mostly used for support vector machines but can be used in neural
    networks as well. It is meant to compute a loss among two target classes, `1`
    and -`1`. In the following code, we are using the target value `1`, so the closer
    our predictions are to `1`, the lower the loss value:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Hinge 损失主要用于支持向量机，但也可以用于神经网络。它用于计算两个目标类别之间的损失，`1` 和 -`1`。在以下代码中，我们使用目标值`1`，因此我们的预测值越接近`1`，损失值就越低：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Cross-entropy loss for a binary case is also sometimes referred to as the **logistic
    loss function**. It comes about when we are predicting the two classes `0` or
    `1`. We wish to measure a distance from the actual class (`0` or `1`) to the predicted
    value, which is usually a real number between `0` and `1`. To measure this distance,
    we can use the cross-entropy formula from information theory, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类情况下的交叉熵损失有时也被称为**逻辑损失函数**。当我们预测 `0` 或 `1` 两个类别时，便会使用该函数。我们希望衡量实际类别（`0` 或
    `1`）与预测值之间的距离，预测值通常是介于 `0` 和 `1` 之间的实数。为了衡量这个距离，我们可以使用信息理论中的交叉熵公式，如下所示：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Sigmoid cross-entropy loss is very similar to the previous loss function except
    we transform the `x` values using the sigmoid function before we put them in the
    cross-entropy loss, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 交叉熵损失与之前的损失函数非常相似，不同之处在于我们在将 `x` 值代入交叉熵损失之前，使用 sigmoid 函数对其进行转换，如下所示：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Weighted cross-entropy loss is a weighted version of sigmoid cross-entropy
    loss. We provide a weight on the positive target. For an example, we will weight
    the positive target by `0.5`, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 加权交叉熵损失是 sigmoid 交叉熵损失的加权版本。我们对正类目标进行加权。作为示例，我们将正类目标的权重设为 `0.5`，如下所示：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Softmax cross-entropy loss operates on non-normalized outputs. This function
    is used to measure a loss when there is only one target category instead of multiple.
    Because of this, the function transforms the outputs into a probability distribution
    via the softmax function and then computes the loss function from a true probability
    distribution, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 交叉熵损失适用于非归一化的输出。该函数用于在目标类别只有一个而不是多个时，计算损失。因此，该函数通过 softmax 函数将输出转换为概率分布，然后根据真实的概率分布计算损失函数，如下所示：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Sparse softmax cross-entropy loss is almost the same as softmax cross-entropy
    loss, except instead of the target being a probability distribution, it is an
    index of which category is `true`. Instead of a sparse all-zero target vector
    with one value of `1`, we just pass in the index of the category that is the `true`
    value, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏 softmax 交叉熵损失与 softmax 交叉熵损失几乎相同，区别在于目标不是概率分布，而是表示哪个类别是`true`的索引。我们传入的是该类别的索引，而不是一个稀疏的全零目标向量，其中有一个值是`1`，如下所示：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now let's understand better how such loss functions operate by plotting them
    on a graph.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过将这些损失函数绘制在图上，进一步理解它们是如何工作的。
- en: How it works...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Here is how to use `matplotlib` to plot the regression loss functions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用`matplotlib`绘制回归损失函数：
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We get the following plot as output from the preceding code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前面的代码中得到以下图示：
- en: '![](img/B16254_02_01.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_01.png)'
- en: 'Figure 2.1: Plotting various regression loss functions'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：绘制不同的回归损失函数
- en: 'Here is how to use `matplotlib` to plot the various classification loss functions:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用`matplotlib`绘制不同的分类损失函数：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We get the following plot from the preceding code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前面的代码中得到以下图示：
- en: '![](img/B16254_02_02.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_02.png)'
- en: 'Figure 2.2: Plots of classification loss functions'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：分类损失函数的绘图
- en: Each of these loss curves provides different advantages to the neural network
    optimizing it. We are now going to discuss this a little bit more.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每条损失曲线都为神经网络优化提供了不同的优势。接下来我们将进一步讨论这些内容。
- en: There's more...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Here is a table summarizing the properties and benefits of the different loss functions
    that we have just graphically described:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一张表格，总结了我们刚刚以图形方式描述的不同损失函数的属性和优点：
- en: '| Loss function | Use | Benefits | Disadvantages |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 | 用途 | 优势 | 缺点 |'
- en: '| L2 | Regression | More stable | Less robust |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| L2 | 回归 | 更稳定 | 鲁棒性差 |'
- en: '| L1 | Regression | More robust | Less stable |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| L1 | 回归 | 更具鲁棒性 | 稳定性差 |'
- en: '| Pseudo-Huber | Regression | More robust and stable | One more parameter |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 伪-Huber | 回归 | 更具鲁棒性和稳定性 | 多了一个参数 |'
- en: '| Hinge | Classification | Creates a max margin for use in SVM | Unbounded
    loss affected by outliers |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Hinge | 分类 | 在支持向量机（SVM）中创建最大间隔 | 损失无界，受异常值影响 |'
- en: '| Cross-entropy | Classification | More stable | Unbounded loss, less robust
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵 | 分类 | 更稳定 | 损失无界，鲁棒性差 |'
- en: The remaining classification loss functions all have to do with the type of
    cross-entropy loss. The cross-entropy sigmoid loss function is for use on unscaled
    logits and is preferred over computing the sigmoid loss and then the cross-entropy
    loss, because TensorFlow has better built-in ways to handle numerical edge cases.
    The same goes for softmax cross-entropy and sparse softmax cross-entropy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的分类损失函数都与交叉熵损失的类型有关。交叉熵 Sigmoid 损失函数适用于未缩放的 logits，并且优先于计算 Sigmoid 损失，然后计算交叉熵损失，因为
    TensorFlow 有更好的内置方法来处理数值边缘情况。Softmax 交叉熵和稀疏 softmax 交叉熵也是如此。
- en: Most of the classification loss functions described here are for two-class predictions.
    This can be extended to multiple classes by summing the cross-entropy terms over
    each prediction/target.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的大多数分类损失函数适用于两类预测。可以通过对每个预测/目标求和来将其扩展到多类。
- en: 'There are also many other metrics to look at when evaluating a model. Here
    is a list of some more to consider:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，还有许多其他指标可供参考。以下是一些需要考虑的其他指标列表：
- en: '| Model metric | Description |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 模型指标 | 描述 |'
- en: '| R-squared (coefficient of determination) | For linear models, this is the
    proportion of variance in the dependent variable that is explained by the independent
    data. For models with a larger number of features, consider using adjusted R-squared.
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| R-squared (决定系数) | 对于线性模型，这是因变量方差中由独立数据解释的比例。对于具有较多特征的模型，请考虑使用调整后的 R-squared。
    |'
- en: '| Root mean squared error | For continuous models, this measures the difference
    between prediction and actual via the square root of the average squared error.
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 均方根误差 | 对于连续模型，这衡量了预测值与实际值之间的差异，通过平均平方误差的平方根。 |'
- en: '| Confusion matrix | For categorical models, we look at a matrix of predicted
    categories versus actual categories. A perfect model has all the counts along
    the diagonal. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 混淆矩阵 | 对于分类模型，我们查看预测类别与实际类别的矩阵。完美的模型在对角线上有所有计数。 |'
- en: '| Recall | For categorical models, this is the fraction of true positives over
    all predicted positives. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 对于分类模型而言，这是真正例占所有预测正例的比例。 |'
- en: '| Precision | For categorical models, this is the fraction of true positives
    over all actual positives. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | 对于分类模型而言，这是真正例占所有实际正例的比例。 |'
- en: '| F-score | For categorical models, this is the harmonic mean of precision
    and recall. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| F-score | 对于分类模型而言，这是精确度和召回率的调和平均值。 |'
- en: In your choice of the right metric, you have to both evaluate the problem you
    have to solve (because each metric will behave differently and, depending on the
    problem at hand, some loss minimization strategies will prove better than others
    for our problem), and to experiment with the behavior of the neural network.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择正确的指标时，您必须同时评估您要解决的问题（因为每个指标的行为会有所不同，并且根据手头的问题，一些损失最小化策略对我们的问题可能比其他更好），并且对神经网络的行为进行实验。
- en: Implementing backpropagation
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施反向传播
- en: One of the benefits of using TensorFlow is that it can keep track of operations
    and automatically update model variables based on backpropagation. In this recipe,
    we will introduce how to use this aspect to our advantage when training machine
    learning models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的好处之一是它可以跟踪操作并根据反向传播自动更新模型变量。在这个教程中，我们将介绍如何在训练机器学习模型时利用这一方面。
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Now, we will introduce how to change our variables in the model in such a way
    that a loss function is minimized. We have learned how to use objects and operations,
    and how to create loss functions that will measure the distance between our predictions
    and targets. Now, we just have to tell TensorFlow how to backpropagate errors
    through our network in order to update the variables in such a way to minimize
    the loss function. This is achieved by declaring an optimization function. Once
    we have an optimization function declared, TensorFlow will go through and figure
    out the backpropagation terms for all of our computations in the graph. When we
    feed data in and minimize the loss function, TensorFlow will modify our variables
    in the network accordingly.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将介绍如何以使得损失函数最小化的方式改变模型中的变量。我们已经学会了如何使用对象和操作，以及如何创建损失函数来衡量预测与目标之间的距离。现在，我们只需告诉
    TensorFlow 如何通过网络反向传播错误，以便更新变量，使损失函数最小化。这通过声明一个优化函数来实现。一旦声明了优化函数，TensorFlow 将遍历计算图中的所有计算，找出反向传播项。当我们输入数据并最小化损失函数时，TensorFlow
    将相应地修改网络中的变量。
- en: For this recipe, we will do a very simple regression algorithm. We will sample
    random numbers from a normal distribution, with mean 1 and standard deviation
    0.1\. Then, we will run the numbers through one operation, which will be to multiply
    them by a weight tensor and then adding a bias tensor. From this, the loss function
    will be the L2 norm between the output and the target. Our target will show a
    high correlation with our input, so the task won't be too complex, yet the recipe
    will be interestingly demonstrative, and easily reusable for more complex problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们将做一个非常简单的回归算法。我们将从一个正态分布中采样随机数，均值为1，标准差为0.1。然后，我们将这些数字通过一次操作，操作是将它们乘以一个权重张量，然后加上一个偏置张量。由此，损失函数将是输出和目标之间的L2范数。我们的目标将与输入高度相关，因此任务不会太复杂，但该食谱会非常具有示范性，并且可以轻松地用于更复杂的问题。
- en: The second example is a very simple binary classification algorithm. Here, we
    will generate 100 numbers from two normal distributions, *N(-3,1)* and *N(3,1)*.
    All the numbers from *N(-3, 1)* will be in target class `0`, and all the numbers
    from *N(3, 1)* will be in target class `1`. The model to differentiate these classes
    (which are perfectly separable) will again be a linear model optimized accordingly
    to the sigmoid cross-entropy loss function, thus, at first operating a sigmoid
    transformation on the model result and then computing the cross-entropy loss function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个例子是一个非常简单的二分类算法。在这里，我们将从两个正态分布中生成100个数字，*N(-3,1)* 和 *N(3,1)*。所有来自 *N(-3,
    1)* 的数字将属于目标类别 `0`，而所有来自 *N(3, 1)* 的数字将属于目标类别 `1`。用于区分这些类别的模型（它们是完全可分的）将再次是一个线性模型，并根据sigmoid交叉熵损失函数进行优化，因此，首先对模型结果进行sigmoid转换，然后计算交叉熵损失函数。
- en: While specifying a good learning rate helps the convergence of algorithms, we
    must also specify a type of optimization. From the preceding two examples, we
    are using standard gradient descent. This is implemented with the `tf.optimizers.SGD` TensorFlow
    function.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然指定合适的学习率有助于算法的收敛，但我们还必须指定一种优化类型。从前面的两个例子中，我们使用的是标准的梯度下降法。这是通过 `tf.optimizers.SGD`
    TensorFlow函数实现的。
- en: How to do it...
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We''ll start with the regression example. First, we load the usual numerical
    Python packages that always accompany our recipes, `NumPy` and `TensorFlow`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从回归示例开始。首先，我们加载通常伴随我们食谱的数值Python库，`NumPy` 和 `TensorFlow`：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we create the data. In order to make everything easily replicable, we
    want to set the random seed to a specific value. We will always repeat this in
    our recipes, so we exactly obtain the same results; check yourself how chance
    may vary the results in the recipes, by simply changing the seed number.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建数据。为了使一切易于复制，我们希望将随机种子设置为特定值。我们将在我们的食谱中始终重复这一点，这样我们就能精确获得相同的结果；你可以自己检查，通过简单地更改种子数值，随机性是如何影响食谱中的结果的。
- en: 'Moreover, in order to get assurance that the target and input have a good correlation,
    plot a scatterplot of the two variables:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了确保目标和输入有很好的相关性，可以绘制这两个变量的散点图：
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/B16254_02_03.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_03.png)'
- en: 'Figure 2.3: Scatterplot of x_vals and y_vals'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：x_vals和y_vals的散点图
- en: 'We add the structure of the network (a linear model of the type *bX + a*) as
    a function:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将网络的结构（类型为 *bX + a* 的线性模型）添加为一个函数：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we add our L2 Loss function to be applied to the results of the network:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的L2损失函数添加到网络的结果中进行应用：
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now, we have to declare a way to optimize the variables in our graph. We declare
    an optimization algorithm. Most optimization algorithms need to know how far to
    step in each iteration. Such a distance is controlled by the learning rate. Setting
    it to a correct value is specific to the problem we are dealing with, so we can
    figure out a suitable setting only by experimenting. Anyway, if our learning rate
    is too high, our algorithm might overshoot the minimum, but if our learning rate
    is too low, our algorithm might take too long to converge.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须声明一种方法来优化图中的变量。我们声明一个优化算法。大多数优化算法需要知道在每次迭代中应该走多远。这种距离由学习率控制。设置正确的值对于我们处理的问题是特定的，因此我们只能通过实验来找到合适的设置。无论如何，如果我们的学习率太高，算法可能会超过最小值，但如果学习率太低，算法可能会需要太长时间才能收敛。
- en: 'The learning rate has a big influence on convergence and we will discuss it
    again at the end of the section. While we''re using the standard gradient descent
    algorithm, there are many other alternative options. There are, for instance,
    optimization algorithms that operate differently and can achieve a better or worse
    optimum depending on the problem. For a great overview of different optimization
    algorithms, see the paper by Sebastian Ruder in the *See also* section at the
    end of this recipe:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率对收敛有很大影响，我们将在本节末尾再次讨论它。虽然我们使用的是标准的梯度下降算法，但还有许多其他替代选择。例如，有些优化算法的操作方式不同，取决于问题的不同，它们可以达到更好的或更差的最优解。关于不同优化算法的全面概述，请参见
    Sebastian Ruder 在 *另见* 部分列出的文章：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: There is a lot of theory on which learning rates are best. This is one of the
    harder things to figure out in machine learning algorithms. Good papers to read
    about how learning rates are related to specific optimization algorithms are listed
    in the *See also* section at the end of this recipe.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最优学习率的理论有很多。这是机器学习算法中最难解决的问题之一。有关学习率与特定优化算法之间关系的好文章，列在此食谱末尾的 *另见* 部分。
- en: 'Now we can initialize our network variables (`weights` and `biases`) and set
    a recording list (named `history`) to help us visualize the optimization steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以初始化网络变量（`weights` 和 `biases`），并设置一个记录列表（命名为 `history`），以帮助我们可视化优化步骤：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The final step is to loop through our training algorithm and tell TensorFlow
    to train many times. We will do this 100 times and print out results every 25^(th)
    iteration. To train, we will select a random *x* and *y* entry and feed it through
    the graph. TensorFlow will automatically compute the loss, and slightly change
    the weights and biases to minimize the loss:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是循环我们的训练算法，并告诉 TensorFlow 进行多次训练。我们将训练 100 次，并在每 25 次迭代后打印结果。为了训练，我们将选择一个随机的
    *x* 和 *y* 输入，并将其输入到图中。TensorFlow 会自动计算损失，并略微调整权重和偏置，以最小化损失：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the loops, `tf.GradientTape()` allows TensorFlow to track the computations
    and calculate the gradient with respect to the observed variables. Every variable
    that is within the `GradientTape()` scope is monitored (please keep in mind that
    constants are not monitored, unless you explicitly state it with the command `tape.watch(constant)`).
    Once you've completed the monitoring, you can compute the gradient of a target
    in respect of a list of sources (using the command `tape.gradient(target, sources)`)
    and get back an eager tensor of the gradients that you can apply to the minimization
    process. The operation is automatically concluded with the updating of your sources
    (in our case, the `weights` and `biases` variables) with new values.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环中，`tf.GradientTape()` 使 TensorFlow 能够追踪计算过程，并计算相对于观测变量的梯度。`GradientTape()`
    范围内的每个变量都会被监控（请记住，常量不会被监控，除非你明确使用命令 `tape.watch(constant)` 来监控它）。一旦完成监控，你可以计算目标相对于一组源的梯度（使用命令
    `tape.gradient(target, sources)`），并获得一个渐变的急切张量，可以应用于最小化过程。此操作会自动通过用新值更新源（在我们的例子中是
    `weights` 和 `biases` 变量）来完成。
- en: 'When the training is completed, we can visualize how the optimization process
    operates over successive gradient applications:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成后，我们可以可视化优化过程在连续梯度应用下的变化：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/B16254_02_04.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_04.png)'
- en: 'Figure 2.4: L2 loss through iterations in our recipe'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：我们方法中 L2 损失随迭代的变化
- en: At this point, we will introduce the code for the simple classification example.
    We can use the same TensorFlow script, with some updates. Remember, we will attempt
    to find an optimal set of weights and biases that will separate the data into
    two different classes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个阶段，我们将介绍简单分类示例的代码。我们可以使用相同的 TensorFlow 脚本，做一些更新。记住，我们将尝试找到一组最优的权重和偏置，使得数据能够分成两类。
- en: 'First, we pull in the data from two different normal distributions, `N(-3,
    1)` and `N(3, 1)`. We will also generate the target labels and visualize how the
    two classes are distributed along our predictor variable:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从两个不同的正态分布 `N(-3, 1)` 和 `N(3, 1)` 拉取数据。我们还将生成目标标签，并可视化这两类如何在我们的预测变量上分布：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/B16254_02_05.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_05.png)'
- en: 'Figure 2.5: Class distribution on x_vals'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：x_vals 上的类别分布
- en: 'Because the specific loss function for this problem is sigmoid cross-entropy,
    we update our loss function:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个问题的特定损失函数是 Sigmoid 交叉熵，我们更新我们的损失函数：
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we initialize our variables:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化变量：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we loop through a randomly selected data point several hundred times
    and update the `weights` and `biases` variables accordingly. As we did before,
    every 25 iterations we will print out the value of our variables and the loss:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对一个随机选择的数据点进行了数百次循环，并相应地更新了`weights`和`biases`变量。像之前一样，每进行25次迭代，我们会打印出变量的值和损失值：
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'A plot, also in this case, will reveal how the optimization proceeded:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图表，也可以在这种情况下，揭示优化过程的进展：
- en: '[PRE42]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/B16254_02_06.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_06.png)'
- en: 'Figure 2.6: Sigmoid cross-entropy loss through iterations in our recipe'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：我们方法中通过迭代的Sigmoid交叉熵损失
- en: The directionality of the plot is clear, though the trajectory is a bit bumpy
    because we are learning one example at a time, thus making the learning process
    decisively stochastic. The graph could also point out the need to try to decrease
    the learning rate a bit.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的方向性很清晰，尽管轨迹有些崎岖，因为我们一次只学习一个示例，从而使学习过程具有决定性的随机性。图表还可能指出需要稍微降低学习率的必要性。
- en: How it works...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'For a recap and explanation, for both examples, we did the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 总结和解释一下，对于这两个示例，我们做了以下操作：
- en: We created the data. Both examples needed to load data into specific variables
    used by the function that computes the network.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了数据。两个示例都需要将数据加载到计算网络所用的特定变量中。
- en: We initialized variables. We used some random Gaussian values, but initialization
    is a topic on its own, since much of the final results may depend on how we initialize
    our network (just change the random seed before initialization to find it out).
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化了变量。我们使用了一些随机高斯值，但初始化本身是一个独立的话题，因为最终的结果可能在很大程度上取决于我们如何初始化网络（只需在初始化前更改随机种子即可找到答案）。
- en: We created a loss function. We used the L2 loss for regression and the cross-entropy
    loss for classification.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个损失函数。回归使用了L2损失，分类使用了交叉熵损失。
- en: We defined an optimization algorithm. Both algorithms used gradient descent.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个优化算法。两个算法都使用了梯度下降法。
- en: We iterated across random data samples to iteratively update our variables.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历了随机数据样本，以逐步更新我们的变量。
- en: There's more...
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'As we mentioned before, the optimization algorithm is sensitive to the choice
    of learning rate. It is important to summarize the effect of this choice in a
    concise manner:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，优化算法对学习率的选择很敏感。总结这一选择的影响非常重要，可以简明扼要地表述如下：
- en: '| Learning rate size | Advantages/disadvantages | Uses |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 学习率大小 | 优点/缺点 | 用途 |'
- en: '| **Smaller learning rate** | Converges slower but more accurate results |
    If the solution is unstable, try lowering the learning rate first |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| **较小的学习率** | 收敛较慢，但结果更准确 | 如果解不稳定，首先尝试降低学习率 |'
- en: '| **Larger learning rate** | Less accurate, but converges faster | For some
    problems, helps prevent solutions from stagnating |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **较大的学习率** | 准确性较低，但收敛速度较快 | 对于某些问题，有助于防止解停滞 |'
- en: Sometimes, the standard gradient descent algorithm can be stuck or slow down
    significantly. This can happen when the optimization is stuck in the flat spot
    of a saddle. To combat this, the solution is taking into account a momentum term,
    which adds on a fraction of the prior step's gradient descent value. You can access
    this solution by setting the momentum and the Nesterov parameters, along with
    your learning rate, in `tf.optimizers.SGD` (see [https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD)
    for more details).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，标准的梯度下降算法可能会被卡住或显著变慢。这可能发生在优化停留在鞍点的平坦区域时。为了解决这个问题，解决方案是考虑动量项，它加上了上一步梯度下降值的一部分。你可以通过在`tf.optimizers.SGD`中设置动量和Nesterov参数，以及你的学习率，来实现这个解决方案（详细信息请参见
    [https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD)）。
- en: Another variant is to vary the optimizer step for each variable in our models.
    Ideally, we would like to take larger steps for smaller moving variables and shorter
    steps for faster changing variables. We will not go into the mathematics of this
    approach, but a common implementation of this idea is called the **Adagrad algorithm**.
    This algorithm takes into account the whole history of the variable gradients.
    The function in TensorFlow for this is called `AdagradOptimizer()` ([https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种变体是对我们模型中每个变量的优化器步骤进行变化。理想情况下，我们希望对移动较小的变量采取较大的步长，对变化较快的变量采取较小的步长。我们不会深入探讨这种方法的数学原理，但这种思想的常见实现被称为**Adagrad
    算法**。该算法考虑了变量梯度的整个历史。在 TensorFlow 中，这个函数被称为 `AdagradOptimizer()` ([https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad))。
- en: Sometimes, Adagrad forces the gradients to zero too soon because it takes into
    account the whole history. A solution to this is to limit how many steps we use.
    This is called the **Adadelta algorithm**. We can apply this by using the `AdadeltaOptimizer()` function
    ([https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，Adagrad 会因为考虑到整个历史而过早地将梯度推到零。解决这个问题的方法是限制我们使用的步数。这就是**Adadelta 算法**。我们可以通过使用
    `AdadeltaOptimizer()` 函数来应用这一方法 ([https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta))。
- en: There are a few other implementations of different gradient descent algorithms.
    For these, refer to the TensorFlow documentation at [https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他不同梯度下降算法的实现。关于这些，请参阅 TensorFlow 文档 [https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers)。
- en: See also
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on optimization algorithms and learning rates, see the
    following papers and articles:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化算法和学习率的一些参考文献，请参阅以下论文和文章：
- en: 'Recipes from this chapter, as follows:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的食谱如下：
- en: The Implementing Loss Functions section.
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现损失函数**部分。'
- en: The Implementing Backpropagation section.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现反向传播**部分。'
- en: '**Kingma, D., Jimmy, L. Adam**: *A Method for Stochastic Optimization. ICLR*
    2015 [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kingma, D., Jimmy, L. Adam**:  *随机优化方法。ICLR* 2015 [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
- en: '**Ruder, S.** *An Overview of Gradient Descent Optimization Algorithms*. 2016 [https://arxiv.org/pdf/1609.04747v1.pdf](https://arxiv.org/pdf/1609.04747v1.pdf)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ruder, S.**  *梯度下降优化算法概述*。2016 [https://arxiv.org/pdf/1609.04747v1.pdf](https://arxiv.org/pdf/1609.04747v1.pdf)'
- en: '**Zeiler, M.** *ADADelta: An Adaptive Learning Rate Method*. 2012 [https://arxiv.org/pdf/1212.5701.pdf](https://arxiv.org/pdf/1212.5701.pdf)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Zeiler, M.**  *ADADelta: 一种自适应学习率方法*。2012 [https://arxiv.org/pdf/1212.5701.pdf](https://arxiv.org/pdf/1212.5701.pdf)'
- en: Working with batch and stochastic training
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用批量和随机训练
- en: While TensorFlow updates our model variables according to backpropagation, it
    can operate on anything from a one-datum observation (as we did in the previous
    recipe) to a large batch of data at once. Operating on one training example can
    make for a very erratic learning process, while using too large a batch can be
    computationally expensive. Choosing the right type of training is crucial for
    getting our machine learning algorithms to converge to a solution.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当 TensorFlow 根据反向传播更新我们的模型变量时，它可以处理从单个数据观察（如我们在之前的食谱中所做的）到一次处理大量数据的情况。仅处理一个训练示例可能导致非常不稳定的学习过程，而使用过大的批量则可能在计算上非常昂贵。选择合适的训练类型对于使我们的机器学习算法收敛到一个解非常关键。
- en: Getting ready
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order for TensorFlow to compute the variable gradients for backpropagation
    to work, we have to measure the loss on a sample or multiple samples. Stochastic
    training only works on one randomly sampled data-target pair at a time, just as
    we did in the previous recipe. Another option is to put a larger portion of the
    training examples in at a time and average the loss for the gradient calculation.
    The sizes of the training batch can vary, up to and including the whole dataset
    at once. Here, we will show how to extend the prior regression example, which
    used stochastic training, to batch training.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 TensorFlow 计算反向传播所需的变量梯度，我们必须在一个或多个样本上测量损失。随机训练每次只处理一个随机抽样的数据-目标对，就像我们在前面的示例中所做的那样。另一个选择是一次处理一大部分训练样本，并对梯度计算进行损失平均。训练批次的大小可以有所不同，最大可以包括整个数据集。这里，我们将展示如何将之前使用随机训练的回归示例扩展为批量训练。
- en: 'We will start by loading `NumPy`, `matplotlib`, and `TensorFlow`, as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载 `NumPy`、`matplotlib` 和 `TensorFlow`，如下所示：
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now we just have to script our code and test our recipe in the *How to do it…*
    section.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需编写脚本并在 *如何做...* 部分中测试我们的配方。
- en: How to do it...
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We start by declaring a batch size. This will be how many data observations
    we will feed through the computational graph at one time:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从声明批次大小开始。这将是我们一次性通过计算图输入的观察数据量：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we just apply small modifications to the code used before for the regression
    problem:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需对之前用于回归问题的代码进行一些小的修改：
- en: '[PRE45]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Since our previous recipe, we have learned how to use matrix multiplication
    in our network and in our cost function. At this point, we just need to deal with
    inputs that are made of more rows as batches instead of single examples. We can
    even compare it with the previous approach, which we can now name stochastic optimization:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 自从上一个示例以来，我们已经学会了在网络和成本函数中使用矩阵乘法。此时，我们只需要处理由更多行组成的输入，即批量而非单个样本。我们甚至可以将其与之前的方法进行比较，后者我们现在可以称之为随机优化：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Just running the code will retrain our network using batches. At this point,
    we need to evaluate the results, get some intuition about how it works, and reflect
    on the results. Let's proceed to the next section.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，将使用批量数据重新训练我们的网络。此时，我们需要评估结果，获取一些关于它如何工作的直觉，并反思这些结果。让我们继续进入下一部分。
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Batch training and stochastic training differ in their optimization methods
    and their convergence. Finding a good batch size can be difficult. To see how
    convergence differs between batch training and stochastic training, you are encouraged
    to change the batch size to various levels.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 批量训练和随机训练在优化方法和收敛性方面有所不同。找到合适的批量大小可能是一个挑战。为了观察批量训练和随机训练在收敛上的差异，建议你调整批次大小至不同的水平。
- en: 'A visual comparison of the two approaches will explain better how using batches
    for this problem resulted in the same optimization as stochastic training, though
    there were fewer fluctuations during the process. Here is the code to produce
    the plot of both the stochastic and batch losses for the same regression problem.
    Note that the batch loss is much smoother and the stochastic loss is much more
    erratic:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法的视觉对比能更好地解释，使用批量训练如何在这个问题上得到与随机训练相同的优化结果，尽管在过程中波动较少。以下是生成同一回归问题的随机训练和批量损失图的代码。请注意，批量损失曲线更加平滑，而随机损失曲线则更加不稳定：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/B16254_02_07.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_07.png)'
- en: 'Figure 2.7: Comparison of L2 loss when using stochastic and batch optimization'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：使用随机优化和批量优化时的 L2 损失比较
- en: Now our graph displays a smoother trend line. The persistent presence of bumps
    could be solved by reducing the learning rate and adjusting the batch size.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的图表显示了一个更平滑的趋势线。存在的波动问题可以通过降低学习率和调整批次大小来解决。
- en: There's more...
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '| Type of training | Advantages | Disadvantages |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 训练类型 | 优势 | 劣势 |'
- en: '| Stochastic | Randomness may help move out of local minimums | Generally needs
    more iterations to converge |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 随机训练 | 随机性有助于摆脱局部最小值 | 通常需要更多的迭代才能收敛 |'
- en: '| Batch | Finds minimums quicker | Takes more resources to compute |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 批量训练 | 更快地找到最小值 | 计算所需资源更多 |'
- en: Combining everything together
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切结合在一起
- en: In this section, we will combine everything we have illustrated so far and create
    a classifier for the iris dataset. The iris dataset is described in more detail
    in the *Working with data sources* recipe in *Chapter 1*, *Getting Started with
    TensorFlow*. We will load this data and make a simple binary classifier to predict
    whether a flower is the species Iris setosa or not. To be clear, this dataset
    has three species, but we will only predict whether a flower is a single species,
    Iris setosa or not, giving us a binary classifier.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将结合迄今为止的所有内容，并创建一个用于鸢尾花数据集的分类器。鸢尾花数据集在*第1章*《TensorFlow入门》中的*与数据源一起使用*部分中有更详细的描述。我们将加载该数据并创建一个简单的二分类器，用于预测一朵花是否为鸢尾花setosa。需要明确的是，这个数据集有三种花卉，但我们只预测一朵花是否为某种单一的品种，即鸢尾花setosa或非setosa，从而使我们得到一个二分类器。
- en: Getting ready
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'We will start by loading the libraries and data and then transform the target
    accordingly. First, we load the libraries needed for our recipe. For the Iris
    dataset, we need the TensorFlow Datasets module, which we haven''t used before
    in our recipes. Note that we also load `matplotlib` here, because we would like
    to plot the resultant line afterward:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载所需的库和数据，然后相应地转换目标数据。首先，我们加载所需的库。对于鸢尾花数据集，我们需要TensorFlow Datasets模块，这是我们在之前的例子中没有用到的。请注意，我们还在这里加载了`matplotlib`，因为我们接下来想要绘制结果图：
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How to do it...
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'As a starting point, let''s first declare our batch size using a global variable:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，我们首先使用全局变量声明我们的批次大小：
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we load the iris data. We will also need to transform the target data
    to be just `1` or `0`, whether the target is setosa or not. Since the iris dataset
    marks setosa as a `0`, we will change all targets with the value `0` to `1`, and
    the other values all to `0`. We will also only use two features, petal length
    and petal width. These two features are the third and fourth entry in each row
    of the dataset:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载鸢尾花数据集。我们还需要将目标数据转换为`1`或`0`，表示该目标是否为setosa。由于鸢尾花数据集中setosa的标记为`0`，我们将把所有目标值为`0`的标签改为`1`，其他值则改为`0`。我们还将只使用两个特征，花瓣长度和花瓣宽度。这两个特征分别是数据集每行中的第三和第四个条目：
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As shown in the previous chapter, we use the TensorFlow dataset functions to
    both load and operate the necessary transformations by creating a data generator
    that can dynamically feed our network with data, instead of keeping it in an in-memory
    NumPy matrix. As a first step, we load the data, specifying that we want to split
    it (using the parameters `split='train[:90%]'` and `split='train[90%:]'`). This
    allows us to reserve a part (10%) of the dataset for the model evaluation, using
    data that has not been part of the training phase.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所示，我们使用TensorFlow的数据集函数加载数据并执行必要的转换，通过创建一个数据生成器动态地向网络提供数据，而不是将数据保存在内存中的NumPy矩阵中。第一步，我们加载数据，并指定要将其拆分（使用参数`split='train[:90%]'`和`split='train[90%:]'`）。这样我们可以保留数据集的10%作为模型评估所用，使用未参与训练阶段的数据。
- en: We also specify the parameter, `as_supervised=True`, that will allow us to access
    the data as tuples of features and labels when iterating from the dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了参数`as_supervised=True`，该参数将允许我们在遍历数据集时，将数据作为特征和标签的元组进行访问。
- en: Now we transform the dataset into an iterable generator by applying successive
    transformations. We shuffle the data, we define the batch to be returned by the
    iterable, and, most important, we apply a custom function that filters and transforms
    the features and labels returned from the dataset at the same time.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过应用连续的转换，将数据集转换为一个可迭代的生成器。我们打乱数据，定义生成器返回的批次大小，最重要的是，我们应用一个自定义函数，过滤并同时转换数据集返回的特征和标签。
- en: 'Then, we define the linear model. The model will take the usual form *bX+a*.
    Remember that TensorFlow has loss functions with the sigmoid built in, so we just
    need to define the output of the model prior to the sigmoid function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义线性模型。该模型将采用通常的形式 *bX+a*。请记住，TensorFlow内置了带有sigmoid函数的损失函数，因此我们只需要在sigmoid函数之前定义模型的输出：
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we add our sigmoid cross-entropy loss function with TensorFlow''s built-in `sigmoid_cross_entropy_with_logits()` function:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用TensorFlow内置的`sigmoid_cross_entropy_with_logits()`函数添加sigmoid交叉熵损失函数：
- en: '[PRE52]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We also have to tell TensorFlow how to optimize our computational graph by
    declaring an optimizing method. We will want to minimize the cross-entropy loss.
    We will also choose `0.02` as our learning rate:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须告诉TensorFlow如何通过声明优化方法来优化我们的计算图。我们将希望最小化交叉熵损失。我们还选择了`0.02`作为我们的学习率：
- en: '[PRE53]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we will train our linear model with 300 iterations. We will feed in the
    three data points that we require: petal length, petal width, and the target variable.
    Every 30 iterations, we will print the variable values:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用300次迭代来训练我们的线性模型。我们将输入所需的三个数据点：花瓣长度、花瓣宽度和目标变量。每30次迭代，我们会打印变量的值：
- en: '[PRE54]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'If we plot the loss against the iterations, we can acknowledge from the smoothness
    of the reduction of the loss over time how the learning has been quite an easy
    task for the linear model:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制损失与迭代次数的关系图，我们可以从损失随时间平滑减少的趋势中看出，线性模型的学习过程相对简单：
- en: '[PRE55]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/B16254_02_08.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_08.png)'
- en: 'Figure 2.8: Cross-entropy error for the Iris setosa data'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：Iris setosa数据的交叉熵误差
- en: 'We''ll conclude by checking the performance on our reserved test data. This
    time we just take the examples from the test dataset. As expected, the resulting
    cross-entropy value is analogous to the training one:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过检查在保留的测试数据上的表现来结束。本次我们仅取测试数据集中的示例。如预期的那样，得到的交叉熵值与训练时的值相似：
- en: '[PRE56]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The next set of commands extracts the model variables and plots the line on
    a graph:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下一组命令提取模型变量并在图表上绘制这条线：
- en: '[PRE57]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The resultant graph is in the *How it works...* section, where we also discuss
    the validity and reproducibility of the obtained results.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表位于*它是如何工作的...*一节中，我们还讨论了所获得结果的有效性和可重现性。
- en: How it works...
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Our goal was to fit a line between the Iris setosa points and the other two
    species using only petal width and petal length. If we plot the points, and separate
    the area of the plot where classifications are zero from the area where classifications
    are one with a line, we see that we have achieved this:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是通过仅使用花瓣宽度和花瓣长度，在Iris setosa点和其他两种物种之间拟合一条直线。如果我们绘制这些点，并用一条线将分类为零的区域与分类为一的区域分开，我们可以看到我们已经实现了这一目标：
- en: '![](img/B16254_02_09.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_02_09.png)'
- en: 'Figure 2.9: Plot of Iris setosa and non-setosa for petal width versus petal
    length; the solid line is the linear separator that we achieved after 300 iterations'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：Iris setosa和非setosa的花瓣宽度与花瓣长度关系图；实线是我们在300次迭代后获得的线性分隔线
- en: The way the separating line is defined depends on the data, the network architecture,
    and the learning process. Different starting situations, even due to the random
    initialization of the neural network's weights, may provide you with a slightly
    different solution.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔线的定义方式取决于数据、网络架构和学习过程。不同的初始情况，甚至由于神经网络权重的随机初始化，可能会为你提供稍微不同的解决方案。
- en: There's more...
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While we achieved our objective of separating the two classes with a line, it
    may not be the best model for separating two classes. For instance, after adding
    new observations, we may realize that our solution badly separates the two classes.
    As we progress into the next chapter, we will start dealing with recipes that
    address these problems by providing testing, randomization, and specialized layers
    that will increase the generalization capabilities of our recipes.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们成功地实现了用一条线分隔两个类别的目标，但这可能不是最适合分隔两个类别的模型。例如，在添加新观察数据后，我们可能会发现我们的解决方案不能很好地分隔这两个类别。随着我们进入下一章，我们将开始处理那些通过提供测试、随机化和专用层来解决这些问题的方案，这些方法将增强我们方案的泛化能力。
- en: See also
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: For information about the Iris dataset, see the documentation at [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于Iris数据集的更多信息，请参见文档：[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)。
- en: 'If you want to understand more about decision boundaries drawing for machine
    learning algorithms, we warmly suggest this excellent Medium article from Navoneel
    Chakrabarty: [https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d](https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想更深入了解机器学习算法的决策边界绘制，我们强烈推荐Navoneel Chakrabarty在Medium上发表的这篇精彩文章：[https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d](https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d)
