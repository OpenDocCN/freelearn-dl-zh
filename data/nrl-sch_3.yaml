- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: System Design and Engineering Challenges
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统设计与工程挑战
- en: Understanding **Machine Learning** (**ML**) and deep learning concepts is essential,
    but if you’re looking to build an effective search solution powered by **Artificial
    Intelligence** (**AI**) and deep learning, you need production engineering capabilities
    as well. Effectively deploying ML models requires competencies more commonly found
    in technical fields such as software engineering and DevOps. These competencies
    are called **MLOps**. This is particularly the case for a search system that requires
    high useability and low latency.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理解**机器学习**（**ML**）和深度学习的概念是至关重要的，但如果你希望构建一个由**人工智能**（**AI**）和深度学习驱动的高效搜索解决方案，你还需要具备生产工程能力。有效地部署机器学习模型需要的软件工程和DevOps等技术领域中的能力。这些能力被称为**MLOps**。特别是对于一个需要高可用性和低延迟的搜索系统，这一点尤为重要。
- en: In this chapter, you will learn the basics of designing a search system. You
    will understand core concepts such as **indexing** and **querying** and how to
    use them to save and retrieve information.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习设计搜索系统的基础知识。你将理解**索引**和**查询**等核心概念，并了解如何使用它们来保存和检索信息。
- en: 'In this chapter, we’re going to cover the following main topics in particular:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将特别讨论以下主要内容：
- en: Indexing and querying
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引与查询
- en: Evaluating a neural search system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估神经搜索系统
- en: Engineering challenges in building a neural search system
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建神经搜索系统中的工程挑战
- en: By the end of the chapter, you will have a full understanding of the capabilities
    and possible difficulties to overcome when putting a neural search into production.
    You will be able to assess when it is useful to use neural search and which approach
    would be the best for your own search system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将全面了解将神经搜索投入生产时可能遇到的能力和困难。你将能够评估何时使用神经搜索以及哪种方法最适合你自己的搜索系统。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: A laptop with a minimum of 4 GB of RAM; 8 GB is suggested.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配备至少4 GB RAM的笔记本电脑；建议使用8 GB。
- en: Python installed, with version 3.7, 3.8, or 3.9 on a Unix-like operating system,
    such as macOS or Ubuntu.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在类 Unix 操作系统（如 macOS 或 Ubuntu）上安装了版本为 3.7、3.8 或 3.9 的 Python。
- en: The code files for the chapter are available at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在[https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)找到。
- en: Introducing indexing and querying
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入索引和查询
- en: 'In this section, you will go through two important high-level tasks to build
    a search system:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将通过两个重要的高层任务来构建搜索系统：
- en: '**Indexing**: This is the process of collecting, parsing, and storing data
    to facilitate fast and accurate information retrieval. This includes adding, updating,
    deleting, and reading documents to be indexed.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：这是收集、解析和存储数据的过程，以便于快速和准确的信息检索。这包括添加、更新、删除和读取待索引的文档。'
- en: '**Querying**: Querying is the process of parsing, matching, and ranking the
    user query and sending relevant information back to the user.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**：查询是解析、匹配和排名用户查询，并将相关信息返回给用户的过程。'
- en: In a neural search system, both indexing and querying are composed of a sequence
    of tasks. Let’s take a deep look at indexing and querying components.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经搜索系统中，索引和查询都由一系列任务组成。我们将深入探讨索引和查询的各个组成部分。
- en: Indexing
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: Indexing is an important process in search systems. It forms the core functionality
    since it assists in retrieving information efficiently. Indexing reduces the documents
    to the useful information contained in them. It maps the terms to the respective
    documents containing the information. The process of finding a relevant document
    in a search system is essentially identical to the process of looking at a dictionary,
    where the index helps you find words effectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 索引是搜索系统中的一个重要过程。它构成了核心功能，因为它有助于高效地检索信息。索引将文档缩减为其中包含的有用信息。它将术语映射到包含相关信息的相应文档中。在搜索系统中找到相关文档的过程，本质上与查阅字典的过程相同，其中索引帮助你有效地查找单词。
- en: 'Before introducing the details, we start by asking the following questions
    to understand where we stand:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍详细内容之前，我们先通过以下问题来了解我们的当前状况：
- en: What are the major components of an indexing pipeline?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引管道的主要组件有哪些？
- en: What content can be indexed?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么内容可以被索引？
- en: How do we index incrementally and how do we index at speed?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何进行增量索引，以及如何实现快速索引？
- en: If you don’t know the answers to these questions, don’t worry. Just keep reading!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道这些问题的答案，不用担心，继续阅读！
- en: 'In an indexing pipeline, we normally have three major components:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引管道中，我们通常有三个主要组件：
- en: '`text/plain`, we might need a tokenizer and a stemmer, as was introduced in
    [*Chapter 1*](B17488_01.xhtml#_idTextAnchor014), *Neural Networks for Neural Search*.
    If we want to index an image of modality `image/jpeg`, we might want a component
    to resize or transform the input image into the expected format of the neural
    networks. It highly depends on your task and input data.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`text/plain`，我们可能需要一个分词器和词干提取器，如在[*第1章*](B17488_01.xhtml#_idTextAnchor014)中介绍的那样，*神经网络与神经搜索*。如果我们想要索引一种`image/jpeg`格式的图像，我们可能需要一个组件来调整大小或将输入图像转换为神经网络所期望的格式。这高度依赖于你的任务和输入数据。
- en: '**Encoder**: An encoder, in a neural search system, is identical to neural
    networks. This neural network takes your preprocessed input as a vector representation
    (embeddings). After this step, each raw document composed of text, images, videos,
    or even DNA information should be represented as a vector of numerical values.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：在神经搜索系统中，编码器与神经网络是相同的。这个神经网络将你的预处理输入作为向量表示（嵌入）。在这一步之后，每个由文本、图像、视频甚至DNA信息组成的原始文档应该被表示为一组数值向量。'
- en: '**Indexer (for storage)**: An indexer, better known as **StorageIndexer**,
    at the indexing stage stores the vectors produced from the encoder into storage,
    such as memory or a database. This includes relational databases (PostgresSQL),
    NoSQL (MongoDB), or even better, a vector database, such as Elasticsearch.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引器（用于存储）**：在索引阶段，索引器，也称为**存储索引器**，将从编码器产生的向量存储到存储设备中，如内存或数据库。这包括关系型数据库（如PostgresSQL）、NoSQL（如MongoDB），甚至更好的是，向量数据库，如Elasticsearch。'
- en: 'It should be noted that every indexing task is independent. It could vary from
    different perspectives. For instance, if you are working on a multi-model search
    engine in an e-commerce context, your objective is to create a search system that
    can take both text and an image as a query and find the most relevant products.
    In this case, your indexing might have two pathways:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，每个索引任务是独立的，它可以从不同的角度有所不同。例如，如果你在电子商务环境中构建一个多模态搜索引擎，你的目标是创建一个能够同时接受文本和图像作为查询，找到最相关产品的搜索系统。在这种情况下，你的索引可能有两个路径：
- en: Textual information should be preprocessed and encoded using text-based preprocessors
    and encoders.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本信息应该经过预处理，并使用基于文本的预处理器和编码器进行编码。
- en: Likewise, image data should be preprocessed and encoded using image-based preprocessors
    and encoders.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，图像数据应该经过预处理，并使用基于图像的预处理器和编码器进行编码。
- en: 'You might wonder what can be indexed. Anything, as long as you have an encoder
    and your data can be encoded. Some common data types you can index include text,
    image, video, and audio. As we discussed in previous chapters, you can encode
    source code to build a source code search system or gene information to build
    a search system around that. The following figure illustrates an indexing pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，什么可以被索引。任何东西，只要你有编码器并且数据可以被编码。一些常见的可索引数据类型包括文本、图像、视频和音频。正如我们在前面的章节中讨论的，你可以对源代码进行编码，以构建源代码搜索系统，或者对基因信息进行编码，构建一个围绕基因信息的搜索系统。下面的图示说明了一个索引管道：
- en: '![Figure 3.1 – A simple indexing pipeline takes documents as input and applies
    preprocessing and encoding. In the end, save the encoded features into storage
    ](img/Figure_3.1_B17488.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 一个简单的索引管道将文档作为输入，并应用预处理和编码。最后，将编码后的特征保存到存储中](img/Figure_3.1_B17488.jpg)'
- en: Figure 3.1 – A simple indexing pipeline takes documents as input and applies
    preprocessing and encoding. In the end, save the encoded features into storage
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 一个简单的索引管道将文档作为输入，并应用预处理和编码。最后，将编码后的特征保存到存储中
- en: 'Now, we move on to an important topic: incremental indexing. Firstly, let’s
    discuss what incremental indexing is.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入一个重要的话题：增量索引。首先，让我们讨论一下什么是增量索引。
- en: Understanding incremental indexing
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解增量索引
- en: '**Incremental indexing** is a crucial feature for any search system. Given
    the fact that the data collection we want to index is likely to change dramatically
    every day, we cannot afford to index the entire data collection each time there
    is a small change.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**增量索引**是任何搜索系统中的一个关键特性。考虑到我们要索引的数据集合可能每天都会发生显著变化，我们无法每次在数据发生小的变化时都重新索引整个数据集合。'
- en: 'In general, there are two common practices to perform an indexing task, as
    follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有两种常见的做法来执行索引任务，如下所示：
- en: '**Real-time indexing**: Given any data being sent to the collection, the indexer
    immediately adds the document to the index.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时索引**：对于任何发送到集合中的数据，索引器会立即将文档添加到索引中。'
- en: '**Scheduled indexing**: Given any data being sent to the collection, the scheduler
    triggers the indexing task and performs the indexing job.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定时索引**：对于任何发送到集合中的数据，调度程序触发索引任务并执行索引工作。'
- en: The preceding practices have their advantages and disadvantages. In real-time
    indexing, the user gets newly added documents immediately (if it is a match),
    while also consuming more system resources and potentially introducing data inconsistency.
    However, in the case of scheduled indexing, users don’t get access to newly added
    results in real time but it is less error prone and easier to manage.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上述做法各有优缺点。在实时索引中，用户可以立即获取新添加的文档（如果匹配的话），但也会消耗更多的系统资源，并可能引入数据不一致。然而，在定时索引的情况下，用户不能实时访问新添加的结果，但它的错误率较低，且更易于管理。
- en: The indexing strategy you choose depends on your task. If the task is time sensitive,
    it is better to use real-time indexing. Otherwise, it is good to set up a cron
    job and index your data incrementally at a certain time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的索引策略取决于你的任务。如果任务是时间敏感的，最好使用实时索引。否则，设置一个定时任务并在某个时间增量索引数据会是一个不错的选择。
- en: Speeding up indexing
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提速索引
- en: Another crucial issue when performing an indexing task in neural search is the
    speed of indexing. While a symbolic search system works only on textual data,
    the input of a neural search system could be three-dimensional (*Height * Width
    * ColorChannel*), such as an RGB image, or four-dimensional (*Frame * Height *
    Width * ColorChannel*), such as a video. This kind of data can be indexed with
    different modalities, which can dramatically slow down the data preprocessing
    and encoding process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行神经搜索索引任务时，另一个关键问题是索引的速度。传统的符号搜索系统只处理文本数据，而神经搜索系统的输入可以是三维的（*高度 * 宽度 * 颜色通道*），如
    RGB 图像，或者是四维的（*帧 * 高度 * 宽度 * 颜色通道*），如视频。这类数据可以通过不同的模态进行索引，这会显著放慢数据预处理和编码过程。
- en: 'In general, there are several strategies that we can use to boost the indexing
    speed. Some of these are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以使用几种策略来提升索引速度。以下是其中一些策略：
- en: '**Preprocessor**: Applying certain preprocessing on certain datasets could
    greatly boost your indexing speed. For instance, if you want to index high-resolution
    images, it’s better to resize them and make them smaller.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理器**：对某些数据集应用特定的预处理操作可以显著提高索引速度。例如，如果你要索引高分辨率图像，最好将它们缩小尺寸。'
- en: '**GPU inference**: In a neural search system, encoding takes most of the indexing
    time. To be more specific, given a preprocessed document, it takes time to employ
    the deep neural networks to encode the document into a vector. It could be greatly
    improved by making use of a GPU instance for encoding. Since the GPU has much
    higher bandwidth memory and L1 cache, the GPU is suitable for ML tasks.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU 推理**：在神经搜索系统中，编码占用了大部分的索引时间。更具体地说，给定一个预处理文档，使用深度神经网络将文档编码成向量是需要时间的。通过利用
    GPU 实例进行编码，这一过程可以得到极大的改善。由于 GPU 具有更高的带宽内存和 L1 缓存，GPU 非常适合用于机器学习任务。'
- en: '**Horizontal scaling**: Indexing a huge amount of data on a single machine
    makes the process slow, but it could be much faster if we distribute data across
    multiple machines and perform indexing in parallel. For example, the following
    figure demonstrates assigning more encoders to the pipeline:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**横向扩展**：在单台机器上索引大量数据会使得过程变得缓慢，但如果将数据分布到多台机器上并进行并行索引，速度可以大大提高。例如，下面的图示展示了为管道分配更多编码器：'
- en: '![Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference
    in parallel ](img/Figure_3.2_B17488.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 使用三台编码器并行利用 GPU 推理快速进行索引](img/Figure_3.2_B17488.jpg)'
- en: Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference in
    parallel
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 使用三台编码器并行利用 GPU 推理快速进行索引
- en: 'It is worth mentioning that if you come from a text retrieval background, **index
    compression** also matters when constructing an inverted index for symbolic search.
    This is not exactly the same in a neural search system anymore:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，如果您来自文本检索背景，构建符号搜索的倒排索引时也需要考虑**索引压缩**的问题。在神经搜索系统中，情况不再完全相同：
- en: First of all, the encoder takes a document as input and encodes the content
    of the document into an N-dimensional vector (embeddings). Thus, we can think
    of the encoder itself as a compression function.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，编码器将文档作为输入并将文档内容编码成一个N维向量（嵌入）。因此，我们可以将编码器本身视为一个压缩函数。
- en: Second, compression of the dense vectors will eventually sacrifice the quality
    of the vectors. Normally, larger-dimensionality vectors bring better search results
    since they can better represent the documents being encoded.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，压缩密集向量最终会牺牲向量的质量。通常，较高维度的向量能够带来更好的搜索结果，因为它们能更好地表示被编码的文档。
- en: In practice, we need to find a balance point between dimensionality and memory
    usage in order to load all vectors into memory to perform a large-scale similarity
    search. In the next section, we will dive into the querying part, which will allow
    you to understand how to conduct a large-scale similarity search.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们需要在维度和内存使用之间找到一个平衡点，以便将所有向量加载到内存中进行大规模相似性搜索。在接下来的部分中，我们将深入探讨查询部分，这将使您能够了解如何进行大规模相似性搜索。
- en: Querying
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询
- en: 'When it comes to the querying pipeline, it has a lot of components overlapping
    with the indexing pipeline, but with a few modifications and additional components,
    such as a ranker. At this stage, the input of the pipeline is a single user query.
    There are four major components of a typical querying task:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到查询流水线时，它与索引流水线有很多组件重叠，但有一些修改和额外的组件，如排序器。在此阶段，流水线的输入是单个用户查询。典型查询任务有四个主要组件：
- en: '**Preprocessor**: This component is similar to the preprocessor in the indexing
    pipeline. It takes the query document as input and applies the same preprocessors
    as the indexing pipeline to the input.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理器**：此组件类似于索引流水线中的预处理器。它将查询文档作为输入，并对输入应用与索引流水线相同的预处理器。'
- en: '**Encoder**: The encoder takes the preprocessed query document as input and
    produces vectors as output. It should be noted that in a cross-modal search system,
    your encoder from indexing might be different from the encoder at the querying
    step. This will be explained in [*Chapter 7*](B17488_07.xhtml#_idTextAnchor101),
    *Exploring Advanced Use Cases of Jina*.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：编码器将预处理后的查询文档作为输入，并产生向量作为输出。需要注意的是，在跨模态搜索系统中，您的索引编码器可能与查询步骤中的编码器不同。这将在[*第7章*](B17488_07.xhtml#_idTextAnchor101)，*深入探讨
    Jina 的高级使用案例*中解释。'
- en: '**Indexer**: This indexer, better named **SearchIndexer**, takes vectors produced
    from the encoder as input and conducts a large-scale similarity search over all
    indexed documents. This is called **Approximate Nearest Neighbor** (**ANN**) search.
    We’ll elaborate more on this concept in the following section.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引器**：这个索引器，更好地称为**搜索索引器**，接收编码器生成的向量作为输入，并在所有索引文档上进行大规模相似性搜索。这被称为**近似最近邻**（**ANN**）搜索。我们将在接下来的部分详细解释这个概念。'
- en: '**Ranker**: The ranker takes the query vector and similarity scores against
    each of the item within the collection, produces a ranked list in descending order,
    and returns results to the user.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排序器**：排序器接收查询向量和每个集合项的相似度分数，按降序生成一个排名列表，并将结果返回给用户。'
- en: One major difference between indexing and querying is that indexing (in most
    cases) is an offline task while querying is an online task. To be more concrete,
    when we bootstrap a neural search system and create a query, the system will return
    an empty list since nothing has been indexed at the moment. Before we *expose*
    the search system to the user, we should have pre-indexed all the documents within
    the data collection. This indexing is performed offline.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 索引和查询的一个主要区别在于索引（在大多数情况下）是离线任务，而查询是在线任务。具体来说，当我们引导一个神经搜索系统并创建一个查询时，系统会返回一个空列表，因为此时尚未索引任何内容。在将搜索系统*暴露*给用户之前，我们应该预先索引数据集中的所有文档。这种索引是在离线状态下进行的。
- en: On the other hand, in a querying task, the user sends one query to the system
    and expects to get matches immediately. All the preprocessing, encoding, index
    searching, and ranking should be finished during the waiting time. Thus, it is
    an online task.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在查询任务中，用户发送一个查询给系统，并期望立即获得匹配结果。所有的预处理、编码、索引搜索和排名应该在等待时间内完成。因此，这是一个在线任务。
- en: Important Note
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Real-time indexing can be considered an online task.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实时索引可以视为一个在线任务。
- en: Unlike indexing, while querying, each user sends a single document as a query
    to the system. The preprocessing and encoding take a very short time. On the other
    hand, finding similar items in the index storage becomes a critical engineering
    challenge that impacts the neural search system performance. Why is that?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与索引不同，在查询时，每个用户将单个文档作为查询发送给系统。预处理和编码只需要很短的时间。另一方面，在索引存储中查找相似项成为一个关键的工程挑战，这会影响神经搜索系统的性能。为什么会这样？
- en: For instance, you have pre-indexed a billion documents, and at querying time,
    the user sends one query to the system, the document is then preprocessed and
    encoded into a vector (embedding). Given the query vector, now you need to find
    the top N similar vectors out of 1 million vectors. How do you achieve that? Conducting
    similarity searches by computing distances between vectors one by one could take
    ages. Rather than that, we perform an ANN search.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您已经预先索引了十亿个文档，在查询时，用户将一个查询发送给系统，文档随后被预处理并编码为向量（嵌入）。给定查询向量，您现在需要在 100 万个向量中找到前
    N 个相似向量。如何实现这一点？通过计算向量之间的距离逐个进行相似性搜索可能需要很长时间。与其如此，我们执行 ANN 搜索。
- en: Important Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When we’re talking about ANN search, we’re considering a million-/billion-scale
    search. If you want to build a toy example and search across hundreds or thousands
    of documents, a normal linear scan is fast enough. In a production environment,
    please follow the selection strategy as will be introduced in the next section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论 ANN 搜索时，我们是在考虑百万级/十亿级的搜索。如果你想构建一个玩具示例并搜索几百或几千个文档，普通的线性扫描就足够快了。在生产环境中，请按照下一节将介绍的选择策略进行操作。
- en: ANN search
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ANN 搜索
- en: 'ANN search, as defined by its name, is a trade-off between different factors:
    accuracy, runtime, and memory consumption. Compared with brute-force search, it
    ensures the running time will be accepted by the user while sacrificing precision/recall
    to a certain degree. How fast can it achieve? Given a billion 100-dimensional
    vectors, it can be fitted into a server with 32 GB memory with a 10 ms response
    rate. Before diving into details about ANN search, let’s first take a look at
    the following figure:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，ANN 搜索是不同因素之间的权衡：准确性、运行时间和内存消耗。与暴力搜索相比，它确保了运行时间用户可以接受，同时牺牲了某些程度的精度/召回率。它能够达到多快的速度？给定十亿个
    100 维向量，它可以适配到内存为 32 GB 的服务器上，响应时间为 10 毫秒。在深入了解 ANN 搜索的细节之前，让我们先看一下下面的图示：
- en: '![Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor
    Search, Yusuke Matsui) ](img/Figure_3.3_B17488.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor
    Search, Yusuke Matsui)](img/Figure_3.3_B17488.jpg)'
- en: 'Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor
    Search, Yusuke Matsui)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor
    Search, Yusuke Matsui)'
- en: The preceding figure illustrates *how to select the ANN library given your search
    system*. In the figure, *N* represents the number of documents inside your *StorageIndexer*.
    Different numbers of N can be optimized using different ANN search libraries,
    such as `FAISS` ([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))
    or `NMSLIB` ([https://github.com/nmslib/nmslib](https://github.com/nmslib/nmslib)).
    Meanwhile, as you’re most likely to be a Python user, the `Annoy` library ([https://github.com/spotify/annoy](https://github.com/spotify/annoy))
    has provided a user-friendly interface with reasonable performance, and it works
    well enough for a million-scale vector search.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示说明了*如何根据您的搜索系统选择 ANN 库*。在图示中，*N*表示您*StorageIndexer* 中文档的数量。不同的 N 数量可以通过不同的
    ANN 搜索库进行优化，例如 `FAISS` ([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))
    或 `NMSLIB` ([https://github.com/nmslib/nmslib](https://github.com/nmslib/nmslib))。与此同时，由于您很可能是
    Python 用户，`Annoy` 库 ([https://github.com/spotify/annoy](https://github.com/spotify/annoy))
    提供了一个用户友好的接口，并具有合理的性能，足以应对百万级向量搜索。
- en: The aforementioned libraries were implemented based on different algorithms,
    the most popular ones being **KD-Trees**, **Locally Sensitive Hashing** (**LSH**),
    and **Product Quantization** (**PQ**).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上述库是基于不同算法实现的，其中最流行的包括 **KD-Tree**、**局部敏感哈希** (**LSH**) 和 **产品量化** (**PQ**)。
- en: 'KD-Trees follows an iterative process to construct a tree. To make the visualization
    easier, we suppose the data only consists of two features, *f1* (*x* axis) and
    *f2* (*y* axis), which looks like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: KD-Tree 遵循一个迭代过程来构建树。为了简化可视化，我们假设数据只包含两个特征，*f1*（*x* 轴）和 *f2*（*y* 轴），其形态如下：
- en: '![Figure 3.4 – KD-Trees, sample dataset to index ](img/Figure_3.4_B17488.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – KD-Tree，样本数据集以进行索引](img/Figure_3.4_B17488.jpg)'
- en: Figure 3.4 – KD-Trees, sample dataset to index
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – KD-Tree，样本数据集以进行索引
- en: 'Construction of a KD-Tree starts with selecting a practical feature and setting
    a threshold for this feature. To illustrate the idea, we begin with a manual selection
    of f1 and a feature threshold of 0.5\. To this end, we get a boundary like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: KD-Tree 的构建从选择一个实际的特征并为该特征设置阈值开始。为了说明这个概念，我们从手动选择 f1 和特征阈值 0.5 开始。为此，我们得到如下的边界：
- en: '![Figure 3.5 – KD-Trees construction iteration 1 ](img/Figure_3.5_B17488.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – KD-Tree 构建迭代 1](img/Figure_3.5_B17488.jpg)'
- en: Figure 3.5 – KD-Trees construction iteration 1
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – KD-Tree 构建迭代 1
- en: 'As you can see from *Figure 3.5*, the feature space has been split into two
    parts by our first selection of f1 with a threshold of 0.5\. How is it reflected
    for the tree? When building the index, we’re essentially creating a binary search
    tree. The first selection of f1 with a threshold of 0.5 became our root node.
    Given each data point, if the f1 is greater than 0.5, it will be placed to the
    right of the node. Otherwise, as shown in *Figure 3.6*, we put it to the left
    of the node:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 3.5* 中可以看出，特征空间已经通过我们首次选择的 f1 阈值 0.5 分为两部分。那么它是如何反映在树中的呢？在构建索引时，我们实际上是在创建一棵二叉搜索树。我们首次选择
    f1 阈值 0.5 成为根节点。给定每个数据点，如果 f1 大于 0.5，它将被放置在节点的右侧。否则，如 *图 3.6* 所示，我们将其放置在节点的左侧：
- en: '![Figure 3.6 – KD-Trees construction iteration 2 ](img/Figure_3.6_B17488.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – KD-Tree 构建迭代 2](img/Figure_3.6_B17488.jpg)'
- en: Figure 3.6 – KD-Trees construction iteration 2
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – KD-Tree 构建迭代 2
- en: 'We continue from the preceding tree. In the second iteration, let’s define
    our rule as: given f1 > 0.5, select f2 with threshold 0.5\. As was shown in the
    preceding graph, now we split the feature space again based on the new rule, and
    it is also reflected on our tree: we created a new node, **f2-0.5**, in the figure
    (the **none** node is only for visualization purpose; we haven’t created this
    node). This is shown in the following figure:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前面的树继续。在第二次迭代中，我们定义规则为：给定 f1 > 0.5，选择 f2 阈值为 0.5。如前面的图所示，我们现在根据新规则再次拆分特征空间，这也体现在我们的树上：我们在图中创建了一个新节点
    **f2-0.5**（**none** 节点仅用于可视化；我们尚未创建此节点）。如下图所示：
- en: '![Figure 3.7 – KD-Tree construction iteration N (final iteration) ](img/Figure_3.7_B17488.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – KD-Tree 构建迭代 N（最后一次迭代）](img/Figure_3.7_B17488.jpg)'
- en: Figure 3.7 – KD-Tree construction iteration N (final iteration)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – KD-Tree 构建迭代 N（最后一次迭代）
- en: 'As shown in *Figure 3.7*, the entire feature space has been split into six
    bins. Compared with before, we added three new nodes, including two leaf nodes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.7* 所示，整个特征空间已被分割成六个区间。与之前相比，我们新增了三个节点，其中包括两个叶节点：
- en: The previous **none** was replaced by an actual node, **f2-0.65**; this node
    split the space of f2 based on the threshold 0.65, and it only happens when f1<0.5.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前的 **none** 被一个实际的节点 **f2-0.65** 替代；这个节点基于阈值 0.65 对 f2 的空间进行了拆分，且仅在 f1<0.5
    时发生。
- en: When f2<0.65, we further split f1 by a threshold of 0.2.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 f2<0.65 时，我们进一步通过阈值 0.2 对 f1 进行拆分。
- en: When f2>0.65, we further split f1 by a threshold of 0.3.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 f2>0.65 时，我们进一步通过阈值 0.3 对 f1 进行拆分。
- en: To this end, our tree has three leaf nodes, each leaf node can construct two
    bins (less/larger than the threshold), and we have six bins in total. Also, each
    data point can be placed into one of the bins. Then, we finish the construction
    of the KD-Tree. It should be noted that constructing a KD-Tree could be non-trivial
    since you need to consider some hyperparameters, such as how to set the threshold
    or how many bins we should create (or the stop criteria). In practice, there are
    no golden rules. Normally, the mean or median can be used to set the threshold.
    The number of bins could be highly dependent on the evaluation of the results
    and fine-tuning.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们的树有三个叶节点，每个叶节点可以构造两个区间（小于/大于阈值），总共有六个区间。此外，每个数据点可以放入其中一个区间。然后，我们完成了KD树的构建。需要注意的是，构建KD树可能并不简单，因为你需要考虑一些超参数，例如如何设置阈值或应该创建多少个区间（或者停止标准）。在实践中，并没有固定的规则。通常，可以使用均值或中位数来设置阈值。区间的数量可能会高度依赖于结果的评估和微调。
- en: At search time, given a user query, it can be placed into one of the bins inside
    the feature space. We are able to compute the distance between the query and all
    the items within the bin as candidates for nearest neighbors. We also need to
    compute the minimum distance between the query and all other bins. If the distance
    between the query vector and other bins is greater than the distance between the
    query vector and the candidate for nearest neighbors, we can ignore all data points
    within that bin by pruning the leaf node of the tree. Otherwise, we consider the
    data points within that bin as candidates for nearest neighbors as well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询时，给定一个用户查询，它可以被放置到特征空间中的某个区间。我们可以计算查询与区间内所有项目之间的距离，将它们作为最近邻候选项。我们还需要计算查询与其他区间之间的最小距离。如果查询向量与其他区间的距离大于查询向量与最近邻候选项之间的距离，我们可以通过修剪树的叶节点来忽略该区间内的所有数据点。否则，我们也将该区间内的数据点视为最近邻候选项。
- en: By constructing a KD-Tree, we do not necessarily compute the similarity between
    a query vector and each document anymore. Only a certain number of bins should
    be considered as candidates. Thus, the search time can be greatly reduced.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建KD树，我们不再需要计算查询向量与每个文档之间的相似性。只有某些数量的区间应该被视为候选项。因此，搜索时间可以大大减少。
- en: In practice, KD-Trees suffer from the curse of dimensionality. It is tricky
    to apply them to high-dimensional data because there are so many bins to search
    through simply because for each feature, we always create several thresholds.
    **Locality Sensitive Hashing** (**LSH**) could be a good alternative algorithm.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，KD树受到维度灾难的困扰。将其应用于高维数据时会很棘手，因为每个特征我们总是创建多个阈值，因此需要搜索的区间非常多。**局部敏感哈希**（**LSH**）可能是一个很好的替代算法。
- en: 'The basic idea behind LSH is that similar documents share the same hash code
    and it is designed to maximize collisions. To be more concrete: given a set of
    vectors, we want to have a hashing function that is able to encode similar documents
    into the same hashing bucket. Then, we only need to find similar vectors within
    the bucket (without the need to scan all the data).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: LSH的基本思想是相似的文档共享相同的哈希码，并且它的设计旨在最大化冲突。更具体地说：给定一组向量，我们希望有一个哈希函数能够将相似的文档编码到相同的哈希桶中。然后，我们只需在桶内查找相似的向量（无需扫描所有数据）。
- en: Let’s start with LSH index construction. At indexing time, we first need to
    create random planes (hyperplanes) to split the feature space into *bins*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从LSH索引构建开始。在索引时，我们首先需要创建随机的超平面（平面）来将特征空间分割成*区间*。
- en: '![Figure 3.8 – LSH index construction with random hyperplanes ](img/Figure_3.8_B17488.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 使用随机超平面构建LSH索引](img/Figure_3.8_B17488.jpg)'
- en: Figure 3.8 – LSH index construction with random hyperplanes
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 使用随机超平面构建LSH索引
- en: 'In *Figure 3.8*, we have created six hyperplanes. Each hyperplane is able to
    split our feature space into two bins, either left/right or up/bottom, which can
    be represented as binary codes (or signs): 0 or 1\. This is called the index of
    a bin.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 3.8*中，我们创建了六个超平面。每个超平面能够将我们的特征空间分割成两个区间，可以是左/右或上/下，这些区间可以用二进制代码（或符号）表示：0
    或 1。这被称为一个区间的索引。
- en: 'Let’s try to get the bin index of the bottom-right bin (which has four points
    in the bin). The bin is located at the following points:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试获取右下角区间的索引（该区间内有四个点）。该区间位于以下几个点：
- en: Right of **plane1**, so the sign at position 0 is 1
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**plane1**的右侧，所以位置0的符号为1。
- en: Right of **plane2**, so the sign at position 1 is 1
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plane2**的右侧，因此位置1的符号为1  '
- en: Right of **plane3**, so the sign at position 2 is 1
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plane3**的右侧，因此位置2的符号为1  '
- en: Right of **plane4**, so the sign at position 3 is 1
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plane4**的右侧，因此位置3的符号为1  '
- en: Bottom of **plane5**, so the sign at position 4 is 0
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plane5**的底部，因此位置4的符号为0  '
- en: Bottom of **plane6**, so the sign at position 5 is 0
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plane6**的底部，因此位置5的符号为0  '
- en: So, we can represent the bottom-right bin as 111100\. If we iterate this process
    and annotate each bin with a bin index, we’ll end up with a hash map. The keys
    of the hash map are the bin indexes, while the values of the hash map are the
    IDs of the data points within the bin.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们可以将右下角的桶表示为111100。如果我们迭代这个过程，并为每个桶注释一个桶索引，我们将得到一个哈希映射。哈希映射的键是桶索引，而哈希映射的值是桶内数据点的ID。  '
- en: '![Figure 3.9 – LSH index construction with bin index ](img/Figure_3.9_B17488.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 带桶索引的LSH索引构建](img/Figure_3.9_B17488.jpg)  '
- en: Figure 3.9 – LSH index construction with bin index
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.9 – 带桶索引的LSH索引构建  '
- en: Searching on top of LSH is easy. Intuitively, given a query, you can just search
    all data points within its own bin, or you can search through its neighboring
    bins.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在LSH的顶部进行搜索是简单的。直观地说，给定一个查询，你可以直接搜索其所在桶中的所有数据点，或者你可以搜索其相邻的桶。  '
- en: How do you search through its neighboring bins? Take a look at *Figure 3.9*.
    The bin index is represented as binary code; the neighboring bins will only have
    a 1-bit difference compared with its own bin index. Apparently, you can consider
    the difference between bin index as a hyperparameter, and search through more
    neighboring bins. For example, if you set the hyper parameters as 2, means you
    allow LSH to search through 2 neighbor bins.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '如何搜索其相邻的桶？看一下*图3.9*。桶索引以二进制代码表示；相邻的桶与其自身的桶索引相比只有1位差异。显然，你可以将桶索引之间的差异视为一个超参数，并通过更多相邻的桶进行搜索。例如，如果你将超参数设置为2，意味着你允许LSH搜索2个相邻的桶。  '
- en: 'To better understand this, we’ll look into the Annoy implementation of LSH,
    namely LSH with random projection. Given a list of vectors produced by a deep
    neural network, we first do the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更好地理解这一点，我们将查看LSH的Annoy实现，即带有随机投影的LSH。给定由深度神经网络生成的向量列表，我们首先执行以下操作：  '
- en: Randomly initialize a hyperplane.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '随机初始化一个超平面。  '
- en: Dot product the normal (the vector that is perpendicular to the hyperplane)
    against the vectors. For each vector, if the value is positive, we generate a
    binary code of 1, otherwise 0.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算法线（垂直于超平面的向量）与各个向量的点积。对于每个向量，如果值为正，我们生成一个二进制代码1，否则为0。  '
- en: We generate N hyperplanes and iterate the process N times. At the end, each
    vector is represented by a binary vector of 0s and 1s.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们生成N个超平面并迭代这个过程N次。最后，每个向量将由一组0和1组成的二进制向量表示。  '
- en: We treat each binary code as a bucket and save all documents with the same binary
    code into the same bucket.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们将每个二进制代码视为一个桶，并将所有具有相同二进制代码的文档保存在同一个桶中。  '
- en: 'The following code block demonstrates a simple implementation of LSH with random
    projection:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '以下代码块展示了一个带有随机投影的LSH简单实现：  '
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We preprocess two pieces of sentences into buckets:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将两段句子预处理为桶：  '
- en: '[PRE1]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this way, we map millions of documents into multiple buckets. At search time,
    we use the same hyperplanes to encode the search document, get the binary code,
    and find similar documents within the same bucket.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这种方式，我们将数百万个文档映射到多个桶中。在搜索时，我们使用相同的超平面来编码搜索文档，获取二进制代码，并在同一桶内查找相似的文档。  '
- en: 'In the Annoy implementation, search speed is dependent on two parameters:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '在Annoy实现中，搜索速度依赖于两个参数：  '
- en: '`search_k`: This parameter is the top `k` elements you want to get back from
    the index.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`search_k`：该参数表示你希望从索引中返回的前`k`个元素。'
- en: '`N_trees`: This parameter represents the number of buckets you want to search
    from.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N_trees`：该参数表示你想从中搜索的桶的数量。  '
- en: It is obvious that the search runtime highly depends on these two parameters,
    and the user needs to fine-tune the parameters based on their use case.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '显然，搜索运行时非常依赖于这两个参数，用户需要根据自己的使用场景来微调这些参数。  '
- en: 'Another popular ANN search algorithm is PQ. Before we dive into PQ, it is important
    to understand what *quantization* is. Suppose you have a million documents to
    index, and you created 100 *centroids* for all documents. A **quantizer** is a
    function that can map a vector to a centroid. You might find the idea very familiar.
    Actually, the K-means algorithm is a function that can help you generate such
    centroids. If you do not remember, K-means works as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的 ANN 搜索算法是 PQ。在我们深入讨论 PQ 之前，理解什么是 *量化* 非常重要。假设你有一百万个文档需要索引，并且你为所有文档创建了
    100 个 *中心点*。**量化器**是一个能够将向量映射到中心点的函数。你可能会觉得这个想法很熟悉。实际上，K-means 算法就是一个能够帮助你生成这种中心点的函数。如果你不记得了，K-means
    的工作原理如下：
- en: Randomly initialize `k` centroids.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化 `k` 个中心点。
- en: Assign each vector to its closest centroid. Each centroid represents a cluster.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个向量分配给其最近的中心点。每个中心点代表一个聚类。
- en: Compute new centroids based on the mean of all assignments, until converge.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于所有分配的均值计算新的中心点，直到收敛。
- en: Once K-means converge, we get K clusters given all vectors to index. For each
    document to index, we create a map between the document ID and cluster index.
    At search time, we compute the distance query vector against the centroids and
    get the closest clusters, then find the closest vectors within these clusters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 K-means 收敛，我们就会得到 K 个聚类，以便为所有需要索引的向量生成聚类。对于每个需要索引的文档，我们创建文档 ID 和聚类索引之间的映射。在搜索时，我们计算查询向量与中心点的距离，并得到最接近的聚类，然后在这些聚类内找到最接近的向量。
- en: This quantization algorithm has a relatively good compression ratio. You don’t
    have to linearly scan all vectors in order to get the closest ones; you only need
    to scan certain clusters produced by the quantizer. On the other hand, the recall
    rate at searching time could be very low if the number of centroids is small.
    This is because there are too many edge cases that cannot be correctly distributed
    to the correct cluster. Also, if we simplify the set number of centroids to a
    large number, our K-means operations will take a long time to converge. This becomes
    a bottleneck at both offline indexing time and online searching time.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种量化算法具有相对较好的压缩比。你不需要线性扫描所有向量来找到最接近的向量；你只需要扫描由量化器生成的某些聚类。另一方面，如果中心点的数量较少，搜索时的召回率可能会非常低。这是因为有太多边界情况无法正确分配到正确的聚类中。另外，如果我们将中心点的数量简化为一个很大的数字，我们的
    K-means 操作将需要很长时间才能收敛。这将成为离线索引和在线搜索时间的瓶颈。
- en: 'The basic idea behind PQ is to split high-dimensional vectors into subvectors,
    as illustrated in the following steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: PQ 的基本思想是将高维向量分割成子向量，如下所示的步骤所示：
- en: We split each vector into *m* subvectors.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将每个向量分割成 *m* 个子向量。
- en: For each subvector, we apply quantization. To this end, for each subvector,
    we have a unique cluster ID (the closest cluster of the subvector to its centroids).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个子向量，我们应用量化。为此，我们为每个子向量分配一个唯一的聚类 ID（该子向量与其中心点的最近聚类）。
- en: For the full vector, we have a list of cluster IDs, which can be used as the
    codebook of the full vector. The dimensionality of the codebook is identical to
    the number of subvectors.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于整个向量，我们有一个聚类 ID 列表，可以作为整个向量的词典。词典的维度与子向量的数量相同。
- en: 'The following figure illustrates the PQ algorithm: given a vector, we cut it
    into subvectors of lower dimensionality and apply quantization. To this end, each
    quantized subvector gets a code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 PQ 算法：给定一个向量，我们将其切分成低维的子向量并应用量化。为此，每个量化后的子向量都会得到一个代码：
- en: '![Figure 3.10 – Product quantization ](img/Figure_3.10_B17488.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 产品量化](img/Figure_3.10_B17488.jpg)'
- en: Figure 3.10 – Product quantization
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 产品量化
- en: At search time, again, we split the high-dimensional query vector into subvectors
    and generate a codebook (bucket). We compute subvector-level cosine similarity
    against each of the vectors inside the collection and sum up the subvector-level
    similarity score. We sort the final results based on the vector-level cosine similarity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索时，我们再次将高维查询向量分割成子向量，并生成一个词典（桶）。我们计算每个子向量与集合内每个向量的余弦相似度，并将子向量级别的相似度得分求和。然后根据向量级别的余弦相似度对最终结果进行排序。
- en: In practice, FAISS has a high-performant implementation of PQ (and beyond PQ).
    For more info, please refer to the documentation ([https://github.com/facebookresearch/faiss/wiki](https://github.com/facebookresearch/faiss/wiki)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，FAISS 有一个高性能的 PQ（以及超越 PQ）实现。更多信息，请参考文档（[https://github.com/facebookresearch/faiss/wiki](https://github.com/facebookresearch/faiss/wiki)）。
- en: Now we have learned two fundamental tasks, indexing and querying, for neural
    search. In the next section, we are going to cover neural search system evaluation
    to make your neural search system complete and production-ready.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了神经搜索的两个基本任务——索引和查询。在接下来的部分，我们将介绍神经搜索系统评估，使你的神经搜索系统更加完整并具备生产就绪的能力。
- en: Evaluating a neural search system
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估神经搜索系统
- en: Evaluating the effectiveness of a neural search system is critical once you
    set up some baseline. By monitoring the evaluation metrics, you can immediately
    know how well your system performs. By diving deep into the queries, you can also
    conduct failure analysis and learn how to improve your system.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 评估神经搜索系统的有效性在你设定了基准之后是至关重要的。通过监控评估指标，你可以立即了解系统的表现如何。通过深入分析查询，你还可以进行失败分析，并学习如何改进系统。
- en: In this section, we will give a brief overview of the most commonly used evaluation
    metrics. If you want to have a more detailed mathematical understanding of this
    topic, we strongly recommend you go through *Evaluation in information retrieval*
    ([https://nlp.stanford.edu/IR-book/pdf/08eval.pdf](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要概述最常用的评估指标。如果你想对这个主题有更详细的数学理解，我们强烈建议你阅读《信息检索中的评估》([https://nlp.stanford.edu/IR-book/pdf/08eval.pdf](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf))。
- en: 'In general, given the difference between search tasks, normally we can group
    search evaluation into two categories:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，考虑到搜索任务的差异，我们可以将搜索评估分为两类：
- en: '**Evaluation of unranked results**: These metrics are widely used in some retrieval
    or classification tasks, including precision, recall, and F-Score.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未排名结果的评估**：这些指标广泛用于一些检索或分类任务，包括精度、召回率和F值。'
- en: '**Evaluation of ranked results**: These metrics are mainly used in typical
    search applications given the results are ordered (ranked).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排名结果的评估**：这些指标主要用于典型的搜索应用，因为结果是有序的（已排名）。'
- en: 'First, let’s start with precision, recall, and F-Score:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从精度、召回率和F值开始：
- en: 'In a typical search scenario, precision is defined as follows:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在典型的搜索场景中，精度的定义如下：
- en: '*Precision = (Num of Relevant Documents Retrieved) / (Num of Retrieved Documents)*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*精度 = （检索到的相关文档数）/（检索到的文档总数）*'
- en: The idea is straightforward. Suppose our search system returns 10 documents,
    where 7 out of 10 are relevant, then the precision would be 0.7.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路很简单。假设我们的搜索系统返回了10篇文档，其中7篇是相关的，那么精度就是0.7。
- en: It should be noted that during evaluation, we care about the top `k` retrieved
    results. Just like the aforementioned example, we evaluate relevant documents
    in the top 10 results. This is normally referred to as precision, such as **Precision@10**.
    It also applies to other metrics we will introduce later in this section, such
    as Recall@10, mAP@10, and nDCG@10.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在评估过程中，我们关注的是前`k`个检索结果。就像前面提到的例子，我们评估的是前10个结果中的相关文档。这通常被称为精度，例如**Precision@10**。它同样适用于我们将在本节稍后介绍的其他指标，比如Recall@10、mAP@10和nDCG@10。
- en: 'Similarly, recall is defined as follows:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，召回率的定义如下：
- en: '*Recall = (Num of Relevant Documents Retrieved) / (Num of Relevant Documents)*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率 = （检索到的相关文档数）/（相关文档总数）*'
- en: For instance, if we search `cat` in our system, and we already know that there
    are 100 cat-related images being indexed, and 80 images are returned, then the
    recall rate is 0.8\. It is an evaluation metric to measure the *completeness*
    of how the search system performs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们在系统中搜索`cat`，并且我们知道系统中有100张与猫相关的图片已经被索引，且返回了80张图片，那么召回率就是0.8。它是一个衡量搜索系统执行*完整性*的评估指标。
- en: Important Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Recall is the most important evaluation metric to evaluate the performance of
    an ANN algorithm since it depicts the fraction of true nearest neighbors found
    for all the queries on average.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是评估ANN算法性能最重要的评估指标，因为它描述了在所有查询中，找到的真实最近邻的比例。
- en: Important Note
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Accuracy can be a good metric for typical ML tasks, such as classification.
    But this is not the case for search tasks since most of the datasets in search
    tasks are skewed/imbalanced.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率可以作为典型机器学习任务的一个良好指标，例如分类。但是对于搜索任务来说情况不同，因为大多数搜索任务的数据集都是偏斜的/不平衡的。
- en: 'As a search system designer, you might already notice that these two numbers
    are trade-offs against each other: with an increased number of K, we can always
    expect lower precision but higher recall and vice versa. It is your decision to
    optimize precision or recall or optimize these two numbers as one evaluation metric,
    that is, F1-Score.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 作为搜索系统设计师，您可能已经注意到这两个数值是相互权衡的：随着 K 值的增加，我们通常会期望精确度降低，但召回率提高，反之亦然。您可以决定优化精确度或召回率，或者将这两个数值作为一个评估指标进行优化，即
    F1-Score。
- en: 'F1-Score is defined as follows:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1-Score 定义如下：
- en: '*F1-Score = (2 * Precision * Recall) / (Precision + Recall)*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1-Score = (2 * 精确度 * 召回率) / (精确度 + 召回率)*'
- en: 'It is a weighted harmonic mean of precision and recall. In reality, a higher
    recall tends to be associated with a lower precision rate. Imagine you are evaluating
    a ranked list and you care about the top 10 items being retrieved (and there are
    10 relevant documents in the entire collection):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 它是精确度和召回率的加权调和平均值。实际上，更高的召回率通常会伴随较低的精确度。假设你正在评估一个排序列表，并且你关心的是前 10 个项目被检索出来（并且在整个集合中有
    10 个相关文档）：
- en: '| **Document** | **Label** | **Precision** | **Recall** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **文档** | **标签** | **精确度** | **召回率** |'
- en: '| Doc1 | Relevant | 1/1 | 1/10 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Doc1 | 相关 | 1/1 | 1/10 |'
- en: '| Doc2 | Relevant | 2/2 | 2/10 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Doc2 | 相关 | 2/2 | 2/10 |'
- en: '| Doc3 | Irrelevant | 2/3 | 2/10 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Doc3 | 无关 | 2/3 | 2/10 |'
- en: '| Doc4 | Irrelevant | 2/4 | 2/10 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Doc4 | 无关 | 2/4 | 2/10 |'
- en: '| Doc5 | Relevant | 3/5 | 3/10 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Doc5 | 相关 | 3/5 | 3/10 |'
- en: '| Doc6 | Irrelevant | 3/6 | 3/10 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Doc6 | 无关 | 3/6 | 3/10 |'
- en: '| Doc7 | Irrelevant | 3/7 | 3/10 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Doc7 | 无关 | 3/7 | 3/10 |'
- en: '| Doc8 | Relevant | 4/8 | 4/10 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Doc8 | 相关 | 4/8 | 4/10 |'
- en: '| Doc9 | Irrelevant | 4/9 | 4/10 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Doc9 | 无关 | 4/9 | 4/10 |'
- en: '| Doc10 | Irrelevant | 4/10 | 4/10 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Doc10 | 无关 | 4/10 | 4/10 |'
- en: Table 3.1 – Precision recall for 10 of the top 10 documents
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 – 前 10 个文档的精确度召回率
- en: '*Table 3.1* shows the precision and recall at different levels given binary
    labels.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 3.1* 显示了给定二进制标签下，不同层次的精确度和召回率。'
- en: Being familiar with precision, we can now move on to calculate the **Average
    Precision** (**AP**). This metric will give us a better understanding of our search
    system’s ability to sort the results of a query.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉了精确度后，我们可以继续计算 **平均精度**（**AP**）。这个指标将帮助我们更好地理解搜索系统对查询结果排序的能力。
- en: 'Specifically, given the preceding-ranked list, `aP@10` is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，给定前面排名的列表，`aP@10` 如下：
- en: '`aP@10 = (1/1 + 2/2 + 3/5 + 4/8) / 10 = 0.31`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`aP@10 = (1/1 + 2/2 + 3/5 + 4/8) / 10 = 0.31`'
- en: Note that only the precision of relevant documents is taken into consideration
    when computing aP.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在计算 aP 时，只考虑相关文档的精确度。
- en: Now, the aP has been calculated against one specific user query. However, to
    give a more robust search system evaluation, we want to evaluate the performance
    of a collection of user queries as a test set. This is called `aP@k`, then we
    average all the aPs over a set of queries to get the mAP score.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，aP 已经针对一个特定的用户查询进行了计算。然而，为了更稳健地评估搜索系统的性能，我们希望将多个用户查询作为测试集进行评估。这就是 `aP@k`，然后我们将所有查询的
    aP 平均，得到 mAP 得分。
- en: 'mAP is one of the most important search system evaluation metrics given an
    ordered rank list. To conduct mAP evaluation on your search system, normally you
    have to follow these steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: mAP 是在给定有序排名列表的情况下，最重要的搜索系统评估指标之一。要对搜索系统进行 mAP 评估，通常需要遵循以下步骤：
- en: Compose a list of queries to have a good representation of users’ information
    needs. The number is dependent on your situation, such as 50, 100, or 200.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个查询列表，以便很好地代表用户的信息需求。查询的数量取决于您的情况，例如 50、100 或 200。
- en: If your documents already have labels that indicate the degree of relevance,
    use the labels directly to compute aP per query. If your documents do not contain
    any relevant information against each query, we need expert annotation or pooling
    to access relevant degrees.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的文档已经有标签来指示相关性程度，直接使用这些标签来计算每个查询的 **平均精度**（aP）。如果文档中没有与每个查询相关的内容，我们需要专家注释或池化来访问相关程度。
- en: Compute mAP over a list of queries by taking the average of the aP. As was mentioned
    previously, if you do not have a relevant assessment for ranked documents, one
    common technique is called **pooling**. It requires us to set up multiple search
    systems (such as three) for testing. Given each query, we collect the top K documents
    returned by each of these three search systems. A human annotator judges the degree
    of relevance of all 3 * K documents. Afterward, we consider all documents outside
    this pool as irrelevant, while all documents inside this pool are relevant. Then,
    the search results can be evaluated on top of the pools.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对查询列表计算mAP，方法是取平均值的aP。正如之前提到的，如果你没有针对排名文档的相关性评估，一种常用的技术叫做**池化**。这要求我们设置多个搜索系统（例如三个）进行测试。对于每个查询，我们收集这三个搜索系统返回的前K个文档。然后，由人工标注员评判所有3
    * K个文档的相关性程度。之后，我们将所有不在这个池中的文档视为不相关，而池中的所有文档视为相关。然后，搜索结果可以在这些池的基础上进行评估。
- en: 'At this point, even though mAP is evaluating a ranked list, the nature of the
    definition of precision still neglects some of the nature of a search task: precision
    is evaluated based on binary labels, either relevant or irrelevant. It does not
    reflect the *relatedness* of the query against documents. **Normalized Discounted
    Cumulative Gain** (**nDCG**) is used for evaluating search system performance
    over the degree of relatedness.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，尽管mAP在评估一个排序列表，但精度的定义性质仍然忽略了搜索任务的一些本质：精度是基于二元标签进行评估的，要么是相关，要么是无关。它并没有反映查询与文档之间的*相关性*。**归一化折扣累积增益**（**nDCG**）则用于评估搜索系统在相关性程度上的表现。
- en: nDCG can have multiple levels of rating for each document, such as *irrelevant*,
    *relevant*, or *highly relevant*. In this case, mAP does not work anymore.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: nDCG可以对每个文档进行多个等级的评分，例如*不相关*、*相关*或*高度相关*。在这种情况下，mAP不再适用。
- en: 'For instance, given three degrees of relevance (irrelevant, relevant, and highly
    relevant), these differences in degrees of relevance can be represented as information
    gain that users can obtain by getting each document. For highly relevant documents,
    the gain could be assigned a value of *3*, relevant could be *1*, and not relevant
    could be set to *0*. Then, if a highly relevant document is ranked higher than
    documents that are not relevant, the user could cumulate more *gain*, which is
    referred to as **Cumulative Gain** (**CG**). The following table shows the information
    gain we get per document given the top 10 ranked documents produced by a search
    system:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定三种相关性程度（不相关、相关和高度相关），这些相关性差异可以表示为用户通过获取每个文档所能获得的信息增益。对于高度相关的文档，可以赋予增益值*3*，相关的文档可以赋予增益值*1*，不相关的文档增益值设为*0*。然后，如果一个高度相关的文档排名高于不相关的文档，用户可以获得更多的*增益*，这被称为**累积增益**（**CG**）。下表展示了基于搜索系统返回的前10个排名文档，我们得到的信息增益：
- en: '| **Document** | **Label** | **Gain** |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **文档** | **标签** | **增益** |'
- en: '| Doc1 | Highly Relevant | 3 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Doc1 | 高度相关 | 3 |'
- en: '| Doc2 | Relevant | 1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Doc2 | 相关 | 1 |'
- en: '| Doc3 | Irrelevant | 0 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Doc3 | 不相关 | 0 |'
- en: '| Doc4 | Irrelevant | 0 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Doc4 | 不相关 | 0 |'
- en: '| Doc5 | Relevant | 1 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Doc5 | 相关 | 1 |'
- en: '| Doc6 | Irrelevant | 0 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Doc6 | 不相关 | 0 |'
- en: '| Doc7 | Irrelevant | 0 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Doc7 | 不相关 | 0 |'
- en: '| Doc8 | Highly Relevant | 3 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Doc8 | 高度相关 | 3 |'
- en: '| Doc9 | Irrelevant | 0 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Doc9 | 不相关 | 0 |'
- en: '| Doc10 | Irrelevant | 0 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Doc10 | 不相关 | 0 |'
- en: Table 3.2 – Top 10 documents with information gain
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 – 带有信息增益的前10个文档
- en: 'In the preceding table, the system returned the top 10 ranked documents to
    the user. Based on the relevance degree, we assign 3 as the gain to highly relevant
    documents, 1 as the gain to relevant documents, and 0 as the gain to irrelevant
    documents. The CG is the sum of all gains in the top 10 documents, such as the
    following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，系统返回了前10个排名文档给用户。根据相关性程度，我们将3作为高度相关文档的增益，1作为相关文档的增益，0作为不相关文档的增益。CG是前10个文档的所有增益之和，如下所示：
- en: '`CG@10 = 3 + 1 + 1 + 3 = 8`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`CG@10 = 3 + 1 + 1 + 3 = 8`'
- en: 'But think about the nature of a search engine: the users scan the ranked list
    from top to bottom. So, by nature, the top-ranked documents should have more gains
    than the documents ranked lower so that our search system will try to rank highly
    relevant documents in higher positions. So, in practice, we will penalize the
    gain by the position. See the following example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 但考虑一下搜索引擎的本质：用户从上到下浏览排序列表。所以，从本质上讲，排名靠前的文档应该比排名靠后的文档有更多的增益，这样我们的搜索系统才会尽量把高度相关的文档排在更高的位置。因此，在实践中，我们会根据文档的位置来惩罚增益。请看下面的示例：
- en: '| **Document** | **Label** | **Gain** | **Discounted Gain** |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **文档** | **标签** | **增益** | **折扣增益** |'
- en: '| Doc1 | Highly Relevant | 3 | 3 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Doc1 | 高度相关 | 3 | 3 |'
- en: '| Doc2 | Relevant | 1 | 1/log2 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Doc2 | 相关 | 1 | 1/log2 |'
- en: '| Doc3 | Irrelevant | 0 | 0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Doc3 | 无关 | 0 | 0 |'
- en: '| Doc4 | Irrelevant | 0 | 0 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Doc4 | 无关 | 0 | 0 |'
- en: '| Doc5 | Relevant | 1 | 1/log5 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Doc5 | 相关 | 1 | 1/log5 |'
- en: '| Doc6 | Irrelevant | 0 | 0 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Doc6 | 无关 | 0 | 0 |'
- en: '| Doc7 | Irrelevant | 0 | 0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Doc7 | 无关 | 0 | 0 |'
- en: '| Doc8 | Highly Relevant | 3 | 3/log8 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Doc8 | 高度相关 | 3 | 3/log8 |'
- en: '| Doc9 | Irrelevant | 0 | 0 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Doc9 | 无关 | 0 | 0 |'
- en: '| Doc10 | Irrelevant | 0 | 0 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Doc10 | 无关 | 0 | 0 |'
- en: Table 3.3 – Top 10 documents of gain and discounted gain
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3 – 前10名文档的增益和折扣增益
- en: 'In the preceding table, given the gain of a document and its ranked position,
    we penalize the gain a little bit by dividing the gain by a factor. In this case,
    it’s the logarithm of the ranked position. The sum of the gain is called the **Discounted
    Cumulative Gain** (**DCG**):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，根据文档的增益和其排名位置，我们通过将增益除以一个因子来稍微惩罚增益。在这种情况下，它是排名位置的对数。增益的总和称为**折扣累积增益**（**DCG**）：
- en: '`DCG@10 = 3 + 1/log2 + 1/log5 + 3/log8 = 6.51`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`DCG@10 = 3 + 1/log2 + 1/log5 + 3/log8 = 6.51`'
- en: 'Before we start to compute nDCG, it’s important to understand the concept of
    ideal DCG. It simply means the best-ranking result we could achieve. In the preceding
    case, if we look at the top 10 positions, ideally the ranked list should contain
    all highly relevant documents with a gain of 3\. So, the iDCG should be as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始计算nDCG之前，理解理想DCG的概念非常重要。它简单来说就是我们能够实现的最佳排名结果。在前面的例子中，如果我们看前10个位置，理想情况下，排名列表应该包含所有高度相关的文档，并且增益为3。因此，iDCG应如下所示：
- en: '`iDCG@10 = 3 + 3/log2 + 3/log3 + 3/log4 + 3/log5 + 3/log6 + 3/log7 + 3/log8
    + 3/log9 + 3/log10 = 21.41`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`iDCG@10 = 3 + 3/log2 + 3/log3 + 3/log4 + 3/log5 + 3/log6 + 3/log7 + 3/log8
    + 3/log9 + 3/log10 = 21.41`'
- en: 'In the end, the final nDCG is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，最终的nDCG如下所示：
- en: '`nDCG = DCG/iDCG`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`nDCG = DCG/iDCG`'
- en: 'In our preceding example, we have the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们前面的例子中，得到了以下结果：
- en: '`nDCG@10 = 6.51/21.41 = 0.304`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`nDCG@10 = 6.51/21.41 = 0.304`'
- en: It is worth mentioning that even though nDCG is well suited to evaluating a
    search system that reflects the degree of relevance, the *relatedness* itself
    is biased toward different factors, such as search context and user preference.
    It is non-trivial to perform such an evaluation in a real-world scenario. In the
    next chapter, we will dive into the details of such challenges and briefly introduce
    how to resolve them.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，尽管nDCG非常适合评估一个能够反映相关度的搜索系统，*相关性*本身却会受到诸如搜索上下文和用户偏好等不同因素的影响。在现实世界的场景中进行这种评估并非易事。在接下来的章节中，我们将深入探讨这些挑战的细节，并简要介绍如何解决它们。
- en: Engineering challenges of building a neural search system
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经搜索系统的工程挑战
- en: Now, you would have noticed that the most important building blocks of the neural
    search system are the encoder and indexer. The quality of encoding posts has a
    direct impact on the final search result, while the speed of the indexer determines
    the scalability of your neural search system.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该注意到，神经搜索系统的最重要构建模块是编码器和索引器。编码帖子质量直接影响最终的搜索结果，而索引器的速度决定了神经搜索系统的可扩展性。
- en: 'Meanwhile, this is still not enough to make your neural search system ready
    to use. Many other topics need to be taken into consideration as well. The first
    question is: does your encoder (neural model) have the same distribution as your
    data? For new people coming into the neural search system world who are using
    a pretrained deep neural network, such as ResNet trained on ImageNet, it is trivial
    to quickly set up a search system. However, if your target is to build a neural
    search system on a specific domain, let’s say a fashion product image search,
    it is not going to produce satisfying results.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，这仍然不足以让您的神经搜索系统准备好投入使用。许多其他问题也需要考虑。第一个问题是：您的编码器（神经模型）是否与您的数据分布一致？对于刚接触神经搜索系统的新人，使用例如在ImageNet上训练的ResNet等预训练深度神经网络，快速搭建一个搜索系统是非常简单的。然而，如果您的目标是基于特定领域构建神经搜索系统，例如时尚产品图片搜索，那么它的效果可能不会令人满意。
- en: One important topic before we really start creating an encoder and setting up
    our search system is applying transfer learning to your dataset and evaluating
    the match results. This means taking a pretrained deep learning model, such as
    ResNet, chopping off the head layer, freezing the weights of the pretrained model,
    and attaching a new embedding layer to the end of the model, then training it
    on your new dataset on your domain. This could greatly boost search performance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们真正开始创建编码器并设置搜索系统之前，有一个重要的主题是将迁移学习应用于您的数据集并评估匹配结果。这意味着采用预训练的深度学习模型，例如ResNet，去掉其头部层，冻结预训练模型的权重，并在模型末尾附加一个新的嵌入层，然后在您的领域中的新数据集上进行训练。这可能会极大地提升搜索性能。
- en: Apart from that, in some vision-based search systems, purely relying on the
    encoder might not be sufficient. For instance, a lot of vision-based search systems
    rely heavily on an object detector. Before sending the full image into the encoder,
    it should be sent to the object detector first and the meaningful part of the
    image extracted (and remove the background noise). This is likely to improve the
    embedding quality. Meanwhile, some vision-based classification models could also
    be employed to enrich the search context as a hard filter. For instance, if you
    are building a neural search system that allows people to search similar automobiles
    given an image as a query, a pretrained brand classifier could be useful. To be
    more concrete, you pretrain an automobile brand classifier to *recognize* different
    car brands based on images and apply the recognition to the indexing and searching
    pipeline. Once a vision-based search is complete, you can apply the recognized
    brand as a hard filter to filter out cars of other brands.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，在一些基于视觉的搜索系统中，仅依赖编码器可能不足够。例如，许多基于视觉的搜索系统严重依赖物体检测器。在将完整图像发送到编码器之前，应先将其发送到物体检测器，提取图像的有意义部分（并去除背景噪声）。这可能会提高嵌入质量。同时，一些基于视觉的分类模型也可以用作硬过滤器来丰富搜索上下文。例如，如果您正在构建一个神经搜索系统，允许人们在以图像作为查询的情况下搜索相似的汽车，那么预训练的品牌分类器可能会很有用。更具体地说，您可以预训练一种汽车品牌分类器来*识别*不同的汽车品牌基于图像，并将识别应用于索引和搜索管道。一旦基于视觉的搜索完成，您可以将识别出的品牌作为硬过滤器来筛选出其他品牌的汽车。
- en: Similarly, for text-based search, when a user provides keywords as a query,
    directly applying an embedding-based similarity search might be insufficient.
    For example, you can create a **Named Entity Recognition** (**NER**) module in
    your indexing and querying pipeline to enrich the metadata.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于基于文本的搜索，当用户提供关键词作为查询时，直接应用基于嵌入的相似性搜索可能不足够。例如，您可以在索引和查询管道中创建一个**命名实体识别**（**NER**）模块来丰富元数据。
- en: For web-based search engines such as Google, Bing, or Baidu, it is very common
    to see query automatic completion. It might be also very interesting to add a
    deep neural network-powered keyword extraction component to your indexing and
    searching pipeline to use a similar user experience.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像谷歌、必应或百度这样的基于网络的搜索引擎，自动完成查询非常普遍。在您的索引和搜索管道中添加一个由深度神经网络驱动的关键词提取组件，以提供类似的用户体验也许会非常有趣。
- en: To summarize, to build a production-ready neural search system, it is very challenging
    to design a feature-complete indexing and querying pipeline, given the fact that
    search is such a complex task. Designing such a system is already challenging,
    let alone engineering the infrastructure. Luckily, Jina can already help you with
    most of the most challenging tasks.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，要构建一个可用于生产的神经搜索系统，设计一个功能完备的索引和查询管道是非常具有挑战性的，考虑到搜索是如此复杂的任务。设计这样的系统本身就很具有挑战性，更不用说工程基础设施了。幸运的是，Jina已经可以帮助您解决大部分最具挑战性的任务。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed the fundamental tasks to build a neural search
    system, which are the indexing and querying pipelines. We looked into both of
    them and introduced the most challenging part, such as encoding and indexing.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了构建神经搜索系统的基本任务，即索引和查询管道。我们深入研究了它们，并介绍了编码和索引等最具挑战性的部分。
- en: You should have basic knowledge of the basic building blocks of indexing and
    querying, such as preprocessing, encoding, and indexing. You should also notice
    that the quality of the search results highly depends on the encoder, while the
    scalability of the neural search system highly depends on the indexer and the
    most popular algorithms behind the indexer.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该具备索引和查询的基础构建模块的基本知识，比如预处理、编码和索引。你还应该注意到，搜索结果的质量很大程度上取决于编码器，而神经搜索系统的可扩展性则很大程度上取决于索引器和索引器背后的最流行算法。
- en: As you need to build a production-ready search system, you will realize that
    purely relying on the basic building blocks is not enough. As a search system
    is complex to implement, it is always needed to design and add your own building
    blocks to the indexing and querying pipeline, in order to bring better search
    results.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你需要构建一个可投入生产的搜索系统，你会意识到单纯依赖基础构建模块是不够的。因为搜索系统的实现非常复杂，总是需要设计并添加你自己的构建模块到索引和查询管道中，以提供更好的搜索结果。
- en: In the next chapter, we will start to introduce Jina, the most popular framework
    that helps you engineer a neural search system. You will realize that Jina has
    tackled the most difficult problems for you and could make your life as a neural
    search system engineer/scientist much easier.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始介绍Jina，这个帮助你构建神经搜索系统的最流行框架。你会发现，Jina已经为你解决了最困难的问题，它能让你作为神经搜索系统工程师/科学家的工作变得更加轻松。
- en: 'Part 2: Introduction to Jina Fundamentals'
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：Jina基础介绍
- en: 'In this part, you will learn about what Jina is and its basic components. You
    will understand its architecture and how can it be used to develop deep-learning
    searches on the cloud. The following chapters are included in this part:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将了解Jina是什么以及它的基本组件。你将理解它的架构，以及它如何被用来在云端开发深度学习搜索。接下来的章节包括以下内容：
- en: '[*Chapter 4*](B17488_04.xhtml#_idTextAnchor054), *Learning Jina''s Basics*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第四章*](B17488_04.xhtml#_idTextAnchor054)，*学习Jina的基础*'
- en: '[*Chapter 5*](B17488_05.xhtml#_idTextAnchor068), *Multiple Search Modalities*'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第五章*](B17488_05.xhtml#_idTextAnchor068)，*多模态搜索*'
