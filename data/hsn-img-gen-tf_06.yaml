- en: '*Chapter 4*: Image-to-Image Translation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：图像到图像的翻译'
- en: In part one of the book, we learned to generate photorealistic images with VAE
    and GANs. The generative models can turn some simple random noise into high-dimensional
    images with complex distribution! However, the generation processes are unconditional,
    and we have fine control over the images to be generated. If we use MNIST as an
    example, we will not know which digit will be generated; it is a bit of a lottery.
    Wouldn't it be nice to be able to tell GAN what we want it to generate? This is
    what we will learn in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一部分，我们学习了如何使用VAE和GAN生成逼真的图像。生成模型能够将一些简单的随机噪声转化为具有复杂分布的高维图像！然而，生成过程是无条件的，我们对生成的图像几乎没有控制。如果我们使用MNIST作为例子，我们无法知道会生成哪个数字，这有点像彩票。是不是很希望能够告诉GAN我们想要它生成什么呢？这就是我们在本章中要学习的内容。
- en: 'We will first learn to build a **conditional GAN** (**cGAN**) that allows us
    to specify the class of images to generate. This lays the foundation for more
    complex networks that follow. We will learn to build a GAN known as **pix2pix**
    to perform **image-to-image translation**, or **image translation** for short.
    This will enable a lot of cool applications such as converting sketches to real
    images. After that, we will build **CycleGAN**, an upgrade from pix2pix that could
    turn a horse into a zebra and then back to a horse! Finally, we will build **BicyleGAN**,
    to translate not only high quality but also diversified images with different
    styles. The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先学习构建一个**条件生成对抗网络**（**cGAN**），它允许我们指定要生成的图像类别。这为后续更复杂的网络奠定了基础。我们将学习构建一个名为**pix2pix**的生成对抗网络，用于执行**图像到图像的转换**，简称**图像翻译**。这将使得许多酷炫的应用成为可能，比如将草图转换成真实的图像。之后，我们将构建**CycleGAN**，它是pix2pix的升级版，能够将马转变为斑马，然后再变回马！最后，我们将构建**BicycleGAN**，不仅能够翻译出高质量的图像，还能生成具有不同风格的多样化图像。本章将涵盖以下内容：
- en: Conditional GANs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件生成对抗网络（Conditional GANs）
- en: Image translation with pix2pix
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pix2pix进行图像翻译
- en: Unpaired image translation with CycleGAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CycleGAN进行无配对图像翻译
- en: Diversifying translation with BicycleGAN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BicycleGAN进行多样化翻译
- en: 'The following topics will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: In this chapter, we will reuse code and network blocks from [*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060),
    *Generative Adversarial Network*, such as upsampling and downsampling blocks of
    DCGAN. This will allow us to focus on higher-level architectures of new GANS and
    to cover more GANs in this chapter. The latter three GANs were created in chronological
    order and share many common blocks. Thus, you should read them in order, beginning
    with pix2pix, followed by CycleGAN, and finishing with BicycleGAN, which will
    make a lot more sense than jumping to BicycleGAN, which is the most complex model
    in this book so far.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重用[*第三章*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)《生成对抗网络》中的代码和网络模块，如DCGAN的上采样和下采样模块。这将使我们能够专注于新GAN的更高层次架构，并在本章中介绍更多的GAN。后面提到的三个GAN是按时间顺序创建的，并共享许多共同的模块。因此，你应该按顺序阅读它们，首先是pix2pix，然后是CycleGAN，最后是BicycleGAN，这样会比直接跳到BicycleGAN（这是本书中最复杂的模型）更容易理解。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The Jupyter notebooks can be found at the following link:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的Jupyter笔记本可以在以下链接找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter04).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter04)'
- en: 'The notebooks used in this chapter are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的笔记本如下：
- en: '`ch4_cdcgan_mnist.ipynb`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch4_cdcgan_mnist.ipynb`'
- en: '`ch4_cdcgan_fashion_mnist.ipynb`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch4_cdcgan_fashion_mnist.ipynb`'
- en: '`ch4_pix2pix.ipynb`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch4_pix2pix.ipynb`'
- en: '`ch4_cyclegan_facade.ipynb`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch4_cyclegan_facade.ipynb`'
- en: '`ch4_cyclegan_horse2zebra.ipynb`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch4_cyclegan_horse2zebra.ipynb`'
- en: '`ch4_bicycle_gan.ipynb`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch4_bicycle_gan.ipynb`'
- en: Conditional GANs
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件生成对抗网络（Conditional GANs）
- en: The first goal of a generative model is to be able to produce good quality images.
    Then we would like to be able to have some control over the images that are to
    be generated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的第一个目标是能够生成高质量的图像。接着，我们希望能够对生成的图像进行一定的控制。
- en: In [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017), *Getting Started
    with Image Generation Using TensorFlow*, we learned about conditional probability
    and generated faces with certain attributes using a simple conditional probabilistic
    model. In that model, we generated a smiling face by forcing the model to only
    sample from the images that had a smiling face. When we condition on something,
    that thing will always be present and will no longer be a variable with random
    probability. You can also see that the probability of having those conditions
    is set to *1*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 1 章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017) *使用 TensorFlow 开始进行图像生成*
    中，我们了解了条件概率，并使用简单的条件概率模型生成了具有特定属性的面孔。在那个模型中，我们通过强制模型仅从具有微笑面孔的图像中采样来生成微笑的面孔。当我们对某些事物进行条件化时，该事物将始终存在，并且不再是具有随机概率的变量。你也可以看到，这些条件的概率被设置为
    *1*。
- en: 'To enforce the condition on a neural network is simple. We simply need to show
    the labels to the network during training and inference. For example, if we want
    the generator to generate the digit 1, we will need to present the label of 1
    in addition to the usual random noise as input to the generator. There are several
    ways of implementing it. The following diagram shows one implementation as it
    appeared in the *Conditional Generative Adversarial Nets* paper that first introduced
    the idea of cGAN:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络上强制条件很简单。我们只需要在训练和推理过程中向网络展示标签。例如，如果我们希望生成器生成数字 1，我们需要将数字 1 的标签与常规的随机噪声一起输入到生成器中。实现这一点有几种方法。下图展示了一种实现方式，最初出现在
    *条件生成对抗网络* 论文中，该论文首次提出了 cGAN 的概念：
- en: '![Figure 4.1 – Condition by concatenating labels and inputs'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – 通过连接标签和输入来实现条件'
- en: '(Redrawn from: M. Mirza, S. Osindero, 2014, Conditional Generative Adversarial
    Nets – https://arxiv.org/abs/1411.1784)](img/B14538_04_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: （改绘自：M. Mirza, S. Osindero, 2014, 条件生成对抗网络 – https://arxiv.org/abs/1411.1784)](img/B14538_04_01.jpg)
- en: 'Figure 4.1 – Condition by concatenating labels and inputs (Redrawn from: M.
    Mirza, S. Osindero, 2014, Conditional Generative Adversarial Nets – https://arxiv.org/abs/1411.1784)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 通过连接标签和输入来实现条件（改绘自：M. Mirza, S. Osindero, 2014, 条件生成对抗网络 – https://arxiv.org/abs/1411.1784）
- en: 'In unconditional GAN, the generator input is only the latent vector *z*. In
    conditional GAN, the latent vector *z* joins with a one-hot encoded input label
    *y* to form a longer vector, as shown in the preceding diagram. The following
    table shows one-hot encoding using `tf.one_hot()`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在无条件 GAN 中，生成器的输入只有潜在向量 *z*。在条件 GAN 中，潜在向量 *z* 与一热编码的输入标签 *y* 结合，形成一个更长的向量，如前面的图所示。下表显示了使用
    `tf.one_hot()` 的一热编码：
- en: '![Figure 4.2 – Table showing one-hot encoding for classes of 10 in TensorFlow'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 显示 TensorFlow 中 10 类的一热编码的表格'
- en: '](img/B14538_04_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_02.jpg)'
- en: Figure 4.2 – Table showing one-hot encoding for classes of 10 in TensorFlow
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 显示 TensorFlow 中 10 类的一热编码的表格
- en: One-hot encoding converts a label into a vector with dimensions equal to the
    number of classes. The vectors have all zeros, apart from one unique position
    that is filled with 1\. Some machine learning frameworks use a different order
    of 1 in the vector; for example, class label `0` is encoded as `0000000001` where
    the `1` is in the right-most position. The order doesn't matter as long as they
    are consistently used in both training and inference. This is because one-hot
    encoding is only used to represent categorical classes and does not have semantic
    meaning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码将标签转换为一个与类别数相等的向量。除了一个唯一的位置被填充为 1 之外，向量中的其他位置都为 0。一些机器学习框架在向量中的 1 的顺序不同；例如，类别标签
    `0` 被编码为 `0000000001`，其中 `1` 位于最右侧的位置。顺序并不重要，只要在训练和推理过程中始终如一地使用。这是因为一热编码仅用于表示类别类，并没有语义意义。
- en: Implementing a conditional DCGAN
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现一个条件 DCGAN
- en: Now, let's implement a conditional DCGAN on MNIST. We have implemented a DCGAN
    in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039), *Variational
    Autoencoder*, and therefore we extend the network by adding the conditional bits.
    The notebook for this exercise is `ch4_cdcgan_mnist.ipynb`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 MNIST 上实现一个条件 DCGAN。我们在 [*第 2 章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)
    *变分自编码器* 中实现了一个 DCGAN，因此我们通过添加条件部分来扩展网络。本练习的笔记本文件是 `ch4_cdcgan_mnist.ipynb`。
- en: 'Let''s first look at the generator:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们来看一下生成器：
- en: 'The first step is to one-hot encode the class label. As `tf.one_hot`([1], 10)
    will create a shape of (1, 10), we''ll need to reshape it to a 1D vector of (10)
    so that we can concatenate with the latent vector `z`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是对类别标签进行独热编码。由于`tf.one_hot`([1], 10)将生成形状为（1, 10）的张量，我们需要将其重塑为形状为（10）的 1D
    向量，以便与潜在向量`z`进行连接：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next step is to join the vectors together by using the `Concatenate` layer.
    By default, concatenation happens across the last dimension (axis=-1). Therefore,
    concatenating latent variables with a shape of (`batch_size`, 100) with one-hot
    labels of (`batch_size`, 10) will produce a tensor shape of (`batch_size`, 110).
    The code is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是通过使用`Concatenate`层将向量连接在一起。默认情况下，连接发生在最后一个维度（axis=-1）。因此，将形状为（`batch_size`,
    100）的潜在变量与形状为（`batch_size`, 10）的独热标签连接，会生成形状为（`batch_size`, 110）的张量。代码如下：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That is the only change required for the generator. As we have already covered
    the details of DCGAN architecture, I won''t be repeating them in here. For a quick
    recap, the input will go through a dense layer, followed by several upsampling
    and convolutional layers to generate an image with a shape of (32, 32, 3), as
    shown in the following model diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成器所需的唯一更改。由于我们已经讨论过 DCGAN 架构的细节，本文不再重复。在此简要回顾一下，输入将通过一个全连接层，然后经过若干上采样和卷积层，生成形状为（32,
    32, 3）的图像，如下所示的模型图：
- en: '![Figure 4.3 – Generator model diagram of a conditional DCGAN'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – 条件 DCGAN 的生成器模型图'
- en: '](img/B14538_04_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_03.jpg)'
- en: Figure 4.3 – Generator model diagram of a conditional DCGAN
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 条件 DCGAN 的生成器模型图
- en: The next step is to inject the label into the discriminator as it is not enough
    that the discriminator is able to tell whether the image is real or fake, but
    also to tell whether it is the correct image.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将标签注入判别器中，因为仅仅判别图像是否真实或虚假是不够的，还需要判断它是否是正确的图像。
- en: The original cGAN uses only dense layers in the network. The input image is
    flattened and concatenates with a one-hot encoded class label. However, this doesn't
    work well with DCGAN as the first layer of the discriminator is a convolutional
    layer that is expecting a 2D image as input. If we use the same approach, we will
    end up with an input vector of 32×32×1 + 10 = 1,034, and that can't be reshaped
    to a 2D image. We will need another way to project the one-hot vector into a tensor
    of the correct shape.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 cGAN 仅在网络中使用全连接层。输入图像被展开并与独热编码的类别标签连接。然而，这种方法对于 DCGAN 并不适用，因为判别器的第一层是卷积层，期待一个二维图像作为输入。如果我们使用相同的方法，最终会得到一个输入向量
    32×32×1 + 10 = 1,034，这不能被重塑为二维图像。我们需要另一种方式将独热向量投影到正确形状的张量中。
- en: 'One way to do this is to use a dense layer to project the one-hot vector into
    the shape of an input image (32,32,1), and concatenate it to produce a shape of
    (32, 32, 2). The first color channel will be our grayscale image, and the second
    channel will be the projected one-hot labels. Again, the rest of the discriminator
    network is unchanged, as shown in the following model summary:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实现方式是使用全连接层将独热向量投影到输入图像（32,32,1）的形状，并进行连接，生成（32, 32, 2）的形状。第一个颜色通道是我们的灰度图像，第二个通道是投影后的独热标签。同样，判别器网络的其余部分保持不变，如下所示的模型摘要：
- en: '![Figure 4.4 – Inputs to discriminator of a conditional DCGAN'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – 条件 DCGAN 判别器的输入'
- en: '](img/B14538_04_04.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_04.jpg)'
- en: Figure 4.4 – Inputs to discriminator of a conditional DCGAN
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 条件 DCGAN 判别器的输入
- en: 'As we have seen, the only change made to networks is by adding another path
    that takes class labels as input. The last remaining bit to do before starting
    the model training is to add the additional label class into the model''s input.
    To create a model with multiple inputs, we pass a list of input layers as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，对网络所做的唯一更改是增加了另一个路径，该路径接受类别标签作为输入。在开始模型训练之前，最后剩下的工作是将附加的标签类别添加到模型的输入中。为了创建具有多个输入的模型，我们按照以下方式传递输入层列表：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly, we pass a list of `images` and `labels` in the same order when performing
    a forward pass:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们在执行前向传播时，以相同顺序传递`images`和`labels`列表：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'During training, we create random labels for the generator as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们为生成器创建随机标签，如下所示：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We use a DCGAN training pipeline and loss function. Here are samples of the
    digits generated by conditioning on the input labels from 0 to 9:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 DCGAN 训练管道和损失函数。以下是通过对输入标签（从 0 到 9）进行条件化生成的数字样本：
- en: '![Figure 4.5 – Hand-written digits generated by a conditional DCGAN'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – 由条件 DCGAN 生成的手写数字'
- en: '](img/B14538_04_05.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_05.jpg)'
- en: Figure 4.5 – Hand-written digits generated by a conditional DCGAN
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 由条件 DCGAN 生成的手写数字
- en: 'We can also train cDCGAN on Fashion-MNIST without any change. The result samples
    are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在不做任何修改的情况下，使用 Fashion-MNIST 训练 cDCGAN。结果样本如下：
- en: '![Figure 4.6 – Image generated by a conditional DCGAN'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – 由条件 DCGAN 生成的图像'
- en: '](img/B14538_04_06.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_06.jpg)'
- en: Figure 4.6 – Image generated by a conditional DCGAN
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 由条件 DCGAN 生成的图像
- en: Conditional GAN works really well on MNIST and Fashion-MNIST! Next, we will
    look at different ways of applying the class conditions on GANs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 GAN 在 MNIST 和 Fashion-MNIST 上表现非常好！接下来，我们将探讨不同的将类别条件应用于 GAN 的方法。
- en: Variants of cGAN
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: cGAN 的变体
- en: We implemented conditional DCGAN by one-hot encoding the labels, passing it
    through a dense layer (for discriminator), and concatenating the input layer.
    The implementation is simple and gives good results. We will introduce a few other
    popular methods of implementing conditional GANs and you are encouraged to implement
    the code on your own to try them out.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一热编码标签，传递给密集层（用于判别器），并与输入层拼接，来实现条件 DCGAN。这个实现简单且效果良好。我们将介绍一些其他流行的条件 GAN
    实现方法，建议你自己动手实现代码并试一试。
- en: Using the embedding layer
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用嵌入层
- en: 'One popular implementation is to replace one-hot encoding and the dense layer
    with the `embedding` layer. The embedding layer takes categorical values as input,
    and the output is a vector, such as a dense layer. In other words, it has the
    same input and output shapes as the `label->one-hot-encoding->dense` block. The
    code snippet is shown here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的实现方法是用 `embedding` 层替代一热编码和密集层。嵌入层接受类别值作为输入，输出是一个向量，类似于密集层。换句话说，它具有与 `label->one-hot-encoding->dense`
    块相同的输入和输出形状。代码片段如下：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Both methods produce similar results, albeit the `embedding` layer is more computationally
    efficient as the size of the one-hot vector can grow quickly for large numbers
    of classes. Embedding is used extensively to encode words due to a large number
    of vocabularies. For small classes such as MNIST, the computational advantage
    is negligible.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法生成的结果相似，尽管由于一热编码的向量维度可能会随着类别数的增加而迅速增大，`embedding` 层在计算效率上更为高效。由于词汇量庞大，嵌入层在编码词语时被广泛应用。对于像
    MNIST 这样的类别较少的情况，计算上的优势几乎可以忽略不计。
- en: Element-wise multiplication
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元素级乘法
- en: 'Concatenating a latent vector with an input image increases the dimensions
    and the first layer of the network. Instead of concatenating, we could also perform
    element-wise multiplication of the label embedding with the original network input
    and keep the original input shape. The origin of this approach is unclear. However,
    a few industry experts carried out experiments on **Natural Language Processing**
    tasks and found this method to outperform that of one-hot encoding. The code snippet
    to perform element-wise multiplication between an image and embedding is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将潜在向量与输入图像拼接会增加网络的维度以及第一层的大小。我们也可以选择将标签嵌入与原始网络输入进行元素级乘法操作，而不是拼接，从而保持原始输入形状。这种方法的起源不明确。然而，一些行业专家在**自然语言处理**任务中进行实验，发现这种方法比一热编码方法效果更好。执行图像与嵌入之间元素级乘法的代码片段如下：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Combining the preceding code with the embedding layer gives us the following
    graph, as implemented in `ch4_cdcgan_fashion_mnist.ipynb`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将前述代码与嵌入层结合后，我们得到如下图表，代码实现见 `ch4_cdcgan_fashion_mnist.ipynb`：
- en: '![Figure 4.7 – Implementation of cDCGAN using embedding and element-wise multiplication'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 使用嵌入和元素级乘法实现 cDCGAN'
- en: '](img/B14538_04_07.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_07.jpg)'
- en: Figure 4.7 – Implementation of cDCGAN using embedding and element-wise multiplication
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 使用嵌入和元素级乘法实现 cDCGAN
- en: Next, we will see why inserting labels into the intermediate layer is popular.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到为什么将标签插入到中间层中是一种流行的做法。
- en: Inserting labels in the intermediate layer
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将标签插入到中间层
- en: Instead of inserting the label into the first layer of the network, we can choose
    to do this in the intermediate layer. This approach is popular for generators
    with encoder-decoder architectures, where the label is inserted into a layer that
    is close to the end of the encoder with the smallest dimensions. Some insert the
    label embedding toward the discriminator output, so the majority of the discriminator
    can focus on deciding whether the images look real. The only part of the last
    few layers' capacity is used in deciding whether the image matches the label.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择将标签插入到网络的中间层，而不是第一层。这种方法对于编码器-解码器架构的生成器非常流行，其中标签被插入到接近编码器末端且维度最小的层。一些方法将标签嵌入到接近判别器输出的地方，这样判别器的大部分可以专注于判断图像是否真实。最后几层的容量只用于判断图像是否与标签匹配。
- en: We will learn how to insert label embedding into intermediate and normalization
    layers when we implement advanced models in [*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation*. We have now understood how to use class
    labels conditioned to generate images. For the rest of the chapter, we will use
    an image as a condition to perform image-to-image translation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[*第8章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)中实现高级模型时，我们将学习如何将标签嵌入到中间层和归一化层，*自注意力机制用于图像生成*。我们现在已经理解了如何使用条件类标签生成图像。在本章的其余部分，我们将使用图像作为条件来执行图像到图像的翻译。
- en: Image translation with pix2pix
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pix2pix进行图像翻译
- en: 'The introduction of pix2pix in 2017 caused quite a stir, not only within the
    research community, but also the wider population. This can be attributed in part
    to the [https://affinelayer.com/pixsrv/](https://affinelayer.com/pixsrv/) website,
    which puts the models online and allows people to translate their sketches into
    cats, shoes, and bags. You should try it too! The following screenshot is taken
    from their website to give you a glimpse of how it works:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年pix2pix的介绍引起了极大的轰动，不仅在研究界，而且在更广泛的公众中。这部分归功于[https://affinelayer.com/pixsrv/](https://affinelayer.com/pixsrv/)网站，它将模型在线化，并允许人们将草图转换为猫、鞋子和包包。你也应该试试！以下截图取自该网站，给你一个了解其如何工作的窗口：
- en: '![Figure 4.8 – Application of turning a sketch of a cat into a real image'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 – 将猫的草图转化为真实图像的应用'
- en: '(Source: https://affinelayer.com/pixsrv/)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '（来源: https://affinelayer.com/pixsrv/）'
- en: '](img/B14538_04_08.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_08.jpg)'
- en: 'Figure 4.8 – Application of turning a sketch of a cat into a real image (Source:
    https://affinelayer.com/pixsrv/)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.8 – 将猫的草图转化为真实图像的应用（来源: https://affinelayer.com/pixsrv/）'
- en: 'Pix2pix came from a research paper entitled *Image-to-Image Translation with
    Conditional Adversarial Networks*. From the paper title, we can tell that pix2pix
    is a conditional GAN that performs image-to-image translation. The model can be
    trained to perform general image translation, but we will need to have image pairs
    in the dataset. In our pix2pix implementation, we will translate masks of building
    façades into realistic-looking building façades, as shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2pix来源于一篇名为*条件对抗网络的图像到图像翻译*的研究论文。从论文标题中我们可以看出，pix2pix是一种执行图像到图像翻译的条件GAN模型。该模型可以被训练用于执行一般的图像翻译，但我们需要数据集中包含图像对。在我们的pix2pix实现中，我们将建筑立面的掩码翻译成逼真的建筑立面，如下所示：
- en: '![Figure 4.9 – Mask and real image of a building façade'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 – 建筑立面掩码与真实图像'
- en: '](img/B14538_04_09.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_09.jpg)'
- en: Figure 4.9 – Mask and real image of a building façade
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 建筑立面掩码与真实图像
- en: In the preceding screenshot, the picture on the left shows an example of the
    semantic segmentation mask used as input of pix2pix where the building parts are
    encoded in different colors. On the right is the target real image of a building
    façade.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的截图中，左边的图片展示了作为pix2pix输入的语义分割掩码示例，其中建筑部分通过不同的颜色进行编码。右边是建筑立面的目标真实图像。
- en: Discarding random noise
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 丢弃随机噪声
- en: In all the GANs we have learned so far, we always sample from random distribution
    as input to the generator. We require that randomness, otherwise the generator
    will produce deterministic outputs and fail to learn data distribution. Pix2pix
    breaks away from that tradition by removing random noise from GANs. As the authors
    pointed out in the *Image-to-Image Translation with Conditional Adversarial Networks*
    paper, they could not get the conditional GAN to work with an image and noise
    as input as the GAN would simply ignore the noise.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止学习的所有 GAN 中，我们总是从随机分布中采样作为生成器的输入。我们需要这种随机性，否则生成器将产生确定性的输出，无法学习数据分布。Pix2pix
    打破了这个传统，通过去除 GAN 中的随机噪声。正如作者在 *《基于条件对抗网络的图像到图像翻译》* 论文中指出的那样，他们无法让条件 GAN 使用图像和噪声作为输入，因为
    GAN 会忽略噪声。
- en: As a result, the authors turned to the use of dropout in generator layers to
    provide randomness. A side effect is that this is minor randomness; hence, little
    variations are seen in the output and they tend to look similar in styles. This
    problem is overcome with BicycleGAN, which we will learn about later.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，作者开始在生成器层中使用 dropout 来引入随机性。一个副作用是，这种随机性较小，因此输出中会看到一些微小的变化，而且它们的风格通常相似。这个问题在
    BicycleGAN 中得到了解决，我们将在后续学习到它。
- en: U-Net as a generator
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-Net 作为生成器
- en: 'The notebook for this tutorial is `ch4_pix2pix.ipynb`. The architecture of
    generators and discriminators is rather different from DCGAN and we will go through
    each of them in detail. Without the use of random noise as input, all that is
    left to the generator input is the input image that is used as the condition.
    Thus, both the input and output are an image of the same shape, which is (256,
    256, 3) in our examples. Pix2pix uses U-Net, which is an encoder-decoder-like
    architecture similar to an autoencoder, but with skip connections between the
    encoder and decoder. Following is the architecture diagram of the original U-Net:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的笔记本文件是 `ch4_pix2pix.ipynb`。生成器和判别器的架构与 DCGAN 有很大不同，我们将详细讲解每一部分。由于不使用随机噪声作为输入，生成器的输入仅剩下作为条件的输入图像。因此，输入和输出都是相同形状的图像，在我们的示例中是
    (256, 256, 3)。Pix2pix 使用 U-Net，这是一种类似于自编码器的编码器-解码器架构，但在编码器和解码器之间有跳跃连接。以下是原始 U-Net
    的架构图：
- en: '![Figure 4.10 – Original U-Net architecture (Source: O. Ronneberger et al.,
    2015, “U-Net: Convolutional Networks for Biomedical Image Segmentation” – https://arxiv.org/abs/1505.04597)](img/B14538_04_10.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 原始 U-Net 架构（来源：O. Ronneberger 等，2015，《U-Net: Convolutional Networks
    for Biomedical Image Segmentation》– https://arxiv.org/abs/1505.04597)](img/B14538_04_10.jpg)'
- en: 'Figure 4.10 – Original U-Net architecture (Source: O. Ronneberger et al., 2015,
    “U-Net: Convolutional Networks for Biomedical Image Segmentation” – https://arxiv.org/abs/1505.04597)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.10 – 原始 U-Net 架构（来源：O. Ronneberger 等，2015，《U-Net: Convolutional Networks
    for Biomedical Image Segmentation》– https://arxiv.org/abs/1505.04597）'
- en: In [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Variational
    Autoencoder*, we saw how an autoencoder downsamples a high-dimension input image
    into low-dimension latent variables before upsampling it back to the original
    size. During the downsampling process, the high frequency content of images (the
    texture details) is lost. As a result, the restored image can appear to be blurry.
    By passing the high spatial resolution content from an encoder to a decoder via
    the skip connections, the decoder could capture and generate those details to
    make the images look sharper. As a matter of fact, U-Net was first used to translate
    medical images into semantic segmentation masks, which is the reverse of what
    we are trying to do in this chapter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第二章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*，变分自编码器* 中，我们看到自编码器如何将高维输入图像下采样成低维的潜在变量，然后再上采样回原始大小。在下采样过程中，图像的高频内容（即纹理细节）会丢失。因此，恢复的图像可能会显得模糊。通过通过跳跃连接将来自编码器的高空间分辨率内容传递给解码器，解码器可以捕获并生成这些细节，使图像看起来更加清晰。事实上，U-Net
    最初用于将医学图像转化为语义分割掩码，这正是我们在本章中所尝试做的事情的反向。
- en: 'To make the construction of the generator easier, we first write a function
    to create a block for downsampling with a default stride of `2`. This consists
    of convolution and optional normalization, `activation`, and `dropout` layers,
    as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化生成器的构建，我们首先编写一个函数，创建一个默认步幅为 `2` 的下采样块。它由卷积层和可选的归一化、`激活` 和 `dropout` 层组成，具体如下：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `upsample` block is similar, but with an additional `UpSampling2D` before
    `Conv2D` and has strides of `1`, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`upsample` 块类似，但在 `Conv2D` 之前添加了一个 `UpSampling2D` 层，并且步幅为 `1`，如下所示：'
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will first construct the downsampling path, where the feature map sizes
    are halved after every downsampling block as follows. It is important to note
    the output shapes as we will need to match those with the upsampling path for
    skip connections as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建下采样路径，在每个下采样块之后，特征图的尺寸会减半，如下所示。需要注意的是输出形状，因为我们需要将其与上采样路径的跳跃连接进行匹配，如下所示：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the upsampling path, we concatenate the previous layer''s output with a
    skip connection from the downsampling path to form input to the `upsample` block.
    We use `dropout` in the first three layers as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在上采样路径中，我们将前一层的输出与来自下采样路径的跳跃连接进行拼接，以形成 `upsample` 块的输入。我们在前三层使用了 `dropout`，如下所示：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This last layer of the generator is Conv2D, with a channel size of 3 to match
    the image channel numbers. Like DCGAN, we normalize the images to the range of
    [-1, +1], using `tanh` as the activation function, and binary cross-entropy as
    the loss function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的最后一层是 Conv2D，通道数为 3，以匹配图像的通道数。像 DCGAN 一样，我们将图像标准化到 [-1, +1] 范围内，使用 `tanh`
    作为激活函数，并采用二元交叉熵作为损失函数。
- en: Loss functions
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'Pix2pix uses standard GAN loss functions of binary cross-entropy for both the
    generator and discriminator, just like DCGAN. Now that we have a target image
    to generate, we can therefore add L1 reconstruction loss to the generator. In
    the paper, the ratio of the reconstruction loss to binary cross-entropy is set
    to 100:1\. The following code snippet shows how to compile combined generator-discriminator
    with losses:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2pix 使用标准的 GAN 损失函数（二元交叉熵）来训练生成器和判别器，就像 DCGAN 一样。既然我们有了目标图像可以生成，那么我们就可以为生成器添加
    L1 重建损失。在论文中，重建损失与二元交叉熵的比例设定为 100:1。以下代码片段展示了如何编译结合生成器和判别器的损失函数：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`bce` stands for `mae` stands for **mean absolute entropy loss**, or is more
    commonly known as **L1 loss**.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`bce` 代表 `mae` 代表 **平均绝对熵损失**，或更常见的称为 **L1 损失**。'
- en: Implementing a PatchGAN discriminator
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 PatchGAN 判别器
- en: Researchers found that L2 or L1 loss produces blurry results on image generation
    problems. Although they fail to encourage high-frequency crispness, they can capture
    low-frequency content well. We can see low-frequency information as content, such
    as the building structures, while high-frequency information provides the style
    information, such as the fine-detail textures and colors of building façades.
    To capture high-frequency information, a new discriminator known as PatchGAN was
    used. Don't be misled by its name; PatchGAN is not a GAN but a **Convolutional
    Neural Network** (**CNN**).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，L2 或 L1 损失会导致图像生成问题产生模糊的结果。尽管它们未能鼓励高频的清晰度，但能够很好地捕捉低频内容。我们可以将低频信息视为内容，如建筑结构，而高频信息则提供样式信息，如建筑外立面的细节纹理和颜色。为了捕捉高频信息，研究人员使用了一种新的判别器，称为
    PatchGAN。不要被它的名字误导；PatchGAN 不是 GAN，而是 **卷积神经网络** (**CNN**)。
- en: The conventional GAN discriminator looks at the entire image and judges whether
    that entire image is real or fake. Instead of looking at the entire image, PatchGAN
    looks at patches of images, hence the name. The receptive field of a convolutional
    layer is the number of input points that are mapped to one output point or, in
    other words, represents the size of the convolutional kernel. For a kernel size
    of N×N, each output of the layer is mapped to N×N pixels of the input tensor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 GAN 判别器查看整张图像并判断整张图像是实图还是假图。而 PatchGAN 并非查看整张图像，而是查看图像的若干小块，因此得名 PatchGAN。卷积层的感受野是指映射到一个输出点的输入点的数量，换句话说，就是卷积核的大小。对于一个
    N×N 的卷积核，层的每个输出都对应输入张量的 N×N 像素。
- en: As we go deeper along the network, the next layer gets to see a larger patch
    of input images and the effective receptive field of the output increases. The
    default PatchGAN is designed to have an effective field of 70×70\. The original
    PatchGAN has an output shape of 30×30 due to the careful padding, but we will
    use only the 'same' padding to give the output shape of 29×29\. Each of the 29×29
    patches looks at different and overlapping 70x70 patches of input images.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络的深入，下一层将看到更大块的输入图像，输出的有效感受野也随之增加。默认的 PatchGAN 被设计为具有 70×70 的有效感受野。原始的 PatchGAN
    由于经过精心的填充，输出形状为 30×30，但我们将仅使用“相同”填充以得到 29×29 的输出形状。每个 29×29 的小块查看不同且重叠的 70×70
    输入图像块。
- en: In other words, the discriminator tries to predict whether each of the patches
    is real or fake. By zooming into local patches, the discriminator is encouraged
    to look at high-frequency information of the images. To summarize, we use L1 reconstruction
    loss to capture the low-frequency content, and PatchGAN to encourage high-frequency-style
    details.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，判别器试图预测每个小块是实时的还是假的。通过放大局部小块，判别器被鼓励去关注图像的高频信息。总结一下，我们使用 L1 重建损失来捕捉低频内容，使用
    PatchGAN 来鼓励高频样式细节。
- en: 'PatchGAN is simply a CNN and can be implemented using several downsampling
    blocks as shown in the following code. We will use the notation A to refer to
    the input (source) image, and B for the output (target) image. Like cGAN, the
    discriminator requires two inputs – the condition, which is image A, and the output
    image B, which can be a real one from the dataset or a fake one from the generator.
    We concatenate the two images together at the beginning of the discriminator,
    hence, PatchGAN looks at both image A (condition) and image B (output image or
    fake image) together to decide whether it is real or fake. The code is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: PatchGAN 本质上是一个卷积神经网络（CNN），可以通过几个下采样块来实现，如以下代码所示。我们将使用符号 A 来表示输入（源）图像，B 来表示输出（目标）图像。与cGAN类似，判别器需要两个输入——条件图像
    A 和输出图像 B，后者可以是真实的（来自数据集的）图像，也可以是生成的假图像。我们在判别器的输入端将这两张图像拼接在一起，因此 PatchGAN 会同时查看图像
    A（条件图像）和图像 B（输出图像或假图像），以决定它们是真实的还是假的。代码如下：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The discriminator model summary is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器模型的总结如下：
- en: '![Figure 4.11 – Discriminator model summary'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11 – 判别器模型总结'
- en: '](img/B14538_04_11.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_11.jpg)'
- en: Figure 4.11 – Discriminator model summary
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 判别器模型总结
- en: 'Note that the output layer has the shape of (*29, 29, 1)*. Therefore, we will
    create labels that match its output shape as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出层的形状是 (*29, 29, 1)*。因此，我们将创建与其输出形状匹配的标签，如下所示：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we are ready to train pix2pix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好训练pix2pix了。
- en: Training pix2pix
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 pix2pix
- en: It was well known as a result of the invention of pix2pix that batch normalization
    is bad for image generation as the statistics from the batch of images tend to
    make the generated images look more similar and blurrier. The pix2pix authors
    noticed that generated images look better when the batch size is set to `1`. When
    the batch size is `1`, batch normalization becomes a special case of **instance
    normalization**, but the latter can be applied for any batch size. To recap on
    normalization, for an image batch with a shape of (N, H, W, C), batch normalization
    uses statistics across (N, H, W), while instance normalization uses statistics
    from individual images across dimensions (H,W). This prevents the statistics from
    other images from creeping in.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 pix2pix 的发明，大家都知道批量归一化对于图像生成是不利的，因为批量图像的统计信息往往会使得生成的图像看起来更加相似且模糊。pix2pix
    的作者注意到，当批量大小设置为 `1` 时，生成的图像看起来更好。当批量大小为 `1` 时，批量归一化变成了 **实例归一化** 的一种特殊情况，但后者适用于任何批量大小。回顾归一化，对于形状为
    (N, H, W, C) 的图像批次，批量归一化使用的是 (N, H, W) 的统计信息，而实例归一化使用的是单个图像在 (H, W) 维度上的统计信息。这可以防止其他图像的统计信息干扰进来。
- en: 'Therefore, to get good results, we can either use batch normalization with
    a batch size of `1`, or we replace it with instance normalization. Instance normalization
    is not available as a standard Keras layer at the time of writing, perhaps this
    hasn''t gained mainstream usage beyond image generation. However, instance normalization
    is available from the `tensorflow_addons` module. After importing from the module,
    it is a drop-in replacement for batch normalization:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了获得良好的结果，我们可以选择使用批量大小为 `1` 的批量归一化，或者用实例归一化来替代。写本文时，实例归一化并未作为标准的 Keras 层提供，可能是因为它在图像生成之外尚未得到广泛应用。不过，实例归一化可以从
    `tensorflow_addons` 模块中找到。在从该模块导入后，它可以直接替代批量归一化：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We train pix2pix using a DCGAN pipeline and it is surprisingly easy to train
    compared with DCGAN. This is because the probability distribution to cover an
    input image is narrower than the one from random noise. The following images show
    the image samples after 100 epochs of training. The image on the left is the segmentation
    mask, the middle one is the ground truth, and the one on the right is generated:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 DCGAN 流水线来训练 pix2pix，与 DCGAN 相比，它的训练过程出乎意料的简单。这是因为输入图像的概率分布比随机噪声的分布要窄。以下图像展示了训练
    100 个 epochs 后的样本图像。左边的图像是分割掩膜，中间的是地面真实图像，右边的是生成的图像：
- en: '![Figure 4.12 – Images generated by pix2pix after 100 epochs of training. Left:
    Input mask. Middle: Ground truth. Right: Generated image ](img/B14538_04_12.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 经过 100 轮训练后，pix2pix 生成的图像。左：输入遮罩。中：真实图像。右：生成的图像](img/B14538_04_12.jpg)'
- en: 'Figure 4.12 – Images generated by pix2pix after 100 epochs of training. Left:
    Input mask. Middle: Ground truth. Right: Generated image'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 经过 100 轮训练后，pix2pix 生成的图像。左：输入遮罩。中：真实图像。右：生成的图像
- en: Images generated by pix2pix capture the image content correctly due to the large
    weight (lambda=100) of reconstruction loss. For example, the doors and windows
    are almost always in the correct places and correct shapes. However, it lacks
    variation in styles as the generated buildings have mostly the same color, as
    are the styles of windows. This is due to the absence of random noise in the model
    as mentioned earlier and acknowledged by the authors. Nevertheless, pix2pix opens
    the floodgates for image-to-image translation using GAN.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于重建损失的权重较大（lambda=100），pix2pix 生成的图像能够正确地捕捉图像内容。例如，门窗几乎总是位于正确的位置且形状正确。然而，它在风格上缺乏变化，因为生成的建筑物大多呈现相同的颜色，窗户的风格也相似。这是由于模型中缺乏前面提到的随机噪声，作者也对此表示认可。尽管如此，pix2pix
    仍然为使用 GAN 进行图像到图像的翻译打开了大门。
- en: Unpaired image translation with CycleGAN
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CycleGAN 进行无配对图像翻译
- en: CycleGAN was created by the same research group who invented pix2pix. CycleGAN
    could train with unpaired images using two generators and two discriminators.
    However, by using pix2pix as a foundation, CycleGAN is actually quite simple to
    implement once you understand how the cycle consistency loss works. Before this,
    let's try to understand the advantage of CycleGAN over pix2pix in the following
    sections.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 是由与发明 pix2pix 相同的研究小组创建的。CycleGAN 可以通过使用两个生成器和两个判别器，在无配对图像的情况下进行训练。然而，一旦理解了循环一致性损失的工作原理，基于
    pix2pix 的 CycleGAN 实现起来其实相当简单。在此之前，我们先试着理解 CycleGAN 相较于 pix2pix 的优势。
- en: Unpaired dataset
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无配对数据集
- en: One drawback of pix2pix is that it requires a paired training dataset. For some
    applications, we can create a dataset rather easily. A grayscale-to-color images
    dataset and vice-versa is probably the simplest to create using any image processing
    software libraries such as OpenCV or Pillow. Similarly, we could also easily create
    sketches from real images using edge detection techniques. For a photo-to-artistic-painting
    dataset, we can use neural style transfer (we'll cover this in [*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104),
    *Style Transfer*) to create artistic painting from real images.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix 的一个缺点是它需要配对的训练数据集。对于某些应用，我们可以相对容易地创建数据集。灰度到彩色的图像数据集及其反向数据集可能是最简单的创建方法，可以使用
    OpenCV 或 Pillow 等图像处理软件库来完成。类似地，我们也可以通过边缘检测技术轻松地从真实图像中创建素描。对于照片到艺术画的图像数据集，我们可以使用神经风格迁移（我们将在[*第
    5 章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)，*风格迁移* 中讲解）将真实图像转换为艺术画。
- en: However, there are some datasets that cannot be automated, such as day-to-night
    scenes. Some have to be labeled manually, which can be expensive to do, such as
    the segmentation masks for building façades. Then, some image pairs are simply
    impossible to collect or create, such as a horse-to-zebra image translation. This
    is where CycleGAN excels as it does not require paired data. CycleGAN could train
    on unpaired datasets and then translate images in either direction!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些数据集无法自动化生成，例如从白天到晚上的场景。有些数据需要手动标注，这可能会很昂贵，例如建筑外立面的分割遮罩。然后，一些图像对根本无法收集或创建，例如马到斑马的图像翻译。这正是
    CycleGAN 的优势所在，它不需要配对数据。CycleGAN 可以在无配对数据集上进行训练，然后进行双向图像翻译！
- en: Cycle consistency loss
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环一致性损失
- en: In a generative model, the generator translates from domain A (source) to domain
    B (target), for example, from orange to apple. By conditioning on the image from
    A (orange), the generator creates images with pixel distributions of B (apple).
    However, this does not guarantee that those images are paired in a meaningful
    way.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成模型中，生成器将从 A 域（源）翻译到 B 域（目标），例如，从橙子到苹果。通过对来自 A（橙子）的图像进行条件处理，生成器创造出具有 B（苹果）像素分布的图像。然而，这并不保证这些图像在有意义的方式上是配对的。
- en: We will use language translation as an analogy. Let's assume you are tourist
    in a foreign country and you ask a local to help translate an English sentence
    into the local language and she replies with a beautifully sounding sentence.
    OK, it does sound real, but is the translation correct? You walk down the street
    and ask another person to explain that sentence into English. If that translation
    matches your original English sentence, then we know the translation was correct.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用语言翻译作为类比。假设你是一个外国游客，你请求当地人帮助将一句英文句子翻译成当地语言，她回答了一个听起来很美丽的句子。好的，它听起来很真实，但翻译正确吗？你走在街上，向另一个人询问这句翻译成英文的意思。如果这次翻译与最初的英文句子相匹配，那么我们就知道翻译是正确的。
- en: 'Using the same concept, CycleGAN adopts a translation cycle to ensure that
    the mapping is correct in both directions. The following diagram shows the architecture
    of CycleGAN that forms a cycle between two generators:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的概念，CycleGAN 采用了一个翻译循环，以确保映射在两个方向上都是正确的。下图展示了 CycleGAN 的架构，它在两个生成器之间形成一个循环：
- en: '![Figure 4.13 – Architecture of CycleGAN. (The solid arrows show the flow of
    the forward cycle, while the dashed arrow path is not used in the forward cycle
    but is drawn to show the overall connections between blocks.)](img/B14538_04_13.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – CycleGAN 的架构。（实线箭头表示前向循环的流动，而虚线箭头路径不用于前向循环，但为了显示各个模块之间的整体连接关系而绘制。）](img/B14538_04_13.jpg)'
- en: Figure 4.13 – Architecture of CycleGAN. (The solid arrows show the flow of the
    forward cycle, while the dashed arrow path is not used in the forward cycle but
    is drawn to show the overall connections between blocks.)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – CycleGAN 的架构。（实线箭头表示前向循环的流动，而虚线箭头路径不用于前向循环，但为了显示各个模块之间的整体连接关系而绘制。）
- en: 'In the preceding diagram, we have image domain **A** on the left and domain
    **B** on the right. The procedure that is followed is listed here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，左侧是图像域 **A**，右侧是图像域 **B**。以下是所遵循的过程：
- en: '**GAB** is a generator that translates from **A** to fake **B**; the generated
    image then goes to the discriminator **DB**. This is the standard GAN data path.
    Next, the fake image **B** is translated back into domain **A** via **GBA** and
    that completes the forward path. At this point, we have a reconstructed image,
    **A**. If the translations went perfectly, then it should look identical to the
    source image **A**.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAB** 是一个生成器，它将 **A** 转换为伪造的 **B**；生成的图像随后传递给鉴别器 **DB**。这是标准的 GAN 数据路径。接下来，伪造的图像
    **B** 会通过 **GBA** 转换回域 **A**，完成前向路径。此时，我们得到了一张重建的图像，**A**。如果翻译完美无缺，那么它应该与源图像 **A**
    一模一样。'
- en: We also come across **cycle consistency loss**, which is an L1 loss between
    the source image and the reconstructed image. Similarly, for the backward path,
    we start the cycle by translating from domain **B** to **A**.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还遇到了 **循环一致性损失**，它是源图像与重建图像之间的 L1 损失。同样，对于反向路径，我们从将图像从域 **B** 翻译到 **A** 开始这个循环。
- en: In training, we show CycleGAN with two images from domains `ch4_cyclegan_facade.ipynb`
    notebook.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，我们使用来自 `ch4_cyclegan_facade.ipynb` 笔记本的两个图像来展示 CycleGAN。
- en: CycleGAN also uses what is known as **identity loss**, which is equivalent to
    the reconstruction loss of pix2pix. **GAB** translates image **A** into the fake
    **B**, while the forward identity loss is the L1 distance between the fake **B**
    and the real **B**. Similarly, there is also a backward identity loss in the reverse
    direction. With façade datasets, the weight of identity loss should be set to
    low. This is because some of the real images in this dataset have parts of their
    images blacked out. This dataset was meant to let a machine learning algorithm
    guess the missing pixels. Thus, we use a low weight to discourage the network
    from translating the blackout.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 还使用了所谓的 **身份损失**，它等同于 pix2pix 的重建损失。**GAB** 将图像 **A** 转换为伪造的 **B**，而前向身份损失是伪造的
    **B** 与真实的 **B** 之间的 L1 距离。同样，在反向方向上也有一个身份损失。对于外立面数据集，身份损失的权重应该设置得较低。这是因为该数据集中的一些真实图像有部分图像被遮挡。这个数据集的目的是让机器学习算法猜测缺失的像素。因此，我们使用低权重来防止网络翻译这些遮挡区域。
- en: Building CycleGAN models
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 CycleGAN 模型
- en: We will now build the discriminators and generators of CycleGAN. The discriminator
    is PatchGAN, like pix2pix, with two changes. First, the discriminator only sees
    the images from its domain and thus, only one image inputs into discriminators
    rather than both images from A and B. In other words, the discriminators only
    need to judge whether the images are real or fake in their own domain.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将构建CycleGAN的判别器和生成器。判别器是PatchGAN，类似于pix2pix，具有两个变化。首先，判别器只看到来自其领域的图像，因此，输入到判别器的只有一张图像，而不是A和B的两张图像。换句话说，判别器只需要判断图像是否在其领域内是真实的还是假的。
- en: 'Second, sigmoid is removed from the output layer. This is because CycleGAN
    uses a different adversarial loss function called **least-squares loss**. We haven''t
    covered LSGAN in this book, but it is sufficient to know that this loss is more
    stable than **log-loss**, and we can implement it using the Keras **mean squared
    loss** (**MSE**) function. We train discriminators with the usual training step
    as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，输出层去掉了sigmoid。这是因为CycleGAN使用了不同的对抗损失函数，称为**最小二乘损失**。我们在本书中没有介绍LSGAN，但足够知道这种损失比**对数损失**更稳定，并且我们可以使用Keras的**均方误差**（**MSE**）函数来实现它。我们按以下步骤训练判别器：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For the generator, the original CycleGAN uses a residual block for improved
    performance, but we will reuse U-Net from pix2pix, so we can focus more on CycleGAN's
    high-level architecture and training steps.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器，原始的CycleGAN使用了残差块以提高性能，但我们将重用pix2pix中的U-Net，这样我们可以更专注于CycleGAN的高级架构和训练步骤。
- en: 'Now, let''s instantiate two pairs of generators and discriminators:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实例化两对生成器和判别器：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here comes the core of CycleGAN, which is to implement the combined model to
    train generators. All we need to do is to follow the arrows in the architecture
    diagram to feed input into the generator to generate a fake image that goes to
    the discriminator and cycles back as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是CycleGAN的核心，实现在组合模型中训练生成器。我们只需要按照架构图中的箭头将输入传递给生成器，生成一个假图像，然后送入判别器，并按以下方式循环返回：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The final step is to create a model with those inputs and outputs:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用这些输入和输出创建模型：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we need to assign correct losses and weights to them. As mentioned earlier,
    we use `mae` (L1 loss) for cycle consistency loss and `mse` (mean squared error)
    for adversarial loss, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要为它们分配正确的损失和权重。如前所述，我们使用`mae`（L1损失）作为循环一致性损失，使用`mse`（均方误差）作为对抗损失，如下所示：
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In each training step, we first train both discriminators in both directions,
    from A to B and from B to A. The `train_discriminator()` function includes training
    with fake and real images as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练步骤中，我们首先训练两个方向的判别器，分别是从A到B和从B到A。`train_discriminator()`函数包括使用假图像和真实图像进行训练，如下所示：
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is followed by training generators. The inputs are real images A and B.
    With regard to the labels, the first pair is real/fake labels, the second pair
    is the cycle reconstructed images, and the last pair is for identity loss:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练生成器。输入是来自A和B的真实图像。关于标签，第一个对是真实/假标签，第二个对是循环重建图像，最后一对是身份损失：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Then we can start training.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以开始训练。
- en: Analysis of CycleGAN
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN分析
- en: 'The following are some of the building façades generated by CycleGAN:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些CycleGAN生成的建筑立面：
- en: '![Figure 4.14 – Building façades generated by CycleGAN'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14 – CycleGAN生成的建筑立面'
- en: '](img/B14538_04_14.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_14.jpg)'
- en: Figure 4.14 – Building façades generated by CycleGAN
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – CycleGAN生成的建筑立面
- en: 'Although they look good, they are not necessarily better than pix2pix. The
    strength of CycleGAN compared to pix2pix lies in its ability to train on unpaired
    data. In order to test this, I have created `ch4_cyclegan_horse2zebra.ipynb` to
    train it on unpaired horse and zebra images. Just so you know, training on unpaired
    images is a lot harder. Therefore, have fun trying! The following images show
    image-to-image translation between horses and zebras:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们看起来不错，但不一定比pix2pix更好。与pix2pix相比，CycleGAN的优势在于能够在无配对数据上进行训练。为了测试这一点，我创建了`ch4_cyclegan_horse2zebra.ipynb`，在无配对的马和斑马图像上进行训练。只要你知道，在无配对图像上进行训练要困难得多。所以，祝你玩得开心！以下是马和斑马之间的图像到图像翻译：
- en: '![Figure 4.15 – Translation between horse and zebra'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.15 – 马和斑马之间的翻译'
- en: '(Source: J-Y. Zhu et al., “Unpaired Image-to-Image Translation Using Cycle-Consistent
    Adversarial Networks” – https://arxiv.org/abs/1703.10593)](img/B14538_04_15.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：J-Y. Zhu 等人，"使用循环一致对抗网络进行无配对图像到图像的翻译" – https://arxiv.org/abs/1703.10593）](img/B14538_04_15.jpg)
- en: 'Figure 4.15 – Translation between horse and zebra (Source: J-Y. Zhu et al.,
    “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks”
    – https://arxiv.org/abs/1703.10593)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 马和斑马之间的翻译（来源：J-Y. Zhu 等人，“使用周期一致的对抗网络进行未配对图像到图像的翻译” – https://arxiv.org/abs/1703.10593）
- en: Pix2pix and CycleGAN are popular GANs that are used by many. However, they both
    have one shortcoming; that is the image outputs almost always look identical.
    For example, if we were to perform zebra-to-horse translation, the horse will
    always have the same skin color. This is due to the inherent nature of GANs that
    learns to reject the randomness of noise. In the next section, we will look at
    how BicycleGAN solves this problem to generate richer variations of images.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2pix 和 CycleGAN 是许多人使用的流行 GAN。然而，它们都有一个缺点：即输出的图像几乎总是相同的。例如，如果我们执行斑马到马的翻译，马的皮肤颜色总是相同的。这是由于
    GAN 本身的特性，它会学习排除噪声的随机性。在下一节中，我们将讨论 BicycleGAN 如何解决这个问题，生成更丰富的图像变化。
- en: Diversifying translation with BicyleGAN
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BicycleGAN 实现翻译多样化
- en: Both Pix2pix and CycleGAN came from the **Berkeley AI Research** (**BAIR**)
    laboratory at UC Berkeley. They are popular and have a number of tutorials and
    blogs about them online, including on the official TensorFlow site. BicycleGAN
    is what I see as the last of the image-to-image translation trilogy from that
    research group. However, you don't find a lot of example code online, perhaps
    due to its complexity.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2pix 和 CycleGAN 都来自 UC Berkeley 的**伯克利人工智能研究**（**BAIR**）实验室。它们非常受欢迎，并且在网上有许多教程和博客，包括官方的
    TensorFlow 网站。BicycleGAN 是我认为该研究小组的图像到图像翻译三部曲中的最后一部。然而，你在网上可能找不到太多示例代码，也许是由于它的复杂性。
- en: In order to build the most advanced network in this book up to this point, we
    will throw in all the knowledge you have acquired in this chapter, plus the last
    two chapters. Maybe that is why it is regarded as advanced by many. Don't worry;
    you already have all the prerequisite knowledge. Let's jump in!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建本书到目前为止最先进的网络，我们将结合你在本章及前两章所学的所有知识。也许这就是为什么它被许多人视为高级内容的原因。别担心，你已经掌握了所有必要的前置知识。让我们开始吧！
- en: Understanding architecture
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构理解
- en: Before jumping straight into implementation, let me give you an overview of
    BicycleGAN. From the name, you may naturally think that BicycleGAN is an upgrade
    of CycleGAN by adding another cycle (from unicycle to bicycle). No, it is not!
    It has nothing to do with CycleGAN; it is rather an improvement to pix2pix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接进入实现之前，让我先给你概述一下 BicycleGAN。从名字上看，你可能自然会认为 BicycleGAN 是通过增加另一个周期（从独轮车到双轮车）对
    CycleGAN 的升级。其实不是！它与 CycleGAN 无关，而是对 pix2pix 的改进。
- en: As mentioned earlier, pix2pix is a one-to-one mapping where the output is always
    the same for a given input. The authors tried to add noise to the generator input,
    but it simply ignores the noise and fails to create variations in the output image.
    Therefore, they searched for a method where the generator does not ignore the
    noise, but instead uses the noise to generate diversified images, hence, one-to-many
    mapping.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，pix2pix 是一种一对一的映射，其中给定输入时输出始终相同。作者尝试向生成器输入添加噪声，但它简单地忽略了噪声，未能在输出图像中创造变化。因此，他们寻找了一种方法，使生成器不忽略噪声，而是利用噪声生成多样化的图像，从而实现一对多的映射。
- en: 'In the following screenshot, we can see different models and configurations
    related to BicycleGAN. Diagram *(a)* is the configuration for inference where
    image **A** is combined with input noise to generate image **B**. This is essentially
    the cGAN at the beginning of the chapter, except for the role reversal between
    image **A** and noise. In cGAN, noise plays the leading role, with 100 dimensions
    and a condition of 10 class labels. In BicyleGAN, image **A** with a shape of
    (256, 256, 3) is the condition, while the noise sampled from latent *z* has a
    dimension of 8\. *Figure (b)* is the training configuration for *pix2pix + noise*.
    The two configurations at the bottom of the diagram are used by BicycleGAN, and
    we will look at these shortly:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们可以看到与 BicycleGAN 相关的不同模型和配置。图 *(a)* 是用于推理的配置，其中图像 **A** 与输入噪声结合生成图像
    **B**。这本质上就是本章开头的 cGAN，只不过图像 **A** 和噪声角色颠倒了。在 cGAN 中，噪声是主导因素，维度为 100，且有 10 个类别标签作为条件。在
    BicycleGAN 中，形状为（256，256，3）的图像 **A** 是条件，而从潜在 *z* 中采样的噪声维度为 8。*图 (b)* 是用于 *pix2pix
    + 噪声* 的训练配置。图中底部的两个配置是 BicycleGAN 使用的，我们稍后会仔细看这些：
- en: '![Figure 4.16 – Models within BicycleGAN'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.16 – BicycleGAN 中的模型'
- en: '(Source: J-Y. Zhu, “Toward Multimodal Image-to-Image Translation” – https://arxiv.org/abs/1711.11586)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：J-Y. Zhu，“迈向多模态图像到图像的转换”– https://arxiv.org/abs/1711.11586）
- en: '](img/B14538_04_16.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_04_16.jpg)'
- en: 'Figure 4.16 – Models within BicycleGAN (Source: J-Y. Zhu, “Toward Multimodal
    Image-to-Image Translation” – https://arxiv.org/abs/1711.11586)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – BicycleGAN中的模型（来源：J-Y. Zhu，“迈向多模态图像到图像的转换”– https://arxiv.org/abs/1711.11586）
- en: The main concept of BicycleGAN is to find a relation between the latent code
    *z* and the target image **B**, so the generator can learn to generate a different
    image **B** when given a different *z*. BicycleGAN does it by combining the two
    methods, **cVAE-GAN** and **cLR-GAN**, as shown in the preceding diagram.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: BicycleGAN的主要概念是找到潜在代码*z*与目标图像**B**之间的关系，以便生成器可以在给定不同*z*时学会生成不同的图像**B**。BicycleGAN通过结合两种方法，**cVAE-GAN**和**cLR-GAN**，来实现这一点，如前面的图示所示。
- en: cVAE-GAN
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cVAE-GAN
- en: 'Let''s go over some background to **VAE-GAN**. The authors of VAE-GAN argue
    that L1 loss is not a good metric for measuring the visual perception of an image.
    If the image moves a few pixels to the right, it may look no different to the
    human eye, but can result in a large L1 loss. Why not let a network learn what
    is the appropriate objective function to use? Indeed, they use GAN''s discriminator
    to learn the objective function to tell whether the fake image looks real and
    use VAE as a generator. As a result, the generated images appear sharper. If we
    look at *Figure (c)* from the preceding diagram and ignore image **A**, that is
    a VAE-GAN. With **A** as a condition, it becomes a conditional cVAE-GAN. The training
    steps are as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下**VAE-GAN**的一些背景。VAE-GAN的作者认为L1损失并不是衡量图像视觉感知的好度量标准。如果图像向右移动几个像素，可能对人眼看不出任何不同，但却可能导致较大的L1损失。为什么不让网络学习什么是合适的目标函数呢？实际上，他们使用GAN的判别器来学习目标函数，判断假图像是否看起来真实，并使用VAE作为生成器。因此，生成的图像看起来更清晰。如果我们看前面图中的*图(c)*并忽略图像**A**，那就是VAE-GAN。如果有**A**作为条件，它就变成了条件cVAE-GAN。训练步骤如下：
- en: The VAE encodes the real image **B** into latent code of a multivariate Gaussian
    mean and log variance, and then samples from them to create noise input. This
    flow is the standard VAE workflow. Please refer to [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039),
    *Variational Autoencoder*, for a refresher.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VAE将真实图像**B**编码为多元高斯均值和对数方差的潜在代码，然后从中采样以创建噪声输入。这个流程是标准的VAE工作流程。有关详细信息，请参考[*第2章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)，*变分自编码器*。
- en: Now with the condition **A**, the noise sampled from the latent vector *z* is
    used to generate a fake image **B**.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在条件**A**下，从潜在向量*z*中采样的噪声被用来生成一张假的图像**B**。
- en: 'The information flow is ![](img/Formula_04_001.png) (the solid lined arrow
    in *Figure (c)*). There are three loses:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 信息流是 ![](img/Formula_04_001.png) （*图(c)*中的实线箭头）。有三个损失：
- en: '![](img/Formula_04_002.png): Adversarial loss'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_002.png)：对抗损失'
- en: '![](img/Formula_04_003.png): L1 reconstruction loss'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_003.png)：L1重建损失'
- en: '![](img/Formula_04_004.png): KL divergence loss'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_004.png)：KL散度损失'
- en: cLR-GAN
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cLR-GAN
- en: The theory behind Conditional Latent Regressor GAN is beyond the scope of this
    book. However, we will focus on how this is applied in BicycleGAN. In cVAE-GAN,
    we encode a real image *B* to provide the ground truth of a latent vector and
    sample from it. However, cLR-GAN does things differently by first letting the
    generator generate a fake image *B* from random noise, and then encoding the fake
    image *B* and seeing how it deviates from the input random noise.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 条件潜在回归GAN背后的理论超出了本书的范围。然而，我们将重点介绍它在BicycleGAN中的应用。在cVAE-GAN中，我们将真实图像**B**编码以提供潜在向量的真实标签，并从中采样。而cLR-GAN通过让生成器先从随机噪声生成一张假的图像**B**，然后再对假图像**B**进行编码，查看它与输入的随机噪声之间的偏差，做出了不同的处理。
- en: 'The forward steps are as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 前向步骤如下：
- en: Like cGAN, we randomly generate some noise, and then concatenate with image
    *A* to generate a fake image *B*.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像cGAN一样，我们随机生成一些噪声，然后将其与图像**A**连接，生成一个假的图像**B**。
- en: Then we use the same encoder from VAE-GAN to encode the fake image *B* into
    latent vectors.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用VAE-GAN中的相同编码器，将假的图像**B**编码成潜在向量。
- en: We then sample *z* from encoded latent vectors, and compute the point loss with
    the input noise *z*.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们从编码后的潜在向量中采样*z*，并计算输入噪声*z*的点损失。
- en: 'The flow is ![](img/Formula_04_005.png) (solid lined arrow in *Figure (d)*).
    There are two losses as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 流程是 ![](img/Formula_04_005.png) （*图(d)*中的实线箭头）。有两个损失如下：
- en: '![](img/Formula_04_006.png): Adversarial loss'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_006.png)：对抗损失'
- en: '![](img/Formula_04_007.png): L1 loss between noise *N(z)* and the encoded mean'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_007.png)：噪声*N(z)*和编码均值之间的L1损失'
- en: 'By combining these two flows, we got a bijection cycle between the output and
    the latent space. The *bi* in BicycleGAN comes from *bijection*, which is a mathematical
    term that roughly means one-to-one mapping and is reversible. In this case, BicycleGAN
    maps the output to a latent space, and similarly from the latent space to the
    output. The total loss is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这两种流，我们得到了输出和潜在空间之间的双射循环。BicycleGAN中的*bi*来自于*bijection*，这是一个数学术语，大致意思是“一对一映射并且是可逆的”。在这种情况下，BicycleGAN将输出映射到潜在空间，并且同样可以从潜在空间映射回输出。总损失如下：
- en: '![](img/Formula_04_008.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_008.jpg)'
- en: Where *λ=10*, *λ*latent *= 0.5*, and *λ*latent*=0.01* are used in the default
    configuration.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认配置中，*λ=10*，*λ*latent *= 0.5*，和*λ*latent*=0.01*。
- en: Now that we understand the BicycleGAN architecture and loss functions, we can
    now go on to implement them.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了BicycleGAN的架构和损失函数，接下来我们可以开始实现它们了。
- en: Implementing BicycleGAN
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现BicycleGAN
- en: We will use the `ch4_bicycle_gan.ipynb` notebook here. There are three types
    of networks in BicycleGAN – the generator, discriminator, and encoder. We will
    reuse the discriminator (PatchGAN) from pix2pix and the encoder from VAE from
    [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039), *Variational Autoencoder*.
    The encoder is bulked up with more filters and deeper layers as the input image
    size is larger. The code can look slightly different, but essentially the concept
    is the same as before. The original BicycleGAN uses two PatchGANs with effective
    receptive fields of 70x70 and 140x140\.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用`ch4_bicycle_gan.ipynb`笔记本。BicycleGAN有三种类型的网络——生成器、判别器和编码器。我们将重用来自pix2pix的判别器（PatchGAN）和来自[*第二章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)的VAE编码器，*变分自编码器*。由于输入图像的尺寸较大，编码器增加了更多的滤波器和更深的层级。代码可能看起来略有不同，但本质上概念与之前相同。原始的BicycleGAN使用了两个PatchGAN，具有有效感受野70x70和140x140。
- en: For simplicity, we'll use only one 70x70 PatchGAN. Using a separate discriminator
    for cVAE-GAN and cLR-GAN improves image quality, meaning we have four networks
    in total – the generator, encoder, and two discriminators.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便，我们只使用一个70x70的PatchGAN。使用单独的判别器分别用于cVAE-GAN和cLR-GAN可以提高图像质量，这意味着我们总共有四个网络——生成器、编码器和两个判别器。
- en: Inserting latent code into the generator
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将潜在代码插入生成器
- en: 'The authors tried two methods of inserting latent code into the generator,
    one involving concatenating with the input image, and the other involving inserting
    it into other layers in the downsampling path of the generator, as shown in the
    following diagram. It was found that the former works well. Let''s implement this
    simple method:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作者尝试了两种将潜在代码插入生成器的方法，一种是与输入图像连接，另一种是在生成器的下采样路径中的其他层中插入潜在代码，如下图所示。结果发现，前者效果较好。让我们实现这个简单的方法：
- en: '![Figure 4.17 – Different ways of injecting z into the generator (Redrawn from:
    J-Y. Zhu, “Toward Multimodal Image-to-Image Translation” – https://arxiv.org/abs/1711.11586)](img/B14538_04_17.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图4.17 – 向生成器注入z的不同方式（重新绘制自：J-Y. Zhu，“多模态图像到图像转换” – https://arxiv.org/abs/1711.11586)](img/B14538_04_17.jpg)'
- en: 'Figure 4.17 – Different ways of injecting z into the generator (Redrawn from:
    J-Y. Zhu, “Toward Multimodal Image-to-Image Translation” – https://arxiv.org/abs/1711.11586)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – 向生成器注入z的不同方式（重新绘制自：J-Y. Zhu，“多模态图像到图像转换” – https://arxiv.org/abs/1711.11586）
- en: As we have learned at the beginning of this chapter, there are several ways
    to join the input and conditions of different shapes. BicycleGAN's method is to
    repeat the latent code multiple times and concatenate with the input image.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头所学到的，连接不同形状的输入和条件有多种方法。BicycleGAN的方法是将潜在代码重复多次并与输入图像连接。
- en: 'Let''s use a concrete example. In BicycleGAN, the latent code length is 8\.
    We draw 8 samples from noise distribution, and each sample is repeated H×W times
    to form a tensor with the shape of (H,W,8). In other words, in each of the 8 channels,
    its (H, W) feature map is made up of the same repeated number from that channel.
    The following is the code snippet of `build_generator()`, which shows the tiling
    and concatenation of latent code. The remainder of the code is the same as the
    pix2pix generator:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个具体的例子。在BicycleGAN中，潜在编码的长度是8。我们从噪声分布中抽取8个样本，每个样本被重复H×W次，形成一个形状为(H, W,
    8)的张量。换句话说，在每个8个通道中，其(H, W)特征图由该通道中的相同重复数值构成。以下是`build_generator()`的代码片段，展示了潜在编码的平铺和拼接。其余代码与pix2pix生成器相同：
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The next step is to create two models, cVAE-GAN and cLR-GAN, to incorporate
    the networks and create the forward path flow.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建两个模型，cVAE-GAN 和 cLR-GAN，以便将网络结合起来并创建前向路径流。
- en: cVAE-GAN
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cVAE-GAN
- en: 'Here is the code to create a model for cVAE-GAN. This is the implementation
    of the forward pass, as mentioned earlier:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建cVAE-GAN模型的代码。这是前向传播的实现，正如前面提到的：
- en: '[PRE23]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We include the KL divergence loss in the model as opposed to that in the custom
    loss function. This is simpler and more efficient as `kl_loss` can be calculated
    directly from the mean and log variance without needing external labels to be
    passed in from a training step.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将KL散度损失包含在模型中，而不是自定义损失函数中。这样做更简单也更高效，因为`kl_loss`可以直接通过均值和对数方差计算，而无需从训练步骤中传入外部标签。
- en: cLR-GAN
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cLR-GAN
- en: 'Here is the implementation of cLR-GAN. One thing to note is that this has different
    inputs to images A and B that are separate from cVAE-GAN:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是cLR-GAN的实现。需要注意的是，这与cVAE-GAN有不同的输入，它们分别对应于图像A和B：
- en: '[PRE24]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Alright, we now have the models defined. The next step is to implement the training
    step.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经定义了模型。接下来的步骤是实现训练步骤。
- en: Training step
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练步骤
- en: 'Both models train together in one step, but with different image pairs. Therefore,
    in each training step, we fetch the data twice, once for each model. Some do it
    by creating data pipelines that load the batch size twice and then split them
    into two halves, as shown in the following code snippet:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型在一步训练中一起训练，但使用不同的图像对。因此，在每个训练步骤中，我们需要获取两次数据，每次对应一个模型。有些方法是创建数据流水线，加载两倍批量大小，然后将其拆分为两部分，代码示例如下：
- en: '[PRE25]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Previously, we used two different methods to perform the training step. One
    is to define and compile a Keras model with an optimizer and loss function, and
    then call `train_on_batch()` to perform the training step. This is simple and
    works well on well-defined models. Alternatively, we can also use `tf.GradientTape`
    to allow finer control of the gradients and update. We have been using both of
    them in our models, where we use `train_on_batch()` for the generator and `tf.GradientTape`
    for the discriminator.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们使用了两种不同的方法来执行训练步骤。一种是定义并编译一个Keras模型，配备优化器和损失函数，然后调用`train_on_batch()`来执行训练步骤。这种方法简单，并且在定义良好的模型上运行良好。另一种方法是使用`tf.GradientTape`，允许对梯度和更新进行更细粒度的控制。我们在模型中同时使用了这两种方法，其中我们对生成器使用`train_on_batch()`，对判别器使用`tf.GradientTape`。
- en: 'The purpose was to familiarize ourselves with both methods so that if we need
    to implement complex training steps with low-level code, we know how to do it,
    and now is the time. BicycleGAN has two models that share a generator and encoder,
    but we update them using different combinations of loss functions, which make
    the `train_on_batch` method unfeasible without modifying the original settings.
    Therefore, we will combine both the generator and discriminator of both models
    into a single training step using `tf.GradientTape` as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是让我们熟悉这两种方法，以便在需要使用低级代码实现复杂训练步骤时，能够知道如何操作，现在就是时候了。BicycleGAN有两个共享生成器和编码器的模型，但我们使用不同的损失函数组合来更新它们，这使得`train_on_batch`方法在不修改原始设置的情况下不可行。因此，我们将结合两个模型的生成器和判别器，将它们合并为一个训练步骤，使用`tf.GradientTape`，如下所示：
- en: 'The first step is to perform a forward pass and collect the outputs from both
    models:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是执行前向传播并收集两个模型的输出：
- en: '[PRE26]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we backpropagate and update the discriminators:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们进行反向传播并更新判别器：
- en: '[PRE27]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we calculate the losses from the models'' outputs. Similar to CycleGAN,
    BicycleGAN also uses the LSGAN loss function, which is the mean squared error:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算模型输出的损失。类似于CycleGAN，BicycleGAN也使用LSGAN损失函数，即均方误差：
- en: '[PRE28]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, there is the update to the generator''s and encoder''s weights. The
    L1 latent code loss is only used to update the generator, and not the encoder.
    It was found that optimizing them simultaneously for the loss would encourage
    them to hide information relating to the latent code and not learn meaningful
    modes. Therefore, we calculate separate losses for the generator and encoder and
    update the weights accordingly:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后是生成器和编码器的权重更新。L1潜在编码损失只用于更新生成器，而不更新编码器。研究发现，如果同时优化它们的损失，会促使它们隐藏与潜在编码相关的信息，从而无法学习到有意义的模式。因此，我们为生成器和编码器计算了独立的损失，并相应地更新了权重：
- en: '[PRE29]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There you go. You can now train your BicycleGAN. There are two datasets you
    can choose from in the notebook – building façades or edges to shoes. The shoe
    dataset has simpler images and therefore is easier to train. The following images
    are examples from the original BicycleGAN paper. The first real image on the left
    is the ground truth and the four images on the right are generated ones:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。现在你可以开始训练你的BicycleGAN了。在笔记本中有两个数据集可以选择——建筑立面或鞋子轮廓。鞋子数据集的图像较简单，因此训练起来也更容易。以下是原始BicycleGAN论文中的一些示例。左侧的第一张真实图像是地面真值，右侧的四张图像是生成的图像：
- en: '![Figure 4.18 – Examples of transforming sketches to images with a variety
    of styles. Source: J-Y. Zhu, “Toward Multimodal Image-to-Image Translation”](img/B14538_04_18.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.18 – 将草图转换为各种风格的图像的示例。来源：J-Y. Zhu，《迈向多模态图像到图像转换》](img/B14538_04_18.jpg)'
- en: 'Figure 4.18 – Examples of transforming sketches to images with a variety of
    styles. Source: J-Y. Zhu, “Toward Multimodal Image-to-Image Translation”'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 – 将草图转换为各种风格的图像的示例。来源：J-Y. Zhu，《迈向多模态图像到图像转换》
- en: You may struggle to notice the difference between them on this grayscale page
    because their differences are mainly in color. It captures the structure of the
    shoes and bags almost perfectly, but not so much in terms of the fine details.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能很难在这个灰度页面上注意到它们之间的差异，因为它们的区别主要体现在颜色上。它几乎完美地捕捉了鞋子和包的结构，但在细节方面表现不那么出色。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We began this chapter by learning how the basic cGAN enforces the class label
    as a condition to generate MNIST. We implemented two different ways of injecting
    the condition, one being to one-hot encode the class labels to a dense layer,
    reshape them to match the channel dimensions of the input noise, and then concatenate
    them together. The other way is to use the `embedding` layer and element-wise
    multiplication.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习基本的cGAN如何强制将类别标签作为条件生成MNIST来开始这一章的内容。我们实现了两种不同的条件注入方式，一种是将类别标签进行one-hot编码后传入一个稠密层，将其重塑为与输入噪声的通道维度匹配，然后将它们拼接在一起。另一种方法是使用`embedding`层和逐元素相乘。
- en: Next, we learned to implement pix2pix, a special type of condition GAN for image-to-image
    translation. It uses PatchGAN as a discriminator, which looks at patches of images
    to encourage fine details or high-frequency components in the generated image.
    We also learned about a popular network architecture, U-Net, that has been used
    for various applications. Although pix2pix can generate high-quality image translation,
    the image is one-to-one mapping without diversification of the output. This is
    due to the removal of input noise. This was overcome by BicycleGAN, which learned
    the mapping between the latent code and the output image so that the generator
    doesn't ignore the input noise. With that, we are one step closer toward multimodal
    image translation.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何实现pix2pix，这是一种专门用于图像到图像转换的条件GAN。它使用PatchGAN作为判别器，该判别器通过查看图像的局部区域来鼓励生成图像中的细节或高频成分。我们还学习了一个广泛应用的网络架构——U-Net，已经被用于多种应用。尽管pix2pix可以生成高质量的图像转换，但生成的图像是一个一对一的映射，并没有输出的多样性。这是因为输入噪声被移除。这个问题被BicycleGAN克服了，BicycleGAN学习了潜在编码与输出图像之间的映射，以确保生成器不会忽略输入噪声。这样，我们就更接近于多模态图像转换。
- en: In the timeline between pix2pix and BicycleGAN, CycleGAN was invented. Its two
    generators and two discriminators use the cycle consistency loss to allow training
    with unpaired data. In total, we have implemented four GANs in this chapter and
    they are not easy ones. Well done! In the next chapter, we will look at style
    transfer, which entangles an image into content code and style code. This has
    had a profound influence on the development of new GANs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在pix2pix和BicycleGAN之间的时间线中，发明了CycleGAN。它的两个生成器和两个判别器使用循环一致性损失来实现无配对数据的训练。总的来说，我们在这一章中实现了四种GAN，它们都不是简单的。干得好！在下一章，我们将探讨风格迁移，它将图像分解为内容编码和风格编码。这对新GAN的发展产生了深远的影响。
