- en: '*Chapter 9*: Fundamentals of Deep Learning Explainability'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：深度学习可解释性基础'
- en: 'Explainability is providing selective human-understandable explanations for
    a decision provided by an automated system. In the context of this book, during
    the full life cycle of **deep learning** (**DL**) development, explainability
    should be emphasized as a first-class artifact, along with the other three pillars:
    data, code, and model. This is because different stakeholders and regulators,
    model developers, and final consumers of the model output may have different needs
    to understand how the data is used and why the model produces certain predictions
    or classifications. Without such understanding, it will be difficult to gain the
    trust of the consumers of the model output or to diagnose what could have gone
    wrong when model output results drift. This also means that explainability tools
    should be employed not only for explaining prediction results from a deployed
    model in production or during offline experimentation, but also for understanding
    the data characteristics and differences between the datasets used in offline
    model training and the ones encountered in online model operation.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是为自动化系统提供选择性人类可理解的决策解释。在本书的背景下，在**深度学习**（**DL**）开发的整个生命周期中，应将可解释性视为与数据、代码和模型这三大支柱一样重要的产物。这是因为不同的利益相关者和监管者、模型开发者以及模型输出的最终消费者可能对了解数据如何使用以及模型为何产生特定预测或分类有不同的需求。如果没有这样的理解，将难以赢得模型输出消费者的信任，或者在模型输出结果漂移时诊断可能出现的问题。这也意味着可解释性工具不仅应用于解释在生产中部署的模型的预测结果或离线实验期间的情况，还应用于理解离线模型训练中使用的数据特性和在线模型操作中遇到的数据集之间的差异。
- en: In addition, in many highly regulated industries, such as autonomous driving,
    medical diagnosis, banking, and finance, there is also a legal mandate that demands
    the **right to explanation** ([https://academic.oup.com/idpl/article/7/4/233/4762325](https://academic.oup.com/idpl/article/7/4/233/4762325))
    for any individual to get an explanation for an output of the algorithm. Finally,
    a recent survey showed that over 82% of CEOs believe that AI-based decisions must
    be explainable to be trusted as enterprises accelerate their investment in developing
    and deploying AI-based initiatives ([https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models](https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models)).
    Therefore, it is important to learn the fundamentals of explainability and the
    related tools so that we know when to use what tools for what audience to provide
    a relevant, accurate, and consistent explanation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在许多高度受监管的行业，如自动驾驶、医疗诊断、银行业和金融业，还有法律法规要求任何个体都有权利获取算法输出的解释的要求。最后，最近的一项调查显示，超过82%的CEO认为基于AI的决策必须是可解释的，以便作为企业加速其投资于开发和部署基于AI的倡议的信任基础（[https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models](https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models)）。因此，学习可解释性的基础知识和相关工具是很重要的，这样我们就知道在何时为何种观众使用何种工具来提供相关、准确和一致的解释。
- en: By the end of this chapter, you will be confident to know what a good explanation
    is and what tools exist for different explainability purposes and will gain hands-on
    experience in using two explainability toolboxes for explaining DL sentiment classification
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将能够自信地知道什么是良好的解释，以及存在哪些工具用于不同的可解释性目的，并且将获得使用两个解释性工具箱来解释深度学习情感分类模型的实际经验。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要话题：
- en: Understanding the categories and audience of explainability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解解释性的类别和受众
- en: Exploring the SHAP Explainability toolbox
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索SHAP可解释性工具包
- en: Exploring the Transformers Interpret toolbox
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Transformers Interpret工具箱
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements are necessary to complete the learning in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章学习需要满足以下要求：
- en: 'SHAP Python library: [https://github.com/slundberg/shap](https://github.com/slundberg/shap)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP Python库：[https://github.com/slundberg/shap](https://github.com/slundberg/shap)
- en: 'Transformers Interpret Python library: [https://github.com/cdpierse/transformers-interpret](https://github.com/cdpierse/transformers-interpret)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Transformers 解释性 Python 库: [https://github.com/cdpierse/transformers-interpret](https://github.com/cdpierse/transformers-interpret)'
- en: 'Captum Python library: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Captum Python 库: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)'
- en: 'Code from the GitHub repository for this chapter: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '本章 GitHub 仓库中的代码: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09)'
- en: Understanding the categories and audience of explainability
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解可解释性的类别和受众
- en: As this chapter's opening texts imply, explainability for a DL system becomes
    increasingly critical, sometimes even mandatory, in highly regulated industries
    such as financial, legal, governmental, and medical application domains. An example
    lawsuit partially due to the lack of ML explainability is the case of **B2C2 v
    Quoine** ([https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning](https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning)),
    where automated AI trading algorithms mistakenly placed an order with 250 times
    the market price for bitcoin trading. The recent successful applications of DL
    models in production stimulate active and abundant research and development in
    the explainability area due to the need to understand why and how a DL model works.
    You may have heard of the term **explainable artificial intelligence** (**XAI**),
    which was started by the **US Defense Advanced Research Projects Agency** (**DARPA**)
    in 2015 for its XAI program with the goal of enabling end users to better understand,
    trust, and effectively manage AI systems ([https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61](https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61)).
    However, the concept of explainability goes way back to the early days of expert
    systems in the 1980s or even earlier ([https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391)),
    and the recent surge of attention on the topic of explainability just highlights
    how important it is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头所述，深度学习系统的可解释性变得越来越重要，有时在一些高度监管的行业中，如金融、法律、政府和医疗领域，甚至是强制性的。一个因缺乏机器学习可解释性而导致的示例诉讼是**B2C2诉Quoine案**([https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning](https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning))，其中自动化的AI交易算法错误地以市场价格的250倍下单购买比特币。深度学习模型在生产中的成功应用促进了对可解释性领域的积极研究和开发，因为我们需要理解深度学习模型是如何运作的。你可能听说过**可解释人工智能**（**XAI**）这个术语，它由**美国国防高级研究计划局**（**DARPA**）于2015年为其XAI计划提出，旨在帮助最终用户更好地理解、信任和有效管理AI系统([https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61](https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61))。然而，可解释性的概念早在1980年代或更早期的专家系统时代就已出现([https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391))，而近期对于可解释性话题的关注高潮，只是突显了它的重要性。
- en: So, what's an explanation? It turns out that this is still an active research
    topic in the ML/DL/AI community. From a practical purpose, a precise definition
    of explanation depends on who wants the explanations for what purpose at what
    time across the ML/DL/AI life cycle ([https://dl.acm.org/doi/abs/10.1145/3461778.3462131](https://dl.acm.org/doi/abs/10.1145/3461778.3462131)).
    So, explainability can be defined as *the capability to provide an audience-appropriate,
    human-understandable interpretation of why and how a model provides certain predictions*.
    This may also include the data explainability aspect, where and how the data was
    used through provenance tracking, what the data characteristics are, or whether
    it has changed due to unexpected events. For example, sales and marketing emails
    changed due to an unexpected COVID outbreak ([https://www.validity.com/resource-center/disruption-in-email/](https://www.validity.com/resource-center/disruption-in-email/)).
    Such data changes will unexpectedly change the distribution of model prediction
    results. We need to take into account such data changes when explaining the model
    drift. This means the complexity of the explanations needs to be tailored and
    selective to the receiving audience without overwhelming information. For example,
    a complex explanation with many technical jargons such as *activation* might not
    work as well as a simple text summary with business-friendly terms. This further
    shows that explainability is also a **Human-Computer Interface/Interaction** (**HCI**)
    topic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是解释？事实证明，这仍然是机器学习（ML）、深度学习（DL）和人工智能（AI）领域的一个活跃研究课题。从实际应用的角度来看，解释的精确定义取决于谁在什么时间、出于什么目的，基于机器学习/深度学习/人工智能的生命周期需求进行解释（[https://dl.acm.org/doi/abs/10.1145/3461778.3462131](https://dl.acm.org/doi/abs/10.1145/3461778.3462131)）。因此，可解释性可以被定义为*为受众提供适当的、易于理解的解释，阐明模型为什么以及如何给出某些预测结果*的能力。这也可能包括数据可解释性的方面，涉及数据是如何通过溯源追踪被使用的，数据的特征是什么，或是数据是否由于意外事件而发生了变化。例如，由于突如其来的新冠疫情，销售和营销邮件发生了变化（[https://www.validity.com/resource-center/disruption-in-email/](https://www.validity.com/resource-center/disruption-in-email/)）。这种数据的变化将意外改变模型预测结果的分布。在解释模型漂移时，我们需要考虑到这种数据的变化。这意味着，解释的复杂性需要根据接收受众进行定制和选择，而不会提供过多的信息。例如，包含许多技术术语的复杂解释，如*激活*，可能不如使用商业友好的术语进行简单文本总结的效果好。这进一步表明，可解释性也是一个**人机界面/交互**（**HCI**）话题。
- en: 'To get the big picture of what the explainability categories and corresponding
    audiences look like, we consider the eight dimensions of explanations shown in
    *Figure 9.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面了解可解释性类别及其对应的受众，我们考虑了*图9.1*中展示的八个维度：
- en: '![Figure 9.1 – Eight dimensions to understand explainability'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1 – 理解可解释性的八个维度'
- en: '](img/B18120_09_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_001.jpg)'
- en: Figure 9.1 – Eight dimensions to understand explainability
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 理解可解释性的八个维度
- en: As can be seen from *Figure 9.1*, the complexity of explainability can be understood
    from eight dimensions. This is not necessarily an exhaustive categorization, but
    rather a guide to understanding different perspectives from HCI, the full life
    cycle of AI/ML/DL, and different technical approaches. In the following discussion,
    we will highlight the dimensions and their inter-relationships that are most relevant
    to DL applications, since the focus of this chapter is on DL explainability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图9.1*可以看出，可解释性的复杂性可以从八个维度进行理解。这不一定是一个详尽的分类，而是帮助理解来自HCI、人工智能/机器学习/深度学习完整生命周期以及不同技术方法的不同视角的指南。在接下来的讨论中，我们将重点介绍与深度学习应用最相关的维度及其相互关系，因为本章的重点是深度学习的可解释性。
- en: 'Audience: who needs to know'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受众：谁需要知道
- en: As pointed out recently by a study ([https://dl.acm.org/doi/abs/10.1145/3461778.3462131](https://dl.acm.org/doi/abs/10.1145/3461778.3462131)),
    it is important to understand who needs to know what kind of explanations at what
    stage across an AI project life cycle. This will also affect the explanation output
    formats. An earlier study ([https://arxiv.org/pdf/1702.08608.pdf](https://arxiv.org/pdf/1702.08608.pdf))
    also points out that depending on whether a domain expert is involved in a real
    application task (for example, a medical doctor in a diagnosis of cancer), the
    cost of validating an explanation could also be high since it requires an actual
    human in a real work environment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如最近一项研究指出的（[https://dl.acm.org/doi/abs/10.1145/3461778.3462131](https://dl.acm.org/doi/abs/10.1145/3461778.3462131)），了解谁在什么阶段需要知道什么类型的解释是非常重要的，这将在整个AI项目生命周期中产生影响。这也将影响解释的输出格式。另一项早期研究（[https://arxiv.org/pdf/1702.08608.pdf](https://arxiv.org/pdf/1702.08608.pdf)）也指出，根据是否有领域专家参与实际应用任务（例如，诊断癌症的医学专家），验证一个解释的成本可能也很高，因为这需要一个实际的人类在实际工作环境中参与。
- en: For current practical DL projects, we need to tailor our methods and presentations
    of explanations depending on the target audience, such as data scientists, ML
    engineers, business stakeholders, **User Experience (UX)** designers, or end users,
    as there is no one-size-fits-all approach.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前的实际深度学习项目，我们需要根据目标受众（如数据科学家、机器学习工程师、业务利益相关者、**用户体验（UX）**设计师或最终用户）定制解释方法和展示方式，因为没有一种通用的方法适用于所有情况。
- en: '**Stage: when to provide an explanation in the DL life cycle**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**阶段：在深度学习生命周期中何时提供解释**'
- en: A **stage** usually refers to when the explanations can be provided during the
    model development life cycle. For a model such as a decision tree, since it is
    a white-box model, we say we can provide **ante-hoc** explainability. However,
    currently, most DL models are mostly treated as black-box models even though self-explaining
    DL models are being gradually developed with ante-hoc explainability ([https://arxiv.org/abs/2108.11761](https://arxiv.org/abs/2108.11761)).
    Therefore, for current practical DL applications, **post-hoc** explainability
    is needed. In addition, when the model development stages are in training, validation,
    or production, the explainability scope can be global, cohort, or local, even
    using the same post-hoc explainability tools ([https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f](https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段**通常指的是在模型开发生命周期中可以提供解释的时机。对于像决策树这样的模型，由于它是一个白盒模型，我们说我们可以提供**事前**可解释性。然而，目前大多数深度学习（DL）模型通常被视为黑盒模型，尽管自解释的深度学习模型正在逐渐开发，且具有事前可解释性（[https://arxiv.org/abs/2108.11761](https://arxiv.org/abs/2108.11761)）。因此，对于当前的实际深度学习应用，需要**事后**可解释性。此外，当模型开发阶段处于训练、验证或生产时，解释的范围可以是全局的、群体的或局部的，即使使用相同的事后可解释性工具（[https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f](https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f)）。'
- en: 'Scope: which prediction needs explanation'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 范围：哪些预测需要解释
- en: '**Scope** refers to whether we can provide the explanation for all predictions,
    a subset of the predictions, or just one specific prediction, even if we use the
    same post-hoc tool for a black-box DL model. The most common global explainability
    is to describe **feature importance** and allow users to know which feature is
    the most impactful one for the overall model performance. Local explainability
    is about **feature attribution** for a specific prediction instance. The difference
    between feature attribution and feature importance is that feature attribution
    not only quantifies the ranking and magnitude of the feature impact, but also
    the direction of the impact (for example, whether a feature is positively or negatively
    affecting the prediction).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**范围**指的是我们是否能够为所有预测、部分预测或仅仅一个特定预测提供解释，即使我们为黑盒深度学习模型使用相同的事后工具。最常见的全局可解释性是描述**特征重要性**，并允许用户了解哪些特征对整体模型性能最有影响。局部可解释性则是关于特定预测实例的**特征归因**。特征归因与特征重要性的区别在于，特征归因不仅量化了特征影响的排名和大小，还量化了影响的方向（例如，一个特征是正向还是负向地影响了预测）。'
- en: Many of the post-hoc tools for DL models are very good at local explainability.
    Cohort explainability is useful for identifying potential model bias for some
    specific groups such as age or race groups. For a DL model, if we want to have
    a global explanation, we often need to use a surrogate model such as a decision
    tree model to emulate the behavior of a DL model ([https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a](https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a)).
    However, this approach does not always work well as it is very difficult to know
    whether the surrogate model is approximating the predictions of the original black-box
    model well enough. So, in practice, local explainability tools for DL models are
    often used, such as **SHapley Additive exPlanations** (**SHAP**), which we will
    explain in the method dimension.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多后期工具对于深度学习模型在局部可解释性方面表现优秀。群体可解释性有助于识别一些特定群体（如年龄或种族群体）可能存在的模型偏差。对于深度学习模型，如果我们想要得到全局解释，通常需要使用替代模型（如决策树模型）来模拟深度学习模型的行为（[https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a](https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a)）。然而，这种方法并不总是奏效，因为很难判断替代模型是否足够准确地逼近原始黑箱模型的预测。因此，在实际操作中，深度学习模型通常使用局部可解释性工具，如**SHapley加法解释**（**SHAP**），我们将在方法维度中对此进行解释。
- en: '**Input data format: what is the format of the input data**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**输入数据格式：什么是输入数据的格式**'
- en: '**Input data format** refers to what kind of input data we are dealing with
    when developing and using the model. While a simple model might only focus on
    a single type of input data format such as text, many complex models might require
    using a mix of structured tabular data plus unstructured data such as images or
    texts. In addition, there is also a separate need to understand the input data
    hidden bias (during model training and validation) or drifting (during production).
    As such, this is quite a complex topic. The data explanation can also be used
    for monitoring data outliers and drifting during production. This is applicable
    to all types of ML/DL models.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入数据格式**指的是在开发和使用模型时，我们处理的输入数据类型。一个简单的模型可能只关注单一类型的输入数据格式，例如文本，而许多复杂的模型可能需要结合结构化的表格数据和非结构化数据，如图像或文本。此外，还需要单独理解输入数据的潜在偏差（在模型训练和验证期间）或漂移（在生产环境中）。因此，这是一个相当复杂的话题。数据解释也可以用于监控生产环境中的数据异常和漂移。这适用于所有类型的机器学习/深度学习模型。'
- en: '**Output data format: what is the format of the output explanation**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**输出数据格式：什么是输出解释的格式**'
- en: '**Output explanation format** refers to how we present the explanations to
    our target audience. Often, an image explanation might be a bar chart showing
    the feature importance with the top few features and their scores, or a saliency
    map that highlights the spatial support of a particular class in each image for
    image-related ML problems. For a textual output, it could be an English sentence
    to say why a credit application is rejected because of a few factors that are
    understandable to the applicants. **Natural language processing (NLP)** model
    explainability could also be through interactive exploration that uses salience
    maps, attention, and other rich visualization (see examples in Google''s **Language
    Interpretability Tool** (**LIT**): [https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html](https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html)).
    As there is no silver bullet for explainability of these complex output formats,
    it is critical to meet the needs, experiences, and expectations of the audience
    that asks for the explanation.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出解释格式**指的是我们如何向目标受众呈现解释。通常，图像解释可能是一个条形图，显示特征的重要性及其得分，或者是一个显著性图，突出显示每个图像中某一特定类别的空间支持，适用于与图像相关的机器学习问题。对于文本输出，它可能是一个英语句子，解释为何某个信用申请因几个可理解的因素而被拒绝。**自然语言处理（NLP）**模型的可解释性也可以通过交互式探索实现，使用显著性图、注意力机制以及其他丰富的可视化手段（参见Google的**语言可解释性工具**（**LIT**）示例：[https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html](https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html)）。由于没有一劳永逸的解释方法适用于这些复杂的输出格式，因此，满足受众对解释的需求、经验和期望是至关重要的。'
- en: '**Problem type: what is the machine learning problem type**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题类型：什么是机器学习问题类型**'
- en: '**Problem type** refers to all kinds of ML/AI problems broadly, but for practical
    purposes, current commercially successful problems are mostly around classification,
    regression, and clustering. Reinforcement learning and recommendation systems
    also see increasingly successful adoption in the industry. DL models are now often
    used in all these types of problems or are at least being evaluated as a potential
    candidate model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题类型**广泛地指代所有种类的机器学习/人工智能问题，但就实际应用而言，目前商业上成功的主要问题大多集中在分类、回归和聚类。强化学习和推荐系统在行业中的应用也越来越成功。深度学习模型现在常常应用于所有这些类型的问题，或者至少正在被评估作为潜在的候选模型。'
- en: '**Objectives type: what is the motivation or goal to explain**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**目标类型：解释的动机或目标是什么**'
- en: '**Objectives type** refers to the motivation of using explainability in AI/ML
    projects. It has been argued that the number one objective of explainability is
    to gain trust by providing a sufficient understanding of the AI system behavior
    and uncovering vulnerabilities, biases, and flaws of the system. An additional
    motivation is to infer the causal relationship from the input and output prediction.
    Other objectives include improving the model accuracy through a better understanding
    of the inner workings of the AI/ML systems, and justifying the model behavior
    and decisions through transparent explanations when potentially severe consequences
    are involved. It is even possible to reveal unknown insights and rules that are
    based on explanations (https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465).
    Overall, it is very desirable to break the black box so that when being used in
    a real production system, the AI/ML models and systems can be used with confidence.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标类型**指的是在人工智能/机器学习项目中使用可解释性的动机。人们认为，可解释性的首要目标是通过提供足够的理解来获得信任，揭示AI系统行为中的脆弱性、偏差和缺陷。另一个动机是从输入和输出预测中推断因果关系。其他目标包括通过更好地理解AI/ML系统的内部工作原理来提高模型的准确性，以及在可能涉及严重后果时，通过透明的解释来为模型行为和决策提供正当理由。甚至可能通过解释揭示基于解释的未知见解和规则（[https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465](https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465)）。总的来说，打破黑箱非常重要，以便在真实的生产系统中使用AI/ML模型和系统时，能够以信心进行使用。'
- en: 'Method type: what is the specific post-hoc explanation method used'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法类型：使用的具体后验解释方法是什么
- en: '**Method type (post-hoc)** refers to post-hoc methods that are very relevant
    to the DL models. There are two major categories of post-hoc methods: perturbation-based
    and gradient-based. Recent work has started to unify these two approaches, although
    it is not yet widely applicable for practical usage ( [https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based](https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based)).
    The following is a brief discussion on these two types of methods:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法类型（后验分析）**指的是与深度学习模型密切相关的后验分析方法。后验分析方法主要分为两类：基于扰动的方法和基于梯度的方法。最近的研究开始尝试将这两种方法统一起来，尽管这种统一方法尚未广泛应用于实际中（[https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based](https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based)）。以下是对这两种方法的简要讨论：'
- en: Perturbation-based methods leverage perturbations of individual instances to
    construct interpretable local approximations using linear models to explain the
    predictions. The most popular perturbation-based methods include **Local Interpretable
    Model-Agnostic Explanations** (**LIME**), ([https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf)),
    SHAP, and variants of LIME and SHAP such as BayesLIME and BayesSHAP, TreeSHAP,
    and many more ([https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df](https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df)).
    LIME can be used for tabular, image, and textual input data and is model agnostic.
    That's to say, LIME can be used for any type of classifiers (tree-based or DL
    models) regardless of the algorithms being used. SHAP uses principles from cooperative
    game theory to identify the contribution of different features to the prediction
    in order to quantify the impact of each feature. SHAP produces a so-called shapely
    value, which is the average of all the marginal contributions to all possible
    coalitions or combinations of different features. It works well for many types
    of models, including DL models, although the computational time could be much
    faster for tree-based models such as XGBoost or LightGBM ([https://github.com/slundberg/shap](https://github.com/slundberg/shap)).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于扰动的方法利用对单个实例的扰动来构建可解释的局部近似模型，使用线性模型来解释预测结果。最受欢迎的基于扰动的方法包括**局部可解释模型无关解释方法**（**LIME**），([https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf))，SHAP，以及LIME和SHAP的变种，如BayesLIME、BayesSHAP、TreeSHAP等
    ([https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df](https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df))。LIME可以用于表格数据、图像和文本输入数据，并且是模型无关的。这意味着，LIME可以用于任何类型的分类器（无论是基于树的模型还是深度学习模型），不受所使用算法的限制。SHAP使用来自合作博弈论的原理，确定不同特征对预测的贡献，从而量化每个特征的影响。SHAP生成所谓的Shapley值，它是所有可能的联盟或特征组合的边际贡献的平均值。它适用于多种类型的模型，包括深度学习模型，尽管对于基于树的模型，如XGBoost或LightGBM，计算时间通常会更快
    ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))。
- en: Gradient-based methods, such as SmoothGrad ([https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825))
    and Integrated Gradients ([https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf](https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf)),
    leverages gradients computed with respect to input dimensions of individual instances
    to explain model predictions. They can be applied to both image and textual input
    data, although sometimes, textual input could suffer a manipulation or adversary
    attack ([https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269](https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269)),
    which will change the feature importance undesirably.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于梯度的方法，如SmoothGrad ([https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825))
    和集成梯度 ([https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf](https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf))，通过计算相对于输入维度的梯度来解释模型预测。它们可以应用于图像和文本输入数据，尽管有时文本输入可能会受到操控或对抗性攻击的影响
    ([https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269](https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269))，这将不希望地改变特征重要性。
- en: Note that there are additional types of methods such as counterfactual (https://christophm.github.io/interpretable-ml-book/counterfactual.html)
    or prototype-based methods ([https://christophm.github.io/interpretable-ml-book/proto.html](https://christophm.github.io/interpretable-ml-book/proto.html)),
    which we will not cover in this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，还有一些其他类型的方法，比如反事实方法 ([https://christophm.github.io/interpretable-ml-book/counterfactual.html](https://christophm.github.io/interpretable-ml-book/counterfactual.html))
    或基于原型的方法 ([https://christophm.github.io/interpretable-ml-book/proto.html](https://christophm.github.io/interpretable-ml-book/proto.html))，这些方法我们在本书中不会涉及。
- en: 'Having discussed the many dimensions of explainability, it is important to
    know that XAI is still an emerging area ([https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf](https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf))
    and it is sometimes even difficult to find agreement among different explainability
    methods when applying to the same dataset or models (see a recent study on the
    topic of disagreement problems in explainable ML from the practitioners'' perspective:
    [https://arxiv.org/abs/2202.01602](https://arxiv.org/abs/2202.01602)). In the
    end, it does require some experimentation to find out which explainability provides
    the human validated explanations that are meeting the requirements for a specific
    prediction task in the real world.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了可解释性的多个维度后，重要的是要知道XAI（可解释人工智能）仍然是一个新兴领域（[https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf](https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf)），有时甚至很难在应用到同一数据集或模型时找到不同可解释性方法之间的一致性（请参见关于可解释机器学习中的分歧问题的最新研究，站在从业者的角度：[https://arxiv.org/abs/2202.01602](https://arxiv.org/abs/2202.01602)）。最终，确实需要进行一些实验，才能找出哪些可解释性方法提供了经过人工验证的解释，并满足现实世界中某个特定预测任务的需求。
- en: In the next two sections of this chapter, we will focus on providing some hands-on
    experiments using some popular and emerging toolkits to learn how to do explainability.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的接下来的两个部分中，我们将重点介绍一些流行的、正在兴起的工具包，并通过一些实践实验来学习如何进行可解释性分析。
- en: Exploring the SHAP Explainability toolbox
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索SHAP可解释性工具箱
- en: 'For our learning purpose, let''s review some popular explainability toolboxes
    while experimenting with some examples. Based on the number of GitHub stars (16,000
    as of April 2022, [https://github.com/slundberg/shap](https://github.com/slundberg/shap)),
    SHAP is the most widely used and integrated open source model explainability toolbox.
    It is also the foundation explanation tool that is integrated with MLflow. Here,
    we would like to run a small experiment to get some hands-on experience on how
    this works. Let''s use a sentimental analysis NLP model to explore how SHAP can
    be used for explaining the model behavior:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的学习目的，让我们在实验一些示例时回顾一些流行的可解释性工具箱。根据GitHub的星标数量（截至2022年4月为16,000个，[https://github.com/slundberg/shap](https://github.com/slundberg/shap)），SHAP是最广泛使用和集成的开源模型可解释性工具箱。它也是与MLflow集成的基础解释工具。在这里，我们将进行一个小实验，亲身体验这种工具是如何工作的。让我们使用一个情感分析NLP模型，探索SHAP如何用于解释模型的行为：
- en: 'Set up the virtual environment on your local environment after checking out
    this chapter''s code from GitHub. Running the following command will create a
    new virtual environment called `dl-explain`:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在从GitHub检查本章的代码后，在本地环境中设置虚拟环境。运行以下命令将创建一个名为`dl-explain`的新虚拟环境：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will install SHAP and its related dependencies such as `matplotlib` in
    this virtual environment. Once this virtual environment is created, activate this
    virtual environment by running the following command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装SHAP及其相关依赖项，例如`matplotlib`，并将它们添加到该虚拟环境中。创建此虚拟环境后，通过运行以下命令来激活该虚拟环境：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, we are ready to run the experiment with SHAP.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好使用SHAP进行实验了。
- en: 'You can check out the `shap_explain.ipynb` notebook to follow through with
    the exploration. The first step in this notebook is to import the relevant Python
    libraries:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以查看`shap_explain.ipynb`笔记本，跟随其中的实验。该笔记本的第一步是导入相关的Python库：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These imports will allow us to use the Hugging Face transformers pipeline API
    to get a pre-trained NLP model and SHAP functions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导入将允许我们使用Hugging Face的transformers管道API，获取一个预训练的NLP模型并使用SHAP函数。
- en: 'We then create `dl_model` using the transformers pipeline API for `sentiment_analysis`.
    Note this is a pretrained pipeline so we can use this without additional finetuning.
    The default transformer model used in this pipeline is `distilbert-base-uncased-finetuned-sst-2-english`
    (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english):'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用transformers管道API创建`dl_model`来进行`sentiment_analysis`。请注意，这是一个预训练的管道，因此我们可以在不进行额外微调的情况下使用它。该管道中使用的默认转换器模型是`distilbert-base-uncased-finetuned-sst-2-english`（https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english）：
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will produce a model ready to predict positive or negative sentiment for
    an input sentence.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个准备好预测输入句子情感（正面或负面）的模型。
- en: 'Try this `dl_model` with two input sentences and see whether the output makes
    sense:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试用两个输入句子测试这个`dl_model`，看看输出是否有意义：
- en: '[PRE4]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will produce an output of the labels and probability scores for each sentence
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出每句话的标签和概率分数，如下所示：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It seems that the first sentence was predicted with a high probability to be
    `POSITIVE`, and the second sentence was predicted with a high probability to be
    `NEGATIVE`. Now, if we take a deep look at the first sentence, we may think the
    model prediction was incorrect, as there is a subtle negative emotion in the second
    part of the sentence (`no taste`). So, we want to know why the model made such
    a prediction. This is where model explainability comes into play.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来第一句话被预测为`POSITIVE`的概率很高，而第二句话则被预测为`NEGATIVE`的概率很高。现在，如果我们仔细观察第一句话，我们可能会认为模型的预测是错误的，因为句子的后半部分（`no
    taste`）带有微妙的负面情绪。因此，我们想知道模型为何做出这样的预测。这就是模型可解释性发挥作用的地方。
- en: 'Now, let''s use the SHAP API, `shap.Explainer`, to get the Shapley values for
    the two sentences we are interested in explaining:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用SHAP API，`shap.Explainer`，获取我们感兴趣的两句话的Shapley值：
- en: '[PRE6]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we have `shap_values`, we can visualize the Shapley values using different
    visualization techniques. The first one is to use `shap.plot.text` to visualize
    the first sentence''s Shapley values when the prediction label is `POSITIVE`:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了`shap_values`，就可以使用不同的可视化技术来展示Shapley值。第一种方法是使用`shap.plot.text`来可视化第一句话的Shapley值，当预测标签为`POSITIVE`时：
- en: '[PRE7]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will produce the plot as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下图表：
- en: '![Figure 9.2 – SHAP visualization for sentence 1 with a positive prediction'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 句子 1 的 SHAP 可视化，带有正向预测'
- en: '](img/B18120_09_002.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_002.jpg)'
- en: Figure 9.2 – SHAP visualization for sentence 1 with a positive prediction
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 句子 1 的 SHAP 可视化，带有正向预测
- en: 'As can be seen in *Figure 9.2*, the word `great` has a very large SHAP value
    that dominates the influence of the final prediction, while the word `no` has
    less effect on the final prediction. This results in the final prediction result
    of `POSITIVE`. So, what about the second sentence with a `NEGATIVE` prediction?
    Running the following command will produce a similar plot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 9.2*所示，词语`great`的SHAP值非常大，主导了最终预测的影响，而词语`no`对最终预测的影响较小。这导致了最终的`POSITIVE`预测结果。那么，第二句话带有`NEGATIVE`预测的情况如何呢？运行以下命令将生成一个类似的图表：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This command creates the following plot:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将创建如下图表：
- en: '![Figure 9.3 – SHAP visualization for sentence 2 with a negative prediction'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – 句子 2 的 SHAP 可视化，带有负向预测'
- en: '](img/B18120_09_003.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_003.jpg)'
- en: Figure 9.3 – SHAP visualization for sentence 2 with a negative prediction
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 句子 2 的 SHAP 可视化，带有负向预测
- en: As can be seen from *Figure 9.3*, the word `Not` has a strong influence on the
    final prediction, while the word `good` has a very small influence, resulting
    in the final prediction of a `NEGATIVE` sentiment. This makes a lot of sense,
    which is a good explanation of the model's behavior.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 9.3*所示，词语`Not`对最终预测有很强的影响，而词语`good`的影响非常小，导致最终预测为`NEGATIVE`情感。这是非常有道理的，对模型行为的解释很到位。
- en: 'We can also visualize `shap_values` using different plots. A common one is
    the bar plot, which plots the feature contribution to the final prediction. Running
    the following command will produce a plot for the first sentence:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用不同的图表来可视化`shap_values`。一种常见的方式是条形图，它显示特征对最终预测的贡献。运行以下命令将为第一句话生成一个图表：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will produce a bar chart as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下的条形图：
- en: '![Figure 9.4 – SHAP bar chart for sentence 1 with a positive prediction'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – 句子 1 的 SHAP 条形图，带有正向预测'
- en: '](img/B18120_09_004.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_004.jpg)'
- en: Figure 9.4 – SHAP bar chart for sentence 1 with a positive prediction
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 句子 1 的 SHAP 条形图，带有正向预测
- en: As can be seen from *Figure 9.4*, the chart ranks the most important features
    from top to bottom, where the top ones with a positive influence on the final
    prediction are plotted on the positive side of the *x* axis, while the negative
    contribution is plotted on the negative side of the *x* axis. The *x* axis is
    the value of each token or word's SHAP value with a sign (+ or -). This clearly
    shows the word `great` is a strong positive factor that impacts the final prediction,
    while `have no taste` has some negative effect but not enough to change the direction
    of the final prediction.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 9.4*中可以看出，该图表按重要性对特征进行了从上到下的排名，其中对最终预测有正面影响的特征绘制在*x*轴的正侧，而负面贡献则绘制在*x*轴的负侧。*x*轴表示每个标记或单词的
    SHAP 值，并带有符号（+ 或 -）。这清楚地表明，单词`great`是一个强正面因素，影响了最终预测，而`have no taste`有一定的负面影响，但不足以改变最终预测的方向。
- en: 'Similarly, we can plot a bar chart for the second sentence as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以为第二个句子绘制如下的条形图：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will produce the following bar chart:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下的条形图：
- en: '![Figure 9.5 – SHAP bar chart for sentence 2 with a negative prediction'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 带有负面预测的第二个句子的 SHAP 条形图](img/B18120_09_005.jpg)'
- en: '](img/B18120_09_005.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_005.jpg)'
- en: Figure 9.5 – SHAP bar chart for sentence 2 with a negative prediction
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 带有负面预测的第二个句子的 SHAP 条形图
- en: As can be seen from *Figure 9.5*, the word `Not` has a strong contribution to
    the final prediction, while the word `good` is second. These two words have the
    opposite effect on the final prediction, but apparently, the word `Not` is much
    stronger and has a much larger SHAP value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 9.5*中可以看出，单词`Not`对最终预测有很强的贡献，而单词`good`排在第二位。这两个词对最终预测的影响是相反的，但显然，单词`Not`的影响要强得多，SHAP值也大得多。
- en: If you have followed along with this example and seen the SHAP charts in your
    notebook, congratulations! This means you have successfully run the SHAP Explainability
    tool to explain the DL transformer model for the NLP text sentiment analysis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经跟着这个例子走，并在你的笔记本中看到了 SHAP 图表，那么恭喜你！这意味着你已经成功地运行了 SHAP 可解释性工具，为 NLP 文本情感分析中的深度学习转换器模型提供了解释。
- en: Let's further explore another popular explainability tool to see how they perform
    different explanations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探索另一种流行的可解释性工具，看看它们如何提供不同的解释。
- en: Exploring the Transformers Interpret toolbox
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Transformers Interpret 工具箱
- en: 'As we already reviewed in the first section of this chapter, there are two
    major methods: perturbation-based and gradient-based post-hoc explainability tools.
    SHAP belongs to the perturbation-based family. Now, let''s look at a gradient-based
    toolbox called **Transformers Interpret** ([https://github.com/cdpierse/transformers-interpret](https://github.com/cdpierse/transformers-interpret)).
    This is a relatively new tool, but it is built on top of a unified model interpretability
    and understanding library for PyTorch called **Captum** ([https://github.com/pytorch/captum](https://github.com/pytorch/captum)),
    which provides a unified API to use either perturbation or gradient-based tools
    ([https://arxiv.org/abs/2009.07896](https://arxiv.org/abs/2009.07896)). Transformers
    Interpret further simplifies the API of Captum so that we can quickly explore
    gradient-based explainability methods to get some hands-on experience.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在本章的第一节中回顾的那样，有两种主要方法：基于扰动的和基于梯度的事后可解释性工具。SHAP属于基于扰动的方法家族。现在，让我们来看一个基于梯度的工具箱，名为**Transformers
    Interpret**（[https://github.com/cdpierse/transformers-interpret](https://github.com/cdpierse/transformers-interpret)）。这是一个相对较新的工具，但它建立在一个名为**Captum**（[https://github.com/pytorch/captum](https://github.com/pytorch/captum)）的统一模型可解释性和理解库之上，该库为
    PyTorch 提供了统一的 API，可以使用基于扰动或梯度的工具（[https://arxiv.org/abs/2009.07896](https://arxiv.org/abs/2009.07896)）。Transformers
    Interpret 进一步简化了 Captum 的 API，使我们能够快速探索基于梯度的可解释性方法，从而获得实践经验。
- en: 'To get started, first make sure you already have the `dl-explain` virtual environment
    set up and activated, as described in the previous section. Then, we can use the
    same Hugging Face transformer sentiment analysis model to explore some NLP sentiment
    classification examples. Then, we can perform the following steps to learn how
    to use Transformers Interpret to do the model explanation. You may want to check
    out the `gradient_explain.ipynb` notebook to follow the instructions:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保您已经根据前一节的描述，设置并激活了`dl-explain`虚拟环境。然后，我们可以使用相同的 Hugging Face 转换器情感分析模型，探索一些
    NLP 情感分类示例。接下来，我们可以执行以下步骤，学习如何使用 Transformers Interpret 进行模型解释。您可能想查看`gradient_explain.ipynb`笔记本，以跟随说明操作：
- en: 'Import relevant packages into the notebook as follows:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式将相关包导入笔记本：
- en: '[PRE11]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will use Hugging Face's transformer model and tokenizer, as well as the
    explainability function from `transformers_interpret`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用 Hugging Face 的 transformer 模型和分词器，以及来自`transformers_interpret`的可解释性功能。
- en: 'Create the model and the tokenizer using the same pre-trained model as previous
    section, which is the `distilbert-base-uncased-finetuned-sst-2-english` model:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与前一节相同的预训练模型创建模型和分词器，该模型是`distilbert-base-uncased-finetuned-sst-2-english`模型：
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have the model and tokenizer, we can create an explainability variable
    using the `SequenceClassificationExplainer` API.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了模型和分词器，我们可以使用`SequenceClassificationExplainer` API创建一个可解释性变量。
- en: 'Create an explainer and give an example sentence to get the `word` attribution
    from the explainer:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个解释器并给出示例句子，以获取来自解释器的`word`归因：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can also get the prediction label before we check the `word` attributions
    by running the following command:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以在检查`word`归因之前，通过运行以下命令获取预测标签：
- en: '[PRE14]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will produce a result of `Negative`, which means the prediction is a negative
    sentiment. So, let's see how the explainer provides an explanation for this prediction.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生`Negative`的结果，意味着预测为负面情绪。那么，让我们看看解释器是如何为这个预测提供解释的。
- en: 'We can just display the `word_attributions` value, or we can visualize it.
    The value of `word_attributions` is as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以仅显示`word_attributions`值，或者我们可以将其可视化。`word_attributions`的值如下：
- en: '![Figure 9.6 – Layered integrated gradient word attribution values with a negative
    prediction'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6 – 使用分层集成梯度的词归因值，结果为负面预测'
- en: '](img/B18120_09_006.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_006.jpg)'
- en: Figure 9.6 – Layered integrated gradient word attribution values with a negative
    prediction
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 使用分层集成梯度的词归因值，结果为负面预测
- en: 'As can be seen from *Figure 9.6*, using the layered integrated gradient method,
    which is the current explainer''s default method implemented in the Transformers
    Interpret library, the word `not` contributed positively to the final prediction
    result, which is a negative sentiment. This makes sense. Notice that several other
    words, such as `to spend time on`, also have a strong positive influence on the
    final prediction. Given the cross-attention mechanism, it seems the model is trying
    to extract `not to spend time on` as the main attribution to the final prediction.
    Note we can also visualize these `word` attributions as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 9.6*可以看出，使用分层集成梯度方法，这是当前解释器在 Transformers Interpret 库中实现的默认方法，`not`这个词对最终的预测结果产生了积极的影响，最终的预测是负面情绪。这是有道理的。请注意，其他一些词，如`to
    spend time on`，对最终预测也有较强的积极影响。考虑到交叉注意力机制，似乎模型正试图提取`not to spend time on`作为最终预测的主要归因。请注意，我们还可以像下面这样可视化这些`word`归因：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will produce the follow plot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![Figure 9.7 – Layered integrated gradient word attribution values with a negative
    prediction'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 使用分层集成梯度的词归因值，结果为负面预测'
- en: '](img/B18120_09_007.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_09_007.jpg)'
- en: Figure 9.7 – Layered integrated gradient word attribution values with a negative
    prediction
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 使用分层集成梯度的词归因值，结果为负面预测
- en: As can be seen in *Figure 9.7*, it highlights the word importance of `not to
    spend time on` to positively impact the final negative prediction.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 9.7*可以看出，它突出了`not to spend time on`这个词对最终负面预测结果的积极影响。
- en: Now that we have experimented with both perturbation and gradient-based explainability
    methods, we have successfully completed our hands-on exploration of using the
    explainability tool for post-hoc local explanation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实验了基于扰动和梯度的可解释性方法，我们已成功完成了使用可解释性工具进行事后本地解释的动手探索。
- en: Next, we will summarize what we learned in this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将总结在本章中学到的内容。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we reviewed explainability in AI/ML through an eight-dimension
    categorization. Although this is not necessarily a comprehensive or exhaustive
    overview, this does give us a big picture of who to explain to, different stages
    and scopes to explain, various kinds of input and output formats of the explanation,
    common ML problems and objectives types, and finally, different post-hoc explainability
    methods. We then provided two concrete exercises to explore the SHAP and Transformers
    Interpret toolboxes, which can provide perturbation and gradient-based feature
    attribution explanations for NLP text sentiment DL models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过八维分类回顾了AI/ML中的可解释性。虽然这不一定是一个全面或详尽的概述，但它为我们提供了一个大致的框架，包括谁需要解释、不同阶段和范围的解释、各种输入输出格式的解释、常见的ML问题和目标类型，以及最后的不同后验可解释性方法。接着，我们提供了两个具体的练习来探索SHAP和Transformers可解释工具箱，它们可以为NLP文本情感DL模型提供扰动和基于梯度的特征归因解释。
- en: This gives us a solid foundation for using explainability tools for DL models.
    However, given the active development of XAI, this is only the beginning of using
    XAI in DL models. Additional explainability toolboxes such as TruLens ([https://github.com/truera/trulens](https://github.com/truera/trulens)),
    Alibi ([https://github.com/SeldonIO/alibi](https://github.com/SeldonIO/alibi)),
    Microsoft Responsible AI Toolbox ([https://github.com/microsoft/responsible-ai-toolbox](https://github.com/microsoft/responsible-ai-toolbox)),
    and IBM AI Explainability 360 Toolkit ([https://github.com/Trusted-AI/AIX360](https://github.com/Trusted-AI/AIX360))
    are all in active development and worthy of investigation and future learning.
    Additional links are also provided in the *Further reading* section to help you
    continue to learn this topic.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们使用DL模型的可解释性工具奠定了坚实的基础。然而，考虑到XAI的积极发展，这仅仅是将XAI应用于DL模型的开始。其他可解释性工具箱，如TruLens（[https://github.com/truera/trulens](https://github.com/truera/trulens)）、Alibi（[https://github.com/SeldonIO/alibi](https://github.com/SeldonIO/alibi)）、微软的负责任AI工具箱（[https://github.com/microsoft/responsible-ai-toolbox](https://github.com/microsoft/responsible-ai-toolbox)）和IBM的AI可解释性360工具包（[https://github.com/Trusted-AI/AIX360](https://github.com/Trusted-AI/AIX360)）都在积极开发中，值得进一步研究和学习。*深入阅读*部分还提供了额外的链接，帮助你继续学习这一主题。
- en: Now that we know the fundamentals of explainability, in the next chapter, we
    will learn how to implement explainability in the MLflow framework so that we
    can provide a unified way to support explanation within the MLflow framework.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了可解释性的基础知识，在下一章中，我们将学习如何在MLflow框架中实现可解释性，从而为在MLflow框架内提供统一的解释方式。
- en: Further reading
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '*New frontiers in Explainable AI*: [https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348](https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释AI的新前沿*：[https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348](https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348)'
- en: '*Towards a Rigorous Science of Interpretable Machine Learning*: [https://arxiv.org/pdf/1702.08608.pdf](https://arxiv.org/pdf/1702.08608.pdf)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*走向严谨的可解释机器学习科学*：[https://arxiv.org/pdf/1702.08608.pdf](https://arxiv.org/pdf/1702.08608.pdf)'
- en: '*The Toolkit Approach to Trustworthy AI*: [https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/](https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*值得信赖的AI工具包方法*：[https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/](https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/)'
- en: '*A Framework for Learning Ante-hoc Explainable Models via Concepts*: [https://arxiv.org/abs/2108.11761](https://arxiv.org/abs/2108.11761)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过概念学习Ante-hoc可解释模型的框架*：[https://arxiv.org/abs/2108.11761](https://arxiv.org/abs/2108.11761)'
- en: '*Demystifying Post-hoc Explainability for ML models*: [https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability](https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解密ML模型的后验可解释性*：[https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability](https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability)'
- en: '*A Look Into Global, Cohort and Local Model Explainability*: [https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f](https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全球、群体和局部模型可解释性探讨*：[https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f](https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f)'
- en: '*What Are the Prevailing Explainability Methods?* [https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df](https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当前主流的可解释性方法是什么？* [https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df](https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df)'
- en: '*Explainable Artificial Intelligence: Objectives, Stakeholders, and Future
    Research Opportunities*: [https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465](https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释人工智能：目标、利益相关者与未来研究机会*：[https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465](https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465)'
