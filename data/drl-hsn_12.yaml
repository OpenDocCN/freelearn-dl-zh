- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: 'Actor-Critic Method: A2C and A3C'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论员方法：A2C和A3C
- en: In Chapter [11](ch015.xhtml#x1-18200011), we started to investigate a policy-based
    alternative to the familiar value-based methods family. In particular, we focused
    on the method called REINFORCE and its modification, which uses discounted reward
    to obtain the gradient of the policy (which gives us the direction in which to
    improve the policy). Both methods worked well for a small CartPole problem, but
    for a more complicated Pong environment, we got no convergence.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[11](ch015.xhtml#x1-18200011)章中，我们开始研究一种基于策略的方法，作为传统值基方法的替代方案。特别地，我们重点关注了名为REINFORCE的方法及其修改版，该方法使用折扣奖励来获得策略的梯度（该梯度告诉我们改善策略的方向）。这两种方法在小型的CartPole问题上表现良好，但在更复杂的Pong环境中，我们没有得到收敛。
- en: Here, we will discuss another extension to the vanilla policy gradient method,
    which magically improves the stability and convergence speed of that method. Despite
    the modification being only minor, the new method has its own name, actor-critic,
    and it’s one of the most powerful methods in deep reinforcement learning (RL).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论另一种对普通策略梯度方法的扩展，它神奇地改善了该方法的稳定性和收敛速度。尽管这种修改只是微小的，但新方法有了自己的名字——演员-评论员，它是深度强化学习（RL）中最强大的方法之一。
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Explore how the baseline impacts statistics and the convergence of gradients
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索基准方法如何影响统计数据和梯度的收敛性
- en: Cover an extension of the baseline idea
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展基准方法的概念
- en: Implement the advantage actor-critic (A2C) method and check it on the Pong environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现优势演员评论员（A2C）方法，并在Pong环境中进行测试
- en: 'Add asynchronous execution to the A2C method using two different ways: data
    parallelism and gradient parallelism'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两种不同的方法：数据并行和梯度并行，为A2C方法增加异步执行
- en: Variance reduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差减少
- en: 'In the previous chapter, I briefly mentioned that one of the ways to improve
    the stability of policy gradient methods is to reduce the variance of the gradient.
    Now let’s try to understand why this is important and what it means to reduce
    the variance. In statistics, variance is the expected square deviation of a random
    variable from the expected value of that variable:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我简要提到过，改善策略梯度方法稳定性的一种方式是减少梯度的方差。现在让我们尝试理解为什么这很重要，以及减少方差意味着什么。在统计学中，方差是随机变量与该变量的期望值之间的平方偏差的期望值：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq44.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq44.png)'
- en: Variance shows us how far values are dispersed from the mean. When variance
    is high, the random variable can take values that deviate widely from the mean.
    In the following plot, there is a normal (Gaussian) distribution with the same
    value for the mean, μ = 10, but with different values for the variance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 方差展示了数值与均值之间的分散程度。当方差较高时，随机变量可能会取到与均值相差较大的值。在下图中，存在一个均值为μ = 10的正态（高斯）分布，但其方差值不同。
- en: '![PIC](img/B22150_12_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_01.png)'
- en: 'Figure 12.1: The effect of variance on Gaussian distribution'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：方差对高斯分布的影响
- en: Now let’s return to policy gradients. It was stated in the previous chapter
    that the idea is to increase the probability of good actions and decrease the
    chance of bad ones. In math notation, our policy gradient was written as ∇J ≈𝔼[Q(s,a)∇log
    π(a|s)]. The scaling factor Q(s,a) specifies how much we want to increase or decrease
    the probability of the action taken in the particular state. In the REINFORCE
    method, we used the discounted total reward as the scaling of the gradient. In
    an attempt to increase REINFORCE stability, we subtracted the mean reward from
    the gradient scale.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到策略梯度。前一章中提到过，策略梯度的核心思想是提高良好动作的概率并降低不良动作的概率。在数学表示中，我们的策略梯度被写为∇J ≈𝔼[Q(s,a)∇log
    π(a|s)]。缩放因子Q(s,a)指定了我们希望在特定状态下增加或减少动作概率的多少。在REINFORCE方法中，我们使用折扣总奖励作为梯度的缩放因子。为了提高REINFORCE的稳定性，我们从梯度的缩放因子中减去了平均奖励。
- en: 'To understand why this helped, let’s consider the very simple scenario of an
    optimization step on which we have three actions with different total discounted
    rewards: Q[1], Q[2], and Q[3]. Now let’s check what will happen with policy gradients
    with regard to the relative values of those Q[s].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么这有帮助，我们来看一个非常简单的优化步骤场景，在这个场景中，我们有三种动作，它们的总折扣奖励不同：Q[1]、Q[2]和Q[3]。现在让我们检查在关于这些Q[s]的相对值的情况下，策略梯度会发生什么。
- en: As the first example, let both Q[1] and Q[2] be equal to some small positive
    number and Q[3] be a large negative number. So, actions at the first and second
    steps led to some small reward, but the third step was not very successful. The
    resulting combined gradient for all three steps will try to push our policy far
    from the action at step three and slightly toward the actions taken at steps one
    and two, which is a totally reasonable thing to do.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个例子，假设Q[1]和Q[2]都等于某个小的正数，而Q[3]是一个较大的负数。因此，第一步和第二步的行动获得了一些小的奖励，但第三步的结果并不理想。所有三步的综合梯度将尝试将我们的策略远离第三步的行动，并稍微推动它朝着第一步和第二步的行动方向发展，这完全是合理的做法。
- en: 'Now let’s imagine that our reward is always positive and only the value is
    different. This corresponds to adding some constant to each of the rewards from
    the previous example: Q[1], Q[2], and Q[3]. In this case, Q[1] and Q[2] will become
    large positive numbers and Q[3] will have a small positive value. However, our
    policy update will become different! We will try hard to push our policy toward
    actions at the first and second steps, and slightly push it toward an action at
    step three. So, strictly speaking, we are no longer trying to avoid the action
    taken for step three, despite the fact that the relative rewards are the same.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们假设我们的奖励始终为正且只有数值不同。这相当于在前一个例子中的每个奖励值上添加一个常量：Q[1]、Q[2]和Q[3]。在这种情况下，Q[1]和Q[2]将变为大的正数，而Q[3]将具有一个小的正值。然而，我们的策略更新将变得不同！我们将努力将策略推向第一步和第二步的行动，并稍微推向第三步的行动。因此，严格来说，尽管相对奖励相同，我们不再试图避免第三步的行动。
- en: This dependency of our policy update on the constant added to the reward can
    slow down our training significantly, as we may require many more samples to average
    out the effect of such a shift in the policy gradient. Even worse, as our total
    discounted reward changes over time, with the agent learning how to act better
    and better, our policy gradient variance can also change. For example, in the
    Atari Pong environment, the average reward in the beginning is −21… − 20, so all
    the actions look almost equally bad.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略更新依赖于添加到奖励中的常量，这可能会显著减缓训练进度，因为我们可能需要更多的样本来平滑这种策略梯度的变化。更糟的是，随着我们总的折扣奖励随时间变化，代理不断学习如何做得更好，我们的策略梯度方差也可能发生变化。例如，在Atari
    Pong环境中，开始时的平均奖励是−21...−20，因此所有的行动看起来几乎同样糟糕。
- en: 'To overcome this in the previous chapter, we subtracted the mean total reward
    from the Q-value and called this mean the baseline. This trick normalized our
    policy gradient: in the case of the average reward being −21, getting a reward
    of −20 looks like a win for the agent and it pushes its policy toward the taken
    actions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，在上一章中，我们从Q值中减去了总奖励的均值，并称这个均值为基准。这一技巧将我们的策略梯度归一化：例如，当平均奖励为−21时，获得−20的奖励看起来像是代理的胜利，这将推动其策略朝着采取的行动方向发展。
- en: CartPole variance
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CartPole方差
- en: 'To check this theoretical conclusion in practice, let’s plot our policy gradient
    variance during the training for both the baseline version and the version without
    the baseline. The complete example is in Chapter12/01_cartpole_pg.py, and most
    of the code is the same as in Chapter [11](ch015.xhtml#x1-18200011). The differences
    in this version are the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实践中验证这个理论结论，让我们绘制基准版本和不使用基准版本的训练过程中的策略梯度方差。完整示例位于Chapter12/01_cartpole_pg.py，且大部分代码与第[11章](ch015.xhtml#x1-18200011)相同。该版本的不同之处如下：
- en: It now accepts the command-line option --baseline, which enables the mean subtraction
    from the reward. By default, no baseline is used.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它现在接受命令行选项`--baseline`，启用从奖励中减去均值。默认情况下，不使用基准。
- en: On every training loop, we gather the gradients from the policy loss and use
    this data to calculate the variance.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个训练循环中，我们从策略损失中获取梯度，并使用这些数据来计算方差。
- en: 'To gather only the gradients from the policy loss and exclude the gradients
    from the entropy bonus added for exploration, we need to calculate the gradients
    in two stages. Luckily, PyTorch allows this to be done easily. In the following
    code, only the relevant part of the training loop is included to illustrate the
    idea:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了仅收集来自策略损失的梯度，并排除为了探索而添加的熵奖励的梯度，我们需要分两阶段计算梯度。幸运的是，PyTorch使得这一操作变得简单。以下代码中仅包含了训练循环的相关部分，用于说明这一思路：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We calculate the policy loss as before, by calculating the log from the probabilities
    of taken actions and multiplying it by policy scales (which are the total discounted
    reward if we are not using the baseline or the total reward minus the baseline).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样计算策略损失，通过计算已采取动作的概率的对数并将其乘以策略尺度（如果我们没有使用基准线，它是总折扣奖励，或者是总奖励减去基准线）。
- en: 'In the next step, we ask PyTorch to backpropagate the policy loss, calculating
    the gradients and keeping them in our model’s buffers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们请求 PyTorch 反向传播策略损失，计算梯度并将它们保存在模型的缓冲区中：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we have previously performed optimizer.zero_grad(), those buffers will contain
    only the gradients from the policy loss. One tricky thing here is the retain_graph=True
    option when we call backward(). It instructs PyTorch to keep the graph structure
    of the variables. Normally, this is destroyed by the backward() call, but in our
    case, this is not what we want. In general, retaining the graph could be useful
    when we need to backpropagate the loss multiple times before the call to the optimizer,
    although this is not a very common situation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们之前执行了 optimizer.zero_grad()，这些缓冲区将只包含来自策略损失的梯度。这里有一个棘手的地方是我们在调用 backward()
    时使用了 retain_graph=True 选项。它指示 PyTorch 保留变量的图结构。通常，调用 backward() 时会销毁图结构，但在我们的情况下，这不是我们想要的。一般来说，当我们需要在调用优化器之前多次反向传播损失时，保留图结构可能会很有用，尽管这不是一种非常常见的情况。
- en: 'Then, we iterate all parameters from our model (every parameter of our model
    is a tensor with gradients) and extract their grad field in a flattened NumPy
    array:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历模型中的所有参数（模型的每个参数都是一个包含梯度的张量），并将它们的 grad 字段提取到一个展平的 NumPy 数组中：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This gives us one long array with all gradients from our model’s variables.
    However, our parameter update should take into account not only the policy gradient
    but also the gradient provided by our entropy bonus. To achieve this, we calculate
    the entropy loss and call backward() again. To be able to do this the second time,
    we need to pass retain_graph=True.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给我们一个包含模型变量中所有梯度的长数组。然而，我们的参数更新不仅应该考虑策略梯度，还应考虑由熵奖励提供的梯度。为了实现这一点，我们计算熵损失并再次调用
    backward()。为了能够第二次执行这一操作，我们需要传递 retain_graph=True。
- en: 'On the second backward() call, PyTorch will backpropagate our entropy loss
    and add the gradients to the internal gradients’ buffers. So, what we now need
    to do is just ask our optimizer to perform the optimization step using those combined
    gradients:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次调用 backward() 时，PyTorch 将反向传播我们的熵损失，并将梯度添加到内部梯度缓冲区中。因此，我们现在需要做的就是请求优化器使用这些合并的梯度执行优化步骤：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Later, the only thing we need to do is write statistics that we are interested
    in into TensorBoard:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要做的唯一事情就是将我们感兴趣的统计数据写入 TensorBoard：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'By running this example twice, once with the --baseline command-line option
    and once without it, we get a plot of variance of our policy gradient. The following
    charts show the smoothed reward (average for last 100 episodes) and variance (smoothed
    with window 20):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行这个示例两次，一次使用 --baseline 命令行选项，一次不使用，我们可以得到策略梯度的方差图。以下图表显示了平滑的奖励（过去 100 集的平均值）和方差（使用窗口
    20 平滑）：
- en: '![PIC](img/B22150_12_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_02.png)'
- en: 'Figure 12.2: Smoothed reward (left) and variance (right)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：平滑奖励（左）和方差（右）
- en: 'These next two charts show the gradients’ magnitude (L2) and maximum value.
    All values are smoothed with window 20:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个图表显示了梯度的大小（L2 范数）和最大值。所有值都经过窗口 20 平滑处理：
- en: '![PIC](img/B22150_12_03.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_03.png)'
- en: 'Figure 12.3: Gradients’ L2 norm (left) and maximum value (right)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：梯度的 L2 范数（左）和最大值（右）
- en: As you can see, variance for the version with the baseline is two to three orders
    of magnitude lower than the version without one, which helps the system to converge
    faster.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，带有基准线的版本的方差比没有基准线的版本低两个到三个数量级，这有助于系统更快地收敛。
- en: Advantage actor-critic (A2C)
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势行为者-评论员（A2C）
- en: 'The next step in reducing the variance is making our baseline state-dependent
    (which is a good idea, as different states could have very different baselines).
    Indeed, to decide on the suitability of a particular action in some state, we
    use the discounted total reward of the action. However, the total reward itself
    could be represented as a value of the state plus the advantage of the action:
    Q(s,a) = V (s) + A(s,a). You saw this in Chapter [8](ch012.xhtml#x1-1240008),
    when we discussed DQN modifications, particularly dueling DQN.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 减少方差的下一步是使我们的基准状态依赖性（这是个好主意，因为不同的状态可能具有非常不同的基准）。实际上，为了决定某个状态下某个动作的适用性，我们使用该动作的折扣总奖励。然而，总奖励本身可以表示为状态的值加上动作的优势：Q(s,a)
    = V (s) + A(s,a)。你在第[8](ch012.xhtml#x1-1240008)章中见过这种方法，当时我们讨论了 DQN 的修改，特别是对抗
    DQN。
- en: 'So, why can’t we use V (s) as a baseline? In that case, the scale of our gradient
    will be just advantage, A(s,a), showing how this taken action is better in respect
    to the average state’s value. In fact, we can do this, and it is a very good idea
    for improving the policy gradient method. The only problem here is that we don’t
    know the value, V (s), of the state that we need to subtract from the discounted
    total reward, Q(s,a). To solve this, let’s use another neural network, which will
    approximate V (s) for every observation. To train it, we can exploit the same
    training procedure we used in DQN methods: we will carry out the Bellman step
    and then minimize the mean square error to improve V (s) approximation.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们不能用 V(s) 作为基准呢？在这种情况下，我们的梯度规模将只是优势 A(s,a)，表示此动作相对于平均状态值的改善。实际上，我们可以这样做，这对于改进策略梯度方法是一个非常好的主意。唯一的问题是我们不知道需要从折扣总奖励
    Q(s,a) 中减去的状态值 V(s)。为了解决这个问题，我们使用另一个神经网络，它将为每个观测值近似 V(s)。为了训练它，我们可以利用在 DQN 方法中使用的相同训练过程：我们将执行贝尔曼步骤，然后最小化均方误差来改进
    V(s) 的近似。
- en: 'When we know the value for any state (or at least have some approximation of
    it), we can use it to calculate the policy gradient and update our policy network
    to increase probabilities for actions with good advantage values and decrease
    the chance of actions with bad advantage values. The policy network (which returns
    a probability distribution of actions) is called the actor, as it tells us what
    to do. Another network is called critic, as it allows us to understand how good
    our actions were by returning V (s). This improvement is known under a separate
    name, the advantage actor-critic method, which is often abbreviated to A2C. Figure [12.4](#x1-206002r4)
    is an illustration of its architecture:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们知道任何状态的值（或至少有一些近似值）时，我们可以利用它来计算策略梯度，并更新我们的策略网络，以增加具有良好优势值的动作的概率，并减少具有不良优势值的动作的机会。策略网络（返回动作概率分布）被称为演员（actor），因为它告诉我们该做什么。另一个网络称为评论员（critic），因为它通过返回
    V(s) 让我们了解我们的动作有多好。这种改进有一个独立的名称，称为优势演员-评论员方法，通常缩写为 A2C。图[12.4](#x1-206002r4)是其架构的示意图：
- en: '![PVoalliuceynneett OπVbs((((eacrasrci|s)vtt)aoictr))ions ](img/B22150_12_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![PVoalliuceynneett OπVbs((((eacrasrci|s)vtt)aoictr))ions ](img/B22150_12_04.png)'
- en: 'Figure 12.4: The A2C architecture'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：A2C 架构
- en: In practice, the policy and value networks partially overlap, mostly due to
    efficiency and convergence considerations. In this case, the policy and value
    are implemented as different heads of the network, taking the output from the
    common body and transforming it into the probability distribution and a single
    number representing the value of the state.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，策略网络和值网络部分重叠，主要是出于效率和收敛性的考虑。在这种情况下，策略和值被实现为网络的不同“头部”，它们从共享的主体获取输出，并将其转化为概率分布和一个表示状态值的单一数字。
- en: 'This helps both networks to share low-level features (such as convolution filters
    in the Atari agent), but combine them in a different way. The following figure
    shows this architecture:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于两个网络共享低层次特征（例如 Atari 代理中的卷积滤波器），但以不同的方式将它们结合起来。下图展示了这种架构：
- en: '![CPVooamllmiuocenynneetntet OπVbs((((e(acrasrbci|s)vott)adoictyr))io)ns ](img/B22150_12_05.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![CPVooamllmiuocenynneetntet OπVbs((((e(acrasrbci|s)vott)adoictyr))io)ns ](img/B22150_12_05.png)'
- en: 'Figure 12.5: The A2C architecture with a shared network body'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：带有共享网络主体的 A2C 架构
- en: 'From a training point of view, we complete these steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练的角度来看，我们完成以下步骤：
- en: Initialize network parameters, 𝜃, with random values.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用随机值初始化网络参数，𝜃。
- en: Play N steps in the environment, using the current policy, π[𝜃], and saving
    the state, s[t], action, a[t], and reward, r[t].
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中执行 N 步，使用当前策略 π[𝜃]，并保存状态 s[t]、动作 a[t] 和奖励 r[t]。
- en: Set R ← 0 if the end of the episode is reached or V [𝜃](s[t]).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果到达回合结束或 V [𝜃](s[t])，则设置 R ← 0。
- en: 'For i = t − 1…t[start] (note that steps are processed backward):'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 i = t − 1…t[start]（注意步骤是逆向处理的）：
- en: R ←r[i] + γR
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: R ←r[i] + γR
- en: 'Accumulate the policy gradients:'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积策略梯度：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq45.png)'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq45.png)'
- en: 'Accumulate the value gradients:'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积值梯度：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq46.png)'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq46.png)'
- en: Update the network parameters using the accumulated gradients, moving in the
    direction of the policy gradients, ∂𝜃[π], and in the opposite direction of the
    value gradients, ∂𝜃[v].
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用累积的梯度更新网络参数，沿着策略梯度 ∂𝜃[π] 的方向移动，反方向则是值梯度 ∂𝜃[v]。
- en: Repeat from step 2 until convergence is reached.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第 2 步开始重复，直到收敛。
- en: 'This algorithm is just an outline and similar to those that are usually printed
    in research papers. In practice, several extensions to improve the stability of
    the method may be used:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法只是一个大致的框架，类似于通常在研究论文中打印的内容。实际上，可能会使用一些扩展方法来提高该方法的稳定性：
- en: 'An entropy bonus is usually added to improve exploration. It’s typically written
    as an entropy value added to the loss function:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常会添加一个熵奖励来改善探索。这通常表现为一个熵值，添加到损失函数中：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq47.png)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq47.png)'
- en: This function has a minimum when the probability distribution is uniform, so
    by adding it to the loss function, we push our agent away from being too certain
    about its actions. The value of β is a hyperparameter scaling the entropy bonus
    and prioritizing the exploration during the training. Normally, it is constant
    or linearly decreased during the training.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当概率分布是均匀时，这个函数有一个最小值，因此通过将其添加到损失函数中，我们可以让智能体避免对自己的动作过于确定。β 的值是一个超参数，用来缩放熵奖励并在训练过程中优先进行探索。通常情况下，它是常数或在训练过程中线性递减的。
- en: 'Gradient accumulation is usually implemented as a loss function combining all
    three components: policy loss, value loss, and entropy loss. You should be careful
    with signs of these losses, as policy gradients show you the direction of policy
    improvement, but both the value and entropy losses should be minimized.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度累积通常作为一个损失函数实现，结合了三个部分：策略损失、值损失和熵损失。你应该注意这些损失的符号，因为策略梯度显示了策略改进的方向，但值损失和熵损失应该最小化。
- en: To improve stability, it’s worth using several environments, providing you with
    observations concurrently (when you have multiple environments, your training
    batch will be created from their observations). We will look at several ways of
    doing this later in this chapter when we discuss the A3C method.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了提高稳定性，值得使用多个环境，提供并行的观察数据（当你有多个环境时，训练批次将从这些观察数据中创建）。我们将在本章后续讨论 A3C 方法时探讨几种实现方式。
- en: The version of the preceding method that uses several environments running in
    parallel is called advantage asynchronous actor-critic, which is also known as
    A3C. The A3C method will be discussed later, but for now, let’s implement A2C.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 前面方法的版本，通过并行运行多个环境来实现，称为优势异步演员-评论员方法，也被称为 A3C。A3C 方法将在后续讨论，但现在，我们先实现 A2C。
- en: A2C on Pong
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A2C 在 Pong 中的应用
- en: In the previous chapter, you saw a (not very successful) attempt to solve our
    favorite Pong environment with policy gradient methods. Let’s try it again with
    the actor-critic method at hand. The full source code is available in Chapter12/02_pong_a2c.py.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你看到了一次（不太成功的）尝试，使用策略梯度方法解决我们最喜欢的 Pong 环境。让我们再尝试一下，手头有演员-评论员方法。完整的源代码可以在
    Chapter12/02_pong_a2c.py 中找到。
- en: 'We start, as usual, by defining hyperparameters (imports are omitted):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样，从定义超参数开始（省略了导入部分）：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These values are not tuned, which is left as an exercise for the reader. We
    have one new value here: CLIP_GRAD. This hyperparameter specifies the threshold
    for gradient clipping, which basically prevents our gradients from becoming too
    large at the optimization stage and pushing our policy too far. Clipping is implemented
    using the PyTorch functionality, but the idea is very simple: if the L2 norm of
    the gradient is larger than this hyperparameter, then the gradient vector is clipped
    to this value.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值并未调整，这部分留给读者自己完成。这里有一个新的值：CLIP_GRAD。这个超参数指定了梯度裁剪的阈值，基本上它防止了在优化阶段梯度变得过大，从而使我们的策略过于偏离。裁剪是使用PyTorch的功能实现的，但这个概念非常简单：如果梯度的L2范数大于这个超参数，则梯度向量会被裁剪到这个值。
- en: The REWARD_STEPS hyperparameter determines how many steps ahead we will take
    to approximate the total discounted reward for every action.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: REWARD_STEPS超参数确定我们将向前走多少步，以近似每个行动的总折扣奖励。
- en: 'In the policy gradient methods, we used about 10 steps, but in A2C, we will
    use our value approximation to get a state value for further steps, so it will
    be fine to decrease the number of steps. The following is our network architecture:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度方法中，我们使用了大约10步，但在A2C中，我们将使用我们的值近似来获得进一步步骤的状态值，因此减少步数是可以的。以下是我们的网络架构：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It has a shared convolution body and two heads: the first returns the policy
    with the probability distribution over our actions and the second head returns
    one single number, which will approximate the state’s value. It might look similar
    to our dueling DQN architecture from Chapter [8](ch012.xhtml#x1-1240008), but
    our training procedure is different.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有共享的卷积体和两个头部：第一个返回包含我们行动概率分布的策略，第二个头部返回一个单一数字，该数字将近似于状态的值。它可能看起来与我们在第[8](ch012.xhtml#x1-1240008)章中提到的对抗性DQN架构相似，但我们的训练过程不同。
- en: 'The forward pass through the network returns a tuple of two tensors – policy
    and value:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的前向传递返回一个包含两个张量的元组——策略和值：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we have to discuss a large and important function, which takes the batch
    of environment transitions and returns three tensors: the batch of states, batch
    of actions taken, and batch of Q-values calculated using the formula Q(s,a) =
    ∑ [i=0]^(N−1)γ^ir[i] + γ^NV (s[N]). This Q-value will be used in two places: to
    calculate mean squared error (MSE) loss to improve the value approximation in
    the same way as DQN, and to calculate the advantage of the action.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要讨论一个重要的大函数，它接受环境转移的批次并返回三个张量：状态批次、采取的行动批次和使用公式Q(s,a) = ∑ [i=0]^(N−1)γ^ir[i]
    + γ^NV(s[N])计算的Q值批次。这个Q值将在两个地方使用：计算均方误差（MSE）损失以改善值的近似，就像DQN一样；以及计算行动的优势。
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the beginning, we just walk through our batch of transitions and copy their
    fields into the lists. Note that the reward value already contains the discounted
    reward for REWARD_STEPS, as we use the ptan.ExperienceSourceFirstLast class. We
    also need to handle episode-ending situations and remember indices of batch entries
    for non-terminal episodes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我们只需要遍历我们的转移批次并将它们的字段复制到列表中。注意，奖励值已经包含了REWARD_STEPS的折扣奖励，因为我们使用了ptan.ExperienceSourceFirstLast类。我们还需要处理回合结束的情况，并记住非终止回合的批次条目索引。
- en: 'In the following code, we convert the gathered state and actions into a PyTorch
    tensor and copy them into the graphics processing unit (GPU) if needed:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将收集到的状态和动作转换为PyTorch张量，并根据需要将其复制到图形处理单元（GPU）中：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the extra call to np.asarray() might look redundant, but without it,
    the performance of tensor creation degrades 5-10x. This is known as [issue #13918](https://github.com/pytorch/pytorch/issues/13918)
    in PyTorch, and at the time of writing, it hasn’t been solved, so one solution
    is to pass a single NumPy array instead of a list of arrays.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，对np.asarray()的额外调用可能看起来是多余的，但没有它，张量创建的性能会降低5到10倍。这在PyTorch中被称为[问题 #13918](https://github.com/pytorch/pytorch/issues/13918)，并且在写作时尚未解决，因此一种解决方案是传递一个单一的NumPy数组，而不是数组列表。'
- en: 'The rest of the function calculates Q-values, taking into account the terminal
    episodes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的其余部分计算Q值，考虑了终止回合的情况：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code prepares the variable with the last state in our transition
    chain and queries our network for V (s) approximation. Then, this value is multiplied
    by the discount factor and added to the immediate rewards.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码准备了变量，存储我们转移链中的最后一个状态，并查询我们的网络以获取V(s)的近似值。然后，将该值乘以折扣因子并加上即时奖励。
- en: 'At the end of the function, we pack our Q-values into the tensor and return
    it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的末尾，我们将Q值打包到张量中并返回：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following code, you can notice a new way to create environments, the
    class gym.vector.SyncVectorEnv, which is being passed a list of lambda functions
    creating the underlying environments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，你可以注意到一种新的创建环境的方式，使用类gym.vector.SyncVectorEnv，它传入一个包含创建底层环境的lambda函数的列表：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The class gym.vector.SyncVectorEnv is provided by Gymnasium and allows wrapping
    several environments into one single “vectorized” environment. Underlying environments
    have to have identical action and observation spaces, which allows the vectorized
    environment to accept a vector of actions and return batches of observations and
    rewards. You can find more details in the Gymnasium documentation: [https://gymnasium.farama.org/api/vector/](https://gymnasium.farama.org/api/vector/).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 类gym.vector.SyncVectorEnv是Gymnasium提供的，允许将多个环境封装成一个单一的“向量化”环境。底层环境必须具有相同的动作空间和观察空间，这使得向量化环境能够接受一组动作并返回一批观察和奖励。你可以在Gymnasium文档中找到更多细节：[https://gymnasium.farama.org/api/vector/](https://gymnasium.farama.org/api/vector/)。
- en: Synchronized vectorized environments (the SyncVectorEnv class) are almost identical
    to the optimization we used in Chapter [9](ch013.xhtml#x1-1600009), in the section
    Several environments, where we passed multiple gym environments into the experience
    source to increase the performance of the DQN training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 同步向量化环境（SyncVectorEnv类）几乎与我们在第[9](ch013.xhtml#x1-1600009)章“多个环境”部分中使用的优化完全相同，当时我们将多个gym环境传入经验源以提高DQN训练的性能。
- en: 'But in the case of vectorized environments, a different experience source class
    has to be used: VectorExperienceSourceFirstLast, which takes into account vectorization
    and optimizes the agent application to the observation. From the outside, the
    interface of this experience source is exactly as before.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但在向量化环境的情况下，必须使用不同的经验源类：VectorExperienceSourceFirstLast，它考虑了向量化，并优化了代理对观察的应用。从外部看，这个经验源的接口与之前完全相同。
- en: The command-line argument --use-async (which switches our wrapper class from
    SyncVectorEnv to AsyncVectorEnv) is not relevant at the moment – we will use it
    later, when discussing the A3C method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行参数--use-async（它将我们的包装类从SyncVectorEnv切换为AsyncVectorEnv）目前不相关——我们稍后会使用它，在讨论A3C方法时。
- en: 'Then, we create the network, agent, and experience source:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建网络、代理和经验源：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One very important detail here is passing the eps parameter to the optimizer.
    If you’re familiar with the Adam algorithm, you may know that epsilon is a small
    number added to the denominator to prevent zero-division situations. Normally,
    this value is set to some small number, such as 10^(−8) or 10^(−10), but in our
    case, these values turned out to be too small. I have no mathematically strict
    explanation for this, but with the default value of epsilon, the method does not
    converge at all. Very likely, the division to a small value of 10^(−8) makes the
    gradients too large, which turns out to be fatal for training stability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个非常重要的细节是将eps参数传递给优化器。如果你熟悉Adam算法，你可能知道epsilon是一个加到分母上的小数，用来防止零除错误。通常，这个值设置为一些小数字，如10^(-8)或10^(-10)，但在我们的情况下，这些值太小了。我没有严格的数学解释，但使用默认的epsilon值时，方法根本无法收敛。很可能，除以一个小值10^(-8)会导致梯度过大，这对训练稳定性来说是致命的。
- en: Another detail is to use VectorExperienceSourceFirstLast instead of ExperienceSourceFirstLast.
    This is required because of the vectorized environment wrapping several normal
    Atari environments. The vectorized environment also exposes the attributes single_observation_space
    and single_action_space, which are the observation and action spaces of an individual
    environment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个细节是使用VectorExperienceSourceFirstLast而不是ExperienceSourceFirstLast。这是必要的，因为向量化环境将多个普通的Atari环境封装在一起。向量化环境还暴露了single_observation_space和single_action_space这两个属性，它们分别是单个环境的观察空间和动作空间。
- en: 'In the training loop, we use two wrappers:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环中，我们使用两个包装器：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The first wrapper in this code is already familiar to you: common.RewardTracker,
    which computes the mean reward for the last 100 episodes and tells us when this
    mean reward exceeds the desired threshold. Another wrapper, TBMeanTracker, is
    from the PTAN library and is responsible for writing into TensorBoard the mean
    of the measured parameters for the last 10 steps. This is helpful, as training
    can take millions of steps and we don’t want to write millions of points into
    TensorBoard, but rather write smoothed values every 10 steps.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的第一个包装器你已经很熟悉：common.RewardTracker，它计算最后100个回合的平均奖励，并告诉我们当这个平均奖励超过所需阈值时。另一个包装器TBMeanTracker来自PTAN库，负责将最后10步中测量的参数的平均值写入TensorBoard。这是非常有帮助的，因为训练可能需要上百万步，我们不希望每一步都写入TensorBoard，而是每10步写入平滑后的值。
- en: 'The next code chunk is responsible for our calculation of losses, which is
    the core of the A2C method. First, we unpack our batch using the function we described
    earlier and ask our network to return the policy and values for this batch:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码负责我们计算损失的部分，这是A2C方法的核心。首先，我们使用之前描述的函数解包批次，并要求网络返回该批次的策略和值：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The policy is returned in an unnormalized form, so to convert it into the probability
    distribution, we need to apply softmax to it. As the policy loss requires the
    logarithm of the probability distribution, we will use the function log_softmax,
    which is more numerically stable than calling softmax and then log.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 策略以未归一化的形式返回，因此为了将其转换为概率分布，我们需要对其应用softmax。由于策略损失需要概率分布的对数，我们将使用log_softmax函数，这比先调用softmax再取对数更加稳定。
- en: 'In the value loss part, we calculate the MSE between the value returned by
    our network and the approximation we performed using the Bellman equation unrolled
    four steps forward:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在价值损失部分，我们计算网络返回的值与我们通过展开四步的贝尔曼方程所进行的近似之间的均方误差（MSE）：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we calculate the policy loss to obtain the policy gradient:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算策略损失以获得策略梯度：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first two steps obtain a log of our policy and calculate the advantage of
    actions, which is A(s,a) = Q(s,a) −V (s). The call to value_t.detach() is important,
    as we don’t want to propagate the policy gradient into our value approximation
    head. Then, we take the log of probability for the actions taken and scale them
    with the advantage. Our policy gradient loss value will be equal to the negated
    mean of this scaled log of policy, as the policy gradient directs us toward policy
    improvement, but loss value is supposed to be minimized.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前两步获得我们策略的日志并计算行动的优势，优势A(s,a) = Q(s,a) −V (s)。调用 value_t.detach() 很重要，因为我们不希望将策略梯度传播到我们的价值近似头部。然后，我们对采取的行动的概率取对数，并用优势对其进行缩放。我们的策略梯度损失值将等于该缩放后的策略对数的负均值，因为策略梯度引导我们朝着策略改进的方向，但损失值应该最小化。
- en: 'The last piece of our loss function is entropy loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们损失函数的最后一部分是熵损失：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Entropy loss is equal to the scaled entropy of our policy, taken with the opposite
    sign (entropy is calculated as H(π) = −∑ π log π).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 熵损失等于我们策略的缩放熵，并取其相反符号（熵的计算公式是H(π) = −∑ π log π）。
- en: 'In the following code, we calculate and extract gradients of our policy, which
    will be used to track the maximum gradient, its variance, and the L2 norm:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们计算并提取我们策略的梯度，这些梯度将用于追踪最大梯度、其方差和L2范数：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As the final step of our training, we backpropagate the entropy loss and the
    value loss, clip gradients, and ask our optimizer to update the network:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 作为训练的最后一步，我们反向传播熵损失和价值损失，裁剪梯度，并要求优化器更新网络：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At the end of the training loop, we track all of the values that we are going
    to monitor in TensorBoard:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环的最后，我们追踪所有需要在TensorBoard中监控的值：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There are plenty of values that we need to monitor and we will discuss them
    in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多值需要监控，我们将在下一部分中讨论它们。
- en: Results
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'To start the training, run 02_pong_a2c.py with the --dev (for GPU) and -n options
    (which provides a name for the run for TensorBoard):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，请运行 02_pong_a2c.py 并使用 --dev（表示使用GPU）和 -n 选项（为TensorBoard提供一个运行名称）：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As a word of warning, the training process is lengthy. With the original hyperparameters,
    it requires about 10 million frames to solve, which is about three hours on a
    GPU.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为警告，训练过程比较漫长。使用原始超参数，它大约需要1000万帧来解决问题，大约在GPU上需要三个小时。
- en: Later in the chapter, we’ll check the asynchronous version of the A2C method,
    which executes the environment in a separate process (which increases both training
    stability and performance). But first, let’s focus on our plots in TensorBoard.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章后面，我们将查看A2C方法的异步版本，它在一个单独的进程中执行环境（这提高了训练的稳定性和性能）。但首先，让我们集中关注TensorBoard中的图表。
- en: 'The reward dynamics look much better than in the example from the previous
    chapter:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励动态比上一章的示例要好得多：
- en: '![PIC](img/B22150_12_06.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_06.png)'
- en: 'Figure 12.6: Smoothed reward (left) and mean batch values (right)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：平滑奖励（左侧）和平均批次值（右侧）
- en: The left plot is the mean training episodes reward averaged over the 100 last
    episodes. The right plot, “batch value,” shows Q-values approximated using the
    Bellman equation and an overall positive dynamic in Q approximation. This shows
    that our training process is improving more or less consistently over time.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图是过去100个训练回合的平均奖励。右侧的图，“批次值”，展示了使用贝尔曼方程近似的Q值以及Q近似的整体正向动态。这表明我们的训练过程在时间上基本上是持续改进的。
- en: 'The next four charts are related to our loss and include the individual loss
    components and the total loss:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的四个图与我们的损失相关，包含了各个损失组件和总损失：
- en: '![PIC](img/B22150_12_07.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_07.png)'
- en: 'Figure 12.7: Entropy loss (left) and policy loss (right)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7：熵损失（左侧）和策略损失（右侧）
- en: '![PIC](img/B22150_12_08.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_08.png)'
- en: 'Figure 12.8: Value loss (left) and total loss (right)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8：价值损失（左侧）和总损失（右侧）
- en: 'Here, we must note the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须注意以下几点：
- en: First, our value loss (Figure [12.8](#x1-208037r8), on the left) is decreasing
    consistently, which shows that our V (s) approximation is improving during the
    training.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们的价值损失（图 [12.8](#x1-208037r8)，在左侧）持续减少，这表明我们的V(s)近似值在训练过程中得到了改善。
- en: The second observation is that our entropy loss (Figure [12.7](#x1-208036r7),
    on the left) is growing in the middle of the training, but it doesn’t dominate
    in the total loss. This basically means that our agent becomes more confident
    in its actions as the policy becomes less uniform.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个观察结果是我们的熵损失（图 [12.7](#x1-208036r7)，左侧）在训练的中期增长，但它在总损失中并不占主导地位。这基本上意味着随着策略变得不再均匀，我们的代理在其动作上变得更加自信。
- en: The last thing to note here is that policy loss (Figure [12.7](#x1-208036r7),
    on the right) is decreasing most of the time and is correlated to the total loss,
    which is good, as we are interested in the gradients for our policy first of all.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里最后需要注意的是，策略损失（图 [12.7](#x1-208036r7)，右侧）大多数时候在减少，并且与总损失相关联，这是好的，因为我们首先关注的是我们策略的梯度。
- en: 'The last set of plots displays the advantage value and policy gradient metrics:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一组图显示了优势值和策略梯度度量：
- en: '![PIC](img/B22150_12_09.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_09.png)'
- en: 'Figure 12.9: Advantage (left) and L2 of gradients (right)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9：优势（左侧）和梯度的L2范数（右侧）
- en: '![PIC](img/B22150_12_10.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_10.png)'
- en: 'Figure 12.10: Max of gradients (left) and gradients variance (right)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10：梯度的最大值（左侧）和梯度方差（右侧）
- en: The advantage is a scale of our policy gradients, and it equals Q(s,a) −V (s).
    We expect it to oscillate around 0 (because, on average, the effect of the single
    action on the state’s value shouldn’t be large), and the chart meets our expectations.
    The gradient charts demonstrate that our gradients are not too small and not too
    large. Variance is very small at the beginning of the training (for 2 million
    frames), but starts to grow later, which means that our policy is changing.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 优势是我们策略梯度的尺度，它等于Q(s,a) − V(s)。我们期望它在0周围波动（因为从平均而言，单一动作对状态值的影响不应该很大），而图表符合我们的预期。梯度图表表明我们的梯度既不太小也不太大。方差在训练的最初阶段（前200万帧）非常小，但后来开始增长，这意味着我们的策略在发生变化。
- en: Asynchronous Advantage Actor-Critic (A3C)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步优势演员评论员（A3C）
- en: In this section, we will extend the A2C method. This extension adds true asynchronous
    environment interaction, and is called asynchronous advantage actor-critic (A3C).
    This method is one of the most widely used by RL practitioners.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将扩展A2C方法。这个扩展加入了真正的异步环境交互，被称为异步优势演员评论员（A3C）。该方法是RL实践者最广泛使用的算法之一。
- en: 'We will take a look at two approaches for adding asynchronous behavior to the
    basic A2C method: data-level and gradient-level parallelism. They have different
    resource requirements and characteristics, which makes them applicable to different
    situations.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍两种为基础A2C方法添加异步行为的方式：数据级并行和梯度级并行。它们有不同的资源需求和特点，这使得它们适用于不同的情况。
- en: Correlation and sample efficiency
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性与样本效率
- en: One of the approaches to improving the stability of the policy gradient family
    of methods is using multiple environments in parallel. The reason behind this
    is the fundamental problem we discussed in Chapter [6](#), when we talked about
    the correlation between samples, which breaks the independent and identically
    distributed (iid) assumption, which is critical for stochastic gradient descent
    (SGD) optimization. The negative consequence of such correlation is very high
    variance in gradients, which means that our training batch contains very similar
    examples, all of them pushing our network in the same direction. However, this
    may be totally the wrong direction in the global sense, as all those examples
    may be from one single lucky or unlucky episode.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 改进策略梯度方法稳定性的一种方式是使用多个环境并行训练。其背后的原因是我们在第[6](#)章中讨论的基本问题，即样本之间的相关性，这破坏了独立同分布（iid）假设，而这个假设对于随机梯度下降（SGD）优化至关重要。相关性的负面影响是梯度方差非常大，这意味着我们的训练批次包含了非常相似的样本，它们都会将我们的网络推向相同的方向。然而，这个方向在全局上可能完全是错误的，因为所有这些样本可能来自同一个幸运或不幸运的回合。
- en: With our deep Q-network (DQN), we solved the issue by storing a large number
    of previous states in the replay buffer and sampling our training batch from this
    buffer. If the buffer is large enough, the random sample from it will be a much
    better representation of the states’ distribution at large. Unfortunately, this
    solution won’t work for policy gradient methods. This is because most of them
    are on-policy, which means that we have to train on samples generated by our current
    policy, so remembering old transitions will not be possible anymore. You can try
    to do this, but the resulting policy gradient will be for the old policy used
    to generate the samples and not for your current policy that you want to update.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的深度Q网络（DQN），我们通过在重放缓冲区中存储大量的历史状态，并从这个缓冲区中抽取训练批次来解决这个问题。如果缓冲区足够大，从中随机抽取的样本将更好地代表状态的整体分布。不幸的是，这个方法不能应用于策略梯度方法。这是因为大多数策略梯度方法是基于当前策略进行训练的，也就是说，我们必须使用当前策略生成的样本来训练，因此不能再记住旧的转换。你可以尝试这样做，但最终得到的策略梯度会是基于旧策略生成样本的梯度，而不是你想更新的当前策略。
- en: Researchers have focused on this issue for many years. Several ways to address
    it have been proposed, but the problem is still far from being solved. The most
    commonly used solution is gathering transitions using several parallel environments,
    all of them exploiting the current policy. This breaks the correlation within
    one single episode, as we now train on several episodes obtained from different
    environments. At the same time, we are still using our current policy. The one
    very large disadvantage of this is sample inefficiency, as we basically throw
    away all the experience that we have obtained after one single training round.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经研究这个问题多年，提出了几种解决方案，但这个问题仍远未解决。最常用的解决方案是通过多个并行环境收集转换，这些环境都利用当前的策略。这种方法打破了单一回合中的相关性，因为我们现在是在多个不同环境中收集的多个回合上进行训练。同时，我们依然使用当前的策略。这种方法的一个重大缺点是样本效率低，因为我们基本上会丢弃在单一训练轮次中获得的所有经验。
- en: It’s very simple to compare DQN with policy gradient approaches. For example,
    for DQN, if we use 1 million samples of a replay buffer and a training batch size
    of 32 samples for every new frame, every single transition will be used approximately
    32 times before it is pushed from the experience replay. For the priority replay
    buffer, which was discussed in Chapter [8](ch012.xhtml#x1-1240008), this number
    could be much higher, as the sample probability is not uniform. In the case of
    policy gradient methods, each experience obtained from the environment can be
    used only once, as our method requires fresh data, so the data efficiency of policy
    gradient methods could be an order of magnitude lower than the value-based, off-policy
    methods.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将DQN与策略梯度方法进行比较非常简单。例如，对于DQN，如果我们使用100万个回放缓冲区样本，每个新帧的训练批量大小为32个样本，那么每个过渡状态大约会在从经验回放中推送之前被使用32次。对于优先回放缓冲区（在第[8](ch012.xhtml#x1-1240008)章讨论过），这个数字可能会高得多，因为样本的选择概率不是均匀的。在策略梯度方法的情况下，从环境中获得的每个经验只能使用一次，因为我们的方法需要新鲜的数据，因此策略梯度方法的数据效率可能比基于价值的离线方法低一个数量级。
- en: On the other hand, our A2C agent converged on Pong in 8 million frames, which
    is just eight times more than 1 million frames for basic DQN in Chapter [6](#)
    and Chapter [8](ch012.xhtml#x1-1240008). So, this shows us that policy gradient
    methods are not completely useless; they’re just different and have their own
    specificities that you need to take into account on method selection. If your
    environment is “cheap” in terms of the agent interaction (the environment is fast,
    has a low memory footprint, allows parallelization, and so on), policy gradient
    methods could be a better choice. On the other hand, if the environment is “expensive”
    and obtaining a large amount of experience could slow down the training process,
    the value-based methods could be a smarter way to go.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们的A2C代理在Pong上收敛用了800万帧，这仅是第[6](#)章和第[8](ch012.xhtml#x1-1240008)章中基本DQN的100万帧的八倍。因此，这向我们展示了策略梯度方法并非完全无用；它们只是不同的，并且有其自身的特点，你在选择方法时需要考虑这些特点。如果你的环境在代理交互方面是“便宜”的（环境响应快速，内存占用低，支持并行化等），那么策略梯度方法可能是更好的选择。另一方面，如果环境“昂贵”，并且获取大量经验可能会减慢训练过程，那么基于价值的方法可能是更聪明的选择。
- en: Adding an extra “A” to A2C
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向A2C中添加额外的“A”
- en: 'From a practical point of view, communicating with several parallel environments
    is simple. We already did this in Chapter [9](ch013.xhtml#x1-1600009) and earlier
    in the current chapter, but it wasn’t explicitly stated. In the A2C agent, we
    passed an array of Gym environments into the ExperienceSource class, which switched
    it into round-robin data gathering mode. This means that every time we ask for
    a transition from the experience source, the class uses the next environment from
    our array (of course, keeping the state for every environment). This simple approach
    is equivalent to parallel communication with environments, but with one single
    difference: communication is not parallel in the strict sense but performed in
    a serial way. However, samples from our experience source are shuffled. This idea
    is shown in the following figure:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从实践角度看，与多个并行环境进行通信是简单的。我们已经在第[9](ch013.xhtml#x1-1600009)章和当前章节的前面部分做过这件事，但并没有明确说明。在A2C代理中，我们将一个Gym环境的数组传递给ExperienceSource类，该类将其切换为轮询数据收集模式。这意味着每次我们从经验源请求过渡时，该类会使用我们数组中的下一个环境（当然，会为每个环境保持状态）。这种简单的方法相当于与环境进行并行通信，但有一个小小的区别：通信不是严格意义上的并行，而是以串行的方式进行。然而，我们的经验源中的样本是打乱的。这个思路在下图中展示：
- en: '![PIC](img/B22150_12_11.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_11.png)'
- en: 'Figure 12.11: An agent training from multiple environments in parallel'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：一个代理在多个环境中并行训练
- en: This method works fine and helped us to get convergence in the A2C method, but
    it is still not perfect in terms of computing resource utilization, as all the
    processing is done sequentially. Even a modest workstation nowadays has several
    CPU cores, which can be used for computation, such as training and environment
    interaction. On the other hand, parallel programming is harder than the traditional
    paradigm, when you have a clear stream of execution. Luckily, Python is a very
    expressive and flexible language with lots of third-party libraries, which allows
    you to do parallel programming without much trouble. We have already seen the
    example of the torch.multiprocessing library in Chapter [9](ch013.xhtml#x1-1600009),
    where we parallelized agents’ execution during the DQN training. But there are
    other higher-level libraries, like ray, which allow us to parallelize execution
    of the code, hiding the low-level communication details.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法运行良好，并帮助我们在A2C方法中达到了收敛，但在计算资源利用方面仍不完美，因为所有处理都是顺序进行的。即使是现在的一台普通工作站，也拥有多个CPU核心，可以用于计算，如训练和环境交互。另一方面，平行编程比传统范式更难，当你有一个清晰的执行流时，传统方法相对更简单。幸运的是，Python是一种非常具有表达力和灵活性的语言，拥有大量的第三方库，允许你轻松进行平行编程。我们已经在第 [9](ch013.xhtml#x1-1600009)章中看到过torch.multiprocessing库的示例，在DQN训练中我们平行化了代理的执行。但还有其他更高级的库，如ray，它允许我们平行化代码执行，隐藏底层的通信细节。
- en: 'With regard to actor-critic parallelization, two approaches exist:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 关于演员-评论家并行化，有两种方法：
- en: 'Data parallelism: We can have several processes, each of them communicating
    with one or more environments and providing us with transitions (s,r,a,s′). All
    those samples are gathered together in one single training process, which calculates
    losses and performs an SGD update. Then, the updated neural network (NN) parameters
    need to be broadcast to all other processes to use in future environment communications.
    This model is illustrated in Figure [12.12](#x1-211008r12).'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据并行性：我们可以有多个进程，每个进程与一个或多个环境进行通信，并提供过渡数据（s,r,a,s′）。所有这些样本将汇聚到一个单独的训练过程中，计算损失并执行SGD更新。然后，更新后的神经网络（NN）参数需要广播到所有其他进程，以便在未来的环境通信中使用。这个模型在图 [12.12](#x1-211008r12)中有所说明。
- en: 'Gradients parallelism: As the goal of the training process is the calculation
    of gradients to update our NN, we can have several processes calculating gradients
    on their own training samples. Then, these gradients can be summed together to
    perform the SGD update in one process. Of course, updated NN weights also have
    to be propagated back to all workers to keep data on-policy. This is illustrated
    in Figure [12.13](#x1-211009r13).'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度并行性：由于训练过程的目标是计算梯度来更新我们的神经网络（NN），我们可以有多个进程在各自的训练样本上计算梯度。然后，这些梯度可以汇总在一个进程中执行SGD更新。当然，更新后的NN权重也必须传播回所有工作进程，以保持数据的一致性。这在图 [12.13](#x1-211009r13)中有所说明。
- en: '![PIC](img/B22150_12_12.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_12.png)'
- en: 'Figure 12.12: The first approach to actor-critic parallelism, based on distributed
    training samples being gathered'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12：第一种演员-评论家并行化方法，基于分布式训练样本的汇集
- en: '![PIC](img/B22150_12_13.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_13.png)'
- en: 'Figure 12.13: The second approach to parallelism, gathering gradients for the
    model'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：第二种并行化方法，为模型汇集梯度
- en: The difference between the two methods might not look very significant from
    the diagrams, but you need to be aware of the computation cost. The heaviest operation
    in A2C optimization is the training process, which consists of loss calculation
    from data samples (forward pass) and the calculation of gradients with respect
    to this loss. The SGD optimization step is quite lightweight – basically, just
    adding the scaled gradients to the NN’s weights. By moving the computation of
    loss and gradients in the second approach (gradient parallelism) from the central
    process, we eliminated the major potential bottleneck and made the whole process
    significantly more scalable.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法之间的差异从图表上看可能并不十分显著，但你需要意识到计算成本的差异。A2C优化中的最重操作是训练过程，它包括从数据样本中计算损失（前向传播）以及根据该损失计算梯度。SGD优化步骤相对轻量——基本上只是将缩放后的梯度加到神经网络（NN）的权重上。通过将第二种方法（梯度并行）中的损失计算和梯度计算从中央处理过程移出，我们消除了主要的潜在瓶颈，并使整个过程变得更加可扩展。
- en: In practice, the choice of the method mainly depends on your resources and your
    goals. If you have one single optimization problem and lots of distributed computation
    resources, such as a couple of dozen GPUs spread over several machines in the
    networks, then gradients parallelism will be the best approach to speed up your
    training.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，选择哪种方法主要取决于你的资源和目标。如果你有一个单一的优化问题，并且拥有大量分布式计算资源，比如在网络中分布的数十个GPU，那么梯度并行将是加速训练的最佳方法。
- en: However, in the case of one single GPU, both methods will provide a similar
    performance, but the first approach is generally simpler to implement, as you
    don’t need to deal with low-level gradient values. In this chapter, we will compare
    both methods on our favorite Pong game to see the difference between the approaches
    and look at PyTorch’s multiprocessing capabilities.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果只有一个GPU，两种方法会提供类似的性能，但第一种方法通常更易于实现，因为你无需处理低级别的梯度值。在本章中，我们将通过我们最喜欢的Pong游戏比较这两种方法，看看它们的差异，并探索PyTorch的多进程能力。
- en: A3C with data parallelism
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A3C与数据并行
- en: The first version of A3C parallelization that we will check (which was outlined
    in Figure [12.12](#x1-211008r12)) has both one main process that carries out training
    and several child processes communicating with environments and gathering experience
    to train on.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查的A3C并行化的第一个版本（如图[12.12](#x1-211008r12)所示）有一个主要进程负责执行训练，多个子进程与环境进行通信并收集经验进行训练。
- en: In fact, we already implemented this version in Chapter [9](ch013.xhtml#x1-1600009)
    when we ran several agents in subprocesses when we trained the DQN model (then
    we got a speed-up of 27% in terms of FPS). In this section, I’m not going to reimplement
    the same approach with the A3C method, but rather want to illustrate the “power
    of libraries.”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们在第[9章](ch013.xhtml#x1-1600009)中已经实现了这个版本，当时我们在子进程中运行多个代理，训练DQN模型（那时我们在FPS方面获得了27%的加速）。在这一部分，我不会用A3C方法重新实现相同的方法，而是想展示“库的力量”。
- en: 'We already briefly mentioned the class gym.vector.SyncVectorEnv from Gymnasium
    (it exists only in the Farama fork, not in the original OpenAI Gym) and the PTAN
    experience source, which supports “vectorized” environments: VectorExperienceSourceFirstLast.
    The class SyncVectorEnv handles wrapped environments sequentially, but there is
    a drop-in replacement class, AsyncVectorEnv, which uses mp.multiprocessing for
    subenvironments. So, to get the data-parallel version of the A2C method, we just
    need to replace SyncVectorEnv with AsyncVectorEnv and we’re done.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要提到过Gymnasium中的类`gym.vector.SyncVectorEnv`（它仅存在于Farama的分支中，而不在原始的OpenAI
    Gym中）和PTAN经验源，它支持“向量化”环境：`VectorExperienceSourceFirstLast`。类`SyncVectorEnv`按顺序处理封装的环境，但有一个替代类`AsyncVectorEnv`，它使用`mp.multiprocessing`处理子环境。所以，为了获得A2C方法的数据并行版本，我们只需要将`SyncVectorEnv`替换为`AsyncVectorEnv`，这样就完成了。
- en: The code in Chapter12/02_pong_a2c.py already supports this replacement, which
    is done by passing the --use-async command-line option.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第12章的代码（Chapter12/02_pong_a2c.py）已经支持这个替换操作，方法是传递`--use-async`命令行选项。
- en: Results
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果
- en: 'The asynchronous version with 50 environments shows a performance of 2000 FPS,
    which is a 2x improvement over the sequential version. The following charts compare
    the performance and reward dynamics of these two versions:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 异步版本在50个环境下展示了2000 FPS的性能，比顺序版本提高了2倍。以下图表比较了这两个版本的性能和奖励动态：
- en: '![PIC](img/B22150_12_14.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_14.png)'
- en: 'Figure 12.14: Comparison of A2C and A3C in terms of reward (left) and speed
    (right)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：A2C和A3C在奖励（左）和速度（右）上的比较
- en: A3C with gradient parallelism
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A3C与梯度并行
- en: The next approach that we will consider to parallelize A2C implementation will
    have several child processes, but instead of feeding training data to the central
    training loop, they will calculate the gradients using their local training data,
    and send those gradients to the central master process. This process is responsible
    for combining those gradients (which is basically just summing them) and performing
    an SGD update on the shared network.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑一种并行化A2C实现的方法，它将有多个子进程，但它们不是将训练数据传递给中央训练循环，而是使用它们本地的训练数据计算梯度，并将这些梯度发送给中央主进程。这个主进程负责将这些梯度合并（基本上就是求和）并对共享网络进行SGD更新。
- en: The difference might look minor, but this approach is much more scalable, especially
    if you have several powerful nodes with multiple GPUs connected to the network.
    In this case, the central process in the data-parallel model quickly becomes a
    bottleneck, as the loss calculation and backpropagation are computationally demanding.
    Gradient parallelization allows for the spreading of the load on several GPUs,
    performing only a relatively simple operation of gradient combination in a central
    place.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个差异看起来可能微不足道，但这种方法的可扩展性更强，特别是当你有多个强大的节点并且这些节点通过多个 GPU 连接到网络时。在这种情况下，数据并行模型中的中央处理过程很快就会成为瓶颈，因为损失计算和反向传播是计算密集型的。梯度并行化可以将负载分配到多个
    GPU 上，在中央位置仅执行相对简单的梯度组合操作。
- en: Implementation
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: The complete example is in the Chapter12/03_a3c_grad.py file, and it uses the
    same Chapter12/lib/common.py module that we’ve already seen.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的示例在 Chapter12/03_a3c_grad.py 文件中，并且使用我们已经看到的 Chapter12/lib/common.py 模块。
- en: 'As usual, we first define the hyperparameters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先定义超参数：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'These are mostly the same as in the previous example, except BATCH_SIZE is
    replaced by two parameters: GRAD_BATCH and TRAIN_BATCH. The value of GRAD_BATCH
    defines the size of the batch used by every child process to compute the loss
    and get the value of the gradients. The second parameter, TRAIN_BATCH, specifies
    how many gradient batches from the child processes will be combined on every SGD
    iteration. Every entry produced by the child process has the same shape as our
    network parameters, and we sum up TRAIN_BATCH values of them together. So, for
    every optimization step, we use the TRAIN_BATCH * GRAD_BATCH training samples.
    As the loss calculation and backpropagation are quite heavy operations, we use
    a large GRAD_BATCH to make them more efficient.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些与之前的示例大致相同，唯一的区别是 BATCH_SIZE 被两个参数取代：GRAD_BATCH 和 TRAIN_BATCH。GRAD_BATCH 的值定义了每个子进程用于计算损失并获取梯度值的批次大小。第二个参数
    TRAIN_BATCH 指定了每个 SGD 迭代中将结合来自子进程的多少个梯度批次。每个由子进程产生的条目具有与我们的网络参数相同的形状，我们将其 TRAIN_BATCH
    的值相加。因此，对于每一步优化，我们使用 TRAIN_BATCH * GRAD_BATCH 个训练样本。由于损失计算和反向传播是相当繁重的操作，我们使用较大的
    GRAD_BATCH 来提高它们的效率。
- en: Due to this large batch, we should keep TRAIN_BATCH relatively low to keep our
    network update on policy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个大批次，我们应该保持相对较低的 TRAIN_BATCH，以保持网络的策略更新。
- en: 'Now we have two functions – make_env(), which is used to create a wrapped Pong
    environment, and grads_func(), which is much more complicated and implements most
    of the training logic we normally do in the training loop. As a compensation,
    the training loop in the main process becomes almost trivial:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个函数——make_env()，用于创建一个封装的 Pong 环境，以及 grads_func()，它更加复杂，实现了我们通常在训练循环中执行的大部分训练逻辑。作为补偿，主进程中的训练循环变得几乎是微不足道的：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'On the creation of the child process, we pass several arguments to the grads_func()
    function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建子进程时，我们将几个参数传递给 grads_func() 函数：
- en: The name of the process, which is used to create the TensorBoard writer. In
    this example, every child process writes its own TensorBoard dataset.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于创建 TensorBoard 写入器的进程名称。在这个示例中，每个子进程都写入自己的 TensorBoard 数据集。
- en: The shared NN.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享神经网络。
- en: A torch.device instance, specifying the computation device.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 torch.device 实例，用于指定计算设备。
- en: The queue used to deliver the calculated gradients to the central process.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将计算出的梯度传递给中央处理过程的队列。
- en: 'Our child process function looks very similar to the main training loop in
    the data-parallel version, which is not surprising, as the responsibilities of
    our child process increased. However, instead of asking the optimizer to update
    the network, we gather gradients and send them to the queue. The rest of the code
    is almost the same:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的子进程函数与数据并行版本中的主训练循环非常相似，这并不令人惊讶，因为我们的子进程承担的责任增加了。然而，我们并没有要求优化器更新网络，而是收集梯度并将其发送到队列中。其余的代码几乎没有变化：
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Up to this point, we’ve gathered the batch with transitions and handled the
    end-of-episode rewards.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经收集了包含转换的批次并处理了回合结束的奖励。
- en: 'In the next part of the function, we calculate the combined loss from the training
    data and perform backpropagation of the loss:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的下一部分，我们从训练数据中计算组合损失并执行损失的反向传播：
- en: '[PRE26]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the following code, we send our intermediate values that we’re going to
    monitor during the training to TensorBoard:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们将传送在训练过程中要监视的中间值到 TensorBoard：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At the end of the loop, we need to clip the gradients and extract them from
    the network’s parameters into a separate buffer (to prevent them from being corrupted
    by the next iteration of the loop). Here, we effectively store gradients in the
    tensor.grad field for every network parameter. This could be done without bothering
    with synchronization with other workers, as our network’s parameters are shared,
    but the gradients are locally allocated by every process:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环结束时，我们需要剪裁梯度，并将其从网络的参数中提取到一个单独的缓冲区中（以防它们被下次循环的迭代损坏）。在这里，我们实际上将梯度存储在每个网络参数的`tensor.grad`字段中。这可以在不需要与其他工作进程同步的情况下完成，因为我们的网络参数是共享的，但梯度是由每个进程本地分配的：
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The last line in grads_func puts None into the queue, signaling that this child
    process has reached the game solved state and training should be stopped.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`grads_func`中的最后一行将None放入队列，表示该子进程已达到游戏解决状态，训练应当停止。'
- en: 'The main process starts with the creation of the network and sharing of its
    weights:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程从创建网络并共享其权重开始：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, as in the previous section, we need to set a start method for torch.multiprocessing
    and limit the number of threads started by OpenMP. This is done by setting the
    environment variable OMP_NUM_THREADS, which instructs the OpenMP library about
    the number of threads it can start. OpenMP ([https://www.openmp.org/](https://www.openmp.org/))
    is heavily used by the Gym and OpenCV libraries to provide a speed-up on multicore
    systems, which is a good thing most of the time. By default, the process that
    uses OpenMP starts a thread for every core in the system. But in our case, the
    effect from OpenMP is the opposite: as we’re implementing our own parallelism,
    by launching several processes, extra threads overload the cores with frequent
    context switches, which negatively impacts performance. To avoid this, we explicitly
    limit the amount of threads to one thread. If you want, you can experiment yourself
    with this parameter. On my system, I experienced a 3-4x performance drop without
    this environment variable assignment.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与前一部分一样，我们需要为`torch.multiprocessing`设置启动方法，并限制OpenMP启动的线程数量。这是通过设置环境变量`OMP_NUM_THREADS`来完成的，该变量告诉OpenMP库可以启动的线程数。OpenMP（[https://www.openmp.org/](https://www.openmp.org/)）被Gym和OpenCV库广泛使用，以在多核系统上提供加速，大多数情况下这是好事。默认情况下，使用OpenMP的进程会为系统中的每个核心启动一个线程。但在我们的案例中，OpenMP的效果正好相反：由于我们正在实现自己的并行化，通过启动多个进程，额外的线程会通过频繁的上下文切换给核心带来负担，从而对性能产生负面影响。为避免这种情况，我们明确将线程数限制为一个线程。如果你愿意，可以自己尝试调整这个参数。在我的系统上，没有设置该环境变量时，我体验到了3-4倍的性能下降。
- en: 'Then, we create the communication queue and spawn the required count of child
    processes:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建通信队列并生成所需数量的子进程：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can get to the training loop:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进入训练循环：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The major difference from the data-parallel version of A3C lies in the training
    loop, which is much simpler here, as child processes have done all the heavy calculations
    for us. In the beginning of the loop, we handle the situation when one of the
    processes has reached the required mean reward (when this happens, we have None
    in the queue). In this case, we just exit the loop to stop the training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据并行版本的A3C相比，主要的区别在于训练循环，这里更加简单，因为子进程已经为我们完成了所有繁重的计算。在循环开始时，我们处理当某个进程已达到所需的平均奖励时的情况（此时队列中为None）。在这种情况下，我们直接退出循环以停止训练。
- en: 'We sum gradients together for all the parameters in our network:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有网络参数的梯度加总在一起：
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When we have accumulated enough gradient pieces, we convert the sum of the
    gradients into the PyTorch FloatTensor and assign them to the grad field of the
    network parameters. To average the gradients from different children, we call
    the optimizer’s step() function for every TRAIN_BATCH gradient obtained. For intermediate
    steps, we just sum the corresponding gradients together:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们累积了足够的梯度片段后，我们将梯度总和转换为PyTorch的FloatTensor，并将其赋值给网络参数的`grad`字段。为了对不同子进程的梯度进行平均，我们会为每个获取到的`TRAIN_BATCH`梯度调用优化器的step()函数。对于中间步骤，我们仅仅将对应的梯度加在一起：
- en: '[PRE33]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: After that, all we need to do is call the optimizer’s step() method to update
    the network parameters using the accumulated gradients.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们所需要做的就是调用优化器的step()方法，通过累积的梯度更新网络参数。
- en: 'On the exit from the training loop, we stop all child processes to make sure
    that we terminated them, even if Ctrl + C was pressed to stop the optimization:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当退出训练循环时，我们停止所有子进程，以确保它们已被终止，即使按下Ctrl + C停止优化：
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This step is needed to prevent zombie processes from occupying GPU resources.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤是为了防止僵尸进程占用GPU资源。
- en: Results
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果
- en: This example can be started the same way as the previous example, and after
    a while, it should start displaying the speed and mean reward. However, you need
    to be aware that displayed information is local for every child process, which
    means that speed, the count of games completed, and the number of frames need
    to be multiplied by the number of processes. My benchmarks have shown speed to
    be around 500-600 FPS for every child, which gives 2,000-2,400 FPS in total.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例可以像之前的示例一样启动，过一段时间后，应该开始显示速度和平均奖励。然而，你需要注意，显示的信息对于每个子进程都是局部的，这意味着速度、完成的游戏数量和帧数需要乘以进程的数量。我的基准测试显示每个子进程的速度大约是500-600
    FPS，总计大约是2,000-2,400 FPS。
- en: 'Convergence dynamics are also very similar to the previous version. The total
    number of observations is about 8…10 million, which requires about 1.5 hours to
    complete. The reward chart on the left shows individual processes, but the speed
    chart on the right shows the sum of all processes. As you can see, gradient parallelism
    gives slightly higher performance than data parallelism:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛动态与先前版本非常相似。总的观察次数大约为800万到1000万，完成这些需要大约1.5小时。左侧的奖励图显示了各个进程，右侧的速度图显示了所有进程的总和。如你所见，梯度并行比数据并行略微提高了性能：
- en: '![PIC](img/B22150_12_15.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_12_15.png)'
- en: 'Figure 12.15: Comparison of A2C and A3C in terms of reward (left) and speed
    (right)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.15：A2C和A3C在奖励（左）和速度（右）方面的比较
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned about one of the most widely used methods in deep
    RL: A2C, which wisely combines the policy gradient update with the value of the
    state approximation. We analyzed the effect of the baseline on the statistics
    and convergence of gradients. Then, we checked the extension of the baseline idea:
    A2C, where a separate network head provides us with the baseline for the current
    state. In addition, we discussed why it is important for policy gradient methods
    to gather training data from multiple environments, due to their on-policy nature.
    We also implemented two different approaches to A3C, in order to parallelize and
    stabilize the training process. Parallelization will come up once again in this
    book, when we discuss black-box methods (Chapter [17](ch021.xhtml#x1-31100017)).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了深度强化学习中最广泛使用的方法之一：A2C，该方法巧妙地将策略梯度更新与状态近似值结合起来。我们分析了基准（baseline）对统计量和梯度收敛的影响。然后，我们检查了基准思想的扩展：A2C，其中一个独立的网络头为我们提供当前状态的基准。此外，我们讨论了为什么对于策略梯度方法来说，从多个环境收集训练数据非常重要，因为它们是基于策略的（on-policy）。我们还实现了两种不同的A3C方法，以实现训练过程的并行化和稳定化。并行化将在本书中再次出现，我们将在讨论黑盒方法时提到（第[17](ch021.xhtml#x1-31100017)章）。
- en: In the next two chapters, we will take a look at practical problems that can
    be solved using policy gradient methods, which will wrap up the policy gradient
    methods part of the book.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两章中，我们将探讨使用策略梯度方法可以解决的实际问题，这也将总结本书关于策略梯度方法的部分内容。
- en: Join our community on Discord
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的社区
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、深度学习专家以及作者本人一起阅读本书。提问，向其他读者提供解决方案，通过“问我任何问题”（Ask Me Anything）环节与作者交流，等等。扫描二维码或访问链接加入社区。[https://packt.link/rl](https://packt.link/rl)
- en: '![PIC](img/file1.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
