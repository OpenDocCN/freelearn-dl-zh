- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Other Useful Deep Learning Libraries
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他有用的深度学习库
- en: TensorFlow from Google is not the only framework available for deep learning
    tasks. There is a good range of libraries and frameworks available, each with
    its special features, capabilities, and use cases. In this chapter, we will explore
    some of the popular deep learning libraries and compare their features.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Google 的 TensorFlow 并不是唯一可用于深度学习任务的框架。市场上有许多不同的库和框架，每个都有其独特的功能、特性和应用场景。在本章中，我们将探索一些流行的深度学习库，并比较它们的功能。
- en: 'The chapter will include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包括：
- en: Hugging Face
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face
- en: H2O
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O
- en: PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: ONNX
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ONNX
- en: Open AI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open AI
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp16](https://packt.link/dltfchp16).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 [https://packt.link/dltfchp16](https://packt.link/dltfchp16) 找到。
- en: Let’s begin!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Hugging Face
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Hugging Face is not new for us; *Chapter 6*, *Transformers*, introduced us to
    the library. Hugging Face is an NLP-centered startup, founded by Delangue and
    Chaumond in 2016\. It has, in a short time, established itself as one of the best
    tools for all NLP-related tasks. The AutoNLP and accelerated inference API are
    available for a price. However, its core NLP libraries datasets, tokenizers, Accelerate,
    and transformers (*Figure 16.1*) are available for free. It has built a cool community-driven
    open-source platform.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 对我们来说并不陌生；*第 6 章*，*Transformers*，向我们介绍了这个库。Hugging Face 是一家以 NLP
    为中心的初创公司，由 Delangue 和 Chaumond 于 2016 年创办。它在短时间内已成为处理所有 NLP 相关任务的最佳工具之一。AutoNLP
    和加速推理 API 需要付费使用，但其核心 NLP 库 datasets、tokenizers、Accelerate 和 transformers（*图 16.1*）是免费的。它已经建立了一个由社区驱动的开源平台。
- en: '![Diagram  Description automatically generated with medium confidence](img/B18331_16_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成，置信度中等](img/B18331_16_01.png)'
- en: 'Figure 16.1: NLP libraries from Hugging Face'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1：来自 Hugging Face 的 NLP 库
- en: 'The core of the Hugging Face ecosystem is its transformers library. The Tokenizers
    and Datasets libraries support the Transformers library. To use these libraries,
    we need to install them first. Transformers can be installed using a simple `pip
    install` command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 生态系统的核心是其 transformers 库。Tokenizers 和 Datasets 库支持 transformers
    库。要使用这些库，我们需要先安装它们。Transformers 可以通过简单的 `pip install` 命令进行安装：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Some of the out-of-the-box models available with Hugging Face are text summarization,
    question answering, text classification, audio classification, automatic speech
    recognition, feature extraction, image classification, and translation. In *Figure
    16.2*, we can see the result of the out-of-the-box summarization model available
    with Hugging Face.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 提供的一些开箱即用的模型包括文本摘要、问答、文本分类、音频分类、自动语音识别、特征提取、图像分类和翻译。在*图 16.2*中，我们可以看到
    Hugging Face 提供的开箱即用的摘要模型的结果。
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18331_16_02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含图形用户界面的图片 说明自动生成](img/B18331_16_02.png)'
- en: 'Figure 16.2: Out-of-the-box text summarization using Hugging Face'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2：使用 Hugging Face 进行开箱即用的文本摘要
- en: Besides these out-of-the-box models, we can use the large number of models and
    datasets available at Hugging Face Hub and can use them with PyTorch, TensorFlow,
    and JAX to build customized models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些开箱即用的模型，我们还可以使用 Hugging Face Hub 上提供的大量模型和数据集，并可以与 PyTorch、TensorFlow 和
    JAX 一起使用来构建定制化的模型。
- en: OpenAI
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI
- en: OpenAI is another well-known name for people working in the field of reinforcement
    learning. Their Gym module is a standard toolkit used by developers across the
    globe for developing and comparing reinforcement learning algorithms. In *Chapter
    11*, *Reinforcement Learning*, we have already covered the Gym module in detail.
    In this chapter, we will explore two more offerings by OpenAI.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 是另一个在强化学习领域中为人所知的名字。他们的 Gym 模块是全球开发人员用于开发和比较强化学习算法的标准工具包。在*第 11 章*，*强化学习*，我们已经详细介绍了
    Gym 模块。在本章中，我们将探讨 OpenAI 提供的另外两个工具。
- en: OpenAI GPT-3 API
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI GPT-3 API
- en: “OpenAI GPT3 is a machine learning platform that allows developers to build
    custom algorithms for deep learning. This platform was released in December of
    2017 and has been widely used by businesses and individuals in the field of artificial
    intelligence. One of the primary reasons that GPT3 has been so successful is because
    it is easy to use and has a wide range of features. This platform is able to learn
    from data and can be used for a variety of tasks, including deep learning, natural
    language processing, and image recognition. GPT3 is also popular because it is
    open source and can be used by anyone. This makes it an ideal platform for anyone
    who wants to learn about deep learning and the various ways that it can be used.
    Overall, GPT3 is a powerful and easy-to-use machine learning platform that has
    been widely used by businesses and individuals in the field of artificial intelligence.”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “OpenAI GPT-3 是一个机器学习平台，允许开发者为深度学习构建自定义算法。该平台于2017年12月发布，并已广泛应用于人工智能领域的企业和个人。GPT-3
    成功的主要原因之一是它易于使用，并且具有广泛的功能。这个平台能够从数据中学习，并可用于多种任务，包括深度学习、自然语言处理和图像识别。GPT-3 还因为是开源的，任何人都可以使用，因此它成为了任何想要学习深度学习及其各种应用方式的人的理想平台。总的来说，GPT-3
    是一个功能强大且易于使用的机器学习平台，在人工智能领域被企业和个人广泛应用。”
- en: 'This is the text generated by the OpenAI GPT-3 API, when asked to write on
    GPT-3 itself ([https://beta.openai.com/playground](https://beta.openai.com/playground)):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 OpenAI GPT-3 API 生成的文本，当被要求撰写关于 GPT-3 本身的内容时 ([https://beta.openai.com/playground](https://beta.openai.com/playground))：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_16_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 说明自动生成](img/B18331_16_03.png)'
- en: 'Figure 16.3: Text generation using OpenAI GPT-3 API'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：使用 OpenAI GPT-3 API 进行文本生成
- en: 'The OpenAI GPT-3 API offers the following tasks:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT-3 API 提供以下任务：
- en: '**Text Completion**: Here, the GPT-3 API is used to generate or manipulate
    text and even code. You can use it to write a tagline, an introduction, or an
    essay, or you can leave a sentence half-written and ask it to complete it. People
    have used it to generate stories and advertisement leads.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本补全**：在这里，GPT-3 API 被用来生成或操作文本，甚至是代码。你可以用它来写广告语、介绍或文章，或者你可以留下半句未写完的句子，要求它补全。人们已经用它来生成故事和广告文案。'
- en: '**Semantic Search**: This allows you to do a semantic search over a set of
    documents. For example, you can upload documents using the API; it can handle
    up to 200 documents, where each file can be a maximum of 150 MB in size, and the
    total limited to 1 GB at any given time. The API will take your query and rank
    the documents based on the semantic similarity score (ranges normally between
    0-300).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义搜索**：这允许你对一组文档进行语义搜索。例如，你可以通过 API 上传文档；它最多可以处理200个文档，每个文件的最大大小为150MB，总大小限制为1GB。API
    会根据语义相似度得分（通常在0-300之间）对文档进行排名。'
- en: '**Question Answering**: This API uses the documents uploaded as the source
    of truth; the API first searches the documents for relevance to the question.
    Then it ranks them based on the semantic relevance and finally, answers the question.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答系统**：该 API 使用上传的文档作为真实来源；API 首先搜索文档中与问题相关的内容。然后根据语义相关性对其进行排序，最后回答问题。'
- en: '**Text Classification**: The text classification endpoint of OpenAI GPT-3 takes
    as input a labeled set of examples and then uses the labels in it to label the
    query text. There are a lot of examples where this feature has been used to perform
    sentiment analysis.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：OpenAI GPT-3 的文本分类端点接受一组带标签的示例作为输入，然后利用其中的标签对查询文本进行标注。有许多例子表明，这一功能已被用于情感分析。'
- en: Initially, the OpenAI GPT-3 was available only after applying for it, but now,
    anyone can use the API; there is no longer a waitlist.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，OpenAI GPT-3 仅在申请后才可使用，但现在，任何人都可以使用该 API；不再需要排队等待。
- en: OpenAI DALL-E 2
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI DALL-E 2
- en: 'The GPT-3 API by OpenAI deals with all things related to NLP; DALL-E 2 goes
    a step further. DALL-E was originally released by OpenAI in January 2021\. It
    claims to produce photorealistic images based on the textual description provided
    to the model. It can also make realistic edits to existing images; you can use
    it to add or remove objects and elements from the image, and when it does so,
    it considers the effect on shadows, reflections, and texture. *Figure 16.4* shows
    some of the remarkable feats by DALL-E 2\. In the figures on the top row, I gave
    DALL-E 2 a text describing what I want: “Albert Einstein flying on dinosaur over
    the Amazon Forest.” It generated a cartoon-like image. The images in the lower
    row are generated using the image-editor feature of DALL-E 2\. I added the image
    on the left, and it generated four variations. The variations look very realistic
    if you ignore that the faces are blurred:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 GPT-3 API 涉及与自然语言处理（NLP）相关的所有内容；而 DALL-E 2 更进一步。DALL-E 最初由 OpenAI 于2021年1月发布。它声称能够根据提供给模型的文本描述生成照片级真实感图像。它还可以对现有图像进行真实感编辑；你可以使用它添加或删除图像中的物体和元素，当它执行这些操作时，会考虑阴影、反射和纹理的影响。*图16.4*
    展示了 DALL-E 2 的一些令人瞩目的成就。在顶部的图像中，我给 DALL-E 2 提供了一个描述我想要的文本：“阿尔伯特·爱因斯坦骑着恐龙飞越亚马逊雨林。”它生成了一个卡通风格的图像。下排的图像是使用
    DALL-E 2 的图像编辑功能生成的。我添加了左侧的图像，它生成了四个变体。如果忽略面部模糊的话，变体看起来非常逼真：
- en: '![](img/B18331_16_04_1.png)![](img/B18331_16_04_2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_16_04_1.png)![](img/B18331_16_04_2.png)'
- en: 'Figure 16.4: On top is the image generated by DALL-E 2, and below are the images
    edited by DALL-E 2'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：上方是 DALL-E 2 生成的图像，下方是 DALL-E 2 编辑的图像
- en: At the time of writing this book (August 2022), DALL-E 2 is not available for
    public use. But imagine the possibilities for artists and professionals working
    in creating digital media once the model is available as an API.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时（2022年8月），DALL-E 2 还不能公开使用。但想象一下，当这个模型作为 API 发布后，艺术家和从事数字媒体创作的专业人士将拥有怎样的无限可能。
- en: OpenAI Codex
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Codex
- en: When a student starts with their first lessons of programming, as a teacher,
    I often recommend that they think of a program as a set of instructions – the
    only important thing to master is writing those instructions as clearly as possible
    in whatever language you know.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当学生开始学习编程的第一课时，作为老师，我通常建议他们将程序看作一组指令——唯一需要掌握的就是用你知道的语言尽可能清晰地编写这些指令。
- en: Well, Codex makes it happen, you just need to give it the instructions of what
    you want to achieve, and it will generate the respective code for you.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，Codex 使这一切成为可能，你只需要给它提供你想要实现的指令，它就会为你生成相应的代码。
- en: OpenAI launches it as a general-purpose programming model, and it has been trained
    on publicly available GitHub codes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 将其推出作为一个通用编程模型，并且它已经在公开的 GitHub 代码上进行了训练。
- en: 'Below are a few snippets of the task and corresponding code generated by Codex:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Codex 生成的任务和相应代码的一些片段：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first task, as you can see, is done flawlessly. In the second task, we asked
    it to find the sum of the Fibonacci sequence; instead, it generated the Fibonacci
    sequence, which is a more common problem. This tells us that while it is great
    at doing run-of-the-mill jobs, the need for real programmers is still there.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务，如你所见，完成得非常完美。在第二个任务中，我们要求它计算斐波那契数列的和；然而，它生成了斐波那契数列本身，这是一个更常见的问题。这告诉我们，尽管它在处理常见任务方面非常出色，但对于真正的程序员的需求依然存在。
- en: PyTorch
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch
- en: Like TensorFlow, PyTorch is a full-fledged deep learning framework. In AI-based
    social groups, you will often find die-hard fans of PyTorch and TensorFlow arguing
    that theirs is best. PyTorch, developed by Facebook (Meta now), is an open-source
    deep learning framework. Many researchers prefer it for its flexible and modular
    approach. PyTorch also has stable support for production deployment. Like TF,
    the core of PyTorch is its tensor processing library and its automatic differentiation
    engine. In a C++ runtime environment, it leverages TorchScript for an easy transition
    between graph and eager mode. The major feature that makes PyTorch popular is
    its ability to use dynamic computation, i.e., its ability to dynamically build
    the computational graph – this gives the programmer flexibility to modify and
    inspect the computational graphs anytime.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TensorFlow 类似，PyTorch 是一个完整的深度学习框架。在基于 AI 的社交圈中，你经常会看到 PyTorch 和 TensorFlow
    的死忠粉丝争论谁才是最好的。PyTorch 由 Facebook（现在是 Meta）开发，是一个开源的深度学习框架。许多研究人员偏爱它，因为它具有灵活和模块化的特点。PyTorch
    还对生产部署提供了稳定的支持。像 TensorFlow 一样，PyTorch 的核心是其张量处理库和自动微分引擎。在 C++ 运行时环境中，它利用 TorchScript
    实现图模式和急切模式之间的轻松转换。使 PyTorch 流行的主要特性是其支持动态计算，即能够动态构建计算图——这使得程序员能够在任何时候修改和检查计算图，增加了灵活性。
- en: 'The PyTorch library consists of many modules, which are used as building blocks
    to make complex models. Additionally, PyTorch also provides convenient functions
    to transfer variables and models between different devices viz CPU, GPU, or TPU.
    Of special mention are the following three powerful modules:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 库由多个模块组成，这些模块作为构建复杂模型的基础块。此外，PyTorch 还提供了方便的功能来在不同设备之间传输变量和模型，例如 CPU、GPU
    或 TPU。特别需要提到的是以下三个强大的模块：
- en: '**NN Module**: This is the base class where all layers and functions to build
    a deep learning network are. Below, you can see the code snippet where the NN
    module is used to build a network. The network can then be instantiated using
    the statement `net = My_Net(1,10,5)`; this creates a network with one input channel,
    10 output neurons, and a kernel of size `5x5`:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NN 模块**：这是所有层和构建深度学习网络所需函数的基类。下面，你可以看到一个代码片段，展示了如何使用 NN 模块构建网络。然后可以使用语句 `net
    = My_Net(1,10,5)` 来实例化该网络；这会创建一个具有一个输入通道、10 个输出神经元和 `5x5` 大小卷积核的网络：'
- en: '[PRE2]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is a summary of the network:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是该网络的概述：
- en: '[PRE3]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Autograd Module**: This is the heart of PyTorch. The module provides classes
    and functions that are used for implementing automatic differentiation. The module
    creates an acyclic graph called the dynamic computational graph; the leaves of
    this graph are the input tensors, and the root is the output tensors. It calculates
    a gradient by tracing the root to the leaf and multiplying every gradient in the
    path using the chain rule. The following code snippet shows how to use the Autograd
    module for calculating gradients. The `backward()` function computes the gradient
    of the loss with respect to all the tensors whose `requires_grad` is set to `True`.
    So suppose you have a variable `w`, then after the call to `backward(),` the tensor
    `w.grad` will give us the gradient of the loss with respect to `w`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Autograd 模块**：这是 PyTorch 的核心。该模块提供了用于实现自动微分的类和函数。该模块创建了一个称为动态计算图的无环图；图的叶子是输入张量，根是输出张量。它通过沿着路径从根追踪到叶子并利用链式法则相乘，来计算梯度。下面的代码片段展示了如何使用
    Autograd 模块来计算梯度。`backward()` 函数计算损失相对于所有 `requires_grad` 设置为 `True` 的张量的梯度。假设你有一个变量
    `w`，那么在调用 `backward()` 后，张量 `w.grad` 将给出损失相对于 `w` 的梯度。'
- en: 'We can then use this to update the variable `w` as per the learning rule:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们可以利用这个规则来根据学习规则更新变量 `w`：
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Optim Module**: The Optim module implements various optimization algorithms.
    Some of the optimizer algorithms available in Optim are SGD, AdaDelta, Adam, SparseAdam,
    AdaGrad, and LBFGS. One can also use the Optim module to create complex optimizers.
    To use the Optim module, one just needs to construct an optimizer object that
    will hold the current state and will update the parameters based on gradients.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Optim 模块**：Optim 模块实现了各种优化算法。Optim 中提供的一些优化器算法包括 SGD、AdaDelta、Adam、SparseAdam、AdaGrad
    和 LBFGS。用户还可以使用 Optim 模块创建复杂的优化器。要使用 Optim 模块，只需构建一个优化器对象，该对象将保存当前状态并根据梯度更新参数。'
- en: 'PyTorch is used by many companies for their AI solutions. **Tesla** uses PyTorch
    for **AutoPilot**. The Tesla Autopilot, which uses the footage from eight cameras
    around the vehicle, passes that footage through 48 neural networks for object
    detection, semantic segmentation, and monocular depth estimation. The system provides
    level 2 vehicle automation. They take video from all eight cameras to generate
    road layout, any static infrastructure (e.g., buildings and traffic/electricity
    poles), and 3D objects (other vehicles, persons on the road, and so on). The networks
    are trained iteratively in real time. While a little technical, this 2019 talk
    by Andrej Karpathy, Director of AI at Tesla, gives a bird’s-eye view of Autopilot
    and its capabilities: [https://www.youtube.com/watch?v=oBklltKXtDE&t=670s](https://www.youtube.com/watch?v=oBklltKXtDE&t=670s).
    Uber’s Pyro, a probabilistic deep learning library, and OpenAI are other examples
    of big AI companies using PyTorch for research and development.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch被许多公司用于他们的AI解决方案。**特斯拉**使用PyTorch进行**自动驾驶**。特斯拉的自动驾驶系统使用车辆周围八个摄像头的视频，将这些视频传递通过48个神经网络进行物体检测、语义分割和单眼深度估计。该系统提供了2级车辆自动化。他们从所有八个摄像头获取视频，以生成道路布局、任何静态基础设施（如建筑物和交通/电力杆）以及3D物体（其他车辆、路上的行人等）。这些网络通过实时迭代进行训练。尽管有些技术性，但这是特斯拉AI主管Andrej
    Karpathy在2019年的一场演讲，它从鸟瞰视角展示了自动驾驶的能力：[https://www.youtube.com/watch?v=oBklltKXtDE&t=670s](https://www.youtube.com/watch?v=oBklltKXtDE&t=670s)。Uber的Pyro，一个概率深度学习库，以及OpenAI是其他使用PyTorch进行研发的大型AI公司的例子。
- en: ONNX
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ONNX
- en: '**Open Neural Network Exchange** (**ONNX**) provides an open-source format
    for AI models. It supports both deep learning models and traditional machine learning
    models. It is a format designed to represent any type of model, and it achieves
    this by using an intermediate representation of the computational graph created
    by different frameworks. It supports PyTorch, TensorFlow, MATLAB, and many more
    deep learning frameworks. Thus, using ONNX, we can easily convert models from
    one framework to another. This helps in reducing the time from research to deployment.
    For example, you can use ONNX to convert a PyTorch model to ONNX.js form, which
    can then be directly deployed on the web.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放神经网络交换** (**ONNX**) 提供了一个用于AI模型的开源格式。它支持深度学习模型和传统的机器学习模型。ONNX是一个设计用来表示任何类型模型的格式，它通过使用不同框架创建的计算图的中间表示来实现这一目标。它支持PyTorch、TensorFlow、MATLAB等多个深度学习框架。因此，使用ONNX，我们可以轻松地将模型从一个框架转换到另一个框架。这有助于减少从研究到部署的时间。例如，您可以使用ONNX将PyTorch模型转换为ONNX.js格式，然后可以直接在网页上部署。'
- en: H2O.ai
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O.ai
- en: H2O is a fast, scalable machine learning and deep learning framework developed
    by H2O.ai, released under the open-source Apache license. According to the company
    website, as of the time of writing this book, more than 20,000 organizations use
    H2O for their ML/deep learning needs. The company offers many products like H2O
    AI cloud, H2O Driverless AI, H2O wave, and Sparkling Water. In this section, we
    will explore its open-source product, H2O.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: H2O是由H2O.ai开发的快速、可扩展的机器学习和深度学习框架，遵循开源Apache许可发布。根据公司官网的信息，截至本书编写时，已有超过20,000个组织使用H2O来满足他们的机器学习/深度学习需求。公司提供了许多产品，如H2O
    AI云、H2O Driverless AI、H2O wave和Sparkling Water。在本节中，我们将探索其开源产品H2O。
- en: It works on big data infrastructure on Hadoop, Spark, or Kubernetes clusters
    and it can also work in standalone mode. It makes use of distributed systems and
    in-memory computing, which allows it to handle a large amount of data in memory,
    even with a small cluster of machines. It has an interface for R, Python, Java,
    Scala, and JavaScript, and even has a built-in web interface.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在Hadoop、Spark或Kubernetes集群的大数据基础设施上运行，也可以在独立模式下工作。它利用分布式系统和内存计算，使其能够在即使只有少量机器的集群中，也能处理大量数据。它有R、Python、Java、Scala和JavaScript的接口，甚至还有一个内置的Web界面。
- en: H2O includes a large number of statistical-based ML algorithms such as generalized
    linear modeling, Naive Bayes, random forest, gradient boosting, and all major
    deep learning algorithms. The best part of H2O is that one can build thousands
    of models, compare the results, and even do hyperparameter tuning with only a
    few lines of code. H2O also has better data pre-processing tools.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: H2O包括大量基于统计的机器学习算法，如广义线性模型、朴素贝叶斯、随机森林、梯度提升以及所有主要的深度学习算法。H2O的最佳部分是用户可以仅通过几行代码构建数千个模型、比较结果，甚至进行超参数调优。H2O还拥有更好的数据预处理工具。
- en: 'H2O requires Java, therefore, ensure that Java is installed on your system.
    You can install H2O to work in Python using PyPi, as shown in the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: H2O需要Java环境，请确保系统已安装Java。您可以使用PyPi安装H2O，以便在Python中使用，如下所示的代码：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: H2O AutoML
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H2O的AutoML
- en: One of the most exciting features of H2O is **AutoML**, the automatic ML. It
    is an attempt to develop a user-friendly ML interface that can be used by beginners
    and non-experts. H2O AutoML automates the process of training and tuning a large
    selection of candidate models. Its interface is designed so that users just need
    to specify their dataset, input and output features, and any constraints they
    want on the number of total models trained, or time constraints. The rest of the
    work is done by the AutoML itself, in the specified time constraint; it identifies
    the best performing models and provides a **leaderboard**. It has been observed
    that usually, the Stacked Ensemble model, the ensemble of all the previously trained
    models, occupies the top position on the leaderboard. There is a large number
    of options that advanced users can use; details of these options and their various
    features are available at [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: H2O最令人兴奋的功能之一是**AutoML**，即自动机器学习。它是开发的一种旨在为初学者和非专家提供友好的ML接口的尝试。H2O AutoML自动化了训练和调整大量候选模型的过程。其界面设计得使用户只需指定其数据集、输入和输出特征以及任何对总训练模型数或时间约束的限制。其余工作由AutoML本身完成，在指定的时间限制内，它确定表现最佳的模型并提供一个**排行榜**。通常观察到，堆叠集成模型，即所有先前训练模型的集成，通常占据排行榜的顶部位置。高级用户可以使用大量选项；有关这些选项及其各种特性的详细信息，请参阅[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml)。
- en: 'To learn more about H2O, visit their website: [http://h2o.ai](http://h2o.ai).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于H2O的信息，请访问它们的网站：[http://h2o.ai](http://h2o.ai)。
- en: AutoML using H2O
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用H2O的AutoML
- en: 'Let us try H2O AutoML on a synthetically created dataset. We use the scikit-learn
    `make_circles` method to create the data and save it as a CSV file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个合成创建的数据集上尝试H2O AutoML。我们使用scikit-learn的`make_circles`方法创建数据，并将其保存为CSV文件：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Before we can use H2O, we need to initiate its server, which is done using
    the `init()` function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用H2O之前，我们需要初始化其服务器，这通过`init()`函数完成。
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following shows the output we will receive after initializing the H2O server:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了在初始化H2O服务器后我们将收到的输出：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We read the file containing the synthetic data that we created earlier. Since
    we want to treat the problem as a classification problem, whether the points lie
    in a circle or not, we redefine our label `''y''` as `asfactor()` – this will
    tell the H2O AutoML module to treat the variable `y` as categorical, and thus
    the problem as classification. The dataset is split into training, validation,
    and test datasets in a ratio of 60:20:20:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取之前创建的合成数据文件。由于我们希望将问题视为分类问题，即点是否在圆内，我们将我们的标签`'y'`重新定义为`asfactor()` – 这将告诉H2O的AutoML模块将变量`y`视为分类变量，从而将问题视为分类问题。数据集按60:20:20的比例分为训练、验证和测试数据集：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And now we invoke the AutoML module from H2O and train on our training dataset.
    AutoML will search a maximum of 10 models, but you can change the parameter `max_models`
    to increase or decrease the number of models to test:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从H2O调用AutoML模块，并在我们的训练数据集上进行训练。AutoML将搜索最多10个模型，但您可以更改参数`max_models`来增加或减少要测试的模型数量：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For each of the models, it gives a performance summary, for example, in *Figure
    16.5*, you can see the evaluation summary for a binomial GLM:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，它都提供了性能摘要，例如，在*图16.5*中，您可以看到二项式GLM的评估摘要：
- en: '![Text  Description automatically generated](img/B18331_16_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本描述](img/B18331_16_05.png)'
- en: 'Figure 16.5: Performance summary of one of the models by H2O AutoML'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：H2O AutoML模型之一的性能摘要
- en: 'You can check the performance of all the models evaluated by H2O AutoML on
    a leaderboard:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查H2O AutoML评估的所有模型的性能在排行榜上：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the snippet of the leaderboard:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是排行榜的片段：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: H2O model explainability
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H2O模型可解释性
- en: 'H2O provides a convenient wrapper for a number of explainability methods and
    their visualizations using a single function `explain()` with a dataset and model.
    To get explainability on our test data for the models tested by AutoML, we will
    use `aml.explain()`. Below, we use the `explain` module for the `StackedEnsemble_BestOfFamily`
    model – the topmost in the leaderboard (we are continuing with the same data that
    we created in the previous section):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: H2O提供了一个便捷的包装器，使用单个函数`explain()`结合数据集和模型，支持多种可解释性方法及其可视化。为了获取关于我们用于AutoML测试的模型的测试数据可解释性，我们将使用`aml.explain()`。下面，我们使用`explain`模块对`StackedEnsemble_BestOfFamily`模型（排行榜上的最高模型）进行解释（我们继续使用在上一节中创建的数据）：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results are:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_16_06.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 说明自动生成](img/B18331_16_06.png)'
- en: 'Figure 16.6: A confusion matrix on test dataset generated by H2O explain module'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.6：由H2O解释模块生成的测试数据集混淆矩阵
- en: The ground truth is displayed in rows and the prediction by the model in columns.
    For our data, 0 was predicted correctly 104 times, and 1 was predicted correctly
    88 times.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 真实值显示在行中，模型预测显示在列中。对于我们的数据，0被正确预测了104次，1被正确预测了88次。
- en: Partial dependence plots
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分依赖图
- en: '**Partial Dependence Plots** (**PDP**) provide a graphical depiction of the
    marginal effect of a variable on the response of the model. It can tell us about
    the relationship between the output label and the input feature. *Figure 16.7*
    shows the PDP plots as obtained from the H2O `explain` module on our synthetic
    dataset:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**部分依赖图**（**PDP**）提供了变量对模型响应的边际效应的图形表示。它可以告诉我们输出标签与输入特征之间的关系。*图 16.7* 显示了通过H2O
    `explain`模块在我们的合成数据集上获得的PDP图：'
- en: '![A picture containing chart  Description automatically generated](img/B18331_16_07.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图表 说明自动生成](img/B18331_16_07.png)'
- en: 'Figure 16.7: PDP for input features x[1] and x[2]'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.7：输入特征 x[1] 和 x[2] 的PDP
- en: For building PDP plots for each feature, H2O considers the rest of the features
    as constant. So, in the PDP plot for x[1] (x[2]), the feature x[2] (x[1]) is kept
    constant and the mean response is measured, as x[1] (x[2]) is varied. The graph
    shows that both features play an important role in determining if the point is
    a circle or not, especially for values lying between [-0.5, 0.5].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个特征构建PDP图时，H2O会将其余特征视为常数。因此，在x[1]（x[2]）的PDP图中，特征x[2]（x[1]）保持不变，测量均值响应，同时x[1]（x[2]）发生变化。图表显示，两个特征在决定点是否为圆形时都起着重要作用，尤其是对于值位于[-0.5,
    0.5]之间的情况。
- en: Variable importance heatmap
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变量重要性热图
- en: 'We can also check the importance of variables across different models:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查不同模型中变量的重要性：
- en: '![Icon  Description automatically generated](img/B18331_16_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图标 说明自动生成](img/B18331_16_08.png)'
- en: 'Figure 16.8: Variable importance heatmap for input features x[1] and x[2]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8：输入特征x[1]和x[2]的变量重要性热图
- en: '*Figure 16.8* shows how much importance was given to the two input features
    by different algorithms. We can see that the models that gave almost equal importance
    to the two features are doing well on the leaderboard, while **GLM_1**, which
    treated both features quite differently, has only about 41% accuracy.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.8* 显示了不同算法对两个输入特征赋予的权重。我们可以看到，那些几乎对两个特征赋予相等重要性的模型在排行榜上表现良好，而**GLM_1**则对这两个特征处理得差异较大，其准确率仅为约41%。'
- en: Model correlation
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型相关性
- en: 'The prediction between different models is correlated; we can check this correlation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型之间的预测是相关的；我们可以检查这种相关性：
- en: '![Chart, histogram  Description automatically generated](img/B18331_16_09.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图 说明自动生成](img/B18331_16_09.png)'
- en: 'Figure 16.9: Model correlation'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9：模型相关性
- en: '*Figure 16.9* shows the model correlation; it shows the correlation between
    the predictions on the test dataset for different models. It measures the frequency
    of identical predictions to calculate correlations. Again, we can see that except
    for **GLM_1**, most other models perform almost equally, with accuracy ranging
    from 84-93% on the leaderboard.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.9* 显示了模型相关性；它显示了不同模型在测试数据集上的预测相关性。它通过计算相同预测的频率来衡量相关性。同样，我们可以看到，除了**GLM_1**之外，大多数其他模型的表现几乎相同，排行榜上的准确率在84%到93%之间。'
- en: What we have discussed here is just the tip of the iceberg; each of the frameworks
    listed here has entire books on their features and applications. Depending upon
    your use case, you should choose the respective framework. If you are building
    a model for production, TensorFlow is a better choice for both web-based and Edge
    applications. If you are building a model where you need better control of training
    and how the gradients are updated, then PyTorch is better suited. If you need
    to work cross-platform very often, ONNX can be useful. And finally, H2O and platforms
    like OpenAI GPT-3 and DALL-E 2 provide a low-threshold entry into the field of
    artificial intelligence and deep learning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的内容只是冰山一角；每个框架都有关于其功能和应用的完整书籍。根据你的使用场景，你应该选择相应的框架。如果你正在为生产环境构建模型，那么 TensorFlow
    更适合 Web 和边缘应用。如果你需要更好地控制训练过程和梯度更新，PyTorch 更为合适。如果你经常需要跨平台工作，那么 ONNX 会很有用。最后，像
    H2O 和 OpenAI 的 GPT-3 与 DALL-E 2 等平台提供了一个低门槛进入人工智能和深度学习领域的途径。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we briefly covered the features and capabilities of some other
    popular deep learning frameworks, libraries, and platforms. We started with Hugging
    Face, a popular framework for NLP. Then we explored OpenAI’s GPT-3 and DALL-E
    2, both very powerful frameworks. The GPT-3 API can be used for a variety of NLP-related
    tasks, and DALL-E 2 uses GPT-3 to generate images from textual descriptions. Next,
    we touched on the PyTorch framework. According to many people, PyTorch and TensorFlow
    are equal competitors, and PyTorch indeed has many features comparable to TensorFlow.
    In this chapter, we briefly talked about some important features like the NN module,
    Optim module, and Autograd module of PyTorch. We also discussed ONNX, the open-source
    format for deep learning models, and how we can use it to convert the model from
    one framework to another. Lastly, the chapter introduced H2O and its AutoML and
    `explain` modules.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了其他一些流行的深度学习框架、库和平台的特点和功能。我们从 Hugging Face 开始，这是一个流行的 NLP 框架。接着，我们探索了
    OpenAI 的 GPT-3 和 DALL-E 2，它们都是非常强大的框架。GPT-3 API 可以用于各种与 NLP 相关的任务，而 DALL-E 2 使用
    GPT-3 从文本描述中生成图像。然后，我们介绍了 PyTorch 框架。许多人认为 PyTorch 和 TensorFlow 是平起平坐的竞争对手，事实上，PyTorch
    确实有许多与 TensorFlow 相媲美的功能。在本章中，我们简要讨论了 PyTorch 的一些重要模块，如 NN 模块、Optim 模块和 Autograd
    模块。我们还讨论了 ONNX，这是一个开源深度学习模型格式，并介绍了如何使用它将模型从一个框架转换到另一个框架。最后，本章介绍了 H2O 及其 AutoML
    和 `explain` 模块。
- en: In the next chapter, we will learn about graph neural networks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习图神经网络。
- en: Join our book’s Discord space
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人交流，并与超过2000名成员一起学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
