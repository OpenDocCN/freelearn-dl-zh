- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Creating Single- and Multi-Agent Systems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建单代理和多代理系统
- en: In previous chapters, we discussed a number of components or tools that can
    be associated with LLMs to extend their capabilities. In *Chapters 5* and *6*,
    we addressed in detail how external memory can be used to enrich the context.
    This allows the model to obtain additional information to be able to answer user
    questions when it does not know the answer (when it hasn’t seen the document during
    pre-training or it relates to information after the date of their training). Similarly,
    in [*Chapter 7*](B21257_07.xhtml#_idTextAnchor113), we saw that knowledge graphs
    can be used to extend the model’s knowledge. These components attempt to solve
    one of the most problematic limitations of LLMs, namely, hallucinations (an output
    produced by the model that is not factually correct). In addition, we saw that
    the use of graphs allows the model to conduct graph reasoning and thus adds new
    capabilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了许多可以与LLMs相关联的组件或工具，以扩展其功能。在*第5章*和*第6章*中，我们详细讨论了如何使用外部记忆来丰富上下文。这允许模型获得额外的信息，以便在它不知道答案时（当它在预训练期间没有看到文档或它涉及训练日期之后的信息）回答用户的问题。同样，在[*第7章*](B21257_07.xhtml#_idTextAnchor113)中，我们看到了知识图谱可以用来扩展模型的知识。这些组件试图解决LLMs最令人头疼的限制之一，即幻觉（模型产生的非事实正确的输出）。此外，我们还看到，使用图可以使模型进行图推理，从而增加新的能力。
- en: In [*Chapter 8*](B21257_08.xhtml#_idTextAnchor137), we saw the intersection
    of RL and LLMs. One of the problems associated with LLMs is that they could produce
    harmful content (such as biased or toxic content or misinformation). RL algorithms
    allow us to align the behavior of the model with human preferences, thus allowing
    us to reduce the risk of harmful content.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B21257_08.xhtml#_idTextAnchor137)中，我们看到了强化学习（RL）和LLMs的交集。与LLMs相关的一个问题是它们可能会产生有害内容（如偏见或有害内容或错误信息）。RL算法使我们能够使模型的行为与人类偏好保持一致，从而降低有害内容的风险。
- en: 'We can use similar approaches to make the model more capable of performing
    tasks or following instructions. In the future, these reinforcement learning algorithms
    could be useful in overcoming an important limitation of LLMs: a lack of continual
    learning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类似的方法使模型更擅长执行任务或遵循指令。在未来，这些强化学习算法可能有助于克服LLMs的一个重要限制：缺乏持续学习。
- en: The definition of tools, as we will see, is quite broad. In fact, any software
    or algorithm can be a tool. As we have already seen in previous chapters, LLMs
    can execute code or connect to **application programming Interfaces** (**APIs**).
    But this means that they can also invoke other models to perform tasks that they
    are unable to accomplish on their own.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，工具的定义相当广泛。实际上，任何软件或算法都可以是工具。正如我们在前面的章节中已经看到的，LLMs可以执行代码或连接到**应用程序编程接口**（**APIs**）。但这意味着它们也可以调用其他模型来执行它们自己无法完成的任务。
- en: In any case, all these elements have set the seed for what is called the agent
    revolution, in which an LLM can interact with the environment and perform tasks
    in the real world (be it the internet or, in the future, beyond the constraint
    of a computer).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，所有这些元素都为所谓的代理革命奠定了基础，在这个革命中，LLM可以与环境交互并在现实世界中执行任务（无论是互联网还是未来，超越计算机的限制）。
- en: In this chapter, we focus on LLMs, its various tools, and how these can be combined
    to interact with the environment. We will start with the definition of an autonomous
    agent and continue with what the tools (APIs, models, and so on) are and how they
    can be organized. We will see how using prompt engineering techniques (which we
    addressed in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042)) allows us to create
    different types of agents. After that, we will discuss several strategies that
    have been used previously in the literature to connect an LLM to its tools.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于大型语言模型（LLMs），其各种工具，以及如何将这些工具结合起来与环境交互。我们将从自主代理的定义开始，然后继续讨论工具（APIs、模型等）是什么以及它们如何被组织。我们将看到使用提示工程技巧（我们在[*第3章*](B21257_03.xhtml#_idTextAnchor042)中讨论过）如何使我们能够创建不同类型的代理。之后，我们将讨论文献中先前使用的一些策略，这些策略用于将LLM连接到其工具。
- en: This will allow us to see in detail how some technical limitations and challenges
    have been solved. We will then talk in detail about HuggingGPT (an LLM connected
    to hundreds of models), which was a turning point in agent creation. We will see
    how HuggingGPT allows an LLM to solve complex tasks using other expert models.
    Then, we will see how instead of a single agent, we can create multi-agent platforms.
    The interaction of different agents will allow us to solve increasingly complex
    tasks and issues. In addition, we will see how these approaches can be applied
    to complex domains, such as healthcare, chemistry, and law. We will then put what
    we have seen into practice using HuggingGPT. Next, we will extend this concept
    with a multi-agent platform that will allow us to understand how modern systems
    work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够详细了解一些技术限制和挑战是如何被解决的。然后，我们将详细讨论HuggingGPT（一个连接到数百个模型的LLM），这是代理创建的一个转折点。我们将看到HuggingGPT如何允许LLM使用其他专家模型解决复杂任务。然后，我们将看到如何创建多代理平台，而不是单个代理。不同代理的交互将使我们能够解决越来越复杂的问题。此外，我们将看到这些方法如何应用于复杂领域，如医疗保健、化学和法律。然后，我们将使用HuggingGPT将我们所学的内容付诸实践。接下来，我们将通过一个多代理平台扩展这一概念，这将使我们能够理解现代系统是如何工作的。
- en: Once we have seen how agents or multi-agents work, we will discuss in detail
    the new business paradigms that are emerging, such as **Software as a Service**
    (**SaaS**), **Model as a Service** (**MaaS**), **Data as a Service** (**DaaS**),
    and **Results as a Service** (**RaaS**) or **Outcome as a Service** (**OaaS**).
    As we will see in this chapter, each of these business models has advantages and
    disadvantages.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们了解了代理或多代理的工作方式，我们将详细讨论新兴的新商业模式，例如**软件即服务**（**SaaS**）、**模型即服务**（**MaaS**）、**数据即服务**（**DaaS**）和**结果即服务**（**RaaS**）或**成果即服务**（**OaaS**）。正如我们将在本章中看到的，这些商业模式各有优缺点。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to autonomous agents
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自主代理简介
- en: HuggingGPT and other approaches
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingGPT和其他方法
- en: Working with HuggingGPT
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与HuggingGPT一起工作
- en: Multi-agent system
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多代理系统
- en: SaaS, MaaS, DaaS, and RaaS
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SaaS、MaaS、DaaS和RaaS
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in this chapter requires the use of a GPU. For the section on using
    HuggingGPT in particular, both a GPU and plenty of space on the hard disk drive
    are required (several models will be downloaded, including diffusion models. For
    this, it will be necessary to use Git **Large File Storage** (**LFS**), which
    allows downloading wide files via Git). Anaconda should be installed to obtain
    the various libraries (the necessary libraries will be set up directly during
    installation). For readers who do not have these resources, the *Using HuggingGPT
    on the web* section shows how you can use HuggingGPT on the web. For local use
    of HuggingGPT, it is necessary to have an OpenAI token, while for web use, it
    is also necessary to have a Hugging Face token. The multi-agent system is based
    on Python libraries (NumPy, scikit-learn, SentenceTransformers, and Transformers).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码需要使用GPU。特别是对于使用HuggingGPT的部分，需要GPU和硬盘驱动器上的大量空间（将下载多个模型，包括扩散模型。为此，将需要使用Git
    **大型文件存储**（**LFS**），它允许通过Git下载大文件）。应安装Anaconda以获取各种库（必要的库将在安装过程中直接设置）。对于没有这些资源的读者，*使用HuggingGPT在网络上*部分展示了如何在网上使用HuggingGPT。对于本地使用HuggingGPT，需要OpenAI令牌，而对于网络使用，也需要Hugging
    Face令牌。多代理系统基于Python库（NumPy、scikit-learn、SentenceTransformers和Transformers）。
- en: 'HuggingGPT should be run on a GPU. The multi-agent system should be run on
    a GPU, but it could also be run on a CPU; this is, however, highly discouraged.
    The code can be found on GitHub: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr9](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr9).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingGPT应在GPU上运行。多代理系统应在GPU上运行，但也可以在CPU上运行；然而，这被高度不建议。代码可以在GitHub上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr9](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr9)。
- en: Introduction to autonomous agents
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自主代理简介
- en: In the context of AI, **autonomous agents** refer to systems or entities that
    can perform tasks or make decisions independently without the need for human intervention.
    These agents are designed to perceive their environment, reason about it, make
    decisions based on their goals, and take action accordingly to achieve those goals.
    Autonomous agents are considered an important step toward **artificial general
    intelligence** (**AGI**), which is expected to conduct autonomous planning and
    actions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的背景下，**自主代理**指的是可以独立执行任务或做出决策的系统或实体，而无需人类干预。这些代理被设计为感知其环境，对其进行分析，根据其目标做出决策，并据此采取行动以实现这些目标。自主代理被认为是通往**通用人工智能**（AGI）的重要一步，AGI预计将进行自主规划和行动。
- en: The main reason for using LLMs as agents lies in the fact that LLMs have shown
    some reasoning and thus planning capabilities. LLMs use reasoning to interpret
    input, draw inferences, and make decisions (showing some extent of deductive,
    inductive, and abductive reasoning). This allows LLMs to apply general rules to
    specific cases (deductive reasoning), learn patterns from examples (inductive
    reasoning), and infer explanations from incomplete data (abductive reasoning).
    In addition, LLMs are capable of conducting step reasoning by chaining ideas,
    thus enabling them to be able to solve equations or debug code. Also, solving
    some problems (such as math problems) requires following a series of steps. Intrinsically,
    an LLM must often decompose a task into a series of actions, anticipate the results
    of these actions, and adjust its behavior in response to the results. These capabilities,
    however, are limited to the context provided by the user or knowledge gained during
    pre-training, and for fields such as medicine or finance, this is not enough to
    solve most problems. Therefore, the natural response to this limitation is to
    extend the capabilities of the LLM with external tools, or otherwise connect an
    LLM to the external environment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLMs作为代理的主要原因在于LLMs已经显示出一些推理和规划能力。LLMs使用推理来解释输入，进行推理，并做出决策（显示出一定程度的演绎、归纳和类比推理）。这使得LLMs能够将一般规则应用于特定情况（演绎推理），从例子中学习模式（归纳推理），并从不完全数据中推断解释（类比推理）。此外，LLMs能够通过串联想法进行逐步推理，从而使其能够解决方程或调试代码。解决某些问题（如数学问题）需要遵循一系列步骤。本质上，一个LLM必须经常将任务分解成一系列动作，预测这些动作的结果，并根据结果调整其行为。然而，这些能力仅限于用户提供的上下文或预训练期间获得的知识，对于医学或金融等领域，这不足以解决大多数问题。因此，对此限制的自然反应是扩展LLM的能力，使用外部工具，或者将LLM连接到外部环境。
- en: The purpose of some studies and research is therefore to extend the capabilities
    of LLMs with a set of tools. These works and derived libraries try to equip LLMs
    with human capabilities, such as memory and planning, to make them behave like
    humans and complete various tasks effectively.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一些研究和研究的目的是通过一系列工具来扩展大型语言模型（LLMs）的能力。这些工作和由此产生的库试图为LLMs配备人类能力，例如记忆和规划，使它们的行为像人类一样，并有效地完成各种任务。
- en: As the capabilities of LLMs have developed, interest in these agents has grown,
    and numerous articles and frameworks have been published.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs能力的不断发展，对这些代理的兴趣也在增长，已经发表了众多文章和框架。
- en: '![Figure 9.1 – Growing interest in LLM autonomous agents (https://arxiv.org/pdf/2308.11432)](img/B21257_09_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 对LLM自主代理的兴趣日益增长 (https://arxiv.org/pdf/2308.11432)](img/B21257_09_01.jpg)'
- en: Figure 9.1 – Growing interest in LLM autonomous agents ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 对LLM自主代理的兴趣日益增长 ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
- en: 'The first aspect to consider when building these types of systems is the design
    of the architecture and how to use it to perform tasks. Autonomous agents must
    perform different roles, perceive the environment, and learn from it. The purpose
    of the architecture is to assist an LLM in maximizing its capabilities in order
    to be used as an agent. To this end, several modules have been developed, which
    can be divided into four main groups: profiling, memory, planning, and action.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这些类型的系统时，首先要考虑的是架构的设计以及如何使用它来执行任务。自主代理必须执行不同的角色，感知环境，并从中学习。架构的目的是帮助LLM最大化其能力，以便用作代理。为此，已经开发出几个模块，可以分为四个主要组：配置文件、记忆、规划和行动。
- en: '![Figure 9.2 – Possible modules to build LLM-based autonomous agents (https://arxiv.org/pdf/2308.11432)](img/B21257_09_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 构建基于LLM的自主代理的可能模块 (https://arxiv.org/pdf/2308.11432)](img/B21257_09_02.jpg)'
- en: Figure 9.2 – Possible modules to build LLM-based autonomous agents ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 构建基于LLM的自主代理的可能模块 ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
- en: 'Let’s go through each of these in a bit more detail:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地逐一介绍这些内容：
- en: '**Profiling module**: Often, agents perform tasks in specific roles (also called
    personas), such as coders, domain experts, teachers, or assistants. The profiling
    module deals with defining these roles (characteristics, role, psychological and
    social information, and relationships with other agents) in a specific prompt
    given to the LLM. These profiles can then be handwritten (handwritten profiles
    are manually crafted personas or roles defined by developers or domain experts);
    for example, for a system for software development, we can create different job
    roles (“you are a software engineer responsible for code review”). Handwritten
    profiles allow a high degree of control, enriching context, and can be highly
    domain-specific (addressing nuances, soft skills, sophisticated knowledge). Although
    the handwritten approach is very flexible, it is time-consuming and has limited
    scalability. So, some studies have explored systems where LLMs automatically generate
    profiles (using few-shot examples, rules, and templates, or specific external
    datasets as job descriptions). This approach is much more scalable and adaptable
    to different situations (especially if the system is to be dynamic or if feedback
    is received from users). On the other hand, however, there is less control (the
    system loses nuance and depth, with the risk of being generic), the quality is
    variable (depending on the prompt engineering technique, some examples might be
    of poor quality), and it still requires verification by a human.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置文件模块**：通常，代理在特定的角色（也称为角色扮演）中执行任务，例如编码者、领域专家、教师或助手。配置文件模块负责在给定的LLM特定提示中定义这些角色（特征、角色、心理和社会信息，以及与其他代理的关系）。然后，这些配置文件可以是手写的（手写配置文件是由开发者或领域专家手动制作的个人或角色定义）；例如，对于软件开发系统，我们可以创建不同的工作角色（“你是一名负责代码审查的软件工程师”）。手写配置文件允许高度的控制、丰富上下文，并且可以高度特定于领域（解决细微差别、软技能、复杂知识）。尽管手写方法非常灵活，但它耗时且可扩展性有限。因此，一些研究探索了LLM自动生成配置文件的系统（使用少量示例、规则和模板，或特定外部数据集作为工作描述）。这种方法具有更高的可扩展性和对不同情况的适应性（特别是如果系统需要动态或如果收到用户反馈）。然而，另一方面，控制程度较低（系统失去了细微差别和深度，存在变得通用的风险），质量参差不齐（取决于提示工程技术，一些示例可能质量较差），并且仍然需要人工验证。'
- en: '**Memory module**: The memory module stores information perceived by the system
    from the environment or other sources; these memories then facilitate future actions.
    Dedicated memory components can also be sophisticated and inspired by human cognition,
    with components dedicated to perceptual, short- or long-term information. Commonly
    found memories are then entered into the system prompt (so the context length
    of the LLM is the limit for the memory that can be used for the agent). An example
    is the history of chats with a user that is needed for task accomplishment. As
    another example, an agent assisting in the development of a game will have just-occurring
    events and other descriptions as short-term memory. **Hybrid memory** is a way
    of extending memory, where past events and thoughts are saved and found again
    to facilitate the agent’s behavior. Hybrid memory combines short-term (within
    an LLM context) and long-term (external) memory to extend the agent’s capacity
    beyond the LLM’s context window. These thoughts, conversations, or other information
    can be saved via RAG or other systems (database, knowledge graph, and so on).
    When needed, relevant information is retrieved and injected into the LLM prompt,
    allowing the agent to act on prior knowledge without exceeding context limits.
    For example, in RAG, a search mechanism pulls relevant documents or memory fragments
    based on the current query, making responses more informed and consistent over
    time. In addition, this module should cover three operations: memory reading (extracting
    useful information for the agent’s action), memory writing (storing information
    about the environment that may be useful in the future while avoiding duplicates
    and memory overflow), and memory reflection (evaluating and inferring more abstract,
    complex, and high-level information). Specifically, memory reading retrieves information
    to support the agent’s decisions (increasing context continuity and consistency),
    memory writing allows for saving information that is useful for the agent’s interaction
    with the environment (thus reducing redundancy and allowing for overcoming the
    limitations of a noneditable memory), and memory reflection allows for deriving
    insights from the analysis of stored information, thus allowing for adjusting
    behavior to achieve goals.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆模块**：记忆模块存储系统从环境或其他来源感知到的信息；这些记忆随后有助于未来的行动。专门的记忆组件也可以是复杂的，并受到人类认知的启发，具有针对感知、短期或长期信息的组件。常见的记忆随后被输入到系统提示中（因此LLM的上下文长度是可用于代理的记忆的限制）。例如，完成任务所需的与用户的聊天历史。作为另一个例子，协助游戏开发的代理将具有刚刚发生的事件和其他描述作为短期记忆。**混合记忆**是一种扩展记忆的方法，其中过去的事件和思想被保存并再次找到，以促进代理的行为。混合记忆结合了短期（在LLM上下文中）和长期（外部）记忆，以扩展代理的能力，使其超越LLM的上下文窗口。这些思想、对话或其他信息可以通过RAG或其他系统（数据库、知识图谱等）保存。当需要时，相关信息被检索并注入到LLM提示中，使代理能够根据先前知识行事，而不超过上下文限制。例如，在RAG中，搜索机制根据当前查询检索相关的文档或记忆片段，使响应在时间上更加知情和一致。此外，此模块应涵盖三个操作：记忆读取（提取对代理行动有用的信息）、记忆写入（存储可能对未来有用的环境信息，同时避免重复和内存溢出），以及记忆反思（评估和推断更抽象、复杂和高级的信息）。具体来说，记忆读取检索信息以支持代理的决策（增加上下文的连续性和一致性），记忆写入允许保存对代理与环境交互有用的信息（从而减少冗余并允许克服不可编辑记忆的限制），而记忆反思允许从存储信息的分析中得出见解，从而允许调整行为以实现目标。'
- en: '**Planning module**: The planning module is generally used to deconstruct complex
    tasks into more manageable tasks, to make LLMs behave more reasonably, powerfully,
    and reliably. The planning module can include or not include feedback.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划模块**：规划模块通常用于将复杂任务分解成更易于管理的任务，以使大型语言模型（LLM）的行为更加合理、强大和可靠。规划模块可以包含或不包含反馈。'
- en: In planning without feedback, the agent does not receive feedback that influences
    its future behavior after it has conducted an action. In single-path reasoning,
    the task is divided into several intermediate steps connected in a cascading sequence.
    **Chain of thought** (**CoT**) reasoning is often employed to develop a step-by-step
    plan for this strategy. In contrast, multi-path reasoning involves a tree-like
    structure where each intermediate step can branch into multiple subsequent steps.
    These approaches typically leverage **self-consistent CoT** (**CoT-SC**) or **tree
    of thoughts** (**ToT**) frameworks, enabling the evaluation of all intermediate
    steps to identify the optimal strategy. The tree can be even coupled with sophisticated
    strategies such as **Monte Carlo Tree Search** (**MCTS**) or an external planner.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在没有反馈的规划中，代理在执行行动后不会收到影响其未来行为的反馈。在单路径推理中，任务被分为几个中间步骤，这些步骤以级联序列连接。**思维链**（**CoT**）推理通常用于为这种策略制定逐步计划。相比之下，多路径推理涉及一个树状结构，其中每个中间步骤都可以分支成多个后续步骤。这些方法通常利用**自洽思维链**（**CoT-SC**）或**思维树**（**ToT**）框架，以评估所有中间步骤以确定最佳策略。该树甚至可以与复杂的策略相结合，如**蒙特卡洛树搜索**（**MCTS**）或外部规划器。
- en: Planning with feedback is mainly used for long-term tasks, where it is difficult
    to generate an effective plan from the beginning or the dynamics may change. So,
    you can incorporate feedback from the environment and observations. For example,
    the ReAct framework uses thought-act-observation triplets. Another alternative
    is using human feedback or another model to improve the agent’s planning ability.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带反馈的规划主要用于长期任务，在这种情况下，从开始就难以生成有效的计划，或者动态可能发生变化。因此，你可以结合来自环境和观察的反馈。例如，ReAct框架使用思考-行动-观察三元组。另一种选择是使用人类反馈或另一个模型来提高代理的规划能力。
- en: '![Figure 9.3 – Comparison between the strategies of single-path and multi-path
    reasoning (https://arxiv.org/pdf/2308.11432)](img/B21257_09_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 单路径和多路径推理策略的比较](https://arxiv.org/pdf/2308.11432)(img/B21257_09_03.jpg)'
- en: Figure 9.3 – Comparison between the strategies of single-path and multi-path
    reasoning ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 单路径和多路径推理策略的比较([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
- en: '**Action module**: The action module is responsible for translating the planning
    into a specific outcome; this module is then responsible for the interaction.
    In general, this module focuses on the execution of the task and then actions
    with a specific goal. The module is also responsible for communicating with other
    agents (if they are present), exploring the environment, finding the necessary
    memory, and executing the plan. To accomplish these goals, the LLM can use either
    the knowledge gained from the LLM during the pre-training phase or external tools
    (external models, APIs, databases, or other tools). Pre-training knowledge allows
    the LLM to carry out many tasks using learned information, such as generating
    text, answering questions, or making decisions based on prior data. However, for
    more dynamic, real-time, or specialized tasks, the action module uses external
    tools such as APIs, databases, software applications, or other models. These tools
    enable the agent to access up-to-date information, manipulate data, perform calculations,
    or trigger operations in external systems. Together, pre-trained knowledge and
    external tools allow the agent to interact meaningfully with its environment,
    carry out goals, and adapt based on the outcomes of its actions. The action of
    the model has an impact on the environment or internal state of the model, and
    this is evaluated and taken into account by this module.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作模块**：动作模块负责将规划转化为特定的结果；然后该模块负责交互。一般来说，该模块专注于任务的执行以及具有特定目标的行为。该模块还负责与其他代理（如果存在）通信、探索环境、寻找必要的记忆以及执行计划。为了实现这些目标，LLM可以使用在预训练阶段从LLM获得的知识或外部工具（外部模型、API、数据库或其他工具）。预训练知识允许LLM使用学习到的信息执行许多任务，例如生成文本、回答问题或根据先前数据做出决策。然而，对于更动态、实时或专业的任务，动作模块使用外部工具，如API、数据库、软件应用程序或其他模型。这些工具使代理能够访问最新信息、操作数据、执行计算或在外部系统中触发操作。预训练知识和外部工具共同使代理能够与环境进行有意义的交互，实现目标，并根据其行动的结果进行适应。模型的行为对环境或模型内部状态有影响，并由该模块评估和考虑。'
- en: Apart from system architecture, we should also consider strategies to develop
    better agents. Typically, one of the most used strategies is conducting fine-tuning
    of the model. Fine-tuning plays a key role in improving agent performance by adapting
    a general-purpose LLM to specific tasks, domains, or behavioral goals. It helps
    align the model with human values (safety), improve instruction following, or
    specialize in areas such as education or e-commerce. In most cases, human-annotated
    datasets are used for specific tasks. As we discussed in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042),
    this can be for security reasons (alignment with human values), to make it more
    responsive to following instructions (instruction tuning), or to train to a specific
    domain or task. To fine-tune an agent, in the WebShop example ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432)),
    the authors of the paper collected 1.2 million world products from [amazon.com](http://amazon.com)
    and created a simulated e-commerce website. After that, they collected human behaviors
    on the website (when users browse and perform actions on the website, their behaviors
    are registered), thus creating a dataset for fine-tuning specifically for an agent
    dedicated to helping with product selection. Or, in the EduChat example ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432)),
    to create an agent for educational scenarios, the authors collected an annotated
    dataset covering various educational scenarios (the dataset was evaluated and
    edited by specialized personnel, such as psychologists).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了系统架构之外，我们还应该考虑开发更好代理的策略。通常，最常用的策略之一是对模型进行微调。微调通过将通用 LLM 适应特定任务、领域或行为目标，在提高代理性能方面发挥着关键作用。它有助于使模型与人类价值观（安全性）保持一致，提高指令遵循性，或在教育或电子商务等领域进行专业化。在大多数情况下，用于特定任务的数据集都是人工标注的。正如我们在[*第
    3 章*](B21257_03.xhtml#_idTextAnchor042)中讨论的那样，这可能是出于安全原因（与人类价值观保持一致）、使其更易于遵循指令（指令调整）或训练特定领域或任务。以
    WebShop 为例([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))，论文的作者们收集了来自
    [amazon.com](http://amazon.com) 的 120 万个世界产品，并创建了一个模拟的电子商务网站。之后，他们收集了网站上的用户行为（当用户在网站上浏览和执行操作时，他们的行为会被记录），从而创建了一个专门用于帮助产品选择的代理的微调数据集。或者，在
    EduChat 的例子([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))中，为了创建一个用于教育场景的代理，作者们收集了一个覆盖各种教育场景的标注数据集（数据集由心理学家等专业人士评估和编辑）。
- en: 'Collecting these datasets is expensive and requires specialized personnel in
    several cases. Therefore, an alternative is to use an LLM to annotate the dataset.
    When this approach is followed, there is a trade-off between quality and cost:
    the dataset is not as good as that annotated by humans, but the costs are much
    reduced. For example, in ToolBench (an agent system where the LLM is connected
    to APIs), the authors of that work ([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))
    collected more than 16,000 real-world APIs and then annotated this dataset with
    ChatGPT. Then, they fine-tuned LLaMA on this dataset. The fine-tuned model was
    much more performant in using these APIs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 收集这些数据集成本高昂，并且在某些情况下需要专业的技术人员。因此，一个替代方案是使用大型语言模型（LLM）来标注数据集。当采用这种方法时，质量和成本之间存在权衡：数据集不如人工标注的好，但成本大幅降低。例如，在
    ToolBench（一个将 LLM 连接到 API 的代理系统）中，该工作的作者们([https://arxiv.org/pdf/2308.11432](https://arxiv.org/pdf/2308.11432))收集了超过
    16,000 个真实世界的 API，然后使用 ChatGPT 对此数据集进行标注。然后，他们在该数据集上微调了 LLaMA。微调后的模型在使用这些 API
    时表现更出色。
- en: '![Figure 9.4 – Construction of ToolBench (https://arxiv.org/pdf/2307.16789)](img/B21257_09_04.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – ToolBench 的构建](https://arxiv.org/pdf/2307.16789)(img/B21257_09_04.jpg)'
- en: Figure 9.4 – Construction of ToolBench ([https://arxiv.org/pdf/2307.16789](https://arxiv.org/pdf/2307.16789))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – ToolBench 的构建([https://arxiv.org/pdf/2307.16789](https://arxiv.org/pdf/2307.16789))
- en: Alternatively, you can collect a large amount of data that is not annotated,
    so that the model figures out on its own during fine-tuning . For example, Mind2Web
    collected a large amount of data for web browsing ([https://arxiv.org/abs/2306.06070](https://arxiv.org/abs/2306.06070)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以收集大量未标注的数据，这样模型就可以在微调过程中自行找出。例如，Mind2Web 收集了大量用于网络浏览的数据([https://arxiv.org/abs/2306.06070](https://arxiv.org/abs/2306.06070))。
- en: The trade-off between annotated and self-annotated datasets is that LLM-labeled
    data may lack the accuracy, nuance, or reliability of human annotation, potentially
    affecting performance. Still, it allows broader coverage and faster iteration.
    In practice, combining both methods—using LLMs for bulk labeling and humans for
    validation or high-stakes tasks—offers a balance between quality and cost, making
    fine-tuning more accessible while still enhancing agent capabilities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 标注数据集和自标注数据集之间的权衡在于，LLM标注的数据可能缺乏人类标注的准确性、细微差别或可靠性，这可能会影响性能。然而，它允许更广泛的覆盖范围和更快的迭代。在实践中，结合这两种方法——使用LLM进行大量标注，而人类进行验证或高风险任务——在质量和成本之间提供了平衡，使微调更加容易，同时仍然增强了代理的能力。
- en: Because interactions with the model are typically conducted with the prompt,
    many developers simply use prompt engineering without the need for fine-tuning.
    The rationale is that the necessary knowledge already exists in the parameters
    of the LLM and we want to use a prompt that allows the model to use it to its
    best advantage. Other approaches add agents that act as critics, other agents
    that debate, or other variations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与模型的交互通常是通过提示进行的，因此许多开发者简单地使用提示工程，而不需要微调。其理由是必要的知识已经存在于LLM的参数中，我们希望使用一个提示，使模型能够最大限度地利用它。其他方法添加了充当评论家的代理、进行辩论的其他代理或其他变体。
- en: What we have seen so far enables us to understand what an autonomous agent is
    and how it is composed. As we have seen, an agent has, at its core, an LLM and
    a sophisticated ecosystem around it that can be composed of different elements
    as the researcher chooses. In the following sections, we will look in detail at
    different approaches to autonomous agents that allow us to understand some of
    the solutions that have been implemented in the literature.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所看到的内容使我们能够理解什么是自主代理以及它是如何构成的。正如我们所见，一个代理的核心是一个LLM，以及围绕它的复杂生态系统，这些元素可以根据研究者的选择进行组合。在接下来的章节中，我们将详细探讨不同的自主代理方法，这些方法使我们能够理解文献中实施的一些解决方案。
- en: Toolformer
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Toolformer
- en: Toolformer (Schick, 2023) is a pioneering work using the idea that an LLM can
    access external tools to solve tasks (search engines, calculators, and calendars)
    without sacrificing their generality or requiring large-scale human annotation.
    The key innovation of Toolformer lies in treating tool use as a generalizable
    skill, not bound to a specific task. Rather than designing separate systems for
    each tool or task, Toolformer teaches the model to make intelligent decisions
    about which tool to use, when to use it, and how to use it, all within a unified
    language modeling framework.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Toolformer（Schick, 2023）是一项开创性的工作，它采用了这样的想法：一个大型语言模型（LLM）可以访问外部工具来解决任务（如搜索引擎、计算器和日历）而不牺牲其通用性或需要大规模的人类标注。Toolformer的关键创新在于将工具使用视为一种可推广的技能，而不是局限于特定任务。Toolformer不是为每个工具或任务设计独立的系统，而是教会模型在统一的语言建模框架内做出关于使用哪个工具、何时使用以及如何使用工具的智能决策。
- en: 'According to the authors, an LLM should learn the use of tools according to
    two principles: in a self-supervised way and preserving the generality of the
    model. Toolformer is designed to learn in a largely self-supervised manner, addressing
    a major bottleneck in AI development: the cost and effort of human-labeled data.
    Instead of manually annotating data with tool usage, the model is shown a few
    examples of how tools (API calls) work. It then automatically annotates a large,
    unlabeled dataset with tool-use opportunities during language modeling. These
    annotated sequences are used to fine-tune the model, enabling it to learn tool
    interactions naturally. This is important because there is a cost associated with
    annotating a dataset, but it also teaches an LLM how to use the tools. A central
    goal is to ensure that the LLM retains its broad capabilities across tasks while
    gaining the ability to use tools. Tool use is not hardcoded for specific prompts—it
    becomes part of the model’s general skillset. The LLM learns when a tool improves
    performance and chooses to invoke it only when necessary, maintaining flexibility
    and avoiding over-dependence. In short, tool use is not associated with a specific
    task but becomes a general concept. The idea behind Toolformer is it is a model
    that treats a tool as a call to an API. This abstraction simplifies integration
    and scales easily to different tools. For instance, the model might decide to
    call a calculator API when faced with a math problem or a search engine when external
    knowledge is needed. Given a series of human-written examples of how an API can
    be used, the authors used an LLM to annotate a huge language modeling dataset
    with potential API calls. After that, the authors conduct fine-tuning of the model
    to improve the model’s capabilities. With this approach, an LLM learns how to
    control a variety of tools and when it should use them.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据作者的观点，一个大型语言模型（LLM）应该根据两个原则来学习工具的使用：以自监督的方式学习，并保持模型的一般性。Toolformer旨在以大量自监督的方式进行学习，解决了人工智能发展中的一大瓶颈：人工标注数据的成本和努力。模型不是通过手动标注工具使用的数据，而是被展示了一些工具（API调用）的工作示例。然后，它在语言建模过程中自动标注了一个大型、未标记的数据集，其中包含工具使用的机遇。这些标注序列被用来微调模型，使其能够自然地学习工具交互。这一点很重要，因为标注数据集是有成本的，但它也教会了LLM如何使用工具。一个核心目标是确保LLM在执行不同任务时保持其广泛的技能，同时获得使用工具的能力。工具的使用不是为特定提示硬编码的——它成为模型一般技能集的一部分。LLM学习何时工具可以提高性能，并只在必要时调用它，以保持灵活性并避免过度依赖。简而言之，工具的使用与特定任务无关，而成为一个一般概念。Toolformer背后的理念是将工具视为对API的调用。这种抽象简化了集成，并且可以轻松扩展到不同的工具。例如，当面对数学问题时，模型可能会决定调用计算器API，或者当需要外部知识时，调用搜索引擎。在给出一系列人类编写的API使用示例后，作者使用LLM对大量语言建模数据集进行了标注，其中包含潜在的API调用。之后，作者对模型进行了微调，以提升模型的能力。采用这种方法，LLM学习如何控制各种工具，以及何时应该使用它们。
- en: '![Figure 9.5 – Toolformer approach (https://arxiv.org/pdf/2302.04761)](img/B21257_09_05.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – Toolformer方法](https://arxiv.org/pdf/2302.04761)](img/B21257_09_05.jpg)'
- en: Figure 9.5 – Toolformer approach ([https://arxiv.org/pdf/2302.04761](https://arxiv.org/pdf/2302.04761))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – Toolformer方法 ([https://arxiv.org/pdf/2302.04761](https://arxiv.org/pdf/2302.04761))
- en: HuggingGPT
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HuggingGPT
- en: 'HuggingGPT (Shen, 2023) introduces a powerful concept: using language as a
    generic interface that enables LLMs to collaborate with external AI models across
    various modalities, such as vision, speech, and structured data. Instead of being
    limited to textual tasks, the LLM gains the ability to manage and orchestrate
    other models to solve complex, real-world problems. HuggingGPT is based on two
    ideas: an LLM is limited if it cannot access information beyond text (such as
    vision and speech), and in the real world, complex tasks can be decomposed into
    smaller tasks that are more manageable. For specific tasks, LLMs have excellent
    capabilities in zero-shot or few-shot learning, but generalist models are less
    capable than specific trained models. So, for the authors, the solution is that
    an LLM must be able to coordinate with external models to harness their powers.
    In the article, they focus on finding suitable middleware to bridge the connections
    between LLMs and AI models. In other words, the idea is that LLMs can dialogue
    with other models and thus exploit their capabilities. The intuition behind it
    is that each AI model can be described in the form of language by summarizing
    its function. In other words, each model can be described functionally and textually.
    This description can then be used by an LLM. For the authors, this represents
    the introduction of a new concept: *Language as a generic interface for LLMs to
    collaborate with AI models*. In this system, the LLM acts as the “brain,” responsible
    for interpreting the user’s request, decomposing it into subtasks, selecting the
    appropriate models based on their textual descriptions, scheduling and coordinating
    model execution, integrating results, and generating a final response.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingGPT (Shen, 2023) 提出了一个强大的概念：将语言作为一种通用接口，使大型语言模型（LLM）能够与各种模态的外部AI模型协作，例如视觉、语音和结构化数据。LLM不再局限于文本任务，而是获得了管理和协调其他模型以解决复杂、现实世界问题的能力。HuggingGPT基于两个想法：如果LLM无法访问文本之外的信息（如视觉和语音），则其能力有限；在现实世界中，复杂任务可以分解为更易于管理的较小任务。对于特定任务，LLM在零样本或小样本学习方面表现出色，但通用模型的能力不如特定训练模型。因此，对于作者来说，解决方案是LLM必须能够与外部模型协调以利用它们的强大功能。在文章中，他们专注于寻找合适的中间件来连接LLM和AI模型之间的联系。换句话说，这个想法是LLM可以与其他模型进行对话，从而利用它们的特性。其背后的直觉是每个AI模型都可以通过总结其功能来用语言描述。换句话说，每个模型都可以从功能上和文本上进行描述。这种描述可以被LLM使用。对于作者来说，这代表了新概念的引入：*语言作为LLM与AI模型协作的通用接口*。在这个系统中，LLM充当“大脑”，负责解释用户请求，将其分解为子任务，根据它们的文本描述选择适当的模型，调度和协调模型执行，整合结果，并生成最终响应。
- en: Since interaction with an LLM is through a prompt, a model’s function description
    can be entered in the LLM prompt. An LLM then can be seen as the brain that manages
    AI models for planning, scheduling, and cooperation. So, an LLM does not accomplish
    the task directly but invokes specific models to solve tasks. For example, if
    a user asks, “*What animal is in the image?*”, the LLM processes the question
    and reasons what type of model it should use (i.e., an image classifier); the
    model is invoked, which returns an output (the animal present), and the LLM generates
    a textual output to answer “*the animal is* *a chicken*.”
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与LLM的交互是通过提示进行的，因此可以将模型的功能描述输入到LLM提示中。然后，LLM可以被视为管理AI模型以进行规划、调度和协作的“大脑”。因此，LLM不是直接完成任务，而是调用特定模型来解决任务。例如，如果用户问，“*图片中有什么动物？*”，LLM会处理这个问题，并推理出它应该使用哪种类型的模型（即图像分类器）；模型被调用，返回输出（出现的动物），然后LLM生成文本输出以回答“*这只动物是*
    *一只鸡*。”
- en: At this point, the main problem is collecting these textual descriptions of
    the functions of the models. Fortunately, the **machine learning** (**ML**) community
    provides quality descriptions for specific tasks and the models used to solve
    them (language, vision, speech, and so on). So, what we need is to tie LLMs to
    the community (GitHub, Hugging Face, and so on).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，主要问题在于收集这些模型功能的文本描述。幸运的是，**机器学习**（**ML**）社区为特定任务及其使用的模型（语言、视觉、语音等）提供了高质量的描述。因此，我们需要将LLM与社区（GitHub、Hugging
    Face等）联系起来。
- en: 'In short, HuggingGPT is an LLM-powered agent designed to solve a variety of
    complex tasks autonomously. HuggingGPT connects an LLM (in the original article,
    it is ChatGPT) with the ML community (Hugging Face, but the principle can be generalized);
    the LLM can take different modalities as input and accomplish different tasks.
    The LLM acts as a brain, divides the user’s request into subtasks, and then assigns
    them to specialized models (in accordance with the model description); it then
    executes these models and integrates the results. These principles are highlighted
    in the following figure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，HuggingGPT是一个由LLM驱动的代理，旨在自主解决各种复杂任务。HuggingGPT将LLM（原文中是ChatGPT）与ML社区（Hugging
    Face，但原理可以推广）连接起来；LLM可以接受不同模态作为输入并完成不同的任务。LLM充当大脑，将用户的请求分解为子任务，然后将它们分配给专业模型（根据模型描述）；然后执行这些模型并整合结果。以下图示强调了这些原则：
- en: '![Figure 9.6 – HuggingGPT general scheme (https://arxiv.org/pdf/2303.17580)](img/B21257_09_06.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – HuggingGPT总体方案 (https://arxiv.org/pdf/2303.17580)](img/B21257_09_06.jpg)'
- en: Figure 9.6 – HuggingGPT general scheme ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – HuggingGPT总体方案([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: 'The whole HuggingGPT process can then be divided into four steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 整个HuggingGPT过程可以分成四个步骤：
- en: 'Task planning: ChatGPT analyzes the requests by the user (understands the intention)
    and transforms the question into possible solvable tasks.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务规划：ChatGPT分析用户请求（理解意图）并将问题转化为可能可解的任务。
- en: 'Model selection: ChatGPT selects the appropriate models (expert models) that
    are present in Hugging Face (the models are selected based on the provided description).'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型选择：ChatGPT选择Hugging Face中存在的适当模型（专家模型）（模型的选择基于提供的描述）。
- en: 'Task execution: The model is invoked and executed, and then the results are
    returned to ChatGPT.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务执行：模型被调用并执行，然后将结果返回给ChatGPT。
- en: 'Response generation: ChatGPT integrates the results of the models and generates
    the answers.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 响应生成：ChatGPT整合模型的结果并生成答案。
- en: In Toolformer, we have an LLM where the model calls a tool via an API call.
    HuggingGPT uses a similar approach but without the need for fine-tuning. In HuggingGPT,
    an LLM can be seen as a controller that routes user requests to expert models.
    In other words, the LLM understands the task and plans the action, but this action
    is then conducted by expert models (the LLM just integrates the results). The
    LLM here is just a facilitator that organizes the cooperation of different models
    to solve different tasks in different domains. The LLM then maintains its generality
    and can choose which tool to use and when to use it (in this case, the models
    are the tools). For example, if an LLM does not have capabilities in a certain
    mode, it exploits the capabilities of an expert model to be able to accomplish
    the task. The LLM just needs to know which model to call to solve a specific task.
    HuggingGPT thus represents a flexible system, where we only need textual descriptions
    to provide to the LLM, and then the LLM will integrate the different expert models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在Toolformer中，我们有一个LLM，模型通过API调用调用工具。HuggingGPT采用类似的方法，但无需微调。在HuggingGPT中，一个LLM可以被视为一个控制器，将用户请求路由到专家模型。换句话说，LLM理解任务并规划行动，但这个行动由专家模型（LLM只是整合结果）执行。这里的LLM只是一个促进者，组织不同模型在不同领域解决不同任务的协作。LLM随后保持其通用性，并可以选择使用哪个工具以及何时使用它（在这种情况下，模型就是工具）。例如，如果一个LLM在某个模式下没有能力，它将利用专家模型的能力来完成该任务。LLM只需要知道调用哪个模型来解决特定任务。因此，HuggingGPT代表了一个灵活的系统，我们只需要向LLM提供文本描述，然后LLM就会整合不同的专家模型。
- en: '![Figure 9.7 – HuggingGPT process (https://arxiv.org/pdf/2303.17580)](img/B21257_09_07.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – HuggingGPT过程 (https://arxiv.org/pdf/2303.17580)](img/B21257_09_07.jpg)'
- en: Figure 9.7 – HuggingGPT process ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – HuggingGPT过程([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: Task planning
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务规划
- en: In the first step, **task planning**, the LLM must understand the task and break
    it down into subtasks. In the real world, user requests can be complex and their
    intentions intricate, requiring task decomposition. This is because a single model
    may not be capable of solving the entire task; instead, multiple models might
    be necessary to address different aspects. An LLM then needs to decompose the
    task into a series of subtasks and understand the dependency between these tasks
    and in what order they should be executed. This is conducted by creating a specific
    prompt.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步，**任务规划**，LLM 必须理解任务并将其分解为子任务。在现实世界中，用户请求可能很复杂，其意图复杂，需要任务分解。这是因为单个模型可能无法解决整个任务；相反，可能需要多个模型来处理不同的方面。然后，LLM
    需要将任务分解为一系列子任务，并理解这些任务之间的依赖关系以及它们应该执行的顺序。这是通过创建一个特定的提示来完成的。
- en: 'To standardize the system, the authors of HuggingGPT used a set of specific
    instructions. An LLM must then adhere to these specifications in order to conduct
    task planning. They designed a standardized template for tasks and instructed
    the LLM to conduct task parsing through slot filling. The LLM is guided to fill
    this template using slot filling, allowing for the consistent parsing and execution
    of subtasks. There are four slots that the template must fill:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化系统，HuggingGPT 的作者使用了一套特定的指令。然后，LLM 必须遵守这些规范以进行任务规划。他们设计了一个标准化的任务模板，并指示
    LLM 通过槽位填充进行任务解析。LLM 在槽位填充的指导下填写此模板，从而实现子任务的持续解析和执行。模板必须填充以下四个槽位：
- en: '**Task ID**: The model provides a unique identifier for each task. This ID
    is used to identify both the task and dependent tasks, as well as all the resources
    that are generated.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务 ID**：模型为每个任务提供唯一的标识符。此 ID 用于识别任务及其依赖任务，以及所有生成的资源。'
- en: '**Task type**: This slot includes the task type; each task can be of various
    types (language, visual, video, audio, and so on).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务类型**：此槽位包括任务类型；每个任务可以是各种类型（语言、视觉、视频、音频等）。'
- en: '**Task dependencies**: This slot defines the prerequisites for each task (the
    model only launches a task if all its prerequisites are complete).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务依赖关系**：此槽位定义了每个任务的前提条件（模型只有在所有前提条件都完成的情况下才会启动任务）。'
- en: '**Task arguments**: This slot contains all the arguments that are required
    for the execution of a task (from text to images or other resources). These contents
    can be derived from the user’s query or from the results of other tasks.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务参数**：此槽位包含执行任务所需的所有参数（从文本到图像或其他资源）。这些内容可以来自用户的查询或其他任务的输出。'
- en: '![Figure 9.8 – HuggingGPT type of task (https://arxiv.org/pdf/2303.17580)](img/B21257_09_08.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – HuggingGPT 类型的任务](https://arxiv.org/pdf/2303.17580)(img/B21257_09_08.jpg)'
- en: Figure 9.8 – HuggingGPT type of task ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – HuggingGPT 类型的任务 ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: The authors use demonstrations to direct the model to perform a task (such as
    image-to-text, summarization, and so on). As we saw in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042),
    adding demonstrations allows the model to map the task (few-shot prompting and
    in-context learning). These demonstrations tell the model how it should divide
    the task, in what order, and whether there are dependencies. In addition, to support
    complex tasks, the authors include chat logs (previous discussions that were conducted
    with the user) as a kind of tool. This way, the model can be aware if additional
    resources or requests have been indicated that can help with the task.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用演示来指导模型执行任务（如图像到文本、摘要等）。正如我们在[*第 3 章*](B21257_03.xhtml#_idTextAnchor042)中看到的，添加演示允许模型将任务（少样本提示和情境学习）映射。这些演示告诉模型如何划分任务，顺序如何，以及是否存在依赖关系。此外，为了支持复杂任务，作者包括聊天记录（与用户进行的先前讨论）作为一种工具。这样，模型就可以知道是否已经指示了可以有助于任务的其他资源或请求。
- en: The prompt provides all the information needed for the LLM. In the prompt, we
    provide instructions on its task (planning the task breakdown), where to retrieve
    information, examples of how it should perform the task, and what output we expect.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提示提供了 LLM 所需的所有信息。在提示中，我们提供了关于其任务的说明（规划任务分解）、信息检索的位置、如何执行任务的示例以及我们期望的输出。
- en: '![Figure 9.9 – Details of the prompt design in HuggingGPT (https://arxiv.org/pdf/2303.17580)](img/B21257_09_09.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – HuggingGPT 中提示设计的细节](https://arxiv.org/pdf/2303.17580)(img/B21257_09_09.jpg)'
- en: Figure 9.9 – Details of the prompt design in HuggingGPT ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – HuggingGPT中提示设计的细节 ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: Model selection
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型选择
- en: After planning the task, the model proceeds to select appropriate models for
    the task, or **model selection**. Once we have a list of subtasks, we need to
    choose the appropriate model. This is possible because we have descriptions of
    the models and what they do. The authors of this work have collected descriptions
    of expert models from the ML community (e.g., Hugging Face). In fact, on Hugging
    Face, it is often the model’s developers themselves who describe the model in
    terms of functionality, architecture, supported languages and domains, licensing,
    and so on.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划任务之后，模型开始选择适合任务的适当模型，或**模型选择**。一旦我们有一个子任务的列表，我们需要选择适当的模型。这是可能的，因为我们有模型及其功能的描述。这项工作的作者已经从机器学习社区（例如，Hugging
    Face）收集了专家模型的描述。实际上，在Hugging Face上，通常是模型的开发者自己用功能、架构、支持的语言和领域、许可等方面的术语来描述模型。
- en: '![Figure 9.10 – Screenshot of an example of the description of a model on Hugging
    Face (https://huggingface.co/docs/transformers/model_doc/bert)](img/B21257_09_10.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – Hugging Face上模型描述示例的截图 (https://huggingface.co/docs/transformers/model_doc/bert)](img/B21257_09_10.jpg)'
- en: Figure 9.10 – Screenshot of an example of the description of a model on Hugging
    Face ([https://huggingface.co/docs/transformers/model_doc/bert](https://huggingface.co/docs/transformers/model_doc/bert))
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – Hugging Face上模型描述示例的截图 ([https://huggingface.co/docs/transformers/model_doc/bert](https://huggingface.co/docs/transformers/model_doc/bert))
- en: 'Model assignment is thus formulated as a single-choice model, in which an LLM
    must choose which model is the best among those available given a particular context.
    Then, considering the user’s requirements and the context, an LLM can choose which
    expert model is best suited to perform the task. Of course, there is a limit to
    the context length, and you cannot enter all the model descriptions without exceeding
    this length. To address this, the HuggingGPT system applies a two-stage filtering
    and ranking process. First, models are filtered based on the task type identified
    during task planning (e.g., language, vision, or audio). Only models that are
    relevant to the specific subtask type are retained, narrowing down the pool significantly.
    Among the filtered models, the system sorts them based on the number of downloads,
    which acts as a proxy for quality, reliability, and community trust. The assumption
    is that widely used models are more likely to perform well. Finally, the system
    selects the top-k model descriptions (where k is a configurable hyperparameter)
    and includes them in the prompt. The LLM then performs single-choice model selection,
    evaluating the context and user requirements to choose the most appropriate model
    from the shortlist. This strategy offers a balanced trade-off: it keeps the prompt
    within manageable token limits while still allowing the LLM enough options to
    make an informed and effective model selection.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型分配被表述为一个单选模型，其中LLM必须在给定的特定上下文中从可用的模型中选择最佳模型。然后，考虑到用户的需求和上下文，LLM可以选择最适合执行任务的专家模型。当然，上下文长度是有限的，你不能输入所有模型描述而不超过这个长度。为了解决这个问题，HuggingGPT系统应用了一个两阶段过滤和排名过程。首先，根据任务规划期间识别的任务类型（例如，语言、视觉或音频）对模型进行过滤。只有与特定子任务类型相关的模型被保留，显著缩小了模型池。在过滤后的模型中，系统根据下载量对它们进行排序，这作为质量、可靠性和社区信任的代理。假设广泛使用的模型更有可能表现良好。最后，系统选择前k个模型描述（其中k是一个可配置的超参数）并将它们包含在提示中。然后，LLM执行单选模型选择，评估上下文和用户需求，从候选列表中选择最合适的模型。这种策略提供了一个平衡的权衡：它保持了提示在可管理的令牌限制内，同时仍然允许LLM有足够的选择来做出明智和有效的模型选择。
- en: '![Figure 9.11 – Details of the prompt design in HuggingGPT for model selection
    (https://arxiv.org/pdf/2303.17580)](img/B21257_09_11.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图9.11 – HuggingGPT中用于模型选择的提示设计细节 (https://arxiv.org/pdf/2303.17580)](img/B21257_09_11.jpg)'
- en: Figure 9.11 – Details of the prompt design in HuggingGPT for model selection
    ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – HuggingGPT中用于模型选择的提示设计细节 ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: Model execution
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型执行
- en: Once a specific model has been assigned to a specific task, the model must be
    executed. Note that these models are used only in inference. These models are
    used through the Hugging Face API. To speed up execution, HuggingGPT uses hybrid
    inference endpoints. The selected model takes the task arguments as input and
    then sends the results back to the language model (ChatGPT). Moreover, if the
    model has no resource dependencies, its inference can be parallelized. In other
    words, tasks that are not dependent on each other can be executed simultaneously.
    Otherwise, the system takes into account how much the output of one model and
    the input of another are connected (e.g., if one task must have the output of
    another subtask in order to be carried out). To perform inference, HuggingGPT
    uses hybrid inference endpoints, primarily relying on Hugging Face APIs. When
    models are available and functional via these APIs, the system executes them remotely.
    However, if API endpoints are unavailable or slow or face network issues, local
    inference is used as a fallback. This hybrid setup ensures flexibility and robustness
    in execution.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将特定模型分配给特定任务，就必须执行该模型。请注意，这些模型仅在推理中使用。这些模型通过 Hugging Face API 使用。为了加快执行速度，HuggingGPT
    使用混合推理端点。选定的模型将任务参数作为输入，然后将结果发送回语言模型（ChatGPT）。此外，如果没有资源依赖关系，其推理可以并行化。换句话说，相互不依赖的任务可以同时执行。否则，系统会考虑一个模型的输出与另一个模型的输入之间有多少关联（例如，如果一个任务必须有一个子任务的输出才能执行）。为了进行推理，HuggingGPT
    使用混合推理端点，主要依赖于 Hugging Face API。当模型通过这些 API 可用且功能正常时，系统会远程执行它们。然而，如果 API 端点不可用或速度慢或遇到网络问题，则使用本地推理作为后备。这种混合设置确保了执行过程中的灵活性和鲁棒性。
- en: 'The authors note: “*Despite HuggingGPT’s ability to develop the task order
    through task planning, it can still be challenging to effectively manage resource
    dependencies between tasks in the task execution stage*.” To solve this problem,
    the authors simply used a unique symbol, `<resource>`, to handle the dependencies.
    `<resource>` is a special token that represents the resource required for a task
    (this matches the task identifier), and if the required task is completed, the
    token is replaced with the resource.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出：“*尽管 HuggingGPT 能够通过任务规划来开发任务顺序，但在任务执行阶段，仍然可能难以有效管理任务之间的资源依赖关系*。”为了解决这个问题，作者简单地使用了一个独特的符号
    `<resource>` 来处理依赖关系。`<resource>` 是一个特殊标记，代表任务所需的资源（这与任务标识符相匹配），如果所需的任务已完成，则该标记将被资源替换。
- en: '![Figure 9.12 – Model execution (https://arxiv.org/pdf/2303.17580)](img/B21257_09_12.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – 模型执行（https://arxiv.org/pdf/2303.17580）](img/B21257_09_12.jpg)'
- en: Figure 9.12 – Model execution ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – 模型执行（[https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580)）
- en: Response generation
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 响应生成
- en: Once all the tasks are executed, the response must be generated. HuggingGPT
    integrates all the information that was obtained in the previous steps (task planning,
    model selection, and task execution) into a kind of concise summary (the tasks,
    the models used, and the results of the models). Note that the model integrates
    results of several other models, especially those obtained by inference and that
    may be of different formats. These results are presented in a structured format
    (as in, bounding boxes, probabilities, and so on), and HuggingGPT takes these
    results and transforms them into natural language to respond to a user. So, HuggingGPT
    not only gets results for the task but also responds to the user in a human-friendly
    way.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有任务都执行完毕，就必须生成响应。HuggingGPT 将之前步骤中获得的所有信息（任务规划、模型选择和任务执行）整合成一种简明的摘要（任务、使用的模型和模型的结果）。请注意，模型整合了多个其他模型的结果，特别是通过推理获得的结果，这些结果可能具有不同的格式。这些结果以结构化格式呈现（例如，边界框、概率等），HuggingGPT
    将这些结果转换为自然语言以响应用户。因此，HuggingGPT 不仅为任务获取结果，而且以人性化的方式响应用户。
- en: '![Figure 9.13 – Response generation (https://arxiv.org/pdf/2303.17580)](img/B21257_09_13.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – 响应生成（https://arxiv.org/pdf/2303.17580）](img/B21257_09_13.jpg)'
- en: Figure 9.13 – Response generation ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 响应生成（[https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580)）
- en: Qualitatively, we can see that the model is capable of solving several tasks.
    Thus, the model is able to divide the task into various subtasks, choose appropriate
    models, retrieve the results, and integrate them efficiently. For example, the
    model can do image captioning, pose generation, and even pose conditional image
    generation tasks. Not only that but the tasks can be multimodal (such as text-to-video
    generation, adding audio to a video, and so on). One of the most interesting aspects
    is that all of this is conducted without any additional LLM training. In fact,
    everything is done in inference (for both LLMs and models in inference). The advantage
    is that you can integrate additional models for additional tasks without any training;
    you only need to add a functional description of the new models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 定性来看，我们可以看到模型能够解决多个任务。因此，模型能够将任务分解成各种子任务，选择合适的模型，检索结果并有效地整合它们。例如，模型可以进行图像标题、姿态生成，甚至姿态条件图像生成任务。不仅如此，任务可以是多模态的（如文本到视频生成、为视频添加音频等）。其中最有趣的一个方面是，所有这些都是在没有任何额外LLM训练的情况下完成的。事实上，所有这些都是在推理过程中完成的（对于LLMs和推理中的模型）。优势在于，你可以不进行任何训练就集成额外的模型以处理额外任务；你只需要添加新模型的函数描述。
- en: 'For example, in this case, we can see the execution of a multimodal task (text,
    video, and audio). The model is asked to perform two tasks: generate a video from
    a description and dub the video. The model performs these two actions in parallel.
    In the bottom part of the following figure, the model must instead perform the
    two tasks in series: the model first generates text from the image and then generates
    audio.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在这种情况下，我们可以看到多模态任务（文本、视频和音频）的执行。模型被要求执行两个任务：根据描述生成视频和为视频配音。模型并行执行这两个动作。在以下图的底部部分，模型必须依次执行这两个任务：模型首先从图像生成文本，然后生成音频。
- en: '![Figure 9.14 – Qualitative analysis of multi-model cooperation on video and
    audio modalities (https://arxiv.org/pdf/2303.17580)](img/B21257_09_14.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14 – 视频和音频模态多模型合作的定性分析 (https://arxiv.org/pdf/2303.17580)](img/B21257_09_14.jpg)'
- en: Figure 9.14 – Qualitative analysis of multi-model cooperation on video and audio
    modalities ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 – 视频和音频模态多模型合作的定性分析 ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: The authors of the study also explore more complex tasks where an LLM must organize
    the cooperation of multiple models to succeed in solving the task. HuggingGPT
    can organize the cooperation of multiple models through the task planning step.
    The results show that HuggingGPT can cope with complex tasks in a multi-round
    conversation scenario (where the user divides their requests into several rounds).
    Moreover, the model can solve complex tasks by assigning an expert model to each
    task. For example, “*Describe the image in as much detail as possible*” requires
    the model to solve five tasks (image caption, image classification, object detection,
    segmentation, and visual question-answering tasks). These five tasks are not solved
    by one model but by five different models that are called and executed. Each of
    these models then provides information that must be integrated into a detailed
    answer. These models work in parallel in inference and then the final information
    is merged.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的作者还探索了更复杂的任务，在这些任务中，一个大型语言模型必须组织多个模型的合作才能成功解决问题。HuggingGPT可以通过任务规划步骤来组织多个模型的合作。结果显示，HuggingGPT能够在多轮对话场景（用户将请求分成几个轮次）中应对复杂任务。此外，该模型可以通过为每个任务分配一个专家模型来解决复杂任务。例如，“尽可能详细地描述图像”需要模型解决五个任务（图像标题、图像分类、目标检测、分割和视觉问答任务）。这五个任务不是由一个模型解决，而是由五个不同的模型调用和执行。然后，这些模型各自提供必须整合到详细答案中的信息。这些模型在推理过程中并行工作，然后合并最终信息。
- en: '![Figure 9.15 – Case study on complex tasks (https://arxiv.org/pdf/2303.17580)](img/B21257_09_15.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图9.15 – 复杂任务案例研究 (https://arxiv.org/pdf/2303.17580)](img/B21257_09_15.jpg)'
- en: Figure 9.15 – Case study on complex tasks ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 – 复杂任务案例研究 ([https://arxiv.org/pdf/2303.17580](https://arxiv.org/pdf/2303.17580))
- en: HuggingGPT limitations
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HuggingGPT的局限性
- en: 'However, some limitations remain:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍存在一些局限性：
- en: '**Efficiency**: HuggingGPT requires multiple calls from an LLM; this occurs
    in three of the four process steps (task planning, model selection, and response
    generation). These interactions are expensive and can lead to response latency
    and degradation of the user experience. In addition, closed-source models (GPT-3.5
    and GPT-4) were used in the original article, leading to additional costs. Technically,
    the same approach could have been carried out with models that are open source.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：HuggingGPT需要从LLM中进行多次调用；这发生在四个过程中的三个步骤中（任务规划、模型选择和响应生成）。这些交互成本高昂，可能导致响应延迟和用户体验下降。此外，原始文章中使用了闭源模型（GPT-3.5和GPT-4），这导致了额外的成本。技术上，可以使用开源模型以相同的方法进行操作。'
- en: '**Planning**: Planning depends on the capabilities of the LLM. Obviously, the
    more capable an LLM, the better the system’s capabilities, but an LLM has limited
    reasoning capabilities, so planning may not always be optimal or feasible. You
    could then test different LLMs or use LLMs that are fine-tuned to create an efficient
    plan or models that have been fine-tuned to the reasoning chain.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**：规划取决于LLM的能力。显然，LLM的能力越强，系统的能力就越好，但LLM的推理能力有限，因此规划可能并不总是最优或可行的。您可以选择测试不同的LLM或使用经过微调以创建高效计划的LLM，或者使用经过推理链微调的模型。'
- en: '**Context lengths**: The context length of a model has a definite limit, and
    for complex tasks, this is a problem. In the original article, the authors note
    that 32K for some tasks is enough (especially if several models are connected).
    The solution, then, may be to use models with a longer context length. To date,
    though, it seems that models don’t use long context efficiently. Another solution
    might be to use summarization.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文长度**：模型的上下文长度有一个明确的限制，对于复杂任务来说，这是一个问题。在原始文章中，作者指出，对于某些任务来说，32K就足够了（特别是如果连接了多个模型）。因此，解决方案可能是使用上下文长度更长的模型。然而，到目前为止，似乎模型并没有有效地使用长上下文。另一个解决方案可能是使用摘要。'
- en: '**Instability**: This stems from the stochastic nature of the LLM. Although
    LLMs are trained to generate text, and in this case we provide context, the model
    can ignore context and hallucinate. The authors of the article note that the model
    may fail to conform to instructions or give incorrect answers during the prediction.
    This generates program flow errors or incorrect answers. Hallucinations are still
    an open problem for LLMs, but there are strategies to mitigate them.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不稳定性**：这源于LLM的随机性质。尽管LLM被训练生成文本，并且在这种情况下我们提供了上下文，但模型可以忽略上下文并产生幻觉。文章的作者指出，模型在预测过程中可能无法遵守指令或给出错误答案。这会产生程序流程错误或错误答案。幻觉仍然是LLM的一个开放性问题，但有一些策略可以减轻它们。'
- en: HuggingGPT, then, is a system capable of solving complex tasks by orchestrating
    different expert models using the language as an interface. The LLM acts here
    only as a controller and manager of the various AI models. Its only tasks are
    to orchestrate the models and then generate a response. The model then generates
    a plan, selects the models, and then integrates the results into the final response.
    By itself, the LLM does not perform any tasks but demands resolution from the
    various expert models. All this is conducted in inference without any training.
    The user then provides their question, and the system conducts the process and
    then responds in natural language, thus making the interaction human-friendly
    and fluid.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，HuggingGPT是一个能够通过使用语言作为接口来协调不同专家模型以解决复杂任务的系统。在这里，LLM仅作为各种AI模型的控制器和管理者。它的唯一任务就是协调模型并生成响应。然后模型生成计划，选择模型，并将结果整合到最终响应中。LLM本身不执行任何任务，而是要求各种专家模型提供解决方案。所有这些都是在推理过程中进行的，而不涉及任何训练。用户提出问题后，系统执行过程，并以自然语言进行响应，从而使交互人性化且流畅。
- en: In the following subsections, we will examine various models designed to overcome
    the limitations of HuggingGPT or address critical challenges in other specialized
    domains. Through these explorations, you will gain insight into different strategies
    and learn how these agents can be applied to real-world scenarios.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们将探讨各种旨在克服HuggingGPT局限性或解决其他专业领域关键挑战的模型。通过这些探索，您将深入了解不同的策略，并学习如何将这些代理应用于现实世界场景。
- en: ChemCrow
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChemCrow
- en: We previously saw HuggingGPT as a system that orchestrates different tools (models),
    acting as a generalist model for general tasks. In this subsection, we want to
    discuss a similar system applied to a specialized field. ChemCrow (Bran, 2023)
    follows a similar design philosophy to HuggingGPT, but applies it to a specialized
    field—chemistry.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前将HuggingGPT视为一个协调不同工具（模型）的系统，作为一个通用型模型用于通用任务。在本小节中，我们想要讨论一个应用于专门领域的类似系统。ChemCrow（Bran，2023）遵循与HuggingGPT相似的设计理念，但将其应用于专门领域——化学。
- en: The limitation of generalist LLMs is that they have generalist knowledge and
    therefore are neither specialized for a field nor updated with the latest information.
    This can be a problematic limitation for many application fields (especially specialized
    ones such as science, finance, and healthcare). In addition, LLMs conduct calculations
    using a bag of heuristics and not by a rigorous process. For fields such as chemistry,
    this is a problem, so it is natural to think about extending the models’ capabilities
    with external tools. External tools then provide the exact answer and compensate
    for the deficiencies of LLMs in specific domains. Thus, having an integration
    of an LLM with several tools can allow an LLM to be used even in fields where
    its inherent characteristics constitute a limitation to its applicability.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通用型大型语言模型（LLM）的局限性在于它们具有通用知识，因此既未针对特定领域进行专门化，也未更新最新信息。这对于许多应用领域（尤其是科学、金融和医疗保健等专门领域）可能是一个问题。此外，LLM使用一组启发式方法进行计算，而不是通过严格的过程。对于化学等领域的应用，这是一个问题，因此自然想到通过外部工具扩展模型的能力。外部工具提供确切答案，并弥补LLM在特定领域的不足。因此，将LLM与多个工具集成可以使LLM在即使其固有特性构成其适用性限制的领域也能被使用。
- en: 'One field that can benefit from the use of LLMs is scientific research. On
    the one hand, LLMs have shown some ability to understand chemistry, and on the
    other hand, there are many specialized models for chemistry, or at least for specific
    applications. Many of these tools have been developed by the open source community
    and are accessible through APIs. Nevertheless, integrating these tools is not
    easy and requires expertise in computational coding, which is often not among
    the skills of chemistry researchers. Inspired by previous work, the authors of
    this study (Bran, 2023) proposed what they call an LLM-powered chemistry engine
    (ChemCrow) to “*streamline the reasoning process for various common chemical tasks
    across areas such as drug and materials design and synthesis*.” ChemCrow is very
    similar to what we have seen with HuggingGPT, in which we have a central LLM (GPT-4)
    that orchestrates a number of tools (in this case, highly specialized for chemistry).
    The central LLM is prompted with specific instructions and information in order
    to perform the tasks specifically and respond in a specific format. To guide the
    LLM’s reasoning and tool use, ChemCrow adopts a structured prompting format known
    as Thought, Action, Action Input, and Observation, to prompt the model to reason
    about the task (and its current state), how the current state relates to the final
    goal, and how to plan the next steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以从LLM的使用中受益的领域是科学研究。一方面，LLM已经显示出理解化学的能力，另一方面，有许多针对化学的专门模型，或者至少是针对特定应用的模型。许多这些工具都是由开源社区开发的，并且可以通过API访问。尽管如此，整合这些工具并不容易，需要计算编码方面的专业知识，而这通常不是化学研究者的技能之一。受先前工作的启发，本研究（Bran，2023）的作者们提出了他们所谓的LLM驱动的化学引擎（ChemCrow），旨在“*简化跨药物和材料设计及合成等领域的各种常见化学任务的推理过程*。”ChemCrow与我们所看到的HuggingGPT非常相似，其中我们有一个中央LLM（GPT-4），协调多个工具（在这种情况下，高度专门化于化学）。中央LLM被提示特定的指令和信息，以便执行特定任务并以特定格式响应。为了指导LLM的推理和工具使用，ChemCrow采用了一种称为“思考、行动、行动输入和观察”的结构化提示格式，以提示模型对任务（及其当前状态）进行推理，当前状态如何与最终目标相关联，以及如何规划下一步：
- en: '**Thought**: The model reflects on the current problem, considers its progress,
    and outlines reasoning toward the final goal'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思考**：模型反思当前问题，考虑其进展，并概述通向最终目标的推理'
- en: '**Action**: It selects the appropriate tool to use next (e.g., a molecule generator
    or a reaction predictor)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动**：它选择下一个要使用的适当工具（例如，分子生成器或反应预测器）'
- en: '**Action input**: It specifies what input should be sent to the chosen tool'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动输入**：它指定应发送到所选工具的输入'
- en: '**Observation**: It records the tool’s output, which is then incorporated into
    the next reasoning cycle'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察**：它记录工具的输出，然后将其纳入下一个推理周期'
- en: '![Figure 9.16 – Overview of ChemCrow (https://arxiv.org/pdf/2304.05376)](img/B21257_09_16.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – ChemCrow概述](https://arxiv.org/pdf/2304.05376)(img/B21257_09_16.jpg)'
- en: Figure 9.16 – Overview of ChemCrow ([https://arxiv.org/pdf/2304.05376](https://arxiv.org/pdf/2304.05376))
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – ChemCrow概述([https://arxiv.org/pdf/2304.05376](https://arxiv.org/pdf/2304.05376))
- en: So, in this system, the model proceeds with a Thought step (which can be thought
    of as action planning) and uses a tool and an input to this tool (selecting and
    using the model). The model gets the results, observes them, and conducts a Thought
    step again until the answer is reached. The process is similar to what we saw
    in the previous section, but there is a greater emphasis on reasoning and a specialization
    of the model. Also, among the tools are not only models but also the ability to
    search the internet or the literature; the model can also run code. So, we also
    have an extension of the capabilities and flexibility of the system. Thus, the
    authors of the study see this system as a kind of researcher’s assistant to perform
    chemical tasks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个系统中，模型通过一个思维步骤（可以将其视为行动规划）并使用工具及其输入（选择和使用模型）来进行。模型获取结果，观察它们，然后再次进行思维步骤，直到达到答案。这个过程与我们之前看到的类似，但更加重视推理和模型的专门化。此外，工具不仅包括模型，还包括搜索互联网或文献的能力；模型还可以运行代码。因此，我们也有系统能力和灵活性的扩展。因此，该研究的作者将这个系统视为一种化学任务的科研助手。
- en: '![Figure 9.17 – Human/model interaction leading to the discovery of a novel
    molecule (https://arxiv.org/pdf/2304.05376)](img/B21257_09_17.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – 人/模型交互导致新分子的发现](https://arxiv.org/pdf/2304.05376)(img/B21257_09_17.jpg)'
- en: Figure 9.17 – Human/model interaction leading to the discovery of a novel molecule
    ([https://arxiv.org/pdf/2304.05376](https://arxiv.org/pdf/2304.05376))
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – 人/模型交互导致新分子的发现([https://arxiv.org/pdf/2304.05376](https://arxiv.org/pdf/2304.05376))
- en: So, the idea is to combine LLM reasoning skills with expert knowledge and chemical
    computational tools. The results show that similar approaches can lead to real-world
    applications in specific fields, such as chemistry.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，想法是将LLM的推理技能与专业知识以及化学计算工具相结合。结果显示，类似的方法可以导致特定领域的实际应用，例如化学。
- en: SwiftDossier
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwiftDossier
- en: 'SwiftDossier is a notable example of applying agent-based systems in the scientific
    and healthcare domains, with a particular focus on addressing one of the most
    critical challenges in these areas: hallucinations. In fields such as medicine
    and pharmaceuticals, hallucinated outputs—that is, confident but false or unverifiable
    information—can lead to serious legal, ethical, and safety risks. An LLM has a
    huge memory but generates text stochastically, without obviously verifying its
    sources. This is problematic for the pharmaceutical industry or potential use
    in medicine. To solve this problem in SwiftDossier, RAGs and LLM-powered agents
    are used to force model generation. Instead of relying solely on the LLM’s internal
    knowledge—which is vast but generated stochastically and without source verification—the
    system forces the model to ground its responses in external, reliable data sources.
    The system uses a different set of tools to be able to answer different questions:
    scientific articles, internet access, databases, and other ML models. Using this
    set of tools, an LLM can succeed in generating reports and minimize the risk of
    hallucination.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: SwiftDossier是应用基于代理系统在科学和医疗领域的一个显著例子，特别关注解决这些领域中最关键的挑战之一：幻觉。在医学和制药等领域，幻觉输出——即自信但错误或无法验证的信息——可能导致严重的法律、伦理和安全风险。LLM拥有巨大的内存，但生成文本是随机的，没有明显验证其来源。这对制药行业或潜在的医疗用途来说是个问题。为了在SwiftDossier中解决这个问题，使用了RAGs和LLM驱动的代理来强制模型生成。该系统不是仅仅依赖LLM的内部知识——虽然知识量巨大，但生成是随机的且没有来源验证——而是迫使模型将其响应建立在外部、可靠的数据源上。系统使用不同的工具集来回答不同的问题：科学文章、互联网访问、数据库和其他ML模型。使用这一套工具，LLM可以成功生成报告并最小化幻觉的风险。
- en: '![Figure 9.18 – SwiftDossier architecture (https://arxiv.org/pdf/2409.15817)](img/B21257_09_18.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图9.18 – SwiftDossier架构](https://arxiv.org/pdf/2409.15817)(img/B21257_09_18.jpg)'
- en: Figure 9.18 – SwiftDossier architecture ([https://arxiv.org/pdf/2409.15817](https://arxiv.org/pdf/2409.15817))
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 – SwiftDossier架构([https://arxiv.org/pdf/2409.15817](https://arxiv.org/pdf/2409.15817))
- en: ChemAgent
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChemAgent
- en: In the two examples seen previously, we have an agent to which tools are added
    to make up for the knowledge deficiencies of a generalist LLM. In other words,
    we try to make up for the shortcomings of an LLM by using either external information
    or tools to conduct operations. Moreover, if the task itself is complex, several
    approaches try to decompose it into more manageable subtasks. An agent first produces
    a schedule and then executes the various subtasks, thus combining reasoning and
    execution. Despite all this, an LLM may still generate errors, especially in complex
    domains such as chemistry.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前看到的两个例子中，我们有一个添加了工具的代理来弥补通用LLM的知识缺陷。换句话说，我们试图通过使用外部信息或工具来执行操作来弥补LLM的不足。此外，如果任务本身很复杂，几种方法试图将其分解成更可管理的子任务。代理首先生成一个日程表，然后执行各种子任务，从而结合推理和执行。尽管如此，LLM仍然可能产生错误，尤其是在化学等复杂领域。
- en: 'LLMs, while powerful general-purpose tools, face several challenges in the
    chemistry domain, where tasks require precise reasoning, accurate calculations,
    and deep domain knowledge. These challenges arise due to the limitations in how
    LLMs generate text and code, and they become more pronounced in scientific applications
    where small errors can lead to significant inaccuracies:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs，虽然作为强大的通用工具，在化学领域面临一些挑战，在这些领域中，任务需要精确推理、准确计算和深厚的领域知识。这些挑战源于LLMs生成文本和代码的局限性，并且在科学应用中更为明显，因为小的错误可能导致重大不准确：
- en: '**Struggles with domain-specific formulas**: LLMs may misinterpret or incorrectly
    apply specialized chemical equations or notation, especially when the required
    formulas are not commonly found in general training data'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在特定领域公式上的挣扎**：LLMs可能会误解或错误地应用专业的化学方程式或符号，尤其是在所需的公式在一般训练数据中不常见的情况下'
- en: '**Incorrect intermediate reasoning steps**: In complex, multi-step tasks (e.g.,
    synthesis planning or property prediction), an error in just one step can cascade
    and lead to faulty final outputs'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中间推理步骤错误**：在复杂的多步骤任务中（例如，合成规划或性质预测），仅一步的错误可能会级联并导致最终输出错误'
- en: '**Errors in code generation**: When combining textual reasoning with code (typically
    Python), LLMs often hallucinate functions, use incorrect libraries, produce syntax
    errors, or generate code that fails to execute—especially for scientific calculations
    that require precise library calls and numerical stability'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成错误**：当将文本推理与代码（通常是Python）结合时，LLMs经常虚构函数、使用错误的库、产生语法错误或生成无法执行的代码——尤其是对于需要精确库调用和数值稳定性的科学计算'
- en: '![Figure 9.19 – Examples of LLM failure in chemistry domain (https://arxiv.org/pdf/2501.06590)](img/B21257_09_19.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图9.19 – 化学领域LLM失败示例 (https://arxiv.org/pdf/2501.06590)](img/B21257_09_19.jpg)'
- en: Figure 9.19 – Examples of LLM failure in chemistry domain ([https://arxiv.org/pdf/2501.06590](https://arxiv.org/pdf/2501.06590))
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 – 化学领域LLM失败示例 ([https://arxiv.org/pdf/2501.06590](https://arxiv.org/pdf/2501.06590))
- en: 'Human beings, unlike LLMs, learn from their past experiences and mistakes.
    For LLMs, it is not possible to learn after the end of pre-training (fine-tuning
    is an expensive approach and cannot be used repeatedly), so continual learning
    remains an open problem of AI. Humans, on the other hand, can remember strategies
    used for similar problems; once they encounter new problems, they learn new strategies
    that can be used in the future. Therefore, in ChemAgent, the authors try to find
    a way to simulate this process. They propose a dynamic library that allows iterative
    problem-solving to be facilitated by continuously updating and refining its content.
    The library serves as a repository for decomposed chemical tasks. In other words,
    a task is broken down into various subtasks and then the solutions are saved in
    the library for future use. Once a new task arrives, the library is updated with
    the new subtasks and corresponding solutions, keeping the library relevant and
    improving its usefulness over time. Inspired by human cognition, the system has
    three different memory components: planning memory (high-level strategies), execution
    memory (specific task solutions), and knowledge memory (fundamental chemistry
    principles). These memory components are stored externally, allowing the system
    to find the information again when needed, and are dynamically updated.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLMs不同，人类从过去的经验和错误中学习。对于LLMs来说，在预训练结束后无法进行学习（微调是一种昂贵的方法，且不能重复使用），因此持续学习仍然是人工智能的一个开放性问题。另一方面，人类可以记住用于类似问题的策略；一旦遇到新问题，他们会学习新的策略，这些策略可以在未来使用。因此，在ChemAgent中，作者试图找到一种模拟这一过程的方法。他们提出一个动态库，允许通过持续更新和改进其内容来促进迭代问题解决。该库作为分解化学任务的存储库。换句话说，一个任务被分解成各种子任务，然后解决方案被保存在库中以备将来使用。一旦出现新的任务，库就会更新新的子任务和相应的解决方案，保持库的相关性，并随着时间的推移提高其有用性。受人类认知的启发，该系统有三个不同的记忆组件：计划记忆（高级策略）、执行记忆（特定任务解决方案）和知识记忆（基本的化学原理）。这些记忆组件存储在外部，允许系统在需要时再次找到信息，并且是动态更新的。
- en: '![Figure 9.20 – ChemAgent framework (https://arxiv.org/pdf/2501.06590)](img/B21257_09_20.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20 – ChemAgent框架](img/B21257_09_20.jpg)'
- en: Figure 9.20 – ChemAgent framework ([https://arxiv.org/pdf/2501.06590](https://arxiv.org/pdf/2501.06590))
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 – ChemAgent框架([https://arxiv.org/pdf/2501.06590](https://arxiv.org/pdf/2501.06590))
- en: ChemAgent thus doesn’t just passively use what it finds in memory but rather
    allows the system to update the memory dynamically. It also uses memory partitioning
    to improve the various stages of problem-solving. ChemAgent divides the process
    into planning and execution (to which it associates a specific memory for each
    step) and adds memory that functions as a reference for fundamental chemistry
    principles and formulas. When a problem occurs, it is divided into a series of
    subtasks, which are solved, and these solutions are saved in memory.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ChemAgent不仅被动地使用它在记忆中找到的内容，而且允许系统动态地更新记忆。它还使用内存分区来改进问题解决的各个阶段。ChemAgent将过程分为计划和执行（为每个步骤关联一个特定的记忆）并添加一个作为基本化学原理和公式的参考的记忆。当出现问题时，它被分解成一系列子任务，这些子任务被解决，这些解决方案被保存在记忆中。
- en: Multi-agent for law
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 法律领域的多代理
- en: Another area that could benefit from the use of agents is the legal sector.
    Legal services are essential to protect citizens’ rights, but they can be particularly
    expensive and there are not always enough lawyers. Moreover, fair judgment is
    a fundamental right, but human beings also exhibit bias. Using agents in this
    field could revolutionize legal services by lowering costs and allowing more equitable
    access. In the legal field, hallucinations are particularly problematic and should
    be, if not eliminated, reduced as much as possible. Hallucinations arise from
    both the stochastic nature of the models and the quality of the data with which
    they are trained. Therefore, action must be taken on two axes in order to mitigate
    the phenomenon.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能从代理的使用中受益的领域是法律行业。法律服务对于保护公民权利至关重要，但它们可能特别昂贵，而且律师的数量并不总是足够。此外，公正的判决是一项基本权利，但人类也表现出偏见。在这个领域使用代理可以通过降低成本和允许更公平的访问来彻底改变法律服务。在法律领域，幻觉尤其成问题，如果不是消除，也应尽可能减少。幻觉既源于模型的随机性，也源于训练它们的数据质量。因此，必须从两个轴向上采取行动，以减轻这种现象。
- en: 'In this subsection, we want to present two law-focused approaches to present
    some interesting elements that have been used. Again, the principle is the same:
    everything revolves around a central element, which is an LLM. For example, Chatlaw
    focuses on data quality to mitigate the risk of LLM hallucination. Also, to make
    the most of the quality dataset the authors have collected, they use a knowledge
    graph. In addition, instead of using a single agent, they use a multi-agent system.
    Using multiple agents allows the system to simulate different areas of expertise,
    thanks to the flexibility of prompts when interacting with LLMs. The use of multi-agents
    makes it possible to emulate the process within a law firm. The authors developed
    a protocol to allow effective collaboration among agents: “*four independent intelligent
    agent roles responsible for initial information gathering, in-depth material research,
    legal advice, and final consultation report writing*.” In this way, the process
    is more thorough. Again, they used only one LLM for the whole system (the authors
    used GPT-4).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们希望介绍两种以法律为中心的方法来展示一些已经使用过的有趣元素。再次强调，原则是相同的：一切围绕一个中心元素展开，这个元素是一个LLM。例如，Chatlaw关注数据质量以减轻LLM幻觉的风险。此外，为了充分利用作者收集到的质量数据集，他们使用了一个知识图谱。此外，他们没有使用单个智能体，而是使用了一个多智能体系统。使用多个智能体使得系统能够模拟与LLM交互时不同领域的专业知识，这得益于提示的灵活性。使用多智能体使得在律师事务所内部模拟过程成为可能。作者开发了一个协议，以允许智能体之间有效协作：“*四个独立的智能体角色，分别负责初步信息收集、深入材料研究、法律咨询和最终咨询报告撰写*。”这样，过程就更加彻底。再次强调，他们为整个系统只使用了一个LLM（作者使用了GPT-4）。
- en: '![Figure 9.21 – Chatlaw, a multi-agent collaboration (https://arxiv.org/pdf/2306.16092v2)](img/B21257_09_21.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.21 – Chatlaw，多智能体协作](https://arxiv.org/pdf/2306.16092v2)(img/B21257_09_21.jpg)'
- en: Figure 9.21 – Chatlaw, a multi-agent collaboration ([https://arxiv.org/pdf/2306.16092v2](https://arxiv.org/pdf/2306.16092v2))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 – Chatlaw，多智能体协作([https://arxiv.org/pdf/2306.16092v2](https://arxiv.org/pdf/2306.16092v2))
- en: Another interesting approach is one in which the authors (Hamilton, 2023; [https://arxiv.org/pdf/2301.05327](https://arxiv.org/pdf/2301.05327))
    mimic the judgment of a court using an LLM. Here, too, a multi-agent system is
    used, in which each agent represents a judge. Each judge produces an opinion and
    then a majority opinion is obtained. So, when a case is sent to nine judges, the
    system receives nine opinions, and then it produces a single opinion. This approach
    then relies on conducting nine evaluations in parallel and the consistency of
    these evaluations (the majority vote wins).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有趣的方法是作者（Hamilton，2023；[https://arxiv.org/pdf/2301.05327](https://arxiv.org/pdf/2301.05327)）使用LLM模拟法院的判决。在这里，同样使用了一个多智能体系统，其中每个智能体代表一位法官。每位法官提出一个意见，然后得出多数意见。因此，当案件被发送给九位法官时，系统收到九个意见，然后产生一个单一的意见。这种方法依赖于并行进行九次评估以及这些评估的一致性（多数票获胜）。
- en: '![Figure 9.22 – Multi-judge system (https://arxiv.org/pdf/2301.05327)](img/B21257_09_22.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – 多法官系统](https://arxiv.org/pdf/2301.05327)(img/B21257_09_22.jpg)'
- en: Figure 9.22 – Multi-judge system ([https://arxiv.org/pdf/2301.05327](https://arxiv.org/pdf/2301.05327))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – 多法官系统([https://arxiv.org/pdf/2301.05327](https://arxiv.org/pdf/2301.05327))
- en: This work shows how to leverage an LLM to create multiple agents that work together
    to be able to mitigate hallucinations. The authors are further evidence of the
    flexibility that can be achieved by using an LLM as the center of the system.
    A limitation of this study is the use of homogeneous judges (it would be better
    to build the ensemble with different models, to avoid the various judges having
    the same bias), risking repetitive opinions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作展示了如何利用LLM创建多个智能体，它们可以协同工作以减轻幻觉。作者进一步证明了使用LLM作为系统中心可以实现的灵活性。这项研究的局限性在于使用了同质化的法官（最好是构建由不同模型组成的集成，以避免不同法官具有相同的偏见），这可能导致重复的意见。
- en: Multi-agent for healthcare applications
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多智能体在医疗应用中的使用
- en: Interdisciplinary research is complex and usually requires teams composed of
    researchers with different areas of expertise. Typically, scientific research
    is conducted by teams where each researcher deals with a particular aspect and
    masters different techniques. For example, AlphaFold 2 is the product of 34 researchers
    with different expertise (computer science, bioinformatics, and structural biology).
    Obviously, recruiting large teams of experts takes time (and it is not always
    easy to find people with the right expertise) and is expensive. Only a few institutions
    and companies can afford the most ambitious projects. Recently created LLMs, though,
    have increasingly broad knowledge of scientific topics, and we saw previously
    that this knowledge can be connected to the use of tools. ChemCrow is an example
    of how to solve a chemical problem, but it cannot tackle an open-ended, interdisciplinary
    research problem. Recently, efforts have been made to solve this problem by creating
    pipelines that can handle the end-to-end process. For example, an AI scientist
    (Lu, 2024) carries out a process that starts with conceptualizing an idea and
    ends with writing a scientific paper on ML. The AI scientist is given a broad
    research direction, produces an idea, conducts the literature search, plans and
    executes experiments, writes a manuscript, and, finally, proofreads it. All this
    is done by an LLM-like agent that is connected to tools and proceeds sequentially.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 跨学科研究是复杂的，通常需要由不同领域专家组成的研究团队。通常，科学研究是由每个研究人员处理特定方面并掌握不同技术的团队进行的。例如，AlphaFold
    2是34位不同专业知识（计算机科学、生物信息学和结构生物学）的研究人员的成果。显然，招募大量专家团队需要时间（而且并不总是容易找到具有正确专业知识的人），而且成本高昂。只有少数机构和公司能够承担最雄心勃勃的项目。然而，最近创建的LLM在科学主题上的知识越来越广泛，我们之前已经看到这种知识可以与工具的使用联系起来。ChemCrow是解决化学问题的例子，但它无法处理开放式的跨学科研究问题。最近，人们已经通过创建可以处理端到端过程的管道来解决这一问题。例如，一位AI科学家（Lu，2024）进行了一个从构思想法开始，以在机器学习上撰写科学论文结束的过程。AI科学家被赋予了一个广泛的研究方向，产生了一个想法，进行文献搜索，规划并执行实验，撰写手稿，最后校对。所有这些工作都是由一个类似于LLM的代理完成的，该代理连接到工具并按顺序进行。
- en: '![Figure 9.23 – Illustration of the AI scientist process (https://arxiv.org/pdf/2408.06292)](img/B21257_09_23.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图9.23 – AI科学家过程的说明](https://arxiv.org/pdf/2408.06292)(img/B21257_09_23.jpg)'
- en: Figure 9.23 – Illustration of the AI scientist process ([https://arxiv.org/pdf/2408.06292](https://arxiv.org/pdf/2408.06292))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23 – AI科学家过程的说明([https://arxiv.org/pdf/2408.06292](https://arxiv.org/pdf/2408.06292))
- en: Other works also show similar processes, but they are still localized to specific
    fields and linear processes. For scientific research, we want to find ways to
    combine different expertise. Swanson (2024), therefore, proposes a Virtual Lab
    for human-AI collaboration with the purpose of performing interdisciplinary science
    on complex questions. In the Virtual Lab, a human leads a set of interdisciplinary
    agents to manage a complex process. The different agents have different expertise
    and are run by an LLM. Each of these agents interacts with both other agents and
    a human being. In this way, the authors of the study build a flexible architecture.
    Here, the human being provides guidance to the agents, while the agents are the
    ones that decide on search directions and design solutions to the problem. Each
    agent is controlled by a prompt (which contains information about the role, expertise,
    goal, and available tools) provided to an LLM (GPT-4 in the article). The Virtual
    Lab then conducts the research in group or individual meetings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究也展示了类似的过程，但它们仍然局限于特定的领域和线性过程。对于科学研究，我们希望找到结合不同专业知识的方法。因此，Swanson (2024)
    提出了一种旨在对复杂问题进行跨学科科学研究的虚拟实验室，用于人机协作。在虚拟实验室中，人类引导一组跨学科代理来管理复杂过程。不同的代理拥有不同的专业知识，并由一个LLM运行。每个代理都与其他代理和人类进行交互。通过这种方式，研究的作者构建了一个灵活的架构。在这里，人类为代理提供指导，而代理则决定搜索方向和设计解决问题的方案。每个代理都由一个提供给LLM（文章中为GPT-4）的提示（包含关于角色、专业知识、目标和可用工具的信息）控制。然后，虚拟实验室通过小组或个人会议进行研究。
- en: The human provides the question and agenda to start the discussion. In team
    meetings, agents discuss the research question and work together toward the global
    goal. In individual meetings, a single agent has to solve a task (such as writing
    code) and the agent works alone or together with another agent who provides critical
    feedback. With a series of global and individual meetings, the team solves a research
    question.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 人类提供问题和议程以启动讨论。在团队会议中，代理讨论研究问题并共同努力实现全局目标。在个人会议中，单个代理必须解决一个任务（例如编写代码），代理可以单独工作或与另一个提供关键反馈的代理一起工作。通过一系列全局和个人会议，团队解决研究问题。
- en: '![Figure 9.24 – Architecture of a Virtual Lab (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)](img/B21257_09_24.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图9.24 – 虚拟实验室架构 (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)](img/B21257_09_24.jpg)'
- en: Figure 9.24 – Architecture of a Virtual Lab ([https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24 – 虚拟实验室架构 ([https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full))
- en: 'In the Virtual Lab, there is a **Principal Investigator** (**PI**) whose purpose
    is to maximize the impact of the research, and who automatically creates a set
    of appropriate scientist agents (biologists or computer scientists) for the project
    (based on the project description provided by the PI). The PI defines each agent’s
    role, expertise, and goal in a prompt. In addition, there may be an agent dedicated
    to project critique. After that, the meetings begin. Each meeting follows a set
    of inputs organized into a structure: agenda (a description of what is to be discussed),
    agenda questions (a set of questions to be answered in the meeting), agenda rules
    (a set of optional rules to make the meeting smoother), summaries (optional summaries
    of previous meetings), contexts (additional information that can help the meeting),
    and rounds (the number of rounds of discussion to prevent the discussion from
    continuing endlessly). In the team meeting, all agents participate in the discussion,
    the human writes the agenda (optionally, along with rules and questions), and
    different rounds of discussion follow. The PI starts and then each of the scientist
    agents (plus the critic agent) gives their thoughts on the discussion. At the
    end, the PI summarizes the points posed by the agents, makes a decision on the
    agents’ inputs, and asks follow-up questions. After the various rounds, the PI
    writes a final summary that the human can read.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟实验室中，有一个**主要研究员**（**PI**），其目的是最大化研究的影响力，并自动为项目创建一组适当的科学家代理（生物学家或计算机科学家）（基于PI提供的项目描述）。PI在提示中定义每个代理的角色、专业知识和目标。此外，还可能有一个专门用于项目批评的代理。之后，会议开始。每次会议都遵循一组组织成结构的输入：议程（讨论内容的描述）、议程问题（会议中要回答的问题集）、议程规则（使会议更顺畅的可选规则）、摘要（前次会议的可选摘要）、背景（有助于会议的额外信息）和回合（讨论回合的数量，以防止讨论无休止地进行）。在团队会议中，所有代理都参与讨论，人类编写议程（可选地，包括规则和问题），然后进行不同回合的讨论。PI开始，然后每位科学家代理（加上批评代理）对讨论发表意见。最后，PI总结代理提出的问题点，对代理的输入做出决定，并提出后续问题。经过各种回合后，PI编写一个人类可以阅读的最终摘要。
- en: In individual meetings, the human provides the agenda and selects the agent,
    and the agent performs the task (there may, in addition, be the critic agent,
    who provides critiques). After a series of rounds between the agent and critic,
    the agent provides the response. In addition, parallel meetings may be conducted,
    in which multiple agents perform the same task, and in a final meeting with the
    PI, the final answer is arrived at.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在个人会议中，人类提供议程并选择代理，代理执行任务（此外，还可能有批评代理，提供批评）。在代理和批评者之间经过一系列回合后，代理提供回应。此外，还可以进行并行会议，其中多个代理执行相同任务，并在与PI的最终会议中得出最终答案。
- en: '![Figure 9.25 – Virtual Lab parallel meetings (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)](img/B21257_09_25.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图9.25 – 虚拟实验室并行会议 (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)](img/B21257_09_25.jpg)'
- en: Figure 9.25 – Virtual Lab parallel meetings ([https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25 – 虚拟实验室并行会议 ([https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full))
- en: In this way, the authors have created a flexible framework that combines heterogeneous
    agents that work in both single and collaborative settings. It should be noted
    that in this approach, there is a human in the loop; that is, a human being is
    at the center of the system and actively collaborates with the AI. This process
    mimics (though, of course, in a simplified way) the work and decision-making process
    of a human team when it has to solve a complex problem. To test the usefulness
    of this work, the authors tested the Virtual Lab on designing antibodies or nanobodies
    that can bind to the spike protein of the KP.3 variant of SARS-CoV-2\. This is
    a complex problem because SARS-CoV-2 evolves rapidly, so a fast system must be
    found to design antibodies that can block it. The Virtual Lab started by creating
    a team that could tackle the problem (the PI created the right team of researchers
    for the problem). In a team meeting, the direction of the project was described
    and the principal details were discussed. There was then a team meeting about
    which tools could be used and were selected, as well as a series of individual
    meetings where the researchers used the various tools to create the antibody design
    workflow. In a meeting with the PI, the workflow was defined.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，作者创建了一个灵活的框架，结合了在单人和协作环境中工作的异构代理。需要注意的是，在这种方法中，有一个人类参与其中；也就是说，人类是系统的中心，并积极与AI合作。这个过程模仿（尽管当然是以简化的方式）了人类团队在解决复杂问题时的工作和决策过程。为了测试这项工作的实用性，作者测试了虚拟实验室在设计和结合SARS-CoV-2
    KP.3变异株刺突蛋白的抗体或纳米抗体方面的能力。这是一个复杂的问题，因为SARS-CoV-2进化迅速，因此必须找到一个快速的系统来设计可以阻止它的抗体。虚拟实验室首先创建了一个能够应对这个问题的团队（PI为这个问题组建了合适的研究人员团队）。在团队会议上，项目方向被描述，并讨论了主要细节。然后，举行了一次关于可以使用哪些工具以及如何选择的团队会议，以及一系列个人会议，研究人员使用各种工具创建抗体设计工作流程。在与PI的会议中，定义了工作流程。
- en: '![Figure 9.26 – Virtual Lab for antibody design (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)](img/B21257_09_26.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图9.26 – 抗体设计虚拟实验室 (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)](img/B21257_09_26.jpg)'
- en: Figure 9.26 – Virtual Lab for antibody design ([https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26 – 抗体设计虚拟实验室 ([https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full))
- en: The Virtual Lab managed to design antibodies that they then validated experimentally.
    The system managed to create a complex workflow that used serial models to design
    antibodies (thus solving a real and complex problem). Building this would usually
    require a multidisciplinary team because the problem needs to be solved with different
    expertise. Thus, having agents with different expertise allows the problem to
    be discussed from different angles, to which a fundamental element of scientific
    research (critique) is added. This is done through a series of meetings, where
    the AI is a partner to the human being. What we see here is the creation of a
    multi-agent and heterogeneous system with multiple rounds of meetings (group and
    individual) to create a system that is flexible and sophisticated at the same
    time.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟实验室成功地设计出了抗体，随后通过实验进行了验证。该系统成功地创建了一个复杂的流程，使用序列模型来设计抗体（从而解决了一个真实且复杂的问题）。构建这样的系统通常需要一个跨学科团队，因为需要用不同的专业知识来解决问题。因此，拥有不同专业知识的代理可以从前不同的角度讨论问题，从而为科学研究的基本要素（批判性）增添了内容。这是通过一系列会议来完成的，其中AI是人类合作伙伴。我们看到的是，通过多轮会议（小组和个人）的创建，形成了一个既灵活又复杂的系统。
- en: There are still limitations at this stage. For example, the models have knowledge
    up to a certain cut-off point, so they may not be aware of the latest published
    tools and could thus suggest old models (or ones that have problems in implementation).
    The solution to this problem might be to use RAG or an internet search. Another
    limitation is that the system is not exactly self-contained; it comes with both
    an agenda and a set of prompts that have been carefully designed. In this system,
    human beings are still involved and must provide guidance. Without guidance, the
    AI models may give vague answers or not make decisions unless specifically requested.
    Also, sometimes they do not accomplish the task or they deviate from what they
    are supposed to do. In any case, this system is flexible and can be applied agnostically
    to many other problems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段仍然存在一些限制。例如，模型的知识截止到某个点，因此它们可能不知道最新发布的工具，因此可能会建议旧模型（或实现中存在问题的模型）。解决这个问题的一个方法可能是使用RAG或互联网搜索。另一个限制是，该系统并不完全自包含；它附带了一个议程和一组经过精心设计的提示。在这个系统中，人类仍然参与其中，必须提供指导。没有指导，AI模型可能会给出模糊的答案，或者除非特别要求，否则不会做出决定。有时它们无法完成任务，或者偏离了它们应该做的事情。无论如何，这个系统是灵活的，可以无差别地应用于许多其他问题。
- en: 'Combining different expertise with human feedback seems to be the key to better
    results. In a similar vein, Agent Laboratory is designed to generate an entire
    research workflow (from literature review and experimentation to report writing),
    all from an initial human-provided research idea. In this system, the process
    begins with the collection and analysis of relevant papers, followed by collaborative
    planning and data preparation, a series of experiments, and report generation.
    The process can be divided into three stages:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同领域的专业知识与人类反馈相结合似乎是取得更好成果的关键。在这方面，代理实验室被设计用来生成整个研究工作流程（从文献综述和实验到报告撰写），这一切都基于人类提供的初始研究想法。在这个系统中，过程从收集和分析相关论文开始，接着是协作规划和数据准备，一系列实验，以及报告生成。这个过程可以分为三个阶段：
- en: '**Literature review**: In this stage, articles are collected for the given
    research idea. A PhD agent utilizes the arXiv API to retrieve related papers,
    synthesizes them, and provides insights. This agent uses search APIs, summarization
    models, and bibliographic management systems as tools. The process is iterated
    until it reaches a certain number of relevant articles.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文献综述阶段**：在这个阶段，为给定的研究想法收集文章。博士代理利用arXiv API检索相关论文，综合它们，并提供见解。这个代理使用搜索API、摘要模型和文献管理系统作为工具。这个过程会迭代，直到达到一定数量的相关文章。'
- en: '**Experimentation**: The first step is plan formulation, where a plan is generated
    based on the literature review and the research goal. At this stage, the PhD and
    Postdoc agents collaborate and discuss how to achieve the goals, generating a
    plan that defines which ML models to implement, which datasets to use, and other
    necessary experimental steps. Once the plan is finalized, the data preparation
    phase begins, during which the code for data preparation is generated based on
    the defined plan. An ML engineer agent has access to Hugging Face datasets, and
    the code is then compiled and submitted. During the running experiments phase,
    the ML engineer agent executes the experimental plan. At this stage, the code
    is generated, tested, and refined. The results are then interpreted. At the end
    of this phase, the PhD and Postdoc agents discuss the results. If they agree on
    the validity of the findings, they submit the results, which will serve as the
    basis of the report.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验阶段**：第一步是制定计划，根据文献综述和研究目标生成一个计划。在这个阶段，博士和博士后代理进行协作和讨论如何实现目标，生成一个定义了要实施哪些机器学习模型、使用哪些数据集以及其他必要实验步骤的计划。一旦计划确定，数据准备阶段就开始了，在这个阶段，根据定义的计划生成数据准备代码。机器学习工程师代理可以访问Hugging
    Face数据集，然后代码被编译并提交。在运行实验阶段，机器学习工程师代理执行实验计划。在这个阶段，代码被生成、测试和改进。然后对结果进行解释。在这个阶段结束时，博士和博士后代理讨论结果。如果他们同意发现的有效性，他们将提交结果，这些结果将成为报告的基础。'
- en: '**Report writing**: In the report writing phase, the PhD and professor agents
    synthesize the research findings into a comprehensive academic report. Starting
    with an initial scaffold (abstract, introduction, background, related work, methods,
    experimental setup, results, and discussion), they begin generating the text (which
    is written in LaTeX for easy revision and correction). During writing, the system
    accesses the literature and iteratively corrects the article for accuracy, clarity,
    and alignment with the research goals. Finally, a sort of paper review is conducted
    to ensure the article is correct. Note that during this process, the system receives
    feedback from humans.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**报告撰写**：在报告撰写阶段，博士和教授代理将研究结果综合成一份全面的学术报告。从初始框架（摘要、引言、背景、相关工作、方法、实验设置、结果和讨论）开始，他们开始生成文本（使用LaTeX编写，以便于修订和校正）。在撰写过程中，系统访问文献，并迭代地纠正文章以确保准确性、清晰度和与研究目标的契合度。最后，进行一种论文审查以确保文章的正确性。请注意，在此过程中，系统会收到来自人类的反馈。'
- en: The key features of this system are that the agents perform repetitive tasks
    (e.g., literature searches and coding) autonomously but allow for human input
    where creativity or judgment is essential. The agents communicate intermediate
    results with each other to ensure cohesion among the parties. At each stage, there
    is iterative improvement through reflection and feedback.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统的关键特性是代理能够自主执行重复性任务（例如，文献检索和编码），但在需要创造力和判断力的情况下允许人类输入。代理之间通过沟通中间结果来确保各方之间的协同。在每一个阶段，通过反思和反馈进行迭代改进。
- en: '![Figure 9.27 – Agent Laboratory workflow (https://arxiv.org/pdf/2501.04227)](img/B21257_09_27.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图9.27 – 代理实验室工作流程 (https://arxiv.org/pdf/2501.04227)](img/B21257_09_27.jpg)'
- en: Figure 9.27 – Agent Laboratory workflow ([https://arxiv.org/pdf/2501.04227](https://arxiv.org/pdf/2501.04227))
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.27 – 代理实验室工作流程 ([https://arxiv.org/pdf/2501.04227](https://arxiv.org/pdf/2501.04227))
- en: Agent Laboratory is designed to explore ideas quickly and help researchers in
    being able to explore multiple lines of research at the same time. The structure
    of Agent Laboratory allows it to conduct the entire workflow from an idea suggested
    by a human researcher. In this work, they focus on not only the accuracy of the
    results but also on trying to find a more efficient way of solving the task (previous
    work required too much computational cost).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 代理实验室旨在快速探索想法并帮助研究人员能够同时探索多个研究方向。代理实验室的结构允许它从由人类研究人员提出的一个想法开始，执行整个工作流程。在这项工作中，他们不仅关注结果的准确性，还试图找到一种更有效率的任务解决方法（先前的工作需要过多的计算成本）。
- en: '![Figure 9.28 – Agent Laboratory scheme (https://arxiv.org/pdf/2501.04227)](img/B21257_09_28.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图9.28 – 代理实验室方案 (https://arxiv.org/pdf/2501.04227)](img/B21257_09_28.jpg)'
- en: Figure 9.28 – Agent Laboratory scheme ([https://arxiv.org/pdf/2501.04227](https://arxiv.org/pdf/2501.04227))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.28 – 代理实验室方案 ([https://arxiv.org/pdf/2501.04227](https://arxiv.org/pdf/2501.04227))
- en: The authors point out that incorporating human feedback at various stages significantly
    improved the quality of the research outputs. Furthermore, they state that ML
    code generated by Agent Laboratory achieved performance comparable to existing
    state-of-the-art methods and that the reports generated were of notably good quality
    for humans reading them.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，在各个阶段融入人类反馈显著提高了研究成果的质量。此外，他们还表示，由代理实验室生成的机器学习代码达到了现有最先进方法的性能，并且为人类阅读的报告质量显著良好。
- en: 'These systems show that by incorporating human feedback, sophisticated tasks
    can be solved. However, these systems are dependent on human feedback because
    LLMs to date are not capable of true reasoning. There are several limitations
    to this: the system may struggle with designing innovative experiments beyond
    standard methodologies, particularly in areas requiring creative problem-solving
    or novel approaches. The system still generates errors in the code (bugs or inefficiencies),
    it continues to maintain a high computational cost (several LLM calls), communication
    between agents is not yet perfect, report generation is still suboptimal in comparison
    to an expert, it generalizes poorly to highly specialized or niche research areas
    (they are poorly represented in training data and literature), and several ethical
    issues remain open.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统表明，通过结合人类反馈，可以解决复杂任务。然而，这些系统依赖于人类反馈，因为截至目前，大型语言模型（LLMs）尚不具备真正的推理能力。这有几个局限性：系统可能在设计超出标准方法的创新实验方面遇到困难，尤其是在需要创造性问题解决或新颖方法的应用领域。系统仍然会在代码中产生错误（错误或低效），它继续维持高计算成本（多个
    LLM 调用），智能体之间的通信尚未完美，与专家相比，报告生成仍然不够优化，它对高度专业化的或利基研究领域的泛化能力较差（它们在训练数据和文献中表现不佳），并且存在几个尚未解决的伦理问题。
- en: In this section, we looked at different systems with a single agent or multiple
    agents. In the next section, we will see how HuggingGPT works in practice and
    how we can create multi-agent systems.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了具有单个智能体或多个智能体的不同系统。在下一节中，我们将看到 HuggingGPT 的实际工作方式以及我们如何创建多智能体系统。
- en: Working with HuggingGPT
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 HuggingGPT 一起工作
- en: 'There are two ways you can use HuggingGPT:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用两种方式使用 HuggingGPT：
- en: Clone the repository locally
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地克隆仓库
- en: Use the web service
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络服务
- en: Here, we will look at the two methods. The main difference is that when we clone
    the repository locally, we download all the models, and the system execution will
    be conducted locally. In contrast, the web service method requires that the execution
    is conducted in a service. In both cases, all models are used in inference; the
    difference lies in where the models are executed and the resources employed. Additionally,
    both approaches support the use of a web-based GUI.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨两种方法。主要区别在于，当我们本地克隆仓库时，我们会下载所有模型，系统执行将在本地进行。相比之下，网络服务方法要求执行在服务中进行。在两种情况下，所有模型都用于推理；区别在于模型的执行位置和使用的资源。此外，两种方法都支持使用基于网络的图形用户界面。
- en: Using HuggingGPT locally
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在本地使用 HuggingGPT
- en: To clone HuggingGPT (the corresponding repository is called Jarvis), it is useful
    to use Git LFS. Git LFS is an open source extension of Git. Git is designed to
    manage code repositories but not large binary files (such as videos, datasets,
    or high-resolution images). Git LFS is crucial for repositories that include large
    assets (e.g., datasets, videos, or binaries) because Git is otherwise inefficient
    at handling large files. Git LFS solves this problem by storing large files outside
    the regular repository objects and replacing them with lightweight references
    (pointers) in the Git repository. Git LFS keeps repository size manageable by
    storing large files outside the repository’s regular objects, allows for better
    standardization when using large objects, and improves performance during operation
    with GitHub repositories (such as cloning, pushing, and pulling). The pointer
    contains various metadata about the file (e.g., size, hash, and location), and
    when we clone a repository, Git LFS downloads the files by exploiting the information
    in these pointers. This then allows us to separate operations on the code from
    those conducted on the large files. In general, it is common to use Git LFS for
    projects involving ML, game development, or video editing, because it allows for
    simplification and speeding up of the download process. In ML projects, the model
    weights are very large and can be frequently updated; using Git LFS allows us
    to efficiently track and manage these files—such as downloaded models—without
    bloating the main repository without bloating the repository. As we mentioned,
    HuggingGPT uses several large models (for example, there are different diffusion
    models that can occupy several gigabytes), and Git LFS allows for easier management.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要克隆HuggingGPT（相应的仓库称为Jarvis），使用Git LFS非常有用。Git LFS是Git的开源扩展。Git旨在管理代码仓库，但不处理大型二进制文件（如视频、数据集或高分辨率图像）。Git
    LFS对于包含大型资产（例如，数据集、视频或二进制文件）的仓库至关重要，因为否则Git在处理大文件时效率低下。Git LFS通过在常规仓库对象之外存储大文件并替换Git仓库中的轻量级引用（指针）来解决此问题。Git
    LFS通过在仓库的常规对象之外存储大文件，使使用大型对象时标准化更好，并在与GitHub仓库（如克隆、推送和拉取）操作时提高性能。指针包含有关文件的各种元数据（例如，大小、哈希和位置），当我们克隆一个仓库时，Git
    LFS通过利用这些指针中的信息下载文件。这使我们能够将代码操作与对大文件的操作分开。一般来说，对于涉及机器学习、游戏开发或视频编辑的项目，通常使用Git LFS，因为它可以简化并加快下载过程。在机器学习项目中，模型权重非常大且经常更新；使用Git
    LFS允许我们高效地跟踪和管理这些文件——例如下载的模型——而不会使主仓库膨胀。正如我们提到的，HuggingGPT使用几个大型模型（例如，有不同类型的扩散模型，可能占用几个GB），Git
    LFS允许更容易地管理。
- en: 'To install Git LFS, you can go to the official website ([https://git-lfs.github.com/](https://git-lfs.github.com/))
    and download the installer for your operating system (Windows, macOS, or Linux).
    Run the downloaded installer. On macOS, double-click the `.pkg` file or use the
    Homebrew package manager:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Git LFS，您可以访问官方网站 ([https://git-lfs.github.com/](https://git-lfs.github.com/))
    并下载适用于您操作系统的安装程序（Windows、macOS或Linux）。运行下载的安装程序。在macOS上，双击`.pkg`文件或使用Homebrew包管理器：
- en: '[PRE0]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Run the following command to enable Git LFS for your user:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以启用Git LFS对您的用户：
- en: '[PRE1]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you have installed Git LFS as a Git extension on your computer, it will
    automatically recognize and track when there are large files in the repository
    and manage them. It modifies or creates a few Git configuration entries (such
    as in `~/.gitconfig`) so that future clones and repositories you create can use
    LFS without extra hassle.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您将Git LFS作为Git扩展安装到您的计算机上，它将自动识别和跟踪仓库中是否存在大文件，并对其进行管理。它修改或创建一些Git配置条目（例如在`~/.gitconfig`中），以便您创建的未来克隆和仓库可以无额外麻烦地使用LFS。
- en: 'Cloning an LFS-enabled repository is as simple as if it were a regular repository
    (Git LFS takes care of the files in the background and large files are managed
    automatically):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆启用了LFS的仓库就像它是一个常规仓库一样简单（Git LFS会在后台处理文件，并自动管理大文件）：
- en: '[PRE2]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we want, we can easily conduct large file tracking:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想，我们可以轻松地进行大文件跟踪：
- en: '[PRE3]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Git LFS is compatible with classical Git commands. Pull/push operations are
    conducted as in normal Git workflows—no special steps are required unless a repository
    demands specific credentials or tokens.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Git LFS与经典Git命令兼容。拉取/推送操作与正常Git工作流程中的操作一样——除非仓库需要特定的凭据或令牌，否则不需要特殊步骤。
- en: At this point, we can proceed with the installation of HuggingGPT. The HuggingGPT
    repository is stored at [https://github.com/microsoft/JARVIS](https://github.com/microsoft/JARVIS).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以继续安装HuggingGPT。HuggingGPT仓库存储在[https://github.com/microsoft/JARVIS](https://github.com/microsoft/JARVIS)。
- en: '![Figure 9.29 – Microsoft HuggingGPT](img/B21257_09_29.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图9.29 – Microsoft HuggingGPT](img/B21257_09_29.jpg)'
- en: Figure 9.29 – Microsoft HuggingGPT
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.29 – Microsoft HuggingGPT
- en: 'The first step is to clone the repository:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是克隆仓库：
- en: '[PRE4]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 9.30 – Microsoft HuggingGPT cloning](img/B21257_09_30.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图9.30 – Microsoft HuggingGPT克隆](img/B21257_09_30.jpg)'
- en: Figure 9.30 – Microsoft HuggingGPT cloning
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.30 – Microsoft HuggingGPT克隆
- en: 'The `git clone` command initiates the download of the repository from the remote
    URL. The terminal output indicates the repository being downloaded: objects (metadata
    and changes) and delta compression (a process that minimizes the amount of data
    transmitted by only sending differences between versions). Notice the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone`命令从远程URL启动仓库的下载。终端输出指示正在下载的仓库：对象（元数据和更改）和差异压缩（一个通过仅发送版本之间的差异来最小化传输数据量的过程）。注意以下内容：'
- en: '`Receiving objects: 100% (150/150), done.`: This confirms that all objects
    (files and history) have been received'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`接收对象：100% (150/150)，完成。`：这确认了所有对象（文件和历史记录）都已接收'
- en: '`Resolving deltas: 100% (85/85), done.`: Git reconstructs the actual repository
    state by applying the changes (deltas) received'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`解析差异：100% (85/85)，完成。`：Git通过应用接收到的更改（差异）来重建实际的仓库状态'
- en: 'Once we have cloned the repository, we can go to the local repository (the
    local folder):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们克隆了仓库，我们就可以转到本地仓库（本地文件夹）：
- en: '[PRE5]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This step is in preparation for creating or managing the `conda` environment,
    ensuring that the actions are performed in the context of the relevant project
    directory.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是为创建或管理`conda`环境做准备，确保操作在相关项目目录的上下文中执行。
- en: 'Then, we create a new `conda` environment named `jarvis` (or we can choose
    another name) and specify that it should use Python version 3.8:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个新的名为`jarvis`的`conda`环境（或我们可以选择其他名称），并指定它应使用Python版本3.8：
- en: '[PRE6]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that `-n` means we want a new environment for our project, and `python=3.8`
    means we are explicitly defining the Python version to be 3.8 for this environment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`-n`表示我们想要为我们的项目创建一个新的环境，而`python=3.8`表示我们明确指定此环境的Python版本为3.8：
- en: A `conda` environment allows us to isolate dependencies and avoid conflicts
    with global Python installations or other projects.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda`环境允许我们隔离依赖关系，避免与全局Python安装或其他项目发生冲突。'
- en: 'Note that `conda` is handling the following processes:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`conda`正在处理以下过程：
- en: '`conda` fetches information about the required packages and dependencies from
    its repositories. This ensures compatibility between Python 3.8 and any other
    libraries to be installed.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conda`从其仓库中检索有关所需包和依赖项的信息。这确保了Python 3.8与任何其他要安装的库之间的兼容性。'
- en: '`conda` resolves potential dependency conflicts and finalizes the list of packages
    to be installed.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conda`解决潜在的依赖冲突，并最终确定要安装的包列表。'
- en: 'Since you may have installed `conda` previously, we just need to update it:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你可能之前已经安装了`conda`，我们只需要更新它：
- en: '[PRE7]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 9.31 – Updating conda](img/B21257_09_31.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图9.31 – 更新conda](img/B21257_09_31.jpg)'
- en: Figure 9.31 – Updating conda
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.31 – 更新conda
- en: After solving the environment and preparing to create it, `conda` installs the
    required base packages for the new environment. Each package is listed alongside
    the repository (`pkgs/main`) and its specific version (in this case, we are using
    macOS).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决环境并准备创建它之后，`conda`安装新环境所需的基本包。每个包都列在仓库（`pkgs/main`）及其特定版本（在这种情况下，我们使用macOS）旁边。
- en: The terminal prompts us with `Proceed ([y]/n)?`. Remember to respond with `y`
    to confirm the installation of these packages.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 终端提示我们`继续([y]/n)?`。请记住，用`y`响应以确认安装这些包。
- en: 'Note these elements:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下元素：
- en: '`conda` ensures that the necessary dependencies are ready to be installed without
    conflicts'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conda`确保必要的依赖关系准备就绪，且无冲突'
- en: '**Verifying transaction**: It checks the integrity of the package metadata
    and ensures compatibility between all packages'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证交易**：它检查包元数据的完整性，并确保所有包之间的兼容性'
- en: '`conda` installs the packages into the specified environment'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conda`将包安装到指定的环境中'
- en: Once these steps are completed, the new environment (`jarvis`) is ready for
    use.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这些步骤，新的环境（`jarvis`）就准备好使用了。
- en: Upon successful creation, `conda` provides the user with commands for managing
    the new environment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 成功创建后，`conda`为用户提供管理新环境的命令。
- en: 'To activate this environment, use the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要激活此环境，请使用以下命令：
- en: '[PRE8]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To deactivate an active environment, use the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要停用活动环境，请使用以下命令：
- en: '[PRE9]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remember that the activation switches the user’s terminal session to use the
    `jarvis` environment, isolating its dependencies and Python version. Notice that
    the prompt changes from `(base)` to `(jarvis)`, indicating that the terminal is
    now operating within the `jarvis` environment. The environment’s isolated Python
    version (3.8) and its dependencies are now being used. Any libraries or tools
    installed from this point will remain confined to this environment, avoiding interference
    with other projects.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，激活操作会将用户的终端会话切换到使用`jarvis`环境，隔离其依赖和Python版本。注意，提示符从`(base)`变为`(jarvis)`，表示终端现在正在`jarvis`环境中运行。该环境的隔离Python版本（3.8）及其依赖现在正在使用。从此点开始安装的任何库或工具都将局限于该环境，避免与其他项目发生干扰。
- en: '![Figure 9.32 – conda activation](img/B21257_09_32.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图9.32 – conda激活](img/B21257_09_32.jpg)'
- en: Figure 9.32 – conda activation
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.32 – conda激活
- en: 'At this point, we begin to install the various requirements:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们开始安装各种需求：
- en: '[PRE10]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following command uses `pip` to install dependencies listed in a `requirements.txt`
    file (most often, a list of packages is provided in a requirements file). These
    requirements are necessary to install HuggingGPT:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令使用`pip`安装`requirements.txt`文件中列出的依赖项（通常，在需求文件中提供了一个包列表）。这些需求是安装HuggingGPT所必需的：
- en: '[PRE11]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following comment in HuggingGPT emphasizes that Git LFS must be installed.
    This script (provided as part of the project) automates the download of model
    files required for local or hybrid inference modes. As a reminder, local means
    the model runs entirely on the local machine and hybrid means the inference involves
    a mix of local and remote execution, as was described in the HuggingGPT paper
    ([https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580)) and in
    the preceding section:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingGPT中的以下注释强调必须安装Git LFS。此脚本（作为项目的一部分提供）自动化下载本地或混合推理模式所需的模型文件。提醒一下，本地意味着模型完全在本地机器上运行，混合意味着推理涉及本地和远程执行的混合，正如HuggingGPT论文（[https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580)）和前述章节所述：
- en: '[PRE12]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once we have installed everything, we can start the execution:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完毕，我们就可以开始执行：
- en: '[PRE13]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'There are different scripts in the repository:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中有不同的脚本：
- en: '`model_server.py`: This script runs a model server, which processes ML models
    based on the configuration file (`config/config.default.yaml`). The configuration
    file specifies parameters such as inference mode (local or hybrid), paths to the
    models, and hardware requirements.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_server.py`：此脚本运行一个模型服务器，根据配置文件（`config/config.default.yaml`）处理机器学习模型。配置文件指定了参数，例如推理模式（本地或混合）、模型路径和硬件要求。'
- en: '`awesome_chat.py`: This script starts a server for text generation or chatbot
    functionality.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`awesome_chat.py`：此脚本启动一个用于文本生成或聊天机器人功能的服务器。'
- en: '![Figure 9.33 – Microsoft HuggingGPT finalizing installation](img/B21257_09_33.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图9.33 – Microsoft HuggingGPT完成安装](img/B21257_09_33.jpg)'
- en: Figure 9.33 – Microsoft HuggingGPT finalizing installation
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.33 – Microsoft HuggingGPT完成安装
- en: Since we have initialized `awesome_chat.py`, we can use a user-friendly web
    page.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经初始化了`awesome_chat.py`，我们可以使用一个用户友好的网页。
- en: Using HuggingGPT on the web
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在网上使用HuggingGPT
- en: 'If you do not want to install HuggingGPT, you can use the online suite instead
    (on Hugging Face Gradio: [https://huggingface.co/gradio](https://huggingface.co/gradio)).
    **Hugging Face Gradio** is a Python library that simplifies the process of creating
    user-friendly web-based interfaces for ML models and other Python applications.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想安装HuggingGPT，可以使用在线套件（在Hugging Face Gradio上：[https://huggingface.co/gradio](https://huggingface.co/gradio)）。**Hugging
    Face Gradio**是一个Python库，简化了创建用户友好的基于Web的界面以供机器学习模型和其他Python应用程序使用的流程。
- en: With Gradio, developers can quickly build interactive demos for tasks such as
    text generation, image classification, and audio processing. These interfaces
    allow users to test models directly in their browser by providing inputs (e.g.,
    text, images, or audio) and viewing real-time outputs. Gradio is highly customizable,
    supports integration with popular ML frameworks (such as PyTorch, TensorFlow,
    and Hugging Face models), and enables easy sharing of demos through public links
    or embedding in web applications.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gradio，开发者可以快速构建交互式演示，例如文本生成、图像分类和音频处理等任务。这些界面允许用户通过提供输入（例如文本、图像或音频）并在浏览器中查看实时输出，直接在浏览器中测试模型。Gradio高度可定制，支持与流行的ML框架（如PyTorch、TensorFlow和Hugging
    Face模型）集成，并可通过公共链接或嵌入到Web应用程序中轻松共享演示。
- en: 'The authors created a Gradio interface (launching Jarvis from local allows
    such an interface). The Gradio space can be accessed here: [https://huggingface.co/spaces/microsoft/HuggingGPT](https://huggingface.co/spaces/microsoft/HuggingGPT).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建了一个Gradio界面（从本地启动Jarvis允许此类界面）。Gradio空间可以通过以下链接访问：[https://huggingface.co/spaces/microsoft/HuggingGPT](https://huggingface.co/spaces/microsoft/HuggingGPT)。
- en: 'As said, HuggingGPT is a system that connects LLMs with the ML community. As
    seen previously in the description of the system and its installation, the web
    interface also does exactly the same: connect an LLM with a set of ML models that
    are hosted on Hugging Face. In the web interface, only a few models are deployed
    on the `local/inference` endpoint due to hardware limitations (this interface
    serves as an example to understand and see in action how the system works).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '正如之前在系统及其安装描述中所述，HuggingGPT是一个将LLM与ML社区连接起来的系统。在Web界面上，它也执行了完全相同的功能：将一个LLM与托管在Hugging
    Face上的ML模型集连接起来。由于硬件限制，Web界面仅在`local/inference`端点部署了少量模型（此界面作为了解和查看系统工作原理的示例）。 '
- en: 'Note that we need two tokens, which a user needs to obtain from each website:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要两个令牌，用户需要从每个网站获取：
- en: '**Hugging Face token**: This is a personal authentication key that allows users
    to securely access Hugging Face’s services, including their API, models, datasets,
    and other resources hosted on the platform. This token acts as an identifier for
    your account, ensuring that your requests to Hugging Face’s systems are authorized
    and linked to your account. The token is then used to authenticate and use the
    models in inference. Hugging Face enforces rate limits for some services, especially
    for web inference.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face令牌**：这是一个个人认证密钥，允许用户安全地访问Hugging Face的服务，包括其API、模型、数据集以及平台托管的其他资源。此令牌作为您账户的标识符，确保您的请求得到Hugging
    Face系统的授权并与您的账户关联。该令牌随后用于验证和使用推理中的模型。Hugging Face对某些服务实施速率限制，尤其是对Web推理。'
- en: '**OpenAI key**: This is a unique authentication key provided by OpenAI that
    enables developers to securely access and interact with OpenAI’s APIs and services,
    such as GPT (e.g., GPT-3.5 or GPT-4), DALL·E, Codex, and Whisper. This key acts
    as a personalized credential that identifies your account and authorizes your
    usage of OpenAI’s platform. The key is required to authenticate requests sent
    to OpenAI’s API endpoints. OpenAI uses your API key to track your usage (e.g.,
    the number of API calls made and tokens processed) and bill your account accordingly.
    In this case, the connection to GPT-4 is used.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI密钥**：这是OpenAI提供的唯一认证密钥，允许开发者安全地访问和交互OpenAI的API和服务，例如GPT（例如GPT-3.5或GPT-4）、DALL·E、Codex和Whisper。此密钥作为个性化凭证，用于识别您的账户并授权您使用OpenAI的平台。该密钥是向OpenAI
    API端点发送请求进行验证所必需的。OpenAI使用您的API密钥跟踪您的使用情况（例如，API调用次数和处理的令牌数）并相应地向您收费。在这种情况下，使用的是GPT-4的连接。'
- en: Once we have our tokens ready, we can enter our question and click **Submit**.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了令牌，我们就可以输入我们的问题并点击**提交**。
- en: '![Figure 9.34 – HuggingGPT interface](img/B21257_09_34.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图9.34 – HuggingGPT界面](img/B21257_09_34.jpg)'
- en: Figure 9.34 – HuggingGPT interface
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.34 – HuggingGPT界面
- en: 'We can see that there are two main panels:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有两个主要面板：
- en: '**Left panel**: A text input box labeled **Chatbot** is provided. This field
    is intended for user inputs, such as questions or commands, to interact with the
    HuggingGPT system.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**左侧面板**：提供了一个标记为**聊天机器人**的文本输入框。该字段用于用户输入，例如问题或命令，以与HuggingGPT系统交互。'
- en: '**Right panel**: There is an empty box next to the chatbot reserved for responses
    or outputs generated by HuggingGPT.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**右侧面板**：在聊天机器人旁边预留了一个空白的框，用于显示HuggingGPT生成的响应或输出。'
- en: Below the chatbox, there is a button labeled **Send**, allowing users to submit
    their queries to HuggingGPT.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天框下方，有一个标有**发送**的按钮，允许用户将他们的查询提交给HuggingGPT。
- en: 'Note that the system already provides ready-made examples that we can use:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，系统已经提供了我们可以使用的现成示例：
- en: '![Figure 9.35 – HuggingGPT interface provided examples](img/B21257_09_35.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.35 – HuggingGPT 提供的示例](img/B21257_09_35.jpg)'
- en: Figure 9.35 – HuggingGPT interface provided examples
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35 – HuggingGPT 提供的示例
- en: 'We enter our tokens for both OpenAI and Hugging Face. Using the text input
    box labeled **Chatbot**, we can send natural language queries to HuggingGPT (“*Can
    you tell me which kind of pizza you see in the picture?*”) and send the query
    with the **Send** button. In addition, images or other multimedia elements can
    be added (in our case, we have added a picture of a pizza):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入OpenAI和Hugging Face的标记。使用标有**聊天机器人**的文本输入框，我们可以向HuggingGPT发送自然语言查询（“*你能告诉我你在图片中看到哪种披萨吗？*”）并通过**发送**按钮发送查询。此外，还可以添加图像或其他多媒体元素（在我们的案例中，我们添加了一张披萨的图片）：
- en: '![Figure 9.36 – Example of HuggingGPT interaction](img/B21257_09_36.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.36 – HuggingGPT 交互示例](img/B21257_09_36.jpg)'
- en: Figure 9.36 – Example of HuggingGPT interaction
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.36 – HuggingGPT 交互示例
- en: 'In the panel on the right-hand side of the figure, we see the process that
    the system is working through: *1 pepperoni pizza on a wooden table.* This indicates
    that the system successfully processed the input image and identified the object
    depicted as *pepperoni pizza*. This is a typical object detection task, and the
    system is using a model to identify the object (it is not an LLM that conducts
    the image recognition but a specialized model that is invoked by the LLM).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在图右侧的面板上，我们看到系统正在处理的过程：*1 个放在木桌上的意大利辣香肠披萨.* 这表明系统成功处理了输入图像，并识别出描绘为*意大利辣香肠披萨*的对象。这是一个典型的目标检测任务，系统正在使用一个模型来识别对象（这并不是一个进行图像识别的LLM，而是一个由LLM调用的专门模型）。
- en: 'The chatbot provides a detailed answer based on the inference results:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人根据推理结果提供详细的答案：
- en: '[PRE14]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'HuggingGPT explains the process:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingGPT 解释了过程：
- en: The first step involves the use of an image-to-text model to get a description
    of the image. **ViT-GPT2-COCO-EN** is a vision-language model that combines a
    **Vision Transformer** (**ViT**) for image encoding and **GPT-2** for natural
    language generation, fine-tuned on the **COCO dataset** for image captioning tasks.
    The model generates descriptive captions in English for input images, effectively
    translating visual content into coherent textual descriptions. It leverages the
    power of ViT for extracting detailed image features and GPT-2’s language generation
    capabilities to produce accurate and contextually rich captions.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步涉及使用图像到文本模型来获取图像的描述。**ViT-GPT2-COCO-EN** 是一个视觉语言模型，它结合了一个用于图像编码的**视觉Transformer**（**ViT**）和一个用于自然语言生成的**GPT-2**，在**COCO数据集**上进行了图像标题任务的微调。该模型为输入图像生成英文描述性标题，有效地将视觉内容转换为连贯的文本描述。它利用ViT提取详细图像特征，并利用GPT-2的语言生成能力来生成准确且上下文丰富的标题。
- en: Then, HuggingGPT uses an object detection model to identify objects within an
    image. This object detection model also provides a similar response because it
    identifies both a pizza and a dining table. **DETR-ResNet-101** is a vision model
    designed for object detection and image segmentation. It combines a **ResNet-101**
    backbone (a convolutional neural network) for feature extraction with a **transformer-based
    architecture** for detecting and localizing objects in an image. **DEtection TRansformer**
    (**DETR**) uses transformers to model global relationships in an image, allowing
    for more accurate object detection without the need for traditional region proposal
    networks.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，HuggingGPT使用一个目标检测模型来识别图像中的对象。这个目标检测模型也提供了类似的响应，因为它识别了披萨和餐桌。**DETR-ResNet-101**
    是一个用于目标检测和图像分割的视觉模型。它结合了一个用于特征提取的**ResNet-101** 主干（卷积神经网络）和一个用于检测和定位图像中对象的**基于transformer的架构**。**DEtection
    TRansformer**（**DETR**）使用transformer来建模图像中的全局关系，从而在不需要传统区域提议网络的情况下实现更准确的目标检测。
- en: Then, a visual-answering model confirms what type of pizza is in the image.
    **ViLT-B/32-Finetuned-VQA** is a vision-and-language transformer model fine-tuned
    for **Visual Question-Answering** (**VQA**) tasks. It combines a lightweight **Vision-and-Language
    Transformer** (**ViLT**) architecture with a patch-based image tokenizer and transformer
    layers to process both visual and textual inputs jointly. The B/32 refers to the
    use of a 32 x 32 pixel patch size for image encoding. Fine-tuned specifically
    for VQA datasets, the model is designed to answer natural language questions about
    input images by reasoning over the visual and textual information.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，一个视觉回答模型确认图像中是哪种披萨。**ViLT-B/32-Finetuned-VQA** 是一个针对**视觉问答**（**VQA**）任务进行微调的视觉和语言转换器模型。它结合了一个轻量级的**视觉和语言转换器**（**ViLT**）架构、基于补丁的图像分词器和转换器层，以联合处理视觉和文本输入。B/32指的是使用32
    x 32像素的补丁大小进行图像编码。该模型专门针对VQA数据集进行微调，旨在通过推理视觉和文本信息来回答关于输入图像的自然语言问题。
- en: Finally, the LLM observes that the three models are in agreement and thus is
    confident in responding.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，LLM观察到三个模型意见一致，因此对响应有信心。
- en: To recap, HuggingGPT receives a request from the user and selects patterns.
    These patterns are executed, and outputs are collected. The system analyzes what
    these outputs are and generates a final response.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，HuggingGPT接收用户的请求并选择模式。这些模式被执行，输出被收集。系统分析这些输出是什么，并生成最终响应。
- en: '![Figure 9.37 – Example of HuggingGPT response](img/B21257_09_37.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图9.37 – HuggingGPT响应示例](img/B21257_09_37.jpg)'
- en: Figure 9.37 – Example of HuggingGPT response
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.37 – HuggingGPT响应示例
- en: HuggingGPT shows with a simple example how a multimodal task can be solved with
    an LLM. This is all done using information in the prompt and a set of tools.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingGPT通过一个简单的例子展示了如何使用LLM解决多模态任务。这一切都是使用提示中的信息和一系列工具完成的。
- en: In this section, we have seen a single LLM (a single agent) process a task,
    divide it into subtasks, and execute different models. A more elegant approach
    is to use multiple agents that approach a task from different perspectives, collaborate,
    and interact to solve a task. In the next subsection, we will see how this can
    be achieved.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了一个单个LLM（单个代理）处理任务，将其分解为子任务，并执行不同的模型。一个更优雅的方法是使用多个代理从不同的角度处理任务，协作并交互以解决问题。在下一个小节中，我们将看到如何实现这一点。
- en: Multi-agent system
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多代理系统
- en: In this section, we see how we can create a system that considers different
    agents and a set of tools (such as ML models). The entire code can be found in
    the `Multi_Model–Travel_Planning_System.py` script.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何创建一个考虑不同代理和一系列工具（如ML模型）的系统。整个代码可以在`Multi_Model–Travel_Planning_System.py`脚本中找到。
- en: 'As a general overview, the system implements a travel planning assistant that
    uses several agents to create personalized travel plans. The system then combines
    weather prediction, hotel recommendations, itinerary planning, and email summarization.
    In other words, we have four different agents, each dealing with a different aspect
    of travel planning:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般概述，该系统实现了一个使用多个代理创建个性化旅行计划的旅行规划助手。然后，系统结合天气预测、酒店推荐、行程规划和电子邮件摘要。换句话说，我们有四个不同的代理，每个代理处理旅行规划的不同方面：
- en: '`WeatherAnalysisAgent`: Uses a random forest regressor to predict the best
    time to visit a location based on historical weather data. Trains on past weather
    data (month, latitude, longitude, and weather score) and predicts the best months
    for travel based on weather scores. This agent then uses an ML model to conduct
    predictions (a model that is trained specifically for the system).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WeatherAnalysisAgent`：使用随机森林回归器根据历史天气数据预测最佳访问地点的时间。在过去的天气数据（月份、纬度、经度和天气评分）上训练，并根据天气评分预测最佳的旅行月份。然后，该代理使用一个ML模型进行预测（一个专门为系统训练的模型）。'
- en: '`HotelRecommenderAgent`: Uses Sentence Transformer embeddings to find hotels
    based on user preferences. Stores hotel descriptions and converts them into embeddings,
    after which it matches user preferences with the most relevant hotels using semantic
    similarity. This agent, based on user preferences, searches its library for possible
    solutions.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HotelRecommenderAgent`：使用句子转换器嵌入根据用户偏好查找酒店。存储酒店描述并将它们转换为嵌入，然后使用语义相似性将用户偏好与最相关的酒店匹配。基于用户偏好，该代理在其库中搜索可能的解决方案。'
- en: '`ItineraryPlannerAgent`: Uses GPT-2 (text-generation pipeline) to create personalized
    travel itineraries. The agent generates trip plans based on destination, weather
    prediction, and hotel recommendations.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ItineraryPlannerAgent`：使用GPT-2（文本生成管道）创建个性化的旅行行程。代理根据目的地、天气预测和酒店推荐生成行程计划。'
- en: '`SummaryAgent`: Uses GPT-2 to generate a summary email for the client. This
    summary includes the hotel cost (per night cost × duration) and additional daily
    expenses. After that, it generates a personalized email with trip details, cost
    breakdown, and itinerary highlights.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SummaryAgent`：使用GPT-2为客户生成摘要电子邮件。此摘要包括酒店费用（每晚费用×持续时间）和额外的每日费用。之后，它生成包含旅行详情、费用细分和行程高光的个性化电子邮件。'
- en: 'The following figure presents a schema of the agents and the process:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了代理和流程的架构：
- en: '![Figure 9.38 – Activity diagram of the AI Travel Planning System workflow
    showing the full sequence from data loading and agent initialization to trip planning
    and result output](img/B21257_09_38.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图9.38 – AI旅行规划系统工作流程的活动图，显示了从数据加载和代理初始化到行程规划和结果输出的完整序列](img/B21257_09_38.jpg)'
- en: Figure 9.38 – Activity diagram of the AI Travel Planning System workflow showing
    the full sequence from data loading and agent initialization to trip planning
    and result output
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.38 – AI旅行规划系统工作流程的活动图，显示了从数据加载和代理初始化到行程规划和结果输出的完整序列
- en: '`TravelPlanningSystem` links all agents together and is basically the main
    controller of the system. The system thus mimics this flow:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`TravelPlanningSystem`将所有代理连接在一起，基本上是系统的主控制器。因此，系统模仿了以下流程：'
- en: The user provides the destination, preferences, and duration.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户提供目的地、偏好和持续时间。
- en: The weather agent predicts the best time to visit.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天气代理预测最佳访问时间。
- en: The hotel agent finds matching accommodation.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 酒店代理找到匹配的住宿。
- en: The itinerary agent creates daily plans.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行程代理创建每日计划。
- en: The summary agent generates an email and calculates costs.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摘要代理生成电子邮件并计算费用。
- en: 'Going into detail, we can see that agents here are defined as classes. `WeatherAnalysisAgent`
    is an ML-based component that analyzes historical weather data and predicts the
    best months to visit a given location. It does this using a Random Forest Regressor.
    We can see it as an agent using an ML model to perform a task. This snippet is
    initializing the agent:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 详细来说，我们可以看到这里的代理被定义为类。`WeatherAnalysisAgent`是一个基于机器学习的组件，它分析历史天气数据并预测给定位置的最好访问月份。它使用随机森林回归器来完成这项工作。我们可以将其视为一个使用机器学习模型执行任务的代理。此片段正在初始化代理：
- en: '[PRE15]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This agent creates a `RandomForestRegressor` model (`n_estimators=100` means
    the model consists of 100 decision trees) that must learn patterns from historical
    weather data, and then must predict weather scores for different months and locations:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此代理创建一个`RandomForestRegressor`模型（`n_estimators=100`表示模型由100个决策树组成），必须从历史天气数据中学习模式，然后必须预测不同月份和位置的天气分数：
- en: '[PRE16]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As mentioned before, this model is not trained (i.e., it is not used in inference)
    but is trained on the spot. For this, we have within our class a `train` method.
    Random forest uses month, latitude, and longitude for a location to learn to predict
    a `weather_score` value (a numerical score representing how good the weather is
    in that month). In this snippet, the data is processed and the model is trained.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，此模型不是预先训练的（即，它不用于推理）而是在现场训练的。为此，我们类中有一个`train`方法。随机森林使用月份、纬度和经度来学习预测一个`weather_score`值（表示该月天气好坏的数值分数）。在这个片段中，数据被处理，模型被训练。
- en: 'At this point, we can use `predict_best_time` as a method that predicts the
    best months to visit a location based on the trained weather model. In this case,
    the method takes only two inputs (the latitude and longitude of the location)
    and returns its predictions:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用`predict_best_time`作为方法，根据训练的天气模型预测最佳访问月份。在这种情况下，该方法只接受两个输入（位置的纬度和经度）并返回其预测：
- en: '[PRE17]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we initialize predictions, which will contain all scores for 12 months
    (in fact, predictions are conducted in a loop through all 12 months, from January
    to December). Finally, we reorder the list from best to worst to identify the
    best months to visit. The method then returns a list of the top three months with
    the highest predicted weather scores.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们初始化预测，它将包含12个月的所有分数（实际上，预测是通过循环所有12个月进行的，从一月到十二月）。最后，我们将列表从最好到最坏排序，以确定最佳访问月份。然后，该方法返回具有最高预测天气分数的前三个月份的列表。
- en: '`HotelRecommenderAgent` is a hotel recommendation system that utilizes semantic
    similarity to match hotels with user preferences and uses natural language processing
    (**NLP**) to understand and compare hotel descriptions and user preferences:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`HotelRecommenderAgent`是一个酒店推荐系统，它利用语义相似性将酒店与用户偏好相匹配，并使用自然语言处理（**NLP**）来理解和比较酒店描述和用户偏好：'
- en: '[PRE18]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: During agent initialization, `all-MiniLM-L6-v2` (a pre-trained NLP model designed
    for semantic similarity) is loaded. This model is an embedder (as described in
    [*Chapter 5*](B21257_05.xhtml#_idTextAnchor077)), converting text (hotel descriptions
    and user preferences) into vector embeddings (numerical representations in a multi-dimensional
    space). Once we have vectors, we can measure the similarity between two vectors
    (user preferences and hotel descriptions). The agent retrieves the available hotels
    (`self.hotels_db`) and can store precomputed embeddings (numerical vectors) for
    all hotel descriptions (`self.hotels_embeddings`).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理初始化期间，加载了`all-MiniLM-L6-v2`（一个为语义相似性设计的预训练NLP模型）。这个模型是一个嵌入器（如[*第五章*](B21257_05.xhtml#_idTextAnchor077)中所述），将文本（酒店描述和用户偏好）转换为向量嵌入（多维空间中的数值表示）。一旦我们有了向量，我们就可以测量两个向量之间的相似度（用户偏好和酒店描述）。代理检索可用的酒店（`self.hotels_db`）并可以存储所有酒店描述的预计算嵌入（数值向量）。
- en: 'Next, in the following snippet, we have `add_hotels`, which adds hotels to
    the database and computes the embedding for the description, and then adds it
    to our embeddings database. `find_hotels` finds hotels that match the user’s preferences
    using semantic similarity:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在下面的代码片段中，我们有一个`add_hotels`函数，它将酒店添加到数据库中，并计算描述的嵌入，然后将它添加到我们的嵌入数据库中。`find_hotels`函数通过语义相似性找到符合用户偏好的酒店：
- en: '[PRE19]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: What happens is that we conduct embedding of a user’s preferences and then compute
    the cosine similarity with all stored hotel vectors. In this case, we then select
    the five hotels that are closest to our hotel description (`top_k=5` means selecting
    the top five hotels).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的情况是我们对用户的偏好进行嵌入，然后计算与所有存储的酒店向量的余弦相似度。在这种情况下，我们选择与我们的酒店描述最接近的五个酒店（`top_k=5`表示选择前五个酒店）。
- en: '`ItineraryPlannerAgent` is responsible for automatically generating travel
    itineraries based on destination information (city or attractions), weather predictions
    (best months to visit), hotel recommendations (selected accommodation), and trip
    duration (number of days). It uses a natural language model (GPT-2) to generate
    customized travel itineraries based on these inputs:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`ItineraryPlannerAgent`负责根据目的地信息（城市或景点）、天气预测（最佳访问月份）、酒店推荐（选择的住宿）和旅行时长（天数）自动生成旅行行程。它使用自然语言模型（GPT-2）根据这些输入生成定制的旅行行程：'
- en: '[PRE20]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The agent initializes an NLP model (GPT-2 Model, which is a pre-trained language
    model for text generation) using the Hugging Face transformers library. We select
    a pipeline that is focused on text generation (`"text-generation"` means the model
    will generate text based on a prompt). Other parameters mean we limit the generated
    text to 500 tokens (`max_length=500`) and we ensure truncation.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 代理使用Hugging Face transformers库初始化一个NLP模型（GPT-2模型，这是一个用于文本生成的预训练语言模型）。我们选择一个专注于文本生成的管道（`"text-generation"`表示模型将根据提示生成文本）。其他参数意味着我们将生成的文本限制在500个标记以内（`max_length=500`）并确保截断。
- en: 'Since we interact with LLMs through prompts, we have a method that allows us
    to create a structured prompt that we will then use to interact with the model.
    This prompt is designed to be able to generate a travel plan, where it enters
    some specific information: the length of stay (duration), the destination, weather
    information (the best mounts we identified earlier), hotel selection (which were
    identified with the previous agent), and a list of attractions:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通过提示与LLMs交互，我们有一个方法允许我们创建一个结构化的提示，然后我们将使用这个提示与模型交互。这个提示被设计成能够生成一个旅行计划，其中它输入一些特定的信息：停留时长（持续时间）、目的地、天气信息（我们之前确定的最佳月份）、酒店选择（由之前的代理识别），以及一个景点列表：
- en: '[PRE21]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At this point, we can create the itinerary; the `create_itinerary` method precisely
    takes the previous prompt that contains all the information we need (destination,
    weather, hotel selection, and trip duration). Inside the `create_itinerary` method
    is a method called `_create_prompt` to generate the prompt. The GPT-2 model takes
    the input prompt and produces a detailed itinerary:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以创建行程；`create_itinerary` 方法精确地接受包含我们所需所有信息的先前提示（目的地、天气、酒店选择和旅行时长）。在
    `create_itinerary` 方法内部有一个名为 `_create_prompt` 的方法来生成提示。GPT-2 模型接受输入提示并生成详细的行程：
- en: '[PRE22]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The final agent, that is, `SummaryAgent`, is responsible for summarizing trip
    details, calculating the total estimated cost, and generating a personalized email
    for the client using GPT-2\. Our agent is initialized similar to the previous
    agent; the only difference is that in this case, the generation length is greater
    (`max_length=1000`):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个代理，即 `SummaryAgent`，负责总结旅行详情，计算总估算成本，并使用 GPT-2 为客户生成个性化电子邮件。我们的代理初始化与之前的代理类似；唯一的区别在于，在这种情况下，生成长度更大（`max_length=1000`）：
- en: '[PRE23]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`calculate_total_price` is a tool that is used by the agent to be able to calculate
    the total cost of the trip (remember that LLMs are not good at arithmetic, so
    it is better to use an external tool):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_total_price` 是代理用来计算旅行总成本的工具（记住，LLMs 在算术方面并不擅长，因此最好使用外部工具）：'
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The agent does a series of very simple calculations:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 代理执行一系列非常简单的计算：
- en: The hotel price per night is multiplied by the duration of the stay
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每晚的酒店价格乘以停留时间
- en: A fixed daily expense of $100 is used to estimate costs for meals, transport,
    activities, and sightseeing tickets
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定的每日费用 $100 来估算餐饮、交通、活动和观光门票的成本
- en: Hotel and additional costs are added to return the final estimate
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将酒店和额外费用添加以返回最终估算
- en: '`create_email` allows you to create the email summary that will be sent to
    the customer:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_email` 允许您创建将发送给客户的电子邮件摘要：'
- en: '[PRE25]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we can see, the email will be structured to include costs (we use the method
    described previously) and the other information we obtained earlier. Note that
    we use a template.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，电子邮件将被结构化以包含成本（我们使用之前描述的方法）以及我们之前获得的其他信息。请注意，我们使用了一个模板。
- en: 'Remember, `TravelPlanningSystem` is the main controller that integrates all
    AI agents for automated travel planning:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`TravelPlanningSystem` 是主控制器，它集成了所有 AI 代理以实现自动旅行规划：
- en: '[PRE26]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the first step, we initialize our four agents. Each agent will handle a
    specific task. If you noticed, we have used a modular system. The advantages of
    this are as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们初始化我们的四个代理。每个代理将处理一个特定的任务。如果您注意到了，我们使用了一个模块化系统。其优势如下：
- en: Each component operates independently, making the system scalable
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组件独立运行，使系统可扩展
- en: Components can be updated or replaced without affecting others
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件可以更新或替换，而不会影响其他组件
- en: It follows the **Single Responsibility Principle** (**SRP**) for clean code
    architecture
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它遵循 **单一职责原则**（**SRP**）以实现干净的代码架构
- en: 'At this point, we can start the setup – getting the best hotels and the best
    months to visit:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以开始设置——获取最佳酒店和最佳访问月份：
- en: '[PRE27]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, you have to coordinate the entire trip and then generate the summary
    email with cost estimates and the itinerary:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您必须协调整个旅行并生成包含成本估算和行程安排的总结电子邮件：
- en: '[PRE28]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that we have created the multi-agent platform, we have to execute it. The
    `main()` function serves as the entry point for running the *Travel Planning System*.
    It demonstrates the system’s functionality by doing the following:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了多代理平台，我们必须执行它。`main()` 函数作为运行 *旅行规划系统* 的入口点。它通过以下方式演示了系统的功能：
- en: Initializing sample data (weather history and hotels)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化样本数据（天气历史和酒店）
- en: Setting up and training AI models
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置和训练 AI 模型
- en: Executing the travel planning process
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行旅行规划过程
- en: Printing the generated trip summary
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印生成的旅行摘要
- en: 'We provide the system with various information about the weather, destination,
    hotels, and so on. After that, the system is initialized and executed. At this
    point, it prints travel summary details and the personalized email generated by
    GPT-2, and it shows the estimated total trip cost:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向系统提供了有关天气、目的地、酒店等方面的各种信息。之后，系统被初始化并执行。此时，它打印旅行摘要详情和由 GPT-2 生成的个性化电子邮件，并显示估计的总旅行成本：
- en: '[PRE29]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Ensure that the `main()` script runs only if the script is executed directly:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 确保只有当脚本直接执行时，`main()` 脚本才运行：
- en: '[PRE30]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At this point, we just have to test it. Once you have run the script, this
    should be the result:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只需要对其进行测试。一旦运行了脚本，结果应该是这样的：
- en: '![Figure 9.39 – Screenshots showing the execution](img/B21257_09_39.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图9.39 – 展示执行的截图](img/B21257_09_39.jpg)'
- en: Figure 9.39 – Screenshots showing the execution
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.39 – 展示执行的截图
- en: This *Travel Planning System* is a prototype demonstrating how AI agents can
    collaborate to automate a real-world problem.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这个**旅行规划系统**是一个原型，展示了AI智能体如何协作来自动化现实世界的问题。
- en: 'Of course, a whole series of improvements can be made to make the system more
    useful:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以对系统进行一系列改进，使其更有用：
- en: The data used is static (it is a toy example). You could connect with a number
    of APIs to obtain real-time data for the weather (OpenWeatherMap or AccuWeather),
    hotels (Booking.com or Expedia API), and destinations (Google Places API or Yelp).
    Extensions such as flights and transportation could also be added (Google Flights
    API or Rome2Rio).
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的数据是静态的（这是一个玩具示例）。你可以连接到多个API以获取实时数据，例如天气（OpenWeatherMap或AccuWeather）、酒店（Booking.com或Expedia
    API）和目的地（Google Places API或Yelp）。还可以添加扩展，如航班和交通（Google Flights API或Rome2Rio）。
- en: GPT-2 is outdated (we used it because it is much smaller than other models)
    and not fine-tuned for travel. You can replace GPT-2 with a larger or travel-optimized
    model. For example, you could use larger models such as GPT-4 or Claude, or open
    source alternatives such as LLaMA. Also, open source models can be fine-tuned
    on real travel itineraries from Tripadvisor, Lonely Planet, or Reddit.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2已经过时了（我们使用它是因为它比其他模型小得多）并且没有针对旅行进行微调。你可以用更大的或针对旅行优化的模型替换GPT-2。例如，你可以使用更大的模型，如GPT-4或Claude，或者开源替代品，如LLaMA。此外，开源模型可以在Tripadvisor、Lonely
    Planet或Reddit的真实旅行行程上进行微调。
- en: The itinerary is generic and not adaptable to different types of travelers.
    You could ask for different information from the traveler, such as budget preferences,
    what kinds of activities they prefer (cultural, adventure, food, family-friendly,
    and so on), or whether they need special accommodations (wheelchair, traveling
    with elderly, or pet-friendly). This requires a larger model, and you can also
    test recommendation models. In addition, there are methods and models that implement
    **Multi-Criteria Decision-Making** (**MCDM**) to conduct more sophisticated rankings.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行程是通用的，不能适应不同类型的旅行者。你可以向旅行者询问不同的信息，例如预算偏好、他们喜欢什么样的活动（文化、冒险、美食、家庭友好型等），或者他们是否需要特殊住宿（轮椅、与老年人同行或宠物友好型）。这需要一个更大的模型，你也可以测试推荐模型。此外，还有实现**多标准决策制定**（**MCDM**）的方法和模型，以进行更复杂的排名。
- en: 'In any case, this system, though simple, allows us to see several interesting
    elements:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，尽管这个系统很简单，但它让我们看到了几个有趣的因素：
- en: Instead of using one large monolithic AI model, the system is broken down into
    specialized agents. This idea can come in handy for modern software design.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与使用一个大型的单体AI模型不同，该系统被分解为专门的智能体。这个想法对于现代软件设计来说非常有用。
- en: This simple example mimics how multi-agent AI platforms work in autonomous vehicles,
    finance, healthcare, and robotics. In fact, multi-agent collaboration is a system
    designed with scalability, modularity, and efficiency in mind, which are necessary
    for real-world applications.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个简单的例子模拟了多智能体AI平台在自动驾驶汽车、金融、医疗保健和机器人技术中的工作方式。实际上，多智能体协作是一个旨在考虑可扩展性、模块化和效率的系统，这对于现实世界的应用是必要的。
- en: The system can dynamically generate personalized recommendations (although in
    our case, it is hardcoded, we are mimicking what happens when a user enters their
    preferences).
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该系统可以动态生成个性化推荐（尽管在我们的案例中是硬编码的，我们模拟了用户输入偏好时发生的情况）。
- en: The system also analyzes multiple factors (weather, hotels, and attractions)
    and optimizes travel plans. Modern systems that do something similar use precise
    ML models (we used random forest in our example), have vast databases (in our
    case, we are mimicking a database of hotels), take user preferences into account,
    and use automated systems to respond to the customer (our email).
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该系统还分析了多个因素（天气、酒店和景点）并优化旅行计划。类似地做这件事的现代系统使用精确的机器学习模型（在我们的例子中，我们使用了随机森林），拥有庞大的数据库（在我们的案例中，我们模拟了一个酒店数据库），考虑用户偏好，并使用自动化系统响应用户（我们的电子邮件）。
- en: 'Although this is a very simple system, we can think about how a similar system
    could be used in various other industries:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个非常简单的系统，但我们可以思考一个类似的系统如何在其他各种行业中得到应用：
- en: AI medical assistants that recommend treatments, optimize hospital schedules,
    and predict disease risks
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI医疗助手推荐治疗方案、优化医院日程安排并预测疾病风险
- en: AI shopping assistants that recommend products based on user preferences and
    purchase history
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户偏好和购买历史推荐产品的AI购物助手
- en: Multi-agent AI systems for self-driving cars (navigation, pedestrian detection,
    or traffic optimization)
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于自动驾驶汽车的多智能体AI系统（导航、行人检测或交通优化）
- en: AI-driven advisors that help with investment strategies, risk management, and
    fraud detection
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由人工智能驱动的顾问，帮助制定投资策略、风险管理以及欺诈检测
- en: An AI-powered urban planner that optimizes traffic, energy use, and public transport
    routes
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由人工智能驱动的城市规划师，优化交通、能源使用和公共交通路线
- en: In this section, we looked at how to create a multi-agent system. In the next
    section, we will discuss how multi-agent systems fit into the various business
    models that exist today or are under greater development. This will provide an
    important perspective, as it will allow you to understand how to adapt your multi-agent
    platform to the needs of businesses.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了如何创建一个多智能体系统。在下一节中，我们将讨论多智能体系统如何适应今天存在或正在进一步发展的各种商业模式。这将提供一个重要的视角，因为它将使你能够了解如何调整你的多智能体平台以满足企业的需求。
- en: SaaS, MaaS, DaaS, and RaaS
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件即服务（SaaS）、移动即服务（MaaS）、数据即服务（DaaS）和机器人即服务（RaaS）
- en: In this section, we will explore various business models influenced by recent
    advancements in AI. While multi-agent LLMs represent cutting-edge technology,
    their value lies in being adaptable to meet business needs, enabling them to be
    effectively packaged, marketed, and delivered to businesses and consumers. Considering
    that these systems are extremely expensive to develop and maintain, it is important
    for the reader to understand what the revenue models are so that they can think
    about, design, and develop products that align with the company’s strategy. Understanding
    these models allows us to grasp that a multi-agent system is not a standalone
    item but should be considered a product and that this product can be marketed
    in various ways. In addition, LLMs are extremely expensive products, and each
    of these business models has advantages and disadvantages in terms of continuous
    updates, scalability, and flexibility in AI deployment. At the same time, these
    business models regulate access to technology whether you are interested in developing
    AI models or are a customer. These choices (about the platform, business models,
    and so on) must be made before the product is developed, and they determine its
    development, since the costs do not allow for trial and error. The choice of business
    model is defined by the structure of the product and the multi-agent system, as
    well as the economic viability of the company.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨受最近人工智能进步影响的多种商业模式。虽然多智能体LLM代表了尖端技术，但它们的价值在于能够适应满足商业需求，使它们能够有效地打包、营销并交付给企业和消费者。考虑到这些系统开发和维护成本极高，读者了解收入模式非常重要，这样他们就可以思考、设计和开发与公司战略一致的产品。理解这些模式使我们能够理解多智能体系统不是一个独立的项目，而应该被视为一个产品，并且这个产品可以通过多种方式来营销。此外，LLM是极其昂贵的产品，并且每个商业模式在持续更新、可扩展性和人工智能部署的灵活性方面都有其优势和劣势。同时，这些商业模式规范了技术访问，无论你是想开发人工智能模型还是作为客户。在产品开发之前必须做出这些选择（关于平台、商业模式等），因为成本不允许试错。商业模式的选择由产品的结构和多智能体系统，以及公司的经济可行性所定义。
- en: Software as a Service (SaaS)
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件即服务（SaaS）
- en: SaaS is a service model in which software is hosted in the cloud by a provider
    and is made available to users over the internet. In the traditional model, software
    is provided to the user to be installed and used locally (on the user’s device).
    SaaS, on the other hand, allows access over the internet, usually on the web browser
    or with a mobile app. Often, SaaS is provided via subscription rather than through
    a one-time purchase. The SaaS paradigm began in 1999 when Salesforce launched
    its **Customer Relationship Management** (**CRM**) as a cloud-hosted service.
    SaaS is now the most widely used sales paradigm by different companies, especially
    for **Business-to-Business** (**B2B**) applications. Its popularity is growing,
    and it is expected that SaaS software revenue will grow more and more in the coming
    years.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: SaaS是一种服务模式，其中软件由提供商托管在云端，并通过互联网向用户提供服务。在传统模式中，软件提供给用户安装和本地使用（在用户的设备上）。另一方面，SaaS允许通过互联网访问，通常是通过网页浏览器或移动应用。通常，SaaS是通过订阅而不是一次性购买来提供的。SaaS范式始于1999年，当时Salesforce推出了其**客户关系管理**（**CRM**）作为云托管服务。SaaS现在是不同公司最广泛使用的销售范式，特别是对于**企业对企业**（**B2B**）应用。其受欢迎程度正在增长，预计未来几年SaaS软件的收入将不断增长。
- en: SaaS applications are typically built to be hosted in the cloud (they are called
    cloud-native). The company developing these apps can decide whether to host on
    its own infrastructure or leverage that of cloud service providers (examples are
    Google Cloud, IBM Cloud, OVH, Aruba, **Amazon Web Services** (**AWS**), and Microsoft
    Azure). Given the demand for app providers, some providers create focused infrastructure
    for hosting these apps, and so we also talk about **Platform as a** **Service**
    (**PaaS**).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: SaaS应用通常是为了托管在云端而构建的（它们被称为云原生）。开发这些应用的公司可以决定是否在自己的基础设施上托管，或者利用云服务提供商的基础设施（例如Google
    Cloud、IBM Cloud、OVH、Aruba、**亚马逊网络服务**（**AWS**）和Microsoft Azure）。鉴于对应用提供商的需求，一些提供商为托管这些应用创建了专门的基础设施，因此我们也谈论**平台即服务**（**PaaS**）。
- en: In PaaS solutions, a provider conducts hosting of both hardware and software
    through dedicated infrastructure that is made available to product developers.
    This allows developers to focus on coding without having to worry about maintaining
    or managing the infrastructure behind it. The platform allows the hosting of both
    the application and the data, or even the training of a model, leaving only the
    coding to the developer. This has enabled accelerated product development by many
    businesses, who have managed to avoid investing in expensive infrastructure (although
    extensive use of these platforms can have a high cost, especially when the applications
    are generative AI). Although PaaS allows a simplification of the process, developers
    are forced to conform their applications to the requirements of the platforms
    and environment. This is not always possible, resulting in difficulties in deployment
    or other issues. Therefore, an alternative paradigm has emerged that allows the
    user greater flexibility, control, and adaptability, especially when the application
    or business requires it. This paradigm is called **Infrastructure as a Service**
    (**IaaS**) and emerged around 2010\. In IaaS, a user can access computing resources
    through web services, thus being able to rent infrastructure (servers, networking,
    and storage) as needed. The user retains more control over the infrastructure,
    while the provider focuses on the hardware (examples include Google Compute Engine,
    DigitalOcean, and Amazon Elastic Compute Cloud). PaaS and IaaS can thus be seen
    as extensions of SaaS or as services for businesses that need a supporting ecosystem.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在PaaS解决方案中，提供商通过专用基础设施托管硬件和软件，这些基础设施可供产品开发者使用。这允许开发者专注于编码，而无需担心维护或管理其背后的基础设施。该平台允许托管应用程序和数据，甚至训练模型，只留下编码工作给开发者。这已经使许多企业能够加速产品开发，他们设法避免了投资昂贵的基础设施（尽管广泛使用这些平台可能成本高昂，尤其是在应用是生成式AI时）。尽管PaaS简化了流程，但开发者被迫使他们的应用程序符合平台和环境的要求。这并不总是可能的，导致部署或其他问题。因此，出现了一种新的范式，使用户具有更大的灵活性、控制和适应性，尤其是在应用程序或业务需要时。这种范式被称为**基础设施即服务**（**IaaS**）并大约在2010年出现。在IaaS中，用户可以通过网络服务访问计算资源，从而能够根据需要租赁基础设施（服务器、网络和存储）。用户对基础设施的控制更多，而提供商则专注于硬件（例如Google
    Compute Engine、DigitalOcean和Amazon Elastic Compute Cloud）。因此，PaaS和IaaS可以被视为SaaS的扩展，或为需要支持生态系统的企业提供服务。
- en: '![Figure 9.40 – Comparison between different paradigms (https://arxiv.org/pdf/2311.05804)](img/B21257_09_40.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.40 – 不同范式的比较](https://arxiv.org/pdf/2311.05804)(img/B21257_09_40.jpg)'
- en: Figure 9.40 – Comparison between different paradigms ([https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.40 – 不同范式的比较([https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804))
- en: SaaS applications are therefore designed to be accessible via an internet connection
    from a device that must be connected to the internet in order to access the application
    (a device that is not connected cannot access the application and it is not a
    requirement to allow access locally). Software is developed to be used through
    a web browser or with a specific app (mobile software). Some SaaS applications
    (as in the case of Adobe Acrobat) may require the user to download and install
    a dedicated client (a light program, which is not the full application, that has
    to be installed on a local PC) on their computers (but this is generally a minority
    of cases). A SaaS application is generally a **multi-tenant software architecture**,
    where a single instance of a software application (along with its database and
    hardware) serves different user accounts (or multiple tenants). A tenant is what
    is called a user of the software, and it is a user or group of users within an
    organization.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SaaS 应用程序被设计为可以通过互联网连接从必须连接到互联网才能访问应用程序的设备（未连接的设备无法访问应用程序，并且允许本地访问不是必需的）进行访问。软件是开发用来通过网络浏览器或特定应用程序（移动软件）使用的。某些
    SaaS 应用程序（例如 Adobe Acrobat）可能要求用户在他们的计算机上下载并安装一个专用的客户端（一个轻量级程序，不是完整的应用程序，需要安装在本地
    PC 上），但这通常是少数情况。SaaS 应用程序通常是一个 **多租户软件架构**，其中软件应用程序的单个实例（包括其数据库和硬件）为不同的用户账户（或多个租户）提供服务。租户就是所谓的软件用户，它是一个组织内的单个用户或用户组。
- en: In SaaS, it is crucial to have an architecture that ensures each tenant’s data
    is isolated and inaccessible to other tenants. This approach offers the advantage
    of cost reduction by enabling the software to be optimized for a single piece
    of hardware and infrastructure, which is then shared among all users. It also
    allows for greater scalability, easier customization, and maintenance (providers
    can conduct the update on their own infrastructure and on a single architecture).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SaaS 中，拥有一个确保每个租户的数据被隔离且对其他租户不可访问的架构至关重要。这种方法通过使软件能够针对单一硬件和基础设施进行优化，然后由所有用户共享，从而降低了成本。它还允许实现更大的可扩展性、更易于定制的维护（提供商可以在自己的基础设施和单一架构上自行进行更新）。
- en: 'SaaS is therefore one of the most widely used paradigms because it has a number
    of advantages:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: SaaS 因此成为最广泛使用的范式之一，因为它具有许多优势：
- en: '**Cost efficiency**: There are no upfront costs to the customer, such as expenses
    for hardware or a software license. In SaaS, the customer either pays by subscription
    or on a pay-as-you-go basis.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：客户无需承担任何前期成本，例如硬件或软件许可的费用。在 SaaS 中，客户可以选择按订阅或按使用付费。'
- en: '**Scalability**: SaaS scales easily for the customer and does not require additional
    hardware. Similarly, software is structured to make it easy to scale up customers.
    In the case of AI models, the customer does not need large hardware but can directly
    leverage that provided by the provider.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：SaaS 对客户来说易于扩展，且不需要额外的硬件。同样，软件的结构使得扩展客户变得容易。在人工智能模型的情况下，客户不需要大型硬件，可以直接利用提供商提供的硬件。'
- en: '**Accessibility**: The customer can access the application from anywhere in
    the world via an internet connection. Also, using the web browser, the software
    is optimized for whatever hardware the client has. SaaS also reduces the barrier
    of access to AI for clients (fewer resources and less need for expertise) through
    the use of templates, APIs, and frameworks.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：客户可以通过互联网连接从世界任何地方访问应用程序。此外，使用网络浏览器，软件针对客户拥有的任何硬件进行了优化。SaaS 还通过使用模板、API
    和框架，降低了客户访问人工智能的门槛（资源较少且对专业知识的需求较低）。'
- en: '**Ease of integration and customization**: It is much easier for the developer
    to provide updates, security patches, and maintenance, in terms of both resources
    and time. The ability to manage customization for the client is usually provided
    in an easier way, while at the same time maintaining control. Equally, for an
    AI system, updated templates can be provided.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成和定制的便捷性**：从资源和时间方面来看，开发者提供更新、安全补丁和维护要容易得多。通常以更简单的方式为客户提供定制管理的能力，同时保持控制。同样，对于AI系统，可以提供更新的模板。'
- en: '**Fast deployment**: SaaS reduces deployment and market access time by being
    immediately available in the marketplace.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速部署**：SaaS通过在市场上立即可用来减少部署和市场接入时间。'
- en: '**Data and model sharing**: Model and data access can be easily allowed to
    users from different teams or in various locations simultaneously and effectively
    and efficiently.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据和模型共享**：模型和数据的访问可以轻松地同时允许来自不同团队或不同地点的用户，并且高效且有效地进行。'
- en: 'There are, of course, also some limitations and disadvantages to SaaS:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，SaaS也有一些限制和缺点：
- en: '**Dependency on internet connectivity**: SaaS requires a stable connection,
    and connection disruptions can stop critical processes and errors. Rural areas
    and countries with little infrastructure may not be covered.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对互联网连接的依赖**：SaaS需要稳定的连接，连接中断可能会停止关键流程并引发错误。农村地区和基础设施薄弱的国家可能无法覆盖。'
- en: '**Limited customization**: SaaS solutions are developed with the idea of covering
    as much business as possible with one product. Typically, they provide a limited
    number of customization possibilities that may not cover all the needs of a particular
    business. This is also true in the case of an AI system; the client has little
    control over the models and the models may not be able to meet client requirements.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制有限**：SaaS解决方案是在一个产品中尽可能覆盖更多业务的想法下开发的。通常，它们提供有限的定制选项，可能无法满足特定企业的所有需求。在AI系统的案例中也是如此；客户对模型的控制很少，模型可能无法满足客户的要求。'
- en: '**Data security and privacy concerns**: Hosting on third-party servers brings
    the risk of data breaches or unauthorized access. In addition, there may be compliance
    issues with regulations in countries such as the European Union (e.g., data must
    be maintained on servers in certain countries). Training or using AI models may
    require having to share sensitive data, and this may be against GDPR or other
    regulations (as well as an additional privacy risk).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据安全和隐私问题**：托管在第三方服务器上带来了数据泄露或未经授权访问的风险。此外，可能与欧盟等国家的法规不符（例如，数据必须存储在特定国家的服务器上）。训练或使用AI模型可能需要共享敏感数据，这可能违反GDPR或其他法规（以及额外的隐私风险）。'
- en: '**Vendor lock-in**: Businesses may remain anchored to a particular SaaS provider
    and then be unable to migrate to other platforms due to cost and complexity. In
    addition, different providers may terminate the service (or be acquired), increase
    costs abruptly, or eliminate features considered essential. SaaS can become expensive
    over a period of time, especially when subscription-based (some providers charge
    more as users increase).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**供应商锁定**：企业可能依赖于特定的SaaS提供商，然后由于成本和复杂性而无法迁移到其他平台。此外，不同的提供商可能会终止服务（或被收购），突然增加成本，或取消被认为至关重要的功能。随着时间的推移，SaaS可能会变得昂贵，尤其是当基于订阅时（一些提供商随着用户数量的增加而收费更多）。'
- en: '**Performance issues**: Shared resources in multi-tenant architectures can
    lead to slower performance during peak usage. In addition, there may be unexpected
    server downtime or maintenance schedules that hurt the business (for example,
    if maintenance is conducted at night on Pacific Time, it disrupts business hours
    in Europe) and over which the customer has no control. AI systems that must run
    in real time may have latency or performance problems (both in training and inference).
    In addition, the provider may not provide cutting-edge AI or may not have implemented
    it yet (or they may use models that do not fit the customer’s needs).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能问题**：在多租户架构中共享资源可能导致高峰时段性能变慢。此外，可能会有意外的服务器停机或维护计划，这会损害业务（例如，如果维护在太平洋时间晚上进行，将干扰欧洲的业务时间），而客户对此无能为力。必须实时运行的AI系统可能存在延迟或性能问题（无论是在训练还是在推理）。此外，提供商可能不提供最先进的AI，或者尚未实施（或者他们可能使用不适合客户需求的模型）。'
- en: '**High computational costs**: SaaS has an infrastructure cost for the developer,
    and in the case of AI, this cost can be higher (use of GPUs or large storage costs).
    Some of these services are particularly expensive for users.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高计算成本**：SaaS 对开发者来说有基础设施成本，在 AI 的情况下，这种成本可能更高（使用 GPU 或大存储成本）。其中一些服务对用户来说特别昂贵。'
- en: Model as a Service (MaaS)
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型即服务 (MaaS)
- en: MaaS is a new paradigm that was born with the development of big data, AI, and
    Web 3.0\. MaaS is a cloud computing-based service paradigm that offers AI and
    ML models and related IaaS to developers and enterprises.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: MaaS 是随着大数据、AI 和 Web 3.0 的发展而产生的一种新范式。MaaS 是一种基于云计算的服务范式，为开发者和企业提供 AI 和 ML 模型以及相关的
    IaaS。
- en: MaaS seeks to simplify access to AI for businesses that have neither the expertise
    nor the infrastructure to train generative AI or broad models in general. MaaS
    enables the use of pre-trained ML models and algorithms through the use of simple
    interfaces, APIs, or the browser. Just like with SaaS, access to models is through
    the internet (and requires the business to have an internet connection). The provider
    must then conduct the hosting of the models and allow developers access to the
    models that have been trained. Developers can then use these models to add AI
    functions to their systems and apps. MaaS is often a platform where models that
    have been trained on a large amount of data or optimized for a possible task are
    hosted. MaaS reduces the complexity of managing these models (especially training
    and deployment) and allows developers to focus on using the models or how to integrate
    them for specific applications. Developers save time and resources since they
    do not have to train these models from scratch. MaaS thus has certain similarities
    to PaaS and IaaS but conducts an additional level of abstraction and focuses on
    AI solutions. In a sense, MaaS can be viewed as an intermediate solution between
    SaaS and PaaS or IaaS. It not only provides a service but also offers an infrastructure
    that enables the development of custom products.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: MaaS 旨在简化那些既没有专业知识也没有基础设施来训练生成 AI 或一般模型的企业的 AI 访问。MaaS 通过简单的界面、API 或浏览器使用预训练的
    ML 模型和算法。就像 SaaS 一样，模型的访问是通过互联网进行的（并且要求企业有互联网连接）。然后，提供商必须托管模型并允许开发者访问已训练的模型。开发者可以使用这些模型将
    AI 功能添加到他们的系统和应用中。MaaS 通常是一个平台，用于托管在大量数据上训练或针对可能任务优化的模型。MaaS 减少了管理这些模型的复杂性（尤其是培训和部署），并允许开发者专注于使用模型或如何将它们集成到特定应用中。由于开发者不必从头开始训练这些模型，因此他们节省了时间和资源。因此，MaaS
    在某些方面与 PaaS 和 IaaS 相似，但进行了一个额外的抽象级别，并专注于 AI 解决方案。从某种意义上说，MaaS 可以被视为 SaaS 和 PaaS
    或 IaaS 之间的中间解决方案。它不仅提供了一种服务，还提供了一种基础设施，使定制产品的开发成为可能。
- en: Another difference between SaaS and MaaS is in the underlying architecture of
    the two paradigms. SaaS focuses on applications (application layer) that depend
    on an operating system (whether mobile or desktop application) that allows them
    to run, as well as on a layer that allows the app to be hosted. In the case of
    MaaS, the architecture focuses on the model that needs a specific framework to
    be hosted.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: SaaS 和 MaaS 之间的另一个区别在于两种范式的底层架构。SaaS 侧重于应用程序（应用层），这些应用程序依赖于一个操作系统（无论是移动还是桌面应用程序）来运行，以及一个允许应用程序托管的层。在
    MaaS 的情况下，架构侧重于需要特定框架来托管的模型。
- en: '![Figure 9.41 – Comparison between traditional and model-based technology stacks
    (https://arxiv.org/pdf/2311.05804)](img/B21257_09_41.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.41 – 传统技术与基于模型的技术堆栈的比较](https://arxiv.org/pdf/2311.05804)(img/B21257_09_41.jpg)'
- en: Figure 9.41 – Comparison between traditional and model-based technology stacks
    ([https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804))
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.41 – 传统技术与基于模型的技术堆栈的比较([https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804))
- en: 'In MaaS, the following elements are often present:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MaaS 中，以下元素通常存在：
- en: '**Cloud computing**: MaaS is based on an infrastructure on the cloud where
    various models are maintained and deployed. This allows easy access to the models
    and enables greater scalability.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云计算**：MaaS 基于云上的基础设施，其中维护和部署了各种模型。这使得对模型的访问变得容易，并实现了更大的可扩展性。'
- en: '**Model training and optimization**: MaaS providers take care of the training
    of large models on large datasets. MaaS providers also take care of the entire
    ecosystem to enable more effective exploitation of models. For example, they can
    provide models of different sizes, including quantized or fine-tuned versions
    for specific applications.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练和优化**：MaaS（模型即服务）提供商负责在大数据集上训练大型模型。他们还负责整个生态系统，以实现模型更有效的利用。例如，他们可以提供不同大小的模型，包括针对特定应用的量化或微调版本。'
- en: '**API and development tools**: MaaS providers also provide APIs and tools that
    allow the developer to use the models for their applications easily. The purpose
    is to allow easy integration of models into other applications and infrastructures.
    So, the API acts as an endpoint, takes data, and returns predictions.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 和开发工具**：MaaS 提供商还提供 API 和工具，允许开发者轻松地将模型用于其应用程序。目的是允许轻松地将模型集成到其他应用程序和基础设施中。因此，API
    作为端点，接收数据，并返回预测。'
- en: '**Monitoring and analytics**: To date, there is increasing focus on how to
    monitor models once they are in production. MaaS providers typically provide a
    number of tools to monitor model performance, identify the presence of issues,
    integrate feedback, or improve resource allocation.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和分析**：迄今为止，越来越多的关注点是如何在生产环境中监控模型。MaaS 提供商通常提供一系列工具来监控模型性能、识别问题存在、集成反馈或改进资源分配。'
- en: '**Scalability, security, and privacy**: MaaS providers focus on the scalability
    of their systems by allowing customers to be able to manage multiple users at
    the same time (thus allocating different bandwidth, computing power, or storage
    as needed). At the same time, today there is more attention to privacy and security
    (especially as there is much more regulation). Platforms often have a number of
    tools to be able to increase the privacy and security of applications that integrate
    their models.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性、安全性和隐私性**：MaaS提供商通过允许客户同时管理多个用户（从而根据需要分配不同的带宽、计算能力或存储）来关注其系统的可扩展性。同时，今天对隐私和安全性的关注越来越多（尤其是在有更多监管的情况下）。平台通常有一系列工具，可以增加集成其模型的应用的隐私性和安全性。'
- en: Hugging Face is an example of a MaaS provider. Hugging Face provides access
    to thousands of pre-trained models (from the company itself, other companies,
    or users) for computer vision, NLP, audio, video, and more. These models are hosted
    on their Model Hub and can be either used via an API or installed locally. So,
    a user who doesn’t want to download models can use an inference API without owning
    the infrastructure needed to manage the model (this API uses the pay-as-you-go
    system). Developers who do not have the expertise or resources can directly use
    the endpoint API to directly integrate AI models within their applications. In
    addition, Hugging Face also offers a platform for hosting and deploying both the
    model and application, extending MaaS capabilities and providing flexibility to
    customers who want to use their custom models. Hugging Face also provides tools
    to improve the scalability of models and open source libraries to facilitate model
    development or integration (e.g., Transformers, Datasets, Diffusers, sentence
    embedding, and so on), as well as offering a forum to enable user exchange, educational
    resources for users, and other services. There are other MaaS providers, such
    as Google AI (pre-trained models for NLP (Natural Language API), vision (Vision
    API), speech to text, translation, or custom model training with Vertex AI) and
    AWS (which offers pre-trained models for language, image, and text (e.g., AWS
    Comprehend, Rekognition, and Translate) or infrastructure for custom models).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 是一个 MaaS 提供商的例子。Hugging Face 为计算机视觉、NLP、音频、视频等领域提供数千个预训练模型（来自公司本身、其他公司或用户）。这些模型托管在其模型中心，可以通过
    API 使用或本地安装。因此，不想下载模型的用户可以使用推理 API，而不需要拥有管理模型所需的基础设施（此 API 使用按使用付费系统）。没有专业知识或资源的开发者可以直接使用端点
    API，将其应用程序中的 AI 模型直接集成。此外，Hugging Face 还提供了一个平台，用于托管和部署模型以及应用程序，扩展 MaaS 功能，并为希望使用自定义模型的客户提供灵活性。Hugging
    Face 还提供工具以提高模型的可扩展性，开源库以促进模型开发或集成（例如，Transformers、Datasets、Diffusers、句子嵌入等），以及提供论坛以促进用户交流、用户教育资源和其他服务。还有其他
    MaaS 提供商，例如 Google AI（提供 NLP（自然语言 API）、视觉（视觉 API）、语音转文本、翻译或使用 Vertex AI 的自定义模型训练）和
    AWS（提供语言、图像和文本（例如，AWS Comprehend、Rekognition 和 Translate）的预训练模型或自定义模型的基础设施）。
- en: 'MaaS has the following advantages, especially regarding the AI domain:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: MaaS在AI领域具有以下优势：
- en: '**Simplified model development and deployment**: MaaS lowers the technical
    barrier to using generative AI. Companies do not need developers who are experts
    in the technology or different algorithms because most models are delivered via
    endpoints. This allows companies to focus on applications and model integration
    for their products. If needed, MaaS also simplifies the approach to fine-tuning
    models for their applications. MaaS, as opposed to SaaS, is tailored to the entire
    AI workflow and offers tools for deploying, training, managing, and scaling models,
    thus enabling better support for companies interested in using AI.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化的模型开发和部署**：MaaS降低了使用生成式人工智能的技术门槛。公司不需要对技术或不同算法有专长的开发者，因为大多数模型都是通过端点交付的。这使得公司能够专注于其产品的应用和模型集成。如果需要，MaaS还简化了为应用微调模型的方法。与SaaS不同，MaaS针对整个AI工作流程进行了定制，并提供用于部署、训练、管理和扩展模型的工具，从而为有兴趣使用人工智能的公司提供更好的支持。'
- en: '**High performance and scalability**: The use of cloud computing facilitates
    system scaling. In fact, the use of AI can require high costs and large resources
    (especially when it comes to using LLMs), and MaaS allows for better resource
    management by facilitating access to large models without initial entry costs
    for different businesses. Typically, users pay for their consumption and receive
    computing according to their needs, thus enabling better performance and scalability.
    Since MaaS is optimized for AI workloads, it can scale easily when there are fluctuating
    computational demands (SaaS typically focuses on allocating a variable number
    of users, but users may have a different need for computing depending on the different
    usage of models).'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能和可扩展性**：云计算的使用促进了系统扩展。实际上，使用人工智能可能需要高昂的成本和大量的资源（尤其是在使用大型语言模型时），而MaaS通过便于访问大型模型而无需不同企业承担初始进入成本，从而实现了更好的资源管理。通常，用户根据其需求支付消费费用并接收计算服务，从而实现更好的性能和可扩展性。由于MaaS针对人工智能工作负载进行了优化，因此当计算需求波动时，它可以轻松扩展（SaaS通常侧重于分配可变数量的用户，但用户对计算的需求可能因模型的不同使用而不同）。'
- en: '**Shared knowledge and collaboration**: MaaS is built on collecting large datasets
    and training large models. These pre-trained models can then be fine-tuned by
    developers interested in adapting the models to particular applications. This
    means that developers need to collect much less data and do not have to train
    large models from scratch. This saves both resources and costs (fine-tuning is
    much less computationally expensive than pre-training). In addition, MaaS allows
    standardization that reduces the technical knowledge required to be able to use
    these models and allows information and tutorials to be obtained easily. Models
    can then also be shared by the community on platforms on which both information
    and experiences are also exchanged (this promotes a collaborative environment
    and accelerates the development of new models).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享知识和协作**：MaaS建立在收集大量数据集和训练大型模型的基础上。这些预训练模型随后可以被对将模型适应特定应用感兴趣的开发商微调。这意味着开发商需要收集的数据量更少，也不必从头开始训练大型模型。这节省了资源和成本（微调的计算成本远低于预训练）。此外，MaaS允许标准化，从而降低了使用这些模型所需的技术知识，并允许轻松获取信息和教程。模型还可以在平台上由社区共享，这些平台上也交换信息和经验（这促进了协作环境并加速了新模型的发展）。'
- en: '**Business support**: MaaS uses a flexible payment model, such as subscription
    based, where you pay only for current consumption. Generally, this solution is
    cost effective and affordable for many small businesses. It is convenient for
    providers because once they choose a technology and integrate it into their products,
    users remain loyal. Model integration allows businesses to gain insights in an
    easy and inexpensive way (models for forecasts or other predictions, report writing,
    and visualizations).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**商业支持**：MaaS采用灵活的支付模式，例如基于订阅的支付方式，您只需为当前消费付费。通常，这种解决方案对许多小型企业来说既经济又实惠。对提供商来说很方便，因为一旦他们选择了一种技术并将其集成到他们的产品中，用户就会保持忠诚。模型集成使企业能够以简单且经济的方式获得洞察力（用于预测或其他预测的模型、报告编写和可视化）。'
- en: '**Flexibility**: MaaS provides models for a large number of applications and
    allows businesses to integrate a large number of potential models, providing wide
    flexibility (e.g., NLP, computer vision, time series, and so many other applications).
    In addition, developers can test many pre-trained models quickly without changing
    setups (e.g., Hugging Face offers thousands of models that can be used with just
    a few pipelines). Similarly, MaaS providers often offer many tools to simplify
    the AI life cycle (data labeling, data format integration, monitoring tools, and
    so on) from training to deployment.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：MaaS为大量应用程序提供模型，并允许企业集成大量潜在模型，提供广泛的灵活性（例如，自然语言处理、计算机视觉、时间序列等众多其他应用）。此外，开发者可以快速测试许多预训练模型，而无需更改设置（例如，Hugging
    Face提供了数千个只需少量管道即可使用的模型）。同样，MaaS提供商通常提供许多工具来简化AI生命周期（数据标注、数据格式集成、监控工具等），从训练到部署。'
- en: 'MaaS is a new paradigm, and the field of generative AI is also in active development,
    so there are challenges and possible drawbacks that need to be addressed:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: MaaS是一种新的范式，生成式AI领域也在积极发展中，因此存在需要解决的一些挑战和可能的缺点：
- en: '**Security and privacy**: Often, a large amount of data is transferred, especially
    for model training, which can be intercepted. In addition, models trained on sensitive
    data can end up outputting sensitive data. These models could also be trained
    on copyrighted data, and the legislation on training with such data is not entirely
    clear. So, organizations that adhere to particularly regulated industries may
    not adopt MaaS. Data is the basis of these models, but the models could be trained
    on, or become biased due to, low-quality data. Often, there is no information
    on what data these models were trained on. In these cases, both the platform and
    the businesses using these models may be subject to fines or other regulations.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和隐私**：通常，在模型训练过程中会传输大量数据，这可能会被拦截。此外，基于敏感数据训练的模型最终可能会输出敏感数据。这些模型也可能是在受版权保护的数据上训练的，而使用此类数据进行训练的立法并不完全明确。因此，坚持特别受监管行业的组织可能不会采用MaaS。数据是这些模型的基础，但模型可能是基于低质量数据进行训练的，或者可能因为低质量数据而出现偏差。通常，没有关于这些模型训练数据的信息。在这些情况下，平台和这些模型的使用企业都可能面临罚款或其他监管。'
- en: '**Vendor lock-in**: MaaS providers use proprietary tools and APIs, which does
    not make it easy to change from one provider to another (e.g., changing providers
    complicates model integration or exporting models that have been fine-tuned).
    This difficulty can reduce flexibility and innovation and can make a business
    dependent on a single provider. There may be downtime or service disruption that
    impacts built applications. It also makes it more difficult to experiment locally.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**供应商锁定**：MaaS提供商使用专有工具和API，这使得从一个提供商切换到另一个提供商变得困难（例如，更换提供商会复杂化模型集成或导出经过微调的模型）。这种困难可能会降低灵活性和创新，并使企业依赖于单一提供商。可能会有停机或服务中断，这会影响构建的应用程序。这也使得本地实验更加困难。'
- en: '**Limited customization**: Not all MaaS providers allow the fine-tuning or
    modification of pre-trained models. Pre-trained models may not be suitable for
    some particular operations, or a business may need to have control over hyperparameters
    and infrastructure. In addition, MaaS providers may make changes or plan updates
    that impact the business or no longer allow some core features of their applications.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的定制性**：并非所有MaaS（移动即服务）提供商都允许对预训练模型进行微调或修改。预训练模型可能不适合某些特定操作，或者企业可能需要控制超参数和基础设施。此外，MaaS提供商可能会进行更改或计划更新，这可能会影响业务或不再允许其应用程序的一些核心功能。'
- en: '**Interpretability of model and results**: A model is often a black box, and
    a user cannot access the decision-making process. Especially for GenAI models,
    it is difficult to understand how the model processes the input and gets the output.
    For sensitive applications, this could cause problems, especially when the model
    produces hallucinations or incorrect outputs. In addition, the lack of transparency
    of the platforms may affect the ability to diagnose errors or know how to correct
    them.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型和结果的解释性**：模型通常是一个黑盒，用户无法访问决策过程。特别是对于生成式AI模型，很难理解模型如何处理输入并得到输出。对于敏感应用，这可能会引起问题，特别是当模型产生幻觉或错误的输出时。此外，平台的缺乏透明度可能会影响诊断错误或了解如何纠正它们的能力。'
- en: '**Performance and cost**: Latency refers to the time elapsed between a request
    and its corresponding response. The latency of models depends on the underlying
    infrastructure, which can experience strain during periods of peak usage. Shared
    multi-tenant environments in MaaS platforms can lead to resource bottlenecks during
    peak usage times. Businesses may encounter a considerable increase in latency
    that makes their applications unusable. MaaS allows pay as you go, but large-scale
    training or inference can quickly become expensive.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能和成本**：延迟指的是请求与相应响应之间经过的时间。模型的延迟取决于底层基础设施，在高峰使用期间可能会承受压力。在MaaS平台中的共享多租户环境在高峰使用时段可能会导致资源瓶颈。企业可能会遇到延迟显著增加的情况，使得他们的应用程序无法使用。MaaS允许按需付费，但大规模训练或推理可能会迅速变得昂贵。'
- en: MaaS remains an expanding paradigm for several businesses. For example, MaaS
    could have a big impact in healthcare where there are large amounts of data, and
    many models have already been developed. The models could be available on a platform
    and be used when needed by practitioners or pharmaceutical companies. Obviously,
    in healthcare, data security and output consistency are critical (especially if
    these applications are used for hospitals or other health providers). MaaS is
    also growing in other domains, such as finance, blockchain, and Web 3.0.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: MaaS对于许多企业来说仍然是一个不断发展的范式。例如，在医疗保健领域，由于存在大量数据，并且已经开发了许多模型，MaaS可能会产生重大影响。这些模型可以在平台上提供，并在需要时由从业者或制药公司使用。显然，在医疗保健领域，数据安全和输出一致性至关重要（尤其是如果这些应用程序用于医院或其他医疗机构）。MaaS在其他领域也在增长，例如金融、区块链和Web
    3.0。
- en: '![Figure 9.42 – The applications of various industries within MaaS (https://arxiv.org/pdf/2311.05804)](img/B21257_09_42.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![图9.42 – MaaS中各种行业的应用（https://arxiv.org/pdf/2311.05804）](img/B21257_09_42.jpg)'
- en: Figure 9.42 – The applications of various industries within MaaS ([https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804))
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.42 – MaaS中各种行业的应用（[https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804)）
- en: Data as a Service (DaaS)
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据即服务（DaaS）
- en: DaaS is a business model where data is delivered on demand to users regardless
    of their geographical location or organizational boundaries. In DaaS, data is
    stored in the cloud and a client can access it (with or without additional tools)
    by paying a subscription to a provider. DaaS, therefore, is built around the concept
    that data is an asset and can be provided to users on demand. This access can
    then be conducted through a platform, the use of APIs, or additional means. In
    addition, the provider can provide either raw data or data that has been normalized
    to be machine-readable or machine-ready.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: DaaS是一种商业模式，无论用户地理位置或组织边界如何，都可以按需向用户提供数据。在DaaS中，数据存储在云端，客户可以通过向提供商支付订阅费来访问它（无论是否需要额外工具）。因此，DaaS围绕数据是资产并且可以按需提供给用户的概念构建。这种访问可以通过平台、API的使用或其他方式来实现。此外，提供商可以提供原始数据或经过标准化以供机器读取或准备好的数据。
- en: 'AI models are notoriously data hungry, and retrieving quality data may not
    be easy. So, there are players who focus on collecting hard-to-access data and
    then selling it to other players. For example, patient data can be difficult to
    collect, and a company may collect and process the data and then sell it to pharmaceutical
    companies. Alternatively, DaaS allows companies to create a new business model,
    using data collected during their normal operations as an asset they can sell.
    For example, a telecom company that has collected data from its users can sell
    the anonymized data to retailers. This data is sold through a secure portal and
    can be charged for on a per-access basis or through a subscription. Subscription
    is usually the most popular method and can be divided into three subcategories:
    time model, quantity-based pricing model, and pay-per-call or data type base model.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型以数据需求量大而闻名，获取高质量数据可能并不容易。因此，有一些参与者专注于收集难以获取的数据，然后将它们卖给其他参与者。例如，患者数据可能难以收集，一家公司可能会收集和处理数据，然后将数据卖给制药公司。或者，DaaS允许公司创建新的商业模式，利用他们在正常运营期间收集的数据作为可以出售的资产。例如，一家收集了其用户数据的电信公司可以将匿名数据卖给零售商。这些数据通过安全门户出售，可以按访问次数收费或通过订阅收费。订阅通常是最受欢迎的方法，可以分为三种子类别：时间模型、基于数量的定价模型和按呼叫或数据类型付费的模型。
- en: A DaaS provider may just sell the raw data it has collected, but more often
    it also processes it and makes it analyzable by models. Some DaaS providers aggregate
    different sources, process them, and thus simplify the analysis process for a
    client. In fact, the purpose of this data is to improve business processes and
    decision-making for customers, or to allow customers to train their AI models.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: DaaS 提供商可能只是销售其收集的原始数据，但更常见的是，它还会对其进行处理，使其可以通过模型进行分析。一些 DaaS 提供商汇总不同的来源，进行处理，从而简化了客户的分析过程。实际上，这些数据的目的在于改善客户的业务流程和决策，或者允许客户训练他们的
    AI 模型。
- en: There can also be bidirectionality, in which the provider collects the data
    and harmonizes it to integrate it with its own, before making it accessible to
    the client again. In this way, by relating it to other data, the client can extract
    additional value from its own data.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 还可能存在双向性，其中提供商收集数据并将其协调以与自己的数据集成，然后再将其提供给客户端。通过将其与其他数据相关联，客户端可以从其自身数据中提取额外的价值。
- en: 'DaaS has some advantages:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: DaaS 有一些优点：
- en: '**Cost efficiency**: DaaS reduces customers’ need to build and maintain data
    infrastructure and teams. It also reduces the cost of data access because of its
    flexibility. Customers do not need to store data; they can directly access the
    data stream when they need it.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：DaaS 减少了客户构建和维护数据基础设施和团队的需求。由于其灵活性，它还降低了数据访问的成本。客户不需要存储数据；他们可以在需要时直接访问数据流。'
- en: '**Ease of access**: Providing data on demand allows real-time access and saves
    time and expertise to obtain data information. Users do not need to know the data
    and the structure behind it, but they can easily learn how to use it. Also, as
    long as there is an internet connection, the client can always access the data.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于访问**：按需提供数据允许实时访问，节省了获取数据信息的时间和专业知识。用户不需要了解数据及其背后的结构，但可以轻松学习如何使用它。此外，只要有互联网连接，客户端就可以始终访问数据。'
- en: '**Scalability**: It easily scales to accommodate increasing data needs without
    requiring additional infrastructure investment. Customers can easily choose the
    data workload they need or can handle.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：它易于扩展以适应不断增长的数据需求，而无需额外的基础设施投资。客户可以轻松选择他们需要或能够处理的数据工作量。'
- en: '**Centralized data management**: DaaS enables consistent and centralized data
    storage, reducing both inconsistencies and redundancies in data. This enables
    simplified data governance and compliance with regulations.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中式数据管理**：DaaS 实现了一致和集中的数据存储，减少了数据的不一致性和冗余。这简化了数据治理并符合法规要求。'
- en: '**Focus on core activities**: DaaS saves resources and time, allowing businesses
    to focus on extracting value from data rather than managing it. In addition, it
    enables better collaboration among the various team members and collaborators,
    which can then access the same data (in the same format).'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专注于核心活动**：DaaS 节省资源和时间，使企业能够专注于从数据中提取价值，而不是管理数据。此外，它还促进了不同团队成员和协作者的更好合作，他们可以访问相同的数据（以相同的格式）。'
- en: '**Integration with other services**: DaaS makes it easy to integrate data with
    other services in the business, especially when it comes to analytics platforms,
    visualization tools, and other cloud services. Likewise, it facilitates the regular
    updating of datasets and allows users to have access to the most accurate and
    current data.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他服务的集成**：DaaS 使数据与业务中的其他服务集成变得容易，尤其是在分析平台、可视化工具和其他云服务方面。同样，它还促进了数据集的定期更新，并使用户能够访问最准确和最新的数据。'
- en: '**Data quality**: As data is centralized, data quality tends to improve. Once
    this data is tested, if there are no updates, there is no need for further testing.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**：由于数据集中，数据质量往往得到改善。一旦这些数据经过测试，如果没有更新，就无需进一步测试。'
- en: 'The disadvantages of DaaS are similar to the other models associated with cloud
    computing:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: DaaS 的缺点与其他与云计算相关的模型类似：
- en: '**Data security and privacy risks**: Obviously, the location of data on the
    cloud can mean that sensitive and proprietary data can be accessed by third parties
    or be at risk of breach. Providers must comply with regulations, which are increasingly
    stringent today. The costs of securing infrastructure are growing, and data piracy
    attacks are on the rise. In addition, although data is sold anonymized, in some
    cases, it is possible to reconstruct the information.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据安全和隐私风险**：显然，数据在云上的位置可能意味着敏感和专有数据可以被第三方访问或面临泄露风险。提供商必须遵守日益严格的法规。保护基础设施的成本正在增加，数据盗版攻击也在上升。此外，尽管数据以匿名方式出售，但在某些情况下，有可能重建信息。'
- en: '**Dependency on providers**: DaaS creates a reliance on external providers
    for critical data. Service outages or disruptions on the provider’s end impact
    the client and all services that are related to accessing this data. The client
    normally has access to the data stream but does not download the data, so it can
    be cut off from data that is necessary to its business.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对提供商的依赖性**：DaaS导致对外部提供商的临界数据产生依赖。提供商端的服务中断或故障会影响客户以及所有与访问此数据相关的服务。客户通常可以访问数据流，但不会下载数据，因此可能会被切断对其业务必要的数据。'
- en: '**Limited customization**: DaaS may not provide data in the format needed or
    have the right granularity. Providers have an interest in providing data in a
    format that is useful to as many clients as possible, but specific clients may
    have different requirements. An inadequate format makes it more complicated to
    integrate into existing systems or their own workflows, requiring costs to be
    incurred in order to adapt either the systems or the data.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的定制性**：DaaS可能无法提供所需格式的数据或具有正确的粒度。提供商有提供对尽可能多的客户有用的数据的兴趣，但特定客户可能有不同的需求。不合适的格式使得将其集成到现有系统或自己的工作流程中变得更加复杂，需要承担适应系统或数据的成本。'
- en: '**Quality assurance**: In DaaS, quality in terms of accuracy of data is key,
    and poor-quality data can lead to flawed decision-making or errors in related
    services. The quality, accuracy, and reliability of the data depend on the provider.
    Therefore, the provider must ensure that the data is relevant, updated, and of
    good quality.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量保证**：在DaaS中，数据的准确性方面的质量至关重要，低质量的数据可能导致决策失误或相关服务中的错误。数据的质量、准确性和可靠性取决于提供商。因此，提供商必须确保数据的相关性、更新性和高质量。'
- en: '**Latency and performance issues**: Accessing data over the internet can lead
    to introducing latency (especially when the connection is not good or the datasets
    are very large). In addition, this latency can reduce performance if the data
    stream is embedded in additional services.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟和性能问题**：通过互联网访问数据可能导致引入延迟（尤其是在连接不好或数据集非常大时）。此外，这种延迟如果数据流嵌入到其他服务中，可能会降低性能。'
- en: Results as a Service (RaaS)
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果即服务（RaaS）
- en: RaaS, or OaaS, is a new paradigm that has developed in recent years. RaaS is
    a business model where a service provider delivers specific results or outcomes
    instead of providing tools, platforms, or raw data. This model has attracted attention
    in fields such as data analytics, AI, and automation. In RaaS, AI (including LLMs
    and agents) is used by the provider to provide personalized insights for customers.
    While the provider conducts the entire analysis, the client can focus on business
    insights without the need for specialized technology staff. In general, instead
    of paying a lump sum for a service, the client pays through a subscription to
    receive analytics at constant intervals.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: RaaS，或OaaS，是近年来发展起来的一种新范式。RaaS是一种商业模式，服务提供商提供特定的结果或成果，而不是提供工具、平台或原始数据。这种模式在数据分析、人工智能和自动化等领域引起了人们的关注。在RaaS中，提供商使用人工智能（包括LLMs和代理）为顾客提供个性化的见解。虽然提供商进行整个分析，但客户可以专注于业务洞察，无需专门的技术人员。一般来说，客户不是一次性支付服务费用，而是通过订阅定期接收分析。
- en: Since customers increasingly demand value from models (businesses are more interested
    in the value obtained from models than from an additional tool), RaaS focuses
    on providing an outcome rather than a model (or data). In addition, customers
    are looking for ways to reduce the costs of adopting a technology but preserving
    its value, and RaaS thus seeks to reduce the initial cost to a business. The provider
    focuses on identifying the technology or what tool is needed to achieve the outcome,
    while the customer explains what their needs and requirements are.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 由于客户越来越要求模型的价值（企业对模型获得的价值比对额外工具的兴趣更大），RaaS专注于提供结果，而不是模型（或数据）。此外，客户正在寻找降低采用技术成本但保留其价值的方法，因此RaaS寻求将企业的初始成本降至最低。提供商专注于确定实现结果所需的技术或工具，而客户则说明他们的需求和需求。
- en: The purpose of RaaS is to build customer loyalty, and so a provider has every
    interest in automating the analysis process. Therefore, AI agents can be envisioned
    as a new core component of this business model. By itself, an LLM is capable of
    almost instantaneously producing a possible report and thus generating insights
    for a customer. These reports can be personalized using LLMs and provide insights
    tailored to the clients. The addition of tools and databases allows for both adding
    a quantitative component and extending the capabilities of an LLM. Agents then
    allow tasks to be completed automatically and routinely. In fact, agents can analyze
    large amounts of data and can be complemented with additional models. The reports
    (or even presentations) generated can be used to make informed decisions.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: RaaS的目的是建立客户忠诚度，因此提供商有每个兴趣自动化分析过程。因此，可以设想AI代理成为这种商业模式的新核心组件。LLM本身能够几乎瞬间生成可能的报告，从而为客户生成见解。这些报告可以使用LLM进行个性化，并提供针对客户的定制见解。添加工具和数据库既可添加定量组件，又可扩展LLM的功能。然后，代理允许自动和常规地完成任务。实际上，代理可以分析大量数据，并可以补充额外的模型。生成的报告（甚至演示）可用于做出明智的决策。
- en: 'RaaS thus has several advantages:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RaaS具有以下优势：
- en: '**Outcome-focused approach**: The business pays only for results (and thus
    for the value that is delivered) and not for tools, infrastructure, and expertise.
    This reduces risk for a business, since it has no responsibility for either using
    software or conducting analysis.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以结果为导向的方法**：企业只为结果（以及交付的价值）付费，而不是为工具、基础设施和专业知识付费。这降低了企业的风险，因为它对使用软件或进行分析没有责任。'
- en: '**Cost efficiency**: For the customer, there is no need to spend money to build
    infrastructure and expertise. Instead, the service provider can automate the process
    and reduce costs (it can be rather expensive for a small business). Also, the
    client can adopt a subscription plan at an agreed price (with the added benefit
    that outcome-based pricing models align costs directly with results achieved),
    and the provider instead gets a stable monthly income.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：对于客户来说，无需花费金钱来构建基础设施和专业知识。相反，服务提供商可以自动化流程并降低成本（对于小企业来说可能相当昂贵）。此外，客户可以以商定的价格采用订阅计划（附带的好处是结果导向的定价模型直接将成本与实现的结果相匹配），而服务提供商则获得稳定的月收入。'
- en: '**Focus on core competencies**: Since a company does not have to invest resources
    in building and maintaining systems or managing processes, RaaS provides a large
    time advantage. This also allows the business to implement new capabilities, demanding
    execution only from the provider. The customer can then focus on its core competencies
    and incorporate the results directly into its pipeline.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注核心竞争力**：由于公司不必投资资源来构建和维护系统或管理流程，RaaS提供了巨大的时间优势。这也允许企业实施新能力，只需从提供商那里执行。然后，客户可以专注于其核心竞争力，并将结果直接纳入其流程中。'
- en: '**Scalability, accuracy, and flexibility**: The system is scalable and flexible,
    as the provider can reuse the technology for different clients. Providers are
    incentivized to deliver high-quality outcomes since their payment or reputation
    depends on the success of the service.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性、准确性和灵活性**：该系统具有可扩展性和灵活性，因为提供商可以为不同的客户重复使用这项技术。由于他们的支付或声誉取决于服务的成功，因此提供商有动力提供高质量的结果。'
- en: 'RaaS can also have some disadvantages:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: RaaS也可能存在一些劣势：
- en: '**Loss of control**: Clients have limited control over how these outcomes are
    achieved. They can’t track the process or diagnose potential problems that arise
    during the process. In addition, there could be potential concerns over compliance,
    quality, or ethical practices on the part of the provider that the client might
    not notice. In general, RaaS does not promote transparency, and it relies on the
    client’s trust in the provider.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制权丧失**：客户对如何实现这些结果的控制有限。他们无法跟踪过程或诊断过程中出现的潜在问题。此外，还可能存在客户可能没有注意到的合规性、质量或道德实践方面的潜在问题。总的来说，RaaS不促进透明度，它依赖于客户对提供商的信任。'
- en: '**Dependency on providers**: For customers, RaaS means heavy reliance on a
    service provider, which can lead to vendor lock-in, difficulty in changing providers,
    or high costs in changing a provider. Any failure or inefficiency on the provider’s
    part has a direct impact on customer operations. In these cases, the customer
    has limited options.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对提供商的依赖**：对于客户来说，RaaS意味着对服务提供商的高度依赖，这可能导致供应商锁定、更换供应商困难或更换供应商成本高昂。任何提供商方面的失败或低效都会直接影响客户运营。在这些情况下，客户的选择有限。'
- en: '**Data security and privacy risks**: Sensitive data may need to be shared with
    the service provider, creating privacy and security concerns. Businesses may not
    be able to share this data due to regulation, risking potential breaches and hefty
    fines. At the same time, if sensitive data were intercepted, businesses could
    face serious reputational damage or fines. RaaS service providers, therefore,
    come with large costs to maintain system security, data storage, and connections.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据安全和隐私风险**：敏感数据可能需要与服务提供商共享，从而产生隐私和安全方面的担忧。由于监管规定，企业可能无法共享这些数据，这可能导致潜在的安全漏洞和巨额罚款。同时，如果敏感数据被截获，企业可能会面临严重的声誉损害或罚款。因此，RaaS服务提供商需要承担大量成本来维护系统安全、数据存储和连接。'
- en: '**Complexity in measuring results**: Defining clear, measurable outcomes can
    be challenging, especially when the goal or analysis is complex. Misaligned expectations
    between the client and the provider may lead to disputes about whether outcomes
    have been achieved. These disputes can become costly lawsuits and impact the provider’s
    reputation.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**衡量结果复杂性**：定义明确、可衡量的结果可能具有挑战性，尤其是在目标或分析复杂的情况下。客户和提供商之间的期望不一致可能导致关于是否实现结果的争议。这些争议可能变成昂贵的诉讼，并影响提供商的声誉。'
- en: '**Potential for higher costs**: On the one hand, RaaS reduces upfront costs,
    but in the long run, the service can become expensive for a business. Also, there
    may be added costs for further analysis, or if there is misalignment in performance
    and goals.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在的高成本**：一方面，RaaS可以降低前期成本，但从长远来看，这项服务对于企业来说可能会变得昂贵。此外，还可能产生进一步分析的成本，或者如果性能和目标不一致，也可能产生额外成本。'
- en: '**Limited customization**: RaaS solutions may be defined by broad application,
    and may not meet specific, niche requirements of a business. A service provider
    has every interest in automating tasks and creating solutions that are useful
    to the greatest number of customers. This means specific customer needs may have
    additional costs, not be addressed, or not be fully understood by the provider.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的定制性**：RaaS解决方案可能由广泛的应用定义，可能无法满足企业特定的、细分的需求。服务提供商有自动化任务和创建对最多客户有用的解决方案的强烈兴趣。这意味着特定客户的需求可能需要额外成本，可能不会被解决，或者可能不被提供商充分理解。'
- en: '**Quality assurance challenges**: The provider has an interest in reducing
    costs; this is done through automation and trying to achieve a solution that fits
    all clients. A provider may cut corners to achieve outcomes quickly, potentially
    compromising long-term value.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量保证挑战**：提供商有降低成本的兴趣；这是通过自动化和尝试实现适合所有客户的解决方案来实现的。提供商可能会为了快速实现结果而走捷径，这可能会损害长期价值。'
- en: RaaS, in any case, is a growing business model, especially with the growing
    interest in AI and generative AI (many businesses want to integrate AI services
    but have neither the expertise nor the infrastructure to do so). Many companies
    are only interested in the outcome of the model (such as predictions for maintenance
    or a patient’s outcome) rather than the model itself. Many businesses would be
    interested in tailoring the outcome to their specific needs, without needing to
    develop the entire process. Therefore, as competition increases, different providers
    are beginning to specialize in highly specific offerings for different types of
    industries. This drives innovation as companies strive to cover needs that are
    currently unmet. With more offerings, customers’ needs will also evolve, allowing
    companies to focus on improving crucial elements of their business.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，RaaS都是一个不断发展的商业模式，尤其是在对AI和生成式AI的兴趣日益增长的情况下（许多企业希望整合AI服务，但既没有专业知识也没有基础设施来实现这一点）。许多公司只对模型的结果（如维护预测或患者的预后）感兴趣，而不是模型本身。许多企业会对结果进行定制，以满足他们的特定需求，而不需要开发整个流程。因此，随着竞争加剧，不同的提供商开始为不同类型的行业提供高度专业化的产品。这推动了创新，因为公司努力满足目前尚未满足的需求。随着提供的产品增多，客户的需求也将不断发展，使公司能够专注于改善其业务的关键要素。
- en: A comparison of the different paradigms
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同范例的比较
- en: 'We can summarize the choice of paradigm as follows:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将范例的选择总结如下：
- en: '**SaaS**: A provider should choose SaaS when they want to offer a steady and
    predictable revenue stream through subscriptions, their product is scalable to
    a large number of customers (thus reducing the cost of their solution), it is
    easy to support updates and maintenance, they have capabilities to leverage cloud
    infrastructure to minimize hardware costs, they can guarantee frequent software
    improvements, and ensure customer loyalty. A customer should choose SaaS when
    they need quick access to software without having to invest in hardware or maintenance,
    software flexibility and scalability are critical, or they prefer paying for software
    on a subscription basis rather than making large upfront investments. SaaS is
    also a good choice when customers prefer that updates, maintenance, and security
    are handled by an external provider or they are interested in applications that
    are remotely accessible (e.g., they have teams that are spread across various
    countries or various locations). Examples of companies using SaaS are Salesforce
    (a cloud-based CRM system widely used across industries), Microsoft 365 (offers
    productivity tools such as Word, Excel, and Teams via cloud subscription), Adobe
    Creative Cloud (provides access to creative tools such as Photoshop and Illustrator
    with continuous cloud updates), and Slack (a communication platform used by distributed
    teams for messaging and collaboration).'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SaaS**：当提供商希望通过订阅提供稳定和可预测的收入流时，应选择SaaS；他们的产品可扩展到大量客户（从而降低解决方案的成本），易于支持更新和维护，他们有利用云基础设施来最小化硬件成本的能力，可以保证频繁的软件改进，并确保客户忠诚度。当客户需要快速访问软件而不必投资硬件或维护时，应选择SaaS；软件的灵活性和可扩展性至关重要，或者他们更愿意按订阅方式支付软件费用而不是进行大量前期投资。当客户更喜欢由外部提供商处理更新、维护和安全时，或者他们对远程可访问的应用程序感兴趣（例如，他们有分布在不同国家或不同地点的团队），SaaS也是一个不错的选择。使用SaaS的例子包括Salesforce（一个在各个行业广泛使用的基于云的CRM系统）、Microsoft
    365（通过云订阅提供Word、Excel和Teams等生产力工具）、Adobe Creative Cloud（提供Photoshop和Illustrator等创意工具的访问权限，并持续进行云更新），以及Slack（一个由分布式团队用于消息和协作的通信平台）。'
- en: '**MaaS**: A provider should look to MaaS when they can reduce the cost of model
    delivery with other partners (or have a solid infrastructure), have developed
    high-performing AI/ML models that can serve various industries (e.g., healthcare,
    finance, or retail), want to monetize the developed models or expertise without
    sharing the algorithms, and can securely and reliably guarantee the model access.
    Users should consider these solutions when they require advanced AI/ML models
    but lack the resources to build or train them in-house, or prefer outsourcing
    model maintenance, retraining, and optimization rather than managing it internally.
    These models should also be considered when cost efficiency and flexibility are
    priorities, especially for start-ups and businesses experimenting with AI/ML,
    as well as when time to market for AI/ML-driven applications is critical. Examples
    of companies using MaaS are OpenAI (provides access to GPT models through APIs
    for tasks such as text generation or summarization), Google Cloud AI Platform
    (offers models for translation, vision, speech recognition, and more), AWS SageMaker
    JumpStart (lets businesses quickly deploy pre-trained models for tasks such as
    fraud detection), and Hugging Face (through its Inference API, offers hosted access
    to thousands of open source models).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaaS**: 当提供商能够与其他合作伙伴（或拥有稳固的基础设施）降低模型交付成本时，应考虑采用MaaS；如果他们已经开发出高性能的AI/ML模型，这些模型可以服务于多个行业（例如，医疗保健、金融或零售），希望在不共享算法的情况下货币化开发出的模型或专业知识，并且能够安全可靠地保证模型访问，也应考虑MaaS。当用户需要高级AI/ML模型但缺乏内部构建或训练的资源，或者更愿意外包模型维护、再培训和优化而不是内部管理时，应考虑这些解决方案。当成本效益和灵活性是优先考虑的因素时，特别是对于正在尝试AI/ML的初创公司和企业，以及当AI/ML驱动应用的上市时间至关重要时，也应考虑这些模型。使用MaaS的例子包括OpenAI（通过API提供GPT模型的访问权限，用于文本生成或摘要等任务）、Google
    Cloud AI平台（提供翻译、视觉、语音识别等模型）、AWS SageMaker JumpStart（允许企业快速部署预训练模型，用于欺诈检测等任务），以及Hugging
    Face（通过其推理API，提供数千个开源模型的托管访问权限）。'
- en: '**DaaS**: A provider should choose DaaS if they have access to high-value,
    unique datasets that can benefit multiple industries, they want to capitalize
    on the growing reliance on data for decision-making and analytics, they want to
    create an additional business opportunity for their company (e.g., selling data
    that has been acquired over time), they can ensure compliance with data protection
    regulations (e.g., GDPR or CCPA), they have the infrastructure to be able to conduct
    data sharing, or they provide (or intend to) added value beyond raw data, such
    as insights, visualizations, or integration with tools. A client should consider
    DaaS if they need large volumes of data but do not want to invest in storage and
    processing infrastructure, their business relies on external or specialized datasets
    (e.g., market data, weather data, geolocation data, financial data, healthcare
    data, and so on), they prefer flexibility in accessing different datasets and
    scaling, or they do not want to deal with data compliance, maintenance, and security.
    Examples include Snowflake (a cloud data platform that enables secure data sharing
    across organizations), Quandl by Nasdaq (offers financial, economic, and alternative
    data to analysts and institutions), Clearbit (provides B2B data for sales and
    marketing enrichment), and the Climate Data Store from Copernicus (offers environmental
    and climate datasets for scientific and commercial use).'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DaaS**: 如果提供商能够访问高价值、独特的数据集，这些数据集可以惠及多个行业，他们希望利用对数据在决策和数据分析中日益增长的依赖性，他们希望为公司创造额外的商业机会（例如，销售随着时间的推移所获得的数据），他们能够确保符合数据保护法规（例如，GDPR或CCPA），他们拥有进行数据共享的基础设施，或者他们提供（或打算提供）超出原始数据的价值，例如见解、可视化或与工具的集成，应选择DaaS。如果客户需要大量数据但不想投资于存储和处理基础设施，他们的业务依赖于外部或专业数据集（例如，市场数据、天气数据、地理位置数据、财务数据、医疗保健数据等），他们更喜欢访问不同数据集的灵活性和可扩展性，或者他们不想处理数据合规性、维护和安全，应考虑DaaS。例子包括Snowflake（一个云数据平台，允许组织之间安全地共享数据）、Quandl
    by Nasdaq（为分析师和机构提供金融、经济和替代数据）、Clearbit（提供B2B数据以丰富销售和营销）、以及Copernicus的气候数据存储（提供用于科学和商业用途的环境和气候数据集）。'
- en: '**RaaS**: A provider may consider RaaS if they have the appropriate infrastructure
    to guarantee reliable and measurable outcomes to customers, prefer to differentiate
    themselves by focusing on delivering value and results rather than selling products
    or services, can measure performance and guaranteed outcomes to the customer,
    and have expertise in mitigating risks and guaranteeing performance. Customers
    should choose RaaS when they want to achieve specific outcomes without managing
    the underlying processes, infrastructure, or technology; when their focus is on
    outcomes (e.g., performance improvement or operational efficiency) rather than
    tools or inputs; when they want to minimize risks by paying only for successful
    outcomes or results; when they lack expertise in achieving some complex and specialized
    outcomes; or when they want to reduce costs and spread them out over time. Examples
    of companies that use RaaS are Pymetrics (delivers hiring recommendations based
    on neuroscience and AI without exposing internal mechanisms), Afiniti (uses AI
    to optimize call center pairings and charges based on improved performance), Uptake
    (provides predictive maintenance in industrial contexts tied to uptime or efficiency
    gains), and ZS Associates (offers analytics-driven solutions in healthcare and
    pharma, charging based on KPIs and performance improvements).'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RaaS**：如果提供商拥有适当的基础设施来保证向客户提供可靠和可衡量的结果，并倾向于通过专注于提供价值和结果来区分自己，能够衡量性能并向客户保证结果，并且有专业知识来减轻风险和保证性能，他们可能会考虑RaaS。当客户希望实现特定结果而不需要管理底层流程、基础设施或技术；当他们的重点是结果（例如，性能改进或运营效率）而不是工具或输入；当他们希望通过仅支付成功的成果或结果来最小化风险；当他们缺乏实现某些复杂和特定结果的专业知识；或者当他们希望降低成本并在一段时间内分散成本时，他们应该选择RaaS。使用RaaS的公司的例子包括Pymetrics（基于神经科学和AI提供招聘建议，而不暴露内部机制）、Afiniti（使用AI优化呼叫中心配对并基于改进的性能收费）、Uptake（在工业环境中提供与正常运行时间或效率提升相关的预测性维护）和ZS
    Associates（在医疗保健和制药领域提供基于KPI和性能改进的分析驱动解决方案）。'
- en: 'The following table provides a summary of the advantages and disadvantages
    of each paradigm for providers and users:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 下表提供了每个范例对提供商和用户的优缺点总结：
- en: '| **Category** | **SaaS** | **MaaS** | **DaaS** | **RaaS** |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **SaaS** | **MaaS** | **DaaS** | **RaaS** |'
- en: '| **Advantages (****provider)** | - Recurring revenue model.- Scalable infrastructure-
    Easier software updates.- Cost-efficient development life cycle. | - Enables monetization
    of AI/ML models.- Scalable distribution of computational resources.- Supports
    various industries such as healthcare and finance.- Reduced infrastructure needs
    (e.g., cloud-hosted ML models).- Opportunity to expand into niche AI/ML applications.
    | - Data monetization opportunities.- Centralized management of data.- Predictable
    revenue.- Ability to leverage existing datasets.- Flexibility in serving different
    industries. | - Steady and predictable revenue streams.- Encourages value-based
    pricing for outcomes.- Differentiates offering in competitive markets.- Enables
    providers to focus on delivering outcomes rather than selling products.- Improved
    customer retention. |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| **优点（****提供商）** | - 可重复收入模式。- 可扩展的基础设施。- 更容易的软件更新。- 成本效益的开发生命周期。 | - 使AI/ML模型货币化。-
    可扩展的计算资源分配。- 支持医疗保健和金融等各个行业。- 减少基础设施需求（例如，云托管ML模型）。- 扩展到利基AI/ML应用的机会。 | - 数据货币化机会。-
    数据集中管理。- 可预测的收入。- 利用现有数据集的能力。- 为不同行业提供服务时的灵活性。 | - 稳定且可预测的收入流。- 鼓励基于结果的定价。- 在竞争市场中区分产品。-
    使提供商能够专注于交付结果而不是销售产品。- 提高客户保留率。 |'
- en: '| **Advantages (****User)** | - Low upfront cost.- Easy access to the latest
    software versions.- Accessibility from anywhere.- Flexibility in subscriptions
    to match business needs. | - Access to advanced models without the need to build
    or train them.- Scalable computing power to process models efficiently.- Flexibility
    in using models for predictions or automation.- Cost savings by avoiding the need
    to build in-house AI/ML infrastructure- Enables faster time to market for AI-powered
    applications. | - Easy and quick access to curated, usable data.- Lower cost of
    ownership for data systems.- Eliminates the need for large data storage/processing
    infrastructure.- Flexible scaling. | - Reduced risk with outcome-based payments.-
    Focus on results without worrying about underlying infrastructure.- Predictable
    performance and value.- No need for large initial investments.- Simplifies achieving
    desired results with expert support. |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| **优点（****用户）** | - 低前期成本。- 容易访问最新的软件版本。- 从任何地方都可以访问。- 订阅的灵活性以适应业务需求。 | -
    无需构建或训练即可访问高级模型。- 可扩展的计算能力以高效处理模型。- 使用模型进行预测或自动化的灵活性。- 通过避免构建内部AI/ML基础设施节省成本-
    使AI应用更快上市。 | - 容易且快速访问经过策划、可用的数据。- 数据系统的拥有成本较低。- 消除了需要大型数据存储/处理基础设施的需求。- 可伸缩性。
    | - 基于结果的支付降低了风险。- 专注于结果，无需担心底层基础设施。- 可预测的性能和价值。- 无需进行大量初始投资。- 在专家支持下简化实现预期结果。
    |'
- en: '| **Disadvantages (****provider)** | - High competition and customer churn.-
    Ongoing costs for infrastructure and updates.- Challenges with regional regulations
    and compliance. | - High initial development cost for models.- Ensuring fairness,
    reliability, and compliance in AI/ML models is challenging.- Managing performance
    expectations of models across diverse use cases.- Resource-intensive model updates
    and retraining. | -Privacy/security concerns with data usage.- Infrastructure
    for real-time data delivery.- Need for compliance with complex data regulations
    (e.g., GDPR). | - Revenue depends on the successful delivery of outcomes.- High
    upfront costs for performance guarantees.- Complex measurement and accountability
    metrics.- Risk of lower margins if outcomes are hard to deliver or expectations
    are misaligned. |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| **缺点（****提供者）** | - 竞争激烈和客户流失。- 基础设施和更新的持续成本。- 与区域法规和合规性方面的挑战。 | - 模型的初始开发成本高。-
    确保AI/ML模型中的公平性、可靠性和合规性具有挑战性。- 管理模型在多种用例中的性能预期。- 资源密集型的模型更新和再训练。 | - 数据使用中的隐私/安全问题。-
    实时数据交付的基础设施。- 需要遵守复杂的数据法规（例如，GDPR）。 | - 收入取决于结果的成功交付。- 为性能保证而支付的高前期成本。- 测量和问责制指标复杂。-
    如果结果难以交付或期望不一致，则存在利润率降低的风险。 |'
- en: '| **Disadvantages (****user)** | - Dependence on internet connectivity.- Data
    security and privacy risks.- Long-term costs may exceed owning software outright.
    | - Dependence on third-party models.- Potential for bias or errors in AI/ML models.-
    May incur long-term costs if frequently needed.- Limited ability to customize
    models for highly specific needs.- Privacy concerns in certain AI/ML applications.
    | - Concerns about data ownership and vendor lock-in.- Potential for high long-term
    costs.- Possible over-reliance on third-party data.- Security risks with sensitive
    data. | - Dependence on vendor for outcome success.- Lack of transparency in how
    the processes achieve results.- Limited flexibility to modify outcomes during
    contracts.- May not suit users with highly specific, non- standardized needs.-
    Costs can escalate if outcomes are not well defined. |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| **缺点（****用户）** | - 依赖互联网连接。- 数据安全和隐私风险。- 长期成本可能超过直接购买软件。 | - 依赖第三方模型。- AI/ML模型中可能存在偏差或错误。-
    如果经常需要，可能会产生长期成本。- 有限的能力为高度特定的需求定制模型。- 在某些AI/ML应用中存在隐私问题。 | - 对数据所有权和供应商锁定问题的担忧。-
    可能存在高额的长期成本。- 可能过度依赖第三方数据。- 敏感数据的安全风险。 | - 依赖于供应商以实现结果的成功。- 过程如何实现结果缺乏透明度。- 在合同期间修改结果的能力有限。-
    可能不适合具有高度特定、非标准化需求的用户。- 如果结果定义不明确，成本可能会上升。 |'
- en: Table 9.1 – Advantages and disadvantages for providers and users
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 – 提供者和用户的优缺点
- en: The choice of business paradigm is an important one. Each paradigm has an impact
    on both a user and a business. Finding the right paradigm saves resources and
    increases revenue. The choice of paradigm impacts the technical choices for developing
    a multi-agent system.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 商业范式的选择非常重要。每个范式都会对用户和业务产生影响。找到正确的范式可以节省资源并增加收入。范式的选择会影响开发多智能体系统的技术选择。
- en: Summary
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have seen how the tools we looked at in previous chapters
    can be added to an LLM. We saw that an LLM is capable of planning and reasoning,
    but it produces weaker results when it comes to execution. An LLM is capable of
    generating text, but at the same time, the enormous amount of information learned
    allows it to develop skills beyond text generation. While it is a computational
    waste to ask an LLM to classify an image, an LLM can use a specialized model to
    solve the task. As we saw with HuggingGPT, a model can invoke other models to
    identify a pizza in an image. In that case, we saw an LLM invoke more than one
    model, collect their outputs, and conduct reasoning about the results (observe
    that the models agreed on the type of pizza in the image). The LLM can then conduct
    reasoning and choose which models need to run to complete the task, collect the
    outputs, and observe whether the task is completed.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何将之前章节中讨论的工具添加到LLM中。我们了解到LLM具有规划和推理的能力，但在执行方面产生的结果较弱。LLM能够生成文本，但与此同时，学习到的海量信息也允许它发展超越文本生成的技能。虽然要求LLM对图像进行分类是一种计算上的浪费，但LLM可以使用专门的模型来完成任务。正如我们通过HuggingGPT所看到的，一个模型可以调用其他模型来识别图像中的披萨。在这种情况下，我们看到了LLM调用多个模型，收集它们的输出，并对结果进行推理（注意，模型在图像中披萨的类型上达成一致）。然后LLM可以进行推理，选择需要运行的模型，收集输出，并观察任务是否完成。
- en: 'This concept makes it possible to revolutionize various industrial applications.
    For example, a customer can request by email to exchange an item because the size
    they purchased was too small. An LLM understands the complaint, devises a plan,
    and executes it. The model can use tools to verify the purchase, another tool
    to see whether the size up is in stock, software to order the shipment, and, once
    the order is complete, respond to the customer that their request has been fulfilled.
    Agents therefore enable the automation of various tasks, as they allow an LLM
    to use other tools necessary for task completion. As we have seen, this approach
    extends to many other applications: agents in the law field, agents for research
    in chemistry and biology, and so on. For example, AI agents could be legal assistants
    to help write papers, assist professors in creating lectures, or help researchers
    define scientific hypotheses.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念使得彻底改变各种工业应用成为可能。例如，客户可以通过电子邮件请求更换商品，因为他们购买的商品尺寸太小。大型语言模型（LLM）能够理解投诉，制定计划并执行它。该模型可以使用工具来验证购买，另一个工具来查看是否备有所需尺寸的商品，软件来安排发货，一旦订单完成，就向客户回复他们的请求已经得到满足。因此，代理能够自动化各种任务，因为它们允许LLM使用完成任务所需的其它工具。正如我们所看到的，这种方法可以扩展到许多其他应用：法律领域的代理，化学和生物学研究中的代理，等等。例如，人工智能代理可以作为法律助理帮助撰写论文，协助教授创建讲座，或者帮助研究人员定义科学假设。
- en: 'Although these seem like advanced scenarios, it must be understood that LLMs
    have limitations in reasoning, and at present, they can automate simple tasks
    but not yet complex business needs. For this, there needs to be human oversight,
    and developers need to be aware of what the limitations of the system are. In
    addition, LLMs consume resources, and these systems can be computationally expensive.
    Scalability is one of the main issues for a business that wants to adopt agents.
    Therefore, in the last section of this chapter, we discussed the various business
    paradigms that open up with the arrival of LLMs. SaaS is the classic paradigm
    that has dominated the last three decades; it was conceived during the internet
    revolution but before the arrival of AI as a mass product. DaaS focuses on AI
    and businesses’ need for quality data to make informed decisions. MaaS is dedicated
    to companies that want to provide ML and AI models, while RaaS focuses only on
    the output of these models. There are clear similarities between SaaS and these
    paradigms, but they take into consideration two factors: AI models require infrastructure
    and resources to train and use, and developing and maintaining these models requires
    considerable expertise. MaaS and RaaS thus allow a business to reduce the initial
    investment into infrastructure, training, and expertise. The choice of provider
    or client is different depending on their needs and resources, so we have provided
    a comparative table and some guidelines.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些看起来像是高级场景，但必须理解LLMs在推理方面的局限性，并且目前它们可以自动化简单任务，但还不能满足复杂的商业需求。为此，需要有人工监督，并且开发者需要意识到系统的局限性。此外，LLMs消耗资源，这些系统可能计算成本高昂。可扩展性是想要采用代理的企业面临的主要问题之一。因此，在本章的最后部分，我们讨论了随着LLMs的到来而开放的多种商业范式。SaaS是过去三十年中主导的经典范式；它是在互联网革命期间构思的，但在AI作为大众产品到来之前。DaaS专注于AI和企业在做出明智决策时对高质量数据的需求。MaaS致力于那些想要提供ML和AI模型的公司，而RaaS则仅关注这些模型的输出。SaaS和这些范式之间存在明显的相似之处，但它们考虑了两个因素：AI模型需要基础设施和资源来训练和使用，而开发和维护这些模型需要相当的专业知识。因此，MaaS和RaaS允许企业减少在基础设施、培训和专业知识方面的初始投资。根据他们的需求和资源，提供者或客户的选择是不同的，因此我们提供了一张比较表和一些指导方针。
- en: In this chapter, therefore, we have defined what an agent is in practice (or
    a group of agents in the case of a multi-agent platform) and discussed how these
    agents can be integrated into the business. In other words, we have defined an
    agent-based system. This system is not an isolated entity; in the next chapter,
    we will focus on the ecosystem around an agent and how an agent integrates into
    it.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们定义了在实践中什么是代理（或对于多代理平台而言，一组代理）以及这些代理如何被整合到商业活动中。换句话说，我们定义了一个基于代理的系统。这个系统不是一个孤立的实体；在下一章中，我们将关注代理周围的生态系统以及代理如何融入其中。
- en: Further reading
  id: totrans-478
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Shen, *HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging
    Face*, 2023, [https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen, *HuggingGPT: 使用Hugging Face中的ChatGPT及其朋友解决AI任务*, 2023, [https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580)'
- en: Wang, *A Survey on Large Language Model based Autonomous Agents*, 2023, [https://arxiv.org/abs/2308.11432](https://arxiv.org/abs/2308.11432)
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, *基于大型语言模型的自主导航代理综述*, 2023, [https://arxiv.org/abs/2308.11432](https://arxiv.org/abs/2308.11432)
- en: 'Raieli, *HuggingGPT: Give Your Chatbot an AI* *Army*, [https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98](https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98)'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raieli, *HuggingGPT: 给你的聊天机器人配备一个AI* *军队*, [https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98](https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98)'
- en: 'Schick, *Toolformer: Language Models Can Teach Themselves to Use Tools*, 2023,
    [https://arxiv.org/abs/2302.04761](https://arxiv.org/abs/2302.04761)'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick, *Toolformer: 语言模型可以教会自己使用工具*, 2023, [https://arxiv.org/abs/2302.04761](https://arxiv.org/abs/2302.04761)'
- en: 'Bran, *ChemCrow: Augmenting* *Large Language Models with Chemistry Tools*,
    2023, [https://arxiv.org/abs/2304.05376](https://arxiv.org/abs/2304.05376)'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bran, *ChemCrow: 增强大型语言模型中的化学工具*, 2023, [https://arxiv.org/abs/2304.05376](https://arxiv.org/abs/2304.05376)'
- en: 'Cui, *Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph
    Enhanced Mixture-of-Experts Large Language Model*, 2023, [https://arxiv.org/abs/2306.16092v2](https://arxiv.org/abs/2306.16092v2)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui, *Chatlaw: 基于知识图谱增强混合专家大型语言模型的多人协作法律助手*, 2023, [https://arxiv.org/abs/2306.16092v2](https://arxiv.org/abs/2306.16092v2)'
- en: 'Hamilton, *Blind Judgement: Agent-Based Supreme Court Modelling With GPT*,
    2023, [https://arxiv.org/abs/2301.05327](https://arxiv.org/abs/2301.05327)'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton, *盲判：基于GPT的基于代理的最高法院建模*，2023，[https://arxiv.org/abs/2301.05327](https://arxiv.org/abs/2301.05327)
- en: 'Cheng, *Exploring Large Language Model based Intelligent Agents: Definitions,
    Methods, and Prospects*, 2024, [https://arxiv.org/pdf/2401.03428](https://arxiv.org/pdf/2401.03428)'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng, *探索基于大型语言模型的智能代理：定义、方法和前景*，2024，[https://arxiv.org/pdf/2401.03428](https://arxiv.org/pdf/2401.03428)
- en: 'Swanson, *The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with
    Experimental Validation*, 2024, [https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swanson, *虚拟实验室：AI代理设计新的SARS-CoV-2纳米抗体，并进行实验验证*，2024，[https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full](https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)
- en: 'Lu, *The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery*,
    2024, [https://arxiv.org/abs/2408.06292](https://arxiv.org/abs/2408.06292)'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu, *AI科学家：向完全自动化的开放式科学发现迈进*，2024，[https://arxiv.org/abs/2408.06292](https://arxiv.org/abs/2408.06292)
- en: 'Fossi, *SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs
    and Agents*, 2024, [https://arxiv.org/abs/2409.15817](https://arxiv.org/abs/2409.15817)'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fossi, *SwiftDossier：使用LLM和代理为药物发现量身定制的自动档案*，2024，[https://arxiv.org/abs/2409.15817](https://arxiv.org/abs/2409.15817)
- en: Si, *Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with
    100+ NLP Researchers*, 2024, [https://arxiv.org/abs/2409.04109](https://arxiv.org/abs/2409.04109)
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si, *大型语言模型能否生成新颖的研究想法？一项包含100+ NLP研究者的大规模人类研究*，2024，[https://arxiv.org/abs/2409.04109](https://arxiv.org/abs/2409.04109)
- en: Raieli, *AI Planning or Serendipity? Where Do the Best Research Ideas Come*
    *From?*, [https://ai.gopubby.com/ai-planning-or-serendipity-where-do-the-best-research-ideas-come-from-f8e5e6692964](https://ai.gopubby.com/ai-planning-or-serendipity-where-do-the-best-research-ideas-come-from-f8e5e6692964)
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *AI规划或偶然？最佳研究想法从何而来*，[https://ai.gopubby.com/ai-planning-or-serendipity-where-do-the-best-research-ideas-come-from-f8e5e6692964](https://ai.gopubby.com/ai-planning-or-serendipity-where-do-the-best-research-ideas-come-from-f8e5e6692964)
- en: 'Raieli, *A Brave New World for Scientific Discovery: Are AI Research Ideas*
    *Better?*, [https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182](https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182)'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *科学发现的崭新世界：AI研究想法* *是否更好*？，[https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182](https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182)
- en: 'Schmidgall, *Agent Laboratory: Using LLM Agents as Research Assistants*, 2024,
    [https://arxiv.org/abs/2501.04227](https://arxiv.org/abs/2501.04227)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidgall, *代理实验室：使用LLM代理作为研究助手*，2024，[https://arxiv.org/abs/2501.04227](https://arxiv.org/abs/2501.04227)
- en: 'Tang, *ChemAgent: Self-updating Library in Large Language Models Improves Chemical
    Reasoning*, 2025, [https://arxiv.org/abs/2501.06590](https://arxiv.org/abs/2501.06590)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang, *ChemAgent：大型语言模型中的自更新库，提高化学推理能力*，2025，[https://arxiv.org/abs/2501.06590](https://arxiv.org/abs/2501.06590)
- en: Raieli, *Can AI Replace Human* *Researchers*, [https://levelup.gitconnected.com/can-ai-replace-human-researchers-50fcc43ea587](https://levelup.gitconnected.com/can-ai-replace-human-researchers-50fcc43ea587)
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *能否用AI取代人类研究者*，[https://levelup.gitconnected.com/can-ai-replace-human-researchers-50fcc43ea587](https://levelup.gitconnected.com/can-ai-replace-human-researchers-50fcc43ea587)
- en: '*European* *Cloud Computing* *Platforms*, [https://european-alternatives.eu/category/cloud-computing-platforms](https://european-alternatives.eu/category/cloud-computing-platforms)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧洲* *云计算* *平台*，[https://european-alternatives.eu/category/cloud-computing-platforms](https://european-alternatives.eu/category/cloud-computing-platforms)'
- en: IBM, *What is* *Multi**-tenant?*, [https://www.ibm.com/topics/multi-tenant](https://www.ibm.com/topics/multi-tenant)
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM, *什么是* *多租户*？，[https://www.ibm.com/topics/multi-tenant](https://www.ibm.com/topics/multi-tenant)
- en: 'Gan, 2023, *Model-as-a-Service (MaaS): A* *Survey*, [https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804)'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan, 2023，*模型即服务（MaaS）*，[https://arxiv.org/pdf/2311.05804](https://arxiv.org/pdf/2311.05804)
- en: Abe, *A Data as a Service (DaaS) Model for GPU-based Data Analytics*, 2018,
    [https://arxiv.org/abs/1802.01639](https://arxiv.org/abs/1802.01639)
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abe, *基于GPU的数据分析的数据即服务（DaaS）模型*，2018，[https://arxiv.org/abs/1802.01639](https://arxiv.org/abs/1802.01639)
- en: 'Forbes, *AI Agents: The Next Frontier In Intelligent* *Automation*, [https://www.forbes.com/councils/forbestechcouncil/2025/01/02/ai-agents-the-next-frontier-in-intelligent-automation/](https://www.forbes.com/councils/forbestechcouncil/2025/01/02/ai-agents-the-next-frontier-in-intelligent-automation/)'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Forbes，*人工智能代理：智能* *自动化* *的下一个前沿*，[https://www.forbes.com/councils/forbestechcouncil/2025/01/02/ai-agents-the-next-frontier-in-intelligent-automation/](https://www.forbes.com/councils/forbestechcouncil/2025/01/02/ai-agents-the-next-frontier-in-intelligent-automation/)
- en: World Economic Forum, *Why* *Should Manufacturers Embrace AI's Next Frontier*
    *– AI agents –* *Now**?*, [https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/](https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/)
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界经济论坛，*为什么* *制造商现在* *应该拥抱人工智能的下一个前沿* *– 人工智能代理 –* *？*，[https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/](https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/)
- en: 'Deng, 2023, *Mind2Web: Towards a Generalist Agent for the* *Web*, [https://arxiv.org/abs/2306.06070](https://arxiv.org/abs/2306.06070)'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邓某，2023，*Mind2Web：迈向网络* *通用代理*，[https://arxiv.org/abs/2306.06070](https://arxiv.org/abs/2306.06070)
