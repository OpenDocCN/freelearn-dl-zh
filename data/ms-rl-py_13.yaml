- en: '*Chapter 10*: Machine Teaching'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：机器教学'
- en: 'The great excitement about **reinforcement learning** (**RL**) is, to a significant
    extent, due to its similarities to human learning: an RL agent learns from experience.
    This is also why many consider it as the path to artificial general intelligence.
    On the other hand, if you think about it, reducing human learning to just trial
    and error would be a gross underestimation. We don''t discover everything we know,
    in science, art, engineering, and so on, from scratch when we are born! Instead,
    we build on knowledge and intuition that have been accumulated over thousands
    of years! We transfer this knowledge among us through different, structured or
    unstructured, forms of **teaching**. This capability makes it possible for us
    to gain skills relatively quickly and advance common knowledge.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）的巨大兴奋感，在很大程度上源于它与人类学习的相似性：RL智能体通过经验进行学习。这也是为什么许多人认为它是通向人工通用智能的路径。另一方面，如果你仔细想想，将人类学习仅仅归结为反复试验，实在是大大的低估了人类的学习过程。我们并非从出生开始就从零发现我们所知道的一切——无论是在科学、艺术、工程等领域！相反，我们建立在数千年来积累的知识和直觉之上！我们通过各种不同的、有结构的或无结构的**教学**形式将这些知识在我们之间传递。这种能力使得我们能够相对快速地获得技能并推动共识知识的进步。'
- en: 'When we think about it from this perspective, what we are doing with machine
    learning seems quite inefficient: we dump a bunch of raw data into algorithms,
    or expose them to an environment, in the case of RL, and train them with virtually
    no guidance. This is partly why machine learning requires so much data and fails
    at times.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们使用机器学习的方式似乎相当低效：我们将大量原始数据投入到算法中，或者让它们暴露于环境中（对于RL而言），几乎没有任何指导。这部分也是为什么机器学习需要如此大量的数据，并且有时会失败的原因。
- en: '**Machine teaching** (**MT**) is an emerging approach that shifts the focus
    to extracting knowledge from a teacher, rather than raw data, which guides the
    process of training machine learning algorithms. In turn, learning new skills
    and mappings is achieved more efficiently and with less data, time, and compute.
    In this chapter, we will introduce the components of MT for RL and some of its
    most important methods, such as reward function engineering, curriculum learning,
    demonstration learning, and action masking. At the end, we will also discuss the
    downsides and the future of MT. More concretely, we will cover the following topics
    in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器教学**（**MT**）是一种新兴的方式，它将重点转向从教师中提取知识，而不是仅仅依赖原始数据，从而指导训练机器学习算法的过程。反过来，学习新技能和映射的过程变得更加高效，且所需的数据、时间和计算资源更少。在本章中，我们将介绍MT在RL中的组成部分及其一些最重要的方法，例如奖励函数工程、课程学习、示范学习和动作屏蔽。最后，我们还将讨论MT的缺点与未来发展。具体而言，我们将在本章中涵盖以下内容：'
- en: Introduction to MT
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MT简介
- en: Engineering the reward function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数的设计
- en: Curriculum learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程学习
- en: Warm starts and demonstration learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热启动与示范学习
- en: Action masking
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作屏蔽
- en: Concept networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念网络
- en: Downsides and the promises of MT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MT的缺点与前景
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All the code for the chapter can be found at the following GitHub URL:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码可以在以下GitHub链接找到：
- en: '[https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python)'
- en: Introduction to MT
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MT简介
- en: MT is the name of a general approach and collection of methods to efficiently
    transfer knowledge from a teacher – a subject matter expert – to a machine learning
    algorithm. With that, we aim to make the training much more efficient, and even
    feasible for tasks that would be impossible to achieve otherwise. Let's talk about
    what MT is in more detail, why we need it, and what its components are.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MT是一种通用方法及其相关技术的集合，旨在高效地将知识从教师——即学科专家——转移到机器学习算法中。通过这种方式，我们旨在使训练过程更加高效，甚至使那些否则不可能完成的任务变得可行。接下来，我们将详细讨论MT是什么，为什么我们需要它，以及它的组成部分。
- en: Understanding the need for MT
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解MT的需求
- en: 'Did you know that the United States is expected to spend about 1.25 trillion
    dollars, around 5% of its gross domestic product, on education in 2021? This should
    speak to the existential significance of education to our society and civilization
    (and many would argue that we should spend more). We humans have built such a
    giant education system, which we expect people to spend many years in, because
    we don''t expect ourselves to be able to decipher the alphabet or math on our
    own. Not just that, we continuously learn from teachers around us, about how to
    use software, how to drive, how to cook, and so on. These teachers don''t have
    to be human teachers: books, blog posts, manuals, and course materials all distill
    valuable information for us so that we can learn, not just in school but throughout
    our lives.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗？美国预计在2021年将花费约1.25万亿美元，约占其国内生产总值的5%，用于教育开支。这应该能反映教育对我们社会和文明的生死攸关的重要性（许多人会争辩说，我们应该投入更多）。我们人类建立了这样一个庞大的教育系统，我们期望人们在其中花费多年，因为我们不指望自己能够独立解码字母表或数学。不仅如此，我们不断从身边的老师那里学习，如何使用软件、如何开车、如何做饭，等等。这些老师不必是人类教师：书籍、博客文章、手册和课程材料都为我们提炼了有价值的信息，让我们能够学习，不仅是在学校，而是在整个生活过程中。
- en: I hope this convinces you of the importance of teaching. But if you found this
    example too populist and perhaps a bit irrelevant to RL, let's discuss how MT
    could specifically help in RL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这能让你信服教学的重要性。但如果你觉得这个例子过于民粹化，或许与RL有些不相关，那么让我们来讨论机器翻译（MT）如何在RL中发挥特定作用。
- en: Feasibility of learning
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习的可行性
- en: Have you ever felt overwhelmed when trying to learn something on your own, without
    a (good) teacher? This is akin to an RL agent not figuring out a good policy for
    the problem at hand due to an overwhelming number of possible policies. One of
    the main obstacles in this process is the lack of proper feedback about their
    quality. You can also think of **hard exploration problems** with sparse rewards
    in the same context, a serious challenge in RL.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾在没有（好的）老师的情况下，尝试独自学习某些东西时感到力不从心？这就像一个RL代理由于可能的策略数量过多，无法为当前问题找到合适的策略。这个过程中主要的障碍之一是缺乏关于策略质量的适当反馈。你也可以将**困难的探索问题**与稀疏奖励相提并论，这是RL中的一个严重挑战。
- en: 'Consider the following example: an RL agent is trying to learn chess against
    a competitive opponent with a reward of +1 for winning, 0 for draw, and -1 for
    losing at the end of the game. The RL agent needs to stumble upon tens of "good
    moves," one after another, and among many alternative moves at each step, to be
    able to get its first 0 or +1 reward. Since this is a low likelihood, the training
    is likely to fail without a huge exploration budget. A teacher, on the other hand,
    might guide the exploration so that the RL agent knows at least a few ways to
    succeed, from which it can gradually improve upon the winning strategies.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下例子：一个RL代理正在与一个竞争对手对弈国际象棋，胜利奖励为+1，平局奖励为0，失败奖励为-1。RL代理需要不断“偶然”地找到数十个“好棋步”，一招接一招，在每一步的众多备选棋步中，才能获得第一次0或+1的奖励。由于这种情况的发生概率较低，如果没有巨大的探索预算，训练很可能会失败。另一方面，教师可以引导探索，使得RL代理至少知道几种成功的方式，从而逐渐改进其胜利策略。
- en: Info
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: When DeepMind created its AlphaStar agent to play StarCraft II, they used supervised
    learning to train the agent on past human game logs before going into RL-based
    training. Human players in some sense were the first teachers of the agent, and
    without them, the training would be impractical/too costly. To support this argument,
    you can take the example of the OpenAI Five agent trained to play Dota 2\. It
    took almost a year to train the agent.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当DeepMind创建其AlphaStar代理来玩《星际争霸II》时，他们首先使用监督学习训练代理，基于过去的人类游戏日志，然后才进入基于强化学习（RL）的训练。在某种意义上，人类玩家是代理的第一批教师，没有他们，这种训练将变得不可行或成本过高。为了支持这一论点，可以以训练OpenAI
    Five代理玩《Dota 2》的例子为例。训练该代理花费了近一年的时间。
- en: 'The following figure shows the agent in action:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了代理在行动中的情形：
- en: '![Figure 10.1 – DeepMind''s AlphaStar agent in action (source: The AlphaStar
    Team, 2019)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – DeepMind的AlphaStar代理在行动中（来源：AlphaStar团队，2019）'
- en: '](img/B14160_10_1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_1.jpg)'
- en: 'Figure 10.1 – DeepMind''s AlphaStar agent in action (source: The AlphaStar
    Team, 2019)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – DeepMind的AlphaStar代理在行动中（来源：AlphaStar团队，2019）
- en: In summary, having access to a teacher could make the learning feasible in a
    reasonable amount of time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，有一个教师可以让学习在合理的时间内变得可行。
- en: Time, data, and compute efficiency
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间、数据和计算效率
- en: Let's say you have enough compute resources and can afford to try an enormous
    number of sequences of moves for the RL agent to discover winning strategies in
    an environment. Just because you can, doesn't mean that you should do it and waste
    all those resources. A teacher could help you to greatly reduce the training time,
    data, and compute. You can use the resources you saved to iterate on your ideas
    and come up with better agents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有足够的计算资源，并且能够尝试大量的动作序列，让RL代理在一个环境中发现获胜策略。仅仅因为你能做到，并不意味着你应该这样做并浪费所有这些资源。教师可以帮助你大大减少训练时间、数据和计算量。你可以利用节省下来的资源来反复改进你的想法，开发更好的代理人。
- en: Tip
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Do you have mentors in your life to help you with your career, education, marriage,
    and so on? Or do you read books about these topics? What is your motivation? You
    don't want to repeat the mistakes of others or reinvent what others already know
    just to waste your time, energy, and opportunities, do you? MT similarly helps
    your agent jumpstart its task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否有导师帮助你在职业、教育、婚姻等方面的发展？或者你是否阅读过关于这些主题的书籍？你的动力是什么？你不想重复他人的错误，也不想重新发明别人已经知道的东西，浪费你的时间、精力和机会，对吧？MT以类似的方式帮助你的代理人快速开始任务。
- en: 'The benefit of MT goes beyond the feasibility of learning or its efficiency.
    Next, let''s talk about another aspect: the safety of your agent.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MT的好处不仅仅在于学习的可行性或效率。接下来，我们来谈谈另一个方面：你代理人的安全性。
- en: Ensuring the safety of the agent
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确保代理人的安全性
- en: A teacher is a subject matter expert on a topic. Therefore, a teacher usually
    has a pretty good idea about what actions under which conditions can get the agent
    in trouble. The teacher can inform the agent about these conditions by limiting
    the actions it could take to ensure its safety. For example, while training an
    RL agent for a self-driving car, it is natural to limit the speed of the car depending
    on the conditions of the road. This is especially needed if the training happens
    in the real world so that the agent does not blindly explore crazy actions to
    discover how to drive. Even when the training happens in a simulation, imposing
    these limitations will help with the efficient use of the exploration budget,
    related to the tip in the previous section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 教师是某个主题的专家。因此，教师通常对在什么条件下采取哪些行动可能会让代理人陷入困境有一个相当清晰的认识。教师可以通过限制代理人可以采取的行动来告知代理人这些条件，从而确保其安全。例如，在为自动驾驶汽车训练强化学习（RL）代理时，根据路况限制汽车的速度是很自然的。如果训练发生在现实世界中，这一点尤为重要，以确保代理人不会盲目探索危险的行为来发现如何驾驶。即便训练发生在模拟环境中，施加这些限制也有助于高效使用探索预算，这与上一节中的提示相关。
- en: Democratizing machine learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习的民主化
- en: When teachers train students, they do not worry about the details of the biological
    mechanisms of learning, such as which chemicals are transferred between which
    brain cells. Those details are abstracted away from the teacher; neuroscientists
    and experts who study the brain put out research about effective teaching and
    learning techniques.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当教师训练学生时，他们不会担心学习的生物机制细节，比如哪些化学物质在什么脑细胞之间传递。这些细节被从教师的视野中抽象了出来；神经科学家和研究大脑的专家会发布关于有效教学和学习技术的研究。
- en: Just like how teachers don't have to be neuroscientists, subject matter experts
    don't have to be machine learning experts to train machine learning algorithms.
    The MT paradigm suggests abstracting the low-level details of machine learning
    away from the machine teacher by developing effective and intuitive teaching methods.
    With that, it would be much easier for subject matter experts to infuse their
    knowledge into machines. Eventually, this would lead to the democratization of
    machine learning and its much greater use in many applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就像教师不需要是神经科学家一样，学科专家也不必是机器学习专家才能训练机器学习算法。MT范式通过开发有效且直观的教学方法，建议将机器学习的低层细节从机器教师中抽象出来。这样，学科专家将更容易将他们的知识注入到机器中。最终，这将导致机器学习的民主化，并使其在更多的应用中得到更广泛的使用。
- en: Data science, in general, requires combining business insights and expertise
    with mathematical tools and software to create value. When you want to apply RL
    to business problems, the situation is the same. This often requires either the
    data scientist to learn about the business or the subject matter expert to learn
    about data science, or people from both fields working together in a team. This
    poses a high bar for the adoption of (advanced) machine learning techniques in
    many settings, because it is rare for these two types of people to exist at the
    same time in the same place.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学通常需要将商业洞察和专业知识与数学工具和软件相结合，以创造价值。当你想将强化学习（RL）应用于商业问题时，情况也是一样的。这通常要求数据科学家了解商业领域，或者要求领域专家了解数据科学，或者让两者领域的人一起合作组成团队。这给许多场景下（高级）机器学习技术的采用带来了很高的门槛，因为很少能在同一个地方找到这两类人。
- en: Info
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: A McKinsey study shows that lack of analytical talent is a major barrier to
    unlocking the value in data and analytics. MT, its specific tools aside, is a
    paradigm to overcome these barriers by lowering the bar for entry for non-machine
    learning experts through the creation of intuitive tools to this end. To check
    out the study, visit [https://mck.co/2J3TFEj](https://mck.co/2J3TFEj).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一项麦肯锡的研究表明，缺乏分析人才是释放数据和分析价值的主要障碍。机器教学，除了它的特定工具外，是一种克服这些障碍的范式，通过创建直观的工具降低非机器学习专家的入门门槛，以此来解决问题。要查看这项研究，请访问[https://mck.co/2J3TFEj](https://mck.co/2J3TFEj)。
- en: 'This vision we have just mentioned is aimed more for the long term as it requires
    a lot of research and abstractions on the machine learning side. The methods we
    will cover in this section will be pretty technical. For example, we will discuss
    the **action masking** method to limit the available actions for the agent depending
    on the state it is in, which will require coding and modifying the neural network
    outcomes. However, you can imagine an advanced MT tool listening to the teacher
    saying "don''t go over 40 miles per hour within the city limits," parsing that
    command, and implementing action masking under the hood for a self-driving car
    agent:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提到的这个愿景更侧重于长远目标，因为它需要大量的研究和机器学习方面的抽象。我们将在本节中讨论的方法将非常技术性。例如，我们将讨论**动作屏蔽**方法，根据代理所处的状态限制可用的动作，这将需要编码并修改神经网络的输出。然而，你可以想象一个高级机器教学工具，听到老师说“在城市限速范围内不要超过
    40 英里每小时”，然后解析这个命令，在后台为自动驾驶汽车代理实现动作屏蔽：
- en: '![Figure 10.2 – The future of MT?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2 – 机器教学的未来？'
- en: '](img/B14160_10_2.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_2.jpg)'
- en: Figure 10.2 – The future of MT?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 机器教学的未来？
- en: Before closing this section and diving into the details of MT, let me put out
    a necessary disclaimer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束这一节并深入探讨机器教学的细节之前，让我先声明一下必要的免责声明。
- en: Disclaimer
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明
- en: One of the most vocal proponents of the MT approach is Microsoft and its Autonomous
    Systems division. As of the time of writing this book, I am an employee of the
    Autonomous Systems organization of Microsoft, working toward the mission of using
    MT to create intelligent systems. However, my goal here is not to promote any
    Microsoft product or discourse, but to tell you about this emerging topic that
    I find important. In addition, I do not officially represent Microsoft in any
    capacity and my views on the topic may not necessarily align with the company's.
    If you are curious about Microsoft's view on MT, check out the blog post at [https://blogs.microsoft.com/ai/machine-teaching/](https://blogs.microsoft.com/ai/machine-teaching/)
    and the Autonomous Systems website at [https://www.microsoft.com/en-us/ai/autonomous-systems](https://www.microsoft.com/en-us/ai/autonomous-systems).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 机器教学（MT）方法的最 vocal 拥护者之一是微软及其自动化系统部门。截至本书编写时，我是微软自动化系统组织的一名员工，致力于利用机器教学创建智能系统。然而，我在这里的目标并不是推广任何微软的产品或话语，而是向你介绍这个我认为很重要的新兴话题。此外，我并没有以任何身份正式代表微软，我对这个话题的看法也不一定与公司立场一致。如果你对微软对机器教学的看法感兴趣，可以查看[https://blogs.microsoft.com/ai/machine-teaching/](https://blogs.microsoft.com/ai/machine-teaching/)上的博客文章以及[https://www.microsoft.com/en-us/ai/autonomous-systems](https://www.microsoft.com/en-us/ai/autonomous-systems)上的自动化系统网站。
- en: Now, it is time to make the discussion more concrete. In the next section, let's
    look at the elements of MT.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候让讨论变得更具体了。在下一节中，我们将看看机器教学的要素。
- en: Exploring the elements of MT
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索机器教学的要素
- en: As MT is an emerging field, it is hard to formally define its elements. Still,
    let's look into some of the common components and themes used in it. We have already
    discussed who the machine teacher is, but let's start with that for the sake of
    completeness. Then, we will look into concepts, lessons, curriculum, training
    data, and feedback.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器教学是一个新兴领域，定义其元素是非常困难的。不过，让我们来看一下其中一些常见的组成部分和主题。我们已经讨论了机器教师是谁，但为了完整性，让我们从这点开始。接着，我们将探讨概念、课程、训练数据和反馈。
- en: Machine teacher
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器教师
- en: The **machine teacher**, or simply the **teacher**, is the subject matter expert
    of the problem at hand. In the absence of abstractions that decouple machine learning
    from teaching, this will be the data scientist – you – but this time with the
    explicit concern of guiding the training using your knowledge of the problem domain.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器教师**，或简称**教师**，是当前问题的主题专家。在没有将机器学习与教学解耦的抽象概念的情况下，教师通常是数据科学家——也就是你——但这次的角色是明确地利用你对问题领域的知识来指导训练。'
- en: Concept
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概念
- en: 'The **concept** is a specific part of the skillset that is needed to solve
    the problem. Think about training a basketball player in real life. Training does
    not consist of only practice games but is divided into mastering individual skills
    as well. Some of these skills are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念**是解决问题所需技能集的一个特定部分。想象一下在现实生活中训练一名篮球运动员。训练不仅仅是进行练习赛，还包括掌握个别技能。一些技能如下：'
- en: Shooting
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投篮
- en: Passing
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传球
- en: Dribbling
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运球
- en: Stopping and landing
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止与着陆
- en: 'Conventional training of an RL agent playing basketball would be through playing
    entire games, with which we would expect the agent to pick up these individual
    skills. MT suggests breaking the problem down into smaller concepts to learn,
    such as the skills we listed previously. This has several benefits:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的强化学习训练篮球智能体通常是通过进行整场比赛来实现，期望智能体通过比赛掌握这些个别技能。机器教学建议将问题拆解成更小的概念进行学习，比如我们之前列出的那些技能。这样做有几个好处：
- en: A monolithic task often comes with sparse rewards, which is challenging for
    an RL agent to learn from. For example, winning the basketball game would be +1
    and losing would be -1\. However, the machine teacher would know that winning
    a game would be possible through mastering individual skills. To train the agent
    on individual skills and concepts, there will be rewards assigned to them. This
    is helpful to get around the sparse reward issue and provide more frequent feedback
    to the agent in a manner that facilitates learning.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一的任务往往伴随着稀疏的奖励，这对于强化学习智能体来说是很难从中学习的。例如，赢得篮球比赛的奖励是+1，输掉比赛则是-1。然而，机器教师会知道，通过掌握个别技能，赢得比赛是可能的。为了训练智能体掌握个别技能和概念，将会为这些技能和概念分配奖励。这有助于绕过稀疏奖励的问题，并为智能体提供更频繁的反馈，从而促进学习。
- en: The credit assignment problem is a serious challenge in RL, which is about the
    difficulty of attributing the reward in later stages to individual actions in
    the earlier ones. When the training is broken down into concepts, it is easier
    to see the concepts that the agent is not good at. To be specific, this does not
    solve the credit assignment problem in itself. It is still the teacher that determines
    whether mastering a particular concept is important. But once these concepts are
    defined by the teacher, it is easier to isolate what the agent is and isn't good
    at.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用分配问题是强化学习中的一个严重挑战，它涉及到将后期阶段的奖励归因于早期阶段个别动作的难度。当训练被拆分成概念时，更容易看到智能体在哪些概念上不擅长。具体来说，这并不能解决信用分配问题。仍然是教师决定是否掌握某个特定概念很重要。但一旦这些概念被教师定义，就更容易隔离出智能体擅长和不擅长的部分。
- en: As a corollary to the preceding point, the teacher can allocate more of the
    training budget to concepts that need more training and/or are difficult to learn.
    This results in more efficient use of time and compute resources.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为前述观点的推论，教师可以将更多的训练预算分配给那些需要更多训练和/或难以学习的概念。这样可以更高效地利用时间和计算资源。
- en: For all these reasons, a task that is impractical or costly to solve monolithically
    can be efficiently solved by breaking it down into concepts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，一个不切实际或代价高昂的任务，可以通过将其拆分成概念来高效地解决。
- en: Lessons and curriculum
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 课程和教学大纲
- en: Another important element in MT is called **curriculum learning**. While training
    the agent on a concept, exposing it to an expert-level difficulty may derail the
    training. Instead, what makes more sense is to start with some easy settings and
    increase the difficulty gradually. Each of these difficulty levels makes a separate
    **lesson**, and they, together with the success thresholds that define the transition
    criteria from one lesson to the next, comprise a **curriculum**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 机器教学中的另一个重要元素是**课程学习**。在训练智能体某个概念时，暴露给它专家级难度可能会偏离训练目标。相反，更合理的做法是从一些简单的设置开始，逐步增加难度。这些难度级别分别构成一个**课程**，它们与定义从一个课程到下一个课程的过渡标准的成功阈值一起，组成了整个课程体系。
- en: Curriculum learning is one of the most important research areas in RL and we
    will elaborate more on it later. A curriculum may be designed by hand by the teacher,
    or an **auto-curriculum** algorithm can be used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习是强化学习中最重要的研究领域之一，我们将在后续详细讲解。课程可以由教师手动设计，也可以使用**自动课程**算法来生成。
- en: Training material/data
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练材料/数据
- en: Related to the previous point, another aspect of MT is to engineer the data
    that the agent will learn from. For example, the machine teacher could seed the
    training with data that includes successful episodes while using off-policy methods,
    which can overcome hard exploration tasks. This data could be obtained from an
    existing non-RL controller or the teacher's actions. This approach is also called
    **demonstration learning**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的观点相关，机器教学的另一个方面是设计智能体学习的数据。例如，机器教师可以通过使用非策略方法来为训练提供包含成功案例的数据，从而克服困难的探索任务。这些数据可以通过现有的非
    RL 控制器或教师的动作获取。这种方法也被称为**示范学习**。
- en: Info
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Demonstration learning is a popular method to train RL agents, especially in
    robotics. An ICRA paper by Nair et al. shows robots how to pick and place objects
    to seed the training of an RL agent. Check out the video at [https://youtu.be/bb_acE1yzDo](https://youtu.be/bb_acE1yzDo).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 示范学习是一种流行的强化学习智能体训练方法，特别是在机器人领域。Nair 等人在 ICARA 论文中展示了如何通过示范机器人如何拾取和放置物体来为强化学习智能体的训练提供种子。请查看视频：[https://youtu.be/bb_acE1yzDo](https://youtu.be/bb_acE1yzDo)。
- en: Conversely, the teacher could steer the agent away from bad actions. An effective
    way of achieving this is through **action masking**, which limits the available
    action space given for observation to a desirable set of actions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，教师可以引导智能体远离不良行为。一种有效的实现方式是通过**行动掩蔽**，即将观察到的可用动作空间限制为一组期望的动作。
- en: Another way of engineering the training data that the agent consumes is to monitor
    the performance of the agent, identify the parts of the state space that it needs
    more training in, and expose the agent to these states to improve the performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种设计智能体训练数据的方式是监控智能体的表现，识别其在状态空间中需要更多训练的部分，并将智能体暴露于这些状态中以提升其表现。
- en: Feedback
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反馈
- en: RL agents learn through feedback in the form of rewards. Engineering the reward
    function to make the learning easy – and even feasible in some cases that would
    have been infeasible otherwise – is one of the most important tasks of the machine
    teacher. This is usually an iterative process. It is common to revise the reward
    function many times during the course of a project to get the agent to learn the
    desired behavior. A futuristic MT tool could involve interacting with the agent
    through natural language to provide this feedback, which shapes the reward function
    used under the hood.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RL 智能体通过奖励的反馈进行学习。设计奖励函数使得学习变得简单——甚至在某些情况下，原本不可能实现的情况也能变得可行——这是机器教师最重要的任务之一。通常这是一个迭代过程。在项目的过程中，奖励函数往往需要多次修订，以促使智能体学习期望的行为。未来的机器教学工具可能涉及通过自然语言与智能体互动，提供这种反馈，从而塑造背后使用的奖励函数。
- en: With this, we have introduced you to MT and its elements. Next, we will look
    into specific methods. Rather than going over an entire MT strategy for a sample
    problem, we will next focus on individual methods. You can use them as building
    blocks of your MT strategy depending on what your problem needs. We will start
    with the most common one, reward function engineering, which you might have already
    used before.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，我们已经向您介绍了机器教学及其元素。接下来，我们将探讨具体的方法。与其讨论一个样本问题的完整机器教学策略，不如专注于单个方法。根据您的问题需求，您可以将它们作为机器教学策略的构建模块。我们将从最常见的方法开始——奖励函数设计，您可能已经使用过它。
- en: Engineering the reward function
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励函数设计
- en: Reward function engineering means crafting the reward dynamics of the environment
    in an RL problem so that it reflects the exact objective you have in mind for
    your agent. How you define your reward function might make the training easy,
    difficult, or even impossible for the agent. Therefore, in most RL projects, significant
    effort is dedicated to designing the reward. In this section, we will cover some
    specific cases where you will need to do it and how, then provide a specific example,
    and finally, discuss the challenges that come with engineering the reward function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数工程是指在强化学习问题中设计环境的奖励动态，使其反映出你为智能体设定的确切目标。你如何定义奖励函数可能会使训练变得容易、困难，甚至是不可能的。因此，在大多数强化学习项目中，会投入大量精力来设计奖励。在本节中，我们将介绍一些需要设计奖励的具体案例以及如何设计奖励，接着提供一个具体的例子，最后讨论进行奖励函数设计时所面临的挑战。
- en: When to engineer the reward function
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时进行奖励函数设计
- en: Multiple times in this book, including in the previous section when we discussed
    concepts, we have mentioned how sparse rewards pose a problem for learning. One
    way of dealing with this is to **shape the reward** to make it non-sparse. The
    sparse reward case, therefore, is a common reason why we may want to do reward
    function engineering. Yet, it is not the only one. Not all environments/problems
    have a predefined reward for you like in an Atari game. In addition, in some cases,
    there are multiple objectives that you want your agent to achieve. For all these
    reasons, many real-life tasks require the machine teacher to specify the reward
    function based on their expertise. Let's look into these cases next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中多次提到，包括在我们讨论概念的上一节中，我们讨论了稀疏奖励给学习带来的问题。解决这一问题的一种方法是**奖励塑形**，使奖励变得不再稀疏。因此，稀疏奖励是我们需要进行奖励函数设计的常见原因之一。然而，这并不是唯一的原因。并非所有环境/问题都像Atari游戏那样为你预定义了奖励。此外，在某些情况下，你可能希望你的智能体实现多个目标。由于这些原因，许多现实生活中的任务需要机器教师根据他们的专业知识来指定奖励函数。接下来，我们将探讨这些情况。
- en: Sparse rewards
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏奖励
- en: When the reward is sparse, meaning that the agent sees a change in the reward
    (from a constant 0 to positive/negative, from a constant negative to positive,
    and so on) with an unlikely sequence of random actions, the learning gets difficult.
    That is because the agent needs to stumble upon this sequence through random trial
    and error, which makes the problem exploration hard.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当奖励稀疏时，意味着智能体通过一系列不太可能的随机动作，看到奖励的变化（从恒定的0变为正数/负数，从恒定的负数变为正数，等等），学习就变得困难。这是因为智能体需要通过随机试错来偶然碰到这个序列，这使得问题的探索变得困难。
- en: 'Learning chess against a competitive player where the reward is +1 for winning,
    0 for draw, and -1 for losing at the very end is a good example of an environment
    with sparse rewards. A classic example used in RL benchmarks is Montezuma''s Revenge,
    an Atari game in which the player needs to collect equipment (keys, torch, and
    so on), open doors, and so on to be able to make any progress, which is very unlikely
    just by taking random actions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与一位竞争性玩家对弈时，奖励设定为赢得比赛为+1、平局为0、输掉比赛为-1，最后的这个例子就是一个稀疏奖励环境的典型例子。强化学习基准测试中常用的经典例子是《蒙特祖玛的复仇》，这是一款Atari游戏，玩家需要收集装备（钥匙、火把等），开门等，才能取得进展，而仅凭随机行动几乎不可能完成：
- en: '![Figure 10.3 – Montezuma''s Revenge'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 《蒙特祖玛的复仇》'
- en: '](img/B14160_10_3.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_3.jpg)'
- en: Figure 10.3 – Montezuma's Revenge
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 《蒙特祖玛的复仇》
- en: In such hard exploration problems, a common strategy is **reward shaping**,
    which is to modify the reward to steer the agent toward high rewards. For example,
    a reward shaping strategy could be to give -0.1 reward to the agent learning chess
    if it loses the queen, and smaller penalties when other pieces are lost. With
    that, the machine teacher conveys their knowledge about the queen being an important
    piece in the game to the agent, although not losing the queen or any other pieces
    (except the king) in itself is not the game's objective.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的困难探索问题中，一个常见的策略是**奖励塑形**，即修改奖励，以引导智能体朝向高奖励方向。例如，奖励塑形策略可以是在学习国际象棋时，如果智能体丢掉了皇后，就给它-0.1的惩罚奖励，丢失其他棋子的惩罚也较小。通过这种方式，机器教师将关于皇后是游戏中重要棋子的知识传达给智能体，尽管丢失皇后或其他棋子（除国王外）本身并不是游戏的目标。
- en: We will talk more about reward shaping in detail later.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续部分更详细地讨论奖励塑形。
- en: Qualitative objectives
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定性目标
- en: Let's say you are trying to teach a humanoid robot how to walk. Well, what is
    walking? How can you define it? How can you define it mathematically? What kind
    of walking gets a high reward? Is it just about moving forward or are there some
    elements of aesthetics? As you can see, it is not easy to put what you have in
    your mind about walking into mathematical expressions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在试图教一个类人机器人如何走路。那么，什么是走路？你如何定义走路？如何用数学定义走路？什么样的走路会得到高奖励？仅仅是前进就够了吗，还是其中有一些美学的元素？如你所见，把你脑海中的走路概念转化为数学表达式并不容易。
- en: 'In their famous work, researchers at DeepMind used the following reward function
    for the humanoid robot they trained to walk:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的著名研究中，DeepMind 的研究人员为他们训练的类人机器人走路使用了以下奖励函数：
- en: '![](img/Formula_10_001.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_001.png)'
- en: Here, ![](img/Formula_10_002.png) and ![](img/Formula_10_003.png) are velocities
    along the ![](img/Formula_10_004.png) and ![](img/Formula_10_005.png) axes, ![](img/Formula_10_006.png)
    is the position on the ![](img/Formula_10_007.png) axis, ![](img/Formula_10_008.png)
    is a cutoff for the velocity reward, and ![](img/Formula_05_281.png) is the control
    applied on the joints. As you can see, there are many arbitrary coefficients that
    are likely to differ for other kinds of robots. In fact, the paper uses three
    separate functions for three separate robot bodies.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_10_002.png) 和 ![](img/Formula_10_003.png) 是沿着 ![](img/Formula_10_004.png)
    和 ![](img/Formula_10_005.png) 轴的速度，![](img/Formula_10_006.png) 是在 ![](img/Formula_10_007.png)
    轴上的位置，![](img/Formula_10_008.png) 是速度奖励的截止值，而 ![](img/Formula_05_281.png) 是施加在关节上的控制。正如你所看到的，这里有许多任意系数，很可能对于其他种类的机器人会有所不同。实际上，这篇论文为三种不同的机器人身体使用了三个独立的函数。
- en: Info
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'If you are curious about how the robot walks after being trained with this
    reward, watch this video: [https://youtu.be/gn4nRCC9TwQ](https://youtu.be/gn4nRCC9TwQ).
    The way the robot walks is, how can I put it, a bit weird…'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机器人在经过这种奖励训练后的走路方式感到好奇，可以观看这个视频：[https://youtu.be/gn4nRCC9TwQ](https://youtu.be/gn4nRCC9TwQ)。机器人的走路方式，怎么说呢，有点奇怪……
- en: So, in short, qualitative objectives require crafting a reward function to obtain
    the behavior intended.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，定性目标需要精心设计奖励函数，以实现预期的行为。
- en: Multi-objective tasks
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多目标任务
- en: A common situation in RL is to have multi-objective tasks. On the other hand,
    conventionally, RL algorithms optimize a scalar reward. As a result, when there
    are multiple objectives, they need to be reconciled into a single reward. This
    often results in mixing apples and oranges, and appropriately weighing them in
    the reward could be quite painful.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个常见的情况是任务是多目标的。另一方面，传统上，强化学习算法优化的是标量奖励。因此，当存在多个目标时，它们需要被调和成一个单一的奖励。这通常导致“把苹果和橙子混在一起”，而在奖励中恰当地加权这些目标可能相当困难。
- en: When the task objective is qualitative, it is often also multi-objective. For
    example, the task of driving a car includes elements of speed, safety, fuel efficiency,
    equipment wear and tear, comfort, and so on. You can guess that it is not easy
    to express what comfort means mathematically. But there are also many tasks in
    which multiple quantitative objectives need to be optimized concurrently. An example
    of this is to control an HVAC system to keep the room temperature as close to
    the specified setpoint as possible while minimizing the cost of energy. In this
    problem, it is the machine teacher's duty to balance these trade-offs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务目标是定性时，它通常也是多目标的。例如，驾驶汽车的任务包含了速度、安全性、燃油效率、设备磨损、舒适度等因素。你可以猜到，用数学表达舒适度是件不容易的事情。但也有许多任务需要同时优化多个定量目标。一个例子是控制暖通空调（HVAC）系统，使房间温度尽可能接近指定的设定点，同时最小化能源成本。在这个问题中，机器学习的任务是平衡这些权衡。
- en: It is very common for an RL task to involve one or more of the preceding situations.
    Then, engineering the reward function becomes a major challenge.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）任务中，涉及到前述一种或多种情况是非常常见的。接着，设计奖励函数成为了一个主要的挑战。
- en: After this much discussion, let's focus on reward shaping a little bit more.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过这么多讨论后，我们稍微更专注于奖励塑造（reward shaping）一点。
- en: Reward shaping
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励塑造
- en: The idea behind reward shaping is to incentivize the agent to move toward success
    states and discourage it from reaching failure states using positive and negative
    rewards that are relatively smaller in magnitude with respect to the actual reward
    (and punishment). This will usually shorten the training time as the agent will
    not spend as much time trying to discover how to reach a success state. Here is
    a simple example to make our discussion more concrete.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励塑造的背后思想是通过使用相对较小的正负奖励（相较于实际奖励和惩罚），激励智能体朝着成功状态前进，并阻止其到达失败状态。这通常会缩短训练时间，因为智能体不会花费过多时间去探索如何达到成功状态。为了让讨论更具体，这里有一个简单的例子。
- en: Simple robot example
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单机器人示例
- en: 'Suppose that a robot is moving on a horizontal axis with 0.01 step sizes. The
    goal is to reach +1 and avoid -1, which are the terminal states, as shown here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个机器人在一个水平坐标轴上以0.01的步长移动。目标是到达+1并避免-1，它们是终止状态，如下所示：
- en: '![Figure 10.4 – Simple robot example with sparse rewards'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4 – 带稀疏奖励的简单机器人示例'
- en: '](img/B14160_10_4.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_4.jpg)'
- en: Figure 10.4 – Simple robot example with sparse rewards
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 带稀疏奖励的简单机器人示例
- en: As you can imagine, it is very difficult for the robot to discover the trophy
    when we use sparse rewards, such as giving +1 for reaching the trophy and -1 for
    reaching the failure state. If there is a timeout for the task, let's say after
    200 steps, the episode is likely to end with that.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想，使用稀疏奖励时，例如为到达奖杯给+1，为到达失败状态给-1，机器人很难发现奖杯。如果任务有超时限制，比如200步后，任务可能就会结束。
- en: In this example, we can guide the robot by giving a reward that increases as
    it moves toward the trophy. A simple choice could be setting ![](img/Formula_10_010.png),
    where ![](img/Formula_10_011.png) is the position on the axis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以通过给机器人提供一个随着其朝奖杯移动而增加的奖励来引导它。一个简单的选择是设定！[](img/Formula_10_010.png)，其中！[](img/Formula_10_011.png)是坐标轴上的位置。
- en: 'There are two potential problems with this reward function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个奖励函数有两个潜在问题：
- en: As the robot moves right, the incremental relative benefit of moving even further
    right gets smaller. For example, going from ![](img/Formula_10_012.png) to ![](img/Formula_10_013.png)
    increases the step reward by 10% but going from ![](img/Formula_10_014.png) to
    ![](img/Formula_10_015.png) only increases it by 1.1%.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着机器人向右移动，继续向右移动的相对增益逐渐变小。例如，从！[](img/Formula_10_012.png)到！[](img/Formula_10_013.png)的步进奖励增加了10%，但从！[](img/Formula_10_014.png)到！[](img/Formula_10_015.png)的奖励只增加了1.1%。
- en: Since the goal of the agent is to maximize the total cumulative reward, it is
    not in the best interest of the agent to reach the trophy since that will terminate
    the episode. Instead, the agent might choose to hang out at 0.99 forever (or until
    the time limit is reached).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于智能体的目标是最大化总的累计奖励，因此，智能体最好的选择并不是去达成奖杯目标，因为那样会终止任务。相反，智能体可能会选择永远停留在0.99处（或直到时间限制到达）。
- en: 'We can address the first issue by shaping the reward in such a way that the
    agent gets increasing additional rewards for moving toward the success state.
    For example, we can set the reward to be ![](img/Formula_10_016.png) within the
    ![](img/Formula_10_017.png) range:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过塑造奖励来解决第一个问题，使得智能体朝成功状态移动时获得越来越多的附加奖励。例如，我们可以将奖励设置为！[](img/Formula_10_016.png)，范围为！[](img/Formula_10_017.png)：
- en: '![Figure 10.5 – A sample reward shaping where'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5 – 奖励塑造的示例，其中'
- en: '](img/B14160_10_5.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_5.jpg)'
- en: Figure 10.5 – A sample reward shaping where ![](img/Formula_10_018.png)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 奖励塑造的示例，其中！[](img/Formula_10_018.png)
- en: With this, the amount of incentive accelerates as the robot gets closer to the
    trophy, encouraging the robot even further to go right. The situation is similar
    for the punishment for going to left.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，随着机器人接近奖杯，激励的强度加大，进一步鼓励机器人向右移动。对于向左移动的惩罚情况也是类似的。
- en: To address the latter, we should encourage the agent to finish the episode as
    soon as possible. We need to do that by punishing the agent for every time step
    it spends in the environment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决后者，我们应鼓励智能体尽快结束任务。我们需要通过对每一步时间进行惩罚，来促使智能体尽早结束任务。
- en: 'This example shows two things: designing the reward function can get tricky
    even in such simple problems, and we need to consider the design of the reward
    function together with the terminal conditions we set.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了两点：即使在如此简单的问题中，设计奖励函数也可能变得复杂，并且我们需要将奖励函数的设计与我们设置的终止条件一起考虑。
- en: Before going into specific suggestions for reward shaping, let's also discuss
    how terminal conditions play a role in agent behavior.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入具体的奖励塑形建议之前，让我们也讨论一下终端条件在智能体行为中的作用。
- en: Terminal conditions
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终端条件
- en: Since the goal of the agent is to maximize the expected cumulative reward over
    an episode, how the episode ends will directly affect the agent's behavior. So,
    we can and should leverage a good set of terminal conditions to guide the agent.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于智能体的目标是最大化整个剧集的期望累积奖励，因此剧集的结束方式将直接影响智能体的行为。因此，我们可以并且应该利用一组好的终端条件来引导智能体。
- en: 'We can talk about several types of terminal conditions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以讨论几种类型的终端条件：
- en: '**Positive terminal** indicates the agent has accomplished the task (or part
    of it, depending on how you define success). This terminal condition comes with
    a significant positive reward to encourage the agent to reach it.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向终端**表示智能体已经完成任务（或部分任务，具体取决于您如何定义成功）。该终端条件伴随着显著的正奖励，以鼓励智能体达成目标。'
- en: '**Negative terminal** indicates a failure state and yields a significant negative
    reward. The agent will try to avoid these conditions.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负向终端**表示失败状态，并会给予显著的负奖励。智能体将尽力避免这些条件。'
- en: '**Neutral terminal** is neither success nor failure in itself, but it indicates
    that the agent has no path to success, and the episode is terminated with a zero
    reward in the last step. The machine teacher doesn''t want the agent to spend
    any time after that point in the environment but to reset back to the initial
    conditions. Although this does not directly punish the agent, it prevents it from
    collecting additional rewards (or penalties). So, it is implicit feedback to the
    agent.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中性终端**本身既不是成功也不是失败，但它表明智能体没有成功的路径，且在最后一步时，剧集会以零奖励结束。机器教师不希望智能体在此后继续在环境中待下去，而是应该重置回初始条件。虽然这不会直接惩罚智能体，但它防止了智能体收集额外的奖励（或惩罚）。因此，这是对智能体的隐性反馈。'
- en: '**Time limit** bounds the number of time steps spent in the environment. It
    encourages the agent to seek high rewards within this budget, rather than wandering
    around forever. It works as feedback about what sequences of actions are rewarding
    in a reasonable amount of time and which ones are not.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间限制**限制了在环境中所花费的时间步数。它鼓励智能体在这一时间预算内寻求高奖励，而不是永远游荡。它作为反馈，告知哪些行动序列在合理的时间内能够获得奖励，哪些则不能。'
- en: In some environments, terminal conditions are preset; but in most cases, the
    machine teacher has the flexibility to set them.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些环境中，终端条件是预设的；但在大多数情况下，机器教师具有设置终端条件的灵活性。
- en: Now that we have all the components described, let's discuss some practical
    tips for reward shaping.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了所有组件，接下来让我们讨论一些奖励塑形的实用技巧。
- en: Practical tips for reward shaping
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励塑形的实用技巧
- en: 'Here are some general guidelines you should keep in mind while designing the
    reward function:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计奖励函数时，您应该牢记以下一些一般性指导原则：
- en: Keep the step reward between ![](img/Formula_10_019.png) and ![](img/Formula_10_020.png)
    for numerical stability, whenever possible.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能保持步奖励在![](img/Formula_10_019.png)和![](img/Formula_10_020.png)之间，以保证数值稳定性。
- en: Express your reward (and state) with terms that are generalizable to other versions
    of your problem. For example, rather than rewarding the agent for reaching a point,
    ![](img/Formula_10_021.png), you can incentivize reducing the distance to the
    target, based on ![](img/Formula_10_022.png).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用可以推广到问题其他版本的术语来表达您的奖励（和状态）。例如，与其奖励智能体到达某一点！[](img/Formula_10_021.png)，不如根据![](img/Formula_10_022.png)激励其减少与目标的距离。
- en: Having a smooth reward function will provide the agent with feedback that is
    easy to follow.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有平滑的奖励函数将为智能体提供易于跟随的反馈。
- en: The agent should be able to correlate the reward with its observations. In other
    words, observations must contain some information about what is leading to high
    or low rewards. Otherwise, there won't be much for the agent to base its decisions
    on.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体应该能够将奖励与其观察结果相关联。换句话说，观察结果必须包含一些信息，说明什么行为导致了高奖励或低奖励。否则，智能体就无法根据这些信息做出决策。
- en: The total incentive for getting close to the target states should not outweigh
    the actual reward of reaching the target state. Otherwise, the agent will prefer
    to focus on accumulating the incentives, rather than achieving the actual goal.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近目标状态的总激励不应超过到达目标状态的实际奖励。否则，智能体将倾向于集中精力积累这些激励，而不是实现实际目标。
- en: If you would like your agent to complete a task as soon as possible, assign
    a negative reward to each time step. The agent will try to finish the episode
    to avoid accumulating negative rewards.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望智能体尽快完成任务，可以为每个时间步分配一个负奖励。智能体将尝试完成回合，以避免积累负奖励。
- en: If the agent can collect more positive rewards by not reaching a terminal state,
    it will try to collect them before reaching a terminal condition. If the agent
    is likely to collect only negative rewards by staying inside an episode (such
    as when there is a penalty per time step), it will try to reach a terminal condition.
    The latter can result in suicidal behavior if the life is too painful for the
    agent, meaning that the agent can seek any terminal state, including natural ones
    or failure states, to avoid incurring excessive penalties by staying alive.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果智能体能够通过不到达终止状态来收集更多的正奖励，它将尝试在达到终止条件之前收集奖励。如果智能体在一个回合内可能只会收集到负奖励（例如，每个时间步都有惩罚），它将尝试尽快到达终止状态。如果智能体的生命过于痛苦，它可能会产生自杀行为，即智能体会寻求任何终止状态，包括自然的或失败的状态，以避免因保持生命而遭受过度的惩罚。
- en: Now, we will look at an example of reward shaping using OpenAI.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将来看一个使用OpenAI进行奖励塑形的示例。
- en: Example – reward shaping for a mountain car
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 – 山地车的奖励塑形
- en: 'In OpenAI''s mountain car environment, the goal of the car is to reach the
    goal point on top of one of the hills, which is illustrated in *Figure 10.6*.
    The action space is to push the car to the left, push to the right, or apply no
    force:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI的山地车环境中，汽车的目标是到达其中一座山丘顶部的目标点，如*图10.6*所示。动作空间包括将汽车推向左、推向右或不施加任何力量：
- en: '![Figure 10.6 – Mountain car environment'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6 – 山地车环境'
- en: '](img/B14160_10_6.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_6.jpg)'
- en: Figure 10.6 – Mountain car environment
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 山地车环境
- en: Since the force we apply is not enough to climb the hill and reach the goal,
    the car needs to gradually accumulate potential energy by climbing in opposite
    directions. Figuring this out is non-trivial, because the car does not know what
    the goal is until it reaches it, which can be achieved after 100+ steps of correct
    actions. The only reward in the default environment is -1 per time step to encourage
    the car to reach the goal as quickly as possible to avoid accumulating negative
    rewards. The episode terminates after 200 steps.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们施加的力量不足以爬上山丘并到达目标，汽车需要通过朝相反方向逐渐积累势能来实现目标。弄清楚这一点并不简单，因为汽车在到达目标之前并不知道目标是什么，而这一点可以通过100多个正确动作来实现。在默认环境中，唯一的奖励是在每个时间步内给予-1，以鼓励汽车尽可能快地到达目标，从而避免积累负面奖励。该过程在200步后结束。
- en: We will use various MT techniques to train our agent throughout the chapter.
    To that end, we will have a custom training flow and a customized environment
    with which we can experiment with these methods. Let's get things set up first.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用各种MT技术来训练我们的智能体。为此，我们将有一个自定义的训练流程和一个可以用来实验这些方法的定制化环境。首先，先进行设置。
- en: Setting up the environment
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'Our custom `MountainCar` environment wraps OpenAI''s `MountainCar-v0`, which
    looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自定义`MountainCar`环境包装了OpenAI的`MountainCar-v0`，其外观如下：
- en: Chapter10/custom_mcar.py
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter10/custom_mcar.py
- en: '[PRE0]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you visit that file now, it may look complicated since it includes some add-ons
    we are yet to cover. For now, just know that this is the environment we will use.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在访问该文件，它可能看起来很复杂，因为它包含了一些我们还未讲解的附加组件。目前，只需要知道这是我们将要使用的环境。
- en: 'We will use Ray/RLlib''s Ape-X DQN throughout the chapter to train our agents.
    Make sure that you have them installed, preferably within a virtual environment:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Ray/RLlib的Ape-X DQN来训练我们的智能体。请确保已安装这些工具，最好是在虚拟环境中：
- en: '[PRE1]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With that, next, let's get a baseline performance by training an agent without
    any MT.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，接下来让我们通过训练一个不使用任何MT的智能体来获取基线表现。
- en: Getting a baseline performance
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取基线表现
- en: 'We will use a single script for all of the training. We define a `STRATEGY`
    constant at the top of the script, which will control the strategy to be used
    in the training:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个脚本进行所有的训练。在脚本的顶部，我们定义一个`STRATEGY`常量，用于控制训练中使用的策略：
- en: Chapter10/mcar_train.py
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter10/mcar_train.py
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For each strategy, we will kick off five different training sessions of 2 million
    time steps each, so we set `NUM_TRIALS = 5` and `MAX_STEPS = 2e6`. At the end
    of each training session, we will evaluate the trained agent over `NUM_FINAL_EVAL_EPS
    = 20` episodes. Therefore, the result for each strategy will reflect the average
    length of 100 test episodes, where a lower number indicates better performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种策略，我们将启动五次不同的训练，每次训练 200 万时间步，因此我们设置 `NUM_TRIALS = 5` 和 `MAX_STEPS = 2e6`。在每次训练结束时，我们将对训练好的代理进行评估，共进行
    `NUM_FINAL_EVAL_EPS = 20` 次回合评估。因此，每种策略的结果将反映 100 个测试回合的平均长度，较低的数字表示更好的表现。
- en: 'For most of the strategies, you will see that we have two variants: with and
    without dueling networks enabled. When the dueling network is enabled, the agent
    achieves a near-optimal result (around 100 steps to reach the goal), so it becomes
    uninteresting for our case. Moreover, when we implement action masking later in
    the chapter, we won''t use dueling networks to avoid complexities in RLlib. Therefore,
    we will focus on the no-dueling network case in our example. Finally, note that
    the results of the experiments will be written to `results.csv`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数策略，你会看到我们有两种变体：启用和不启用对战网络。当启用对战网络时，代理会取得接近最优的结果（大约 100 步达到目标），因此对于我们的案例来说变得不再有趣。此外，当我们在本章后续实现动作掩蔽时，为了避免
    RLlib 中的复杂性，我们将不会使用对战网络。因此，在我们的示例中，我们将重点关注不使用对战网络的情况。最后，请注意，实验结果将写入 `results.csv`
    文件。
- en: 'With that, let''s train our first agents. When I used no MT, in my case, I
    obtained the following average episode lengths:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就开始训练我们的第一个代理。当我没有使用 MT 时，得到的平均回合长度如下：
- en: '[PRE3]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The outcome is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's see next whether reward shaping helps us here.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看奖励塑造是否能在这里帮助我们。
- en: Solving the problem with a shaped reward
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过奖励塑造解决问题
- en: 'Anyone looking at the mountain car problem could tell that we should encourage
    the car to go right, at least eventually. In this section, that is what we''ll
    do. The dip position of the car corresponds to an ![](img/Formula_10_023.png)
    position of -0.5\. We modify the reward function to give a quadratically increasing
    reward to the agent for going beyond this position toward the right. This happens
    inside the custom `MountainCar` environment:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人看到山地车问题时都会明白，我们应该鼓励汽车向右行驶，至少最终如此。在本节中，我们就是这样做的。汽车的下沉位置对应于 ![](img/Formula_10_023.png)
    的 -0.5 位置。我们修改奖励函数，为代理提供一个二次增长的奖励，当它越过该位置向右行驶时。这发生在自定义的 `MountainCar` 环境中：
- en: '[PRE5]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Of course, feel free to try your own reward shaping here to gain a better idea.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，欢迎在这里尝试自己定义奖励塑造，以获得更好的理解。
- en: Tip
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Applying a constant reward, such as -1, is an example of sparse reward, although
    the step reward is not 0\. That is because the agent does not get any feedback
    until the very end of the episode; none of its actions change the default reward
    for a long time.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 应用恒定奖励，例如 -1，是稀疏奖励的一个例子，尽管步长奖励不是 0。之所以如此，是因为代理在整个回合结束前没有任何反馈；其行动在长时间内不会改变默认奖励。
- en: 'We can enable the custom (shaped) reward strategy with the following flag:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下标志启用自定义（塑造的）奖励策略：
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The outcome we get is something like the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果大致如下：
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is obviously a significant gain, so kudos to our shaped reward function!
    Admittedly, though, it took me several iterations to figure out something that
    brings in significant improvement. This is because the behavior we encourage is
    more complex than just going right: we want the agent to go between left and right
    to speed up. This is a bit hard to capture in a reward function, and it could
    easily give you headaches.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一个显著的收益，因此我们的奖励塑造函数值得赞扬！不过，诚然，我花了几次迭代才找到能够带来显著改进的方式。这是因为我们鼓励的行为比单纯地向右行驶更复杂：我们希望代理在左右之间来回移动，以加速。这在奖励函数中有点难以捕捉，且很容易让人头疼。
- en: Reward function engineering in general can get quite tricky and time-consuming
    – so much so that this topic deserves a dedicated section to discuss, which we
    will turn to next.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数工程通常会变得相当棘手且耗时——这让这个话题值得专门的章节来讨论，我们接下来将深入探讨。
- en: Challenges with engineering the reward function
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励函数工程的挑战
- en: The objective of RL is to find a policy that maximizes the expected cumulative
    reward the agent collects. We design and use very sophisticated algorithms to
    overcome this optimization challenge. In some problems, we use billions of training
    samples to this end and try to squeeze out a little extra reward. After all this
    hassle, it is not uncommon to observe that your agent obtains a great reward,
    but the behavior it exhibits is not exactly what you intended. In other words,
    the agent learns something different than what you want it to learn. If you run
    into such a situation, don't get too mad. That is because the agent's sole purpose
    is to maximize the reward you specified. If that reward does not exactly reflect
    the objective you had in mind, which is much more challenging than you may think,
    neither will the behavior of the agent.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到一个策略，最大化代理所收集的期望累计奖励。我们设计并使用非常复杂的算法来克服这一优化挑战。在某些问题中，我们使用数十亿个训练样本来实现这一目标，并尽力挤出一点额外的奖励。经过这一切折腾后，观察到你的代理获得了很高的奖励，但它展示的行为并不是你所期望的，这并不罕见。换句话说，代理学到的东西与你希望它学到的不同。如果你遇到这种情况，不要太生气。因为代理的唯一目的是最大化你指定的奖励。如果这个奖励并不能准确反映你心中的目标，这比你想象的要复杂得多，那么代理的行为也不会符合你的预期。
- en: Info
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'A famous example of a misbehaving agent due to an incorrectly specified reward
    is OpenAI''s CoastRunners agent. In the game, the agent is expected to finish
    the boat race as quickly as possible while collecting rewards along the way. After
    training, the agent figured out a way of collecting higher rewards without having
    to finish the race, defeating the original purpose. You can read more about it
    on OpenAI''s blog: [https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于奖励函数指定错误，OpenAI的CoastRunners代理是一个著名的行为异常例子。在游戏中，代理的目标是尽可能快地完成船只竞速，同时沿途收集奖励。经过训练后，代理找到了一种收集更多奖励而不必完成比赛的方法，这违背了原始目的。你可以在OpenAI的博客中阅读更多关于它的信息：[https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/)。
- en: As a result, it is of tremendous importance that you specify a good reward function
    for your task, especially when it includes qualitative and/or complex objectives.
    Unfortunately, designing a good reward function is more of an art than science,
    and you will gain intuition through practice and trial and error.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为你的任务指定一个好的奖励函数是至关重要的，尤其是当它包括定性和/或复杂的目标时。不幸的是，设计一个好的奖励函数更多的是一种艺术而非科学，你将通过实践和试错获得直觉。
- en: Tip
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'Alex Irpan, a machine learning researcher at Google, beautifully expresses
    how important and challenging designing the reward function is: "I''ve taken to
    imagining deep RL as a demon that''s deliberately misinterpreting your reward
    and actively searching for the laziest possible local optima. It''s a bit ridiculous,
    but I''ve found it''s actually a productive mindset to have." (Irpan, 2018). François
    Chollet, the author of Keras, says "loss function engineering is probably going
    to be a job title in the future." ([https://youtu.be/Bo8MY4JpiXE?t=5270](https://youtu.be/Bo8MY4JpiXE?t=5270)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的机器学习研究员Alex Irpan优美地表达了奖励函数设计的重要性和挑战：“我开始把深度强化学习想象成一个恶魔，它故意误解你的奖励并积极寻找最懒的局部最优解。这有点荒谬，但我发现这实际上是一种富有成效的思维方式。”（Irpan,
    2018）。Keras的作者François Chollet说：“损失函数工程可能会成为未来的职业头衔。”（[https://youtu.be/Bo8MY4JpiXE?t=5270](https://youtu.be/Bo8MY4JpiXE?t=5270)）。
- en: Despite these challenges, the tricks we have just covered should give you a
    running start. The rest will come with experience.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，我们刚才讨论的技巧应该能给你一个良好的开端。其余的将随着经验积累而到来。
- en: With that, let's conclude our discussion on reward function engineering. This
    was a long yet necessary one. In the next section, we will discuss another topic,
    curriculum learning, which is important not just in MT but in RL as a whole.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们的奖励函数工程讨论就到此为止。这是一个漫长但必要的讨论。在下一节中，我们将讨论另一个主题——课程学习，这不仅在MT中重要，在强化学习中也是如此。
- en: Curriculum learning
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程学习
- en: When we learn a new skill, we start with the basics. Bouncing and dribbling
    are the first steps when learning basketball. Doing alley-oops is not something
    to try to teach in the first lesson. You need to gradually proceed to advanced
    lessons after getting comfortable with the earlier ones. This idea of following
    a curriculum, from the basics to advanced levels, is the basis of the whole education
    system. The question is whether machine learning models can benefit from the same
    approach. It turns out that they can!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们学习一项新技能时，我们从基础开始。学习篮球时，弹跳和运球是第一步。做空中接力不是第一课就该尝试的内容。你需要在掌握了前面的基本技能后，逐步进入更高级的课程。按照从基础到高级的课程体系学习，这也是整个教育系统的基础。问题是，机器学习模型是否也能从这种方法中获益？结果证明，确实可以！
- en: In the context of RL, when we create a curriculum, we similarly start with "easy"
    environment configurations for the agent. This way, the agent can get an idea
    about what success means early on, rather than spending a lot of time blindly
    exploring the environment with the hope of stumbling upon success. We then gradually
    increase the difficulty if we observe that the agent is exceeding a certain reward
    threshold. Each of these difficulty levels is considered a **lesson**. Curriculum
    learning has been shown to increase training efficiency and makes tasks that are
    infeasible to achieve feasible for the agent.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）的背景下，当我们创建课程时，我们类似地从“简单”的环境配置开始。这种方式可以让智能体尽早了解什么是成功，而不是花费大量时间盲目探索环境，希望能偶然找到成功的办法。如果我们观察到智能体超过了某个奖励阈值，我们就会逐步增加难度。这些难度级别中的每一个都被视为一个**课程**。已经证明，课程学习能提高训练效率，并使那些对智能体来说无法实现的任务变得可行。
- en: Tip
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Designing lessons and transition criteria is a non-trivial undertaking. It requires
    significant thought and subject matter expertise. Although we follow manual curriculum
    design in this chapter, when we revisit the topic in [*Chapter 11*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239),
    *Generalization and Partial Observability*, and [*Chapter 14*](B14160_14_Final_SK_ePub.xhtml#_idTextAnchor306),
    *Robot Learning*, we will discuss automatic curriculum generation methods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 设计课程和过渡标准是一项复杂的工作。它需要深思熟虑和领域专业知识。尽管我们在本章中遵循手动课程设计，但当我们在[*第 11 章*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239)《泛化与部分可观察性》和[*第
    14 章*](B14160_14_Final_SK_ePub.xhtml#_idTextAnchor306)《机器人学习》中重新讨论该主题时，我们将讨论自动课程生成方法。
- en: In our mountain car example, we create lessons by modifying the initial conditions
    of the environment. Normally, as the episode beginnings, the environment randomizes
    the car position around the valley dip (![](img/Formula_10_024.png)) and sets
    the velocity (![](img/Formula_08_050.png)) to 0\. In our curriculum, the car will
    start in the first lesson close to the goal and with a high velocity toward the
    right. With that, it will easily reach the goal. We will make things gradually
    closer to the original difficulty as the curriculum progresses.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的山地车示例中，我们通过修改环境的初始条件来创建课程。通常，当剧集开始时，环境会随机化汽车在山谷底部的位置（![](img/Formula_10_024.png)），并将速度（![](img/Formula_08_050.png)）设置为
    0。在我们的课程中，汽车将在第一课中靠近目标并朝右以较高的速度启动。这样，它将很容易到达目标。随着课程的进展，我们将逐渐将难度恢复到原本的状态。
- en: 'Specifically, here is how we define the lessons:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，下面是我们如何定义课程的：
- en: '**Lesson 0**: ![](img/Formula_10_026.png), ![](img/Formula_10_027.png)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**课程 0**: ![](img/Formula_10_026.png), ![](img/Formula_10_027.png)'
- en: '**Lesson 1**: ![](img/Formula_10_028.png), ![](img/Formula_10_029.png)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**课程 1**: ![](img/Formula_10_028.png), ![](img/Formula_10_029.png)'
- en: '**Lesson 2**: ![](img/Formula_10_030.png), ![](img/Formula_10_031.png)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**课程 2**: ![](img/Formula_10_030.png), ![](img/Formula_10_031.png)'
- en: '**Lesson 3**: ![](img/Formula_10_032.png), ![](img/Formula_10_033.png)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**课程 3**: ![](img/Formula_10_032.png), ![](img/Formula_10_033.png)'
- en: '**Lesson 4 (final / original)**: ![](img/Formula_10_034.png), ![](img/Formula_10_035.png)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**课程 4（最终版 / 原版）**: ![](img/Formula_10_034.png), ![](img/Formula_10_035.png)'
- en: 'This is how it is set inside the environment:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何在环境中设置的：
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will let the agent proceed to the next lesson once it has been successful
    enough in the current one. We define this threshold as having an average episode
    length of less than 150 over 10 evaluation episodes. We set the lessons in training
    and the evaluation workers with the following functions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦智能体在当前课程中取得了足够的成功，我们将允许它进入下一个课程。我们将这一成功标准定义为在 10 次评估剧集中，平均剧集长度小于 150。我们在训练和评估工作中使用以下功能来设置课程：
- en: '[PRE9]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'These are then used inside the training flow:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置随后会被用于训练流程中：
- en: '[PRE10]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, this is how we implement a manual curriculum. Say you train the agent with
    this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是我们如何实现手动课程安排。假设你用这个方法训练智能体：
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You will see that the performance we get is near optimal!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，我们得到的性能接近最优！
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You just taught a machine something using a curriculum! Pretty cool, isn't it?
    Now it should feel like MT!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚通过课程安排教会了机器某些东西！挺酷的，对吧？现在它应该感觉像是 MT 了！
- en: 'Next, we will look at another interesting approach: MT using demonstrations.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看另一种有趣的方法：使用示范的 MT 方法。
- en: Warm starts and demonstration learning
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 热启和示范学习
- en: 'A popular technique to demonstrate to the agent a way to success is to train
    it on data that is coming from a reasonably successful controller, such as humans.
    In RLlib, this can be done by saving the human play data from the mountain car
    environment:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的技巧是通过训练智能体使用来自合理成功控制器（如人类）的数据，来向其展示成功的方式。在 RLlib 中，可以通过保存来自山地车环境的人类游戏数据来实现：
- en: Chapter10/mcar_demo.py
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter10/mcar_demo.py
- en: '[PRE13]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This data can then be fed to the training, which is implemented in `Chapter10/mcar_train.py`.
    When I tried it, RLlib got stuck with NaNs in multiple attempts when the training
    was seeded using this method. So, now that you know about it, we will leave the
    details of this to RLlib's documentation at [https://docs.ray.io/en/releases-1.0.1/rllib-offline.html](https://docs.ray.io/en/releases-1.0.1/rllib-offline.html)
    and not focus on it here.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据然后可以输入到训练中，该训练实现于 `Chapter10/mcar_train.py`。当我尝试时，RLlib 在多次尝试中都在训练时由于使用这种方法进行初始化时遇到
    NaN 值而停滞。因此，现在你知道了这点，我们将把这个细节留给 RLlib 的文档：[https://docs.ray.io/en/releases-1.0.1/rllib-offline.html](https://docs.ray.io/en/releases-1.0.1/rllib-offline.html)，而不是在这里详细讨论。
- en: Action masking
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作屏蔽
- en: 'One final MT approach we will use is action masking. With that, we can prevent
    the agent from taking certain actions in certain steps based on conditions we
    define. For the mountain car, assume that we have this intuition of building momentum
    before trying to climb the hill. So, we want the agent to apply force to the left
    if the car is already moving left around the valley. So, for these conditions,
    we will mask all the actions except left:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的最后一种 MT 方法是动作屏蔽（action masking）。通过这种方法，我们可以根据我们定义的条件，防止智能体在某些步骤中执行特定的动作。对于山地车问题，假设我们有一个直觉：在尝试爬坡之前需要积累动能。因此，如果汽车已经在山谷周围向左移动，我们希望智能体施加左侧的力。那么，在这些条件下，我们将屏蔽所有动作，除了左移：
- en: '[PRE14]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to be able to use this masking, we need to build a custom model. For
    the masked actions, we push down all the logits to negative infinity:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用这个屏蔽，我们需要构建一个自定义模型。对于被屏蔽的动作，我们将所有的 logits 推向负无穷大：
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, when using this model, we turn off dueling to avoid an overly complicated
    implementation. Also, we register our custom model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用这个模型时，我们关闭对抗网络，以避免过于复杂的实现。此外，我们注册我们的自定义模型：
- en: '[PRE16]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In order to train your agent with this strategy, set the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用这种策略训练智能体，请设置以下内容：
- en: '[PRE17]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The performance will be as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 性能将如下所示：
- en: '[PRE18]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is definitely an improvement over the default case, yet it is behind the
    reward shaping and curriculum learning approaches. Having smarter masking conditions
    and adding dueling networks can further help with the performance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这肯定比默认情况下有所改进，但它仍落后于奖励塑形和课程学习方法。更智能的屏蔽条件和添加对抗网络可以进一步提高性能。
- en: This is the end of the MT techniques we use for the mountain car problem. Before
    we wrap up, let's check one more important topic in MT.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在山地车问题中使用的 MT 技巧的结束部分。在总结之前，我们再看一个 MT 中的重要话题。
- en: Concept networks
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念网络
- en: An important part of the MT approach is to divide the problem into concepts
    that correspond to different skills to facilitate learning. For example, for an
    autonomous car, training separate agents for cruising on a highway and passing
    a car could help with performance. In some problems, divisions between concepts
    are even more clear. In those cases, training a single agent for the entire problem
    will often result in better performance.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: MT 方法的一个重要部分是将问题划分为对应不同技能的概念，以便促进学习。例如，对于自动驾驶汽车，为高速公路上的巡航和超车训练不同的智能体可能有助于提高性能。在某些问题中，概念之间的划分更加明确。在这些情况下，为整个问题训练一个单一的智能体通常会带来更好的表现。
- en: Before closing this chapter, let's talk about some of the potential downsides
    of the MT approach.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束这一章之前，我们来谈谈 MT 方法的一些潜在缺点。
- en: Downsides and the promises of MT
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MT 的缺点与前景
- en: There are two potential downsides of the MT approach.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: MT 方法有两个潜在的缺点。
- en: First, it is usually non-trivial to come up with good reward shaping, a good
    curriculum, a set of action masking conditions, and so on. This also in some ways
    defeats the purposes of learning from experience and not having to do feature
    engineering. On the other hand, whenever we are able to do so, feature engineering
    and MT could be immensely helpful for the agent to learn and increase its data
    efficiency.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通常很难提出好的奖励塑造、好的课程设计、一组动作屏蔽条件等等。这在某些方面也违背了从经验中学习、无需进行特征工程的初衷。另一方面，凡是我们能够做到的，特征工程和MT在帮助智能体学习并提高数据效率方面可能非常有帮助。
- en: Second, when we adopt a MT approach, it is possible to inject the bias of the
    teacher to the agent, which could prevent it from learning better strategies.
    The machine teacher needs to avoid such biases whenever possible.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当我们采用MT方法时，可能会将教师的偏见传递给智能体，这可能会阻碍其学习更好的策略。机器教师需要尽量避免这种偏见。
- en: Awesome job! We have reached the end of an exciting chapter here. Let's summarize
    what we have covered next.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们已经完成了这一激动人心章节的内容。接下来，让我们总结一下我们所覆盖的内容。
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we covered an emerging paradigm in artificial intelligence,
    MT, which is about effectively conveying the expertise of a subject matter expert
    (teacher) to machine learning algorithms. We discussed how this is similar to
    how humans are educated: by building on others'' knowledge, usually without reinventing
    it. The advantage of this approach is that it greatly increases data efficiency
    in machine learning, and, in some cases, makes learning possible that would have
    been impossible without a teacher. We discussed various methods in this paradigm,
    including reward function engineering, curriculum learning, demonstration learning,
    action masking, and concept networks. We observed how some of these methods have
    improved vanilla use of Ape-X DQN significantly. At the end, we also introduced
    potential downsides of this paradigm, namely the difficulty of designing the teaching
    process and tools, and possible bias introduced into learning. Despite these downsides,
    MT will become a standard part of an RL scientist''s toolbox in the near future.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了人工智能中的一个新兴范式——MT，它涉及将学科专家（教师）的专业知识有效传递给机器学习算法。我们讨论了这与人类教育的相似之处：通过在他人知识的基础上建立，通常不需要重新发明这些知识。这种方法的优点是极大提高了机器学习中的数据效率，并且在某些情况下，使得没有教师的情况下本不可能实现的学习变得可能。我们讨论了该范式中的各种方法，包括奖励函数工程、课程学习、示范学习、动作屏蔽和概念网络。我们观察到这些方法中的一些显著提高了Ape-X
    DQN的基本应用。最后，我们还介绍了这一范式的潜在弊端，即教学过程和工具的设计难度，以及可能引入的偏见。尽管有这些弊端，MT将在不久的将来成为强化学习科学家工具箱中的标准组成部分。
- en: In the next chapter, we will discuss generalization and partial observability,
    a key topic in RL. In doing so, we will visit curriculum learning again and see
    how it helps in creating robust agents.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论泛化和部分可观察性，这是强化学习中的一个关键主题。在这个过程中，我们将再次回顾课程学习，并看看它如何帮助创建鲁棒的智能体。
- en: See you on the other side!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一面见！
- en: References
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bonsai. (2017). *Deep Reinforcement Learning Models: Tips & Tricks for Writing
    Reward Functions*. Medium. URL: [https://bit.ly/33eTjBv](https://bit.ly/33eTjBv)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonsai. (2017). *深度强化学习模型：奖励函数编写技巧与窍门*。Medium。网址：[https://bit.ly/33eTjBv](https://bit.ly/33eTjBv)
- en: 'Weng, L. (2020). *Curriculum for Reinforcement Learning*. Lil''Log. URL: [https://bit.ly/39foJvE](https://bit.ly/39foJvE)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng, L. (2020). *强化学习课程设计*。Lil'Log。网址：[https://bit.ly/39foJvE](https://bit.ly/39foJvE)
- en: 'OpenAI. (2016). *Faulty Reward Functions in the Wild*. OpenAI blog. URL: [https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI. (2016). *野外中的有缺陷奖励函数*。OpenAI博客。网址：[https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/)
- en: 'Irpan, A. (2018). *Deep Reinforcement Learning Doesn''t Work Yet*. Sorta Insightful.
    URL: [https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irpan, A. (2018). *深度强化学习尚未成功*。Sorta Insightful。网址：[https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)
- en: 'Heess, N. et al. (2017). *Emergence of Locomotion Behaviours in Rich Environments*.
    arXiv.org. URL: [http://arxiv.org/abs/1707.02286](http://arxiv.org/abs/1707.02286)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess, N. 等. (2017). *在丰富环境中运动行为的出现*。arXiv.org。网址：[http://arxiv.org/abs/1707.02286](http://arxiv.org/abs/1707.02286)
- en: 'Bonsai. (2017). *Writing Great Reward Functions* – Bonsai. YouTube. URL: [https://youtu.be/0R3PnJEisqk](https://youtu.be/0R3PnJEisqk)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonsai. (2017). *编写优秀的奖励函数* – Bonsai。YouTube。网址：[https://youtu.be/0R3PnJEisqk](https://youtu.be/0R3PnJEisqk)
- en: 'Badnava, B. & Mozayani, N. (2019). *A New Potential-Based Reward Shaping for
    Reinforcement Learning Agent*. arXiv.org. URL: [http://arxiv.org/abs/1902.06239](http://arxiv.org/abs/1902.06239)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Badnava, B. & Mozayani, N. (2019). *一种基于潜力的新型奖励塑形方法用于强化学习代理*. arXiv.org. URL:
    [http://arxiv.org/abs/1902.06239](http://arxiv.org/abs/1902.06239)'
- en: 'Microsoft Research. (2019). *Reward Machines: Structuring Reward Function Specifications
    and Reducing Sample Complexity*. YouTube. URL: [https://youtu.be/0wYeJAAnGl8](https://youtu.be/0wYeJAAnGl8)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '微软研究院. (2019). *奖励机器：构建奖励函数规范并减少样本复杂度*. YouTube. URL: [https://youtu.be/0wYeJAAnGl8](https://youtu.be/0wYeJAAnGl8)'
- en: 'US Government Finances. (2020). URL: [https://www.usgovernmentspending.com/](https://www.usgovernmentspending.com/)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '美国政府财政. (2020). URL: [https://www.usgovernmentspending.com/](https://www.usgovernmentspending.com/)'
- en: 'The AlphaStar team. (2019). *AlphaStar: Mastering the Real-Time Strategy Game
    StarCraft II*. DeepMind blog. URL: [https://bit.ly/39fpDIy](https://bit.ly/39fpDIy)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AlphaStar团队. (2019). *AlphaStar：掌握即时战略游戏《星际争霸 II》*. DeepMind 博客. URL: [https://bit.ly/39fpDIy](https://bit.ly/39fpDIy)'
