- en: 4\. Evaluating Your Model with Cross-Validation Using Keras Wrappers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 使用 Keras 包装器进行交叉验证评估模型
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces you to building Keras wrappers with scikit-learn. You
    will learn to apply cross-validation to evaluate deep learning models, and create
    user-defined functions to implement deep learning models along with cross-validation.
    By the end of this chapter, you will be able to build robust models that perform
    as well on new, unseen data as they do on the trained data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何使用 scikit-learn 构建 Keras 包装器。你将学习如何应用交叉验证来评估深度学习模型，并创建用户定义的函数来实现深度学习模型及其交叉验证。到本章结束时，你将能够构建出在新数据和训练数据上表现一致、且稳健的模型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, we experimented with different neural network architectures.
    We were able to evaluate the performance of the different models by observing
    the loss and accuracy during the course of the training process. This helped us
    determine when the model was underfitting or overfitting the training data and
    how to use techniques such as early stopping to prevent overfitting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们实验了不同的神经网络架构。通过观察训练过程中损失值和准确率的变化，我们能够评估不同模型的表现。这帮助我们判断模型是否存在欠拟合或过拟合问题，并学习如何使用如提前停止等技术来防止过拟合。
- en: In this chapter, you will learn about **cross-validation**. This is a **resampling
    technique** that leads to a very accurate and robust estimation of a model's performance,
    in comparison to the model evaluation approaches we discussed in the previous
    chapters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习**交叉验证**。这是一种**重采样技术**，相较于我们在前几章讨论的模型评估方法，它能提供更加准确和稳健的模型性能估计。
- en: This chapter starts with an in-depth discussion about why we need to use cross-validation
    for model evaluation, the underlying basics of cross-validation, its variations,
    and a comparison between them. Next, we will implement cross-validation on Keras
    deep learning models. We will also use Keras wrappers with scikit-learn to allow
    Keras models to be treated as estimators in a scikit-learn workflow. You will
    then learn how to implement cross-validation in scikit-learn and finally bring
    it all together and perform cross-validation using scikit-learn on Keras deep
    learning models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从深入讨论为什么需要使用交叉验证进行模型评估开始，讲解交叉验证的基本原理、变种以及它们之间的比较。接下来，我们将对 Keras 深度学习模型进行交叉验证的实现。我们还将结合
    scikit-learn 使用 Keras 包装器，使得 Keras 模型能够作为估计器在 scikit-learn 的工作流中进行处理。然后，你将学习如何在
    scikit-learn 中实现交叉验证，最终将所有内容结合起来，在 scikit-learn 中对 Keras 深度学习模型进行交叉验证。
- en: Lastly, you will learn about how to use cross-validation to perform more than
    just model evaluation, and how a cross-validation estimation of model performance
    can be used to compare different models and select the one that results in the
    best performance on a particular dataset. You will also use cross-validation to
    improve the performance of a given model by finding the best set of hyperparameters
    for it. We will implement the concepts that we will learn about in this chapter
    in three activities, each involving a real-life dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将学习如何利用交叉验证不仅仅进行模型评估，还可以利用交叉验证对模型性能进行估计，从而比较不同模型，并选择在特定数据集上表现最好的模型。你还将使用交叉验证来提高给定模型的表现，通过寻找最佳超参数集合。我们将在三项活动中实现本章所学的概念，每项活动都涉及一个真实数据集。
- en: Cross-Validation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: '`Resampling techniques` are an important group of techniques in statistical
    data analysis. They involve repeatedly drawing samples from a dataset to create
    the training set and the test set. At each repetition, they fit and evaluate the
    model using the samples drawn from the dataset for the training set and the test
    set at that repetition.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`重采样技术`是统计数据分析中的一组重要技术。它们涉及反复从数据集中抽取样本来创建训练集和测试集。在每次重复中，使用从数据集中抽取的样本来训练和评估模型。'
- en: Using these techniques can provide us with information about the model that
    is otherwise not obtainable by fitting and evaluating the model only once, using
    one training set and one test set. Since resampling methods involve fitting a
    model to the training data several times, they are computationally expensive.
    Therefore, when it comes to deep learning, we only implement them in the cases
    where the dataset and the network are relatively small, and the available computational
    power allows us to do so.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些技术可以为我们提供模型的信息，这是通过仅使用一个训练集和一个测试集来拟合和评估模型无法获得的。由于重采样方法涉及多次对训练数据拟合模型，因此计算开销较大。因此，在深度学习中，我们仅在数据集和网络相对较小，并且可用计算能力允许时，才会实现这些方法。
- en: In this section, you will learn about a very important resampling method called
    `cross-validation`. Cross-validation is one of the most important and most commonly
    used resampling methods. It computes the best estimation of model performance
    on new, unseen examples when given a limited dataset. We will also explore the
    basics of cross-validation, its two variations, and a comparison between them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习一种非常重要的重采样方法——`交叉验证`。交叉验证是最重要和最常用的重采样方法之一。它计算了在给定有限数据集的情况下，模型在新未见过的样本上的最佳性能估计。我们还将探讨交叉验证的基础知识，它的两种变体以及它们之间的比较。
- en: Drawbacks of Splitting a Dataset Only Once
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅进行一次数据集划分的缺点
- en: 'In the previous chapter, we mentioned that evaluating a model on the same dataset
    that''s used to train the model is a methodological mistake. Since the model has
    been trained to reduce the error on this particular set of examples, its performance
    on it is highly biased. That is why the error rate on training data is always
    an underestimation of the error rate on new examples. We learned that one way
    to solve this problem is to randomly hold out a subset of the data as a test set
    for evaluation and fit the model on the rest of the data, which is called the
    training set. An illustration of this approach can be seen in the following image:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们提到过，在用于训练模型的同一数据集上评估模型是一个方法上的错误。由于模型已经训练以减少这个特定数据集上的误差，因此它在该数据集上的表现具有很强的偏倚性。这就是为什么训练数据上的误差率总是低估新样本上的误差率的原因。我们了解到，解决这个问题的一种方法是随机地从数据中留出一部分作为测试集进行评估，并在其余数据上拟合模型，这部分数据被称为训练集。以下图展示了这种方法的示例：
- en: '![Figure 4.1: Overview of training set/test set split'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1：训练集/测试集划分概述'
- en: '](img/B15777_04_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_01.jpg)'
- en: 'Figure 4.1: Overview of training set/test set split'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：训练集/测试集划分概述
- en: As we mentioned previously, assigning the data to either the training set or
    the test set is completely random. This means that if we repeat this process,
    different data will be assigned to the test set and training set each time. The
    test error rate that's reported by this approach can vary a lot, depending on
    which examples are in the test set and which examples are in the training set.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，将数据分配到训练集或测试集完全是随机的。这意味着如果我们重复此过程，每次分配给测试集和训练集的数据会有所不同。通过这种方法报告的测试误差率可能会有所不同，这取决于哪些样本被分配到测试集，哪些样本被分配到训练集。
- en: '**Example**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**'
- en: 'Let''s look at an example. Here, we have built a single-layer neural network
    for the hepatitis C dataset that you saw in *Activity 3.02*, *Advanced Fibrosis
    Diagnosis with Neural Networks* in *Chapter 3*, *Deep Learning with Keras*. We
    used the training set/test set approach to compute the test error associated with
    this model. Instead of splitting and training only once, if we split the data
    into five separate datasets and repeated this process five times, we might expect
    five different plots for the test error rates. The test error rates for each of
    these five experiments can be seen in the following plot:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。在这里，我们为你在*活动 3.02*、*使用神经网络进行高级纤维化诊断*中的*第 3 章*、*使用 Keras 的深度学习*部分看到的丙型肝炎数据集构建了一个单层神经网络。我们使用了训练集/测试集的方法来计算与此模型相关的测试误差。与一次性划分和训练不同，如果我们将数据划分为五个独立的数据集，并重复此过程五次，我们可能会得到五个不同的测试误差率图。以下图表展示了这五次实验的测试误差率：
- en: '![Figure 4.2: Plot of test error rates with five different training set/test
    set splits on an example dataset'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：在一个示例数据集上，使用五种不同的训练集/测试集划分绘制的测试误差率图'
- en: '](img/B15777_04_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_02.jpg)'
- en: 'Figure 4.2: Plot of test error rates with five different training set/test
    set splits on an example dataset'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：在一个示例数据集上，使用五种不同的训练集/测试集划分绘制的测试误差率图
- en: As you can see, the test error rate is quite different in each experiment. This
    variation in the models' evaluation results indicates that the simple strategy
    of splitting the dataset into a training set and a test set only once may not
    lead to a robust and accurate estimation of the model's performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每次实验中的测试误差率差异很大。模型评估结果的这种变化表明，仅仅将数据集分为训练集和测试集一次的简单策略可能无法提供对模型性能的稳健且准确的估计。
- en: 'To summarize, the training set/test set approach that we learned about in the
    previous chapter has the obvious advantage of being simple, easy to implement,
    and computationally inexpensive. However, it has drawbacks too, which are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们在上一章学习的训练集/测试集方法有一个明显的优点，那就是简单、易于实现且计算成本低。然而，它也有一些缺点，具体如下：
- en: The first drawback is that its estimation of the model's error rate strongly
    depends on exactly which data is assigned to the test set and which data is assigned
    to the training set.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个缺点是，它对模型误差率的估计在很大程度上依赖于到底是哪些数据被分配到测试集，哪些数据被分配到训练集。
- en: The second drawback is that, in this approach, we are only training the model
    on a subset of the data. Machine learning models tend to perform worse when they
    are trained using a small amount of data.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个缺点是，在这种方法中，我们只在数据的一个子集上训练模型。当使用较少数据进行训练时，机器学习模型的表现通常会更差。
- en: 'Since the performance of a model can be improved by training it on the entire
    dataset, we are always looking for ways to include all the available data points
    in training. Additionally, we are interested in finding a robust estimation of
    the model''s performance by including all the available data points in the evaluation.
    These objectives can be accomplished with the use of cross-validation techniques.
    The following are the two methods of cross-validation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的性能可以通过在整个数据集上训练来提高，我们始终在寻找将所有可用数据点纳入训练的方法。此外，我们还希望通过将所有可用数据点纳入评估来获得对模型性能的稳健估计。这些目标可以通过使用交叉验证技术来实现。以下是两种交叉验证方法：
- en: '**K-fold cross-validation**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K折交叉验证**'
- en: '**Leave-one-out cross-validation**'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**留一法交叉验证**'
- en: K-Fold Cross-Validation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: In `k-fold cross-validation`, instead of dividing the dataset into two subsets,
    we divide the dataset into `k` approximately equal-sized subsets or folds. In
    the first iteration of the method, the first fold is considered a test set. The
    model is trained on the remaining `k-1` folds, and then it is evaluated on the
    first fold (the first fold is used to estimate the test error rate).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在`k`折交叉验证中，我们不是将数据集划分为两个子集，而是将数据集划分为`k`个大小相近的子集或折叠。在方法的第一次迭代中，第一个折叠被作为测试集。模型在剩余的`k-1`个折叠上进行训练，然后在第一个折叠上进行评估（第一个折叠用于估算测试误差率）。
- en: This process is repeated `k` times, and a different fold is used as the test
    set in each iteration, while the remaining folds are used as the training set.
    Eventually, the method results in `k` different test error rates. The final k-fold
    cross-validation estimate of the model's error rate is computed by averaging these
    `k` test error rates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复`k`次，每次迭代使用不同的折叠作为测试集，而其余的折叠作为训练集。最终，这种方法会得到`k`个不同的测试误差率。模型误差率的最终`k`折交叉验证估计是通过平均这`k`个测试误差率计算得出的。
- en: 'The following diagram illustrates the dataset splitting process in the `k-fold
    cross-validation` method:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了`k`折交叉验证方法中的数据集划分过程：
- en: '![Figure 4.3: Overview of dataset splitting in the k-fold cross-validation
    method'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3：k折交叉验证方法中的数据集划分概述'
- en: '](img/B15777_04_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_03.jpg)'
- en: 'Figure 4.3: Overview of dataset splitting in the k-fold cross-validation method'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：k折交叉验证方法中的数据集划分概述
- en: In practice, we usually perform `k-fold cross-validation` with `k=5` or `k=10`,
    and these are the recommended values if you are struggling to select a value for
    your dataset. Deciding on the number of folds to use is dependent on the number
    of examples in the dataset and the available computational power. If `k=5`, the
    model will be trained and evaluated five times, while if `k=10`, this process
    will be repeated 10 times. The higher the number of folds, the longer it will
    take to perform k-fold cross-validation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常执行`k-fold交叉验证`，其中`k=5`或`k=10`，如果你在选择适合你数据集的值时遇到困难，这些是推荐的值。决定使用多少折叠取决于数据集中的示例数量和可用的计算能力。如果`k=5`，模型将被训练和评估五次，而如果`k=10`，这个过程将重复10次。折叠数越高，执行k折交叉验证所需的时间就越长。
- en: 'In k-fold cross-validation, the assignment of examples to each fold is completely
    random. However, by looking at the preceding diagram, you will see that, in the
    end, every single piece of data is used for both training and evaluation. That''s
    why if you repeat k-fold cross-validation many times on the same dataset and the
    same model, the final reported test error rates will be almost identical. Therefore,
    k-fold cross-validation does not suffer from high variance in its results, in
    contrast to the training set/test set approach. Now, we will take a look at the
    second form of cross-validation: leave-one-out validation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，示例分配到每个折叠是完全随机的。然而，通过查看前面的示意图，你会发现，最终每个数据点都会同时用于训练和评估。这就是为什么如果你在相同的数据集和相同的模型上重复多次k折交叉验证，最终报告的测试误差率几乎是相同的。因此，与训练集/测试集方法相比，k折交叉验证的结果不会受到高方差的影响。现在，我们来看一下交叉验证的第二种形式：留一法验证。
- en: Leave-One-Out Cross-Validation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 留一法交叉验证
- en: '`Leave-One-Out` `(LOO)` is a variation of the cross-validation technique in
    which, instead of dividing the dataset into two comparable-sized subsets for the
    training set and test set, only one single piece of data is used for evaluation.
    If there are `n` data examples in the entire dataset, at each iteration of `LOO
    cross-validation`, the model is trained on `n-1` examples and the single remaining
    example is used to compute the test error rate.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`留一法`（`LOO`）是交叉验证技术的一种变体，在这种方法中，数据集不会被划分为两个大小相当的子集用于训练集和测试集，而是仅使用一个数据点进行评估。如果整个数据集中有`n`个数据示例，在每次`LOO交叉验证`迭代中，模型会在`n-1`个示例上进行训练，而剩下的单个示例会用于计算测试误差率。'
- en: Using only one example for estimating the test error rate leads to an unbiased
    but high variance estimation of model performance; it is unbiased because this
    one example has not been used in training the model, it has high variance because
    it is computed based on only one data example, and it will vary depending on which
    exact data example is used. This process is repeated `n` times, and, at each iteration,
    a different data example is used for evaluation. In the end, the method will result
    in `n` different test error rates, and the final `LOO cross-validation` test error
    estimation is computed by averaging these `n` error rates.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用一个示例来估计测试误差率会导致对模型性能的无偏但高方差的估计；它是无偏的，因为这个示例没有参与训练模型，它具有高方差，因为它仅基于一个数据示例来计算，并且会根据使用的具体数据示例有所变化。这个过程会重复`n`次，在每次迭代中，使用不同的数据示例进行评估。最终，方法会得到`n`个不同的测试误差率，最终的`LOO交叉验证`测试误差估计是通过平均这`n`个误差率来计算的。
- en: 'An illustration of the dataset splitting process in the `LOO cross-validation`
    method can be seen in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`LOO交叉验证`方法中数据集划分过程的示意图如下所示：'
- en: '![Figure 4.4: Overview of dataset splitting in the LOO cross-validation method'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4：LOO交叉验证方法中数据集划分的概述'
- en: '](img/B15777_04_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_04.jpg)'
- en: 'Figure 4.4: Overview of dataset splitting in the LOO cross-validation method'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：LOO交叉验证方法中数据集划分的概述
- en: In each iteration of `LOO cross-validation`, almost all the examples in the
    dataset are used to train the model. On the other hand, in the training set/test
    set approach, a relatively large subset of data is used for evaluation and not
    used in training. Therefore, the `LOO` estimation of model performance is much
    closer to the performance of a model that is trained on the entire dataset, and
    this is the main advantage of `LOO cross-validation` over the `training set`/`test
    set` approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次`留一交叉验证`的迭代中，几乎所有的数据示例都用于训练模型。另一方面，在训练集/测试集方法中，数据的一个相对较大的子集用于评估，而不参与训练。因此，`留一交叉验证`对模型性能的估计更接近于在整个数据集上训练的模型的性能，这也是`留一交叉验证`相对于`训练集`/`测试集`方法的主要优势。
- en: Additionally, since in each iteration of `LOO cross-validation` only one unique
    data example is used for evaluation, and every single data example is used for
    training as well, there is no randomness associated with this method. Therefore,
    if you repeat `LOO cross-validation` many times on the same dataset and the same
    model, the final reported test error rates will be exactly the same each time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于在每次`留一交叉验证`的迭代中，仅使用一个唯一的数据示例进行评估，并且每个数据示例也都用于训练，因此这种方法没有随机性。因此，如果你在相同的数据集和相同的模型上重复进行多次`留一交叉验证`，最终报告的测试误差率每次都会完全相同。
- en: The drawback of `LOO cross-validation` is that it is computationally expensive.
    The reason for this is that the model needs to be trained `n` times, and in cases
    where `n` is large and/or the network is large, it will take a long time to complete.
    Both `LOO` and `k-fold cross-validation` have their advantages and disadvantages,
    all of which we will compare in the next section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`留一交叉验证`的缺点是计算开销大。其原因是模型需要训练`n`次，在`n`较大和/或网络较大的情况下，完成训练所需的时间会很长。`留一交叉验证`和`k折交叉验证`各有其优缺点，接下来我们将进行详细比较。'
- en: Comparing the K-Fold and LOO Methods
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较K折交叉验证和留一交叉验证方法
- en: By comparing the two preceding diagrams, it is obvious that `LOO cross-validation`
    is, in fact, a special case of `k-fold cross-validation`, where `k=n`. However,
    as was mentioned previously, choosing `k=n` is computationally very expensive
    in comparison to choosing `k=5` or `k=10`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较前两个图表，可以明显看出`留一交叉验证`实际上是`k折交叉验证`的一个特殊情况，其中`k=n`。然而，正如之前提到的，选择`k=n`在计算上非常昂贵，相比之下，选择`k=5`或`k=10`更为高效。
- en: 'Therefore, the first advantage of `k-fold cross-validation` over `LOO cross-validation`
    is that it is computationally less expensive. The following table compares the
    `k-fold with low-k`, `k-fold with high-k` and `LOO`, and `no cross-validation`
    with respect to `bias` and `variance`. The table shows that the highest bias comes
    with a simple `train-test split approach` and that the highest variance comes
    with leave-one-put cross-validation. In the middle is `k-fold cross-validation`.
    This is why k-fold cross-validation is generally the most appropriate choice for
    most machine learning tasks:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`k折交叉验证`相对于`留一交叉验证`的第一个优势是计算开销较小。下表比较了`低k折交叉验证`、`高k折交叉验证`和`留一交叉验证`，以及`无交叉验证`在`偏差`和`方差`方面的差异。表格显示，最大的偏差出现在简单的`训练集-测试集划分方法`中，最大的方差出现在留一交叉验证中。`k折交叉验证`位于两者之间。这也是为什么`k折交叉验证`通常是大多数机器学习任务中最合适的选择：
- en: '![Figure 4.5: Comparing the train-test split, k-fold cross-validation, and
    LOO'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5：比较训练集-测试集划分、k折交叉验证和留一交叉验证](img/B15777_04_06.jpg)'
- en: cross-validation methods
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证方法
- en: '](img/B15777_04_05.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_06.jpg)'
- en: 'Figure 4.5: Comparing the train-test split, k-fold cross-validation, and LOO
    cross-validation methods'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：比较训练集-测试集划分、k折交叉验证和留一交叉验证方法
- en: 'The following plot compares the `training set`/`test set` approach, `k-fold
    cross-validation`, and `LOO cross-validation` in terms of `bias` and `variance`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表比较了`训练集`/`测试集`方法、`k折交叉验证`和`留一交叉验证`在`偏差`和`方差`方面的差异：
- en: '![Figure 4.6: Comparing the training set/test set approach, k-fold cross-validation,
    and LOO cross-validation in terms of bias and variance'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6：比较训练集/测试集方法、k折交叉验证和留一交叉验证在偏差和方差方面的差异](img/B15777_04_06.jpg)'
- en: '](img/B15777_04_06.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_05.jpg)'
- en: 'Figure 4.6: Comparing the training set/test set approach, k-fold cross-validation,
    and LOO cross-validation in terms of bias and variance'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：比较训练集/测试集方法、k折交叉验证和留一交叉验证在偏差和方差方面的差异
- en: Generally, in machine learning and data analysis, the most desirable model is
    the one with the `lowest bias` and the `lowest variance`. As shown in the preceding
    plot, the region labeled in the middle of the graph, where both `bias` and `variance`
    are low, is of interest. It turns out that this region is equivalent to `k-fold
    cross-validation` with `k` between `5` and `10`. In the next section, we will
    explore how to implement various methods of cross-validation in practice.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在机器学习和数据分析中，最理想的模型是具有`最低偏差`和`最低方差`的模型。如前面的图所示，图中间标记的区域，其中`偏差`和`方差`都较低，是我们关注的重点。事实证明，这个区域等同于
    `k-fold 交叉验证`，其中 `k` 的值介于 `5` 到 `10` 之间。在下一节中，我们将探索如何在实践中实现不同的交叉验证方法。
- en: Cross-Validation for Deep Learning Models
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型的交叉验证
- en: In this section, you will learn about using the Keras wrapper with scikit-learn,
    which is a helpful tool that allows us to use Keras models as part of a scikit-learn
    workflow. As a result, scikit-learn methods and functions, such as the one for
    performing cross-validation, can easily be applied to Keras models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何使用 Keras 封装器与 scikit-learn 配合使用，这是一种有用的工具，可以让我们将 Keras 模型作为 scikit-learn
    工作流的一部分。因此，像交叉验证这样的 scikit-learn 方法和函数可以轻松地应用到 Keras 模型中。
- en: You will learn, step-by-step, how to implement what you learned about cross-validation
    in the previous section using scikit-learn. Furthermore, you will learn how to
    use cross-validation to evaluate Keras deep learning models using the Keras wrapper
    with scikit-learn. Lastly, you will practice what you have learned by solving
    a problem involving a real dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你将一步步学习如何在本节中使用 scikit-learn 实现你在上一节中学到的交叉验证方法。此外，你还将学习如何使用交叉验证评估 Keras 深度学习模型，并使用
    Keras 封装器与 scikit-learn 配合使用。最后，你将通过解决一个实际数据集中的问题来实践所学的内容。
- en: Keras Wrapper with scikit-learn
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 封装器与 scikit-learn
- en: When it comes to general machine learning and data analysis, the scikit-learn
    library is much richer and easier to use than Keras. That is why being able to
    use scikit-learn methods on Keras models will be of great value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般的机器学习和数据分析中，scikit-learn 库比 Keras 更加丰富且易于使用。这就是为什么能够在 Keras 模型上使用 scikit-learn
    方法会非常有价值的原因。
- en: 'Fortunately, Keras comes with a helpful wrapper, `keras.wrappers.scikit_learn`,
    that allows us to build scikit-learn interfaces for deep learning models that
    can be used as classification or regression estimators in scikit-learn. There
    are two types of wrapper: one for classification estimators and one for regression
    estimators. The following code is used to define these scikit-learn interfaces:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Keras 提供了一个有用的封装器 `keras.wrappers.scikit_learn`，它允许我们为深度学习模型构建 scikit-learn
    接口，这些模型可以作为分类或回归估计器在 scikit-learn 中使用。封装器有两种类型：一种用于分类估计器，另一种用于回归估计器。以下代码用于定义这些
    scikit-learn 接口：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `build_fn` argument needs to be a callable function where a Keras sequential
    model is defined, compiled and returned inside its body.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_fn` 参数需要是一个可调用的函数，在其内部定义、编译并返回一个 Keras 顺序模型。'
- en: The `sk_params` argument can take parameters for building the model (such as
    activation functions for layers) and parameters for fitting the model (such as
    the number of epochs and batch size). This will be put into practice in the following
    exercise, where we will use Keras wrappers for a regression problem.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`sk_params` 参数可以接受用于构建模型的参数（如层的激活函数）和用于拟合模型的参数（如训练轮数和批量大小）。这一点将在接下来的练习中得以应用，我们将在该练习中使用
    Keras 封装器解决回归问题。'
- en: Note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'All the activities in this chapter will be developed in a Jupyter notebook.
    Please download this book''s GitHub repository, along with all the prepared templates,
    which can be found here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有活动将在 Jupyter notebook 中开发。请下载本书的 GitHub 仓库以及所有已准备好的模板，点击以下链接即可找到：
- en: '[https://packt.live/3btnjfA](https://packt.live/3btnjfA).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.live/3btnjfA](https://packt.live/3btnjfA)。'
- en: 'Exercise 4.01: Building the Keras Wrapper with scikit-learn for a Regression
    Problem'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.01：为回归问题构建 Keras 封装器与 scikit-learn 配合使用
- en: 'In this exercise, you will learn the step-by-step process of building the wrapper
    for a Keras deep learning model so that it can be used in a scikit-learn workflow.
    First, load in the dataset of `908` data points of a regression problem, where
    each record describes six attributes of a chemical, and the target is the acute
    toxicity toward the fish Pimephales promelas, or `LC50`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，你将学习如何一步一步构建Keras深度学习模型的包装器，使其可以在scikit-learn工作流中使用。首先，加载一个包含`908`个数据点的回归问题数据集，其中每个记录描述了化学物质的六个属性，目标是预测鱼类Pimephales
    promelas的急性毒性，即`LC50`：
- en: Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Watch out for the slashes in the string below. Remember that the backslashes
    ( `\` ) are used to split the code across multiple lines, while the forward slashes
    ( `/` ) are part of the path.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意下面字符串中的斜杠。记住，反斜杠（`\`）用于将代码分割为多行，而正斜杠（`/`）是路径的一部分。
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is the expected output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since the output in this dataset takes a numerical value, this is a regression
    problem. The goal is to build a model that predicts the acute toxicity toward
    the fish `LC50`, given the other attributes of the chemical. Now, let''s go through
    the steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该数据集的输出是一个数值，因此这是一个回归问题。目标是构建一个模型，预测给定化学物质的其他属性时，鱼类的急性毒性`LC50`。现在，让我们一步步来看：
- en: 'Define a function that builds and returns a Keras model for this regression
    problem. The Keras model that you define must have a single hidden layer of size
    `8` with `ReLU activation` functions. Also, use the `Mean Squared Error` (`MSE`)
    loss function and the `Adam optimizer` to compile the model:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来构建并返回用于回归问题的Keras模型。你定义的Keras模型必须有一个隐藏层，大小为`8`，并使用`ReLU激活`函数。同时，使用`均方误差`（`MSE`）损失函数和`Adam优化器`来编译模型：
- en: '[PRE3]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, use the Keras wrapper with scikit-learn to create the scikit-learn interface
    for your model. Remember that you need to provide the `epochs`, `batch_size`,
    and `verbose` arguments here:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用Keras包装器和scikit-learn创建scikit-learn接口来构建你的模型。记住，你需要在这里提供`epochs`、`batch_size`和`verbose`参数：
- en: '[PRE4]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, `YourModel` is ready to be used as a regression estimator in scikit-learn.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，`YourModel`已经可以作为scikit-learn中的回归估计器使用。
- en: In this exercise, we learned how to build a Keras wrapper with scikit-learn
    for a regression problem by using a simulated dataset.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们学习了如何使用模拟数据集通过scikit-learn构建Keras包装器来解决回归问题。
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/38nuqVP](https://packt.live/38nuqVP).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参阅[https://packt.live/38nuqVP](https://packt.live/38nuqVP)。
- en: You can also run this example online at [https://packt.live/31MLgMF](https://packt.live/31MLgMF).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，访问[https://packt.live/31MLgMF](https://packt.live/31MLgMF)。
- en: We will continue implementing cross-validation using this dataset in the rest
    of the exercises in this chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章剩余的练习中继续使用此数据集实现交叉验证。
- en: Cross-Validation with scikit-learn
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行交叉验证
- en: 'In the previous chapter, you learned that you can perform `training set`/`test
    set` splitting easily in scikit-learn. Let''s assume that your original dataset
    is stored in `X` and `y` arrays. You can split them randomly into a training set
    and a test set using the following commands:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学会了如何在scikit-learn中轻松进行`训练集`/`测试集`的划分。假设你的原始数据集存储在`X`和`y`数组中。你可以使用以下命令将它们随机划分为训练集和测试集：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `test_size` argument can be assigned to any number between `0` and `1`,
    depending on how large you would like the test set to be. By providing an `int`
    number for a `random_state` argument, you will be able to select the seed for
    the random number generator.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_size`参数可以设置为介于`0`和`1`之间的任何数值，具体取决于你希望测试集的大小。通过为`random_state`参数提供一个`int`类型的数字，你将能够选择随机数生成器的种子。'
- en: 'The easiest way to perform cross-validation in scikit-learn is by using the
    `cross_val_score` function. In order to do this, you need to define your estimator
    first (in our case, the estimator will be a Keras model). Then, you will be able
    to perform cross-validation on your estimator/model using the following commands:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中执行交叉验证的最简单方法是使用`cross_val_score`函数。为此，你需要首先定义你的估计器（在我们的案例中，估计器将是一个Keras模型）。然后，你将能够使用以下命令对估计器/模型进行交叉验证：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Notice that we provide the Keras model and the original dataset as arguments
    to the `cross_val_score` function, along with the number of folds (the `cv` argument).
    Here, we used `cv=5`, so the `cross_val_score` function will randomly split the
    dataset into five-folds and perform training and fitting on the model five times
    using five different training and test sets. It will compute the default metric
    for model evaluation (or the metrics given to the Keras model when defining it)
    at each iteration/fold and store them in scores. We can print the final cross-validation
    score as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将 Keras 模型和原始数据集作为参数传递给 `cross_val_score` 函数，并指定折数（即 `cv` 参数）。在这里，我们使用了
    `cv=5`，所以 `cross_val_score` 函数会将数据集随机划分为五个折，并使用五个不同的训练集和测试集对模型进行五次训练和拟合。它将在每次迭代/折叠时计算默认的模型评估指标（或在定义
    Keras 模型时提供的指标），并将它们存储在 scores 中。我们可以如下打印最终的交叉验证得分：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Earlier, we mentioned that the score that's returned by the `cross_val_score`
    function is the default metric for our model or the metric that we determined
    for it when defining our model. However, it is possible to change the cross-validation
    metric by providing the desired metric as a `scoring` argument when calling the
    `cross_val_score` function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们提到过，`cross_val_score` 函数返回的得分是我们模型的默认指标，或者是我们在定义模型时为其确定的指标。然而，也可以通过在调用 `cross_val_score`
    函数时提供所需的指标作为 `scoring` 参数，来更改交叉验证的指标。
- en: Note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can learn more about how to provide the desired metric in the `scoring`
    argument of the `cross_val_score` function here: [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里了解如何在 `cross_val_score` 函数的 `scoring` 参数中提供所需的评估指标：[https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)。
- en: 'By providing an integer number for the `cv` argument of the `cross_val_score`
    function, we are telling the function to perform k-fold cross-validation on the
    dataset. However, there are several other iterators available in scikit-learn
    that we can assign to `cv` to perform other variations of cross-validation on
    the dataset. For example, the following code block will perform `LOO cross-validation`
    on the dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为 `cross_val_score` 函数的 `cv` 参数提供一个整数，我们告诉函数在数据集上执行 k-fold 交叉验证。然而，scikit-learn
    中还有几种其他的迭代器可以分配给 `cv`，以执行数据集的其他交叉验证变种。例如，以下代码块将对数据集执行 `LOO 交叉验证`：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the next section, we will explore k-fold cross-validation in scikit-learn
    and see how it can be used with Keras models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索 scikit-learn 中的 k-fold 交叉验证，并看看它如何与 Keras 模型一起使用。
- en: Cross-Validation Iterators in scikit-learn
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn 中的交叉验证迭代器
- en: 'A list of the most commonly used cross-validation iterators available in scikit-learn
    is provided here, along with a brief description of each of them:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了 scikit-learn 中最常用的交叉验证迭代器的列表，并简要描述了它们的功能：
- en: '`KFold(n_splits=?)`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KFold(n_splits=?)`'
- en: This divides the dataset into k folds or groups. The `n_splits` argument is
    required to determine how many folds to use. If `n_splits=n`, it will be equivalent
    to `LOO cross-validation`.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将把数据集划分为 k 个折或组。`n_splits` 参数是必需的，用于确定使用多少个折。如果 `n_splits=n`，它将等同于 `LOO 交叉验证`。
- en: '`RepeatedKFold(n_splits=?, n_repeats=?, random_state=random_state)`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RepeatedKFold(n_splits=?, n_repeats=?, random_state=random_state)`'
- en: This will repeat k-fold cross-validation `n_repeats` times.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将重复执行 k-fold 交叉验证 `n_repeats` 次。
- en: '`LeaveOneOut()`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LeaveOneOut()`'
- en: This will split the dataset for `LOO cross-validation`.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将对数据集进行 `LOO 交叉验证` 的划分。
- en: '`ShuffleSplit(n_splits=?, test_size=?, random_state= random_state)`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ShuffleSplit(n_splits=?, test_size=?, random_state=random_state)`'
- en: This will generate an `n_splits` number of random and independent training set/test
    set dataset splits. It is possible to store the seed for the random number generator
    using the `random_state` argument; if you do this, the dataset splits will be
    reproducible.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成 `n_splits` 个随机且独立的训练集/测试集数据集划分。可以使用 `random_state` 参数存储随机数生成器的种子；如果这么做，数据集的划分将是可重现的。
- en: In addition to the regular iterators, such as the ones mentioned here, there
    are `stratified` versions as well. Stratified sampling is useful when the number
    of examples in different classes of a dataset is unbalanced. For example, imagine
    that we want to design a classifier to predict whether someone will default on
    their credit card debt, where almost `95%` of the examples in the dataset are
    in the `negative` class. Stratified sampling makes sure that the relative class
    frequencies are preserved in each `training set`/`test set` split. It is recommended
    to use the stratified versions of iterators for such cases.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常规迭代器（例如这里提到的迭代器），还有 `分层` 版本。分层抽样在数据集的不同类别的样本数量不平衡时非常有用。例如，假设我们想设计一个分类器来预测某人是否会拖欠信用卡债务，其中数据集中几乎有
    `95%` 的样本属于 `负类`。分层抽样确保在每个 `训练集`/`测试集` 划分中保留类别的相对频率。对于这种情况，建议使用分层版本的迭代器。
- en: 'Usually, before using a training set to train and evaluate a model, we perform
    preprocessing on it to scale the examples so that they have a mean equal to `0`
    and a standard deviation equal to `1`. In the `training set`/`test set` approach,
    we need to scale the training set and store the transformation. The following
    code block will do this for us:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在使用训练集训练和评估模型之前，我们会对其进行预处理，以便将样本缩放，使其均值为 `0`，标准差为 `1`。在 `训练集`/`测试集` 方法中，我们需要缩放训练集并存储该转换。以下代码块将为我们完成这项工作：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here''s an example of performing `stratified k-fold cross-validation` with
    `k=5` on our `X`, `y` dataset:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在我们的 `X`、`y` 数据集上执行 `k=5` 的 `分层 K 折交叉验证` 的示例：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can learn more about cross-validation iterators in scikit-learn here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于 scikit-learn 中交叉验证迭代器的内容：
- en: '[https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators)。'
- en: Now that we understand cross-validation iterators, we can put them into practice
    in an exercise.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了交叉验证迭代器，可以在练习中将它们付诸实践。
- en: 'Exercise 4.02: Evaluating Deep Neural Networks with Cross-Validation'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.02：使用交叉验证评估深度神经网络
- en: 'In this exercise, we will bring all the concepts and methods that we have learned
    about in this topic about cross-validation together. We will go through all the
    steps one more time, from defining a Keras deep learning model to transferring
    it to a scikit-learn workflow and performing cross-validation in order to evaluate
    its performance. In a sense, this exercise is a recap of what we have learned
    so far, and what is covered here will be extremely helpful for *Activity 4.01*,
    *Model Evaluation Using Cross-Validation for an Advanced Fibrosis Diagnosis Classifier*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次练习中，我们将把我们在本主题中学习到的所有交叉验证相关概念和方法结合起来。我们将再次经历所有步骤，从定义 Keras 深度学习模型到将其转移到 scikit-learn
    工作流并执行交叉验证以评估其性能。从某种意义上说，这个练习是我们到目前为止所学内容的回顾，涵盖的内容对于 *活动 4.01*（*使用交叉验证评估先进的纤维化诊断分类器模型*）将非常有帮助：
- en: 'The first step is always to load the dataset that you would like to build the
    model for. First, load in the dataset of `908` data points of a regression problem,
    where each record describes six attributes of a chemical and the target is the
    acute toxicity toward the fish Pimephales promelas, or `LC50`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步始终是加载你想要构建模型的数据集。首先，加载回归问题的 `908` 个数据点的数据集，每个记录描述了一个化学物质的六个属性，目标是预测对鱼类 Pimephales
    promelas 的急性毒性，即 `LC50`：
- en: '[PRE11]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the function that returns the Keras model with a single hidden layer
    of size `8` with `ReLU activation` functions using the `Mean Squared Error` (`MSE`)
    loss function and the `Adam optimizer`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，返回一个具有单个隐藏层（大小为 `8`，使用 `ReLU 激活` 函数）的 Keras 模型，使用 `均方误差` (`MSE`) 损失函数和
    `Adam 优化器`：
- en: '[PRE13]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Set the `seed` and use the wrapper to build the scikit-learn interface for
    the Keras model we defined in the function in `step 2`:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`seed`并使用包装器构建我们在`步骤 2`中定义的 Keras 模型的 scikit-learn 接口：
- en: '[PRE14]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the iterator to use for cross-validation. Let''s perform `5-fold cross-validation`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义要用于交叉验证的迭代器。我们来进行 `5 折交叉验证`：
- en: '[PRE15]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Call the `cross_val_score` function to perform cross-validation. This step
    will take a while to complete, depending on the computational power that''s available:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `cross_val_score` 函数来执行交叉验证。根据可用的计算能力，这一步可能需要一些时间才能完成：
- en: '[PRE16]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once cross-validation has been completed, print the `final cross-validation`
    estimation of model performance (the default metric for performance will be the
    test loss):'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦交叉验证完成，打印`最终交叉验证`模型性能评估（性能的默认评估指标为测试损失）：
- en: '[PRE17]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here''s an example output:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个示例输出：
- en: '[PRE18]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The cross-validation loss states that the Keras model that was trained on this
    dataset is able to predict the `LC50` of the chemicals with an average loss of
    `0.9680`. We will try to examine this model further in the next exercise.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证损失表明，在该数据集上训练的Keras模型能够以`0.9680`的平均损失预测化学物质的`LC50`值。我们将在下一次练习中进一步研究该模型。
- en: These were all the steps that are required in order to evaluate a Keras deep
    learning model using cross-validation in scikit-learn. Now, we will put them into
    practice in an activity.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是使用scikit-learn中的交叉验证评估Keras深度学习模型所需的所有步骤。现在，我们将在活动中将这些步骤付诸实践。
- en: Note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3eRTlTM](https://packt.live/3eRTlTM).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/3eRTlTM](https://packt.live/3eRTlTM)。
- en: You can also run this example online at [https://packt.live/31IdVT0](https://packt.live/31IdVT0).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行此示例，访问 [https://packt.live/31IdVT0](https://packt.live/31IdVT0)。
- en: 'Activity 4.01: Model Evaluation Using Cross-Validation for an Advanced Fibrosis
    Diagnosis Classifier'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动4.01：使用交叉验证评估高级纤维化诊断分类器的模型
- en: 'We learned about the hepatitis C dataset in *Activity 3.02,* *Advanced Fibrosis
    Diagnosis with Neural Networks* of *Chapter 3*, *Deep Learning with Keras*. The
    dataset consists of information for `1385` patients who underwent treatment dosages
    for hepatitis C. For each patient, `28` different attributes are available, such
    as age, gender, and BMI, as well as a class label, which can only take two values:
    `1`, indicating advanced fibrosis, and `0`, indicating no indication of advanced
    fibrosis. This is a `binary`/`two-class` classification problem with an input
    dimension equal to `28`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3.02活动*中了解了肝炎C数据集，*第3章*，*Keras深度学习*中的*高级纤维化诊断与神经网络*。该数据集包含了`1385`名接受肝炎C治疗剂量的患者的信息。每位患者都有`28`个不同的属性可供参考，如年龄、性别、BMI等，以及一个类标签，该标签只能取两个值：`1`，表示高级纤维化；`0`，表示没有高级纤维化的迹象。这是一个`二元`/`两类`分类问题，输入维度为`28`。
- en: 'In *Chapter 3*, *Deep Learning with Keras*, we built Keras models to perform
    classification on this dataset. We trained and evaluated the models using `training
    set`/`test set` splitting and reported the test error rate. In this activity,
    we are going to use what we learned in this topic to train and evaluate a deep
    learning model using `k-fold cross-validation`. We will use the model that resulted
    in the best test error rate from the previous activity. The goal is to compare
    the cross-validation error rate with the training set/test set approach error
    rate:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*Keras深度学习*中，我们构建了Keras模型来对该数据集进行分类。我们使用`训练集`/`测试集`划分训练并评估模型，并报告了测试误差率。在本活动中，我们将运用本主题中学到的知识，使用`k折交叉验证`来训练和评估深度学习模型。我们将使用在前一个活动中得出的最佳测试误差率的模型。目标是将交叉验证误差率与训练集/测试集方法的误差率进行比较：
- en: Import the necessary libraries. Load the dataset from the `data` subfolder of
    the `Chapter04` folder from GitHub using `X = pd.read_csv('../data/HCV_feats.csv'),
    y = pd.read_csv('../data/HCV_target.csv')`. Print the number of examples in the
    dataset, the number of features available, and the possible values for the class
    labels.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。从GitHub的`Chapter04`文件夹中的`data`子文件夹加载数据集，使用`X = pd.read_csv('../data/HCV_feats.csv'),
    y = pd.read_csv('../data/HCV_target.csv')`。打印数据集中的示例数量、可用的特征数量以及类标签的可能值。
- en: 'Define the function that returns the Keras model. The Keras model will be a
    deep neural network with two hidden layers, where the `first hidden layer` is
    of `size 4` and the `second hidden layer` is of `size 2`, and use the `tanh activation`
    function to perform the classification. Use the following values for the hyperparameters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来返回Keras模型。该Keras模型将是一个深度神经网络，包含两个隐藏层，其中`第一个隐藏层`的`大小为4`，`第二个隐藏层`的`大小为2`，并使用`tanh激活函数`进行分类。使用以下超参数值：
- en: '`optimizer = ''adam'', loss = ''binary_crossentropy'', metrics = [''accuracy'']`'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`optimizer = ''adam'', loss = ''binary_crossentropy'', metrics = [''accuracy'']`'
- en: Build the scikit-learn interface for the Keras model with `epochs=100`, `batch_size=20`,
    and `shuffle=False`. Define the cross-validation iterator as `StratifiedKFold`
    with `k=5`. Perform k-fold cross-validation on the model and store the scores.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Keras模型构建scikit-learn接口，设置`epochs=100`、`batch_size=20`，并将`shuffle=False`。将交叉验证迭代器定义为`StratifiedKFold`，并设置`k=5`。对模型进行k折交叉验证，并存储分数。
- en: Print the accuracy for each iteration/fold, plus the overall cross-validation
    accuracy and its associated standard deviation.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每次迭代/折叠的准确性，以及总体交叉验证准确性和其相关标准差。
- en: Compare this result with the result from *Activity 3.02*, *Advanced Fibrosis
    Diagnosis with Neural Networks* of *Chapter 3*, *Deep Learning with Keras*.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此结果与*第3章*《使用Keras的深度学习》中的*活动3.02*《使用神经网络进行高级纤维化诊断》中的结果进行比较。
- en: 'After implementing the preceding steps, the expected output will look as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前述步骤后，期望的输出将如下所示：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 381.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第381页找到。
- en: The accuracy we received from training set/test set approach we performed in
    *Activity 3.02*, *Advanced Fibrosis Diagnosis with Neural Networks* of *Chapter
    3*, *Deep Learning with Keras*, was `49.819%`, which is lower than the test accuracy
    we achieved when performing `5-fold cross-validation` on the same deep learning
    model and the same dataset, but lower than the accuracy on one of the folds.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*《使用Keras的深度学习》中的*活动3.02*《使用神经网络进行高级纤维化诊断》中，我们通过训练集/测试集方法得到的准确率为`49.819%`，这低于我们在对相同深度学习模型和相同数据集进行`5折交叉验证`时所取得的测试准确率，但低于其中一折的准确率。
- en: The reason for this difference is that the test error rate resulting from the
    training set/test set approach was computed by only including a subset of the
    data points in the model's evaluation. On the other hand, the test error rate
    here is computed by including all the data points in the evaluation, and therefore
    this estimation of the model's performance is more accurate and more robust, performing
    better on the unseen test dataset.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异的原因在于，通过训练集/测试集方法计算的测试错误率是通过仅将数据点的一个子集纳入模型评估得出的。另一方面，这里计算的测试错误率是通过将所有数据点纳入评估，因此，这种模型性能的估计更为准确且更具鲁棒性，在未见过的测试数据集上表现更好。
- en: In this activity, we used cross-validation to perform a model evaluation on
    a problem involving a real dataset. Improving model evaluation is not the only
    purpose of using cross-validation, and it can be used to select the best model
    or parameters for a given problem as well.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们使用交叉验证对涉及真实数据集的问题进行模型评估。提高模型评估的准确性并不是使用交叉验证的唯一目的，它还可以用来为给定的问题选择最佳模型或参数。
- en: Model Selection with Cross-Validation
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证进行模型选择
- en: Cross-validation provides us with robust estimation of model performance on
    unseen examples. For this reason, it can be used to decide between two models
    for a particular problem or to decide which model parameters (or hyperparameters)
    to use for a particular problem. In these cases, we would like to find out which
    model or which set of model parameters/hyperparameters results in the lowest test
    error rate. Therefore, we will select that model or that set of parameters/hyperparameters
    for our problem.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证为我们提供了模型在未见过的示例上的鲁棒性估计。因此，它可以用来在特定问题中决定两个模型之间的优劣，或者决定在特定问题中使用哪一组模型参数（或超参数）。在这些情况下，我们希望找出哪个模型或哪一组模型参数/超参数会导致最低的测试错误率。因此，我们将选择该模型或该组参数/超参数作为我们问题的解决方案。
- en: In this section, you are going to practice using cross-validation for this purpose.
    You will learn how to define a set of hyperparameters for your deep learning model
    and then write user-defined functions in order to perform cross-validation on
    your model for each of the possible combinations of hyperparameters. Then, you
    will observe which combination of hyperparameters leads to the lowest test error
    rate, and that combination will be your choice for your final model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将练习使用交叉验证来完成这一任务。你将学习如何为深度学习模型定义一组超参数，然后编写用户定义的函数，对模型进行交叉验证，涵盖每种可能的超参数组合。然后，你将观察哪一组超参数组合导致最低的测试错误率，这组超参数将成为你最终模型的选择。
- en: Cross-Validation for Model Evaluation versus Model Selection
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与模型选择中的交叉验证
- en: In this section, we are going to go deeper into what it means to use cross-validation
    for model evaluation versus model selection. So far, we have learned that evaluating
    a model on the training set results in an underestimation of the model's error
    rate on unseen examples. Splitting the dataset into a `training set` and a `test
    set` gives us a more accurate estimation of the model's performance but suffers
    from high variance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨使用交叉验证进行模型评估与模型选择之间的区别。到目前为止，我们已经了解到，在训练集上评估模型会导致对模型在未见样本上的错误率的低估。将数据集分为`训练集`和`测试集`可以更准确地估计模型的表现，但会面临较高的方差问题。
- en: Lastly, cross-validation results in a much more robust and accurate estimation
    of the model's performance on unseen examples. An illustration of the error rate
    estimations resulting from these three approaches for model evaluation can be
    seen in the following plot.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，交叉验证能够更稳健、准确地估计模型在未见样本上的表现。以下图示展示了这三种模型评估方法所产生的错误率估计。
- en: 'The following plot shows the case where the error rate estimation in the training
    set/test set approach is slightly lower than the cross-validation estimation.
    However, it is important to remember that the `training set`/`test set` error
    rate can be higher than the cross-validation estimation error rate as well, depending
    on what data is included in the test set (hence the high variance problem). On
    the other hand, the error rate resulting from performing an evaluation on the
    training set is always lower than the other two approaches:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了在训练集/测试集方法的错误率估计稍低于交叉验证估计的情况。然而，重要的是要记住，`训练集`/`测试集`的错误率也可能高于交叉验证估计的错误率，具体取决于测试集中包含的数据（因此会存在高方差问题）。另一方面，在训练集上进行评估所得到的错误率始终低于其他两种方法：
- en: '![Figure 4.7: Illustration of the error rate estimations resulting from the
    three approaches to model evaluation'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7：展示三种模型评估方法所产生的错误率估计'
- en: '](img/B15777_04_07.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_07.jpg)'
- en: 'Figure 4.7: Illustration of the error rate estimations resulting from the three
    approaches to model evaluation'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：展示三种模型评估方法所产生的错误率估计
- en: 'We have established that cross-validation leads to the best estimation of a
    model''s performance on independent data examples. Knowing this, we can use cross-validation
    to decide which model to use for a particular problem. For example, if we have
    four different models and we would like to decide which one is a better fit for
    a particular dataset, we can train and evaluate each of the four models using
    cross-validation and choose the model with the lowest cross-validation error rate
    as our final model for the dataset. The following plot shows the cross-validation
    error rate associated with four hypothetical models. From this, we can conclude
    that **Model 1** is the best fit for the problem, while **Model 4** is the worst
    choice. These four models could be deep neural networks with a different number
    of hidden layers and a different number of units in their hidden layers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，交叉验证提供了模型在独立数据样本上的最佳表现估计。知道这一点后，我们可以使用交叉验证来决定针对特定问题使用哪个模型。例如，如果我们有四个不同的模型，并希望确定哪个模型最适合某一数据集，我们可以使用交叉验证训练并评估每个模型，选择交叉验证错误率最低的模型作为最终模型。以下图展示了与四个假设模型相关的交叉验证错误率。由此，我们可以得出结论，**模型1**最适合该问题，而**模型4**是最差的选择。这四个模型可能是深度神经网络，它们具有不同数量的隐藏层和隐藏层中不同数量的单元：
- en: '![Figure 4.8: Illustration of cross-validation error rates associated with
    four'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8：展示与四个假设模型相关的交叉验证错误率'
- en: hypothetical models
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模型
- en: '](img/B15777_04_08.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_08.jpg)'
- en: 'Figure 4.8: Illustration of cross-validation error rates associated with four
    hypothetical models'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：展示与四个假设模型相关的交叉验证错误率
- en: After we have found out which model is the best fit for a particular problem,
    the next step is choosing the best set of parameters or hyperparameters for that
    model. Previously, we discussed that when building a deep neural network, several
    hyperparameters need to be selected for the model, and several choices are available
    for each of these hyperparameters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了哪种模型最适合特定问题后，下一步是为该模型选择最佳的参数集或超参数。之前我们讨论了，在构建深度神经网络时，模型需要选择多个超参数，而且每个超参数都有多个选择。
- en: These hyperparameters include the type of activation function, loss function,
    and optimizer, plus the number of epochs and batch size. We can define the set
    of possible choices for each of these hyperparameters and then implement the model,
    along with cross-validation, to find the best combination of hyperparameters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数包括激活函数的类型、损失函数和优化器，以及训练的轮次和批次大小。我们可以定义每个超参数的可能选择集合，然后实现模型并结合交叉验证，找出最佳的超参数组合。
- en: 'An illustration of the cross-validation error rates associated with four different
    sets of hyperparameters for a hypothetical deep learning model is shown in the
    following plot. From this, we can conclude that `Set 1` is the best choice for
    this model since the line corresponding to `Hyperparameters Set 1` has the lowest
    value for the cross-validation error rate:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了与四组不同超参数集相关的交叉验证误差率的插图。从中我们可以得出结论，`超参数集 1`是该模型的最佳选择，因为与`超参数集 1`对应的线条在交叉验证误差率上具有最低的值：
- en: '![Figure 4.9: Illustration of cross-validation error rates associated with
    four different sets of hyperparameters for a hypothetical deep learning model'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9：展示与四组不同超参数集相关的交叉验证误差率的插图，针对一个假设的深度学习模型'
- en: '](img/B15777_04_09.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_04_09.jpg)'
- en: 'Figure 4.9: Illustration of cross-validation error rates associated with four
    different sets of hyperparameters for a hypothetical deep learning model'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：展示与四组不同超参数集相关的交叉验证误差率的插图，针对一个假设的深度学习模型
- en: In the next exercise, we will learn how to iterate through different model architectures
    and hyperparameters to find the set that results in an optimal model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将学习如何遍历不同的模型架构和超参数，以找到结果最优的模型集合。
- en: 'Exercise 4.03: Writing User-Defined Functions to Implement Deep Learning Models
    with Cross-Validation'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.03：编写用户定义的函数来实现带交叉验证的深度学习模型
- en: In this exercise, you will learn how to use cross-validation for the purpose
    of model selection.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，您将学习如何使用交叉验证进行模型选择。
- en: 'First, load in the dataset of `908` data points of a regression problem, where
    each record describes six attributes of a chemical and the target is the acute
    toxicity toward the fish Pimephales promelas, or `LC50`. The goal is to build
    a model to predict the `LC50` of each chemical, given the chemical attributes:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载包含`908`个数据点的回归问题数据集，每条记录描述了一种化学品的六个属性，目标是其对鱼类Pimephales promelas的急性毒性，或`LC50`。目标是构建一个模型，根据化学品属性预测每种化学品的`LC50`：
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Follow these steps to complete this exercise:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成本次练习：
- en: 'Define three functions to return three Keras models. The first model should
    have one hidden layer of `size 4`, the second model should have one hidden layer
    of `size 8`, and the third model should have two hidden layers, with the first
    layer of `size 4` and the second layer of `size 2`. Use a `ReLU activation` function
    for all the hidden layers. The goal is to find out which of these three models
    leads to the lowest cross-validation error rate:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三个函数，返回三个Keras模型。第一个模型应该有一个`大小为4`的隐藏层，第二个模型应该有一个`大小为8`的隐藏层，第三个模型应该有两个隐藏层，第一个层的`大小为4`，第二个层的`大小为2`。对于所有隐藏层，使用`ReLU激活`函数。目标是找出这三种模型中，哪个模型能带来最低的交叉验证误差率：
- en: '[PRE21]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Write a loop to build the Keras wrapper and perform `3-fold cross-validation`
    on the `three` models. Store the scores for each model:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个循环来构建Keras包装器，并对`三个`模型执行`3折交叉验证`。存储每个模型的得分：
- en: '[PRE22]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Print the final cross-validation error rate for each of the models to find
    out which model has a lower error rate:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每个模型的最终交叉验证误差率，以找出哪个模型的误差率较低：
- en: '[PRE23]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here''s an example output:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是一个输出示例：
- en: '[PRE24]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Model 2` results in the lowest error rate, so we will use it in the steps
    that follow.'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`模型 2`的错误率最低，因此我们将在接下来的步骤中使用它。'
- en: 'Use cross-validation again to determine the number of epochs and batch size
    for the model that resulted in the lowest cross-validation error rate. Write the
    code that performs `3-fold cross-validation` on every possible combination of
    `epochs` and `batch-size` in the ranges `epochs=[100, 150]` and `batch_size=[20,
    15]` and store the scores:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用交叉验证确定导致最低交叉验证误差率的模型的轮次和批次大小。编写代码，对`epochs`和`batch_size`在`epochs=[100, 150]`和`batch_size=[20,
    15]`范围内的所有可能组合执行`3折交叉验证`并存储得分：
- en: '[PRE25]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding code block uses two `for` loops to perform `3-fold cross-validation`
    for all possible combinations of `epochs` and `batch_size`. Since there are two
    choices for each of them, four different pairs are possible and therefore cross-validation
    will be performed four times.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码块使用了两个 `for` 循环来执行 `3折交叉验证`，针对所有可能的 `epochs` 和 `batch_size` 组合。由于每个参数都有两个选择，因此有四种不同的组合，所以交叉验证将进行四次。
- en: 'Print the final cross-validation error rate for each of the `epochs`/`batch_size`
    pairs to find out which pair has the lowest error rate:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每对 `epochs`/`batch_size` 的最终交叉验证错误率，以找出哪个组合的错误率最低：
- en: '[PRE26]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here''s an example output:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个示例输出：
- en: '[PRE27]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, the performance for `epochs=150` and `batch_size=15`, and for
    `epochs=100` and `batch_size=20`, are almost the same. Therefore, we will choose
    `epochs=100` and `batch_size=20` in the next step to speed up this process.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，`epochs=150` 和 `batch_size=15`，以及 `epochs=100` 和 `batch_size=20` 的性能几乎相同。因此，我们将在下一步选择
    `epochs=100` 和 `batch_size=20` 来加快这一过程。
- en: 'Use cross-validation again in order to decide on the activation function for
    the hidden layers and the optimizer for the model from `activations = [''relu'',
    ''tanh'']` and `optimizers = [''sgd'', ''adam'', ''rmsprop'']`. Remember to use
    the best pair of `batch_size` and `epochs` from the previous step:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用交叉验证，以决定隐藏层的激活函数和模型的优化器，选择范围为 `activations = ['relu', 'tanh']` 和 `optimizers
    = ['sgd', 'adam', 'rmsprop']`。记得使用上一步骤中最佳的 `batch_size` 和 `epochs` 组合：
- en: '[PRE28]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that we had to modify the `build_model_2` function by passing the `activation`,
    the `optimizer`, and their default values as arguments of the function.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们必须通过将 `activation`、`optimizer` 及其默认值作为函数的参数来修改 `build_model_2` 函数。
- en: 'Print the final cross-validation error rate for each pair of `activation` and
    `optimizer` to find out which pair has the lower error rate:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每对 `activation` 和 `optimizer` 的最终交叉验证错误率，以找出哪个组合的错误率最低：
- en: '[PRE29]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here''s the output:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE30]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `activation='relu'` and `optimizer='adam'` pair results in the lowest error
    rate. Also, the result for the `activation='relu'` and `optimizer='sgd'` pair
    is almost as good. Therefore, we can use either of these optimizers in the final
    model to predict the aquatic toxicity for this dataset.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`activation=''relu''` 和 `optimizer=''adam''` 的组合产生了最低的错误率。同时，`activation=''relu''`
    和 `optimizer=''sgd''` 的组合结果几乎一样好。因此，我们可以在最终模型中使用这两种优化器之一来预测这个数据集的水生毒性。'
- en: Note
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2BYCwbg](https://packt.live/2BYCwbg).
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2BYCwbg](https://packt.live/2BYCwbg)。
- en: You can also run this example online at [https://packt.live/3gofLfP](https://packt.live/3gofLfP).
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，网址是 [https://packt.live/3gofLfP](https://packt.live/3gofLfP)。
- en: Now, you are ready to practice model selection using cross-validation on another
    dataset. In *Activity 4.02*, *Model Selection Using Cross-Validation for the Advanced
    Fibrosis Diagnosis Classifier*, you will practice these steps further by implementing
    them by yourself on a classification problem with the hepatitis C dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好使用交叉验证在另一个数据集上练习模型选择。在*活动 4.02*，*使用交叉验证进行模型选择以诊断高级纤维化分类器*中，你将通过自己实现这些步骤，在肝炎
    C 数据集的分类问题上进一步练习。
- en: Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Exercise 4.02*, *Evaluating Deep Neural Networks with Cross-Validation*, and
    *Exercise 4.03*, *Writing User-Defined Functions to Implement Deep Learning Models
    with Cross-Validation*, involve performing `k-fold cross-validation` several times,
    so the steps may take several minutes to complete. If they are taking too long
    to complete, you may want to try speeding up the process by decreasing the number
    of folds or epochs or increasing the batch sizes. Obviously, if you do so, you
    will get different results compared to the expected outputs, but the same principles
    still apply for selecting the model and hyperparameters.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*练习 4.02*，*使用交叉验证评估深度神经网络*，以及 *练习 4.03*，*编写用户定义的函数实现带交叉验证的深度学习模型*，涉及多次执行 `k折交叉验证`，因此步骤可能需要几分钟才能完成。如果它们需要太长时间才能完成，你可以尝试通过减少折数或训练轮数（epochs）或增加批次大小来加速过程。显然，如果这样做，你将获得与预期输出不同的结果，但选择模型和超参数的原则仍然适用。'
- en: 'Activity 4.02: Model Selection Using Cross-Validation for the Advanced Fibrosis
    Diagnosis Classifier'
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.02：使用交叉验证进行模型选择以诊断高级纤维化分类器
- en: 'In this activity, we are going to improve our classifier for the hepatitis
    C dataset by using cross-validation for model selection and hyperparameter selection.
    Follow these steps:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将通过使用交叉验证来选择模型和超参数，以改进我们的肝炎C数据集分类器。请按照以下步骤操作：
- en: Import the required packages. Load the dataset from the `data` subfolder of
    the `Chapter04` folder from GitHub using `X = pd.read_csv('../data/HCV_feats.csv'),
    y = pd.read_csv('../data/HCV_target.csv')`.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包。从GitHub的`Chapter04`文件夹中的`data`子文件夹加载数据集，使用`X = pd.read_csv('../data/HCV_feats.csv'),
    y = pd.read_csv('../data/HCV_target.csv')`。
- en: 'Define three functions, each returning a different Keras model. The first Keras
    model will be a deep neural network with three hidden layers all of `size 4` and
    `ReLU activation` functions. The second Keras model will be a deep neural network
    with two hidden layers, the first layer of `size 4` and the second later of `size
    2`, and `ReLU activation` functions. The third Keras model will be a deep neural
    network with two hidden layers, both of `size 8`, and a `ReLU activation` function.
    Use the following values for the hyperparameters:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三个函数，每个函数返回一个不同的Keras模型。第一个Keras模型将是一个深度神经网络，具有三个隐藏层，每个隐藏层的`大小为4`，并使用`ReLU激活`函数。第二个Keras模型将是一个深度神经网络，具有两个隐藏层，第一个隐藏层的`大小为4`，第二个隐藏层的`大小为2`，并使用`ReLU激活`函数。第三个Keras模型将是一个深度神经网络，具有两个隐藏层，两个隐藏层的`大小为8`，并使用`ReLU激活`函数。使用以下超参数值：
- en: '`optimizer = ''adam'', loss = ''binary_crossentropy'', metrics = [''accuracy'']`'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`optimizer = ''adam'', loss = ''binary_crossentropy'', metrics = [''accuracy'']`'
- en: Write the code that will loop over the three models and perform `5-fold cross-validation`
    on each of them (use `epochs=100`, `batch_size=20`, and `shuffle=False` in this
    step). Store all the cross-validation scores in a list and print the results.
    Which model results in the best accuracy?
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，遍历三个模型并对每个模型执行`5折交叉验证`（在此步骤中使用`epochs=100`，`batch_size=20`，`shuffle=False`）。将所有的交叉验证得分存储在一个列表中，并打印结果。哪个模型结果获得了最佳准确率？
- en: Note
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '*Steps* *3*, *4*, and *5* of this activity involve performing `5-fold cross-validation`
    three, four, and six times, respectively. Therefore, they may take some time to
    complete.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的*步骤* *3*、*4*和*5*涉及分别对三个、四个和六个模型执行`5折交叉验证`。因此，它们可能需要一些时间来完成。
- en: Write the code that uses the `epochs = [100, 200]` and `batches = [10, 20]`
    values for `epochs` and `batch_size`. Perform 5-fold cross-validation for each
    possible pair on the Keras model that resulted in the best accuracy from *step
    3*. Store all the cross-validation scores in a list and print the results. Which
    `epochs` and `batch_size` pair results in the best accuracy?
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，使用`epochs = [100, 200]`和`batches = [10, 20]`的值分别作为`epochs`和`batch_size`。对每一对可能的组合在Keras模型上进行5折交叉验证，该模型是在*第3步*中得到的最佳准确率。将所有的交叉验证得分存储在一个列表中，并打印结果。哪个`epochs`和`batch_size`的组合结果获得了最佳准确率？
- en: Write the code that uses the `optimizers = ['rmsprop', 'adam','sgd']` and `activations
    = ['relu', 'tanh']` values for `optimizer` and `activation`. Perform 5-fold cross-validation
    for each possible pair on the Keras model that resulted in the best accuracy from
    *step 3*. Use the `batch_size` and `epochs` values that resulted in the best accuracy
    from *step 4*. Store all the cross-validation scores in a list and print the results.
    Which `optimizer` and `activation` pair results in the best accuracy?
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，使用`optimizers = ['rmsprop', 'adam', 'sgd']`和`activations = ['relu', 'tanh']`的值分别作为`optimizer`和`activation`。对每一对可能的组合在Keras模型上进行5折交叉验证，该模型是在*第3步*中得到的最佳准确率。使用在*第4步*中得到的最佳准确率的`batch_size`和`epochs`值。将所有的交叉验证得分存储在一个列表中，并打印结果。哪个`optimizer`和`activation`的组合结果获得了最佳准确率？
- en: Note
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that there is randomness associated with initializing weights and
    biases in a deep neural network, as well as with selecting which examples to include
    in each fold when performing k-fold cross-validation. Therefore, you might get
    a completely different result if you run the exact same code twice. For this reason,
    it is important to set up seeds when building and training neural networks, as
    well as when performing cross-validation. By doing this, you can make sure that
    you are repeating the exact same neural network initialization and the exact same
    training sets and test sets when you rerun the code.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，初始化权重和偏差以及在执行k折交叉验证时选择哪些示例纳入每一折中，都存在随机性。因此，如果你运行相同的代码两次，可能会得到完全不同的结果。为此，在构建和训练神经网络时，以及在执行交叉验证时，设置随机种子非常重要。通过这样做，你可以确保每次重新运行代码时，使用的是完全相同的神经网络初始化，以及完全相同的训练集和测试集。
- en: 'After implementing these steps, the expected output will be as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些步骤后，预期输出将如下所示：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 384.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第 384 页找到。
- en: In this activity, you learned how to use cross-validation to evaluate deep neural
    networks in order to find the model that results in the lowest error rate for
    a classification problem. You also learned how to improve a given classification
    model by using cross-validation in order to find the best set of hyperparameters
    for it. In the next activity, we repeat this activity with a regression task.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，你学习了如何使用交叉验证评估深度神经网络，以找出使分类问题的错误率最低的模型。你还学习了如何通过使用交叉验证来改进给定的分类模型，从而找到最佳的超参数集。在下一次活动中，我们将重复这个过程，并将任务转换为回归问题。
- en: 'Activity 4.03: Model Selection Using Cross-validation on a Traffic Volume Dataset'
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.03：使用交叉验证进行交通流量数据集的模型选择
- en: In this activity, you are going to practice model selection using cross-validation
    one more time. Here, we are going to use a simulated dataset that represents a
    target variable representing the volume of traffic in cars/hour across a city
    bridge and various normalized features related to traffic data such as the time
    of day and the traffic volume on the previous day. Our goal is to build a model
    that predicts the traffic volume across the city bridge given the various features.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，你将再次通过交叉验证进行模型选择练习。在这里，我们将使用一个模拟数据集，数据集表示一个目标变量，代表城市桥梁上每小时的交通流量，以及与交通数据相关的各种归一化特征，如一天中的时间和前一天的交通流量。我们的目标是构建一个模型，基于这些特征预测城市桥梁上的交通流量。
- en: 'The dataset contains `10000` records, and for each of them, `10` attributes/features
    are included. The goal is to build a deep neural network that receives the `10`
    features and predicts the traffic volume across the bridge. Since the output is
    a number, this is a regression problem. Let''s get started:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含`10000`条记录，每条记录包括`10`个属性/特征。目标是构建一个深度神经网络，该网络接收`10`个特征并预测桥上的交通流量。由于输出是一个数字，这个问题是一个回归问题。让我们开始吧：
- en: Import all the required packages.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的包。
- en: Print the input and output sizes to check the number of examples in the dataset
    and the number of features for each example. Also, you can print the range of
    the output (the output in this dataset represents the median value of owner-occupied
    homes in thousands of dollars).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印输入和输出的大小，检查数据集中示例的数量以及每个示例的特征数量。此外，你还可以打印输出的范围（该数据集中的输出代表拥有者自住房屋的中位数，单位为千美元）。
- en: Define three functions, each returning a different Keras model. The first Keras
    model will be a shallow neural network with one hidden layer of `size 10` and
    a `ReLU activation` function. The second Keras model will be a deep neural network
    with two hidden layers of `size 10` and a `ReLU activation` function in each layer.
    The third Keras model will be a deep neural network with three hidden layers of
    `size 10` and a `ReLU activation` function in each layer.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三个函数，每个函数返回一个不同的 Keras 模型。第一个 Keras 模型将是一个浅层神经网络，包含一个隐藏层，隐藏层大小为`10`，并使用`ReLU
    激活`函数。第二个 Keras 模型将是一个深层神经网络，包含两个隐藏层，每个隐藏层的大小为`10`，并使用每层的`ReLU 激活`函数。第三个 Keras
    模型将是一个深层神经网络，包含三个隐藏层，每个隐藏层的大小为`10`，并使用每层的`ReLU 激活`函数。
- en: 'Use the following values as well:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还需使用以下值：
- en: '`optimizer = ''adam'', loss = ''mean_squared_error''`'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`optimizer = ''adam'', loss = ''mean_squared_error''`'
- en: Note
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '*Steps* *4*, *5*, and *6* of this activity involve performing `5-fold cross-validation`
    three, four, and three times, respectively. Therefore, they may take some time
    to complete.'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*步骤* *4*、*5* 和 *6* 涉及分别执行`5折交叉验证`三次、四次和三次。因此，它们可能需要一些时间才能完成。'
- en: Write the code to loop over the three models and perform `5-fold cross-validation`
    on each of them (use `epochs=100`, `batch_size=5`, and `shuffle=False` in this
    step). Store all the cross-validation scores in a list and print the results.
    Which model results in the lowest test error rate?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，循环遍历这三个模型，并对每个模型执行`5折交叉验证`（在这一步中使用`epochs=100`，`batch_size=5`，并且`shuffle=False`）。将所有交叉验证得分存储在一个列表中并打印结果。哪个模型得到了最低的测试错误率？
- en: Write the code that uses the `epochs = [80, 100]` and `batches = [5, 10]` values
    for `epochs` and `batch_size`. Perform 5-fold cross-validation for each possible
    pair on the Keras model that resulted in the lowest test error rate from *step
    4*. Store all the cross-validation scores in a list and print the results. Which
    `epochs` and `batch_size` pair results in the lowest test error rate?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，使用`epochs = [80, 100]`和`batches = [5, 10]`作为`epochs`和`batch_size`的值。对在*第4步*中获得最低测试误差率的Keras模型进行5折交叉验证。将所有交叉验证的得分存储在列表中并打印结果。哪一对`epochs`和`batch_size`值导致最低的测试误差率？
- en: Write the code that uses `optimizers = ['rmsprop', 'sgd', 'adam']` and perform
    `5-fold cross-validation` for each possible optimizer on the Keras model that
    resulted in the lowest test error rate from *step 4*. Use the `batch_size` and
    `epochs` values that resulted in the lowest test error rate from *step 5*. Store
    all the cross-validation scores in a list and print the results. Which `optimizer`
    results in the lowest test error rate?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，使用`optimizers = ['rmsprop', 'sgd', 'adam']`并对每个可能的优化器进行`5折交叉验证`，以评估在*第4步*中获得最低测试误差率的Keras模型。使用在*第5步*中获得最低测试误差率的`batch_size`和`epochs`值。将所有交叉验证的得分存储在列表中并打印结果。哪种`优化器`导致最低的测试误差率？
- en: 'After implementing these steps, the expected output will be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些步骤后，预期输出如下：
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 391.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解答可在第391页找到。
- en: In this activity, you learned how to use cross-validation to evaluate deep neural
    networks in order to find the model that results in the lowest error rate for
    a `regression problem`. Also, you learned how to improve a given regression model
    by using cross-validation in order to find the best set of hyperparameters for
    it.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你学习了如何使用交叉验证来评估深度神经网络，从而找到产生最低误差率的`回归问题`模型。此外，你还学会了如何通过交叉验证改进给定的回归模型，以便找到最佳的超参数集。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about cross-validation, which is one of the most
    important resampling methods. It results in the best estimation of model performance
    on independent data. This chapter covered the basics of cross-validation and its
    two different variations, leave-one-out and k-fold, along with a comparison of
    them.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了交叉验证，这是最重要的重采样方法之一。它能够对模型在独立数据上的表现进行最佳估计。本章介绍了交叉验证的基本知识以及它的两种不同变体，留一法和k折交叉验证，并进行了比较。
- en: Next, we covered the Keras wrapper with scikit-learn, which is a very helpful
    tool that allows scikit-learn methods and functions that perform cross-validation
    to be easily applied to Keras models. Following this, you were shown a step-by-step
    process of implementing cross-validation in order to evaluate Keras deep learning
    models using the Keras wrapper with scikit-learn.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了带有scikit-learn的Keras封装器，这是一个非常有用的工具，它允许将执行交叉验证的scikit-learn方法和函数轻松应用于Keras模型。随后，你将学习如何按照一步步的过程实现交叉验证，以便使用带有scikit-learn的Keras封装器评估Keras深度学习模型。
- en: Finally, you learned that cross-validation estimations of model performance
    can be used to decide between different models for a particular problem or to
    decide which parameters (or hyperparameters) should be used for a particular model.
    You practiced using cross-validation for this purpose by writing user-defined
    functions in order to perform cross-validation on different models or different
    possible combinations of hyperparameters and selecting the model or the set of
    hyperparameters that leads to the lowest test error rate for your final model.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你了解到，交叉验证对模型性能的估计可以用于在不同模型之间做出选择，或者决定某个模型应使用哪些参数（或超参数）。你通过编写用户自定义函数并执行交叉验证，练习了如何使用交叉验证来选择最终模型的最佳模型或超参数组合，从而使测试误差率最小。
- en: In the next chapter, you will learn that what we did here in order to find the
    best set of hyperparameters for our model is, in fact, a technique called `hyperparameter
    tuning` or `hyperparameter optimization`. Also, you will learn how to perform
    hyperparameter tuning in scikit-learn by using a method called `grid search` and
    without the need to write user-defined functions to loop over possible combinations
    of hyperparameters.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习到，实际上我们在这里为模型寻找最佳超参数集的过程，称为`超参数调优`或`超参数优化`。此外，你将学习如何通过scikit-learn中的一种方法`网格搜索`来进行超参数调优，并且无需编写用户自定义函数来遍历可能的超参数组合。
