- en: Generating Book Scripts Using LSTMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LSTM 生成书籍脚本
- en: '**Natural language generation** (**NLG**), which is a sub-field of artificial
    intelligence, is a natural language processing task of generating human-readable
    text from various data inputs. It is an active area of research that has achieved
    great popularity in recent times.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言生成**（**NLG**），作为人工智能的一个子领域，是从各种数据输入中生成可读的文本的自然语言处理任务。这是一个活跃的研究领域，近年来已获得广泛关注。'
- en: The ability to generate natural language through machines can have wide variety
    of applications, including text autocomplete feature in phones, generating the
    summary of a document, and even generating new scripts for comedies. Google's
    Smart Reply also uses a technology that runs on similar lines to give reply suggestions
    when you're writing an email.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器生成自然语言的能力可以有广泛的应用，包括手机中的文本自动补全功能、文档摘要生成，甚至是为喜剧创作新剧本。Google 的智能回复也使用了一种类似的技术来在你写电子邮件时提供回复建议。
- en: In this chapter, we will look at an NLG task of generating a book script from
    another Packt book that goes by the name of *Mastering PostgreSQL 10*. We took
    almost 100 pages of this book and removed any figures, tables, and SQL code. The
    data is fairly large and has enough words for a neural network to learn the nuances
    of the dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究一个 NLG 任务——从另一本名为《*Mastering PostgreSQL 10*》的 Packt 书籍中生成书籍脚本。我们从这本书中挑选了近
    100 页，移除了所有图表、表格和 SQL 代码。数据量相当大，足以让神经网络学习数据集的细微差别。
- en: 'We will learn how to generate book scripts using reinforcement neural networks
    by going through the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下主题学习如何使用强化学习神经网络生成书籍脚本：
- en: Introduction to recurrent neural networks and LSTMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络和 LSTM 简介
- en: Description of the book script dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书籍脚本数据集的描述
- en: Modeling using LSTMs and generating a new book script
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LSTM 建模并生成新的书籍脚本
- en: Understanding recurrent neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解循环神经网络
- en: '**Recurrent neural networks** (**RNNs**) have become extremely popular for
    any task that involves sequential data. The core idea behind RNNs is to exploit
    the sequential information present in the data. Under usual circumstances, every
    neural network assumes that all of the inputs are independent of each other. However,
    if we are trying to predict the next word in a sequence or the next point in a
    time series, it is imperative to use information based on the words used prior or
    on the historical points in the time series.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）已成为任何涉及序列数据的任务中极为流行的选择。RNNs 的核心理念是利用数据中存在的序列信息。通常情况下，每个神经网络假设所有输入彼此独立。然而，如果我们要预测序列中的下一个词或时间序列中的下一个点，就必须使用基于之前使用的词或时间序列中的历史点的信息。'
- en: One way to perceive the concept of RNNs is that they have a memory that stores
    information about historical data in a sequence. In theory, RNNs can remember
    history for arbitrarily long sequences, however, in practice, they do a bad job
    in tasks where the historical information needs to be retained for more than a
    few steps back.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一种理解循环神经网络（RNNs）概念的方法是，它们具有一个内存，能够存储关于序列中历史数据的信息。理论上，RNNs 可以记住任意长序列的历史，但在实际应用中，它们在需要保留超过几个步骤的历史信息的任务中表现不佳。
- en: 'The typical structure of a RNN is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的典型结构如下：
- en: '![](img/f8d2ba9e-f120-4469-96d3-28b8893e4eed.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8d2ba9e-f120-4469-96d3-28b8893e4eed.png)'
- en: In the preceding diagram, *Xt* is the sequence value at different time steps.
    RNNs are called **recurrent** as they apply the exact same operation on every
    element of the sequence, with the output being dependent on the preceding steps.
    The connection between cells can be clearly observed. These connections help to
    transfer information from the previous step to the next.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，*Xt* 是不同时间步长的序列值。RNNs 被称为**循环**，因为它们对序列中的每个元素应用相同的操作，输出依赖于前一步的结果。可以清楚地观察到单元之间的连接，这些连接帮助将信息从前一步传递到下一步。
- en: 'As mentioned previously, RNNs are not great at capturing long-term dependencies.
    There are different variants of RNNs. A few of them are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，RNNs 并不擅长捕捉长期依赖关系。RNNs 有不同的变种，其中一些如下：
- en: '**Long Short-Term Memory** (**LSTMs**)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTMs**）'
- en: '**Gated recurrent units** (**GRU**)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**门控循环单元**（**GRU**）'
- en: Peephole LSTMs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盯孔 LSTM（Peephole LSTMs）
- en: 'LSTMs are better at capturing long-term dependencies in comparison to vanilla
    RNNs. LSTMs have become very popular regarding tasks such as word/sentence prediction,
    image caption generation, and even forecasting time series data that requires
    long-term dependencies. The following are some of the advantages of using LSTMs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的RNN相比，LSTM在捕捉长期依赖方面表现更好。LSTM在诸如单词/句子预测、图像字幕生成，甚至是需要长期依赖的时间序列数据预测等任务中变得非常流行。以下是使用LSTM的一些优点：
- en: Great at modeling tasks involving long-term dependencies
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 擅长建模涉及长期依赖的任务
- en: Weight sharing between different time steps greatly reduces the number of parameters
    in the model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同时间步之间的权重共享大大减少了模型中的参数数量
- en: Suffers less from the vanishing and exploding gradient problem faced by traditional
    RNNs
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比传统的RNN更少受到梯度消失和梯度爆炸问题的困扰
- en: 'The following are some of the disadvantages of using LSTMs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用LSTM的一些缺点：
- en: LSTMs are data-hungry. They usually require a lot of training data to produce
    any meaningful results.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM对数据有较高需求。通常需要大量的训练数据才能产生有意义的结果。
- en: Slower to train than traditional neural networks.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度比传统神经网络慢。
- en: There are computationally more efficient RNN variants such as GRU that achieve
    a similar performance as LSTMs.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在计算效率更高的RNN变种，如GRU，它们能实现与LSTM相似的性能。
- en: A discussion of other types of RNNs is outside the scope of this chapter. If
    you are interested, you can refer to the sequential modeling chapter in the *Deep
    Learning* Book ([https://www.deeplearningbook.org/contents/rnn.html](https://www.deeplearningbook.org/contents/rnn.html)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的内容不涉及其他类型的RNN。如果您感兴趣，可以参考*《深度学习》*书中的序列建模章节([https://www.deeplearningbook.org/contents/rnn.html](https://www.deeplearningbook.org/contents/rnn.html))。
- en: Pre-processing the data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'As mentioned previously, the dataset used in this project is from a popular
    Packt book that goes by the name of *Mastering PostgreSQL 10*, and was written
    by Hans-Jürgen Schönig ([https://www.cybertec-postgresql.com](https://www.cybertec-postgresql.com)). We
    considered text from the first 100 pages of the book, excluding any figures, tables,
    and SQL code. The cleaned dataset is stored, alongside the code, in a text file.
    The dataset contains almost 44,000 words, which is just enough to train the model.
    The following are a few lines from the script:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本项目使用的数据集来自一本流行的Packt书籍，书名为*《Mastering PostgreSQL 10》*，作者是Hans-Jürgen Schönig
    ([https://www.cybertec-postgresql.com](https://www.cybertec-postgresql.com))。我们考虑了书籍前100页的文本，排除了所有图形、表格和SQL代码。清理后的数据集与代码一起存储在一个文本文件中。数据集包含近44,000个单词，足以用来训练模型。以下是脚本中的几行：
- en: '*"PostgreSQL Overview*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*"PostgreSQL概述*'
- en: '*PostgreSQL is one of the world''s most advanced open source database systems,
    and it has many features that are widely used by developers and system administrators
    alike. Starting with PostgreSQL 10, many new features have been added to PostgreSQL,
    which contribute greatly to the success of this exceptional open source product.
    In this book, many of these cool features will be covered and discussed in great
    detail.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*PostgreSQL是世界上最先进的开源数据库系统之一，具有许多功能，广泛受到开发者和系统管理员的使用。从PostgreSQL 10开始，许多新功能已被添加到PostgreSQL中，这些新功能极大地促进了这一卓越开源产品的成功。在本书中，将详细讲解和讨论许多这些酷炫的功能。*'
- en: '*In this chapter, you will be introduced to PostgreSQL and the cool new features
    available in PostgreSQL 10.0 and beyond. All relevant new functionalities will
    be covered in detail. Given the sheer number of changes made to the code and given
    the size of the PostgreSQL project, this list of features is of course by far
    not complete, so I tried to focus on the most important aspects that are relevant
    to most people.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本章中，您将了解PostgreSQL以及PostgreSQL 10.0及更高版本中的一些酷炫的新功能。所有相关的新功能将详细讲解。鉴于代码修改的数量以及PostgreSQL项目的庞大规模，这个功能列表显然远非完整，因此我尝试专注于最重要、最相关的大多数人都会用到的方面。*'
- en: '*The features outlined in this chapter will be split into the following categories
    Database administration*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章中概述的功能将分为以下几个类别：数据库管理*'
- en: '*SQL and developer related Backup, recovery, and replication Performance related
    topics*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*与SQL和开发者相关的备份、恢复和复制、性能相关主题*'
- en: '*What is new in PostgreSQL 10.0.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*PostgreSQL 10.0中的新功能。*'
- en: '*PostgreSQL 10.0 was released in late 2017 and is the first version that follows
    the new numbering scheme introduced by the PostgreSQL community. From now on,
    the way major releases are done will change and therefore, the next major version
    after PostgreSQL*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*PostgreSQL 10.0于2017年底发布，是第一个采用PostgreSQL社区引入的新编号方案的版本。从现在开始，主要版本的发布方式将发生变化，因此，PostgreSQL之后的下一个主要版本*'
- en: '*10.0 will not be 10.1 but PostgreSQL 11\. Versions 10.1 and 10.2 are merely
    service releases and will only contain bug fixes."*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*10.0不会是10.1，而是PostgreSQL 11。版本10.1和10.2只是服务版本，只会包含错误修复。*'
- en: 'For pre-processing the data so that we prepare it for a LSTM model, go through
    the following steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预处理数据并为LSTM模型做准备，请按照以下步骤进行：
- en: '**Tokenize punctuation**: While pre-processing, we consider the splitting criteria
    as words using spaces. However, in this scenario, the neural network will have
    a hard time distinguishing words such as Hello and Hello!. Due to this limitation,
    it is required that you tokenize the punctuations in the dataset. For example,
    `!` will be mapped to `_Sym_Exclamation_`. In the code, we implement a function
    named `define_tokens`. This is used to create a dictionary. Here, each piece of
    punctuation is considered a key, and its respective token is a value. In this
    example, we will create tokens for the following symbols:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标记化标点符号**：在预处理过程中，我们将拆分标准设定为使用空格分隔的单词。然而，在这种情况下，神经网络将难以区分像“Hello”和“Hello!”这样的词。由于这个限制，需要在数据集中对标点符号进行标记化。例如，`!`
    将被映射为 `_Sym_Exclamation_`。在代码中，我们实现了一个名为 `define_tokens` 的函数。它用于创建一个字典，在这个字典中，每个标点符号是键，对应的标记是值。这个示例中，我们将为以下符号创建标记：'
- en: Period ( . )
  id: totrans-39
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句号 ( . )
- en: Comma ( , )
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逗号 ( , )
- en: Quotation mark (" )
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引号 ( " )
- en: Semicolon ( ; )
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分号 ( ; )
- en: Exclamation mark ( ! )
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感叹号 ( ! )
- en: Question mark ( ? )
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问号 ( ? )
- en: Left parenthesis ( ( )
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左括号 ( ( )
- en: Right parenthesis ( ) )
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右括号 ( ) )
- en: Dash ( -- )
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连字符 ( -- )
- en: Return ( \n )
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回 ( \n )
- en: Avoid using a token that will probably be a word in the dataset. For example,
    `?` is replaced by `_Sym_Question_`, which is not a word in the dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使用数据集中可能出现的词语。例如，`?` 被替换为 `_Sym_Question_`，这在数据集中不是一个单词。
- en: '**Lower and split**: We must convert all of the uppercase letters in the text
    to lowercase so that the neural network will learn that the two words "Hello"
    and "hello" are actually the same. As the basic unit of input to neural networks
    will be words, the very next step would be to split the sentences in the text
    into words.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转为小写并分割**：我们必须将文本中的所有大写字母转换为小写字母，以便神经网络能够学习到“Hello”和“hello”实际上是相同的两个词。由于神经网络的基本输入单元是单词，下一步就是将文本中的句子拆分为单词。'
- en: '**Map creation**: Neural networks do not accept text as an input, and so we
    need to map these words to indexes/IDs. To do this, we must create two dictionaries,
    as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**映射创建**：神经网络不能直接接受文本作为输入，因此我们需要将这些单词映射到索引/ID。为此，我们必须创建两个字典，如下所示：'
- en: '`Vocab_to_int`: Mapping of each word in the text to its unique ID'
  id: totrans-52
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Vocab_to_int`：将文本中的每个单词映射到其唯一的ID'
- en: '`Int_to_vocab`: Inverse dictionary which maps IDs to their corresponding words'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Int_to_vocab`：反向字典，将ID映射到对应的单词'
- en: Defining the model
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'Before training the model using the pre-processed data, let''s understand the
    model definition for this problem. In the code, we define a model class in the `model.py` file.
    The class contains four major components, and are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用预处理数据训练模型之前，我们先了解一下这个问题的模型定义。在代码中，我们在 `model.py` 文件中定义了一个模型类。该类包含四个主要部分，具体如下：
- en: '**Input**: We define the TensorFlow placeholders in the model for both input
    (X) and target (Y).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：我们在模型中定义了TensorFlow的占位符，用于输入（X）和目标（Y）。'
- en: '**Network definition**: There are four components of the network for this model.
    They are as follows:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络定义**：该模型的网络有四个组件，具体如下：'
- en: '**Initializing the LSTM cell**: To do this, we begin by stacking two layers
    of LSTMs together. We then set the size of the LSTM to be a `RNN_SIZE` parameter,
    which is as defined in the code. RNN is then initialized with a zero state.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始化LSTM单元**：为此，我们首先将两层LSTM堆叠在一起。然后，我们将LSTM的大小设置为代码中定义的 `RNN_SIZE` 参数。接着，RNN被初始化为零状态。'
- en: '**Word embeddings**: We encode the words in the text using word embeddings
    rather than one-hot encoding. This is done, mainly, to reduce the dimension of
    the training set, which can help neural networks learn faster. We generate embeddings
    from a uniform distribution for each word in the vocabulary and use TensorFlow''s
    `embedding_lookup` function to get embedding sequences for the input data.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：我们使用词嵌入来对文本中的词进行编码，而不是使用one-hot编码。这样做的主要目的是减少训练集的维度，从而帮助神经网络更快地学习。我们从均匀分布中为词汇表中的每个词生成嵌入，并使用TensorFlow的`embedding_lookup`函数来获取输入数据的嵌入序列。'
- en: '**Building LSTMs**: To obtain the final state of LSTMs, we use TensorFlow''s
    `tf.nn.dynamic_rnn` function with the initial cell and the embeddings of the input
    data.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建LSTM**：为了获得LSTM的最终状态，我们使用TensorFlow的`tf.nn.dynamic_rnn`函数，并传入初始单元和输入数据的嵌入。'
- en: '**Probability generation**: After obtaining the final state and output from
    LSTM, we pass it through a fully connected layer to generate logits for predictions.
    We convert those logits into probability estimates by using the `softmax` function.
    The code is as follows:'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率生成**：在获得LSTM的最终状态和输出后，我们将其通过一个全连接层生成预测的logits。我们使用`softmax`函数将这些logits转换为概率估计。代码如下：'
- en: '**Sequence loss**: We must define the loss, which in this case is the Sequence
    loss. This is nothing but a weighted cross entropy for a sequence of logits. We
    equally weight the observations across batch and time.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列损失**：我们必须定义损失函数，在这个情况下是序列损失。这实际上只是对一系列logits进行加权交叉熵损失计算。我们在批次和时间上对观测值进行等权重处理。'
- en: '**Optimizer**: We will use the Adam optimizer, along with its default parameters.
    We will also clip the gradients to keep it within the range of -1 to 1\. Gradient
    clipping is a common phenomenon in recurrent neural networks. When gradients are
    backpropagated in time, they can vanish if they are constantly multiplied by numbers
    less than 1, or can explode due to being multiplied by numbers greater than 1\.
    Gradient clipping will help to resolve both of these problems by restricting the
    gradient to be between -1 and 1.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：我们将使用Adam优化器，并保持其默认参数。我们还将裁剪梯度，确保其在-1到1的范围内。梯度裁剪是递归神经网络中的常见现象。当梯度在时间上反向传播时，如果它们不断地与小于1的数相乘，梯度可能会消失；或者如果与大于1的数相乘，梯度可能会爆炸。梯度裁剪通过将梯度限制在-1到1之间，帮助解决这两个问题。'
- en: Training the model
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Before understanding the implementation of the training loop, let's take a closer
    look at how we can generate batches of data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解训练循环的实现之前，让我们仔细看看如何生成数据批次。
- en: It is common knowledge that batches are used in neural networks to speed up
    the training of the model and to consume less memory. Batches are samples of the
    original dataset that are used for a forward and backward pass to the network.
    The forward pass refers to the process of multiplying inputs with weights of different
    layers in the network and obtaining the final output. The backward pass, on the
    other hand, refers to the process of updating the weights in the neural network
    based on the loss obtained from the outputs of the forward pass.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，神经网络使用批次来加速模型训练，并减少内存消耗。批次是原始数据集的样本，用于网络的正向传播和反向传播。正向传播指的是将输入与网络中不同层的权重相乘并获得最终输出的过程。反向传播则是基于正向传播输出的损失，更新神经网络中的权重。
- en: 'In this model, since we are predicting the next set of words given a set of
    previous words to generate the TV script, the targets are basically the next few
    (depending on sequence length) words in the original training dataset. Let''s
    consider an example where the training dataset contains the following line:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，由于我们是在根据一组前置词来预测下一个词组以生成电视脚本，目标基本上是原始训练数据集中下一个词（根据序列长度）的几个词。我们来看一个例子，假设训练数据集包含如下内容：
- en: '*The quick brown fox jumps over the lazy dog*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*The quick brown fox jumps over the lazy dog*'
- en: 'If the sequence length (the number of words to process together) used is 4,
    then the following are true:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用的序列长度（处理的词数）是4，那么以下内容成立：
- en: X is the sequence of every four words, for example, [*The quick brown fox*,
    *quick brown fox jumps* …..] .
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X是每四个词的序列，例如，[*The quick brown fox*, *quick brown fox jumps* …..]。
- en: Y is the sequence of every four words, skipping the first word, for example,
    [*quick brown fox jumps*, *brown fox jumps over* …].
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y是每四个词的序列，跳过第一个词，例如，[*quick brown fox jumps*, *brown fox jumps over* …]。
- en: Defining and training a text-generating model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义并训练一个文本生成模型
- en: 'Begin by loading the saved text data for pre-processing with the help of the
    `load_data` function:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`load_data`函数加载保存的文本数据以进行预处理：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Implement `define_tokens`, as defined in the *Pre-processing the data* section
    of this chapter. This will help us create a dictionary of the key words and their
    corresponding tokens:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`define_tokens`，如本章*数据预处理*部分所定义。这将帮助我们创建一个关键字及其相应tokens的字典：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dictionary that we've created will be used to replace the punctuation marks
    in the dataset with their respective tokens and delimiters (space in this case)
    around them. For example, `Hello!` will be replaced with `Hello _Sym_Exclamation_`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的字典将用于用相应的tokens和分隔符（此处为空格）替换数据集中的标点符号。例如，`Hello!`将被替换为`Hello _Sym_Exclamation_`。
- en: Note that there is a space between `Hello` and the token. This will help the
    LSTM model treat each punctuation marks as its own word.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`Hello`和token之间有一个空格。这将帮助LSTM模型将每个标点符号当作独立的单词来处理。
- en: 'Map the words to indexes/IDs with the help of the `Vocab_to_int` and `int_to_vocab` dictionaries.
    We are doing this since neural networks do not accept text as input:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Vocab_to_int`和`int_to_vocab`字典帮助将单词映射到索引/ID。我们这样做是因为神经网络不接受文本作为输入：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Combine all of the preceding steps to create a function that will pre-process
    the data that''s available for us:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前面所有步骤结合起来，创建一个函数来预处理我们可用的数据：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will then generate integer text for the mapping dictionaries and dump the
    pre-processed data and relevant dictionaries in a `pickle` file.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将为映射字典生成整数文本，并将预处理后的数据和相关字典存储到`pickle`文件中。
- en: 'To define our model, we will create a model class in the `model.py` file. We
    will begin by defining the input:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了定义我们的模型，我们将在`model.py`文件中创建一个模型类。我们将首先定义输入：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We must define variable type to be integers since the words in the dataset have
    been transformed to integers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将变量类型定义为整数，因为数据集中的单词已被转换为整数。
- en: 'Define the network of our model by defining the LSTM cell, word embeddings, building
    LSTMs, and probability generation. To define the LSTM cell, stack two LSTM layers
    and set the size of the LSTM to be a `RNN_SIZE` parameter. Assign RNN the value
    0:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过定义LSTM单元、词嵌入、构建LSTM和概率生成来定义我们的模型网络。为了定义LSTM单元，堆叠两个LSTM层，并将LSTM的大小设置为`RNN_SIZE`参数。将RNN的值设置为0：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To reduce the dimension of the training set and increase the speed of the neural
    network, generate and look up embeddings using the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少训练集的维度并提高神经网络的速度，使用以下代码生成并查找嵌入：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Run the `tf.nn.dynamic_rnn` function to find the final state of the LSTMs:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`tf.nn.dynamic_rnn`函数以找到LSTM的最终状态：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Convert the logits obtained from the final state of the LSTMs to a probability
    estimate by using the `softmax` function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`softmax`函数将LSTM最终状态获得的logits转换为概率估计：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define a weighted cross entropy or sequence loss for a sequence of logits,
    which further helps fine-tune our network:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为logits序列定义加权交叉熵或序列损失，这有助于进一步微调我们的网络：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Implement the Adam optimizer with the default parameters, and clip the gradients
    to keep it within the range of `-1` to `1` to avoid diminishing the gradient when
    it is backpropagated in time:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认参数实现Adam优化器，并将梯度裁剪到`-1`到`1`的范围内，以避免在反向传播过程中梯度消失：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the sequence length using the `generate_batch_data` function. This helps
    generate batches that are necessary for the neural network training:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`generate_batch_data`函数定义序列长度。这有助于生成神经网络训练所需的批次：
- en: The input for this function will be the text data that is encoded as integers,
    batch size, and sequence length.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该函数的输入将是编码为整数的文本数据、批次大小和序列长度。
- en: 'The output will be a numpy array with the shape [# batches, 2, batch size,
    sequence length]. Each batch contains two parts, defined as follows:'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出将是一个形状为[#批次，2，批次大小，序列长度]的numpy数组。每个批次包含两个部分，定义如下：
- en: X with shape [batch size, sequence length]
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: X的形状为[批次大小，序列长度]。
- en: 'Y with shape [batch size, sequence length]:'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y的形状为[批次大小，序列长度]：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Train the model using the following parameters:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下参数训练模型：
- en: Num Epochs = 500
  id: totrans-106
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮次 = 500
- en: Learning Rate = 0.001
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 = 0.001
- en: Batch Size = 128
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次大小 = 128
- en: RNN Size = 128
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN大小 = 128
- en: 'Sequence Length= 32:'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度 = 32：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since the dataset wasn't very large, the code was executed on the CPU itself. We
    will save the output graph, since it will come in useful for generating book scripts.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集不是很大，代码是在CPU上执行的。我们将保存输出图，因为它将对生成书籍脚本非常有用。
- en: Generating book scripts
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成书籍脚本
- en: 'Now that the model has been trained, we can have some fun with it. In this
    section, we will see how we can use the model to generate book scripts. Use the
    following parameters:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练好了，我们可以玩得更开心。在本节中，我们将看到如何使用模型生成书籍脚本。使用以下参数：
- en: Script Length = 200 words
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脚本长度 = 200 个单词
- en: Starting word = `postgresql`
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起始词 = `postgresql`
- en: 'Follow these steps to generate the model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤生成模型：
- en: Load the graph of the trained model.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练模型的图。
- en: 'Extract four tensors, as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取四个张量，如下所示：
- en: Input/input:0
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入/input:0
- en: Network/initial_state:0
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络/initial_state:0
- en: Network/final_state:0
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络/final_state:0
- en: Network/probs:0
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络/probs:0
- en: 'Extract the four tensors using the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码提取四个张量：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the starting word and obtain an initial state from the graph, which
    will be used later:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义起始词并从图中获得初始状态，稍后会使用这个初始状态：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Given a starting word and an initial state, proceed to iterate over a for loop
    to generate the next word for the script. In each iteration of the for loop, generate
    the probabilities from the model using the previously generated sequence as input
    and select the word with a maximum probability using the `select_next_word` function:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个起始词和初始状态，继续通过for循环迭代生成脚本中的下一个单词。在for循环的每次迭代中，使用之前生成的序列作为输入，从模型中生成概率，并使用`select_next_word`函数选择概率最大的单词：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create a loop to generate the next word in the sequence:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个循环来生成序列中的下一个单词：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Join all of the words in the sentences using a space delimiter and replace
    the punctuation tokens with the actual symbols. The obtained script is then saved
    in a text file for future reference:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用空格分隔符将句子中的所有单词连接起来，并将标点符号替换为实际符号。然后将获得的脚本保存在文本文件中，供以后参考：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is a sample of the text that was generated from the execution:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是执行过程中生成的文本示例：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Interestingly, the model learns to use a full stop after a sentence, leaves
    a blank line between paragraphs, and follows basic grammar. The model has learned
    all of this by itself, without us having to provide any guidance/rules. Despite
    the fact that the script is far from perfect, it's amazing how a machine is able
    to generate real-sounding sentences of a book. We can further fine-tune the hyperparameters
    of the model to generate more meaningful text.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，模型学会了在句子后使用句号，在段落之间留空行，并遵循基本语法。模型通过自我学习掌握了这一切，我们无需提供任何指导或规则。尽管生成的脚本远未完美，但看到机器能够生成类似书籍的真实句子，实在令人惊讶。我们可以进一步调整模型的超参数，生成更有意义的文本。
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about how LSTMs can be used to generate book scripts.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了如何使用LSTM生成书籍脚本。
- en: We began by looking at the basics of RNNs and its popular variant, commonly
    known as LSTMs. We learned that RNNs are hugely successful in predicting datasets
    that involve sequences such as time series, next word prediction in natural language
    processing tasks, and so on. We also looked at the advantages and disadvantages
    of using LSTMs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从RNN的基础知识以及其流行变体——通常称为LSTM的模型开始学习。我们了解到，RNN在预测涉及时间序列、自然语言处理任务中的下一个单词预测等顺序数据集方面非常成功。我们还了解了使用LSTM的优缺点。
- en: This chapter then helped us understand how to pre-process text data and prepare
    it so that we can feed it into LSTMs. We also looked at the model's structure
    for training. Next, we looked at how to train the neural networks by creating
    batches of data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本章帮助我们理解了如何对文本数据进行预处理，并将其准备好，以便可以输入到LSTM模型中。我们还了解了用于训练的模型结构。接下来，我们学习了如何通过创建数据批次来训练神经网络。
- en: Finally, we understood how to generate book script using the TensorFlow model
    we trained. Although the script that was generated doesn't make complete sense,
    it was amazing to observe the neural network generate a book's sentences. We then
    saved the generated book script in a text file for future reference.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们理解了如何使用我们训练的TensorFlow模型生成书籍脚本。尽管生成的脚本并不完全有意义，但看到神经网络生成书籍的句子仍然令人惊叹。然后我们将生成的书籍脚本保存到文本文件中，供以后参考。
- en: In the next chapter, we shall play Pac-Man using deep reinforcement learning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用深度强化学习玩吃豆人游戏。
- en: Questions
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following are the questions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是问题：
- en: Can you try to use from a different book to see how well the model is able to
    generate new text?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能尝试使用另一本书来看看模型生成新文本的效果如何吗？
- en: What happens to the generated text if you double the batch size and decrease
    the learning rate?
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你将批处理大小加倍并减小学习率，生成的文本会发生什么变化？
- en: Can you train the model without gradient clipping and see if the result improves?
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在不使用梯度裁剪的情况下训练模型，看看结果是否有所改进吗？
