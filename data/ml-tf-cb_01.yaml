- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Getting Started with TensorFlow 2.x
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 TensorFlow 2.x
- en: Google's TensorFlow engine has a unique way of solving problems, allowing us
    to solve machine learning problems very efficiently. Nowadays, machine learning
    is used in almost all areas of life and work, with famous applications in computer
    vision, speech recognition, language translations, healthcare, and many more.
    We will cover the basic steps to understand how TensorFlow operates and eventually
    build up to production code techniques later in the pages of this book. For the
    moment, the fundamentals presented in this chapter are paramount in order to provide
    you with a core understanding for the recipes found in the rest of this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的 TensorFlow 引擎有一种独特的解决问题方式，使我们能够非常高效地解决机器学习问题。如今，机器学习已应用于几乎所有的生活和工作领域，著名的应用包括计算机视觉、语音识别、语言翻译、医疗保健等。我们将在本书的后续页面中覆盖理解
    TensorFlow 操作的基本步骤，并最终讲解如何构建生产代码。此时，本章中呈现的基础内容至关重要，它们将为你提供核心理解，帮助你更好地理解本书中其他部分的食谱。
- en: 'In this chapter, we''ll start by covering some basic recipes and helping you
    to understand how TensorFlow 2.x works. You''ll also learn how to access the data
    used to run the examples in this book, and how to get additional resources. By
    the end of this chapter, you should have knowledge of the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从一些基本的食谱开始，帮助你理解 TensorFlow 2.x 的工作原理。你还将学习如何访问本书中示例所使用的数据，以及如何获取额外的资源。到本章结束时，你应该掌握以下知识：
- en: Understanding how TensorFlow 2.x works
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 TensorFlow 2.x 的工作原理
- en: Declaring and using variables and tensors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明和使用变量与张量
- en: Working with matrices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与矩阵合作
- en: Declaring operations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明操作
- en: Implementing activation functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现激活函数
- en: Working with data sources
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据源合作
- en: Finding additional resources
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找额外资源
- en: Without any further ado, let's begin with the first recipe, which presents in
    an easy fashion the way TensorFlow deals with data and computations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不再多说，让我们从第一个食谱开始，它以简易的方式展示了 TensorFlow 如何处理数据和计算。
- en: How TensorFlow works
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 的工作原理
- en: 'Started as an internal project by researchers and engineers from the Google
    Brain team, initially named **DistBelief**, an open source framework for high
    performance numerical computations was released in November 2015 under the name
    TensorFlow (tensors are a generalization of scalars, vectors, matrices, and higher
    dimensionality matrices). You can read the original paper on the project here:
    [http://download.tensorflow.org/paper/whitepaper2015.pdf](http://download.tensorflow.org/paper/whitepaper2015.pdf).
    After the appearance of version 1.0 in 2017, last year, Google released TensorFlow
    2.0, which continues the development and improvement of TensorFlow by making it
    more user-friendly and accessible.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 最初是由 Google Brain 团队的研究人员和工程师作为一个内部项目开始的，最初名为 **DistBelief**，并于 2015
    年 11 月发布为一个开源框架，名称为 TensorFlow（张量是标量、向量、矩阵及更高维度矩阵的广义表示）。你可以在这里阅读关于该项目的原始论文：[http://download.tensorflow.org/paper/whitepaper2015.pdf](http://download.tensorflow.org/paper/whitepaper2015.pdf)。在
    2017 年发布 1.0 版本后，去年，Google 发布了 TensorFlow 2.0，它在继续发展和改进 TensorFlow 的同时，使其变得更加用户友好和易于使用。
- en: Production-oriented and capable of handling different computational architectures
    (CPUs, GPUs, and now TPUs), TensorFlow is a framework for any kind of computation
    that requires high performance and easy distribution. It excels at deep learning,
    making it possible to create everything from shallow networks (neural networks
    made of a few layers) to complex deep networks for image recognition and natural
    language processing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个面向生产的框架，能够处理不同的计算架构（CPU、GPU，现在还有 TPU），适用于需要高性能和易于分布式的各种计算。它在深度学习领域表现出色，可以创建从浅层网络（由少数层组成的神经网络）到复杂的深度网络，用于图像识别和自然语言处理。
- en: In this book, we're going to present a series of recipes that will help you
    use TensorFlow for your deep learning projects in a more efficient way, cutting
    through complexities and helping you achieve both a wider scope of applications
    and much better results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将呈现一系列食谱，帮助你以更高效的方式使用 TensorFlow 进行深度学习项目，减少复杂性，帮助你实现更广泛的应用并取得更好的结果。
- en: 'At first, computation in TensorFlow may seem needlessly complicated. But there
    is a reason for it: because of how TensorFlow deals with computation, when you
    become accustomed to TensorFlow style, developing more complicated algorithms
    becomes relatively easy. This recipe will guide us through the pseudocode of a
    TensorFlow algorithm.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，TensorFlow 中的计算可能看起来不必要地复杂。但这背后是有原因的：由于 TensorFlow 处理计算的方式，当你习惯了 TensorFlow
    风格时，开发更复杂的算法会变得相对容易。本方案将引导我们通过 TensorFlow 算法的伪代码。
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Currently, TensorFlow is tested and supported on the following 64-bit systems:
    Ubuntu 16.04 or later, macOS 10.12.6 (Sierra) or later (no GPU support, though),
    Raspbian 9.0 or later, and Windows 7 or later. The code for this book has been
    developed and tested on an Ubuntu system, but it should run fine on any other
    system as well. The code for the book is available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook),
    which acts as the book repository for all the code and some data.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，TensorFlow 已在以下 64 位系统上测试并获得支持：Ubuntu 16.04 或更高版本、macOS 10.12.6（Sierra）或更高版本（不过不支持
    GPU）、Raspbian 9.0 或更高版本，以及 Windows 7 或更高版本。本书中的代码已在 Ubuntu 系统上开发并测试，但它在其他任何系统上也应该能正常运行。本书的代码可在
    GitHub 上找到，地址为 [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)，它作为本书所有代码和一些数据的代码库。
- en: Throughout this book, we'll only concern ourselves with the Python library wrapper
    of TensorFlow, although most of the original core code for TensorFlow is written
    in C++. TensorFlow operates nicely with Python, ranging from version 3.7 to 3.8\.
    This book will use Python 3.7 (you can get the plain interpreter at [https://www.python.org](https://www.python.org))
    and TensorFlow 2.2.0 (you can find all the necessary instructions to install it
    at [https://www.tensorflow.org/install](https://www.tensorflow.org/install)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将只关注 TensorFlow 的 Python 库封装，尽管 TensorFlow 的大多数核心代码是用 C++ 编写的。TensorFlow
    与 Python 很好地兼容，支持 3.7 至 3.8 版本。此书将使用 Python 3.7（你可以在 [https://www.python.org](https://www.python.org)
    获取该解释器）和 TensorFlow 2.2.0（你可以在 [https://www.tensorflow.org/install](https://www.tensorflow.org/install)
    查找安装它所需的所有说明）。
- en: While TensorFlow can run on the CPU, most algorithms run faster if processed
    on a GPU, and it is supported on graphics cards with Nvidia Compute Capability
    3.5 or higher (preferable when running complex networks that are more computationally
    intensive).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TensorFlow 可以在 CPU 上运行，但大多数算法如果在 GPU 上处理，运行会更快，并且支持在具有 Nvidia 计算能力 3.5 或更高版本的显卡上运行（特别是在运行计算密集型的复杂网络时更为推荐）。
- en: All the recipes you'll find in the book are compatible with TensorFlow 2.2.0\.
    Where necessary, we'll point out the differences in syntax and execution with
    the previous 2.1 and 2.0 versions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你在书中找到的所有方案都与 TensorFlow 2.2.0 兼容。如有必要，我们将指出与以前的 2.1 和 2.0 版本在语法和执行上的区别。
- en: Popular GPUs for running scripts based on TensorFlow on a workstation are Nvidia
    Titan RTX and Nvidia Quadro RTX models, whereas in data centers, we instead commonly
    find Nvidia Tesla architectures with at least 24 GB of memory (for instance, Google
    Cloud Platform offers GPU Nvidia Tesla K80, P4, T4, P100 and V100 models). To
    run properly on a GPU, you will also need to download and install the Nvidia CUDA
    toolkit, version 5.x+ ([https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作站上运行基于 TensorFlow 的脚本时，常用的 GPU 有 Nvidia Titan RTX 和 Nvidia Quadro RTX，而在数据中心，我们通常会找到至少配备
    24 GB 内存的 Nvidia Tesla 架构（例如，Google Cloud Platform 提供了 Nvidia Tesla K80、P4、T4、P100
    和 V100 型号）。要在 GPU 上正常运行，你还需要下载并安装 Nvidia CUDA 工具包，版本为 5.x+（[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)）。
- en: Some of the recipes in this chapter will rely on an installation of the current
    versions of SciPy, NumPy, and Scikit-learn Python packages. These accompanying
    packages are also included in the Anaconda package (https://www.anaconda.com/products/individual#Downloads).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的一些方案将依赖于安装当前版本的 SciPy、NumPy 和 Scikit-learn Python 包。这些附带包也包含在 Anaconda 包中（https://www.anaconda.com/products/individual#Downloads）。
- en: How to do it…
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行…
- en: 'Here, we''ll introduce the general flow of TensorFlow algorithms. Most recipes
    will follow this outline:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍 TensorFlow 算法的一般流程。大多数方案将遵循这个大纲：
- en: '**Import or generate datasets**: All of our machine learning algorithms will
    depend on datasets. In this book, we''ll either generate data or use an outside
    source of datasets. Sometimes, it''s better to rely on generated data because
    we can control how to vary and verify the expected outcome. Most of the time,
    we will access public datasets for the given recipe. The details on accessing
    these datasets can be found in the *Additional resources* recipe at the end of
    this chapter:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入或生成数据集**：我们所有的机器学习算法都依赖于数据集。在本书中，我们将生成数据或使用外部数据源。有时，依赖生成的数据更好，因为我们可以控制如何变化并验证预期结果。大多数情况下，我们将访问给定食谱的公共数据集。有关如何访问这些数据集的详细信息，请参见本章末尾的
    *附加资源* 章节：'
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Transform and normalize data**: Generally, input datasets do not come in
    the exact form we want for what we intend to achieve. TensorFlow expects us to
    transform the data into the accepted shape and data type. In fact, the data is
    usually not in the correct dimension or type that our algorithms expect, and we
    will have to transform it properly before we can use it. Most algorithms also
    expect normalized data (which implies variables whose mean is zero and whose standard
    deviation is one) and we will look at how to accomplish this here as well. TensorFlow
    offers built-in functions that can load your data, split your data into batches,
    and allow you to transform variables and normalize each batch using simple NumPy
    functions, including the following:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转换和规范化数据**：通常，输入数据集的形式并不是我们实现目标所需要的精确形式。TensorFlow 期望我们将数据转换为接受的形状和数据类型。实际上，数据通常不符合我们算法所期望的正确维度或类型，我们必须在使用之前正确地转换它。大多数算法还期望规范化数据（这意味着变量的均值为零，标准差为一），我们也将在这里讨论如何实现这一点。TensorFlow
    提供了内置函数，可以加载数据、将数据拆分为批次，并允许您使用简单的 NumPy 函数转换变量和规范化每个批次，包括以下内容：'
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Partition the dataset into training, test, and validation sets**: We generally
    want to test our algorithms on different sets that we have trained on. Many algorithms
    also require hyperparameter tuning, so we set aside a validation set for determining
    the best set of hyperparameters.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据集划分为训练集、测试集和验证集**：我们通常希望在不同的数据集上测试我们的算法，这些数据集是我们训练过的。许多算法还需要进行超参数调整，因此我们预留了一个验证集，用于确定最佳的超参数组合。'
- en: '**Set algorithm parameters (hyperparameters)**: Our algorithms usually have
    a set of parameters that we hold constant throughout the procedure. For example,
    this could be the number of iterations, the learning rate, or other fixed parameters
    of our choice. It''s considered good practice to initialize these together using
    global variables, so that the reader or user can easily find them, as follows:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置算法参数（超参数）**：我们的算法通常会有一组参数，这些参数在整个过程中保持不变。例如，这可能是迭代次数、学习率或我们选择的其他固定参数。通常建议将这些参数一起初始化为全局变量，以便读者或用户能够轻松找到它们，如下所示：'
- en: '[PRE2]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Initialize variables**: TensorFlow depends on knowing what it can and cannot
    modify. TensorFlow will modify/adjust the variables (model weights/biases) during
    optimization to minimize a loss function. To accomplish this, we feed in data
    through input variables. We need to initialize both variables and placeholders
    with size and type so that TensorFlow knows what to expect. TensorFlow also needs
    to know the type of data to expect. For most of this book, we will use `float32`.
    TensorFlow also provides `float64` and `float16` data types. Note that more bytes
    are used for precision results in slower algorithms, but fewer bytes results in
    less precision of the resulting algorithm. Refer to the following code for a simple
    example of how to set up an array of weights and a vector of biases in TensorFlow:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化变量**：TensorFlow 依赖于知道它可以修改什么以及不能修改什么。在优化过程中，TensorFlow 将修改/调整变量（模型的权重/偏置），以最小化损失函数。为了实现这一点，我们通过输入变量输入数据。我们需要初始化变量和占位符的大小和类型，以便
    TensorFlow 知道该期待什么。TensorFlow 还需要知道期望的数据类型。在本书的大部分内容中，我们将使用 `float32`。TensorFlow
    还提供了 `float64` 和 `float16` 数据类型。请注意，使用更多字节来获得更高精度会导致算法变慢，而使用更少字节则会导致结果算法的精度降低。请参考以下代码，了解如何在
    TensorFlow 中设置一个权重数组和一个偏置向量的简单示例：'
- en: '[PRE3]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Define the model structure**: After we have the data, and have initialized
    our variables, we have to define the model. This is done by building a computational
    graph. The model for this example will be a logistic regression model (logit *E*(*Y*)
    = b*X* + a):'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义模型结构**：在获得数据并初始化变量之后，我们必须定义模型。这是通过构建计算图来完成的。这个示例中的模型将是一个逻辑回归模型（logit *E*(*Y*)
    = b*X* + a）：'
- en: '[PRE4]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Declare the loss functions**: After defining the model, we must be able to
    evaluate the output. This is where we declare the loss function. The loss function
    is very important as it tells us how far off our predictions are from the actual
    values. The different types of loss function are explored in greater detail in
    the *Implementing Backpropagation* recipe in *Chapter 2*, *The TensorFlow Way.*
    Here, as an example, we implement the cross entropy with logits, which computes
    softmax cross entropy between logits and labels:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**声明损失函数**：在定义模型之后，我们必须能够评估输出。这就是我们声明损失函数的地方。损失函数非常重要，因为它告诉我们预测值与实际值之间的偏差。不同类型的损失函数将在
    *第二章*，*TensorFlow 实践方式* 中的 *实现反向传播* 这一章节中详细探讨。在此，我们以交叉熵为例，使用 logits 计算 softmax
    交叉熵与标签之间的差异：'
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Initialize and train the model**: Now that we have everything in place, we
    need to create an instance of our graph, feed in the data, and let TensorFlow
    change the variables to predict our training data better. Here is one way to initialize
    the computational graph and, by means of multiple iterations, converge the weights
    in the model structure using the SDG optimizer:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化并训练模型**：现在我们已经准备好了一切，需要创建图的实例，输入数据，并让 TensorFlow 调整变量，以更好地预测我们的训练数据。以下是初始化计算图的一种方法，通过多次迭代，使用
    SDG 优化器收敛模型结构中的权重：'
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Evaluate the model**: Once we''ve built and trained the model, we should
    evaluate the model by looking at how well it does with new data through some specified
    criteria. We evaluate on the training and test set, and these evaluations will
    allow us to see whether the model is under or overfitting. We will address this
    in later recipes. In this simple example, we evaluate the final loss and compare
    the fitted values against the ground truth training ones:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：一旦我们建立并训练了模型，我们应该通过某些指定的标准评估模型，看看它在新数据上的表现如何。我们会在训练集和测试集上进行评估，这些评估将帮助我们判断模型是否存在过拟合或欠拟合的问题。我们将在后续的内容中讨论这个问题。在这个简单的例子中，我们评估最终的损失，并将拟合值与真实的训练值进行比较：'
- en: '[PRE7]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Tune hyperparameters**: Most of the time, we will want to go back and change
    some of the hyperparameters, checking the model''s performance based on our tests.
    We then repeat the previous steps with different hyperparameters and evaluate
    the model on the validation set.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整超参数**：大多数时候，我们会希望回到之前的步骤，调整一些超参数，并根据我们的测试结果检查模型的性能。然后，我们使用不同的超参数重复之前的步骤，并在验证集上评估模型。'
- en: '**Deploy/predict new outcomes**: It is also a key requirement to know how to
    make predictions on new and unseen data. We can achieve this easily with TensorFlow
    with all of our models once we have them trained.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署/预测新结果**：了解如何对新数据和未见过的数据进行预测也是一个关键要求。一旦我们训练好模型，就可以通过 TensorFlow 轻松实现这一点。'
- en: How it works…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In TensorFlow, we have to set up the data, input variables, and model structure
    before we can tell the program to train and tune its weights to improve predictions.
    TensorFlow accomplishes this through computational graphs. These computational
    graphs are directed graphs with no recursion, which allows for computational parallelism.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们必须先设置数据、输入变量和模型结构，然后才能告诉程序训练并调整其权重，以提高预测效果。TensorFlow 通过计算图来完成这项工作。计算图是没有递归的有向图，允许并行计算。
- en: To do this, we need to create a loss function for TensorFlow to minimize. TensorFlow
    accomplishes this by modifying the variables in the computational graph. TensorFlow
    knows how to modify the variables because it keeps track of the computations in
    the model and automatically computes the variable gradients (how to change each
    variable) to minimize the loss. Because of this, we can see how easy it can be
    to make changes and try different data sources.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要创建一个损失函数，以便 TensorFlow 最小化它。TensorFlow 通过修改计算图中的变量来实现这一点。TensorFlow 能够修改变量，因为它跟踪模型中的计算，并自动计算变量的梯度（如何改变每个变量），以最小化损失。因此，我们可以看到，进行更改并尝试不同数据源是多么容易。
- en: See also
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For a further introduction to TensorFlow and more on its resources, refer to
    the official documentation and tutorials at TensorFlow''s official page: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 TensorFlow 的进一步介绍以及更多资源，请参考 TensorFlow 官方页面上的官方文档和教程：[https://www.tensorflow.org/](https://www.tensorflow.org/)
- en: Within the official pages, a more encyclopedic place to start with is the official
    Python API documentation, [https://www.tensorflow.org/api_docs/python/](https://www.tensorflow.org/api_docs/python/),
    where you will find all the possible commands enumerated
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在官方页面中，一个更具百科全书性质的入门位置是官方 Python API 文档，[https://www.tensorflow.org/api_docs/python/](https://www.tensorflow.org/api_docs/python/)，在那里您可以找到所有可能的命令列表。
- en: 'There are also tutorials available: [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有教程可供学习：[https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)
- en: 'Besides that, an unofficial collection of TensorFlow tutorials, projects, presentations,
    and code repositories can be found here: [https://github.com/dragen1860/TensorFlow-2.x-Tutorials](https://github.com/dragen1860/TensorFlow-2.x-Tutorials
    )'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外，还可以在这里找到一个非官方的 TensorFlow 教程、项目、演示和代码库集合：[https://github.com/dragen1860/TensorFlow-2.x-Tutorials](https://github.com/dragen1860/TensorFlow-2.x-Tutorials
    )
- en: Declaring variables and tensors
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明变量和张量
- en: Tensors are the primary data structure that TensorFlow uses to operate on the
    computational graph. Even if now, in TensorFlow 2.x, this aspect is hidden, the
    data flow graph is still operating behind the scenes. This means that the logic
    of building a neural network doesn't change all that much between TensorFlow 1.x
    and TensorFlow 2.x. The most eye-catching aspect is that you no longer have to
    deal with placeholders, the previous entry gates for data in a TensorFlow 1.x
    graph.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是 TensorFlow 用于在计算图上进行操作的主要数据结构。即使在 TensorFlow 2.x 中，这一方面被隐藏了，但数据流图仍然在幕后运行。这意味着构建神经网络的逻辑在
    TensorFlow 1.x 和 TensorFlow 2.x 之间并没有发生太大变化。最引人注目的变化是，您不再需要处理占位符，后者是 TensorFlow
    1.x 图中数据的输入门。
- en: Now, you simply declare tensors as variables and proceed to building your graph.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您只需将张量声明为变量，然后继续构建图。
- en: A *tensor* is a mathematical term that refers to generalized vectors or matrices.
    If vectors are one-dimensional and matrices are two-dimensional, a tensor is *n*-dimensional
    (where *n* could be 1, 2, or even larger).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*张量* 是一个数学术语，指的是广义的向量或矩阵。如果向量是一维的，矩阵是二维的，那么张量就是 *n* 维的（其中 *n* 可以是 1、2 或更大）。'
- en: We can declare these tensors as variables and use them for our computations.
    To do this, first, we must learn how to create tensors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些张量声明为变量，并将它们用于计算。为了做到这一点，我们首先需要学习如何创建张量。
- en: Getting ready
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: When we create a tensor and declare it as a variable, TensorFlow creates several
    graph structures in our computation graph. It is also important to point out that
    just by creating a tensor, TensorFlow is not adding anything to the computational
    graph. TensorFlow does this only after running an operation to initialize the
    variables. See the next section, on variables and placeholders, for more information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个张量并将其声明为变量时，TensorFlow 会在我们的计算图中创建多个图结构。还需要指出的是，仅仅创建一个张量并不会向计算图中添加任何内容。TensorFlow
    仅在执行操作以初始化变量后才会这样做。有关更多信息，请参阅下节关于变量和占位符的内容。
- en: How to do it…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: Here, we will cover the four main ways in which we can create tensors in TensorFlow.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍在 TensorFlow 中创建张量的四种主要方式。
- en: We will not be unnecessarily exhaustive in this recipe or others. We will tend
    to illustrate only the mandatory parameters of the different API calls, unless
    you might find it interesting for the recipe to cover any optional parameter;
    when that happens, we'll justify the reasoning behind it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱或其他食谱中，我们不会进行不必要的详细说明。我们倾向于仅说明不同 API 调用中的必需参数，除非您认为覆盖某些可选参数对食谱有帮助；当这种情况发生时，我们会说明其背后的理由。
- en: 'Fixed size tensors:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 固定大小的张量：
- en: 'In the following code, we create a zero-filled tensor:'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们创建了一个全为 0 的张量：
- en: '[PRE8]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the following code, we create a one-filled tensor:'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们创建了一个全为 1 的张量：
- en: '[PRE9]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the following code, we create a constant-filled tensor:'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们创建了一个常量填充的张量：
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the following code, we create a tensor out of an existing constant:'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们从一个现有常量创建了一个张量：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the `tf.constant()` function can be used to broadcast a value into
    an array, mimicking the behavior of `tf.fill()` by writing `tf.constant(42, [row_dim,
    col_dim])`.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，`tf.constant()` 函数可以用来将一个值广播到数组中，通过写 `tf.constant(42, [row_dim, col_dim])`
    来模仿 `tf.fill()` 的行为。
- en: '**Tensors of similar shape**: We can also initialize variables based on the
    shape of other tensors, as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相似形状的张量**：我们也可以根据其他张量的形状初始化变量，如下所示：'
- en: '[PRE12]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that since these tensors depend on prior tensors, we must initialize them
    in order. Attempting to initialize the tensors in a random order will result in
    an error.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，由于这些张量依赖于先前的张量，我们必须按顺序初始化它们。尝试以随机顺序初始化张量会导致错误。
- en: '**Sequence tensors**: In TensorFlow, all parameters are documented as tensors.
    Even when scalars are required, the API mentions these as zero-dimensional scalars.
    It won''t therefore be a surprise that TensorFlow allows us to specify tensors
    that contain defined intervals. The following functions behave very similarly
    to NumPy''s `linspace()` outputs and `range()` outputs (for reference: [https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)).
    See the following function:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**序列张量**：在TensorFlow中，所有的参数都被文档化为张量。即使需要标量，API也会将其作为零维标量提及。因此，TensorFlow允许我们指定包含定义区间的张量也就不足为奇了。以下函数的行为与NumPy的`linspace()`输出和`range()`输出非常相似（参考：
    [https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)）。请看以下函数：'
- en: '[PRE13]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that the start and stop parameters should be float values, and that `num`
    should be an integer.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，起始值和终止值参数应为浮点值，而`num`应为整数。
- en: 'The resultant tensor has a sequence of [0.0, 0.5, 1.0] (the `print(linear_tsr`
    command will provide the necessary output). Note that this function includes the
    specified stop value. See the following `tf.range` function for comparison:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果张量的序列为[0.0, 0.5, 1.0]（`print(linear_tsr`命令将提供必要的输出）。请注意，此函数包括指定的终止值。以下是`tf.range`函数的对比：
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result is the sequence [6, 9, 12]. Note that this function does not include
    the limit value and it can operate with both integer and float values for the
    start and limit parameters.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果是序列[6, 9, 12]。请注意，此函数不包括限制值，并且可以处理起始值和限制值的整数和浮点数。
- en: '**Random tensors**: The following generated random numbers are from a uniform
    distribution:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机张量**：以下生成的随机数来自均匀分布：'
- en: '[PRE15]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that this random uniform distribution draws from the interval that includes
    `minval` but not `maxval` (`minval <= x < maxval`). Therefore, in this case, the
    output range is [0, 1). If, instead, you need to draw only integers and not floats,
    just add the `dtype=tf.int32` parameter when calling the function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种随机均匀分布是从包含`minval`但不包含`maxval`的区间中抽取的（`minval <= x < maxval`）。因此，在这种情况下，输出范围是[0,
    1)。如果你需要仅抽取整数而不是浮点数，只需在调用函数时添加`dtype=tf.int32`参数。
- en: 'To get a tensor with random draws from a normal distribution, you can run the
    following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获得从正态分布中随机抽取的张量，可以运行以下代码：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'There are also times where we want to generate normal random values that are
    assured within certain bounds. The `truncated_normal()` function always picks
    normal values within two standard deviations of the specified mean:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些情况，我们希望生成在某些范围内保证的正态分布随机值。`truncated_normal()`函数总是从指定均值的两个标准差内挑选正态值：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We might also be interested in randomizing entries of arrays. To accomplish
    this, two functions can help us: `random.shuffle()`and `image.random_crop()`.
    The following code performs this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能还对随机化数组的条目感兴趣。为此，两个函数可以帮助我们：`random.shuffle()`和`image.random_crop()`。以下代码执行此操作：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Later on in this book, we''ll be interested in randomly cropping images of
    size (height, width, 3) where there are three-color spectrums. To fix a dimension
    in `cropped_output`, you must give it the maximum size in that dimension:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书后续的内容中，我们将关注对大小为（高度，宽度，3）的图像进行随机裁剪，其中包含三种颜色光谱。为了在`cropped_output`中固定某一维度，你必须为该维度指定最大值：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This code snippet will generate random noise images that will be cropped, halving
    both the height and width, but the depth dimension will be untouched because you
    fixed its maximum value as a parameter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将生成随机噪声图像，这些图像会被裁剪，既能减小高度和宽度的一半，但深度维度不会受影响，因为你已将其最大值固定为参数。
- en: How it works…
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Once we have decided how to create the tensors, we may also create the corresponding
    variables by wrapping the tensor in the `Variable()` function, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定了如何创建张量，我们还可以通过将张量包装在`Variable()`函数中来创建相应的变量，如下所示：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There's more on this in the following recipes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中会有更多内容介绍。
- en: There's more…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We are not limited to the built-in functions: we can convert any NumPy array
    into a Python list, or a constant into a tensor using the `convert_to_tensor()`
    function. Note that this function also accepts tensors as an input in case we
    wish to generalize a computation inside a function.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不局限于内置函数：我们可以将任何 NumPy 数组转换为 Python 列表，或使用 `convert_to_tensor()` 函数将常量转换为张量。注意，这个函数也接受张量作为输入，以便我们希望在函数内对计算进行通用化时使用。
- en: Using eager execution
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用急切执行
- en: When developing deep and complex neural networks, you need to continuously experiment
    with architectures and data. This proved difficult in TensorFlow 1.0 because you
    always need to run your code from the beginning to end in order to check whether
    it worked. TensorFlow 2.x works in eager execution mode as default, which means
    that you develop and check your code step by step as you progress into your project.
    This is great news; now we just have to understand how to experiment with eager
    execution, so we can use this TensorFlow 2.x feature to our advantage. This recipe
    will provide you with the basics to get started.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发深度且复杂的神经网络时，你需要不断地尝试不同的架构和数据。这在 TensorFlow 1.0 中是困难的，因为你总是需要从头到尾运行代码，以检查是否成功。TensorFlow
    2.x 默认在急切执行模式下工作，这意味着你可以随着项目的进展，逐步开发并检查代码。这是个好消息；现在我们只需要理解如何在急切执行模式下进行实验，这样我们就能充分利用
    TensorFlow 2.x 的这一特性。本教程将为你提供入门的基础知识。
- en: Getting ready
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: TensorFlow 1.x performed optimally because it executed its computations after
    compiling a static computational graph. All computations were distributed and
    connected into a graph as you compiled your network and that graph helped TensorFlow
    to execute computations, leveraging the available resources (multi-core CPUs of
    multiple GPUs) in the best way, and splitting operations between the resources
    in the most timely and efficient way. That also meant, in any case, that once
    you defined and compiled your graph, you could not change it at runtime but had
    to instantiate it from scratch, thereby incurring some extra work.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 1.x 的表现最优，因为它在编译静态计算图之后执行计算。所有计算都被分配并连接成一个图，你编译网络时，该图帮助 TensorFlow
    执行计算，利用可用资源（多核 CPU 或多个 GPU）以最佳方式，并在资源之间以最及时高效的方式分配操作。这也意味着，无论如何，一旦你定义并编译了图，就不能在运行时对其进行更改，而必须从头开始实例化，这会带来额外的工作量。
- en: In TensorFlow 2.x, you can still define your network, compile it, and run it
    optimally, but the team of TensorFlow developers has now favored, by default,
    a more experimental approach, allowing immediate evaluation of operations, thus
    making it easier to debug and to try network variations. This is called eager
    execution. Operations now return concrete values instead of pointers to parts
    of a computational graph to be built later. More importantly, you can now have
    all the functionality of the host language available while your model is executing,
    making it easier to write more complex and sophisticated deep learning solutions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 2.x 中，你仍然可以定义你的网络，编译并以最佳方式运行它，但 TensorFlow 开发团队现在默认采用了一种更为实验性的方式，允许立即评估操作，从而使调试更容易，尝试网络变种也更加方便。这就是所谓的急切执行。现在，操作返回的是具体的值，而不是指向稍后构建的计算图部分的指针。更重要的是，你现在可以在模型执行时使用主机语言的所有功能，这使得编写更复杂、更精细的深度学习解决方案变得更容易。
- en: How to do it…
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'You basically don''t have to do anything; eager execution is the default way
    of operating in TensorFlow 2.x. When you import TensorFlow and start using its
    functions, you operate in eager execution since you can perform checks when executing:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你基本上不需要做任何事情；在 TensorFlow 2.x 中，**急切执行（eager execution）**是默认的操作方式。当你导入 TensorFlow
    并开始使用它的功能时，你会在急切执行模式下工作，因为你可以在执行时进行检查：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That's all you need to do.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要做的全部。
- en: How it works…
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Just run TensorFlow operations and the results will return immediately:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 只需运行 TensorFlow 操作，结果将立即返回：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That's all there is to it!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这么简单！
- en: There's more…
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: As TensorFlow is now set on eager execution as default, you won't be surprised
    to hear that `tf.Session` has been removed from the TensorFlow API. You no longer
    need to build a computational graph before running a computation; all you have
    to do now is build your network and test it along the way. This opens the road
    to common software best practices, such as documenting the code, using object-oriented
    programming when scripting your code, and organizing it into reusable self-contained
    modules.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TensorFlow 现在默认启用了即时执行模式，你不会惊讶地发现 `tf.Session` 已经从 TensorFlow API 中移除。你不再需要在运行计算之前构建计算图；现在你只需构建你的网络，并在过程中进行测试。这为常见的软件最佳实践打开了道路，比如文档化代码、在编写代码时使用面向对象编程，以及将代码组织成可重用的自包含模块。
- en: Working with matrices
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与矩阵的操作
- en: Understanding how TensorFlow works with matrices is very important when developing
    the flow of data through computational graphs. In this recipe, we will cover the
    creation of matrices and the basic operations that can be performed on them with
    TensorFlow.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 TensorFlow 如何与矩阵协作在开发数据流通过计算图时非常重要。在这个方案中，我们将涵盖矩阵的创建以及可以使用 TensorFlow 执行的基本操作。
- en: 'It is worth emphasizing the importance of matrices in machine learning (and
    mathematics in general): machine learning algorithms are computationally expressed
    as matrix operations. Knowing how to perform matrix computations is a plus when
    working with TensorFlow, though you may not need it often; its high-end module,
    Keras, can deal with most of the matrix algebra stuff behind the scenes (more
    on Keras in *Chapter 3,* *Keras*).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，矩阵在机器学习（以及数学中的一般应用）中的重要性：机器学习算法在计算上是通过矩阵运算来表示的。了解如何进行矩阵运算是使用 TensorFlow
    时的一个加分项，尽管你可能不需要经常使用它；其高级模块 Keras 可以在后台处理大多数矩阵代数内容（更多关于 Keras 的内容请参见*第 3 章*，*Keras*）。
- en: This book does not cover the mathematical background on matrix properties and
    matrix algebra (linear algebra), so the unfamiliar reader is strongly encouraged
    to learn enough about matrices to be comfortable with matrix algebra. In the *See
    also* section, you can find a couple of resources to help you to revise your calculus
    skills or build them from scratch, and get even more out of TensorFlow.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涵盖矩阵属性和矩阵代数（线性代数）的数学背景，因此不熟悉的读者强烈建议学习足够的矩阵知识，以便能够熟练掌握矩阵代数。在*另见*部分，你可以找到一些资源，帮助你复习微积分技巧或从零开始学习，以便从
    TensorFlow 中获得更多收益。
- en: Getting ready
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Many algorithms depend on matrix operations. TensorFlow gives us easy-to-use
    operations to perform such matrix calculations. You just need to import TensorFlow
    and follow this section to the end; if you're not a matrix algebra expert, please
    first have a look at the *See also* section of this recipe for resources to help
    you to get the most out of the following recipe.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法依赖于矩阵运算。TensorFlow 为我们提供了易于使用的操作来执行这些矩阵计算。你只需要导入 TensorFlow，并按照本节内容进行操作；如果你不是矩阵代数专家，请首先查看本方案的*另见*部分，寻找帮助你充分理解下面方案的资源。
- en: How to do it…
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We proceed as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按如下步骤进行：
- en: '**Creating matrices**: We can create two-dimensional matrices from NumPy arrays
    or nested lists, as described in the *Declaring and using variables and tensors*
    recipe at the beginning of this chapter. We can use the tensor creation functions
    and specify a two-dimensional shape for functions such as `zeros()`, `ones()`,
    and `truncated_normal()`. TensorFlow also allows us to create a diagonal matrix
    from a one-dimensional array or list using the `diag()` function, as follows:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建矩阵**：我们可以从 NumPy 数组或嵌套列表创建二维矩阵，正如本章开头的*声明和使用变量与张量*方案中所描述的那样。我们可以使用张量创建函数，并为如
    `zeros()`、`ones()` 和 `truncated_normal()` 等函数指定二维形状。TensorFlow 还允许我们使用 `diag()`
    函数从一维数组或列表创建对角矩阵，示例如下：'
- en: '[PRE23]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Please note that the C tensor is created in a random way, and it will probably
    differ in your session from what is represented in this book.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，C 张量是随机创建的，它可能与你的会话中显示的内容有所不同。
- en: '[PRE24]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Addition, subtraction, and multiplication**: To add, subtract, or multiply
    matrices of the same dimension, TensorFlow uses the following function:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加法、减法和乘法**：要对相同维度的矩阵进行加法、减法或乘法，TensorFlow 使用以下函数：'
- en: '[PRE25]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It is important to note that the `matmul()` function has arguments that specify
    whether or not to transpose the arguments before multiplication (the Boolean parameters,
    `transpose_a` and `transpose_b`), or whether each matrix is sparse (`a_is_sparse`
    and `b_is_sparse`).
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要注意的是，`matmul()`函数有一些参数，用于指定是否在乘法前转置参数（布尔参数`transpose_a`和`transpose_b`），或者每个矩阵是否为稀疏矩阵（`a_is_sparse`和`b_is_sparse`）。
- en: 'If, instead, you need element-wise multiplication between two matrices of the
    same shape and type (this is very important or you will get an error), you just
    use the `tf.multiply` function:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你需要对两个形状和类型相同的矩阵进行逐元素乘法（这非常重要，否则会报错），只需使用`tf.multiply`函数：
- en: '[PRE26]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that matrix division is not explicitly defined. While many define matrix
    division as multiplying by the inverse, it is fundamentally different from real-numbered
    division.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，矩阵除法没有明确定义。虽然许多人将矩阵除法定义为乘以逆矩阵，但它在本质上不同于实数除法。
- en: '**The transpose**: Transpose a matrix (flip the columns and rows) as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转置**：转置矩阵（翻转列和行），如下所示：'
- en: '[PRE27]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Again, it is worth mentioning that reinitializing gives us different values
    than before.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次提到，重新初始化会给我们不同于之前的值。
- en: '**Determinant:** To calculate the determinant, use the following code:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**行列式**：要计算行列式，请使用以下代码：'
- en: '[PRE28]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Inverse**: To find the inverse of a square matrix, see the following:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逆矩阵**：要找到一个方阵的逆矩阵，请参阅以下内容：'
- en: '[PRE29]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The inverse method is based on Cholesky decomposition only if the matrix is
    symmetric positive definite. If the matrix is not symmetric positive definite,
    then it is based on LU decomposition.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逆矩阵方法仅在矩阵是对称正定时，基于Cholesky分解。如果矩阵不是对称正定的，则基于LU分解。
- en: '**Decompositions**: For Cholesky decomposition, use the following code:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分解**：对于Cholesky分解，请使用以下代码：'
- en: '[PRE30]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Eigenvalues and eigenvectors**: For eigenvalues and eigenvectors, use the
    following code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征值和特征向量**：对于特征值和特征向量，请使用以下代码：'
- en: '[PRE31]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that the `tf.linalg.eigh()` function outputs two tensors: in the first,
    you find the **eigenvalues** and, in the second tensor, you have the **eigenvectors**.
    In mathematics, such an operation is known as the **eigendecomposition** of a
    matrix.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`tf.linalg.eigh()`函数输出两个张量：第一个张量包含**特征值**，第二个张量包含**特征向量**。在数学中，这种操作称为矩阵的**特征分解**。
- en: How it works…
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理……
- en: TensorFlow provides all the tools for us to get started with numerical computations
    and adding these computations to our neural networks.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow为我们提供了所有开始进行数值计算的工具，并将这些计算添加到我们的神经网络中。
- en: See also
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you need to build your calculus skills quickly and understand more about
    TensorFlow operations, we suggest the following resources:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要快速提升微积分技能，并深入了解更多关于TensorFlow操作的内容，我们建议以下资源：
- en: 'The free book *Mathematics for Machine Learning*, which can be found here:
    [https://mml-book.github.io/](https://mml-book.github.io/). This contains everything
    you need to know if you want to operate successfully with machine learning in
    general.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这本免费的书籍《机器学习数学》（Mathematics for Machine Learning），可以在这里找到：[https://mml-book.github.io/](https://mml-book.github.io/)。如果你想在机器学习领域中成功操作，这本书包含了你需要知道的一切。
- en: For an even more accessible source, watch the lessons about vectors and matrices
    from the Kahn Academy ([https://www.khanacademy.org/math/precalculus](https://www.khanacademy.org/math/precalculus))
    to get to work with the most basic data elements of a neural network.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个更加易于获取的资源，可以观看Khan Academy的关于向量和矩阵的课程（[https://www.khanacademy.org/math/precalculus](https://www.khanacademy.org/math/precalculus)），以便学习神经网络中最基本的数据元素。
- en: Declaring operations
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明操作
- en: Apart from matrix operations, there are hosts of other TensorFlow operations
    we must at least be aware of. This recipe will provide you with a quick and essential
    glance at what you really need to know.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了矩阵操作，TensorFlow还有许多其他操作我们至少应该了解。这个教程将为你提供一个简要且必要的概览，帮助你掌握真正需要知道的内容。
- en: Getting ready
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好
- en: 'Besides the standard arithmetic operations, TensorFlow provides us with more
    operations that we should be aware of. We should acknowledge them and learn how
    to use them before proceeding. Again, we just import TensorFlow:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的算术操作外，TensorFlow还为我们提供了更多需要了解的操作。在继续之前，我们应该认识到这些操作并学习如何使用它们。再次提醒，我们只需要导入TensorFlow：
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now we're ready to run the code to be found in the following section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备运行接下来的代码。
- en: How to do it…
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'TensorFlow has the standard operations on tensors, that is, `add()`, `subtract()`,
    `multiply()`, and `division()` in its `math` module. Note that all of the operations
    in this section will evaluate the inputs elementwise, unless specified otherwise:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了张量的标准操作，即`add()`、`subtract()`、`multiply()`和`division()`，它们都位于`math`模块中。请注意，本节中的所有操作，除非另有说明，否则都将逐元素计算输入：
- en: TensorFlow provides some variations of `division()` and the relevant functions.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow还提供了`division()`及相关函数的变种。
- en: 'It is worth mentioning that `division()` returns the same type as the inputs.
    This means that it really returns the floor of the division (akin to Python 2)
    if the inputs are integers. To return the Python 3 version, which casts integers
    into floats before dividing and always returns a float, TensorFlow provides the
    `truediv()` function, as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值得注意的是，`division()`返回与输入相同类型的结果。这意味着如果输入是整数，它实际上返回除法的地板值（类似Python 2）。要返回Python
    3版本的除法，即在除法前将整数转换为浮点数并始终返回浮点数，TensorFlow提供了`truediv()`函数，具体如下：
- en: '[PRE33]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If we have floats and want integer division, we can use the `floordiv()` function.
    Note that this will still return a float, but it will be rounded down to the nearest
    integer. This function is as follows:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们有浮点数并且需要整数除法，可以使用`floordiv()`函数。请注意，这仍然会返回浮点数，但它会向下舍入到最接近的整数。该函数如下：
- en: '[PRE34]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Another important function is `mod()`. This function returns the remainder
    after division. It is as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个重要的函数是`mod()`。该函数返回除法后的余数，具体如下：
- en: '[PRE35]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The cross product between two tensors is achieved by the `cross()` function.
    Remember that the cross product is only defined for two three-dimensional vectors,
    so it only accepts two three-dimensional tensors. The following code illustrates
    this use:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个张量的叉积通过`cross()`函数实现。请记住，叉积仅对两个三维向量定义，因此它只接受两个三维张量。以下代码展示了这一用法：
- en: '[PRE36]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here''s a compact list of the more common math functions. All of these functions
    operate elementwise:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是常用数学函数的简明列表。所有这些函数均逐元素操作：
- en: '| Function | Operation |'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 函数 | 操作 |'
- en: '| `tf.math.abs()` | Absolute value of one input tensor |'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.abs()` | 输入张量的绝对值 |'
- en: '| `tf.math.ceil()` | Ceiling function of one input tensor |'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.ceil()` | 输入张量的向上取整函数 |'
- en: '| `tf.math.cos()` | Cosine function of one input tensor |'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.cos()` | 输入张量的余弦函数 |'
- en: '| `tf.math.exp()` | Base *e* exponential of one input tensor |'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.exp()` | 输入张量的底数 *e* 指数函数 |'
- en: '| `tf.math.floor()` | Floor function of one input tensor |'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.floor()` | 输入张量的向下取整函数 |'
- en: '| `tf.linalg.inv()` | Multiplicative inverse (1/x) of one input tensor |'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.linalg.inv()` | 输入张量的乘法逆（1/x） |'
- en: '| `tf.math.log()` | Natural logarithm of one input tensor |'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.log()` | 输入张量的自然对数 |'
- en: '| `tf.math.maximum()` | Elementwise maximum of two tensors |'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.maximum()` | 两个张量的逐元素最大值 |'
- en: '| `tf.math.minimum()` | Elementwise minimum of two tensors |'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.minimum()` | 两个张量的逐元素最小值 |'
- en: '| `tf.math.negative()` | Negative of one input tensor |'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.negative()` | 输入张量的负值 |'
- en: '| `tf.math.pow()` | The first tensor raised to the second tensor elementwise
    |'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.pow()` | 第一个张量按逐元素方式升至第二个张量 |'
- en: '| `tf.math.round()` | Rounds one input tensor |'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.round()` | 对输入张量进行四舍五入 |'
- en: '| `tf.math.rsqrt()` | The reciprocal of the square root of one tensor |'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.rsqrt()` | 一个张量的平方根倒数 |'
- en: '| `tf.math.sign()` | Returns -1, 0, or 1, depending on the sign of the tensor
    |'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.sign()` | 根据张量的符号返回 -1、0 或 1 |'
- en: '| `tf.math.sin()` | Sine function of one input tensor |'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.sin()` | 输入张量的正弦函数 |'
- en: '| `tf.math.sqrt()` | Square root of one input tensor |'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.sqrt()` | 输入张量的平方根 |'
- en: '| `tf.math.square()` | Square of one input tensor |'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `tf.math.square()` | 输入张量的平方 |'
- en: '**Specialty mathematical functions**: There are some special math functions
    that are often used in machine learning that are worth mentioning, and TensorFlow
    has built-in functions for them. Again, these functions operate elementwise, unless
    specified otherwise:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特殊数学函数**：有一些在机器学习中经常使用的特殊数学函数值得一提，TensorFlow为它们提供了内建函数。再次强调，除非另有说明，否则这些函数都是逐元素操作：'
- en: '| `tf.math.digamma()` | Psi function, the derivative of the `lgamma()` function
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.digamma()` | Psi函数，即`lgamma()`函数的导数 |'
- en: '| `tf.math.erf()` | Gaussian error function, element-wise, of one tensor |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.erf()` | 一个张量的高斯误差函数（逐元素） |'
- en: '| `tf.math.erfc()` | Complementary error function of one tensor |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.erfc()` | 一个张量的互补误差函数 |'
- en: '| `tf.math.igamma()` | Lower regularized incomplete gamma function |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.igamma()` | 下正则化不完全伽马函数 |'
- en: '| `tf.math.igammac()` | Upper regularized incomplete gamma function |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.igammac()` | 上不完全伽马函数的正则化形式 |'
- en: '| `tf.math.lbeta()` | Natural logarithm of the absolute value of the beta function
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.lbeta()` | beta 函数绝对值的自然对数 |'
- en: '| `tf.math.lgamma()` | Natural logarithm of the absolute value of the gamma
    function |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.lgamma()` | 伽马函数绝对值的自然对数 |'
- en: '| `tf.math.squared_difference()` | Computes the square of the differences between
    two tensors |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `tf.math.squared_difference()` | 计算两个张量之间差值的平方 |'
- en: How it works…
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'It is important to know which functions are available to us so that we can
    add them to our computational graphs. We will mainly be concerned with the preceding
    functions. We can also generate many different custom functions as compositions
    of the preceding, as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 了解哪些函数对我们可用是很重要的，这样我们才能将它们添加到我们的计算图中。我们将主要关注前面提到的函数。我们也可以通过组合这些函数生成许多不同的自定义函数，如下所示：
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The complex layers that constitute a deep neural network are just composed of
    the preceding functions, so now, thanks to this recipe, you have all the basics
    you need to create anything you want.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 组成深度神经网络的复杂层仅由前面的函数组成，因此，凭借这个教程，你已经掌握了创建任何你想要的内容所需的所有基础知识。
- en: There's more…
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'If we wish to add other operations to our graphs that are not listed here,
    we must create our own from the preceding functions. Here is an example of an
    operation that wasn''t used previously that we can add to our graph. We can add
    a custom polynomial function, *3 * x^2 - x + 10*, using the following code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望向图中添加其他未列出的操作，我们必须从前面的函数中创建自己的操作。下面是一个示例，这是之前未使用过的操作，我们可以将其添加到图中。我们可以使用以下代码添加一个自定义的多项式函数，*3
    * x^2 - x + 10*：
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There's no limit to the custom functions you can create now, though I always
    recommend that you first consult the TensorFlow documentation. Often, you don't
    need to reinvent the wheel; you can find that what you need has already been coded.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以创建无限制的自定义函数，不过我始终建议你首先查阅 TensorFlow 文档。通常，你不需要重新发明轮子；你会发现你需要的功能已经被编码实现了。
- en: Implementing activation functions
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现激活函数
- en: Activation functions are the key for neural networks to approximate non-linear
    outputs and adapt to non-linear features. They introduce non-linear operations
    into neural networks. If we're careful as to which activation functions are selected
    and where we put them, they're very powerful operations that we can tell TensorFlow
    to fit and optimize.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经网络逼近非线性输出并适应非线性特征的关键。它们在神经网络中引入非线性操作。如果我们小心选择激活函数并合理放置它们，它们是非常强大的操作，可以指示
    TensorFlow 进行拟合和优化。
- en: Getting ready
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: When we start to use neural networks, we'll use activation functions regularly
    because activation functions are an essential part of any neural network. The
    goal of an activation function is just to adjust weight and bias. In TensorFlow,
    activation functions are non-linear operations that act on tensors. They are functions
    that operate in a similar way to the previous mathematical operations. Activation
    functions serve many purposes, but the main concept is that they introduce a non-linearity
    into the graph while normalizing the outputs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用神经网络时，我们会经常使用激活函数，因为激活函数是任何神经网络的重要组成部分。激活函数的目标就是调整权重和偏置。在 TensorFlow
    中，激活函数是对张量进行的非线性操作。它们的作用类似于之前的数学操作。激活函数有很多用途，但主要的概念是它们在图中引入非线性，同时对输出进行归一化。
- en: How to do it…
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'The activation functions live in the **neural network** (**nn**) library in
    TensorFlow. Besides using built-in activation functions, we can also design our
    own using TensorFlow operations. We can import the predefined activation functions
    (from `tensorflow import nn`) or be explicit and write `nn` in our function calls.
    Here, we''ll choose to be explicit with each function call:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数位于 TensorFlow 的 **神经网络** (**nn**) 库中。除了使用内置的激活函数外，我们还可以使用 TensorFlow 操作设计自己的激活函数。我们可以导入预定义的激活函数（通过
    `tensorflow import nn`），或者在函数调用中明确写出 `nn`。在这里，我们选择对每个函数调用显式声明：
- en: 'The rectified linear unit, known as ReLU, is the most common and basic way
    to introduce non-linearity into neural networks. This function is just called
    `max(0,x)`. It is continuous, but not smooth. It appears as follows:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）是最常见和最基本的方式，用于在神经网络中引入非线性。这个函数就叫做 `max(0,x)`。它是连续的，但不光滑。它的形式如下：
- en: '[PRE39]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'There are times where we''ll want to cap the linearly increasing part of the
    preceding ReLU activation function. We can do this by nesting the `max(0,x)` function
    in a `min()` function. The implementation that TensorFlow has is called the ReLU6
    function. This is defined as `min(max(0,x),6)`. This is a version of the hard-sigmoid
    function, is computationally faster, and does not suffer from vanishing (infinitesimally
    near zero) or exploding values. This will come in handy when we discuss deeper
    neural networks in later chapters on convolutional neural networks and recurrent
    ones. It appears as follows:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时我们希望限制前面 ReLU 激活函数的线性增涨部分。我们可以通过将 `max(0,x)` 函数嵌套在 `min()` 函数中来实现。TensorFlow
    实现的版本被称为 ReLU6 函数，定义为 `min(max(0,x),6)`。这是一个硬 sigmoid 函数的版本，计算速度更快，并且不容易遇到梯度消失（趋近于零）或梯度爆炸的问题。这在我们后续讨论卷积神经网络和递归神经网络时会非常有用。它的形式如下：
- en: '[PRE40]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The sigmoid function is the most common continuous and smooth activation function.
    It is also called a logistic function and has the form *1 / (1 + exp(-x))*. The
    sigmoid function is not used very often because of its tendency to zero-out the
    backpropagation terms during training. It appears as follows:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid 函数是最常见的连续且平滑的激活函数。它也被称为 logistic 函数，形式为 *1 / (1 + exp(-x))*。由于在训练过程中容易导致反向传播的梯度消失，sigmoid
    函数并不常用。它的形式如下：
- en: '[PRE41]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We should be aware that some activation functions, such as the sigmoid, are
    not zero-centered. This will require us to zero-mean data prior to using it in
    most computational graph algorithms.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们应该意识到一些激活函数，如 sigmoid，并不是零中心的。这将要求我们在使用这些函数前对数据进行零均值化处理，特别是在大多数计算图算法中。
- en: 'Another smooth activation function is the hyper tangent. The hyper tangent
    function is very similar to the sigmoid except that instead of having a range
    between 0 and 1, it has a range between -1 and 1\. This function has the form
    of the ratio of the hyperbolic sine over the hyperbolic cosine. Another way to
    write this is as follows:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个平滑的激活函数是双曲正切函数。双曲正切函数与 sigmoid 函数非常相似，只不过它的范围不是 0 到 1，而是 -1 到 1。这个函数的形式是双曲正弦与双曲余弦的比值。另一种写法如下：
- en: '[PRE42]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This activation function is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个激活函数如下：
- en: '[PRE43]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `softsign` function is also used as an activation function. The form of
    this function is *x/(|x| + 1)*. The `softsign` function is supposed to be a continuous
    (but not smooth) approximation to the sign function. See the following code:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`softsign` 函数也被用作激活函数。这个函数的形式是 *x/(|x| + 1)*。`softsign` 函数应该是符号函数的一个连续（但不光滑）近似。见以下代码：'
- en: '[PRE44]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Another function, the `softplus` function, is a smooth version of the ReLU
    function. The form of this function is *log(exp(x) + 1)*. It appears as follows:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个函数是 `softplus` 函数，它是 ReLU 函数的平滑版本。这个函数的形式是 *log(exp(x) + 1)*。它的形式如下：
- en: '[PRE45]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `softplus` function goes to infinity as the input increases, whereas the
    `softsign` function goes to 1\. As the input gets smaller, however, the `softplus`
    function approaches zero and the `softsign` function goes to -1.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`softplus` 函数随着输入的增大趋于无穷大，而 `softsign` 函数则趋向 1。不过，随着输入变小，`softplus` 函数接近零，而
    `softsign` 函数则趋向 -1。'
- en: 'The **Exponential Linear Unit** (**ELU**) is very similar to the softplus function
    except that the bottom asymptote is -1 instead of 0\. The form is *(exp(x) + 1)*
    if *x < 0,* else *x*. It appears as follows:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指数线性单元** (**ELU**) 与 softplus 函数非常相似，只不过它的下渐近线是 -1，而不是 0。其形式为 *（exp(x) +
    1)*，当 *x < 0* 时；否则为 *x*。它的形式如下：'
- en: '[PRE46]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, from this recipe, you should understand the basic key activations. Our
    list of the existing activation functions is not exhaustive, and you may discover
    that for certain problems, you need to try some of the lesser known among them.
    Apart from the activations from this recipe, you can find even more activations
    on the Keras activation pages: [https://www.tensorflow.org/api_docs/python/tf/keras/activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过这个公式，你应该能理解基本的关键激活函数。我们列出的现有激活函数并不全面，你可能会发现对于某些问题，你需要尝试其中一些不太常见的函数。除了这个公式中的激活函数，你还可以在
    Keras 激活函数页面上找到更多激活函数：[https://www.tensorflow.org/api_docs/python/tf/keras/activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)
- en: How it works…
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理…
- en: These activation functions are ways that we can introduce non-linearity in neural
    networks or other computational graphs in the future. It is important to note
    where in our network we are using activation functions. If the activation function
    has a range between 0 and 1 (sigmoid), then the computational graph can only output
    values between 0 and 1\. If the activation functions are inside and hidden between
    nodes, then we want to be aware of the effect that the range can have on our tensors
    as we pass them through. If our tensors were scaled to have a mean of zero, we
    will want to use an activation function that preserves as much variance as possible
    around zero.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数是我们未来可以在神经网络或其他计算图中引入非线性的方法。需要注意的是，我们在网络中的哪个位置使用了激活函数。如果激活函数的值域在 0 和 1
    之间（如 sigmoid），那么计算图只能输出 0 到 1 之间的值。如果激活函数位于节点之间并被隐藏，那么我们需要注意这个范围对张量的影响，特别是在通过张量时。如果我们的张量被缩放为零均值，我们将希望使用一个能够尽可能保持零附近方差的激活函数。
- en: This would imply that we want to choose an activation function such as the **hyperbolic
    tangent** (**tanh**) or the **softsign**. If the tensors were all scaled to be
    positive, then we would ideally choose an activation function that preserves variance
    in the positive domain.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们希望选择一个激活函数，比如**双曲正切**（**tanh**）或**softsign**。如果张量都被缩放为正数，那么我们理想中会选择一个能够保持正域方差的激活函数。
- en: There's more…
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We can even easily create custom activations such as the Swish, which is *x**sigmoid(*x*)
    (see *Swish: a Self-Gated Activation Function*, Ramachandran et al., 2017, [https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)),
    which can be used as a more performing replacement for ReLU activations in image
    and tabular data problems:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '我们甚至可以轻松创建自定义的激活函数，如 Swish，公式为 *x**sigmoid(*x*)*（参见 *Swish: a Self-Gated Activation
    Function*, Ramachandran 等，2017，[https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)），它可以作为
    ReLU 激活函数在图像和表格数据问题中的一个更高效的替代品：'
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: After having tried the activations proposed by TensorFlow, your next natural
    step will be to replicate the ones you find on deep learning papers or that you
    create by yourself.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试过 TensorFlow 提供的激活函数后，你的下一步自然是复制那些你在深度学习论文中找到的激活函数，或者你自己创建的激活函数。
- en: Working with data sources
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据源
- en: For most of this book, we will rely on the use of datasets to fit machine learning
    algorithms. This section has instructions on how to access each of these datasets
    through TensorFlow and Python.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容都将依赖于使用数据集来训练机器学习算法。本节提供了如何通过 TensorFlow 和 Python 访问这些数据集的说明。
- en: 'Some of the data sources rely on the maintenance of outside websites so that
    you can access the data. If these websites change or remove this data, then some
    of the following code in this section may need to be updated. You can find the
    updated code on this book''s GitHub page:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据源依赖于外部网站的维护，以便你能够访问数据。如果这些网站更改或删除了数据，那么本节中的部分代码可能需要更新。你可以在本书的 GitHub 页面上找到更新后的代码：
- en: '[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)'
- en: Getting ready
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Throughout the book, the majority of the datasets that we will be using are
    accessible using TensorFlow Datasets, whereas some others will require some extra
    effort by using a Python script to download, or by manually downloading them through
    the internet.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用的大多数数据集可以通过 TensorFlow 数据集（TensorFlow Datasets）访问，而一些其他的数据集则需要额外的努力，可能需要使用
    Python 脚本来下载，或者通过互联网手动下载。
- en: '**TensorFlow Datasets** (**TFDS**) is a collection of datasets ready to use
    (you can find the complete list here: [https://www.tensorflow.org/datasets/catalog/overview](https://www.tensorflow.org/datasets/catalog/overview)).
    It automatically handles downloading and preparation of the data and, being a
    wrapper around `tf.data`, constructs efficient and fast data pipelines.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 数据集**（**TFDS**）是一个现成可用的数据集集合（完整列表可以在此处找到：[https://www.tensorflow.org/datasets/catalog/overview](https://www.tensorflow.org/datasets/catalog/overview)）。它自动处理数据的下载和准备，并且作为
    `tf.data` 的封装器，构建高效且快速的数据管道。'
- en: 'In order to install TFDS, just run the following installation command on your
    console:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装 TFDS，只需在控制台中运行以下安装命令：
- en: '[PRE48]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We can now move on to explore the core datasets that you will be using in this
    book (not all of these datasets are included here, just the most common ones.
    Some other very specific datasets will be introduced in different chapters throughout
    the book).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续探索本书中你将使用的核心数据集（并非所有这些数据集都会包含在内，只有最常见的几个数据集会被介绍，其他一些非常特定的数据集将在本书的不同章节中介绍）。
- en: How to do it…
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: '**Iris data**: This dataset is arguably the classic structured dataset used
    in machine learning and perhaps in all examples of statistics. It is a dataset
    that measures sepal length, sepal width, petal length, and petal width of three
    different types of iris flowers: *Iris setosa*, *Iris virginica,* and *Iris versicolor*.
    There are 150 measurements in total, which means that there are 50 measurements
    for each species. To load the dataset in Python, we will use TFDS functions, as
    follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**鸢尾花数据集**：这个数据集可以说是机器学习中经典的结构化数据集，可能也是所有统计学示例中的经典数据集。它是一个测量三种不同类型鸢尾花的萼片长度、萼片宽度、花瓣长度和花瓣宽度的数据集：*Iris
    setosa*、*Iris virginica* 和 *Iris versicolor*。总共有150个测量值，这意味着每个物种有50个测量值。要在Python中加载该数据集，我们将使用TFDS函数，代码如下：'
- en: '[PRE49]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'When you are importing a dataset for the first time, a bar will point out where
    you are as you download the dataset. If you prefer, you can deactivate it if you
    type the following:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你第一次导入数据集时，下载数据集时会显示一个进度条，指示你所在的位置。如果你不想看到进度条，可以通过输入以下代码来禁用它：
- en: '`tfds.disable_progress_bar()`'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tfds.disable_progress_bar()`'
- en: '**Birth weight data**: This data was originally from Baystate Medical Center,
    Springfield, Mass, 1986\. This dataset contains measurements including childbirth
    weight and other demographic and medical measurements of the mother and the family
    history. There are 189 observations of eleven variables. The following code shows
    you how you can access this data as `tf.data.dataset`:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**出生体重数据**：该数据最初来自1986年Baystate医疗中心（马萨诸塞州斯普林菲尔德）。该数据集包含了出生体重和母亲的其他人口统计学及医学测量数据，以及家庭病史的记录。数据集有189条记录，包含11个变量。以下代码展示了如何将该数据作为`tf.data.dataset`来访问：'
- en: '[PRE50]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Boston housing data**: Carnegie Mellon University maintains a library of
    datasets in their `StatLib` Library. This data is easily accessible via The University
    of California at Irvine''s machine learning repository ([https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)).
    There are 506 observations of house worth, along with various demographic data
    and housing attributes (14 variables). The following code shows you how to access
    this data in TensorFlow:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**波士顿房价数据集**：卡内基梅隆大学在其`StatLib`库中维护了一系列数据集。该数据可以通过加州大学欧文分校的机器学习仓库轻松访问（[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)）。该数据集包含506条房价观察记录，以及各种人口统计数据和房屋属性（14个变量）。以下代码展示了如何在TensorFlow中访问该数据：'
- en: '[PRE51]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**MNIST handwriting data**: The **Mixed National Institute of Standards and
    Technology** (**MNIST**) dataset is a subset of the larger NIST handwriting database.
    The MNIST handwriting dataset is hosted on Yann LeCun''s website ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    It is a database of 70,000 images of single-digit numbers (0-9), with about 60,000
    annotated for a training set and 10,000 for a test set. This dataset is used so
    often in image recognition that TensorFlow provides built-in functions to access
    this data. In machine learning, it is also important to provide validation data
    to prevent overfitting (target leakage). Because of this, TensorFlow sets aside
    5,000 images of the training set in a validation set. The following code shows
    you how to access this data in TensorFlow:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MNIST手写数字数据集**：**美国国家标准与技术研究院**（**NIST**）的手写数据集的子集即为**MNIST**数据集。MNIST手写数字数据集托管在Yann
    LeCun的网站上（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）。该数据库包含70,000张单个数字（0-9）的图像，其中大约60,000张用于训练集，10,000张用于测试集。这个数据集在图像识别中使用非常频繁，以至于TensorFlow提供了内置函数来访问该数据。在机器学习中，提供验证数据以防止过拟合（目标泄露）也是很重要的。因此，TensorFlow将训练集中的5,000张图像分配为验证集。以下代码展示了如何在TensorFlow中访问此数据：'
- en: '[PRE52]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**Spam-ham text data**. UCI''s machine learning dataset library also holds
    a spam-ham text message dataset. We can access this `.zip` file and get the spam-ham
    text data as follows:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**垃圾邮件-正常邮件文本数据**。UCI的机器学习数据集库也包含了一个垃圾邮件-正常邮件文本数据集。我们可以访问这个`.zip`文件并获取垃圾邮件-正常邮件文本数据，方法如下：'
- en: '[PRE53]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Movie review data**: Bo Pang from Cornell has released a movie review dataset
    that classifies reviews as good or bad. You can find the data on the Cornell University
    website: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/).
    To download, extract, and transform this data, we can run the following code:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**电影评论数据**：康奈尔大学的 Bo Pang 发布了一个电影评论数据集，将评论分类为好或坏。你可以在康奈尔大学网站上找到该数据：[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)。要下载、解压并转换这些数据，我们可以运行以下代码：'
- en: '[PRE54]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '**CIFAR-10 image data**: The Canadian Institute for Advanced Research has released
    an image set that contains 80 million labeled colored images (each image is scaled
    to 32 x 32 pixels). There are 10 different target classes (airplane, automobile,
    bird, and so on). CIFAR-10 is a subset that includes 60,000 images. There are
    50,000 images in the training set, and 10,000 in the test set. Since we will be
    using this dataset in multiple ways, and because it is one of our larger datasets,
    we will not run a script each time we need it. To get this dataset, just execute
    the following code to download the CIFAR-10 dataset (this may take a long time):'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**CIFAR-10 图像数据**：加拿大高级研究院发布了一个包含 8000 万标记彩色图像的图像集（每张图像的尺寸为 32 x 32 像素）。该数据集包含
    10 个不同的目标类别（如飞机、汽车、鸟类等）。CIFAR-10 是一个子集，包含 60,000 张图像，其中训练集有 50,000 张图像，测试集有 10,000
    张图像。由于我们将在多种方式下使用该数据集，并且它是我们较大的数据集之一，我们不会每次都运行脚本来获取它。要获取该数据集，只需执行以下代码来下载 CIFAR-10
    数据集（这可能需要较长时间）：'
- en: '[PRE55]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '**The works of Shakespeare text data**: Project Gutenberg is a project that
    releases electronic versions of free books. They have compiled all of the works
    of Shakespeare together. The following code shows you how to access this text
    file through TensorFlow:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**莎士比亚作品文本数据**：Project Gutenberg 是一个发布免费书籍电子版的项目。他们已经将莎士比亚的所有作品汇编在一起。以下代码展示了如何通过
    TensorFlow 访问这个文本文件：'
- en: '[PRE56]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**English-German sentence translation data**: The Tatoeba project ([http://tatoeba.org](http://tatoeba.org))
    collects sentence translations in many languages. Their data has been released
    under the Creative Commons license. From this data, ManyThings.org ([http://www.manythings.org](http://www.manythings.org))
    has compiled sentence-to-sentence translations in text files that are available
    for download. Here, we will use the English-German translation file, but you can
    change the URL to whichever languages you would like to use:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**英语-德语句子翻译数据**：Tatoeba 项目（[http://tatoeba.org](http://tatoeba.org)）收集了多种语言的句子翻译。他们的数据已根据创意共享许可证发布。从这些数据中，ManyThings.org（[http://www.manythings.org](http://www.manythings.org)）编译了可供下载的文本文件，包含逐句翻译。在这里，我们将使用英语-德语翻译文件，但你可以根据需要更改
    URL 来使用其他语言：'
- en: '[PRE57]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: With this last dataset, we have completed our review of the datasets that you
    will most frequently encounter when using the recipes you will find in this book.
    At the start of each recipe, we'll remind you how to download the relevant dataset
    and explain why it is relevant for the recipe in question.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 使用完这个数据集后，我们已完成对本书中您在使用配方时最常遇到的数据集的回顾。在每个配方开始时，我们会提醒您如何下载相关数据集，并解释它为何与该配方相关。
- en: How it works…
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理……
- en: When it comes to using one of these datasets in a recipe, we'll refer you to
    this section and assume that the data is loaded in the ways we've just described.
    If further data transformation or preprocessing is necessary, then that code will
    be provided in the recipe itself.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到在某个配方中使用这些数据集时，我们将参考本节内容，并假定数据已经按照我们刚才描述的方式加载。如果需要进一步的数据转换或预处理，相关代码将在配方中提供。
- en: 'Usually, the approach will simply be as follows when we use data from TensorFlow
    datasets:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们使用来自 TensorFlow 数据集的数据时，方法通常如下所示：
- en: '[PRE58]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In any case, depending on the location of the data, it may turn out to be necessary
    to download it, extract it, and transform it.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，根据数据的位置，可能需要下载、解压并转换它。
- en: See also
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Here are some additional references for the data resources we use in this book:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在本书中使用的部分数据资源的附加参考资料：
- en: 'Hosmer, D.W., Lemeshow, S., and Sturdivant, R. X. (2013) *Applied Logistic
    Regression: 3rd Edition*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosmer, D.W., Lemeshow, S., 和 Sturdivant, R. X.（2013年） *《应用逻辑回归：第3版》*
- en: 'Lichman, M. (2013). *UCI machine learning repository*: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).
    Irvine, CA: University of California, School of Information and Computer Science'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lichman, M. (2013). *UCI 机器学习库*：[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院
- en: 'Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan, *Thumbs up? Sentiment classification
    using machine learning techniques*, Proceedings of EMNLP 2002: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bo Pang, Lillian Lee 和 Shivakumar Vaithyanathan，*好评？使用机器学习技术进行情感分类*，EMNLP 2002
    会议论文：[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
- en: 'Krizhevsky. (2009). *Learning Multiple Layers of Features from Tiny Images*:
    [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky. (2009). *从小图像中学习多层特征*： [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)
- en: '*Project Gutenberg. Accessed* April 2016: [http://www.gutenberg.org/](http://www.gutenberg.org/
    )'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Project Gutenberg. 访问于* 2016年4月：[http://www.gutenberg.org/](http://www.gutenberg.org/
    )'
- en: Additional resources
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他资源
- en: In this section, you will find additional links, documentation sources, and
    tutorials that will be of great assistance when learning and using TensorFlow.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将找到更多的链接、文档资源和教程，这些在学习和使用 TensorFlow 时会提供很大帮助。
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: When learning how to use TensorFlow, it helps to know where to turn for assistance
    or pointers. This section lists some resources to get TensorFlow running and to
    troubleshoot problems.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何使用 TensorFlow 时，知道在哪里寻求帮助或提示是很有帮助的。本节列出了启动 TensorFlow 和解决问题的一些资源。
- en: How to do it…
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'Here is a list of TensorFlow resources:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 TensorFlow 资源的列表：
- en: 'The code for this book is available online at the Packt repository: [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书的代码可以在 Packt 仓库在线访问：[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)
- en: The official TensorFlow Python API documentation is located at [https://www.tensorflow.org/api_docs/python](https://www.tensorflow.org/api_docs/python).
    Here, there is documentation and examples of all of the functions, objects, and
    methods in TensorFlow.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 官方 Python API 文档位于 [https://www.tensorflow.org/api_docs/python](https://www.tensorflow.org/api_docs/python)。在这里，你可以找到所有
    TensorFlow 函数、对象和方法的文档及示例。
- en: TensorFlow's official tutorials are very thorough and detailed. They are located
    at [https://www.tensorflow.org/tutorials/index.html](https://www.tensorflow.org/tutorials/index.html).
    They start covering image recognition models, and work through Word2Vec, RNN models,
    and sequence-to-sequence models. They also have additional tutorials for generating
    fractals and solving PDE systems. Note that they are continually adding more tutorials
    and examples to this collection.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 的官方教程非常全面和详细，位于 [https://www.tensorflow.org/tutorials/index.html](https://www.tensorflow.org/tutorials/index.html)。它们从图像识别模型开始，接着讲解
    Word2Vec、RNN 模型以及序列到序列模型。它们还提供了生成分形图形和求解 PDE 系统的额外教程。请注意，他们会不断地向这个合集添加更多教程和示例。
- en: TensorFlow's official GitHub repository is available via [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow).
    Here, you can view the open source code and even fork or clone the most current
    version of the code if you want. You can also see current filed issues if you
    navigate to the `issues` directory.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 的官方 GitHub 仓库可以通过 [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)
    访问。在这里，你可以查看开源代码，甚至如果你愿意，可以分叉或克隆当前版本的代码。你也可以通过访问 `issues` 目录来查看当前已提交的问题。
- en: A public Docker container that is kept up to date by TensorFlow is available
    on Dockerhub at [https://hub.docker.com/r/tensorflow/tensorflow/](https://hub.docker.com/r/tensorflow/tensorflow/).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 提供的一个由官方维护的公共 Docker 容器，始终保持最新版本，位于 Dockerhub：[https://hub.docker.com/r/tensorflow/tensorflow/](https://hub.docker.com/r/tensorflow/tensorflow/)。
- en: A great source for community help is Stack Overflow. There is a tag for TensorFlow.
    This tag seems to be growing in interest as TensorFlow is gaining in popularity.
    To view activity on this tag, visit [http://stackoverflow.com/questions/tagged/Tensorflow](http://stackoverflow.com/questions/tagged/Tensorflow).
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stack Overflow 是一个很好的社区帮助源。在这里有一个 TensorFlow 标签。随着 TensorFlow 越来越受欢迎，这个标签的讨论也在增长。要查看该标签的活动，可以访问
    [http://stackoverflow.com/questions/tagged/Tensorflow](http://stackoverflow.com/questions/tagged/Tensorflow)。
- en: While TensorFlow is very agile and can be used for many things, the most common
    use of TensorFlow is deep learning. To understand the basis of deep learning,
    how the underlying mathematics works, and to develop more intuition on deep learning,
    Google has created an online course that's available on Udacity. To sign up and
    take this video lecture course, visit [https://www.udacity.com/course/deep-learning--ud730](https://www.udacity.com/course/deep-learning--ud730).
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 TensorFlow 非常灵活，能够用于许多用途，但它最常见的用途是深度学习。为了理解深度学习的基础、底层数学如何运作，并培养更多的深度学习直觉，Google
    创建了一个在线课程，课程在 Udacity 上提供。要注册并参加这个视频讲座课程，请访问[https://www.udacity.com/course/deep-learning--ud730](https://www.udacity.com/course/deep-learning--ud730)。
- en: TensorFlow has also made a site where you can visually explore training a neural
    network while changing the parameters and datasets. Visit [http://playground.tensorflow.org/](http://playground.tensorflow.org/)
    to explore how different settings affect the training of neural networks.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 还创建了一个网站，你可以在其中通过调整参数和数据集来直观地探索训练神经网络。访问[http://playground.tensorflow.org/](http://playground.tensorflow.org/)来探索不同设置如何影响神经网络的训练。
- en: 'Andrew Ng teaches an online course called Neural Networks and Deep Learning
    : [https://www.coursera.org/learn/neural-networks-deep-learning](https://www.coursera.org/learn/neural-networks-deep-learning)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew Ng 讲授了一门名为《神经网络与深度学习》的在线课程：[https://www.coursera.org/learn/neural-networks-deep-learning](https://www.coursera.org/learn/neural-networks-deep-learning)
- en: 'Stanford University has an online syllabus and detailed course notes for *Convolutional
    Neural Networks for Visual Recognition*: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福大学提供了一个在线大纲和详细的课程笔记，内容涉及*卷积神经网络与视觉识别*：[http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)
