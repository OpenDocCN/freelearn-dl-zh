- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: 'Surveying GenAI Types and Modes: An Overview of GANs, Diffusers, and Transformers'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述GenAI类型和模式：GANs、扩散器和Transformer概述
- en: In the previous chapter, we established the key distinction between generative
    and discriminative models. Discriminative models focus on predicting outputs by
    learning `p(output``∣``input)`, or the conditional probability of some expected
    output given an input or set of inputs. In contrast, generative models, such as
    `p(next token``∣``previous tokens)`, based on the probabilities of possible continuations
    given the current context. Tokens are represented as vectors containing embeddings
    that capture latent features and rich semantic dependencies learned through extensive
    training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们确立了生成模型和判别模型之间的关键区别。判别模型专注于通过学习`p(output|input)`，即给定输入或输入集的某些预期输出的条件概率来预测输出。相比之下，生成模型，如`p(next
    token|previous tokens)`，基于给定当前上下文的可能续集的概率。标记表示为包含嵌入的向量，这些嵌入通过大量训练捕获了潜在特征和丰富的语义依赖关系。
- en: We briefly surveyed leading generative approaches, including **Generative Adversarial
    Networks** (**GANs**), **Variational Autoencoders** (**VAEs**), diffusion models,
    and autoregressive transformers. Each methodology possesses unique strengths suitable
    for different data types and tasks. For example, GANs are adept at generating
    high-fidelity photographic images through an adversarial process. Diffusion models
    take a probabilistic approach, iteratively adding and removing noise from data
    to learn robust generative representations. Autoregressive transformers leverage
    self-attention and massive scale to achieve remarkable controlled text generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要概述了领先的生成方法，包括**生成对抗网络（GANs**）、**变分自编码器（VAEs**）、扩散模型和自回归Transformer。每种方法都具有适合不同数据类型和任务的独特优势。例如，GANs通过对抗过程擅长生成高保真度的照片图像。扩散模型采用概率方法，通过迭代地向数据中添加和去除噪声来学习鲁棒的生成表示。自回归Transformer利用自注意力和大规模来实现令人瞩目的可控文本生成。
- en: In this chapter, we will explore the theoretical foundations and real-world
    applications of these techniques in greater depth. We will make direct comparisons,
    elucidating architectural innovations and enhancements that improve training stability
    and output quality over time. Through practical examples, we will see how researchers
    have adapted these models to produce art, music, videos, stories, and so on.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨这些技术的理论基础和实际应用。我们将进行直接比较，阐明随着时间的推移提高训练稳定性和输出质量的架构创新和改进。通过实际示例，我们将看到研究人员如何将这些模型应用于艺术、音乐、视频、故事等。
- en: To enable an unbiased comparison, we will concentrate primarily on image synthesis
    tasks. GANs and diffusion models are specifically architected for image data,
    harnessing advances in convolutional processing and computer vision. Transformers,
    powered by self-attention, excel at language modeling but can also generate images.
    This will allow us to benchmark performance on a common task.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行无偏比较，我们将主要集中研究图像合成任务。生成对抗网络（GANs）和扩散模型专门为图像数据设计，利用了卷积处理和计算机视觉方面的进步。由自注意力驱动的Transformer在语言建模方面表现出色，同时也能生成图像。这将使我们能够在共同的任务上对性能进行基准测试。
- en: By the end of this chapter, we will have implemented state-of-the-art image
    generation models and explored how these core methods enhance and complement each
    other.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '到本章结束时，我们将实现最先进的图像生成模型，并探讨这些核心方法如何相互增强和补充。 '
- en: Understanding General Artificial Intelligence (GAI) Types – distinguishing features
    of GANs, diffusers, and transformers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解通用人工智能（GAI）类型——GANs、扩散器和Transformer的区别特征
- en: The often-stunning human-like quality we experience from GAI can be attributed
    to deep-generative machine learning advances. In particular, three fundamental
    methods have inspired many derivative innovations – GANs, diffusion models, and
    transformers. Each has its distinct strengths and is particularly well-suited
    for specific applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从通用人工智能（GAI）中体验到的常常令人惊叹的人类似质量可以归因于深度生成机器学习的进步。特别是，三种基本方法激发了众多衍生创新——生成对抗网络（GANs）、扩散模型和Transformer。每种方法都有其独特的优势，特别适合特定的应用。
- en: We briefly described GANs, a groundbreaking approach that exploits the adversarial
    interplay between two competing neural networks – a generator and a discriminator
    – to generate hyper-realistic synthetic data. Over time, GANs have seen substantial
    advancements, achieving greater control in data generation, higher image fidelity,
    and enhanced training stability. For instance, NVIDIA’s StyleGAN has created highly
    detailed and realistic human faces. The adversarial training process of GANs,
    where one network generates data and the other evaluates it, allows you to create
    highly refined and detailed synthetic images, enhancing realism with each training
    iteration. The synthetic images generated can be utilized in a plethora of domains.
    In the entertainment industry, they can be used to create realistic characters
    for video games or films. In research, they provide a means to augment datasets,
    especially in scenarios where real data is scarce or sensitive. Moreover, in computer
    vision, these synthetic images aid in training and fine-tuning other machine-learning
    models, advancing applications like facial recognition.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要介绍了 GANs，这是一种开创性的方法，它利用两个竞争性神经网络——生成器和判别器——之间的对抗性交互来生成超逼真的合成数据。随着时间的推移，GANs
    已经取得了重大进展，在数据生成、图像保真度和训练稳定性方面取得了更大的控制。例如，NVIDIA 的 StyleGAN 创建了高度详细和逼真的人类面孔。GANs
    的对抗性训练过程，其中一个网络生成数据而另一个网络评估它，允许你创建高度精细和详细的合成图像，并在每次训练迭代中增强逼真度。生成的合成图像可以应用于众多领域。在娱乐行业中，它们可以用于创建视频游戏或电影中的逼真角色。在研究方面，它们提供了一种扩充数据集的手段，特别是在真实数据稀缺或敏感的场景中。此外，在计算机视觉中，这些合成图像有助于训练和微调其他机器学习模型，推进如人脸识别等应用。
- en: Diffusion models, an innovative generative modeling alternative, explicitly
    address some GAN limitations. As discussed briefly in [*Chapter 1*](B21773_01.xhtml#_idTextAnchor015),
    diffusion models adopt a unique approach to introducing and systematically removing
    noise, enabling high-quality image synthesis with less training complexity. In
    medical imaging, diffusion models can significantly enhance image clarity by generating
    high-resolution synthetic examples to train other machine-learning models. Introducing
    and then iteratively removing noise can help reconstruct high-fidelity images
    from lower-quality inputs, which is invaluable in scenarios where obtaining high-resolution
    medical images is challenging.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型，一种创新的生成建模替代方案，明确解决了某些 GAN 的局限性。如[*第一章*](B21773_01.xhtml#_idTextAnchor015)中简要讨论的，扩散模型采用独特的方法引入和系统地去除噪声，以较低的训练复杂度实现高质量的图像合成。在医学成像中，扩散模型可以通过生成高分辨率合成示例来训练其他机器学习模型，从而显著提高图像清晰度。引入噪声然后迭代去除噪声可以帮助从低质量输入中重建高保真图像，这在获取高分辨率医学图像具有挑战性的场景中非常有价值。
- en: Simultaneously, generative transformers, initially designed for language modeling,
    have been adopted for multimodal synthesis. Today, transformers are not confined
    to language and have permeated into audio, images, and video applications. For
    instance, OpenAI’s GPT-4 excels in processing and generating text, while DALL-E
    creates images from textual descriptions, a perfect example of the interplay between
    methods. When integrated, GPT-4 and DALL-E form a robust multimodal system. GPT-4
    processes and understands textual instructions, while DALL-E takes the interpreted
    instructions to generate corresponding visual representations. A practical application
    of this combination could be automated digital advertisement creation. For example,
    given textual descriptions of a product and the desired aesthetic, GPT-4 could
    interpret these instructions, and DALL-E could generate visually compelling advertisements
    accordingly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，最初为语言建模设计的生成式变压器已被用于多模态合成。如今，变压器不仅限于语言，还渗透到音频、图像和视频应用中。例如，OpenAI 的 GPT-4
    在处理和生成文本方面表现出色，而 DALL-E 则能根据文本描述创建图像，这是方法之间交互的完美例子。当集成时，GPT-4 和 DALL-E 形成了一个强大的多模态系统。GPT-4
    处理和理解文本指令，而 DALL-E 则根据解释后的指令生成相应的视觉表示。这种组合的实际应用可以是自动化数字广告创作。例如，给定产品的文本描述和期望的美学，GPT-4
    可以解释这些指令，而 DALL-E 则可以相应地生成视觉上吸引人的广告。
- en: Deconstructing GAI methods – exploring GANs, diffusers, and transformers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构 GAI 方法 - 探索 GANs、diffusers 和 transformers
- en: Let’s deconstruct these core approaches to understand their distinct characteristics
    and illustrate their transformative role in advancing generative machine learning.
    As GAI continues to move forward, it’s crucial to understand how these approaches
    drive innovation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这些核心方法，以了解它们的独特特征，并展示它们在推进生成式机器学习方面的变革性作用。随着GAI不断前进，了解这些方法如何推动创新至关重要。
- en: A closer look at GANs
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解GANs
- en: GANs, introduced by Goodfellow et al. in 2014, primarily consist of two neural
    networks – the **Generator** (**G**) and the **Discriminator** (**D**). G aims
    to create synthetic data resembling real data, while D strives to distinguish
    real from synthetic data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GANs由Goodfellow等人于2014年引入，主要由两个神经网络组成——**生成器（G**）和**判别器（D**）。G的目标是创建类似于真实数据的合成数据，而D的目标是区分真实数据和合成数据。
- en: 'In this setup, the following occurs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，以下情况发生：
- en: G receives input from a “latent space,” a high-dimensional space representing
    structured randomness. This structured randomness serves as a seed to generate
    synthetic data, transforming it into meaningful information.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G从“潜在空间”接收输入，这是一个表示结构化随机性的高维空间。这种结构化随机性作为生成合成数据的种子，将其转化为有意义的信息。
- en: D evaluates the generated data, attempting to differentiate between real (or
    reference) and synthetic data.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D评估生成数据，试图区分真实（或参考）数据和合成数据。
- en: In short, the process begins with G deriving random noise from the latent space
    to create data. This synthetic data, along with real data, is supplied to D, which
    then tries to discern between the two. Feedback from D informs the parameters
    of G to refine its data generation process. The adversarial interaction continues
    until an equilibrium is reached.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简而言之，这个过程从G从潜在空间中提取随机噪声来创建数据开始。这些合成数据与真实数据一起提供给D，然后D试图区分这两者。D的反馈会告知G的参数以优化其数据生成过程。这种对抗性交互会持续进行，直到达到平衡。
- en: '**Equilibrium** in GANs occurs when D can no longer differentiate between real
    and synthetic data, assigning an equal probability of 0.5 to both. Arriving at
    this state signals that the synthetic data produced by G is indistinguishable
    from real data, which is the core objective of the synthesis process.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GANs中，当D无法区分真实和合成数据，对两者都分配0.5的相等概率时，就会达到**平衡**。达到这种状态表明G产生的合成数据与真实数据无法区分，这是合成过程的核心目标。
- en: Ultimately, the success of GANs has had meaningful implications for various
    sectors. In the automotive industry, GANs have been used to simulate real-world
    scenarios for autonomous vehicle testing. In the entertainment sector, GANs are
    deployed to generate digital characters and realistic environments for filmmaking
    and game design. In the art world, GANs can literally craft new words. Moreover,
    the development of GANs has continued to move forward over the years with significant
    improvements in quality, control, and overall performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，GANs的成功对各个行业产生了有意义的启示。在汽车行业中，GANs被用于模拟真实世界场景以进行自动驾驶车辆的测试。在娱乐行业中，GANs被部署用于生成电影制作和游戏设计中的数字角色和逼真环境。在艺术界，GANs实际上可以创造新的词汇。此外，GANs的发展在质量、控制和整体性能方面持续取得显著进步。
- en: Advancement of GANs
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GANs的进步
- en: 'Since its inception, GAN technology has evolved significantly with several
    notable advancements:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自其诞生以来，GAN技术已经发生了显著演变，取得了几个显著的进步：
- en: '**Conditional GANs (cGANs)**: Introduced by Mirza and Osindero in 2014, conditional
    GANs incorporated specific conditions during data generation, enabling more controlled
    outputs. cGANs have been used in tasks such as image-to-image translation (e.g.,
    converting photos into paintings).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件生成对抗网络（cGANs）**：由Mirza和Osindero于2014年引入，条件GANs在数据生成过程中加入了特定的条件，从而实现了更可控的输出。cGANs已被用于图像到图像翻译等任务（例如，将照片转换为画作）。'
- en: '**Deep Convolutional GANs (DCGANs)**: In 2015, Radford et al. enhanced GANs
    by integrating convolutional layers, which help to analyze image data in small,
    overlapping regions to capture fine granularity, substantially improving the visual
    quality of the synthetic output. DCGANs can generate realistic images for applications
    such as fashion design, where the model evolves new designs from existing trends.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积生成对抗网络（DCGANs）**：2015年，Radford等人通过整合卷积层增强了GANs，这有助于在小的、重叠的区域分析图像数据以捕捉精细粒度，从而显著提高了合成输出的视觉质量。DCGANs可以生成用于时尚设计等应用的逼真图像，其中模型从现有趋势中演变出新的设计。'
- en: '**Wasserstein GANs (WGANs)**: Introduced by Arjovsky et al. in 2017, Wasserstein
    GANs applied the Wasserstein distance metric to GANs’ objective function, facilitating
    a more accurate measurement of differences between real and synthetic data. Specifically,
    the metric helps you find the most efficient way to make the generated data distribution
    resemble the real data distribution. This small adjustment leads to a more stable
    learning process, minimizing volatility during training. WGANs have helped generate
    realistic medical imagery to aid in training diagnostic AI algorithms, improving
    a model’s ability to generalize from synthetic to actual data.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wasserstein GANs (WGANs)**：由Arjovsky等人于2017年提出，Wasserstein GANs将Wasserstein距离度量应用于GANs的目标函数，从而促进了真实数据和合成数据之间差异的更精确测量。具体来说，该度量有助于找到使生成数据分布接近真实数据分布的最有效方法。这一小调整导致学习过程更加稳定，最小化了训练过程中的波动。WGANs有助于生成逼真的医学图像，以辅助训练诊断人工智能算法，提高模型从合成数据到实际数据的泛化能力。'
- en: 'Following the advent of Wasserstein GANs, the landscape experienced a surge
    of inventive expansions, each tailor-made to address specific challenges or open
    new avenues in synthetic data generation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Wasserstein GANs的出现，该领域经历了创新扩展的激增，每个扩展都是专门针对解决特定挑战或开辟合成数据生成的新途径而量身定制的：
- en: '**Progressively growing GANs** incrementally increase the resolution during
    training, starting with lower-resolution images and gradually moving to higher
    resolution. This approach allows the model to learn coarse-to-fine details effectively,
    making training more manageable and generating high-quality images (Karras et
    al. 2017). These high-resolution images can enhance the realism and immersion
    of virtual reality environments.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**渐进式增长的GANs**在训练过程中逐步增加分辨率，从低分辨率图像开始，逐渐过渡到高分辨率。这种方法允许模型有效地学习从粗略到精细的细节，使训练更加容易管理，并生成高质量的图像（Karras等人，2017年）。这些高分辨率图像可以增强虚拟现实环境的真实感和沉浸感。'
- en: '**CycleGANs** facilitates image-to-image translations, bridging domain adaptation
    tasks (Zhu et al., 2017). For example, a CycleGAN could transform a summer scene
    into a winter scene without requiring example pairs (e.g., summer-winter) during
    training. CycleGANs have been used to simulate weather conditions in autonomous
    vehicle testing, evaluating system performance under varying environmental conditions.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CycleGANs**促进了图像到图像的转换，连接了领域适应任务（Zhu等人，2017年）。例如，CycleGAN可以将夏季场景转换为冬季场景，而无需在训练期间提供示例对（例如，夏季-冬季）。CycleGANs已被用于模拟自动驾驶汽车测试中的天气条件，评估系统在不同环境条件下的性能。'
- en: '**BigGANs** push the boundaries in high-resolution image generation, showcasing
    the versatility of GANs in complex generation tasks. They achieve this by scaling
    up the size of the model (more layers and units per layer) and the batch size
    during training, alongside other architectural and training innovations (Brock
    et al., 2018). BigGANs have been used to generate realistic textures for video
    games, enhancing gaming environments’ realism.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BigGANs**在高质量图像生成方面推动了边界，展示了GANs在复杂生成任务中的多功能性。他们通过在训练过程中扩大模型的大小（更多层和每层的单元）以及批量大小，以及其他架构和训练创新（Brock等人，2018年）来实现这一点。BigGANs已被用于生成视频游戏中的逼真纹理，增强了游戏环境的真实感。'
- en: These developments significantly broadened what GANs could achieve, ranging
    from high-resolution image synthesis to domain adaptation and cross-modal generation
    tasks. However, despite these incredible advancements, GANs have suffered from
    some continual limitations, which inspired alternative approaches such as diffusion.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展显著扩大了生成对抗网络（GANs）所能实现的范围，从高分辨率图像合成到领域适应和跨模态生成任务。然而，尽管取得了这些令人难以置信的进步，GANs仍然存在一些持续的局限性，这促使人们探索了如扩散等替代方法。
- en: Limitations and challenges of GANs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GANs的局限性和挑战
- en: 'The training process of GANs requires a careful balance between the G and D
    networks. It requires substantial computational resources, often demanding powerful
    GPUs and enormous datasets to achieve desirable outcomes. Moreover, there are
    complexities in training GANs that arise from challenges such as vanishing gradients
    and mode collapse. While the vanishing gradient problem is a problem broadly affecting
    deep neural networks, mode collapse is a challenge that is particularly unique
    to the training of GANs. Let’s explore these a bit further:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 的训练过程需要在 G 和 D 网络之间进行仔细的平衡。它需要大量的计算资源，通常需要强大的 GPU 和庞大的数据集才能实现理想的结果。此外，GANs
    的训练中存在一些复杂性，这些复杂性源于诸如梯度消失和模式坍塌等挑战。虽然梯度消失问题是广泛影响深度神经网络的普遍问题，但模式坍塌是 GANs 训练中特有的挑战。让我们进一步探讨这些问题：
- en: '**Vanishing gradients**: This issue arises during the neural network training
    phase when the gradient of the loss function diminishes to a point where the learning
    either drastically slows or halts. The crux of GANs lies in the delicate balance
    of learning between the G and D models. Disproportionate learning can hinder the
    overall training process. In practical terms, the issue of vanishing gradients
    can lead to longer training times and increased computational costs, which might
    render GANs impractical for time-sensitive or resource-constrained applications.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失**：这个问题出现在神经网络训练阶段，当损失函数的梯度减小到一个点，导致学习速度急剧减慢或停止。GANs 的核心在于 G 和 D 模型之间学习的微妙平衡。不均衡的学习可能会阻碍整体训练过程。在实践中，梯度消失的问题可能导致训练时间更长，计算成本增加，这可能会使
    GANs 对于时间敏感或资源受限的应用变得不切实际。'
- en: '**Mode collapse**: Inherent to GANs, mode collapse occurs when the G starts
    producing a narrow variety of samples, thereby stifling output diversity and undermining
    a network’s effectiveness. Techniques such as a gradient penalty and spectral
    normalization have alleviated these issues. This phenomenon can significantly
    degrade the quality of generated data, limiting the use of GANs in applications
    that require diverse outputs, such as data augmentation for machine learning or
    generating diverse design alternatives in creative industries.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式坍塌**：这是 GANs 的固有特性，当 G 开始产生狭窄的样本种类时，就会发生模式坍塌，从而抑制输出多样性，损害网络的有效性。梯度惩罚和谱归一化等技术已经缓解了这些问题。这种现象可能会显著降低生成数据的质量，限制
    GANs 在需要多样化输出的应用中的使用，例如机器学习中的数据增强或创意产业中生成多样化的设计替代方案。'
- en: Of course, GANs carry the same ethical considerations as any state-of-the-art
    generative synthesis. For instance, they can be used to create deepfakes or generate
    biased outputs that reinforce societal prejudices. For example, when GANs, often
    used to generate synthetic data (e.g., faces), underrepresent certain groups,
    downstream applications may exhibit gender or racial bias (Kenfack et al., 2021).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，GANs 与任何最先进的生成合成一样，具有相同的伦理考量。例如，它们可以被用来创建深度伪造或生成具有社会偏见的输出。例如，当 GANs（常用于生成合成数据，例如人脸）未能充分代表某些群体时，下游应用可能会表现出性别或种族偏见（Kenfack
    等人，2021 年）。
- en: Even with the advent of other generative models such as diffusion models and
    Transformer-based image generators, GANs have played a seminal role in shaping
    the trajectory of generative image synthesis, showcasing both the potential and
    some of the challenges inherent in this domain.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 即使随着扩散模型和基于 Transformer 的图像生成器等其他生成模型的兴起，GANs 仍在塑造生成图像合成轨迹方面发挥了开创性的作用，展示了该领域潜力和一些固有的挑战。
- en: Now that we better understand GANs in the context of deep generative models,
    let’s shift our focus to a successor in image generation, the diffusion model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更好地理解了在深度生成模型背景下 GANs，让我们将焦点转向图像生成领域的继任者，即扩散模型。
- en: A closer look at diffusion models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨扩散模型
- en: Having explored the dynamics of GANs, let’s transition our attention to a subsequent
    innovation in image generation – the diffusion model. Initially proposed by Sohl-Dickstein
    et al. in 2015, diffusion models present a novel approach, where a neural network
    iteratively introduces and subsequently removes noise from data to generate highly
    refined images. Unlike GANs, which leverage an adversarial mechanism involving
    two contrasting models, diffusion models apply a more gradual, iterative process
    of noise manipulation within the data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了生成对抗网络（GANs）的动力学之后，让我们将注意力转向图像生成领域的后续创新——扩散模型。扩散模型最初由Sohl-Dickstein等人于2015年提出，它提供了一种新颖的方法，其中神经网络通过迭代地向数据中引入和随后移除噪声来生成高度精细的图像。与利用两个对比模型之间的对抗机制的GANs不同，扩散模型在数据中应用了一种更渐进、迭代的噪声操纵过程。
- en: In practical terms, GANs have shown substantial merit in art and design, creating
    realistic faces or generating sharp, high-fidelity images from descriptions. They
    are also used in data augmentation, expanding datasets by generating realistic
    synthetic data to augment the training of machine learning models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，GANs在艺术和设计领域显示出巨大的价值，可以创建逼真的面孔或从描述中生成清晰、高保真度的图像。它们还用于数据增强，通过生成逼真的合成数据来扩展数据集，以增强机器学习模型的训练。
- en: Conversely, diffusion models excel in tasks requiring a structured approach
    to image generation, such as in medical imaging. Their iterative process can enhance
    the quality of medical images, such as MRI or CT scans, where noise reduction
    and clarity are paramount. This makes diffusion models invaluable in clinical
    settings, aiding in better diagnostics and analysis. Moreover, their controlled
    and gradual process offers a more predictable or stable training process compared
    to the adversarial and dynamic training of GANs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，扩散模型在需要结构化方法进行图像生成的任务中表现出色，例如在医学成像中。它们的迭代过程可以提升医学图像的质量，如MRI或CT扫描，在这些图像中，噪声减少和清晰度至关重要。这使得扩散模型在临床环境中变得非常有价值，有助于更好的诊断和分析。此外，与GANs的对抗性和动态训练相比，它们的控制和渐进过程提供了一个更可预测或稳定的训练过程。
- en: 'The foundation of diffusion models is anchored in two primary processes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的基础建立在两个主要过程之上：
- en: '`x``₀`) and iteratively introduces Gaussian noise, akin to progressively applying
    a fog-like filter, transforming the data into indistinguishable noise (`x``ₜ`).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`x``₀`)并迭代地引入高斯噪声，类似于逐渐应用一种雾状过滤器，将数据转换为不可区分的噪声(`x``ₜ`)。
- en: '`p``θ`) attempts to eliminate (or de-fog) the noise from the noisy data (`x``ₜ`),
    aiming to revert to the original clean state (`x``ₜ₋₁`). Specifically, this reversion
    is orchestrated by estimating the probability of transitioning from the noisy
    state back to the clear state, using a conditional distribution denoted as `p``θ``(x``ₜ₋₁``|x``ₜ``)`.
    A **conditional distribution** tells us the likelihood of one event happening
    when we know another related event has occurred. In this case, the reversion estimates
    the likelihood of reverting to the original state, given some amount of noise.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`p``θ`)试图消除（或去雾）噪声数据(`x``ₜ`)中的噪声，旨在恢复到原始的干净状态(`x``ₜ₋₁`)。具体来说，这种恢复是通过估计从噪声状态回到清晰状态的概率来实现的，使用一个条件分布表示为`p``θ``(x``ₜ₋₁``|x``ₜ``)`。**条件分布**告诉我们，当我们知道另一个相关事件已经发生时，一个事件发生的可能性。在这种情况下，恢复估计了在给定一定量的噪声的情况下恢复到原始状态的可能性。
- en: In the pivotal work *Score-Based Generative Modeling through Stochastic Differential
    Equations*, the authors propose a novel framework that unifies score-based generative
    models and diffusion probabilistic modeling by employing `p``θ`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键工作《基于分数的生成模型通过随机微分方程》中，作者们提出了一种新颖的框架，通过使用`p``θ`将基于分数的生成模型和扩散概率建模统一起来。
- en: The reverse model (`p``θ`) was implemented using convolutional networks to predict
    variations in the Gaussian noise distribution – a critical component of the noise-introduction
    process within the forward diffusion. Initially, the efficacy of this approach
    was validated on more straightforward datasets. However, the methodology’s applicability
    was later significantly improved to handle more complex images (Ho et al., 2020).
    This expansion demonstrated the practical potential of diffusion models in generating
    highly refined images across a broader spectrum of complexities.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模型（`p``θ`）使用卷积网络实现，以预测高斯噪声分布的变化——这是正向扩散中噪声引入过程中的一个关键组件。最初，这种方法的有效性在更简单的数据集上得到了验证。然而，该方法的应用性后来得到了显著提高，以处理更复杂的图像（Ho等人，2020）。这一扩展展示了扩散模型在生成具有更广泛复杂性的高度精细图像中的实际潜力。
- en: Advancement of diffusion models
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩散模型的进步
- en: 'Since its inception, diffusion model technology has witnessed key advancements,
    propelling its capabilities in image generation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自从其诞生以来，扩散模型技术已经见证了关键进步，推动了其在图像生成方面的能力：
- en: '**Simplified training objectives**: Ho et al. proposed simplified training
    objectives that predict Gaussian noise directly, eliminating the need for conditional
    means and facilitating the application to more complex datasets (Ho et al., 2020).
    This advancement facilitated handling more complex datasets, potentially aiding
    in tasks such as anomaly detection or complex data synthesis, which could be resource-intensive
    with traditional models.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化的训练目标**：Ho等人提出了简化的训练目标，这些目标直接预测高斯噪声，消除了对条件均值的需求，并促进了其在更复杂数据集上的应用（Ho等人，2020）。这一进步有助于处理更复杂的数据集，可能有助于异常检测或复杂数据合成等任务，这些任务可能需要传统模型的大量资源。'
- en: '**UNet modules with self-attention**: Ho et al. also incorporated UNet modules
    with self-attention into the diffusion model architecture, inspired by PixelCNN++
    by Salimans et al. (2017), enhancing a model’s performance on complex datasets
    (Ho et al., 2020). Again, enhancing performance on complex datasets facilitates
    better image restoration, which is particularly beneficial in fields such as medical
    imaging or satellite imagery analysis, where high-fidelity image reconstruction
    is crucial.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有自注意力的UNet模块**：Ho等人还将带有自注意力的UNet模块纳入扩散模型架构中，受到Salimans等人（2017）的PixelCNN++的启发，增强了模型在复杂数据集上的性能（Ho等人，2020）。再次，提高复杂数据集上的性能有助于更好的图像恢复，这在医学成像或卫星图像分析等领域尤其有益，在这些领域中，高保真图像重建至关重要。'
- en: '**Synchronization with SDEs**: Song et al. defined diffusion models as solutions
    to SDEs, linking score learning with denoising score-matching losses and expanding
    model usage for image generation, editing, in-painting, and colorization (Song
    et al., 2020).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与SDEs同步**：Song等人将扩散模型定义为SDEs的解，将分数学习与去噪分数匹配损失联系起来，并扩展了模型在图像生成、编辑、修复和着色等领域的应用（Song等人，2020）。'
- en: 'Following these foundational advancements, diffusion models witnessed a wave
    of innovative enhancements as researchers introduced novel methodologies to address
    existing challenges and broaden a model’s applicability in generative modeling
    tasks. These advancements include the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基础进步之后，扩散模型见证了创新增强的浪潮，研究人员引入了新的方法来解决现有挑战并扩大模型在生成建模任务中的应用范围。这些进步包括以下内容：
- en: '**Noise conditioning and annealing strategies**: Song et al. improved score-based
    models by including noise conditioning and annealing strategies, achieving performance
    comparable to GANs on benchmark datasets like the Flickr-Faces-HQ dataset (Song
    et al., 2021), which is a high-quality image dataset of human faces designed to
    measure GAN performance. Achieving performance comparable to GANs could make diffusion
    models a viable alternative for high-fidelity image generation tasks in areas
    where GANs are traditionally used.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声条件和退火策略**：Song等人通过包括噪声条件和退火策略改进了基于分数的模型，在Flickr-Faces-HQ数据集（Song等人，2021）等基准数据集上实现了与GANs相当的性能，该数据集是一个高质量的人脸图像数据集，旨在衡量GANs的性能。实现与GANs相当的性能可以使扩散模型成为GANs传统应用领域内高保真图像生成任务的可行替代方案。'
- en: '**Latent Diffusion Models** (**LDMs**): Rombach et al. addressed computational
    inefficiency by proposing LDMs, which operate in a compressed latent space learned
    by autoencoders, employing perceptual losses to create a visually equivalent,
    reduced latent space (Rombach et al., 2021). By addressing computational inefficiency,
    LDMs could expedite the image generation process, making them suitable for real-time
    applications or scenarios where computational resources are limited.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在扩散模型（LDMs）**：Rombach等人通过提出LDMs来解决计算效率低下的问题，这些模型在自动编码器学习的压缩潜在空间中运行，使用感知损失来创建一个视觉上等效的、减少的潜在空间（Rombach等人，2021）。通过解决计算效率低下的问题，LDMs可以加速图像生成过程，使其适用于实时应用或计算资源有限的场景。'
- en: '**Classifier-free guidance**: Ho & Salimans introduced classifier-free guidance
    for controlled generation without relying on pre-trained networks, marking a step
    toward more flexible generation techniques (Ho & Salimans, 2022). This advancement
    led to more flexible generation techniques, enabling more controlled and customized
    image generation in applications such as design, advertising, or content creation
    without relying on pre-trained networks.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无分类器引导**：Ho和Salimans引入了无分类器引导，以实现不受预训练网络依赖的受控生成，这标志着向更灵活的生成技术的迈进（Ho & Salimans，2022）。这一进步导致了更灵活的生成技术，使得在诸如设计、广告或内容创作等应用中，可以实现更受控和定制的图像生成，而不依赖于预训练网络。'
- en: 'Subsequent explorations in the diffusion model domain extended its applications,
    showcasing versatility:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型领域的后续探索扩展了其应用，展示了其多功能性：
- en: '**Video generation**: Ho et al. adapted diffusion models for video generation,
    demonstrating their utility beyond static image generation (Ho et al., 2022)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频生成**：Ho等人将扩散模型应用于视频生成，证明了其在静态图像生成之外的实用性（Ho等人，2022）。'
- en: '**3D data processing**: Luo & Hu extended the application to 3D data processing,
    showcasing the flexibility of diffusion models (Luo & Hu, 2021)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3D数据处理**：Luo和Hu将应用扩展到3D数据处理，展示了扩散模型的灵活性（Luo & Hu，2021）。'
- en: The evolution of diffusion models has led to enhanced image generation and expanded
    applications in video, 3D data processing, and rapid learning methodologies. However,
    the methodology does have its challenges and limitations, outlined in some detail
    in the section that follows..
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的演变导致了图像生成的增强以及在视频、3D数据处理和快速学习方法中的应用扩展。然而，该方法确实存在其挑战和局限性，以下章节将详细阐述。
- en: Limitations and challenges of diffusion models
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩散模型的局限性和挑战
- en: 'Despite their evident benefits and notable progress, diffusion models have
    some unique limitations, such as the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管扩散模型具有明显的优点和显著的进步，但它们仍有一些独特的局限性，如下所述：
- en: '**Sampling speed**: A notable limitation of diffusion models is the slow sampling
    process, particularly when compared to GANs. Sampling, in this context, refers
    to the process of generating new data points from the learned distribution of
    a model. The speed at which new samples can be generated is crucial for many real-time
    or near-real-time applications, and the slower sampling speed of diffusion models
    can be a significant drawback.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样速度**：扩散模型的一个显著局限性是缓慢的采样过程，尤其是在与GANs相比时。在此背景下，采样指的是从模型学习到的分布中生成新的数据点的过程。新样本生成的速度对于许多实时或准实时应用至关重要，而扩散模型的较慢采样速度可能是一个重大的缺点。'
- en: '**Stability during large-scale training**: The stability of diffusion models
    during large-scale training is another area requiring further exploration. Large-scale
    training refers to training a model on a substantial amount of data, sometimes
    leading to instability in the model’s learning process. Ensuring stability during
    this phase is crucial to achieve reliable and consistent performance from the
    model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大规模训练中的稳定性**：扩散模型在大规模训练过程中的稳定性是另一个需要进一步探索的领域。大规模训练指的是在大量数据上训练模型，有时会导致模型学习过程中的不稳定性。确保这一阶段的稳定性对于模型实现可靠和一致的性能至关重要。'
- en: A close examination of the societal impact of the media generated by these models
    is crucial, especially given the level of fine control now possible over the generated
    content. However, diffusion models’ inherent simplicity, versatility, and positive
    inductive biases signify a bright future. These attributes suggest a trajectory
    of rapid development within generative modeling, potentially integrating diffusion
    models as pivotal components in various disciplines, such as computer vision and
    graphics.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考察这些模型产生的媒体对社会的影响至关重要，尤其是在现在对生成内容可以进行精细控制的情况下。然而，扩散模型固有的简单性、灵活性和积极的归纳偏差预示着光明的未来。这些属性表明，在生成建模领域将出现快速发展的轨迹，可能将扩散模型作为各个学科（如计算机视觉和图形学）中的关键组成部分。
- en: A closer look at generative transformers
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成变压器的深入探讨
- en: The revolutionary advent of transformer models has significantly impacted the
    task of generating high-fidelity images from text descriptions. Notable models
    such as **CLIP** (**Contrastive Language-Image Pretraining**) and DALL-E utilized
    transformers in unique ways to create images based on natural language captions.
    This section will discuss the transformer-based approach for text-to-image generation,
    its foundations, the key techniques, the resulting benefits, and some challenges.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型革命性的出现对从文本描述生成高保真图像的任务产生了重大影响。如**CLIP**（**对比语言-图像预训练**）和DALL-E等显著模型以独特的方式利用变压器根据自然语言标题创建图像。本节将讨论基于变压器的文本到图像生成方法，其基础、关键技术、带来的好处以及一些挑战。
- en: A brief overview of transformer architecture
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变压器架构的简要概述
- en: The original transformer architecture, introduced by Vaswani et al. in 2017,
    is a cornerstone of many modern language-processing systems. In fact, the transformer
    may be considered the most important architecture in the area of GAI, as it is
    foundational to the GPT series of models and many other state-of-the-art generative
    methods. As such, we’ll cover the architecture briefly in our survey of generative
    approaches but will have a dedicated chapter, where we will have the opportunity
    to deconstruct and implement the transformer from scratch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人于2017年提出的原始变压器架构是许多现代语言处理系统的基石。事实上，变压器可能被认为是GAI领域最重要的架构，因为它构成了GPT系列模型和许多其他最先进生成方法的基础。因此，在我们的生成方法调查中，我们将简要介绍其架构，但将在专门的章节中详细解析和从头实现变压器。
- en: At the core of the transformer architecture lies the **self-attention mechanism**,
    a unique approach that captures complex relationships among different elements
    within an ordered data sequence. These elements, known as **tokens**, represent
    words in a sentence or characters in a word based on the level of granularity
    chosen for **tokenization**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构的核心是**自注意力机制**，这是一种独特的方法，能够捕捉有序数据序列中不同元素之间的复杂关系。这些元素被称为**标记**，根据选择的**标记化**粒度，代表句子中的词语或单词中的字符。
- en: The principle of **attention** in this architecture enables a model to focus
    on certain pivotal aspects of the input data while potentially disregarding less
    significant parts. This mechanism augments the model’s understanding of the context
    and the relative importance of words in a sentence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构中**注意力**原理使模型能够专注于输入数据的某些关键方面，同时可能忽略不那么重要的部分。这种机制增强了模型对上下文以及句子中词语相对重要性的理解。
- en: The transformer bifurcates into two main segments, the **encoder** and the **decoder**,
    each comprising multiple layers of self-attention mechanisms. While the encoder
    discerns relationships between different positions in the input sequence, the
    decoder focuses on the outputs from the encoder, employing a variant of self-attention
    termed **masked self-attention** to prevent consideration of future outputs it
    hasn’t generated yet.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器分为两个主要部分，即**编码器**和**解码器**，每个部分都包含多个自注意力机制层。编码器能够识别输入序列中不同位置之间的关系，而解码器则专注于编码器的输出，使用一种称为**掩码自注意力**的自注意力变体，以防止考虑它尚未生成的未来输出。
- en: The calculation of **attention weights** through the scaled dot-product of query
    and key vectors plays a crucial role in determining the level of focus on different
    parts of the input. Additionally, **multi-head attention** allows the model to
    channel attention toward multiple data points simultaneously.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查询向量和键向量的缩放点积计算**注意力权重**在确定对输入不同部分的关注程度方面起着关键作用。此外，**多头注意力**允许模型同时将注意力导向多个数据点。
- en: Lastly, to retain the sequence order of data, the model adopts a strategy known
    as **positional encoding**. This mechanism is vital for tasks requiring an understanding
    of sequence or temporal dynamics, ensuring the model preserves the initial order
    of data throughout its processing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了保留数据的序列顺序，模型采用了一种称为**位置编码**的策略。这种机制对于需要理解序列或时间动态的任务至关重要，确保模型在整个处理过程中保留数据的初始顺序。
- en: Again, we will revisit the transformer architecture in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081)
    to further reinforce our understanding, as it is foundational to the continued
    research and evolution of generative AI. Nevertheless, with at least a fundamental
    grasp of the Transformer architecture, we are better positioned to dissect transformer-driven
    generative modeling paradigms across a spectrum of applications.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将回顾[第3章](B21773_03.xhtml#_idTextAnchor081)中的转换器架构，以进一步巩固我们的理解，因为它对于生成AI的持续研究和演变是基础性的。然而，至少对转换器架构有一个基本的了解，我们就能更好地分析转换器驱动的生成建模范式在各个应用领域中的表现。
- en: Generative modeling paradigms with transformers
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于转换器的生成建模范式
- en: 'In tackling various tasks, transformers adopt distinct training paradigms aligning
    with the task at hand. For example, discriminative tasks such as classification
    might use a masking paradigm:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理各种任务时，转换器采用与任务相匹配的不同训练范式。例如，分类等判别性任务可能使用掩码范式：
- en: '**Masked Language Modeling** (**MLM**): MLM is a discriminative pretraining
    technique used by models such as **BERT** (**Bidirectional Encoder Representations
    from Transformers**). During training, some percentage of input tokens are randomly
    masked out. The model must then predict the original masked words based on the
    context of the surrounding unmasked words. This teaches the model to build robust
    context-based representations, facilitating many downstream **natural language
    processing** (**NLP**) tasks.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩码语言建模**（**MLM**）：MLM是**BERT**（**转换器双向编码器表示**）等模型使用的判别性预训练技术。在训练过程中，随机掩码掉一部分输入标记。然后模型必须根据周围未掩码的单词的上下文预测原始掩码词。这教会模型构建基于上下文的鲁棒表示，从而促进许多下游**自然语言处理**（**NLP**）任务。'
- en: MLM, as utilized in BERT, has been instrumental in enhancing the performance
    of NLP systems across various domains. For instance, it can power medical coding
    systems in healthcare by accurately identifying and categorizing medical terms
    within clinical notes. This automatic coding can save significant time and reduce
    errors in medical documentation, thereby improving the efficiency and accuracy
    of healthcare data management.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT中使用的MLM对于提高各个领域的NLP系统的性能至关重要。例如，它可以通过准确识别和分类临床笔记中的医疗术语来为医疗编码系统提供动力。这种自动编码可以节省大量时间并减少医疗文档中的错误，从而提高医疗数据管理的效率和准确性。
- en: 'For generative tasks, the focus shifts to creating new data sequences, requiring
    different training paradigms:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成任务，重点转向创建新的数据序列，需要不同的训练范式：
- en: '**Sequence-to-sequence modeling**: Sequence-to-sequence models employ both
    an encoder and a decoder. The encoder maps the input sequence to a latent representation.
    The decoder then generates the target sequence token by token from that representation.
    This paradigm is useful for tasks such as translation, summarization, and question-answering.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到序列建模**：序列到序列模型同时使用编码器和解码器。编码器将输入序列映射到一个潜在表示。然后解码器从这个表示中逐个生成目标序列的标记。这种范式对于翻译、摘要和问答等任务非常有用。'
- en: '**Autoregressive modeling**: Autoregressive modeling generates sequences by
    predicting the next token conditioned only on previous tokens. The model produces
    outputs one step at a time, with each new token depending on those preceding it.
    Autoregressive transformers such as GPT leverage this technique for controlled
    text generation.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归建模**：自回归建模通过仅根据前一个标记预测下一个标记来生成序列。模型逐个步骤地产生输出，每个新标记都依赖于前面的标记。GPT等自回归转换器利用这种技术进行可控文本生成。'
- en: Transformers combine self-attention for long-range dependencies, pre-trained
    representations, and autoregressive decoding to adapt to discriminative and generative
    tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器结合自注意力、预训练表示和自回归解码以适应判别性和生成性任务。
- en: Advanced generative synthesis can be achieved with different architectures that
    make trade-offs between complexity, scalability, and specialization. For example,
    instead of using both the encoder and decoder, many state-of-the-art generative
    models employ a decoder-only or encoder-only approach. The encoder-decoder framework
    is often the most computationally intensive learning to specialize in, as it increases
    model size. Decoder-only architectures leverage powerful pre-trained language
    models such as GPT as the decoder, reducing parameters through weight sharing.
    Encoder-only methods forego decoding, instead, they encode inputs and perform
    regression or search on the resulting embeddings. Each approach has advantages
    that suit certain use cases, datasets, and computational budgets. In the following
    sections, we explore examples of models that employ these derivative transformer
    architectures for creative applications, such as image generation and captioning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在不同架构之间进行权衡，可以在复杂度、可扩展性和专业化之间实现高级生成合成。例如，许多最先进的生成模型不是同时使用编码器和解码器，而是采用仅解码器或仅编码器方法。编码器-解码器框架通常是计算最密集的学习，因为它增加了模型大小。仅解码器架构利用强大的预训练语言模型（如GPT）作为解码器，通过权重共享来减少参数。仅编码器方法放弃了解码，相反，它们对输入进行编码，并在生成的嵌入上进行回归或搜索。每种方法都有其优势，适用于某些用例、数据集和计算预算。在以下章节中，我们将探讨采用这些衍生变压器架构进行创意应用的模型示例，例如图像生成和字幕。
- en: Encoder-only approach
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅编码器方法
- en: In certain models, only the encoder network maps the input to an embedding space.
    The output is then generated directly from this embedding, eliminating the need
    for a decoder. While this straightforward architecture has typically found its
    place in classification or regression tasks, recent advancements have broadened
    its application to more complex tasks. In particular, models developed for tasks
    such as image synthesis leverage the encoder-only setup to process both text and
    visual inputs, creating a multimodal relationship that facilitates the generation
    of high-fidelity images from natural language instruction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些模型中，只有编码器网络将输入映射到嵌入空间。然后直接从这个嵌入生成输出，消除了对解码器的需求。虽然这种直接的架构通常在分类或回归任务中找到其位置，但最近的进展已将其应用扩展到更复杂的任务。特别是，为图像合成等任务开发的模型利用仅编码器设置来处理文本和视觉输入，创建了一个多模态关系，从而有助于从自然语言指令生成高保真图像。
- en: Decoder-only approach
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅解码器方法
- en: Similarly, some models operate using a decoder-only strategy, where a singular
    decoder network is tasked with both encoding the input and generating output.
    This mechanism starts by joining the input and output sequences, which the decoder
    processes. Despite its simplicity and the characteristic sharing of parameters
    between input and output stages, the effectiveness of this architecture relies
    heavily on the pretraining of robust decoders. Recently, even more complex tasks
    such as text-to-image synthesis have seen the successful deployment of the decoder-only
    architecture, illustrating its versatility and adaptability to diverse applications.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，一些模型仅使用解码器策略进行操作，其中单个解码器网络负责编码输入并生成输出。此机制首先将输入和输出序列连接起来，由解码器进行处理。尽管其简单且输入和输出阶段之间存在参数共享的特征，但该架构的有效性在很大程度上依赖于鲁棒解码器的预训练。最近，甚至更复杂的任务，如文本到图像合成，也成功部署了仅解码器架构，展示了其多功能性和对不同应用的适应性。
- en: Advancement of transformers
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变压器的进步
- en: Transformer mechanisms with other novel techniques to tackle generative tasks.
    This evolution led to distinct approaches to handling text and image generation.
    In this section, we will explore some of these innovative models and their unique
    methodologies in advancing GAI.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他新颖技术解决生成任务的变压器机制。这种演变导致了处理文本和图像生成的不同方法。在本节中，我们将探讨一些这些创新模型及其在推进GAI方面的独特方法。
- en: Encoder-decoder image generation with DALL-E
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DALL-E的编码器-解码器图像生成
- en: 'Introduced by Ramesh et al. in 2021, DALL-E employs an encoder-decoder framework
    to facilitate text-to-image generation. This model comprises two primary components:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由Ramesh等人于2021年提出，DALL-E采用编码器-解码器框架以促进文本到图像的生成。此模型包含两个主要组件：
- en: '**Text encoder**: Applies the transformer’s encoder, processing plain text
    to derive a semantic embedding that serves as the context for the image decoder.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本编码器**：应用transformer的编码器，处理纯文本以推导出语义嵌入，作为图像解码器的上下文。'
- en: '**Image decoder**: Applies the transformer’s decoder to generate the image
    autoregressively, predicting each pixel based on the text embedding and previously
    predicted pixels.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像解码器**：应用transformer的解码器以自回归方式生成图像，根据文本嵌入和先前预测的像素预测每个像素。'
- en: By training on image-caption datasets, DALL-E refines the transition from text
    to detailed image renderings. This setup underscores the capability of dedicated
    encoder and decoder modules for conditional image generation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在图像-字幕数据集上训练，DALL-E改进了从文本到详细图像渲染的过渡。这种设置强调了专用编码器和解码器模块在条件图像生成中的能力。
- en: Encoder-only image captioning with CLIP
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用CLIP进行仅编码器的图像字幕
- en: CLIP, conceptualized by Radford et al. in 2021, adopts an encoder-only approach
    for image-text tasks. Key components include a visual encoder and a text encoder.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP由Radford等人于2021年提出，采用仅编码器的方法进行图像-文本任务。关键组件包括视觉编码器和文本编码器。
- en: Visual Encoder and Text Encoder process the image and candidate captions, respectively,
    determining the matching caption based on encoded representations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器和文本编码器分别处理图像和候选字幕，根据编码表示确定匹配的字幕。
- en: Pretraining on extensive image-text datasets enables CLIP to establish a shared
    embedding space, facilitating efficient inference for retrieval-based captioning.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛的电影-文本数据集上进行预训练使CLIP能够建立共享嵌入空间，从而促进基于检索的字幕的快速推理。
- en: Improving image fidelity with scaled transformers (DALL-E 2)
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用扩展的Transformer提高图像保真度（DALL-E 2）
- en: 'Ramesh et al. in 2022 extended DALL-E to DALL-E 2, showcasing techniques to
    enhance visual quality:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Ramesh等人于2022年将DALL-E扩展到DALL-E 2，展示了提高视觉质量的技术：
- en: '**A scaled-up decoder**: By expanding the decoder to 3.5 billion parameters
    and applying classifier-free guidance during sampling, visual quality in complex
    image distributions such as human faces is significantly improved.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展的解码器**：通过将解码器扩展到35亿个参数，并在采样期间应用无分类器指导，显著提高了复杂图像分布（如人脸）中的视觉质量。'
- en: '**Hierarchical decoding for high-resolution images (GLIDE)**: Proposed by Nichol
    et al. in 2021, GLIDE employs a hierarchical generation strategy.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于高分辨率图像的分层解码（GLIDE）**：由Nichol等人于2021年提出，GLIDE采用分层生成策略。'
- en: '**A coarse-to-fine approach**: This entails an initial low-resolution image
    prediction followed by progressive detailing through up-sampling and refining,
    capturing global structure and high-frequency textures.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由粗到细的方法**：这包括初始的低分辨率图像预测，然后通过上采样和细化进行渐进式细化，捕捉全局结构和高频纹理。'
- en: Multimodal image generation with GPT-4
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GPT-4进行多模态图像生成
- en: 'GPT-4 developed by OpenAI, is a powerful multimodal model based on the Transformer
    architecture. GPT-4 demonstrates a capability for conditional image generation
    without requiring continued training or fine-tuning:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI开发的GPT-4是一个基于Transformer架构的强大多模态模型。GPT-4展示了在无需持续训练或微调的情况下进行条件图像生成的能力：
- en: '**Pretraining and fine-tuning**: The massive scale of GPT-4 and its pretraining
    on diverse datasets enable a robust understanding of relationships between textual
    and visual data.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练和微调**：GPT-4的巨大规模及其在多样化数据集上的预训练，使其能够对文本和视觉数据之间的关系有稳健的理解。'
- en: '**Multimodal generation:** GPT-4 can generate images based on text descriptions.
    The model uses a deep neural network to encode the semantic meaning of the text
    into a visual representation. Given a text prompt, GPT-4 generates an image by
    predicting the visual content consistent with the provided text. This involves
    taking high-dimensional text embeddings and processing them through successive
    neural network layers to generate a corresponding visual representation.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态生成**：GPT-4可以根据文本描述生成图像。该模型使用深度神经网络将文本的语义意义编码为视觉表示。给定一个文本提示，GPT-4通过预测与提供的文本一致的视觉内容来生成图像。这涉及到通过连续的神经网络层处理高维文本嵌入，以生成相应的视觉表示。'
- en: Using a pretrained multimodal model eliminates the need for a separate encoder
    module for image inputs, facilitating rapid adaptation for image generation tasks.
    This approach underscores the versatility and power of Transformer architectures
    in generative tasks, providing a streamlined methodology to translate text into
    high-quality images.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的多模态模型消除了为图像输入单独编码器模块的需求，从而促进了图像生成任务的快速适应。这种方法强调了Transformer架构在生成任务中的灵活性和强大功能，提供了一种将文本转换为高质量图像的简化方法。
- en: Transformer architectures offer many benefits for controlled image generation
    when compared to GANs. Their autoregressive nature ensures precise control over
    image construction while allowing you to adapt to varying computational needs
    and diverse downstream applications. However, transformers also introduce new
    challenges in this domain.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与GANs相比，Transformer架构在可控图像生成方面提供了许多好处。它们的自回归特性确保了对图像构建的精确控制，同时允许您适应不同的计算需求和多样化的下游应用。然而，Transformer也引入了这一领域的新挑战。
- en: Limitations and challenges of transformer-based approaches
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于Transformer的方法的局限性和挑战
- en: Some early transformers-based approaches demonstrated slower sampling speed
    and restricted fidelity compared to GANs. Generating or manipulating images while
    maintaining precise control over specific attributes or characteristics of the
    objects within those images remains challenging. Additionally, training large-scale
    transformers that can overcome these challenges demands extensive computing resources.
    Notwithstanding, current multimodal results demonstrate a rapidly evolving and
    promising landscape.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基于早期Transformer的方法在采样速度和保真度方面比GANs慢，同时保持对图像中特定属性或特征的精确控制，在生成或操作图像时仍然具有挑战性。此外，训练能够克服这些挑战的大规模Transformer需要大量的计算资源。尽管如此，当前的多模态结果展示了一个快速发展和充满希望的局面。
- en: We must also remember that alongside technical challenges there are broader
    sociotechnical implications and considerations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须记住，在技术挑战的同时，还存在更广泛的社会技术影响和考虑因素。
- en: Bias and ethics in generative models
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型中的偏差和伦理问题
- en: Significant advancements in generative models such as GANs, diffusers, and transformers
    necessitate serious contemplation of potential bias and ethical implications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型（如GANs、diffusers和transformers）的显著进步需要认真思考潜在的偏差和伦理影响。
- en: We need to remain alert to the risk of reinforcing prejudices and stereotypes
    that reflect skewed training data. For instance, diffusion models trained on data
    that over-represents specific demographics might propagate these biases in their
    output. Analogously, language models exposed to toxic or violent content during
    training might generate similar content.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要保持警惕，防止强化反映训练数据偏差的偏见和刻板印象。例如，在训练数据中过度代表特定人群的扩散模型可能会在其输出中传播这些偏差。类似地，在训练过程中接触到有毒或暴力内容的语言模型可能会生成类似的内容。
- en: The directive nature of prompt-based generation also, unfortunately, opens doors
    to misuse if deployed carelessly. Transformers risk facilitating impersonation,
    misinformation, and the creation of deceptive content. Image synthesis models
    such as GANs could potentially be exploited to generate non-consensual deepfakes
    or artificial media.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的生成指令性质，不幸的是，如果部署不当，也会打开滥用的大门。Transformer可能会促进模仿、错误信息和欺骗性内容的创建。GANs等图像合成模型可能被用于生成非同意的深度伪造或人工媒体。
- en: Additionally, the potential for ultra-realistic output prompts ethical dilemmas
    regarding consent, privacy, identity, and copyright. The ability to create convincingly
    real yet fictional faces or voices complicates the distinction between real and
    synthetic, necessitating careful examination of training data sources and generative
    capabilities.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，超逼真输出的可能性引发了关于同意、隐私、身份和版权的伦理困境。能够创建令人信服的虚构面孔或声音，使得真实与合成之间的区别变得复杂，需要仔细检查训练数据来源和生成能力。
- en: Further, as these technologies become ubiquitous, their societal impact must
    be considered. Defining clear policies will be crucial as the distinction between
    authentic and AI-generated content becomes increasingly ambiguous. Upholding principles
    of integrity, attribution, and consent remains vital.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着这些技术的普及，必须考虑其社会影响。随着真实和AI生成内容之间的区别变得越来越模糊，制定明确的政策将至关重要。坚持诚信、归属和同意的原则仍然至关重要。
- en: Despite these risks, the potential benefits of generative models are substantial.
    Addressing bias proactively, advocating transparency, auditing data and models,
    and implementing safeguards become increasingly critical as technologies evolve.
    Ultimately, the responsibility to ensure fairness, accountability, and ethical
    practice falls on all developers and practitioners.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些风险，生成模型潜在的好处是巨大的。随着技术的演变，积极应对偏见、倡导透明度、审计数据和模型以及实施保障措施变得越来越关键。最终，确保公平性、责任和道德实践的责任落在所有开发者和从业者身上。
- en: Applying GAI models – image generation using GANs, diffusers, and transformers
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用GAI模型——使用GANs、diffusers和transformers进行图像生成
- en: In this hands-on section, we’ll reinforce the concepts discussed throughout
    the chapter by putting them into practice. You’ll get a first-hand experience
    and deep dive into the actual implementation of generative models, specifically
    GANs, diffusion models, and transformers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节实践环节中，我们将通过实际操作来巩固本章讨论的概念。你将获得第一手的经验，并深入探究生成模型的实际实现，特别是GANs、扩散模型和transformers。
- en: The Python code provided will guide you through this process. Manipulating and
    observing the code in action will build your understanding of the intricate workings
    and potential applications of these models. This exercise will provide insight
    into model capabilities for tasks like generating art from prompts and synthesizing
    hyper-realistic images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的Python代码将指导你完成这个过程。操作并观察代码的实际运行将帮助你理解这些模型的复杂运作和潜在应用。这项练习将提供关于模型在生成艺术作品、从提示中生成以及合成超逼真图像等任务中的能力见解。
- en: We’ll be utilizing the highly versatile `PyTorch` library, a popular choice
    among machine learning practitioners, to facilitate our operations. `PyTorch`
    provides a powerful and dynamic toolset to define and compute gradients, which
    is central to training these models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用功能强大的`PyTorch`库，这是机器学习从业者中流行的选择，以促进我们的操作。`PyTorch`提供了一组强大且动态的工具，用于定义和计算梯度，这对于训练这些模型至关重要。
- en: In addition, we’ll also use the `diffusers` library. It’s a specialized library
    that provides functionality to implement diffusion models. This library enables
    us to reproduce state-of-the-art diffusion models directly from our workspace.
    It underpins the creation, training, and usage of denoising diffusion probabilistic
    models at an unprecedented level of simplicity, without compromising the models’
    complexity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将使用`diffusers`库。这是一个专门提供实现扩散模型功能的库。这个库使我们能够直接从我们的工作空间中重现最先进的扩散模型。它以前所未有的简单性支撑了去噪扩散概率模型创建、训练和使用的各个方面，同时不牺牲模型的复杂性。
- en: Through this practical session, we’ll explore how to operate and integrate these
    libraries and implement and manipulate GANs, diffusers, and transformers using
    the Python programming language. This hands-on experience will complement the
    theoretical knowledge we have gained in the chapter, enabling us to see these
    models in action in the real world.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次实践课程，我们将探讨如何操作和集成这些库，并使用Python编程语言实现和操作GANs、diffusers和transformers。这种实践经验将补充我们在本章中获得的理论知识，使我们能够在现实世界中看到这些模型的实际应用。
- en: By the end of this section, you will not only have a conceptual understanding
    of these generative models but also understand how they are implemented, trained,
    and used for several innovative applications in data science and machine learning.
    You’ll have a much deeper understanding of how these models work and the experience
    of implementing them yourself.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到本节结束时，你不仅将对这些生成模型有一个概念性的理解，还将了解它们是如何实现、训练以及用于数据科学和机器学习中的多个创新应用的。你将更深入地理解这些模型的工作原理，并亲身体验到实现它们的过程。
- en: Working with Jupyter Notebook and Google Colab
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Jupyter Notebook和Google Colab进行工作
- en: Jupyter notebooks enable live code execution, visualization, and explanatory
    text, suitable for prototyping and data analysis. Google Colab, conversely, is
    a cloud-based version of Jupyter Notebook, designed for machine learning prototyping.
    It provides free GPU resources and integrates with Google Drive for file storage
    and sharing. We’ll leverage Colab as our prototyping environment going forward.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本支持实时代码执行、可视化以及解释性文本，非常适合原型设计和数据分析。相反，Google Colab是Jupyter笔记本的云端版本，专为机器学习原型设计。它提供免费的GPU资源，并与Google
    Drive集成以实现文件存储和共享。我们将利用Colab作为后续的原型环境。
- en: Stable diffusion transformer
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散transformer
- en: We begin with a pre-trained stable diffusion model, a text-to-image latent diffusion
    model created by researchers and engineers from CompVis, Stability AI, and LAION
    (Patil et al., 2022). The diffusion process is used to draw samples from complex,
    high-dimensional distributions, and when it interacts with the text embeddings,
    it creates a powerful conditional image synthesis model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从预训练的稳定扩散模型开始，这是一个由 CompVis、Stability AI 和 LAION 的研究人员和工程师创建的文本到图像的潜在扩散模型（Patil
    等人，2022 年）。扩散过程用于从复杂的高维分布中抽取样本，当它与文本嵌入交互时，它创建了一个强大的条件图像合成模型。
- en: The term “stable” in this context refers to the fact that during training, a
    model maintains certain properties that stabilize the learning process. Stable
    diffusion models offer rich potential to create entirely new samples from a given
    data distribution, based on text prompts.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，“稳定”一词指的是在训练过程中，模型保持某些属性以稳定学习过程的事实。稳定的扩散模型提供了丰富的潜力，可以从给定的数据分布中创建全新的样本，基于文本提示。
- en: Again, for our practical example, we will Google Colab to alleviate a lot of
    initial setups. Colab also provides all of the computational resources needed
    to begin experimenting right away. We start by installing some libraries, and
    with three simple functions, we will build out a minimal `StableDiffusionPipeline`
    using a well-established open-source implementation of the stable diffusion method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了我们的实际示例，我们将使用 Google Colab 来减轻许多初始设置。Colab 还提供了开始实验所需的全部计算资源。我们首先安装一些库，然后通过三个简单的函数，我们将使用稳定的扩散方法的成熟开源实现来构建一个最小的
    `StableDiffusionPipeline`。
- en: First, let’s navigate to our pre-configured Python environment, Google Colab,
    and install the `diffusers` open-source library, which will provide most of the
    key underlying components we need for our experiment.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导航到我们的预配置的 Python 环境，Google Colab，并安装 `diffusers` 开源库，它将为我们实验提供大部分关键的基础组件。
- en: 'In the first cell, we install all dependencies using the following `bash` command.
    Note the exclamation point at the beginning of the line, which tells our environment
    to reach down to its underlying process and install the packages we need:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个单元格中，我们使用以下 `bash` 命令安装所有依赖项。注意行首的感叹号，它告诉我们的环境深入到底层进程并安装所需的软件包：
- en: '[PRE0]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we import the libraries we’ve just installed to make them available to
    our Python program:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们导入我们刚刚安装的库，使它们可供我们的 Python 程序使用：
- en: '[PRE1]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we’re ready for our three functions, which will execute the three tasks
    – loading the pre-trained model, generating the images based on prompting, and
    rendering the images:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好我们的三个函数，它们将执行三个任务——加载预训练模型、根据提示生成图像以及渲染图像：
- en: '[PRE2]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In summary, `load_model` loads a machine learning model identified by `model_id`
    onto a GPU for faster processing. The `generate_images` function takes this model
    and a list of prompts to create our images. Within this function, you will notice
    `torch.autocast("cuda")`, which is a special command that allows PyTorch (our
    underlying machine learning library) to perform operations faster while maintaining
    accuracy. Lastly, the `render_images` function displays these images in a simple
    grid format, making use of the `matplotlib` visualization library to render our
    output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，`load_model` 函数将使用 `model_id` 识别的机器学习模型加载到 GPU 上以实现更快的处理。`generate_images`
    函数接受这个模型和一系列提示来创建我们的图像。在这个函数中，你会注意到 `torch.autocast("cuda")`，这是一个特殊的命令，允许 PyTorch（我们的底层机器学习库）在保持准确性的同时更快地执行操作。最后，`render_images`
    函数以简单的网格格式显示这些图像，利用 `matplotlib` 可视化库来渲染我们的输出。
- en: 'With our functions defined, we select our model version, define our pipeline,
    and execute our image generation process:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了我们的函数后，我们选择我们的模型版本，定义我们的流水线，并执行我们的图像生成过程：
- en: '[PRE3]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output in *Figure 2**.1* is a vivid example of the imaginativeness and creativity
    we typically expect from human art, generated entirely by the diffusion process.
    Except, how do we measure whether the model was faithful to the text provided?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.1* 的输出是一个生动的例子，展示了我们通常期望从人类艺术中得到的想象力和创造力，这些完全是由扩散过程生成的。但是，我们如何衡量模型是否忠实于提供的文本呢？'
- en: '![Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly
    lion” (left) and “A stylized oil painting of a NYC Brownstone” (right)](img/B21773_02_01.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1：提示“一张超逼真的友好狮子照片”（左）和“一张纽约布朗石风格化的油画”（右）的输出](img/B21773_02_01.jpg)'
- en: 'Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion”
    (left) and “A stylized oil painting of a NYC Brownstone” (right)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：对提示“一张逼真的友好狮子照片”（左）和“一张纽约布朗石风格化的油画”（右）的输出
- en: The next step is to evaluate the quality and relevance of our generated images
    in relation to the prompts. This is where CLIP comes into play. CLIP is designed
    to measure the alignment between text and images by analyzing their semantic similarities,
    giving us a true quantitative measure of the fidelity of our synthetic images
    to the prompts.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是评估我们生成的图像与提示之间的质量和相关性。这正是 CLIP 发挥作用的地方。CLIP 通过分析它们的语义相似性来设计测量文本和图像之间的对齐程度，为我们提供了对合成图像与提示之间忠实度的真正定量度量。
- en: Scoring with the CLIP model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CLIP 模型评分
- en: CLIP is trained to understand the relationship between text and images by learning
    to place similar images and text near each other in a shared space. When evaluating
    a generated image, CLIP checks how closely the image aligns with the textual description
    provided. A higher score indicates a better match, meaning the image accurately
    represents the text. Conversely, a lower score suggests a deviation from the text,
    indicating a lesser quality or fidelity to the prompt, providing a quantitative
    measure of how well the generated image adheres to the intended description.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 通过学习在共享空间中将相似图像和文本放置在一起来训练理解文本和图像之间的关系。在评估生成的图像时，CLIP 会检查图像与提供的文本描述的匹配程度。分数越高表示匹配度越好，意味着图像准确地代表了文本。相反，分数较低则表明图像与文本存在偏差，表明质量或对提示的忠实度较低，提供了对生成的图像如何遵守预期描述的定量度量。
- en: 'Again, we will import the necessary libraries:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将导入必要的库：
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We begin by loading the CLIP model, processor, and necessary parameters:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载 CLIP 模型、处理器和必要的参数：
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we define a processing function to adjust the textual prompts and images,
    ensuring that they are in the correct format for CLIP inference:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个处理函数来调整文本提示和图像，确保它们以正确的格式适用于 CLIP 推理：
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this step, we initiate the evaluation process by inputting the images and
    textual prompts into the CLIP model. This is done in parallel across multiple
    devices to optimize performance. The model then computes similarity scores, known
    as logits, for each image-text pair. These scores indicate how well each image
    corresponds to the text prompts. To interpret these scores more intuitively, we
    convert them into probabilities, which indicate the likelihood that an image aligns
    with any of the given prompts:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们通过将图像和文本提示输入到 CLIP 模型中来启动评估过程。这是在多个设备上并行进行的，以优化性能。然后，模型为每个图像-文本对计算相似度分数，称为
    logits。这些分数表示每个图像与文本提示的对应程度。为了更直观地解释这些分数，我们将它们转换为概率，这表示图像与任何给定提示对齐的可能性：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, we display the images along with their scores, visually representing
    how well each image adheres to the provided prompts:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们显示图像及其分数，直观地表示每个图像如何遵守提供的提示：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With everything detailed, let’s execute the pipeline as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 详细说明完毕后，让我们按照以下步骤执行管道：
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We now have scores for each of our synthetic images that quantify the fidelity
    of the synthetic image to the text provided, based on the CLIP model, which interprets
    both image and text data as one combined mathematical representation (or geometric
    space) and can measure their similarity.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了基于 CLIP 模型的每个合成图像的分数，该模型将图像和文本数据解释为一种结合的数学表示（或几何空间），并可以测量它们的相似性。
- en: '![Figure 2.2: CLIP scores](img/B21773_02_02.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2：CLIP 分数](img/B21773_02_02.jpg)'
- en: 'Figure 2.2: CLIP scores'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：CLIP 分数
- en: For our “friendly lion,” we computed scores of 83% and 17% for each prompt,
    which we can interpret as an 83% likelihood that the image aligns with the first
    prompt.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的“友好狮子”，我们对每个提示计算了 83% 和 17% 的分数，我们可以将其解释为有 83% 的可能性图像与第一个提示相符。
- en: 'In practical scenarios, this metric can be applied across various domains:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，此指标可以应用于各个领域：
- en: '**Content moderation**: Automatically moderating or flagging inappropriate
    content by comparing images to a set of predefined descriptive prompts'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容审核**：通过将图像与一组预定义的描述性提示进行比较，自动审核或标记不适当的内容'
- en: '**Image retrieval**: Facilitating refined image searches by matching textual
    queries to a vast database of images, hence narrowing down the search to the most
    relevant visuals'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像检索**：通过将文本查询与庞大的图像数据库匹配，从而缩小搜索范围，仅针对最相关的视觉内容进行搜索'
- en: '**Image captioning**: Assisting in generating accurate captions for images
    by identifying the most relevant descriptive prompts'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像标题生成**：通过识别最相关的描述性提示来协助生成图像的准确标题'
- en: '**Advertising**: Tailoring advertisements based on the content of images on
    a web page to enhance user engagement'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广告**：根据网页上图像的内容定制广告以提高用户参与度'
- en: '**Accessibility**: Enhancing accessibility features by providing accurate descriptions
    of images for individuals with visual impairments'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无障碍性**：通过为视障人士提供图像的准确描述来增强无障碍功能'
- en: This evaluation method not only speeds up processes that would otherwise require
    manual inspection but also lends itself to many applications that could benefit
    from a deeper understanding and contextual analysis of visual data. We will revisit
    the CLIP evaluation in [*Chapter 4*](B21773_04.xhtml#_idTextAnchor123), where
    we simulate a real-world scenario to determine the quality and appropriateness
    of automatically generated captions for a set of product images.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种评估方法不仅加快了原本需要人工检查的过程，而且适用于许多可以从对视觉数据更深入理解和上下文分析中受益的应用。我们将在[*第4章*](B21773_04.xhtml#_idTextAnchor123)中回顾CLIP评估，在那里我们将模拟一个真实世界场景，以确定一组产品图像自动生成标题的质量和适宜性。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored the theoretical underpinnings and real-world applications
    of leading GAI techniques, including GANs, diffusion models, and transformers.
    We examined their unique strengths, including GANs’ ability to synthesize highly
    realistic images, diffusion models’ elegant image generation process, and transformers’
    exceptional language generation capabilities.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了领先生成式人工智能（GAI）技术的理论基础和实际应用，包括生成对抗网络（GANs）、扩散模型和转换器。我们考察了它们的独特优势，包括GANs合成高度逼真图像的能力、扩散模型优雅的图像生成过程以及转换器卓越的语言生成能力。
- en: Using a cloud-based Python environment, we implemented these models to generate
    compelling images and evaluated their output quality using CLIP. We analyzed how
    techniques such as progressive growing and classifier guidance enhanced output
    fidelity over time. We also considered societal impacts, urging developers to
    address potential harm through transparency and ethical practices.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于云的Python环境，我们实现了这些模型以生成引人入胜的图像，并使用CLIP评估了它们的输出质量。我们分析了渐进式增长和分类器引导等技术如何随着时间的推移增强输出保真度。我们还考虑了社会影响，敦促开发者通过透明度和道德实践来应对潜在的伤害。
- en: Generative methods have unlocked remarkable creative potential, but thoughtful
    oversight is critical as capabilities advance. We can guide these technologies
    toward broadly beneficial outcomes by grounding ourselves in core methodologies,
    scrutinizing their limitations, and considering downstream uses. The path ahead
    will require continued research and ethical reflection to unlock AI’s creative
    promise while mitigating risks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式方法释放了显著的创意潜力，但随着能力的提升，深思熟虑的监管至关重要。我们可以通过扎根于核心方法、审视它们的局限性并考虑下游用途，来引导这些技术走向广泛有益的结果。未来的道路将需要持续的研究和道德反思，以释放人工智能的创意潜力，同时减轻风险。
- en: References
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本参考文献部分作为本书中引用的资源的存储库；您可以探索这些资源，以进一步加深对主题的理解和知识：
- en: Kenfack, P. J., Arapov, D. D., Hussain, R., Ahsan Kazmi, S. M., & Khan, A. (2021).
    *On the fairness of generative adversarial networks (**GANs)*. [Arxiv.org](https://Arxiv.org).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kenfack, P. J., Arapov, D. D., Hussain, R., Ahsan Kazmi, S. M., & Khan, A. (2021).
    *关于生成对抗网络（GANs）的公平性*. [Arxiv.org](https://Arxiv.org)。
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., & Bengio, Y. (2014). *Generative adversarial nets. Advances
    in neural information processing* *systems*, 27.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., & Bengio, Y. (2014). *生成对抗网络：神经信息处理系统进展*，第27卷。
- en: 'Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever,
    I., & Chen, M. (2021). *GLIDE: Towards photorealistic image generation and editing
    with text-guided diffusion models*. arXiv preprint arXiv:2112.10741.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever,
    I., & Chen, M. (2021). *GLIDE：使用文本引导的扩散模型实现逼真图像生成和编辑*. arXiv预印本 arXiv:2112.10741。
- en: Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). *Learning
    Transferable Visual Models From Natural Language Supervision.* ArXiv. /abs/2103.00020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). *从自然语言监督中学习可迁移的视觉模型.*
    ArXiv. /abs/2103.00020.
- en: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
    & Sutskever, I. (2022). *Hierarchical text-conditional image generation with clip
    latents*. arXiv preprint arXiv:2204.06125.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
    & Sutskever, I. (2022). *分层文本条件图像生成与clip潜在*. arXiv预印本 arXiv:2204.06125.
- en: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
    & Sutskever, I. (2021). *Zero-shot text-to-image generation. In International
    Conference on Machine Learning* (pp. 8821–8831). PMLR.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
    & Sutskever, I. (2021). *零样本文本到图像生成. 国际机器学习会议论文集 (pp. 8821–8831). PMLR.* '
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, L., & Polosukhin, I. (2017). *Attention is all you need. Advances in neural
    information processing* *systems*, 30.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, L., & Polosukhin, I. (2017). *注意力即所需. 神经信息处理系统进展*, 30.
- en: Arjovsky, M., Chintala, S. & Bottou, L. (2017). *Wasserstein GAN. In Proceedings
    of the 31st International Conference on Neural Information Processing* *System
    (NIPS).*
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arjovsky, M., Chintala, S. & Bottou, L. (2017). *Wasserstein GAN. 在第31届国际神经网络信息处理系统会议
    (NIPS) 上.* '
- en: 'Brock, A., Donahue, J., & Simonyan, K. (2018). *BigGANs: Large Scale GAN Training
    for High Fidelity Natural Image* *Synthesis.* [https://arxiv.org/abs/1809.11096](https://arxiv.org/abs/1809.11096).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brock, A., Donahue, J., & Simonyan, K. (2018). *BigGANs: 大规模GAN训练以实现高保真自然图像合成.*
    [https://arxiv.org/abs/1809.11096](https://arxiv.org/abs/1809.11096).'
- en: Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). *Progressive Growing
    of GANs for Improved Quality, Stability, and* *Variation.* [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). *GANs的渐进式增长以实现改进的质量、稳定性和多样性.*
    [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).
- en: Mirza, M., & Osindero, S. (2014). *Conditional Generative Adversarial* *Nets*.
    [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirza, M., & Osindero, S. (2014). *条件生成对抗网络*. [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
- en: Radford, A., Metz, L., & Chintala, S. (2015). *Unsupervised representation learning
    with deep convolutional generative adversarial networks. 3rd International Conference
    for* *Learning Representations.*
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Radford, A., Metz, L., & Chintala, S. (2015). *无监督表示学习与深度卷积生成对抗网络. 第三届学习表示国际会议.* '
- en: Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). *Unpaired Image-to-Image
    Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the
    IEEE International Conference on Computer* *Vision (ICCV).*
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). *使用循环一致对抗网络进行无配对图像到图像翻译.
    IEEE国际计算机视觉会议论文集 (ICCV).* '
- en: Ho, J., & Salimans, T. (2022). *Classifier-Free Diffusion Guidance. Advances
    in Neural Information Processing* *Systems*, 34.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho, J., & Salimans, T. (2022). *无分类器扩散引导. 神经信息处理系统进展*, 34.
- en: Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., & Fleet, D. J.
    (2022). *Video diffusion models*. arXiv preprint arXiv:2205.10477.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., & Fleet, D. J.
    (2022). *视频扩散模型*. arXiv预印本 arXiv:2205.10477.
- en: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising diffusion probabilistic models.
    Advances in Neural Information Processing Systems*, 33, 6840–6851.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho, J., Jain, A., & Abbeel, P. (2020). *去噪扩散概率模型. 神经信息处理系统进展*, 33, 6840–6851.
- en: Luo, S., & Hu, W. (2021). *Diffusion probabilistic models for 3d point cloud
    generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern*
    *Recognition*, 2837–2845.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo, S., & Hu, W. (2021). *用于3D点云生成的扩散概率模型. IEEE/CVF计算机视觉与模式识别会议论文集*, 2837–2845.
- en: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). *High-resolution
    image synthesis with latent diffusion models. Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern* *Recognition*, 10684–10695.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). *使用潜在扩散模型进行高分辨率图像合成.
    IEEE/CVF计算机视觉与模式识别会议论文集*, 10684–10695.
- en: 'Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). *PixelCNN++:
    Improving the pixelcnn with discretized logistic mixture likelihood and other
    modifications*. arXiv preprint arXiv:1701.05517.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). *PixelCNN++:
    通过离散逻辑混合似然和其他修改改进PixelCNN*. arXiv预印本 arXiv:1701.05517.'
- en: Song, Y., Meng, C., & Ermon, S. (2021). *Denoising diffusion implicit models*.
    arXiv preprint arXiv:2010.02502.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song, Y., Meng, C., & Ermon, S. (2021). *去噪扩散隐式模型*. arXiv预印本 arXiv:2010.02502.
- en: Song, Y., & Ermon, S. (2021). *Improved techniques for training score-based
    generative models. Advances in Neural Information Processing Systems*, 33, 12438–12448.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song, Y., & Ermon, S. (2021). *基于分数的生成模型训练改进技术. 神经信息处理系统进展*, 33, 12438–12448.
- en: Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., & Ganguli, S. (2015).
    *Deep unsupervised learning using nonequilibrium thermodynamics*. arXiv preprint
    arXiv:1503.03585.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., & Ganguli, S. (2015).
    *使用非平衡热力学进行深度无监督学习*. arXiv预印本 arXiv:1503.03585.
- en: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising diffusion probabilistic models.
    Advances in Neural Information Processing Systems*, 33, 6840–6851.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho, J., Jain, A., & Abbeel, P. (2020). *去噪扩散概率模型. 神经信息处理系统进展*, 33, 6840–6851.
- en: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... & Sutskever,
    I. (2022). *Zero-shot text-to-image generation. International Conference on Machine*
    *Learning*, 8821-8831.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... & Sutskever,
    I. (2022). *零样本文本到图像生成. 国际机器学习会议*, 8821-8831.
- en: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ...
    & Amodei, D. (2020). *Language models are few-shot learners. Advances in neural
    information processing systems*, 33, 1877–1901.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ...
    & Amodei, D. (2020). *语言模型是少样本学习者. 神经信息处理系统进展*, 33, 1877–1901.
- en: Patil, S., Cuenca, P., Lambert, N., & von Platen, P. (2022). *Stable diffusion
    with diffusers*. Hugging Face Blog. [https://huggingface.co/blog/stable_diffusion](https://huggingface.co/blog/stable_diffusion).
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil, S., Cuenca, P., Lambert, N., & von Platen, P. (2022). *使用扩散器进行稳定扩散*.
    Hugging Face博客. [https://huggingface.co/blog/stable_diffusion](https://huggingface.co/blog/stable_diffusion).
- en: Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc
    Lê, Luke, Ritobrata Ghosh. (2022, June 4). *DALL-E Mini Explained*. W&B; Weights
    & Biases, Inc. [https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc
    Lê, Luke, Ritobrata Ghosh. (2022, June 4). *DALL-E Mini 解释*. W&B; Weights & Biases,
    Inc. [https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA).
