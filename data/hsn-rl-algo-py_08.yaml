- en: Learning Stochastic and PG Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习随机优化与PG优化
- en: So far, we've addressed and developed value-based reinforcement learning algorithms.
    These algorithms learn a value function in order to be able to find a good policy. Despite
    the fact that they exhibit good performances, their application is constrained
    by some limits that are embedded in their inner workings. In this chapter, we'll
    introduce a new class of algorithms called policy gradient methods, which are
    used to overcome the constraints of value-based methods by approaching the RL
    problem from a different perspective.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨并开发了基于价值的强化学习算法。这些算法通过学习一个价值函数来找到一个好的策略。尽管它们表现良好，但它们的应用受限于一些内在的限制。在本章中，我们将介绍一类新的算法——策略梯度方法，它们通过从不同的角度处理强化学习问题来克服基于价值方法的限制。
- en: Policy gradient methods select an action based on a learned parametrized policy,
    instead of relying on a value function. In this chapter, we will also elaborate
    on the theory and intuition behind these methods, and with this background, develop
    the most basic version of a policy gradient algorithm, named **REINFORCE**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法基于学习到的参数化策略来选择动作，而不是依赖于价值函数。在本章中，我们还将详细阐述这些方法背后的理论和直觉，并在此基础上开发出最基本版本的策略梯度算法，称为**REINFORCE**。
- en: REINFORCE exhibits some deficiencies due to its simplicity, but these can be
    mitigated with only a small amount of additional effort. Thus, we'll present two
    improved versions of REINFORCE, called **REINFORCE** with baseline and **actor-critic**
    (**AC**) models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE由于其简单性存在一些不足，但这些不足只需要稍加努力就可以得到缓解。因此，我们将展示REINFORCE的两个改进版本，分别是带基线的**REINFORCE**和**演员-评论员**（**AC**）模型。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Policy gradient methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度方法
- en: Understanding the REINFORCE algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解REINFORCE算法
- en: REINFORCE with a baseline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带基线的REINFORCE
- en: Learning the AC algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习AC算法
- en: Policy gradient methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度方法
- en: The algorithms that have been learned and developed so far are value-based,
    which, at their core, learn a value function, *V(s)*, or action-value function, *Q(s,
    a)*. A value function is a function that defines the total reward that can be
    accumulated from a given state or state-action pair. An action can then be selected,
    based on the estimated action (or state) values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，学习和开发的算法都是基于价值的，它们的核心是学习一个价值函数，*V(s)*，或动作价值函数，*Q(s, a)*。价值函数是一个定义从给定状态或状态-动作对所能累积的总奖励的函数。然后，可以基于估计的动作（或状态）值来选择一个动作。
- en: 'Therefore, a greedy policy can be defined as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，贪婪策略可以定义如下：
- en: '[![](img/65fc237e-c05d-401a-990e-1965179d5114.png)]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/65fc237e-c05d-401a-990e-1965179d5114.png)]'
- en: Value-based methods, when combined with deep neural networks, can learn very
    sophisticated policies in order to control agents that operate in high-dimensionality
    spaces. Despite these great qualities, they suffer when dealing with problems
    with a large number of actions, or when the action space is continuous.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当结合深度神经网络时，基于价值的方法可以学习非常复杂的策略，从而控制在高维空间中操作的智能体。尽管这些方法有着很大的优点，但在处理具有大量动作或连续动作空间的问题时，它们会遇到困难。
- en: In such cases, maximum operation is not feasible. **Policy gradient** (**PG**)
    algorithms exhibit incredible potential in such contexts, as they can be easily
    adapted to continuous action spaces.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最大化操作是不可行的。**策略梯度**（**PG**）算法在这种背景下展现了巨大的潜力，因为它们可以很容易地适应连续的动作空间。
- en: PG methods belong to the broader class of policy-based methods, including evolution
    strategies, which are studied later in [Chapter 11](dab022a7-3243-4e45-9f91-39a82df3a248.xhtml),
    *Understanding Black-Box Optimization Algorithms*. The distinctiveness of PG algorithms
    is in their use of the gradient of the policy, hence the name **policy gradient**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PG方法属于更广泛的基于策略的方法类，其中包括进化策略，这将在[第11章](dab022a7-3243-4e45-9f91-39a82df3a248.xhtml)《理解黑盒优化算法》中进一步探讨。PG算法的独特性在于它们使用策略的梯度，因此得名**策略梯度**。
- en: 'A more concise categorization of RL algorithms, with respect to the one reported
    in [Chapter 3](f2414b11-976a-4410-92d8-89ee54745d99.xhtml), *Solving Problems
    with Dynamic Programming*, is shown in the following diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于[第3章](f2414b11-976a-4410-92d8-89ee54745d99.xhtml)《使用动态规划解决问题》中报告的强化学习算法分类，下面的图示展示了一种更简洁的分类方式：
- en: '![](img/e4912522-6ce0-48c0-a369-1cf6ecc6b79c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4912522-6ce0-48c0-a369-1cf6ecc6b79c.png)'
- en: Examples of policy gradient methods are **REINFORCE** and **AC **that will be
    introduced in the next sections.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法的例子有**REINFORCE**和**AC**，将在接下来的章节中介绍。
- en: The gradient of the policy
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略的梯度
- en: 'The objective of RL is to maximize the expected return (the total reward, discounted
    or undiscounted) of a trajectory. The objective function, can then be expressed
    as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是最大化一个轨迹的期望回报（总奖励，无论是否折扣）。目标函数可以表示为：
- en: '![](img/50c66cf3-976b-4e4e-ae01-e5cc9abf869b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50c66cf3-976b-4e4e-ae01-e5cc9abf869b.png)'
- en: Where *θ* is the parameters of the policy, such as the trainable variables of
    a deep neural network.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*θ*是策略的参数，例如深度神经网络的可训练变量。
- en: In PG methods, the maximization of the objective function is done through the
    gradient of the objective function [![](img/2efadf0f-29b6-46b5-8e32-ec49106fa982.png)].
    Using gradient ascent, we can improve [![](img/eab1062a-1560-4650-ba0f-baf11baf24ee.png)] by moving
    the parameters toward the direction of the gradient, as the gradient points in
    the direction in which the function increases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在PG方法中，目标函数的最大化是通过目标函数的梯度[![](img/2efadf0f-29b6-46b5-8e32-ec49106fa982.png)]来实现的。通过梯度上升，我们可以通过将参数朝着梯度的方向移动来改善[![](img/eab1062a-1560-4650-ba0f-baf11baf24ee.png)]，因为梯度指向函数增大的方向。
- en: We have to take the same direction of the gradient, because we aim to maximize
    the objective function (6.1).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须沿着梯度的方向前进，因为我们的目标是最大化目标函数（6.1）。
- en: Once the maximum is found, the policy, *π[θ]*, will produce trajectories with
    the highest possible return. On an intuitive level, policy gradient incentivizes
    good policies by increasing their probability while punishing bad policies by
    reducing their probabilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到最大值，策略*π[θ]*将产生回报最高的轨迹。从直观上看，策略梯度通过增加好策略的概率并减少差策略的概率来激励好的策略。
- en: 'Using equation (6.1), the gradient of the objective function is defined as
    follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程（6.1），目标函数的梯度定义如下：
- en: '![](img/ea2657c5-0fb7-4bd5-a3d2-afc8e12e9819.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea2657c5-0fb7-4bd5-a3d2-afc8e12e9819.png)'
- en: By relating to the concepts from the previous chapters, in policy gradient methods,
    policy evaluation is the estimation of the return, ![](img/7706f255-9576-478c-aaf4-c7ffdf9a32be.png).
    Instead, policy improvement is the optimization step of the parameter ![](img/22e0eb66-b158-4ef4-a799-6fce86165959.png).
    Thus, policy gradient methods have to symbiotically carry on both phases in order
    to improve the policy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与前几章的概念相关联，在策略梯度方法中，策略评估是回报的估计，![](img/7706f255-9576-478c-aaf4-c7ffdf9a32be.png)。而策略改进则是参数![](img/22e0eb66-b158-4ef4-a799-6fce86165959.png)的优化步骤。因此，策略梯度方法必须协同进行这两个阶段，以改进策略。
- en: Policy gradient theorem
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度定理
- en: 'An initial problem is encountered when looking at equation (6.2), because,
    in its formulation, the gradient of the objective function depends on the distribution
    of the states of a policy; that is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看方程（6.2）时遇到一个初始问题，因为在其公式中，目标函数的梯度依赖于策略的状态分布；即：
- en: '![](img/9f8a5642-590c-4fca-80ce-db0033892f03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f8a5642-590c-4fca-80ce-db0033892f03.png)'
- en: We would use a stochastic approximation of that expectation, but to compute
    the distribution of the states, ![](img/52a2958e-64e4-44ac-9fe7-64ed137ebf42.png),
    we still need a complete model of the environment. Thus, this formulation isn't
    suitable for our purposes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会使用该期望的随机逼近方法，但为了计算状态的分布，![](img/52a2958e-64e4-44ac-9fe7-64ed137ebf42.png)，我们仍然需要一个完整的环境模型。因此，这种公式不适用于我们的目的。
- en: 'The policy gradient theorem comes to the rescue here. Its purpose is to provide
    an analytical formulation to compute the gradient of the objective function, with
    respect to the parameters of the policy, without involving the derivative of the
    state distribution. Formally, the policy gradient theorem, enables us to express
    the gradient of the objective function as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度定理在这里提供了解决方案。它的目的是提供一个分析公式，用来计算目标函数相对于策略参数的梯度，而无需涉及状态分布的导数。形式上，策略梯度定理使我们能够将目标函数的梯度表示为：
- en: '![](img/9c17f233-6856-4b70-938a-29d6a56f1a4b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c17f233-6856-4b70-938a-29d6a56f1a4b.png)'
- en: The proof of the policy gradient theorem is beyond the scope of this book, and
    thus, isn't included. However, you can find it in the book by Sutton and Barto ([http://incompleteideas.net/book/the-book-2nd.htmlor](http://incompleteideas.net/book/the-book-2nd.htmlor))
    or from other online resources.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度定理的证明超出了本书的范围，因此未包含。然而，你可以在Sutton和Barto的书中找到相关内容 ([http://incompleteideas.net/book/the-book-2nd.htmlor](http://incompleteideas.net/book/the-book-2nd.htmlor))，或者通过其他在线资源查找。
- en: 'Now that the derivative of the objective doesn''t involve the derivative of
    the state distribution, the expectation can be estimated by sampling from the
    policy. Thus, the derivative of the objective can be approximated as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在目标函数的导数不涉及状态分布的导数，可以通过从策略中采样来估计期望值。因此，目标函数的导数可以近似如下：
- en: '![](img/b267976e-96b6-4923-8137-cb7c810ccfaf.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b267976e-96b6-4923-8137-cb7c810ccfaf.png)'
- en: 'This can be used to produce a stochastic update with gradient ascent:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用来通过梯度上升法生成一个随机更新：
- en: '![](img/19e5ff29-d46f-4579-90a2-5f09fd3f80bb.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19e5ff29-d46f-4579-90a2-5f09fd3f80bb.png)'
- en: Note, that because the goal is to maximize the objective function, gradient
    ascent is used to move the parameters in the same direction as the gradient (contrary
    to gradient descent, which performs ![](img/bc2a2826-6786-41f1-ae1e-21dbfbae61c4.png)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于目标是最大化目标函数，因此使用梯度上升来将参数朝梯度的方向移动（与梯度下降相反，梯度下降执行![](img/bc2a2826-6786-41f1-ae1e-21dbfbae61c4.png)）。
- en: The idea behind equation (6.5) is to increase the probability that good actions
    will be re-proposed in the future, while reducing the probability of bad actions.
    The quality of the actions is carried on by the usual scalar value of ![](img/fcb741e8-5b10-4d2a-a4d3-d84fdca36d90.png),
    which gives the quality of the state-action pair.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (6.5) 背后的思想是增加未来重新提出好动作的概率，同时减少坏动作的概率。动作的质量由通常的标量值![](img/fcb741e8-5b10-4d2a-a4d3-d84fdca36d90.png)传递，这给出了状态-动作对的质量。
- en: Computing the gradient
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算梯度
- en: As long as the policy is differentiable, its gradient can be easily computed,
    taking advantage of modern automatic differentiation software.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 只要策略是可微的，它的梯度就可以很容易地计算，借助现代自动微分软件。
- en: To do that in TensorFlow, we can define the computational graph and call `tf.gradient(loss_function,variables)` to
    calculate the gradient of the loss function (`loss_function`) with respect to
    the `variables` trainable parameters. An alternative would be to directly maximize
    the `objective` function using the stochastic gradient descent optimizer, for
    example, by calling `tf.train.AdamOptimizer(lr).minimize(-objective_function)`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，我们可以定义计算图并调用 `tf.gradient(loss_function,variables)` 来计算损失函数(`loss_function`)相对于`variables`可训练参数的梯度。另一种方法是直接使用随机梯度下降优化器最大化`objective`函数，例如，调用
    `tf.train.AdamOptimizer(lr).minimize(-objective_function)`。
- en: 'The following snippet is an example of the steps that are required to compute
    the approximation in formula (6.5), with a policy of discrete action space of
    the `env.action_space.n` dimension:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是计算公式 (6.5) 中近似值所需步骤的示例，使用的是`env.action_space.n`维度的离散动作空间策略：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`tf.one_hot` produces a one-hot encoding of the `actions` actions. That is,
    it produces a mask with `1`, corresponding with the numerical value of the action,
    `0`, in the others.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.one_hot`生成`actions`动作的独热编码。也就是说，它生成一个掩码，其中 `1` 对应动作的数值，其他位置为 `0`。'
- en: 'Then, in the third line of the code, the mask is multiplied by the logarithm
    of the action probability, in order to obtain the log probability of the `actions`
    actions. The fourth line computes the loss as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在代码的第三行中，掩码与动作概率的对数相乘，以获得`actions`动作的对数概率。第四行按如下方式计算损失：
- en: '![](img/a4e87877-72a6-462b-a265-9f2bd0e8d3ea.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4e87877-72a6-462b-a265-9f2bd0e8d3ea.png)'
- en: And finally, `tf.gradient` calculates the gradients of `pi_loss`, with respect
    to the `variables` parameter, as in formula (6.5).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`tf.gradient`计算`pi_loss`的梯度，关于`variables`参数，如公式(6.5)所示。
- en: The policy
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: In the case that the actions are discrete and limited in number, the most common
    approach is to create a parameterized policy that produces a numerical value for
    each action.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作是离散且数量有限，最常见的方法是创建一个参数化策略，为每个动作生成一个数值。
- en: Note that, differently from the Deep Q-Network algorithm, here, the output values
    of the policy aren't the *Q(s,a)* action values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与深度Q网络（Deep Q-Network）算法不同，这里策略的输出值不是 *Q(s,a)* 动作值。
- en: 'Then, each output value is converted to a probability. This operation is performed
    with the softmax function, which is given as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个输出值会被转换成概率。此操作是通过softmax函数执行的，函数如下所示：
- en: '![](img/68fae670-41fd-4d26-8ac0-4bff1fdfcaa5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68fae670-41fd-4d26-8ac0-4bff1fdfcaa5.png)'
- en: The softmax values are normalized to have a sum of one, so as to produce a probability
    distribution where each value corresponds to the probability of selecting a given
    action.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: softmax值被归一化以使其总和为1，从而产生一个概率分布，其中每个值对应于选择给定动作的概率。
- en: 'The next two plots show an example of five action-value predictions before
    (the plot on the left) and after (the right plot) they are applied to the softmax
    function. Indeed, from the plot on the right, you can see that, after the softmax
    is computed, the sum of the new values is one, and that they all have values greater
    than zero:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个图表展示了在应用softmax函数之前（左侧的图）和之后（右侧的图）的五个动作值预测示例。实际上，从右侧的图中可以看到，经过softmax计算后，新值的总和为1，并且它们的值都大于零：
- en: '![](img/e3493f9f-b5bc-4e94-bc9a-cc20b61e8b5d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3493f9f-b5bc-4e94-bc9a-cc20b61e8b5d.png)'
- en: The right plot indicates that actions 0,1,2,3, and 4, will be selected approximately,
    with probabilities of 0.64, 0.02, 0.09, 0.21, and 0.02, correspondingly.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的图表表示，动作0、1、2、3和4将分别以0.64、0.02、0.09、0.21和0.02的概率被选择。
- en: 'To use a softmax distribution on the action values that are returned by the
    parameterized policy, we can use the code that is given in the *Computing the
    gradient* section, with only one change, which has been highlighted in the following
    snippet:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在由参数化策略返回的动作值上使用softmax分布，我们可以使用*计算梯度*部分给出的代码，只需做一个改动，以下代码片段中已做突出显示：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we used `tf.nn.log_softmax`, because it's been designed to be more stable
    than first calling `tf.nn.softmax`, and then `tf.math.log`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`tf.nn.log_softmax`，因为它比先调用`tf.nn.softmax`，再调用`tf.math.log`更稳定。
- en: An advantage of having actions according to stochastic distribution, is in the
    intrinsic randomness of the actions selected, which enable a dynamic exploration
    of the environment. This can seem like a side effect, but it's very important
    to have a policy that can adapt the level of exploration by itself.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 按照随机分布选择动作的一个优势在于动作选择的内在随机性，这使得环境的动态探索成为可能。这看起来像是一个副作用，但拥有能够自主调整探索程度的策略非常重要。
- en: In the case of DQN, we had to use a hand-crafted ![](img/38d16fa6-2b85-4103-87a9-b626e9a67487.png) variable
    to adjust the exploration throughout all the training, using linear ![](img/8ec787b5-7c40-4a5a-a8b4-cd885d8f2355.png) decay.
    Now that the exploration is built into the policy, at most, we have to add a term
    (the entropy) in the loss function in order to incentivize it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN的情况下，我们不得不用手工调整的！[](img/38d16fa6-2b85-4103-87a9-b626e9a67487.png)变量来调整整个训练过程中的探索，使用线性！[](img/8ec787b5-7c40-4a5a-a8b4-cd885d8f2355.png)衰减。现在，探索已经内建到策略中，我们最多只需在损失函数中添加一个项（熵），以此来激励探索。
- en: On-policy PG
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略性PG
- en: A very important aspect of policy gradient algorithms is that they are *on-policy*.
    Their on-policy nature comes from the formula (6.4), as it is dependent on the
    current policy. Thus, unlike off-policy algorithms such as DQN, on-policy methods
    aren't allowed to reuse old experiences.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度算法的一个非常重要的方面是它们是*策略性算法*。它们的策略性特征来自公式（6.4），因为它依赖于当前的策略。因此，与DQN等*非策略性*算法不同，策略性方法不允许重用旧的经验。
- en: This means that all the experience that has been collected with a given policy
    has to be discarded once the policy changes. As a side effect, policy gradient
    algorithms are less sample efficient, meaning that they are required to gain more
    experience to reach the same performance as the off-policy counterpart. Moreover,
    they usually tend to generalize slightly worse.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，一旦策略发生变化，所有使用给定策略收集的经验都必须被丢弃。作为副作用，策略梯度算法的样本效率较低，这意味着它们需要获取更多的经验才能达到与非策略性算法相同的表现。此外，它们通常会稍微泛化得较差。
- en: Understanding the REINFORCE algorithm
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解REINFORCE算法
- en: The core of policy gradient algorithms has already been covered, but we have
    another important concept to explain. We are yet to look at how action values
    are computed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度算法的核心已经介绍过了，但我们还有一个*重要的概念*需要解释。我们还需要看一下如何计算动作值。
- en: 'We already saw with the formula (6.4):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在公式（6.4）中看到了：
- en: '![](img/6455ffe3-0183-406a-9a13-25258d760860.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6455ffe3-0183-406a-9a13-25258d760860.png)'
- en: that we are able to estimate the gradient of the objective function by sampling
    directly from the experience that is collected following the *[![](img/6cf0573f-766a-4776-a94f-18061e74eb65.png)]*
    policy.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够通过直接从经验中采样来估计目标函数的梯度，该经验是通过遵循*[*![](img/6cf0573f-766a-4776-a94f-18061e74eb65.png)]*策略收集的。
- en: The only two terms that are involved are the values of [![](img/80fd981c-9df3-4958-aa68-bdfda5ee263a.png)] and
    the derivative of the logarithm of the policy, which can be obtained through modern
    deep learning frameworks (such as TensorFlow and PyTorch). While we defined [![](img/6cf0573f-766a-4776-a94f-18061e74eb65.png)],
    we haven't explained how to estimate the action-value function, yet.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一涉及的两个项是[![](img/80fd981c-9df3-4958-aa68-bdfda5ee263a.png)]的值和策略对数的导数，这可以通过现代深度学习框架（如TensorFlow和PyTorch）获得。虽然我们已经定义了[![](img/6cf0573f-766a-4776-a94f-18061e74eb65.png)]，但我们尚未解释如何估计动作值函数。
- en: 'The simpler way, introduced for the first time in the REINFORCE algorithm by
    Williams, is to estimate the return is using **Monte Carlo** (**MC**) returns.
    For this reason, REINFORCE is considered an MC algorithm. If you remember, MC
    returns are the return values of sampled trajectories run with a given policy.
    Thus, we can rewrite equation (6.4), changing the action-value function, [![](img/62ce1473-0e9d-40ab-8380-0a8582619415.png)],
    with the MC return, [![](img/b1c3bbc1-40ca-430b-9d92-5de9efad9894.png)]:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首次由Williams在REINFORCE算法中提出的更简单方法是使用**蒙特卡洛**（**MC**）回报来估计回报。因此，REINFORCE被认为是一个MC算法。如果你还记得，MC回报是通过给定策略运行的采样轨迹的回报值。因此，我们可以重写方程（6.4），将动作值函数[![](img/62ce1473-0e9d-40ab-8380-0a8582619415.png)]替换为MC回报[![](img/b1c3bbc1-40ca-430b-9d92-5de9efad9894.png)]：
- en: '![](img/7105239c-6d19-41c4-91fc-ef132185c2ba.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7105239c-6d19-41c4-91fc-ef132185c2ba.png)'
- en: The ![](img/99c1ce90-6620-46ac-8ab2-f040e9e4f939.png) return is computed from
    a complete trajectory, implying that the PG update is available only after![](img/674dc43c-0ce5-40ad-9c80-59cbf1c101b8.png)
    steps, where ![](img/0d80fa61-df32-4816-8ec3-6fa90e1e9de2.png) is the total number
    of steps in a trajectory. Another consequence is that the MC return is well defined
    only in episodic problems, where there is an upper bound to the maximum number
    of steps (the same conclusions that we came up with in the other MC algorithms
    that we previously learned).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/99c1ce90-6620-46ac-8ab2-f040e9e4f939.png)回报是通过完整的轨迹计算得出的，这意味着PG更新只有在完成![](img/674dc43c-0ce5-40ad-9c80-59cbf1c101b8.png)步骤后才可用，其中![](img/0d80fa61-df32-4816-8ec3-6fa90e1e9de2.png)是轨迹中的总步骤数。另一个后果是，MC回报仅在情节性问题中定义良好，在这种问题中，最大步骤数有一个上限（这与我们之前学习的其他MC算法得出的结论相同）。'
- en: 'To get more practical, the discounted return at time ![](img/61b732c8-d3ab-4f9a-84fb-55930cb8428f.png),
    which can also be called the *reward to go*, as it uses only future rewards, is
    as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际一点，时间点![](img/61b732c8-d3ab-4f9a-84fb-55930cb8428f.png)的折扣回报，也可以称为*未来回报*，因为它只使用未来的回报，如下所示：
- en: '![](img/e80d67af-90fc-4e63-b4e6-e5e752037fc7.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e80d67af-90fc-4e63-b4e6-e5e752037fc7.png)'
- en: 'This can be rewritten recursively, as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以递归地重写如下：
- en: '![](img/14e68d1c-fbe4-4af4-b9e6-372e070041d3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14e68d1c-fbe4-4af4-b9e6-372e070041d3.png)'
- en: 'This function can be implemented by proceeding in reverse order, starting from
    the last reward, as shown here:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数可以按相反的顺序实现，从最后一个回报开始，如下所示：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, in the first place, a NumPy array is created, and the value of the last
    reward is assigned to the `rtg` variable. This is done because, at time ![](img/ae84e3b5-19cc-44c1-85de-178788d74d91.png), ![](img/7b3ac824-c869-4c08-b21e-d523ff5f84e3.png).
    Then, the algorithm computes `rtg[i]` backward, using the subsequent value.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，首先创建一个NumPy数组，并将最后一个回报的值分配给`rtg`变量。之所以这样做，是因为在时间点![](img/ae84e3b5-19cc-44c1-85de-178788d74d91.png)，![](img/7b3ac824-c869-4c08-b21e-d523ff5f84e3.png)。然后，算法使用后续值反向计算`rtg[i]`。
- en: 'The main cycle of the REINFORCE algorithm involves running a few epochs until
    it gathers enough experience, and optimizing the policy parameter. To be effective,
    the algorithm has to complete at least one epoch before performing the update
    step (it needs at least a full trajectory to compute the reward to go (![](img/ce76b287-a263-4070-81d0-e973431e5bbd.png))).
    REINFORCE is summarized in the following pseudocode:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE算法的主要循环包括运行几个周期，直到收集到足够的经验，并优化策略参数。为了有效，算法必须在执行更新步骤之前完成至少一个周期（它需要至少一个完整轨迹来计算回报函数（![](img/ce76b287-a263-4070-81d0-e973431e5bbd.png)））。REINFORCE的伪代码总结如下：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Implementing REINFORCE
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现REINFORCE
- en: It's time to implement REINFORCE. Here, we provide a mere implementation of
    the algorithm, without the procedures for its debugging and monitoring. The complete
    implementation is available in the GitHub repository. So, make sure that you check
    it out.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候实现 REINFORCE 了。在这里，我们仅提供算法的实现，而不包括调试和监控过程。完整实现可以在 GitHub 仓库中找到。所以，务必检查一下。
- en: 'The code is divided into three main functions, and one class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码分为三个主要函数和一个类：
- en: '`REINFORCE(env_name, hidden_sizes, lr, num_epochs, gamma, steps_per_epoch)`:
    This is the function that contains the main implementation of the algorithm.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`REINFORCE(env_name, hidden_sizes, lr, num_epochs, gamma, steps_per_epoch)`：这是包含算法主要实现的函数。'
- en: '`Buffer`: This is a class that is used to temporarily store the trajectories.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Buffer`：这是一个类，用于临时存储轨迹。'
- en: '`mlp(x, hidden_layer, output_size, activation, last_activation)`: This is used
    to build a multi-layer perceptron in TensorFlow.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp(x, hidden_layer, output_size, activation, last_activation)`：这是用来在 TensorFlow
    中构建多层感知器的。'
- en: '`discounted_rewards(rews, gamma)`: This computes the discounted reward to go.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`discounted_rewards(rews, gamma)`：该函数计算折扣奖励。'
- en: We'll first look at the main `REINFORCE` function, and then implement the supplementary
    functions and class.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看主要的 `REINFORCE` 函数，然后实现补充的函数和类。
- en: The `REINFORCE `function is divided into two main parts. In the first part,
    the computational graph is created, while in the second, the environment is run
    and the policy is optimized cyclically until a convergence criterion is met.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`REINFORCE` 函数分为两个主要部分。在第一部分，创建计算图；而在第二部分，运行环境并循环优化策略，直到满足收敛标准。'
- en: 'The `REINFORCE` function takes the name of the `env_name` environment as the
    input, a list with the sizes of the hidden layers—`hidden_sizes`, the learning
    rate—`lr`, the number of training epochs—`num_epochs`, the discount value—`gamma`,
    and the minimum number of steps per epoch—`steps_per_epoch`. Formally, the heading
    of `REINFORCE` is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`REINFORCE` 函数以 `env_name` 环境的名称作为输入参数，包含隐藏层大小的列表—`hidden_sizes`，学习率—`lr`，训练周期数—`num_epochs`，折扣因子—`gamma`，以及每个周期的最小步骤数—`steps_per_epoch`。正式地，`REINFORCE`
    的函数头如下：'
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At the beginning of `REINFORCE(..)`, the TensorFlow default graph is reset,
    an environment is created, the placeholder is initialized, and the policy is created.
    The policy is a fully connected multi-layer perceptron, with an output for each
    action, and `tanh` activation, on each hidden layer. The outputs of the multi-layer
    perceptron are the unnormalized values of the actions, called logits. All this
    is done in the following snippet:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `REINFORCE(..)` 开始时，TensorFlow 默认图被重置，环境被创建，占位符被初始化，策略被创建。策略是一个全连接的多层感知器，每个动作对应一个输出，且每一层的激活函数为
    `tanh`。多层感知器的输出是未归一化的动作值，称为 logits。所有这些操作都在以下代码片段中完成：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can then create an operation that will compute the loss function, and one
    that will optimize the policy. The code is similar to the code that we saw earlier,
    in the *The policy* section. The only difference is that now the actions are sampled
    by `tf.random.multinomial` , which follows the action distribution that is returned
    by the policy. This function draws samples from a categorical distribution. In
    our case, it chooses a single action (depending on the environment, it could be
    more than one action).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们可以创建一个操作，用来计算损失函数，并优化策略。代码与我们之前在 *The policy* 部分看到的代码类似。唯一的不同是现在通过 `tf.random.multinomial`
    来采样动作，该函数根据策略返回的动作分布来选择动作。此函数从类别分布中抽取样本。在我们的例子中，它选择一个单一的动作（根据环境，也可能选择多个动作）。
- en: 'The following snippet is the implementation of the REINFORCE update:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是 REINFORCE 更新的实现：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A mask is created over the actions that are chosen during the interaction with
    the environment and multiplied by `log_softmax` in order to obtain ![](img/80cb1341-b1db-4b7f-91f8-618d43bad519.png). Then,
    the full loss function is computed. Be careful—there is a minus sign before `tf.reduce_sum`.
    We are interested in the maximization of the objective function. But because the
    optimizer needs a function to minimize, we have to pass a loss function. The last
    line optimizes the PG loss function using `AdamOptimizer`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在与环境交互过程中，创建一个针对选择的动作的掩码，并与 `log_softmax` 相乘，以便计算 ![](img/80cb1341-b1db-4b7f-91f8-618d43bad519.png)。然后，计算完整的损失函数。注意—在
    `tf.reduce_sum` 前面有一个负号。我们关注的是目标函数的最大化。但因为优化器需要一个最小化的函数，所以我们必须传递一个损失函数。最后一行使用
    `AdamOptimizer` 优化 PG 损失函数。
- en: 'We are now ready to start a session, reset the global variables of the computational
    graph, and initialize some further variables that we''ll use later:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始一个会话，重置计算图的全局变量，并初始化一些稍后会用到的变量：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we create the two inner cycles that will interact with the environment
    to gather experience and optimize the policy, and print a few statistics:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建两个内部循环，这些循环将与环境交互以收集经验并优化策略，并打印一些统计数据：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The two cycles follow the usual flow, with the exception that the interaction
    with the environment stops whenever the trajectory ends, and the temporary buffer
    has enough transitions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个循环遵循通常的流程，唯一的例外是，当轨迹结束时，与环境的交互会停止，并且临时缓冲区有足够的转移。
- en: 'We can now implement the `Buffer` class that contains the data of the trajectories:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现一个`Buffer`类，用于包含轨迹数据：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And finally, we can implement the function that creates the neural network
    with an arbitrary number of hidden layers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以实现一个函数，创建一个具有任意数量隐藏层的神经网络：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, `activation` is the non-linear function that is applied to the hidden
    layers, and `last_activation` is the non-linearity function that is applied to
    the output layer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`activation`是应用于隐藏层的非线性函数，而`last_activation`是应用于输出层的非线性函数。
- en: Landing a spacecraft using REINFORCE
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用REINFORCE着陆航天器
- en: The algorithm is complete however, the most interesting part has yet to be explained.
    In this section, we'll apply REINFORCE to `LunarLander-v2`, an episodic Gym environment
    with the aim of landing a lunar lander.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 算法已经完成，但最有趣的部分还没有解释。在本节中，我们将应用REINFORCE到`LunarLander-v2`，这是一个周期性的Gym环境，目标是让月球着陆器着陆。
- en: 'The following is a screenshot of the game in its initial position, and a hypothetical
    successful final position:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是游戏初始位置的截图，以及一个假设的成功最终位置：
- en: '![](img/b23124f7-ca9a-40e2-8d2f-294d33d31aab.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b23124f7-ca9a-40e2-8d2f-294d33d31aab.png)'
- en: This is a discrete problem, and the lander has to land at coordinates (0,0),
    with a penalty if it lands far from that point. The lander has a positive reward
    when it moves from the top of the screen to the bottom, but when it fires the
    engine to slow down, it loses 0.3 points on each frame.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个离散问题，着陆器必须在坐标(0,0)处着陆，如果远离该点则会受到惩罚。着陆器从屏幕顶部移动到底部时会获得正奖励，但当它开启引擎减速时，每一帧会损失0.3分。
- en: Moreover, depending on the conditions of the landing, it receives an additional
    -100 or +100 points. The game is considered solved with a total of 200 points.
    Each game is run for a maximum of 1,000 steps.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据着陆条件，它会获得额外的-100或+100分。游戏被认为在获得200分时解决。每局游戏最多进行1,000步。
- en: For that last reason, we'll gather at least 1,000 steps of experience, to be
    sure that at least one full episode has been completed (this value is set by the
    `steps_per_epoch` hyperparameter).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，我们将至少收集1,000步的经验，以确保至少完成了一个完整的回合（这个值由`steps_per_epoch`超参数设置）。
- en: 'REINFORCE is run calling the function with the following hyperparameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用带有以下超参数的函数来运行REINFORCE：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Analyzing the results
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析结果
- en: Throughout the learning, we monitored many parameters, including `p_loss` (the
    loss of the policy), `old_p_loss` (the policy's loss before the optimization phase),
    the total rewards, and the length of the episodes, in order to get a better understanding
    of the algorithm, and to properly tune the hyperparameters. We also summarized
    some histograms. Look at the code in the book's repository to learn more about
    the TensorBoard summaries!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个学习过程中，我们监控了许多参数，包括`p_loss`（策略的损失）、`old_p_loss`（优化阶段前的策略损失）、总奖励和每局的长度，以便更好地理解算法，并合理调整超参数。我们还总结了一些直方图。要了解更多关于TensorBoard汇总的内容，请查看书籍仓库中的代码！
- en: 'In the following figure, we have plotted the mean of the total rewards of the
    full trajectories that were obtained during training:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们绘制了训练过程中获得的完整轨迹的总奖励的均值：
- en: '![](img/738017d2-2d7e-46aa-b2cd-4159ce1a0cd6.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/738017d2-2d7e-46aa-b2cd-4159ce1a0cd6.png)'
- en: From this plot, we can see that it reaches a mean score of 200, or slightly
    less, in about 500,000 steps; therefore requiring about 1,000 full trajectories,
    before it is able to master the game.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中，我们可以看到，它在大约500,000步时达到了200的平均分数，或者稍微低一点；因此，在能够掌握游戏之前，约需要1,000个完整的轨迹。
- en: 'When plotting the training performance, remember that it is likely that the
    algorithm is still exploring. To check whether this is true, monitor the entropy
    of the actions. If it''s higher than 0, it means that the algorithm is uncertain
    about the actions selected, and it will keep exploring—choosing the other actions,
    and following their distribution. In this case, after 500,000 steps, the agent
    is also exploring the environment, as shown in the following plot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘制训练性能图时，请记住，算法可能仍在探索中。要检查是否如此，可以监控动作的熵。如果熵大于0，意味着算法对于所选动作不确定，并且它会继续探索——选择其他动作，并遵循它们的分布。在这种情况下，经过500,000步后，智能体仍在探索环境，如下图所示：
- en: '![](img/09e63e62-ee86-4032-8ec3-3459730ece14.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09e63e62-ee86-4032-8ec3-3459730ece14.png)'
- en: REINFORCE with baseline
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带基线的REINFORCE
- en: REINFORCE has the nice property of being unbiased, due to the MC return, which
    provides the true return of a full trajectory. However, the unbiased estimate
    is to the detriment of the variance, which increases with the length of the trajectory. Why?
    This effect is due to the stochasticity of the policy. By executing a full trajectory,
    you would know its true reward. However, the value that is assigned to each state-action
    pair may not be correct, since the policy is stochastic, and executing it another
    time may lead to a new state, and consequently, a different reward. Moreover,
    you can see that the higher the number of actions in a trajectory, the more stochasticity
    you will have introduced into the system, therefore, ending up with higher variance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE具有一个很好的特性，即由于MC回报，它是无偏的，这提供了完整轨迹的真实回报。然而，无偏估计会以方差为代价，方差随着轨迹的长度增加而增大。为什么？这种效应是由于策略的随机性。通过执行完整的轨迹，你会知道它的真实奖励。然而，分配给每个状态-动作对的值可能并不正确，因为策略是随机的，重新执行可能会导致不同的状态，从而产生不同的奖励。此外，你会看到，轨迹中动作的数量越多，系统中引入的随机性就越大，因此，最终会得到更高的方差。
- en: 'Luckily, it is possible to introduce a baseline, ![](img/9a81020c-7604-437d-8bc7-2e35a47209c9.png),
    in the estimation of the return, therefore decreasing the variance, and improving
    the stability and performance of the algorithm. The algorithms that adopt this
    strategy is called **REINFORCE** with baseline, and the gradient of its objective
    function is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，可以在回报估计中引入基线，![](img/9a81020c-7604-437d-8bc7-2e35a47209c9.png)，从而减少方差，并提高算法的稳定性和性能。采用这种策略的算法称为**带基线的REINFORCE**，其目标函数的梯度如下所示：
- en: '![](img/a23a9b32-1a0c-4f25-a33f-89ae7bb58c97.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a23a9b32-1a0c-4f25-a33f-89ae7bb58c97.png)'
- en: 'This trick of introducing a baseline is possible, because the gradient estimator
    still remains unchanged in bias:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 引入基线的这个技巧之所以可行，是因为梯度估计器在偏差上仍然保持不变：
- en: '![](img/d7f552ec-7f1f-4340-b684-0f0ede810167.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7f552ec-7f1f-4340-b684-0f0ede810167.png)'
- en: At the same time, for this equation to be true, the baseline must be a constant
    with respect to the actions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，为了使这个方程成立，基线必须对动作保持常数。
- en: Our job now is to find a good ![](img/94943530-1306-4e84-984e-e6af2b7ca089.png)
    baseline. The simplest way is to subtract the average return.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的任务是找到一个合适的![](img/94943530-1306-4e84-984e-e6af2b7ca089.png)基线。最简单的方法是减去平均回报。
- en: '![](img/48c9de86-fd72-4f33-a17a-e8d21d185829.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48c9de86-fd72-4f33-a17a-e8d21d185829.png)'
- en: 'If you would like to implement this in the REINFORCE code, the only change
    is in the `get_batch()` function of the `Buffer` class:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在REINFORCE代码中实现这一点，唯一需要更改的是`Buffer`类中的`get_batch()`函数：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Although this baseline decreases the variance, it''s not the best strategy.
    As the baseline can be conditioned on the state, a better idea is to use an estimate
    of the value function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个基线减少了方差，但它并不是最佳策略。因为基线可以根据状态进行条件化，一个更好的想法是使用值函数的估计：
- en: '![](img/cd704c56-e37e-4f18-860a-fd35b85acf00.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd704c56-e37e-4f18-860a-fd35b85acf00.png)'
- en: Remember that the ![](img/4f0a54d4-2ee9-43e5-b13f-f4e28cf2f16b.png) value function
    is, on average, the return that is obtained following the ![](img/ff4f4ab5-b993-4e8a-9be3-1b581925803c.png)
    policy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，![](img/4f0a54d4-2ee9-43e5-b13f-f4e28cf2f16b.png)值函数平均值是通过![](img/ff4f4ab5-b993-4e8a-9be3-1b581925803c.png)策略获得的回报。
- en: This variation introduces more complexity into the system, as we have to design
    an approximation of the value function, but it's very common to use, and it considerably
    increases the performance of the algorithm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变体给系统带来了更多复杂性，因为我们必须设计一个值函数的近似，但它是非常常见的，并且能显著提高算法的性能。
- en: 'To learn ![](img/0d4c296b-488f-479e-b008-bfa7f9a5388e.png), the best solution
    is to fit a neural network with MC estimates:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习！[](img/0d4c296b-488f-479e-b008-bfa7f9a5388e.png)，最佳的解决方案是用MC估计拟合一个神经网络：
- en: '![](img/8747c6e4-8e9e-439b-b8db-c0dbb5913b75.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8747c6e4-8e9e-439b-b8db-c0dbb5913b75.png)'
- en: In the preceding equation, ![](img/4f581b80-60d7-46c9-8188-737e83a0c188.png) is
    the parameters of the neural network to be learned.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![](img/4f581b80-60d7-46c9-8188-737e83a0c188.png)是需要学习的神经网络参数。
- en: In order to not overrun the notation, from now on, we'll neglect to specify
    the policy, so that ![](img/35844138-3049-42c1-b4b6-67d55ada95e5.png) will become ![](img/40c76756-1c31-42c1-afbb-0c17efb507e4.png).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不使符号过于复杂，从现在开始，我们将省略指定策略的部分，因此！[](img/35844138-3049-42c1-b4b6-67d55ada95e5.png)将变为！[](img/40c76756-1c31-42c1-afbb-0c17efb507e4.png)。
- en: 'The neural network is trained on the same trajectories'' data that is used
    for learning ![](img/76bcd8d3-a24b-41c2-b16b-84cfdbe47bdd.png), without requiring
    additional interaction with the environment. Once computed, the MC estimates,
    for example, with `discounted_rewards(rews, gamma)`, will become the ![](img/512d6543-23dd-4aac-b864-4ec3642a4a86.png)
    target values, and the neural network will be optimized in order to minimize the
    mean square error (MSE) loss—just as you''d do in a supervised learning task:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在与学习相关的相同轨迹数据上进行训练！[](img/76bcd8d3-a24b-41c2-b16b-84cfdbe47bdd.png)，无需与环境进行额外的交互。计算后，MC估计（例如，使用`discounted_rewards(rews,
    gamma)`）将成为！[](img/512d6543-23dd-4aac-b864-4ec3642a4a86.png)目标值，并且神经网络将被优化，以最小化均方误差（MSE）损失——就像你在监督学习任务中做的那样：
- en: '![](img/cbe98b06-473b-4ca4-823a-dc4dafbcb72c.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbe98b06-473b-4ca4-823a-dc4dafbcb72c.png)'
- en: Here, ![](img/aeca18ae-fab6-443e-b0a6-d6237d029998.png) is the weights of the
    value function neural network, and each element of the dataset contains the ![](img/dbeecb7f-e955-4daf-9d4b-06ca14894ff8.png)
    state, and the target value ![](img/4ee8929c-cacf-4a63-933f-c63dfee82aeb.png).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/aeca18ae-fab6-443e-b0a6-d6237d029998.png)是价值函数神经网络的权重，每个数据集元素包含！[](img/dbeecb7f-e955-4daf-9d4b-06ca14894ff8.png)状态，以及目标值！[](img/4ee8929c-cacf-4a63-933f-c63dfee82aeb.png)。
- en: Implementing REINFORCE with baseline
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现带基准的REINFORCE
- en: 'The value function that baseline approximated with a neural network can be
    implemented by adding a few lines to our previous code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基准用神经网络逼近的价值函数可以通过在我们之前的代码中添加几行来实现：
- en: 'Add the neural network, the operations for computing the MSE loss function,
    and the optimization procedure to the computational graph:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将神经网络、计算MSE损失函数的操作和优化过程添加到计算图中：
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Run `s_values`, and store the ![](img/0037c50a-75e4-49bb-b6b0-ab2095b92691.png)predictions,
    as later we''ll need to compute ![](img/8df6a5dd-8373-4462-961f-805dc3f8eba3.png).
    This operation can be done in the innermost cycle (the differences from the REINFORCE
    code are shown in bold):'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`s_values`，并存储！[](img/0037c50a-75e4-49bb-b6b0-ab2095b92691.png)预测值，因为稍后我们需要计算！[](img/8df6a5dd-8373-4462-961f-805dc3f8eba3.png)。此操作可以在最内层的循环中完成（与REINFORCE代码的不同之处用粗体显示）：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Retrieve `rtg_batch`, which contains the "target" values from the buffer, and
    optimize the value function:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索`rtg_batch`，它包含来自缓冲区的“目标”值，并优化价值函数：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Compute the reward to go (![](img/f0dd0b06-c9c8-43ee-940e-8e21ac00b4ac.png)),
    and the target values ![](img/de87b8d5-2dfe-4c84-b1f7-af49aba9e641.png). This
    change is done in the `Buffer` class. We have to create a new empty `self.rtg`
    list in the initialization method of the class, and modify the `store` and `get_batch`
    functions, as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算奖励目标（![](img/f0dd0b06-c9c8-43ee-940e-8e21ac00b4ac.png)）和目标值！[](img/de87b8d5-2dfe-4c84-b1f7-af49aba9e641.png)。此更改在`Buffer`类中完成。我们需要在该类的初始化方法中创建一个新的空`self.rtg`列表，并修改`store`和`get_batch`函数，具体如下：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can now test the REINFORCE with baseline algorithm on whatever environment
    you want, and compare the performance with the basic REINFORCE implementation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以在任何你想要的环境中测试带基准的REINFORCE算法，并将其性能与基本的REINFORCE实现进行比较。
- en: Learning the AC algorithm
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习AC算法
- en: Simple REINFORCE has the notable property of being unbiased, but it exhibits
    high variance. Adding a baseline reduces the variance, while keeping it unbiased
    (asymptotically, the algorithm will converge to a local minimum). A major drawback
    of REINFORCE with baseline is that it'll converge very slowly, requiring a consistent
    number of interactions with the environment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的REINFORCE具有不偏性的显著特点，但它表现出较高的方差。添加基准可以减少方差，同时保持不偏（从渐近的角度来看，算法将收敛到局部最小值）。带基准的REINFORCE的一个主要缺点是它收敛得非常慢，需要与环境进行一致的交互。
- en: An approach to speed up training is called bootstrapping. This is a technique
    that we've already seen many times throughout the book. It allows the estimation
    of the return values from the subsequent state values. The policy gradient algorithms
    that use this techniques is called actor-critic (AC). In the AC algorithm, the
    actor is the policy, and the critic is the value function (typically, a state-value
    function) that "critiques" the behavior of the actor, to help him learn faster. The
    advantages of AC methods are multiple, but the most important is their ability
    to learn in non-episodic problems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 加速训练的一种方法叫做引导法（bootstrapping）。这是我们在本书中已经多次看到的技巧。它允许从后续的状态值估算回报值。使用这种技巧的策略梯度算法称为演员-评论者（AC）。在
    AC 算法中，演员是策略，评论者是价值函数（通常是状态值函数），它对演员的行为进行“批评”，帮助他更快学习。AC 方法的优点有很多，但最重要的是它们能够在非阶段性问题中学习。
- en: It's not possible to solve continuous tasks with REINFORCE, as to compute the
    reward to go, they need all the rewards until the end of the trajectory (if the
    trajectories are infinite, there is no end). Relying on the bootstrapping technique,
    AC methods are also able to learn action values from incomplete trajectories.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 无法使用 REINFORCE 解决连续任务，因为要计算奖赏到达目标，它们需要直到轨迹结束的所有奖赏（如果轨迹是无限的，就没有结束）。依靠引导技术，AC
    方法也能够从不完整的轨迹中学习动作值。
- en: Using a critic to help an actor to learn
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用评论者帮助演员学习
- en: 'The action-value function that uses one-step bootstrapping is defined as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一步引导法的动作值函数定义如下：
- en: '![](img/b8a275fb-6b31-4f5d-ac38-f809d016d179.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8a275fb-6b31-4f5d-ac38-f809d016d179.png)'
- en: Here, ![](img/b0dac0f6-61a1-4938-a6dd-6c3420dcce52.png) is the notorious next state.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/b0dac0f6-61a1-4938-a6dd-6c3420dcce52.png) 是臭名昭著的下一个状态。
- en: 'Thus, with an ![](img/3e0578d1-d7b6-4d9a-9346-71df6842bea9.png) actor, and
    a ![](img/bd9cfb2d-0a7d-4f54-9b50-e2bb986c5bf0.png) critic using bootstrapping,
    we obtain a one-step AC step:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用一个 ![](img/3e0578d1-d7b6-4d9a-9346-71df6842bea9.png) 角色和一个 ![](img/bd9cfb2d-0a7d-4f54-9b50-e2bb986c5bf0.png)
    评论者使用引导法（bootstrapping），我们可以得到一步的 AC 步骤：
- en: '![](img/0b0dacf6-6010-4b45-b86c-b744ef400c29.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b0dacf6-6010-4b45-b86c-b744ef400c29.png)'
- en: 'This will replace the REINFORCE step with a baseline:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这将用一个基准替代 REINFORCE 步骤：
- en: '![](img/8de0363f-6c5d-4c22-bd2d-01f4a07d7738.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8de0363f-6c5d-4c22-bd2d-01f4a07d7738.png)'
- en: Note the difference between the use of the state-value function in REINFORCE
    and AC. In the former, it is used only as a baseline, to provide the state value
    of the current state. In the latter example, the state-value function is used
    to estimate the value of the next state, so as to only require the current reward
    to estimate ![](img/0745c16f-3926-466d-9316-31a1f3bdbbaf.png). Thus, we can say
    that the one-step AC model is a fully online, incremental algorithm.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 REINFORCE 和 AC 中使用状态值函数的区别。在前者中，它仅作为基准，用来提供当前状态的状态值。在后者示例中，状态值函数用于估算下一个状态的价值，从而只需要当前的奖励来估算
    ![](img/0745c16f-3926-466d-9316-31a1f3bdbbaf.png)。因此，我们可以说，一步 AC 模型是一个完全在线的增量算法。
- en: The n-step AC model
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: n 步 AC 模型
- en: In reality, as we already saw in TD learning, a fully online algorithm has low
    variance but high bias, the opposite of MC learning. However, usually, a middle-ground strategy,
    between fully online and MC methods, is preferred. To balance this trade-off,
    an n-step return can replace a one-step return of online algorithms.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，正如我们在 TD 学习中所看到的，完全在线的算法具有低方差但高偏差，这与 MC 学习相反。然而，通常，介于完全在线和 MC 方法之间的中间策略是首选。为了平衡这种权衡，n
    步回报可以替代在线算法中的一步回报。
- en: If you remember, we already implemented n-step learning in the DQN algorithm.
    The only difference is that DQN is an off-policy algorithm, and in theory, n-step
    can be employed only on on-policy algorithms. Nevertheless, we showed that with
    a small ![](img/298ede41-f52b-47b1-b30d-bce9761e87bb.png), the performance increased.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们已经在 DQN 算法中实现了 n 步学习。唯一的区别是 DQN 是一个脱离策略的算法，而理论上，n 步学习只能在在线策略算法中使用。然而，我们展示了通过一个小的
    ![](img/298ede41-f52b-47b1-b30d-bce9761e87bb.png)，性能有所提高。
- en: 'AC algorithms are on-policy, therefore, as far as the performance increase
    goes, it''s possible to use arbitrary large ![](img/150066f6-e6c8-4530-8594-857271431f41.png) values.
    The integration of n-step in AC is pretty straightforward; the one-step return
    is replaced by ![](img/6d88c26c-8b10-4908-8a5c-137d77e8c487.png), and the value
    function is taken in the ![](img/93cfe315-7e80-4559-84f7-64247c471eba.png) state:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: AC算法是基于策略的，因此，就性能提升而言，可以使用任意大的 ![](img/150066f6-e6c8-4530-8594-857271431f41.png) 值。在AC中集成n步是相当直接的；一步返回被 ![](img/6d88c26c-8b10-4908-8a5c-137d77e8c487.png) 替代，值函数被带入 ![](img/93cfe315-7e80-4559-84f7-64247c471eba.png) 状态：
- en: '![](img/f5a7b562-3d72-4b5b-a82b-1b5c491644bd.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5a7b562-3d72-4b5b-a82b-1b5c491644bd.png)'
- en: Here, ![](img/b03016a1-1fb3-45ec-93e4-1cbfaa795bd2.png). Pay attention here
    to how, if ![](img/ded9562c-a399-4da7-b4f9-f63eb41c13d9.png) is a final state, ![](img/bdabdf88-e089-4606-a3ed-eae8804cb52e.png).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， ![](img/b03016a1-1fb3-45ec-93e4-1cbfaa795bd2.png) 。请注意，如果 ![](img/ded9562c-a399-4da7-b4f9-f63eb41c13d9.png) 是一个最终状态， ![](img/bdabdf88-e089-4606-a3ed-eae8804cb52e.png)。
- en: Besides reducing the bias, the n-step return propagates the subsequent returns
    faster, making the learning much more efficient.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少偏差外，n步返回还可以更快地传播后续的回报，从而使得学习更加高效。
- en: 'Interestingly, the ![](img/ac9730d0-848f-492e-91b9-323c0ab42870.png)quantity
    can be seen as an estimate of the advantage function. In fact, the advantage function
    is defined as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是， ![](img/ac9730d0-848f-492e-91b9-323c0ab42870.png) 量可以看作是优势函数的估计。事实上，优势函数定义如下：
- en: '![](img/ef6264dd-3aa0-4949-a184-b5d2c7bd4e69.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef6264dd-3aa0-4949-a184-b5d2c7bd4e69.png)'
- en: Due to the fact that ![](img/68902dba-0710-4680-97f7-ddb38b1e7c5c.png) is an
    estimate of ![](img/8d5a0ace-b449-4f15-8396-d61102b8254c.png), we obtain an estimate
    of the advantage function. Usually, this function is easier to learn, as it only
    denotes the preference of one particular action over the others in a particular
    state. It doesn't have to learn the value of that state.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ![](img/68902dba-0710-4680-97f7-ddb38b1e7c5c.png) 是 ![](img/8d5a0ace-b449-4f15-8396-d61102b8254c.png) 的估计，我们得到优势函数的估计。通常，这个函数更容易学习，因为它仅表示在特定状态下一个特定动作相对于其他动作的偏好。它不需要学习该状态的值。
- en: 'Regarding the optimization of the weights of the critic, it is optimized using
    one of the well-known SGD optimization methods, minimizing the MSE loss:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评论员权重的优化，它使用一种著名的SGD优化方法来进行优化，最小化MSE损失：
- en: '![](img/4d75e122-9f78-42ae-bf15-f5b497f411f0.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d75e122-9f78-42ae-bf15-f5b497f411f0.png)'
- en: In the previous equation, the target values are computed as follows: ![](img/1c87dffc-1161-4c82-9c9b-053e1098d5a2.png).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，目标值是按如下方式计算的： ![](img/1c87dffc-1161-4c82-9c9b-053e1098d5a2.png)。
- en: The AC implementation
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AC实现
- en: 'Overall, as we have seen so far, the AC algorithm is very similar to the REINFORCE
    algorithm, with the state function as a baseline. But, to provide a recap, the
    algorithm is summarized in the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，正如我们到目前为止所看到的，AC算法与REINFORCE算法非常相似，状态函数作为基准。但为了回顾一下，算法总结如下：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The only differences with REINFORCE are the calculation of the n-step reward
    to go, the advantage function calculation, and a few adjustments of the main function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与REINFORCE的唯一区别是n步奖励的计算、优势函数的计算，以及主函数的一些调整。
- en: 'Let''s first look at the new implementation of the discounted reward. Differently
    to before, the estimated value of the last `last_sv` state is now passed in the
    input and is used to bootstrap, as given in the following implementation:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下折扣奖励的新实现。与之前不同，最后一个`last_sv`状态的估计值现在传递给输入，并用于引导，如以下实现所示：
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The computational graph doesn't change, but in the main cycle, we have to take
    care of a few small, but very important, changes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图不会改变，但在主循环中，我们需要注意一些小的但非常重要的变化。
- en: Obviously, the name of the function is changed to `AC`, and the learning rate
    of the `cr_lr` critic is added as an argument.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，函数的名称已更改为`AC`，并且`cr_lr`评论员的学习率作为一个参数被添加进来。
- en: The first actual change involves the way in which the environment is reset.
    If, in REINFORCE, it was preferred to reset the environment on every iteration
    of the main cycle, in AC, we have to resume the environment from where we left
    off in the previous iteration, resetting it only when it reaches its final state.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实际的变化涉及环境重置的方式。如果在REINFORCE中，偏好在每次主循环迭代时重置环境，那么在AC中，我们必须从上一轮迭代的环境状态继续，只有在环境达到最终状态时才重置它。
- en: 'The second change involves the way in which the action-value function is bootstrapped,
    and how the reward to go is calculated. Remember that ![](img/6cb365a1-c652-4002-8935-7a268cd532e6.png) for
    every state-action pair, except in the case of when ![](img/3657e493-86d3-42ff-9550-ed4ec30b965f.png)
    is a final state. In this case, ![](img/f60da65c-1493-4b3b-917d-714c659448a8.png). Thus,
    we have to bootstrap with a value of `0`, whenever we are in the last state, and
    bootstrap with ![](img/057c60da-c9c2-4750-993b-8af4fd807af8.png) in all the other
    cases. With these changes, the code is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个变化涉及行动价值函数的引导方式，以及如何计算未来的回报。记住，对于每个状态-动作对，除非 ![](img/3657e493-86d3-42ff-9550-ed4ec30b965f.png)
    是最终状态，否则 ![](img/6cb365a1-c652-4002-8935-7a268cd532e6.png)。在这种情况下，![](img/f60da65c-1493-4b3b-917d-714c659448a8.png)。因此，我们必须在最后状态时使用`0`进行引导，并在其他情况下使用
    ![](img/057c60da-c9c2-4750-993b-8af4fd807af8.png) 进行引导。根据这些更改，代码如下：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The third change is in the `store` method of the `Buffer` class. In fact, now,
    we also have to deal with incomplete trajectories. In the previous snippet, we
    saw that the estimated ![](img/057c60da-c9c2-4750-993b-8af4fd807af8.png) state
    values are passed as the third argument to the `store` function. Indeed, we use
    them to bootstrap and to compute the reward to go. In the new version of `store`,
    we call the variable that is associated with the state values, `last_sv`, and
    pass it as the input to the `discounted_reward` function, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个变化发生在`Buffer`类的`store`方法中。实际上，现在我们还需要处理不完整的轨迹。在之前的代码片段中，我们看到估计的 ![](img/057c60da-c9c2-4750-993b-8af4fd807af8.png)
    状态值作为第三个参数传递给`store`函数。事实上，我们使用这些状态值进行引导，并计算"未来回报"。在新版本的`store`中，我们将与状态值相关的变量命名为`last_sv`，并将其作为输入传递给`discounted_reward`函数，代码如下：
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Landing a spacecraft using AC
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AC着陆航天器
- en: We applied AC to LunarLander-v2, the same environment used for testing REINFORCE.
    It is an episodic game, and as such, it doesn't fully emphasize the main qualities
    of the AC algorithm. Nonetheless, it provides a good testbed, and you can freely
    test it in another environment.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将AC应用于LunarLander-v2，这与测试REINFORCE时使用的环境相同。这是一个回合制的游戏，因此它并没有完全强调AC算法的主要特性。尽管如此，它仍提供了一个很好的测试平台，你也可以自由地在其他环境中进行测试。
- en: 'We call the `AC` function with the following hyperparameters:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`AC`函数时使用以下超参数：
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The resulting plot that shows the total reward accumulated in the training
    epochs is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示了训练周期中累计的总回报，图如下：
- en: '![](img/898b67e6-f59f-42db-8618-b8a038f456a8.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/898b67e6-f59f-42db-8618-b8a038f456a8.png)'
- en: 'You can see that AC is faster than REINFORCE, as shown in the following plot.
    However, it is less stable, and after about 200,000 steps, the performance declines
    a little bit, fortunately continuing to increment afterward:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，AC比REINFORCE更快，如下图所示。然而，AC的稳定性较差，经过大约200,000步后，性能有所下降，但幸运的是，之后它继续增长：
- en: '![](img/e162f2ae-1142-428e-8fd9-fe438e87125e.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e162f2ae-1142-428e-8fd9-fe438e87125e.png)'
- en: In this configuration, the AC algorithm updates the actor and critic every 100
    steps. In theory, you could use a smaller `steps_per_epochs` but, usually, it
    makes the training more unstable. Using a longer epoch can stabilize the training,
    but the actor learns more slowly. It's all about finding a good trade-off and
    good learning rates.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配置中，AC算法每100步更新一次演员和评论员。从理论上讲，你可以使用更小的`steps_per_epochs`，但通常这会让训练变得更不稳定。使用更长的周期可以稳定训练，但演员学习速度较慢。一切都在于找到一个好的平衡点和适合的学习率。
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章提到的所有颜色参考，请参见以下链接中的彩色图像包：[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)。
- en: Advanced AC, and tips and tricks
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级AC，以及技巧和窍门
- en: 'There are several further advancements of AC algorithms, and there are many
    tips and tricks to keep in mind, while designing such algorithms:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: AC算法有许多进一步的进展，还有许多技巧和窍门需要记住，在设计此类算法时要加以注意：
- en: '**Architectural design**: In our implementation, we implemented two distinct
    neural networks, one for the critic, and one for the actor. It''s also possible
    to design a neural network that shares the main hidden layers, while keeping the
    heads distinct. This architecture can be more difficult to tune, but overall,
    it increases the efficiency of the algorithms.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构设计**：在我们的实现中，我们实现了两个不同的神经网络，一个用于评估者，另一个用于演员。也可以设计一个共享主要隐藏层的神经网络，同时保持头部的独立性。这种架构可能更难调整，但总体而言，它提高了算法的效率。'
- en: '**Parallel environments**: A widely adopted technique to decrease the variance
    is to collect experience from multiple environments in parallel. The **A3C** (**Asynchronous
    Advantage Actor-Critic**) algorithm updates the global parameters asynchronously.
    Instead, the synchronous version of it, called **A2C** (**Advantage Actor-Critic**)
    waits for all of the parallel actors to finish before updating the global parameters.
    The agent parallelization ensures more independent experience from different parts
    of the environment.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行环境**：减少方差的一个广泛采用的技术是从多个环境中并行收集经验。**A3C**（**异步优势演员-评估者**）算法异步更新全局参数。而它的同步版本，称为
    **A2C**（**优势演员-评估者**），则在更新全局参数之前等待所有并行的演员完成。智能体的并行化确保了来自环境不同部分的更多独立经验。'
- en: '**Batch size**: With respect to other RL algorithms (especially off-policy
    algorithms), policy gradient and AC methods need large batches. Thus, if after
    tuning the other hyperparameters, the algorithm doesn''t stabilize, consider using
    a larger batch size.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：与其他强化学习算法（尤其是脱离策略算法）相比，政策梯度和 AC 方法需要较大的批量。因此，如果在调整其他超参数后，算法仍然无法稳定，考虑使用更大的批量大小。'
- en: '**Learning rate**: Tuning the learning rate in itself is very tricky, so make
    sure that you use a more advanced SGD optimization method, such as Adam or RMSprop.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：调整学习率本身非常棘手，因此请确保使用更先进的 SGD 优化方法，如 Adam 或 RMSprop。'
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about a new class of reinforcement learning algorithms
    called policy gradients. They approach the RL problem in a different way, compared
    to the value function methods that were studied in the previous chapters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一类新的强化学习算法，称为政策梯度。与之前章节中研究的价值函数方法相比，这些算法以不同的方式解决强化学习问题。
- en: The simpler version of PG methods is called REINFORCE, which was learned, implemented,
    and tested throughout the course of this chapter. We then proposed adding a baseline
    in REINFORCE in order to decrease the variance and increase the convergence property
    of the algorithm. AC algorithms are free from the need for a full trajectory using
    a critic, and thus, we then solved the same problem using the AC model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: PG 方法的简化版本叫做 REINFORCE，这一方法在本章过程中进行了学习、实现和测试。随后，我们提出在 REINFORCE 中加入基准值，以减少方差并提高算法的收敛性。AC
    算法不需要使用评估者的完整轨迹，因此我们用 AC 模型解决了同样的问题。
- en: With a solid foundation of the classic policy gradient algorithms, we can now
    go further. In the next chapter, we'll look at some more complex, state-of-the-art
    policy gradient algorithms; namely, **Trust Region Policy Optimization** (**TRPO**)
    and **Proximal Policy Optimization** (**PPO**). These two algorithms are built
    on top of the material that we have covered in this chapter, but additionally,
    they propose a new objective function that improves the stability and efficiency
    of PG algorithms.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握经典的政策梯度算法的基础上，我们可以进一步深入。在下一章，我们将介绍一些更复杂、前沿的政策梯度算法；即，**信任区域策略优化**（**TRPO**）和**近端策略优化**（**PPO**）。这两种算法是基于我们在本章中学习的内容构建的，但它们提出了一个新的目标函数，旨在提高
    PG 算法的稳定性和效率。
- en: Questions
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How do PG algorithms maximize the objective function?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PG 算法如何最大化目标函数？
- en: What's the main idea behind policy gradient algorithms?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 政策梯度算法的核心思想是什么？
- en: Why does the algorithm remain unbiased when introducing a baseline in REINFORCE?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在 REINFORCE 中引入基准值后，算法仍保持无偏？
- en: What broader class of algorithms does REINFORCE belong to?
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: REINFORCE 属于更广泛的哪类算法？
- en: How does the critic in AC methods differ from a value function that is used
    as a baseline in REINFORCE?
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AC 方法中的评估者与 REINFORCE 中作为基准的价值函数有什么不同？
- en: If you had to develop an algorithm for an agent that has to learn to move, would
    you prefer REINFORCE or AC?
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你需要为一个必须学习移动的智能体开发算法，你会选择 REINFORCE 还是 AC？
- en: Could you use an n-step AC algorithm as a REINFORCE algorithm?
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能将n步AC算法用作REINFORCE算法吗？
- en: Further reading
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To learn about an asynchronous version of the actor-critic algorithm, read [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解异步版本的演员-评论员算法，请阅读 [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)。
