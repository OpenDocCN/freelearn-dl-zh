- en: The Nuts and Bolts of Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的基本构成
- en: In this chapter, we'll discuss some of the intricacies of neural networks (**NNs**)—the
    cornerstone of **deep learning** (**DL**). We'll talk about their mathematical
    apparatus, structure, and training. Our main goal is to provide you with a systematic understanding
    of NNs. Often, we approach them from a computer science perspective—as a machine
    learning (**ML**) algorithm (or even a special entity) composed of a number of
    different steps/components. We gain our intuition by thinking in terms of neurons,
    layers, and so on (at least I did this when I first learned about this field).
    This is a perfectly valid way to do things and we can still do impressive things
    at this level of understanding. Perhaps this is not the correct approach, though.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论神经网络（**NNs**）的一些复杂细节——它是**深度学习**（**DL**）的基石。我们将讨论它们的数学工具、结构和训练。我们的主要目标是为你提供系统性的神经网络理解。通常，我们从计算机科学的角度来接近它们——作为一种机器学习（**ML**）算法（或甚至一个特殊的实体），由多个不同的步骤/组件组成。我们通过思考神经元、层等来获得直觉（至少当我第一次学习这个领域时，我是这样做的）。这种方式是完全有效的，我们仍然能在这个理解层次上做出令人印象深刻的事情。然而，这可能不是正确的途径。
- en: NNs have solid mathematical foundations and if we approach them from this point
    of view, we'll be able to define and understand them in a more fundamental and
    elegant way. Therefore, in this chapter, we'll try to underscore the analogy between
    NNs from mathematical and computer science points of view. If you are already
    familiar with these topics, you can skip this chapter. Still, I hope that you'll
    find some interesting bits you didn't know about already (we'll do our best to
    keep this chapter interesting!).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）有着坚实的数学基础，如果我们从这一角度来接近它们，就能以更基本和优雅的方式定义和理解它们。因此，在这一章中，我们将试图强调从数学和计算机科学角度看神经网络的类比。如果你已经熟悉这些话题，你可以跳过这一章。不过，我希望你能发现一些你之前没有了解的有趣内容（我们会尽力让这一章保持有趣！）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论以下内容：
- en: The mathematical apparatus of NNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的数学工具
- en: A short introduction to NNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Training NNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: The mathematical apparatus of NNs
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的数学工具
- en: In the next few sections, we'll discuss the mathematical branches related to
    NNs. Once we've done this, we'll connect them to NNs themselves.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论与神经网络相关的数学分支。完成这些内容后，我们将把它们与神经网络本身联系起来。
- en: Linear algebra
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数
- en: Linear algebra deals with linear equations such as [![](img/6f58b451-029b-4567-901b-7d8cee5287f8.png) ]and
    linear transformations (or linear functions) and their representations, such as
    matrices and vectors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数处理线性方程，如[![](img/6f58b451-029b-4567-901b-7d8cee5287f8.png)]，以及线性变换（或线性函数）及其表示形式，如矩阵和向量。
- en: 'Linear algebra identifies the following mathematical objects:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数识别以下数学对象：
- en: '**Scalars**: A single number.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**：一个单独的数字。'
- en: '**Vectors**: A one-dimensional array of numbers (or components). Each component
    of the array has an index. In literature, we will see vectors denoted either with
    a superscript arrow ([![](img/12778eab-f899-4ef1-86a4-963419135aae.png)]) or in
    bold (**x**). The following is an example of a vector:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：一个一维的数字（或分量）数组。数组的每个分量都有一个索引。在文献中，我们会看到向量要么用上标箭头表示（[![](img/12778eab-f899-4ef1-86a4-963419135aae.png)]），要么用粗体（**x**）表示。以下是一个向量的示例：'
- en: '![](img/6081dcdb-46c7-42cc-96ac-9ca33280b3ad.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6081dcdb-46c7-42cc-96ac-9ca33280b3ad.png)'
- en: Throughout this book, we'll mostly use the bold (**x**) graph notations. But
    in some instances, we'll use formulas from different sources and we'll try to
    retain their original notation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们大部分时间将使用粗体（**x**）图形符号。但在某些情况下，我们会使用来自不同来源的公式，并尽力保留它们的原始符号。
- en: 'We can visually represent an *n*-dimensional vector as the coordinates of a
    point in an *n*-dimensional Euclidean space, [![](img/d51dca7b-23f1-4beb-8870-7c33c790cae0.png)] (equivalent
    to a coordinate system). In this case, the vector is referred to as Euclidean
    and each vector component represents the coordinate along the corresponding axis,
    as shown in the following diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个*n*维向量来直观地表示为一个点在*n*维欧几里得空间中的坐标，[![](img/d51dca7b-23f1-4beb-8870-7c33c790cae0.png)]（相当于一个坐标系）。在这种情况下，向量被称为欧几里得向量，每个向量分量表示沿着相应轴的坐标，如下图所示：
- en: '![](img/6b7d4b80-53be-48f8-94b6-734fc3067210.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b7d4b80-53be-48f8-94b6-734fc3067210.png)'
- en: Vector representation in [![](img/0d21b02f-f4ac-400c-9389-ec8cf53ab570.png)] space
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/0d21b02f-f4ac-400c-9389-ec8cf53ab570.png)] 空间中的向量表示'
- en: 'However, the Euclidean vector is more than just a point and we can also represent
    it with the following two properties:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，欧几里得向量不仅仅是一个点，我们也可以通过以下两个特性来表示它：
- en: '**Magnitude** (or **length**) is a generalization of the Pythagorean theorem
    for an *n*-dimensional space:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大小**（或**长度**）是毕达哥拉斯定理在*n*维空间中的推广：'
- en: '![](img/7858b8a3-ac6c-4d81-84f7-bea53f11c884.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7858b8a3-ac6c-4d81-84f7-bea53f11c884.png)'
- en: '**Direction** is the angle of the vector along each axis of the vector space.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方向**是向量在向量空间中沿每个坐标轴的角度。'
- en: '**Matrices**: This is a two-dimensional array of numbers. Each element is identified
    by two indices (row and column). A matrix is usually denoted with a bold capital
    letter; for example, **A**. Each matrix element is denoted with the small matrix
    letter and a subscript index; for example, *a[ij]*. Let''s look at an example
    of the matrix notation in the following formula:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：这是一个二维的数字数组。每个元素由两个索引（行和列）标识。矩阵通常用加粗的大写字母表示，例如，**A**。每个矩阵元素用小写矩阵字母和下标表示，例如，*a[ij]*。让我们看一下以下公式中的矩阵表示法示例：'
- en: '![](img/95cc7e76-7279-4ffd-840e-7c01ea75404b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95cc7e76-7279-4ffd-840e-7c01ea75404b.png)'
- en: We can represent a vector as a single-column *n×1* matrix (referred to as a
    column matrix) or a single -ow *1×n* matrix (referred to as a row matrix).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个向量表示为一个单列的*n×1*矩阵（称为列矩阵）或一个单行的*1×n*矩阵（称为行矩阵）。
- en: '**Tensors**: Before we explain them, we have to start with a disclaimer. Tensors
    originally come from mathematics and physics, where they have existed long before
    we started using them in ML. The tensor definition in these fields differs from
    the ML one. For the purposes of this book, we''ll only consider tensors in the
    ML context. Here, a tensor is a multi-dimensional array with the following properties:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量**：在我们解释张量之前，我们必须先做个声明。张量最初来源于数学和物理学，在我们开始将其应用于机器学习之前，它们早已存在于这些领域。在这些领域中的张量定义与机器学习中的定义不同。为了本书的目的，我们只考虑机器学习中的张量。在这里，张量是一个多维数组，具有以下特性：'
- en: '**Rank**: Indicates the number of array dimensions. For example, a tensor of
    rank 2 is a matrix, a tensor of rank 1 is a vector, and a tensor of rank 0 is
    a scalar. However, the tensor has no limit on the number of dimensions. Indeed,
    some types of NNs use tensors of rank 4.'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**：表示数组的维数。例如，秩为2的张量是一个矩阵，秩为1的张量是一个向量，秩为0的张量是一个标量。然而，张量在维度数上没有限制。实际上，一些类型的神经网络使用秩为4的张量。'
- en: '**Shape**: The size of each dimension.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状**：每个维度的大小。'
- en: '**The data type** of the tensor elements. These can vary between libraries,
    but typically include 16-, 32-, and 64-bit float and 8-, 16-, 32-, and 64-bit
    integers.'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量元素的数据类型**。这些数据类型在不同的库中可能有所不同，但通常包括16位、32位和64位浮动数，以及8位、16位、32位和64位整数。'
- en: Contemporary DL libraries such as TensorFlow and PyTorch use tensors as their
    main data structure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当代深度学习库，如 TensorFlow 和 PyTorch，使用张量作为它们的主要数据结构。
- en: You can find a thorough discussion on the nature of tensors here: [https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors](https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors).
    You can also check the TensorFlow ([https://www.tensorflow.org/guide/tensors](https://www.tensorflow.org/guide/tensors))
    and PyTorch ([https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html))
    tensor definitions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到关于张量性质的详细讨论：[https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors](https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors)。你还可以查看
    TensorFlow ([https://www.tensorflow.org/guide/tensors](https://www.tensorflow.org/guide/tensors))
    和 PyTorch ([https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html))
    的张量定义。
- en: Now that we've introduced the types of objects in linear algebra, in the next
    section, we'll discuss some operations that can be applied to them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了线性代数中的对象类型，在接下来的部分，我们将讨论可以应用于它们的一些运算。
- en: Vector and matrix operations
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量和矩阵运算
- en: 'In this section, we''ll discuss the vector and matrix operations that are relevant
    to NNs. Let''s start:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论与神经网络（NNs）相关的向量和矩阵运算。让我们开始吧：
- en: '**Vector addition** is the operation of adding two or more vectors together
    into an output vector sum. The output is another vector and is computed with the
    following formula:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量加法**是将两个或多个向量相加得到一个输出向量和的运算。输出是另一个向量，其计算公式如下：'
- en: '![](img/0f5341c5-fb2c-45f4-b0e0-75306d43a081.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f5341c5-fb2c-45f4-b0e0-75306d43a081.png)'
- en: 'The **dot** (**or scalar**) **product** takes two vectors and outputs a scalar
    value. We can compute the dot product with the following formula:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点**（**或标量**）**积**对两个向量进行运算，输出一个标量值。我们可以通过以下公式计算点积：'
- en: '![](img/dcfaa2c5-6467-44b6-87b0-69669e7d4f4e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcfaa2c5-6467-44b6-87b0-69669e7d4f4e.png)'
- en: 'Here, |*a*| and |*b*| are the vector magnitudes and θ is the angle between
    the two vectors. Let''s assume that the two vectors are *n*-dimensional and that
    their components are *a[1]*, *b[1]*, *a[2]*, *b[2]*, and so on. Here, the preceding
    formula is equivalent to the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，|*a*|和|*b*|是向量的大小，θ是两个向量之间的角度。假设这两个向量是*n*维的，它们的分量分别是*a[1]*、*b[1]*、*a[2]*、*b[2]*，依此类推。此时，前面的公式等价于以下公式：
- en: '![](img/46537fc8-349f-4046-9417-0543b0524012.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46537fc8-349f-4046-9417-0543b0524012.png)'
- en: 'The dot product of two two-dimensional vectors, **a** and **b**, is illustrated
    in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 两个二维向量**a**和**b**的点积在以下图中示例：
- en: '![](img/0922a3d2-af6c-4c3b-bcbe-df8ca2fbbdd4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0922a3d2-af6c-4c3b-bcbe-df8ca2fbbdd4.png)'
- en: 'The dot product of vectors. Top: vector components; Bottom: dot product of
    the two vectors'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的点积。上：向量分量；下：两个向量的点积
- en: The dot product acts as a kind of similarity measure between the two vectors—if
    the angle θ between the two vectors is small (the vectors have similar directions),
    then their dot product will be higher because of [![](img/2020afd0-3989-43cc-a5d2-d7bdcfb9f1db.png)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 点积作为两个向量之间的一种相似性度量——如果两个向量之间的角度θ较小（即向量方向相似），那么它们的点积会较高，因为[![](img/2020afd0-3989-43cc-a5d2-d7bdcfb9f1db.png)]。
- en: 'Following this idea, we can define a **cosine similarity** between two vectors
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一思想，我们可以定义两个向量之间的**余弦相似度**如下：
- en: '![](img/f82ad3d3-1446-4ef6-af18-997490b7eca9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f82ad3d3-1446-4ef6-af18-997490b7eca9.png)'
- en: 'The **cross** (**or vector**)** product** takes two vectors and outputs another
    vector, which is perpendicular to both initial vectors. We can compute the magnitude
    of the cross product output vector with the following formula:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叉**（**或向量**）**积**对两个向量进行运算，输出一个垂直于这两个初始向量的向量。我们可以通过以下公式计算叉积输出向量的大小：'
- en: '![](img/22fef02f-a59c-4443-ba72-a4ee23e950d3.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22fef02f-a59c-4443-ba72-a4ee23e950d3.png)'
- en: 'The following diagram shows an example of a cross product between two two-dimensional
    vectors:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了两个二维向量之间的叉积示例：
- en: '![](img/78e303b7-c5ed-4a04-b205-36d8a4dbb1cf.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78e303b7-c5ed-4a04-b205-36d8a4dbb1cf.png)'
- en: Cross product of two two-dimensional vectors
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 两个二维向量的叉积
- en: As we mentioned previously, the output vector is perpendicular to the input
    vectors, which also means that the vector is normal to the plane containing them.
    The magnitude of the output vector is equal to the area of the parallelogram with
    the vectors **a** and **b** for sides (denoted in the preceding diagram).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，输出向量垂直于输入向量，这也意味着该向量是包含它们的平面的法向量。输出向量的大小等于以向量**a**和**b**为边的平行四边形的面积（如前图所示）。
- en: 'We can also define a vector through **vector space**, which is a collection
    of objects (in our case, vectors) that can be added together and multiplied by
    a scalar value. The vector space will allow us to define a **linear transformation**
    as a function, *f*, which can transform each vector (point) of vector space, ***V***,
    into a vector (point) of another vector space, ***W*** : [![](img/5c14d1cd-c51d-4987-9a10-9b12d667bf94.png)].
    *f* has to satisfy the following requirements for any two vectors, [![](img/e192b6b7-1389-41ae-a87e-3b72678b2572.png)]:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过**向量空间**来定义一个向量，向量空间是由可以相加和与标量相乘的对象（在我们这里是向量）组成的集合。向量空间将允许我们定义一个**线性变换**，作为一个函数，*f*，该函数可以将向量空间***V***中的每个向量（点）转换为另一个向量空间***W***中的向量（点）：[![](img/5c14d1cd-c51d-4987-9a10-9b12d667bf94.png)]。*f*必须满足以下要求，对于任何两个向量，[![](img/e192b6b7-1389-41ae-a87e-3b72678b2572.png)]：
- en: Additivity: [![](img/7244c5ff-977f-4d71-aa10-073bfcb5bdef.png)]
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加法性：[![](img/7244c5ff-977f-4d71-aa10-073bfcb5bdef.png)]
- en: Homogeneity: [![](img/64244355-8911-4913-9b28-54a739f130b8.png)], where *c* is
    a scalar
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 齐次性：[![](img/64244355-8911-4913-9b28-54a739f130b8.png)]，其中*c*是标量
- en: '**Matrix transpose**: Here, we flip the matrix along its main diagonal (the
    main diagonal is the collection of matrix elements, *a[ij]*, where *i = j*). The
    transpose operation is denoted with superscript, ^(![](img/197f61f9-0945-4ea1-bb90-ce72f751af65.png)).
    To clarify, the cell [![](img/7c130bda-b7e7-4a0d-a965-0cc338c6cb57.png)] of ![](img/57dd28ea-d409-49c2-8032-9fad6261d4c7.png) is
    equal to the cell [![](img/d40013ee-5d21-4cee-83f0-781b154ed773.png)] of [![](img/6c02e597-82aa-4af4-8811-f17e6164c93a.png)]:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵转置**：这里，我们沿着矩阵的主对角线翻转矩阵（主对角线是矩阵元素 *a[ij]* 的集合，其中 *i = j*）。转置操作用上标表示，^(![](img/197f61f9-0945-4ea1-bb90-ce72f751af65.png))。为明确起见，![](img/7c130bda-b7e7-4a0d-a965-0cc338c6cb57.png)
    处的单元格等于 [![](img/d40013ee-5d21-4cee-83f0-781b154ed773.png)] 处的单元格，后者位于 [![](img/6c02e597-82aa-4af4-8811-f17e6164c93a.png)]
    中：'
- en: '![](img/47f10040-ad04-4cf0-bd8e-eb26768f9d0b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47f10040-ad04-4cf0-bd8e-eb26768f9d0b.png)'
- en: 'The transpose of an *m×n* matrix is an *n×m* matrix. The following are a few
    transpose examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *m×n* 矩阵的转置是一个 *n×m* 矩阵。以下是几个转置示例：
- en: '![](img/f70469ff-c9d5-4f61-b017-8bebbc0e8abc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f70469ff-c9d5-4f61-b017-8bebbc0e8abc.png)'
- en: '**Matrix-scalar multiplication** is the multiplication of a matrix by a scalar
    value. In the following example, [![](img/4931bab7-9012-44c1-a6a0-5708f01cc23a.png)] is
    a scalar:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵-标量乘法**是矩阵与标量值的乘法。在以下示例中，[![](img/4931bab7-9012-44c1-a6a0-5708f01cc23a.png)]
    是一个标量：'
- en: '![](img/ece33a00-58a4-462e-89f6-5249e8f3e5f9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ece33a00-58a4-462e-89f6-5249e8f3e5f9.png)'
- en: '**Matrix-matrix addition** is the element-wise addition of one matrix with
    another. For this operation, both matrices must have the same size. The following
    is an example:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵-矩阵加法**是两个矩阵按元素逐一相加的操作。为了使此操作有效，两个矩阵的大小必须相同。以下是一个示例：'
- en: '![](img/8790d323-7f9e-4c7b-bf79-bedba3f4a110.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8790d323-7f9e-4c7b-bf79-bedba3f4a110.png)'
- en: '**Matrix-vector multiplication** is the multiplication of a matrix by a vector.
    For this operation to be valid, the number of matrix columns must be equal to
    the vector length. The result of multiplying the *m×n* matrix and an *n*-dimensional
    vector is an *m*-dimensional vector. The following is an example:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵-向量乘法**是矩阵与向量的乘法。为了使这个操作有效，矩阵的列数必须等于向量的长度。*m×n* 矩阵与 *n* 维向量相乘的结果是一个 *m*
    维向量。以下是一个示例：'
- en: '![](img/335d2d08-a422-4d14-84fc-2ccb407decf5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/335d2d08-a422-4d14-84fc-2ccb407decf5.png)'
- en: 'We can think of each row of the matrix as a separate *n*-dimensional vector. Here,
    each element of the output vector is the dot product between the corresponding
    matrix row and **x**. The following is a numerical example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将矩阵的每一行看作一个独立的 *n* 维向量。在这里，输出向量的每个元素是对应矩阵行与 **x** 的点积。以下是一个数值示例：
- en: '![](img/a390abf7-9e62-4fb5-a511-ca25849b9306.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a390abf7-9e62-4fb5-a511-ca25849b9306.png)'
- en: '**Matrix multiplication** is the multiplication of one matrix with another.
    To be valid, the number of columns of the first matrix has to be equal to the
    number of rows of the second (this is a non-commutative operation). We can think
    of this operation as multiple matrix-vector multiplications, where each column
    of the second matrix is one vector. The result of an *m×n* matrix multiplied by
    an *n×p* matrix is an *m×p* matrix. The following is an example:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵乘法**是一个矩阵与另一个矩阵的乘法。为了使操作有效，第一个矩阵的列数必须等于第二个矩阵的行数（这是一个非交换操作）。我们可以将此操作视为多个矩阵-向量乘法，其中第二个矩阵的每一列都是一个向量。一个
    *m×n* 矩阵与一个 *n×p* 矩阵相乘的结果是一个 *m×p* 矩阵。以下是一个示例：'
- en: '![](img/59628c27-e81f-4945-b00a-b3555d20046e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59628c27-e81f-4945-b00a-b3555d20046e.png)'
- en: If we consider two vectors as row matrices, we can represent a vector dot product
    as matrix multiplication, that is, ![](img/b94c318e-0c3f-4c68-be0a-bec4d81fc96b.png).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将两个向量视为行矩阵，我们可以将向量的点积表示为矩阵乘法，即！[](img/b94c318e-0c3f-4c68-be0a-bec4d81fc96b.png)。
- en: This concludes our introduction to linear algebra. In the next section, we'll
    introduce the probability theory.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是我们对线性代数的介绍。接下来，我们将介绍概率论。
- en: Introduction to probability
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率简介
- en: In this section, we'll discuss some of the aspects of probability and statistics
    that are relevant to NNs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论一些与神经网络相关的概率和统计方面的内容。
- en: 'Let''s start by introducing the concept of a **statistical experiment**, which
    has the following properties:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍**统计实验**的概念，其具有以下特性：
- en: Consists of multiple independent trials.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由多个独立的试验组成。
- en: The outcome of each trial is non-deterministic; that is, it's determined by
    chance.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次试验的结果是非确定性的；也就是说，它是由机会决定的。
- en: It has more than one possible outcome. These outcomes are known as **events**
    (we'll also discuss events in the context of sets in the following section).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有多个可能的结果，这些结果称为**事件**（我们将在接下来的部分中讨论集合中的事件）。
- en: All the possible outcomes of the experiment are known in advance.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有可能的实验结果在实验前都是已知的。
- en: 'One example of a statistical experiment is a coin toss, which has two possible
    outcomes—heads or tails. Another example is a dice throw with six possible outcomes:
    1, 2, 3, 4, 5, and 6.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个统计实验的例子是硬币投掷，它有两个可能的结果——正面或反面。另一个例子是投掷骰子，它有六个可能的结果：1、2、3、4、5 和 6。
- en: We'll define **probability** as the likelihood that some event, **e**, would
    occur and we'll denote it with **P(e)**. The probability is a number in the range
    of [0, 1], where 0 indicates that the event cannot occur and 1 indicates that
    it will always occur. If *P(e) = 0.5*, there is a 50-50 chance the event would
    occur, and so on.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将**概率**定义为某个事件**e**发生的可能性，并用**P(e)**表示它。概率是一个介于 [0, 1] 范围内的数字，其中 0 表示事件无法发生，1
    表示事件一定会发生。如果*P(e) = 0.5*，则表示该事件有 50-50 的机会发生，依此类推。
- en: 'There are two ways we can approach probability:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式来处理概率：
- en: '**Theoretical**: The event we''re interested in compared to the total number
    of possible events. All the events are equally as likely:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论**：我们关心的事件与所有可能事件的总数相比。所有事件的发生概率是相等的：'
- en: '![](img/49b6defa-690f-4e59-a266-fd4d9f6b2c1f.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49b6defa-690f-4e59-a266-fd4d9f6b2c1f.png)'
- en: To understand this, let's use the coin toss example with two possible outcomes.
    The theoretical probability of each possible outcome is P(heads) = P(tails) =
    1/2\. The theoretical probability for each of the sides of a dice throw would
    be 1/6.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个问题，让我们用一个硬币投掷的例子，其中有两个可能的结果。每个可能结果的理论概率是 P(正面) = P(反面) = 1/2。投掷骰子的每一面出现的理论概率是
    1/6。
- en: '**Empirical**: This is the number of times an event we''re interested in occurs
    compared to the total number of trials:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验**：这是我们关心的事件发生的次数与总实验次数之比：'
- en: '![](img/f0d0450e-9cfc-4b25-b874-eef2ccc77de7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0d0450e-9cfc-4b25-b874-eef2ccc77de7.png)'
- en: The result of the experiment may show that the events aren't equally likely.
    For example, let's say that we toss a coin 100 times and that we observe heads
    56 times. Here, the empirical probability for heads is P(heads) = 56 / 100 = 0.56\.
    The higher the number of trials, the more accurate the calculated probability
    is (this is known as the law of large numbers).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的结果可能显示事件的发生概率不相等。例如，假设我们投掷硬币 100 次，观察到正面朝上 56 次。在这种情况下，正面的经验概率是 P(正面) = 56
    / 100 = 0.56。实验次数越多，计算出的概率越精确（这就是大数法则）。
- en: In the next section, we'll discuss probability in the context of sets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论在集合上下文中的概率。
- en: Probability and sets
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率与集合
- en: The collection of all possible outcomes (events) of an experiment is called, **sample
    space**. We can think of the sample space as a mathematical **set. **It is usually
    denoted with a capital letter and we can list all the set outcomes with {} (the
    same as Python sets). For example, the sample space of coin toss events is S[c] =
    {heads, tails}, while for dice rows it's S[d] = {1, 2, 3, 4, 5, 6}. A single outcome
    of the set (for example, heads) is called a **sample point**. An **e****vent** is
    an outcome (sample point) or a combination of outcomes (subset) of the sample
    space. An example of a combined event is for the dice to land on an even number,
    that is, {2, 4, 6}.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的所有可能结果（事件）集合叫做**样本空间**。我们可以将样本空间看作一个数学**集合**。它通常用大写字母表示，所有的集合结果可以用 {} 来列出（与
    Python 集合相同）。例如，硬币投掷事件的样本空间是 S[c] = {正面, 反面}，而骰子投掷的样本空间是 S[d] = {1, 2, 3, 4, 5,
    6}。集合中的单个结果（例如正面）叫做**样本点**。**事件**是样本空间中的一个结果（样本点）或一组结果（子集）。例如，骰子投掷结果为偶数的组合事件是
    {2, 4, 6}。
- en: 'Let''s assume that we have a sample space S = {1, 2, 3, 4, 5} and two subsets
    (events) A = {1, 2, 3} and B = {3, 4, 5}. Here, we can do the following operations
    with them:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个样本空间 S = {1, 2, 3, 4, 5}，以及两个子集（事件）A = {1, 2, 3} 和 B = {3, 4, 5}。在这种情况下，我们可以进行以下操作：
- en: '**Intersection**: The result is a new set that contains only the elements found
    in both sets:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交集**：结果是一个新集合，包含两个集合中都出现的元素：'
- en: '![](img/9750f3fa-904e-4d19-8929-29b91192e4f8.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9750f3fa-904e-4d19-8929-29b91192e4f8.png)'
- en: Sets whose intersections are empty sets {} are **disjoint**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 相交为空集 {} 的集合是**不相交**的。
- en: '**Complement**: The result is a new set that contains all the elements of the
    sample space that aren''t included in a given set:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**补集：** 结果是一个新的集合，包含样本空间中所有不包含在给定集合中的元素：'
- en: '![](img/9faf6393-251d-4453-9452-2a4a88467b1b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9faf6393-251d-4453-9452-2a4a88467b1b.png)'
- en: '**Union:** The result is a new set that contains the elements that can be found
    in either set:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并集：** 结果是一个新的集合，包含可以在任一集合中找到的元素：'
- en: '![](img/a0d19f6d-0d13-44f8-99e9-4b63f679d7b0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0d19f6d-0d13-44f8-99e9-4b63f679d7b0.png)'
- en: 'The following Venn diagrams illustrate these different set relationships:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下维恩图展示了这些不同的集合关系：
- en: '![](img/fa553edf-4bc6-447f-970b-1fb73643b7cd.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa553edf-4bc6-447f-970b-1fb73643b7cd.png)'
- en: Venn diagrams of the possible set relationships
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的集合关系的维恩图
- en: 'We can transfer the set properties to events and their probabilities. We''ll
    assume that the events are **independent**—the occurrence of one event doesn''t
    affect the probability of the occurrence of another. For example, the outcomes
    of the different coin tosses are independent of one another. That being said,
    let''s learn how to translate the set operations in the events domain:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将集合的性质转移到事件及其概率上。我们假设这些事件是**独立的**——一个事件的发生不会影响另一个事件发生的概率。例如，不同掷硬币的结果是相互独立的。话虽如此，接下来我们来学习如何将集合运算转化到事件领域中：
- en: 'The intersection of two events is a subset of the outcomes, contained in both
    events. The probability of the intersection is called **joint probability** and
    is computed via the following formula:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个事件的交集是这两个事件共同包含的结果子集。交集的概率称为**联合概率**，通过以下公式计算：
- en: '![](img/4de15857-c77b-4a52-84e0-53c12fdc63d4.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4de15857-c77b-4a52-84e0-53c12fdc63d4.png)'
- en: Let's say that we want to compute the probability of a card being red (either
    hearts or diamonds) and a Jack. The probability for red is *P(red) = 26/52 = 1/2*.
    The probability for getting a Jack is *P(Jack) = 4/52 = 1/13*. Therefore, the
    joint probability is *P(red, Jack) = (1/2) * (1/13) = 1/26*. In this example,
    we assumed that the two events are independent. However, the two events occur
    at the same time (we draw a single card). Had they occurred successively, for
    example, two card draws, where one is a Jack and the other is red, we would enter
    the realm of conditional probability. This joint probability is also denoted as
    P(A, B) or P(AB).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要计算一张卡片既是红色（红心或方块）又是杰克的概率。红色的概率是 *P(red) = 26/52 = 1/2*。抽到杰克的概率是 *P(Jack)
    = 4/52 = 1/13*。因此，联合概率是 *P(red, Jack) = (1/2) * (1/13) = 1/26*。在这个例子中，我们假设这两个事件是独立的。然而，两个事件同时发生（我们抽了一张牌）。如果它们是依次发生的，例如抽两张卡片，其中一张是杰克，另一张是红色的，我们将进入条件概率的领域。这个联合概率也可以表示为
    P(A, B) 或 P(AB)。
- en: The probability of the occurrence of a single event P(A) is also known as **marginal
    probability **(as opposed to joint probability).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 单一事件发生的概率 P(A) 也称为**边际概率**（与联合概率相对）。
- en: 'Two events are disjoint (or **mutually exclusive**) if they don''t share any
    outcomes. That is, their respective sample space subsets are disjoint. For example,
    the events of odd or even dice rows are disjoint. The following is true for the
    probability of disjoint events:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个事件是互斥的（或**互斥事件**），如果它们没有共同的结果。也就是说，它们各自的样本空间子集是互斥的。例如，奇数或偶数掷骰子的事件是互斥的。关于互斥事件的概率，以下内容是成立的：
- en: The joint probability of disjoint events (the probability for these events to
    occur simultaneously) is P(A∩B) = 0.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互斥事件的联合概率（这些事件同时发生的概率）是 P(A∩B) = 0。
- en: The sum of the probabilities of disjoint events is [![](img/82424ff7-15fe-4381-a40a-034f8c46b5d0.png)].
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互斥事件的概率之和是[![](img/82424ff7-15fe-4381-a40a-034f8c46b5d0.png)]。
- en: 'If the subsets of multiple events contain the whole sample space between themselves,
    they are **jointly exhaustive**. Events A and B from the preceding example are
    jointly exhaustive because, together, they fill up the whole sample space (1 through
    5). The following is true for the probability of jointly exhaustive events:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果多个事件的子集包含了整个样本空间，它们是**联合完全的**。前面例子中的事件 A 和 B 是联合完全的，因为它们一起填充了整个样本空间（1 到 5）。关于联合完全事件的概率，以下内容是成立的：
- en: '![](img/f27611cd-f808-40d6-a0c4-cc09f3d0f911.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f27611cd-f808-40d6-a0c4-cc09f3d0f911.png)'
- en: If we only have two events that are disjoint and jointly exhaustive at the same
    time, the events are **complement**. For example, odd and even dice throw events
    are complement.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有两个事件，它们同时是互斥且完全互补的，那么这两个事件是**补集**事件。例如，奇数和偶数的掷骰子事件是补集事件。
- en: 'We''ll refer to outcomes coming from either A or B (not necessarily in both)
    as the union of A and B. The probability of this union is as follows:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将来自 A 或 B 的结果（不一定同时）称为 A 和 B 的并集。这个并集的概率如下：
- en: '![](img/a7739b11-08b3-46e4-80a9-bc940ada1c1c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7739b11-08b3-46e4-80a9-bc940ada1c1c.png)'
- en: So far, we've discussed independent events. In the next section, we'll focus
    on dependent ones.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了独立事件。接下来，我们将专注于依赖事件。
- en: Conditional probability and the Bayes rule
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率和贝叶斯规则
- en: If the occurrence of event A changes the probability of the occurrence of event
    B, where A occurs before B, then the two are dependent. To illustrate this concept,
    let's imagine that we draw multiple cards sequentially from the deck. When the
    deck is full, the probability to draw hearts is *P(hearts) = 13/52 = 0.25*. But
    once we've drawn the first card, the probability to pick hearts on the second
    turn changes. Now, we only have 51 cards and one less heart. We'll call the probability
    of the second draw conditional probability and we'll denote it with P(B|A). This
    is the probability of event B (second draw), given that event A has occurred (first
    draw). To continue with our example, the probability of picking hearts on the
    second draw becomes *P(hearts[2]|hearts[1]) = 12/51 = 0.235*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果事件 A 的发生改变了事件 B 的发生概率，其中 A 在 B 之前发生，那么两者是依赖的。为了说明这个概念，让我们想象从牌组中依次抽取多张卡牌。当牌组完整时，抽到红心的概率为
    *P(红心) = 13/52 = 0.25*。但是一旦我们抽出第一张牌，第二次抽取红心的概率就会改变。现在，我们只剩下 51 张牌和一个少了的红心。我们称第二次抽取的概率为条件概率，并用
    P(B|A) 表示。这是事件 B（第二次抽取），在事件 A（第一次抽取）发生的条件下的概率。继续我们的例子，第二次抽取红心的概率变为 *P(红心[2]|红心[1])
    = 12/51 = 0.235*。
- en: 'Next, we can extend the joint probability formula (introduced in the preceding
    section) in terms of dependent events. The formula is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以扩展联合概率公式（在前一节介绍的基础上），以涉及依赖事件。公式如下：
- en: '![](img/2ccab1c9-ab2d-4913-a6a3-72e19662ec07.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ccab1c9-ab2d-4913-a6a3-72e19662ec07.png)'
- en: 'However, the preceding equation is just a special case for two events. We can
    extend this further for multiple events, A[1], A[2], ..., A[n]. This new generic
    formula is known as the chain rule of probability:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述方程只是两个事件的特例。我们可以进一步扩展到多个事件 A[1], A[2], ..., A[n]。这个新的通用公式被称为概率的链式法则：
- en: '![](img/4e7799cb-67c2-454f-adc4-d50a7d9ef1dd.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e7799cb-67c2-454f-adc4-d50a7d9ef1dd.png)'
- en: 'For example, the chain rule for three events is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，三个事件的链式规则如下：
- en: '![](img/e68d065f-658a-4e69-af1d-e453133b3156.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e68d065f-658a-4e69-af1d-e453133b3156.png)'
- en: 'We can also derive the formula for the conditional probability itself:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以推导出条件概率本身的公式：
- en: '![](img/5d06c169-d129-491f-93ba-dbaefbd62344.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d06c169-d129-491f-93ba-dbaefbd62344.png)'
- en: 'This formula makes sense for the following reasons:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式有以下几个原因：
- en: '**P(A ∩ B)** states that we''re interested in the occurrences of B, given that
    A has already occurred. In other words, we''re interested in the joint occurrence
    of the events, hence the joint probability.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P(A ∩ B)** 表示我们对 B 的发生感兴趣，已知 A 已经发生。换句话说，我们对事件的联合发生感兴趣，因此是联合概率。'
- en: '**P(A)** states that we''re interested only in the subset of outcomes when
    event A has occurred. We already know that A has occurred and therefore we restrict
    our observations to these outcomes.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P(A)** 表示我们只关注事件 A 发生后的子集结果。我们已经知道 A 已经发生，因此我们将观察限制在这些结果上。'
- en: 'The following holds true for dependent events:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖事件，以下内容成立：
- en: '![](img/ca82fc58-9f78-4a59-b73c-00851157a8c8.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca82fc58-9f78-4a59-b73c-00851157a8c8.png)'
- en: 'Using this equation, we can replace the value of P(A∩B) in the conditional
    probability formula to come up with the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程，我们可以在条件概率公式中替换 P(A∩B) 的值，得到如下结果：
- en: '![](img/97ce1d41-4f63-4420-ba47-2c307235e44d.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97ce1d41-4f63-4420-ba47-2c307235e44d.png)'
- en: The preceding formula gives us the ability to compute the conditional probability,
    P(B|A), if we know the opposite conditional probability, P(B|A). This equation
    is known as the **Bayes rule** and is frequently used in ML. In the context of
    Bayesian statistics, P(A) and P(B|A) are known as prior and posterior probability,
    respectively.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式使我们能够计算条件概率 P(B|A)，如果我们知道相反的条件概率 P(B|A)。这个方程被称为**贝叶斯规则**，在机器学习中经常使用。在贝叶斯统计学的背景下，P(A)
    和 P(B|A) 分别称为先验概率和后验概率。
- en: 'The Bayes rule can be illustrated in the realm of medical testing. Let''s say
    that we want to determine whether a patient has a particular disease or not. We
    conduct a medical test, which comes out positive. But this doesn''t necessarily
    mean that the patient has the disease. Most tests have a reliability value, which
    is the percentage chance of the test being positive when administered on people
    with a particular disease. Using this information, we''ll apply the Bayes rule
    to compute the actual probability of the patient having the disease, given that
    the test is positive. We get the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理可以通过医学测试来进行说明。假设我们想要确定一个患者是否患有某种特定疾病。我们进行了一项医学测试，结果呈阳性。但这并不一定意味着患者真的患有该疾病。大多数测试都有一个可靠性值，即当对患有某种特定疾病的人进行测试时，测试结果呈阳性的概率。利用这些信息，我们将应用贝叶斯定理来计算患者在测试结果为阳性的情况下，实际患病的概率。我们得出如下结果：
- en: '![](img/3f66a8ac-fd8c-4842-8d7a-664b37690ab0.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f66a8ac-fd8c-4842-8d7a-664b37690ab0.png)'
- en: Here, *P(has disease)* is the general probability of the disease without any
    prior conditions. Think of this as the probability of the disease in the general
    population.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*P(has disease)* 是没有任何先验条件下的疾病一般概率。可以将其看作是普通人群中患病的概率。
- en: 'Next, let''s make some assumptions about the disease and the test''s accuracy:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们假设一些关于疾病和测试准确性的条件：
- en: 'The test is 98% reliable, that is, if the test is positive, it will also be
    positive in 98% of cases: *P(test=positive|has disease)* = 0.98.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该测试的可靠性为98%，也就是说，如果测试结果为阳性，在98%的情况下结果也为阳性：*P(test=positive|has disease)* = 0.98。
- en: 'Only 2% of the people under 50 have this kind of disease: *P(has disease)*
    = 0.02.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50岁以下只有2%的人患有这种疾病：*P(has disease)* = 0.02。
- en: 'The test that''s administered on people under 50 is positive only for 3.9%
    of the population: *P(test=positive)* = 0.039.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对50岁以下的人群，测试结果呈阳性的人群只占3.9%：*P(test=positive)* = 0.039。
- en: 'We can ask the following question: if a test is 98% accurate for cancer and
    if a 45-year-old person took the test, which turned out to be positive, what is
    the probability that they may have the disease? Using the preceding formula, we
    can calculate the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出以下问题：如果一个测试对癌症的准确性为98%，并且一个45岁的人进行了测试，结果为阳性，那么他患病的概率是多少？利用前述公式，我们可以计算出以下结果：
- en: '![](img/0ba37462-d1ef-4608-bea0-3f1ab81484c8.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ba37462-d1ef-4608-bea0-3f1ab81484c8.png)'
- en: In the next section, we'll go beyond probabilities and we'll discuss random
    variables and probability distributions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将超越概率的讨论，探讨随机变量和概率分布。
- en: Random variables and probability distributions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机变量和概率分布
- en: In statistics, we define a variable as an attribute that describes a given entity.
    The value of the attribute can vary between entities. For example, we can describe
    the height of a person with a variable, which would differ for different people.
    But let's say that we take the height measurement of the same person multiple
    times. We can expect to obtain slightly different values each time due to some
    random factors, such as the person's pose or inaccuracy in our own measurements.
    Therefore, the value of the variable height would differ, despite the fact that
    we are measuring the same thing. To account for these changes, we'll introduce
    random variables. These are variables whose values are determined by some random
    event. Unlike regular variables, a random variable can take multiple values and
    each of these values is associated with some probability.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，我们定义变量为描述某一实体的属性。该属性的值在不同实体之间可能有所不同。例如，我们可以用一个变量来描述一个人的身高，而这个值在不同的人之间会有所不同。但假设我们多次测量同一个人的身高。由于一些随机因素，如人的姿势或我们自身测量的不准确性，我们可以预期每次测量的结果会有轻微的差异。因此，尽管我们在测量相同的东西，变量“身高”的值也会有所不同。为了考虑这些变化，我们引入了随机变量。随机变量是其值由某些随机事件决定的变量。与常规变量不同，随机变量可以取多个值，并且每个值都与某个概率相关联。
- en: 'There are two types of random variables:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量有两种类型：
- en: '**Discrete**, which can take distinct separate values. For example, the number
    of goals in a football match is a discrete variable.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散型**，只能取特定的离散值。例如，足球比赛中的进球数是一个离散变量。'
- en: '**Continuous**, which can take any value within a given interval. For example,
    a height measurement is a continuous variable.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续型**，可以在给定区间内取任何值。例如，身高测量是一个连续变量。'
- en: 'Random variables are denoted with capital letters and the probability of a
    certain value *x* for random variable *X* is denoted with either *P(X = x)* or
    *p(x)*. The collection of probabilities for each possible value of a random variable
    is called the **probability distribution**. Depending on the variable type, we
    have two types of probability distributions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量用大写字母表示，某个随机变量 *X* 取值 *x* 的概率表示为 *P(X = x)* 或 *p(x)*。所有可能值的概率集合称为 **概率分布**。根据变量类型，我们有两种概率分布类型：
- en: '**Probability mass function** (**PMF**) for discrete variables. The following
    is an example of a PMF. The *x *axis shows the possible values and the *y *axis shows
    the probability for each value:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率质量函数** (**PMF**) 用于离散变量。以下是一个 PMF 示例。*x 轴* 显示可能的值，*y 轴* 显示每个值的概率：'
- en: '![](img/b624542d-ad4c-4de3-b8cd-705432da3571.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b624542d-ad4c-4de3-b8cd-705432da3571.png)'
- en: An example of a PMF
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: PMF 示例
- en: The PMF is only defined for the possible values of the random variable. All
    the values of a PMF are non-negative and their sum is 1\. That is, the events
    of the PMF are mutually exclusive and jointly exhaustive. We'll denote PMF with
    P(X), where X is the random variable.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PMF 仅对随机变量的可能值定义。PMF 的所有值都是非负的，且它们的总和为 1。也就是说，PMF 事件是互斥且共同完备的。我们用 P(X) 表示 PMF，其中
    X 是随机变量。
- en: '**Probability density function** (**PDF**) for continuous variables. Unlike
    PMF, the PDF is uninterrupted (defined for every possible value) in the interval
    between two values, thereby reflecting the nature of the continuous variable.
    The following is an example of a PDF:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率密度函数** (**PDF**) 用于连续变量。与 PMF 不同，PDF 在两个值之间的区间内是连续的（定义了所有可能值），从而反映了连续变量的特性。以下是一个
    PDF 示例：'
- en: '![](img/9af07727-a3be-4e97-becd-2e28cf781e10.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9af07727-a3be-4e97-becd-2e28cf781e10.png)'
- en: An example of a PDF
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PDF 示例
- en: In the PDF, the probability is computed for a value interval and is given by
    the surface area under the curve, enclosed by that interval (this is the marked
    area in the preceding diagram). The total area under the curve is 1. We'll denote
    PDF with *f[X]*, where X is the random variable.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PDF 中，概率是为某个值区间计算的，并由该区间下的曲线下的面积表示（这是前面图示中标出的区域）。曲线下的总面积为 1。我们用 *f[X]* 表示
    PDF，其中 X 是随机变量。
- en: 'Next, let''s focus on some of the properties of random variables:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来关注随机变量的一些属性：
- en: 'The **mean** (or **expected value**) is the expected outcome of an experiment
    over many observations. We''ll denote it with μ or [![](img/0e27cda7-f298-4638-bb45-baf9d20f900d.png)].
    For a discrete variable, the mean is the weighted sum of all possible values,
    multiplied by their probabilities:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值**（或 **期望值**）是实验在多次观察中的预期结果。我们用 μ 或 [![](img/0e27cda7-f298-4638-bb45-baf9d20f900d.png)]
    表示它。对于离散变量，均值是所有可能值的加权和，每个值乘以它们的概率：'
- en: '![](img/c0657eab-a8d9-4164-a902-80831922cf56.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0657eab-a8d9-4164-a902-80831922cf56.png)'
- en: Let's use the preceding discrete variable example as an example, where we defined
    a random variable with six possible values (0, 1, 2, 3, 4, 5) and their respective
    probabilities (0.1, 0.2, 0.3, 0.2, 0.1, 0.1). Here, the mean is *μ = 0*0.1 + 1*0.2
    + 2*0.3 + 3*0.2 + 4*0.1 + 5*0.1 = 2.3.*
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前的离散变量示例，其中我们定义了一个有六个可能值（0, 1, 2, 3, 4, 5）和它们各自概率（0.1, 0.2, 0.3, 0.2,
    0.1, 0.1）的随机变量。在这里，均值是 *μ = 0*0.1 + 1*0.2 + 2*0.3 + 3*0.2 + 4*0.1 + 5*0.1 = 2.3*。
- en: 'The mean for a continuous variable is defined as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量的均值定义如下：
- en: '![](img/795d34b0-3db0-4bb7-b329-ba6fb5340ce0.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/795d34b0-3db0-4bb7-b329-ba6fb5340ce0.png)'
- en: While with a discrete variable we can think of the PMF as a lookup table, the
    PDF may be more complex (an actual function or equation), which is why there's
    different notation between the two. We won't go into further details about the
    mean of continuous variables.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散变量，我们可以将 PMF 看作查找表，而 PDF 可能更复杂（是一个实际的函数或方程），这就是它们之间符号不同的原因。我们不会进一步探讨连续变量的均值。
- en: '**Variance** is defined as the expected value of the squared deviation from
    the mean, μ, of a random variable:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差** 定义为一个随机变量与其均值 μ 之间的平方偏差的期望值：'
- en: '![](img/f8904fe0-0265-47a6-a282-560d4650cef1.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8904fe0-0265-47a6-a282-560d4650cef1.png)'
- en: In other words, the variance measures how the values of a random variable differ
    from its mean value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，方差衡量的是随机变量的值与其均值的偏离程度。
- en: 'The variance of a discrete random variable is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 离散随机变量的方差如下：
- en: '![](img/5af276dd-d6ad-45d1-9d18-6e96bde7e162.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5af276dd-d6ad-45d1-9d18-6e96bde7e162.png)'
- en: Let's use the preceding example, where we calculated the mean value to be 2.3\.
    The new variance would be *Var(X)* = *(0 - 2.3)² * 0 + (1 - 2.3)² *** 1 + ...
    + (5- 2.3)² * 5 = 2.01*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用前面的例子，其中我们计算得出均值为2.3。新的方差为*Var(X)* = *(0 - 2.3)² * 0 + (1 - 2.3)² * 1 +
    ... + (5- 2.3)² * 5 = 2.01*。
- en: 'The variance of a continuous variable is defined as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量的方差定义如下：
- en: '![](img/ab614d73-6d72-434e-bb88-e4aaca8c203c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab614d73-6d72-434e-bb88-e4aaca8c203c.png)'
- en: 'The **standard deviation** measures the degree to which the values of the random
    variable differ from the expected value. If this definition sounds similar to
    variance, it''s because it is. In fact, the formula for standard deviation is
    as follows:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准差**衡量随机变量的值与期望值的差异程度。如果这个定义听起来像方差，那是因为它确实如此。事实上，标准差的公式如下：'
- en: '![](img/aa212f96-53f9-4902-b157-a7794dae687a.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa212f96-53f9-4902-b157-a7794dae687a.png)'
- en: 'We can also define the variance in terms of standard deviation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过标准差定义方差：
- en: '![](img/95089bd7-c563-4642-9f9e-9ffd955b814c.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95089bd7-c563-4642-9f9e-9ffd955b814c.png)'
- en: The difference between standard deviation and variance is that the standard
    deviation is expressed in the same units as the mean value, while the variance
    uses squared units.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差和方差的区别在于，标准差使用与均值相同的单位表示，而方差使用平方单位。
- en: In this section, we defined what a probability distribution is. Next, let's
    discuss different types of probability distributions.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义了什么是概率分布。接下来，让我们讨论不同类型的概率分布。
- en: Probability distributions
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率分布
- en: 'We''ll start with the **binomial distribution** for discrete variables in binomial
    experiments. A binomial experiment has only two possible outcomes: success or
    failure. It also satisfies the following requirements:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从**二项分布**开始，适用于离散变量的二项实验。二项实验只有两个可能的结果：成功或失败。它还满足以下要求：
- en: Each trial is independent of the others.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次试验相互独立。
- en: The probability of success is always the same.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功的概率始终相同。
- en: An example of a binomial experiment is the coin toss experiment.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二项实验的例子是抛硬币实验。
- en: 'Now, let''s assume that the experiment consists of *n* trials. *x* of them
    are successful, while the probability of success at each trial is *p*. The formula
    for a binomial PMF of variable X (not to be confused with *x*) is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设实验包含*n*次试验，其中*x*次成功，每次试验的成功概率为*p*。变量X的二项PMF公式（不要与*x*混淆）如下所示：
- en: '![](img/ecbfa31a-d56f-4dcb-9901-471b1326841d.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecbfa31a-d56f-4dcb-9901-471b1326841d.png)'
- en: Here, [![](img/54708b4a-3bf1-4a19-9f3b-5c06c5cd6035.png)] is the binomial coefficient.
    This is the number of combinations of *x* successful trials, which we can select
    from the *n* total trials. If *n=1*, then we have a special case of binomial distribution
    called **Bernoulli distribution**.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是二项式系数[![](img/54708b4a-3bf1-4a19-9f3b-5c06c5cd6035.png)]。这是*x*次成功试验的组合数，我们可以从*n*次总试验中选择。如果*n=1*，那么我们有一个特殊的二项分布案例，称为**伯努利分布**。
- en: 'Next, let''s discuss the normal (or Gaussian) distribution for continuous variables, which
    closely approximates many natural processes. The normal distribution is defined
    with the following exponential PDF formula, known as normal equation (one of the
    most popular notations):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论适用于连续变量的正态（或高斯）分布，它 closely approximates 很多自然过程。正态分布使用以下指数PDF公式定义，称为正态方程（最常见的表示法之一）：
- en: '![](img/27332c0a-c327-44ac-a6de-6fc4308703ea.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27332c0a-c327-44ac-a6de-6fc4308703ea.png)'
- en: 'Here, *x* is the value of the random variable, *μ* is the mean, *σ* is the
    standard deviation, and *σ²* is the variance. The preceding equation produces
    a bell-shaped curve, which is shown in the following diagram:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*是随机变量的值，*μ*是均值，*σ*是标准差，*σ²*是方差。前述公式产生了一个钟形曲线，显示如下图所示：
- en: '![](img/299a86be-c077-4a2e-8565-9e5bd48b9636.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/299a86be-c077-4a2e-8565-9e5bd48b9636.png)'
- en: Normal distribution
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布
- en: 'Let''s discuss some of the properties of the normal distribution, in no particular
    order:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些正态分布的性质，顺序不分先后：
- en: The curve is symmetric along its center, which is also the maximum value.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线是对称的，围绕其中心，这也是最大值所在。
- en: 'The shape and location of the curve are fully described by the mean and standard
    deviation, where we have the following:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线的形状和位置完全由均值和标准差描述，公式如下：
- en: The center of the curve (and its maximum value) is equal to the mean. That is,
    the mean determines the location of the curve along the *x* axis.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线的中心（及其最大值）等于均值。也就是说，均值决定了曲线在 *x* 轴上的位置。
- en: The width of the curve is determined by the standard deviation.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线的宽度由标准差决定。
- en: 'In the following diagram, we can see examples of normal distributions with
    different *μ* and *σ* values:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到具有不同 *μ* 和 *σ* 值的正态分布示例：
- en: '![](img/966244d3-6838-4232-8d95-efcc2db6718b.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/966244d3-6838-4232-8d95-efcc2db6718b.png)'
- en: Examples of normal distributions with different *μ* and *σ* values
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不同 *μ* 和 *σ* 值的正态分布示例
- en: The normal distribution approaches 0 toward +/- infinity, but it never becomes
    0\. Therefore, a random variable under normal distribution can have any value
    (albeit some values with a tiny probability).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态分布在正负无穷大处趋近于0，但永远不会变为0。因此，服从正态分布的随机变量可以取任何值（尽管某些值的概率非常小）。
- en: The surface area under the curve is equal to 1, which is ensured by the constant, [![](img/02c31240-b7e4-4022-86be-883f26fb9ced.png)],
    being before the exponent.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线下的面积等于1，这由常数[![](img/02c31240-b7e4-4022-86be-883f26fb9ced.png)]确保，它位于指数前面。
- en: '[![](img/6c9b2a78-408e-49a3-a1bc-d1462d3c6964.png)] (located in the exponent) is
    called the standard score (or z-score). A standardized normal variable has a mean
    of 0 and a standard deviation of 1\. Once transformed, the random variable participates
    in the equation in its standardized form.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/6c9b2a78-408e-49a3-a1bc-d1462d3c6964.png)]（位于指数中）被称为标准分数（或z分数）。标准化的正态变量具有0的均值和1的标准差。转换后，随机变量以标准化的形式参与方程。'
- en: In the next section, we'll introduce the multidisciplinary field of information
    theory, which will help us use probability theory in the context of NNs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍信息论这一跨学科领域，它将帮助我们在神经网络（NNs）的背景下使用概率论。
- en: Information theory
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息论
- en: 'Information theory attempts to determine the amount of information an event
    has. The amount of information is guided by the following principles:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论试图确定一个事件所包含的信息量。信息量由以下原则指导：
- en: The higher the probability of an event, the less informative the event is considered.
    Conversely, if the probability is lower, the event carries more informational
    content. For example, the outcome of a coin flip (with a probability of 1/2) provides
    less information than the outcome of a dice throw (with a probability of 1/6).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件的概率越高，事件的含信息量越少。相反，如果概率较低，事件则携带更多的信息量。例如，掷硬币的结果（概率为1/2）提供的信息量少于掷骰子的结果（概率为1/6）。
- en: The information that's carried by independent events is the sum of their individual
    information contents. For example, two dice rows that come up on the same side
    of the dice (let's say, 4) are twice as informative as the individual rows.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立事件携带的信息是它们各自信息量的总和。例如，两个骰子掷出的相同点数（假设是4）比单个点数的两倍信息量。
- en: 'We''ll define the amount of information (or self-information) of event *x*
    as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将事件 *x* 的信息量（或自信息）定义如下：
- en: '![](img/bc0a3a5c-f8c5-416c-860a-9ff406d5bddc.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc0a3a5c-f8c5-416c-860a-9ff406d5bddc.png)'
- en: Here, *log* is the natural logarithm. For example, if the probability of event
    is *P(x) = 0.8*, then *I(x) = 0.22*. Alternatively, if *P(x)** = 0.2*, then *I(x)
    = 1.61*. We can see that the event information content is opposite to the event
    probability. The amount of self-information I(x) is measured in natural units
    of information (**nat**). We can also compute I(x) with a base 2 logarithm [![](img/c6e7e6d6-823e-41fd-b98b-53f85660ed0b.png)],
    in which case we measure it in bits. There is no principal difference between
    the two versions. For the purposes of this book, we'll stick with the natural
    logarithm version.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， *log* 是自然对数。例如，如果事件的概率是 *P(x) = 0.8*，那么 *I(x) = 0.22*。或者，如果 *P(x) = 0.2*，则
    *I(x) = 1.61*。我们可以看到，事件的信息量与事件的概率是相反的。自信息量 *I(x)* 是以自然信息单位（**nat**）来度量的。我们也可以使用以2为底的对数
    [![](img/c6e7e6d6-823e-41fd-b98b-53f85660ed0b.png)] 来计算 *I(x)*，在这种情况下我们以比特为单位度量。两者之间没有本质的区别。为了本书的目的，我们将坚持使用自然对数版本。
- en: 'Let''s discuss why we use logarithm in the preceding formula, even though a
    negative probability would also satisfy the reciprocity between self-information
    and probability. The main reason is the product and division rules of logarithms:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下为什么在前面的公式中使用对数，尽管负概率也能满足自信息和概率之间的互惠关系。主要原因是对数的乘法和除法规则：
- en: '![](img/df24a7c6-d531-45f0-8349-f55b4cc9fe81.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df24a7c6-d531-45f0-8349-f55b4cc9fe81.png)'
- en: Here, *x[1]* and *x[2]* are scalar values. Without going into too much detail,
    note that these properties allow us to easily minimize the error function during
    network training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x[1]*和*x[2]*是标量值。无需过多细节，注意这些属性使得我们在网络训练过程中可以轻松地最小化误差函数。
- en: So far, we've defined the information content of a single outcome. But what
    about other outcomes? To measure them, we have to measure the amount of information
    over the probability distribution of the random variable. Let's denote it with I(*X*),
    where *X* is a random discrete variable (we'll focus on discrete variables here)*.* Recall
    that, in the *Random variables and probability distributions* section, we defined
    the mean (or expected value) of a discrete random variable as the weighted sum
    of all possible values, multiplied by their probabilities. We'll do something
    similar here, but we'll multiply the information content of each event by the
    probability of that event.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经定义了单一结果的信息量。那么其他结果呢？为了衡量它们，我们必须衡量整个随机变量的概率分布的信息量。我们用I(*X*)来表示其中的量，其中*X*是一个离散的随机变量（我们这里重点讨论离散变量）。回想一下，在*随机变量和概率分布*部分，我们定义了离散随机变量的均值（或期望值）为所有可能值的加权和，乘以它们的概率。我们在这里也会做类似的事情，但我们将每个事件的信息量乘以该事件的概率。
- en: 'This measure is called Shannon entropy (or just entropy) and is defined as
    follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量称为香农熵（或简称熵），其定义如下：
- en: '![](img/b5be33a0-5072-4c44-a32f-f4afe5eb8a38.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5be33a0-5072-4c44-a32f-f4afe5eb8a38.png)'
- en: 'Here, *x[i]* represents the discrete variable values. Events with higher probabilities
    will carry more weight compared to low-probability ones. We can think of entropy
    as the expected (mean) amount of information about the events (outcomes) of the
    probability distribution. To understand this, let''s try to compute the entropy
    of the familiar coin toss experiment. We''ll calculate two examples:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x[i]*表示离散变量的值。相较于低概率事件，具有较高概率的事件会有更大的权重。我们可以将熵理解为概率分布中事件（结果）信息量的期望（均值）。为了理解这一点，假设我们计算一个熟悉的抛硬币实验的熵。我们将计算两个示例：
- en: 'First, let''s assume that *P(heads) = P(tails) = 0.5*. In this case, the entropy
    is as follows:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，假设*P(正面) = P(反面) = 0.5*。在这种情况下，熵如下：
- en: '![](img/ea728c03-e98b-46d7-a382-4c5a4721f411.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea728c03-e98b-46d7-a382-4c5a4721f411.png)'
- en: 'Next, let''s assume that, for some reason, the outcomes are not equally likely
    and that the probability distribution is *P(heads) = 0.2 and P(tails) = 0.8*.
    The entropy is as follows:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，假设由于某些原因，事件的结果概率并不相等，且概率分布为*P(正面) = 0.2 和 P(反面) = 0.8*。熵如下：
- en: '![](img/8f2e8145-8134-4095-9957-dab1e32d88d4.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f2e8145-8134-4095-9957-dab1e32d88d4.png)'
- en: 'We can see that the entropy is highest when the outcomes are equally likely
    and decreases when one outcome becomes prevalent. In a sense, we can think of
    entropy as a measurement of uncertainty or chaos. The following diagram shows
    a graph of the entropy **H(X)** over a binary event (such as the coin toss), depending
    on the probability distribution of the two outcomes:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当所有结果的概率相等时，熵最大；而当某个结果变得更加常见时，熵减少。从某种意义上说，我们可以将熵视为不确定性或混乱的度量。以下图示显示了在二元事件（如抛硬币）中，熵**H(X)**相对于两个结果的概率分布的变化：
- en: '![](img/3048ac36-93c3-4e88-8c6b-5465ce9619c7.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3048ac36-93c3-4e88-8c6b-5465ce9619c7.png)'
- en: 'Left: entropy with natural logarithm; right: entropy with base 2 logarithm'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：使用自然对数计算熵；右侧：使用以2为底的对数计算熵
- en: 'Next, let''s imagine that we have a discrete random variable, *X*, and two
    different probability distributions over it. This is usually the scenario where
    a NN produces some output probability distribution *Q*(*X*) and we compare it
    to a target distribution, *P*(*X*), during training. We can measure the difference
    between these two distributions with **cross-entropy**, which is defined as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设我们有一个离散的随机变量*X*，并且它有两个不同的概率分布。通常这种情况发生在神经网络产生某个输出概率分布*Q*(*X*)，并且在训练过程中将其与目标分布*P*(*X*)进行比较时。我们可以通过**交叉熵**来衡量这两个分布之间的差异，其定义如下：
- en: '![](img/286a0d80-e65e-4094-9d51-5ed9a35d3c5a.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/286a0d80-e65e-4094-9d51-5ed9a35d3c5a.png)'
- en: 'For example, let''s calculate the cross entropy between the two probability
    distributions of the preceding coin toss scenario. We have predicted distribution
    *Q(heads) = 0.2, Q(tails) = 0.8* and the target (or true) distribution *P(heads)
    = 0.5, P(tails) = 0.5*. The cross entropy is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们计算前述投硬币场景中两个概率分布之间的交叉熵。我们有预测分布*Q(正面) = 0.2, Q(反面) = 0.8*和目标（或真实）分布*P(正面)
    = 0.5, P(反面) = 0.5*。交叉熵如下：
- en: '![](img/f2d9c169-6917-4baa-a640-e127e574dd99.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2d9c169-6917-4baa-a640-e127e574dd99.png)'
- en: 'Another measure of the difference between two probability distributions is
    the **Kullback–Leibler divergence** (**KL divergence**):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量两个概率分布之间差异的另一个方法是**Kullback–Leibler 散度**（**KL 散度**）：
- en: '![](img/cd443f93-4762-4798-97b1-2fad942ff6a8.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd443f93-4762-4798-97b1-2fad942ff6a8.png)'
- en: The product rule of logarithms helped us to transform the first-row formula
    into a more intuitive form on the second row. It is easier to see that the KL
    divergence measures the difference between the target and predicted log probabilities.
    If we derive the equation further, we can also see the relationship between the
    entropy, cross-entropy, and KL divergence.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对数的乘积法则帮助我们将第一行的公式转化为第二行的更直观形式。这样我们可以更清楚地看到，KL 散度衡量的是目标与预测对数概率之间的差异。如果我们进一步推导这个方程，还可以看到熵、交叉熵与
    KL 散度之间的关系。
- en: 'The KL divergence of the coin toss example scenario is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 投硬币的例子场景的KL散度如下：
- en: '![](img/cb7873f1-4967-4cc2-aa2b-9847a61eafeb.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb7873f1-4967-4cc2-aa2b-9847a61eafeb.png)'
- en: In the next section, we'll discuss the field of differential calculus, which
    will help us with training NNs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论微积分领域，这将有助于我们训练神经网络（NN）。
- en: Differential calculus
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微积分
- en: In ML, we are often interested in how to approximate some target function by
    adjusting the parameters of ML algorithms. If we think of the ML algorithm itself
    as a mathematical function (which is the case for NNs), we would like to know
    how the output of that function changes when we change some of its parameters
    (weights). Thankfully, differential calculus deals with the rate of change of
    a function with respect to a variable that the function depends on. The following
    is a (very) short introduction to derivatives.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，我们通常关注的是如何通过调整机器学习算法的参数来逼近某个目标函数。如果我们将机器学习算法本身视为一个数学函数（对于神经网络来说就是如此），我们就想知道当我们改变它的一些参数（权重）时，这个函数的输出如何变化。幸运的是，微积分正是研究函数相对于其所依赖的变量的变化率的工具。以下是导数的（非常）简短介绍。
- en: 'Let''s say that we have a function, *f(x)*, with a single parameter, *x*, which
    has the following graph:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数，*f(x)*，它有一个单一的参数，*x*，其图形如下：
- en: '![](img/6af6853f-95e6-4295-9928-8072b8b3cc4e.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6af6853f-95e6-4295-9928-8072b8b3cc4e.png)'
- en: The graph of *f(x)* and the slope (red dot-dashed line)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x)*的图形和斜率（红色虚线）'
- en: 'We can get a relative idea of how *f(x)* changes with respect to *x* at any
    value of *x* by calculating the slope of the function at that point. If the slope
    is positive, the function increases. Conversely, if it''s negative, it decreases.
    We can calculate the slope with the following equation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算函数在某一点的斜率，得到*f(x)*相对于*x*在任意值处的变化情况。如果斜率为正，说明函数在增大；相反，如果斜率为负，说明函数在减小。我们可以通过以下方程计算斜率：
- en: '![](img/4747290f-0134-4f25-9a76-8cc959826bc4.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4747290f-0134-4f25-9a76-8cc959826bc4.png)'
- en: 'The idea here is simple—we calculate the difference between two values of *f*
    at *x* and *x+Δx: Δy = f(x + Δx) - f(x)*. Then, we calculate the ratio between *Δy* and *Δx*
    to get the slope. But if *Δx* is too big, the measurement won''t be very accurate,
    because the part of the function graph enclosed between *x* and *x+Δx* may change
    drastically. We can use a smaller *Δx* to minimize this error; here, we can focus
    on a smaller part of the graph. If *Δx* approaches 0, we can assume that the slope
    reflects a single point of the graph. In this case, we call the slope the **first
    derivative** of *f(x)*. We can express this in mathematical terms via the following
    equation:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的思路很简单——我们计算*f*在*x*和*x+Δx*这两个值之间的差异：Δy = f(x + Δx) - f(x)。然后，我们计算*Δy*与*Δx*的比值来得到斜率。但如果*Δx*太大，测量结果就不太准确，因为在*x*和*x+Δx*之间包含的函数图形部分可能会发生剧烈变化。我们可以使用更小的*Δx*来最小化这个误差；这样，我们就可以关注图形的更小部分。如果*Δx*趋近于0，我们可以假设斜率反映了图形的某一个点。在这种情况下，我们称斜率为*f(x)*的**一阶导数**。我们可以通过以下方程用数学语言来表示：
- en: '![](img/1346b5cc-2850-455b-b2c2-d08ac1436d63.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1346b5cc-2850-455b-b2c2-d08ac1436d63.png)'
- en: 'Here, *f''(x)* and *dy*/*dx* are Lagrange''s and Leibniz''s notations for derivatives,
    respectively. [![](img/daf6028e-6f96-422b-85ad-2958c188a7de.png)] is the mathematical
    concept of the limit—we can think of it as *Δx* approaches 0\. The process of
    finding the derivative of *f* is called **differentiation**. The following diagram
    shows slopes at different values of *x*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， *f'(x)* 和 *dy*/*dx* 分别是拉格朗日和莱布尼茨的导数表示法。[![](img/daf6028e-6f96-422b-85ad-2958c188a7de.png)]
    是极限的数学概念——我们可以将其视为 *Δx* 趋近于 0 的过程。求 *f* 的导数过程称为 **微分**。以下图展示了不同 *x* 值下的斜率：
- en: '![](img/9be91093-11dc-41c7-934a-35a65f6c27c3.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9be91093-11dc-41c7-934a-35a65f6c27c3.png)'
- en: We can see that the slopes at the **local minimum** and **local maximum** of
    *f* are 0—at these points (known as saddle points), *f* neither increases nor
    decreases as we change *x*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，*f* 在 **局部最小值** 和 **局部最大值** 处的斜率为 0——在这些点（称为鞍点），*f* 在 *x* 变化时既不增加也不减少。
- en: Next, let's assume that we have a function of multiple parameters, [![](img/12ed27f9-3790-42db-8686-9074a9898e0a.png)].
    The derivative of *f* with respect to any of the parameters, *x[i]*, is called
    a partial derivative and is denoted by [![](img/727e193f-e698-4224-a40f-e4f2df518a13.png)].
    When computing the partial derivative, we assume that all the other parameters, [![](img/97873945-7233-4a21-b250-4dd3f2ce4a5f.png)],
    are constants. We'll denote the partial derivatives of the components of a vector
    with [![](img/cf689fe9-76b8-4538-b626-f1a2716c9281.png)].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设我们有一个多个参数的函数，[![](img/12ed27f9-3790-42db-8686-9074a9898e0a.png)]。对于任何参数
    *x[i]*，*f* 关于它的导数称为偏导数，并表示为 [![](img/727e193f-e698-4224-a40f-e4f2df518a13.png)]。计算偏导数时，我们假设所有其他参数，[![](img/97873945-7233-4a21-b250-4dd3f2ce4a5f.png)]，是常数。我们将用
    [![](img/cf689fe9-76b8-4538-b626-f1a2716c9281.png)] 表示向量各分量的偏导数。
- en: 'Finally, let''s mention some useful rules for differentiation:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来介绍一些有用的求导规则：
- en: '**Chain rule**: Let''s say that *f* and *g* are some functions and *h(x)= f(g(x)).* Here, the
    derivative of *f* with respect to *x* for any *x* is as follows:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链式法则**：假设 *f* 和 *g* 是一些函数，且 *h(x)= f(g(x))。* 在这里，任何 *x* 的 *f* 对 *x* 的导数如下：'
- en: '![](img/657a48ea-67e9-4c27-8ae3-1ca8c386e28c.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/657a48ea-67e9-4c27-8ae3-1ca8c386e28c.png)'
- en: '**Sum rule**: Let''s say that *f* and *g* are some functions and *h(x) = f(x)
    + g(x)*. The sum rule states the following:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**和规则**：假设 *f* 和 *g* 是某些函数，且 *h(x) = f(x) + g(x)*。和规则表示以下内容：'
- en: '![](img/e856188c-d37a-44e0-919c-99764acc1029.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e856188c-d37a-44e0-919c-99764acc1029.png)'
- en: '**Common functions**:'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常见函数**：'
- en: '*x'' = 1*'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x'' = 1*'
- en: '*(ax)'' = a*, where *a* is scalar'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(ax)'' = a*，其中 *a* 是常数'
- en: '*a'' = 0*, where *a* is scalar'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a'' = 0*，其中 *a* 是常数'
- en: '*x² = 2x*'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x² = 2x*'
- en: '*(e^x)'' = e^x*'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(e^x)'' = e^x*'
- en: The mathematical apparatus of NNs and NNs themselves form a sort of knowledge
    hierarchy. If we think of implementing a NN as building a house, then the mathematical
    apparatus is like mixing concrete. We can learn how to mix the concrete independently
    of how to build a house. In fact, we can mix concrete for a variety of purposes
    other than the specific goal of building a house. However, we need to know how
    to mix concrete before building the house. To continue with our analogy, now that
    we know how to mix concrete (mathematical apparatus), we'll focus on actually
    building the house (NNs).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）及其数学工具形成了一种知识层次结构。如果我们把实现一个神经网络看作是建造一座房子，那么数学工具就像是混合混凝土。我们可以独立地学习如何混合混凝土，而不需要了解如何建造房子。事实上，我们可以将混凝土用于除建房之外的多种用途。然而，在建房之前，我们需要知道如何混合混凝土。继续我们的类比，现在我们知道如何混合混凝土（数学工具），接下来我们将专注于实际建造房子（神经网络）。
- en: A short introduction to NNs
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: 'A NN is a function (let''s denote it with *f*) that tries to approximate another
    target function, *g*. We can describe this relationship with the following equation:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个函数（我们用 *f* 表示），它试图逼近另一个目标函数 *g*。我们可以用以下方程描述这种关系：
- en: '![](img/a0d8ad81-d35d-4aca-938c-a995ad09c5cf.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0d8ad81-d35d-4aca-938c-a995ad09c5cf.png)'
- en: Here, *x* is the input data and *θ* are the NN parameters (weights). The goal
    is to find such *θ* parameters with the best approximate, *g*. This generic definition
    applies for both regression (approximating the exact value of *g*) and classification
    (assigning the input to one of multiple possible classes) tasks. Alternatively,
    the NN function can be denoted as ![](img/04acee79-fa2e-435c-b648-1ae896c2e1d0.png).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x* 是输入数据，*θ* 是神经网络的参数（权重）。目标是找到最佳近似的 *θ* 参数，使其接近 *g*。这个通用定义适用于回归（逼近 *g*
    的精确值）和分类（将输入分配到多个可能类别中的一个）任务。或者，神经网络函数可以表示为 ![](img/04acee79-fa2e-435c-b648-1ae896c2e1d0.png)。
- en: We'll start our discussion from the smallest building block of the NN—the neuron.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从神经网络最小的构建块——神经元开始讨论。
- en: Neurons
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经元
- en: 'The preceding definition is a bird''s-eye view of a NN. Now, let''s discuss
    the basic building blocks of a NN, namely the neurons (or **units**). Units are
    mathematical functions that can be defined as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义是神经网络（NN）的俯瞰图。现在，让我们讨论神经网络的基本构建块，即神经元（或**单元**）。单元是可以定义为以下内容的数学函数：
- en: '![](img/f16f17a0-7360-485c-a4ca-463feb633388.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f16f17a0-7360-485c-a4ca-463feb633388.png)'
- en: 'Here, we have the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*y* is the unit output (single value).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*是单元的输出（单个值）。'
- en: '*f* is the non-linear differentiable activation function. The activation function
    is the source of non-linearity in a NN—if the NN was entirely linear, it would
    only be able to approximate other linear functions.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*是非线性可微的激活函数。激活函数是神经网络中的非线性来源——如果神经网络完全是线性的，它只能近似其他线性函数。'
- en: The argument of the activation function is the weighted sum (with weights *w[i]*)
    of all the unit inputs *x[i]* (*n* total inputs) and the bias weight *b*. The
    inputs *x[i]* can be either the data input values or outputs of other units.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数的参数是所有单元输入的加权和（权重为*w[i]*），其中输入*x[i]*（总共有*n*个输入）以及偏置权重*b*。输入*x[i]*可以是数据输入值，也可以是其他单元的输出。
- en: 'Alternatively, we can substitute *x[i]* and *w[i]* with their vector representations,
    where [![](img/70c30cb5-49ed-4523-915a-7ad17f211850.png)] and [![](img/5463e3ea-224a-4be3-9db7-87f4172b806d.png)].
    Here, the formula will use the dot product of the two vectors:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以用向量表示替代*x[i]*和*w[i]*，其中[![](img/70c30cb5-49ed-4523-915a-7ad17f211850.png)]和[![](img/5463e3ea-224a-4be3-9db7-87f4172b806d.png)]。在这里，公式将使用两个向量的点积：
- en: '![](img/d9748d38-e79a-4d30-9a70-0558d40a3a9a.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9748d38-e79a-4d30-9a70-0558d40a3a9a.png)'
- en: 'The following diagram (left) shows a unit:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图（左）展示了一个单元：
- en: '![](img/082ab20e-1d14-4d08-9d03-cf22b368c695.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/082ab20e-1d14-4d08-9d03-cf22b368c695.png)'
- en: 'Left: A unit and its equivalent formula; right: A geometric representation
    of a perceptron.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：一个单元及其等效公式；右侧：感知机的几何表示。
- en: The input vector **x** will be perpendicular to the weight vector **w** if **x•
    w = 0**. Therefore, all vectors **x** where **x• w = 0** define a hyperplane in
    the vector space [![](img/4c44a65f-c096-4021-a553-a0ce228357cb.png)], where *n* is
    the dimension of *x*. In the case of two-dimensional input *(x[1], x[2])*, we
    can represent the hyperplane as a line. This could be illustrated with the perceptron (or
    binary classifier)—a unit with a **threshold** activation function ![](img/1ece5653-1f1a-458a-87b4-0d9c75eff95e.png) that
    classifies its input in one of the two classes. The geometric representation of
    the perceptron with two inputs *(x[1], x[2])* is a line (or decision boundary)
    separating the two classes (to the right in the preceding diagram). This imposes
    a serious limitation on the neuron because it cannot classify linearly inseparable
    problems—even simple ones such as XOR.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入向量**x**与权重向量**w**垂直，则**x•w = 0**。因此，所有满足**x•w = 0**的向量**x**定义了向量空间中的超平面[![](img/4c44a65f-c096-4021-a553-a0ce228357cb.png)]，其中*n*是**x**的维度。在二维输入*(x[1],
    x[2])*的情况下，我们可以将超平面表示为一条直线。这可以通过感知机（或二分类器）来说明——一个具有**阈值**激活函数的单元！[](img/1ece5653-1f1a-458a-87b4-0d9c75eff95e.png)，它将输入分类为两个类别之一。感知机的几何表示是带有两个输入*(x[1],
    x[2])*的直线（或决策边界），将两个类别分开（如前图所示的右侧）。这对神经元造成了一个严重的限制，因为它无法分类线性不可分的问题——甚至是像XOR这样的简单问题。
- en: A unit with an identity activation function (*f(x) = x*) is equivalent to multiple
    linear regression, while a unit with a sigmoid activation function is equivalent
    to logistic regression.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 具有恒等激活函数（*f(x) = x*）的单元等同于多元线性回归，而具有Sigmoid激活函数的单元则等同于逻辑回归。
- en: Next, let's learn how to organize the neurons in layers.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习如何将神经元组织成层。
- en: Layers as operations
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层作为操作
- en: 'The next level in the NN organizational structure is the layers of units, where
    we combine the scalar outputs of multiple units in a single output vector. The
    units in a layer are not connected to each other. This organizational structure
    makes sense for the following reasons:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络组织结构的下一级是单元的层，其中我们将多个单元的标量输出组合成一个输出向量。层中的单元之间没有相互连接。这种组织结构有以下几方面的意义：
- en: We can generalize multivariate regression to a layer, as opposed to only linear
    or logistic regression for a single unit. In other words, we can approximate multiple
    values with a layer as opposed to a single value with a unit. This happens in
    the case of classification output, where each output unit represents the probability
    the input belongs to a certain class.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将多元回归推广到层，而不仅仅是单一单元的线性或逻辑回归。换句话说，我们可以通过一个层来逼近多个值，而不是通过单个单元来逼近一个值。这发生在分类输出的情况下，其中每个输出单元表示输入属于某个类别的概率。
- en: A unit can convey limited information because its output is a scalar. By combining
    the unit outputs, instead of a single activation, we can now consider the vector
    in its entirety. In this way, we can convey a lot more information, not only because
    the vector has multiple values, but also because the relative ratios between them
    carry additional meaning.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元能够传递的信息有限，因为其输出是一个标量。通过组合单元的输出，而不是单一的激活，我们现在可以考虑整个向量。这样，我们可以传递更多的信息，不仅因为向量有多个值，还因为它们之间的相对比例传递了额外的意义。
- en: Because the units in a layer have no connections to each other, we can parallelize
    the computation of their outputs (thereby increasing the computational speed).
    This ability is one of the major reasons for the success of DL in recent years.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于层中的单元之间没有相互连接，我们可以并行计算它们的输出（从而提高计算速度）。这种能力是深度学习近年来成功的主要原因之一。
- en: 'In classical NNs (that is, NNs before DL, when they were just one of many ML
    algorithms), the primary type of layer is the **fully connected** (**FC**) layer.
    In this layer, every unit receives weighted input from all the components of the
    input vector, **x**. Let''s assume that the size of the input vector is *m* and
    that the FC layer has *n* units and an activation function *f*, which is the same
    for all the units. Each of the *n* units will have *m* weights: one for each of
    the *m* inputs. The following is a formula we can use for the output of a single
    unit *j* of an FC layer. It''s the same as the formula we defined in the *Neurons*
    section, but we''ll include the unit index here:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典神经网络（即深度学习之前的神经网络，当时它们仅是众多机器学习算法之一）中，主要的层类型是 **全连接**（**FC**）层。在该层中，每个单元都从输入向量
    **x** 的所有组件接收加权输入。假设输入向量的大小为 *m*，而该全连接层有 *n* 个单元，并且每个单元都有相同的激活函数 *f*。每个 *n* 个单元将有
    *m* 个权重：每个输入对应一个权重。以下是我们可以用于单个全连接层单元 *j* 输出的公式。它与我们在 *神经元* 部分定义的公式相同，但这里我们包括了单元索引：
- en: '![](img/3d58e01e-f4a0-4d06-85ec-4bab1f5954c5.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d58e01e-f4a0-4d06-85ec-4bab1f5954c5.png)'
- en: 'Here, *w[ij]* is the weight between the *j*-th layer unit and the *i*-th input
    component. We can represent the weights connecting the input vector to the units
    as an *m×n* matrix **W**. Each matrix column represents the weight vector of all
    the inputs to one layer unit. In this case, the output vector of the layer is
    the result of matrix-vector multiplication. However, we can also combine multiple
    input samples, **x***[i]*, in an input matrix (or **batch**) **X**, which will
    be passed through the layer simultaneously. In this case, we have matrix-matrix
    multiplication and the layer output is also a matrix. The following diagram shows
    an example of an FC layer, as well as its equivalent formulas in the batch and
    single sample scenarios:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w[ij]* 是第 *j* 层单元与第 *i* 输入组件之间的权重。我们可以将连接输入向量到单元的权重表示为一个 *m×n* 的矩阵 **W**。每列矩阵表示一个层单元的所有输入的权重向量。在这种情况下，层的输出向量是矩阵-向量乘法的结果。然而，我们也可以将多个输入样本
    **x**[i] 组合成一个输入矩阵（或 **batch**）**X**，它将同时通过该层。在这种情况下，我们有矩阵-矩阵乘法，层的输出也是一个矩阵。下图展示了一个全连接（FC）层的示例，以及在批处理和单个样本场景中的等效公式：
- en: '![](img/9eca7a6e-1de6-4761-827d-8ad3056d0cd2.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9eca7a6e-1de6-4761-827d-8ad3056d0cd2.png)'
- en: An FC layer with vector/matrix inputs and outputs and its equivalent formulas
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 带有向量/矩阵输入和输出的全连接（FC）层及其等效公式
- en: We have explicitly separated the bias and input weight matrices, but in practice,
    the underlying implementation may use a shared weight matrix and append an additional
    row of 1s to the input data.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经明确分开了偏置和输入权重矩阵，但在实际应用中，底层实现可能会使用共享的权重矩阵，并向输入数据中附加一行额外的1。
- en: 'Contemporary DL is not limited to FC layers. We have many other types, such
    as convolutional, pooling, and so on. Some of the layers have trainable weights
    (FC, convolutional), while others don''t (pooling). We can also use the terms
    functions or operations interchangeably with the layer. For example, in TensorFlow
    and PyTorch, the FC layer we just described is a combination of two sequential
    operations. First, we perform the weighted sum of the weights and inputs and then
    we feed the result as an input to the activation function operation. In practice
    (that is, when working with DL libraries), the basic building block of a NN is
    not the unit but an operation that takes one or more tensors as input and outputs
    one or more tensors:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 当代深度学习（DL）不仅仅局限于全连接层（FC）。我们有许多其他类型，例如卷积层、池化层等等。某些层有可训练的权重（例如FC、卷积层），而其他层则没有（例如池化层）。我们还可以用函数或操作来替代层这个术语。例如，在TensorFlow和PyTorch中，我们刚刚描述的全连接层实际上是两个顺序操作的组合。首先，我们执行权重和输入的加权和，然后将结果作为输入馈送到激活函数操作中。在实际应用中（即在使用深度学习库时），神经网络的基本构建块不是单元，而是一个接受一个或多个张量作为输入并输出一个或多个张量的操作：
- en: '![](img/3c10ceb6-f86f-412b-b227-6761c88280d4.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c10ceb6-f86f-412b-b227-6761c88280d4.png)'
- en: A function with input and output tensors
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有输入和输出张量的函数
- en: Next, let's discuss how to combine the layer operations in a NN.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论如何在神经网络中结合层操作。
- en: NNs
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络（NNs）
- en: In the *Neurons* section, we demonstrated that a neuron (also valid for a layer)
    can only classify linearly separable classes. To overcome this limitation, we
    have to combine multiple layers in a NN. We'll define the NN as a directed graph
    of operations (or layers). The graph nodes are the operations, and the edges between
    them determine the data flow. If two operations are connected, then the output
    tensor of the first will serve as input to the second, which is determined by
    the edge direction. A NN can have multiple inputs and outputs—the input nodes
    only have outgoing edges, while the outputs only have incoming edges.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在*神经元*一节中，我们展示了一个神经元（同样适用于一个层）只能分类线性可分的类别。为了克服这一局限性，我们需要在神经网络中组合多个层。我们将神经网络定义为一个操作（或层）有向图。图中的节点是操作，而节点之间的边决定了数据流向。如果两个操作连接在一起，那么第一个操作的输出张量将作为第二个操作的输入，这由边的方向决定。一个神经网络可以有多个输入和输出——输入节点只有输出边，而输出节点只有输入边。
- en: 'Based on this definition, we can identify two main types of NNs:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个定义，我们可以识别两种主要类型的神经网络：
- en: '**Feed-forward**, which are represented by **acyclic** graphs.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈网络**，它们由**非循环**图表示。'
- en: '**Recurrent** (**RNN**), which are represented by **cyclic** graphs. The recurrence
    is temporal; the loop connection in the graph propagates the output of an operation
    at moment *t-1* and feeds it back into the network at the next moment, *t*. The
    RNN maintains an internal state, which represents a kind of summary of all the
    previous network inputs. This summary, along with the latest input, is fed to
    the RNN. The network produces some output but also updates its internal state
    and waits for the next input value. In this way, the RNN can take inputs with
    variable lengths, such as text sequences or time series.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归网络**（**RNN**），它们由**循环**图表示。递归是时间上的；图中的循环连接传播操作在时刻*t-1*的输出，并将其反馈到下一个时刻*t*的网络中。RNN维护一个内部状态，它代表所有先前网络输入的某种总结。这个总结与最新的输入一起被输入到RNN中。网络产生一些输出，同时更新其内部状态并等待下一个输入值。通过这种方式，RNN可以处理具有可变长度的输入，如文本序列或时间序列。'
- en: 'The following is an example of the two types of networks:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这两种网络类型的示例：
- en: '![](img/5659f4a0-a7f8-4824-8f4f-06316abc9594.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5659f4a0-a7f8-4824-8f4f-06316abc9594.png)'
- en: 'Left: Feed-forward network; Right: Recurrent network'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：前馈网络；右侧：递归网络
- en: 'Let''s assume that, when an operation receives input from more than one operation,
    we use the element-wise sum to combine the multiple input tensors. Then, we can
    represent the NN as a series of nested functions/operations. We''ll denote a NN
    operation with [![](img/02ac42df-f4ea-4c94-baa7-4f4784483f3e.png)], where *i* is
    some index that helps us differentiate between multiple operations. For example,
    the equivalent formula for the feed-forward network on the left is as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，当一个操作接收来自多个操作的输入时，我们使用逐元素求和将多个输入张量结合起来。然后，我们可以将神经网络表示为一系列嵌套的函数/操作。我们用[![](img/02ac42df-f4ea-4c94-baa7-4f4784483f3e.png)]表示一个神经网络操作，其中*i*是一个帮助我们区分多个操作的索引。例如，左侧前馈网络的等效公式如下：
- en: '![](img/1efd317b-048c-4ad8-b062-7b8f76925308.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1efd317b-048c-4ad8-b062-7b8f76925308.png)'
- en: 'The formula for the RNN on the right is as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 右边的RNN公式如下：
- en: '![](img/7dea3971-d074-4813-8c6e-813f7564454a.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dea3971-d074-4813-8c6e-813f7564454a.png)'
- en: 'We''ll also denote the parameters (weights) of an operation with the same index
    as the operation itself. Let''s take an FC network layer with index *l*, which
    takes its input from a previous layer with index *l-1*. The following are the
    layer formulas for a single unit and vector/matrix layer representations with
    layer indexes:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用与操作本身相同的索引来表示操作的参数（权重）。让我们考虑一个带有索引*l*的全连接（FC）网络层，它的输入来自前一层，前一层的索引为*l-1*。以下是单个单元和向量/矩阵层表示的层公式以及层索引：
- en: '![](img/fbb5a8e3-b244-48ce-b989-38f6dae8e12b.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbb5a8e3-b244-48ce-b989-38f6dae8e12b.png)'
- en: Now that we're familiar with the full NN architecture, let's discuss the different
    types of activation functions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了完整的神经网络架构，让我们来讨论一下不同类型的激活函数。
- en: Activation functions
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'Let''s discuss the different types of activation functions, starting with the
    classics:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从经典的激活函数开始，讨论不同类型的激活函数：
- en: '**Sigmoid**: Its output is bounded between 0 and 1 and can be interpreted stochastically
    as the probability of the neuron activating. Because of these properties, the
    sigmoid was the most popular activation function for a long time. However, it
    also has some less desirable properties (more on that later), which led to its
    decline in popularity. The following diagram shows the sigmoid formula, its derivative,
    and their graphs (the derivative will be useful when we discuss backpropagation):'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：它的输出被限制在0和1之间，可以通过概率的角度理解为神经元被激活的概率。由于这些特性，sigmoid曾是最流行的激活函数。然而，它也有一些不太理想的特性（稍后会详细讨论），这导致它的流行程度下降。下图展示了sigmoid的公式、它的导数及其图形（导数将在我们讨论反向传播时用到）：'
- en: '![](img/4e410206-7352-455d-80de-cf5fcf8ffcb4.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e410206-7352-455d-80de-cf5fcf8ffcb4.png)'
- en: Sigmoid activation function
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数
- en: '**Hyperbolic tangent** (**tanh**): The name speaks for itself. The principal
    difference with the sigmoid is that the tanh is in the (-1, 1) range. The following
    diagram shows the tanh formula, its derivative, and their graphs:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双曲正切**（**tanh**）：名字就已经说明了一切。与sigmoid的主要区别在于，tanh的输出范围是（-1，1）。下图展示了tanh的公式、它的导数及其图形：'
- en: '![](img/a197e86d-c3e1-4e94-9430-061a91ecc65c.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a197e86d-c3e1-4e94-9430-061a91ecc65c.png)'
- en: The hyperbolic tangent activation function
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切激活函数
- en: 'Next, let''s focus on the new kids on the block—the *LU (**LU** stands for
    **linear unit**) family of functions. We''ll start with the rectified linear unit
    (**ReLU**), which was first successfully used in 2011 (*Deep Sparse Rectifier
    Neural Networks*, [http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)).
    The following diagram shows the ReLU formula, its derivative, and their graphs:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注一下新兴的激活函数——*LU（**LU**代表**线性单元**）家族。我们将从2011年首次成功使用的**修正线性单元**（**ReLU**）开始（参见《深度稀疏修正神经网络》，[http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)）。下图展示了ReLU的公式、它的导数及其图形：
- en: '![](img/727a2c5f-91fc-45ce-8943-7d095f3cf119.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/727a2c5f-91fc-45ce-8943-7d095f3cf119.png)'
- en: ReLU activation function
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数
- en: 'As we can see, the ReLU repeats its input when **x > 0** and stays at 0 otherwise.
    This activation has several important advantages over sigmoid and tanh:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，ReLU在**x > 0**时会重复其输入，否则保持为0。这个激活函数相较于sigmoid和tanh有几个重要的优势：
- en: Its derivative helps prevent vanishing gradients (more on that in the *Weights
    initialization* section). Strictly speaking, the derivative ReLU at value 0 is
    undefined, which makes the ReLU only semi-differentiable (more information about
    this can be found at [https://en.wikipedia.org/wiki/Semi-differentiability](https://en.wikipedia.org/wiki/Semi-differentiability)).
    But in practice, it works well enough.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的导数有助于防止梯度消失（有关这一点，请参见*权重初始化*部分）。严格来说，ReLU在0值处的导数是未定义的，这使得ReLU仅为半可微函数（更多信息可以参考[https://en.wikipedia.org/wiki/Semi-differentiability](https://en.wikipedia.org/wiki/Semi-differentiability)）。但在实践中，它的表现已经足够好。
- en: 'It''s idempotent—if we pass a value through an arbitrary number of ReLU activations,
    it will not change; for example, *ReLU(2) = 2*, *ReLU(ReLU(2)) = 2*, and so on.
    This is not the case for a sigmoid, where the value is *squashed* on each pass:
    *σ(**σ(2)) = 0.707*. The following is an example of the activation of three consecutive
    sigmoid activations:'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是幂等的——如果我们通过任意次数的 ReLU 激活传递一个值，它不会改变；例如，*ReLU(2) = 2*，*ReLU(ReLU(2)) = 2*，依此类推。这与
    sigmoid 不同，后者在每次传递时会将值*压缩*：*σ(**σ(2)) = 0.707*。以下是三个连续 sigmoid 激活的例子：
- en: '![](img/9b449d44-5940-42ea-888a-5500e5cc386d.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b449d44-5940-42ea-888a-5500e5cc386d.png)'
- en: Consecutive sigmoid activations "squash" the data
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 连续的 sigmoid 激活会“压缩”数据
- en: The idempotence of ReLU makes it theoretically possible to create networks with
    more layers compared to the sigmoid.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的幂等性使得与 sigmoid 相比，它理论上可以创建更多层次的网络。
- en: It creates sparse activations—let's assume that the weights of the network are
    initialized randomly through normal distribution. Here, there is a 0.5 chance
    that the input for each ReLU unit is < 0\. Therefore, the output of about half
    of all activations will also be 0\. The sparse activations have a number of advantages,
    which we can roughly summarize as the Occam's razor in the context of NNs—it's
    better to achieve the same result with a simpler data representation than a complex
    one.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创造了稀疏的激活——假设网络的权重是通过正态分布随机初始化的。在这种情况下，每个 ReLU 单元的输入有 0.5 的概率小于 0。因此，大约一半的激活输出也将为
    0。这种稀疏激活具有许多优点，我们可以大致总结为神经网络中的奥卡姆剃刀原理——用更简单的数据表示来实现相同的结果，比复杂的表示更好。
- en: It's faster to compute in both the forward and backward passes.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前向和反向传递中计算速度更快。
- en: 'However, during training, the network weights can be updated in such a way
    that some of the ReLU units in a layer will always receive inputs smaller than
    0, which in turn will cause them to permanently output 0 as well. This phenomenon
    is known as **dying** ReLUs. To solve this, a number of ReLU modifications have
    been proposed. The following is a non-exhaustive list:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在训练过程中，网络权重可能会被更新到某些 ReLU 单元总是接收到小于 0 的输入，这反过来会导致它们永久地输出 0。这种现象被称为**死** ReLU。为了解决这个问题，提出了许多
    ReLU 的变种。以下是一个非详尽的列表：
- en: '**Leaky ReLU**: When the input is larger than 0, leaky ReLU repeats its input
    in the same way as the regular ReLU does. However, when **x < 0**, the leaky ReLU
    outputs *x* multiplied by some constant *α (0 < α < 1)*, instead of 0\. The following
    diagram shows the leaky ReLU formula, its derivative, and their graphs for α=0.2:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU**：当输入大于 0 时，Leaky ReLU 会像普通 ReLU 一样重复其输入。然而，当 **x < 0** 时，Leaky
    ReLU 输出 *x* 乘以某个常数 *α (0 < α < 1)*，而不是 0。以下图展示了 Leaky ReLU 的公式、其导数及 α=0.2 时的图形：'
- en: '![](img/1a2ece75-3d35-421d-b429-0a00a302ff39.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a2ece75-3d35-421d-b429-0a00a302ff39.png)'
- en: Leaky ReLU activation function
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 激活函数
- en: '**Parametric ReLU** (**PReLU**, *Delving Deep into Rectifiers: Surpassing Human-Level
    Performance on ImageNet Classification*, [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)):
    This activation is the same as the leaky ReLU, but the parameter α is tunable
    and is adjusted during training.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化 ReLU**（**PReLU**，*深入探讨整流函数：在 ImageNet 分类上超越人类水平的性能*， [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)）：该激活函数与
    Leaky ReLU 相同，但参数 α 是可调的，并且在训练过程中进行调整。'
- en: '**Exponential linear units** (**ELU**, *Fast and Accurate Deep Network Learning
    by Exponential Linear Units (ELUs)*, [https://arxiv.org/abs/1511.07289](https://arxiv.org/abs/1511.07289)): When
    the input is larger than 0, ELU repeats its input in the same way as ReLU does.
    However, when *x < 0*, the ELU output becomes [![](img/7facdd5f-4b7c-496b-9faa-7325f05759f9.png)],
    where α is a tunable parameter. The following diagram shows the ELU formula, its
    derivative, and their graphs for α=0.2:'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数线性单元**（**ELU**，*通过指数线性单元（ELU）实现快速准确的深度网络学习*， [https://arxiv.org/abs/1511.07289](https://arxiv.org/abs/1511.07289)）：当输入大于
    0 时，ELU 会像 ReLU 一样重复其输入。然而，当 *x < 0* 时，ELU 输出变为 [![](img/7facdd5f-4b7c-496b-9faa-7325f05759f9.png)]，其中
    α 是一个可调参数。以下图展示了 ELU 的公式、其导数及 α=0.2 时的图形：'
- en: '![](img/4dec517e-6475-4ca9-acc6-f6beecdbcdd2.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dec517e-6475-4ca9-acc6-f6beecdbcdd2.png)'
- en: ELU activation function
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 激活函数
- en: '**Scaled exponential linear units** (**SELU**, *Self-Normalizing Neural Networks*, [https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515)):
    This activation is similar to ELU, except that the output (both smaller and larger
    than 0) is scaled with an additional training parameter, λ. The SELU is part of
    a larger concept called self-normalizing NNs (SNNs), which is described in the
    source paper. The following is the SELU formula:'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放指数线性单元**（**SELU**，*自归一化神经网络*，[https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515)）：该激活函数类似于ELU，不同之处在于输出（无论大于还是小于0）通过一个附加的训练参数λ进行缩放。SELU是一个更大概念的一部分，叫做自归一化神经网络（SNNs），该概念在源论文中有所描述。以下是SELU的公式：'
- en: '![](img/dde66d94-9f7d-42ba-892c-7985c308e100.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde66d94-9f7d-42ba-892c-7985c308e100.png)'
- en: 'Finally, we''ll mention the **softmax**, which is the activation function of
    the output layer in classification problems. Let''s assume that the output of
    the final network layer is a vector, [![](img/1d8758ff-5740-4a18-9ced-3c25f706c7e2.png)],
    where each of the *n* components represents the probability that the input data
    belongs to one of *n* possible classes. Here, the softmax output for each of the
    vector components is as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将提到**softmax**，它是分类问题中输出层的激活函数。假设最终网络层的输出是一个向量，[![](img/1d8758ff-5740-4a18-9ced-3c25f706c7e2.png)]，其中每个*n*分量表示输入数据属于*n*个可能类别之一的概率。这里，每个向量分量的softmax输出如下：
- en: '![](img/9f9f8dd8-dcbe-4f3f-96fd-54a569fe8803.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9f8dd8-dcbe-4f3f-96fd-54a569fe8803.png)'
- en: 'The denominator in this formula acts as a normalizer. The softmax output has
    some important properties:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式中的分母充当归一化器。softmax输出具有一些重要性质：
- en: Every value *f(z[i])* is in the [0, 1] range.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个值*f(z[i])*都在[0, 1]范围内。
- en: 'The total sum of values of **z** is equal to 1: [![](img/8c9d0b4e-4c25-4547-b7b5-16f7f149ef92.png).]'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**z**的总和为1：[![](img/8c9d0b4e-4c25-4547-b7b5-16f7f149ef92.png).]'
- en: An added bonus (in fact, obligatory) is that the function is differentiable.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个额外的奖励（实际上，是强制性的）是该函数是可微分的。
- en: In other words, we can interpret the softmax output as a probability distribution
    of a discrete random variable. However, it also has one more subtle property.
    Before we normalize the data, we transform each vector component exponentially
    with [![](img/2ddaa9ce-bb9f-4662-b18b-864359b51d72.png)]. Let's imagine that two
    of the vector components are *z[1] = 1* and *z[2] = 2*. Here, we would have *exp(1) =
    2.7* and *exp(2) = 7.39*. As we can see, the ratios between the components before
    and after the transformation are very different—0.5 and 0.36\. In effect, the
    softmax increases the probability of the higher scores compared to lower ones.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将softmax输出解释为离散随机变量的概率分布。然而，它还有一个更为微妙的性质。在我们对数据进行归一化之前，我们对每个向量分量进行指数变换，[![](img/2ddaa9ce-bb9f-4662-b18b-864359b51d72.png)]。假设两个向量分量分别为*z[1]
    = 1* 和 *z[2] = 2*。在这里，我们得到*exp(1) = 2.7* 和 *exp(2) = 7.39*。如我们所见，变换前后各分量之间的比例差异很大——0.5和0.36。实际上，softmax通过增加较高分数的概率来相对于较低分数进行调整。
- en: In the next section, we'll shift our attention from the building blocks of the
    NN and focus on its entirety instead. More specifically, we'll demonstrate how
    NNs can approximate any function.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从神经网络的基本构件转向其整体，具体来说，我们将展示神经网络如何逼近任何函数。
- en: The universal approximation theorem
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用逼近定理
- en: The universal approximation theorem was first proved in 1989 for a NN with sigmoid
    activation functions and then in 1991 for NNs with arbitrary non-linear activation
    functions. It states that any continuous function on compact subsets of [![](img/95737d8a-6d84-4a7a-89d1-ec9b27ea62b0.png)] can
    be approximated to an arbitrary degree of accuracy by a feedforward NN with at
    least one hidden layer with a finite number of units and a non-linear activation.
    Although a NN with a single hidden layer won't perform well in many tasks, the
    theorem still tells us that there are no theoretical insurmountable limitations
    in terms of NNs. The formal proof of the theorem is too complex to be explained
    here, but we'll attempt to provide an intuitive explanation using some basic mathematics.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近定理首次在1989年为具有sigmoid激活函数的神经网络（NN）证明，1991年则为具有任意非线性激活函数的神经网络证明。该定理指出，任何在[![](img/95737d8a-6d84-4a7a-89d1-ec9b27ea62b0.png)]的紧致子集上的连续函数，都可以通过至少有一个隐藏层的前馈神经网络逼近到任意精度，且该隐藏层包含有限数量的单元并具有非线性激活。虽然一个只有单一隐藏层的神经网络在许多任务中表现不佳，但该定理依然告诉我们，神经网络在理论上没有无法逾越的限制。定理的正式证明过于复杂，无法在此解释，但我们将尝试使用一些基本数学知识提供直观的解释。
- en: The idea for the following example was inspired by Michael A. Nielsen's book *Neural
    Networks and **Deep Learning *([http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例的灵感来自Michael A. Nielsen的书籍*Neural Networks and **Deep Learning*（[http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)）。
- en: 'We''ll implement a NN that approximates the boxcar function (shown on the right
    in the following diagram), which is a simple type of step function. Since a series
    of step functions can approximate any continuous function on a compact subset
    of *R*, this will give us an idea of why the universal approximation theorem holds:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个NN，来近似箱形函数（如下图右侧所示），这是一种简单的阶跃函数类型。由于一系列阶跃函数可以近似任意连续函数在*R*的紧凑子集上的表现，这将帮助我们理解为什么普适逼近定理成立：
- en: '![](img/c6e00a40-0299-4a5d-b3f6-62404071d039.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6e00a40-0299-4a5d-b3f6-62404071d039.png)'
- en: The diagram on the left depicts continuous function approximation with a series
    of step functions, while the diagram on the right illustrates a single boxcar
    step function.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图示描绘了使用一系列阶跃函数进行的连续函数近似，而右侧的图示则说明了单一的箱形阶跃函数。
- en: 'To understand how this approximation works, we''ll start with a single unit
    with a single scalar input *x* and sigmoid activation. The following is a visualization
    of the unit and its equivalent formula:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这种近似是如何工作的，我们将从一个具有单一标量输入*x*和Sigmoid激活的单一单位开始。以下是该单位及其等效公式的可视化：
- en: '![](img/5ba8fa8c-d68c-46bf-babc-7b4593d951fd.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ba8fa8c-d68c-46bf-babc-7b4593d951fd.png)'
- en: 'In the following diagrams, we can see the graph of the formula for different
    values of *b* and *w* for inputs in the range of [-10: 10]:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '在以下图示中，我们可以看到不同值的*b*和*w*下，输入在[-10: 10]范围内的公式图形：'
- en: '![](img/79c3e582-7a3f-40ae-b2bd-b65568584257.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79c3e582-7a3f-40ae-b2bd-b65568584257.png)'
- en: The neuron output based on different values of *w* and *b*. The network input
    *x* is represented on the x axis.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不同的*w*和*b*值的神经元输出。网络输入*x*在x轴上表示。
- en: 'Upon closer inspection of the formula and the graph, we can see that the steepness
    of the sigmoid function is determined by the weight, *w*. Also, the translation
    of the function along the *x* axis is determined by the formula *t = -b/w*. Let''s
    discuss the different scenarios in the preceding diagram:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细检查公式和图形，我们可以看到，Sigmoid函数的陡峭程度由权重*w*决定。同时，函数在*x*轴上的平移由公式*t = -b/w*决定。我们来讨论一下前面图示中的不同情况：
- en: The top-left graph shows the regular sigmoid.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左上图展示了常规的Sigmoid函数。
- en: The top-right graph demonstrates that a large weight *w* amplifies the input
    *x* to a point, where the unit output resembles threshold activation.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右上图展示了一个较大的权重*w*如何将输入*x*放大到一定程度，直到单位输出类似于阈值激活。
- en: The bottom-left graph shows how the bias *b* translates the unit activation
    along the *x *axis.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左下图展示了偏置*b*如何沿*x*轴平移单位激活。
- en: The bottom-right graph shows that we can simultaneously reverse the activation
    with negative weight *w* and translate the activation along the *x *axis with
    the bias *b*.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右下图显示了我们可以通过负权重*w*同时反转激活，并通过偏置*b*沿*x*轴平移激活。
- en: 'We can intuitively see that the preceding graphs contain all the ingredients
    of the box function. We can combine the different scenarios with the help of a
    NN with one hidden layer, which contains two of the aforementioned units. The
    following diagram shows the network architecture, along with the weights and biases
    of the units, as well as the box function that''s produced by the network:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地看到，前面的图形包含了箱形函数的所有组成部分。我们可以结合不同的情况，通过一个隐藏层的NN来实现，其中包含前述的两个单元。以下图示展示了网络架构，以及单元的权重和偏置，以及网络生成的箱形函数：
- en: '![](img/617c2e78-f676-443e-af6d-f35cb87771f8.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/617c2e78-f676-443e-af6d-f35cb87771f8.png)'
- en: 'Here''s how it works:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是其工作原理：
- en: First, the top unit activates for the upper step of the function and stays active.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，顶部单元激活用于函数的上阶跃，并保持激活状态。
- en: The bottom unit activates afterward for the bottom step of the function and
    stays active. The outputs of the hidden units cancel each other out because of
    the weights in the output layer, which are the same but with opposite signs.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底部单位在函数的底部阶跃之后激活并保持激活状态。隐藏单元的输出因输出层中的权重而相互抵消，这些权重相同但符号相反。
- en: The weights of the output layer determine the height of the boxcar rectangle.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的权重决定了箱形矩形的高度。
- en: The output of this network isn't 0, but only in the (-5, 5) interval. Therefore,
    we can approximate additional boxes by adding more units to the hidden layer in
    a similar manner.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络的输出不是 0，但仅在（-5, 5）区间内。因此，我们可以通过向隐藏层添加更多单元来以类似的方式近似额外的盒子。
- en: Now that we're familiar with the structure of a NN, let's focus on the training
    process.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经熟悉了神经网络的结构，让我们集中精力研究训练过程。
- en: Training NNs
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: In this section, we'll define training a NN as the process of adjusting its
    parameters (weights) *θ* in a way that minimizes the cost function *J(θ)*. The
    cost function is some performance measurement over a training set that consists
    of multiple samples, represented as vectors. Each vector has an associated label
    (supervised learning). Most commonly, the cost function measures the difference
    between the network output and the label.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练神经网络定义为调整其参数（权重）*θ*的过程，目的是以最小化成本函数 *J(θ)*。成本函数是对训练集的某种性能度量，训练集由多个样本组成，并表示为向量。每个向量有一个关联标签（监督学习）。最常见的做法是，成本函数度量网络输出与标签之间的差异。
- en: We'll start this section with a short recap of the gradient descent optimization
    algorithm. If you're already familiar with it, you can skip this.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将以简短回顾梯度下降优化算法开始。如果你已经熟悉它，可以跳过这一部分。
- en: Gradient descent
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'For the purposes of this section, we''ll use a NN with a single regression
    output and **mean square error** (**MSE**) cost function, which is defined as
    follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个具有单一回归输出和**均方误差**（**MSE**）成本函数的神经网络（NN），该函数定义如下：
- en: '![](img/e50aabbd-24c7-4044-8422-09de5c713e04.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e50aabbd-24c7-4044-8422-09de5c713e04.png)'
- en: 'Here, we have the following:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*f[θ]*(**x**^((*i*))) is the output of the NN.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f[θ]*(**x**^((*i*))) 是神经网络的输出。'
- en: '*n* is the total number of samples in the training set.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是训练集中的样本总数。'
- en: '**x**^((*i*)) are the vectors for the training samples, where the superscript
    *i* indicates the *i*-th sample of the dataset. We use superscript because **x**^((*i*)) is
    a vector and the subscript is reserved for each of the vector components. For
    example, [![](img/6a45d5e7-f486-48b8-b2aa-69e1c8f94188.png)] is the *j*-th component
    of the *i*-th training sample.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x**^((*i*)) 是训练样本的向量，其中上标 *i* 表示数据集中的第 *i* 个样本。我们使用上标是因为 **x**^((*i*)) 是一个向量，且下标通常用于表示每个向量的分量。例如，
    [![](img/6a45d5e7-f486-48b8-b2aa-69e1c8f94188.png)] 是第 *i* 个训练样本的 *j* 组件。'
- en: '*t*^((*i*)) is the label associated with sample **x**^((*i*)).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t*^((*i*)) 是与样本 **x**^((*i*)) 关联的标签。'
- en: We shouldn't confuse the *(i)* superscript index of the *i*-th training sample
    with the *(l)* superscript, which represents the layer index of the NN. We'll
    only use the (*i*) sample index notation in the *Gradient descent* and *Cost functions*
    sections, and elsewhere we'll use the *(l)* notation for the layer index*.*
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该将 *i*（第 *i* 个训练样本的上标）与 *(l)* 上标混淆，后者代表神经网络的层索引。在 *梯度下降* 和 *成本函数* 部分中，我们只会使用
    *i* 样本索引的符号，其他地方则使用 *(l)* 符号表示层索引。
- en: 'First, gradient descent computes the derivative (gradient) of *J(θ)* with respect
    to all the network weights. The gradient gives us an indication of how *J(θ)* changes
    with respect to every weight. Then, the algorithm uses this information to update
    the weights in a way that will minimize *J(θ)* in future occurrences of the same
    input/target pairs. The goal is to gradually reach the global minimum of the cost
    function. The following is a visualization of gradient descent for MSE and a NN
    with a single weight:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，梯度下降计算 *J(θ)* 关于所有网络权重的导数（梯度）。梯度为我们提供了 *J(θ)* 如何随每个权重的变化而变化的指示。然后，算法利用这些信息更新权重，以便在未来的相同输入/目标对中最小化
    *J(θ)*。目标是逐渐达到成本函数的全局最小值。以下是均方误差（MSE）和具有单一权重的神经网络的梯度下降可视化：
- en: '![](img/7a7fada6-aac8-4594-b4da-9e415a4cf357.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a7fada6-aac8-4594-b4da-9e415a4cf357.png)'
- en: A visualization of gradient descent for MSE
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的均方误差（MSE）可视化
- en: 'Let''s go over the step-by-step execution of gradient descent:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步地回顾梯度下降的执行过程：
- en: Initialize the network weights *θ* with random values*.*
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化网络权重 *θ*。
- en: 'Repeat until the cost function falls below a certain threshold:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到成本函数降到某一阈值以下：
- en: 'Forward pass: compute the MSE *J(θ)* cost function for all the samples of the
    training set using the preceding formula.'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正向传播：使用前述公式计算所有训练集样本的均方误差（MSE）*J(θ)* 成本函数。
- en: 'Backward pass: compute the derivative of *J(θ)* with respect to all the network
    weights using the chain rule:'
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播：使用链式法则计算 *J(θ)* 关于所有网络权重的导数：
- en: '![](img/d391519c-5add-4895-b0ed-536e1ced832c.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d391519c-5add-4895-b0ed-536e1ced832c.png)'
- en: Let's analyze the derivative [![](img/cdaeaebd-a07c-4e59-96a0-d899abe8f63a.png)]. *J* is
    a function of *θ[j]* by being a function of the network output. Therefore, it
    is also a function of the NN function itself, that is, [![](img/fa690c9c-f79a-4e9f-977d-d708662e3522.png)].
    Then, by following the chain rule, we get [![](img/d806c39d-4564-477f-9d35-55fd146e442f.png)].
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析导数[![](img/cdaeaebd-a07c-4e59-96a0-d899abe8f63a.png)]。*J*是*θ[j]*的函数，因为它是网络输出的函数。因此，它也是NN函数本身的函数，即[![](img/fa690c9c-f79a-4e9f-977d-d708662e3522.png)]。然后，通过链式法则，我们得到[![](img/d806c39d-4564-477f-9d35-55fd146e442f.png)]。
- en: 'Use these derivatives to update each of the network weights:'
  id: totrans-393
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些导数来更新每个网络权重：
- en: '![](img/0f30d60f-7e45-4be6-9a2c-53cd1d36156f.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f30d60f-7e45-4be6-9a2c-53cd1d36156f.png)'
- en: Here, η is the learning rate.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，η是学习率。
- en: 'Gradient descent updates the weights by accumulating the error across all the
    training samples. In practice, we would use two of its modifications:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降通过积累所有训练样本中的误差来更新权重。在实践中，我们会使用它的两个修改：
- en: '**Stochastic** (**or online**) **gradient descent**(**SGD**) updates the weights
    after every training sample.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机**（或在线）**梯度下降**（**SGD**）在每个训练样本后更新权重。'
- en: '**Mini-batch gradient descent **accumulates the error for every *n* samples
    (one mini-batch)and performs one weight update.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**累积每个*n*个样本（一个小批量）的误差，并执行一次权重更新。'
- en: Next, let's discuss the different cost functions we can use with SGD.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论与SGD一起使用的不同代价函数。
- en: Cost functions
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代价函数
- en: 'Besides MSE, there are also a few other loss functions that are commonly used
    in regression problems. The following is a non-exhaustive list:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MSE之外，在回归问题中通常还会使用其他几种损失函数。以下是非穷尽列表：
- en: '**Mean absolute error** (**MAE**) is the mean of the absolute differences (not
    squared) between the network output and the target. The following is the MAE graph
    and formula:'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是网络输出与目标之间绝对差异（未平方）的均值。以下是MAE的图表和公式：'
- en: '![](img/32429f23-4286-46c2-8d93-85eb26c7c50d.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32429f23-4286-46c2-8d93-85eb26c7c50d.png)'
- en: One advantage of MAE over MSE is that it deals with outlier samples better.
    With MSE, if the difference of a sample is [![](img/7e8d1ad7-eed6-4166-a47f-686c534cd901.png)],
    it increases exponentially (because of the square). We'll get an outsized weight
    of this sample compared to the others, which may skew the results. With MAE, the
    difference is not exponential and this issue is less pronounced.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: MAE相对于MSE的一个优势是更好地处理异常样本。使用MSE时，如果一个样本的差异是[![](img/7e8d1ad7-eed6-4166-a47f-686c534cd901.png)],
    它会呈指数增长（因为是平方的）。这会导致这个样本的权重相对于其他样本过大，可能会使结果产生偏差。使用MAE时，差异不是指数增长，这个问题较不明显。
- en: On the other hand, the MAE gradient will have the same value until we reach
    the minimum, where it will become 0 immediately. This makes it harder for the
    algorithm to anticipate how close the cost function minimum is. Compare this to
    MSE, where the slope gradually decreases as we get close to the cost minimum.
    This makes MSE easier to optimize. In conclusion, unless the training data is
    corrupted with outliers, it is usually recommended to use MSE over MAE.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MAE的梯度在达到最小值之前将保持相同值，然后立即变为0。这使得算法难以预测成本函数最小值的接近程度。相比之下，MSE的斜率随着接近成本最小值而逐渐减小，这使得MSE更容易优化。总之，除非训练数据受到异常值的干扰，通常建议使用MSE而不是MAE。
- en: '**Huber loss** attempts to fix the problems of both MAE and MSE by combining
    their properties. In short, when the absolute difference between the output and
    the target data falls below the value of a fixed parameter, δ, the Huber loss
    behaves like MSE. Conversely, when the difference is greater than δ, it resembles
    MAE. In this way, it is less sensitive to outliers (when the difference is big)
    and at the same time, the minimum of the function is properly differentiable.
    The following is the Huber loss graph for three values of δ and its formula for
    a single training sample, which reflects its dualistic nature:'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Huber损失**试图修正MAE和MSE的问题，通过结合它们的特性。简而言之，当输出与目标数据之间的绝对差小于固定参数δ时，Huber损失的行为类似于MSE。相反，当差值大于δ时，它类似于MAE。这样，它对异常值（差值较大时）不太敏感，并且函数的最小值适当可微。以下是三个δ值的Huber损失图表及单个训练样本的公式，反映了它的二元性质：'
- en: '![](img/9e3e7435-9aaa-40b2-a4c7-503bfa747d7a.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e3e7435-9aaa-40b2-a4c7-503bfa747d7a.png)'
- en: Huber loss
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: Huber损失
- en: 'Next, let''s focus on cost functions for classification problems. The following
    is a non-exhaustive list:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们专注于分类问题的代价函数。以下是一个非详尽的列表：
- en: '**Cross-entropy** loss: We have our work cut out for us here as we already
    defined cross-entropy in the *Information theory* section. This loss is usually
    applied over the output of a softmax function. The two work very well together.
    First, the softmax converts the network output into a probability distribution.
    Then, cross-entropy measures the difference between the network output (Q) and
    the true distribution (P), which is provided as a training label. Another nice
    property is that the derivative of *H(P, Q[softmax])* is quite straightforward
    (although the computation isn''t):'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉熵**损失：我们在这里有不少工作要做，因为我们已经在*信息理论*部分定义了交叉熵。这个损失通常应用于 softmax 函数的输出。两者非常配合。首先，softmax
    将网络输出转换为概率分布。然后，交叉熵衡量网络输出（Q）与真实分布（P）之间的差异，后者作为训练标签提供。另一个优点是，*H(P, Q[softmax])*
    的导数相当直接（尽管计算过程并不简单）：'
- en: '![](img/d38c6fac-8614-4e00-92a4-b5ca8514070f.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d38c6fac-8614-4e00-92a4-b5ca8514070f.png)'
- en: Here, *x^((i))/t^((i))* is the *i*-th input/label training pair.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x^((i))/t^((i))* 是第 *i* 个输入/标签训练对。
- en: '**KL Divergence** loss: Like cross-entropy loss, we already did the grunt work
    in the *Information theory* section, where we derived the relationship between
    KL divergence and cross-entropy loss. From their relationship, we can state that
    if we use either of the two as a loss function, we implicitly use the other one
    as well.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KL 散度**损失：像交叉熵损失一样，我们已经在*信息理论*部分完成了繁重的工作，在那里我们推导了 KL 散度与交叉熵损失之间的关系。从它们的关系中，我们可以得出结论，如果我们使用其中一个作为损失函数，我们隐式地也在使用另一个。'
- en: Sometimes, we may encounter the terms loss function and cost function being
    used interchangeably. It is usually accepted that they differ slightly. We'll
    refer to the loss function as the difference between the network output and target
    data for a **single** sample of the training set. The cost function is the same
    thing but is averaged (or summed) over multiple samples (batch) of the training
    set.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们可能会遇到“损失函数”和“代价函数”这两个术语被交替使用的情况。通常认为它们之间有些微小的区别。我们将损失函数定义为网络输出和目标数据之间的差异，针对训练集中的**单个**样本。代价函数则是同样的概念，不过它是对训练集多个样本（批次）进行平均（或求和）后的结果。
- en: Now that we have looked at different cost functions, let's focus on propagating
    the error gradient through the network with backpropagation.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了不同的代价函数，接下来让我们专注于通过反向传播将误差梯度传播到网络中。
- en: Backpropagation
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'In this section, we''ll discuss how to update the network weights in order
    to minimize the cost function. As we demonstrated in the *Gradient descent* section,
    this means finding the derivative of the cost function *J(θ)* with respect to
    each network weight*. *We already took a step in this direction with the help
    of the chain rule:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何更新网络权重，以最小化代价函数。正如我们在*梯度下降*部分演示的那样，这意味着要找出代价函数 *J(θ)* 关于每个网络权重的导数*。*我们已经借助链式法则朝着这个方向迈出了第一步：
- en: '![](img/2e6ef655-6bb5-4a77-92ab-f54faf307fae.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e6ef655-6bb5-4a77-92ab-f54faf307fae.png)'
- en: 'Here, *f(θ)* is the network output and *θ[j]* is the *j*-th network weight*.*
    In this section, we''ll push the envelope further and we''ll learn how to derive
    the NN function itself for all the network weights (hint: chain rule). We''ll
    do this by propagating the error gradient backward through the network (hence
    the name). Let''s start with a few assumptions:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f(θ)* 是网络输出，而 *θ[j]* 是第 *j* 个网络权重*。*在本节中，我们将进一步扩展，我们将学习如何推导出所有网络权重的神经网络（NN）函数（提示：链式法则）。我们将通过将误差梯度反向传播通过网络来实现这一点（因此得名）。让我们从一些假设开始：
- en: For the sake of simplicity, we'll work with a sequential feed-forward NN. Sequential
    means that each layer takes input from the preceding layer and sends its output
    to the following layer.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了简化起见，我们将使用顺序前馈神经网络。顺序意味着每一层从前一层获取输入，并将输出传递给后一层。
- en: We'll define *w[ij]* as the weight between the *i*-th neuron of layer *l* and
    the *j-*th neuron of layer *l+1. *In other words, we use subscripts *i* and *j*,
    where the element with subscript *i* belongs to the preceding layer, which is
    the layer containing the element with subscript *j*. In a multi-layer network, *l* and *l+1* can
    be any two consecutive layers, including input, hidden, and output layers.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义 *w[ij]* 为层 *l* 中第 *i* 个神经元和层 *l+1* 中第 *j* 个神经元之间的权重。换句话说，我们使用下标 *i* 和 *j*，其中下标为
    *i* 的元素属于前一层，即包含下标为 *j* 元素的层。在一个多层网络中，*l* 和 *l+1* 可以是任何两个连续的层，包括输入层、隐藏层和输出层。
- en: We'll denote the output of the *i*-th unit of layer l with [![](img/47df128a-d1f0-403d-9aa3-b8d5351f475b.png)]and the
    output of the *j*-th unit of layer l+1 with [![](img/af8df4bf-f929-49c3-94e8-738416ee0df6.png)].
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用 [![](img/47df128a-d1f0-403d-9aa3-b8d5351f475b.png)] 来表示层 *l* 中第 *i* 个单元的输出，用
    [![](img/af8df4bf-f929-49c3-94e8-738416ee0df6.png)] 来表示层 *l+1* 中第 *j* 个单元的输出。
- en: We'll denote the input to the activation function (that is, the weighted sum
    of the inputs before activation) of unit *j* of layer *l* with *a[j]^((l))*.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用 *a[j]^((l))* 来表示层 *l* 中单元 *j* 的激活函数输入（即激活前输入的加权和）。
- en: 'The following diagram shows all the notations we introduced:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我们引入的所有符号：
- en: '![](img/ad7c70ac-37b4-4ce6-b1e5-7cc5923cebc0.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad7c70ac-37b4-4ce6-b1e5-7cc5923cebc0.png)'
- en: Here, layer l represents the input, layer l+1 represents the output, and *w[ij]* connects
    the *y[i]^([(l)])* activation in layer l to the inputs of the j-th neuron of layer
    l+1
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，层 *l* 代表输入，层 *l+1* 代表输出，*w[ij]* 将层 *l* 中的 *y[i]^([(l)])* 激活值与层 *l+1* 中第
    *j* 个神经元的输入连接起来。
- en: 'Armed with this great knowledge, let''s get down to business:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有了这些宝贵的知识，让我们开始实际操作：
- en: 'First, we''ll assume that *l* and *l+1* are the second-to-last and the last
    (output) network layers, respectively. Knowing this, the derivative of J with
    respect to *w[ij]* is as follows:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们假设 *l* 和 *l+1* 分别是倒数第二层和最后一层（输出层）。知道这一点后，J 相对于 *w[ij]* 的导数如下：
- en: '![](img/2b12510e-19c8-4740-82c3-45b27e4270a0.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b12510e-19c8-4740-82c3-45b27e4270a0.png)'
- en: 'Let''s focus on [![](img/3c0fe6b4-4c70-4de4-9a24-c723eafaaea8.png)]. Here,
    we compute the partial derivative of the weighted sum of the output of layer *l* with
    respect to one of the weights, *w[ij]*. As we discussed in the *Differential calculus* section,
    in partial derivatives, we''ll consider all the function parameters except *w[ij]* constants.
    When we derive *a[j]^([(l+1)])*, they all become 0 and we''re only left with [![](img/638298cb-bba0-48fe-923d-28fd64be27e3.png)].
    Therefore, we get the following:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们关注 [![](img/3c0fe6b4-4c70-4de4-9a24-c723eafaaea8.png)]。在这里，我们计算层 *l* 输出的加权和相对于其中一个权重
    *w[ij]* 的偏导数。正如我们在 *微积分* 部分讨论的那样，在偏导数中，我们会将除 *w[ij]* 之外的所有函数参数视为常数。当我们对 *a[j]^([(l+1)])*
    求导时，其他部分都会变成0，我们只剩下 [![](img/638298cb-bba0-48fe-923d-28fd64be27e3.png)]。因此，我们得到以下结果：
- en: '![](img/687176f6-b628-434b-9438-8f8617082829.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/687176f6-b628-434b-9438-8f8617082829.png)'
- en: 'The formula from point 1 holds for any two consecutive hidden layers, *l* and
    *l+1*, of the network. We know that [![](img/0faa558f-675d-4698-9454-12d5360204f9.png)], and
    we also know that [![](img/6f4c364f-79c1-45fe-99c9-bc175617827e.png)] is the derivative
    of the activation function, which we can calculate (see the *Activation functions*
    section). All we need to do is calculate the derivative [![](img/bd64bfc9-cd1a-479d-98b5-9b5a61f76faf.png)] (recall
    that, here, *l+1* is some hidden layer). Let''s note that this is the derivative
    of the error with respect to the activation function in layer *l+1*. We can now
    calculate all the derivatives, starting from the last layer and moving backward,
    because the following apply:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第1点中的公式适用于网络中任何两个连续的隐藏层 *l* 和 *l+1*。我们知道 [![](img/0faa558f-675d-4698-9454-12d5360204f9.png)]，并且我们也知道
    [![](img/6f4c364f-79c1-45fe-99c9-bc175617827e.png)] 是激活函数的导数，我们可以计算出来（见 *激活函数*
    部分）。我们需要做的就是计算导数 [![](img/bd64bfc9-cd1a-479d-98b5-9b5a61f76faf.png)]（回想一下，这里 *l+1*
    是某个隐藏层）。需要注意的是，这是相对于层 *l+1* 中激活函数的误差导数。现在我们可以从最后一层开始，向后计算所有的导数，因为以下情况适用：
- en: We can calculate this derivative for the last layer.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以计算最后一层的这个导数。
- en: We have a formula that allows us to calculate the derivative for one layer,
    assuming that we can calculate the derivative for the next.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个公式，可以计算一个层的导数，前提是我们可以计算下一个层的导数。
- en: 'With these points in mind, we get the following equation by applying the chain
    rule:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 牢记这些点后，我们通过应用链式法则得到以下方程：
- en: '![](img/9fa73368-1629-499f-aacf-6e8f707914b8.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fa73368-1629-499f-aacf-6e8f707914b8.png)'
- en: The sum over *j* reflects the fact that, in the feedforward part of the network,
    the output* ![](img/178b40b8-5edb-4888-aea4-61ab1d0bdf82.png) *is fed to all the
    neurons in layer *l+1*. Therefore, they all contribute to *y[i]^([(l)])*when the
    error is propagated backward.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对*j*的求和反映了在网络的前馈部分，输出* ![](img/178b40b8-5edb-4888-aea4-61ab1d0bdf82.png) *被馈送到所有*l+1*层的神经元。因此，它们都在误差反向传播时对*y[i]^([(l)])*作出贡献。
- en: Once again, we can calculate [![](img/3e0a68a5-2579-4d38-8687-6634162d3520.png)].
    Following the same logic that we followed in *step 3*, we can compute that [![](img/bc64c3d6-ee8a-4c98-b0a3-28c92ea47bde.png)].
    Therefore, once we know that [![](img/a35b6e03-6a0e-4b88-abeb-f5a509057bd0.png)] , we
    can calculate [![](img/61ecb327-6a60-4342-bb49-e1bdb135e3d5.png)]. Since we can
    calculate ![](img/85b5edb6-75f7-467d-80fb-a18b93c3df98.png) for the last layer,
    we can move backward and calculate [![](img/3c0ace54-6609-4270-9270-3fa338a59359.png)] for
    any layer, and therefore [![](img/944cd03c-040e-45c8-9bb1-1547d1008bd7.png)] for
    any layer.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以计算[![](img/3e0a68a5-2579-4d38-8687-6634162d3520.png)]。按照我们在*步骤3*中遵循的相同逻辑，我们可以计算出[![](img/bc64c3d6-ee8a-4c98-b0a3-28c92ea47bde.png)]。因此，一旦我们知道[![](img/a35b6e03-6a0e-4b88-abeb-f5a509057bd0.png)]，我们就可以计算出[![](img/61ecb327-6a60-4342-bb49-e1bdb135e3d5.png)]。由于我们可以计算出最后一层的![](img/85b5edb6-75f7-467d-80fb-a18b93c3df98.png)，我们可以向后计算并计算出任何一层的[![](img/3c0ace54-6609-4270-9270-3fa338a59359.png)]，因此对于任何一层，计算出[![](img/944cd03c-040e-45c8-9bb1-1547d1008bd7.png)]。
- en: 'To summarize, let''s say we have a sequence of layers where the following applies:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结一下，假设我们有一系列层，其中如下所示：
- en: '![](img/fe64b72a-110b-4725-9b1e-36259211544c.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe64b72a-110b-4725-9b1e-36259211544c.png)'
- en: 'Here, we have the following fundamental equations:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下基本方程：
- en: '![](img/12977aa0-3f9f-4e34-80ec-35760bb18f06.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12977aa0-3f9f-4e34-80ec-35760bb18f06.png)'
- en: '![](img/9fa73368-1629-499f-aacf-6e8f707914b8.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fa73368-1629-499f-aacf-6e8f707914b8.png)'
- en: By using these two equations, we can calculate the derivatives for the cost
    with respect to each layer.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这两个方程，我们可以计算每一层关于成本的导数。
- en: 'If we set ![](img/cdeb3c79-172c-4362-bd0d-0a524fb30cbe.png) , then δ*[j]^([(l+1)])* represents
    the variation in cost with respect to the activation value, and we can think of
    δ*[j]^([(l+1)])* as the error at neuron *y[j]^([(l+1)])*. We can rewrite these
    equations as follows:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们设置![](img/cdeb3c79-172c-4362-bd0d-0a524fb30cbe.png)，那么δ*[j]^([(l+1)])*表示相对于激活值的成本变化，我们可以将δ*[j]^([(l+1)])*视为神经元*y[j]^([(l+1)])*的误差。我们可以将这些方程改写如下：
- en: '![](img/a622a08e-7657-4b31-8480-de8e99875315.png)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a622a08e-7657-4b31-8480-de8e99875315.png)'
- en: 'Following this, we can write the following equation:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以写出以下方程：
- en: '![](img/19ad376c-3380-41ba-8234-1c7ba5b8bdbe.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19ad376c-3380-41ba-8234-1c7ba5b8bdbe.png)'
- en: These two equations provide us with an alternative view of backpropagation since
    there is a variation in cost with respect to the activation value. They provide
    us with a way to calculate the variation for any layer *l* once we know the variation
    for the following layer, *l+1*.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个方程为我们提供了反向传播的另一种视角，因为成本相对于激活值的变化。它们为我们提供了一种方法，可以在知道下一层*l+1*的变化后，计算任何一层*l*的变化。
- en: 'We can combine these equations to show the following:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将这些方程合并，得到以下结果：
- en: '![](img/6e2b1ad1-9a70-419e-acb7-ad1b25411f4b.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e2b1ad1-9a70-419e-acb7-ad1b25411f4b.png)'
- en: 'The updated rule for the weights of each layer is given by the following equation:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一层的权重更新规则由以下方程给出：
- en: '![](img/b266473d-8d0e-4086-9f57-69b057f810d5.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b266473d-8d0e-4086-9f57-69b057f810d5.png)'
- en: 'Now that we''re familiar with backpropagation, let''s discuss another component
    of the training process: weight initialization.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了反向传播，让我们讨论训练过程中的另一个组件：权重初始化。
- en: Weight initialization
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重初始化
- en: One key component of training deep networks is the random weight initialization.
    This matters because some activation functions, such as sigmoid and ReLU, produce
    meaningful outputs and gradients if their inputs are within a certain range.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度网络的一个关键组件是随机权重初始化。这很重要，因为一些激活函数，如sigmoid和ReLU，当其输入在某个范围内时，能够产生有意义的输出和梯度。
- en: A famous example is the vanishing gradient problem. To understand it, let's
    take an FC layer with sigmoid activation (this example is also valid for tanh).
    We saw the sigmoid's graph (blue) and its derivative (green) in the *Activation
    functions* section. If the weighted sum of the inputs falls roughly outside the (-5,
    5) range, the sigmoid activation will be effectively 0 or 1\. In essence, it saturates.
    This is visible during the backward pass where we derive the sigmoid (the formula
    is *σ' = **σ(1 - σ)*). We can see that the derivative is larger than 0 within
    the same (-5, 5) range of the input. Therefore, whatever error we try to propagate
    back to the previous layers, it will vanish if the activation doesn't fall within
    this range (hence the name).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的例子是梯度消失问题。为了理解它，我们来看看一个使用 sigmoid 激活的全连接层（这个例子对于 tanh 也适用）。我们已经在 *激活函数*
    部分看到过 sigmoid 的图形（蓝色）和其导数（绿色）。如果输入的加权和大致超出 (-5, 5) 范围，sigmoid 激活会有效地变为 0 或 1。实质上，它会饱和。在反向传播过程中，我们通过对
    sigmoid 进行求导（公式为 *σ' = **σ(1 - σ)*）可以看到这一点。我们可以看到，在同一个 (-5, 5) 输入范围内，导数大于 0。因此，无论我们试图将多少误差传播回上一层，如果激活值不在此范围内，误差将消失（因此称之为梯度消失）。
- en: Besides the tight meaningful range of the sigmoid derivative, let's note that,
    even under the best conditions, its maximum value is 0.25\. When we propagate
    the gradient through the sigmoid derivative, it will be four times smaller at
    best once it passes through. Because of this, the gradient may vanish in just
    a few layers, even if we don't fall outside the desired range. This is one of
    the major disadvantages of sigmoid over the *LU family of functions, where the
    gradient is 1 in most cases.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 sigmoid 导数的紧致有效范围外，值得注意的是，即使在最佳条件下，其最大值为 0.25。当我们通过 sigmoid 导数传播梯度时，经过传播后，其值最大也会缩小四倍。正因为如此，梯度可能在仅仅几层后就会消失，即使我们没有超出所需范围。这是
    sigmoid 相对于 *LU 函数族* 的主要缺点之一，在后者中，梯度在大多数情况下为 1。
- en: 'One way to solve this problem is to use *LU activations. But even so, it still
    makes sense to use better weight initialization since it can speed up the training
    process. One popular technique is the Xavier/Glorot initializer (often found under
    either of the two names: [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)).
    In short, this technique takes the number of input and output connections of the
    unit into account. There are two variations:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用 *LU 激活函数*。但即便如此，使用更好的权重初始化方法仍然是有意义的，因为它可以加速训练过程。一个常见的技术是 Xavier/Glorot
    初始化器（通常以以下两种名称之一出现：[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)）。简而言之，该技术考虑了单位的输入和输出连接数。它有两种变体：
- en: '**Xavier uniform initializer**, which draws samples from a uniform distribution
    in the range [-a, a]. The parameter, a, is defined as follows:'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier 均匀初始化器**，它从范围为[-a, a]的均匀分布中抽取样本。参数 a 定义如下：'
- en: '![](img/1d4472a0-1575-4512-9897-6b8d20a85b7d.png)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d4472a0-1575-4512-9897-6b8d20a85b7d.png)'
- en: Here, *n[in]* and *n[out]* are the number of inputs and outputs, respectively
    (that is, the number of units that send their output to the current unit and the
    number of units the current unit sends its output to).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n[in]* 和 *n[out]* 分别是输入和输出的数量（即将输出发送到当前单元的单元数，以及当前单元将输出发送到的单元数）。
- en: '**Xavier normal initializer**, which draws samples from a normal distribution
    (see the *Probability distributions* section) with a mean of 0 and variance as
    follows:'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier 正态初始化器**，它从正态分布中抽取样本（参见 *概率分布* 部分），均值为0，方差定义如下：'
- en: '![](img/3391fd8c-4eb8-4a03-a3f3-a554a21786e3.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3391fd8c-4eb8-4a03-a3f3-a554a21786e3.png)'
- en: 'The Xavier/Glorot initialization is recommended for sigmoid or tanh activations
    functions. The paper *Delving Deep into Rectifiers: Surpassing Human-Level Performance
    on ImageNet Classification* ([https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852))
    proposes a similar technique that''s better suited for ReLU activations. Again,
    there are two variations:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐在 sigmoid 或 tanh 激活函数中使用 Xavier/Glorot 初始化。论文 *深入探讨激活函数：超越 ImageNet 分类中的人类水平表现*（[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)）提出了一种类似的技术，更适合用于
    ReLU 激活函数。同样，它也有两种变体：
- en: '**He uniform initializer**, which draws samples from a uniform distribution
    in the range [-a, a]. The parameter, a, is defined as follows:'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**He 均匀初始化器**，它从范围为[-a, a]的均匀分布中抽取样本。参数 a 定义如下：'
- en: '![](img/f7816ef0-6745-4756-b693-71c54dc13181.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7816ef0-6745-4756-b693-71c54dc13181.png)'
- en: '**He normal initializer**, which draws samples from a normal distribution with
    a mean of 0 and variance as follows:'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**He 正态初始化器**，它从一个均值为 0 的正态分布中抽取样本，方差如下：'
- en: '![](img/798e9c12-982e-491f-b9e7-16d7a31ea90c.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](img/798e9c12-982e-491f-b9e7-16d7a31ea90c.png)'
- en: The ReLU output is always 0 when the input is negative. If we assume that the
    initial inputs of the ReLU units are centered around 0, half of them will produce
    0 outputs. The He initializer compensates this by increasing the variance twice,
    compared to the Xavier initialization.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的输出在输入为负时始终为 0。如果我们假设 ReLU 单元的初始输入围绕 0 分布，那么其中一半的输出将是 0。He 初始化器通过将方差增加两倍来补偿这个问题，相较于
    Xavier 初始化。
- en: In the next section, we'll discuss some improvements in the weight update rule
    over the standard SGD.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论权重更新规则相较于标准 SGD 的一些改进。
- en: SGD improvements
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SGD 改进
- en: We'll start with **momentum**, which extends vanilla SGD by adjusting the current
    weight update with the values of the previous weight updates. That is, if the
    weight update at step *t-1* was big, it will also increase the weight update of
    step *t*. We can explain momentum with an analogy. Think of the loss function
    surface as the surface of a hill. Now, imagine that we are holding a ball at the
    top of the hill (maximum). If we drop the ball, thanks to the Earth's gravity,
    it will start rolling toward the bottom of the hill (minimum). The more distance
    it travels, the more its speed will increase. In other words, it will gain momentum
    (hence the name of the optimization).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从**动量**开始，它通过将当前权重更新与前一次权重更新的值结合来扩展基本 SGD。也就是说，如果 *t-1* 步骤的权重更新较大，它也会增加 *t*
    步骤的权重更新。我们可以用一个类比来解释动量。想象一下，损失函数表面就像是一座山丘的表面。现在，假设我们把一个球放在山顶（最大值）上。如果我们把球放下，由于地球引力，它会开始向山底（最小值）滚动。它滚动的距离越长，速度就会越快。换句话说，它会获得动量（这也是优化名称的由来）。
- en: 'Now, let''s look at how to implement momentum in the weight update rule. Recall
    the update rule that we introduced in the *Gradient descent* section, that is, [![](img/2547d3e8-4eab-46b2-be9f-56163d2f81b2.png)].
    Let''s assume that we are at step *t* of the training process:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一下如何在权重更新规则中实现动量。回顾我们在*梯度下降*部分中介绍的更新规则，即 [![](img/2547d3e8-4eab-46b2-be9f-56163d2f81b2.png)]。假设我们正处于训练过程的第
    *t* 步：
- en: 'First, we''ll calculate the current weight update value *v[t]* by also including
    the **velocity** of the previous update *v[t-1]*:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过考虑前一次更新的**速度** *v[t-1]*，计算当前的权重更新值 *v[t]*：
- en: '![](img/ce482d72-124d-4535-93d0-de41f1f1c0c6.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce482d72-124d-4535-93d0-de41f1f1c0c6.png)'
- en: Here, μ is a hyperparameter in the [0:1] range called the momentum rate. *v[t]*
    is initialized as 0 during the first iteration.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，μ 是一个范围在 [0:1] 之间的超参数，称为动量率。*v[t]* 在第一次迭代时初始化为 0。
- en: 'Then, we perform the actual weight update:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们执行实际的权重更新：
- en: '![](img/1716068c-45f5-4605-b5c7-3c9b42665c54.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1716068c-45f5-4605-b5c7-3c9b42665c54.png)'
- en: 'An improvement over the basic momentum is the **Nesterov momentum**. It relies
    on the observation that the momentum from step *t-1* may not reflect the conditions
    at step *t*. For example, let''s say that the gradient at *t-1* is steep and therefore
    the momentum is high. However, after the *t-1* weight update, we actually reach
    the cost function minimum and require only a minor weight update at *t*. Despite
    that, we''ll still get the large momentum from *t-1*, which may lead the adjusted
    weight to jump over the minimum. Nesterov momentum proposes a change in the way
    we compute the velocity of the weight update. We''ll calculate *v[t]* based on
    the gradient of the cost function that''s computed by the potential future value
    of the weight *θ[j]*. The following is the updated velocity formula:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于基本动量，**Nesterov 动量**提出了一种改进。它依赖于这样一个观察：来自 *t-1* 步骤的动量可能无法反映 *t* 步骤时的条件。例如，假设在
    *t-1* 时刻的梯度很陡峭，因此动量很大。然而，在 *t-1* 的权重更新后，我们实际上达到了成本函数的最小值，并且在 *t* 时刻只需要进行一次小的权重更新。尽管如此，我们仍然会得到来自
    *t-1* 的大动量，这可能会导致调整后的权重跳过最小值。Nesterov 动量提出了一种新的计算权重更新速度的方法。我们将基于成本函数梯度计算 *v[t]*，该梯度是通过未来潜在的权重值
    *θ[j]* 计算得出的。以下是更新后的速度公式：
- en: '![](img/03baa9fb-5d82-45f6-8740-095ed92c6f88.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03baa9fb-5d82-45f6-8740-095ed92c6f88.png)'
- en: If the momentum at *t-1* is incorrect with respect to *t*, the modified gradient
    will compensate for this error in the same update step.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 *t-1* 时刻的动量与 *t* 时刻不匹配，修改后的梯度将在同一次更新步骤中弥补这个误差。
- en: 'Next, let''s discuss the Adam adaptive learning rate algorithm (*Adam: A Method
    for Stochastic Optimization*, [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)).
    It calculates individual and adaptive learning rates for every weight based on
    previous weight updates (momentum). Let''s see how that works:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，让我们讨论Adam自适应学习率算法（*Adam: A Method for Stochastic Optimization*，[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)）。它根据之前的权重更新（动量）为每个权重计算个别的自适应学习率。我们来看看它是如何工作的：'
- en: 'First, we need to compute the first moment (or mean) and the second moment
    (or variance) of the gradient:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要计算梯度的第一矩（或均值）和第二矩（或方差）：
- en: '![](img/274ca96c-8518-4c6e-b03e-b1a12f9d7993.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![](img/274ca96c-8518-4c6e-b03e-b1a12f9d7993.png)'
- en: Here, β[1] and β[2] are hyperparameters with default values of 0.9 and 0.999,
    respectively. *m[t]* and *v[t]* act as moving-average values of the gradient,
    somewhat similar to momentum. They are initialized with 0 during the first iteration.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，β[1]和β[2]是超参数，默认值分别为0.9和0.999。*m[t]*和*v[t]*充当梯度的移动平均值，类似于动量。它们在第一次迭代时初始化为0。
- en: 'Since *m[t]* and *v[t]* start as 0, they will have a bias toward 0 in the initial
    phase of the training. For example, let''s say that, at *t=1, β1 = 0.9* and ![](img/3d8d6c89-32c0-4cb3-b1f4-92cd4aec489a.png) =
    10\. Here, *m1 = 0.9 * 0 + (1 - 0.9) * 10 = 1*, which is a lot less than the actual
    gradient of 10\. To compensate for this bias, we''ll compute the bias-corrected
    versions of *m[t]* and *v[t]*:'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于*m[t]*和*v[t]*初始值为0，它们在训练的初期阶段会有偏向于0的偏差。例如，假设在*t=1, β1=0.9*，且![](img/3d8d6c89-32c0-4cb3-b1f4-92cd4aec489a.png)=10。那么，*m1
    = 0.9 * 0 + (1 - 0.9) * 10 = 1*，这比实际的梯度10要小得多。为了补偿这个偏差，我们将计算*m[t]*和*v[t]*的偏差校正版本：
- en: '![](img/8f26742b-e907-4cd6-8414-31dd3745e614.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f26742b-e907-4cd6-8414-31dd3745e614.png)'
- en: 'Finally, we need to perform the weight update using the following formula:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要使用以下公式进行权重更新：
- en: '![](img/f47df8a1-7ac4-49a8-8a56-a0337c443779.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f47df8a1-7ac4-49a8-8a56-a0337c443779.png)'
- en: Here, η is the learning rate and ε is some small value to prevent division by
    0.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，η是学习率，ε是一个小值，用来防止除以0。
- en: Summary
  id: totrans-492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter with a tutorial on the mathematical apparatus that forms
    the foundation of NNs. Then, we recapped on NNs and their architecture. Along
    the way, we tried to explicitly connect the mathematical concepts with the various
    components of the NNs. We paid special attention to the various types of activation
    functions. Finally, we took a comprehensive look at the NN training process. We
    discussed gradient descent, cost functions, backpropagation, weights initialization,
    and SGD optimization techniques.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从本章开始时，通过一个关于神经网络（NN）基础数学工具的教程，然后回顾了神经网络及其架构。在此过程中，我们尝试明确地将数学概念与神经网络的各个组成部分连接起来。我们特别关注了各种类型的激活函数。最后，我们全面回顾了神经网络训练过程，讨论了梯度下降、代价函数、反向传播、权重初始化和SGD优化技术。
- en: In the next chapter, we'll discuss the intricacies of convolutional networks
    and their applications in the computer vision domain.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论卷积网络的复杂性及其在计算机视觉领域的应用。
