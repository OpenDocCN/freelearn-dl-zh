- en: Computer Vision and Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉与神经网络
- en: In recent years, computer vision has grown into a key domain for innovation,
    with more and more applications reshaping businesses and lifestyles. We will start
    this book with a brief presentation of this field and its history so that we can
    get some background information. We will then introduce artificial neural networks
    and explain how they have revolutionized computer vision. Since we believe in
    learning through practice, by the end of this first chapter, we will even have
    implemented our own network from scratch!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉已经成长为创新的一个关键领域，越来越多的应用正在重新塑造企业和生活方式。我们将从简要介绍该领域及其历史开始，以便为读者提供一些背景信息。接着，我们将介绍人工神经网络，并解释它们如何革新计算机视觉。因为我们相信通过实践学习，因此在本章结束时，我们甚至会从零开始实现我们自己的神经网络！
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Computer vision and why it is a fascinating contemporary domain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉及其作为一个迷人的当代领域的原因
- en: How we got there—from local hand-crafted descriptors to deep neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是如何走到这一步的——从本地手工制作的描述符到深度神经网络
- en: Neural networks, what they actually are, and how to implement our own for a
    basic recognition task
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络，它们究竟是什么，以及如何为一个基础的识别任务实现我们自己的神经网络
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Throughout this book, we will be using Python 3.5 (or higher). As a general-purpose
    programming language, Python has become the main tool for data scientists thanks
    to its useful built-in features and renowned libraries.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用Python 3.5（或更高版本）。作为一种通用编程语言，Python凭借其有用的内建特性和著名的库，已成为数据科学家们的主要工具。
- en: For this introductory chapter, we will only use two cornerstone libraries—NumPy
    and Matplotlib. They can be found at and installed from [www.numpy.org](http://www.numpy.org/)
    and [matplotlib.org](https://matplotlib.org/). However, we recommend using Anaconda
    ([www.anaconda.com](https://www.anaconda.com)), a free Python distribution that
    makes package management and deployment easy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本介绍性章节，我们将只使用两个基础库——NumPy和Matplotlib。它们可以在[www.numpy.org](http://www.numpy.org/)和[matplotlib.org](https://matplotlib.org/)上找到并进行安装。然而，我们建议使用Anaconda（[www.anaconda.com](https://www.anaconda.com)），这是一个免费的Python发行版，使得包管理和部署变得简单。
- en: Complete installation instructions—as well as all the code presented alongside
    this chapter—can be found in the GitHub repository at [github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的安装说明以及本章展示的所有代码，可以在GitHub上的[github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01)找到。
- en: We assume that our readers already have some knowledge of Python and a basic
    understanding of image representation (pixels, channels, and so on) and matrix
    manipulation (shapes, products, and so on).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设读者已经具备一些Python的知识，并且对图像表示（像素、通道等）和矩阵操作（形状、乘积等）有基本的理解。
- en: Computer vision in the wild
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉的实际应用
- en: Computer vision is everywhere nowadays, to the point that its definition can
    drastically vary from one expert to another. In this introductory section, we
    will paint a global picture of computer vision, highlighting its domains of application
    and the challenges it faces.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉如今无处不在，以至于它的定义在不同专家之间可能有很大的差异。在本节介绍中，我们将勾画计算机视觉的整体图景，重点介绍它的应用领域和面临的挑战。
- en: Introducing computer vision
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍计算机视觉
- en: Computer vision can be hard to define because it sits at the junction of several
    research and development fields, such as *computer science* (algorithms, data
    processing, and graphics), *physics* (optics and sensors), *mathematics* (calculus
    and information theory), and *biology* (visual stimuli and neural processing).
    At its core, computer vision can be summarized as the *automated extraction of
    information from* *digital images*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉很难定义，因为它位于多个研究和开发领域的交汇点，例如*计算机科学*（算法、数据处理和图形学）、*物理学*（光学和传感器）、*数学*（微积分和信息论）和*生物学*（视觉刺激和神经处理）。从本质上讲，计算机视觉可以总结为*从*
    *数字图像*中自动提取信息。
- en: Our brain works wonders when it comes to vision. Our ability to decipher the
    visual stimuli our eyes constantly capture, to instantly tell one object from
    another, and to recognize the face of someone we have met only once, is just incredible.
    For computers, images are just blobs of pixels, matrices of red-green-blue values
    with no further meaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑在视觉方面的表现令人惊叹。我们能够解读眼睛不断捕捉到的视觉刺激，瞬间区分不同物体，并识别出我们只见过一次的人脸，这一切都不可思议。对于计算机来说，图像只是像素块，是由红绿蓝值组成的矩阵，毫无其他意义。
- en: The goal of computer vision is to teach computers *how to make sense of these
    pixels* the way humans (and other creatures) do, or even better. Indeed, computer
    vision has come a long way and, since the rise of deep learning, it has started
    achieving *super human* performance in some tasks, such as face verification and
    handwritten text recognition.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的目标是教会计算机*如何像人类（及其他生物）一样理解这些像素*，甚至做得更好。事实上，计算机视觉已经取得了长足进展，尤其是在深度学习的兴起后，它在一些任务中已经达到了*超越人类*的表现，例如人脸验证和手写文字识别。
- en: 'With a hyper active research community fueled by the biggest IT companies,
    and the ever-increasing availability of data and visual sensors, more and more
    ambitious problems are being tackled: vision-based navigation for autonomous driving,
    content-based image and video retrieval, and automated annotation and enhancement,
    among others. It is truly an exciting time for experts and newcomers alike.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着由全球最大IT公司推动的研究社区的高度活跃，以及数据和视觉传感器日益普及，越来越多的雄心勃勃的问题正在被解决：基于视觉的自动驾驶导航、基于内容的图像和视频检索，以及自动标注和增强等。这无疑是专家和新手们都充满期待的时代。
- en: Main tasks and their applications
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要任务及其应用
- en: New computer vision-based products are appearing every day (for instance, control
    systems for industries, interactive smartphone apps, and surveillance systems)
    that cover a wide range of tasks. In this section, we will go through the main
    ones, detailing their applications in relation to real-life problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计算机视觉的新产品每天都在出现（例如，工业控制系统、互动智能手机应用和监控系统），它们涵盖了广泛的任务。在本节中，我们将介绍其中的主要应用，并详细说明它们如何解决实际问题。
- en: Content recognition
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容识别
- en: A central goal in computer vision is to *make sense* of images, that is, to
    extract meaningful, semantic information from pixels (such as the objects present
    in images, their location, and their number). This generic problem can be divided
    into several sub-domains. Here is a non-exhaustive list.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的核心目标是*理解*图像，即从像素中提取有意义的、语义层面的信息（例如图像中出现的物体、它们的位置和数量）。这个通用问题可以分为多个子领域。以下是一个非详尽的列表。
- en: Object classification
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体分类
- en: '**Object classification** (or **image classification**) is the task of assigning
    proper labels (or classes) to images among a predefined set and is illustrated
    in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**物体分类**（或称**图像分类**）是将图像分配到预定义类别中的任务，示意图如下：'
- en: '![](img/fd9258d3-5f2a-4bf7-8b63-a752edc36d03.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd9258d3-5f2a-4bf7-8b63-a752edc36d03.png)'
- en: Figure 1.1: Example of a classifier for the labels of people and cars applied
    to an image set
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：对人和车标签的分类器应用示例，应用于图像集
- en: Object classification became famous for being the first success story of deep convolutional
    neural networks being applied to computer vision back in 2012 (this will be presented
    later in this chapter). Progress in this domain has been so fast since then that
    super human performance is now achieved in various use cases (a well-known example
    is the classification of dog breeds; deep learning methods have become extremely
    efficient at spotting the discriminative features of man's best friend).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 物体分类因2012年深度卷积神经网络首次成功应用于计算机视觉而成名（这一点将在本章后面介绍）。自那时以来，这一领域的进展非常迅速，以至于现在在各种应用场景中都达到了超越人类的表现（一个著名的例子是狗品种的分类；深度学习方法已经变得极其高效，能够识别出人类最好的朋友的区分特征）。
- en: Common applications are text digitization (using character recognition) and
    the automatic annotation of image databases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 常见应用包括文本数字化（使用字符识别）和图像数据库的自动标注。
- en: In [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml), *Influential Classification
    Tools*, we will present advanced classification methods and their impact on computer
    vision in general.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)《*有影响力的分类工具*》中，我们将介绍先进的分类方法及其对计算机视觉的整体影响。
- en: Object identification
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体识别
- en: While *object classification* methods assign labels from a predefined set, *object
    identification* (or *instance classification*) methods learn to *recognize specific
    instances of a class*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而*物体分类*方法从预定义的集合中分配标签，*物体识别*（或*实例分类*）方法则学习*识别类别的特定实例*。
- en: 'For example, an *object classification* tool could be configured to return
    images containing faces, while an *identification* method would focus on the face''s
    features to identify the person and recognize them in other images (*identifying*
    each face in all of the images, as shown in the following diagram):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个*物体分类*工具可以配置为返回包含面部的图像，而*识别*方法则会专注于面部特征，以识别该人并在其他图像中识别他们（*识别*每一张面孔，如下图所示）：
- en: '![](img/a2479d1e-b3e6-4302-a9bb-63f142231b17.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2479d1e-b3e6-4302-a9bb-63f142231b17.png)'
- en: Figure 1.2: Example of an identifier applied to portraits
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：应用于肖像的标识符示例
- en: Therefore, object identification can be seen as a procedure to *cluster* a dataset,
    often applying some dataset analysis concepts (which will be presented in [Chapter
    6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml), *Enhancing and Segmenting Images)*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，物体识别可以看作是一个*聚类*数据集的过程，通常应用一些数据集分析概念（将在[第六章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)，*增强与图像分割*中介绍）。
- en: Object detection and localization
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测与定位
- en: Another task is the *detection of specific elements in an image*. It is commonly
    applied to face detection for surveillance applications or even advanced camera
    apps, the detection of cancerous cells in medicine, the detection of damaged components
    in industrial plants, and so on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个任务是*图像中特定元素的检测*。它通常应用于监控应用中的人脸检测，甚至高级相机应用，医学中癌细胞的检测，工业工厂中损坏组件的检测等。
- en: 'Detection is often a preliminary step before further computations, providing
    smaller patches of the image to be analyzed separately (for instance, cropping
    someone''s face for facial recognition, or providing a bounding box around an
    object to evaluate its pose for augmented reality applications), as shown in the
    following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 检测通常是进一步计算的前置步骤，它提供了图像的较小区域，以便分别进行分析（例如，裁剪某人的面部用于人脸识别，或提供围绕物体的边界框以评估其在增强现实应用中的姿态），如下图所示：
- en: '![](img/e667b9c5-8c1c-4fae-8e3c-dc8811da1825.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e667b9c5-8c1c-4fae-8e3c-dc8811da1825.png)'
- en: 'Figure 1.3: Example of a car detector, returning bounding boxes for the candidates'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：汽车检测器示例，返回候选物体的边界框
- en: State-of-the-art solutions will be detailed in [Chapter 5](593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml),
    *Object Detection Models*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的解决方案将在[第五章](593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml)，*物体检测模型*中详细介绍。
- en: Object and instance segmentation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体与实例分割
- en: 'Segmentation can be seen as a more advanced type of detection. Instead of simply
    providing bounding boxes for the recognized elements, segmentation methods *return
    masks labeling all the pixels* belonging to a specific class or to a specific
    instance of a class (refer to the following *Figure 1.4*). This makes the task
    much more complex, and actually one of the few in computer vision where deep neural
    networks are still far from human performance (our brain is indeed remarkably
    efficient at drawing the precise boundaries/contours of visual elements). Object
    segmentation and instance segmentation are illustrated in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 分割可以看作是更高级的检测类型。与仅提供识别元素的边界框不同，分割方法*返回标记所有像素的掩码*，这些像素属于特定类别或特定实例（请参见下图*图1.4*）。这使得任务变得更加复杂，实际上是计算机视觉中少数几个仍远不及人类表现的任务之一（我们的脑袋确实在绘制视觉元素的精确边界/轮廓方面异常高效）。物体分割和实例分割在下图中进行了说明：
- en: '![](img/0918369a-e623-454c-befa-803d8424ee2a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0918369a-e623-454c-befa-803d8424ee2a.png)'
- en: 'Figure 1.4: Comparing the results of object segmentation methods and instance
    segmentation methods for cars'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：比较汽车的物体分割方法和实例分割方法的结果
- en: In *Figure 1.4*, while the object segmentation algorithm returns a single mask
    for all pixels belonging to the *car* class, the instance segmentation one returns
    a different mask for each *car *instance that it recognized. This is a key task
    for robots and smart cars in order to understand their surroundings (for example,
    to identify all the elements in front of a vehicle), but it is also used in medical
    imagery. Precisely segmenting the different tissues in medical scans can enable
    faster diagnosis and easier visualization (such as coloring each organ differently
    or removing clutter from the view). This will be demonstrated in [Chapter 6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml),
    *Enhancing and Segmenting Images*, with concrete experiments for autonomous driving
    applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 1.4*中，虽然物体分割算法为所有属于*汽车*类的像素返回一个单一的掩膜，但实例分割算法则为每个被识别的*汽车*实例返回不同的掩膜。这对于机器人和智能汽车来说是一个关键任务，目的是让它们理解周围环境（例如，识别车辆前方的所有元素），但在医学影像中也有应用。精准分割医学扫描中的不同组织可以加速诊断并简化可视化（例如，为每个器官上色，或去除视图中的杂物）。这将在[第6章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)《图像增强与分割》中进行展示，并针对自动驾驶应用提供具体实验。
- en: Pose estimation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 姿态估计
- en: Pose estimation can have different meanings depending on the targeted tasks.
    For rigid objects, it usually means *the estimation of the objects' positions
    and orientations* relative to the camera in the 3D space. This is especially useful
    for robots so that they can interact with their environment (object picking, collision
    avoidance, and so on). It is also often used in augmented reality to overlay 3D
    information on top of objects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计的含义可能根据目标任务不同而有所不同。对于刚性物体，它通常意味着*估计物体在三维空间中相对于相机的位置和方向*。这对于机器人尤其有用，使它们能够与环境互动（物体拾取、碰撞避免等）。它也常常应用于增强现实，通过叠加三维信息到物体上。
- en: 'For non-rigid elements, pose estimation can also mean *the estimation of the
    positions of their sub-parts relative to each other*. More concretely, when considering
    humans as non-rigid targets, typical applications are the recognition of human
    poses (standing, sitting, running, and so on) or understanding sign language.
    These different cases are illustrated in the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非刚性元素，姿态估计也可以意味着*估计其子部件之间的位置关系*。更具体地说，当将人类视为非刚性目标时，典型应用包括人类姿态的识别（站立、坐着、跑步等）或手语的理解。这些不同的情况在下图中有所示例：
- en: '![](img/1aab463e-4e8b-4894-827c-b0cc110515b5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1aab463e-4e8b-4894-827c-b0cc110515b5.png)'
- en: 'Figure 1.5: Examples of rigid and non-rigid pose estimation'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：刚性与非刚性姿态估计示例
- en: In both cases—that is, for whole or partial elements—the algorithms are tasked
    with evaluating their actual position and orientation relative to the camera in
    the 3D world, based on their 2D representation in an image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下——无论是针对完整还是部分元素——算法的任务都是基于其在图像中的二维表示，评估它们在三维世界中相对于相机的实际位置和方向。
- en: Video analysis
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频分析
- en: Computer vision not only applies to single images, but also to videos. If video
    streams are sometimes analyzed frame by frame, some tasks require that you consider
    an image sequence as a whole in order to take temporal consistency into account
    (this will be one of the topics of [Chapter 8](97884989-bb57-4611-8c66-ebe8ab387965.xhtml),
    *Video and Recurrent Neural Networks*).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉不仅适用于单张图像，还适用于视频。如果视频流有时逐帧分析，则有些任务要求你将图像序列整体考虑，以便考虑时间一致性（这将是[第8章](97884989-bb57-4611-8c66-ebe8ab387965.xhtml)《视频与递归神经网络》中的一个主题）。
- en: Instance tracking
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实例跟踪
- en: Some tasks relating video streams could naively be accomplished by studying
    each frame separately (memory less), but more efficient methods either take into
    account differences from image to image to guide the process to new frames or
    take complete image sequences as input for their predictions. *Tracking*, that
    is, *localizing specific elements in a video stream*, is a good example of such
    a task.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些与视频流相关的任务可以通过简单地逐帧分析来完成（无记忆），但更高效的方法要么考虑图像间的差异，以指导处理到新帧，要么将完整的图像序列作为输入进行预测。*跟踪*，即*在视频流中定位特定元素*，就是这样一个任务的典型示例。
- en: Tracking could be done frame by frame by applying detection and identification
    methods to each frame. However, it is much more efficient to use previous results
    to model the motion of the instances in order to partially predict their locations
    in future frames. **Motion continuity** is, therefore, a key predicate here, though
    it does not always hold (such as for fast-moving objects).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪可以通过对每一帧应用检测和识别方法逐帧完成。然而，使用之前的结果来建模实例的运动，以便部分预测它们在未来帧中的位置要高效得多。因此，**运动连续性**在这里是一个关键前提，尽管它并不总是成立（例如，对于快速移动的物体）。
- en: Action recognition
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作识别
- en: On the other hand, **action recognition** belongs to the list of tasks that
    can only be run with a sequence of images. Similar to how we cannot understand
    a sentence when we are given the words separately and unordered, we cannot recognize
    an action without studying a continuous sequence of images (refer to *Figure 1.6*).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**动作识别**属于只能通过图像序列来执行的任务之一。就像我们不能仅凭单独且无序的词语理解一个句子一样，我们无法在不研究连续图像序列的情况下识别一个动作（参见*图1.6*）。
- en: 'Recognizing an action means recognizing a particular motion among a predefined
    set (for instance, for human actions—dancing, swimming, drawing a square, or drawing
    a circle). Applications range from surveillance (such as the detection of abnormal
    or suspicious behavior) to human-machine interactions (such as for gesture-controlled
    devices):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 识别一个动作意味着从预定义的一组中识别特定的运动（例如，针对人类动作——跳舞、游泳、画正方形，或画圆）。应用范围从监控（例如，检测异常或可疑行为）到人机交互（例如，手势控制设备）：
- en: '![](img/cbb6ea03-2af4-4aba-abbf-b000dddf7d5a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbb6ea03-2af4-4aba-abbf-b000dddf7d5a.png)'
- en: 'Figure 1.6: Is Barack Obama in the middle of waving, pointing at someone, swatting
    a mosquito, or something else?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：巴拉克·奥巴马是在挥手、指向某人、拍打蚊子，还是做其他事情？
- en: Only the complete sequence of frames could help to label this action
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 只有完整的帧序列才能帮助标注这一动作。
- en: Since object recognition can be split into object classification, detection,
    segmentation, and so on, so can action recognition (action classification, detection,
    and so on).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于物体识别可以分为物体分类、检测、分割等，动作识别也可以如此（动作分类、检测等）。
- en: Motion estimation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运动估计
- en: Instead of trying to recognize moving elements, some methods focus on *estimating
    the actual velocity/trajectory* that is captured in videos. It is also common
    to evaluate the motion of the camera itself relative to the represented scene
    (*egomotion*). This is particularly useful in the entertainment industry, for
    example, to capture motion in order to apply visual effects or to overlay 3D information
    in TV streams such as sports broadcasting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法侧重于*估计实际速度/轨迹*，而不是试图识别运动中的元素，这些方法在视频中被捕捉到。评估相对于表示场景的相机自身的运动（*自运动*）也是常见的做法。这在娱乐行业中尤为重要，例如，捕捉运动以便应用视觉效果，或在体育转播等电视直播中叠加3D信息。
- en: Content-aware image edition
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容感知图像编辑
- en: 'Besides the analysis of their content, computer vision methods can also be
    applied to *improve the images themselves*. More and more, basic image processing
    tools (such as low-pass filters for image denoising) are being replaced by *smarter*
    methods that are able to use prior knowledge of the image content to improve its
    visual quality. For instance, if a method learns what a bird typically looks like,
    it can apply this knowledge in order to replace noisy pixels with coherent ones
    in bird pictures. This concept applies to any type of image restoration, whether
    it be denoising, deblurring, or resolution enhancing (*super-resolution*, as illustrated
    in the following diagram):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分析其内容，计算机视觉方法还可以应用于*改善图像本身*。越来越多的基础图像处理工具（例如，图像去噪的低通滤波器）正被*更智能*的方法所取代，这些方法能够利用图像内容的先验知识来改善其视觉质量。例如，如果一个方法学习到鸟类的典型样子，它可以运用这些知识，将鸟类图片中的噪点像素替换为连贯的像素。这个概念适用于任何类型的图像修复，无论是去噪、去模糊，还是分辨率增强（*超分辨率*，如下面的图示所示）：
- en: '![](img/21c96ec9-8078-4c70-a912-935040d9034a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21c96ec9-8078-4c70-a912-935040d9034a.png)'
- en: 'Figure 1.7: Comparison of traditional and deep learning methods for image super-resolution.
    Notice how the details are sharper in the second image'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：传统方法与深度学习方法在图像超分辨率上的比较。注意第二张图像中的细节更加清晰。
- en: Content-aware algorithms are also used in some photography or art applications,
    such as the *smart portrait* or *beauty* modes for smartphones, which aim to enhance some
    of the models' features, or the *smart removing/editing* tools, which get rid
    of unwanted elements and replace them with a coherent background.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 内容感知算法也被应用于一些摄影或艺术应用中，比如智能手机上的*智能人像*或*美颜*模式，旨在增强某些模型的特征，或者*智能去除/编辑*工具，能够去除不需要的元素并用连贯的背景替代。
- en: In [Chapter 6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml), *Enhancing and Segmenting
    Images*, and in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training
    on Complex and Scarce Datasets*, we will demonstrate how such *generative* methods
    can be built and served.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)《图像增强与分割》和[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)《复杂稀缺数据集的训练》中，我们将展示如何构建并实现这些*生成性*方法。
- en: Scene reconstruction
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景重建
- en: Finally, though we won't tackle it in this book, *scene reconstruction* is the
    task of *recovering the 3D geometry of a scene,* given one or more images. A simple
    example, based on human vision, is stereo matching. This is the process of finding
    correspondences between two images of a scene from different viewpoints in order
    to derive the distance of each visualized element. More advanced methods take
    several images and match their content together in order to obtain a 3D model
    of the target scene. This can be applied to the 3D scanning of objects, people,
    buildings, and so on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管我们在本书中不会深入讨论，*场景重建*是指在给定一张或多张图像的情况下，*恢复场景的3D几何结构*。一个简单的基于人类视觉的例子是立体匹配。这个过程是从不同视角对同一场景的两张图像进行匹配，以推算出每个可视化元素的距离。更先进的方法会使用多张图像，并将它们的内容匹配在一起，从而获取目标场景的3D模型。这种技术可以应用于物体、人类、建筑等的3D扫描。
- en: A brief history of computer vision
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉简史
- en: '"Study the past if you would define the future."'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: “如果你想定义未来，就要研究过去。”
- en: – Confucius
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ——孔子
- en: In order to better understand the current stand of the heart and current challenges
    of computer vision, we suggest that we quickly have a look at where it came from
    and how it has evolved in the past decades.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解计算机视觉的现状及其当前面临的挑战，我们建议快速回顾一下它的起源以及过去几十年的演变过程。
- en: First steps to initial successes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从初步成功到第一步
- en: Scientists have long dreamed of developing artificial intelligence, including *visual
    intelligence*. The first advances in computer vision were driven by this idea.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 科学家们长期以来一直梦想着开发人工智能，包括*视觉智能*。计算机视觉的最初进展正是源自这一想法。
- en: Underestimating the perception task
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低估感知任务
- en: Computer vision as a domain started as early as the 60s, among the **Artificial
    Intelligence** (**AI**) research community. Still heavily influenced by the *symbolist*
    philosophy, which considered playing chess and other purely intellectual activities
    the epitome of human intelligence, these researchers underestimated the complexity
    of *lower animal functions* such as **perception**. How these researchers believed
    they could reproduce human perception through a single summer project in 1966
    is a famous anecdote in the computer vision community.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉作为一个领域，最早起步于60年代，源于**人工智能**（**AI**）研究社区。尽管当时深受*符号主义*哲学的影响，该哲学认为下棋和其他纯粹的智力活动是人类智慧的巅峰，这些研究人员低估了*低级动物功能*（如**感知**）的复杂性。1966年，这些研究人员曾相信他们能够通过一个夏季项目重现人类感知，这成为计算机视觉领域的一个著名轶事。
- en: Marvin Minsky was one of the first to outline an approach toward building AI
    systems based on perception (in *Steps toward artificial intelligence*, Proceedings
    of the IRE, 1961). He argued that with the use of lower functions such as pattern
    recognition, learning, planning, and induction, it could be possible to build
    machines capable of solving a broad variety of problems. However, this theory
    was only properly explored from the 80s onward. In *Locomotion, Vision, and Intelligence* in
    1984, Hans Moravec noted that our nervous system, through the process of evolution,
    has developed to tackle perceptual tasks (more than 30% of our brain is dedicated
    to vision!).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 马文·明斯基是最早提出基于感知构建AI系统方法的人之一（在《人工智能的步伐》一文中，发表在1961年的IRE会议录上）。他认为，利用模式识别、学习、规划和归纳等低级功能，有可能构建能够解决各种问题的机器。然而，这一理论直到80年代才得到深入研究。在1984年的《运动、视觉与智能》一书中，汉斯·莫拉维克指出，我们的神经系统通过进化过程已经发展出应对感知任务的能力（我们的大脑中超过30%的区域专门负责视觉！）。
- en: As he noted, even if computers are pretty good at arithmetic, they cannot compete
    with our perceptual abilities. In this sense, programming a computer to solve
    purely intellectual tasks (for example, playing chess) does not necessarily contribute
    to the development of systems that are intelligent in a general sense or relative
    to human intelligence.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如他所指出的，尽管计算机在算术运算方面表现得相当不错，但它们无法与我们感知能力相抗衡。从这个意义上说，编程让计算机解决纯粹的智力任务（例如，下棋）并不一定有助于开发在一般意义上或相对人类智能的智能系统。
- en: Hand-crafting local features
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手工制作局部特征
- en: Inspired by human perception, the basic mechanisms of computer vision are straightforward
    and have not evolved much since the early years—the idea is to *first extract
    meaningful features from the raw pixels*, and *then match these features to known,
    labeled ones* in order to achieve recognition.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类感知的启发，计算机视觉的基本机制是简单直接的，并且自早期以来没有太大演变——其思想是*首先从原始像素中提取有意义的特征*，*然后将这些特征与已知的标记特征进行匹配*，以实现识别。
- en: In computer vision, a **feature** is a piece of information (often mathematically
    represented as a one or two-dimensional vector) that is extracted from data that
    is relevant to the task at hand. Features include some key points in the images,
    specific edges, discriminative patches, and so on. They should be easy to obtain
    from new images and contain the necessary information for further recognition.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，**特征**是从与当前任务相关的数据中提取的信息片段（通常数学上表示为一维或二维向量）。特征包括图像中的一些关键点、特定的边缘、区分性补丁等。它们应该容易从新图像中获取，并包含进一步识别所需的必要信息。
- en: Researchers used to come up with more and more complex features. The extraction
    of edges and lines was first considered for the basic geometrical understanding
    of scenes or for character recognition; then, texture and lighting information
    was also taken into account, leading to early object classifiers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员曾提出越来越复杂的特征。最初考虑提取边缘和线条用于场景的基本几何理解或字符识别；接着，纹理和光照信息也被纳入考虑，促使了早期的物体分类器的发展。
- en: 'In the 90s, features based on statistical analysis, such as **principal component
    analysis** (**PCA**), were successfully applied for the first time to complex
    recognition problems such as face classification. A classic example is the *Eigenface*
    method introduced by Matthew Turk and Alex Pentland (*Eigenfaces for Recognition*,
    MIT Press, 1991). Given a database of face images, the mean image and the *eigenvectors/images*
    (also known as **characteristic vectors/images**) were computed through PCA. This
    small set of *eigenimages* can theoretically be linearly combined to reconstruct
    any face in the original dataset, or beyond. In other words, each face picture
    can be approximated through a weighted sum of the *eigenimages* (refer to *Figure
    1.8*). This means that a particular face can simply be defined by the list of
    reconstruction weights for each *eigenimage*. As a result, classifying a new face
    is just a matter of decomposing it into *eigenimages* to obtain its weight vector,
    and then comparing it with the vectors of known faces:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在90年代，基于统计分析的特征，如**主成分分析**（**PCA**），首次成功应用于面部分类等复杂识别问题。一个经典的例子是由Matthew Turk和Alex
    Pentland提出的*特征脸*方法（《Eigenfaces for Recognition》，MIT出版社，1991年）。给定一个面部图像数据库，通过PCA计算出平均图像和*特征向量/图像*（也称为**特征向量/图像**）。这组小型的*特征图像*理论上可以线性组合，重建原始数据集中的任何面孔，甚至超出该数据集。换句话说，每张面部图像可以通过*特征图像*的加权和来逼近（参见*图1.8*）。这意味着，一个特定的面孔仅通过每个*特征图像*的重建权重列表即可定义。因此，分类一个新面孔只需要将其分解为*特征图像*，获得其权重向量，然后与已知面孔的向量进行比较：
- en: '![](img/ba6a5070-1243-47b6-a044-0e9a2dbc7b7a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba6a5070-1243-47b6-a044-0e9a2dbc7b7a.png)'
- en: 'Figure 1.8: Decomposition of a portrait image into the mean image and weighted
    sum of eigenimages. These mean and eigenimages were computed over a larger face
    dataset'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：将肖像图像分解为平均图像和特征图像的加权和。这些平均图像和特征图像是在更大的面部数据集上计算得出的
- en: 'Another method that appeared in the late 90s and revolutionized the domain
    is called **Scale Invariant Feature Transform** (**SIFT**). As its name suggests,
    this method, introduced by David Lowe (in *Distinctive Image Features from Scale-Invariant
    Keypoints*, Elsevier), represents visual objects by a set of features that are
    robust to changes in scale and orientation. In the simplest terms, this method
    looks for some **key points** in images (searching for discontinuities in their
    *gradient*), extracts a patch around each key point, and computes a feature vector
    for each (for example, a histogram of the values in the patch or in its gradient).
    The **local features** of an image, along with their corresponding key points,
    can then be used to match similar visual elements across other images. In the
    following image, the SIFT method was applied to a picture using OpenCV ([https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html](https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html)).
    For each localized key point, the radius of the circle represents the size of
    the patch considered for the feature computation, and the line shows the feature
    orientation (that is, the main orientation of the neighborhood''s gradient):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在90年代末出现并彻底改变了该领域的方法叫做**尺度不变特征变换**（**SIFT**）。顾名思义，这种方法由David Lowe提出（在《来自尺度不变关键点的独特图像特征》一文中，Elsevier），它通过一组对尺度和方向变化具有鲁棒性的特征来表示视觉对象。简而言之，这种方法在图像中寻找一些**关键点**（通过寻找其*梯度*中的不连续性），提取每个关键点周围的图像块，并为每个关键点计算一个特征向量（例如，图像块中或其梯度中的值的直方图）。然后，图像的**局部特征**及其相应的关键点可以用来在其他图像中匹配相似的视觉元素。在下图中，SIFT方法应用于一张图片，使用了OpenCV（[https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html](https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html)）。对于每个局部化的关键点，圆圈的半径代表考虑用于特征计算的图像块的大小，线条表示特征方向（即邻域梯度的主要方向）：
- en: '![](img/69a21917-259b-4c6b-a9cb-345589cdbc4e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69a21917-259b-4c6b-a9cb-345589cdbc4e.png)'
- en: 'Figure 1.9: Representation of the SIFT key points extracted from an image (using
    OpenCV)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：表示从图像中提取的SIFT关键点（使用OpenCV）
- en: More advanced methods were developed over the years—with more robust ways of
    extracting key points, or computing and combining discriminative features—but
    they followed the same overall procedure (extracting features from one image,
    and comparing them to the features of others).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，出现了更先进的方法——这些方法提供了更鲁棒的方式来提取关键点、计算和结合区分性特征——但它们遵循了相同的总体流程（从一张图像中提取特征，并将其与其他图像的特征进行比较）。
- en: Adding some machine learning on top
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在此基础上加入一些机器学习
- en: It soon appeared clear, however, that extracting robust, discriminative features
    was only half the job for recognition tasks. For instance, different elements
    from the same class can look quite different (such as different-looking dogs)
    and, as a result, share only a small set of common features. Therefore, unlike
    image-matching tasks, higher-level problems such as semantic classification cannot
    be solved by simply comparing pixel features from query images with those from
    labeled pictures (such a procedure can also become sub-optimal in terms of processing
    time if the comparison has to be done with every image from a large labeled dataset).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很快就清楚了，提取鲁棒的、区分度高的特征只是完成识别任务的一半工作。例如，同一类中的不同元素可能看起来完全不同（比如不同品种的狗），因此它们共享的共同特征可能仅限于少数几个。因此，与图像匹配任务不同，更高层次的问题，如语义分类，不能仅仅通过比较查询图像中的像素特征与标签图片中的像素特征来解决（如果必须与大型标签数据集中的每张图片进行比较，这种过程在处理时间上也可能变得不理想）。
- en: This is where *machine learning* come into play. With an increasing number of
    researchers trying to tackle image classification in the 90s, more statistical
    ways to discriminate images based on their features started to appear. **Support
    vector machines** (**SVMs**), which were standardized by Vladimir Vapnik and Corinna
    Cortes (*Support-vector networks*, Springer, 1995), were, for a long time, the
    default solution for learning a mapping from complex structures (such as images)
    to simpler labels (such as classes).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，*机器学习*发挥了作用。随着越来越多的研究人员在90年代尝试解决图像分类问题，基于特征来区分图像的统计方法逐渐出现。**支持向量机**（**SVMs**），由Vladimir
    Vapnik和Corinna Cortes标准化（《支持向量网络》，Springer，1995），在很长一段时间内，是从复杂结构（如图像）到简单标签（如类别）进行映射学习的默认解决方案。
- en: 'Given a set of image features and their binary labels (for example, *cat* or
    *not cat,* as illustrated in *Figure 1.10*), an SVM can be optimized to learn
    the function to separate one class from another, based on extracted features.
    Once this function is obtained, it is just a matter of applying it to the feature
    vector of an unknown image so that we can map it to one of the two classes (SVMs
    that could extend to a larger number of classes were later developed). In the
    following diagram, an SVM was taught to regress a linear function separating two
    classes based on features extracted from their images (features as vectors of
    only two values in this example):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组图像特征及其二进制标签（例如，*猫* 或 *非猫*，如 *图 1.10* 所示），可以优化 SVM 来学习分离一个类别与另一个类别的函数，基于提取的特征。一旦获得该函数，就只需将其应用于未知图像的特征向量，便能将其映射到两个类别中的一个（后来的
    SVM 还可以扩展到更多类别）。在下面的示意图中，SVM 被训练回归一个线性函数，通过从图像中提取的特征来分离两个类别（在这个例子中，特征作为只有两个值的向量）：
- en: '![](img/6fb96b6d-1f26-491c-90c2-6b00a1362d04.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fb96b6d-1f26-491c-90c2-6b00a1362d04.png)'
- en: 'Figure 1.10: An illustration of a linear function regressed by an SVM. Note
    that using a concept known as the kernel trick, SVMs can also find non-linear
    solutions to separate classes'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：一个由 SVM 回归的线性函数的示意图。请注意，使用一种叫做核技巧的概念，SVM 也可以找到非线性解法来分离不同的类别。
- en: Other machine learning algorithms were adapted over the years by the computer
    vision community, such as *random forests*, *bags of words*, *Bayesian* *models*,
    and obviously *neural networks*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，计算机视觉领域也借用了其他机器学习算法，比如*随机森林*、*词袋模型*、*贝叶斯* *模型*，显然还有*神经网络*。
- en: Rise of deep learning
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的崛起
- en: So, how did neural networks take over computer vision and become what we nowadays
    know as **deep learning**? This section offers some answers, detailing the technical
    development of this powerful tool.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，神经网络是如何接管计算机视觉，并成为我们今天所称之为**深度学习**的呢？本节提供了一些答案，详细介绍了这一强大工具的技术发展。
- en: Early attempts and failures
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早期的尝试和失败
- en: It may be surprising to learn that artificial neural networks appeared even
    before modern computer vision. Their development is the typical story of an invention
    too early for its time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人感到惊讶的是，人工神经网络的出现甚至早于现代计算机视觉的诞生。它们的发展是典型的“发明出现得太早”的故事。
- en: Rise and fall of the perceptron
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机的兴起与衰落
- en: 'In the 50s, Frank Rosenblatt came up with the **perceptron**, a machine learning
    algorithm inspired by neurons and the underlying block of the first neural networks
    (*The Perceptron: A Probabilistic Model for Information Storage and Organization
    in the Brain*, American Psychological Association, 1958). With the proper learning
    procedure, this method was already able to recognize characters. However, the
    hype was short-lived. Marvin Minsky (one of the fathers of AI) and Seymor Papert
    quickly demonstrated that the perceptron could not learn a function as simple
    as `XOR` (exclusive *OR*, the function that, given two binary input values, returns
    `1` if one, and only one, input is `1`, and returns `0` otherwise). This makes
    sense to us nowadays—as the perceptron back then was modeled with a linear function
    while `XOR` is a non-linear one—but, at that time, it simply discouraged any further
    research for years.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 50 年代，Frank Rosenblatt 提出了**感知机**，这是一种受神经元启发的机器学习算法，也是第一个神经网络的基础构成元素（《感知机：一种用于信息存储和大脑组织的概率模型》，美国心理学会，1958）。在适当的学习过程中，这种方法已经能够识别字符。然而，这种热潮是短暂的。Marvin
    Minsky（人工智能的奠基人之一）和 Seymor Papert 很快证明感知机无法学习像 `XOR` 这样的简单函数（`XOR`，异或函数，对于两个二进制输入值，当且仅当一个输入为
    `1` 时，输出为 `1`，否则输出 `0`）。今天看来这很有道理——因为当时的感知机是基于线性函数建模的，而 `XOR` 是一个非线性函数——但在那个时候，这让研究进展停滞了多年。
- en: Too heavy to scale
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过于庞大，难以扩展
- en: It was only in the late 70s to early 80s that neural networks got some attention
    put back on them. Several research papers introduced how neural networks, with
    multiple *layers* of perceptrons put one after the other, could be trained using
    a rather straightforward scheme—backpropagation. As we will detail in the next
    section, this training procedure works by computing the network's error and backpropagating
    it through the layers of perceptrons to update their parameters using *derivatives*.
    Soon after, the first **convolutional neural network** (**CNN**), the ancestor
    of current recognition methods, was developed and applied to the recognition of
    handwritten characters with some success.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 直到70年代末到80年代初，神经网络才重新获得一些关注。几篇研究论文介绍了如何通过将多个*感知器*层堆叠在一起，使用一种相当简单的方案——反向传播，来训练神经网络。正如我们在下一部分中将详细介绍的，这一训练过程通过计算网络的误差并将其反向传播通过感知器层，从而使用*导数*更新它们的参数。很快，第一个**卷积神经网络**（**CNN**），即当前识别方法的先驱，被开发出来并成功应用于手写字符的识别。
- en: Alas, these methods were computationally heavy, and just could not scale to
    larger problems. Instead, researchers adopted lighter machine learning methods
    such as SVMs, and the use of neural networks stalled for another decade. So, what
    brought them back and led to the deep learning era we know of today?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜的是，这些方法计算量巨大，根本无法扩展到更大的问题上。因此，研究人员采用了更轻量的机器学习方法，如支持向量机（SVM），神经网络的应用又停滞了十年。那么，是什么使它们复兴，带来了我们今天所知的深度学习时代呢？
- en: Reasons for the comeback
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复兴的原因
- en: The reasons for this comeback are twofold and rooted in the explosive evolution
    of the internet and hardware efficiency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复兴的原因是双重的，根源在于互联网的爆炸性发展和硬件效率的提升。
- en: The internet – the new El Dorado of data science
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互联网——数据科学的新埃尔多拉多
- en: The internet was not only a revolution in communication; it also deeply transformed
    data science. It became much easier for scientists to share images and content
    by uploading them online, leading to the creation of public datasets for experimentation
    and benchmarking. Moreover, not only researchers but soon everyone, all over the
    world, started adding new content online, sharing images, videos, and more at
    an exponential rate. This started *big data* and the *golden age of data science*,
    with the internet as the new El Dorado.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网不仅仅是通信的革命；它也深刻地改变了数据科学。科学家们通过将图片和内容上传到网上，变得更加容易分享，进而催生了用于实验和基准测试的公共数据集。此外，不仅仅是研究人员，很快全世界的每个人都开始在线发布新内容，以指数级的速度分享图片、视频等。这开启了*大数据*和*数据科学的黄金时代*，互联网成为了新的埃尔多拉多。
- en: By simply indexing the content that is constantly published online, image and
    video datasets reached sizes that were never imagined before, from *Caltech-101*
    (10,000 images, published in 2003 by Li Fei-Fei et al., Elsevier) to *ImageNet*
    (14+ million images, published in 2009 by Jia Deng et al., IEEE) or *Youtube-8M*
    (8+ million videos, published in 2016 by Sami Abu-El-Haija et al., including Google).
    Even companies and governments soon understood the numerous advantages of gathering
    and releasing datasets to boost innovation in their specific domains (for example,
    the i-LIDS datasets for video surveillance released by the British government
    and the COCO dataset for image captioning sponsored by Facebook and Microsoft,
    among others).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地索引不断发布的内容，图像和视频数据集的规模达到了前所未有的大小，从*Caltech-101*（10,000张图像，由Li Fei-Fei等人于2003年发布，Elsevier）到*ImageNet*（1400万+张图像，由Jia
    Deng等人于2009年发布，IEEE）或*Youtube-8M*（800万+视频，由Sami Abu-El-Haija等人于2016年发布，包括Google）。甚至企业和政府也很快意识到收集和发布数据集在特定领域推动创新的诸多优势（例如，英国政府发布的视频监控数据集i-LIDS，Facebook和微软资助的图像标注数据集COCO等）。
- en: With so much data available covering so many use cases, new doors were opened
    (*data-hungry* algorithms, that is, methods requiring a lot of training samples
    to converge could finally be applied with success), and new challenges were raised
    (such as how to efficiently process all this information).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大量数据的涌现，涵盖了各种使用场景，新的大门被打开了（*数据饥渴*算法，也就是需要大量训练样本才能收敛的方法终于可以成功应用），同时也提出了新的挑战（例如，如何高效处理这些信息）。
- en: More power than ever
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比以往更强大的力量
- en: Luckily, since the internet was booming, so was computing power. Hardware kept
    becoming cheaper as well as faster, seemingly following Moore's famous law (which
    states that processor speeds should double every two years—this has been true
    for almost four decades, though a deceleration is now being observed). As computers
    got faster, they also became better designed for computer vision. And for this,
    we have to thank video games.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，随着互联网的蓬勃发展，计算能力也在提升。硬件变得越来越便宜，同时也越来越快，似乎遵循了摩尔定律（该定律表明处理器速度每两年应翻倍——这一规律已经保持了近四十年，尽管现在观察到有所放缓）。随着计算机变得更快，它们也变得更适合计算机视觉。对此，我们要感谢视频游戏。
- en: The **graphical processing unit** (**GPU**) is a computer component, that is,
    a chip specifically designed to handle the kind of operations needed to run 3D
    games. Therefore, a GPU is optimized to generate or manipulate images, parallelizing
    these heavy matrix operations. Though the first GPUs were conceived in the 80s,
    they became affordable and popular only with the advent of the new millennium.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**图形处理单元**（**GPU**）是一种计算机组件，即专门设计用于处理运行3D游戏所需操作的芯片。因此，GPU被优化用于生成或操作图像，能够并行化这些繁重的矩阵运算。尽管第一款GPU在80年代就已经构思出来，但它们直到新千年到来时才变得价格合理并流行开来。'
- en: In 2007, NVIDIA, one of the main companies designing GPUs, released the first
    version of **CUDA**, a programming language that allows developers to directly
    program for compatible GPUs. **OpenCL**, a similar language, appeared soon after.
    With these new tools, people started to harness the power of GPUs for new tasks,
    such as machine learning and computer vision.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 2007年，NVIDIA（主要设计GPU的公司之一）发布了**CUDA**的第一个版本，这是一种编程语言，允许开发者直接为兼容的GPU编程。类似的语言**OpenCL**随后也出现了。有了这些新工具，人们开始利用GPU的强大性能来处理新的任务，如机器学习和计算机视觉。
- en: Deep learning or the rebranding of artificial neural networks
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习或人工神经网络的重新品牌化
- en: The conditions were finally there for data-hungry, computationally-intensive
    algorithms to shine. Along with *big data* and *cloud computing*, *deep learning* was
    suddenly everywhere.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 终于，数据需求量大、计算密集型的算法有了展现的机会。随着*大数据*和*云计算*的兴起，*深度学习*突然无处不在。
- en: What makes learning deep?
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么让学习变得更深刻？
- en: Actually, the term **deep learning** had already been coined back in the 80s,
    when neural networks first began stacking two or three layers of neurons. As opposed
    to the early, simpler solutions, *deep learning* regroups *deeper* neural networks,
    that is, networks with multiple *hidden layers—*additional layers set between
    their input and output layers. Each layer processes its inputs and passes the
    results to the next layer, all trained to extract increasingly abstract information.
    For instance, the first layer of a neural network would learn to react to basic
    features in the images, such as edges, lines, or color gradients; the next layer
    would learn to use these cues to extract more advanced features; and so on until
    the last layer, which infers the desired output (such as predicted class or detection
    results).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，**深度学习**这个术语早在80年代就已经被提出，当时神经网络首次开始堆叠两层或三层神经元。与早期更简单的解决方案不同，*深度学习*重新组合了*更深*的神经网络，也就是拥有多个*隐藏层*的网络——这些额外的层设置在输入层和输出层之间。每一层处理它的输入，并将结果传递给下一层，所有层都经过训练以提取越来越抽象的信息。例如，神经网络的第一层将学习如何对图像中的基本特征作出反应，如边缘、线条或颜色渐变；下一层将学习如何利用这些线索提取更高级的特征；如此继续，直到最后一层，它推断出所需的输出（例如预测的类别或检测结果）。
- en: However, *deep learning* only really started being used from 2006, when Geoff
    Hinton and his colleagues proposed an effective solution to train these deeper
    models, one layer at a time, until reaching the desired depth (*A Fast Learning
    Algorithm for Deep Belief Nets*, MIT Press, 2006).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*深度学习*直到2006年才真正开始应用，那时Geoff Hinton和他的同事提出了一种有效的方法，可以一层一层地训练这些更深的模型，直到达到所需的深度（《深度信念网络的快速学习算法》，麻省理工学院出版社，2006年）。
- en: Deep learning era
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习时代
- en: With research into neural networks once again back on track, deep learning started
    growing, until a major breakthrough in 2012, which finally gave it its contemporary
    prominence. Since the publication of ImageNet, a competition (**ImageNet Large
    Scale Visual Recognition Challenge** (**ILSVRC**)—[image-net.org/challenges/LSVRC](http://image-net.org/challenges/LSVRC/))
    has been organized every year for researchers to submit their latest classification
    algorithms and compare their performance on ImageNet with others. The winning
    solutions in 2010 and 2011 had classification errors of 28% and 26% respectively,
    and applied traditional concepts such as SIFT features and SVMs. Then came the
    2012 edition, and a new team of researchers reduced the recognition error to a
    staggering 16%, leaving all the other contestants far behind.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络研究重新回到正轨，深度学习开始快速发展，直到2012年发生了一次重大突破，最终让深度学习在当代声名鹊起。自从ImageNet发布以来，每年都会组织一次比赛（**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）—[image-net.org/challenges/LSVRC](http://image-net.org/challenges/LSVRC/)），供研究人员提交他们最新的分类算法，并与其他参赛者在ImageNet上的表现进行对比。2010年和2011年的获胜解决方案分别有28%和26%的分类错误率，并采用了传统的概念，如SIFT特征和SVM。然后，2012年版出现了，一个新的研究团队将识别错误率降至令人震惊的16%，远远甩开了其他参赛者。
- en: In their paper describing this achievement (*Imagenet Classification with Deep
    Convolutional Neural Networks*, NIPS, 2012), Alex Krizhevsky, Ilya Sutskever,
    and Geoff Hinton presented what would become the basis for modern recognition
    methods. They conceived an 8-layer neural network, later named **AlexNet**, with
    several *convolutional layers* and other modern components such as **dropout**
    and **rectified linear activation units** (**ReLUs**), which will all be presented
    in detail in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, as they have became central to computer vision. More importantly,
    they used CUDA to implement their method so that it can be run on GPUs, finally
    making it possible to train deep neural networks in a reasonable time, iterating
    over datasets as big as ImageNet.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述这一成就的论文中（*使用深度卷积神经网络进行ImageNet分类*，NIPS，2012），Alex Krizhevsky、Ilya Sutskever
    和 Geoff Hinton 提出了现代识别方法的基础。他们设计了一个8层神经网络，后来被命名为**AlexNet**，该网络包含几个*卷积层*以及其他现代组件，如**dropout**和**修正线性激活单元**（**ReLUs**），这些将在[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)中详细介绍，*现代神经网络*，因为它们已成为计算机视觉的核心。更重要的是，他们使用CUDA实现了这一方法，使得它可以在GPU上运行，从而终于使得在合理的时间内训练深度神经网络成为可能，并能够迭代处理像ImageNet这样庞大的数据集。
- en: That same year, Google demonstrated how advances in **cloud computing** could
    also be applied to computer vision. Using a dataset of 10 million random images
    extracted from YouTube videos, they taught a neural network to identify images
    containing cats and parallelized the training process over 16,000 machines to
    finally double the accuracy compared to previous methods.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 同年，Google展示了如何将**云计算**的进步应用于计算机视觉。他们使用从YouTube视频中提取的1000万张随机图像的数据集，训练神经网络识别包含猫的图像，并将训练过程并行化到16,000台机器上，最终使得准确率相比以前的方法翻倍。
- en: And so started the deep learning era we are currently in. Everyone jumped on
    board, coming up with deeper and deeper models, more advanced training schemes,
    and lighter solutions for portable devices. It is an exciting period, as the more
    efficient deep learning solutions become, the more people try to apply them to
    new applications and domains. With this book, we hope to convey some of this current
    enthusiasm and provide you with an overview of the modern methods and how to develop
    solutions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，深度学习时代开始了，我们正身处其中。每个人都加入了这一领域，提出了越来越深的模型、更先进的训练方案和适用于便携设备的轻量化解决方案。这是一个激动人心的时期，随着深度学习解决方案的效率不断提高，越来越多的人尝试将其应用于新的应用和领域。通过本书，我们希望传达一些当前的热情，并为您提供现代方法的概览以及如何开发解决方案。
- en: Getting started with neural networks
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门神经网络
- en: By now, we know that neural networks form the core of deep learning and are
    powerful tools for modern computer vision. But what are they exactly? How do they
    work? In the following section, not only will we tackle the theoretical explanations
    behind their efficiency, but we will also directly apply this knowledge to the
    implementation and application of a simple network to a recognition task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们知道神经网络构成了深度学习的核心，并且是现代计算机视觉的强大工具。但它们究竟是什么呢？它们是如何工作的？在接下来的部分，我们不仅会解决它们效率背后的理论解释，还将直接将这些知识应用于简单网络在识别任务中的实现和应用。
- en: Building a neural network
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: '**Artificial neural networks** (**ANNs**), or simply **neural networks** (**NNs**),
    are powerful machine learning tools that are excellent at processing information,
    recognizing usual patterns or detecting new ones, and approximating complex processes.
    They have to thank their structure for this, which we will now explore.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**），或简称**神经网络**（**NNs**），是强大的机器学习工具，擅长处理信息、识别常见模式或检测新模式，并逼近复杂过程。它们之所以如此强大，归功于它们的结构，接下来我们将探讨这一点。'
- en: Imitating neurons
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟神经元
- en: It is well-known that neurons are the elemental supports of our thoughts and
    reactions. What might be less evident is how they actually work and how they can
    be simulated.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，神经元是我们思维和反应的基本支撑。可能不那么显而易见的是它们是如何工作的，以及它们如何被模拟。
- en: Biological inspiration
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学启发
- en: ANNs are loosely inspired by how animals' brains work. Our brain is a complex
    network of neurons, each passing information to each other and processing sensory
    inputs (as electrical and chemical signals) into thoughts and actions. Each neuron
    receives its electrical inputs from its *dendrites*, which are cell fibers that
    propagate the electrical signal from the *synapses* (the junctions with preceding
    neurons) to the *soma* (the neuron's main body). If the accumulated electrical
    stimulation exceeds a specific threshold, the cell is *activated* and the electrical
    impulse is *propagated further* to the next neurons through the cell's *axon*
    (the neuron's *output cable*, ending with several synapses linking to other neurons).
    Each neuron can, therefore, be seen as a really *simple signal processing unit*,
    which—once stacked together—can achieve the thoughts we are having right now,
    for instance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）大致受动物大脑工作的启发。我们的脑袋是一个复杂的神经元网络，每个神经元相互传递信息并将感官输入（作为电信号和化学信号）转化为思维和行动。每个神经元通过其*树突*接收电输入，树突是细胞纤维，将电信号从*突触*（与前一个神经元的连接处）传播到*胞体*（神经元的主要部分）。如果积累的电刺激超过特定阈值，细胞会被*激活*，电冲动会通过细胞的*轴突*（神经元的*输出电缆*，末端有多个突触连接到其他神经元）向下一个神经元*传播*。因此，每个神经元都可以看作一个非常*简单的信号处理单元*，一旦堆叠在一起，就能实现我们现在正在进行的思维过程。
- en: Mathematical model
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学模型
- en: 'Inspired by its biological counterpart (represented in *Figure 1.11*), the
    artificial neuron takes several *inputs* (each a number), sums them together,
    and finally applies an *activation function* to obtain the *output* signal, which
    can be passed to the following neurons in the network (this can be seen as a directed
    graph):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 受其生物学对照物（见*图1.11*）的启发，人工神经元接收多个*输入*（每个都是一个数字），将它们加总在一起，最后应用*激活函数*以获得*输出*信号，输出信号可以传递给网络中的下一个神经元（这可以视为一个有向图）：
- en: '![](img/ff29311c-b8a5-44cb-9823-92c9c8475829.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff29311c-b8a5-44cb-9823-92c9c8475829.png)'
- en: 'Figure 1.11: On the left, we can see a simplified biological neuron. On the
    right, we can see its artificial counterpart'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11：左侧是一个简化的生物神经元，右侧是其人工对照物
- en: The summation of the inputs is usually done in a weighted way. Each **input**
    is scaled up or down, depending on a weight specific to this particular **input**.
    These *weights* are the parameters that are adjusted during the training phase
    of the network in order for the neuron to react to the correct features. Often,
    another parameter is also trained and used for this summation process—the neuron's
    *bias*. Its value is simply added to the weighted sum as an *offset*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的求和通常是加权进行的。每个**输入**会根据一个特定于此**输入**的权重进行放大或缩小。这些*权重*是网络训练阶段调整的参数，以便神经元对正确的特征做出反应。通常，还会训练并使用另一个参数来进行此求和过程——神经元的*偏置*。其值简单地添加到加权和中，作为*偏移*。
- en: 'Let''s quickly formalize this process mathematically. Suppose we have a neuron
    that takes two input values, *x*[0] and *x*[1]. Each of these values would be
    weighted by a factor, *w*[0] and *w*[1], respectively, before being summed together,
    with an optional bias, *b*. For simplification, we can express the input values
    as a horizontal vector, *x*, and the weights as a vertical vector, *w*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们迅速将这个过程数学化。假设我们有一个神经元，接受两个输入值，*x*[0] 和 *x*[1]。每个值都会分别乘以一个权重因子，*w*[0] 和 *w*[1]，然后将它们加总在一起，再加上一个可选的偏置，*b*。为了简化，我们可以将输入值表示为水平向量，*x*，将权重表示为垂直向量，*w*：
- en: '![](img/194aefc5-8738-44ff-a5e0-8c475607c401.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/194aefc5-8738-44ff-a5e0-8c475607c401.png)'
- en: 'With this formulation, the whole operation can simply be expressed as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种公式，整个操作可以简单地表达如下：
- en: '![](img/5932c702-38e9-4f57-a380-52fc732f36ee.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5932c702-38e9-4f57-a380-52fc732f36ee.png)'
- en: 'This step is straightforward, isn''t it? The *dot product* between the two
    vectors takes care of the weighted summation:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤很简单，对吧？两个向量之间的*点积*处理了加权求和：
- en: '![](img/ad4c7615-419e-47dd-857c-b5a2c17fe3d7.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad4c7615-419e-47dd-857c-b5a2c17fe3d7.png)'
- en: 'Now that the inputs have been scaled and summed together into the result, *z*,
    we have to apply the *activation function* to it in order to get the neuron''s
    output. If we go back to the analogy with the biological neuron, its activation
    function would be a binary function such as *if* *y* *is above a threshold* *t**,
    return an electrical impulse that is 1, or else return 0* (with *t* = 0 usually).
    If we formalize this, the activation function, *y* = *f*(*z*), can be expressed
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，输入已经被缩放并汇总为结果 *z*，我们必须对其应用 *激活函数* 以获得神经元的输出。如果我们回到生物神经元的类比，它的激活函数将是一个二值函数，例如*如果* *y* *超过阈值*
    *t*，则返回 1 的电信号，否则返回 0*（通常 *t* = 0）。如果我们将其形式化，激活函数 *y* = *f*(*z*) 可以表示为：
- en: '![](img/49c11b52-fdff-4288-aa93-f9a6d7ff217e.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49c11b52-fdff-4288-aa93-f9a6d7ff217e.png)'
- en: 'The **step function** is a key component of the original perceptron, but more
    advanced activation functions have been introduced since then with more advantageous
    properties, such as *non-linearity* (to model more complex behaviors) and *continuous
    differentiability* (important for the training process, which we will explain
    later). The most common activation functions are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶跃函数**是原始感知器的关键组成部分，但自那时以来，引入了更先进的激活函数，它们具有更多的优势特性，例如*非线性*（用于建模更复杂的行为）和*连续可微性*（对训练过程至关重要，我们稍后将解释）。最常见的激活函数如下：'
- en: The **sigmoid** function,  ![](img/16c9208a-00ed-4907-90cf-58e1032b1ec5.png) (with 𝑒
    the exponential function)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid** 函数，  ![](img/16c9208a-00ed-4907-90cf-58e1032b1ec5.png) （其中 𝑒 为指数函数）'
- en: The **hyperbolic tangent**, ![](img/d24e0919-a1ab-4c88-b522-585dfcc8e905.png)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双曲正切函数**， ![](img/d24e0919-a1ab-4c88-b522-585dfcc8e905.png)'
- en: The **REctified Linear Unit **(**ReLU**),  ![](img/9a4211de-d66d-4c68-b300-67c9e81acca8.png)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**），  ![](img/9a4211de-d66d-4c68-b300-67c9e81acca8.png)'
- en: 'Plots of the aforementioned common activation functions are shown in the following
    diagram:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上述常见激活函数的图示如下：
- en: '![](img/c243ef46-fa4e-453b-8d4c-fc1c89efff73.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c243ef46-fa4e-453b-8d4c-fc1c89efff73.png)'
- en: 'Figure 1.12: Plotting common activation functions'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：绘制常见激活函数
- en: In any case, that's it! We have modeled a simple artificial neuron. It is able
    to receive a signal, process it, and output a value that can be *forwarded* (a
    term that is commonly used in machine learning) to other neurons, building a network.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，就这样！我们已经建模了一个简单的人工神经元。它能够接收信号、处理它并输出一个可以*转发*（这是机器学习中常用的术语）到其他神经元的值，从而构建网络。
- en: Chaining neurons with no non-linear activation functions would be equivalent
    to having a single neuron. For instance, if we had a linear neuron with parameters
    *w*[*A*] and *b*[*A*] followed by a linear neuron with parameters *w*[*B*] and
    *b*[*B*], then ![](img/b218a0de-15c0-4dc1-8be8-36f0cf1e1015.png), where *w* = *w*[*A**^(![](img/7d79fcb3-e427-486e-9eef-3060c702b2c1.png))*]*w*[*B*]
    and *b* =*b*[*A*] + *b*[*B*]. Therefore, non-linear activation functions are a
    necessity if we want to create complex models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有非线性激活函数，链式神经元将等同于只有一个神经元。例如，如果我们有一个带参数 *w*[*A*] 和 *b*[*A*] 的线性神经元，后接一个带参数
    *w*[*B*] 和 *b*[*B*] 的线性神经元，那么 ![](img/b218a0de-15c0-4dc1-8be8-36f0cf1e1015.png)，其中
    *w* = *w*[*A*]**^(![](img/7d79fcb3-e427-486e-9eef-3060c702b2c1.png))*] *w*[*B*]，并且
    *b* = *b*[*A*] + *b*[*B*]。因此，如果我们想创建复杂的模型，非线性激活函数是必要的。
- en: Implementation
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'Such a model can be implemented really easily in Python (using NumPy for vector
    and matrix manipulations):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的模型可以在 Python 中非常容易地实现（使用 NumPy 进行向量和矩阵操作）：
- en: '[PRE0]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As we can see, this is a direct adaptation of the mathematical model we defined
    previously. Using this artificial neuron is just as straightforward. Let''s instantiate
    a perceptron (a neuron with the step function for the activation method) and forward
    a random input through it:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这直接是我们之前定义的数学模型的适应。使用这个人工神经元同样简单。让我们实例化一个感知器（一个使用阶跃函数作为激活方法的神经元），并将一个随机输入通过它转发：
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We suggest that you take some time and experiment with different inputs and
    neuron parameters before we scale up their dimensions in the next section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在下一节中放大它们的维度之前，花些时间实验不同的输入和神经元参数。
- en: Layering neurons together
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将神经元层叠在一起
- en: Usually, neural networks are organized into *layers*, that is, sets of neurons
    that typically receive the same input and apply the same operation (for example,
    by applying the same activation function, though each neuron first sums the inputs
    with its own specific weights).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络被组织成*层*，也就是一组通常接收相同输入并应用相同操作的神经元（例如，应用相同的激活函数，尽管每个神经元首先会用自己的特定权重对输入进行加权求和）。
- en: Mathematical model
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学模型
- en: 'In networks, the information flows from the input layer to the output layer,
    with one or more *hidden* layers in-between. In *Figure 1.13*, the three neurons
    **A**, **B**, and **C** belong to the input layer, the neuron **H** belongs to
    the output or activation layer, and the neurons **D**, **E**, **F**, and **G** belong
    to the hidden layer. The first layer has an input, *x*, of size 2, the second
    (hidden) layer takes the three activation values of the previous layer as input,
    and so on. Such layers, with each neuron connected to all the values from the
    previous layer, are classed as being **fully connected** or **dense**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中，信息从输入层流向输出层，中间有一个或多个*隐藏层*。在*图1.13*中，三个神经元**A**、**B**和**C**属于输入层，神经元**H**属于输出或激活层，神经元**D**、**E**、**F**和**G**属于隐藏层。第一层有一个大小为2的输入*x*，第二层（隐藏层）将前一层的三个激活值作为输入，依此类推。这样的层，每个神经元都与前一层的所有值相连，被称为**全连接**或**密集**层：
- en: '![](img/e5d3267d-951a-46f0-b216-9823d147580a.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5d3267d-951a-46f0-b216-9823d147580a.png)'
- en: 'Figure 1.13: A 3-layer neural network, with two input values and one final
    output'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13：一个3层神经网络，包含两个输入值和一个最终输出
- en: 'Once again, we can compact the calculations by representing these elements
    with vectors and matrices. The following operations are done by the first layers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以通过用向量和矩阵表示这些元素来简化计算。以下操作由第一层完成：
- en: '![](img/f948048e-840f-4561-bd3c-4691966f767c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f948048e-840f-4561-bd3c-4691966f767c.png)'
- en: 'This can be expressed as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以表达如下：
- en: '![](img/b712dec9-383e-46af-a7dc-bcb79e7928d7.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b712dec9-383e-46af-a7dc-bcb79e7928d7.png)'
- en: 'In order to obtain the previous equation, we must define the variables as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到前面的方程，我们必须按如下方式定义变量：
- en: '![](img/29647de3-3ec2-48f8-b302-dbe8963eaa8b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29647de3-3ec2-48f8-b302-dbe8963eaa8b.png)'
- en: The activation of the first layer can, therefore, be written as a vector, ![](img/20f4ff4b-4c6c-4e71-8e39-692e6e3fe7d7.png),
    which can be directly passed as an input vector to the next layer, and so on until
    the last layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一层的激活可以写成一个向量，![](img/20f4ff4b-4c6c-4e71-8e39-692e6e3fe7d7.png)，该向量可以直接作为输入向量传递到下一层，依此类推，直到最后一层。
- en: Implementation
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'Like the single neuron, this model can be implemented in Python. Actually,
    we do not even have to make too many edits compared to our `Neuron` class:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 像单个神经元一样，这个模型可以在Python中实现。实际上，我们甚至不需要对`Neuron`类做太多编辑：
- en: '[PRE2]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We just have to change the *dimensionality* of some of the variables in order
    to reflect the multiplicity of neurons inside a layer. With this implementation,
    our layer can even process several inputs at once! Passing a single column vector *x*
    (of shape 1 × *s* with *s* number of values in *x*) or a stack of column vectors
    (of shape *n* × *s* with *n* number of samples) does not change anything with
    regard to our matrix calculations, and our layer will correctly output the stacked
    results (assuming *b* is added to each row):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要改变一些变量的*维度*，以反映每层中神经元的多样性。通过这种实现，我们的层甚至可以一次处理多个输入！传递一个单列向量*x*（形状为1 × *s*，其中*x*包含*s*个值）或一堆列向量（形状为*n*
    × *s*，其中*n*为样本数）对于我们的矩阵计算没有任何影响，我们的层将正确输出堆叠的结果（假设*b*被加到每一行）：
- en: '[PRE3]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A stack of input data is commonly called a **batch**.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一堆输入数据通常被称为**批次**。
- en: With this implementation, it is now just a matter of chaining fully connected
    layers together to build simple neural networks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个实现，现在只是将全连接层串联起来，构建简单的神经网络。
- en: Applying our network to classification
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将我们的网络应用于分类
- en: We know how to define layers, but have yet to initialize and connect them into
    networks for computer vision. To demonstrate how to do this, we will tackle a
    famous recognition task.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何定义层，但尚未初始化并将它们连接成计算机视觉的网络。为了演示如何做到这一点，我们将处理一个著名的识别任务。
- en: Setting up the task
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置任务
- en: 'Classifying images of handwritten digits (that is, recognizing whether an image
    contains a `0` or a `1` and so on) is a historical problem in computer vision.
    The **Modified National Institute of Standards and Technology** (**MNIST**) dataset
    ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)), which
    contains 70,000 grayscale images (*28 × 28* pixels) of such digits, has been used
    as a reference over the years so that people can test their methods for this recognition
    task (Yann LeCun and Corinna Cortes hold all copyrights for this dataset, which
    is shown in the following diagram):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对手写数字图像进行分类（即识别图像中是否包含`0`、`1`等数字）是计算机视觉中的一个历史性问题。**修改版国家标准与技术研究院**（**MNIST**）数据集（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)），其中包含了70,000张手写数字的灰度图像（*28
    × 28* 像素），多年来一直作为该识别任务的参考，以便人们可以测试他们的方法（此数据集的所有版权归Yann LeCun和Corinna Cortes所有，见下图）：
- en: '![](img/9c1f9cf5-56a6-4297-b1e9-42b757bced01.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c1f9cf5-56a6-4297-b1e9-42b757bced01.png)'
- en: 'Figure 1.14: Ten samples of each digit from the MNIST dataset'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14：MNIST数据集中每个数字的十个样本
- en: For digit classification, what we want is a network that takes one of these
    images as input and returns an output vector expressing *how strongly the network
    believes the image corresponds to each class*. The input vector has *28 × 28 =
    784* values, while the output has 10 values (for the 10 different digits, from
    `0` to `9`). In-between all of this, it is up to us to define the number of hidden
    layers and their sizes. To predict the class of an image, it is then just a matter
    of *forwarding the image vector through the network, collecting the output*, and
    *returning the class with the highest belief score*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数字分类，我们需要的是一个网络，它接收这些图像中的一张作为输入，并返回一个输出向量，表示*网络认为该图像对应于每个类别的程度*。输入向量包含*28
    × 28 = 784*个值，而输出向量包含10个值（对应10个不同的数字，从`0`到`9`）。在这些之间，定义隐藏层的数量和大小是我们的任务。为了预测图像的类别，只需*将图像向量通过网络前馈，收集输出*，然后*返回得分最高的类别*。
- en: These *belief* scores are commonly transformed into probabilities to simplify
    further computations or the interpretation. For instance, let's suppose that a
    classification network gives a score of 9 to the class *dog*, and a score of 1
    to the other class, *cat*. This is equivalent to saying that *according to this
    network, there is a 9/10 probability that the image shows a dog and a 1/10 probability
    it shows a cat*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些*信念*分数通常会转换为概率，以简化进一步的计算或解释。例如，假设一个分类网络对*狗*类给出了9的分数，对其他类*猫*给出了1的分数。这相当于说，*根据这个网络，图像显示狗的概率为9/10，显示猫的概率为1/10*。
- en: 'Before we implement a solution, let''s prepare the data by loading the MNIST
    data for training and testing methods. For simplicity, we will use the `mnist` Python
    module ([https://github.com/datapythonista/mnist](https://github.com/datapythonista/mnist)),
    which was developed by Marc Garcia (under the BSD 3-Clause *New* or *Revised* license,
    and is already installed in this chapter''s source directory):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现解决方案之前，先通过加载MNIST数据来准备训练和测试方法。为了简便起见，我们将使用`mnist` Python模块（[https://github.com/datapythonista/mnist](https://github.com/datapythonista/mnist)），该模块由Marc
    Garcia开发（采用BSD 3-Clause *New*或*Revised*许可，并且已安装在本章的源代码目录中）：
- en: '[PRE4]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: More detailed operations for the preprocessing and visualization of the dataset
    can be found in this chapter's source code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据集预处理和可视化的更详细操作，请参见本章的源代码。
- en: Implementing the network
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现网络
- en: 'For the neural network itself, we have to wrap the layers together and add
    some methods to forward through the complete network and to predict the class
    according to the output vector. After the layer''s implementation, the following
    code should be self-explanatory:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络本身，我们需要将各个层组合在一起，并添加一些方法，供网络前向传播和根据输出向量预测类别使用。层的实现完成后，以下代码应该不言自明：
- en: '[PRE5]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We just implemented a feed-forward neural network that can be used for classification!
    It is now time to apply it to our problem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现了一个前馈神经网络，可以用于分类！现在是时候将其应用到我们的任务中了：
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We only got an accuracy of ~12.06%. This may look disappointing since it is
    an accuracy that's barely better than random guessing. But it makes sense—right
    now, our network is defined by random parameters. We need to train it according
    to our use case, which is a task that we will tackle in the next section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率只有~12.06%。这可能看起来令人失望，因为这个准确率几乎与随机猜测相当。但这也是可以理解的——目前我们的网络是由随机参数定义的。我们需要根据我们的使用案例来训练它，这一任务将在下一节中处理。
- en: Training a neural network
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: '**Neural networks** are a particular kind of algorithm because they need to
    be *trained*, that is, their parameters need to be optimized for a specific task
    by making them learn from available data. Once the networks are optimized to perform
    well on this *training dataset*, they can be used on new, similar data to provide
    satisfying results (if the training was done properly).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**是一种特殊的算法，因为它们需要进行*训练*，也就是说，它们的参数需要通过从可用数据中学习来优化，以完成特定任务。一旦网络优化到能够在*训练数据集*上表现良好，它们就可以在新的、相似的数据上使用，以提供令人满意的结果（前提是训练得当）。'
- en: Before solving the problem of our MNIST task, we will provide some theoretical
    background, cover different learning strategies, and present how training is actually
    done. Then, we will directly apply some of these notions to our example so that
    our simple network finally learns how to solve the recognition task!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决我们的MNIST任务之前，我们将提供一些理论背景，涵盖不同的学习策略，并展示训练是如何实际进行的。然后，我们将直接将其中的一些概念应用到我们的例子中，以便我们的简单网络最终学会如何解决识别任务！
- en: Learning strategies
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习策略
- en: When it comes to teaching neural networks, there are three main paradigms, depending
    on the task and the availability of training data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在教授神经网络时，根据任务和训练数据的可用性，有三种主要的学习范式。
- en: Supervised learning
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: '*Supervised learning* may be the most common paradigm, and it is certainly
    the easiest to grasp. It applies when we want to *teach neural networks a mapping
    between two modalities* (for example, mapping images to their class labels or
    to their semantic masks). It requires access to a training dataset containing
    both the *images* and their *ground truth labels* (such as the class information
    per image or the semantic masks).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*有监督学习* 可能是最常见的学习范式，它也确实是最容易理解的。当我们想要*教神经网络进行两种模态之间的映射*时（例如，将图像映射到它们的类别标签或语义掩膜），就会应用这种方法。它需要访问一个包含*图像*和它们的*真实标签*（如每张图像的类别信息或语义掩膜）的训练数据集。'
- en: 'With this, the training is then straightforward:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，训练过程变得直接明了：
- en: Give the images to the network and collect its results (that is, predicted labels).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像输入网络并收集其结果（即预测标签）。
- en: Evaluate the network's *loss*, that is, how wrong its predictions are when comparing
    it to the ground truth labels.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估网络的*损失*，即在将网络预测与真实标签对比时，预测的错误程度。
- en: Adjust the network parameters accordingly to reduce this loss.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相应地调整网络参数，以减少损失。
- en: Repeat until the network *converges,* that is, until it cannot improve further
    on this training data.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复直到网络*收敛*，即直到它在这个训练数据上无法再进一步改善。
- en: Therefore, this strategy deserves the adjective *supervised—*an entity (us)
    supervises the training of the network by providing it with feedback for each
    prediction (the loss computed from the ground truths) so that the method can learn
    by repetition (*it was correct/false*; *try again*).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种策略值得称为*有监督的*——一个实体（我们）通过为每个预测提供反馈（从真实标签计算出的损失）来监督网络的训练，这样方法可以通过反复练习来学习（*正确/错误*；*再试一次*）。
- en: Unsupervised learning
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: However, how do we train a network when we do not have any ground truth information
    available? *Unsupervised learning* is one answer to this. The idea here is to
    craft a function that *computes the network's loss only based on its input and
    its corresponding output*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们没有任何真实标签信息时，如何训练一个网络呢？*无监督学习*是一个答案。这里的想法是构建一个函数，*仅基于输入和对应输出来计算网络的损失*。
- en: This strategy applies very well to applications such as clustering (grouping
    images with similar properties together) or compression (reducing the content
    size while preserving some properties). For clustering, the loss function could
    measure how similar images from one cluster are compared to images from other
    clusters. For compression, the loss function could measure how well preserved
    the important properties are in the compressed data compared to the original ones.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略非常适用于聚类（将具有相似属性的图像分组）或压缩（在保持某些属性的同时减少内容大小）等应用。对于聚类，损失函数可以衡量一个簇中的图像与其他簇中的图像的相似度。对于压缩，损失函数可以衡量压缩数据中重要属性与原始数据中重要属性的保持情况。
- en: Unsupervised learning thus requires some *expertise* regarding the use cases
    so that we can come up with meaningful loss functions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习因此需要一些*专业知识*，以便我们能够提出有意义的损失函数。
- en: Reinforcement learning
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: '*Reinforcement learning* is an **interactive strategy**. An *agent* navigates
    through an *environment* (for example, a robot moving around a room or a video
    game character going through a level). The agent has a predefined list of actions
    it can make (*walk*, *turn*, *jump*, and so on) and, after each action, it ends
    up in a new *state*. Some states can bring *rewards*, which are immediate or delayed,
    and positive or negative (for instance, a positive reward when the video game
    character touches a bonus item, and a negative reward when it is hit by an enemy).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是一种**互动策略**。一个*智能体*在*环境*中导航（例如，一个机器人在房间内移动，或者一个视频游戏角色通过关卡）。智能体有一组预定义的动作列表（*走*、*转弯*、*跳跃*等），每次动作后，它都会进入一个新的*状态*。一些状态可能会带来*奖励*，这些奖励可以是即时的或延迟的，也可以是正向的或负向的（例如，视频游戏角色触碰到一个奖励物品时会得到正向奖励，或被敌人击中时会受到负向奖励）。'
- en: At each instant, the neural network is provided only with *observations* from
    the environment (for example, the robot's visual feed, or the video game screen)
    and reward feedback (the *carrot and stick*). From this, it has to learn what
    brings higher rewards and *estimate the best short-term or long-term policy for
    the agent* accordingly. In other words, it has to estimate the series of actions
    that would maximize its end reward.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时刻，神经网络只会接收到来自环境的*观察*（例如，机器人的视觉信息或视频游戏屏幕）和奖励反馈（*胡萝卜与大棒*）。根据这些，它需要学习什么能带来更高的奖励，并*相应地估计智能体的最佳短期或长期策略*。换句话说，它需要估计一系列动作，这些动作将最大化最终的奖励。
- en: Reinforcement learning is a powerful paradigm, but it is less commonly applied
    to computer vision use cases. It won't be presented further here, though we encourage
    machine learning enthusiasts to learn more.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种强大的范式，但它在计算机视觉领域的应用较少。在这里我们将不再进一步介绍，但我们鼓励机器学习爱好者深入了解。
- en: Teaching time
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教学时间
- en: Whatever the learning strategy, the overall training steps are the same. Given
    some training data, the network makes its predictions and receives some feedback
    (such as the results of a loss function), which is then used to update the network's
    parameters. These steps are then repeated until the network cannot be optimized
    further. In this section, we will detail and implement this process, from loss
    computation to weights optimization.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 无论采用什么学习策略，整体的训练步骤都是相同的。给定一些训练数据，网络会做出预测并获得反馈（例如损失函数的结果），然后用这些反馈来更新网络的参数。这些步骤会不断重复，直到网络无法进一步优化为止。在本节中，我们将详细说明并实现这一过程，从损失计算到权重优化。
- en: Evaluating the loss
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估损失
- en: The goal of the *loss function* is to evaluate how well the network, with its
    current weights, is performing. More formally, this function expresses the *quality
    of the predictions as a function of the network's parameters* (such as its weights
    and biases). The smaller the loss, the better the parameters are for the chosen
    task.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*的目标是评估网络在当前权重下的表现。更正式地说，这个函数表示*预测质量与网络参数*（如权重和偏置）之间的关系。损失越小，参数就越适合所选任务。'
- en: 'Since loss functions represent the goal of networks (*return the correct labels*,
    *compress the image while preserving the content*, and so on), there are as many
    different functions as there are tasks. Still, some loss functions are more commonly
    used than others. This is the case for the *sum-of-squares* function, also called
    **L2 loss** (based on the L2 norm), which is omnipresent in supervised learning.
    This function simply computes the squared difference between each element of the
    output vector *y* (the per-class probabilities estimated by our network) and each
    element of the ground truth vector *y^(true)* (the target vector with null values
    for every class but the correct one):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 由于损失函数代表了网络的目标（*返回正确的标签*，*在保持内容的同时压缩图像*，等等），所以任务越多，损失函数的种类就越多。然而，有些损失函数比其他的更常用。这就是*平方和*函数，也叫**L2损失**（基于L2范数），它在监督学习中无处不在。这个函数简单地计算输出向量*y*（我们的网络估计的每类概率）与真实值向量*y^(true)*（每个类别值为零，除了正确的类别）的每个元素之间的平方差：
- en: '![](img/0e0dfbfc-c782-426c-a6fe-76401aa59515.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e0dfbfc-c782-426c-a6fe-76401aa59515.png)'
- en: 'There are plenty of other losses with different properties, such as **L1 loss**,
    which computes the *absolute difference* between the vectors, or **binary cross-entropy**
    (**BCE**) loss, which converts the predicted probabilities into a logarithmic
    scale before comparing them to the expected values:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他具有不同特性的损失函数，例如**L1损失**，它计算向量之间的*绝对差*，或者**二元交叉熵**（**BCE**）损失，它在将预测的概率与期望值比较之前，会将其转换为对数尺度：
- en: '![](img/e5968b1f-69bc-4df1-bae3-7981bc4fd5f7.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5968b1f-69bc-4df1-bae3-7981bc4fd5f7.png)'
- en: The logarithmic operation converts the probabilities from [0, 1] into [-![](img/e7bf8923-5c1e-4e3d-bc50-0f943f7d31e7.png),
    0]. So, by multiplying the results by -1, the loss value moves from +![](img/edb35237-2b09-485c-84e3-05e05cf4f2ba.png) 
    to 0 as the neural network learns to predict properly. Note that the cross-entropy
    function can also be applied to multi-class problems (not just binary).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对数运算将概率从[0, 1]转换为[-![](img/e7bf8923-5c1e-4e3d-bc50-0f943f7d31e7.png)，0]。因此，通过将结果乘以-1，损失值从+![](img/edb35237-2b09-485c-84e3-05e05cf4f2ba.png)移动到0，因为神经网络学习如何正确预测。请注意，交叉熵函数也可以应用于多类别问题（不仅仅是二元问题）。
- en: It is also common for people to divide the losses by the number of elements
    in the vectors, that is, computing the mean instead of the sum. The **mean square
    error** (**MSE**) is the averaged version of the L2 loss, and the **mean absolute
    error** (**MAE**) is the average version of the L1 loss.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常会将损失除以向量中的元素数量，也就是说，计算均值而不是总和。**均方误差**（**MSE**）是L2损失的平均版本，而**均绝对误差**（**MAE**）是L1损失的平均版本。
- en: For now, we will stick with the L2 loss as an example. We will use it for the
    rest of the theoretical explanations, as well as to train our MNIST classifier.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将以L2损失为例。我们将使用它进行后续的理论解释，并用它来训练我们的MNIST分类器。
- en: Backpropagating the loss
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播损失
- en: How can we update the network parameters so that they minimize the loss? For
    each parameter, what we need to know is how slightly changing its value would
    affect the loss. If we know which changes would slightly decrease the loss, then
    it is just a matter of applying these changes and repeating the process until
    reaching a minimum. This is exactly what the *gradient* of the loss function expresses,
    and what the *gradient descent* process is.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何更新网络参数以使它们最小化损失呢？对于每个参数，我们需要知道稍微改变它的值会如何影响损失。如果我们知道哪些改变会稍微减少损失，那么只需要应用这些改变，并重复这一过程直到达到最小值。这正是损失函数的*梯度*所表达的内容，也是*梯度下降*过程的本质。
- en: 'At each training iteration, the derivatives of the loss with respect to each
    parameter of the network are computed. These derivatives indicate which small
    changes to the parameters need to be applied (with a -1 coefficient since the
    gradient indicates the direction of increase of the function, while we want to
    minimize it). It can be seen as walking step by step down the *slope* of the loss
    function with respect to each parameter, hence the name **gradient descent** for
    this iterative process (refer to the following diagram):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练迭代中，都会计算损失对网络每个参数的导数。这些导数表示需要对参数进行哪些小的调整（由于梯度表示的是函数增加的方向，因此需要乘以-1，因为我们希望最小化损失）。可以将其视为沿着损失函数相对于每个参数的*slope*逐步下降，因此这个迭代过程被称为**梯度下降**（参见下图）：
- en: '![](img/2cec2eab-f584-4603-a985-15aa12a040ba.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cec2eab-f584-4603-a985-15aa12a040ba.png)'
- en: 'Figure 1.15: Illustrating the gradient descent to optimize a parameter *P*
    of the neural network'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15：说明梯度下降如何优化神经网络的参数*P*
- en: 'The question now is, how can we compute all of these derivatives (the *slope* values
    as a function of each parameter)? This is where the **chain rule** comes to our
    aid. Without going too deep into calculus, the chain rule tells us that the derivatives
    with respect to the parameters of a layer, *k*, can be *simply* computed with
    the input and output values of that layer (*x*[*k*], *y*[*k*]), and the derivatives
    of the following layer, *k* + 1\. More formally, for the layer''s weights, *W*[*k*],
    we have the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们如何计算所有这些导数（作为每个参数的*斜率*值）？这时，**链式法则**为我们提供了帮助。链式法则告诉我们，关于某一层的参数（*k*）的导数可以通过该层的输入和输出值（*x*[*k*]，*y*[*k*]）以及下一层（*k*
    + 1）的导数*简单*地计算出来。更正式地，对于该层的权重*W*[*k*]，我们有以下公式：
- en: '![](img/aa5594e4-861d-4262-bf99-d7a22d877e22.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa5594e4-861d-4262-bf99-d7a22d877e22.png)'
- en: 'Here, *l*''[*k*+1] is the derivative that is computed for layer *k* + 1 with
    respect to its input, *x*[*k*+1] = *y*[*k*], with *f*''[*k*] being the derivative
    of the layer''s activation function, and ![](img/8df72250-938a-4586-a84c-345f20534504.png)
    being the *transpose* of *x*. Note that *z*[*k*] represents the result of the
    weighted sum performed by the layer *k* (that is, before the input of the layer''s
    activation function), as defined in the *Layering neurons together *section. Finally,
    the ![](img/48ca7008-9ba0-43bc-b7b1-f6e517d07566.png) symbol represents the *element-wise
    multiplication* between two vectors/matrices. It is also known as the *Hadamard
    product*. As shown in the following equation, it basically consists of multiplying
    the elements pair-wise:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*l*'[*k*+1]是计算出的关于层*k* + 1相对于其输入* x*[*k*+1] = *y*[*k*]的导数，其中*f*'[*k*]是该层激活函数的导数，![](img/8df72250-938a-4586-a84c-345f20534504.png)是*x*的*转置*。注意，*z*[*k*]表示层*k*执行的加权求和结果（即，在输入层激活函数之前的结果），如在*层叠神经元*部分中定义。最后，![](img/48ca7008-9ba0-43bc-b7b1-f6e517d07566.png)符号表示两个向量/矩阵之间的*逐元素相乘*，也称为*Hadamard积*。如以下方程所示，它基本上是成对地相乘元素：
- en: '![](img/b9c4ac95-ae33-4c81-be83-e8d48a69ace1.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9c4ac95-ae33-4c81-be83-e8d48a69ace1.png)'
- en: 'Back to the chain rule, the derivatives with respect to the bias can be computed
    in a similar fashion, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 回到链式法则，关于偏置的导数可以通过类似的方式计算，如下所示：
- en: '![](img/8970bce6-2446-4d8f-9913-4df643ef4a6e.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8970bce6-2446-4d8f-9913-4df643ef4a6e.png)'
- en: 'Finally, to be exhaustive, we have the following equation:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了全面性，我们得到以下方程：
- en: '![](img/24941b4e-17b3-4ddf-af12-40a787c5ce4a.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24941b4e-17b3-4ddf-af12-40a787c5ce4a.png)'
- en: These calculations may look complex, but we only need to understand what they
    represent—we can compute how each parameter affects the loss recursively, layer
    by layer, going backward (using the derivatives for a layer to compute the derivatives
    for the previous layer). This concept can also be illustrated by representing
    neural networks as *computational graphs*, that is, as graphs of mathematical
    operations chained together (the weighted summation of the first layer is performed
    and its result is passed to the first activation function, then its own output
    is passed to the operations of the second layer, and so on). Therefore, computing
    the result of a whole neural network with respect to some inputs consists of *forwarding*
    the data through this computational graph, while obtaining the derivatives with
    respect to each of its parameters consists of propagating the resulting loss through
    the graph backward, hence the term **backpropagation**.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算看起来可能很复杂，但我们只需要理解它们表示什么——我们可以递归地计算每个参数如何影响损失，逐层向后进行（使用某一层的导数来计算前一层的导数）。这个概念也可以通过将神经网络表示为*计算图*来说明，即将数学操作链接在一起的图（第一层的加权求和操作执行，然后其结果传递给第一个激活函数，接着它的输出传递给第二层的操作，以此类推）。因此，计算整个神经网络相对于某些输入的结果，实际上就是将数据通过这个计算图进行*前向传递*，而获取相对于每个参数的导数，实际上是通过反向传播结果损失来实现的，因此这个过程被称为**反向传播**。
- en: 'To start this process by the output layer, the derivatives of the loss itself
    with respect to the output values are needed (refer to the previous equation).
    Therefore, it is primordial that the loss function can be easily derived. For
    instance, the derivative of the L2 loss is simply the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从输出层开始这个过程，需要计算损失相对于输出值的导数（参见前面的公式）。因此，损失函数需要容易求导。例如，L2损失的导数就是以下内容：
- en: '![](img/ed9c6804-44af-4058-aab8-6014e2e23030.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed9c6804-44af-4058-aab8-6014e2e23030.png)'
- en: 'As we mentioned earlier, once we know the loss derivatives with respect to
    each parameter, it is just a matter of updating them accordingly:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一旦我们知道每个参数的损失导数，接下来的任务就是相应地更新它们：
- en: '![](img/71e98db2-8295-4c1e-b096-7f5f25d6e86a.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71e98db2-8295-4c1e-b096-7f5f25d6e86a.png)'
- en: 'As we can see, the derivatives are often multiplied by a factor ![](img/5b725f39-5b64-4faf-80c2-6ca9ce5b412d.png)
    (*epsilon*) before being used to update the parameters. This factor is called
    the **learning rate**. It helps to control how strongly each parameter should
    be updated at each iteration. A large learning rate may allow the network to learn
    faster, but with the risk of making steps so big that the network may *miss* the
    loss minimum. Therefore, its value should be set with care. Let''s now summarize
    the complete training process:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，导数通常会乘以一个因子 ![](img/5b725f39-5b64-4faf-80c2-6ca9ce5b412d.png) (*epsilon*)，然后用于更新参数。这个因子被称为**学习率**。它有助于控制每次迭代时每个参数的更新强度。较大的学习率可能让网络学习得更快，但也有可能步伐过大，导致网络
    *错过* 损失的最小值。因此，学习率的值需要小心设置。现在，让我们总结一下完整的训练过程：
- en: Select the *n* next training images and feed them to the network.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择接下来的 *n* 张训练图像，并将它们输入到网络中。
- en: Compute and backpropagate the loss, using the chain rule to get the derivatives
    with respect to the parameters of the layers.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并反向传播损失，使用链式法则计算相对于各层参数的导数。
- en: Update the parameters with the values of the corresponding derivatives (scaled
    with the learning rate).
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相应导数的值（按学习率缩放）来更新参数。
- en: Repeat steps 1 to 3 to iterate over the whole training set.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1到3，遍历整个训练集。
- en: Repeat steps 1 to 4 until convergence or until a fixed number of iterations.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1到4，直到收敛或达到固定的迭代次数。
- en: One iteration over the whole training set (*steps 1* to* 4*) is called an **epoch**.
    If *n* = 1 and the training sample is randomly selected among the remaining images,
    this process is called **stochastic gradient descent** (**SGD**), which is easy
    to implement and visualize, but slower (more updates are done) and *noisier*.
    People tend to prefer *mini-batch stochastic gradient descent*. It implies using
    larger *n* values (limited by the capabilities of the computer) so that the gradient
    is averaged over each *mini-batch* (or, more simply, named *batch*) of *n* random
    training samples (and is thus less noisy).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对整个训练集进行一次迭代（*步骤1*到*4*）被称为**一个周期**（**epoch**）。如果 *n* = 1 且训练样本是从剩余图像中随机选择的，那么这个过程被称为**随机梯度下降**（**SGD**），这种方法易于实现和可视化，但速度较慢（需要更多更新）且
    *更嘈杂*。人们更倾向于使用 *小批量随机梯度下降*。这意味着使用较大的 *n* 值（受到计算机性能的限制），这样梯度会在每个 *mini-batch*（或更简单地称为
    *batch*）的 *n* 个随机训练样本上平均，从而减少了噪声。
- en: Nowadays, the term SGD is commonly used, regardless of the value of *n*.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，术语 SGD 被广泛使用，不论 *n* 的值是多少。
- en: In this section, we have covered how neural networks are trained. It is now
    time to put this into practice!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们已经讲解了神经网络的训练过程。现在是时候将其付诸实践了！
- en: Teaching our network to classify
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教会我们的网络进行分类
- en: 'So far, we have only implemented the feed-forward functionality for our network
    and its layers. First, let''s update our `FullyConnectedLayer` class so that we
    can add methods for backpropagation and optimization:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅实现了网络及其层的前向传播功能。首先，让我们更新 `FullyConnectedLayer` 类，以便我们可以为反向传播和优化添加方法：
- en: '[PRE7]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The code presented in this section has been simplified and stripped of comments
    to keep its length reasonable. The complete sources are available in this book's
    GitHub repository, along with a Jupyter notebook that connects everything together.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的代码经过简化并删除了注释，以保持合理的长度。完整的源代码可以在本书的 GitHub 仓库中找到，并附有一个将所有内容连接起来的 Jupyter
    笔记本。
- en: 'Now, we need to update the `SimpleNetwork` class by adding methods to backpropagate
    and optimize layer by layer, and a final method to cover the complete training:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要通过逐层反向传播和优化的方法，更新 `SimpleNetwork` 类，并添加一个最终的方法来涵盖整个训练过程：
- en: '[PRE8]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Everything is now ready! We can train our model and see how it performs:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都准备好了！我们可以训练我们的模型，看看它的表现：
- en: '[PRE9]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Congratulations! If your machine is powerful enough to complete this training
    (this simple implementation does not take advantage of the GPU), we just obtained
    our very own neural network that is able to classify handwritten digits with an
    accuracy of ~94.8%!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！如果你的机器足够强大，可以完成这个训练（这个简单的实现并未利用GPU），我们刚刚获得了我们自己的神经网络，它能够以大约94.8%的准确率对手写数字进行分类！
- en: Training considerations – underfitting and overfitting
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练考虑因素 – 欠拟合与过拟合
- en: We invite you to play around with the framework we just implemented, trying
    different *hyperparameters* (layer sizes, learning rate, batch size, and so on).
    Choosing the proper topography (as well as other *hyperparameters*) can require
    lots of tweaking and testing. While the sizes of the input and output layers are
    conditioned by the use case (for example, for classification, the input size would
    be the number of pixel values in the images, and the output size would be the
    number of classes to predict from), the hidden layers should be carefully engineered.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请你尝试我们刚刚实现的框架，尝试不同的*超参数*（层大小、学习率、批量大小等）。选择合适的拓扑结构（以及其他*超参数*）可能需要大量的调整和测试。虽然输入和输出层的大小由使用案例决定（例如，对于分类任务，输入大小是图像中的像素值数量，输出大小是要预测的类别数量），但隐藏层应谨慎设计。
- en: For instance, if the network has too few layers, or the layers are too small,
    the accuracy may stagnate. This means the network is **underfitting**, that is,
    it does not have enough parameters for the complexity of the task. In this case,
    the only solution is to adopt a new architecture that is more suited to the application.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果网络层数过少，或层太小，准确率可能会停滞不前。这意味着网络**欠拟合**，即它没有足够的参数来应对任务的复杂性。在这种情况下，唯一的解决方案是采用一种更适合应用的新架构。
- en: 'On the other hand, if the network is too complex and/or the training dataset
    is too small, the network may start **overfitting** the training data. This means
    that the network will learn to fit very well to the training distribution (that
    is, its particular noise, details, and so on), but won''t generalize to new samples
    (since these new images may have a slightly different noise, for instance). The
    following diagram highlights the differences between these two problems. The regression
    method on the extreme left does not have enough parameters to model the data variations,
    while the method on the extreme right has too many, which means it will struggle
    to generalize:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果网络过于复杂和/或训练数据集过小，网络可能会开始**过拟合**训练数据。这意味着网络会很好地拟合训练分布（即它的特定噪声、细节等），但无法对新样本进行泛化（因为这些新图像可能会有稍微不同的噪声，例如）。下面的图表突出了这两种问题之间的区别。最左边的回归方法没有足够的参数来建模数据的变化，而最右边的方法参数过多，这意味着它将难以泛化：
- en: '![](img/df4d89d2-9ef3-4ba9-bb46-d20f537ea3ec.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df4d89d2-9ef3-4ba9-bb46-d20f537ea3ec.png)'
- en: 'Figure 1.16: A common illustration of underfitting and overfitting'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：过拟合和欠拟合的常见示意图
- en: While gathering a larger, more diverse training dataset seems the logical solution
    to overfitting, it is not always possible in practice (for example, due to limited
    access to the target objects). Another solution is to adapt the network or its
    training in order to constrain how much detail the network learns. Such methods
    will be detailed in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, among other advanced neural network solutions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然收集更大、更具多样性的训练数据集似乎是解决过拟合的逻辑方案，但在实践中并不总是可行的（例如，由于访问目标对象的限制）。另一种解决方案是调整网络或其训练，以约束网络学习的细节量。这些方法将在[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)《现代神经网络》中详细介绍，以及其他高级神经网络解决方案。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We covered a lot of ground in this first chapter. We introduced computer vision,
    the challenges associated with it, and some historical methods, such as SIFT and
    SVMs. We got familiar with neural networks and saw how they are built, trained,
    and applied. After implementing our own classifier network from scratch, we can
    now better understand and appreciate how machine learning frameworks work.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们覆盖了很多内容。我们介绍了计算机视觉，相关的挑战，以及一些历史方法，如SIFT和SVM。我们熟悉了神经网络，并了解了它们是如何构建、训练和应用的。在从零开始实现我们自己的分类器网络后，我们现在可以更好地理解和欣赏机器学习框架的工作原理。
- en: With this knowledge, we are now more than ready to start with TensorFlow in
    the next chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些知识，我们现在已经完全准备好在下一章开始使用TensorFlow了。
- en: Questions
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which of the following tasks does not belong to computer vision?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个任务不属于计算机视觉范畴？
- en: A web search for images similar to a query
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据查询搜索相似的图像
- en: A 3D scene reconstruction from image sequences
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像序列中重建3D场景
- en: Animation of a video character
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频角色的动画
- en: Which activation function were the original perceptrons using?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始感知机使用了哪种激活函数？
- en: Suppose we want to train a method to detect whether a handwritten digit is a
    4 or not. How should we adapt the network that we implemented in this chapter
    for this task?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们想训练一种方法来检测手写数字是否为4。我们应该如何调整我们在本章中实现的网络来完成这个任务？
- en: Further reading
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Hands-On Image Processing with Python* ([https://www.packtpub.com/big-data-and-business-intelligence/hands-image-processing-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-image-processing-python)),
    by Sandipan Dey: A great book to learn more about image processing itself, and
    how Python can be used to manipulate visual data'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python图像处理实践*（[https://www.packtpub.com/big-data-and-business-intelligence/hands-image-processing-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-image-processing-python)），作者：Sandipan
    Dey：一本很好的书，帮助你深入了解图像处理本身，以及如何使用Python来处理视觉数据'
- en: '*OpenCV 3.x with Python By Example – Second Edition* ([https://www.packtpub.com/application-development/opencv-3x-python-example-second-edition](https://www.packtpub.com/application-development/opencv-3x-python-example-second-edition)),
    by Gabriel Garrido and Prateek Joshi: Another recent book introducing the famous
    computer vision library *OpenCV*, which has been around for years (it implements
    some of the traditional methods we introduced in this chapter, such as edge detectors,
    SIFT, and SVM)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Python进行OpenCV 3.x实践 – 第二版*（[https://www.packtpub.com/application-development/opencv-3x-python-example-second-edition](https://www.packtpub.com/application-development/opencv-3x-python-example-second-edition)），作者：Gabriel
    Garrido 和 Prateek Joshi：另一本最近出版的书，介绍了著名的计算机视觉库*OpenCV*，这个库已经存在多年（它实现了我们在本章中介绍的一些传统方法，如边缘检测器、SIFT和SVM）'
