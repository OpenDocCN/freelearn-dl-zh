- en: Chapter 13. Extending Deep Learning with Theano
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章：使用Theano扩展深度学习
- en: This chapter gives clues to go further with both Theano and Deep Learning. First,
    it presents how to create new operators for the Theano computation graph in Python
    or C, either for the CPU or the GPU. Then, interactions with other Deep Learning
    frameworks are studied with the support of code repositories and libraries that
    enable back-and-forth conversion with other technologies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为进一步深入Theano和深度学习提供了线索。首先，它介绍了如何在Python或C语言中为Theano计算图创建新的操作符，无论是针对CPU还是GPU。然后，研究了与其他深度学习框架的交互，借助代码库和库的支持，实现与其他技术的双向转换。
- en: Lastly, to complete the possibilities offered by the field of Deep Learning
    with Theano, we develop the concepts of a new **General Artificial Intelligence**
    field.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了完善Theano深度学习领域所提供的可能性，我们发展了一个新的**通用人工智能**领域的概念。
- en: 'The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Writing new operators for Theano computation graphs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为Theano计算图编写新操作符
- en: Python code for CPU and GPU
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对CPU和GPU的Python代码
- en: The C API for CPU and GPU
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对CPU和GPU的C语言API
- en: Sharing models with other Deep Learning frameworks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他深度学习框架共享模型
- en: Cloud GPUs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云GPU
- en: Meta learning, gradual learning, and guided learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元学习、渐进学习和引导学习
- en: General Artificial Intelligence
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般人工智能
- en: This chapter gives a complete overview of Deep Learning with Theano.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章全面概述了使用Theano进行深度学习的内容。
- en: Theano Op in Python for CPU
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的Theano操作（针对CPU）
- en: As a mathematical compilation engine, Theano's purpose is to compile a graph
    of computations in an optimal way for a target platform.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个数学编译引擎，Theano的目的是以最优的方式为目标平台编译计算图。
- en: The development of new operators is possible in Python or C for compilation
    either on the CPU or GPU.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 新操作符的开发可以在Python或C语言中进行编写，进行CPU或GPU的编译。
- en: First, we address the simplest case, in Python for CPU, which will enable you
    to add new operations very easily and quickly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们解决最简单的情况，在CPU的Python中，它将使你能够非常容易和快速地添加新操作。
- en: To fix the ideas, let's implement a simple affine operator that performs the
    affine transformation *a * x + b*, given x as the input.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了固定概念，让我们实现一个简单的仿射操作符，执行仿射变换 *a * x + b*，其中x为输入。
- en: 'The operator is defined by a class deriving from the generic `theano.Op` class:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符是通过从通用的`theano.Op`类派生的类来定义的：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's understand this example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解这个例子。
- en: 'The `__props__` property is set to the two parameter names, `a` and `b`, on
    which the operator depends. It will automatically generate the `__eq__()`, `__hash__()`,
    and `__str_()` methods for us so that if we create two different objects with
    the same values for parameters `a` and `b`, Theano will consider them as equal
    operators:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`__props__`属性被设置为操作符依赖的两个参数名，`a`和`b`。它将自动为我们生成`__eq__()`、`__hash__()`和`__str_()`方法，因此，如果我们创建两个具有相同`a`和`b`参数值的不同对象，Theano会将它们视为相等的操作符：'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Also, the parameters `a` and `b` will appear when printing the op:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，打印操作时，参数`a`和`b`将会出现：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Theano Op in Python for CPU](img/00267.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Theano操作符在Python中的实现（针对CPU）](img/00267.jpeg)'
- en: If `__props__` is not specified, it is required to define the `__eq__()`, `__hash__()`,
    and `__str_()` methods manually.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未指定`__props__`，则需要手动定义`__eq__()`、`__hash__()`和`__str_()`方法。
- en: The `make_node()` method creates the node to be included in the graph and is
    run when the `mult4plus5op` object is applied to the input `x`. Node creation
    is performed with the `theano.Apply()` method that takes as arguments the input
    variables and the type of the output. To enforce that the inputs are variables,
    the `as_tensor_variable()` method is called on the input to transform any NumPy
    array into a variable. This is the place where we define the type of the output
    given the input as well as to check whether the inputs are compatible with the
    operator and raise a TypeError otherwise.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_node()`方法创建将被包含在计算图中的节点，并在将`mult4plus5op`对象应用于输入`x`时运行。节点创建通过`theano.Apply()`方法执行，该方法的参数是输入变量和输出类型。为了强制输入为变量，调用`as_tensor_variable()`方法，将任何NumPy数组转换为变量。这是我们定义输出类型的地方，给定输入时还需检查输入是否与操作符兼容，否则会引发TypeError。'
- en: 'Note that it is possible to generate the `make_node()` method automatically,
    as we did previously with the `__props__` attribute for the `__eq__()` method,
    but in this case, with the `itypes` and `otypes` properties defining the types
    of the inputs and outputs:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如我们之前为 `__eq__()` 方法使用 `__props__` 属性那样，自动生成 `make_node()` 方法也是可能的，但在这种情况下，`itypes`
    和 `otypes` 属性用于定义输入和输出的类型：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `perform()` method defines the computations in Python to be performed for
    this operator. Since it is possible to implement operators on multiple inputs
    that return multiple outputs, the inputs and outputs are given as lists. A second
    output would be stored in `output_storage[1][0]`. Outputs might be already allocated
    by previous values in order to reuse memory. They will always be of the good `dtype`
    object, but not necessary of the right shape and stride. It is good to re-allocate
    them when they are not of the good shape.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`perform()` 方法定义了在 Python 中执行的该操作符的计算。由于可以实现多个输入并返回多个输出的操作符，因此输入和输出以列表形式给出。第二个输出将存储在
    `output_storage[1][0]` 中。输出可能已经由前面的值分配，以便重用内存。它们将始终是合适的 `dtype` 对象，但不一定是正确的形状和步幅。它们不符合正确形状时，最好重新分配。'
- en: 'The last two methods, `infer_shape()` and `grad()`, are optional. The first
    one is used when the output does not need to be computed, but only a shape information
    is necessary to perform the computation—such a case occurs during Theano optimization
    procedures. The second is used when the output needs to be differentiated under
    the `grad()` method:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的两个方法，`infer_shape()` 和 `grad()`，是可选的。第一个方法用于输出无需计算时，仅需形状信息来进行计算的情况——这种情况通常发生在
    Theano 优化过程中。第二个方法用于在 `grad()` 方法下对输出进行求导：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the same way, it is possible to define the R-operator function of the operator.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以相同的方式，可以定义操作符的 R-操作符函数。
- en: Theano Op in Python for the GPU
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 GPU 的 Theano 操作符（Op）
- en: 'Let''s take a look at what happens when we run this operator in a graph in
    the GPU `config` mode:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 GPU `config` 模式下运行该操作符时会发生什么：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since we have only defined a CPU implementation of the new operator in Python
    and the full graph is running on GPU, the data is transferred back and forth to
    CPU in the middle of the graph to apply our new CPU operator:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅在 Python 中定义了新操作符的 CPU 实现，而完整的图计算是在 GPU 上运行的，因此数据会在图的中间来回传输到 CPU，以应用我们新的
    CPU 操作符：
- en: '![Theano Op in Python for the GPU](img/00268.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![用于 GPU 的 Theano 操作符（Op）](img/00268.jpeg)'
- en: To avoid the inefficiency of the transfers inside the graph, let's create the
    same operator in Python for the GPU.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免图中传输的不效率问题，让我们为 GPU 创建相同的 Python 操作符。
- en: 'For this, you will have to simply modify the `make_node()` and `perform()`
    methods of the operator, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您只需修改操作符的 `make_node()` 和 `perform()` 方法，如下所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Not many changes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 没有太多变化。
- en: In the `make_node()` method, `as_tensor_variable()` is replaced by `as_gpuarray_variable()`,
    which requires the context that is one part of the type definition of a GPU variable.
    The `get_context()` method transforms the context name we have chosen for the
    device into a `GPUContext` for the `pygpu` library.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `make_node()` 方法中，`as_tensor_variable()` 被 `as_gpuarray_variable()` 替换，后者需要上下文，这也是
    GPU 变量类型定义的一部分。`get_context()` 方法将我们为设备选择的上下文名称转换为 `pygpu` 库中的 `GPUContext`。
- en: In the `perform()` method, computations are performed on GPU thanks to the `pygpu`
    library that contains an element-wise operator on GPU as well as the **Basic Linear
    Algebra Subprograms** (**BLAS**) methods, such as the **GEneral Matrix to Matrix
    Multiplication** (**GEMM**) and **General Matrix to Vector Multiplication** (**GEMV**)
    operations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `perform()` 方法中，计算在 GPU 上执行，得益于 `pygpu` 库，它包含了 GPU 上的逐元素操作符以及 **基本线性代数子程序**
    (**BLAS**) 方法，如 **通用矩阵乘矩阵运算** (**GEMM**) 和 **通用矩阵向量乘法** (**GEMV**) 操作。
- en: 'Let''s now take a look at the compiled graph when this new operator is inside
    a bigger graph on GPU:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下当这个新操作符在 GPU 上的更大图中时，编译后的图是什么样的：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Theano Op in Python for the GPU](img/00269.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![用于 GPU 的 Theano 操作符（Op）](img/00269.jpeg)'
- en: For readability, we have prefixed the name of the class of the operator for
    GPU with Gpu; for example, GpuAXPBOp.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性，我们将操作符类的名称前缀加上了 Gpu，例如，GpuAXPBOp。
- en: Theano Op in C for CPU
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 CPU 的 Theano 操作符（Op）
- en: Another inefficiency arises from the fact the Python implementation of an operator
    adds a significant overhead each time computations are performed, that is, for
    each instance of our operator in the graph. The Python code is not compiled as
    the rest of the graph by Theano in C and the overhead occurs when the C implementation
    is wrapped into Python and data is exchanged.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个低效之处在于，操作符的Python实现每次执行计算时都会增加显著的开销，即在图中每次操作符实例化时都会发生这种情况。与图的其他部分不同，Python代码不会像Theano将其他图部分用C编译一样进行编译，而是当C实现被包装到Python中并交换数据时，会发生这种开销。
- en: To remedy this, it is possible to directly write some C code that will be incorporated
    into the code of the rest of the graph and compiled together.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一点，可以直接编写一些C代码，将其并入图的其他代码中并一起编译。
- en: 'When implementing an operator directly in C, NumPy is the underlying library
    to manage arrays, with the the NumPy-API extending Python C-API. The Python class
    defining the new C operator does not have to implement the `perform()` method;
    instead, it returns the C code to incorporate in the `c_code()`, `c_support_code()`
    and `c_support_code_apply()` methods:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当直接用C实现操作符时，NumPy是管理数组的底层库，NumPy-API扩展了Python C-API。定义新C操作符的Python类不必实现`perform()`方法；相反，它返回要并入`c_code()`、`c_support_code()`和`c_support_code_apply()`方法的C代码：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s now discuss the different parts:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论不同的部分：
- en: When the `c_code_cache_version()` is implemented, Theano will cache the compiled
    code to save some compilation time the next time the operator is incorporated
    into a graph, but whenever we modify the code of the C op, the version number
    has to be incremented.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现`c_code_cache_version()`时，Theano会缓存已编译的代码，以便下次将操作符纳入图中时节省一些编译时间，但每当我们修改C操作符的代码时，版本号必须递增。
- en: The code placed in the `c_support_code()` and `c_support_code_apply()` methods
    is included in the global scope of the C program. The code placed in the `c_support_code_apply()`
    and `c_code()` methods has to be specific to each apply of the op in the graph;
    in particular, in this case, they depend on the type of the input. And since the
    `c_support_code_apply()` code is included in the global scope, the methods are
    named after the op name.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 放置在`c_support_code()`和`c_support_code_apply()`方法中的代码包含在C程序的全局作用域中。放置在`c_support_code_apply()`和`c_code()`方法中的代码必须是特定于图中每个操作符应用的；特别是，在这种情况下，它们取决于输入的类型。由于`c_support_code_apply()`代码包含在全局作用域中，方法的名称与操作符名称相同。
- en: '`PyArray_NDIM`, `PyArray_DIMS`, `PyArray_STRIDES`, and `PyArray_DATA` are the
    macros to access the number of dimensions, the dimensions, the strides of the
    array, and the data in the array, respectively, for each NumPy array in C, `PyArrayObject`.
    `PyArray_EMPTY` is the equivalent to the Python `numpy.empty()` method in C.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyArray_NDIM`、`PyArray_DIMS`、`PyArray_STRIDES`和`PyArray_DATA`是宏，用于访问每个NumPy数组（C中的`PyArrayObject`）的维数、维度、步长和数据。`PyArray_EMPTY`是C中等同于Python
    `numpy.empty()`方法的函数。'
- en: The NumPy `PyArrayObject` class inherits from the `PyObject` class from the
    Python C-API. The `Py_XDECREF` macro enables us to decrement the reference count
    to the output before memory is allocated for a new output array. As in the Python
    C-API, the NumPy C-API requires to correctly count references to objects. Theano
    does not guarantee that the output array has been allocated, nor does it guarantee
    if it has been allocated with the correct shape. This is why a test is performed
    at the beginning of the `c_code()`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy的`PyArrayObject`类继承自Python C-API中的`PyObject`类。`Py_XDECREF`宏允许我们在为新的输出数组分配内存之前减少输出的引用计数。像Python
    C-API一样，NumPy C-API要求正确计数对象的引用。Theano不能保证输出数组已被分配，也不能保证它是否已经按正确的形状分配。这就是为什么在`c_code()`开始时会进行测试。
- en: Note that arrays can be strided, since they can be a view (or a subtensor) of
    an array (a tensor). It is possible to implement ops that create views or modify
    the inputs, as well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数组可以是有步长的，因为它们可以是数组（张量）的视图（或子张量）。也可以实现创建视图或修改输入的操作。
- en: 'There exist a few other possible methods to go further in the C implementation:
    `c_libraries()` and `c_lib_dirs()` to use external libraries, `c_code_cleanup()`
    to destroy memory allocations, and `c_init_code()` to execute some code at initialization.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他可能的方法可以进一步优化C实现：`c_libraries()`和`c_lib_dirs()`用于使用外部库，`c_code_cleanup()`用于销毁内存分配，`c_init_code()`用于在初始化时执行某些代码。
- en: Lastly, it is also possible to reference some C files inside the code to reduce
    the burden on the Python class. We do not detail these three last specificities.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也可以在代码中引用一些 C 文件，以减少 Python 类的负担。我们不详细说明这三种特性。
- en: Theano Op in C for GPU
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Theano 在 C 中的 GPU 操作
- en: 'As you could have imagined, it is possible to combine both optimizations:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你想象的那样，可以结合两种优化：
- en: Reduce the Python/C overhead by programming directly in C
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过直接使用 C 编程减少 Python/C 开销
- en: Write the code for the GPU
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 GPU 代码
- en: To write CUDA code for GPU, the code that will be run in parallel on the numerous
    cores of the GPU has to be packaged into a special function type named **kernel**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为 GPU 编写 CUDA 代码时，必须将要在 GPU 上多个核心并行运行的代码包装成一种特殊的函数类型，称为 **内核**。
- en: 'For that purpose, the `__init__()`, `make_node()`, and `c_code_cache_version()`
    methods stay the same as for our Python example for GPU, but with a new `gpu_kernels()`
    method to define new GPU kernels and the `c_code()` method (which replaces the
    `perform()` method again) to implement the C code, also named the **host code**,
    that orchestrates how and when to call the different kernels on GPU:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，`__init__()`、`make_node()` 和 `c_code_cache_version()` 方法与我们在 GPU 上的 Python
    示例相同，但新增了 `gpu_kernels()` 方法，用于定义新的 GPU 内核，以及 `c_code()` 方法（它再次替代了 `perform()`
    方法），用于实现 C 代码，也称为 **主机代码**，该代码负责协调何时以及如何调用 GPU 上的不同内核：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that I implemented the operator for matrix (that is, two-dimensional) inputs
    only and the 256 threads will execute the same operations in parallel, while the
    operations could have been split into different groups and assigned to different
    threads.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我只实现了矩阵（即二维）输入的操作符，256 个线程将并行执行相同的操作，而这些操作本可以被分成不同的组，并分配给不同的线程。
- en: The host code run on the CPU manages memory on both the CPU and GPU, and also
    launches kernels which are functions executed on the GPU device.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上运行的主机代码管理 CPU 和 GPU 上的内存，并启动在 GPU 设备上执行的内核函数。
- en: The allocation of a new GPU array is performed with the `pygpu_zeros()` method,
    which will from behind call the `cudamalloc()` method when using CUDA to allocate
    the array directly in the GPU memory. The operator instance does not need to manage
    the release of the memory allocated to outputs as well as data transfer between
    GPU and CPU since this is the role of Theano optimization to decide when to insert
    the transfer operators `HostFromGpu` and `GpuFromHost`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 新 GPU 数组的分配是通过 `pygpu_zeros()` 方法执行的，使用 CUDA 时，它会在后台调用 `cudamalloc()` 方法，直接在
    GPU 内存中分配数组。操作符实例无需管理分配给输出的内存释放以及 GPU 和 CPU 之间的数据传输，因为这是 Theano 优化的职责，决定何时插入传输操作符
    `HostFromGpu` 和 `GpuFromHost`。
- en: The call to the kernel in the C code is performed via `axpb_call()`, that is,
    the name of the kernel followed by `_call()`. Note that there are four more arguments
    in the call than in the definition of the kernel method. These four arguments
    define how `libgpuarray` will execute or deploy the kernel on the cores.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C 代码中调用内核是通过 `axpb_call()` 来执行的，即内核的名称后面跟着 `_call()`。注意，调用中有四个参数，比定义内核方法时的参数多。这四个参数定义了
    `libgpuarray` 如何在核心上执行或部署内核。
- en: 'To understand the GPU execution configuration for parallel programming, let''s
    precise some basic concepts about a GPU first. A CUDA GPU is composed of **Streaming
    Multiprocessors** (**SM**), with a specification given by the compute capability
    in warp size, grid size, block size, the maximum number of threads per SM and
    per block, shared and local memory size, and maximum number of registrars:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 GPU 的并行编程执行配置，首先让我们明确一些关于 GPU 的基本概念。CUDA GPU 由 **流式多处理器**（**SM**）组成，其规格由计算能力、波大小、网格大小、块大小、每个
    SM 和每个块的最大线程数、共享和本地内存大小以及最大寄存器数等参数来定义：
- en: '![Theano Op in C for GPU](img/00270.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![Theano 在 C 中的 GPU 操作](img/00270.jpeg)'
- en: '(Source: [https://en.wikipedia.org/wiki/CUDA](https://en.wikipedia.org/wiki/CUDA))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：[https://en.wikipedia.org/wiki/CUDA](https://en.wikipedia.org/wiki/CUDA))
- en: During execution, multiprocessors execute instructions for a group of 32 threads
    (as described in the preceding table), named warp, in the **Single Instruction
    Multiple Data** (**SIMD**) manner. When programming for parallel execution, you
    need to organize your threads into blocks that are as close as possible to the
    underlying architecture. For example, for an element-wise operation on matrices,
    as our AXPBOp, you could say that each thread is going to perform the operation
    on one element of the matrix. So, a computation on a 224 x 224 image will require
    50,176 threads. Let's say that the GPU has 8 multiprocessors with 1024 cores each.
    In the execution configuration, you can, for example, define a block size of 256
    threads, and the number of blocks required to perform the complete computation
    will be 196 blocks. In order to simplify the development of parallel programs,
    blocks can be organized into a multidimensional grid (up to 3 dimensions for a
    CC above 2.0, as shown in the preceding table), and in the case of an image input,
    it would be natural to use a two-dimensional grid of 14 x 14 blocks. It is up
    to you to organize the threads into blocks organized on a grid, but the best way
    to organize the threads is to follow the dimensionality of the underlying data,
    since it will be easier to split the data and affect it to different threads.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行过程中，多处理器以 **单指令多数据** (**SIMD**) 方式执行一组 32 个线程的指令（如前表所示）。在进行并行执行编程时，你需要将线程组织成尽可能接近底层架构的块。例如，对于矩阵的逐元素操作，比如我们的
    AXPBOp，可以认为每个线程将对矩阵中的一个元素执行操作。所以，对一个 224 x 224 的图像进行计算将需要 50,176 个线程。假设 GPU 有
    8 个多处理器，每个多处理器有 1024 个核心。在执行配置中，你可以定义一个 256 线程的块大小，并且执行完整计算所需的块数将是 196 个。为了简化并行程序的开发，块可以组织成一个多维网格（对于
    CC 大于 2.0 的情况，最多支持 3 维，如前表所示），在图像输入的情况下，使用 14 x 14 块的二维网格是自然的。你可以自行组织线程到网格上的块中，但最佳的线程组织方式是遵循底层数据的维度，因为这样更容易将数据划分并分配给不同的线程。
- en: 'Each thread execution is provided with values to access its position in the
    grid that you can use inside the code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程执行时都会提供值，用于访问它在网格中的位置，这些值可以在代码中使用：
- en: '`gridDim.x`, `gridDim.y`, `gridDim.z` the dimensions of the grid of blocks
    of threads'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gridDim.x`、`gridDim.y`、`gridDim.z` 线程块网格的维度'
- en: '`blockIdx.x`, `blockIdx.y`, `blockIdx.z` the coordinate of the block on the
    grid'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blockIdx.x`、`blockIdx.y`、`blockIdx.z` 块在网格中的坐标'
- en: '`blockDim.x`, `blockDim.y`, `blockDim.z` the dimensions of the block'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blockDim.x`、`blockDim.y`、`blockDim.z` 块的维度'
- en: '`threadIdx.x`, `threadIdx.y`, `threadIdx.z` the coordinate of the thread in
    the block'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threadIdx.x`、`threadIdx.y`、`threadIdx.z` 线程在块中的坐标'
- en: 'In the case of our element-wise AXPBOp with one thread per element, the thread
    can fetch the data element given by the following row indice:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的逐元素 AXPBOp，每个线程处理一个元素，线程可以根据以下行索引获取数据元素：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To deploy, the first new four parameters in the kernel call correspond to:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 部署时，内核调用中的前四个新参数对应于：
- en: Dimensionality of the grid/blocks, in this case 2 for an image/matrice as input
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格/块的维度，在本例中是 2，表示输入为图像/矩阵
- en: The sizes of launch grid, in this case is {14, 14}. Once the number of threads
    per block is defined (256 in our case), the number of blocks per grid is then
    determined by the problem size (here, the size of the matrix).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动网格的大小，在本例中是 {14, 14}。一旦每块线程的数量被定义（在本例中为 256），每个网格的块数就由问题的大小来决定（这里是矩阵的大小）。
- en: 'The sizes of launch blocks, in this case {16, 16} to go for 256 threads per
    block, as it is usually set to 128 or 256\. It is better to choose a multiple
    of the warp size, since execution is performed per warp; if you set it to 250,
    then, 201 of our blocks will underperform: one warp of each block will not be
    used at its full parallel potential. It is possible to try different multiples
    of 32 and make the choice on the most efficient runs.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动块的大小，在本例中是 {16, 16}，用于每块 256 个线程，通常设置为 128 或 256。最好选择一个 warp 大小的倍数，因为执行是按
    warp 进行的；如果你设置为 250，那么 201 个块将表现不佳：每个块的一个 warp 将无法充分发挥并行潜力。可以尝试不同的 32 的倍数，选择最有效的执行结果。
- en: The amount of dynamic shared memory to allocate, which is required when you
    define a shared memory (with the `LOCAL_MEM` macro) that is dynamic (when the
    amount of shared memory is not known at compile time). Shared memory designates
    memory shared between threads belonging to the same block of threads. On devices
    of compute capability 2.x and 3.x, each multiprocessor has 64 KB of on-chip memory
    that can be partitioned between L1 cache and shared memory (16, 32, or 48K). The
    L1 cache coalesces global memory accesses by threads in a warp into as few cache
    lines as possible. The alignment differences between each thread have a negligible
    effect on performance thanks to the cache. Inefficiencies arise in the strided
    access for second and third dimensions; in this case, the use of shared memory
    enables you to extract a 2D tile of a multidimensional array from global memory
    in a coalesced fashion into shared memory and have contiguous threads stride through
    the shared memory tile:![Theano Op in C for GPU](img/00271.jpeg)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配动态共享内存的数量，这是在定义共享内存（使用`LOCAL_MEM`宏）时需要的，尤其是在共享内存的大小在编译时未知的情况下。共享内存指定的是在同一线程块内共享的内存。在计算能力2.x和3.x的设备上，每个多处理器有64KB的片上内存，可以在L1缓存和共享内存之间进行分配（16K、32K或48K）。L1缓存将线程在一个warp中的全局内存访问合并为尽可能少的缓存行。由于缓存的存在，每个线程之间的对齐差异对性能的影响可以忽略不计。第二和第三维度的跨步访问会引发效率问题；在这种情况下，使用共享内存使得你可以以合并的方式从全局内存中提取一个二维块到共享内存，并让连续的线程遍历共享内存块：![GPU上的Theano操作C语言实现](img/00271.jpeg)
- en: Coalesced transpose via shared memory, NVIDIA parallel for all
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过共享内存进行合并转置，NVIDIA并行计算
- en: When the dimension of the data is not divisible into a block size times a grid
    size, threads dealing with data at the border will execute faster than other threads,
    and the kernel code has to be written in a way to check for out-of-bounds memory
    accesses.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据的维度不能整除为块大小与网格大小的乘积时，处理边界数据的线程将比其他线程执行得更快，并且内核代码必须以检查越界内存访问的方式编写。
- en: When programming in parallel, race conditions, as well as memory bank conflicts
    in shared memory, and data that cannot stay local to the thread in the available
    registrars are some new pains to check. Coalescing global memory accesses is by
    far the most critical aspect of achieving good performance. The NVIDIA® Nsight™
    tool will help you develop, debug, and profile the code that executes on CPU and
    GPU.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行编程时，竞态条件、共享内存中的内存银行冲突以及数据无法保留在可用的寄存器中的情况是一些新的问题需要检查。合并全局内存访问是实现良好性能的最关键方面。NVIDIA®
    Nsight™ 工具将帮助你开发、调试和分析在CPU和GPU上执行的代码。
- en: Model conversions
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型转换
- en: 'When a model is saved, the resulting data is simply a list of arrays, that
    is, weight vectors (for biases) and matrices (for multiplications) and a name
    for each layer. It is quite simple to convert a model from one framework to another:
    it consists of loading a numerical array and checking the layer names. Here are
    a few conversion examples from and to Caffe Deep Learning framework written in
    C++:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型被保存时，得到的数据仅仅是一个数组列表，即权重向量（用于偏置）和矩阵（用于乘法运算）以及每一层的名称。将模型从一个框架转换到另一个框架非常简单：它包括加载一个数值数组并检查层的名称。以下是一些从Caffe深度学习框架（用C++编写）到其他框架的转换示例：
- en: '[https://github.com/an-kumar/caffe-theano-conversion](https://github.com/an-kumar/caffe-theano-conversion)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/an-kumar/caffe-theano-conversion](https://github.com/an-kumar/caffe-theano-conversion)'
- en: '[https://github.com/kencoken/caffe-model-convert](https://github.com/kencoken/caffe-model-convert)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/kencoken/caffe-model-convert](https://github.com/kencoken/caffe-model-convert)'
- en: '[https://github.com/piergiaj/caffe-to-theano](https://github.com/piergiaj/caffe-to-theano)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/piergiaj/caffe-to-theano](https://github.com/piergiaj/caffe-to-theano)'
- en: 'To convert variables between the Torch Deep Learning framework (written in
    Lua) and Theano, you simply need a tool to convert data from Lua to Python NumPy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Torch深度学习框架（用Lua编写）与Theano之间转换变量，你只需要一个工具将数据从Lua转换到Python NumPy：
- en: '[https://github.com/imodpasteur/lutorpy](https://github.com/imodpasteur/lutorpy)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/imodpasteur/lutorpy](https://github.com/imodpasteur/lutorpy)'
- en: 'To convert models between Tensorflow and Theano, I would advise you to use
    the Keras library, which will stay up-to-date and enable to train models either
    in Theano or Tensorflow. For example, to convert a model from Tensorflow to Theano,
    keep your Keras install configured with Theano as we have seen in [Chapter 5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 5. Analyzing Sentiment with a Bidirectional LSTM"), *Analyzing Sentiment
    with a Bidirectional LSTM*, load the Tensorflow weights, and modify the layer
    names as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Tensorflow和Theano之间转换模型，我建议您使用Keras库，它将保持最新，并允许您在Theano或Tensorflow中训练模型。例如，要将模型从Tensorflow转换为Theano，请保持您的Keras安装配置为Theano，如我们在[第5章](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "第5章. 使用双向LSTM分析情感")中所见，*使用双向LSTM分析情感*，加载Tensorflow权重，并按如下方式修改层名称：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A mirror sequence of operations enables us to do the contrary, from Theano to
    Tensorflow.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个镜像操作序列使我们能够反向操作，从Theano转换为Tensorflow。
- en: Another advantage of designing networks in Keras is the possibility to train
    them directly in the cloud, using the Google Cloud Machine Learning Engine, built
    with **Tensor Processing Units** (**TPU**), an alternative to GPU, designed from
    the ground for machine learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras设计网络的另一个优点是能够直接在云中进行训练，利用Google Cloud机器学习引擎，内置**张量处理单元**（**TPU**），作为GPU的替代方案，从底层为机器学习设计。
- en: Let's take our example from [Chapter 5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 5. Analyzing Sentiment with a Bidirectional LSTM"), *Analyzing Sentiment
    with a Bidirectional LSTM*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以[第5章](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b "第5章.
    使用双向LSTM分析情感")中的示例为例，*使用双向LSTM分析情感*。
- en: 'To train the model in the cloud, I create a project named *DeepLearning Theano*
    in the Google console [https://console.cloud.google.com/iam-admin/projects](https://console.cloud.google.com/iam-admin/projects),
    and in the API manager of the project, enable the Machine Learning Engine API.
    A few installation requirements might be checked with instructions at: [https://cloud.google.com/ml-engine/docs/quickstarts/command-line](https://cloud.google.com/ml-engine/docs/quickstarts/command-line),
    such as the Google Cloud SDK and the project configuration. With `gcloud` `init`
    command, your SDK configuration can be re-initialize to switch to the *DeepLearning
    Theano* project.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在云中训练模型，我在Google控制台中创建了一个名为*DeepLearning Theano*的项目，地址是[https://console.cloud.google.com/iam-admin/projects](https://console.cloud.google.com/iam-admin/projects)，并在项目的API管理器中启用了机器学习引擎API。可能会有一些安装要求，您可以通过以下链接查看相关说明：[https://cloud.google.com/ml-engine/docs/quickstarts/command-line](https://cloud.google.com/ml-engine/docs/quickstarts/command-line)，例如Google
    Cloud SDK和项目配置。通过`gcloud` `init`命令，您可以重新初始化SDK配置，切换到*DeepLearning Theano*项目。
- en: 'Let''s upload the data in a newly created bucket in the cloud, given the region
    you choose (here `europe-west1`):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据上传到云中新创建的存储桶，具体取决于您选择的区域（这里是`europe-west1`）：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since the model is executed on a instance in the cloud, it is required:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型是在云中的实例上执行的，因此需要：
- en: To modify the Python script to load the file stream from the remote bucket instead
    of a local directory, with the library `tensorflow.python.lib.io.file_io.FileIO(train_file,
    mode='r')` rather than the standard method `open(train_file, mode='r')`, with
    the same usage of the mode argument for both, 'r' for reading, `w` for writing,
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改Python脚本，使其从远程存储桶加载文件流，而不是从本地目录加载，使用库`tensorflow.python.lib.io.file_io.FileIO(train_file,
    mode='r')`，而不是标准方法`open(train_file, mode='r')`，两者的mode参数使用相同，'r'表示读取，`w`表示写入，
- en: 'To define a `setup.py` file to configure the libraries required in the cloud
    instance environment:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义`setup.py`文件以配置云实例环境中所需的库：
- en: '[PRE14]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To define a cloud deployment configuration file, `cloudml-gpu.yaml`:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义云部署配置文件`cloudml-gpu.yaml`：
- en: '[PRE15]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To check the training works locally before submitting it to Google ML Cloud,
    run the following command:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在将训练提交给Google ML Cloud之前，先在本地检查训练是否正常工作，请运行以下命令：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If everything works fine locally, let''s submit it to the cloud:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果本地一切正常，我们就可以将其提交到云端：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Model conversions](img/00272.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![模型转换](img/00272.jpeg)'
- en: Note
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that Google ML Cloud uses Tensorflow as backend.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Google ML Cloud使用Tensorflow作为后端。
- en: The future of artificial intelligence
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的未来
- en: '[Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* presented diverse optimization
    techniques (Adam, RMSProp, and so on) and mentioned second order optimization
    techniques. A generalization would be to also learn the update rule:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b "第2章。使用前馈网络分类手写数字")，*使用前馈网络分类手写数字*介绍了多种优化技术（如Adam、RMSProp等），并提到了二阶优化技术。一个推广是还要学习更新规则：'
- en: '![The future of artificial intelligence](img/00273.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能的未来](img/00273.jpeg)'
- en: 'Here, ![The future of artificial intelligence](img/00274.jpeg) is the parameter
    of the optimizer ![The future of artificial intelligence](img/00275.jpeg) to learn
    from different problem instances, a sort of *generalization* or *transfer learning*
    of the optimizer from problems to learn better on new problems. The objective
    to minimize under this *learning to learn* or *meta-learning* framework has to
    optimize the time to learn correctly and, consequently, be defined on multiple
    timesteps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![人工智能的未来](img/00274.jpeg)是优化器的参数，![人工智能的未来](img/00275.jpeg)用来从不同的问题实例中学习，类似于优化器从问题到问题的*泛化*或*迁移学习*，从而在新问题上学习得更好。在这种*学习如何学习*或*元学习*框架下，目标是最小化学习正确的时间，因此需要在多个时间步长上定义：
- en: '![The future of artificial intelligence](img/00276.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能的未来](img/00276.jpeg)'
- en: 'Where:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![The future of artificial intelligence](img/00277.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能的未来](img/00277.jpeg)'
- en: A recurrent neural network can be used as the optimizer model ![The future of
    artificial intelligence](img/00275.jpeg). Such a generalization technique that
    solves a multi-objective optimization problem improves the learning rate of the
    neural networks in general.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络可以作为优化器模型来使用![人工智能的未来](img/00275.jpeg)。这种解决多目标优化问题的泛化技术能提高神经网络的学习速度。
- en: Researchers have been looking one step further, searching for general artificial
    intelligence, which aims for a human-level skill set with the capacity to improve
    itself and acquire new skills in a gradual way, using its **intrinsic** and previously
    learned skills to search for the solutions of new optimization problems.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员进一步探索，寻找通用人工智能，旨在实现具有人类水平技能的人工智能，具备自我提升的能力，并能以渐进的方式获得新技能，利用其**内在**的、之前学到的技能来寻找新的优化问题的解决方案。
- en: A **skill** could be defined as a tool of intelligence to narrow or constrain
    the search space and restrict the behavior of the robot in the infinite world
    of possibilities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**技能**可以被定义为一种智能工具，用于缩小或约束搜索空间，并限制机器人在无限可能性的世界中的行为。'
- en: Building a **General Artificial Intelligence** requires you to define the architecture
    of the intelligence with the intrinsic skills, which will be hardcoded by programmers
    into the robot and help solve smaller subproblems, as well as to define the order
    in which new skills will be acquired, the **curriculum roadmap** that could be
    taught in a **School for AI**. While **gradual learning** learns skill incrementally
    using simpler skills, **guided learning** involves a teacher who has already discovered
    the skills and will teach them to other AI.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 构建**通用人工智能**要求你定义具有内在技能的智能架构，这些技能将由程序员硬编码到机器人中，并帮助解决较小的子问题，还需要定义新技能将被获得的顺序，即在**人工智能学校**中可以教授的**课程路线图**。而**渐进学习**是通过使用更简单的技能逐步学习技能，**引导学习**则涉及一个已经发现技能的教师，将这些技能教授给其他人工智能。
- en: On natural language translation tasks, smaller networks have been proven to
    learn faster and better from a bigger network, the *mentor*, which would have
    learned to translate and produce the translations for the smaller network to learn
    from, rather than learning directly from a real set of human translations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言翻译任务中，已经证明较小的网络能从较大的网络中更快、更好地学习，后者作为*导师*，已经学会了翻译并生成翻译供较小的网络学习，而不是直接从真实的人类翻译集学习。
- en: '![The future of artificial intelligence](img/00278.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能的未来](img/00278.jpeg)'
- en: The preceding figure represents GoodAI Roadmap Institute to evaluate the learning
    roadmaps for AI.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上图表示GoodAI路线图研究所，用于评估人工智能的学习路线图。
- en: Self exploration, communication with the mentor, and incorporation of negative
    and positive feedback are among the ideas toward autonomous intelligence that
    will develop itself, and the current Deep Learning networks open the way toward
    this future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 自我探索、与导师的沟通以及采纳负面和正面反馈是朝向自主智能发展的一些思想，这种智能将能够自我发展，而当前的深度学习网络为这一未来铺平了道路。
- en: 'Among the companies that work toward this goal, it would be worth to quote
    GoodAI, as well as Amazon with its Echo product and the underlying voice control
    assistant technology, Alexa, that has already learned more than 10,000 skills
    in order to help you organize your life. Alexa''s knowledge has become so vast
    that it becomes hard to dive deep into it and find its limitations. A test environment
    for developers enables them to insert these skills into intelligence tools of
    higher level:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在朝着这一目标努力的公司中，值得一提的是 GoodAI 以及亚马逊及其 Echo 产品和其背后的语音控制助手技术 Alexa，后者已经学会了超过 10,000
    项技能，帮助你组织生活。Alexa 的知识已经变得如此庞大，以至于很难深入了解并找出它的局限性。为开发者提供的测试环境使他们能够将这些技能集成到更高层次的智能工具中：
- en: '![The future of artificial intelligence](img/00279.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能的未来](img/00279.jpeg)'
- en: Further reading
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following articles to learn more:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下文章以了解更多：
- en: '*An E**asy Introduction to CUDA C and C++*, [https://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/](https://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CUDA C 和 C++的简单介绍*，[https://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/](https://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/)'
- en: '*How to Access Global Memory* *Efficiently in CUDA C/C++ Kernels*, [https://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/](https://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何高效访问 CUDA C/C++ 内核中的全局内存*，[https://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/](https://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/)'
- en: '*Using Shared Memory in CUDA C/C++*, [https://devblogs.nvidia.com/parallelforall/using-shared-memory-cuda-cc/](https://devblogs.nvidia.com/parallelforall/using-shared-memory-cuda-cc/)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 CUDA C/C++ 中使用共享内存*，[https://devblogs.nvidia.com/parallelforall/using-shared-memory-cuda-cc/](https://devblogs.nvidia.com/parallelforall/using-shared-memory-cuda-cc/)'
- en: '*Just another Tensorflow beginner guide (Par**t4 - Google Cloud ML + GUP +
    Keras),* [http://liufuyang.github.io/2017/04/02/just-another-tensorflow-beginner-guide-4.html](http://liufuyang.github.io/2017/04/02/just-another-tensorflow-beginner-guide-4.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*另一个 Tensorflow 初学者指南（第**4部分 - Google Cloud ML + GUP + Keras）*，[http://liufuyang.github.io/2017/04/02/just-another-tensorflow-beginner-guide-4.html](http://liufuyang.github.io/2017/04/02/just-another-tensorflow-beginner-guide-4.html)'
- en: Learning to learn by gradient descent by gradient descent, Marcin Andrychowicz,
    Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan
    Shillingford, and Nando de Freitas, 2016
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过梯度下降学习的学习，Marcin Andrychowicz、Misha Denil、Sergio Gomez、Matthew W. Hoffman、David
    Pfau、Tom Schaul、Brendan Shillingford 和 Nando de Freitas，2016
- en: A Framework for Searching for General Artificial Intelligence, Marek Rosa, and
    Jan Feyereisl, The GoodAI Collective, 2016
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种搜索通用人工智能的框架，Marek Rosa 和 Jan Feyereisl，The GoodAI Collective，2016
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter concludes our overview of Deep Learning with Theano.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了我们对 Theano 深度学习概述的内容。
- en: The first set of extensions of Theano, in Python and C for the CPU and GPU,
    has been exposed here to create new operators for the computation graph.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 的第一组扩展，在 Python 和 C 中为 CPU 和 GPU 开发，已经在这里公开，用于为计算图创建新操作符。
- en: Conversion of the learned models from one framework to another is not a complicated
    task. Keras, a high-level library presented many times in this book as an abstraction
    on top of the Theano engine, offers a simple way to work with Theano and Tensorflow
    as well as to push the training of models in the Google ML Cloud.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将已学习的模型从一个框架转换到另一个框架并不是一项复杂的任务。Keras，这本书中多次提到的高层次库，作为 Theano 引擎之上的抽象层，提供了一种简单的方法来同时使用
    Theano 和 Tensorflow，并推动模型在 Google ML Cloud 中的训练。
- en: Lastly, all the networks presented in this book are at the base of General Intelligence,
    which can use these first skills, such as vision or language understanding and
    generation, to learn a wider range of skills, still from experiences on real-world
    data or generated data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本书中呈现的所有网络都是通用智能的基础，这些网络可以利用这些初步技能，如视觉或语言理解与生成，去学习更广泛的技能，这些技能仍然来自于现实世界数据或生成的数据的经验。
