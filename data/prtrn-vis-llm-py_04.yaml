- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Containers and Accelerators on the Cloud
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云上的容器和加速器
- en: In this chapter, you’ll learn how to containerize your scripts and optimize
    them for accelerators on the cloud. We’ll learn about a range of accelerators
    for foundation models, including trade-offs around cost and performance across
    the entire machine learning lifecycle. You’ll learn about key aspects of Amazon
    SageMaker and AWS to train models on accelerators, optimize performance, and troubleshoot
    common issues. if you’re already familiar with containers and accelerators on
    AWS, feel free to skip this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何将脚本容器化，并为云上的加速器进行优化。我们将了解一系列用于基础模型的加速器，包括在整个机器学习生命周期中围绕成本和性能的权衡。你将学习关于Amazon
    SageMaker和AWS的关键方面，以便在加速器上训练模型，优化性能，并排除常见问题。如果你已经熟悉AWS上的容器和加速器，可以跳过本章。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要内容：
- en: What are accelerators and why do they matter for foundation models?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是加速器，为什么它们对基础模型很重要？
- en: Containerize your scripts for accelerators on AWS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的脚本容器化以便在AWS上的加速器使用
- en: Using accelerators with Amazon SageMaker
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用加速器与Amazon SageMaker
- en: Infrastructure optimizations on AWS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS上的基础设施优化
- en: Troubleshooting accelerator performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除加速器性能问题
- en: What are accelerators and why do they matter?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是加速器，为什么它们很重要？
- en: There’s something remarkable about human behavior. We care a lot about our own
    experiences. Many of the arts and sciences, particularly social science, specialize
    in quantifying, predicting, and understanding the implications and particularities
    of human behavior. One of the most obvious of these is human responses to technical
    performance. While this certainly varies among human groups, for the subset that
    chooses to spend a sizeable portion of their time interacting with technology,
    one theorem is self-evident. Faster and easier is always better.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有些关于人类行为的事情是值得注意的。我们非常关心自己的体验。许多艺术和科学，特别是社会科学，专注于量化、预测和理解人类行为的含义和特性。其中最明显的一种就是人类对技术性能的反应。虽然这在不同人群之间有所差异，但对于选择花费大量时间与技术互动的群体来说，有一个定理是不言而喻的：更快、更简便永远是更好的。
- en: Take video games, for example. While the 1940s and 50s saw some of the earliest
    video games, these didn’t come to massive popularity until arcade games such as
    *Pong* emerged in the early 70s. Perhaps unsurprisingly, this was nearly exactly
    the same time as the introduction of the original **Graphics Processor Unit**
    (**GPU**), in 1973 *(1)*! 1994 gave us the *PlayStation1* with its Sony GPU. As
    a child, I spent many hours loving the graphics performance on my Nintendo 64,
    with games such as *Zelda*, *Super Smash Brothers*, *Mario Kart 64*, and more!
    These days you only need to look at games such as *Roblox*, *League of Legends*,
    and *Fortnite* to understand how crucial graphics performance is to the success
    of the gaming industry. For decades, gaming has served as one of the most important
    signals in the market for GPUs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以视频游戏为例。虽然20世纪40年代和50年代见证了一些最早的视频游戏，但直到70年代初期，像*乒乓*这样的街机游戏才开始获得广泛的流行。或许并不奇怪，这几乎与原始**图形处理单元**（**GPU**）的诞生时间相吻合，GPU于1973年问世*(1)*！1994年，PlayStation1推出，配备了索尼GPU。作为一个孩子，我花了许多小时沉迷于任天堂64的图形性能，玩着*塞尔达传说*、*超级大乱斗*、*马里奥赛车64*等游戏！如今，你只需要看看像*Roblox*、*英雄联盟*和*堡垒之夜*这样的游戏，就能明白图形性能对游戏行业成功的重要性。几十年来，游戏一直是GPU市场中最重要的信号之一。
- en: Until machine learning, that is. In [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016),
    we learned about the ImageNet dataset and briefly introduced its 2012 champion,
    AlexNet. To efficiently train their model on the large ImageNet dataset, the authors
    used GPUs! At the time, the GPUs were quite small, only offering 3 GB of memory,
    so they needed to implement a *model parallel strategy*. This used two GPUs to
    hold the entire model in memory. These enhancements, in addition to other modifications
    such as using the ReLU activation function and overlapped pooling, led AlexNet
    to win the challenge by a landslide.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 直到机器学习才是。我们在[*第1章*](B18942_01.xhtml#_idTextAnchor016)中了解了ImageNet数据集，并简要介绍了2012年的冠军AlexNet。为了高效地在大型ImageNet数据集上训练模型，作者使用了GPU！当时，GPU的性能较低，仅提供3
    GB的内存，因此他们需要实现*模型并行策略*。该策略使用两个GPU将整个模型加载到内存中。这些增强功能，除了其他修改（如使用ReLU激活函数和重叠池化）外，使得AlexNet凭借压倒性的优势赢得了挑战。
- en: Since that achievement more than 10 years ago, most of the best machine learning
    models have used GPUs. From transformers to reinforcement learning, training to
    inference, and vision to language, the overwhelming majority of state-of-the-art
    machine learning models require GPUs to perform optimally. For the right type
    of processing, GPUs can be many orders of magnitude faster than CPUs. When training
    or hosting deep learning models, the simple choice of using GPUs or CPUs can frequently
    make a difference of hours to days in completing a task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自从十多年前取得这一成就以来，大多数最优秀的机器学习模型都使用了GPU。从变换器到强化学习，从训练到推理，从计算机视觉到自然语言处理，绝大多数最先进的机器学习模型都需要GPU才能达到最佳性能。在适合的处理类型下，GPU的速度可以比CPU快数个数量级。在训练或托管深度学习模型时，选择使用GPU还是CPU，通常会导致完成任务所需时间的差异，可能是几小时到几天。
- en: 'We know GPUs have a lot of promising benefits as compared to standard CPU processing,
    but how? What’s so different about them at a fundamental level? The answer may
    surprise you: *distribution*! Let’s take a look at this figure to understand the
    differences between CPUs and GPUs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，与标准的CPU处理相比，GPU有很多有前景的优点，但究竟是如何实现的呢？从根本上说，GPU与CPU有什么不同？答案可能会让你吃惊：*分布式*！让我们看一下这张图，来理解CPU和GPU之间的区别：
- en: '![Figure 4.1 – Differences between CPU and GPU](img/B18942_Figure_04_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – CPU 与 GPU 的区别](img/B18942_Figure_04_01.jpg)'
- en: Figure 4.1 – Differences between CPU and GPU
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – CPU 与 GPU 的区别
- en: CPUs have just a few cores but a lot of memory. This means they can do only
    a few operations at once, but they can execute these very quickly. Think of low
    latency. CPUs operate almost like a cache; they’re great at handling lots of tasks
    that rely on interactivity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CPU只有少数几个核心，但拥有大量内存。这意味着它们一次只能执行少数几个操作，但可以非常快速地完成这些操作。可以把它想象为低延迟。CPU几乎像一个缓存一样工作；它们非常擅长处理需要交互的任务。
- en: On the other hand, GPUs have thousands of cores. NVIDIA’s latest generation
    GH100 chips, for example, have 18,432 cores. This means they are excellent at
    processing many operations at once, such as matrix multiplication on the millions
    to billions of parameters in your neural networks. Think of high throughput.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GPU拥有成千上万的核心。例如，英伟达最新一代的GH100芯片就拥有18,432个核心。这意味着它们非常擅长同时处理许多操作，比如在神经网络中的数百万到数十亿个参数上进行矩阵乘法。可以把它想象为高吞吐量。
- en: Don’t we care about both low latency and high throughput? Yes, absolutely! This
    is why the majority of compute you work with today, from your cell phone to your
    laptop, your notebook instance to the fleet of instances you need to train that
    state-of-the-art model, use both CPUs and GPUs. The question is, how?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 难道我们不关心低延迟和高吞吐量吗？是的，绝对如此！这就是为什么你今天使用的大多数计算设备，从手机到笔记本电脑，从笔记本实例到你需要训练最先进模型的大量实例，都同时使用了CPU和GPU。问题是，如何做到的呢？
- en: 'As you might imagine, writing a software program to successfully run complex
    operations across tens of thousands of microprocessors isn’t exactly the easiest
    thing in the world. This is why, in order to write code for a GPU, you need a
    specialized software framework purpose-built for hyper-distribution of operations.
    Enter **CUDA**, NVIDIA’s **Compute Unified Device Architecture**. CUDA abstracts
    away the orchestration of those underlying distributed microprocessors from the
    consumer, allowing them to leverage the massive distribution without needing to
    be an expert in its specific architecture. CUDA comes in two parts: drivers that
    work directly with the hardware, and a toolkit that exposes this hardware to developers.
    Python can work directly with CUDA, for example. PyTorch and TensorFlow interact
    with CUDA as well.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能想象的那样，编写一个软件程序，成功地在成千上万的微处理器上运行复杂的操作，显然不是世界上最简单的事情。这就是为什么，为了编写GPU的代码，你需要一个专门为超大规模操作分发而构建的软件框架。这里就引入了**CUDA**，英伟达的**计算统一设备架构**。CUDA抽象了底层分布式微处理器的协调工作，允许用户在无需成为该架构专家的情况下，利用其强大的分发能力。例如，Python可以直接与CUDA一起工作。PyTorch和TensorFlow也可以与CUDA进行交互。
- en: NVIDIA certainly isn’t the only vendor providing high-performance distributed
    microprocessors. Commonly called **accelerators**, GPU-like massively parallel
    processing units are available from Amazon (Inferentia and Trainium), Google (TPUs),
    Intel (Habna Gaudi), AMD (ROCm), and more. However, each of these requires specialized
    steps to utilize the underlying distributed hardware. While there are clear advantages
    when these apply to your use case, for the purposes of a beginner’s book, we’ll
    just stick with GPUs. We’ll dive into using Amazon’s accelerators, Trainium and
    Inferentia, in [*Chapter 9*](B18942_09.xhtml#_idTextAnchor138)*, Advanced* *Training
    Concepts*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA显然不是唯一提供高性能分布式微处理器的供应商。通常称为**加速器**，类似GPU的大规模并行处理单元可以从亚马逊（Inferentia和Trainium）、谷歌（TPU）、英特尔（Habna
    Gaudi）、AMD（ROCm）等处获得。然而，利用底层分布式硬件需要专门的步骤。当这些硬件适用于你的用例时，它们显然具有明显的优势，但对于初学者的书籍目的而言，我们将只关注GPU。我们将深入研究亚马逊的加速器Trainium和Inferentia，位于[*第9章*](B18942_09.xhtml#_idTextAnchor138)*，高级培训概念*。
- en: Now you’ve been introduced to accelerators, let’s figure out how to use them!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了加速器，让我们弄清楚如何使用它们！
- en: Getting ready to use your accelerators
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备使用加速器
- en: 'Let’s start with learning how to use your accelerators:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从学习如何使用你的加速器开始：
- en: '*Step one: acquisition*. You definitely can’t train a model on a GPU without
    first getting your hands on at least one of the GPUs. Fortunately, there are a
    few free options for you. One of my projects at Amazon was actually writing the
    original doc for this: SageMaker Studio Lab! Studio Lab is one way to run a free
    Jupyter Notebook server in the cloud. If you’d like to use a no-cost notebook
    environment on CPUs or GPUs, store your files, collaborate with others, and connect
    to AWS or any other service, Studio Lab is a great way to get started.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第一步：获取*。在没有至少一台GPU的情况下，你绝对不能在GPU上训练模型。幸运的是，有一些免费选择供你选择。亚马逊的一个项目中，我实际上是编写了关于这个的原始文档：SageMaker
    Studio Lab！Studio Lab是在云中运行免费Jupyter Notebook服务器的一种方式。如果你想在CPU或GPU上使用免费的笔记本环境，存储你的文件，与他人合作，并连接到AWS或任何其他服务，Studio
    Lab是一个很好的开始方式。'
- en: '*Step two: containers*. Once you’re in a Jupyter notebook and are trying to
    run some example code, you’ll realize that everything hinges on installing the
    right packages. Even once you have the packages installed, connecting them to
    the GPU depends on the CUDA installation in your notebook. If the version of PyTorch
    or TensorFlow you’re trying to use doesn’t work nicely with that specific CUDA
    install, you’re out of luck!'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第二步：容器*。一旦你进入Jupyter笔记本并尝试运行一些示例代码，你会意识到一切都依赖于安装正确的软件包。即使安装了这些软件包，将它们连接到GPU还取决于笔记本中的CUDA安装。如果你尝试使用的PyTorch或TensorFlow版本与特定的CUDA安装不兼容，那就没办法了！'
- en: This is why using the right container as your base image is the perfect way
    to start developing, especially for deep learning on GPUs. AWS, NVIDIA, PyTorch,
    and TensorFlow all provide base images you can use to start working with deep
    learning frameworks. At AWS, we have 70+ containers across multiple frameworks
    and key versions of these *(2)*. We provide these containers across CPU, GPU,
    training, hosting, SageMaker, and our container services.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么选择正确的容器作为你的基础镜像是开始开发的完美方式，特别是在GPU上进行深度学习时。AWS、NVIDIA、PyTorch和TensorFlow都提供了可以用来开始使用深度学习框架的基础镜像。在AWS，我们有70多个跨多个框架和关键版本的容器*(2)*。我们提供这些容器，涵盖CPU、GPU、训练、托管、SageMaker和我们的容器服务。
- en: What’s a container, you ask? Imagine writing a Python script with 5, 10, 15,
    or even more than 100 software packages. Installing all of those is really time-consuming
    and error-prone! Think how hard it is just to install one package successfully
    on your own; all of that complexity, time, and careful solution you found can
    literally be transferred anywhere you like. How? Through containers! Containers
    are a powerful tool. Learn how to make them your friend.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你问什么是容器？想象一下编写一个Python脚本，其中包含5、10、15甚至超过100个软件包。安装所有这些软件包真的非常耗时且容易出错！试想一下，单独成功安装一个软件包是多么困难；所有这些复杂性、时间和你找到的小心解决方案都可以轻松地转移到任何你喜欢的地方。如何实现？通过容器！容器是一个强大的工具。学会如何让它们成为你的朋友。
- en: Now that you have some idea about working with containers, especially how they
    serve as the intermediary between your model and your GPUs, let’s talk through
    the options for where to run your GPUs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对使用容器有了一些了解，特别是它们作为模型与GPU之间中介的角色，让我们讨论一下在哪里运行GPU的选项。
- en: 'Here, I’d like to emphasize that obviously, I’ve spent many years working at
    AWS. I love it! It’s been an amazing place for me to grow my skills, learn about
    the world, practice deep learning, serve customers, and collaborate with some
    amazing people. I’ve also spent many hours obsessing about the trade-offs in running
    compute on the cloud versus running compute on-premises. Actually, before working
    at AWS, I spent time at many different organizations: a few start-ups, a bank,
    a university, restaurants, policy organizations, and a non-profit. Each of these
    handled their compute slightly differently.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想强调的是，显然我在AWS工作了很多年。我非常喜欢它！这段经历为我提供了一个极好的平台来提升我的技能、了解世界、实践深度学习、服务客户，并与一些非常优秀的人一起合作。我还花了很多时间研究云计算与本地计算之间的权衡。事实上，在加入AWS之前，我曾在很多不同的组织工作过：一些初创公司、一家银行、一所大学、餐馆、政策组织和一个非营利机构。这些地方的计算资源管理方式各不相同。
- en: On the one hand, purchasing compute to store on-premises might seem like a safer
    bet upfront. Importantly, you’re actually getting something physical for your
    dollars! You don’t need to pay to use the machine, and it seems easier to secure.
    After all, you can just stash it under your desk. What gives?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，购买本地计算资源可能一开始看起来是更安全的选择。重要的是，你实际上用钱买到了实物！你不需要支付使用机器的费用，而且似乎更容易保护它。毕竟，你可以把它放在桌子下。那么，为什么会这样呢？
- en: 'There are five big problems with running compute on-premises:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行计算存在五个主要问题：
- en: First is **logistics**. Say you actually do buy some local servers with GPUs.
    Where would you put them? How would you connect them to your laptop? How would
    you keep them cold? Power them sufficiently? What would you do if the power spontaneously
    shut down in that room in the middle of your experiment? You also need to wait
    for the GPUs to physically arrive in the mail. Then you need to “rack and stack”
    the boxes, putting them into your growing local data center. Soon enough, these
    auxiliary tasks can become your full-time job, and you’ll need a team of people
    if you intend to run these for your whole organization.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个问题是**后勤**。假设你确实购买了一些带有GPU的本地服务器。你会把它们放在哪里？如何将它们连接到你的笔记本电脑？如何保持它们的低温？如何确保它们有足够的电力？如果在实验过程中房间里的电力突然中断，你该怎么办？你还需要等待GPU通过邮件送达。然后，你需要“安装和配置”这些机器，把它们放入你的本地数据中心。很快，这些辅助任务就可能成为你全职工作的内容，如果你打算为整个组织运行这些计算资源，你还需要一个团队。
- en: Second is **scale**. Say you only buy eight GPUs upfront. You can run a handful
    of experiments, executing them one at a time. But what if you have a great idea
    about a new project you’d like to test out? If you have only your eight GPUs,
    you are physically bound by them and unable to run extra experiments elsewhere.
    At this point, most people simply move to acquire more GPUs to run their extra
    experiments, which leads to the third problem.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个问题是**规模**。假设你一开始只购买了八个GPU。你可以进行少量的实验，每次执行一个。但如果你有一个新的项目想要测试怎么办？如果你只有这八个GPU，你将受到它们的限制，无法在其他地方进行额外的实验。在这种情况下，大多数人会选择购买更多GPU来进行额外的实验，这就引出了第三个问题。
- en: Third is **under-utilization**. When everyone goes home at night and isn’t training
    any models, what’s happening with those expensive GPUs? Probably nothing. They
    may be sitting totally unutilized. If you’re running some experiments overnight,
    or for multiple days /weeks, you may see higher levels of GPU utilization. However,
    it’s not at all uncommon for organizations to heavily invest in expensive GPU
    infrastructure only to see it actually go completely untouched by the very teams
    who requested it! Usually, you’ll see a tiny number of power users, with a very
    long tail of lightly interested parties who may log in occasionally.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个问题是**资源闲置**。当大家晚上回家不再训练模型时，那些昂贵的GPU都发生了什么？可能什么都没发生。它们可能完全没有被利用。如果你晚上或几个星期内进行实验，可能会看到更高的GPU利用率。然而，组织通常会大量投资昂贵的GPU基础设施，但实际使用这些资源的团队往往是最少的！通常，你会看到少数几个高级用户，而大多数轻度使用者只是偶尔登录。
- en: Fourth is **currency**. Hardware updates, rapidly. Many companies are fully
    invested in releasing newer, faster, better versions of their hardware every year.
    Each year, you should expect to see a performance increase with the latest version.
    It’s not a great feeling to have made a big investment in on-premises GPUs, only
    to see them deprecated in a matter of months. It also introduces a risk to your
    experiments; you may not be able to produce state-of-the-art results if you’re
    not able to get the most performance out of your compute budget possible.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四个是**货币**。硬件更新，迅速。许多公司每年都会投入大量资源发布更新、更快、更好的硬件版本。每年，你都应该期望看到最新版本的性能提升。如果你在本地GPU上投入了大量资金，却在几个月内看到它们被淘汰，这种感觉可不好。这还会给你的实验带来风险；如果你不能从计算预算中获得最好的性能，你可能无法取得最先进的结果。
- en: Fifth is **carbon footprint**. How much do you know about the type of energy
    supplying your grid? Is it sustainable? How much extra energy will all of those
    GPUs add to the carbon footprint of your community, not to mention your bill?
    Amazon is actually the largest corporate purchaser of renewable energy in the
    world. We are incredibly careful about the grids supplying our regions and can
    demonstrate that moving to the AWS cloud can reduce the carbon footprint of the
    average data center by up to 80%, with a goal of up to 96% once we’re powered
    with 100% renewable energy.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五个是**碳足迹**。你对为你所在电网提供能源的类型了解多少？它是否可持续？所有这些GPU会为你的社区带来多少额外的碳足迹，更不用说你的账单了？实际上，亚马逊是全球最大的可再生能源企业采购商。我们非常谨慎地选择供应我们区域的电网，并能证明，转向AWS云可以将平均数据中心的碳足迹减少最多80%，当我们完全使用100%可再生能源时，目标是减少至96%。
- en: Another benefit of running your GPUs on Amazon SageMaker specifically is that
    *you are not paying for the instances if you’re not running a job*. When using
    SageMaker to train your machine learning models, the GPUs come online *only when
    you are training the model itself*. This means your overall GPU utilization is
    significantly better by moving to SageMaker, simply because of the system architecture.
    Most data centers aren’t built for the dynamism that deep learning training really
    requires, because when you aren’t training a model, the nodes are still running!
    The same logic holds for Amazon EC2.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在Amazon SageMaker上运行GPU的好处是，*如果你没有运行任务，就不需要为实例付费*。在使用SageMaker训练机器学习模型时，GPU只有在你训练模型时才会上线。也就是说，搬到SageMaker后，你的整体GPU利用率会显著提高，这仅仅是因为系统架构的原因。大多数数据中心并没有为深度学习训练所需要的动态性而建，因为当你不在训练模型时，节点仍然在运行！同样的逻辑也适用于Amazon
    EC2。
- en: Finally, you’ll need to make sure that your project actually uses the GPUs.
    This is much more complex than it sounds. First, the software framework itself
    needs to connect to the underlying CUDA kernels. Then, you’ll want to use some
    tooling to ensure that you keep the GPU utilization as high as possible. There
    are a variety of techniques that you’ll learn about later in the chapter to dive
    deeper into these topics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要确保你的项目实际上使用了GPU。这比听起来复杂得多。首先，软件框架本身需要连接到底层的CUDA内核。然后，你需要使用一些工具来确保GPU的使用率尽可能高。你将在本章后面学到各种技术，深入了解这些话题。
- en: Now that you’ve learned how to get ready to use your accelerators, let’s learn
    how to use them on Amazon SageMaker!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何准备使用加速器，让我们来看看如何在Amazon SageMaker上使用它们吧！
- en: How to use accelerators on AWS – Amazon SageMaker
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在AWS上使用加速器——Amazon SageMaker
- en: As you learned in the previous section, AWS is a great way to get your hands
    on GPUs without needing to provision, store, physically secure, and maintain GPUs.
    Now, we’ll take a look at an easy, efficient, and high-performance way to leverage
    GPUs on AWS – Amazon SageMaker. Let me be clear, SageMaker certainly is not the
    only way to run GPUs or accelerators on AWS. However, it is my personal favorite,
    so we’ll start there.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上一节中学到的，AWS是一个极好的方式，让你在无需配置、存储、物理保护和维护GPU的情况下就能获得GPU。现在，我们将看看在AWS上利用GPU的一种简单、高效且高性能的方法——Amazon
    SageMaker。我要明确的是，SageMaker当然不是在AWS上运行GPU或加速器的唯一方式。然而，它是我个人最喜欢的方法，因此我们从这里开始。
- en: 'There are many books, blog posts, webinars, and re:Invent sessions dedicated
    to introducing and discussing SageMaker. I myself have a 16-video YouTube series
    you can use to learn more about it from me! However, for the purposes of this
    book, there are really three key pieces of SageMaker I want to you understand:
    **Studio**, **Training**, and **Hosting**. And each of these comes down to one
    single common denominator: **instances**.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多书籍、博客文章、网络研讨会和 re:Invent 会议专门介绍和讨论 SageMaker。我自己也有一个包含 16 个视频的 YouTube 系列，你可以通过它来学习更多关于
    SageMaker 的内容！不过，为了本书的目的，我希望你了解 SageMaker 的三个关键部分：**Studio**、**训练**和 **托管**。这些部分的共同点是：**实例**。
- en: '*Instances* is the term we use to describe virtual machines at AWS. The service
    is called **Elastic Compute Cloud**, or **EC2**. Every time you turn on a virtual
    machine, we call it an **instance**. You may have worked with EC2 instances in
    the past, such as turning them on in the AWS console, SSHing into them, and trying
    to write some code. But didn’t you find it a little frustrating when you needed
    to change the size? What about downloading the logs or the output? Sharing your
    notebook? Not to mention your surprise at the bill when you forgot to turn it
    off!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*实例* 是我们用来描述 AWS 上虚拟机的术语。这个服务叫做 **弹性计算云**，简称 **EC2**。每次你启动一个虚拟机，我们都称之为一个 **实例**。你可能以前使用过
    EC2 实例，比如在 AWS 控制台中启动它们，使用 SSH 连接，尝试写一些代码。但你是不是觉得，每当需要更改实例的大小时，这个过程有点儿烦人？下载日志或输出文件呢？分享你的笔记本？更不用提你忘记关机后收到的账单了！'
- en: Now, what if I told you there was an easy way to run notebooks, train models,
    and build business applications around your own data science work products, without
    you needing to manage really anything about that underlying infrastructure? You’d
    probably be interested, right?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我告诉你，有一种简单的方式可以在不需要管理底层基础设施的情况下，运行笔记本、训练模型并围绕你的数据科学工作成果构建业务应用，你是不是会感兴趣呢？
- en: That’s the core idea of SageMaker. We democratize machine learning by making
    it extremely easy for you to run your notebooks, models, jobs, pipelines, and
    processes from one single pane of glass. We also deliver extremely high performance
    at affordable prices. Let’s take a closer look at some of the key pieces of SageMaker.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 SageMaker 的核心理念。我们通过让你能够在一个统一的界面上轻松运行你的笔记本、模型、任务、管道和流程，来实现机器学习的普及化。我们还提供高性能并以实惠的价格交付服务。让我们更仔细地看看
    SageMaker 的一些关键部分。
- en: SageMaker Studio
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker Studio
- en: SageMaker Studio is our flagship development environment that is fully integrated
    with machine learning. What I love most about Studio is that *we decouple the
    compute backing your user interface from the compute running your notebook*. That
    means AWS is managing a Jupyter Server on your behalf, per user, to host your
    visual experience. This includes a huge volume of features purpose-built for machine
    learning, such as Feature Store, Pipelines, Data Wrangler, Clarify, Model Monitor,
    and more.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Studio 是我们旗舰级的开发环境，完全与机器学习集成。我最喜欢 Studio 的一点是，*我们将支撑用户界面的计算资源与运行你的笔记本的计算资源解耦*。这意味着
    AWS 会为每个用户托管一个 Jupyter 服务器，负责你的可视化体验。这包括大量专为机器学习构建的功能，如 Feature Store、Pipelines、Data
    Wrangler、Clarify、Model Monitor 等等。
- en: Then, every time you create a new Jupyter notebook, *we run this on dedicated
    instances*. These are called **Kernel Gateway Applications**, and they let you
    seamlessly run many different projects, with different package requirements and
    different datasets, without leaving your IDE. Even better, the notebooks in Studio
    are incredibly easy to upgrade, downgrade, and change kernels. That means you
    can switch from CPU to GPU, or back, without very much work interruption.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每当你创建一个新的 Jupyter 笔记本，*我们会在专用实例上运行它*。这些实例叫做 **内核网关应用**，它们让你能够在 IDE 中无缝运行许多不同的项目，满足不同的包需求和数据集，而无需离开你的开发环境。更棒的是，Studio
    中的笔记本非常容易升级、降级或更换内核。也就是说，你可以在 CPU 和 GPU 之间切换，而几乎不需要中断工作。
- en: SageMaker Training
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker 训练
- en: Now you have some idea about how to run a notebook on SageMaker, but what about
    a large-scale job with distributed training?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你大概了解了如何在 SageMaker 上运行笔记本，但如果是一个大规模的分布式训练任务呢？
- en: 'For this topic, we’ll need to introduce the second key pillar of SageMaker:
    **Training**. SageMaker Training lets you easily define job parameters, such as
    the instances you need, your scripts, package requirements, software versions,
    and more. Then, when you fit your model, we launch a cluster of remote instances
    on AWS to run your scripts. All of the metadata, package details, job output,
    hyperparameters, data input, and more are stored, searchable, and versioned by
    default. This lets you easily track your work, reproduce results, and find experiment
    details, even many months and years after your job has finished.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个主题，我们需要介绍 SageMaker 的第二个关键支柱：**训练**。SageMaker Training 让你能够轻松定义作业参数，例如所需的实例、脚本、包依赖、软件版本等。然后，当你训练模型时，我们会在
    AWS 上启动一群远程实例来运行你的脚本。所有的元数据、包细节、作业输出、超参数、数据输入等都会被存储、可搜索并默认进行版本管理。这样，你就能轻松追踪工作进展、重现结果，并在作业完成后，即使是几个月或几年后，也能找到实验的详细信息。
- en: In addition, we’ve put a lot of muscle into updating our training backend platform
    to enable extreme-scale modeling. From data optimizations with FSx for Lustre
    to distributed libraries such as Model and Data Parallel, we’re enabling the next
    generation of large models across vision and text to train seamlessly on AWS.
    The next chapter covers this in more detail. Most of the GPUs we’ll analyze in
    this book come under SageMaker Training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在更新训练后端平台方面投入了大量精力，以支持超大规模建模。从使用 FSx for Lustre 的数据优化到分布式库（如模型和数据并行），我们正在推动下一代大规模模型在
    AWS 上无缝训练，涵盖视觉和文本领域。下一章会更详细地介绍这一点。本书中我们分析的大部分 GPU 实例都属于 SageMaker Training。
- en: SageMaker hosting
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker 托管
- en: Lastly, you can also run GPUs on SageMaker hosting. This is valuable when you
    want to build a scalable REST API on top of your core model. You might use a SageMaker
    hosting endpoint to run a search experience, deliver questions and answers, classify
    content, recommend content, and so much more. SageMaker hosting supports GPUs!
    We’ll dive into this in more detail in [*Chapter 12*](B18942_12.xhtml#_idTextAnchor178)*,
    How to Deploy* *Your Model*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你还可以在 SageMaker 托管中运行 GPU。当你想在核心模型之上构建可扩展的 REST API 时，这非常有用。你可能会使用 SageMaker
    托管端点来运行搜索体验、提供问答服务、进行内容分类、推荐内容等多种应用。SageMaker 托管支持 GPU！我们将在 [*第 12 章*](B18942_12.xhtml#_idTextAnchor178)《如何部署你的模型》中更详细地探讨这一点。
- en: 'Now that you understand some of the key pillars of SageMaker, let’s break down
    this concept underlying all of them: instances.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 SageMaker 的一些关键支柱，让我们深入探讨它们背后的概念：实例。
- en: Instance breakdown for GPUs on SageMaker
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker 上 GPU 实例解析
- en: As of November 2022, we support two primary instance families with GPUs on SageMaker,
    two custom accelerators, and Habana Gaudi accelerators. Here, I’ll break down
    how to understand the naming convention for all our instances, in addition to
    describing what you might use each of these for.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2022 年 11 月，我们支持 SageMaker 上两种主要的 GPU 实例系列、两种自定义加速器和 Habana Gaudi 加速器。在这里，我将为你解析所有实例的命名规范，并介绍你可能会使用这些实例的具体场景。
- en: 'The naming convention for all instances is really three parts: first, middle,
    and last. For example, `ml.g4dn.12xlarge`. The `ml` part indicates that it’s actually
    a SageMaker instance, so you won’t see it in the EC2 control plane. The `g` part
    tells you what series of compute the instance is part of, especially the type
    of compute itself. Here, `g` indicates it’s a specific type of GPU accelerator:
    the `g4` instance has NVIDIA T4, and the `g5` has NVIDIA A10G. The number immediately
    after the letter is the version of this instance, with a higher number always
    being more recent. So, `g5` came out more recently than `g4`, and so on. The latest
    version of each instance will always give you better price performance.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实例的命名规范实际上由三部分组成：前缀、中缀和后缀。例如，`ml.g4dn.12xlarge`。其中，`ml` 部分表示这是一个 SageMaker
    实例，因此你不会在 EC2 控制平面中看到它。`g` 部分告诉你这个实例属于哪个计算系列，特别是它的计算类型。在这里，`g` 表示它是一个特定类型的 GPU
    加速器：`g4` 实例配备 NVIDIA T4，而 `g5` 配备 NVIDIA A10G。紧跟字母后的数字是该实例的版本号，数字越大，版本越新。所以，`g5`
    比 `g4` 更新，依此类推。每个实例的最新版本通常会提供更好的性价比。
- en: Here, `g4` indicates you are using NVIDIA T4 GPUs. The letters after the number
    tell you what else is available on that instance, in this case, `d` gives us `n`
    is for `ml.t3.medium` to run your Jupyter notebook, but then upgrade to something
    large such as `ml.g4dn.12xlarge` for development, and ultimately, perhaps, `ml.p4dn.24xlarge`
    for extreme-scale training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`g4` 表示你正在使用 NVIDIA T4 GPU。数字后面的字母告诉你该实例上还可用的其他资源，在这种情况下，`d` 表示我们可以使用 `n`
    来运行 `ml.t3.medium` 的 Jupyter notebook，但之后可以升级到更大的实例，如 `ml.g4dn.12xlarge` 进行开发，最终可能会使用
    `ml.p4dn.24xlarge` 进行大规模训练。
- en: Generally speaking, the `g` instance is great for smaller models. This could
    include development and testing for you, such as running a complex notebook on
    this, using warm pools, or simply a multi-GPU model training with data-parallel.
    The `g5` instance is especially competitive here.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，`g` 实例非常适合较小的模型。这可以包括开发和测试，例如在此上运行复杂的笔记本、使用热池，或者只是使用数据并行的多GPU模型训练。`g5`
    实例在这里尤其具有竞争力。
- en: However, if you want to train large language models, the `p` instance series
    is strongly recommended. This is because the GPUs are actually more performant,
    and also larger. They support larger models and larger batch sizes. `ml.p4dn.24xlarge`,
    with 8 NVIDIA A100s, has 40 GB of GPU memory per card. `ml.g5.48xlarge`, with
    8 NVIDIA A10Gs, has only 24 GB of GPU memory per card.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你想训练大型语言模型，强烈推荐使用 `p` 系列实例。这是因为这些 GPU 实际上性能更强，并且更大。它们支持更大的模型和更大的批量大小。`ml.p4dn.24xlarge`
    配备 8 个 NVIDIA A100，每个卡有 40 GB 的 GPU 内存。`ml.g5.48xlarge` 配备 8 个 NVIDIA A10G，每个卡只有
    24 GB 的 GPU 内存。
- en: As of this writing, Trainium has just become available! This is a custom accelerator
    developed by Amazon to deliver up to 50% better cost performance for customers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Trainium 刚刚上线！这是亚马逊开发的定制加速器，旨在为客户提供高达 50% 的成本性能提升。
- en: Now that you’ve learned about how to use GPUs on AWS, especially on Amazon SageMaker,
    and which instances you want to stay on top of, let’s unpack how to optimize GPU
    performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何在 AWS 上使用 GPU，特别是在 Amazon SageMaker 上使用 GPU，以及哪些实例你应该关注，接下来我们来探讨如何优化
    GPU 性能。
- en: Optimizing accelerator performance
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化加速器性能
- en: There are two ways of approaching this, and both of them are important. The
    first is from a hyperparameter perspective. The second is from an infrastructure
    perspective. Let’s break them down!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两种方法可以处理这个问题，而这两种方法都很重要。第一种是从超参数的角度来看。第二种是从基础设施的角度来看。让我们一一解析吧！
- en: Hyperparameters
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: All of [*Chapter 7*](B18942_07.xhtml#_idTextAnchor116) is devoted to picking
    the right hyperparameters, and optimizing GPU performance is a large driver for
    that. Importantly, as the number of GPUs changes in your cluster, what we call
    your **world size**, you’ll need to modify your hyperparameters to accommodate
    that change. Also, there’s a core trade-off between increasing your overall job
    throughput, say by maxing out your batch size, and finding a smaller batch size,
    which ultimately will give you higher accuracy. Later in the book, you’ll learn
    how to use hyperparameter tuning to bridge that gap.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 [*第七章*](B18942_07.xhtml#_idTextAnchor116) 都致力于选择正确的超参数，而优化 GPU 性能是其中一个重要因素。值得注意的是，随着你集群中
    GPU 数量的变化，也就是我们所说的 **世界大小**，你需要调整你的超参数以适应这种变化。此外，在最大化批量大小以提高整体工作吞吐量和找到一个较小的批量大小以最终提高准确性之间，有一个核心的权衡。书中的后续部分将教你如何通过超参数调优来弥补这一差距。
- en: Infrastructure optimizations for accelerators on AWS
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 上加速器的基础设施优化
- en: 'Here, you’re going to learn about five key topics that can determine how well
    your scripts use the GPU infrastructure available on AWS. At this point in your
    journey, I am not expecting you to be an expert in any of these. I just want to
    you know that they exist and that you may need to update flags and configurations
    related to them later in your workflows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你将学习到五个关键主题，它们能够决定你的脚本如何使用 AWS 上的 GPU 基础设施。在你学习的这个阶段，我不期望你成为这些主题的专家。我只是希望你了解它们的存在，并且知道在以后工作流中，你可能需要更新与它们相关的标志和配置：
- en: '**EFA**: Amazon’s **Elastic Fabric Adapter** is a custom networking solution
    on AWS that provides optimal scale for high-performance deep learning. Purpose-built
    for the Amazon EC2 network topology, it enables seamless networking scale from
    just a few to a few hundred to thousands of GPUs on AWS.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EFA**：亚马逊的 **弹性网络适配器** 是 AWS 上的定制网络解决方案，旨在为高性能深度学习提供最佳规模。专为亚马逊 EC2 网络拓扑结构设计，它使从几个到几百甚至几千个
    GPU 在 AWS 上实现无缝的网络扩展。'
- en: '`ml.p4d.24xlarge`, `ml.p3dn.24xlarge`, `ml.g4dn.12xlarge`, and more.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ml.p4d.24xlarge`、`ml.p3dn.24xlarge`、`ml.g4dn.12xlarge` 等等。'
- en: '`AllReduce`, `Broadcast`, `Reduce`, `AllGather`, and `ReduceScatter`. Most
    distributed training software frameworks you will work with use a combination
    of these or custom implementations of these algorithms in a variety of ways. [*Chapter
    5*](B18942_05.xhtml#_idTextAnchor085) is devoted to exploring this in more detail.
    Another key NVIDIA library to know about is CUDA, as mentioned previously, which
    lets you run your deep learning framework software on the accelerators.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AllReduce`、`Broadcast`、`Reduce`、`AllGather` 和 `ReduceScatter`。你将使用的大多数分布式训练软件框架都采用了这些算法的组合或其自定义实现，并且有多种方式。[*第5章*](B18942_05.xhtml#_idTextAnchor085)将详细探讨这一点。另一个需要了解的关键NVIDIA库是CUDA，正如前面提到的，它允许你在加速器上运行深度学习框架软件。'
- en: '**GPUDirectRDMA**: This is an NVIDIA tool that allows GPUs to communicate directly
    with each other on the same instance without needing to hop onto the CPU. This
    is also available on AWS with select instances.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPUDirectRDMA**：这是NVIDIA的一款工具，允许GPU在同一实例上直接相互通信，而无需经过CPU。这也可以在AWS上通过特定的实例使用。'
- en: '**Open MPI**: **Open Message Passing Interface** is an open source project
    that enables remote machines to easily communicate with each other. The vast majority
    of your distributed training workloads, especially those that run on SageMaker,
    will use MPI as a base communication layer for the various workers to stay in
    sync with each other.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Open MPI**：**开放消息传递接口**是一个开源项目，旨在使远程计算机能够轻松地相互通信。你大多数的分布式训练工作负载，特别是那些在SageMaker上运行的工作负载，将使用MPI作为基础通信层，以确保各个工作节点能够保持同步。'
- en: 'If you’re thinking, "*Now, how do I go use all of these things?",* the answer
    is usually pretty simple. It’s three things, as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想，"*现在，我该如何使用这些东西呢？*"，答案通常非常简单。只需要三步，如下所示：
- en: First, ask yourself, *which base container am I using*? If you’re using one
    of the AWS deep learning containers, then all of these capabilities will be provided
    to you after our extensive tests and checks.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，问问自己，*我使用的是哪种基础容器*？如果你使用的是AWS的深度学习容器，那么在我们进行广泛的测试和检查后，这些能力都将会提供给你。
- en: Second, take a look at which instance you are using. As you learned previously,
    each instance type opens up your application to using, or not using, certain features
    on AWS. Try to make sure you’re getting the best performance you can!
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，查看你使用的是哪种实例。正如你之前学到的，每种实例类型都会影响你在AWS上是否能使用某些功能。确保你获得最佳性能！
- en: Third, look at ways to configure these in your job parameters. In SageMaker,
    we’ll use hyperparameters and settings in your scripts to ensure you’re maxing
    out performance.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，查看如何在作业参数中配置这些内容。在SageMaker中，我们将使用超参数和脚本中的设置，确保你能够最大化性能。
- en: Now that you’ve learned a bit about optimizing GPU performance, let’s take a
    look at troubleshooting performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了一些优化GPU性能的知识，接下来我们来看看如何排查性能问题。
- en: Troubleshooting accelerator performance
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排查加速器性能问题
- en: Before we can analyze our GPU performance, we need to understand generally how
    to debug and analyze performance on our training platform. SageMaker has some
    really nice solutions for this. First, all of your logs are sent to **Amazon CloudWatch**,
    another AWS service that can help you monitor your job performance. Each node
    in your cluster will have a full dedicated log stream, and you can read that log
    stream to view your overall training environment, how SageMaker runs your job,
    what status your job is in, and all of the logs your script emits. Everything
    you write to standard out, or print statements, is automatically captured and
    stored in CloudWatch. The first step to debugging your code is to take a look
    at the logs and figure out what really went wrong.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析GPU性能之前，我们需要一般了解如何在训练平台上进行调试和分析性能。SageMaker为此提供了一些非常不错的解决方案。首先，所有的日志都会发送到**Amazon
    CloudWatch**，这是另一个AWS服务，帮助你监控作业性能。你集群中的每个节点都会有一个完整的专用日志流，你可以通过查看日志流来了解整个训练环境，SageMaker如何运行你的作业，你的作业状态，以及你的脚本所生成的所有日志。所有你写入标准输出的内容，或者打印的语句，都会被自动捕获并存储在CloudWatch中。调试代码的第一步是查看日志，弄清楚到底出了什么问题。
- en: Once you know what’s wrong in your script, you’ll probably want to quickly fix
    it and get it back online, right? That’s why we introduced **managed warm pools**
    on SageMaker, a feature that keeps your training cluster online, even after a
    job has finished. With SageMaker warm pools, you can now run new jobs on SageMaker
    Training in just a few seconds!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你发现脚本中存在问题，你可能会想尽快修复并让它恢复运行，对吧？这就是为什么我们在SageMaker上引入了**托管的热池**功能，一个即使在任务完成后也能保持训练集群在线的功能。通过SageMaker热池，你现在可以在几秒钟内就在SageMaker训练上运行新的任务！
- en: With a script working, next, you’ll need to analyze the overall performance
    of your job. This is where debugging tools come in really handy. SageMaker offers
    a debugger and profiler, both of which actually spin up remote instances while
    your job is running to apply rules and check on your tensors throughout the training
    process. The profiler is an especially nice tool to use; it automatically generates
    graphs and charts for you, which you can use to assess the overall performance
    of your job, including which GPUs are being utilized, and how much. NVIDIA also
    offers tooling for GPU debugging and profiling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当脚本工作正常时，接下来你需要分析任务的整体性能。这时调试工具就非常有用。SageMaker提供了调试器和分析器，它们在任务运行时会启动远程实例，应用规则并在整个训练过程中检查张量。分析器是一个特别有用的工具；它会自动生成图表，帮助你评估任务的整体性能，包括哪些GPU正在被使用，以及使用的程度。NVIDIA还提供了GPU调试和分析工具。
- en: As we mentioned before, writing software to seamlessly orchestrate tens of thousands
    of GPU cores is no small task. And as a result, it’s really common for GPUs to
    suddenly go bad. You might see NCCL errors, CUDA errors, or other seemingly unexplainable
    faults. For many of these, SageMaker actually runs GPU health checks ahead of
    time on your behalf! This is why the `p4d` instances take much longer to initialize
    than the smaller instances; we are analyzing the health of the GPUs prior to exposing
    them to you.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，编写软件以无缝协调成千上万的GPU核心绝非易事。因此，GPU突然出现故障的情况非常常见。你可能会看到NCCL错误、CUDA错误或其他看似无法解释的故障。对于其中许多情况，SageMaker实际上会提前为你运行GPU健康检查！这也是为什么`p4d`实例初始化时间比较长的原因；我们在将GPU暴露给你之前，会分析它们的健康状况。
- en: Outside of these known GPU-centric issues, you may see other faults such as
    your loss not decreasing or suddenly exploding, insufficient capacity, oddly low
    GPU throughput, or small changes in the node topology. For many of these, it’s
    common to implement a **Lambda function** in your account to monitor your job.
    You can use this Lambda function to analyze your Cloudwatch logs, trigger alerts,
    restart a job, and more.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些已知的以GPU为中心的问题外，你可能还会遇到其他故障，比如损失值不下降或突然爆炸、容量不足、GPU吞吐量异常低或节点拓扑结构发生小变化。对于这些问题，通常会在你的账户中实现一个**Lambda函数**来监控你的任务。你可以使用这个Lambda函数来分析Cloudwatch日志、触发警报、重启任务等。
- en: Just remember to *checkpoint your model at least every 2 to 3 hours*. We’ll
    cover most of these best practices for training at scale on SageMaker in the coming
    chapters, but for now, simply know that you need to write a full copy of your
    most recently trained model with some regularity throughout the training loop.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 只要记住*每2到3小时检查一次模型*。我们将在接下来的章节中讨论在SageMaker上大规模训练时的最佳实践，但现在请记住，你需要定期写入最近训练的模型的完整副本，以确保训练过程的顺利进行。
- en: Now that you’ve learned about some techniques for troubleshooting GPU performance,
    let’s wrap up everything you’ve just learned in this chapter.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了一些排查GPU性能问题的技术，接下来让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced accelerators for machine learning, including
    how they are different from standard CPU processing and why you need them for
    large-scale deep learning. We covered some techniques for acquiring accelerators
    and getting them ready for software development and model training. We covered
    key aspects of Amazon SageMaker, notably Studio, Training, and hosting. You should
    know that there are key software frameworks that let you run code on GPUs, such
    as NCCL, CUDA, and more. You should also know about the top features that AWS
    provides for high-performance GPU conception to train deep learning models, such
    as EFA, Nitro, and more. We covered finding and building containers with these
    packages preinstalled, to successfully run your scripts on them. We also covered
    debugging your code on SageMaker and troubleshooting GPU performance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习加速器，包括它们如何与标准CPU处理方式不同，以及为何在大规模深度学习中需要它们。我们讲解了获取加速器并为软件开发和模型训练做好准备的一些技术。我们还介绍了Amazon
    SageMaker的关键方面，特别是Studio、Training和托管。你应该知道，有一些关键的软件框架让你可以在GPU上运行代码，例如NCCL、CUDA等。你还应该了解AWS提供的用于高性能GPU计算的顶级功能，以训练深度学习模型，如EFA、Nitro等。我们介绍了如何找到并构建预装了这些软件包的容器，以便成功运行脚本。我们还讲解了如何在SageMaker上调试代码并排查GPU性能问题。
- en: Now that we’ve learned about GPUs in some detail, in the next chapter, we’ll
    explore the fundamentals of distributed training!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细了解了GPU，接下来的章节将探索分布式训练的基础知识！
- en: References
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下内容，了解更多本章中涉及的一些主题：
- en: 'A micro-controlled peripheral processor: [https://dl.acm.org/doi/10.1145/800203.806247](https://dl.acm.org/doi/10.1145/800203.806247)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微控制外围处理器：[https://dl.acm.org/doi/10.1145/800203.806247](https://dl.acm.org/doi/10.1145/800203.806247)
- en: '*AWS, deep* *learning*: [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*AWS，深度* *学习*：[https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)'
