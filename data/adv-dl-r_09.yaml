- en: Applying Autoencoder Neural Networks Using Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 应用自编码器神经网络
- en: 'Autoencoder networks belong to the unsupervised learning category of methods,
    where labeled target values are not available. However, since autoencoders often
    use targets that are some form of input data, they can also be called self-supervised
    learning methods. In this chapter, we will learn how to apply autoencoder neural
    networks using Keras. We will cover three applications of autoencoders: dimension
    reduction, image denoising, and image correction. The examples in this chapter
    will use images of fashion items, images of numbers, and pictures containing people.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器网络属于无监督学习方法的范畴，其中没有可用的标注目标值。然而，由于自编码器通常使用某种形式的输入数据作为目标，它们也可以称为自监督学习方法。在本章中，我们将学习如何使用
    Keras 应用自编码器神经网络。我们将涵盖自编码器的三种应用：降维、图像去噪和图像修复。本章中的示例将使用时尚物品图像、数字图像以及包含人物的图片。
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在本章中，我们将涵盖以下主题：
- en: Types of autoencoders
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的类型
- en: Dimension reduction autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维自编码器
- en: Denoising autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Image correction autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像修复自编码器
- en: Types of autoencoders
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器的类型
- en: 'Autoencoder neural networks consist of two main parts:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器神经网络由两大部分组成：
- en: The first part is called the encoder, which reduces the dimensions of the input
    data. Generally, this is an image. When data from an input image is passed through
    a network that leads to a lower dimension, the network is forced to extract only
    the most important features of the input data.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一部分被称为编码器，它将输入数据的维度降低。通常，这是一张图像。当输入图像的数据通过一个将其降维的网络时，网络被迫只提取输入数据中最重要的特征。
- en: The second part of the autoencoder is called the decoder and it tries to reconstruct
    the original data from whatever is available from the output of the encoder. The
    autoencoder network is trained by specifying what output this network should try
    to match.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的第二部分被称为解码器，它尝试从编码器的输出中重建原始数据。通过指定该网络应尝试匹配的输出，训练自编码器网络。
- en: 'Let''s consider some examples where we will use image data. If the output that''s
    specified is the same image that was given as input, then after training, the
    autoencoder network is expected to provide an image with a lower resolution that
    retains the key features of the input image but misses some finer details that
    were part of the original input image. This type of autoencoder can be used for
    dimension reduction applications. Since autoencoders are based on neural networks
    that are able to capture non-linearity in data, they have superior performance
    compared to methods that only use linear functions. The following diagram shows
    the encoder and decoder parts of autoencoder networks:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些使用图像数据的示例。如果指定的输出是与输入图像相同的图像，那么在训练后，自编码器网络应该提供一张低分辨率的图像，这张图像保留了输入图像的关键特征，但遗漏了一些原始输入图像的细节。此类型的自编码器可用于降维应用。由于自编码器基于能够捕捉数据非线性的神经网络，它们的表现优于仅使用线性函数的方法。下图展示了自编码器网络的编码器和解码器部分：
- en: '![](img/0b47cb56-2078-4095-8607-d50b35bbfe3c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b47cb56-2078-4095-8607-d50b35bbfe3c.png)'
- en: If we train autoencoders so that the input image contains some noise or non-clarity
    and the output as the same image but without any noise, then we can create denoising
    autoencoders. Similarly, if we train autoencoders with such input/output images
    where we have images with and without glasses, or with and without a mustache,
    and so on, we can create networks that help with image correction/modification.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练自编码器，使得输入图像包含噪声或不清晰的内容，而输出则是去除噪声后的相同图像，那么我们就可以创建去噪自编码器。类似地，如果我们使用带有眼镜和不带眼镜的图像，或者带有胡子和不带胡子的图像等输入/输出图像来训练自编码器，那么我们就可以创建有助于图像修复/修改的网络。
- en: 'Next, we will look at three separate examples of how to use an autoencoder:
    using dimension reduction, image denoising, and image correction. We will start
    by using autoencoders for dimension reduction.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将分别通过三个示例来看如何使用自编码器：用于降维、图像去噪和图像修复。我们将从使用自编码器进行降维开始。
- en: Dimension reduction autoencoders
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维自编码器
- en: In this section, we will use fashion-MNIST data, specify the autoencoder model
    architecture, compile the model, fit the model, and then reconstruct the images.
    Note that fashion-MNIST is part of the Keras library.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用时尚-MNIST数据集，指定自编码器模型的架构，编译模型，拟合模型，然后重建图像。请注意，时尚-MNIST是Keras库的一部分。
- en: MNIST fashion data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST时尚数据集
- en: 'We will continue to use the Keras and EBImage libraries. The code for reading
    the fashion-MNIST data is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用Keras和EBImage库。读取时尚-MNIST数据的代码如下：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the training data has 60,000 images and the test data has 10,000 images
    of fashion items. Since we will be using an unsupervised learning approach for
    this example, we will not use the labels that are available for the train and
    test data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，训练数据有60,000张图片，测试数据有10,000张时尚商品图片。由于我们将在此示例中使用无监督学习方法，因此我们不会使用训练和测试数据中的标签。
- en: 'We store the training image data in `trainx` and test image data in `testx`,
    as shown in the following code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练图像数据存储在`trainx`中，将测试图像数据存储在`testx`中，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first 64 images of fashion items can be seen in the following image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前64张时尚商品图片：
- en: '![](img/7a0f5048-5008-4987-95a7-7adc47b687b6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a0f5048-5008-4987-95a7-7adc47b687b6.png)'
- en: 'Next, we will reshape the image data into a suitable format, as shown in the
    following code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图像数据重塑为合适的格式，如下所示：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we have also divided `trainx` and `testx` by 255 to change the range of
    values that are between 0-255 to a range between 0 and 1.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还将`trainx`和`testx`除以255，将原本介于0到255之间的值的范围转换为介于0和1之间的范围。
- en: Encoder model
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器模型
- en: 'To specify the encoder model architecture, we will use the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定编码器模型的架构，我们将使用以下代码：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, for the input to the encoder, we specify the input layer so that it's
    28 x 28 x 1 in size. Two convolutional layers, one with 8 filters and another
    with 4 filters, are used. Activation functions for both of these layers use **rectified
    linear units** (**relus**). The convolutional layer includes `padding = 'same'`,
    which retains the height and width of the input at the time of the output. For
    example, after the first convolution layer, the output has 28 x 28 as its height
    and width. Each convolution layer is followed by pooling layers. After the first
    pooling layer, the height and width change to 14 x 14, and, after the second pooling
    layer, it changes to 7 x 7\. The output of the encoder network in this example
    is 7 x 7 x 4.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于编码器的输入，我们指定了输入层，使其大小为28 x 28 x 1。使用了两个卷积层，一个具有8个过滤器，另一个具有4个过滤器。这两个层的激活函数都使用**修正线性单元**（**relu**）。卷积层包括`padding
    = 'same'`，这保持了输入在输出时的高度和宽度。例如，在第一个卷积层之后，输出的高度和宽度为28 x 28。每个卷积层后面都有池化层。在第一个池化层之后，高度和宽度变为14
    x 14，而在第二个池化层之后，变为7 x 7。此示例中编码器网络的输出为7 x 7 x 4。
- en: Decoder model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器模型
- en: 'To specify the decoder model architecture, we will use the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定解码器模型的架构，我们将使用以下代码：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, the encoder model has become the input for the decoder model. For the
    decoder network, we use a similar structure, with the first convolutional layer
    having 4 filters and the second convolutional layer having 8 filters. In addition,
    instead of pooling layers, we now use up-sampling layers. The first upsampling
    layer changes the height and width to 14 x 14 and the second upsampling layer
    restores it to the original height and width of 28 x 28\. In the last layer, we
    make use of the sigmoid activation function, which ensures that the output values
    remain between 0 and 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，编码器模型已经成为解码器模型的输入。对于解码器网络，我们使用了一个类似的结构，第一层卷积层有4个过滤器，第二层卷积层有8个过滤器。此外，我们现在使用的是上采样层，而不是池化层。第一个上采样层将高度和宽度变为14
    x 14，第二个上采样层将其恢复到原始的高度和宽度28 x 28。在最后一层，我们使用了sigmoid激活函数，确保输出值保持在0到1之间。
- en: Autoencoder model
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器模型
- en: 'The autoencoder model and the summary of the model showing the output shape
    and the number of parameters for each layer is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器模型及其模型摘要，显示每层的输出形状和参数数量如下：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, the autoencoder model has five convolutional layers, two maximum pooling
    layers, and two upsampling layers, apart from the input layer. Here, the total
    number of parameters in this autoencoder model is 889.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，自编码器模型有五个卷积层、两个最大池化层和两个上采样层，除了输入层。此自编码器模型的总参数数量为889。
- en: Compiling and fitting the model
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和拟合模型
- en: 'Next, we will compile and fit the model using the following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码编译并拟合模型：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we compile the model using mean squared error as the loss function and
    specify `adam` as the optimizer. For training the model, we will make use of `trainx `as
    the input and output. We'll use `textx` for validation. We fit the model with
    a batch size of 32 and use 20 epochs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用均方误差作为损失函数编译模型，并指定`adam`作为优化器。为了训练模型，我们将使用`trainx`作为输入和输出。我们将使用`textx`作为验证集。我们使用32的批次大小，并进行20次训练。
- en: 'The following output shows the plot of the loss values for the train and validation
    data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了训练数据和验证数据的损失值图表：
- en: '![](img/76f4d73c-4630-49ef-b704-94b3edda787c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76f4d73c-4630-49ef-b704-94b3edda787c.png)'
- en: The preceding plot shows good convergence and doesn't show any signs of overfitting.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了良好的收敛性，并且没有出现过拟合的迹象。
- en: Reconstructed images
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建图像
- en: 'To obtain reconstructed images, we use `predict_on_batch` to predict the output
    using the autoencoder model. We do this with the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得重建图像，我们使用`predict_on_batch`来使用自编码器模型预测输出。我们使用以下代码来完成这一操作：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The first five fashion images from the training data (first row) and the corresponding
    reconstructed images (second row) are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 来自训练数据的前五张时尚图像（第一行）和对应的重建图像（第二行）如下：
- en: '![](img/c8c31f06-520c-4258-a0b7-9bdb92d08ee3.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8c31f06-520c-4258-a0b7-9bdb92d08ee3.png)'
- en: Here, as expected, the reconstructed images are seen to capture key features
    of the training images. However, it ignores certain finer details. For example,
    the logos that are more clearly visible in the original training images are blurred
    in the reconstructed images.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，正如预期的那样，重建的图像捕捉到了训练图像的关键特征。然而，它忽略了一些更细致的细节。例如，原始训练图像中更清晰可见的徽标在重建图像中变得模糊。
- en: 'We can also take a look at the plot of the original and reconstructed images
    using images from the test data. For this, we can use the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看使用测试数据中的图像绘制的原始图像和重建图像的图表。为此，我们可以使用以下代码：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following image shows the original images (first row) and reconstructed
    images (second row) using the test data:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了使用测试数据的原始图像（第一行）和重建图像（第二行）：
- en: '![](img/a2fc374d-bcf1-4380-b27f-c35a2ded9a8c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2fc374d-bcf1-4380-b27f-c35a2ded9a8c.png)'
- en: Here, the reconstructed images behave as they did previously for the training
    data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，重建的图像与训练数据的表现相同。
- en: In this example, we have used MNIST fashion data to build an autoencoder network
    that helps reduce the dimensions of the images by keeping the main features and
    removing the features that involve finer details. Next, we will look at another
    variant of the autoencoder model that helps remove noise from images.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了MNIST时尚数据构建了一个自编码器网络，通过保留主要特征并去除涉及细节的特征，帮助降低图像的维度。接下来，我们将探讨自编码器模型的另一个变种，它有助于去除图像中的噪声。
- en: Denoising autoencoders
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: In situations where input images contain unwanted noise, autoencoder networks
    can be trained to remove such noise. This is achieved by providing images with
    noise as input and providing a clean version of the same image as output. The
    autoencoder network is trained so that the output of the autoencoder is as close
    to the output image as possible.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入图像包含不必要的噪声的情况下，可以训练自编码器网络来去除这些噪声。这是通过将含有噪声的图像作为输入，并提供相同图像的干净版本作为输出来实现的。自编码器网络被训练，使得自编码器的输出尽可能接近目标图像。
- en: MNIST data
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST数据
- en: 'We will make use of MNIST data that''s available in the Keras package to illustrate
    the steps that are involved in creating a denoising autoencoder network. MNIST
    data can be read using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用Keras包中提供的MNIST数据来演示创建去噪自编码器网络的步骤。可以使用以下代码读取MNIST数据：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The structure of the MNIST data indicates that it contains train and test data,
    along with the respective labels. The training data has 60,000 images of digits
    from 0 to 9\. Similarly, the test data has 10,000 images of digits from 0 to 9\.
    Although each image has a corresponding label identifying the image, in this example,
    the data for labels isn't required and so we will ignore this information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据的结构表明它包含训练数据和测试数据，以及相应的标签。训练数据包含60,000张0到9的数字图像。类似地，测试数据包含10,000张0到9的数字图像。尽管每张图像都有一个相应的标签来标识该图像，但在这个示例中，标签数据不需要，因此我们将忽略这部分信息。
- en: 'We will be storing training images in `trainx` and the test images in `testx`.
    To do this, we will use the following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把训练图像存储在`trainx`中，把测试图像存储在`testx`中。为此，我们将使用以下代码：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following image shows a plot of 64 images in 8 rows and 8 columns based
    on images of digits between 0 and 9 from MNIST:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了基于MNIST中0到9之间数字图像的64张图像，排列成8行8列：
- en: '![](img/a795fbe1-fb3c-48ff-8367-9667b7b0a8fb.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a795fbe1-fb3c-48ff-8367-9667b7b0a8fb.png)'
- en: The preceding plot shows handwritten digits in various writing styles. We will
    reshape this image data in the required format and add random noise to it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图展示了各种书写风格的手写数字。我们将把这些图像数据重新调整为所需格式，并向其添加随机噪声。
- en: Data preparation
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Next, we will reshape images in the required format using the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码按要求的格式重新调整图像：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we've reshaped the training data so that it's 60,000 x 28 x 28 x 1 in
    size and reshaped the test data so that it's 10,000 x 28 x 28 x 1 in size. We
    also divided the pixel values that are between 0 and 255 by 255 to obtain a new
    range that is between 0 and 1.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已将训练数据重新调整为60,000 x 28 x 28 x 1的大小，并将测试数据调整为10,000 x 28 x 28 x 1的大小。我们还将介于0到255之间的像素值除以255，从而得到一个新的范围，介于0和1之间。
- en: Adding noise
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加噪声
- en: 'To add noise to the training images, we need to obtain 60,000 × 28 × 28 random
    numbers between 0 and 1 using uniform distribution using the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要向训练图像添加噪声，我们需要使用以下代码生成60,000 × 28 × 28的随机数，这些随机数介于0和1之间，采用均匀分布：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we're reshaping the random numbers that were generated using uniform distribution
    to match the dimensions of the matrix that we have for the training images. The
    results are plotted in the form of images that show resulting images containing
    noise.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用均匀分布生成的随机数重新调整大小，以匹配我们训练图像矩阵的维度。结果将以图像的形式呈现，展示添加噪声后的图像。
- en: 'The following image shows the images containing noise:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了包含噪声的图像：
- en: '![](img/c4569d41-9dba-4c13-a702-617696522fcc.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4569d41-9dba-4c13-a702-617696522fcc.png)'
- en: 'The images depicting noise are added to the images that are stored in `trainx`.
    We need to divide this by 2 to keep the resulting `trainn` values between 0 and
    1\. We can use the following code to do this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声图像被添加到存储在`trainx`中的图像中。我们需要将其除以2，以保持结果`trainn`的值在0和1之间。我们可以使用以下代码来实现这一点：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The first 64 training images, along with their noise, are shown in the following
    image:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了前64个训练图像及其噪声：
- en: '![](img/385dc121-daff-4c4f-82b2-230468ef3b50.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/385dc121-daff-4c4f-82b2-230468ef3b50.png)'
- en: Although noise is added to the original handwritten digits, the digits are still
    readable. The main objective of using a denoising autoencoder is to train a network
    that retains the handwritten digits and removes noise from the images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管噪声已被添加到原始手写数字中，但这些数字依然可以辨认。使用去噪自编码器的主要目的是训练一个网络，能够保留手写数字并去除图像中的噪声。
- en: 'We will repeat the same steps for the test data using the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码对测试数据重复相同的步骤：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we have added noise to test images and stored them in `testn`. Now, we
    can specify the encoder architecture.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已向测试图像添加噪声，并将它们存储在`testn`中。现在，我们可以指定编码器架构。
- en: Encoder model
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器模型
- en: 'The code that''s used for the encoder network is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编码器网络的代码如下：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, the input layer is specified to be 28 x 28 x 1 in size. We use two convolution
    layers with 32 filters each and a rectifier linear unit as the activation function.
    Each convolution layer is followed by pooling layers. After the first pooling
    layer, the height and width change to 14 x 14, and after the second pooling layer,
    this changes to 7 x 7\. The output of the encoder network in this example has
    7 x 7 x 32 dimensions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，输入层的大小被指定为28 x 28 x 1。我们使用了两个每个有32个滤波器的卷积层，并且使用了修正线性单元作为激活函数。每个卷积层后都接着池化层。第一次池化层后，高度和宽度变为14
    x 14，第二次池化层后，变为7 x 7。本例中，编码器网络的输出具有7 x 7 x 32的维度。
- en: Decoder model
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器模型
- en: 'For the decoder network, we keep the same structure, except that, instead of
    pooling layers, we use upsampling layers. We can use the following code to do
    this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器网络，我们保持相同的结构，不同之处在于，我们使用上采样层，而不是池化层。我们可以使用以下代码来实现这一点：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, the first upsampling layer changes the height and width
    to 14 x 14 and the second upsampling layer restores it to the original height
    and width of 28 x 28\. In the last layer, we use a sigmoid activation function,
    which ensures that the output values remain between 0 and 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，第一个上采样层将高度和宽度改变为14 x 14，第二个上采样层将其恢复到原始的28 x 28高度和宽度。在最后一层，我们使用了sigmoid激活函数，确保输出值保持在0和1之间。
- en: Autoencoder model
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器模型
- en: 'Now, we can specify the autoencoder network. The autoencoder''s model and summary
    is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以指定自编码器网络。自编码器的模型和摘要如下所示：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'From the preceding summary of the autoencoder network, we can see that there
    are 28,353 parameters in total. Next, we will compile this model using the following
    code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面自编码器网络的摘要中，我们可以看到总共有28,353个参数。接下来，我们将使用以下代码编译此模型：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For denoising autoencoders, the`bianary_crossentropy `loss function performs
    better than other options.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于去噪自编码器，`binary_crossentropy`损失函数比其他选项表现更好。
- en: When compiling the autoencoder model, we will use `binary_crossentropy` for
    the loss function since the input values are between 0 and 1\. For the optimizer,
    we will use `adam`. After compiling the model, we are ready to fit it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译自编码器模型时，我们将使用`binary_crossentropy`作为损失函数，因为输入值介于0和1之间。对于优化器，我们将使用`adam`。编译模型后，我们准备好进行拟合。
- en: Fitting the model
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'To train the model, we use images with noise stored in `trainn` as input and
    images without noise stored in `trainx` as output. The code that''s used to fit
    the model is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们使用存储在`trainn`中的带噪声图像作为输入，使用存储在`trainx`中的无噪声图像作为输出。用于拟合模型的代码如下：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, we also use `testn` and `testx` to monitor validation errors. We will
    run 100 epochs with a batch size of 128\. After network training is completed,
    we obtain the loss values for the train and test data using the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还使用`testn`和`testx`来监控验证误差。我们将运行100个epoch，每个批次的大小为128。网络训练完成后，我们可以使用以下代码获取训练数据和测试数据的损失值：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The loss for the training and test data is 0.0743 and 0.0739, respectively.
    The closeness of the two numbers indicates the lack of an overfitting problem.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和测试数据的损失分别为0.0743和0.0739。这两个数字的接近度表明没有过拟合问题。
- en: Image reconstruction
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像重建
- en: 'After fitting the model, we can reconstruct images using the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型后，我们可以使用以下代码重建图像：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding code, we have used `ae_model` to reconstruct the images by
    providing images with noise contained in `trainn`. As shown in the following image,
    we have plotted the first 64 reconstructed images to see if the noisy images become
    clearer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`ae_model`通过提供包含噪声的`trainn`图像来重建图像。如以下图片所示，我们绘制了前64张重建图像，以查看噪声图像是否变得更清晰：
- en: '![](img/639dcb99-aa19-45ac-b4c9-0c984f5ae639.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/639dcb99-aa19-45ac-b4c9-0c984f5ae639.png)'
- en: 'From the preceding plot, we can observe that the autoencoder network has successfully
    removed noise. We can also reconstruct the images for the test data with the help
    of `ae_model` using the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以观察到自编码器网络已经成功去除噪声。我们还可以借助`ae_model`使用以下代码重建测试数据的图像：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting images for the first 64 handwritten digits in the test data are
    as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据中前64个手写数字的重建图像如下所示：
- en: '![](img/cdb8d104-dd40-4bbf-90f1-f12906fd341a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cdb8d104-dd40-4bbf-90f1-f12906fd341a.png)'
- en: 'Here, we can observe that the denoising autoencoder does a decent job of removing
    noise from the images of 0 to 9 digits. To look more closely at the model''s performance,
    we can plot the first image in the test data, the corresponding image with noise,
    and the reconstructed image after noise removal, like so:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以观察到去噪自编码器在去除0到9数字图像的噪声方面表现得相当不错。为了更仔细地观察模型的性能，我们可以绘制测试数据中的第一张图片、带噪声的相应图像以及去噪后重建的图像，如下所示：
- en: '![](img/8631120a-b2aa-4b7e-bf32-bc466e01fb34.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8631120a-b2aa-4b7e-bf32-bc466e01fb34.png)'
- en: In the preceding screenshot, the first image is the original image, while the
    second image is the one that's obtained after adding noise. The autoencoder was
    provided the second image as input and the results that were obtained from the
    model (third image) were made to match the first image. Here, we can see that
    the denoising autoencoder network helps remove noise. Note that the third image
    is unable to retain some of the finer details of the original image that we can
    see in the first image. For example, in the original image, seven appears to be
    slightly thicker at the beginning and toward the lower part compared to the third
    image. However, it does successfully extract the overall pattern of seven from
    the image containing digit seven with noise.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，第一张图像是原始图像，而第二张图像是在添加噪声之后得到的图像。自编码器将第二张图像作为输入，并且从模型获得的结果（第三张图像）被调整与第一张图像相匹配。在这里，我们可以看到去噪自编码器网络帮助去除了噪声。请注意，第三张图像无法保留原始图像中的一些细节，而这些细节在第一张图像中是可见的。例如，在原始图像中，数字七在开始和下部看起来比第三张图像中的七稍微粗一些。然而，它成功地从含有噪声的数字七图像中提取出了整体模式。
- en: Image correction
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像校正
- en: 'In this third application, we will go over an example where we''ll develop
    an autoencoder model to remove certain artificially created marks on various pictures.
    We will use 25 images containing a black line across the picture. The code for
    reading the image files and carrying out the related processing is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第三个应用中，我们将展示一个例子，在这个例子中我们将开发一个自编码器模型，用于去除图像上某些人工创建的标记。我们将使用25张包含横跨图像的黑色线条的图像。读取图像文件并进行相关处理的代码如下：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, we read images with `.jpeg` extensions from the `peoplex`
    folder and resize these images so that they have a height and width of 128 x 128\.
    We also update the dimensions to 128 x 128 x 3 since all the images are color
    images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们从`peoplex`文件夹读取`.jpeg`扩展名的图像，并调整这些图像的尺寸，使其高度和宽度为128 x 128。我们还将尺寸更新为128
    x 128 x 3，因为所有图像都是彩色图像。
- en: Images that need correction
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要校正的图像
- en: 'We will use the following code to combine the 25 images and then plot them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码合并这25张图像，然后绘制它们：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we save the data involving all 25 images after combining them into `trainx`.
    Looking at the structure of `tranix`, we can see that, after combining the image
    data, the dimensions now become 128 x 128 x 3 x 16\. In order to change this to
    the required format of 16 x 128 x 128 x 3, we use the `aperm` function. Then,
    we plot all 25 images. Note that if the images are plotted with rotation, they
    can be adjusted to the correct orientation very easily on any computer. The following
    are the 25 pictures, with a black line across all the images:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将所有25张图像合并后的数据保存到`trainx`中。通过查看`tranix`的结构，我们可以看到，在合并图像数据后，尺寸现在变为128 x
    128 x 3 x 16。为了将其更改为所需的格式16 x 128 x 128 x 3，我们使用`aperm`函数。然后，我们绘制所有25张图像。请注意，如果图像被旋转绘制，它们可以很容易地在任何计算机上调整到正确的方向。以下是带有黑色线条的25张图像：
- en: '![](img/89a138f3-655a-4d24-8728-7160f9b4e5d4.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89a138f3-655a-4d24-8728-7160f9b4e5d4.png)'
- en: The autoencoder model in this application will use these images with a black
    line as input and will be trained so that the black lines are removed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用中的自编码器模型将使用带有黑色线条的图像作为输入，并且会经过训练以去除黑色线条。
- en: Clean images
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 干净的图像
- en: 'We will also read the same 25 images without the black line and save them in
    `trainy`, as shown in the following code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将读取没有黑色线条的相同25张图像，并将它们保存在`trainy`中，如下代码所示：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here, after resizing and changing dimensions, we are combining the images,
    just like we did previously. We also need to make some adjustments to the dimensions
    to obtain the required format. Next, we will plot all 25 clean images, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，在调整大小和更改尺寸后，我们像之前一样合并这些图像。我们还需要对尺寸进行一些调整，以获得所需的格式。接下来，我们将绘制所有25张干净的图像，如下所示：
- en: '![](img/50fd289c-9b2f-4f3c-92d4-3571b323aa59.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50fd289c-9b2f-4f3c-92d4-3571b323aa59.png)'
- en: At the time of training the autoencoder network, we will use these clean images
    as output. Next, we will specify the encoder model architecture.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练自编码器网络时，我们将使用这些干净的图像作为输出。接下来，我们将指定编码器模型架构。
- en: Encoder model
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器模型
- en: 'For the encoder model, we will use three convolutional layers with 512, 512,
    and 256 filters, as shown in the following code:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编码器模型，我们将使用三个卷积层，分别具有512、512和256个滤波器，如下所示的代码所示：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, the encoder network is 16 x 16 x 256 in size. We will keep the other features
    similar to the encoder models that we used in the previous two examples. Now,
    we will specify the decoder architecture of the autoencoder network.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，编码器网络的尺寸是16 x 16 x 256。我们将保持其他特性与之前两个示例中的编码器模型相似。现在，我们将指定自动编码器网络的解码器架构。
- en: Decoder model
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器模型
- en: 'For the decoder model, the first three convolutional layers have 256, 512,
    and 512, filters, as shown in the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器模型，前3个卷积层的滤波器数量分别为256、512和512，如下所示：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we used upsampling layers. In the last convolutional layer, we made use
    of a sigmoid activation function. In the last convolutional layer, we used three
    filters since we are making use of color images. Finally, the output of the decoder
    model has 128 x 128 x 3 dimensions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了上采样层。在最后一个卷积层中，我们使用了sigmoid激活函数。在最后一个卷积层中，我们使用了三个滤波器，因为我们处理的是彩色图像。最后，解码器模型的输出维度为128
    x 128 x 3。
- en: Compiling and fitting the model
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和拟合模型
- en: 'Now, we can compile and fit the model using the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下代码编译和拟合模型：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the preceding code, we compile the autoencoder model using mean squared error
    as the loss function and specify `adam` as the optimizer. We use `trainx`, which
    contains images with a black line across them, as input to the model and `trainy`,
    which contains clean images, as output that the model tries to match. We specify
    the number of epochs as 100 and use a batch size of 128\. Using a validation split
    of 0.2 or 20%, we will use 20 images out of 25 for training and 5 images out of
    25 for computing validation errors.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用均方误差作为损失函数编译自动编码器模型，并指定`adam`作为优化器。我们使用包含黑线的图像`trainx`作为模型输入，使用包含干净图像的`trainy`作为模型尝试匹配的输出。我们将epoch数指定为100，批次大小为128。使用0.2或20%的验证拆分，我们将从25张图像中选择20张用于训练，5张用于计算验证误差。
- en: 'The following graph shows the mean square error for 100 epochs for the training
    and validation images for `model_three`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了`model_three`在100个epoch中训练和验证图像的均方误差：
- en: '![](img/82f0b487-14ef-4626-bac6-cde615926fd0.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82f0b487-14ef-4626-bac6-cde615926fd0.png)'
- en: The plot for the mean square error shows that there is an improvement in model
    performance based on the training and validation data as the model training proceeds.
    We can also see that, between about 80 and 100 epochs, the model's performance
    becomes approximately flat. In addition to this, it's suggested that increasing
    the number of epochs isn't likely to improve model performance any further.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差的图示显示，随着模型训练的进行，基于训练和验证数据，模型性能有了提升。我们还可以看到，在约80到100个epoch之间，模型的性能趋于平稳。此外，建议增加epoch的数量不太可能进一步提升模型性能。
- en: Reconstructing images from training data
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从训练数据重建图像
- en: 'Now, we can reconstruct images from the training data using the model that
    we have obtained. To do this, we can use the following code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用获得的模型从训练数据中重建图像。为此，我们可以使用以下代码：
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the preceding code, we''re using `predict_on_batch` to reconstruct the images
    after feeding `trainx`, which contains images with black lines across them. All
    25 reconstructed images can be seen here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`predict_on_batch`来重建图像，输入的是包含黑线的`trainx`。所有25张重建图像可以在这里看到：
- en: '![](img/05fe2408-d189-4377-a1be-5debedec0717.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05fe2408-d189-4377-a1be-5debedec0717.png)'
- en: From the preceding plot, it can be seen that the autoencoder model has learned
    to remove the black lines from the input images. The pictures are somewhat blurred
    since the autoencoder model tries to output only the main features from the images
    and misses certain details.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示中可以看出，自动编码器模型已经学会了去除输入图像中的黑线。由于自动编码器模型尝试仅输出图像的主要特征，并忽略了某些细节，因此图像会显得有些模糊。
- en: Reconstructing images from new data
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从新数据重建图像
- en: 'To test the autoencoder model with new and unseen data, we will make use of
    25 new images that have black lines across them. To do this, we will use the following
    code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用新数据和未见过的数据测试自动编码器模型，我们将使用25张新图像，这些图像上有黑线。为此，我们将使用以下代码：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As shown in the preceding code, we read the new image data and then formatted
    all images, like we did previously. All 25 new pictures with a black line across
    them are shown in the following plot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们读取了新的图像数据，并将所有图像格式化，像之前一样。所有带有黑线的25张新图像展示在下图中：
- en: '![](img/498d8de0-0595-43d8-b0df-f46cd3507b1f.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/498d8de0-0595-43d8-b0df-f46cd3507b1f.png)'
- en: 'Here, all 25 images have a black line across them. We will use the data from
    these new images and reconstruct the images using the autoencoder model that we''ve
    developed to remove the black lines. The code that''s used for reconstructing
    and plotting the images is as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，所有25张图片上都有一条黑线。我们将使用这些新图片的数据，并利用我们开发的自动编码器模型来重建图片，去除黑线。用于重建和绘制图片的代码如下：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following screenshot shows the reconstructed images after using the autoencoder
    model based on the 25 new images that had a black line across them:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了基于这25张带有黑线的新图片，使用自动编码器模型后重建的图像：
- en: '![](img/4986e67b-57f0-4576-a6fe-3322e07f6c0b.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4986e67b-57f0-4576-a6fe-3322e07f6c0b.png)'
- en: The preceding screenshot once again shows that the autoencoder model successfully
    removes the black lines from all the images. However, as we observed earlier,
    the image quality is low. This example provides promising results. If the results
    that were obtained also had a higher quality output for the images, then we could
    use this in several different situations. For example, we could reconstruct an
    image with glasses as an image without glasses or vice versa, or we may be able
    to reconstruct an image of a person without a smile to an image of them with a
    smile. There are several variants of such approaches that have the potential to
    have significant business value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图再次显示，自动编码器模型成功地去除了所有图像中的黑线。然而，正如我们之前观察到的，图像质量较低。这个例子提供了有希望的结果。如果得到的结果也能提供更高质量的图像输出，那么我们可以在多种不同的场景中使用这一方法。例如，我们可以将带眼镜的图像重建为不带眼镜的图像，反之亦然，或者我们可以将一个没有笑容的人的图像重建为带笑容的图像。这样的思路有许多变种，可能具有显著的商业价值。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went over three application examples of autoencoder networks.
    The first type of autoencoder involved a dimension reduction application. Here,
    we used an autoencoder network architecture that only allowed us to learn about
    the key features of the input image. The second type of autoencoder was illustrated
    using MNIST data containing images of numbers. We artificially added noise to
    the images of numbers and trained the network in such a way that it learned to
    remove noise from the input image. The third type of autoencoder network involved
    image correction application. The autoencoder network in this application was
    trained to remove a black line from input images.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了自动编码器网络的三个应用示例。第一种类型的自动编码器涉及一个降维应用。在这里，我们使用了一种自动编码器网络架构，只允许我们学习输入图像的关键特征。第二种类型的自动编码器使用包含数字图像的MNIST数据进行说明。我们人为地为数字图像添加噪声，并以这样的方式训练网络，使其学会去除输入图像中的噪声。第三种类型的自动编码器网络涉及图像修复应用。在此应用中，自动编码器网络被训练来去除输入图像中的黑线。
- en: In the next chapter, we will go over another class of deep networks, called
    **transfer learning**, and use them for image classification.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍另一类深度网络，称为**迁移学习**，并将其用于图像分类。
