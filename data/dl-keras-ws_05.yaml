- en: 5\. Improving Model Accuracy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 提高模型准确性
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces the concept of regularization for neural networks. Regularization
    aims to prevent the model from overfitting the training data during the training
    process and provides more accurate results when the model is tested on new unseen
    data. You will learn to utilize different regularization techniques—L1 and L2
    regularization and dropout regularization—to improve model performance. Regularization
    is an important component as it prevents neural networks from overfitting the
    training data and helps us build robust, accurate models that perform well on
    new, unseen data. By the end of this chapter, you will be able to implement a
    grid search and random search in scikit-learn and find the optimal hyperparameters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了神经网络的正则化概念。正则化的目的是防止模型在训练过程中对训练数据过拟合，并在模型在新数据上进行测试时提供更准确的结果。你将学会使用不同的正则化技术——L1和L2正则化以及丢弃法正则化——来提高模型的表现。正则化是一个重要的组成部分，因为它可以防止神经网络对训练数据过拟合，帮助我们构建出在新数据上表现良好的健壮、准确的模型。通过本章的学习，你将能够在scikit-learn中实现网格搜索和随机搜索，找到最优的超参数。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, we continued to develop our knowledge of creating accurate
    models with neural networks by experimenting with cross-validation as a method
    to test how various hyperparameters perform in an unbiased manner. We utilized
    leave-one-out cross-validation, in which we leave one record out of the training
    process for use in validation and repeat this for every record in the dataset.
    Then, we looked at k-fold cross-validation, in which we split the training dataset
    into `k` folds, train the model on `k-1` folds, and use the final fold for validation.
    These cross-validation methods allow us to train models with different hyperparameters
    and test their performance on unbiased data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们继续通过实验交叉验证，发展了使用神经网络创建准确模型的知识。交叉验证是一种无偏地测试各种超参数表现的方法。我们使用了留一法交叉验证，其中我们将一个记录从训练过程中留下，作为验证使用，并对数据集中的每一条记录重复这一过程。接着，我们介绍了k折交叉验证，我们将训练数据集分为`k`个折叠，在`k-1`个折叠上训练模型，并使用最后一个折叠进行验证。这些交叉验证方法允许我们使用不同的超参数训练模型，并在无偏数据上测试它们的表现。
- en: Deep learning is not only about building neural networks, training them using
    an available dataset, and reporting the model accuracy. It involves trying to
    understand your model and the dataset, as well as moving beyond a basic model
    by improving it in many aspects. In this chapter, you will learn about two very
    important groups of techniques for improving machine learning models in general,
    and deep learning models in particular. These techniques are regularization methods
    and hyperparameter tuning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不仅仅是构建神经网络，使用现有的数据集训练它们，并报告模型的准确性。它还涉及理解你的模型和数据集，并且通过改善许多方面，使基本模型超越。 在本章中，你将学习到两大类对提高机器学习模型（尤其是深度学习模型）非常重要的技术。这些技术分别是正则化方法和超参数调优。
- en: This chapter will further cover regularization methods—specifically, why we
    need them and how they help. Then, we'll introduce two of the most important and
    most commonly used regularization techniques. Here, you'll learn about parameter
    regularization and its two variations, `L1` and `L2` norm regularizations, in
    great detail. You will then learn about a regularization technique that was specifically
    designed for neural networks called **dropout regulation**. You will also practice
    implementing each of these techniques on Keras models by completing activities
    that involve real-life datasets. We'll end our discussion of regularization by
    briefly introducing some other regularization techniques that you may find helpful
    later in your work.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将进一步介绍正则化方法——特别是为什么我们需要它们，以及它们如何帮助我们。接着，我们将介绍两种最重要和最常用的正则化技术。在这里，你将详细了解参数正则化及其两种变体，`L1`和`L2`范数正则化。然后，你将了解一种专门为神经网络设计的正则化技术——**丢弃法（Dropout
    Regulation）**。你还将通过完成涉及真实数据集的活动，实践在Keras模型上实现这些技术。最后，我们将简要介绍一些其他正则化技术，这些技术在你后续的工作中可能会有所帮助。
- en: Then, we will talk about the importance of **hyperparameter tuning**, especially
    for deep learning models, by exploring how tuning the values of hyperparameters
    can dramatically affect model accuracy, as well as the challenge of tuning the
    many hyperparameters that require it when building deep neural networks. You will
    learn two very helpful methods in scikit-learn that you can use for performing
    hyperparameter tuning on Keras models, the benefits and drawbacks of each method,
    and how to combine them to gain the most from both. Lastly, you will practice
    implementing hyperparameter tuning for Keras models using scikit-learn optimizers
    by completing an activity.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论**超参数调优**的重要性，特别是对于深度学习模型，探讨如何调整超参数的值会显著影响模型的准确性，以及在构建深度神经网络时调优多个超参数的挑战。你将学习到两个非常有用的scikit-learn方法，这些方法可以用来对Keras模型进行超参数调优，并了解每种方法的优缺点，以及如何将它们结合起来，以便从中获得最大的收益。最后，你将通过完成一个实践活动来练习使用scikit-learn优化器为Keras模型实施超参数调优。
- en: Regularization
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: Since deep neural networks are highly flexible models, overfitting is an issue
    that can often arise when training them. Therefore, one very important part of
    becoming a deep learning expert is knowing how to detect overfitting, and subsequently
    how to address the overfitting problem in your model. Overfitting in your models
    will be clear if your model performs excellently on the training data but performs
    poorly on new, unseen data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度神经网络是高度灵活的模型，过拟合是训练过程中经常会遇到的问题。因此，成为深度学习专家的一个非常重要的部分是知道如何检测过拟合，并且随后如何解决模型中的过拟合问题。如果你的模型在训练数据上表现优异，但在新、未见过的数据上表现较差，那么模型的过拟合问题就很明显了。
- en: For example, if you build a model to classify images of dogs and cats into their
    respective classes and your image classifier performs with high accuracy during
    the training process but does not perform well on new examples, then this is an
    indication that your model has overfitted the training data. Regularization techniques
    are an important group of methods specifically aimed at reducing overfitting in
    machine learning models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你构建了一个模型来将狗和猫的图像分类到各自的类别，并且你的图像分类器在训练过程中表现出高准确率，但在新数据上表现不佳，这就表明你的模型对训练数据过拟合。正则化技术是一类重要的方法，专门用于减少机器学习模型的过拟合问题。
- en: Understanding regularization techniques thoroughly and being able to apply them
    to your deep neural networks is an essential step toward building deep neural
    networks in order to solve real-life problems. In this section, you will learn
    about the underlying concepts of regularization, which will provide you with the
    foundation that's required for the following sections, where you will learn how
    to implement various types of regularization methods using Keras.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 彻底理解正则化技术，并能够将其应用到你的深度神经网络中，是构建深度神经网络以解决现实问题的重要一步。在本节中，你将了解正则化的基本概念，这将为后续的章节奠定基础，在这些章节中，你将学习如何使用Keras实现各种类型的正则化方法。
- en: The Need for Regularization
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化的必要性
- en: The main goal of machine learning is to build models that perform well, not
    only on the examples they are trained on but also on new examples that were not
    included in the training. A good machine learning model is one that finds the
    form and the parameters of the true underlying process/function that's producing
    the training examples but does not capture the noise associated with individual
    training examples. Such a machine learning model can generalize well to new data
    that's produced by the same process later.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的主要目标是构建在训练数据上表现良好，并且能在新的、未包含在训练中的示例上同样表现良好的模型。一个好的机器学习模型应该能找到产生训练数据的真实底层过程/函数的形式和参数，而不是捕捉到个别训练数据的噪声。这样的模型能够很好地泛化到后续由相同过程生成的新数据上。
- en: The approaches we discussed previously—such as splitting a dataset into a training
    set and a test set, and cross-validation—were all designed to estimate the generalization
    ability of a trained model. In fact, the term that's used to refer to a test set
    error and cross-validation error is "generalization error." This simply means
    the error rate on examples that were not used in training. Once again, the main
    goal of machine learning is to build models with low generalization error rates.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的几种方法——比如将数据集分为训练集和测试集，以及交叉验证——都是为了估计训练模型的泛化能力。事实上，用于描述测试集误差和交叉验证误差的术语是“泛化误差”。这意味着在未用于训练的样本上的误差率。再次强调，机器学习的主要目标是构建具有低泛化误差率的模型。
- en: 'In *Chapter 3*, *Deep Learning with Keras*, we discussed two very important
    issues with machine learning models: overfitting and underfitting. We stated that
    underfitting is the scenario where the estimated model is not flexible or complex
    enough to capture all the relations and patterns associated with the true process.
    This is a model with `high bias` and is detected when the training error is high.
    On the other hand, overfitting is the scenario where the model that''s used for
    estimating the true process is too flexible or complex. This is a model with `high
    variance` and is diagnosed when there is a large gap between the training error
    and the generalization error. An overview of these scenarios for a binary classification
    problem can be seen in the following images:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*《使用 Keras 进行深度学习》*中，我们讨论了机器学习模型的两个非常重要的问题：过拟合和欠拟合。我们指出，欠拟合是指估计的模型不够灵活或复杂，无法捕捉到与真实过程相关的所有关系和模式。这是一个`高偏差`的模型，并且在训练误差较高时会被发现。另一方面，过拟合是指用于估计真实过程的模型过于灵活或复杂。这是一个`高方差`的模型，并且当训练误差和泛化误差之间存在较大差距时被诊断出来。以下图像概述了二分类问题中的这些情境：
- en: '![Figure 5.1: Underfitting'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：欠拟合'
- en: '](img/B15777_05_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_01.jpg)'
- en: 'Figure 5.1: Underfitting'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：欠拟合
- en: As you can see above, Underfitting is a less problematic issue than overfitting.
    In fact, underfitting can be fixed easily by making the model more flexible/complex.
    In deep neural networks, this means changing the architecture of the network,
    making the network larger by adding more layers to it or increasing the number
    of units in the layers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，欠拟合比过拟合是一个较少有问题的情况。事实上，欠拟合可以通过让模型变得更加灵活/复杂来轻松修复。在深度神经网络中，这意味着改变网络的架构，通过添加更多层或增加层中的单元数来使网络变大。
- en: 'Now let''s look at overfitting image below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看下面的过拟合图像：
- en: '![Figure 5.2: Overfitting'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：过拟合'
- en: '](img/B15777_03_16.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_03_16.jpg)'
- en: 'Figure 5.2: Overfitting'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：过拟合
- en: Similarly, there are simple solutions for addressing overfitting, such as making
    the model less flexible/complex (again, by changing the architecture of the network)
    or providing the network with more training examples. However, making the network
    less complex sometimes comes at the cost of a dramatic increase in bias or training
    error rate. The reason for this is that most of the time, the cause of overfitting
    is not the flexibility of the model but too few training examples. On the other
    hand, providing more data examples in order to decrease overfitting is not always
    possible. As a result, finding ways to reduce the generalization error while keeping
    model complexity and the number of training examples fixed is both important and
    challenging.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，解决过拟合有一些简单的方案，比如让模型变得不那么灵活/复杂（同样是通过改变网络的架构）或者提供更多的训练样本。然而，让网络变得不那么复杂有时会导致偏差或训练误差率的剧烈增加。原因在于，大多数情况下，过拟合的原因不是模型的灵活性，而是训练样本太少。另一方面，为了减少过拟合，提供更多的数据样本并非总是可能的。因此，找到在保持模型复杂度和训练样本数量不变的情况下减少泛化误差的方法，既重要又具有挑战性。
- en: 'Now let''s look at the Right fit image below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看下面的右侧拟合图像：
- en: '![Figure 5.3: Right Fit'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：右侧拟合'
- en: '](img/B15777_05_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_03.jpg)'
- en: 'Figure 5.3: Right Fit'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：右侧拟合
- en: That is why we need regularization techniques when building highly flexible
    machine learning models, such as deep neural networks, to suppress the flexibility
    of the model so that it cannot overfit to individual examples. In the next section,
    we will describe how regularization methods reduce the overfitting of models on
    the training data to reduce the variance in the model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在构建高度灵活的机器学习模型（如深度神经网络）时，我们需要正则化技术，以抑制模型的灵活性，避免其对单个样本进行过拟合。在接下来的章节中，我们将描述正则化方法如何减少模型在训练数据上的过拟合，从而降低模型的方差。
- en: Reducing Overfitting with Regularization
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过正则化减少过拟合
- en: '`Regularization` methods try to modify the learning algorithm in a way that
    reduces the variance of the model. By decreasing the variance, regularization
    techniques intend to reduce the generalization error while not increasing the
    training error (or, at least, not increasing the training error drastically).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`正则化`方法试图通过修改学习算法，减少模型的方差。通过减少方差，正则化技术旨在降低泛化误差，同时不会显著增加训练误差（或者至少不会大幅增加训练误差）。'
- en: Regularization methods provide some kind of restriction that helps with the
    stability of the model. There are several ways that this can be achieved. One
    of the most common ways of performing regularization on deep neural networks is
    by putting some type of penalizing term on weights to keep the weights small.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化方法提供了一种限制，有助于模型的稳定性。实现这一点有几种方式。对深度神经网络进行正则化的最常见方法之一是对权重施加某种惩罚项，以保持权重较小。
- en: Keeping the weights small makes the network less sensitive to noise in individual
    data examples. Weights in a neural network are, in fact, the coefficients that
    determine how big or small an effect each processing unit will have on the final
    output of the network. If the units have large weights, it means that each of
    them will have a significant influence on the output. Combining all the large
    influences that are caused by each processing unit will result in many fluctuations
    in the final output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 保持权重较小使得网络对个别数据样本中的噪声不那么敏感。在神经网络中，权重实际上是决定每个处理单元对网络最终输出影响大小的系数。如果单元的权重大，这意味着每个单元对输出的影响都会很大。将所有处理单元产生的巨大影响结合起来，最终输出将会有很多波动。
- en: 'On the other hand, keeping the weights small reduces the amount of influence
    each unit will have on the final output. Indeed, by keeping the weights near zero,
    some of the units will have almost no effect on the output. Training a large neural
    network where each unit has little or no effect on the output is the equivalent
    of training a much simpler network, and so variance and overfitting are reduced.
    The following figure shows the schematic view of how regularization zeroes out
    the effect of some units in a large network:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，保持权重较小减少了每个单元对最终输出的影响。实际上，通过将权重保持接近零，某些单元将几乎对输出没有影响。训练一个大型神经网络，其中每个单元对输出的影响微乎其微，相当于训练一个更简单的网络，从而减少了方差和过拟合。下图展示了正则化如何使大网络中某些单元的影响为零的示意图：
- en: '![Figure 5.4: Schematic view of how regularization zeroes out the effect of
    some units in a large network'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：正则化如何使大网络中某些单元的影响为零的示意图'
- en: '](img/B15777_05_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_04.jpg)'
- en: 'Figure 5.4: Schematic view of how regularization zeroes out the effect of some
    units in a large network'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：正则化如何使大网络中某些单元的影响为零的示意图
- en: The preceding diagram is a schematic view of the regularization process. The
    top network shows a network without regularization, while the bottom network shows
    an example of a network with regularization in which the white units represent
    units that have little to no effect on the output because they have been penalized
    by the regularization process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图是正则化过程的示意图。顶部的网络展示了没有正则化的网络，而底部的网络则展示了一个应用了正则化的网络示例，其中白色单元代表那些由于正则化过程的惩罚几乎对输出没有影响的单元。
- en: So far, we have learned about the concepts behind regularization. In the next
    section, we will look at the most common methods of regularization for deep learning
    models—`L1`, `L2`, and dropout regularization—along with how to implement them
    in Keras.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了正则化背后的概念。在接下来的章节中，我们将了解深度学习模型最常见的正则化方法——`L1`、`L2` 和 dropout 正则化，以及如何在
    Keras 中实现它们。
- en: L1 and L2 Regularization
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化
- en: 'The most common type of regularization for deep learning models is the one
    that keeps the weights of the network small. This type of regularization is called
    **weight regularization** and has two different variations: **L2 regularization**
    and **L1 regularization**. In this section, you will learn about these regularization
    methods in detail, along with how to implement them in Keras. Additionally, you
    will practice applying them to real-life problems and observe how they can improve
    the performance of a model.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型中最常见的正则化类型是保持网络权重较小的正则化。这种正则化被称为**权重正则化**，并且有两种不同的变体：**L2正则化**和**L1正则化**。在本节中，您将详细了解这些正则化方法，并学习如何在Keras中实现它们。此外，您还将练习将它们应用于现实问题，并观察它们如何提高模型的性能。
- en: L1 and L2 Regularization Formulation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1和L2正则化公式
- en: In weight regularization, a penalizing term is added to the loss function. This
    term is either the `L1 regularization`. If the L2 norm is used, then it will be
    called `L2 regularization`. In each case, the sum is multiplied by a hyperparameter
    called a **regularization parameter** (**lambda**).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重正则化中，一个惩罚项被添加到损失函数中。这个项通常是`L1正则化`。如果使用L2范数，则称之为`L2正则化`。在每种情况下，这个和将乘以一个超参数，称为**正则化参数**（**lambda**）。
- en: 'Therefore, for `L1 regularization`, the formula is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于`L1正则化`，公式如下：
- en: '*Loss function = Old loss function + lambda * sum of absolute values of the
    weights*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数 = 旧损失函数 + lambda * 权重的绝对值和*'
- en: 'And for `L2 regularization`, the formula is as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`L2正则化`，公式如下：
- en: '*Loss function = Old loss function + lambda * sum of squared values of the
    weights*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数 = 旧损失函数 + lambda * 权重的平方和*'
- en: '`Lambda` can take any value between `0` and `1`, where `lambda=0` means no
    penalty at all (equivalent to a network with no regularization) and `lambda=1`
    means full penalty.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`Lambda`可以取任意值，范围从`0`到`1`，其中`lambda=0`意味着没有惩罚（相当于一个没有正则化的网络），`lambda=1`意味着完全惩罚。'
- en: Like every other hyperparameter, the right value for `lambda` can be selected
    by trying out different values and observing which value provides a lower generalization
    error. In fact, it's good practice to start with a network with no regularization
    and observe the results. Then, you should perform regularization with increasing
    values of lambda, such as `0.001`, `0.01`, `0.1`, `0.5`, …, and observe the results
    in each case in order to figure out how much penalizing on the weight's values
    is suitable for a particular problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 和其他超参数一样，`lambda`的正确值可以通过尝试不同的值并观察哪个值提供较低的泛化误差来选择。实际上，最好从没有正则化的网络开始，并观察结果。然后，您应该使用逐渐增大的`lambda`值进行正则化，如`0.001`、`0.01`、`0.1`、`0.5`，并观察每种情况下的结果，以找出对特定问题来说，惩罚权重值的合适程度。
- en: In each iteration of the optimization algorithm with regularization, the weights
    (w) become smaller and smaller. That is why weight regularization is commonly
    referred to as **weight decay**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次带有正则化的优化算法迭代中，权重（w）会变得越来越小。因此，权重正则化通常被称为**权重衰减**。
- en: So far, we have only discussed regularizing weights in a deep neural network.
    However, you need to keep in mind that the same procedure can be applied to biases
    as well. More precisely, we can update the loss function again by adding a bias
    penalizing term to it as well and therefore keep the values of biases small during
    the training of a neural network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅讨论了在深度神经网络中正则化权重。然而，您需要记住，同样的过程也可以应用于偏置。更准确地说，我们可以通过向损失函数中添加一个惩罚偏置的项来更新损失函数，从而在神经网络训练过程中保持偏置的值较小。
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you perform regularization by adding two terms to the loss function (one
    for penalizing weights and one for penalizing biases), then we call it **parameter
    regularization** instead of weight regularization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过向损失函数添加两个项（一个惩罚权重，一个惩罚偏置）来执行正则化，那么我们称之为**参数正则化**，而不是权重正则化。
- en: However, regularizing bias values is not very common in deep learning. The reason
    for this is that weights are much more important parameters of neural networks.
    In fact, usually, adding another term to regularize biases will not change the
    results dramatically in comparison to only regularizing the weight values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深度学习中，正则化偏置值并不常见。原因在于，权重是神经网络中更为重要的参数。事实上，通常，添加另一个项来正则化偏置与仅正则化权重值相比，不会显著改变结果。
- en: '`L2 regularization` is the most common regularization technique that''s used
    in machine learning in general. The difference between `L1 regularization` and
    `L2 regularization` is that `L1` results in a sparser weights matrix, meaning
    there are more weights equal to zero, and therefore more nodes that are completely
    removed from the network. `L2 regularization`, on the other hand, is more subtle.
    It decreases the weights drastically, but at the same time leaves you with fewer
    weights equal to 0\. It is also possible to perform both `L1` and `L2 regularization`
    at the same time.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`L2正则化`是一般机器学习中最常见的正则化技术。与`L1正则化`的不同之处在于，`L1`会导致更稀疏的权重矩阵，意味着有更多的权重等于零，因此有更多的节点完全从网络中移除。另一方面，`L2正则化`则更为微妙。它会显著减少权重，但同时让你留下更少的权重等于零。同时进行`L1`和`L2正则化`也是可能的。'
- en: Now that you have learned about how `L1` and `L2 regularization` work, you are
    ready to move on to implementing `L1` and `L2 regularization` on deep neural networks
    in Keras.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了`L1`和`L2正则化`的工作原理，可以继续在Keras中实现深度神经网络上的`L1`和`L2正则化`了。
- en: L1 and L2 Regularization Implementation in Keras
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 中的 L1 和 L2 正则化实现
- en: Keras provides a regularization API that can be used to add penalizing terms
    to the loss function in order to regularize weights or biases in each layer of
    a deep neural network. To define the penalty term or `regularizer`, you need to
    define the desired regularization method under `keras.regularizers`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一个正则化API，可以用来在每一层的深度神经网络中向损失函数添加惩罚项，以对权重或偏置进行正则化。要定义惩罚项或`正则化器`，你需要在`keras.regularizers`下定义所需的正则化方法。
- en: 'For example, to define an `L1 regularizer` with `lambda=0.01`, you can write
    this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要定义一个`L1正则化器`，使用`lambda=0.01`，你可以这样写：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Similarly, to define an `L2 regularizer` with `lambda=0.01`, you can write
    this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，要定义一个`L2正则化器`，使用`lambda=0.01`，你可以这样写：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, to define both `L1` and `L2 regularizers` with `lambda=0.01`, you
    can write this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要同时定义`L1`和`L2正则化器`，使用`lambda=0.01`，你可以这样写：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Each of these `regularizers` can be applied to weights and/or biases in a layer.
    For example, if we would like to apply `L2 regularization` (with `lambda=0.01`)
    on both the weights and biases of a dense layer with eight nodes, we can write
    this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`正则化器`可以应用于层中的权重和/或偏置。例如，如果我们想在具有八个节点的密集层上同时应用`L2正则化`（使用`lambda=0.01`）到权重和偏置上，我们可以这样写：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will practice implementing `L1` and `L2 regularization` further in *Activity
    5.01*, *Weight Regularization on an Avila Pattern Classifier*, in which you will
    apply regularization on the deep learning model for the diabetes dataset and observe
    how the results change in comparison to previous activities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*活动5.01*中进一步实践实现`L1`和`L2正则化`，*阿维拉模式分类器的权重正则化*，你将为糖尿病数据集的深度学习模型应用正则化，并观察结果与先前活动的比较。
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities in this chapter will be developed in Jupyter Notebooks. Please
    download this book's GitHub repository, along with all the prepared templates,
    from [https://packt.live/2OOBjqq](https://packt.live/2OOBjqq).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有活动将在Jupyter笔记本中开发。请从[https://packt.live/2OOBjqq](https://packt.live/2OOBjqq)下载本书的GitHub存储库以及所有准备好的模板。
- en: 'Activity 5.01: Weight Regularization on an Avila Pattern Classifier'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.01：阿维拉模式分类器的权重正则化
- en: The Avila dataset has been extracted from `800` images of the `Avila Bible`,
    a giant 12th-century Latin copy of the Bible. The dataset consists of various
    features about the images of the text, such as intercolumnar distance and the
    margins of the text. The dataset also contains a class label that indicates if
    a pattern of the image falls into the most frequently occurring category or not.
    In this activity, you will build a Keras model to perform classification on this
    dataset according to given network architecture and hyperparameter values. The
    goal is to apply different types of weight regularization on the model and observe
    how each type changes the result.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 阿维拉数据集是从`阿维拉圣经`的800幅图像中提取的，这是12世纪的拉丁文圣经的巨大复制品。该数据集包含有关文本图像的各种特征，例如列间距和文本的边距。数据集还包含一个类标签，指示图像模式是否属于最频繁出现的类别。在这个活动中，你将构建一个Keras模型，根据给定的网络架构和超参数值对该数据集进行分类。目标是在模型上应用不同类型的权重正则化，并观察每种类型如何改变结果。
- en: In this activity, we will use the `training set`/`test set` approach to perform
    the evaluation for two reasons. First, since we are going to try several different
    regularizers, performing cross-validation will take a long time. Second, we would
    like to plot the trends in the training error and the test error in order to understand,
    in a visual way, how regularization prevents the model from overfitting to data
    examples.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们将使用`训练集`/`测试集`方法进行评估，原因有两个。首先，由于我们将尝试几种不同的正则化器，执行交叉验证将需要很长时间。其次，我们希望绘制训练误差和测试误差的趋势图，以便通过视觉方式理解正则化如何防止模型对数据样本过拟合。
- en: 'Follow these steps to complete this activity:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成此活动：
- en: Load the dataset from the `data` subfolder of `Chapter05` from GitHub using
    `X = pd.read_csv('../data/avila-tr_feats.csv')` and `y = pd.read_csv('../data/avila-tr_target.csv')`.
    Split the dataset into a training set and a test set using the `sklearn.model_selection.train_test_split`
    method. Hold back `20%` of the data examples for the test set.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从GitHub的`Chapter05`文件夹中的`data`子文件夹加载数据集，使用`X = pd.read_csv('../data/avila-tr_feats.csv')`和`y
    = pd.read_csv('../data/avila-tr_target.csv')`。使用`sklearn.model_selection.train_test_split`方法将数据集拆分为训练集和测试集。保留`20%`的数据样本作为测试集。
- en: 'Define a Keras model with three hidden layers, the first of `size 10`, the
    second of `size 6`, and the third of `size 4`, to perform the classification.
    Use these values for the hyperparameters: `activation=''relu''`, `loss=''binary_crossentropy''`,
    `optimizer=''sgd''`, `metrics=[''accuracy'']`, `batch_size=20`, `epochs=100`,
    and `shuffle=False`.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Keras模型，包含三个隐藏层，第一个隐藏层的`大小为10`，第二个隐藏层的`大小为6`，第三个隐藏层的`大小为4`，用于执行分类任务。使用以下超参数：`activation='relu'`，`loss='binary_crossentropy'`，`optimizer='sgd'`，`metrics=['accuracy']`，`batch_size=20`，`epochs=100`，`shuffle=False`。
- en: Train the model on the training set and evaluate it with the test set. Store
    the training loss and test loss at every iteration. After training is complete,
    plot the trends in `training error` and `test error` (change the limits of the
    vertical axis to (`0, 1`) so that you can observe the changes in losses better).
    What is the minimum error rate on the test set?
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练模型，并使用测试集进行评估。在每次迭代中存储训练损失和测试损失。训练完成后，绘制`训练误差`和`测试误差`的趋势图（将纵轴的限制调整为（`0,
    1`），这样可以更好地观察损失的变化）。在测试集上的最小误差率是多少？
- en: Add `L2 regularizers` with `lambda=0.01` to the hidden layers of your model
    and repeat the training. After training is complete, plot the trends in training
    error and test error. What is the minimum error rate on the test set?
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型的隐藏层添加`L2正则化器`，其中`lambda=0.01`，并重复训练。训练完成后，绘制训练误差和测试误差的趋势图。在测试集上的最小误差率是多少？
- en: Repeat the previous step for `lambda=0.1` and `lambda=0.005`, train the model
    for each value of `lambda`, and report the results. Which value of `lambda` is
    a better choice for performing `L2 regularization` on this deep learning model
    and this dataset?
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`lambda=0.1`和`lambda=0.005`重复前面的步骤，针对每个`lambda`值训练模型，并报告结果。哪个`lambda`值对于在该深度学习模型和数据集上执行`L2正则化`更为合适？
- en: Repeat the previous step, this time with `L1 regularizers` for `lambda=0.01`
    and `lambda=0.005`, train the model for each value of `lambda`, and report the
    results. Which value of `lambda` is a better choice for performing `L1 regularization`
    on this deep learning model and this dataset?
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，这次使用`L1正则化器`，针对`lambda=0.01`和`lambda=0.005`训练模型，并报告结果。哪个`lambda`值对于在该深度学习模型和数据集上执行`L1正则化`更为合适？
- en: Add `L1_L2 regularizers` with the `L1 lambda=0.005` and the `L2 lambda=0.005`
    to the hidden layers of your model and repeat the training. After training is
    complete, plot the trends in training error and test error. What is the minimum
    error rate on the test set?
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型的隐藏层添加`L1_L2正则化器`，其中`L1 lambda=0.005`，`L2 lambda=0.005`，并重复训练。训练完成后，绘制训练误差和测试误差的趋势图。在测试集上的最小误差率是多少？
- en: 'After implementing these steps, you should get the following expected output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你应该得到以下预期输出：
- en: '![Figure 5.5: A plot of the training error and validation error during training
    for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5：模型在训练过程中，L1 lambda为0.005，L2 lambda为0.005时的训练误差和验证误差的趋势图]'
- en: '](img/B15777_05_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_05.jpg)'
- en: 'Figure 5.5: A plot of the training error and validation error during training
    for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：模型在训练过程中，L1 lambda为0.005，L2 lambda为0.005时的训练误差和验证误差的趋势图
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: The solution for this activity can be found on page 398.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第398页找到。
- en: In this activity, you practiced implementing `L1` and `L2 weight regularizations`
    for a real-life problem and compared the results of the regularized model with
    those of a model without any regularization. In the next section, we will explore
    the regulation of a different technique, known as dropout regularization.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你实践了为实际问题实现`L1`和`L2`权重正则化，并将正则化模型的结果与没有任何正则化的模型进行比较。在下一节中，我们将探讨另一种技术的正则化方法，称为dropout正则化。
- en: Dropout Regularization
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout正则化
- en: In this section, you will learn how dropout regularization works, how it helps
    with reducing overfitting, and how to implement it using Keras. Lastly, you will
    practice what you have learned about dropout by completing an activity involving
    a real-life dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习dropout正则化的工作原理，它如何帮助减少过拟合，以及如何使用Keras实现它。最后，你将通过完成一个涉及实际数据集的活动，来实践你学到的关于dropout的知识。
- en: Principles of Dropout Regularization
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout正则化的原理
- en: 'Dropout regularization works by randomly removing nodes from a neural network
    during training. More precisely, dropout sets up a probability on each node. This
    probability refers to the chance that the node is included in the training at
    each iteration of the learning algorithm. Imagine we have a large neural network
    where a dropout chance of `0.5` is assigned to each node. In such a case, at each
    iteration, the learning algorithm flips a coin for each node to decide whether
    that node will be removed from the network or not. An illustration of such a process
    can be seen in the following diagram:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout正则化通过在训练过程中随机移除神经网络中的节点来工作。更准确地说，dropout在每个节点上设置一个概率。这个概率表示在每次学习算法的迭代中，该节点被包含在训练中的可能性。假设我们有一个大型神经网络，其中每个节点的dropout概率为`0.5`。在这种情况下，在每次迭代中，学习算法会为每个节点掷一次硬币，决定是否将该节点从网络中移除。以下图示展示了这一过程：
- en: '![Figure 5.6: Illustration of removing nodes from a deep neural network using
    dropout regularization'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6：使用dropout正则化从深度神经网络中移除节点的示意图'
- en: '](img/B15777_05_06.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_06.jpg)'
- en: 'Figure 5.6: Illustration of removing nodes from a deep neural network using
    dropout regularization'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：使用dropout正则化从深度神经网络中移除节点的示意图
- en: This process is repeated at each iteration; this means that, at each iteration,
    randomly selected nodes are removed from the network, which means the parameter-updating
    process is done on a different smaller network. For example, the network shown
    at the bottom of the preceding diagram would be used for one iteration of the
    training only. For the next iteration, some other randomly selected nodes would
    be crossed out from the top network so the network that results from removing
    those nodes would be different from the bottom network in the diagram.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在每次迭代时都会重复；这意味着，在每次迭代中，随机选择的节点会从网络中移除，这也意味着参数更新过程将在一个不同的较小网络上进行。例如，前图底部显示的网络只会用于一次训练迭代。对于下一次迭代，另一些随机选中的节点会从顶部网络中被删除，因此从这些节点中移除后的网络将与图中的底部网络不同。
- en: When some nodes are chosen to be removed/ignored in an iteration of a learning
    algorithm, it means that they won't participate in the parameter-updating process
    at all in that iteration. More precisely, the forward propagation to predict the
    output, the loss computation, and the backpropagation to compute the derivatives
    are all to be done on the smaller network with some nodes removed. Consequently,
    parameter updating will only be done on the nodes that are present in the network
    in that iteration; the weights and biases of removed nodes won't be updated.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当某些节点在学习算法的某次迭代中被选择移除/忽略时，这意味着它们在该次迭代中的参数更新过程中完全不参与。更准确地说，前向传播以预测输出、损失计算和反向传播以计算导数的所有操作都将在移除一些节点的较小网络上进行。因此，参数更新将仅在该次迭代中存在于网络中的节点上进行；被移除节点的权重和偏置将不会被更新。
- en: However, it is important to keep in mind that to evaluate the performance of
    the model on the test set or hold-out set, the original complete network is always
    used. If we perform the evaluation of a network with random nodes deleted from
    it, the noise will be introduced to the results, and this is not desirable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要牢记的是，在测试集或保留集上评估模型的性能时，始终使用原始的完整网络。如果我们用随机删除节点的网络进行评估，结果会引入噪声，这样是不可取的。
- en: In `dropout regularization`, `training` is always performed on the networks
    that result from randomly selected nodes being removed from the original network.
    `Evaluation` is always performed using the original network. In the next section,
    we will gain an understanding of why dropout regularization helps prevent overfitting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dropout 正则化`中，`训练`始终在通过随机移除原始网络中的部分节点所得到的网络上进行。`评估`则始终使用原始网络进行。在接下来的部分，我们将了解为什么
    dropout 正则化有助于防止过拟合。
- en: Reducing Overfitting with Dropout
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dropout 减少过拟合
- en: In this section, we are going to discuss the concepts behind dropout as a regularization
    method. As we discussed previously, the goal of regularization techniques is to
    prevent a model from overfitting data. Therefore, we are going to look at how
    randomly removing a portion of nodes from a neural network helps reduce variance
    and overfitting.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 dropout 作为正则化方法背后的概念。正如我们之前讨论的，正则化技术的目标是防止模型对数据过拟合。因此，我们将研究如何通过随机移除神经网络中的一部分节点来帮助减少方差和过拟合。
- en: The most obvious explanation of why removing random nodes from the network prevents
    overfitting is that by removing nodes from a network, we are performing training
    on a smaller network in comparison to the original network. As you learned previously,
    a smaller neural network provides less flexibility, so the chance of the network
    overfitting to data is lower.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 删除网络中的随机节点防止过拟合的最明显解释是，通过从网络中移除节点，我们是在对比原始网络训练一个更小的网络。如你之前所学，一个更小的神经网络提供的灵活性较低，因此网络对数据过拟合的可能性较小。
- en: There is another reason why dropout regularization does such a good job of reducing
    overfitting. By randomly removing inputs at each layer in a deep neural network,
    the overall network becomes less sensitive to single inputs. We know that, while
    training a neural network, the weights are updated in a way that the final model
    will fit to the training examples. By removing some of the weights from the training
    process at random, dropout forces other weights to participate in learning the
    patterns related to the training examples at that iteration, and so the final
    weight values will better spread out more.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个原因，解释了为什么 dropout 正则化如此有效地减少过拟合。通过在深度神经网络的每一层随机移除输入，整个网络变得对单一输入不那么敏感。我们知道，在训练神经网络时，权重会以某种方式更新，使得最终的模型能够适应训练样本。通过随机移除一些权重参与训练过程，dropout
    强制其他权重参与学习与训练样本相关的模式，因此最终的权重值会更好地分散。
- en: In other words, instead of some weights updating too much in order to fit some
    input values, all the weights learn to participate in learning those input values
    and, consequently, overfitting decreases. This is why performing dropout results
    in a much more robust model—performing better on new, unseen data—in comparison
    to simply using a smaller network. In fact, `dropout regularization` tends to
    work better on larger networks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，不是某些权重为了拟合某些输入值而过度更新，而是所有权重都参与学习这些输入值，从而导致过拟合减少。这就是为什么执行 dropout 比仅仅使用更小的网络更能产生一个更强大的模型——在新数据上表现更好。实际上，`dropout
    正则化`在更大的网络上效果更佳。
- en: Now that you have learned all about the underlying procedure of dropout and
    the reasons behind its effectiveness, we can move on to implementing `dropout
    regularization` in Keras.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 dropout 的基本过程及其有效性背后的原因，我们可以继续在 Keras 中实现`dropout 正则化`。
- en: 'Exercise 5.01: Dropout Implementation in Keras'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.01：在 Keras 中实现 Dropout
- en: '`Dropout regularization` is provided as a core layer in Keras. As such, you
    can add dropout to your model in the same way that you would add layers to your
    network. When defining a dropout layer in Keras, you need to provide the `rate`
    hyperparameter as an argument. `rate` can take any value between `0` and `1` and
    determines the portions of the input units to be removed or ignored. In this exercise,
    you will learn the step-by-step process of implementing a Keras deep learning
    model with dropout layers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout 正则化`作为 Keras 中的核心层提供。因此，你可以像添加其他层到网络一样，将 dropout 添加到模型中。在 Keras 中定义
    dropout 层时，你需要提供 `rate` 超参数作为参数。`rate` 可以是一个介于 `0` 和 `1` 之间的值，决定了要移除或忽略的输入单元的比例。在这个练习中，你将学习如何一步步实现带有
    dropout 层的 Keras 深度学习模型。'
- en: 'Our simulated dataset represents various measurements of trees, such as height,
    the number of branches, and the girth of the trunk at the base. Our goal is to
    classify the records into either deciduous (a class value of `1`) or coniferous
    (a class value of `0`) type trees based on the measurements given. The dataset
    consists of `10000` records that consist of two classes, representing the two
    tree types, and each data example has `10` feature values. Follow these steps
    to complete this exercise:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模拟数据集表示了树木的各种测量数据，如树高、树枝数和树干底部的胸围。我们的目标是根据给定的测量值将记录分类为落叶树（类值为`1`）或针叶树（类值为`0`）。数据集包含`10000`条记录，代表两种树种的两个类别，每个数据实例有`10`个特征值。请按照以下步骤完成此练习：
- en: 'First, execute the following code block to load in the dataset and split the
    dataset into a `training set` and a `test set`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，执行以下代码块以加载数据集，并将数据集拆分为`训练集`和`测试集`：
- en: '[PRE4]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Import all the necessary dependencies. Build a four-layer Keras sequential
    model without `dropout regularization`. Build the network with 16 units in the
    first hidden layer, `12` units in the second hidden layer, `8` units in the third
    hidden layer, and `4` units in the fourth hidden layer, all with `ReLU activation`
    functions. Add an output layer with a `sigmoid activation` function:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的依赖项。构建一个四层的 Keras 顺序模型，且不使用`dropout 正则化`。构建该网络时，第一隐藏层有 16 个单元，第二隐藏层有`12`个单元，第三隐藏层有`8`个单元，第四隐藏层有`4`个单元，所有层都使用`ReLU
    激活`函数。添加一个具有`sigmoid 激活`函数的输出层：
- en: '[PRE5]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Compile the model with `binary cross-entropy` as the `loss` function and `sgd`
    as the optimizer and train the model for `300` epochs with `batch_size=50` on
    the `training set`. Then, evaluate the trained model on the `test set`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`binary cross-entropy`作为`loss`函数，`sgd`作为优化器，编译模型，并在`训练集`上以`batch_size=50`进行`300`个
    epochs 的训练。然后，在`测试集`上评估训练好的模型：
- en: '[PRE6]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here''s the expected output:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '[PRE7]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Therefore, the test error rate for predicting the species of tree after training
    the model for `300` epochs is equal to `16.98%`.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，经过`300`个 epochs 训练后的树种预测测试误差率为`16.98%`。
- en: 'Redefine the model with the same number of layers and same size in each layer
    as the prior model. However, add a `dropout regularization` of `rate=0.1` to the
    first hidden layer of your model and repeat the compilation, training, and evaluation
    steps of the model on the test data:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与之前模型相同的层数和每层相同大小重新定义模型。然而，在模型的第一隐藏层添加`rate=0.1`的`dropout 正则化`，然后重复编译、训练和在测试数据上评估模型的步骤：
- en: '[PRE8]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here''s the expected output:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '[PRE9]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After adding a dropout regularization of `rate=0.1` to the first layer of the
    network, the test error rate is reduced from `16.98%` to `16.89%`.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在网络的第一层添加`rate=0.1`的 dropout 正则化后，测试误差率从`16.98%`降低到了`16.89%`。
- en: 'Redefine the model with the same number of layers and the same size in each
    layer as the prior model. However, add a dropout regularization of `rate=0.2`
    to the first hidden layer and `rate=0.1` to the remaining layers of your model
    and repeat the compilation, training, and evaluation steps of the model on the
    test data:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与之前模型相同的层数和每层相同大小重新定义模型。然而，在第一隐藏层添加`rate=0.2`的 dropout 正则化，并在模型的其余层添加`rate=0.1`的
    dropout 正则化，然后重复编译、训练和在测试数据上评估模型的步骤：
- en: '[PRE10]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here''s the expected output:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '[PRE11]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: By keeping the dropout regularization of `rate=0.2` in the first layer while
    adding dropout regularizations of `rate=0.1` to the subsequent layers, the test
    error rate increased from `16.89%` to `19.39%`. Like the `L1` and `L2 regularizations`,
    adding too much dropout can prevent the model from learning the underlying function
    associated with the training data and leads to higher bias than without dropout
    regularization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层保持`rate=0.2`的 dropout 正则化的同时，添加`rate=0.1`的 dropout 正则化到后续层，测试误差率从`16.89%`上升到`19.39%`。像`L1`和`L2
    正则化`一样，添加过多的 dropout 会阻止模型学习与训练数据相关的潜在函数，从而导致比没有 dropout 正则化时更高的偏差。
- en: As you saw in this exercise, you can also apply dropout with different rates
    to the different layers depending on how much overfitting you think can happen
    in those layers. Usually, we prefer not to perform dropout on the input layer
    and the output layer. Regarding the hidden layers, we need to tune the `rate`
    values and observe the results in order to decide what value is best suited to
    a particular problem.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在本次练习中看到的，你还可以根据认为在各个层上可能发生的过拟合情况，对不同层使用不同的 dropout 比例。通常，我们不建议在输入层和输出层使用
    dropout。对于隐藏层，我们需要调整 `rate` 值，并观察结果，以便确定哪个值最适合特定问题。
- en: Note
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3iugM7K](https://packt.live/3iugM7K).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此特定部分的源代码，请参考[https://packt.live/3iugM7K](https://packt.live/3iugM7K)。
- en: You can also run this example online at [https://packt.live/31HlSYo](https://packt.live/31HlSYo).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是[https://packt.live/31HlSYo](https://packt.live/31HlSYo)。
- en: In the following activity, you will practice implementing deep learning models
    along with dropout regularization in Keras on the Traffic Volume dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的活动中，你将练习在 Keras 中实现深度学习模型，并结合 Dropout 正则化来处理交通流量数据集。
- en: 'Activity 5.02: Dropout Regularization on the Traffic Volume Dataset'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.02：在交通流量数据集上应用 Dropout 正则化
- en: In *Activity 4.03*, *Model Selection Using Cross-Validation on a Traffic Volume
    Dataset*, of *Chapter 4*, *Evaluating Your Model with Cross-Validation Using Keras
    Wrappers*, you used the Traffic Volume dataset to build a model for predicting
    the volume of traffic across a city bridge when given various normalized features
    related to traffic data such as the time of day and the volume on the previous
    day, among others. The dataset contains `10000` records and for each of them,
    `10` attributes/features are included in the dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *活动 4.03*，*在交通流量数据集上使用交叉验证进行模型选择*，*第 4 章*，*使用 Keras 包装器评估模型并进行交叉验证* 中，你使用交通流量数据集构建了一个模型，预测给定一系列与交通数据相关的标准化特征（如一天中的时间和前一天的交通量等）时城市桥梁上的交通流量。该数据集包含
    `10000` 条记录，每条记录包含 `10` 个特征。
- en: 'In this activity, you will start with the model from *Activity 4.03*, *Model
    Selection Using Cross-Validation on a Traffic Volume Dataset*, of *Chapter 4*,
    *Evaluating Your Model with Cross-Validation Using Keras Wrappers*. You will use
    the training set/test set approach to train and evaluate the model, plot the trends
    in training error and the generalization error, and observe the model overfitting
    data examples. Then, you will attempt to improve model performance by addressing
    the overfitting issue through the use of dropout regularization. In particular,
    you will try to find out which layers you should add dropout regularization to
    and what `rate` value will improve this specific model the most. Follow these
    steps to complete this activity:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，你将从 *活动 4.03*，*在交通流量数据集上使用交叉验证进行模型选择*，*第 4 章*，*使用 Keras 包装器评估模型并进行交叉验证*
    开始。你将使用训练集/测试集方法来训练和评估模型，绘制训练误差和泛化误差的趋势，并观察模型的过拟合情况。接下来，你将尝试通过使用 dropout 正则化来解决过拟合问题，从而提升模型性能。具体来说，你将尝试找出应该在哪些层添加
    dropout 正则化，并找到最佳的 `rate` 值来最大程度地改进该模型。完成此活动的步骤如下：
- en: Load the dataset using the pandas `read_csv` function. The dataset is also stored
    in the `data` subfolder of the *Chapter05* GitHub repository. Split the dataset
    into a training set and a test set with an `80-20` ratio.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `read_csv` 函数加载数据集。数据集也存储在 *Chapter05* GitHub 仓库的 `data` 子文件夹中。将数据集按
    `80-20` 比例分割为训练集和测试集。
- en: 'Define a Keras model with two hidden layers of `size` `10` to predict the traffic
    volume. Use these values for the following hyperparameters: `activation=''relu''`,
    `loss=''mean_squared_error''`, `optimizer=''rmsprop''`, `batch_size=50`, `epochs=200`,
    and `shuffle=False`.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 Keras 模型，包含两个隐藏层，每个隐藏层的`size`为`10`，用于预测交通流量。使用以下超参数：`activation='relu'`，`loss='mean_squared_error'`，`optimizer='rmsprop'`，`batch_size=50`，`epochs=200`，`shuffle=False`。
- en: Train the model on the training set and evaluate on the test set. Store the
    training loss and test loss at every iteration.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练模型，并在测试集上评估。在每次迭代时存储训练损失和测试损失。
- en: After training is completed, plot the trends in training error and test error.
    What are the lowest error rates on the training set and the test set?
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，绘制训练误差和测试误差的趋势。训练集和测试集的最低误差率是多少？
- en: Add dropout regularization with `rate=0.1` to the first hidden layer of your
    model and repeat the training process (since training with dropout takes longer,
    train for `200` epochs). After training is completed, plot the trends in training
    error and test error. What are the lowest error rates on the training set and
    the test set?
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向你的模型的第一个隐藏层添加`rate=0.1`的dropout正则化，并重复训练过程（由于训练时使用了dropout，训练时间较长，请训练`200`个epoch）。训练完成后，绘制训练误差和测试误差的趋势。训练集和测试集上的最低误差率分别是多少？
- en: Repeat the previous step, this time adding dropout regularization with `rate=0.1`
    to both hidden layers of your model and train the model and report the results.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复之前的步骤，这次向你的模型的两个隐藏层添加`rate=0.1`的dropout正则化，并训练模型并报告结果。
- en: Repeat the previous step, this time with `rate=0.2` on the first layer and `0.1`
    on the second layer and train the model and report the results.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复之前的步骤，这次在第一个隐藏层使用`rate=0.2`，在第二个隐藏层使用`0.1`，训练模型并报告结果。
- en: Which dropout regularization has resulted in the best performance on this deep
    learning model and this dataset so far?
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，哪种dropout正则化方法在这个深度学习模型和数据集上取得了最佳性能？
- en: 'After implementing these steps, you should get the following expected output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些步骤后，你应该得到以下预期的输出：
- en: '![Figure 5.7: A plot of training errors and validation errors while training
    the model with dropout regularization, with rate=0.2 in the first layer and rate=0.1
    in the second layer'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.7：训练过程中使用dropout正则化的训练误差和验证误差图，第一个隐藏层的rate=0.2，第二个隐藏层的rate=0.1'
- en: '](img/B15777_05_07.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_07.jpg)'
- en: 'Figure 5.7: A plot of training errors and validation errors while training
    the model with dropout regularization, with rate=0.2 in the first layer and rate=0.1
    in the second layer'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：训练过程中使用dropout正则化的训练误差和验证误差图，第一个隐藏层的rate=0.2，第二个隐藏层的rate=0.1
- en: Note
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 413.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第413页找到。
- en: In this activity, you learned how to implement dropout regularization in Keras
    and practiced using it on a problem involving the Traffic Volume dataset. `Dropout
    regularization` is specifically designed for the purpose of reducing overfitting
    in neural networks and works by randomly removing nodes from a neural network
    during the training process. This procedure results in a neural network with well
    spread out weight values, which leads to less overfitting in individual data examples.
    In the next section, we will discuss other regularization methods that can be
    applied to prevent a model overfitting the training data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你学习了如何在Keras中实现dropout正则化，并在涉及交通流量数据集的问题中进行了实践。`Dropout正则化`专门用于减少神经网络中的过拟合，其原理是通过在训练过程中随机去除神经网络中的节点。这个过程导致神经网络的权重值分布更加均匀，从而减少了单个数据样本的过拟合。接下来的章节中，我们将讨论其他可以应用于防止模型在训练数据上过拟合的正则化方法。
- en: Other Regularization Methods
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他正则化方法
- en: In this section, you will briefly learn about some other regularization techniques
    that are commonly used and have been shown to be effective in deep learning. It
    is important to keep in mind that regularization is a wide-ranging and active
    research field in machine learning. As a result, covering all the available regularization
    methods in one chapter is not possible (and most likely not necessary, especially
    in a book on applied deep learning). Therefore, in this section, we will briefly
    cover three more regularization methods, called **early stopping**, **data augmentation**,
    and **adding noise**. You will learn about their underlying ideas and gain a few
    tips and recommendations on how to use them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，你将简要了解一些常用的正则化技术，这些技术在深度学习中被证明是有效的。需要牢记的是，正则化是机器学习中一个广泛且活跃的研究领域。因此，在一章中涵盖所有可用的正则化方法是不可能的（而且大多数情况下并不必要，尤其是在一本关于应用深度学习的书中）。因此，在这一节中，我们将简要介绍另外三种正则化方法，分别是**提前停止**、**数据增强**和**添加噪声**。你将了解它们的基本原理，并获得一些如何使用它们的技巧和建议。
- en: Early Stopping
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止
- en: Earlier in this chapter, we discussed that the main assumption in machine learning
    is that there is a true function or process that produces training examples. However,
    this process is unknown and there is no explicit way to find it. Not only is there
    no way to find the exact underlying process but choosing a model with the right
    level of flexibility or complexity for estimating the process is challenging as
    well. Therefore, one good practice is to select a highly flexible model, such
    as a deep neural network, to model the process and monitor the training process
    carefully.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本章早些时候我们讨论了机器学习的主要假设是存在一个真实的函数或过程来生成训练样本。然而，这个过程是未知的，且没有明确的方法可以找到它。不仅找不到确切的底层过程，而且选择一个具有适当灵活性或复杂度的模型来估计这个过程也很具挑战性。因此，一种好的做法是选择一个高灵活度的模型，比如深度神经网络，来建模这个过程，并仔细监控训练过程。
- en: By monitoring the training process, we can train the model just enough for it
    to capture the form of the process, and we can stop the training right before
    it starts to overfit to individual data examples. This is the underlying idea
    behind early stopping. We discussed the idea of early stopping briefly in the
    *Model Evaluation* section of *Chapter 3*, *Deep Learning with Keras*. We stated
    that, by monitoring and observing the changes in `training error` and `test error`
    during training, we can determine how little training is too little and how much
    training is too much.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监控训练过程，我们可以在模型捕捉到过程的形式时及时停止训练，避免在模型开始对单个数据样本过拟合时继续训练。这就是早停背后的基本思想。我们在*第 3
    章*，*使用 Keras 的深度学习*的*模型评估*部分简要讨论了早停的概念。我们提到，通过监控和观察训练过程中 `训练误差` 和 `测试误差` 的变化，我们可以判断训练量过少或过多的界限。
- en: 'The following plot shows a view of the changes in training error and test error
    when a highly flexible model is trained on a dataset. As we can see, the training
    needs to stop in the region labeled **Right Fit** to avoid overfitting:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在训练高度灵活的模型时，训练误差和测试误差的变化情况。正如我们所见，训练需要在标记为**合适拟合**的区域停止，以避免过拟合：
- en: '![Figure 5.8: Plot of the training error and test error while training a model'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8：训练模型时训练误差和测试误差的变化图'
- en: '](img/B15777_05_08.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_08.jpg)'
- en: 'Figure 5.8: Plot of the training error and test error while training a model'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：训练模型时训练误差和测试误差的变化图
- en: 'In *Chapter 3*, *Deep Learning with Keras*, we practiced storing and plotting
    changes in training error and test error in order to identify overfitting. You
    learned that you can provide a validation set or test set when training a Keras
    model and store the metrics values for each of them at each epoch of training
    by using the following code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*，*使用 Keras 的深度学习*中，我们练习了存储和绘制训练误差和测试误差的变化，以识别过拟合。你学到，在训练 Keras 模型时，你可以提供验证集或测试集，并通过以下代码在每个训练周期中存储它们的指标值：
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this section, you are going to learn how to implement early stopping in Keras.
    This means forcing the Keras model to stop the training when a desired metric—for
    example, the `test error rate`—is not improving anymore. In order to do so, you
    need to define an `EarlyStopping()` callback and provide it as an argument to
    `model.fit()`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何在 Keras 中实现早停。这意味着在 Keras 模型训练时，当某个期望的指标——例如，`测试误差率`——不再改善时，强制停止训练。为此，你需要定义一个
    `EarlyStopping()` 回调函数，并将其作为参数提供给 `model.fit()`。
- en: When defining an `EarlyStopping()` callback, you need to provide it with the
    right arguments. The first argument is `monitor`, which determines what metric
    will be monitored during training for the purpose of performing early stopping.
    Usually, `monitor='val_loss'` is a good choice, meaning that we would like to
    monitor the test error rate.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义 `EarlyStopping()` 回调函数时，你需要为其提供正确的参数。第一个参数是 `monitor`，它决定了在训练过程中监控哪个指标来执行早停。通常，`monitor='val_loss'`
    是一个不错的选择，这意味着我们希望监控测试误差率。
- en: 'Also, depending on what argument you have chosen for the `monitor`, you need
    to set the `mode` argument to either `''min''` or `''max''`. If the metric is
    error/loss, we would like to minimize it. For example, the following code block
    defines an `EarlyStopping()` callback that monitors the test error during training
    and detects if it is not decreasing anymore:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据你为 `monitor` 选择的参数，你需要将 `mode` 参数设置为 `'min'` 或 `'max'`。如果指标是误差/损失，我们希望将其最小化。例如，以下代码块定义了一个
    `EarlyStopping()` 回调函数，用于在训练过程中监控测试误差，并检测其是否不再减少：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If there are a lot of fluctuations or noise in the error rates, it is probably
    not a good idea to stop the training when the loss begins to increase at all.
    For this reason, we can set the `patience` argument to a number of epochs to give
    the early stopping method some time to monitor the desired metric for longer before
    stopping the training process:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果误差率波动很大或噪声较多，那么在损失开始增加时就立即停止训练可能并不是一个好主意。因此，我们可以将`patience`参数设置为一定的epoch数量，给早停方法一些时间，在停止训练过程之前，能够监控目标度量值更长时间：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can also modify the `EarlyStopping()` callback to stop the training process
    if a minimal improvement in the `monitor` metric has not happened in the past
    `epoch`, or the `monitor` metric has reached a baseline level:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以修改`EarlyStopping()`回调函数，如果在过去的`epoch`内，`monitor`度量没有发生最小的改进，或者`monitor`度量已达到基准水平时，停止训练过程：
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After defining the `EarlyStopping()` callback, you can provide it as a `callbacks`
    argument to `model.fit()` and train the model. The training will automatically
    stop according to the `EarlyStopping()` callback:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了`EarlyStopping()`回调函数后，可以将其作为`callbacks`参数传递给`model.fit()`并训练模型。训练将根据`EarlyStopping()`回调函数自动停止：
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will explore how early stopping can be achieved in practice in the next exercise.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一个练习中探索如何在实际中实现早停。
- en: 'Exercise 5.02: Implementing Early Stopping in Keras'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习5.02：在Keras中实现早停
- en: In this exercise, you will learn how to implement early stopping on a Keras
    deep learning model. The dataset we will use is a simulated dataset that represents
    various measurements of trees, such as height, the number of branches and the
    girth of the trunk at the base. Our goal is to classify the records into either
    deciduous or coniferous trees based on the measurements given.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将学习如何在Keras深度学习模型中实现早停。我们将使用的数据集是一个模拟数据集，包含表示树木不同测量值的数据，例如树高、树枝数量和树干基部的周长。我们的目标是根据给定的测量值将记录分类为落叶树或针叶树。
- en: First, execute the following code block to load a simulated dataset of `10000`
    records that consist of two classes representing the two tree species, with a
    class value of `1` for deciduous tree species and a class value of `0` for coniferous
    tree species. Each record has `10` feature values.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，执行以下代码块以加载包含`10000`条记录的模拟数据集，这些记录包括两类，表示两种树种，其中落叶树的类值为`1`，针叶树的类值为`0`。每条记录有`10`个特征值。
- en: 'The goal is to build a model in order to predict the species of the tree when
    given the measurements of the tree. Now, let''s go through the steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是构建一个模型，以便在给定树木的测量值时预测树的种类。现在，让我们按照以下步骤操作：
- en: 'Load the dataset using the pandas `read_csv` function and split the dataset
    in an `80-20` split using the `train_test_split` function:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`read_csv`函数加载数据集，并使用`train_test_split`函数将数据集按`80-20`比例拆分：
- en: '[PRE17]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Import all the necessary dependencies. Build a three-layer Keras sequential
    model without early stopping. The first layer will have `16` units, the second
    layer will have `8` units, and the third layer will have `4` units, all with `ReLU
    activation` functions. Add the `output layer` with a `sigmoid activation function`:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的依赖项。构建一个没有早停的三层Keras顺序模型。第一层将有`16`个单元，第二层有`8`个单元，第三层有`4`个单元，所有层均使用`ReLU激活函数`。添加`输出层`并使用`sigmoid激活函数`：
- en: '[PRE18]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Compile the model with the `loss` function as binary cross-entropy and the
    optimizer as `SGD`. Train the model for `300` epochs with `batch_size=50`, all
    while storing the `training error` and the `test error` at every iteration:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二进制交叉熵作为`loss`函数，并将优化器设为`SGD`来编译模型。训练模型`300`个epoch，`batch_size=50`，同时在每次迭代时记录`训练误差`和`测试误差`：
- en: '[PRE19]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Import the required packages for plotting:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入绘图所需的包：
- en: '[PRE20]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Plot the `training error` and `test error` that are stored in the variable
    that was created during the fitting process:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制在拟合过程中存储的`训练误差`和`测试误差`：
- en: '[PRE21]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here''s the expected output:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '![Figure 5.9: Plot of the training error and validation error while training
    the model without early stopping'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.9：训练模型时，未使用早停的训练误差和验证误差图'
- en: '](img/B15777_05_09.jpg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_05_09.jpg)'
- en: 'Figure 5.9: Plot of the training error and validation error while training
    the model without early stopping'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.9：训练模型时，未使用早停的训练误差和验证误差图
- en: As you can see from the preceding plot, training the model for `300` epochs
    results in a gap that grows between the `training error` and `validation error`,
    which is indicative of overfitting beginning to happen.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，训练模型 `300` 个 epoch 后，`训练误差`和`验证误差`之间的差距不断扩大，这表明过拟合已经开始发生。
- en: 'Redefine the model by creating the model with the same number of layers and
    with the same number of units within each layer. This ensures the model is initialized
    in the same way. Add a callback `es_callback = EarlyStopping(monitor=''val_loss'',
    mode=''min'')` to the training process. Repeat *step 4* to plot the training error
    and validation error:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过创建具有相同层数和每层相同单位数的模型来重新定义模型。这确保了模型以相同的方式初始化。向训练过程中添加回调`es_callback = EarlyStopping(monitor='val_loss',
    mode='min')`。重复*步骤 4*以绘制训练误差和验证误差：
- en: '[PRE22]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now plot the loss values:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在绘制损失值：
- en: '[PRE23]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here''s the expected output:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '![Figure 5.10: Plot of training error and validation error while training the
    model with early stopping (patience=0)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.10：使用提前停止（patience=0）训练模型时的训练误差和验证误差图'
- en: '](img/B15777_05_10.jpg)'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_05_10.jpg)'
- en: 'Figure 5.10: Plot of training error and validation error while training the
    model with early stopping (patience=0)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.10：使用提前停止（patience=0）训练模型时的训练误差和验证误差图
- en: By adding the early stopping callback with `patience=0` to the model, the training
    process automatically stops after about `39` epochs.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将`patience=0`的提前停止回调添加到模型中，训练过程将在大约`39`个 epoch 后自动停止。
- en: 'Repeat *step 5* while adding `patience=10` to your early stopping callback.
    Repeat *step 3* to plot the `training error` and `validation error`:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 5*，同时将`patience=10`添加到你的提前停止回调中。重复*步骤 3*以绘制`训练误差`和`验证误差`：
- en: '[PRE24]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then plot the loss again:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后再次绘制损失图：
- en: '[PRE25]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here''s the expected output:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '![Figure 5.11: Plot of training error and validation error while training the
    model with early stopping (patience=10)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：使用提前停止（patience=10）训练模型时的训练误差和验证误差图'
- en: '](img/B15777_05_11.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_11.jpg)'
- en: 'Figure 5.11: Plot of training error and validation error while training the
    model with early stopping (patience=10)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：使用提前停止（patience=10）训练模型时的训练误差和验证误差图
- en: By adding the early stopping callback with `patience=10` to the model, the training
    process automatically stops after about `150` epochs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`patience=10`的提前停止回调添加到模型中，训练过程将在大约`150`个 epoch 后自动停止。
- en: In this exercise, you learned how to stop the model to prevent your Keras model
    from overfitting the training data. To do this, you utilized the `EarlyStopping`
    callback and trained the model with it. We used this callback to stop the model
    any time the validation loss increased and added a `patience` parameter, which
    waits for a given number of epochs before stopping. We practiced using this callback
    on a problem involving the Traffic Volume dataset to train our Keras model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你学会了如何停止模型训练，以防止你的 Keras 模型在训练数据上过拟合。为此，你使用了`EarlyStopping`回调并在训练时应用了它。我们使用这个回调在验证损失增加时停止模型，并添加了一个`patience`参数，它会在停止之前等待给定的
    epoch 数量。我们在一个涉及交通流量数据集的问题上练习了使用这个回调来训练 Keras 模型。
- en: Note
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3iuM4eL](https://packt.live/3iuM4eL).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/3iuM4eL](https://packt.live/3iuM4eL)。
- en: You can also run this example online at [https://packt.live/38AbweB](https://packt.live/38AbweB).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/38AbweB](https://packt.live/38AbweB)上在线运行这个示例。
- en: In the next section, we will discuss other regularization methods that can be
    applied to prevent overfitting.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将讨论其他可以应用的正则化方法，以防止过拟合。
- en: Data Augmentation
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: '**Data augmentation** is a regularization technique that tries to address overfitting
    by training the model on more training examples in an inexpensive way. In data
    augmentation, the available data is transformed in different ways and fed to the
    model as new training data. This type of regularization has been shown to be effective,
    especially for some specific applications, such as object detection/recognition
    in computer vision and speech processing.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据增强**是一种正则化技术，试图通过以廉价的方式在更多的训练样本上训练模型来解决过拟合问题。在数据增强中，现有数据会以不同的方式进行转换，并作为新的训练数据输入到模型中。这种类型的正则化已被证明是有效的，特别是在一些特定的应用中，例如计算机视觉中的目标检测/识别和语音处理。'
- en: For example, in computer vision applications, you can simply double or triple
    the size of your training dataset by adding mirrored versions and rotated versions
    of each image to the dataset. The new training examples that are generated by
    these transformations are obviously not as good as the original training examples.
    However, they are shown to improve the model in terms of overfitting.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在计算机视觉应用中，您可以通过将每个图像的镜像版本和旋转版本添加到数据集中，轻松地将训练数据集的大小加倍或加三倍。通过这些变换生成的新训练示例显然不如原始训练示例好。然而，它们已被证明能改善模型的过拟合问题。
- en: One challenging aspect of performing data augmentation is choosing the right
    transformations to be performed on data. Transformations need to be selected carefully,
    depending on the type of dataset and the application.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 执行数据增强时的一个挑战是选择在数据上执行的正确变换。根据数据集的类型和应用，变换需要谨慎选择。
- en: Adding Noise
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加噪声
- en: The underlying idea behind regularizing a model by adding noise to the data
    is the same as that for data augmentation regularization. Training a deep neural
    network on a small dataset increases the chance of the network memorizing single
    data examples as opposed to capturing the relations between inputs and outputs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向数据中添加噪声来对模型进行正则化的基本思想与数据增强正则化相同。在小数据集上训练深度神经网络会增加网络记住单一数据示例的概率，而不是捕捉输入与输出之间的关系。
- en: This will result in poor performance on new data later, which is indicative
    of the model overfitting the training data. In contrast, training a model on a
    large dataset increases the chance of the model capturing the true underlying
    process instead of memorizing single data points, and therefore reduces the chances
    of overfitting.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这样会导致在新数据上的表现不佳，表明模型对训练数据进行了过拟合。相反，在大数据集上训练模型可以增加模型捕捉真实底层过程的概率，而不是记住单个数据点，从而减少过拟合的机会。
- en: One way to expand the training data and reduce overfitting is to generate new
    data examples by injecting noise into the available data. This type of regularization
    has been shown to reduce overfitting to an extent that is comparable to weight
    regularization techniques.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展训练数据并减少过拟合的一种方法是通过向现有数据中注入噪声来生成新的数据示例。这种正则化方式已被证明能够减少过拟合，其效果与权重正则化技术相当。
- en: By adding different versions of a single example to the training data (each
    created by adding a small amount of noise to the original example), we can ensure
    that the model will not fit the noise in the data. Additionally, increasing the
    size of the training dataset by including these modified examples provides the
    model with a better representation of the underlying data generation process and
    increases the chance of the model learning the true process.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将单个示例的不同版本添加到训练数据中（每个版本通过在原始示例中加入少量噪声创建），我们可以确保模型不会过拟合数据中的噪声。此外，通过包含这些修改后的示例来增加训练数据集的大小，可以为模型提供更好的底层数据生成过程的表示，并增加模型学习真实过程的机会。
- en: In deep learning applications, you can improve model performance by adding noise
    to the weights or activations of the hidden layers, or gradients of the network,
    or even to the output layer, as well as by adding noise to the training examples
    (input layer). Deciding where to add noise in a deep neural network is another
    challenge that needs to be addressed by trying different networks and observing
    the results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习应用中，您可以通过向隐藏层的权重或激活值、网络的梯度，甚至输出层添加噪声，或向训练示例（输入层）添加噪声来提高模型性能。决定在深度神经网络中添加噪声的位置是另一个需要通过尝试不同网络并观察结果来解决的挑战。
- en: 'In Keras, you can easily define noise as a layer and add it to your model.
    For example, to add `Gaussian noise` with a `standard deviation` of `0.1` (the
    mean is equal to `0`) to your model, you can write this:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，您可以轻松地将噪声定义为一个层并将其添加到模型中。例如，要向模型添加`高斯噪声`，标准差为`0.1`（均值为`0`），可以编写如下代码：
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following code will add `Gaussian noise` to the outputs/activations of
    the first hidden layer of the model:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将向模型的第一个隐藏层的输出/激活值添加`高斯噪声`：
- en: '[PRE27]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In this section, you learned about three regularization methods: `early stopping`,
    `data augmentation`, and `adding noise`. In addition to their basic concepts and
    procedures, you also learned about how they reduce overfitting and were given
    some tips and recommendations on how to use them. In the next section, you will
    learn how to tune hyperparameters using functions provided by scikit-learn. By
    doing this, we can incorporate Keras models into a scikit-learn workflow.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了三种正则化方法：`early stopping`、`data augmentation` 和 `adding noise`。除了它们的基本概念和流程外，你还了解了它们如何减少过拟合，并且提供了一些使用它们的技巧和建议。在下一节中，你将学习如何使用
    scikit-learn 提供的函数来调优超参数。通过这样做，我们可以将 Keras 模型整合到 scikit-learn 的工作流中。
- en: Hyperparameter Tuning with scikit-learn
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行超参数调优
- en: Hyperparameter tuning is a very important technique for improving the performance
    of deep learning models. In *Chapter 4*, *Evaluating Your Model with Cross-Validation
    Using Keras Wrappers*, you learned about using a Keras wrapper with scikit-learn,
    which allows for Keras models to be used in a scikit-learn workflow. As a result,
    different general machine learning and data analysis tools and methods that are
    available in scikit-learn can be applied to Keras deep learning models. Among
    those methods are scikit-learn hyperparameter optimizers.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是提高深度学习模型性能的一个非常重要的技术。在*第 4 章*，*使用 Keras 包装器进行交叉验证评估模型*中，你学习了如何使用 scikit-learn
    的 Keras 包装器，这使得 Keras 模型能够在 scikit-learn 的工作流中使用。因此，scikit-learn 中可用的不同通用机器学习和数据分析工具与方法可以应用于
    Keras 深度学习模型。其中包括 scikit-learn 的超参数优化器。
- en: In the previous chapter, you learned how to perform hyperparameter tuning by
    writing user-defined functions to loop over possible values for each hyperparameter.
    In this section, you will learn how to perform it in a much easier way by using
    the various hyperparameter optimization methods that are available in scikit-learn.
    You will also get to practice applying those methods by completing an activity
    involving a real-life dataset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何通过编写用户定义的函数，遍历每个超参数的可能值来进行超参数调优。在本节中，你将学习如何通过使用 scikit-learn 中可用的各种超参数优化方法，以更简单的方式进行调优。你还将通过完成涉及实际数据集的活动来实践应用这些方法。
- en: Grid Search with scikit-learn
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行网格搜索
- en: So far, we have established that building deep neural networks involves making
    decisions about several hyperparameters. The list of hyperparameters includes
    the number of hidden layers, the number of units in each hidden layer, the activation
    function for each layer, the loss function for the network, the type of optimizer
    and its parameters, the type of regularizer and its parameters, the batch size,
    the number of epochs, and others. We also observed that different values of hyperparameters
    can affect the performance of a model significantly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经确认，构建深度神经网络涉及对多个超参数做出决策。超参数的列表包括隐藏层的数量、每个隐藏层中单元的数量、每层的激活函数、网络的损失函数、优化器的类型及其参数、正则化器的类型及其参数、批次大小、训练的轮次等。我们还观察到，不同的超参数值会显著影响模型的性能。
- en: Therefore, finding the best values for hyperparameters is one of the most important
    and challenging parts of becoming a deep learning expert. Since there are no absolute
    rules for picking the hyperparameters that work for every dataset and every problem,
    deciding on the values of hyperparameters needs to be done through trial and error
    for each particular problem. This process of training and evaluating models with
    different hyperparameters and deciding about the final hyperparameters based on
    model performance is called **hyperparameter tuning** or **hyperparameter optimization**.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，找到最佳超参数值是成为深度学习专家过程中最重要也是最具挑战性的部分之一。由于没有适用于每个数据集和每个问题的超参数绝对规则，因此确定超参数的值需要通过试验和错误来针对每个特定问题进行调整。这个过程——用不同的超参数训练和评估模型，并根据模型表现决定最终的超参数——被称为**超参数调优**或**超参数优化**。
- en: 'Having a range or a set of possible values for each hyperparameter that we
    are interested in tuning can create a grid such as the one shown in the following
    image. Therefore, hyperparameter tuning can be seen as a grid search problem;
    we would like to try every cell in the grid (every possible combination of hyperparameters)
    and find the one cell that results in the best performance for the model:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们希望调整的每个超参数，设置一组可能的取值范围可以创建一个网格，如下图所示。因此，超参数调整可以看作是一个网格搜索问题；我们希望尝试网格中的每一个单元格（每一个可能的超参数组合），并找到那个能为模型带来最佳性能的单元格：
- en: '![Figure 5.12: A hyperparameter grid created by some values for optimizer,
    batch_size, and epochs'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12：通过优化器、批处理大小和 epoch 的一些值创建的超参数网格'
- en: '](img/B15777_05_12.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_05_12.jpg)'
- en: 'Figure 5.12: A hyperparameter grid created by some values for optimizer, batch_size,
    and epochs'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：通过优化器、批处理大小和 epoch 的一些值创建的超参数网格
- en: Scikit-learn provides a parameter optimizer called `GridSearchCV()` to perform
    this exhaustive grid search. `GridSearchCV()` receives the model as the `estimator`
    argument and the dictionary containing all possible values for the hyperparameters
    as the `param_grid` argument. Then, it goes through every point in the grid, performs
    cross-validation on the model using the hyperparameter values at that point, and
    returns the best cross-validation score, along with the values of the hyperparameters
    that led to that score.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了一个名为 `GridSearchCV()` 的参数优化器，用于执行这种穷举的网格搜索。`GridSearchCV()`
    接收模型作为 `estimator` 参数，并接收包含所有可能的超参数值的字典作为 `param_grid` 参数。然后，它会遍历网格中的每个点，使用该点的超参数值对模型进行交叉验证，并返回最佳的交叉验证得分，以及导致该得分的超参数值。
- en: 'In the previous chapter, you learned that in order to use Keras models in scikit-learn,
    you need to define a function that returns a Keras model. For example, the following
    code block defines a Keras model that we would like to perform hyperparameter
    tuning on later:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了为了在 scikit-learn 中使用 Keras 模型，你需要定义一个返回 Keras 模型的函数。例如，下面的代码块定义了一个
    Keras 模型，我们希望在之后对其进行超参数调整：
- en: '[PRE28]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The next step would be to define the grid of parameters. For example, say we
    would like to tune over `optimizer=[''rmsprop'', ''adam'', ''sgd'', ''adagrad'']`,
    `epochs = [100, 150]`, `batch_size = [1, 5, 10]`. To do so, we would write the
    following:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义超参数网格。例如，假设我们想要调整 `optimizer=['rmsprop', 'adam', 'sgd', 'adagrad']`，`epochs
    = [100, 150]`，`batch_size = [1, 5, 10]`。为了做到这一点，我们将编写如下代码：
- en: '[PRE29]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now that the hyperparameter grid has been created, we can create the wrapper
    so that we can build the interface for the Keras model and use it as an estimator
    to perform the grid search:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在超参数网格已经创建完毕，我们可以创建封装器，以便构建 Keras 模型的接口，并将其用作估计器来执行网格搜索：
- en: '[PRE30]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding code through goes through every cell in the grid exhaustively
    and performs 10-fold cross-validation using hyperparameter values in each cell
    (here, it performs `10-fold cross-validation` 4*2*3=24 times). Then, it returns
    the cross-validation score for each of these `24` cells, along with the one that
    resulted in the best score.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会穷举地遍历网格中的每个单元格，并使用每个单元格中的超参数值进行 10 折交叉验证（这里，它会执行 `10 折交叉验证` 4*2*3=24 次）。然后，它返回每个这些
    `24` 个单元格的交叉验证得分，并返回获得最佳得分的那个。
- en: Note
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Performing k-fold cross-validation on many possible combinations of hyperparameters
    sure takes a long time. For this reason, you can parallelize the process by passing
    the `n_jobs=-1` argument to `GridSearchCV()`, which results in using every processor
    available to perform the grid search. The default value for this argument is `n_jobs=1`,
    which means no parallelization.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对多个可能的超参数组合执行 k 折交叉验证确实需要很长时间。因此，你可以通过将 `n_jobs=-1` 参数传递给 `GridSearchCV()` 来并行化这个过程，这样会使用所有可用的处理器来执行网格搜索。该参数的默认值是
    `n_jobs=1`，意味着不进行并行化。
- en: Creating a hyperparameter grid is just one way to iterate through hyperparameters
    to find the optimal selection. Another way is to simply randomize the selection
    of hyperparameters, which we will learn about in the next topic.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 创建超参数网格只是通过超参数迭代找到最优选择的一种方式。另一种方法是简单地随机选择超参数，这将在下一个主题中介绍。
- en: Randomized Search with scikit-learn
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行随机化搜索
- en: As you may have realized, an exhaustive grid search may not be the best choice
    for tuning the hyperparameters of a deep learning model since it is not very efficient.
    There are many hyperparameters in deep learning, and especially if you would like
    to try a large range of values for each, an exhaustive grid search would simply
    take too long to complete. An alternative way to perform hyperparameter optimization
    is to perform random sampling on the grid and perform k-fold cross-validation
    on some randomly selected cells. Scikit-learn provides an optimizer called `RandomizedSearchCV()`
    to perform a random search for the purpose of hyperparameter optimization.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经意识到的那样，穷举网格搜索可能不是调优深度学习模型超参数的最佳选择，因为它效率不高。深度学习中有许多超参数，尤其是当你想为每个超参数尝试一个大范围的值时，穷举网格搜索将需要花费过长的时间才能完成。进行超参数优化的另一种方式是，在网格上进行随机采样，并对一些随机选择的单元格进行k折交叉验证。Scikit-learn提供了一个名为`RandomizedSearchCV()`的优化器，用于执行超参数优化的随机搜索。
- en: 'For example, we can change the code from the previous section from an exhaustive
    grid search to a random search like so:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将前一节的代码从穷举网格搜索更改为随机搜索，如下所示：
- en: '[PRE31]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice that `RandomizedSearchCV()` requires the extra `n_iter` argument, which
    determines how many random cells must be selected. This determines how many times
    k-fold cross-validation will be performed. Therefore, by choosing a smaller number,
    fewer hyperparameter combinations will be considered and the method will take
    less time to complete. Also, please note that the `param_grid` argument is changed
    to `param_distributions` here. The `param_distributions` argument can take a dictionary
    with parameter names as keys, and either list of parameters or distributions as
    values for each key.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`RandomizedSearchCV()`需要额外的`n_iter`参数，它决定了必须选择多少个随机单元格。这决定了k折交叉验证将执行多少次。因此，通过选择较小的值，将考虑较少的超参数组合，方法完成的时间也会更短。另外，请注意，这里`param_grid`参数已更改为`param_distributions`。`param_distributions`参数可以接受一个字典，字典的键是参数名称，值可以是参数的列表或每个键的分布。
- en: It could be argued that `RandomizedSearchCV()` is not as good as `GridSearchCV()`
    since it does not consider all the possible values and combinations of values
    for hyperparameters, which is reasonable. As a result, one smart way of performing
    hyperparameter tuning for deep learning models is to start with either `RandomizedSearchCV()`
    on many hyperparameters, or `GridSearchCV()` on fewer hyperparameters with larger
    gaps between them.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，`RandomizedSearchCV()`不如`GridSearchCV()`好，因为它没有考虑所有可能的超参数值及其组合，这一点是合理的。因此，一种进行深度学习模型超参数调优的聪明方法是，首先对许多超参数使用`RandomizedSearchCV()`，或对较少的超参数使用`GridSearchCV()`，并且这些超参数之间的间隔较大。
- en: By beginning with a randomized search on many hyperparameters, we can determine
    which hyperparameters have the most influence on a model's performance. It can
    also help narrow down the range for important hyperparameters. Then, you can complete
    your hyperparameter tuning by performing `GridSearchCV()` on the smaller number
    of hyperparameters and the smaller ranges for each of them. This is called the
    **coarse-to-fine** approach to hyperparameter tuning.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从许多超参数的随机搜索开始，我们可以确定哪些超参数对模型性能的影响最大。这也有助于缩小重要超参数的范围。然后，你可以通过对较少的超参数及其较小的范围执行`GridSearchCV()`来完成超参数调优。这种方法称为**粗到精**的超参数调优方法。
- en: Now, you are ready to practice implementing hyperparameter tuning using scikit-learn
    optimizers. In the next activity, you will try to improve your model for the diabetes
    dataset by tuning the hyperparameters.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你准备好实践使用scikit-learn优化器进行超参数调优了。在接下来的活动中，你将通过调优超参数来改进你的糖尿病数据集模型。
- en: 'Activity 5.03: Hyperparameter Tuning on the Avila Pattern Classifier'
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.03：在Avila模式分类器上进行超参数调优
- en: 'The Avila dataset has been extracted from `800` images of the Avila Bible,
    a giant 12th-century Latin copy of the Bible. The dataset consists of various
    features about the images of the text, such as intercolumnar distance and margins
    of the text. The dataset also contains a class label that indicates if the pattern
    of the image falls into the most frequently occurring category or not. In this
    activity, you will build a Keras model similar to those in the previous activities,
    but this time, you will add regularization methods to your model as well. Then,
    you will use scikit-learn optimizers to perform tuning on the model hyperparameters,
    including the hyperparameters of the regularizers. Here are the steps you need
    to complete in this activity:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Avila 数据集是从`800`张 Avila 圣经图像中提取的，Avila 圣经是 12 世纪的拉丁文巨型圣经副本。该数据集包含有关文本图像的各种特征，如行间距和文本边距。数据集还包含一个类别标签，指示图像的模式是否属于最常见的类别。在本活动中，您将构建一个与前几个活动类似的
    Keras 模型，但这次您将为模型添加正则化方法。然后，您将使用 scikit-learn 优化器来调整模型的超参数，包括正则化器的超参数。以下是您在本活动中需要完成的步骤：
- en: Load the dataset from the `data` subfolder of the `Chapter05` folder from GitHub
    using `X = pd.read_csv('../data/avila-tr_feats.csv')` and `y = pd.read_csv('../data/avila-tr_target.csv')`.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`X = pd.read_csv('../data/avila-tr_feats.csv')`和`y = pd.read_csv('../data/avila-tr_target.csv')`从
    GitHub 上的`Chapter05`文件夹中的`data`子文件夹加载数据集。
- en: 'Define a function that returns a Keras model with three hidden layers, the
    first of `size 10`, the second of `size 6`, and the third of `size 4`, all with
    `L2 weight regularizations`. Use these values as the hyperparameters for your
    model: `activation=''relu''`, `loss=''binary_crossentropy''`, `optimizer=''sgd''`,
    and `metrics=[''accuracy'']`. Also, make sure to pass the `L2 lambda` hyperparameter
    as an argument to your function so that we can tune it later.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个返回 Keras 模型的函数，该模型包含三个隐藏层，第一层的`size 10`，第二层的`size 6`，第三层的`size 4`，且均带有`L2
    权重正则化`。使用以下值作为模型的超参数：`activation='relu'`，`loss='binary_crossentropy'`，`optimizer='sgd'`，`metrics=['accuracy']`。另外，确保将`L2
    lambda`超参数作为参数传递给函数，以便我们以后进行调整。
- en: 'Create the wrapper for your Keras model and perform `GridSearchCV()` on it
    using `cv=5`. Then, add the following values in the parameter grid: `lambda_parameter
    = [0.01, 0.5, 1]`, `epochs = [50, 100]`, and `batch_size = [20]`. This might take
    some time to process. Once the parameter search is complete, print the accuracy
    and the hyperparameters of the best cross-validation score. You can also print
    every other cross-validation score, along with the hyperparameters that resulted
    in that score.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的 Keras 模型创建包装器，并使用`cv=5`对其执行`GridSearchCV()`。然后，在参数网格中添加以下值：`lambda_parameter
    = [0.01, 0.5, 1]`，`epochs = [50, 100]`，以及`batch_size = [20]`。这可能需要一些时间来处理。参数搜索完成后，打印最佳交叉验证分数的准确率和超参数。您还可以打印每个其他交叉验证分数，以及导致该分数的超参数。
- en: Repeat the previous step, this time using `GridSearchCV()` on a narrower range
    with `lambda_parameter = [0.001, 0.01, 0.05, 0.1]`, `epochs = [400]`, and `batch_size
    = [10]`. It might take some time to process.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前一步，这次使用`GridSearchCV()`在更窄的范围内进行搜索，参数为`lambda_parameter = [0.001, 0.01, 0.05,
    0.1]`，`epochs = [400]`，以及`batch_size = [10]`。这可能需要一些时间来处理。
- en: 'Repeat the previous step, but remove the `L2 regularizers` from your Keras
    model and instead of adding dropout regularization with the `rate` parameter at
    each hidden layer. Perform `GridSearchCV()` on the model using the following values
    in the parameter grid and print the results: `rate = [0, 0.2, 0.4]`, `epochs =
    [350, 400]`, and `batch_size = [10]`.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前一步，但从 Keras 模型中移除`L2 正则化器`，而是使用`rate`参数在每个隐藏层中添加 dropout 正则化。使用以下值在参数网格中执行`GridSearchCV()`并打印结果：`rate
    = [0, 0.2, 0.4]`，`epochs = [350, 400]`，以及`batch_size = [10]`。
- en: Repeat the previous step using `rate = [0.0, 0.05, 0.1]` and `epochs=[400]`.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前一步，使用`rate = [0.0, 0.05, 0.1]`和`epochs=[400]`。
- en: 'After implementing these steps, you should see the following expected output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，您应该看到以下预期输出：
- en: '[PRE32]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 422.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 422 页找到。
- en: In this activity, we learned how to implement hyperparameter tuning on a Keras
    model with regularizers to perform classification using a real-life dataset. We
    learned how to use scikit-learn optimizers to perform tuning on model hyperparameters,
    including the hyperparameters of the regularizers. In this section, we implemented
    hyperparameter tuning by creating a grid of hyperparameters and iterating through
    them. This allows us to find the optimal set of hyperparameters using a scikit-learn
    workflow.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们学习了如何在 Keras 模型上实现超参数调优，并结合正则化器使用真实数据集进行分类。我们学习了如何使用 scikit-learn 优化器对模型超参数进行调优，包括正则化器的超参数。在这一部分，我们通过创建一个超参数网格并遍历它们来实现超参数调优。这使我们能够使用
    scikit-learn 工作流找到最优的超参数组合。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned about two very important groups of techniques
    for improving the accuracy of your deep learning models: regularization and hyperparameter
    tuning. You learned how regularization helps address the overfitting problem by
    means of several different methods, including L1 and L2 norm regularization and
    dropout regularization—the more commonly used regularization techniques. You discovered
    the importance of hyperparameter tuning for machine learning models and the challenge
    of hyperparameter tuning for deep learning models in particular. You even practiced
    using scikit-learn optimizers to perform hyperparameter tuning on Keras models.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了两组非常重要的技术，用于提高深度学习模型的准确性：正则化和超参数调优。你学习了正则化如何通过多种不同的方法解决过拟合问题，包括 L1
    和 L2 范数正则化以及 dropout 正则化——这些是常用的正则化技术。你发现了超参数调优对于机器学习模型的重要性，特别是对于深度学习模型的挑战。你甚至练习了使用
    scikit-learn 优化器，在 Keras 模型上执行超参数调优。
- en: In the next chapter, you will explore the limitations of accuracy metrics when
    evaluating model performance, as well as other metrics (such as `precision`, `sensitivity`,
    `specificity`, and `AUC-ROC score`), including how to use them in order to gauge
    the quality of your model's performance better.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将探索评估模型性能时准确性指标的局限性，以及其他指标（如 `precision`、`sensitivity`、`specificity`
    和 `AUC-ROC score`），并了解如何使用它们更好地评估模型的性能。
