- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Unsupervised Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'The book till now has focused on supervised learning and the models that learn
    via supervised learning. Starting from this chapter we will explore a less explored
    and more challenging area of unsupervised learning, self-supervised learning,
    and contrastive learning. In this chapter, we will delve deeper into some popular
    and useful unsupervised learning models. In contrast to supervised learning, where
    the training dataset consists of both the input and the desired labels, unsupervised
    learning deals with a case where the model is provided with only the input. The
    model learns the inherent input distribution by itself without any desired label
    guiding it. Clustering and dimensionality reduction are the two most commonly
    used unsupervised learning techniques. In this chapter, we will learn about different
    machine learning and neural network techniques for both. We will cover techniques
    required for clustering and dimensionality reduction, and go into the detail about
    Boltzmann machines, and finally, cover the implementation of the aforementioned
    techniques using TensorFlow. The concepts covered will be extended to build **Restricted
    Boltzmann Machines** (**RBMs**). The chapter will include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书主要集中在监督学习及其通过监督学习进行学习的模型。从这一章开始，我们将探索一个较少被探索且更具挑战性的领域——无监督学习、 self-supervised学习和对比学习。在本章中，我们将深入探讨一些流行且有用的无监督学习模型。与监督学习不同，监督学习中的训练数据集包括输入数据和目标标签，而无监督学习则处理仅提供输入数据的情况。模型通过自身学习输入数据的内在分布，无需任何目标标签的引导。聚类和降维是最常用的两种无监督学习技术。在本章中，我们将学习与这两种技术相关的不同机器学习和神经网络方法。我们将涵盖聚类和降维所需的技术，并详细讲解玻尔兹曼机，最后使用TensorFlow实现上述技术。所涉及的概念将扩展到构建**限制玻尔兹曼机**（**RBMs**）。本章将包括：
- en: Principal component analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: K-means clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类
- en: Self-organizing maps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自组织映射
- en: Boltzmann machines
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玻尔兹曼机
- en: RBMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBMs
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp7](https://packt.link/dltfchp7).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp7](https://packt.link/dltfchp7)找到。
- en: Let us start with the most common and frequently used technique for dimensionality
    reduction, the principal component analysis method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最常见且最常用的降维技术——主成分分析方法开始。
- en: Principal component analysis
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '**Principal component analysis** (**PCA**) is the most popular multivariate
    statistical technique for dimensionality reduction. It analyzes the training data
    consisting of several dependent variables, which are, in general, intercorrelated,
    and extracts important information from the training data in the form of a set
    of new orthogonal variables called principal components.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是最流行的多变量统计降维技术。它分析包含多个相关变量的训练数据，这些变量通常是相互关联的，并从训练数据中提取重要信息，形成一组新的正交变量，称为主成分。'
- en: We can perform PCA using two methods, either **eigen decomposition** or **singular
    value decomposition** (**SVD**).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两种方法进行PCA：**特征分解**或**奇异值分解**（**SVD**）。
- en: 'PCA reduces the *n*-dimensional input data to *r*-dimensional input data, where
    *r<n*. In simple terms, PCA involves translating the origin and performing rotation
    of the axis such that one of the axes (principal axis) has the highest variance
    with data points. A reduced-dimensions dataset is obtained from the original dataset
    by performing this transformation and then dropping (removing) the orthogonal
    axes with low variance. Here, we employ the SVD method for PCA dimensionality
    reduction. Consider *X*, the *n*-dimensional data with *p* points, that is, *X*
    is a matrix of size *p × n*. From linear algebra we know that any real matrix
    can be decomposed using singular value decomposition:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PCA将* n *维输入数据减少到* r *维输入数据，其中* r <n *。简而言之，PCA涉及平移原点并执行坐标轴旋转，使得其中一个轴（主轴）与数据点的方差最大。从原始数据集中，通过进行此变换并删除（移除）方差较小的正交轴，得到一个降维后的数据集。在此，我们使用SVD方法进行PCA降维。考虑*X*，它是一个*
    n *维数据，包含* p *个点，即* X *是一个大小为*p × n*的矩阵。从线性代数中我们知道，任何实矩阵都可以通过奇异值分解进行分解：
- en: '![](img/B18331_07_001.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_001.png)'
- en: Where *U* and *V* are orthonormal matrices (that is, *U.U*^T *= V.V*^T *= 1*)
    of size *p × p* and *n × n* respectively. ![](img/B18331_07_002.png) is a diagonal
    matrix of size *p × n*. The *U* matrix is called the **left singular matrix**,
    and *V* the **right singular matrix**, and ![](img/B18331_07_002.png), the diagonal
    matrix, contains the singular values of *X* as its diagonal elements. Here we
    assume that the *X* matrix is centered. The columns of the *V* matrix are the
    principal components, and columns of ![](img/B18331_07_004.png) are the data transformed
    by principal components.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*U* 和 *V* 是正交矩阵（即 *U.U*^T *= V.V*^T *= 1*），其大小分别为 *p × p* 和 *n × n*。![](img/B18331_07_002.png)
    是一个大小为 *p × n* 的对角矩阵。*U* 矩阵称为**左奇异矩阵**，*V* 矩阵称为**右奇异矩阵**，而 ![](img/B18331_07_002.png)
    这个对角矩阵包含了 *X* 的奇异值，作为其对角元素。这里假设 *X* 矩阵是已居中的。*V* 矩阵的列是主成分，而 ![](img/B18331_07_004.png)
    的列是经过主成分变换后的数据。
- en: 'Now to reduce the dimensions of the data from *n* to *k* (where *k < n*), we
    will select the first *k* columns of *U* and the upper-left *k × k* part of ![](img/B18331_07_002.png).
    The product of the two gives us our reduced-dimensions matrix:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将数据从 *n* 维降至 *k* 维（其中 *k < n*），我们将选择 *U* 的前 *k* 列和 ![](img/B18331_07_002.png)
    左上角的 *k × k* 部分。两者的乘积将给出我们的降维矩阵：
- en: '![](img/B18331_07_006.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_006.png)'
- en: The data *Y* obtained will be of reduced dimensions. Next, we implement PCA
    in TensorFlow 2.0.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的 *Y* 数据将是降维后的数据。接下来，我们将在 TensorFlow 2.0 中实现 PCA。
- en: PCA on the MNIST dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 MNIST 数据集上进行 PCA
- en: 'Let us now implement PCA in TensorFlow 2.0\. We will be definitely using TensorFlow;
    we will also need NumPy for some elementary matrix calculation, and Matplotlib,
    Matplotlib toolkits, and Seaborn for plotting:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在 TensorFlow 2.0 中实现 PCA。我们一定会使用 TensorFlow；此外，我们还需要 NumPy 来进行一些基础的矩阵计算，并使用
    Matplotlib、Matplotlib 工具包以及 Seaborn 来进行绘图：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next we load the MNIST dataset. Since we are doing dimension reduction using
    PCA, we do not need a test dataset or even labels; however, we are loading labels
    so that after reduction we can verify the PCA performance. PCA should cluster
    similar data points in one cluster; hence, if we see that the clusters formed
    using PCA are similar to our labels, it would indicate that our PCA works:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载 MNIST 数据集。由于我们使用 PCA 进行降维，因此不需要测试数据集或标签；然而，我们加载标签是为了在降维后验证 PCA 的效果。PCA
    应该将相似的数据点聚集在一个簇中；因此，如果我们看到使用 PCA 形成的簇与我们的标签相似，那么这就表明我们的 PCA 有效：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before we do PCA, we should preprocess the data. We first normalize it so that
    all data has values between 0 and 1, and then reshape the image from being a 28
    × 28 matrix to a 784-dimensional vector, and finally, center it by subtracting
    the mean:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 PCA 之前，我们需要预处理数据。我们首先对数据进行归一化，使其值介于 0 和 1 之间，然后将图像从 28 × 28 矩阵重塑为一个 784
    维的向量，最后通过减去均值来居中数据：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that our data is in the right format, we make use of TensorFlow’s powerful
    linear algebra (`linalg`) module to calculate the SVD of our training dataset.
    TensorFlow provides the function `svd()` defined in `tf.linalg` to perform this
    task. And then use the `diag` function to convert the sigma array (`s`, a list
    of singular values) to a diagonal matrix:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经是正确的格式，我们利用 TensorFlow 强大的线性代数模块 (`linalg`) 来计算训练数据集的 SVD。TensorFlow
    提供了 `svd()` 函数，定义在 `tf.linalg` 中，用来执行这个任务。然后，使用 `diag` 函数将 sigma 数组（`s`，即奇异值的列表）转换为对角矩阵：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This provides us with a diagonal matrix *s* of size 784 × 784; a left singular
    matrix *u* of size 60,000 × 784; and a right singular matrix *v* of size 784 ×
    784\. This is so because the argument `full_matrices` of the function `svd()`
    is by default set to `False`. As a result it does not generate the full *U* matrix
    (in this case, of size 60,000 × 60,000); instead, if input *X* is of size *m ×
    n*, it generates *U* of size *p = min(m,n)*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供一个大小为 784 × 784 的对角矩阵 *s*；一个大小为 60,000 × 784 的左奇异矩阵 *u*；以及一个大小为 784 ×
    784 的右奇异矩阵 *v*。这是因为函数 `svd()` 的参数 `full_matrices` 默认设置为 `False`。因此，它不会生成完整的 *U*
    矩阵（在这种情况下是 60,000 × 60,000 的矩阵）；相反，如果输入 *X* 的大小为 *m × n*，它会生成大小为 *p = min(m, n)*
    的 *U* 矩阵。
- en: 'The reduced-dimension data can now be generated by multiplying respective slices
    of *u* and *s*. We reduce our data from 784 to 3 dimensions; we can choose to
    reduce to any dimension less than 784, but we chose 3 here so that it is easier
    for us to visualize later. We make use of `tf.Tensor.getitem` to slice our matrices
    in the Pythonic way:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以通过乘以 *u* 和 *s* 的相应切片来生成降维后的数据。我们将数据从 784 维降至 3 维；我们可以选择降至任何小于 784 的维度，但这里我们选择了
    3 维，以便稍后更容易可视化。我们使用 `tf.Tensor.getitem` 以 Pythonic 方式对矩阵进行切片：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A comparison of the original and reduced data shape is done in the following
    code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码进行原始数据和降维数据形状的比较：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, let us plot the data points in the three-dimensional space:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在三维空间中绘制数据点：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Chart, scatter chart, surface chart  Description automatically generated](img/B18331_07_01.png)Figure
    7.1: Scatter plot of MNIST dataset after dimensionality reduction using PCA'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，散点图，表面图 描述自动生成](img/B18331_07_01.png)图 7.1：使用 PCA 降维后的 MNIST 数据集散点图'
- en: You can see that the points corresponding to the same color and, hence, the
    same label are clustered together. We have therefore successfully used PCA to
    reduce the dimensions of MNIST images. Each original image was of size 28 × 28\.
    Using the PCA method we can reduce it to a smaller size. Normally for image data,
    dimensionality reduction is necessary. This is because images are large in size
    and contain a significant amount of redundant data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，颜色相同的点，因此标签相同的点聚集在一起。我们因此成功地使用 PCA 对 MNIST 图像进行了降维处理。每个原始图像的大小为 28 × 28。使用
    PCA 方法，我们可以将其降至更小的尺寸。通常对于图像数据，降维是必要的。这是因为图像数据体积庞大，且包含大量冗余数据。
- en: TensorFlow Embedding API
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 嵌入 API
- en: 'TensorFlow also offers an Embedding API where one can find and visualize PCA
    and tSNE [1] clusters using TensorBoard. You can see the live PCA on MNIST images
    here: [http://projector.tensorflow.org](http://projector.tensorflow.org). The
    following image is reproduced for reference:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 还提供了一个嵌入 API，可以使用 TensorBoard 查找和可视化 PCA 和 tSNE [1] 聚类。您可以在此查看 MNIST
    图像的实时 PCA： [http://projector.tensorflow.org](http://projector.tensorflow.org)。下图为参考复制：
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_07_02.png)Figure
    7.2: A visualization of a principal component analysis, applied to the MNIST dataset'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，散点图 描述自动生成](img/B18331_07_02.png)图 7.2：主成分分析的可视化，应用于 MNIST 数据集'
- en: 'You can process your data using TensorBoard. It contains a tool called **Embedding
    Projector** that allows one to interactively visualize embedding. The Embedding
    Projector tool has three panels:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 TensorBoard 处理您的数据。它包含一个名为 **Embedding Projector** 的工具，允许您交互式地可视化嵌入。Embedding
    Projector 工具有三个面板：
- en: '**Data Panel**: It is located at the top left, and you can choose the data,
    labels, and so on in this panel.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据面板**：它位于左上角，您可以在此面板中选择数据、标签等。'
- en: '**Projections Panel**: Available at the bottom left, you can choose the type
    of projections you want here. It offers three choices: PCA, t-SNE, and custom.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投影面板**：位于左下角，您可以在这里选择所需的投影类型。它提供三种选择：PCA、t-SNE 和自定义。'
- en: '**Inspector Panel**: On the right-hand side, here you can search for particular
    points and see a list of nearest neighbors.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查器面板**：位于右侧，您可以在这里搜索特定的点，并查看最近邻的列表。'
- en: '![Graphical user interface, chart, scatter chart  Description automatically
    generated](img/B18331_07_03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，图表，散点图 描述自动生成](img/B18331_07_03.png)'
- en: 'Figure 7.3: Screenshot of the Embedding Projector tool'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：Embedding Projector 工具的截图
- en: PCA is a useful tool for visualizing datasets and for finding linear relationships
    between variables. It can also be used for clustering, outlier detection, and
    feature selection. Next, we will learn about the k-means algorithm, a method for
    clustering data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 是一个用于可视化数据集和寻找变量间线性关系的有用工具。它也可以用于聚类、异常值检测和特征选择。接下来，我们将学习 K-means 算法，一种聚类数据的方法。
- en: K-means clustering
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: 'K-means clustering, as the name suggests, is a technique to cluster data, that
    is, to partition data into a specified number of data points. It is an unsupervised
    learning technique. It works by identifying patterns in the given data. Remember
    the sorting hat of Harry Potter fame? What it is doing in the book is clustering—dividing
    new (unlabelled) students into four different clusters: Gryffindor, Ravenclaw,
    Hufflepuff, and Slytherin.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类，顾名思义，是一种对数据进行聚类的技术，即将数据划分为指定数量的数据点。它是一种无监督学习技术。它通过识别给定数据中的模式来工作。记得《哈利·波特》中的分院帽吗？它在书中所做的就是聚类——将新的（未标记的）学生分成四个不同的类别：格兰芬多、拉文克劳、赫奇帕奇和斯莱特林。
- en: Humans are very good at grouping objects together; clustering algorithms try
    to give a similar capability to computers. There are many clustering techniques
    available, such as hierarchical, Bayesian, or partitional. K-means clustering
    belongs to partitional clustering; it partitions data into *k* clusters. Each
    cluster has a center, called the centroid. The number of clusters *k* has to be
    specified by the user.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人类非常擅长将物体分组；聚类算法尝试将这种能力赋予计算机。有许多可用的聚类技术，如层次聚类、贝叶斯聚类或划分聚类。K-means 聚类属于划分聚类；它将数据划分为
    *k* 个簇。每个簇都有一个中心，称为质心。簇的数量 *k* 必须由用户指定。
- en: 'The k-means algorithm works in the following manner:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 算法按以下方式工作：
- en: Randomly choose *k* data points as the initial centroids (cluster centers).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择 *k* 个数据点作为初始质心（簇中心）。
- en: Assign each data point to the closest centroid; there can be different measures
    to find closeness, the most common being the Euclidean distance.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点分配给离其最近的质心；可以使用不同的度量来衡量“最近”，最常见的是欧几里得距离。
- en: Recompute the centroids using current cluster membership, such that the sum
    of squared distances decreases.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前的簇成员关系重新计算质心，使得平方距离的总和减少。
- en: Repeat the last two steps until convergence is met.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复最后两个步骤，直到达到收敛。
- en: In the previous TensorFlow versions, the `KMeans` class was implemented in the
    `Contrib` module; however, the class is no longer available in TensorFlow 2.0\.
    Here we will instead use the advanced mathematical functions provided in TensorFlow
    2.0 to implement k-means clustering.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 TensorFlow 版本中，`KMeans` 类是在 `Contrib` 模块中实现的；然而，该类在 TensorFlow 2.0 中已不可用。在这里，我们将改用
    TensorFlow 2.0 提供的高级数学函数来实现 K-means 聚类。
- en: K-means in TensorFlow
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 中的 K-means
- en: 'To demonstrate k-means in TensorFlow, we will use randomly generated data in
    the code that follows. Our randomly generated data will contain 200 samples, and
    we will divide them into three clusters. We start by importing all the required
    modules, defining the variables, and determining the number of sample points (`points_n`),
    the number of clusters to be formed (`clusters_n`), and the number of iterations
    we will be doing (`iteration_n`). We also set the seed for a random number to
    ensure that our work is reproducible:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示 TensorFlow 中的 K-means，我们将在以下代码中使用随机生成的数据。我们生成的数据将包含 200 个样本，我们将其分成三个簇。首先导入所需的所有模块，定义变量，确定样本点的数量（`points_n`）、要形成的簇的数量（`clusters_n`）以及我们将进行的迭代次数（`iteration_n`）。我们还设置随机数种子以确保工作可复现：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we randomly generate data and from the data select three centroids randomly:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们随机生成数据，并从中随机选择三个质心：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let us now plot the points:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制这些点：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can see the scatter plot of all the points and the randomly selected three
    centroids in the following graph:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下图中看到所有点的散点图以及随机选择的三个质心：
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_07_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B18331_07_04.png)'
- en: 'Figure 7.4: Randomly generated data, from three randomly selected centroids,
    plotted'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：从三个随机选择的质心生成的随机数据绘制图
- en: 'We define the function `closest_centroids()` to assign each point to the centroid
    it is closest to:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了函数 `closest_centroids()`，将每个点分配给离其最近的质心：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We create another function `move_centroids()`. It recalculates the centroids
    such that the sum of squared distances decreases:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了另一个函数 `move_centroids()`。它重新计算质心，使得平方距离的总和减少：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we call these two functions iteratively for 100 iterations. We have chosen
    the number of iterations arbitrarily; you can increase and decrease it to see
    the effect:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们迭代调用这两个函数 100 次。我们选择的迭代次数是任意的；你可以增加或减少迭代次数来观察效果：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let us now visualize how the centroids have changed after 100 iterations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们可视化质心在 100 次迭代后的变化：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In *Figure 7.5*, you can see the final centroids after 100 iterations. We have
    also colored the points based on which centroid they are closest to. The yellow
    points correspond to one cluster (nearest the cross in its center), and the same
    is true for the purple and green cluster points:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 7.5* 中，你可以看到经过 100 次迭代后的最终质心。我们还根据每个数据点距离哪个质心最近来为其上色。黄色的点对应一个簇（最接近其中心的交叉点），紫色和绿色的簇点也同样如此：
- en: '![](img/B18331_07_05.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_05.png)'
- en: 'Figure 7.5: Plot of the final centroids after 100 iterations'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：经过 100 次迭代后的最终质心绘图
- en: Please note that the `plot` command works in `Matplotlib 3.1.1` or higher versions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`plot` 命令在 `Matplotlib 3.1.1` 或更高版本中有效。
- en: 'In the preceding code, we decided to limit the number of clusters to three,
    but in most cases with unlabelled data, one is never sure how many clusters exist.
    One can determine the optimal number of clusters using the elbow method. The method
    is based on the principle that we should choose the cluster number that reduces
    the **sum of squared error** (**SSE**) distance. If *k* is the number of clusters,
    then as *k* increases, the SSE decreases, with SSE = 0; when *k* is equal to the
    number of data points, each point is its own cluster. It is clear we do not want
    this as our number of clusters, so when we plot the graph between SSE and the
    number of clusters, we should see a kink in the graph, like the elbow of the hand,
    which is how the method gets its name – the elbow method. The following code calculates
    the sum of squared errors for our data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们决定将簇的数量限制为三个，但在大多数未标记数据的情况下，通常无法确定存在多少个簇。我们可以通过肘部法则来确定最优的簇数。该方法的原理是，我们应该选择一个能减少**平方误差和**（**SSE**）距离的簇数。如果
    *k* 是簇的数量，那么随着 *k* 的增加，SSE 会减少，当 *k* 等于数据点的数量时，SSE 为 0；此时，每个点都是自己的簇。显然，我们不希望将簇的数量设为这个值，因此当我们绘制
    SSE 与簇数之间的图表时，我们应该看到图表中出现一个拐点，就像手肘的形状，这也就是该方法得名——肘部法则。以下代码计算了数据的平方误差和：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let us use the elbow method now for finding the optimum number of clusters
    for our dataset. To do that we will start with one cluster, that is, all points
    belonging to a single cluster, and increase the number of clusters sequentially.
    In the code, we increase the clusters by one, with eleven being the maximum number
    of clusters. For each cluster number value, we use the code above to find the
    centroids (and hence the clusters) and find the SSE:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用肘部法则来寻找数据集的最优簇数。为此，我们将从一个簇开始，也就是所有点都属于同一个簇，然后按顺序增加簇的数量。在代码中，我们每次增加一个簇，最多为十一簇。对于每个簇的数量，我们使用上述代码来找到质心（因此找到簇），并计算
    SSE：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Figure 7.6* shows the different cluster values for the dataset. The kink is
    clearly visible when the number of clusters is four:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.6* 显示了数据集的不同簇值。当簇的数量为四时，拐点非常明显：'
- en: '![](img/B18331_07_06.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_06.png)'
- en: 'Figure 7.6: Plotting SSE against the number of clusters'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：绘制 SSE 与簇数的关系图
- en: K-means clustering is very popular because it is fast, simple, and robust. It
    also has some disadvantages, the biggest being that the user has to specify the
    number of clusters. Second, the algorithm does not guarantee global optima; the
    results can change if the initial randomly chosen centroids change. Third, it
    is very sensitive to outliers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类非常流行，因为它快速、简单且稳健。但它也有一些缺点，最大的缺点是用户必须指定簇的数量。其次，该算法并不保证全局最优解；如果初始随机选择的质心发生变化，结果也可能会变化。第三，它对离群值非常敏感。
- en: Variations in k-means
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 的变种
- en: In the original k-means algorithm each point belongs to a specific cluster (centroid);
    this is called **hard clustering**. However, we can have one point belong to all
    the clusters, with a membership function defining how much it belongs to a particular
    cluster (centroid). This is called *fuzzy clustering* or *soft clustering*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 k-means 算法中，每个点都属于一个特定的簇（质心）；这被称为**硬聚类**。然而，我们可以让一个点同时属于所有簇，并通过隶属度函数来定义它属于某个特定簇（质心）的程度。这被称为*模糊聚类*或*软聚类*。
- en: This variation was proposed in 1973 by J. C. Dunn and later improved upon by
    J. C. Bezdek in 1981\. Though soft clustering takes longer to converge, it can
    be useful when a point is in multiple classes, or when we want to know how similar
    a given point is to different clusters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变体由 J. C. Dunn 于 1973 年提出，后来由 J. C. Bezdek 在 1981 年进行了改进。尽管软聚类收敛的时间较长，但当一个点属于多个类时，或者当我们想知道某个点与不同簇的相似度时，它是非常有用的。
- en: The accelerated k-means algorithm was created in 2003 by Charles Elkan. He exploited
    the triangle inequality relationship (that is, a straight line is the shortest
    distance between two points). Instead of just doing all distance calculations
    at each iteration, he also kept track of the lower and upper bounds for distances
    between points and centroids.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 加速的 k-means 算法是由 Charles Elkan 于 2003 年创建的。他利用了三角不等式关系（即直线是连接两点之间最短的距离）。他不仅在每次迭代时进行所有距离计算，还跟踪了点与质心之间的距离的上下限。
- en: In 2006, David Arthur and Sergei Vassilvitskii proposed the k-means++ algorithm.
    The major change they proposed was in the initialization of centroids. They showed
    that if we choose centroids that are distant from each other, then the k-means
    algorithm is less likely to converge on a suboptimal solution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，David Arthur和Sergei Vassilvitskii提出了k-means++算法。他们提出的主要改进是在质心的初始化上。他们表明，如果选择相距较远的质心，k-means算法就不太可能收敛到一个次优解。
- en: Another alternative can be that at each iteration we do not use the entire dataset,
    instead using mini-batches. This modification was proposed by David Sculey in
    2010\. Now, that we have covered PCA and k-means, we move toward an interesting
    network called self-organized network or winner-take-all units.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种替代方法是在每次迭代时不使用整个数据集，而是使用小批量数据。这一修改由David Sculey在2010年提出。现在，既然我们已经讲解了PCA和k-means，接下来我们将介绍一个有趣的网络——自组织网络（self-organized
    network）或胜者为王单元（winner-take-all units）。
- en: Self-organizing maps
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自组织映射
- en: Both k-means and PCA can cluster the input data; however, they do not maintain
    a topological relationship. In this section, we will consider **Self-Organizing
    Maps** (**SOMs**), sometimes known as **Kohonen networks** or **Winner-Take-All
    Units** (**WTUs**). They maintain the topological relation. SOMs are a very special
    kind of neural network, inspired by a distinctive feature of the human brain.
    In our brain, different sensory inputs are represented in a topologically ordered
    manner. Unlike other neural networks, neurons are not all connected to each other
    via weights; instead, they influence each other’s learning. The most important
    aspect of SOM is that neurons represent the learned inputs in a topographic manner.
    They were proposed by Teuvo Kohonen [7] in 1982.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: k-means和PCA都可以对输入数据进行聚类；然而，它们没有保持拓扑关系。在本节中，我们将讨论**自组织映射**（**SOMs**），有时也被称为**科洪能网络**（**Kohonen
    networks**）或**胜者为王单元**（**WTUs**）。它们保持拓扑关系。SOM是一种非常特殊的神经网络，灵感来源于人类大脑的一个独特特征。在我们的大脑中，不同的感官输入是以拓扑有序的方式表示的。与其他神经网络不同，神经元之间不是通过权重相互连接的；相反，它们通过相互影响来进行学习。SOM的最重要特点是，神经元以拓扑方式表示学习到的输入。它们由Teuvo
    Kohonen于1982年提出[7]。
- en: 'In SOMs, neurons are usually placed on the nodes of a (1D or 2D) lattice. Higher
    dimensions are also possible but are rarely used in practice. Each neuron in the
    lattice is connected to all the input units via a weight matrix. *Figure 7.7*
    shows a SOM with 6 × 8 (48 neurons) and 5 inputs. For clarity, only the weight
    vectors connecting all inputs to one neuron are shown. In this case, each neuron
    will have seven elements, resulting in a combined weight matrix of size 40 × 5:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在自组织映射（SOM）中，神经元通常放置在（1D或2D）格点的节点上。虽然也可以使用更高维度，但在实际应用中很少使用。格点中的每个神经元都通过权重矩阵与所有输入单元相连。*图7.7*展示了一个具有6
    × 8（48个神经元）和5个输入的SOM。为了简洁起见，图中只显示了连接所有输入到一个神经元的权重向量。在这种情况下，每个神经元将有七个元素，从而形成一个大小为40
    × 5的组合权重矩阵：
- en: '![](img/B18331_07_07.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_07.png)'
- en: 'Figure 7.7: A self-organized map with 5 inputs and 48 neurons'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：一个具有5个输入和48个神经元的自组织映射
- en: A SOM learns via competitive learning. It can be considered as a nonlinear generalization
    of PCA and, thus, like PCA, can be employed for dimensionality reduction.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SOM通过竞争学习来学习。它可以看作是PCA的非线性推广，因此，像PCA一样，SOM也可以用于降维。
- en: 'In order to implement SOM, let’s first understand how it works. As a first
    step, the weights of the network are initialized either to some random value or
    by taking random samples from the input. Each neuron occupying a space in the
    lattice will be assigned specific locations. Now as an input is presented, the
    neuron with the least distance from the input is declared the winner (WTU). This
    is done by measuring the distance between the weight vectors (*W*) and input vectors
    (*X*) of all neurons:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现SOM，我们首先需要理解其工作原理。第一步是将网络的权重初始化为某个随机值，或者从输入中随机抽取样本。每个占据格点空间的神经元将被分配特定的位置。现在，当输入被呈现时，与输入距离最小的神经元被宣告为胜者（WTU）。这一过程是通过测量所有神经元的权重向量（*W*）和输入向量（*X*）之间的距离来实现的：
- en: '![](img/B18331_07_007.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_007.png)'
- en: Here, *d*[j] is the distance of the weights of neuron *j* from input *X*. The
    neuron with the lowest *d* value is the winner.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d*[j]是神经元*j*的权重与输入*X*之间的距离。具有最小*d*值的神经元就是胜者。
- en: Next, the weights of the winning neuron and its neighboring neurons are adjusted
    in a manner to ensure that the same neuron is the winner if the same input is
    presented next time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，胜者神经元及其邻近神经元的权重将进行调整，确保下次如果相同的输入被呈现时，同一神经元仍然是胜者。
- en: 'To decide which neighboring neurons need to be modified, the network uses a
    neighborhood function ![](img/B18331_07_008.png); normally, the Gaussian Mexican
    hat function is chosen as a neighborhood function. The neighborhood function is
    mathematically represented as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定哪些邻近神经元需要被修改，网络使用一个邻域函数 ![](img/B18331_07_008.png)；通常，高斯墨西哥帽函数被选作邻域函数。邻域函数在数学上表示如下：
- en: '![](img/B18331_07_009.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_009.png)'
- en: 'Here, ![](img/B18331_07_010.png) is a time-dependent radius of the influence
    of a neuron and *d* is its distance from the winning neuron. Graphically, the
    function looks like a hat (hence its name), as you can see in *Figure 7.8*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B18331_07_010.png)是神经元影响半径的时间依赖性，*d*是神经元与胜出神经元的距离。从图形上看，该函数像一顶帽子（因此得名），如*图7.8*所示：
- en: '![](img/B18331_07_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_08.png)'
- en: 'Figure 7.8: The “Gaussian Mexican hat” function, visualized in graph form'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：以图形形式展示的“高斯墨西哥帽”函数
- en: Another important property of the neighborhood function is that its radius reduces
    with time. As a result, in the beginning, many neighboring neurons’ weights are
    modified, but as the network learns, eventually a few neurons’ weights (at times,
    only one or none) are modified in the learning process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域函数的另一个重要特性是其半径随着时间的推移而减小。因此，在开始时，许多邻近神经元的权重会被修改，但随着网络的学习，最终只有少数神经元的权重（有时，甚至只有一个或没有）会在学习过程中被修改。
- en: 'The change in weight is given by the following equation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的变化由以下方程给出：
- en: '![](img/B18331_07_011.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_011.png)'
- en: The process is repeated for all the inputs for a given number of iterations.
    As the iterations progress, we reduce the learning rate and the radius by a factor
    dependent on the iteration number.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会对所有输入进行多次迭代。随着迭代的进行，我们会根据迭代次数逐步减少学习率和半径。
- en: SOMs are computationally expensive and thus are not really useful for very large
    datasets. Still, they are easy to understand, and they can very nicely find the
    similarity between input data. Thus, they have been employed for image segmentation
    and to determine word similarity maps in NLP.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: SOM计算开销较大，因此对于非常大的数据集并不实用。不过，它们易于理解，并且能够很好地发现输入数据之间的相似性。因此，它们已被用于图像分割和确定自然语言处理中的词相似性映射。
- en: Colour mapping using a SOM
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SOM进行颜色映射
- en: 'Some of the interesting properties of the feature map of the input space generated
    by a SOM are:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SOM生成的输入空间特征图的一些有趣属性包括：
- en: The feature map provides a good representation of the input space. This property
    can be used to perform vector quantization so that we may have a continuous input
    space, and using a SOM we can represent it in a discrete output space.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征图提供了输入空间的良好表示。这一特性可以用于执行向量量化，从而使我们能够拥有连续的输入空间，通过使用SOM，我们可以将其表示为离散的输出空间。
- en: The feature map is topologically ordered, that is, the spatial location of a
    neuron in the output lattice corresponds to a particular feature of the input.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征图是拓扑有序的，即输出格点中神经元的空间位置对应输入的特定特征。
- en: The feature map also reflects the statistical distribution of the input space;
    the domain that has the largest number of input samples gets a wider area in the
    feature map.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征图还反映了输入空间的统计分布；拥有最多输入样本的领域在特征图中会占据更大的区域。
- en: 'These features of SOM make them the natural choice for many interesting applications.
    Here we use SOM for clustering a range of given R, G, and B pixel values to a
    corresponding color map. We start with the importing of modules:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: SOM的这些特性使其成为许多有趣应用的自然选择。这里，我们使用SOM将一系列给定的R、G、B像素值聚类到相应的颜色映射中。我们从导入模块开始：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The main component of the code is our class `WTU`. The class `__init__` function
    initializes various hyperparameters of our SOM, the dimensions of our 2D lattice
    (`m, n`), the number of features in the input (`dim`), the neighborhood radius
    (`sigma`), the initial weights, and the topographic information:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的主要部分是我们的类`WTU`。`__init__`函数初始化了SOM的各种超参数，包括我们2D格点的维度（`m, n`）、输入中的特征数量（`dim`）、邻域半径（`sigma`）、初始权重以及拓扑信息：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The most important function of the class is the `training()` function, where
    we use the Kohonen algorithm as discussed before to find the winner units and
    then update the weights based on the neighborhood function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的最重要功能是`training()`函数，我们在其中使用之前讨论过的Kohonen算法来找到胜出单元，然后基于邻域函数更新权重：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `fit()` function is a helper function that calls the `training()` function
    and stores the centroid grid for easy retrieval:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 函数是一个辅助函数，它调用 `training()` 函数并存储质心网格，以便于后续检索：'
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then there are some more helper functions to find the winner and generate a
    2D lattice of neurons, and a function to map input vectors to the corresponding
    neurons in the 2D lattice:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一些更多的辅助函数来找到胜者并生成一个二维神经元格，还有一个将输入向量映射到二维格中相应神经元的函数：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will also need to normalize the input data, so we create a function to do
    so:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对输入数据进行归一化处理，因此我们创建了一个函数来实现这一操作：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let us read the data. The data contains red, green, and blue channel values
    for different colors. Let us normalize them:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取数据。数据包含不同颜色的红色、绿色和蓝色通道值。让我们对它们进行归一化处理：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let us create our SOM and fit it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的自组织映射（SOM）并进行拟合：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The fit function takes slightly longer to run, since our code is not optimized
    for performance but for explaining the concept. Now, let’s look at the result
    of the trained model. Let us run the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合函数运行稍微长一些，因为我们的代码并没有针对性能优化，而是为了说明概念。现在，让我们看看训练模型的结果。让我们运行以下代码：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can see the color map in the 2D neuron lattice:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到二维神经元格中的彩色图：
- en: '![](img/B18331_07_09.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_09.png)'
- en: 'Figure 7.9: A plotted color map of the 2D neuron lattice'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：二维神经元格的彩色映射图
- en: You can see that neurons that win for similar colors are closely placed. Next,
    we move to an interesting architecture, the restricted Boltzmann machines.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，对于相似颜色的神经元，它们会被紧密地放置在一起。接下来，我们进入一个有趣的架构——限制玻尔兹曼机（RBM）。
- en: Restricted Boltzmann machines
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机（RBM）
- en: The RBM is a two-layered neural network—the first layer is called the **visible
    layer** and the second layer is called the **hidden layer**. They are called **shallow
    neural networks** because they are only two layers deep. They were first proposed
    in 1986 by Paul Smolensky (he called them Harmony Networks [1]) and later by Geoffrey
    Hinton who in 2006 proposed **Contrastive Divergence** (**CD**) as a method to
    train them. All neurons in the visible layer are connected to all the neurons
    in the hidden layer, but there is a **restriction**—no neuron in the same layer
    can be connected. All neurons in the RBM are binary by nature; they will either
    fire or not fire.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 是一个两层的神经网络——第一层称为 **可见层**，第二层称为 **隐藏层**。它们被称为 **浅层神经网络**，因为它们只有两层深。最早由 Paul
    Smolensky 于1986年提出（他称其为和谐网络 [1]），后由 Geoffrey Hinton 在2006年提出 **对比散度**（**CD**）作为训练方法。可见层的所有神经元都与隐藏层的所有神经元相连，但存在一个
    **限制**——同一层中的神经元不能相连。RBM 中的所有神经元本质上是二值的；它们要么激活，要么不激活。
- en: 'RBMs can be used for dimensionality reduction, feature extraction, and collaborative
    filtering. The training of RBMs can be divided into three parts: forward pass,
    backward pass, and then a comparison.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 可以用于降维、特征提取和协同过滤。RBM 的训练可以分为三个部分：前向传播、反向传播，然后进行比较。
- en: 'Let us delve deeper into the math. We can divide the operation of RBMs into
    two passes:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地研究一下数学原理。我们可以将 RBM 的操作分为两次传播：
- en: '**Forward pass**: The information at visible units (*V*) is passed via weights
    (*W*) and biases (*c*) to the hidden units (*h*[0]). The hidden unit may fire
    or not depending on the stochastic probability (![](img/B18331_07_010.png) is
    the stochastic probability), which is basically the sigmoid function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向传播**：可见单元（*V*）的信息通过权重（*W*）和偏置（*c*）传递到隐藏单元（*h*[0]）。隐藏单元是否激活取决于随机概率（![](img/B18331_07_010.png)
    是随机概率），该概率基本上是一个 Sigmoid 函数：'
- en: '![](img/B18331_07_013.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_013.png)'
- en: '**Backward pass**: The hidden unit representation (*h*[0]) is then passed back
    to the visible units through the same weights, *W*, but a different bias, *c*,
    where the model reconstructs the input. Again, the input is sampled:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**：然后，隐藏单元表示（*h*[0]）通过相同的权重 *W* 传回可见单元，但使用不同的偏置 *c*，此时模型重建输入。同样，输入会被采样：'
- en: '![](img/B18331_07_014.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_014.png)'
- en: These two passes are repeated for *k* steps or until the convergence [4] is
    reached. According to researchers, *k=1* gives good results, so we will keep *k
    = 1*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这两次传播会重复 *k* 步骤，或者直到收敛[4]达到为止。根据研究人员的说法，*k=1* 已经能够得到良好的结果，所以我们将设置 *k = 1*。
- en: 'The joint configuration of the visible vector *V* and the hidden vector *h*
    has energy given as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 可见向量 *V* 和隐藏向量 *h* 的联合配置具有如下能量：
- en: '![](img/B18331_07_015.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_015.png)'
- en: 'Also associated with each visible vector *V* is free energy, the energy that
    a single configuration would need to have in order to have the same probability
    as all of the configurations that contain *V*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 每个可见向量 *V* 还与自由能相关，自由能是指某一配置所需的能量，使其与所有包含 *V* 的配置具有相同的概率：
- en: '![](img/B18331_07_016.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_016.png)'
- en: 'Using the contrastive divergence objective function, that is, *Mean(F(V*[original]*))
    - Mean(F(V*[reconstructed]*))*, the change in weights is given by:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对比散度目标函数，即 *Mean(F(V*[original]*)) - Mean(F(V*[reconstructed]*))*，权重的变化由以下公式给出：
- en: '![](img/B18331_07_017.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_017.png)'
- en: Here, ![](img/B18331_01_025.png) is the learning rate. Similar expressions exist
    for the biases *b* and *c*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B18331_01_025.png) 是学习率。对偏置 *b* 和 *c* 也存在类似的表达式。
- en: Reconstructing images using an RBM
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RBM 进行图像重建
- en: 'Let us build an RBM in TensorFlow. The RBM will be designed to reconstruct
    handwritten digits. This is the first generative model that you are learning;
    in the upcoming chapters, we will learn a few more. We import the TensorFlow,
    NumPy, and Matplotlib libraries:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 TensorFlow 中构建一个 RBM。这个 RBM 将被设计用来重建手写数字。这是你学习的第一个生成模型；在接下来的章节中，我们还会学习一些其他的生成模型。我们导入
    TensorFlow、NumPy 和 Matplotlib 库：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We define a class `RBM`. The class `__init_()` function initializes the number
    of neurons in the visible layer (`input_size`) and the number of neurons in the
    hidden layer (`output_size`). The function initializes the weights and biases
    for both hidden and visible layers. In the following code, we have initialized
    them to zero. You can try with random initialization as well:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个类 `RBM`。该类的 `__init_()` 函数初始化了可见层（`input_size`）和隐藏层（`output_size`）中的神经元数量。该函数初始化了隐藏层和可见层的权重和偏置。在下面的代码中，我们将它们初始化为零。你也可以尝试使用随机初始化：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We define methods to provide the forward and backward passes:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了前向和后向传播的函数：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We create a function to generate random binary values. This is because both
    hidden and visible units are updated using stochastic probability, depending upon
    the input to each unit in the case of the hidden layer (and the top-down input
    to visible layers):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个函数来生成随机二进制值。这是因为隐藏单元和可见单元的更新是通过随机概率进行的，具体取决于每个单元的输入（对于隐藏层是每个单元的输入，而对于可见层是自上而下的输入）：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will need functions to reconstruct the input:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些函数来重建输入：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To train the RBM created we define the `train()` function. The function calculates
    the positive and negative gradient terms of contrastive divergence and uses the
    weight update equation to update the weights and biases:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练创建的 RBM，我们定义了 `train()` 函数。该函数计算对比散度的正负梯度项，并使用权重更新公式来更新权重和偏置：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that our class is ready, we instantiate an object of `RBM` and train it
    on the MNIST dataset:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的类已经准备好，我们实例化一个 `RBM` 对象，并在 MNIST 数据集上对其进行训练：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let us plot the learning curve:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制学习曲线：
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the figure below, you can see the learning curve of our RBM:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到我们 RBM 的学习曲线：
- en: '![](img/B18331_07_10.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_10.png)'
- en: 'Figure 7.10: Learning curve for the RBM model'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：RBM 模型的学习曲线
- en: 'Now, we present the code to visualize the reconstructed images:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们展示了用于可视化重建图像的代码：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And the reconstructed images:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以及重建后的图像：
- en: '![](img/B18331_07_11.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_07_11.png)'
- en: 'Figure 7.11: Image reconstruction using an RBM'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：使用 RBM 进行图像重建
- en: The top row is the input handwritten image, and the bottom row is the reconstructed
    image. You can see that the images look remarkably similar to the human handwritten
    digits. In the upcoming chapters, you will learn about models that can generate
    even more complex images such as artificial human faces.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 上排是输入的手写图像，下排是重建的图像。你可以看到这些图像与人类手写的数字非常相似。在接下来的章节中，你将学习可以生成更复杂图像的模型，例如人工人脸。
- en: Deep belief networks
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: Now that we have a good understanding of RBMs and know how to train them using
    contrastive divergence, we can move toward the first successful deep neural network
    architecture, the **deep belief networks** (**DBNs**), proposed in 2006 by Hinton
    and his team in the paper *A fast learning algorithm for deep belief nets*. Before
    this model it was very difficult to train deep architectures, not just because
    of the limited computing resources, but also, as will be discussed in *Chapter
    8*, *Autoencoders*, because of the vanishing gradient problem. In DBNs it was
    first demonstrated how deep architectures can be trained via greedy layer-wise
    training.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对限制玻尔兹曼机（RBMs）有了深入了解，并知道如何通过对比散度进行训练，我们可以继续研究2006年Hinton及其团队提出的第一个成功的深度神经网络架构——**深度信念网络**（**DBNs**），该内容见于论文*深度信念网络的快速学习算法*。在这个模型之前，训练深度架构是非常困难的，不仅仅是因为计算资源有限，还因为正如*第8章*《*自编码器*》中所讨论的那样，存在消失梯度问题。在DBNs中，首次展示了如何通过贪心的逐层训练来训练深度架构。
- en: In the simplest terms, DBNs are just stacked RBMs. Each RBM is trained separately
    using the contrastive divergence. We start with the training of the first RBM
    layer. Once it is trained, we train the second RBM layer. The visible units of
    the second RBM are now fed the output of the hidden units of the first RBM, when
    it is fed the input data. The procedure is repeated with each RBM layer addition.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 用最简单的话来说，DBNs就是堆叠的RBM。每个RBM都是通过对比散度单独训练的。我们从第一个RBM层的训练开始，一旦它训练完成，我们就训练第二个RBM层。第二个RBM的可见单元现在接收到第一个RBM的隐藏单元的输出，当它接收到输入数据时。这个过程在每个RBM层增加时重复进行。
- en: 'Let us try stacking our `RBM` class. To make the DBN, we will need to define
    one more function in the `RBM` class; the output of the hidden unit of one RBM
    needs to feed into the next RBM:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试堆叠我们的`RBM`类。为了构建DBN，我们需要在`RBM`类中定义一个函数；一个RBM的隐藏单元的输出需要传递给下一个RBM：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we can just use the `RBM` class to create a stacked RBM structure. In the
    following code we create an RBM stack: the first RBM will have 500 hidden units,
    the second will have 200 hidden units, and the third will have 50 hidden units:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以直接使用`RBM`类来创建堆叠的RBM结构。在以下代码中，我们创建一个RBM堆叠：第一个RBM将有500个隐藏单元，第二个RBM有200个隐藏单元，第三个RBM有50个隐藏单元：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For the first RBM, the MNIST data is the input. The output of the first RBM
    is then fed as input to the second RBM, and so on through the consecutive RBM
    layers:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个RBM，MNIST数据是输入。第一个RBM的输出被作为输入传递给第二个RBM，依此类推，直到通过连续的RBM层：
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Our DBN is ready. The three stacked RBMs are now trained using unsupervised
    learning. DBNs can also be trained using supervised training. To do so we need
    to fine-tune the weights of the trained RBMs and add a fully connected layer at
    the end. In their publication *Classification with Deep Belief Networks*, Hebbo
    and Kim show how they used a DBN for MNIST classification; it is a good introduction
    to the subject.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的DBN已经准备好了。这三个堆叠的RBM现在通过无监督学习进行了训练。DBNs也可以通过监督学习进行训练。为此，我们需要微调已训练RBM的权重，并在最后添加一个完全连接层。在他们的论文《*使用深度信念网络进行分类*》中，Hebbo和Kim展示了他们如何使用DBN进行MNIST分类；这是一个很好的入门介绍。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the major unsupervised learning algorithms. We went
    through algorithms best suited for dimension reduction, clustering, and image
    reconstruction. We started with the dimension reduction algorithm PCA, then we
    performed clustering using k-means and self-organized maps. After this we studied
    the restricted Boltzmann machine and saw how we can use it for both dimension
    reduction and image reconstruction. Next, we delved into stacked RBMs, that is,
    deep belief networks, and we trained a DBN consisting of three RBM layers on the
    MNIST dataset.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了主要的无监督学习算法。我们探讨了最适合于降维、聚类和图像重建的算法。我们首先讲解了降维算法PCA，然后使用k-means和自组织映射进行聚类。接着，我们研究了限制玻尔兹曼机，并看到了如何将其应用于降维和图像重建。接下来，我们深入探讨了堆叠RBM，即深度信念网络，并在MNIST数据集上训练了一个由三层RBM组成的DBN。
- en: In the next chapter, we will explore another model using an unsupervised learning
    paradigm – autoencoders.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探索另一种使用无监督学习范式的模型——自编码器。
- en: References
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Smith, Lindsay. (2006). *A tutorial on Principal Component Analysis*: [http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Smith, Lindsay. (2006). *主成分分析教程*：[http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)
- en: 'Movellan, J. R. *Tutorial on Principal component Analysis*: [http://mplab.ucsd.edu/tutorials/pca.pdf](http://mplab.ucsd.edu/tutorials/pca.pdf)'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Movellan, J. R. *主成分分析教程*：[http://mplab.ucsd.edu/tutorials/pca.pdf](http://mplab.ucsd.edu/tutorials/pca.pdf)
- en: 'TensorFlow Projector: [http://projector.tensorflow.org/](http://projector.tensorflow.org/)'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 投影器：[http://projector.tensorflow.org/](http://projector.tensorflow.org/)
- en: '**Singular Value Decomposition** (**SVD**) tutorial. MIT: [https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）教程。MIT：[https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)'
- en: 'Shlens, Jonathon. (2014). *A tutorial on principal component analysis*. arXiv
    preprint arXiv:1404.1100: [https://arxiv.org/abs/1404.1100](https://arxiv.org/abs/1404.1100)'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shlens, Jonathon. (2014). *主成分分析教程*。arXiv 预印本 arXiv:1404.1100：[https://arxiv.org/abs/1404.1100](https://arxiv.org/abs/1404.1100)
- en: 'Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep learning*. MIT
    press: [https://www.deeplearningbook.org](https://www.deeplearningbook.org)'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow, I., Bengio, Y., 和 Courville, A. (2016). *深度学习*。MIT出版社：[https://www.deeplearningbook.org](https://www.deeplearningbook.org)
- en: 'Kohonen, T. (1982). *Self-organized formation of topologically correct feature
    maps*. Biological cybernetics 43, no. 1: 59-69.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kohonen, T. (1982). *自组织形成拓扑正确的特征图*。生物控制论 43，第 1 期：59-69。
- en: 'Kanungo, Tapas, et al. (2002). *An Efficient k-Means Clustering Algorithm:
    Analysis and Implementation*. IEEE transactions on pattern analysis and machine
    intelligence 24.7: 881-892.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kanungo, Tapas 等人 (2002). *一种高效的 k-均值聚类算法：分析与实现*。IEEE模式分析与机器智能学报 24.7：881-892。
- en: 'Ortega, Joaquín Pérez, et al. *Research issues on K-means Algorithm: An Experimental
    Trial Using Matlab*. CEUR Workshop Proceedings: Semantic Web and New Technologies.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ortega, Joaquín Pérez 等人。*关于 K-均值算法的研究问题：使用 Matlab 的实验试验*。CEUR工作坊论文集：语义网与新技术。
- en: 'Chen, K. (2009). *On Coresets for k-Median and k-Means Clustering in Metric
    and Euclidean Spaces and Their Applications.* SIAM Journal on Computing 39.3:
    923-947.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen, K. (2009). *关于度量空间和欧几里得空间中 k-中值和 k-均值聚类的核心集及其应用*。SIAM计算学报 39.3：923-947。
- en: '*Determining the number of clusters in a data set*: [https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*确定数据集中的聚类数目*：[https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)'
- en: 'Lloyd, S. P. (1982). *Least Squares Quantization in PCM*: [http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf](http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf)'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lloyd, S. P. (1982). *PCM 中的最小二乘量化*：[http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf](http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf)
- en: 'Dunn, J. C. (1973-01-01). *A Fuzzy Relative of the ISODATA Process and Its
    Use in Detecting Compact Well-Separated Clusters*. Journal of Cybernetics. 3(3):
    32–57.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dunn, J. C. (1973-01-01). *ISODATA 过程的模糊相对物及其在检测紧凑且分离良好聚类中的应用*。控制论学报。3(3)：32-57。
- en: Bezdek, James C. (1981). *Pattern Recognition with Fuzzy Objective Function
    Algorithms*.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bezdek, James C. (1981). *具有模糊目标函数算法的模式识别*。
- en: 'Peters, G., Crespo, F., Lingras, P., and Weber, R. (2013). *Soft clustering–Fuzzy
    and rough approaches and their extensions and derivatives*. International Journal
    of Approximate Reasoning 54, no. 2: 307-322.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Peters, G., Crespo, F., Lingras, P., 和 Weber, R. (2013). *软聚类–模糊与粗糙方法及其扩展与派生*。国际近似推理杂志
    54，第 2 期：307-322。
- en: Sculley, D. (2010). *Web-scale k-means clustering*. In Proceedings of the 19th
    international conference on World wide web, pp. 1177-1178\. ACM.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sculley, D. (2010). *Web规模 k-均值聚类*。第19届国际万维网大会论文集，pp. 1177-1178。ACM。
- en: 'Smolensky, P. (1986). *Information Processing in Dynamical Systems: Foundations
    of Harmony Theory*. No. CU-CS-321-86\. COLORADO UNIV AT BOULDER DEPT OF COMPUTER
    SCIENCE.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Smolensky, P. (1986). *动态系统中的信息处理：和谐理论的基础*。编号 CU-CS-321-86。科罗拉多大学博尔德分校计算机科学系。
- en: Salakhutdinov, R., Mnih, A., and Hinton, G. (2007). *Restricted Boltzmann Machines
    for Collaborative Filtering*. Proceedings of the 24th international conference
    on Machine learning. ACM.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Salakhutdinov, R., Mnih, A., 和 Hinton, G. (2007). *限制玻尔兹曼机在协同过滤中的应用*。第24届国际机器学习会议论文集。ACM。
- en: 'Hinton, G. (2010). *A Practical Guide to Training Restricted Boltzmann Machines*.
    Momentum 9.1: 926.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, G. (2010). *训练限制玻尔兹曼机的实用指南*。Momentum 9.1：926。
- en: Join our book’s Discord space
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相遇，并与超过 2000 名成员一起学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
