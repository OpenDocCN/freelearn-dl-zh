- en: '3\. Real-World Deep Learning: Evaluating the Bitcoin Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 实际深度学习：评估比特币模型
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter focuses on how to evaluate a neural network model. We'll modify
    the network's hyperparameters to improve its performance. However, before altering
    any parameters, we need to measure how the model performs. By the end of this
    chapter, you will be able to evaluate a model using different functions and techniques.
    You will also learn about hypermeter optimization by implementing functions and
    regularization strategies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讲解如何评估神经网络模型。我们将修改网络的超参数以提升其性能。然而，在更改任何参数之前，我们需要先衡量模型的表现。在本章结束时，您将能够使用不同的函数和技术评估模型。您还将通过实现函数和正则化策略，学习超参数优化。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapter, you trained your model. But how will you check its
    performance and whether it is performing well or not? Let''s find out by evaluating
    a model. In machine learning, it is common to define two distinct terms: **parameter**
    and **hyperparameter**. Parameters are properties that affect how a model makes
    predictions from data, say from a particular dataset. Hyperparameters refer to
    how a model learns from data. Parameters can be learned from the data and modified
    dynamically. Hyperparameters, on the other hand, are higher-level properties defined
    before the training begins and are not typically learned from data. In this chapter,
    you will learn about these factors in detail and understand how to use them with
    different evaluation techniques to improve the performance of a model.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您已经训练了您的模型。那么，您如何检查它的性能以及它是否表现良好呢？让我们通过评估模型来找出答案。在机器学习中，通常会定义两个不同的术语：**参数**和**超参数**。参数是影响模型如何从数据中做出预测的属性，比如来自特定数据集的预测。超参数则是模型如何从数据中学习的方式。参数可以从数据中学习并动态修改。而超参数则是在训练开始之前定义的高层次属性，通常不会从数据中学习。在本章中，您将详细了解这些因素，并理解如何使用它们与不同的评估技术相结合，以提高模型的性能。
- en: Note
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: For a more detailed overview of machine learning, refer to *Python Machine Learning*,
    *Sebastian Raschka and Vahid Mirjalili, Packt Publishing, 2017)*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更详细的机器学习概述，请参阅 *Python Machine Learning*（Sebastian Raschka 和 Vahid Mirjalili
    著，Packt Publishing，2017 年）。
- en: Problem Categories
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题类别
- en: 'Generally, there are two categories of problems that can be solved by neural
    networks: **classification** and **regression**. Classification problems concern
    the prediction of the right categories from data—for instance, whether the temperature
    is hot or cold. Regression problems are about the prediction of values in a continuous
    scalar—for instance, what the actual temperature value is.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络可以解决两类问题：**分类**和**回归**。分类问题关注从数据中预测正确的类别——例如，温度是热还是冷。回归问题则是关于预测连续标量中的值——例如，实际的温度值。
- en: 'The problems in these two categories are characterized by the following properties:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这两类问题的特点如下：
- en: '`yes` or `no`. However, they must be clearly assigned to each data element.
    An example of a classification problem would be to assign either the `car` or
    `not a car` labels to an image using a convolutional neural network. The MNIST
    example we explored in *Chapter 1*, *Introduction to Neural Networks and Deep
    Learning*, is another example of a classification problem.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yes` 或 `no`。然而，它们必须清楚地分配给每个数据元素。分类问题的一个例子是使用卷积神经网络将图像标记为 `car` 或 `not a car`。我们在《第一章：神经网络与深度学习概述》中探索的
    MNIST 示例也是一个分类问题的例子。'
- en: '**Regression**: These are problems that are characterized by a continuous variable
    (that is, a scalar). They are measured in terms of ranges, and their evaluations
    consider how close to the real values the network is. An example is a time-series
    classification problem in which a recurrent neural network is used to predict
    the future temperature values. The Bitcoin price-prediction problem is another
    example of a regression problem.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：这些问题的特点是具有连续变量（即标量）。它们按范围进行度量，其评估考虑网络与实际值的接近程度。一个例子是时间序列分类问题，其中使用递归神经网络预测未来的温度值。比特币价格预测问题是另一个回归问题的例子。'
- en: While the overall structure of how to evaluate these models is the same for
    both of these problem categories, we employ different techniques to evaluate how
    models perform. In the next section, we'll explore these techniques for either
    classification or regression problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两类问题的模型评估结构相同，但我们使用不同的技术来评估模型的表现。在接下来的部分，我们将探讨这些评估分类或回归问题的技术。
- en: Loss Functions, Accuracy, and Error Rates
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数、准确率与误差率
- en: Neural networks utilize functions that measure how the networks perform compared
    to a **validation set**—that is, a part of the data that is kept separate to be
    used as part of the training process. These functions are called **loss functions**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用衡量网络与**验证集**表现的函数——即，保留出来的部分数据，在训练过程中作为验证使用。这些函数被称为**损失函数**。
- en: Loss functions evaluate how wrong a neural network's predictions are. Then,
    they propagate those errors back and make adjustments to the network, modifying
    how individual neurons are activated. Loss functions are key components of neural
    networks, and choosing the right loss function can have a significant impact on
    how the network performs. Errors are propagated through a process called **backpropagation**,
    which is a technique for propagating the errors that are returned by the loss
    function to each neuron in a neural network. Propagated errors affect how neurons
    activate, and ultimately, how they influence the output of that network.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数评估神经网络预测的错误程度。然后，它们将这些错误反向传播并调整网络，改变单个神经元的激活方式。损失函数是神经网络的关键组成部分，选择正确的损失函数会对网络的表现产生重大影响。误差通过一种叫做**反向传播**的过程传播，这是一种将损失函数返回的误差传播到神经网络中每个神经元的技术。传播的误差影响神经元的激活方式，最终影响网络输出的结果。
- en: Many neural network packages, including Keras, use this technique by default.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经网络框架，包括Keras，默认使用这一技术。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information about the mathematics of backpropagation, please refer
    to *Deep Learning* by *Ian Goodfellow et. al., MIT Press, 2016*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有关反向传播数学原理的更多信息，请参考*《深度学习》*，*Ian Goodfellow 等著，MIT出版社，2016年*。
- en: We use different loss functions for regression and classification problems.
    For classification problems, we use accuracy functions (that is, the proportion
    of the number of times the predictions were correct to the number of times predictions
    were made). For example, if you predict a toss of a coin that will result in *m*
    times as heads when you toss it *n* times and your prediction is correct, then
    the accuracy will be calculated as *m/n*. For regression problems, we use error
    rates (that is, how close the predicted values were to the observed ones).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归和分类问题，我们使用不同的损失函数。对于分类问题，我们使用准确率函数（即，预测正确的次数与总预测次数的比例）。例如，如果你预测抛硬币的结果，当抛掷
    *n* 次时，其中会出现 *m* 次正面，且预测正确，那么准确率将计算为 *m/n*。对于回归问题，我们使用误差率（即，预测值与观察值的接近程度）。
- en: 'Here''s a summary of common loss functions that can be utilized, alongside
    their common applications:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见的损失函数总结及其常见应用：
- en: '![Figure 3.1: Common loss functions used for classification and regression
    problems'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1：用于分类和回归问题的常见损失函数](img/B15911_03_01.jpg)'
- en: '](img/B15911_03_01.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_01.jpg)'
- en: 'Figure 3.1: Common loss functions used for classification and regression problems'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：用于分类和回归问题的常见损失函数
- en: For regression problems, the MSE function is the most common choice, while for
    classification problems, binary cross-entropy (for binary category problems) and
    categorical cross-entropy (for multi-category problems) are common choices. It
    is advised to start with these loss functions, then experiment with other functions
    as you evolve your neural network, aiming to gain performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，MSE函数是最常用的选择；而对于分类问题，二元交叉熵（适用于二分类问题）和多元交叉熵（适用于多分类问题）是常见的选择。建议从这些损失函数开始，然后在神经网络演变的过程中，尝试其他函数，以期提升性能。
- en: 'The network we developed in *Chapter 2*, *Real-World Deep Learning with TensorFlow
    and Keras: Predicting the Price of Bitcoin*, uses MSE as its loss function. In
    the next section, we''ll explore how that function performs as the network trains.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第二章*中开发的网络，*《用 TensorFlow 和 Keras 进行真实世界的深度学习：预测比特币的价格》*，使用均方误差（MSE）作为损失函数。在接下来的部分，我们将探讨该函数在网络训练过程中的表现。
- en: Different Loss Functions, Same Architecture
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的损失函数，相同的架构
- en: Before moving ahead to the next section, let's explore, in practical terms,
    how these problems are different in the context of neural networks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一部分之前，让我们从实际操作的角度探讨一下这些问题在神经网络中的不同之处。
- en: 'The *TensorFlow Playground* ([https://playground.tensorflow.org/](https://playground.tensorflow.org/))
    application has been made available by the TensorFlow team to help us understand
    how neural networks work. Here, we can see a neural network represented with its
    layers: input (on the left), hidden layers (in the middle), and output (on the
    right).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*TensorFlow Playground*（[https://playground.tensorflow.org/](https://playground.tensorflow.org/)）应用程序是由TensorFlow团队提供的，旨在帮助我们理解神经网络是如何工作的。在这里，我们可以看到一个由输入层（左侧）、隐藏层（中间）和输出层（右侧）组成的神经网络。'
- en: 'Note:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注：
- en: 'These images can be viewed in the repository on GitHub at: [https://packt.live/2Cl1t0H](https://packt.live/2Cl1t0H).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像可以在GitHub上的仓库中查看：[https://packt.live/2Cl1t0H](https://packt.live/2Cl1t0H)。
- en: 'We can also choose different sample datasets to experiment with on the far
    left-hand side. And, finally, on the far right-hand side, we can see the output
    of the network:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择不同的示例数据集进行实验，这些数据集位于最左侧。最后，在最右侧，我们可以看到网络的输出：
- en: '![Figure 3.2: TensorFlow Playground web application'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2：TensorFlow Playground网络应用程序'
- en: '](img/B15911_03_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_02.jpg)'
- en: 'Figure 3.2: TensorFlow Playground web application'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：TensorFlow Playground网络应用程序
- en: 'Take the parameters for a neural network shown in this visualization to gain
    an idea of how each parameter affects the model''s results. This application helps
    us explore the different problem categories we discussed in the previous section.
    When we choose `Regression` (upper right-hand corner), the colors of the dots
    are colored in a range of color values between orange and blue:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看此可视化中的神经网络参数，您可以了解每个参数如何影响模型的结果。此应用程序帮助我们探索上一部分中讨论的不同问题类别。当我们选择`回归`（右上角）时，点的颜色会在橙色和蓝色之间的颜色范围内变化：
- en: '![Figure 3.3: Regression problem example in TensorFlow Playground'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3：TensorFlow Playground中的回归问题示例'
- en: '](img/B15911_03_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_03.jpg)'
- en: 'Figure 3.3: Regression problem example in TensorFlow Playground'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：TensorFlow Playground中的回归问题示例
- en: 'When we choose `Classification` as the `Problem type`, the dots in the dataset
    are colored with only two color values: either blue or orange. When working on
    classification problems, the network evaluates its loss function based on how
    many blues and oranges the network has gotten wrong. It checks how far away to
    the right the color values are for each dot in the network, as shown in the following screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们选择`分类`作为`问题类型`时，数据集中的点只有两种颜色：蓝色或橙色。在处理分类问题时，网络根据网络错误分类的蓝色和橙色点的数量来评估其损失函数。它检查每个点在网络中颜色值右侧的偏移量，如下截图所示：
- en: '![Figure 3.4: Details of the TensorFlow Playground application. Different colors
    are assigned to different classes in classification problems'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4：TensorFlow Playground应用程序的详细信息。不同的颜色被分配给分类问题中的不同类别'
- en: '](img/B15911_03_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_04.jpg)'
- en: 'Figure 3.4: Details of the TensorFlow Playground application. Different colors
    are assigned to different classes in classification problems'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：TensorFlow Playground应用程序的详细信息。不同的颜色被分配给分类问题中的不同类别
- en: After clicking on the play button, we notice that the numbers in the `Training
    loss` area keep going down as the network continuously trains. The numbers are
    very similar in each problem category because the loss functions play the same
    role in both neural networks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 点击播放按钮后，我们会注意到`训练损失`区域中的数字不断下降，因为网络在持续训练。这些数字在每个问题类别中非常相似，因为损失函数在神经网络中起着相同的作用。
- en: However, the actual loss function that's used for each category is different
    and is chosen depending on the problem type.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个类别实际使用的损失函数是不同的，具体选择哪个函数取决于问题的类型。
- en: Using TensorBoard
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard
- en: Evaluating neural networks is where TensorBoard excels. As we explained in *Chapter
    1*, *Introduction to Neural Networks and Deep Learning*, TensorBoard is a suite
    of visualization tools that's shipped with TensorFlow. Among other things, we
    can explore the results of loss function evaluations after each epoch. A great
    feature of TensorBoard is that we can organize the results of each run separately
    and compare the resulting loss function metrics for each run. We can then decide
    on which hyperparameters to tune and have a general sense of how the network is
    performing. The best part is that this is all done in real time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 评估神经网络是TensorBoard的强项。正如我们在*第1章*，*神经网络与深度学习简介*中解释的那样，TensorBoard是与TensorFlow一起提供的一套可视化工具。除了其他功能，我们可以在每个epoch后探索损失函数评估的结果。TensorBoard的一个很棒的特点是，我们可以将每次运行的结果单独组织，并比较每次运行的损失函数度量标准。然后，我们可以决定调整哪些超参数，并对网络的表现有一个大致的了解。最棒的是，这一切都能实时完成。
- en: 'In order to use TensorBoard with our model, we will use Keras'' `callback`
    function. We do this by importing the TensorBoard `callback` and passing it to
    our model when calling its `fit()` function. The following code shows an example
    of how this would be implemented in the Bitcoin model we created in the *Chapter
    2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting the Price
    of Bitcoin*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在我们的模型中使用TensorBoard，我们将使用Keras的`callback`函数。我们通过导入TensorBoard的`callback`并将其传递给模型，在调用`fit()`函数时实现这一点。以下代码展示了如何在我们在*第2章*，*TensorFlow和Keras的实际深度学习：预测比特币价格*中创建的比特币模型中实现这一点： '
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keras `callback` functions are called at the end of each epoch run. In this
    case, Keras calls the TensorBoard `callback` to store the results from each run
    on the disk. There are many other useful `callback` functions available, and you
    can create custom ones using the Keras API.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的`callback`函数会在每个epoch运行结束时被调用。在这种情况下，Keras会调用TensorBoard的`callback`，将每次运行的结果存储到磁盘上。还有许多其他有用的`callback`函数，你可以使用Keras
    API创建自定义的回调函数。
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please refer to the Keras callback documentation ([https://keras.io/callbacks/](https://keras.io/callbacks/))
    for more information.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考Keras回调文档（[https://keras.io/callbacks/](https://keras.io/callbacks/)）以获取更多信息。
- en: After implementing the TensorBoard callback, the loss function metrics are now
    available in the TensorBoard interface. You can now run a TensorBoard process
    (with `tensorboard --logdir=./logs`) and leave it running while you train your
    network with `fit()`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现TensorBoard回调后，损失函数的度量标准现在可以在TensorBoard界面中查看。你可以运行TensorBoard进程（使用`tensorboard
    --logdir=./logs`），并在训练网络时保持它运行（通过`fit()`）。
- en: 'The main graphic to evaluate is typically called loss. We can add more metrics
    by passing known metrics to the metrics parameter in the `fit()` function. These
    will then be available for visualization in TensorBoard, but will not be used
    to adjust the network weights. The interactive graphics will continue to update
    in real time, which allows you to understand what is happening on every epoch.
    In the following screenshot, you can see a TensorBoard instance showing loss function
    results, alongside other metrics that have been added to the metrics parameter:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 评估的主要图形通常称为损失。我们可以通过将已知度量标准传递给`fit()`函数中的metrics参数来添加更多度量标准。这些度量标准会在TensorBoard中进行可视化，但不会用于调整网络的权重。交互式图形会持续实时更新，帮助你了解每个epoch的进展。在下面的截图中，你可以看到一个TensorBoard实例，展示了损失函数的结果，以及已添加到metrics参数中的其他度量标准：
- en: '![Figure 3.5: Screenshot of a TensorBoard instance showing the loss function
    results'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5：展示损失函数结果的TensorBoard实例截图]'
- en: '](img/B15911_03_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_05.jpg)'
- en: 'Figure 3.5: Screenshot of a TensorBoard instance showing the loss function
    results'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：展示损失函数结果的TensorBoard实例截图
- en: In the next section, we will talk more about how to implement the different
    metrics we discussed in this section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细讨论如何实现本节中讨论的不同度量标准。
- en: Implementing Model Evaluation Metrics
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现模型评估度量标准
- en: 'In both regression and classification problems, we split the input dataset
    into three other datasets: train, validation, and test. Both the train and the
    validation sets are used to train the network. The train set is used by the network
    as input, while the validation set is used by the loss function to compare the
    output of the neural network to the real data and compute how wrong the predictions
    are. Finally, the test set is used after the network has been trained to measure
    how the network can perform on data it has never seen before.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归和分类问题中，我们将输入数据集拆分为另外三个数据集：训练集、验证集和测试集。训练集和验证集都用于训练网络。训练集作为输入提供给网络，而验证集则被损失函数用来将神经网络的输出与真实数据进行比较，并计算预测的误差。最后，测试集在网络训练完成后用于衡量网络在未见过的数据上的表现。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There isn't a clear rule for determining how the train, validation, and test
    datasets must be divided. It is a common approach to divide the original dataset
    into 80 percent train and 20 percent test, then to further divide the train dataset
    into 80 percent train and 20 percent validation. For more information about this
    problem, please refer to *Python Machine Learning*, by *Sebastian Raschka and
    Vahid Mirjalili (Packt Publishing, 2017)*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 没有明确的规则来确定训练集、验证集和测试集如何划分。常见的方法是将原始数据集划分为 80% 的训练集和 20% 的测试集，然后将训练集再划分为 80%
    的训练集和 20% 的验证集。有关此问题的更多信息，请参阅 *Python 机器学习*，作者 *Sebastian Raschka 和 Vahid Mirjalili
    (Packt Publishing, 2017)*。
- en: In classification problems, you pass both the data and the labels to the neural
    network as related but distinct data. The network then learns how the data is
    related to each label. In regression problems, instead of passing data and labels,
    we pass the variable of interest as one parameter and the variables that are used
    for learning patterns as another. Keras provides an interface for both of those
    use cases with the `fit()` method.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，您将数据和标签作为相关但不同的数据传递给神经网络。然后，网络学习数据与每个标签之间的关系。在回归问题中，我们不是传递数据和标签，而是将感兴趣的变量作为一个参数传递，将用于学习模式的变量作为另一个参数。Keras
    提供了适用于这两种情况的接口，使用的是 `fit()` 方法。
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `fit()` method can use either the `validation_split` or the `validation_data`
    parameter, but not both at the same time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 方法可以使用 `validation_split` 或 `validation_data` 参数，但不能同时使用两者。'
- en: 'See the following snippet to understand how to use the `validation_split` and
    `validation_data` parameters:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下代码片段，了解如何使用 `validation_split` 和 `validation_data` 参数：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`X_train`: features from training set'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train`：训练集中的特征'
- en: '`Y_train`: labels from training set'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`Y_train`：训练集中的标签'
- en: '`batch_size`: the size of one batch'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_size`：每批的大小'
- en: '`epochs`: the number of iterations'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`epochs`：迭代次数'
- en: '`verbose`: the level of output you want'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`verbose`：您希望的输出级别'
- en: '`callbacks`: call a function after every epoch'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`callbacks`：在每个 epoch 后调用一个函数'
- en: '`validation_split`: validation percentage split if you have not created it
    explicitly'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`validation_split`：如果您没有显式创建验证集，则为验证集的百分比分割'
- en: '`validation_data`: validation data if you have created it explicitly'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`validation_data`：如果您显式创建了验证集，则为验证数据'
- en: Loss functions evaluate the progress of models and adjust their weights on every
    run. However, loss functions only describe the relationship between training data
    and validation data. In order to evaluate if a model is performing correctly,
    we typically use a third set of data—which is not used to train the network—and
    compare the predictions made by our model to the values available in that set
    of data. That is the role of the test set.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数评估模型的进展，并在每次运行时调整其权重。然而，损失函数仅描述训练数据和验证数据之间的关系。为了评估模型是否正确执行，我们通常会使用第三个数据集——该数据集不用于训练网络——并将我们模型的预测与该数据集中的真实值进行比较。这就是测试集的作用。
- en: 'Keras provides the `model.evaluate()` method, which makes the process of evaluating
    a trained neural network against a test set easy. The following code illustrates
    how to use the `evaluate()` method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了 `model.evaluate()` 方法，使得对已训练神经网络在测试集上的评估变得简单。以下代码展示了如何使用 `evaluate()`
    方法：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `evaluate()` method returns both the results of the loss function and the
    results of the functions passed to the `metrics` parameter. We will be using that
    function frequently in the Bitcoin problem to test how the model performs on the
    test set.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate()` 方法返回损失函数的结果以及传递给 `metrics` 参数的函数的结果。我们将在比特币问题中频繁使用该函数，以测试模型在测试集上的表现。'
- en: You will notice that the Bitcoin model we trained previously looks a bit different
    than this example. That is because we are using an LSTM architecture. LSTMs are
    designed to predict sequences.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们之前训练的比特币模型与这个示例有些不同。这是因为我们使用了LSTM架构。LSTM被设计用来预测序列。
- en: Because of that, we do not use a set of variables to predict a different single
    variable—even if it is a regression problem. Instead, we use previous observations
    from a single variable (or set of variables) to predict future observations of
    that same variable (or set). The `y` parameter on `keras.fit()` contains the same
    variable as the `x` parameter, but only the predicted sequences. So, let's have
    a look at how to evaluate the bitcoin model we trained previously.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不使用一组变量来预测另一个单一的变量——即使这是一个回归问题。相反，我们使用来自单一变量（或变量集）的先前观察值来预测该变量（或变量集）未来的观察值。`keras.fit()`中的`y`参数与`x`参数包含相同的变量，但仅包含预测的序列。那么，让我们看看如何评估之前训练的比特币模型。
- en: Evaluating the Bitcoin Model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估比特币模型
- en: We created a test set during our activities in *Chapter 1*, *Introduction to
    Neural Networks and Deep Learning*. That test set contains 21 weeks of daily Bitcoin
    price observations, which is equivalent to about 10 percent of the original dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第一章*，*神经网络与深度学习简介*中的活动中创建了一个测试集。该测试集包含21周的每日比特币价格观察数据，相当于原始数据集的约10%。
- en: 'We also trained our neural network using the other 90 percent of data (that
    is, the train set with 187 weeks of data, minus 1 for the validation set) in *Chapter
    2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting the Price
    of Bitcoin*, and stored the trained network on disk (`bitcoin_lstm_v0`). We can
    now use the `evaluate()` method in each of the 21 weeks of data from the test
    set and inspect how that first neural network performs.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了其余90%的数据（即训练集，包含187周数据，减去1周用于验证集）在*第二章*，*使用TensorFlow和Keras进行真实世界的深度学习：预测比特币价格*中训练了我们的神经网络，并将训练好的网络保存在磁盘上（`bitcoin_lstm_v0`）。现在，我们可以使用`evaluate()`方法来评估测试集中的每一周数据，并查看第一个神经网络的表现。
- en: 'In order to do that, though, we have to provide 186 preceding weeks. We have
    to do this because our network has been trained to predict one week of data using
    exactly 186 weeks of continuous data (we will deal with this behavior by retraining
    our network periodically with larger periods in *Chapter 4*, *Productization*,
    when we deploy a neural network as a web application). The following snippet implements
    the `evaluate()` method to evaluate the performance of our model in a test dataset:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要做到这一点，我们需要提供186周的前期数据。我们必须这么做，因为我们的网络是通过使用恰好186周的连续数据来预测一周数据进行训练的（我们将在*第四章*，*产品化*中通过定期用更长时间段的数据重新训练网络来处理这种行为，当我们将神经网络部署为Web应用程序时）。以下代码片段实现了`evaluate()`方法，用于评估我们模型在测试数据集中的表现：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding code, we evaluate each week using Keras'' `model.evaluate()`
    method, then store its output in the `evaluated_weeks` variable. We then plot
    the resulting MSE for each week, as shown in the following plot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用Keras的`model.evaluate()`方法评估每一周的数据，然后将其输出存储在`evaluated_weeks`变量中。接着，我们绘制每周的MSE结果，如下图所示：
- en: '![Figure 3.6: MSE for each week in the test set'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6：测试集每周的MSE'
- en: '](img/B15911_03_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_06.jpg)'
- en: 'Figure 3.6: MSE for each week in the test set'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：测试集每周的MSE
- en: The resulting MSE from our model suggests that our model performs well during
    most weeks, except for weeks 2, 8, 12, and 16, when its value increases from about
    0.005 to 0.02\. Our model seems to be performing well for almost all of the other
    test weeks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的MSE（均方误差）结果表明，我们的模型在大多数周内表现良好，除了第2、8、12和16周，其值从大约0.005增加到0.02。我们模型似乎在几乎所有其他测试周内表现得都很好。
- en: Overfitting
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合
- en: Our first trained network (`bitcoin_lstm_v0`) may be suffering from a phenomenon
    known as **overfitting**. Overfitting is when a model is trained to optimize a
    validation set, but it does so at the expense of more generalizable patterns from
    the phenomenon we are interested in predicting. The main issue with overfitting
    is that a model learns how to predict the validation set, but fails to predict
    new data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个训练的网络（`bitcoin_lstm_v0`）可能正遭遇一种被称为**过拟合**的现象。过拟合是指模型在优化验证集时，虽然在验证集上表现得很好，但却牺牲了从我们想要预测的现象中学习到的更具泛化能力的模式。过拟合的主要问题是模型学会了如何预测验证集，但却无法预测新的数据。
- en: 'The loss function we used in our model reaches very low levels at the end of
    our training process. Not only that, but this happens early: the MSE loss function
    that''s used to predict the last week in our data decreases to a stable plateau
    around epoch 30\. This means that our model is predicting the data from week 187
    almost perfectly, using the preceding 186 weeks. Could this be the result of overfitting?'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在模型中使用的损失函数在训练过程结束时达到了非常低的水平。不仅如此，这一过程发生得很早：用于预测我们数据最后一周的MSE损失函数在大约第30个周期时降到了一个稳定的平稳值。这意味着我们的模型几乎完美地预测了第187周的数据，使用了前186周的数据。这会不会是过拟合的结果呢？
- en: Let's look at the preceding plot again. We know that our LSTM model reaches
    extremely low values in our validation set (about 2.9 × 10-6), yet it also reaches
    low values in our test set. The key difference, however, is in the scale. The
    MSE for each week in our test set is about 4,000 times bigger (on average) than
    in the test set. This means that the model is performing much worse in our test
    data than in the validation set. This is worth considering.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一遍前面的图表。我们知道我们的LSTM模型在验证集上达到了极低的值（大约为2.9 × 10^-6），但它在测试集上的表现也很低。然而，关键的区别在于规模。在我们的测试集中，每周的MSE大约是验证集的4,000倍（平均值）。这意味着模型在我们的测试数据中的表现远不如在验证集中的表现。这一点值得考虑。
- en: The scale, though, hides the power of our LSTM model; even performing much worse
    in our test set, the predictions' MSE errors are still very, very low. This suggests
    that our model may be learning patterns from the data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，规模掩盖了我们LSTM模型的强大能力；即使在我们的测试集上表现较差，预测的MSE误差仍然非常、非常低。这表明我们的模型可能正在从数据中学习模式。
- en: Model Predictions
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型预测
- en: It's one thing is to measure our model comparing MSE errors, and another to
    be able to interpret its results intuitively.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 比较MSE误差来衡量我们的模型是一回事，而能够直观地解释其结果是另一回事。
- en: Using the same model, let's create a series of predictions for the following
    weeks, using 186 weeks as input. We do that by sliding a window of 186 weeks over
    the complete series (that is, train plus test sets) and making predictions for
    each of those windows.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的模型，让我们为接下来的几周创建一系列预测，使用186周的数据作为输入。我们通过将186周的滑动窗口应用于完整的序列（即训练集加测试集），并为每个窗口做出预测来实现这一点。
- en: 'The following snippet makes predictions for all the weeks of the test dataset
    using the `Keras model.predict()` method:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段使用`Keras model.predict()`方法对测试数据集的所有周进行预测：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding code, we make predictions using the `model.predict()` method,
    then store these predictions in the `predicted_weeks` variable. Then, we plot
    the resulting predictions, making the following plot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`model.predict()`方法进行预测，然后将这些预测存储在`predicted_weeks`变量中。然后，我们绘制结果图，生成以下图表：
- en: '![Figure 3.7: MSE for each week in the test set'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7：测试集中每周的MSE'
- en: '](img/B15911_03_07.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_07.jpg)'
- en: 'Figure 3.7: MSE for each week in the test set'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：测试集中每周的MSE
- en: The results of our model suggest that its performance isn't all that bad. By
    observing the pattern from the `Predicted` line (grey), we can see that the network
    has identified a fluctuating pattern happening on a weekly basis, in which the
    normalized prices go up in the middle of the week, then down by the end of it
    but. However, there's still a lot of room for improvement as it is unable to pick
    up higher fluctuations. With the exception of a few weeks—the same as with our
    previous MSE analysis—most weeks fall close to the correct values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的结果表明，它的表现并没有那么糟糕。通过观察`Predicted`线（灰色）的模式，我们可以看到网络已经识别出了每周波动的模式，规范化价格在一周的中间上涨，到周末时下降。然而，仍然有很大的改进空间，因为它无法捕捉到更大的波动。除了几个星期外——与我们之前的MSE分析相同——大多数周的数据接近正确的值。
- en: 'Now, let''s denormalize the predictions so that we can investigate the prediction
    values using the same scale as the original data (that is, US dollars). We can
    do this by implementing a denormalization function that uses the day index from
    the predicted data to identify the equivalent week on the test data. After that
    week has been identified, the function then takes the first value of that week
    and uses that value to denormalize the predicted values by using the same point-relative
    normalization technique but inverted. The following snippet denormalizes data
    using an inverted point-relative normalization technique:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对预测结果进行反归一化，以便能够使用与原始数据相同的尺度（即美元）来分析预测值。我们可以通过实现一个反归一化函数来完成这一步，该函数利用预测数据中的日期索引来识别测试数据中的等效周。识别出该周后，函数将取该周的第一个值，并利用相同的点相对归一化技术（但反向）来反归一化预测值。以下代码段展示了如何使用反向点相对归一化技术来反归一化数据：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `denormalize()` function takes the first closing price from the test's first
    day of an equivalent week.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`denormalize()`函数会取测试集等效周第一天的收盘价。'
- en: Our results now compare the predicted values with the test set using US dollars.
    As shown in the following plot, the `bitcoin_lstm_v0` model seems to perform quite
    well in predicting the Bitcoin prices for the following 7 days. But how can we
    measure that performance in interpretable terms?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果现在使用美元比较预测值与测试集。如下图所示，`bitcoin_lstm_v0`模型在预测未来7天比特币价格时似乎表现得相当好。但我们如何用可解释的术语来衡量这种表现呢？
- en: '![Figure 3.8: De-normalized predictions per week'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8：每周反归一化预测'
- en: '](img/B15911_03_08.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_08.jpg)'
- en: 'Figure 3.8: De-normalized predictions per week'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：每周反归一化预测
- en: Interpreting Predictions
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释预测结果
- en: Our last step is to add interpretability to our predictions. The preceding plot
    seems to show that our model prediction matches the test data somewhat closely,
    but how closely?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是为预测结果添加可解释性。前面的图表似乎显示我们的模型预测与测试数据的匹配度较高，但到底有多接近呢？
- en: Keras' `model.evaluate()` function is useful for understanding how a model is
    performing at each evaluation step. However, given that we are typically using
    normalized datasets to train neural networks, the metrics that are generated by
    the `model.evaluate()` method are also hard to interpret.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的`model.evaluate()`函数对理解模型在每个评估步骤中的表现非常有用。然而，考虑到我们通常使用归一化数据集来训练神经网络，`model.evaluate()`方法生成的指标也很难解释。
- en: 'In order to solve that problem, we can collect the complete set of predictions
    from our model and compare it with the test set using two other functions from
    *Figure 3.1* that are easier to interpret: MAPE and RMSE, which are implemented
    as `mape()` and `rmse()`, respectively.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以收集模型的完整预测集，并使用来自*图 3.1*的另外两个更易解释的函数进行比较：MAPE 和 RMSE，分别通过`mape()`和`rmse()`实现。
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: These functions are implemented using NumPy. The original implementations come
    from [https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn)
    and [https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy](https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数是使用NumPy实现的。原始实现来自[https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn)和[https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy](https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy)
- en: 'We can see the implementation of these methods in the following snippet:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下代码段中看到这些方法的实现：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After comparing our test set with our predictions using both of those functions,
    we have the following results:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这两个函数比较我们的测试集和预测值之后，我们得到了以下结果：
- en: '**Denormalized RMSE**: $596.6 USD'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反归一化 RMSE**：$596.6 美元'
- en: '**Denormalized MAPE**: 4.7 percent'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反归一化 MAPE**：4.7%'
- en: This indicates that our predictions differ, on average, about $596 from real
    data. This represents a difference of about 4.7 percent from real Bitcoin prices.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的预测与实际数据的差异平均约为$596。这表示与实际比特币价格相比，差异约为4.7%。
- en: These results facilitate the understanding of our predictions. We will continue
    to use the `model.evaluate()` method to keep track of how our LSTM model is improving,
    but will also compute both `rmse()` and `mape()` on the complete series on every
    version of our model to interpret how close we are to predicting Bitcoin prices.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果有助于我们理解预测结果。我们将继续使用`model.evaluate()`方法来跟踪LSTM模型的改进，同时也会计算`rmse()`和`mape()`，并在每个版本的模型上对完整数据系列进行计算，以解释我们距离预测比特币价格有多近。
- en: 'Exercise 3.01: Creating an Active Training Environment'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习3.01：创建一个主动训练环境
- en: In this exercise, we'll create a training environment for our neural network
    that facilitates both its training and evaluation. This environment is particularly
    important for the next topic, in which we'll search for an optimal combination
    of hyperparameters.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将为神经网络创建一个训练环境，以便于其训练和评估。这个环境对于接下来的主题尤为重要，我们将在其中寻找超参数的最佳组合。
- en: 'First, we will start both a Jupyter Notebook instance and a TensorBoard instance.
    Both of these instances can remain open for the remainder of this exercise. Let''s
    get started:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将启动Jupyter Notebook实例和TensorBoard实例。这两个实例可以在本练习的剩余部分保持打开。让我们开始吧：
- en: 'Using your Terminal, navigate to the `Chapter03/Exercise3.01` directory and
    execute the following code to start a Jupyter Notebook instance:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中，导航到`Chapter03/Exercise3.01`目录，并执行以下代码以启动Jupyter Notebook实例：
- en: '[PRE7]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The server should open in your browser automatically.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 服务器应该会自动在你的浏览器中打开。
- en: 'Open the Jupyter Notebook named `Exercise3.01_Creating_an_active_training_environment.ipynb`:![Figure
    3.9: Jupyter Notebook highlighting the section, Evaluate LSTM Model'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开名为`Exercise3.01_Creating_an_active_training_environment.ipynb`的Jupyter Notebook：![图3.9：Jupyter
    Notebook高亮显示了“Evaluate LSTM Model”部分
- en: '](img/B15911_03_09.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_03_09.jpg)'
- en: 'Figure 3.9: Jupyter Notebook highlighting the section, Evaluate LSTM Model'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.9：Jupyter Notebook高亮显示了“Evaluate LSTM Model”部分
- en: 'Also using your Terminal, start a TensorBoard instance by executing the following
    command:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时在你的终端中，通过执行以下命令启动一个TensorBoard实例：
- en: '[PRE8]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Ensure the `logs` directory is empty in the repository.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保`logs`目录在仓库中是空的。
- en: Open the URL that appears on screen and leave that browser tab open, as well.
    Execute the initial cells containing the import statements to ensure that the
    dependencies are loaded.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开屏幕上出现的URL，并保持该浏览器标签页打开。还要执行包含导入语句的初始单元格，以确保加载了所需的依赖项。
- en: 'Execute the two cells under Validation Data to load the train and test datasets
    in the Jupyter Notebook:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行“Validation Data”下的两个单元格，以在Jupyter Notebook中加载训练和测试数据集：
- en: '[PRE9]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Don't forget to change the path (highlighted) of the files based on where they
    are saved on your system.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不要忘记根据文件在你系统上的保存位置更改路径（高亮部分）。
- en: Add TensorBoard callback and retrain the model. Execute the cells under Re-Train
    model with TensorBoard.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加TensorBoard回调并重新训练模型。执行“Re-Train model with TensorBoard”下的单元格。
- en: Now, let's evaluate how our model performed against the test data. Our model
    is trained using 186 weeks to predict a week into the future—that is, the following
    sequence of 7 days. When we built our first model, we divided our original dataset
    between a training and a test set. Now, we will take a combined version of both
    datasets (let's call it a combined set) and move a sliding window of 186 weeks.
    At each window, we execute Keras' `model.evaluate()` method to evaluate how the
    network performed on that specific week.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们评估我们的模型在测试数据上的表现。我们的模型是用186周的数据来预测未来一周的数据——也就是接下来的7天。当我们构建第一个模型时，我们将原始数据集分为训练集和测试集。现在，我们将这两个数据集的结合版本（我们称之为“合并集”）进行滑动窗口操作，窗口大小为186周。在每个窗口中，我们执行Keras的`model.evaluate()`方法，评估网络在该特定周的表现。
- en: 'Execute the cells under the header, `Evaluate LSTM Model`. The key concept
    of these cells is to call the `model.evaluate()` method for each of the weeks
    in the test set. This line is the most important:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行标题为`Evaluate LSTM Model`下的单元格。这些单元格的关键概念是为测试集中的每一周调用`model.evaluate()`方法。这行代码是最重要的：
- en: '[PRE10]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Each evaluation result is now stored in the `evaluated_weeks` variable. That
    variable is a simple array containing the sequence of MSE predictions for every
    week in the test set. Go ahead and plot the results:![Figure 3.10: MSE results
    from the model.evaluate() method for each week of the test set'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个评估结果现在都存储在`evaluated_weeks`变量中。该变量是一个简单的数组，包含测试集每一周的MSE预测序列。继续绘制结果：![图3.10：使用`model.evaluate()`方法对测试集每一周的MSE结果
- en: '](img/B15911_03_10.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_03_10.jpg)'
- en: 'Figure 3.10: MSE results from the model.evaluate() method for each week of
    the test set'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.10：从model.evaluate()方法返回的每周测试集的MSE结果
- en: As we've already discussed, the MSE loss function is difficult to interpret.
    To facilitate our understanding of how our model is performing, we also call the
    `model.predict()` method on each week from the test set and compare its predicted
    results with the set's values.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的，MSE损失函数很难解释。为了帮助我们理解模型的表现，我们还调用`model.predict()`方法，在测试集中的每一周进行预测，并将其预测结果与测试集中的真实值进行比较。
- en: 'Navigate to the *Interpreting Model Results* section and execute the code cells
    under the `Make Predictions` subheading. Notice that we are calling the `model.predict()`
    method, but with a slightly different combination of parameters. Instead of using
    both the `X` and `Y` values, we only use `X`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到*解释模型结果*部分，并在`进行预测`子标题下执行代码单元。请注意，我们正在调用`model.predict()`方法，但使用的是略有不同的参数组合。我们不再同时使用`X`和`Y`值，而只使用`X`：
- en: '[PRE11]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At each window, we will issue predictions for the following week and store
    the results. Now, we can plot the normalized results alongside the normalized
    values from the test set, as shown in the following plot:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个窗口中，我们将为接下来的一周发布预测结果并存储。现在，我们可以将标准化的结果与测试集中的标准化值一起绘制，如下图所示：
- en: '![Figure 3.11: Plotting the normalized values returned from model.predict()
    for each'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.11：绘制从model.predict()返回的每周标准化值'
- en: week of the test set](img/B15911_03_11.jpg)
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试集的周](img/B15911_03_11.jpg)
- en: 'Figure 3.11: Plotting the normalized values returned from model.predict() for
    each week of the test set'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.11：绘制从model.predict()返回的每周标准化值
- en: 'We will also make the same comparisons but using denormalized values. In order
    to denormalize our data, we must identify the equivalent week between the test
    set and the predictions. Then, we can take the first price value for that week
    and use it to reverse the point-relative normalization equation from *Chapter
    2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting the Price
    of Bitcoin*.'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将使用反标准化值进行相同的比较。为了反标准化我们的数据，我们必须找出测试集和预测结果之间的对应周。然后，我们可以获取该周的第一个价格值，并利用它来逆转《第二章》中的基于点的标准化方程，即《现实世界的深度学习与TensorFlow和Keras：预测比特币价格》。
- en: Navigate to the `De-normalized Predictions` header and execute all the cells
    under that header.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`反标准化预测`标题，并执行该标题下的所有单元格。
- en: 'In this section, we defined the `denormalize()` function, which performs the
    complete denormalization process. In contrast to the other functions, this function
    takes in a pandas DataFrame instead of a NumPy array. We do this to use dates
    as an index. This is the most relevant cell block from that header:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本节中，我们定义了`denormalize()`函数，该函数执行完整的反标准化过程。与其他函数不同，这个函数接收一个pandas DataFrame，而不是NumPy数组。我们这样做是为了使用日期作为索引。以下是该标题下最相关的单元格块：
- en: '[PRE12]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our denormalized results (as seen in the following plot) show that our model
    makes predictions that are close to the real Bitcoin prices. But how close?
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的反标准化结果（如以下图所示）表明，我们的模型的预测与真实的比特币价格非常接近。那么，接近程度如何呢？
- en: '![Figure 3.12: Plotting the denormalized values returned from model.predict()'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.12：绘制从model.predict()返回的反标准化值'
- en: for each week of the test set
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于测试集中的每一周
- en: '](img/B15911_03_12.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_03_12.jpg)'
- en: 'Figure 3.12: Plotting the denormalized values returned from model.predict()
    for each week of the test set'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.12：绘制从model.predict()返回的每周反标准化值
- en: The LSTM network uses MSE values as its loss function. However, as we've already
    discussed, MSE values are difficult to interpret. To solve this, we need to implement
    two functions (loaded from the `utilities.py` script) that implement the `rmse()`
    and `mape()` functions. These functions add interpretability to our model by returning
    a measurement on the same scale that our original data used, and by comparing
    the difference in scale as a percentage.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSTM网络使用MSE值作为其损失函数。然而，正如我们已经讨论过的，MSE值很难解释。为了解决这个问题，我们需要实现两个函数（从`utilities.py`脚本中加载），它们实现`rmse()`和`mape()`函数。这些函数通过返回与我们原始数据使用相同尺度的度量，并通过将差异按百分比进行比较，从而为我们的模型增加了可解释性。
- en: 'Navigate to the `De-normalizing Predictions` header and load two functions
    from the `utilities.py` script:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`反标准化预测`标题，并从`utilities.py`脚本中加载两个函数：
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The functions from this script are actually really simple:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个脚本中的函数其实非常简单：
- en: '[PRE14]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Each function is implemented using NumPy's vector-wise operations. They work
    well in vectors of the same length. They are designed to be applied on a complete
    set of results.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个函数都是使用 NumPy 的矢量操作实现的。它们在相同长度的向量中工作良好，旨在应用于一整套结果。
- en: Using the `mape()` function, we can now understand that our model predictions
    are about 4.7 percent away from the prices from the test set. This is equivalent
    to a RSME (calculated using the `rmse()` function) of about $596.6.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 `mape()` 函数，我们现在可以理解我们的模型预测与测试集中的价格大约相差4.7%。这相当于一个通过 `rmse()` 函数计算的 RSME，大约为
    $596.6。
- en: Before moving on to the next section, go back into the Notebook and find the
    `Re-train Model with TensorBoard` header. You may have noticed that we created
    a helper function called `train_model()`. This function is a wrapper around our
    model that trains (using `model.fit()`) our model, storing its respective results
    under a new directory. Those results are then used by TensorBoard in order to
    display statistics for different models.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在进入下一部分之前，返回到笔记本，找到 `Re-train Model with TensorBoard` 这个标题。你可能已经注意到我们创建了一个名为
    `train_model()` 的辅助函数。这个函数是我们模型的一个封装，负责训练（使用 `model.fit()`）我们的模型，并将结果存储在一个新的目录下。这些结果随后会被
    TensorBoard 使用，用于显示不同模型的统计信息。
- en: 'Go ahead and modify some of the values for the parameters that were passed
    to the `model.fit()` function (try epochs, for instance). Now, run the cells that
    load the model into memory from disk (this will replace your trained model):'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续修改一些传递给 `model.fit()` 函数的参数值（例如尝试调整 epochs）。然后，运行加载模型的单元格，将模型从磁盘载入内存（这将替换你训练的模型）：
- en: '[PRE15]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, run the `train_model()` function again, but with different parameters,
    indicating a new run version. When you run this command, you will be able to train
    a newer version of the model and specify the newer version in the version parameter:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，再次运行 `train_model()` 函数，但使用不同的参数，表示一个新的运行版本。当你运行这个命令时，你将能够训练一个新的版本的模型，并在版本参数中指定该版本：
- en: '[PRE16]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZhK4z3](https://packt.live/2ZhK4z3).
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参考 [https://packt.live/2ZhK4z3](https://packt.live/2ZhK4z3)。
- en: You can also run this example online at [https://packt.live/2Dvd9i3](https://packt.live/2Dvd9i3).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/2Dvd9i3](https://packt.live/2Dvd9i3)。你必须执行整个笔记本，才能得到预期的结果。
- en: In this exercise, we learned how to evaluate a network using loss functions.
    We learned that loss functions are key elements of neural networks since they
    evaluate the performance of a network at each epoch and are the starting point
    for the propagation of adjustments back into layers and nodes. We also explored
    why some loss functions can be difficult to interpret (for instance, the MSE function)
    and developed a strategy using two other functions—RMSE and MAPE—to interpret
    the predicted results from our LSTM model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何使用损失函数评估网络。我们了解到损失函数是神经网络的关键元素，因为它们评估网络在每一轮中的表现，并且是反向传播调整到各层和节点的起点。我们还探讨了为什么某些损失函数（例如，MSE
    函数）可能难以解释，并通过使用另外两个函数——RMSE 和 MAPE，来解释我们 LSTM 模型的预测结果。
- en: Most importantly, we've concluded this exercise with an active training environment.
    We now have a system that can train a deep learning model and evaluate its results
    continuously. This will be key when we move on to optimizing our network in the
    next topic.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们已经结束了这个练习，并创建了一个活跃的训练环境。现在我们有了一个可以持续训练深度学习模型并评估其结果的系统。当我们进入下一主题，优化网络时，这将是关键。
- en: Hyperparameter Optimization
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: So far, we have trained a neural network to predict the next 7 days of Bitcoin
    prices using the preceding 76 weeks of prices. On average, this model issues predictions
    that are about 8.4 percent distant from real Bitcoin prices.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经训练了一个神经网络，利用前76周的比特币价格预测接下来7天的比特币价格。平均而言，这个模型的预测与实际比特币价格的偏差约为8.4%。
- en: 'This section describes common strategies for improving the performance of neural
    network models:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了提高神经网络模型性能的常见策略：
- en: Adding or removing layers and changing the number of nodes
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加或移除层，调整节点数量
- en: Increasing or decreasing the number of training epochs
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加或减少训练的轮次
- en: Experimenting with different activation functions
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验不同的激活函数
- en: Using different regularization strategies
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的正则化策略
- en: We will evaluate each modification using the same active learning environment
    we developed by the end of the *Model Evaluation* section, measuring how each
    one of these strategies may help us develop a more precise model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在*模型评估*章节末尾开发的相同主动学习环境来评估每一个修改，衡量这些策略如何帮助我们开发出更精确的模型。
- en: Layers and Nodes – Adding More Layers
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层和节点 – 添加更多层
- en: 'Neural networks with single hidden layers can perform fairly well on many problems.
    Our first Bitcoin model (`bitcoin_lstm_v0`) is a good example: it can predict
    the next 7 days of Bitcoin prices (from the test set) with error rates of about
    8.4 percent using a single LSTM layer. However, not all problems can be modeled
    with single layers.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一隐藏层的神经网络在许多问题上可以表现得相当不错。我们的第一个比特币模型（`bitcoin_lstm_v0`）就是一个很好的例子：它可以用一个LSTM层预测接下来的7天比特币价格（来自测试集），误差率约为8.4%。然而，并非所有问题都可以用单一层来建模。
- en: The more complex the function you are working to predict, the higher the likelihood
    that you will need to add more layers. A good way to determine whether adding
    new layers is a good idea is to understand what their role in a neural network
    is.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你所要预测的函数越复杂，你就越有可能需要添加更多的层。判断是否需要添加新层的一个好方法是理解它们在神经网络中的作用。
- en: Each layer creates a model representation of its input data. Earlier layers
    in the chain create lower-level representations, while later layers create higher-level representations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都会对其输入数据创建一个模型表示。链中的早期层创建较低级的表示，而后续层创建更高级的表示。
- en: 'While this description may be difficult to translate into real-world problems,
    its practical intuition is simple: when working with complex functions that have
    different levels of representation, you may want to experiment with adding layers.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个描述可能难以直接转化为现实世界的问题，但其实际直觉非常简单：在处理具有不同表示层次的复杂函数时，你可能希望尝试添加层。
- en: Adding More Nodes
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加更多节点
- en: 'The number of neurons that your layer requires is related to how both the input
    and output data is structured. For instance, if you are working on a binary classification
    problem to classify a 4 x 4 pixel image, then you can try out the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 层所需的神经元数量与输入和输出数据的结构有关。例如，如果你正在处理一个二分类问题，要对一个 4 x 4 像素的图像进行分类，那么你可以尝试如下操作：
- en: Have a hidden layer that has 12 neurons (one for each available pixel)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个包含12个神经元的隐藏层（每个可用像素一个神经元）
- en: Have an output layer that has only two neurons (one for each predicted class)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个只包含两个神经元的输出层（每个预测类别一个神经元）
- en: It is common to add new neurons alongside the addition of new layers. Then,
    we can add a layer that has either the same number of neurons as the previous
    one, or a multiple of the number of neurons from the previous layer. For instance,
    if your first hidden layer has 12 neurons, you can experiment with adding a second
    layer that has either 12, 6, or 24 neurons.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在添加新层的同时也会添加新神经元。然后，我们可以添加一个层，其神经元数量可以与前一层相同，或者是前一层神经元数量的倍数。例如，如果你的第一隐藏层有12个神经元，你可以尝试添加第二层，神经元数量为12、6或24个。
- en: Adding layers and neurons can have significant performance limitations. Feel
    free to experiment with adding layers and nodes. It is common to start with a
    smaller network (that is, a network with a small number of layers and neurons),
    then grow according to its performance gains.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 添加层和神经元可能会带来显著的性能限制。可以自由地尝试添加层和节点。通常，从一个较小的网络开始（即一个包含较少层和神经元的网络），然后根据性能提升进行扩展。
- en: If this comes across as imprecise, your intuition is right.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来不够精确，你的直觉是正确的。
- en: Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To quote *Aurélien Géron*, YouTube's former lead for video classification, "*Finding
    the perfect amount of neurons is still somewhat of a black art*."
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 引用*Aurélien Géron*（YouTube前视频分类负责人）的话：“*找到合适数量的神经元仍然有些像黑魔法*。”
- en: 'Finally, a word of caution: the more layers you add, the more hyperparameters
    you have to tune—and the longer your network will take to train. If your model
    is performing fairly well and not overfitting your data, experiment with the other
    strategies outlined in this chapter before adding new layers to your network.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提醒一句：你添加的层越多，超参数的调节就越多——网络训练所需的时间也会更长。如果你的模型表现良好且没有对数据过拟合，在添加新层之前，可以尝试本章节中概述的其他策略。
- en: Layers and Nodes – Implementation
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层和节点 – 实现
- en: Now, we will modify our original LSTM model by adding more layers. In LSTM models,
    we typically add LSTM layers in a sequence, making a chain between LSTM layers.
    In our case, the new LSTM layer has the same number of neurons as the original
    layer, so we don't have to configure that parameter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过添加更多的层来修改原始 LSTM 模型。在 LSTM 模型中，我们通常将 LSTM 层按顺序添加，形成 LSTM 层之间的链条。在我们的例子中，新的
    LSTM 层具有与原始层相同数量的神经元，因此我们不需要配置该参数。
- en: We will name the modified version of our model `bitcoin_lstm_v1`. It is good
    practice to name each one of the models in terms of which one is attempting different
    hyperparameter configurations. This helps you to keep track of how each different
    architecture performs, and also to easily compare model differences in TensorBoard.
    We will compare all the different modified architectures at the end of this chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改后的模型命名为 `bitcoin_lstm_v1`。通常的做法是根据不同的超参数配置命名每一个模型，这有助于你追踪每个不同架构的表现，同时也能在
    TensorBoard 中方便地比较模型之间的差异。我们将在本章结束时比较所有不同的修改架构。
- en: Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before adding a new LSTM layer, we need to set the `return_sequences` parameter
    to `True` on the first LSTM layer. We do this because the first layer expects
    a sequence of data with the same input as that of the first layer. When this parameter
    is set to `False`, the LSTM layer outputs the predicted parameters in a different,
    incompatible output.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加新的 LSTM 层之前，我们需要将第一个 LSTM 层的 `return_sequences` 参数设置为 `True`。我们这么做是因为第一个层期望接收与其输入相同的数据序列。当该参数设置为
    `False` 时，LSTM 层输出的预测参数与原来的不兼容。
- en: 'The following code example adds a second LSTM layer to the original `bitcoin_lstm_v0`
    model, making it `bitcoin_lstm_v1`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例将第二个 LSTM 层添加到原始的 `bitcoin_lstm_v0` 模型中，生成 `bitcoin_lstm_v1`：
- en: '[PRE17]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Epochs
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练轮次
- en: '**Epochs** are the number of times the network adjusts its weights in response
    to the data passing through and its resulting loss function. Running a model for
    more epochs can allow it to learn more from data, but you also run the risk of
    overfitting.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练轮次**是指网络在响应通过的数据及其导致的损失函数时调整其权重的次数。运行模型更多轮次可以让模型从数据中学到更多东西，但也有可能导致过拟合的风险。'
- en: When training a model, prefer to increase the epochs exponentially until the
    loss function starts to plateau. In the case of the `bitcoin_lstm_v0` model, its
    loss function plateaus at about 100 epochs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，建议将训练轮次指数级增加，直到损失函数开始趋于平稳。在 `bitcoin_lstm_v0` 模型的情况下，它的损失函数大约在 100 轮时趋于平稳。
- en: Our LSTM model uses a small amount of data to train, so increasing the number
    of epochs does not affect its performance in a significant way. For instance,
    if we attempt to train it at 103 epochs, the model barely gains any improvements.
    This will not be the case if the model being trained uses enormous amounts of
    data. In those cases, a large number of epochs is crucial to achieve good performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 LSTM 模型使用的数据量较少，因此增加训练轮次不会显著影响其性能。例如，如果我们尝试在 103 轮时训练模型，几乎没有任何提升。如果所训练的模型使用的是庞大的数据量，情况就不同了。在这种情况下，大量的训练轮次对取得良好的性能至关重要。
- en: 'I suggest you use the following rule of thumb: *the larger the data used to
    train your model, the more epochs it will need to achieve good performance*.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你遵循以下经验法则：*用于训练模型的数据越大，所需的训练轮次就越多，以达到良好的性能*。
- en: Epochs – Implementation
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练轮次 - 实现
- en: 'Our Bitcoin dataset is rather small, so increasing the epochs that our model
    trains may have only a marginal effect on its performance. In order to have the
    model train for more epochs, we only have to change the `epochs` parameter in
    the `model.fit()` method. In the following snippet, you will see how to change
    the number of epochs that our model trains for:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的比特币数据集相对较小，因此增加模型的训练轮次可能对其性能的影响很小。为了让模型训练更多轮次，我们只需要在 `model.fit()` 方法中更改
    `epochs` 参数。在下面的代码片段中，你将看到如何更改模型的训练轮次：
- en: '[PRE18]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This change bumps our model to `v2`, effectively making it `bitcoin_lstm_v2`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更改将我们的模型升级到 `v2`，有效地使其成为 `bitcoin_lstm_v2`。
- en: Activation Functions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: '**Activation functions** evaluate how much you need to activate individual
    neurons. They determine the value that each neuron will pass to the next element
    of the network, using both the input from the previous layer and the results from
    the loss function—or if a neuron should pass any values at all.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**评估你需要多大程度地激活单个神经元。它们决定每个神经元将传递给网络下一个元素的值，这个值既包括来自上一层的输入，也包括损失函数的结果——或者决定一个神经元是否需要传递任何值。'
- en: Note
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Activation functions are a topic of great interest for those in the scientific
    community researching neural networks. For an overview of research currently being
    done on the topic and a more detailed review on how activation functions work,
    please refer to *Deep Learning by Ian Goodfellow et. al., MIT Press, 2017*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经网络研究领域中科学界关注的一个重要话题。有关当前研究概况以及激活函数如何工作的更详细审查，请参阅 *Ian Goodfellow 等人所著《深度学习》，MIT出版社，2017年*。
- en: TensorFlow and Keras provide many activation functions—and new ones are occasionally
    added. As an introduction, three are important to consider; let's explore each
    of them.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 提供了许多激活函数——并且偶尔会增加新的激活函数。作为介绍，以下三种是需要特别考虑的；让我们一起探讨它们。
- en: Note
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This section has been greatly inspired by the article *Understanding Activation
    Functions in Neural Networks* by Avinash Sharma V, available at [https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容深受 Avinash Sharma V 撰写的文章 *理解神经网络中的激活函数* 的启发，文章可在 [https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
    阅读。
- en: Linear (Identity) Functions
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性（恒等）函数
- en: 'Linear functions only activate a neuron based on a constant value. They are
    defined by the following equation:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数仅基于常数值激活神经元。它们由以下方程定义：
- en: '![Figure 3.13: Formula for linear functions'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13：线性函数的公式'
- en: '](img/B15911_03_13.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_13.jpg)'
- en: 'Figure 3.13: Formula for linear functions'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13：线性函数的公式
- en: 'Here, *c* is the constant value. When *c = 1*, neurons will pass the values
    as is, without any modification needed by the activation function. The issue with
    using linear functions is that, due to the fact that neurons are activated linearly,
    chained layers now function as a single large layer. In other words, we lose the
    ability to construct networks with many layers, in which the output of one influences
    the other:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*c* 是常数值。当 *c = 1* 时，神经元将按原样传递值，不需要激活函数进行任何修改。使用线性函数的问题在于，由于神经元是线性激活的，链式层现在作为一个单一的大层工作。换句话说，我们失去了构建多层网络的能力，在这些网络中，一个层的输出会影响另一个层：
- en: '![Figure 3.14: Illustration of a linear function'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14：线性函数的示意图'
- en: '](img/B15911_03_14.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_14.jpg)'
- en: 'Figure 3.14: Illustration of a linear function'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14：线性函数的示意图
- en: The use of linear functions is generally considered obsolete for most networks
    because they do not compute complex features and do not induce proper non-linearity
    in neurons.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性函数无法计算复杂特征，且不会在神经元中引入适当的非线性，因此它们通常被认为对于大多数网络来说已经过时。
- en: Hyperbolic Tangent (Tanh) Function
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双曲正切（Tanh）函数
- en: 'Tanh is a non-linear function, and is represented by the following formula:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 是一个非线性函数，其表示公式如下：
- en: '![Figure 3.15: Formula for hyperbolic tangent function'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15：双曲正切函数的公式'
- en: '](img/B15911_03_15.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_15.jpg)'
- en: 'Figure 3.15: Formula for hyperbolic tangent function'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：双曲正切函数的公式
- en: 'This means that the effect they have on nodes is evaluated continuously. Also,
    because of its non-linearity, we can use this function to change how one layer
    influences the next layer in the chain. When using non-linear functions, layers
    activate neurons in different ways, making it easier to learn different representations
    from data. However, they have a sigmoid-like pattern that penalizes extreme node
    values repeatedly, causing a problem called vanishing gradients. Vanishing gradients
    have negative effects on the ability of a network to learn:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它们对节点的影响是持续评估的。而且，由于其非线性特性，我们可以使用该函数来改变一个层如何影响链中的下一个层。当使用非线性函数时，层以不同方式激活神经元，使得从数据中学习不同的表示变得更加容易。然而，它们具有类似于
    sigmoid 的模式，会重复惩罚极端节点值，从而导致一种称为梯度消失的问题。梯度消失对网络学习能力产生负面影响：
- en: '![Figure 3.16: Illustration of a tanh function'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.16：双曲正切函数的示意图'
- en: '](img/B15911_03_16.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_16.jpg)'
- en: 'Figure 3.16: Illustration of a tanh function'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16：双曲正切函数的示意图
- en: Tanhs are popular choices, but due to the fact that they are computationally
    expensive, ReLUs are often used instead.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Tanh 是一种常见选择，但由于其计算开销较大，因此通常使用 ReLU 函数作为替代。
- en: Rectified Linear Unit Functions
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修正线性单元函数（ReLU）
- en: '**ReLU** stands for **Rectified Linear Unit**. It filters out negative values
    and keeps only the positive values. ReLU functions are often recommended as great
    starting points before trying other functions. They are defined by the following
    formula:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU** 代表 **修正线性单元**。它滤除负值，仅保留正值。ReLU 函数通常被推荐作为在尝试其他函数之前的一个很好的起点。其公式如下：'
- en: '![Figure 3.17: Formula for ReLU functions'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17：ReLU 函数的公式'
- en: '](img/B15911_03_17.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_17.jpg)'
- en: 'Figure 3.17: Formula for ReLU functions'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17：ReLU 函数的公式
- en: 'ReLUs have non-linear properties:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 具有非线性特性：
- en: '![Figure 3.18: Illustration of a ReLU function'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.18：ReLU 函数的示意图'
- en: '](img/B15911_03_18.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_18.jpg)'
- en: 'Figure 3.18: Illustration of a ReLU function'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18：ReLU 函数的示意图
- en: ReLUs tend to penalize negative values. So, if the input data (for instance,
    normalized between -1 and 1) contains negative values, those will now be penalized
    by ReLUs. That may not be the intended behavior.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 倾向于惩罚负值。因此，如果输入数据（例如，标准化后在 -1 到 1 之间）包含负值，这些值将被 ReLU 惩罚。这可能不是预期的行为。
- en: We will not be using ReLU functions in our network because our normalization
    process creates many negative values, yielding a much slower learning model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在网络中使用 ReLU 函数，因为我们的标准化过程会产生许多负值，导致学习模型变得更加缓慢。
- en: Activation Functions – Implementation
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数 – 实现
- en: The easiest way to implement activation functions in Keras is by instantiating
    the `Activation()` class and adding it to the `Sequential()` model. `Activation()`
    can be instantiated with any activation function available in Keras (for a complete
    list, see [https://keras.io/activations/](https://keras.io/activations/)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中实现激活函数的最简单方法是实例化 `Activation()` 类并将其添加到 `Sequential()` 模型中。`Activation()`
    可以用 Keras 中任何可用的激活函数来实例化（完整列表请参见 [https://keras.io/activations/](https://keras.io/activations/)）。
- en: 'In our case, we will use the `tanh` function. After implementing an activation
    function, we bump the version of our model to `v2`, making it `bitcoin_lstm_v3`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将使用 `tanh` 函数。在实现激活函数后，我们将模型版本提升到 `v2`，使其成为 `bitcoin_lstm_v3`：
- en: '[PRE19]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After executing the `compile` command, your model has been built according to
    the layers specified and is now ready to be trained. There are a number of other
    activation functions worth experimenting with. Both TensorFlow and Keras provide
    a list of implemented functions in their respective official documentations. Before
    implementing your own, start with the ones we've already implemented in both TensorFlow
    and Keras.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 `compile` 命令后，您的模型已经根据指定的层构建完毕，现在可以开始训练了。有许多其他激活函数值得尝试。TensorFlow 和 Keras
    都在各自的官方文档中提供了已实现函数的列表。在实现自己的函数之前，可以先使用我们在 TensorFlow 和 Keras 中已经实现的函数。
- en: Regularization Strategies
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化策略
- en: Neural networks are particularly prone to overfitting. Overfitting happens when
    a network learns the patterns of the training data but is unable to find generalizable
    patterns that can also be applied to the test data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络特别容易发生过拟合。过拟合是指网络学习到训练数据的模式，但无法找到可以应用于测试数据的可泛化模式。
- en: 'Regularization strategies refer to techniques that deal with the problem of
    overfitting by adjusting how the network learns. In the following sections, we''ll
    discuss two common strategies:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化策略指的是通过调整网络学习的方式来处理过拟合问题的技术。在接下来的章节中，我们将讨论两种常见的策略：
- en: L2 Regularization
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化
- en: Dropout
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: L2 Regularization
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L2 正则化
- en: '**L2 regularization** (or **weight decay**) is a common technique for dealing
    with overfitting models. In some models, certain parameters vary in great magnitudes.
    L2 regularization penalizes such parameters, reducing the effect of these parameters
    on the network.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2 正则化**（或 **权重衰减**）是处理过拟合模型的常用技术。在某些模型中，某些参数的变化幅度非常大。L2 正则化会惩罚这些参数，减少它们对网络的影响。'
- en: L2 regularizations use the ![3](img/B15911_03_Formula_01.png) parameter to determine
    how much to penalize a model neuron. We typically set that to a very low value
    (that is, 0.0001); otherwise, we risk eliminating the input from a given neuron
    completely.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化使用 ![3](img/B15911_03_Formula_01.png) 参数来确定惩罚模型神经元的程度。我们通常将该值设置为非常低的数值（即
    0.0001）；否则，我们可能会完全消除给定神经元的输入。
- en: Dropout
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: 'Dropout is a regularization technique based on a simple question: *if we randomly
    take away a proportion of the nodes from the layers, how will the other node adapt?*
    It turns out that the remaining neurons adapt, learning to represent patterns
    that were previously handled by those neurons that are missing.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种基于简单问题的正则化技术：*如果我们随机去除一部分层中的节点，其他节点将如何适应？* 结果证明，剩下的神经元会适应，学会表示那些原本由缺失神经元处理的模式。
- en: The dropout strategy is simple to implement and is typically very effective
    at avoiding overfitting. This will be our preferred regularization.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout策略简单易实现，并且通常能有效避免过拟合。这将是我们首选的正则化方法。
- en: Regularization Strategies – Implementation
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化策略 – 实现
- en: 'In order to implement the dropout strategy using Keras, we''ll import the `Dropout()`
    method and add it to our network immediately after each LSTM layer. This addition
    effectively makes our network `bitcoin_lstm_v4`. In this snippet, we''re adding
    the `Dropout()` step to our model (`bitcoin_lstm_v3`), making it `bitcoin_lstm_v4`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Keras实现Dropout策略，我们将导入`Dropout()`方法，并在每个LSTM层后立即将其添加到网络中。这个添加将使我们的网络变为`bitcoin_lstm_v4`。在这个代码片段中，我们将`Dropout()`步骤添加到我们的模型（`bitcoin_lstm_v3`）中，使其变为`bitcoin_lstm_v4`：
- en: '[PRE20]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We could have used L2 regularization instead of dropout. Dropout drops out random
    neurons in each epoch, whereas L2 regularization penalizes neurons that have high
    weight values. In order to apply L2 regularization, simply instantiate the `ActivityRegularization()`
    class with the L2 parameter set to a low value (for instance, 0.0001). Then, place
    it in the place where the `Dropout()` class has been added to the network. Feel
    free to experiment by adding that to the network while keeping both `Dropout()`
    steps, or simply replace all the `Dropout()` instances with `ActivityRegularization()`
    instead.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用L2正则化来替代Dropout。Dropout在每一轮中随机丢弃神经元，而L2正则化则惩罚那些具有高权重值的神经元。为了应用L2正则化，只需实例化`ActivityRegularization()`类，并将L2参数设置为一个较低的值（例如0.0001）。然后，将它放入原本添加了`Dropout()`类的位置。可以自由尝试将其添加到网络中，同时保留两个`Dropout()`步骤，或者直接用`ActivityRegularization()`替换所有的`Dropout()`实例。
- en: Optimization Results
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化结果
- en: 'All in all, we have created four versions of our model. Three of these versions,
    that is, `bitcoin_lstm_v1`, `bitcoin_lstm_v2`, and `bitcoin_lstm_v3`, were created
    by applying different optimization techniques that were outlined in this chapter.
    Now, we have to evaluate which model performs best. In order to do that, we will
    use the same metrics we used in our first model: MSE, RMSE, and MAPE. MSE is used
    to compare the error rates of the model on each predicted week. RMSE and MAPE
    are computed to make the model results easier to interpret. The following table
    shows this:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们创建了四个版本的模型。三个版本，即`bitcoin_lstm_v1`、`bitcoin_lstm_v2`和`bitcoin_lstm_v3`，是通过应用本章中介绍的不同优化技术创建的。现在，我们需要评估哪个模型表现最好。为此，我们将使用与第一个模型相同的指标：MSE、RMSE和MAPE。MSE用于比较模型在每一周预测中的误差率。RMSE和MAPE则用于使模型结果更易解释。下表显示了这一点：
- en: '![Figure 3.19: Model results for all models'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.19：所有模型的结果'
- en: '](img/B15911_03_19.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_03_19.jpg)'
- en: 'Figure 3.19: Model results for all models'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19：所有模型的结果
- en: Interestingly, our first model (`bitcoin_lstm_v0`) performed the best in nearly
    all defined metrics. We will be using that model to build our web application
    and continuously predict Bitcoin prices.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们的第一个模型（`bitcoin_lstm_v0`）在几乎所有定义的指标中表现最好。我们将使用该模型来构建我们的Web应用程序，并持续预测比特币价格。
- en: 'Activity 3.01: Optimizing a Deep Learning Model'
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动3.01：优化深度学习模型
- en: 'In this activity, we''ll implement different optimization strategies on the
    model we created in *Chapter 2*, *Real-World Deep Learning with TensorFlow and
    Keras: Predicting the Price of Bitcoin* (`bitcoin_lstm_v0`). This model achieves
    a MAPE performance on the complete de-normalization test set of about 8.4 percent.
    We will try to reduce that gap and get more accurate predictions.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将在*第2章*中创建的模型上实现不同的优化策略，*《真实世界的深度学习：使用TensorFlow和Keras预测比特币价格》*（`bitcoin_lstm_v0`）。该模型在完整的反标准化测试集上实现了约8.4%的MAPE性能。我们将尝试缩小这个差距，获得更准确的预测。
- en: 'Here are the steps:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤：
- en: Start TensorBoard from a Terminal.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从终端启动TensorBoard。
- en: Start a Jupyter Notebook.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个Jupyter Notebook。
- en: Load the train and test data and split the `lstm` input in the format required
    by the model.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练和测试数据，并将`lstm`输入拆分为模型所需的格式。
- en: In the previous exercise, we create a model architecture. Copy that model architecture
    and add a new LSTM layer. Compile and create a model.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在之前的练习中，我们创建了一个模型架构。复制该架构并添加一个新的LSTM层。编译并创建模型。
- en: Change the number of epochs in *step 4* by creating a new model. Compile and
    create a new model.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤4*中更改epoch的数量，创建一个新模型。编译并创建新模型。
- en: Change the activation function to `tanh` or `relu` and create a new model. Compile
    and train a new model.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改激活函数为`tanh`或`relu`，并创建一个新模型。编译并训练新模型。
- en: Add a new layer for dropout after the LSTM layer and create a new model. Keep
    values such as `0.2` or `0.3` for dropout. Compile and train a new model.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在LSTM层之后添加一个dropout层并创建新模型。保持dropout值为`0.2`或`0.3`。编译并训练新模型。
- en: Evaluate and compare all the models that were trained in this activity.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估并比较在本活动中训练的所有模型。
- en: Note
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 141.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第141页找到。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to evaluate our model using the MSE, RMSE, and
    MAPE metrics. We computed the latter two metrics in a series of 19-week predictions
    made by our first neural network model. By doing this, we learned that it was
    performing well.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用MSE、RMSE和MAPE指标来评估我们的模型。我们在第一次神经网络模型做出的19周预测中计算了后两个指标。通过这样做，我们了解到模型表现良好。
- en: We also learned how to optimize a model. We looked at optimization techniques,
    which are typically used to increase the performance of neural networks. Also,
    we implemented a number of these techniques and created a few more models to predict
    Bitcoin prices with different error rates.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了如何优化模型。我们研究了通常用于提高神经网络性能的优化技术。此外，我们实现了这些技术并创建了几个新模型，用不同的误差率预测比特币价格。
- en: 'In the next chapter, we will be turning our model into a web application that
    does two things: retrains our model periodically with new data and is able to
    make predictions using an HTTP API interface.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把我们的模型转化为一个Web应用程序，该应用程序有两个功能：定期使用新数据重新训练我们的模型，并能通过HTTP API接口进行预测。
