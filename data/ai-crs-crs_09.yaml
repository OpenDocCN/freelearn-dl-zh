- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Going Pro with Artificial Brains – Deep Q-Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工智能大脑——深度Q学习
- en: This next AI model is fantastic, because it is the first AI model that is really
    inspired by human intelligence. I hope you're ready to go pro on the next exciting
    step in your AI journey; this book is not only a crash course on AI, but also
    an introduction to deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个AI模型非常棒，因为它是第一个真正受到人类智能启发的AI模型。我希望你已经准备好在AI之旅的下一步中大展拳脚；这本书不仅是AI的速成课程，也是深度学习的入门介绍。
- en: Today, some of the top AI models integrate deep learning. They form a new branch
    of AI called deep Reinforcement Learning. The model we'll cover in this chapter
    belongs to that branch, and is called deep Q-learning. You already know what Q-learning
    is all about, but you might not know anything about deep learning and **Artificial
    Neural Networks** (**ANNs**); we'll start with them. Of course, if you are an
    expert in deep learning, you can skip the first sections of this chapter, but
    consider that a little refresher never hurt anyone.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，一些顶尖的AI模型已经融合了深度学习。它们形成了一个新的AI分支，叫做深度强化学习。本章将介绍的模型属于这个分支，称为深度Q学习。你已经知道Q学习的原理，但可能对深度学习和**人工神经网络**（**ANNs**）一无所知；我们将从这些内容开始。当然，如果你是深度学习的专家，你可以跳过本章的前几节，但要考虑到，回顾一下基本知识也无妨。
- en: Before we start going through the theory, you'll begin with real, working code
    written in Python. You'll create some AI first, and then I'll help you understand
    it afterwards. Right now, we're going to build an ANN to predict house prices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始讲解理论之前，你将从写出实际可运行的Python代码开始。你首先会创建一些AI，接下来我会帮助你理解它。现在，我们将构建一个人工神经网络来预测房价。
- en: Predicting house prices
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测房价
- en: 'What we want to do is predict how much a certain house might cost, based on
    some variables. In order to do so you need to follow these four steps:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的是根据一些变量预测某个房子的价格。为了实现这个目标，你需要遵循以下四个步骤：
- en: Get some historical data on house sales; for this example, you'll use a dataset
    of about 20,000 houses in Seattle.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一些关于房屋销售的历史数据；在这个例子中，你将使用一份包含大约20,000套房子的西雅图数据集。
- en: Import this data to your code while applying some scaling to your variables
    (I'll explain scaling to you as we go).
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的代码中导入这些数据，同时对变量应用一些缩放（我会在过程中解释缩放）。
- en: Build an Artificial Neural Network using any library—you'll use Keras, as it
    is simple and reliable.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用任何库构建人工神经网络——你将使用Keras，因为它简单且可靠。
- en: Train your ANN and get the results.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练你的人工神经网络（ANN），并获得结果。
- en: Now that you know the structure of your future code, you can start writing it.
    Since all the libraries that you'll use are available in Google Colab, you can
    easily use it to perform this task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了未来代码的结构，你可以开始编写代码了。由于你将使用的所有库都可以在Google Colab中找到，因此你可以轻松地使用它来完成这项任务。
- en: Uploading the dataset
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上传数据集
- en: Start by creating a new Google Colab notebook. Once we have created your new
    notebook, before you start coding anything, you have to upload your dataset. You
    can find this dataset, called `kc_house_data.csv`, on the GitHub repository in
    the `Chapter 09` folder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从创建一个新的Google Colab笔记本开始。一旦创建了新笔记本，在你开始编码之前，你需要上传数据集。你可以在GitHub的`第9章`文件夹中找到这个数据集，名为`kc_house_data.csv`。
- en: '![](img/B14110_09_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_01.png)'
- en: 'Figure 1: GitHub – Chapter 09'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：GitHub – 第9章
- en: 'Once you have done that, you can upload it to Colab by doing the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，你可以通过以下方式将其上传到Colab：
- en: Click this little arrow here:![](img/B14110_09_02.png)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击这里的小箭头：![](img/B14110_09_02.png)
- en: 'Figure 2: Google Colab – Uploading files (1/3)'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2：Google Colab – 上传文件（1/3）
- en: In the window that pops up, go to **Files**. You should get something like this:![](img/B14110_09_03.png)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出的窗口中，进入**文件**。你应该会看到类似这样的内容：![](img/B14110_09_03.png)
- en: 'Figure 3: Google Colab – Uploading files (2/3)'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3：Google Colab – 上传文件（2/3）
- en: Click on **UPLOAD** and then select the file location where you saved the `kc_house_data`
    dataset.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**上传**，然后选择你保存`kc_house_data`数据集的文件位置。
- en: After you have done that, you should get a new folder with our dataset, like
    this:![](img/B14110_09_04.png)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，你应该会得到一个包含我们数据集的新文件夹，像这样：![](img/B14110_09_04.png)
- en: 'Figure 4: Google Colab – Uploading files (3/3)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4：Google Colab – 上传文件（3/3）
- en: Great! Now you can start coding.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在你可以开始编码了。
- en: Importing libraries
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入库
- en: 'Every time you start coding something you ought to begin by importing the necessary
    libraries. Therefore, we start our code with these lines:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每次开始编码时，你应该先导入必要的库。因此，我们的代码从这些行开始：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In lines 4 and 5, after the comment, you import the `pandas` and `numpy` libraries.
    Pandas will help you read the dataset and NumPy is very useful when you're dealing
    with arrays or lists; you'll use it to drop some unnecessary columns from your
    dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 和第 5 行，在注释之后，你导入了 `pandas` 和 `numpy` 库。Pandas 将帮助你读取数据集，而 NumPy 在处理数组或列表时非常有用；你将用它来删除数据集中一些不必要的列。
- en: In the two subsequent lines you import two useful tools from the Scikit-Learn
    library. The first one is a tool that will help split the dataset into a training
    set and a test set (you should always have both of them; the AI model is trained
    on the training set and then tested on the test set) and the second one is a scaler
    that will help you later when scaling values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两行代码中，你从 Scikit-Learn 库中导入了两个有用的工具。第一个工具帮助你将数据集拆分成训练集和测试集（你应该始终有这两者；AI
    模型在训练集上进行训练，然后在测试集上进行测试），第二个工具是一个缩放器，稍后你在缩放值时会用到它。
- en: Lines 9, 10, and 11 are responsible for importing the `keras` library, which
    you'll use in order to build a neural network. Each of these tools is used later
    in the code.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9、10 和 11 行负责导入 `keras` 库，你将在其中构建神经网络。每个工具稍后会在代码中使用。
- en: 'Now that you have imported your libraries you can read the dataset. Do it by
    using the Pandas library you imported before, with this one line:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经导入了库，可以开始读取数据集。只需使用之前导入的 Pandas 库，通过这一行代码来完成：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since you used `pd` as an abbreviation for the Pandas library when you imported
    it, you can use it to shorten your code. After you call the Pandas library with
    `pd`, you can use one of its functions, `read_csv`, which, as the name suggests,
    reads csv files. Then in the brackets you input the file name, which in your case
    is `kc_house_data.csv`. No other arguments are needed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你在导入 Pandas 库时使用了 `pd` 作为缩写，因此可以利用它来简化代码。在调用 Pandas 库时使用 `pd`，然后你可以使用它的函数之一
    `read_csv`，顾名思义，它用于读取 csv 文件。接着在括号中输入文件名，你的文件名是 `kc_house_data.csv`，不需要其他参数。
- en: Now I have a little exercise for you! Have a look at the dataset and try to
    judge which of the variables will matter for our price prediction. Believe me,
    not all of them are relevant. I strongly suggest that you try to do it alone even
    though we'll discuss them in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我给你一个小练习！看一下数据集，尝试判断哪些变量对我们的价格预测有影响。相信我，并不是所有变量都有用。我强烈建议你先尝试自己做，尽管我们将在下一节讨论这些变量。
- en: Excluding variables
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排除的变量
- en: Were you able to discern which variables are necessary and which are not? Don't
    worry if not; we'll explain them and their relevance right now.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你能分辨出哪些变量是必要的，哪些不是吗？如果没有关系，我们现在就来解释它们及其相关性。
- en: 'The following table explains every column in our dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格解释了我们数据集中每一列的含义：
- en: '| **Variable** | **Description** |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **变量** | **描述** |'
- en: '| --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Id | Unique ID for each household |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Id | 每个家庭的唯一 ID |'
- en: '| Date | Date when the house was sold |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Date | 房屋售出的日期 |'
- en: '| Price | How much the house cost when sold |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Price | 房屋售出时的价格 |'
- en: '| Bedrooms | Number of bedrooms |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Bedrooms | 卧室数量 |'
- en: '| Bathrooms | Number of bathrooms; 0.5 represents room with a toilet but no
    shower |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Bathrooms | 卫生间的数量；0.5 代表有马桶但没有淋浴的房间 |'
- en: '| Sqft_living | Square footage of the apartment''s interior living space |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Sqft_living | 公寓内部生活空间的平方英尺数 |'
- en: '| Sqft_lot | Square footage of the land space |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Sqft_lot | 土地空间的平方英尺数 |'
- en: '| Floors | Number of floors |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Floors | 楼层数 |'
- en: '| Waterfront | 0 if the apartment doesn''t overlooking the waterfront, 1 if
    it does |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Waterfront | 如果公寓没有面朝海滨，则为0；如果有则为1 |'
- en: '| View | Value in the range 0-4 depending on how good the view of the property
    is |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| View | 视野质量的值，范围为0-4，取决于房产视野的好坏 |'
- en: '| Condition | Value from 1-5 defining the condition of the property |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 条件 | 定义属性条件的1-5的值 |'
- en: '| Grade | Value from 1-13 indicating the design and construction of the building
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Grade | 从 1 到 13 的值，表示建筑的设计和构造 |'
- en: '| Sqft_above | The square footage of the interior housing space that is above
    ground level |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Sqft_above | 地面以上的室内空间的平方英尺数 |'
- en: '| Sqft_basement | The square footage of the basement |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Sqft_basement | 地下室的平方英尺数 |'
- en: '| Yr_built | Year when the house was built |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Yr_built | 房屋建造的年份 |'
- en: '| Yr_renovated | Year when the house was renovated (0 if wasn''t) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Yr_renovated | 房屋翻修的年份（如果没有翻修则为0） |'
- en: '| Zipcode | Zip code of the area house is located in |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Zipcode | 房屋所在区域的邮政编码 |'
- en: '| Lat | Latitude |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Lat | 纬度 |'
- en: '| Long | Longitude |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Long | 经度 |'
- en: '| Sqft_living15 | The square footage of the interior housing living space for
    the nearest 15 neighbors |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Sqft_living15 | 最近15个邻居住宅内部生活空间的平方英尺 |'
- en: '| Sqft_lot15 | Square footage of the land lots of the nearest 15 neighbors
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Sqft_lot15 | 最近15个邻居土地面积的平方英尺 |'
- en: It turns out that from those 21 variables, only 18 count. That is because unique,
    category-like values do not have any impact on your prediction. That includes
    Id, Date, and Zipcode. Price is the target of your prediction, and therefore you
    should get rid of that from your variables as well. After all that, you have 17
    independent variables.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，在这21个变量中，只有18个是有效的。因为像Id、Date和Zipcode这样的唯一类别型值对你的预测没有任何影响。Price是你的预测目标，因此你也应该将其从变量中去除。完成这些之后，你将剩下17个独立变量。
- en: Now that we have explained all the variables and decided which are relevant
    and which are not, you can go back to your code. You're going to exclude these
    unnecessary variables and split the dataset into the features and the target (in
    our case the target is price).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了所有变量，并决定了哪些是相关的，哪些是不相关的，你可以回到你的代码了。你将排除这些不必要的变量，并将数据集拆分为特征和目标（在我们的例子中，目标是价格）。
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: On line 17, you take all rows and all columns starting with the fourth one (since
    you're excluding Id, Date, Price) from your dataset and call this new set `X`.
    You use `.iloc` to slice the dataset, and then take `.values` to change it to
    a NumPy object. These will be your features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第17行，你从数据集中选择所有行和从第四列开始的所有列（因为你排除了Id、Date和Price），并将这个新集称为`X`。你使用`.iloc`来切片数据集，然后使用`.values`将其转换为NumPy对象。这些将是你的特征。
- en: Next you need to exclude Zipcode, which quite unfortunately is in the middle
    of the features set. That's why you have to use a NumPy function (`np.r_`) that
    separates `X`, excludes the columns you choose (in this case it is column 14\.
    13 is the index of this column, since indexes in Python start with zero; it's
    also worth mentioning that upper bounds are excluded in Python notation, which
    is why we write `0:13`), and then connects them once again to form a new array.
    In the next line, you get the target of your prediction and call it `y`. This
    corresponds to the third column in your dataset, that is, Price.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你需要排除Zipcode，遗憾的是它正好位于特征集的中间。因此，你必须使用NumPy函数（`np.r_`）来分割`X`，排除你选择的列（在这个案例中是第14列。13是这列的索引，因为Python的索引是从零开始的；值得一提的是，Python中的上界是排除的，所以我们写`0:13`），然后再将它们合并成一个新的数组。在下一行，你获取你的预测目标并将其称为`y`。这对应于数据集中的第三列，即Price。
- en: Data preparation
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Now that you''ve separated your important features and target, you can split
    your `X` and `y` into training and test sets. We do that with the following line:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经分离了重要的特征和目标，你可以将`X`和`y`拆分成训练集和测试集。我们用以下这行代码来做到这一点：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is very important when doing any kind of machine learning. You always have
    to have a training set on which you train your model, and a test set on which
    you test it. You perform that operation using the `train_test_split` function
    you imported before. After doing that, you get `X_train`, which is of equal size
    to `y_train`, and each of them are exactly 80% of our previous `X` and `y` set.
    `X_test` and `y_test` are made up of the remaining 20% of `X` and `y`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是进行任何机器学习时非常重要的一步。你总是需要有一个训练集来训练你的模型，以及一个测试集来测试它。你通过之前导入的`train_test_split`函数来执行这个操作。执行后，你将得到`X_train`，它的大小与`y_train`相等，每个都正好是我们之前`X`和`y`集的80%。`X_test`和`y_test`则由剩下的20%的`X`和`y`组成。
- en: Now that you have both a training set and a test set, what do you think the
    next step is? Well, you have to scale your data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了训练集和测试集，你认为接下来的步骤是什么？嗯，你需要对数据进行缩放。
- en: Scaling data
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据缩放
- en: Now you might be wondering why on earth you have to perform such an operation.
    You already have the data, so why not build and train the neural network already?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会想，为什么你要进行这样的操作？你已经有了数据，为什么不直接构建并训练神经网络呢？
- en: There's a problem with that; if we leave the data as it is, you'll notice that
    your ANN does not learn. The reason for that is because different variables will
    impact your prediction more or less depending on their values.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个问题；如果我们保持数据不变，你会发现你的人工神经网络（ANN）无法学习。原因在于，不同的变量会根据它们的值对你的预测产生不同程度的影响。
- en: Take this graph illustrating what I mean, based on a property that has 3 bedrooms
    and 1,350 square feet of living area.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个图示，说明我想表达的意思，这个图基于一个有3个卧室和1350平方英尺生活空间的房产。
- en: '![](img/B14110_09_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_05.png)'
- en: 'Figure 5: Example for 3 bedrooms and 1350 square feet of living area'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：3卧室和1350平方英尺的居住面积示例
- en: You can clearly see that the number of bedrooms won't affect the prediction
    as much as Sqft_living will. Even we humans cannot see any difference between
    zero bedrooms and three bedrooms on this graph.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到，卧室数量不会像`Sqft_living`那样显著影响预测。即使是我们人类，也看不出在这张图上零卧室和三卧室之间有什么区别。
- en: 'One of many solutions to this problem is to scale all variables to be in a
    range between 0 and 1\. We achieve this by calculating this equation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的众多方法之一是将所有变量缩放到0和1之间。我们通过计算以下公式来实现这一点：
- en: '![](img/B14110_09_001.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_001.png)'
- en: 'where:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*x* – the value we are scaling in our case every value in a column'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* – 在我们这个例子中是列中的每个值'
- en: '*x*[min] – minimum value across all in a column'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[min] – 列中所有值的最小值'
- en: '*x*[max] – maximum value across all in a column'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[max] – 列中所有值的最大值'
- en: '*x*[scaled] – *x* after performing scaling'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[scaled] – 执行缩放后的*x*'
- en: 'After performing this scaling, our previous graph now looks something like
    this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行缩放之后，我们之前的图表现在看起来是这样的：
- en: '![](img/B14110_09_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_06.png)'
- en: 'Figure 6: Same graph after scaling'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：缩放后相同的图表
- en: Now we can undoubtedly say that the number of bedrooms will have a similar impact
    to Sqft_living. We can clearly see the difference between zero bedrooms and three
    bedrooms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以毫无疑问地说，卧室数量将对房价产生类似的影响，和`Sqft_living`一样。我们可以清楚地看到零卧室和三卧室之间的差异。
- en: So, how do we implement that in code? Since you know the equation, I recommend
    that you try to do it yourself. Don't worry if you fail; I'll show you a very
    simple way to do it in the next paragraph.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在代码中实现这一点呢？既然你知道公式了，我建议你自己尝试一下。如果失败了也不用担心，我会在下一段向你展示一种非常简单的实现方法。
- en: 'If you were able to scale the data on your own, then congratulations! If not,
    follow along through this next section to see the answer. You might have noticed
    that you imported a class of Scikit-learn library called `MinMaxScaler`. You can
    use that class to scale the variables with the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够自己完成数据缩放，那么恭喜你！如果没有，继续阅读接下来的部分来查看答案。你可能已经注意到你导入了Scikit-learn库中的`MinMaxScaler`类。你可以使用这个类来缩放变量，代码如下：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code creates two scalers, one to scale the features and one to scale the
    targets. Call them `xscaler` and `yscaler`. The `feature_range` argument is the
    range to which you want your data to be scaled (from 0 to 1 in your case).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了两个缩放器，一个用来缩放特征，另一个用来缩放目标。将它们分别称为`xscaler`和`yscaler`。`feature_range`参数指定你希望数据缩放到的范围（在你的例子中是从0到1）。
- en: Then you use the `fit_transform` method, which scales `X_train` and `y_train`
    and adjusts the scalers based on these sets (`fit` part of this method sets *x*[min]
    and *x*[max]). After that you use the `transform` method to scale `X_test` and
    `y_test` without adjusting `yscaler` and `xscaler`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你使用`fit_transform`方法，它会对`X_train`和`y_train`进行缩放，并根据这两个数据集调整缩放器（该方法的`fit`部分会设置*x*[min]和*x*[max]）。之后，你使用`transform`方法对`X_test`和`y_test`进行缩放，而不调整`yscaler`和`xscaler`。
- en: When scaling the `y` variables, you have to reshape them by using `.reshape(-1,1)`
    in order to create a fake second dimension (so the code can treat this one-dimensional
    array as a two-dimensional array with one column). We need this fake second dimension
    to avoid a format error.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在缩放`y`变量时，必须使用`.reshape(-1,1)`将其重塑，以便创建一个假的第二维（这样代码就可以将这个一维数组当作具有一列的二维数组来处理）。我们需要这个假的第二维来避免格式错误。
- en: If you still do not understand why we have to use scaling, please read this
    section once again. It'll also get clearer once we go through the theory.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然不明白为什么我们必须使用缩放，请再次阅读这一部分内容。等到我们学习完理论部分后，你会更加明白。
- en: Finally, you can proceed to building a neural network! Keep in mind that all
    the theory behind it will be covered later in the chapter, so don't be scared
    if you have trouble understanding something.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以开始构建神经网络了！请记住，所有的理论内容会在后面的章节中讲解，所以如果你遇到理解上的困难，别怕，我会帮助你理清思路。
- en: Building the neural network
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: 'To build the neural network, you can use a highly reliable and easy to use
    library called Keras. Let''s get straight into coding it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建神经网络，你可以使用一个非常可靠且易于使用的库——Keras。让我们直接开始编写代码：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In line 35 of the code block you instantiate your model by using the `Sequential`
    class from the Keras library.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码块的第35行，你通过使用Keras库中的`Sequential`类来实例化你的模型。
- en: Next, you add a line that adds a new layer with 64 neurons to your neural network.
    `kernel_initializer` is an argument that defines the way the initial weights are
    created in the layer, `activation` is the activation function of this layer and
    `input_dim` is the size of the input; in your case, these are the 17 features
    that define how much a house costs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你添加一行代码，为你的神经网络添加一个包含64个神经元的新层。`kernel_initializer`是一个定义层中初始权重创建方式的参数，`activation`是该层的激活函数，`input_dim`是输入大小；在你的情况下，这些是定义房价的17个特征。
- en: Next, you add two more layers, one with 16 neurons and one with 1 neuron that
    will be the output of the neural network.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你添加两个新层，一个有16个神经元，另一个有1个神经元，作为神经网络的输出。
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you don't know what I'm talking about right now, what activations, losses,
    and optimizers are, you don't have to worry. You'll understand them soon, when
    we get to the theory later in the chapter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在不知道我在讲什么，什么是激活函数、损失函数和优化器，你不用担心。你很快就会理解它们，当我们在后面的章节中讲解理论时。
- en: Training the neural network
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Now that you've built your model, you can finally train it!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经建立了模型，终于可以开始训练它了！
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This simple one-liner is responsible for learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的一行代码负责学习。
- en: As the first two arguments of this fit method, you input `X_train` and `y_train`
    which are the sets your model will be trained on. Then you have an argument called
    `batch_size`; this defines after how many records in your dataset you update your
    weights (loss is summed up and back-propagated after `batch_size` inputs). Next
    you have `epochs`, and this value defines how many times you teach your model
    on the entire `X_train` and `y_train` set. The final argument is `validation_data`,
    and there, as you can see, you put `X_test` and `y_test`. This means that after
    every epoch, your model will be tested on this set, but it won't learn from it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作为该`fit`方法的前两个参数，你输入了`X_train`和`y_train`，它们是你的模型将用于训练的集合。然后是一个名为`batch_size`的参数；它定义了在数据集中的多少条记录后，权重会被更新（损失会在`batch_size`输入后进行累加和反向传播）。接下来是`epochs`，这个值定义了你的模型将对整个`X_train`和`y_train`集合学习多少次。最后一个参数是`validation_data`，在这里，你可以看到，你放入了`X_test`和`y_test`。这意味着在每次epoch后，你的模型都会在这个数据集上进行测试，但不会从中学习。
- en: Displaying results
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 显示结果
- en: You're nearly there; you have just one last non-obligatory step to take. You
    calculate the absolute error on the test set and see its real, unscaled predictions
    (actual prices, not in the range (0,1)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你快完成了；只剩下一个非强制性的步骤了。你计算测试集上的绝对误差，并查看它的真实未缩放预测值（实际价格，而非(0,1)区间内的值）。
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You rescale back your `y_test` on line 45\. Then, you make a prediction on your
    test set of features and rescale it back too, since the predictions are also scaled
    down.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在第45行，你将`y_test`重新缩放回去。然后，你对测试集的特征进行预测，并将其也重新缩放回来，因为预测值也已经被缩小。
- en: 'In the last two lines you calculate the absolute error using the formula:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两行，你使用公式计算绝对误差：
- en: '![](img/B14110_09_002.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_002.png)'
- en: Since both prediction and `y_test` are NumPy arrays, you can divide them by
    simply using the `/` symbol. In the last line, you calculate the mean error using
    a NumPy function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预测值和`y_test`都是NumPy数组，你可以直接使用`/`符号进行除法运算。在最后一行，你使用NumPy函数计算平均误差。
- en: Superb! Now that you have it all finished, you can finally run this code and
    see the results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在你已经完成所有工作，终于可以运行这段代码并查看结果了。
- en: '![](img/B14110_09_07.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_07.png)'
- en: 'Figure 7: Results'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：结果
- en: As you can see in the last line, your result is shown. In my case the average
    error was 13.5%. That is a really good result!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如最后一行所示，结果已经显示出来。我的情况下，平均误差是13.5%。这个结果非常好！
- en: Now we can get into the theory behind deep learning, and find out how a neural
    network really works.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进入深度学习背后的理论，了解神经网络是如何工作的。
- en: Deep learning theory
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习理论
- en: 'Here is our plan of attack to go pro and tackle deep learning:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的攻坚计划，准备转行成为深度学习高手：
- en: The neuron
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经元
- en: The activation function
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数
- en: How do neural networks work?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络是如何工作的？
- en: How do neural networks learn?
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络是如何学习的？
- en: Forward-propagation and back-propagation
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正向传播和反向传播
- en: Gradient descent, including Batch, Stochastic, and Mini-Batch methods
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降，包括批量法、随机法和小批量法
- en: I hope you're excited about this section—deep learning is an awesome and powerful
    field to study.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你对这一部分感到兴奋——深度学习是一个很棒且强大的学习领域。
- en: The neuron
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元
- en: The neuron is the basic building block of Artificial Neural Networks, and they
    are based on the neuron cells found the brain.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是人工神经网络的基本构建模块，它们基于大脑中的神经元细胞。
- en: Biological neurons
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生物神经元
- en: 'In the following images are real-life neurons that have been smeared onto a
    slide, colored a little bit, and observed through a microscope:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片是现实生活中的神经元，它们被涂抹到玻片上，稍微上了些颜色，并通过显微镜观察：
- en: '![](img/B14110_09_08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_08.png)'
- en: 'Figure 8: The neuron'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：神经元
- en: 'As you can see, they have the structure of a central body with lots of different
    branches coming out of it. The question is: How can we recreate that in a machine?
    We really want to recreate it in a machine, since the whole purpose of deep learning
    is to mimic how the human brain works in the hope that by doing so we create something
    amazing: a powerful infrastructure for learning machines.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，它们有一个中央主体，周围有许多不同的分支。问题是：我们如何将它在机器中重现呢？我们真的希望在机器中重现它，因为深度学习的整个目的就是模仿人脑的工作方式，希望通过这样做创造出一些惊人的东西：一个强大的学习机器基础设施。
- en: Why do we hope for that? Because the human brain just happens to be one of the
    most powerful learning tools on the planet. We hope that if we recreate it, then
    we'll have something just as awesome as that.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们希望这样做？因为人脑恰好是地球上最强大的学习工具之一。我们希望，如果我们能够重现它，那么我们将拥有与之同样令人惊叹的东西。
- en: Our challenge right now, our very first step in creating artificial neural networks,
    is to recreate a neuron. So how do we do it? Well, first of all let's take a closer
    look at what a neuron actually is.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前面临的挑战，创建人工神经网络的第一步，是重现一个神经元。那么我们该如何做到这一点呢？首先，让我们仔细看看神经元到底是什么。
- en: 'In 1899, the neuroscientist Santiago Ramón y Cajal dyed neurons in actual brain
    tissue, and looked at them under a microscope. While he was looking at them, he
    drew what he saw, which was something very much like the slides we looked at before.
    Today, technology has advanced quite a lot, allowing us to see neurons much more
    closely and in more detail. That means that we can draw what they look like diagrammatically:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 1899年，神经学家圣地亚哥·拉蒙·卡哈尔将神经元染色后放在实际的脑组织上，并用显微镜观察它们。观察时，他画下了他所看到的内容，这和我们之前看到的玻片图非常相似。如今，技术已经取得了很大进步，使我们能够更近距离、更详细地观察神经元。这意味着我们可以将它们的外观以示意图的形式绘制出来：
- en: '![](img/B14110_09_09.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_09.png)'
- en: 'Figure 9: The neuron''s structure'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：神经元的结构
- en: This neuron exchanges signals between its neighbor neurons. The dendrites are
    the receivers of the signal and the axon is the transmitter of the signal.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经元在与其邻近的神经元之间交换信号。树突是信号的接收器，轴突是信号的传递器。
- en: The dendrites of the neuron are connected to the axons of other neurons above
    it. When the neuron fires, the signal travels down its axon and passes on to the
    dendrites of the next neuron. That is how they are connected, and how a neuron
    works. Now we can move from neuroscience to technology.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的树突与其他神经元的轴突连接。当神经元发射信号时，信号沿着轴突传递，并传递给下一个神经元的树突。这就是它们的连接方式，也是神经元工作的方式。现在我们可以从神经科学转向技术领域了。
- en: Artificial neurons
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'Here''s how a neuron is represented inside an Artificial Neural Network:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是神经元在人工神经网络中的表现方式：
- en: '![](img/B14110_09_10.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_10.png)'
- en: 'Figure 10: An Artificial Neural Network with a single neuron'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：带有单个神经元的人工神经网络
- en: Just like a human neuron, it gets some input signals and it has an output signal.
    The blue arrow connecting the input signals to the neuron, and the neuron to the
    output signal, are like the synapses in the human neuron.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人类神经元一样，它接收一些输入信号，并且有一个输出信号。连接输入信号到神经元，以及神经元到输出信号的蓝色箭头，就像人类神经元中的突触。
- en: 'Here in the artificial neuron, what exactly are the input and output signals
    going to be? The input signals are the scaled independent variables composing
    the states of the environment. For example, in the server cooling practical example
    we''ll code later in this book (*Chapter 11*, *AI for Business – Minimize Costs
    with Deep Q-Learning*), these are the temperature of the server, the number of
    users, and the rate of data transmission. The output signal is the output values,
    which in a deep Q-learning model are always the Q-Values. Knowing all that, we
    can make a general representation of a neuron for machines:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个人工神经元中，输入和输出信号究竟是什么呢？输入信号是缩放后的独立变量，构成了环境的状态。例如，在本书稍后会编写的服务器冷却实际示例中（*第 11
    章*，*面向商业的 AI – 使用深度 Q 学习最小化成本*），这些输入信号包括服务器的温度、用户数量和数据传输率。输出信号是输出值，在深度 Q 学习模型中，这些值始终是
    Q 值。了解了这些之后，我们可以对机器的神经元做一个通用的表示：
- en: '![](img/B14110_09_11.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_11.png)'
- en: 'Figure 11: Neuron – The output values'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：神经元 – 输出值
- en: 'To finish describing the neuron, we need to add the last element missing from
    this representation, which is also the most important one: the weights.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成对神经元的描述，我们需要添加这个表示中缺失的最后一个元素，这也是最重要的元素：权重。
- en: 'Each synapse (blue arrow) is attributed a weight. The larger the weight, the
    stronger the signal is through the synapse. What is fundamental to understand
    is that these weights are what the machine updates over time to improve its predictions.
    Let''s add them to the previous graphic, to make sure you can visualize them well:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个突触（蓝色箭头）都会被分配一个权重。权重越大，通过突触的信号就越强。理解这一点的关键是，这些权重就是机器随时间更新的部分，以改进其预测。让我们将它们添加到之前的图表中，确保你能清楚地看到它们：
- en: '![](img/B14110_09_12.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_12.png)'
- en: 'Figure 12: Neuron – The weights'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：神经元 – 权重
- en: That's the neuron. The next thing to understand is the activation function;
    the way the neuron decides what output to produce given a set of inputs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经元。接下来需要理解的是激活函数；它是神经元在给定一组输入时决定产生何种输出的方式。
- en: The activation function
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'The activation function is the function *![](img/B14110_09_050.png)*, operating
    inside the neuron, that takes as inputs the linear sum of the input values multiplied
    by their associated weights, and that returns the output value as shown in the
    following graphic:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是*！[](img/B14110_09_050.png)*，它在神经元内部工作，输入是输入值的加权和，返回的输出值如下面的图所示：
- en: '![](img/B14110_09_13.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_13.png)'
- en: 'Figure 13: The activation function'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：激活函数
- en: 'such that:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示：
- en: '![](img/B14110_09_003.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_003.png)'
- en: 'Your next question is probably: what exactly is the function *![](img/B14110_09_050.png)*?'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你下一个问题可能是：*！[](img/B14110_09_050.png)* 究竟是什么函数？
- en: 'There can be many of them, but here we''ll describe the three most used ones,
    including the one you''ll use in the practical activity:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可能有很多种，但在这里我们将描述三种最常用的，包括你在实际活动中会使用的那种：
- en: The threshold activation function
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阈值激活函数
- en: The sigmoid activation function
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数
- en: The rectifier activation function
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整流激活函数
- en: Let's push your expertise further by having a look at them one by one.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过逐一查看它们来进一步推动你的专业知识。
- en: The threshold activation function
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阈值激活函数
- en: 'The threshold activation function is simply defined by the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值激活函数仅由以下公式定义：
- en: '![](img/B14110_09_004.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_004.png)'
- en: 'and can be represented by the following curve:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以用以下曲线表示：
- en: '![](img/B14110_09_14.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_14.png)'
- en: 'Figure 14: The threshold activation function'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：阈值激活函数
- en: 'This means that the signal passing through the neuron is discontinuous, and
    will only be activated if:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着通过神经元的信号是不连续的，并且只有在以下情况下才会激活：
- en: '![](img/B14110_09_0011.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_0011.png)'
- en: 'Now let''s have a look at the next activation function: the sigmoid activation
    function. The sigmoid activation function is the most effective and widely used
    one in Artificial Neural Networks, but mostly in the last hidden layer that leads
    to the output layer.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看下一个激活函数：Sigmoid 激活函数。Sigmoid 激活函数是人工神经网络中最有效且最广泛使用的激活函数，尤其是在通向输出层的最后一个隐藏层中。
- en: The sigmoid activation function
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数
- en: 'The sigmoid activation function is defined by the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数由以下公式定义：
- en: '![](img/B14110_09_007.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_007.png)'
- en: 'and can be represented by the following curve:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以用以下曲线表示：
- en: '![](img/B14110_09_15.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_15.png)'
- en: 'Figure 15: The sigmoid activation function'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：Sigmoid 激活函数
- en: 'This means that the signal passing through the neuron is continuous and will
    always be activated. And the higher the value of:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着通过神经元传递的信号是连续的，并且会一直被激活。而且值越高：
- en: '![](img/B14110_09_008.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_008.png)'
- en: the stronger the signal.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 信号越强。
- en: 'Now let''s have a look at another widely used activation function: the rectifier
    activation function. You''ll find it in most of the deep neural networks, but
    mostly inside the early hidden layers, as opposed to the sigmoid function, which
    is rather used for the last hidden layer leading to the output layer.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下另一个广泛使用的激活函数：修正线性激活函数（rectifier activation function）。你会在大多数深度神经网络中找到它，但主要是在早期的隐藏层，而不像
    sigmoid 函数，它主要用于最后一个隐藏层，通向输出层。
- en: The rectifier activation function
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 修正线性激活函数
- en: 'The rectifier activation function is simply defined by the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性激活函数的定义非常简单，如下所示：
- en: '![](img/B14110_09_009.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_009.png)'
- en: 'and is therefore represented by the following curve:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它由以下曲线表示：
- en: '![](img/B14110_09_16.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_16.png)'
- en: 'Figure 16: The rectifier activation function'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：修正线性激活函数
- en: 'This means that the signal passing through the neuron is continuous, and will
    only be activated if:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着通过神经元传递的信号是连续的，只有当值满足特定条件时才会被激活：
- en: '![](img/B14110_09_0012.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_0012.png)'
- en: The higher the weighted sum of inputs, the stronger the signal.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输入加权和越高，信号越强。
- en: 'That raises the question: which activation function should you choose, or,
    as it''s more frequently asked, how do you know which one to choose?'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：你应该选择哪个激活函数，或者说，如何知道选择哪个激活函数？
- en: The good news is that the answer is simple. It actually depends on what gets
    returned as the dependent variable. If it's a binary outcome, 0 or 1, then a good
    choice would be the threshold activation function. If what you want returned is
    the probability that the dependent variable is 1, then the sigmoid activation
    function is an excellent choice, since its sigmoid curve is a perfect fit to model
    probabilities.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，答案很简单。其实它取决于作为因变量返回的是什么。如果是二元结果，即 0 或 1，那么一个好的选择是阈值激活函数。如果你想要返回的是因变量为 1
    的概率，那么 sigmoid 激活函数是一个非常合适的选择，因为它的 sigmoid 曲线非常适合建模概率。
- en: 'To recap, here''s the small blueprint highlighted in this figure:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，下面是这个图中突出显示的小蓝图：
- en: '![](img/B14110_09_17.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_17.png)'
- en: 'Figure 17: Activation function blueprint'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：激活函数蓝图
- en: Remember, the rectifier activation function should be used within the hidden
    layers of a deep neural network with more than one hidden layer, and the sigmoid
    activation function should be used in the last hidden layer leading to the output
    layer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，修正线性激活函数应该在具有多个隐藏层的深度神经网络中的隐藏层中使用，而 sigmoid 激活函数应该用于最后一个隐藏层，通向输出层。
- en: 'Let''s highlight this in the following figure so that you can visualize it
    and remember it better:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下面的图中突出显示这一点，以便你能更好地视觉化并记住它：
- en: '![](img/B14110_09_18.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_18.png)'
- en: 'Figure 18: Different activation functions in different layers'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：不同层中的激活函数
- en: We're progressing fast! You already know quite a lot about deep learning. It's
    not over yet though—let's move on to the next section to explain how neural networks
    actually work.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进展得很快！你已经掌握了相当多的深度学习知识。但这还没有结束——让我们继续下一部分，解释神经网络到底是如何工作的。
- en: How do neural networks work?
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络是如何工作的？
- en: 'To explain this, let''s go back to the problem of predicting real estate prices.
    We had some independent variables which we were using to predict the price of
    houses and apartments. For simplicity''s sake, and to be able to represent everything
    in a graph, let''s say that our only independent variables (our predictors) are
    the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们回到预测房地产价格的问题。我们有一些独立变量，用来预测房屋和公寓的价格。为了简化并能够将所有内容表示在图表中，假设我们唯一的独立变量（我们的预测变量）是以下内容：
- en: Area (square feet)
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 面积（平方英尺）
- en: Number of bedrooms
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卧室数量
- en: Distance to city (miles)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到市中心的距离（英里）
- en: Age
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 年龄
- en: Our dependent variable is the apartment price that we're predicting. Here's
    how the magic works in deep learning.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的因变量是我们预测的公寓价格。以下是深度学习中“魔法”是如何运作的。
- en: A weight is attributed to each of the independent, scaled variables in such
    a way that the higher the weight is, the more of an effect the independent variable
    will have on the dependent variable; that is, the stronger a predictor it will
    be of the dependent variable.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个独立的、经过缩放的变量都会被分配一个权重，使得权重越高，独立变量对因变量的影响就越大；也就是说，它将是一个更强的因变量预测器。
- en: As soon as new inputs enter the neural network, the signals are forward-propagated
    from each of the inputs, reaching the neurons of the hidden layer.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦新的输入进入神经网络，信号就会从每个输入开始前向传播，最终到达隐藏层的神经元。
- en: Inside each neuron of the hidden layer, the activation function is applied,
    so that the lower the weight of the input, the more the activation function blocks
    the signal coming from that input, and the higher the weight of that input, the
    more the activation function lets that signal go through.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个隐藏层神经元内部，都会应用激活函数，权重越小，激活函数就越会阻止来自该输入的信号，而权重越大，激活函数则越容易让该信号通过。
- en: 'Finally, all the signals coming from the hidden neurons, more or less blocked
    by the activation functions, are forward propagated to the output layer, to return
    the final outcome: the price prediction.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有从隐藏神经元传来的信号（经过激活函数的部分阻断）都会前向传播到输出层，返回最终结果：价格预测。
- en: 'Here''s a visualization of how that neural network works:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是神经网络如何工作的可视化：
- en: '![](img/B14110_09_19.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_19.png)'
- en: 'Figure 19: How Neural Networks work – Example in real estate price prediction'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：神经网络的工作原理 – 房地产价格预测示例
- en: That covers half of the story. Now we know how a neural network works, we need
    to find out how it learns.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是故事的一半。现在我们知道神经网络如何工作，我们需要了解它是如何学习的。
- en: How do neural networks learn?
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络是如何学习的？
- en: 'Neural networks learn by updating, over many iterations, the weights of all
    the inputs and hidden neurons (when having several hidden layers), always towards
    the same goal: to reduce the loss error between the predictions and the actual
    values.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过在多次迭代中更新所有输入和隐藏神经元的权重（当有多个隐藏层时），始终朝着同一个目标前进：减少预测值和实际值之间的损失误差。
- en: 'In order for neural networks to learn, we need the actual values, which are
    also called the targets. In our preceding example about real estate pricing, the
    actual values are the real prices of the houses and apartments taken from our
    dataset. These real prices depend on the independent variables listed previously
    (area, number of bedrooms, distance to city, and age), and the neural network
    learns to make better predictions of these prices, by running the following process:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让神经网络学习，我们需要实际的值，这些值也称为目标。在我们前面的房地产定价示例中，实际值就是从我们的数据集中提取的房屋和公寓的真实价格。这些真实价格依赖于之前列出的独立变量（面积、卧室数量、距离城市的距离和房龄），而神经网络通过以下过程学习更好地预测这些价格：
- en: The neural network forward propagates the signals coming from the inputs; independent
    variables ![](img/B14110_09_011.png), ![](img/B14110_09_012.png), ![](img/B14110_09_013.png)
    and ![](img/B14110_09_014.png).
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络前向传播来自输入的信号；独立变量 ![](img/B14110_09_011.png)、![](img/B14110_09_012.png)、![](img/B14110_09_013.png)
    和 ![](img/B14110_09_014.png)。
- en: Then it gets the predicted price ![](img/B14110_09_015.png) in the output layer.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它在输出层获得预测价格 ![](img/B14110_09_015.png)。
- en: Then it computes the loss error, *C*, between the predicted price ![](img/B14110_09_016.png)
    (prediction) and the actual price *y* (target):![](img/B14110_09_017.png)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着计算预测价格 ![](img/B14110_09_016.png)（预测值）和实际价格 *y*（目标值）之间的损失误差 *C*：![](img/B14110_09_017.png)
- en: Then this loss error is back-propagated inside the neural network, from right
    to left in our representation.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后这个损失误差会在神经网络中反向传播，从我们表示的右侧传播到左侧。
- en: Then, on each of the neurons, the neural network runs a technique called gradient
    descent (which we will discuss in the next section) to update the weights in the
    direction of loss reduction, that is, into new weights which reduce the loss error
    *C*.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，神经网络在每个神经元上运行一种叫做梯度下降的技术（我们将在下一节中讨论），以更新权重，朝着减少损失的方向前进，也就是更新为减少损失误差 *C* 的新权重。
- en: Then this whole process is repeated many times, with each time new inputs and
    new targets, until we get the desired performance (early stopping) or the last
    iteration (the number of iterations chosen in the implementation).
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后这个过程会反复执行多次，每次都用新的输入和新的目标，直到我们得到期望的性能（早停）或最后一次迭代（实现中的迭代次数）。
- en: Let's show the two main phases, forward-propagation and back-propagation, of
    this whole process in two separate graphics in the next section.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节的两幅单独的图表中展示这个整个过程的两个主要阶段，即前向传播和反向传播的两个主要阶段。
- en: Forward-propagation and back-propagation
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播和反向传播
- en: '**Phase 1: Forward-propagation**:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一阶段：前向传播**：'
- en: 'Here''s how the signal is forward-propagated throughout the artificial neural
    network, from the inputs to the output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是信号如何在人工神经网络中进行前向传播的方式，从输入到输出：
- en: '![](img/B14110_09_20.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_20.png)'
- en: 'Figure 20: Forward-propagation'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：前向传播
- en: Once the signal's been propagated through the entire network, the loss error
    *C* is calculated so that it can be back-propagated.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦信号通过整个网络传播，就会计算损失误差 *C*，以便进行反向传播。
- en: '**Phase 2: Back-propagation**:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二阶段：反向传播**：'
- en: And after forward-propagation comes back-propagation, during which the loss
    error *C* is propagated back into the neural network from the output to the inputs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播后，接着进行反向传播，期间损失误差 *C* 从输出传播回神经网络的输入。
- en: '![](img/B14110_09_21.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_21.png)'
- en: 'Figure 21: Back-propagation'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：反向传播
- en: During back-propagation, the weights are updated to reduce the loss error *C*
    between the predictions (output value) and the targets (actual value). How are
    they updated? This is where gradient descent comes into play.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播期间，权重会更新以减少预测（输出值）与目标（实际值）之间的损失误差 *C*。它们是如何更新的？这就是梯度下降发挥作用的地方。
- en: Gradient Descent
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is an optimization technique that helps us find the minimum
    of a cost function, like the preceding loss error *C* we had:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种优化技术，帮助我们找到代价函数的最小值，就像前面我们所得到的损失误差 *C* 一样：
- en: '![](img/B14110_09_018.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_018.png)'
- en: 'Let''s visualize it in the most intuitive way, like the following ball in a
    bowl (with a little math sprinkled on top):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用最直观的方式来可视化它，就像碗中的这个球（上面稍微加点数学）：
- en: '![](img/B14110_09_22.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_22.png)'
- en: 'Figure 22: Gradient Descent (1/4)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：梯度下降（1/4）
- en: Imagine this is a cross section of a bowl, into which we drop a small red ball
    and let it find its way down to the bottom of the bowl. After some time, it will
    stop rolling, when it finds the sweet spot at the bottom of the bowl.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这是一个碗的横截面，我们往里面放入一个小红球，让它找到碗底的位置。经过一段时间，它会停下来，找到碗底的甜点位置。
- en: You can think about gradient descent in the same way. It starts somewhere in
    the bowl (initial values of parameters) and tries to find the bottom of the bowl,
    or in other words, the minimum of a cost function.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将梯度下降理解为同样的方式。它从碗中的某个位置（参数的初始值）开始，并尝试找到碗底，或者换句话说，代价函数的最小值。
- en: Let's go through the example that is shown in the preceding image. The initial
    values of the parameters have set our ball at the position shown. Based on that
    we get some predictions, which we compare to our target values. The difference
    between these two sets is our loss for the current set of parameters.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过上述图像中显示的示例来进行说明。参数的初始值设置了我们球的位置。基于此，我们得到一些预测结果，然后将其与目标值进行比较。这两组之间的差异就是当前参数集的损失。
- en: Then we calculate the first derivative of the cost function, with respect to
    the parameters. This is where the name **gradient** comes from. Here, this first
    derivative gives us the slope of the tangent to the curve where the ball is. If
    the gradient of the slope is negative, like on the preceding image, we take the
    next step to the right side. If the gradient of the slope is positive, we take
    the next step to the left side.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算成本函数的一阶导数，关于参数的导数。这就是“梯度”一词的来源。在这里，这一阶导数给出了球所在曲线的切线斜率。如果斜率是负的，就像前面的图像上那样，我们向右边迈下一步。如果斜率是正的，我们向左边迈下一步。
- en: 'The name **descent** thus comes from the fact that we always take the next
    step that points downhill, as represented in the following graphic:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，“下降”这个名字来源于我们总是朝着下坡的方向迈下一步，就像下图中所示：
- en: '![](img/B14110_09_23.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_23.png)'
- en: 'Figure 23: Gradient Descent (2/4)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：梯度下降（2/4）
- en: 'In the next position our ball rests on a positive slope, so we have to take
    the next step to the left:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个位置，我们的球停在了一个正斜坡上，所以我们必须向左迈出下一步：
- en: '![](img/B14110_09_24.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_24.png)'
- en: 'Figure 24: Gradient Descent (3/4)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：梯度下降（3/4）
- en: 'Eventually, by repeating the same steps, the ball will end up at the bottom
    of the bowl:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，通过重复相同的步骤，球最终会停在碗底：
- en: '![](img/B14110_09_25.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_25.png)'
- en: 'Figure 25: Gradient Descent (4/4)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25：梯度下降（4/4）
- en: 'And that''s it! That''s how gradient descent operates in one dimension (one
    parameter). Now you might ask: "Great, but how does this scale?" We saw an example
    of one-dimensional optimization, but what about two or even three dimensions?'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这就是梯度下降在一维（一个参数）中的运作方式。现在你可能会问：“好极了，但它如何扩展？”我们看到了一维优化的例子，但二维或三维呢？
- en: 'It''s an excellent question. gradient descent guarantees that this approach
    scales on as many dimensions as needed, provided the cost function is convex.
    In fact, if the cost function is convex, gradient descent will find the absolute
    minimum of the cost function. Following is an example in two dimensions:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的问题。梯度下降保证了这种方法在需要的维度上可以扩展，前提是成本函数是凸的。事实上，如果成本函数是凸的，梯度下降将找到成本函数的绝对最小值。以下是一个二维的例子：
- en: '![](img/B14110_09_26.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_26.png)'
- en: 'Figure 26: Gradient Descent – Convergence guaranteed for convex cost functions'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 26：梯度下降——凸的成本函数保证收敛
- en: 'However, if the cost function is not convex, gradient descent will only find
    a local minimum. Here is an example in three dimensions:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果成本函数不是凸的，梯度下降将只会找到局部最小值。这里是一个三维的例子：
- en: '![](img/B14110_09_27.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_27.png)'
- en: 'Figure 27: Example of non-convergence (right) for a non-convex function (left)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 27：非凸函数（左）的非收敛例子（右）
- en: 'Now that we understand what gradient descent is all about, we can study the
    most advanced and most effective versions of it:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了梯度下降的基本原理，我们可以研究它最先进且最有效的版本：
- en: Batch gradient descent
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Stochastic gradient descent
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Mini-batch gradient descent
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: '"Gradient descent", "batch gradient descent", "mini batch gradient descent",
    "stochastic gradient descent," there are so many terms and someone like you who''s
    just starting may find themselves very confused. Don''t worry—I''ve got your back.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: “梯度下降”，“批量梯度下降”，“小批量梯度下降”，“随机梯度下降”，有这么多术语，对于像你这样的初学者可能会感到非常困惑。别担心——我会帮助你的。
- en: The main difference between all of these versions of gradient descent is just
    the way we feed our data to a model, and how often we update our parameters (weights)
    to move our small red ball. Let's start by explaining batch gradient descent.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些版本的梯度下降之间的主要区别在于我们如何将数据输入模型，以及我们多频繁地更新我们的参数（权重）来移动我们的小红球。我们先从解释批量梯度下降开始。
- en: Batch gradient descent
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Batch gradient descent is when we have a batch of inputs (as opposed to a single
    input) feeding the neural network, forward-propagating them to obtain in the end
    a batch of predictions, which themselves are compared to a batch of targets. The
    global loss error between the predictions and the targets of the two batches is
    then computed as the sum of the loss errors between each prediction and its associated
    target.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降是当我们有一批输入（与单个输入相对）输入神经网络时，前向传播它们，最终获得一批预测，这些预测会与一批目标进行比较。然后，计算预测和目标之间的全局损失误差，作为每个预测与其关联目标之间损失误差的总和。
- en: That global loss is back-propagated into the neural network, where gradient
    descent or stochastic gradient descent is performed to update all the weights,
    according to how much they were responsible for that global loss error.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个全局损失被反向传播到神经网络中，在那里进行梯度下降或随机梯度下降，以根据每个权重对该全局损失误差的责任大小来更新所有权重。
- en: 'Here is an example of batch gradient descent. The problem to solve is about
    predicting the score (from 0 to 100 %) students get in an exam, based on the time
    spent studying (Study Hrs) and the time spent sleeping (Sleep Hrs):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个批量梯度下降的例子。要解决的问题是根据学习时间（Study Hrs）和睡眠时间（Sleep Hrs），预测学生在考试中的得分（从 0 到 100
    %）：
- en: '![](img/B14110_09_28.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_28.png)'
- en: 'Figure 28: Batch Gradient Descent'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28：批量梯度下降
- en: An important thing to note on this preceding graphic is that these are not multiple
    neural networks, but a single one represented by separate weight updates. As we
    can see in this example of batch gradient descent, we feed all of our data into
    the model at once.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，值得注意的一点是，这些并不是多个神经网络，而是由不同的权重更新表示的单个神经网络。正如我们在这个批量梯度下降的例子中看到的那样，我们将所有数据一次性输入模型。
- en: 'This produces collective updates of the weights and fast optimization of the
    network. However, there is a bad side to this as well. There is, once again, the
    possibility of getting stuck in a local minimum, as we can see in the following
    graphic:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了权重的集体更新，并加速了网络的优化。然而，这也有不好的方面。再次出现了可能陷入局部最小值的情况，正如我们在以下图形中看到的那样：
- en: '![](img/B14110_09_29.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_29.png)'
- en: 'Figure 29: Getting stuck in a local minimum'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29：陷入局部最小值
- en: 'We explained the reason why this happens a bit earlier: it is because the cost
    function in the preceding graphic is not convex, and this type of optimization
    (simple gradient descent) requires the cost function to be convex. If that is
    not the case, we can find ourselves stuck in a local minimum and never find the
    global minimum with the optimal parameters. On the other hand, here is an example
    of a convex cost function, the same one as we saw earlier:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经解释了这一现象的原因：是因为前面图示中的成本函数不是凸函数，这种优化方法（简单的梯度下降）要求成本函数是凸的。如果不是这样，我们就可能会陷入局部最小值，无法找到全局最小值和最佳参数。另一方面，这里是一个凸成本函数的例子，和我们之前看到的那个相同：
- en: '![](img/B14110_09_30.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_30.png)'
- en: 'Figure 30: An example of a convex function'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 30：凸函数的例子
- en: In simple terms, a function is convex if it has only one global minimum. And
    the graph of a convex function has the bowl shape. However, in most problems,
    including business problems, the cost function will not be convex (as in the following
    graphic example in 3D), and thus not allow simple gradient descent to perform
    well. This is where stochastic gradient descent comes into play.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，如果一个函数只有一个全局最小值，它就是凸函数。凸函数的图形呈碗状。然而，在大多数问题中，包括商业问题，成本函数通常不是凸函数（如以下 3D 图示例所示），因此，简单的梯度下降法无法表现良好。这就是随机梯度下降发挥作用的地方。
- en: '![](img/B14110_09_31.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_31.png)'
- en: 'Figure 31: Example of non-convergence (right) for a non-convex function (left)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 31：非凸函数（左）未收敛的例子（右）
- en: Stochastic gradient descent
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: '**Stochastic Gradient Descent** (**SGD**) comes to save the day. It provides
    better results overall, preventing the algorithm from getting stuck in a local
    minimum. However, as its name suggests, it is stochastic, or in other words, random.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）来拯救局面。它提供了更好的整体结果，防止算法陷入局部最小值。然而，正如其名字所示，它是随机的，换句话说，它是有随机性的。'
- en: Because of this property, no matter how many times you run the algorithm, the
    process will always be slightly different, regardless of the initialization.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种特性，无论你运行多少次算法，过程都会略有不同，无论初始化如何。
- en: 'SGD does not run on the whole dataset at once, but instead input by input.
    The process goes like this:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 不是一次性在整个数据集上运行，而是逐个输入。过程是这样的：
- en: Input a single observation.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一个单一的观察值。
- en: Forward propagate that input to get a single prediction.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该输入前向传播以得到单一预测值。
- en: Compute the loss error between the prediction (output) and the target (actual
    value).
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测值（输出）与目标值（实际值）之间的损失误差。
- en: Back-propagate the loss error into the neural network.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失误差反向传播到神经网络中。
- en: Update the weights with gradient descent.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新权重。
- en: Repeat steps 1 to 5 through the whole dataset.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对整个数据集重复步骤 1 至 5。
- en: 'Let''s show the first three iterations on the first three single inputs for
    the example we looked at earlier, predicting the scores in an exam:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示前面提到的例子中前 3 次迭代的情况，这里预测的是考试分数：
- en: '**First input row of observation**:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一行观察输入**：'
- en: '![](img/B14110_09_32.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_32.png)'
- en: 'Figure 32: Stochastic Gradient Descent – First input row of observation'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 32：随机梯度下降 - 第一行观察输入
- en: '**Second input row of observation**:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二行观察输入**：'
- en: '![](img/B14110_09_33.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_33.png)'
- en: 'Figure 33: Stochastic Gradient Descent – Second input row of observation'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 33：随机梯度下降 - 第二行观察输入
- en: '**Third input row of observation**:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三行观察输入**：'
- en: '![](img/B14110_09_34.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_34.png)'
- en: 'Figure 34: Stochastic Gradient Descent – Third input row of observation'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 34：随机梯度下降 - 第三行观察输入
- en: Each of the preceding three graphics is an example of one weight's update run
    by SGD. As we can see, each time we only input a single row of observation from
    our dataset to the neural network, then we update the weights accordingly and
    proceed to the next input row of observation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 前面三个图示都是 SGD 更新过程中某个权重的示例。如我们所见，每次我们只将数据集中的一行观测输入神经网络，然后根据该行数据更新权重，并继续输入下一行数据。
- en: At first glance, SGD seems slower, because we input each row separately. In
    reality, it's much faster, because we don't have to load the whole dataset in
    the memory, nor wait for the whole dataset to pass through the model updating
    the weights.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，SGD 似乎更慢，因为我们将每一行数据分别输入。但实际上，它要快得多，因为我们不需要将整个数据集加载到内存中，也不需要等待整个数据集通过模型来更新权重。
- en: 'To finish this section, let''s recap the difference between batch gradient
    descent and SGD with the following graphic:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结这一部分，让我们用下面的图表回顾批量梯度下降和 SGD 之间的区别：
- en: '![](img/B14110_09_35.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_35.png)'
- en: 'Figure 35: Batch Gradient Descent vs. Stochastic Gradient Descent'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 35：批量梯度下降与随机梯度下降
- en: Now we can consider a middle-ground approach; mini-batch gradient descent.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以考虑一种折衷的方法：小批量梯度下降。
- en: Mini-batch gradient descent
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: Mini-batch gradient descent uses the best from both worlds, combining batch
    gradient descent with SGD. This is done by feeding the artificial neural network
    with small batches of data, instead of feeding single input rows of observations
    one by one or the whole dataset at once.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降结合了两者的优点，将批量梯度下降与随机梯度下降（SGD）结合起来。实现方法是通过将小批量数据馈送到人工神经网络中，而不是一行一行地馈送单一输入观测值，或者一次性将整个数据集馈送进网络。
- en: This approach is faster than classic SGD, and still prevents you from getting
    stuck in a local minimum. Mini-batch gradient descent also helps if you don't
    have enough computing resources to load the whole dataset in the memory, or enough
    processing power to get the full benefit of SGD.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法比经典的 SGD 更快，同时仍能防止你陷入局部最小值。小批量梯度下降也很有帮助，特别是当你没有足够的计算资源将整个数据集加载到内存中，或者没有足够的处理能力来充分利用
    SGD 时。
- en: That's all for neural networks! Now you're ready to combine your knowledge of
    neural networks with your knowledge of Q-learning.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是关于神经网络的全部内容！现在你已经准备好将神经网络的知识与 Q-learning 的知识结合起来了。
- en: Deep Q-learning
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度 Q-learning
- en: You've toured the foundations of deep learning, and you already know Q-learning;
    since deep Q-learning consists of combining Q-learning and deep learning, you're
    ready to get an intuitive grasp of deep Q-learning and crush it.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了深度学习的基础，并且已经熟悉了 Q-learning；由于深度 Q-learning 是将 Q-learning 与深度学习结合起来的，你现在准备好直观地掌握深度
    Q-learning，并充分理解它。
- en: Before we start, try to guess some of how this is going to work. I would like
    you to take a moment and think about how you could integrate Q-learning into an
    ANN.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，试着猜一猜这将如何运作。我希望你花点时间思考一下，如何将 Q-learning 整合到人工神经网络（ANN）中。
- en: First things first, you might have guessed what the inputs and outputs of the
    neural network are going to be. The input of the artificial neural network is
    of course going to be the input state, which could be a 1-dimensional vector encoding
    what is happening in the environment, or an image (like the ones seen by a self-driving
    car). And the output is going to be the set of Q-values for each action, meaning
    it is going to be a 1-dimensional vector of several Q-values, one for each action
    that can be performed. Then, just like before, the AI takes the action that has
    the maximum Q-value, and performs it.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可能已经猜到了神经网络的输入和输出是什么。人工神经网络的输入当然是环境的输入状态，它可能是一个 1 维的向量，编码了环境中的变化，或者是一张图像（就像自动驾驶汽车所看到的那样）。输出则是每个动作的
    Q 值集，即它将是一个 1 维的向量，包含多个 Q 值，每个 Q 值对应一个可执行的动作。然后，就像之前一样，AI 会选择具有最大 Q 值的动作并执行。
- en: Very simply, that means that instead of predicting the Q-values through iterative
    updates with the Bellman equation (simple Q-learning), we'll predict them with
    an ANN that takes as inputs the input states, and returns as output the Q-values
    of the different actions.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这意味着我们不再通过贝尔曼方程（简单的 Q-learning）进行迭代更新来预测 Q 值，而是通过人工神经网络来预测 Q 值，神经网络的输入是状态信息，输出则是不同动作的
    Q 值。
- en: 'That raises the question: it''s good that we know what to predict, but what
    are going to be the targets (actual values) of these predictions when we are training
    the AI? As a reminder, the target is the actual value, or what you want your prediction
    to be ideally: the closer your prediction is to the target, the more it is correct.
    That''s why we compute the loss error *C* between the prediction and the target,
    in order to reduce it through back-propagation with stochastic or mini-batch gradient
    descent.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了一个问题：我们知道要预测什么很好，但在训练 AI 时，这些预测的目标（实际值）是什么呢？提醒一下，目标是实际值，或者说你希望预测的理想值：你的预测越接近目标，就越正确。因此，我们计算预测与目标之间的损失误差
    *C*，以便通过反向传播与随机或小批量梯度下降来减少它。
- en: When we were doing simple property price prediction, it was obvious what the
    targets were. They were simply the prices in the dataset that were available to
    us. But what about the targets of Q-values when you are training a self-driving
    car, for example? It's not that obvious, even though it is an explicit function
    of the Q-values and the reward.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们做简单的房价预测时，目标是显而易见的。它们只是数据集中可以获得的价格。但当你在训练自动驾驶汽车时，Q 值的目标是什么呢？这就不那么明显了，尽管它是
    Q 值和奖励的显式函数。
- en: 'The answer is a fundamental formula in deep Q-learning. The target of an input
    state ![](img/B14110_09_019.png) is:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是深度 Q 学习中的一个基本公式。输入状态 ![](img/B14110_09_019.png) 的目标是：
- en: '![](img/B14110_09_020.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_020.png)'
- en: where ![](img/B14110_09_021.png) is the last reward obtained and ![](img/B14110_09_022.png)
    is the discount factor, as seen previously.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B14110_09_021.png) 是最后获得的奖励，![](img/B14110_09_022.png) 是折扣因子，如前所述。
- en: Do you recognize the formula of the target? If you remember Q-learning, you
    should have no problem answering this question.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你认出目标的公式了吗？如果你记得 Q 学习，你应该不难回答这个问题。
- en: 'It''s in the temporal difference, of course! Remember, the temporal difference
    is defined by:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 当然在时间差分中！记住，时间差分定义为：
- en: '![](img/B14110_07_019.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_07_019.png)'
- en: 'So now it''s obvious. The target is simply the first element at the left of
    the temporal difference:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在很明显了。目标就是时间差分左边的第一个元素：
- en: '![](img/B14110_09_024.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_024.png)'
- en: 'so that we get:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得到：
- en: '![](img/B14110_09_025.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_025.png)'
- en: Note that at the beginning, the Q-values are null, so the target is simply the
    reward.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在开始时，Q 值为零，因此目标仅仅是奖励。
- en: There's one more piece to the puzzle before we can say that we really understand
    deep Q-learning; the Softmax method.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能说自己真正理解深度 Q 学习之前，还有一个关键点：Softmax 方法。
- en: The Softmax method
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax 方法
- en: This is the missing piece before we're ready to assemble everything for deep
    Q-learning. The Softmax method is the way we're going to select the action to
    perform after predicting the Q-values. In Q-learning, that was simple; the action
    performed was the one with the highest Q-value. That was the argmax method. In
    deep Q-learning, things are different. The problems are usually more complex,
    and so, in order to find an optimal solution, we must go through a process called
    **Exploration**.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们准备好将一切组合成深度 Q 学习之前的最后一块拼图。Softmax 方法是我们在预测 Q 值后选择执行动作的方式。在 Q 学习中，这很简单；执行的动作是
    Q 值最高的那个。那是 argmax 方法。在深度 Q 学习中，情况不同了。问题通常更加复杂，因此，为了找到最优解，我们必须经历一个叫做 **探索** 的过程。
- en: 'Exploration consists of the following: instead of performing the action that
    has the maximum Q-value (which is called Exploitation), we''re going to give each
    action a probability proportional to its Q-value, such that the higher the Q-value,
    the higher the probability. This creates, exactly, a distribution of the performable
    actions. Then finally, the action performed will be selected as a random draw
    from that distribution. Let me explain with an example.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 探索包括以下内容：我们不会执行具有最大 Q 值的动作（称为利用），而是将每个动作的概率与其 Q 值成正比，即 Q 值越高，概率越高。这样就创建了一个可执行动作的分布。最终，执行的动作将从这个分布中随机抽取。让我用一个例子来说明。
- en: 'Let''s imagine we are building a self-driving car (we actually will, in *Chapter
    10, AI for Autonomous Vehicles - Build a Self-Driving Car*). Let''s say that the
    possible actions to perform are simple: move forward, turn left or turn right.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在构建一辆自动驾驶汽车（实际上我们会在 *第10章，自动驾驶车辆 AI - 构建自动驾驶汽车* 中实现）。假设可以执行的动作很简单：前进、左转或右转。
- en: 'Then, at a specific time, let''s say that our AI predicts the following Q-values:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在某个特定时刻，假设我们的AI预测了以下Q值：
- en: '| Move Forward | Turn Left | Turn Right |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 向前移动 | 向左转 | 向右转 |'
- en: '| 24 | 38 | 11 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 38 | 11 |'
- en: 'The way we can create the distribution of probabilities we need is by dividing
    each Q-value by the sum of the three Q-values, which results each time in the
    probability of a particular action. Let''s perform those sums:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建所需概率分布的方法是将每个Q值除以三个Q值的总和，每次得到一个特定动作的概率。让我们进行这些求和：
- en: '![](img/B14110_09_026.png)Probability of Turning Left![](img/B14110_09_027.png)![](img/B14110_09_028.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_09_026.png)向左转的概率![](img/B14110_09_027.png)![](img/B14110_09_028.png)'
- en: 'Perfect—the probabilities sum to 1 and they are proportional to the Q-values.
    That gives us a distribution of the actions. To perform an action, the Softmax
    method takes a random draw from this distribution, such that:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 完美——这些概率加起来为1，并且与Q值成比例。这为我们提供了动作的分布。为了执行一个动作，Softmax方法从这个分布中随机抽取一个动作，即：
- en: The action of Moving Forward has a 33% chance of being selected.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向前移动的动作有33%的概率被选择。
- en: The action of Turning Left has a 52% chance of being selected.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向左转的动作有52%的概率被选择。
- en: The action of Turning Right has a 15% chance of being selected.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向右转的动作有15%的概率被选择。
- en: Can you feel the difference between Softmax and argmax, and do you understand
    why it is called Exploration instead of Exploitation? With argmax, the action
    *Turn Left* would be the one performed with absolute certainty. That's Exploitation.
    But with Softmax, even though the action *Turn Left* is the one with the highest
    chance of being selected, there's still a chance that the other actions might
    be selected.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 你能感受到Softmax和argmax之间的区别吗？你理解为什么它被称为探索而不是开发吗？在使用argmax时，动作*向左转*将是唯一确定执行的动作。这就是开发。但在使用Softmax时，尽管*向左转*是被选择的概率最高的动作，但仍然有可能选择其他动作。
- en: 'Now, of course, the question is: why do we want to do that? It''s simply because
    we want to explore the other actions, in case they lead to transitions resulting
    in higher rewards than we would obtain with pure exploitation. That often happens
    with complex problems, which are the ones for which deep Q-learning is used to
    find a solution. deep Q-learning finds that solution thanks to its advanced model,
    but also through exploration of the actions. This is a technique in AI called
    Policy Exploration.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当然，问题是：为什么我们要这么做？原因很简单：我们想要探索其他动作，以防它们导致的状态转移带来比纯开发更高的奖励。这通常发生在复杂问题中，而深度Q学习正是用来解决这类问题的。深度Q学习通过其先进的模型找到了解决方案，同时也通过探索动作实现。这是AI中的一种技术，叫做策略探索（Policy
    Exploration）。
- en: As before, the next step is a step back. We're going to recap how deep Q-learning
    works.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所说，下一步是退一步。我们将回顾一下深度Q学习是如何工作的。
- en: Deep Q-learning recap
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度Q学习回顾
- en: Deep Q-learning consists of combining Q-learning with an ANN.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习是将Q学习与人工神经网络（ANN）结合的过程。
- en: Inputs are encoded vectors, each one defining a state of the environment. These
    inputs go into an ANN, where the output contains the predicted Q-values for each
    action.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是编码向量，每个向量定义了环境的一个状态。这些输入进入一个ANN，其中输出包含每个动作的预测Q值。
- en: More precisely, if there are *n* possible actions the AI could take, the output
    of the artificial neural network is a 1D vector comprised of *n* elements, each
    one corresponding to the Q-values of each action that could be performed in the
    current state. Then, the action performed is chosen via the Softmax method.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，如果有*n*个可能的动作可以由AI执行，那么人工神经网络的输出将是一个包含*n*个元素的1D向量，每个元素对应于当前状态下可以执行的每个动作的Q值。然后，所执行的动作是通过Softmax方法选择的。
- en: 'Hence, in each state ![](img/B14110_09_029.png):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每个状态下 ![](img/B14110_09_029.png)：
- en: The prediction is the Q-value ![](img/B14110_09_030.png), where ![](img/B14110_09_031.png)
    is performed by the Softmax method.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测是Q值 ![](img/B14110_09_030.png)，其中 ![](img/B14110_09_031.png) 是通过Softmax方法执行的。
- en: The target is ![](img/B14110_09_032.png).
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标是 ![](img/B14110_09_032.png)。
- en: The loss error between the prediction and the target is the square of the temporal
    difference:![](img/B14110_09_033.png)
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测与目标之间的损失误差是时间差分的平方：![](img/B14110_09_033.png)
- en: This loss error is back-propagated into the neural network, and the weights
    are updated according to how much they contributed to the error, through stochastic
    or mini-batch gradient descent.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失误差会通过反向传播传入神经网络，并且根据它们对误差的贡献，通过随机梯度下降或小批量梯度下降更新权重。
- en: Experience replay
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经验回放
- en: You might noticed that so far we have only considered transitions from one state
    ![](img/B14110_07_018.png) to the next state ![](img/B14110_09_035.png). The problem
    with this is that ![](img/B14110_09_036.png) is most of the time very correlated
    with ![](img/B14110_09_037.png); therefore, the neural network is not learning
    much.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，到目前为止，我们只考虑了从一个状态 ![](img/B14110_07_018.png) 到下一个状态 ![](img/B14110_09_035.png)
    的过渡。这样做的问题在于，![](img/B14110_09_036.png) 与 ![](img/B14110_09_037.png) 大部分时间是高度相关的；因此，神经网络的学习进展不大。
- en: This could be improved if, instead of only considering the last transition each
    time, we considered the last *m* transitions, where *m* is a large number. This
    set of the last *m* transitions is what is called the experience replay memory,
    or simply memory. From this memory we sample some random transitions into small
    batches. Then we train the neural network with these batches to then update the
    weights through mini-batch gradient descent.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们每次不仅仅考虑最后一个过渡，而是考虑最近的 *m* 个过渡，其中 *m* 是一个较大的数值，这样可能会有所改进。这一组最近的 *m* 个过渡被称为经验回放记忆，或简称为内存。从这个内存中，我们会随机抽取一些过渡状态，组成小批量数据。然后，我们用这些批量数据训练神经网络，并通过小批量梯度下降来更新权重。
- en: The whole deep Q-learning algorithm
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整个深度 Q 学习算法
- en: Let's summarize the different steps of the whole deep Q-learning process.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下整个深度 Q 学习过程的不同步骤。
- en: 'Initialization:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：
- en: Initialize the memory of the experience replay to an empty list *M*.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将经验回放的内存初始化为空列表 *M*。
- en: Choose a maximum size for the memory.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个最大内存大小。
- en: 'At each time *t*, we repeat the following process, until the end of the epoch:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间点 *t*，我们重复以下过程，直到一个周期结束：
- en: Predict the Q-values of the current state ![](img/B14110_09_038.png).
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测当前状态的 Q 值 ![](img/B14110_09_038.png)。
- en: Perform the action selected by the Softmax method:![](img/B14110_09_039.png)
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 Softmax 方法选择的动作：![](img/B14110_09_039.png)
- en: Get the reward ![](img/B14110_09_040.png).
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取奖励 ![](img/B14110_09_040.png)。
- en: Reach the next state ![](img/B14110_09_041.png).
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 达到下一个状态 ![](img/B14110_09_041.png)。
- en: Append the transition ![](img/B14110_09_042.png) to the memory *M*.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将过渡状态 ![](img/B14110_09_042.png) 添加到内存 *M* 中。
- en: 'Take a random batch **![](img/B14110_09_043.png)** of transitions. For all
    the transitions ![](img/B14110_09_044.png) of the random batch ![](img/B14110_09_045.png):'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个随机批次中取出 **![](img/B14110_09_043.png)** 的过渡状态。对于随机批次中的所有过渡状态 ![](img/B14110_09_044.png)
    ![](img/B14110_09_045.png)：
- en: 'Get the predictions: ![](img/B14110_09_046.png)'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取预测值： ![](img/B14110_09_046.png)
- en: 'Get the targets: ![](img/B14110_09_047.png)'
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取目标值： ![](img/B14110_09_047.png)
- en: Compute the loss between the predictions and the targets, over the whole batch
    ![](img/B14110_09_048.png):![](img/B14110_09_049.png)
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算整个批次的预测值与目标之间的损失 ![](img/B14110_09_048.png):![](img/B14110_09_049.png)
- en: Back-propagate this loss error back into the neural network, and through stochastic
    gradient descent, update the weights according to how much they contributed to
    the loss error.
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这个损失误差通过反向传播回到神经网络，并通过随机梯度下降，更新权重以反映它们对损失误差的贡献。
- en: 'You''ve just unlocked the full deep Q-learning process! That means that you
    are now able to build powerful real-world AI applications in many fields. Here''s
    a tour of some of the applications where deep Q-learning can create significant
    added value:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚解锁了完整的深度 Q 学习过程！这意味着你现在能够在多个领域构建强大的现实世界 AI 应用。以下是一些深度 Q 学习能够创造显著附加价值的应用场景：
- en: '**Energy**: It was a deep Q-learning model that the DeepMind AI used to reduce
    Google''s Data Center cooling bill by 40%. Also, deep Q-learning can optimize
    the functioning of smart grids; in other words, it can make smart grids even smarter.'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**能源**：正是深度 Q 学习模型帮助 DeepMind AI 将 Google 数据中心的冷却费用减少了 40%。此外，深度 Q 学习还能优化智能电网的运行；换句话说，它能让智能电网变得更智能。'
- en: '**Transport**: Deep Q-learning can optimize traffic light control in order
    to reduce traffic.'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**交通**：深度 Q 学习可以优化交通信号灯控制，以减少交通拥堵。'
- en: '**Autonomous Vehicles**: Deep Q-learning can be used to build self-driving
    cars, which we will illustrate in the next chapter of this book.'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动驾驶汽车**：深度 Q 学习可以用于构建自动驾驶汽车，我们将在本书的下一章进行说明。'
- en: '**Robotics**: Today, many advanced robots are built with deep Q-learning.'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**机器人技术**：今天，许多先进的机器人都采用了深度 Q 学习。'
- en: '**And much more**: Chemistry, recommender systems, advertising, and many more—even
    video games, as you''ll discover in *Chapter 13*, *AI for Games – Become the Master
    at Snake*, when you use deep convolutional Q-learning to train an AI to play Snake.'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**以及更多**：化学、推荐系统、广告等等——甚至是视频游戏，正如你在*第13章*中所发现的，*AI与游戏——成为贪吃蛇游戏的高手*，在这一章中，你将使用深度卷积Q学习训练AI玩贪吃蛇游戏。'
- en: Summary
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: You learned a lot in this chapter; we first discussed ANNs. ANNs are built from
    neurons put in multiple layers. Each neuron from one layer is connected to every
    neuron from the previous layer, and every layer has its own activation function—a
    function that decides how much each output signal should be blocked.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这一章学到了很多内容；我们首先讨论了人工神经网络（ANNs）。ANNs由多个层次的神经元构成，每一层的神经元与上一层的所有神经元连接，每一层都有自己的激活函数——该函数决定了每个输出信号应该被屏蔽多少。
- en: 'The step in which an ANN works out the prediction is called forward-propagation
    and the step in which it learns is called back-propagation. There are three main
    types of back-propagation: batch gradient descent, stochastic gradient descent,
    and the best one, mini-batch gradient descent, which mixes the advantages of both
    previous methods.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络进行预测的步骤称为前向传播，而进行学习的步骤称为反向传播。反向传播有三种主要类型：批量梯度下降、随机梯度下降和最佳的迷你批量梯度下降，它结合了前两种方法的优点。
- en: The last thing we talked about in this chapter was deep Q-learning. This method
    uses Neural Networks to predict the Q-Values of taking certain actions. We also
    mentioned the experience replay memory, which stores a huge chunk of experience
    for our AI.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章最后讨论的内容是深度Q学习。这种方法利用神经网络预测采取某些行动的Q值。我们还提到了经验回放记忆，它为我们的AI存储了大量经验。
- en: In the next chapter, you'll put all of this into practice by coding your very
    own self-driving car.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将通过编写代码实践这一切，打造你自己的自动驾驶汽车。
