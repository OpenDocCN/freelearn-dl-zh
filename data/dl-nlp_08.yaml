- en: '*Chapter 8*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*'
- en: State-of-the-Art Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最先进的自然语言处理
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Evaluate vanishing gradients in long sentences
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估长句中的梯度消失问题
- en: Describe an attention mechanism model as a state-of-the-art NLP domain
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述作为最先进自然语言处理领域的注意力机制模型
- en: Assess one specific attention mechanism architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估一种特定的注意力机制架构
- en: Develop a neural machine translation model using an attention mechanism
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用注意力机制开发神经机器翻译模型
- en: Develop a text summarization model using an attention mechanism
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用注意力机制开发文本摘要模型
- en: This chapter aims to acquaint you with the current practices and technologies
    in the NLP domain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在让你了解当前自然语言处理领域的实践和技术。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: 'In the last chapter, we studied Long Short Term Memory units (LSTMs), which
    help combat the vanishing gradient problem. We also studied GRU in detail, which
    has its own way of handling vanishing gradients. Although LSTM and GRU reduce
    this problem in comparison to simple recurrent neural networks, the vanishing
    gradient problem still manages to prevail in many practical cases. The issue essentially
    remains the same: longer sentences with complex structural dependences are challenging
    for deep learning algorithms to encapsulate. Therefore, one of the most prevalent
    research areas represents the community''s attempts to mitigate the effects of
    the vanishing gradient problem.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了长短期记忆单元（LSTMs），它们有助于解决梯度消失问题。我们还详细研究了GRU，它有自己处理梯度消失问题的方式。尽管与简单的循环神经网络相比，LSTM和GRU减少了这个问题，但梯度消失问题在许多实际案例中仍然存在。问题本质上还是一样：长句子和复杂的结构依赖性使得深度学习算法难以进行封装。因此，当前最普遍的研究领域之一就是社区尝试减轻梯度消失问题的影响。
- en: Attention mechanisms, in the last few years, have attempted to provide a solution
    to the vanishing gradient problem. The basic concept of an attention mechanism
    relies on having access to all parts of the input sentence when arriving at an
    output. This allows the model to lay varying amounts of weight (attention) to
    different parts of the sentence, which allows dependencies to be deduced. Due
    to their uncanny ability to learn such dependencies, attention mechanism-based
    architectures represent the current state of the art in the NLP domain.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，注意力机制尝试为梯度消失问题提供解决方案。注意力机制的基本概念依赖于在得到输出时能够访问输入句子的所有部分。这使得模型能够对句子的不同部分赋予不同的权重（注意力），从而推断出依赖关系。由于其在学习这种依赖关系方面的非凡能力，基于注意力机制的架构代表了自然语言处理领域的最先进技术。
- en: In this chapter, we will learn about attention mechanisms and solve a neural
    machine translation task using a specific architecture based on an attention mechanism.
    We will also mention some other related architectures that are being used in the
    industry today.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习注意力机制，并使用基于注意力机制的特定架构解决神经机器翻译任务。我们还将提及一些当前业界正在使用的其他相关架构。
- en: Attention Mechanisms
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'In the last chapter, we solved a *Neural Language Translation* task. The architecture
    for the translation model adopted by us consists of two parts: *Encoder and Decoder*.
    Refer to the following diagram for the architecture:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们解决了一个*神经语言翻译*任务。我们采用的翻译模型架构由两部分组成：*编码器和解码器*。请参阅以下图示了解架构：
- en: '![Figure 8.1: Neural language translation model](img/C13783_08_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1：神经语言翻译模型](img/C13783_08_01.jpg)'
- en: 'Figure 8.1: Neural language translation model'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.1：神经语言翻译模型
- en: For a neural machine translation task, a sentence is passed into an encoder
    word by word, which produces a single *thought* vector (represented in the preceding
    image as '**S**'), which embeds the meaning of the entire sentence into a single
    representation. The decoder then uses this vector to initialize the hidden states
    and produce a translation word by word.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经机器翻译任务，句子逐字输入到编码器中，生成一个单一的*思想*向量（在前面的图中表示为'**S**'），它将整个句子的意义嵌入到一个单一的表示中。解码器随后使用该向量初始化隐藏状态，并逐字生成翻译。
- en: In the simple encoder-decoder regime, only 1 vector (the thought vector) contains
    the representation of the entire sentence. The longer the sentence, the more difficult
    it becomes for the single thought vector to retain long-term dependencies. The
    use of LSTM units reduces the problem only to some extent. A new concept was developed
    to mitigate the vanishing gradient problem further, and this concept is called
    **Attention mechanisms**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的编码器-解码器模式下，只有一个向量（思维向量）包含整个句子的表示。句子越长，单个思维向量保持长期依赖关系的难度越大。使用 LSTM 单元只能在某种程度上减轻这个问题。为了进一步缓解梯度消失问题，提出了一个新概念，这个概念就是
    **注意力机制**。
- en: 'An attention mechanism aims to mimic a human''s way of learning dependencies.
    Let''s illustrate this with an example sentence:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制旨在模仿人类学习依赖关系的方式。我们用以下示例句子来说明这一点：
- en: '"There have been many incidents of thefts lately in our neighborhood, which
    has forced me to consider hiring a security agency to install a burglar-detection
    system in my house so that I can keep myself and my family safe."'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '"最近我们社区发生了许多盗窃事件，这迫使我考虑雇佣一家安保公司在我家安装防盗系统，以便我能保护自己和家人安全。"'
- en: Note the use of the words 'my', 'I', 'me', 'myself,' and 'our'. These occur
    at distant positions within the sentence but are tightly coupled to each other
    to represent the meaning of the sentence.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意单词“my”，“I”，“me”，“myself”和“our”的使用。它们出现在句子中的不同位置，但彼此紧密相连，共同表达句子的含义。
- en: 'When trying to translate the previous sentence, a traditional encoder-decoder
    functions as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试翻译前述句子时，传统的编码器-解码器功能如下：
- en: Pass the sentence word by word to the encoder.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将句子逐词传递给编码器。
- en: The encoder produces a single thought vector, which represents the entire sentence
    encoding. For a long sentence, such as the previous one, even with the use of
    LSTMs, it would be difficult for the encoder to embed all the dependencies. Therefore,
    the earlier part of the sentence is not as strongly encoded as the later part
    of the sentence, which means the later part of the sentence ends up having a dominant
    influence over the encodings.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器生成一个单一的思维向量，表示整个句子的编码。对于像前面的长句子，即使使用 LSTM，也很难让编码器嵌入所有依赖关系。因此，句子的前半部分编码不如后半部分强，这意味着后半部分对编码的影响占主导地位。
- en: The decoder uses the thought vector to initialize the hidden state vector to
    generate the output translation.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器使用思维向量来初始化隐藏状态向量，以生成输出翻译。
- en: 'A more intuitive way to translate the sentence would be to pay attention to
    the correct positions of words in the input sentence when determining a particular
    word in the target language. As an example, consider the following sentence:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更直观的翻译方法是，在确定目标语言中特定单词时，注意输入句子中单词的正确位置。举个例子，考虑以下句子：
- en: '''*The animal could not walk on the street because it was badly injured.*'''
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '''*那只动物无法走在街上，因为它受伤严重。*'''
- en: 'In this sentence, whom does the word ''it'' refer to? Is it the animal or the
    street? An answer to this question would be possible if the entire sentence were
    considered together and different parts of the sentence were weighed differently
    to determine the answer to the question. An attention mechanism accomplishes this,
    as depicted here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个句子中，'it' 这个词指的是谁？是指动物还是街道？如果将整个句子一起考虑，并对句子的不同部分赋予不同的权重，就能够回答这个问题。注意力机制就完成了这一点，如下所示：
- en: '![Figure 8.2: An example of an attention mechanism](img/C13783_08_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2：注意力机制的示例](img/C13783_08_02.jpg)'
- en: 'Figure 8.2: An example of an attention mechanism'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.2：注意力机制的示例
- en: The diagram shows how much weight each word receives in understanding every
    word in a sentence. As can be seen, the word '**it_**' receives a very strong
    weighting from '**animal_**' and a relatively weaker weighting from '**street_**'.
    Thus, the model can now answer the question of which entity 'it' refers to in
    the sentence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了在理解句子中每个单词时，每个单词所获得的权重。如图所示，单词 '**it_**' 从 '**animal_**' 获得了非常强的权重，而从 '**street_**'
    获得了相对较弱的权重。因此，模型现在可以回答“it”指代句子中的哪个实体的问题。
- en: 'For a translation encoder-decoder model, while generating word-by-word output,
    at a given point in time, not all the words in the input sentence are important
    for the determination of the output word. An attention mechanism implements a
    scheme that does exactly that: weighs different parts of the input sentence with
    all of the input words at each point in the determination of the output. A well-trained
    network with an attention mechanism would learn to apply an appropriate amount
    of weighting to different parts of the sentence. This regime allows the entire
    part of the input sentence to be always available for use at every point of determining
    the output. Thus, instead of one thought vector, the decoder has access to the
    "thought" vector specific for the determination of each word in the output sentence.
    This ability of an attention mechanism is in stark contrast to a traditional LSTM/GRU/RNN-based
    encoder-decoder.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于翻译的编码器-解码器模型，在生成逐词输出时，在某个特定的时刻，并不是输入句子中的所有单词对输出词的确定都很重要。注意力机制实现了一个正是做这件事的方案：在每个输出词的确定时，对输入句子的不同部分进行加权，考虑到所有的输入词。一个经过良好训练的带有注意力机制的网络，会学会对句子的不同部分施加适当的加权。这个机制使得输入句子的全部内容在每次确定输出时都能随时使用。因此，解码器不再仅仅依赖一个思维向量，而是可以访问到每个输出词对应的“思维”向量。这种能力与传统的LSTM/GRU/RNN基础的编码器-解码器模型形成鲜明对比。
- en: An attention mechanism is a general concept. It can be realized in several architectural
    flavors, which are discussed in the later part of the chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是一个通用概念。它可以通过几种不同的架构实现，这些架构将在本章后面部分讨论。
- en: An Attention Mechanism Model
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力机制模型
- en: 'Let''s see how an encoder-decoder architecture could look with an attention
    mechanism in place:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看带有注意力机制的编码器-解码器架构可能是怎样的：
- en: '![Figure 8.3: An attention mechanism model](img/C13783_08_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3：注意力机制模型](img/C13783_08_03.jpg)'
- en: 'Figure 8.3: An attention mechanism model'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.3：注意力机制模型
- en: 'The preceding diagram depicts the training phase of a language translation
    model with an attention mechanism. We can note a few differences compared to a
    basic encoder-decoder regime, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图展示了带有注意力机制的语言翻译模型的训练阶段。与基本的编码器-解码器机制相比，我们可以注意到几个不同之处，如下所示：
- en: The initial states of the decoder get initialized with the encoder output state
    from the last encoder cell. An initial **NULL** word is used to start the translation,
    and the first word is produced as '**Er**'. This is the same as the previous encoder-decoder
    model.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的初始状态会被初始化为最后一个编码器单元的编码器输出状态。使用一个初始的**NULL**词开始翻译，生成的第一个词是‘**Er**’。这与之前的编码器-解码器模型相同。
- en: For the second word, in addition to the input from the previous word and the
    hidden state of the preceding decoder timestep, another vector is fed as input
    to the cell. This vector, generally regarded as '**Context vector**', is a function
    of all the encoder hidden states. From the preceding diagram, it is a weighted
    summation of the hidden states of the encoder for all the timesteps.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第二个词，除了来自前一个词的输入和前一个解码器时间步的隐藏状态外，还会有另一个向量作为输入传递给单元。这个向量通常被认为是‘**上下文向量**’，它是所有编码器隐藏状态的一个函数。从前面的图中来看，它是编码器在所有时间步上的隐藏状态的加权求和。
- en: During the training phase, since the output of each decoder timestep is known,
    we can learn all the parameters of the network. In addition to the usual parameters,
    corresponding to whichever RNN flavor is being used, the parameters specific to
    the attention function are also learned. If the attention function is just a simple
    summation of the hidden state encoder vectors, the weights of the hidden states
    at each encoder timestep can be learned.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练阶段，由于每个解码器时间步的输出是已知的，我们可以学习网络的所有参数。除了与所使用的RNN类型相关的常规参数外，还会学习与注意力函数相关的特定参数。如果注意力函数只是对隐藏状态编码器向量的简单求和，则可以学习每个编码器时间步的隐藏状态的权重。
- en: At inference time, at every timestep, the decoder cell can take as input the
    predicted word from the last timestep, the hidden states from the previous decoder
    cell, and the context vector.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理阶段，在每个时间步，解码器单元可以将上一时间步的预测词、前一个解码器单元的隐藏状态以及上下文向量作为输入。
- en: Let's look at one specific realization of an attention mechanism for neural
    machine translation. In the previous chapter, we built a neural language translation
    model, which is a subproblem area of a more general area of NLP called neural
    machine translation. In the following section, we attempt to solve a date-normalization
    problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下神经机器翻译中注意力机制的一个具体实现。在上一章中，我们构建了一个神经语言翻译模型，这是一个更广泛的自然语言处理（NLP）领域中的子问题，叫做神经机器翻译。在接下来的部分中，我们将尝试解决一个日期规范化问题。
- en: Data Normalization Using an Attention Mechanism
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用注意力机制的数据规范化
- en: Let's say you're maintaining a database that has a table containing a column
    for date. The input for the date is taken from your customers, who fill in a form
    and enter the date in a **date** field. The frontend engineer somehow forgot to
    enforce a scheme upon the field, such that only dates in a "YYYY-MM-DD" format
    are accepted. You are now tasked with normalizing the **date** column of database
    table, such that the user inputs in several formats get converted to a standard
    "YYYY-MM-DD" format.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在维护一个数据库，其中有一张表格包含了一个日期列。日期的输入由你的客户提供，他们填写表单并在**日期**字段中输入日期。前端工程师不小心忘记了对该字段进行验证，使得只有符合“YYYY-MM-DD”格式的日期才会被接受。现在，你的任务是规范化数据库表中的**日期**列，将用户以多种格式输入的日期转换为标准的“YYYY-MM-DD”格式。
- en: 'As an example, the user inputs for date and the corresponding correct normalization
    are shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，用户输入的日期及其对应的正确规范化如下所示：
- en: '![Figure 8.4: Table for date normalization](img/C13783_08_04.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4：日期规范化表格](img/C13783_08_04.jpg)'
- en: 'Figure 8.4: Table for date normalization'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.4：日期规范化表格
- en: You can see that there is a lot of variation in the way a user can input a date.
    There are many more ways in which the date could be specified apart from the examples
    in the table.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，用户输入日期的方式有很大的变化。除了表格中的示例外，还有许多其他方式可以指定日期。
- en: 'This problem is a good candidate to be solved by a neural machine translation
    model as the input has a sequential structure, wherein the meanings of the different
    components in the input need to be learned. This model will have the following
    components:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题非常适合通过神经机器翻译模型来解决，因为输入具有顺序结构，其中输入的不同组成部分的意义需要被学习。该模型将包含以下组件：
- en: Encoder
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器
- en: Decoder
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器
- en: Attention mechanisms
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制
- en: Encoder
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: 'This is a bidirectional LSTM that takes each character of the date as input.
    Thus, at each timestep, the input to the encoder is a single character of the
    input date. Apart from this, the hidden state and memory state is also taken as
    an input from the previous encoder cell. Since this is a bidirectional architecture,
    there are two sets of parameters pertaining to the LSTM: one in the forward direction
    and the other in the backward direction.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个双向 LSTM，它将日期的每个字符作为输入。因此，在每个时间步，编码器的输入是日期输入的单个字符。除此之外，隐藏状态和记忆状态也作为输入从上一个编码器单元传递过来。由于这是一个双向结构，因此与
    LSTM 相关的有两组参数：一组用于正向，另一组用于反向。
- en: Decoder
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: This is a unidirectional LSTM. It takes as input the context vector for this
    timestep. Since each output character is not strictly dependent upon the last
    output character in the case of date normalization, we don't need to feed the
    previous timestep output as an input to the current timestep. Additionally, since
    it is an LSTM unit, the hidden states and memory state from the previous decoder
    timestep are also fed to the current timestep unit for the determination of the
    decoder output at this timestep.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个单向 LSTM。它的输入是当前时间步的上下文向量。由于在日期规范化的情况下，每个输出字符不严格依赖于上一个输出字符，因此我们不需要将前一个时间步的输出作为当前时间步的输入。此外，由于它是一个
    LSTM 单元，上一时间步解码器的隐藏状态和记忆状态也会作为输入传递给当前时间步单元，用于确定当前时间步的解码器输出。
- en: Attention mechanisms
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'Attention mechanisms are explained in this section. For determination of a
    decoder input at a given timestep, a context vector is calculated. A context vector
    is a weighted summation of all the hidden state of an encoder from all timesteps.
    This is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释注意力机制。在给定时间步确定解码器输入时，计算一个上下文向量。上下文向量是所有时间步的编码器隐藏状态的加权总和。其计算方式如下：
- en: '![Figure 8.5: Expression for the context vector](img/C13783_08_05.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5：上下文向量的表达式](img/C13783_08_05.jpg)'
- en: 'Figure 8.5: Expression for the context vector'
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.5：上下文向量的表达式
- en: 'The dot operation is a dot product operation that multiplies weights (represented
    by **alpha**) with the corresponding hidden state vector for all timesteps and
    sums them up. The value of the alpha vector is calculated separately for each
    decoder output timestep. The alphas encapsulate the essence of an attention mechanism,
    that is, determining how much ''attention'' to be given to which part of the input
    to figure out the current timestep output. This can be realized in a diagram,
    as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 点积操作是一个点积运算，它将权重（由**alpha**表示）与每个时间步的相应隐藏状态向量相乘并求和。alpha向量的值是为每个解码器输出时间步单独计算的。alpha值
    encapsulates了注意力机制的本质，即确定在当前时间步的输出中，应该给予输入的哪一部分“关注”。这可以通过一个图示来实现，如下所示：
- en: '![Figure 8.6: Determination of attention to inputs](img/C13783_08_06.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6：输入的注意力权重的确定](img/C13783_08_06.jpg)'
- en: 'Figure 8.6: Determination of attention to inputs'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.6：输入的注意力权重的确定
- en: As an example, let's say that the encoder input has a fixed length of 30 characters,
    and the decoder output has a fixed output length of 10 characters. For the date
    normalization problem, this means that the user input is fixed to be a maximum
    of 30 characters, while the model output is fixed at 10 characters (the number
    of characters in the YYYY-MM-DD format, including the hyphens).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设编码器输入有一个固定长度30个字符，而解码器输出有一个固定的输出长度10个字符。对于日期规范化问题，这意味着用户输入的最大长度固定为30个字符，而模型输出的长度固定为10个字符（即YYYY-MM-DD格式的字符数，包括中划线）。
- en: Let's say that we wish to determine the decoder output at the output timestep=4
    (an arbitrary number chosen to explain the concept; it just needs to be <=10,
    which is the output timestep count). At this step, the weight vector alpha is
    computed. This vector has a dimensionality equal to the number of timesteps of
    the encoder input (as a weight needs to be computed for every encoder input timestep).
    So, in our case, alpha has a dimensionality of 30.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望确定在输出时间步=4时的解码器输出（这是为了说明概念而选择的一个任意数字；它只需要小于等于10，即输出时间步的数量）。在这一步骤中，会计算出权重向量alpha。这个向量的维度等于编码器输入的时间步数（因为需要为每个编码器输入时间步计算一个权重）。因此，在我们的例子中，alpha的维度为30。
- en: Now, we already have the hidden state vector from each of the encoder timesteps,
    so there are a total of 30 hidden state vectors available. The dimensionality
    of the hidden state vector accounts for both the forward and backward components
    of the bidirectional encoder LSTM. For a given timestep, we combine the forward
    hidden state and backward hidden state into a single vector. So, if the dimensionality
    of forward and backward hidden states is 32 each, we put them in a single vector
    of 64 dimensions as [**h_forward**, **h_backward**]. This is a simple concatenation
    function. Let's call this the encoder hidden state vector.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经得到了来自每个编码器时间步的隐藏状态向量，因此一共有30个隐藏状态向量可用。隐藏状态向量的维度同时考虑了双向编码器LSTM的前向和反向组件。对于给定的时间步，我们将前向隐藏状态和反向隐藏状态合并成一个单一的向量。因此，如果前向和反向隐藏状态的维度都是32，我们将它们合并成一个64维的向量，如[**h_forward**,
    **h_backward**]。这只是一个简单的拼接函数。我们称之为编码器隐藏状态向量。
- en: We now have a single 30-dimensional weight vector alpha, and 30 vectors of 64-dimensional
    hidden states. So, we can now multiply each of the 30 hidden state vectors with
    a corresponding entry in the alpha vector. Furthermore, we can sum up these scaled
    representations of hidden states to receive a single 64-dimensional context vector.
    This is essentially the operation performed by the dot operator.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个单一的30维权重向量alpha，以及30个64维的隐藏状态向量。因此，我们可以将这30个隐藏状态向量分别与alpha向量中的对应项相乘。此外，我们还可以将这些加权后的隐藏状态表示求和，得到一个单一的64维上下文向量。这本质上是点积操作所执行的运算。
- en: The Calculation of Alpha
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Alpha的计算
- en: 'The weights can be modeled by a multilayer perceptron (MLP), which is a simple
    neural network consisting of multiple hidden layers. We choose to have two dense
    layers with a **softmax** output. The number of dense layers and units can be
    treated as hyperparameters. The input to this MLP consists of two components:
    these are the hidden state vectors for all timesteps from the encoder bidirectional
    LSTM, as explained in the last point, and the hidden states from the previous
    timestep of the decoder. These are concatenated to form a single vector. So, the
    input to the MLP is: [*encoder hidden state vector*, *previous state vector from
    decoder*]. This is a concatenation operation of tensors: [**H**, **S_prev**].
    **S_prev** refers to the decoder''s hidden state output from the previous timestep.
    If the dimensionality of **S_prev** is 64 (denoting a hidden state dimensionality
    of 64 for the decoder LSTM) and the dimensionality of the encoder''s hidden state
    vector is 64 (from the last point), a concatenation of these two vectors produces
    a vector of size 128.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 权重可以通过多层感知器（MLP）进行建模，它是一个由多个隐藏层组成的简单神经网络。我们选择使用两个密集层和 **softmax** 输出。密集层和单元的数量可以作为超参数处理。该
    MLP 的输入由两个部分组成：这些部分是编码器双向 LSTM 所有时间步的隐藏状态向量，如上一步所解释的，以及来自解码器前一时间步的隐藏状态。这些向量被连接形成一个单一向量。因此，MLP
    的输入是：[*编码器隐藏状态向量*，*来自解码器的前一状态向量*]。这是一种张量的连接操作：[**H**，**S_prev**]。**S_prev** 指的是解码器从前一时间步输出的隐藏状态。如果
    **S_prev** 的维度是 64（表示解码器 LSTM 的隐藏状态维度为 64），且编码器的隐藏状态向量的维度也是 64（如上一点所述），那么这两个向量的连接将生成一个维度为
    128 的向量。
- en: 'Thus, the MLP receives a 128-dimension input for a single encoder timestep.
    As we have fixed the encoder input length to 30 characters, we will have a matrix
    (more accurately, a tensor) of size [30, 128]. The parameters of this MLP are
    learned using the same BPTT regime that is used to learn all the other parameters
    of the model. So, all the parameters of the entire model (encoder + decoder +
    attention function MLP) are learned together. This can be seen in the following
    diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MLP 接收来自单个编码器时间步的 128 维输入。由于我们已将编码器的输入长度固定为 30 个字符，我们将得到一个大小为 [30，128] 的矩阵（更准确地说，是一个张量）。该
    MLP 的参数使用与学习模型其他参数相同的反向传播通过时间（BPTT）机制来学习。因此，整个模型（编码器 + 解码器 + 注意力函数 MLP）的所有参数是一起学习的。可以通过以下图示查看：
- en: '![Figure 8.7: The calculation of alpha](img/C13783_08_07.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7：Alpha 的计算](img/C13783_08_07.jpg)'
- en: 'Figure 8.7: The calculation of alpha'
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.7：Alpha 的计算
- en: 'In the previous step, we learned the weights (alpha vector) for determining
    only one step of the decoder output (we had assumed this timestep to be 4 in an
    earlier point). So, the determination of a single step decoder output requires
    the inputs: **S_prev** and encoder hidden states for calculating the context vector,
    decoder hidden states, and decoder previous timestep memory, which goes as input
    to the decoder unidirectional LSTM. Proceeding to the next decoder timestep requires
    a calculation of a new alpha vector since, for this next step, various parts of
    the input sequence will most likely be weighted differently compared to the previous
    timestep.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们学习了用于确定解码器输出的权重（alpha 向量）（我们假设这一时间步是 4）。因此，单步解码器输出的确定需要输入：**S_prev**
    和编码器隐藏状态，用于计算上下文向量、解码器隐藏状态以及解码器前一时间步的记忆，这些将作为输入传递给解码器单向 LSTM。进入下一个解码器时间步时，需要计算一个新的
    alpha 向量，因为对于这个下一步，输入序列的不同部分与上一个时间步相比，可能会被赋予不同的权重。
- en: Due to the architecture of the model, the training and inference steps are the
    same. The only difference is that, during training, we know the output for each
    decoder timestep and use that to train the model parameters (this technique is
    referred to as 'Teacher Forcing').
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的架构，训练和推理步骤是相同的。唯一的区别是，在训练过程中，我们知道每个解码器时间步的输出，并利用这些输出来训练模型参数（这种技术称为“教师强制”）。
- en: In contrast, during inference time, we predict the output character. Note that
    both during training and inference, we do not feed the previous timestep decoder
    output character as input to the current timestep decoder cell. It should be noted
    that the architecture proposed here is specific to this problem. There are a lot
    of architectures and ways to define an attention function. We will take a brief
    look at some of these in later sections of the chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在推理阶段，我们预测输出字符。请注意，在训练和推理期间，我们都不会将上一个时间步的解码器输出字符作为输入传递给当前时间步的解码器单元。需要注意的是，这里提出的架构是针对这个问题特定的。实际上有很多架构和定义注意力函数的方法。我们将在本章的后续部分简要了解其中一些。
- en: 'Exercise 28: Build a Date Normalization Model for a Database Column'
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习 28: 为数据库列构建日期规范化模型'
- en: 'A database column accepts date inputs from various users in multiple formats.
    In this exercise, we aim to normalize the date column of the database table such
    that the user inputs in several formats get converted to a standard "YYYY-MM-DD"
    format:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据库列接受来自多个用户的日期输入，格式多样。在本练习中，我们旨在规范化数据库表的日期列，使得用户以不同格式输入的日期能够转换为标准的“YYYY-MM-DD”格式：
- en: Note
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Python requirements for running the code are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码所需的Python依赖项如下：
- en: Babel==2.6.0
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Babel==2.6.0
- en: Faker==1.0.2
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Faker==1.0.2
- en: Keras==2.2.4
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Keras==2.2.4
- en: numpy==1.16.1
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: numpy==1.16.1
- en: pandas==0.24.1
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: pandas==0.24.1
- en: scipy==1.2.1
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: scipy==1.2.1
- en: tensorflow==1.12.0
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow==1.12.0
- en: tqdm==4.31.1
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: tqdm==4.31.1
- en: Faker==1.0.2
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Faker==1.0.2
- en: 'We import all the necessary modules:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入所有必要的模块：
- en: '[PRE0]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define some helper functions. We first use the ''`faker`'' and `babel`
    modules to generate data for training. The `format_date` function from `babel`
    generates date in a specific format (using `FORMATS`). Additionally, dates are
    also returned in a human-readable format that emulates the informal user input
    date that we wish to normalize:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一些辅助函数。我们首先使用`'faker'`和`babel`模块生成用于训练的数据。`babel`中的`format_date`函数生成特定格式的日期（使用`FORMATS`）。此外，日期也以人类可读的格式返回，模拟我们希望规范化的非正式用户输入日期：
- en: '[PRE1]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the format of the data we would like to generate:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们希望生成的数据格式：
- en: '[PRE2]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we generate and write a function to load the dataset. In this function,
    examples are created using the `load_date()` function defined earlier. In addition
    to this dataset, the function also returns dictionaries for mapping human-readable
    and machine-readable tokens along with the inverse machine vocabulary:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们生成并编写一个函数来加载数据集。在此函数中，使用之前定义的`load_date()`函数创建示例。除了数据集外，函数还返回用于映射人类可读和机器可读标记的字典，以及逆向机器词汇表：
- en: '[PRE3]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The previous helper functions are used to generate a dataset using the `babel`
    Python package. Additionally, it returns the input and output vocab dictionaries,
    as we have been doing in past exercises.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述辅助函数用于生成一个数据集，使用`babel` Python包。此外，它还返回输入和输出的词汇字典，就像我们在以前的练习中所做的那样。
- en: 'Next, we generate a dataset having 10,000 samples using these helper functions:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用这些辅助函数生成一个包含10,000个样本的数据集：
- en: '[PRE4]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The variables hold values, as depicted:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变量存储值，如下所示：
- en: '![Figure 8.8: Screenshot displaying variable values](img/C13783_08_08.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.8: 显示变量值的截图](img/C13783_08_08.jpg)'
- en: 'Figure 8.8: Screenshot displaying variable values'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.8: 显示变量值的截图'
- en: 'The `human_vocab` is a dictionary that maps input characters to integers. The
    following is the mapping of values for `human_vocab`:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`human_vocab`是一个字典，将输入字符映射到整数。以下是`human_vocab`的值映射：'
- en: '![Figure 8.9: Screenshot for human_vocab dictionary](img/C13783_08_09.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.9: 显示human_vocab字典的截图](img/C13783_08_09.jpg)'
- en: 'Figure 8.9: Screenshot for human_vocab dictionary'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.9: 显示human_vocab字典的截图'
- en: The `machine_vocab` dictionary contains the mapping of the output character
    to integers.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`machine_vocab`字典包含输出字符到整数的映射。'
- en: '![Figure 8.10: Screenshot for the machine_vocab dictionary](img/C13783_08_10.jpg)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.10: 显示machine_vocab字典的截图](img/C13783_08_10.jpg)'
- en: 'Figure 8.10: Screenshot for the machine_vocab dictionary'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.10: 显示machine_vocab字典的截图'
- en: '`inv_machine_vocab` is an inverse mapping of `machine_vocab` to map predicted
    integers back to characters:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`inv_machine_vocab`是`machine_vocab`的逆映射，用于将预测的整数映射回字符：'
- en: '![Figure 8.11: Screenshot for the inv_machine_vocab dictionary](img/C13783_08_11.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.11: 显示inv_machine_vocab字典的截图](img/C13783_08_11.jpg)'
- en: 'Figure 8.11: Screenshot for the inv_machine_vocab dictionary'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.11: 显示inv_machine_vocab字典的截图'
- en: 'Next, we preprocess data such that the input sequences have shape (`10000`,
    `30`, `len(human_vocab)`). Thus, every row in this matrix represents 30 timesteps
    and the one-coded vector, having a value of 1 corresponding to the character at
    a given timestep. Similarly, the Y output gets the shape (`10000`, `10`, `len(machine_vocab)`).
    This corresponds to 10 output timesteps and the corresponding one-hot-coded output
    vector. We first define a function named ''`string_to_int`'' that takes as input
    a single user date and returns a sequence of integers that can be fed to the model:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们预处理数据，使得输入序列的形状为（`10000`，`30`，`len(human_vocab)`）。因此，矩阵中的每一行代表 30 个时间步和对应于给定时间步的字符的独热编码向量。同样，Y
    输出的形状为（`10000`，`10`，`len(machine_vocab)`）。这对应于 10 个输出时间步和相应的独热编码输出向量。我们首先定义一个名为`'string_to_int'`的函数，它接收一个用户日期作为输入，并返回一个可以输入到模型中的整数序列：
- en: '[PRE5]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Change the case to lowercase to standardize the text
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将大小写转换为小写，以标准化文本
- en: '[PRE6]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now utilize this helper function to generate input and output integer
    sequences, as explained previously:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以利用这个辅助函数来生成输入和输出的整数序列，正如之前所解释的那样：
- en: '[PRE7]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Print the shape of the matrices.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印矩阵的形状。
- en: '[PRE8]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of this step is as follows:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一阶段的输出如下所示：
- en: '![](img/C13783_08_12.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13783_08_12.jpg)'
- en: 'Figure 8.12: Screenshot for the shape of matrices'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.12：矩阵形状的截图
- en: 'We can further inspect the shapes of the `X`,`Y`, `Xoh`, and `Yoh` vectors:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以进一步检查`X`、`Y`、`Xoh`和`Yoh`向量的形状：
- en: '[PRE9]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output should be as follows:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![](img/C13783_08_13.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13783_08_13.jpg)'
- en: 'Figure 8.13: Screenshot for the shape of matrices after processing'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.13：处理后矩阵形状的截图
- en: 'We now start defining some functions that we need to build the model. First,
    we define a function that calculates a softmax value given a tensor as input:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在开始定义一些构建模型所需的函数。首先，我们定义一个计算softmax值的函数，给定张量作为输入：
- en: '[PRE10]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we can start to put the model together:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始组装模型：
- en: '[PRE11]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`RepeatVector` serves the purpose of repeating a given tensor multiple times.
    In our case, this is done `Tx` times, which is 30 input timesteps. The repeator
    is used to repeat `S_prev` 30 times. Recall that to calculate the context vector
    for determining one timestep decoder output, `S_prev` needs to be concatenated
    with each of the input encoder timesteps. The `Concatenate` `keras` function accomplishes
    the next step, that is, concatenating the repeated `S_prev` and encoder hidden
    state vector for each timestep. We have also defined MLP layers, which are two
    dense layers (`densor1`, `densor2`). Next, the output of MLP is passed through
    a `softmax` layer. This `softmax` distribution is an alpha vector with each entry
    corresponding to the weight for each concatenated vector. In the end, a `dotor`
    function is defined, which is responsible for calculating the context vector.
    The entire flow corresponds to one step attention (since it is for one decoder
    output timestep):'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`RepeatVector`的作用是重复给定的张量多次。在我们的例子中，这是重复`Tx`次，也就是30个输入时间步。重复器用于将`S_prev`重复30次。回想一下，为了计算上下文向量以确定单个时间步的解码器输出，需要将`S_prev`与每个输入编码器时间步进行连接。`Concatenate`的`keras`函数完成了下一步，即将重复的`S_prev`和每个时间步的编码器隐藏状态向量连接在一起。我们还定义了MLP层，它包括两个全连接层（`densor1`，`densor2`）。接下来，MLP的输出通过一个`softmax`层。这个`softmax`分布就是一个alpha向量，其中每个条目对应于每个连接向量的权重。最后，定义了一个`dotor`函数，它负责计算上下文向量。整个流程对应于一个步骤的注意力机制（因为它是针对单个解码器输出时间步的）：'
- en: '[PRE12]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use `repeator` to repeat `s_prev` to be of shape (`m`, `Tx`, `n_s`) so that
    you can concatenate it with all hidden states, ''`h`'':'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`repeator`将`s_prev`重复至形状（`m`，`Tx`，`n_s`），以便与所有隐藏状态`'h'`进行连接：
- en: '[PRE13]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Use `concatenator` to concatenate `a` and `s_prev` on the last axis:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`concatenator`在最后一个轴上将`a`和`s_prev`连接起来：
- en: '[PRE14]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Use `densor1` to propagate `concat` through a small fully-connected neural
    network to compute the intermediate energies variable, `e`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`densor1`通过一个小型全连接神经网络传播`concat`，以计算中间能量变量`e`：
- en: '[PRE15]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use `densor2` to propagate `e` through a small fully-connected neural network
    to compute the variable energies:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`densor2`通过一个小型全连接神经网络传播`e`，以计算能量变量：
- en: '[PRE16]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Use `activator` on `energies` to compute the attention weights `alphas`:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`activator`作用于`energies`，以计算注意力权重`alphas`：
- en: '[PRE17]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use `dotor` along with `alphas` and `a` to compute the context vector to be
    given to the next (post-attention) LSTM-cell:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dotor`结合`alphas`和`a`来计算上下文向量，以供下一个（注意力后）LSTM单元使用：
- en: '[PRE18]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Up to this point, we still haven''t defined the number of hidden state units
    for the encoder and decoder LSTMs. We also need to define the decoder LSTM, which
    is a unidirectional LSTM:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们仍然没有定义编码器和解码器LSTM的隐藏状态单元数量。我们还需要定义解码器LSTM，这是一个单向LSTM：
- en: '[PRE19]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We now define the encoder and decoder model:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们定义编码器和解码器模型：
- en: '[PRE20]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the inputs of your model with a shape (`Tx,`). Define `s0` and `c0`,
    and the initial hidden state for the decoder LSTM of shape (`n_s,`):'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型的输入形状（`Tx,`）。定义`s0`和`c0`，以及解码器LSTM的初始隐藏状态，形状为（`n_s,`）：
- en: '[PRE21]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Initialize an empty list of `outputs`:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空的`outputs`列表：
- en: '[PRE22]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define your pre-attention Bi-LSTM. Remember to use `return_sequences=True`:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你的预注意力Bi-LSTM。记得使用`return_sequences=True`：
- en: '[PRE23]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Iterate for `Ty` steps:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代`Ty`步：
- en: '[PRE24]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Perform one step of the attention mechanism to get back the context vector
    at step `t`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一次注意力机制步骤，以获得步骤`t`的上下文向量：
- en: '[PRE25]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Apply the post-attention LSTM cell to the `context` vector. Also, pass `initial_state`
    `= [hidden state, cell state]`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将后注意力LSTM单元应用于`context`向量。同时，传递`initial_state` `= [隐藏状态，细胞状态]`：
- en: '[PRE26]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Apply the `Dense` layer to the hidden state output of the post-attention LSTM:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Dense`层应用于后注意力LSTM的隐藏状态输出：
- en: '[PRE27]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create a model instance by taking three inputs and returning the list of outputs:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过接收三个输入并返回输出列表来创建模型实例：
- en: '[PRE28]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output could be as shown in the following figure:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出可能如下面的图所示：
- en: '![Figure 8.14: Screenshot for model summary](img/C13783_08_14.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.14：模型总结的截图](img/C13783_08_14.jpg)'
- en: 'Figure 8.14: Screenshot for model summary'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.14：模型总结的截图
- en: 'We will now compile the model with `categorical_crossentropy` as the loss function
    and `Adam` optimizer as the optimization strategy:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用`categorical_crossentropy`作为损失函数，`Adam`优化器作为优化策略来编译模型：
- en: '[PRE29]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We need to initialize the hidden state vector and memory state for decoder
    LSTM before fitting the model:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在拟合模型之前，我们需要初始化解码器LSTM的隐藏状态向量和记忆状态：
- en: '[PRE30]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This starts the training:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这开始了训练：
- en: '![Figure 8.15: Screenshot for epoch training](img/C13783_08_15.jpg)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.15：训练周期的截图](img/C13783_08_15.jpg)'
- en: 'Figure 8.15: Screenshot for epoch training'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.15：训练周期的截图
- en: 'The model is now trained and can be called for inference:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型现在已训练完成，可以进行推理调用：
- en: '[PRE31]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Expected output:**'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 8.16: Screenshot for normalized date output](img/C13783_08_16.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16：标准化日期输出的截图](img/C13783_08_16.jpg)'
- en: 'Figure 8.16: Screenshot for normalized date output'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.16：标准化日期输出的截图
- en: Other Architectures and Developments
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他架构与发展
- en: The attention mechanism architecture described in the last section is only a
    way of building attention mechanism. In recent times, several other architectures
    have been proposed, which constitute a state of the art in the deep learning NLP
    world. In this section, we will briefly mention some of these architectures.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节描述的注意力机制架构只是构建注意力机制的一种方式。近年来，提出了几种其他架构，它们在深度学习NLP领域构成了最先进技术。在这一节中，我们将简要提及其中一些架构。
- en: Transformer
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器
- en: In late 2017, Google came up with an attention mechanism architecture in their
    seminal paper titled "Attention is all you need." This architecture is considered
    state-of-the-art in the NLP community. The transformer architecture makes use
    of a special multi-head attention mechanism to generate attention at various levels.
    Additionally, it is also employs residual connections to further ensure that the
    vanishing gradient problem has a minimal impact on learning. The special architecture
    of transformers also allows a massive speed up of the training phase while providing
    better quality results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年末，谷歌在其开创性论文《Attention is all you need》中提出了一种注意力机制架构。该架构被认为是自然语言处理（NLP）社区中的最先进技术。变换器架构利用一种特殊的多头注意力机制，在不同的层次生成注意力。此外，它还采用残差连接，进一步确保梯度消失问题对学习的影响最小。变换器的特殊架构还允许在训练阶段大幅加速，同时提供更高质量的结果。
- en: The most commonly used package with transformer architecture is **tensor2tensor**.
    The Keras code for transformer tends to be very bulky and untenable, while **tensor2tensor**
    allows the use of both a Python package and a simple command-line utility that
    can be used to train a transformer model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的带有变换器架构的包是**tensor2tensor**。Keras的变换器代码通常很笨重且难以维护，而**tensor2tensor**允许使用Python包和简单的命令行工具来训练变换器模型。
- en: Note
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on tensor2tensor, refer to https://github.com/tensorflow/tensor2tensor/#t2t-overview
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于tensor2tensor的信息，请参考 [https://github.com/tensorflow/tensor2tensor/#t2t-overview](https://github.com/tensorflow/tensor2tensor/#t2t-overview)
- en: 'Readers interested in learning more about the architecture should read the
    mentioned paper and the associated Google blogpost at this link: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '有兴趣了解架构的读者可以阅读提到的论文以及相关的Google博客：[Transformer: A Novel Neural Network](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)'
- en: BERT
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT
- en: In late 2018, Google open sourced yet another groundbreaking architecture, called
    **BERT** (**Bidirectional Encoder Representations from Transformers**). The deep
    learning community for NLP has been missing the transfer-learning regime for training
    models for a long time. The transfer learning approach to deep learning has been
    state-of-the-art with image-related tasks such as image classification. Images
    are universal in their basic structure, as they do not differ regardless of geographical
    locations. This allows the training of deep learning models on generic images.
    These pre-trained models can then be fine-tuned for a specific task. This saves
    training time and the need for massive amounts of data to achieve a respectable
    model performance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年底，Google再次开源了一个突破性的架构，名为**BERT**（**Bidirectional Encoder Representations
    from Transformers**）。深度学习社区在自然语言处理（NLP）领域已经很久没有看到适合的转移学习机制了。转移学习方法在深度学习中一直是图像相关任务（如图像分类）的最前沿技术。图像在基本结构上是通用的，无论地理位置如何，图像的结构都是一致的。这使得可以在通用图像上训练深度学习模型。这些预训练的模型可以进一步微调以应对特定任务。这节省了训练时间，也减少了对大量数据的需求，能够在较短的时间内达到令人满意的模型表现。
- en: Languages, unfortunately, vary a lot depending upon geographical locations and
    tend to not share basic structures. Hence, transfer learning is not a viable option
    when it comes to NLP tasks. BERT has now made it possible with its new attention
    mechanism architecture, which builds on top of the basic transformer architecture.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，语言因地理位置的不同而差异很大，且往往没有共同的基本结构。因此，转移学习在自然语言处理（NLP）任务中并不是一个可行的选项。BERT通过其新的注意力机制架构，基于基础的Transformer架构，使这一切成为可能。
- en: Note
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on BERT, refer to https://github.com/google-research/bert
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 关于BERT的更多信息，请参考：[BERT GitHub](https://github.com/google-research/bert)
- en: Readers interested in learning more about BERT should take a look at the Google
    blog on it at https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣了解BERT的读者应该查看Google关于BERT的博客：[Open Sourcing BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)。
- en: Open AI GPT-2
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI GPT-2
- en: Open AI also open sourced an architecture called **GPT-2**, which builds upon
    their previous architecture called GPT. The mainstay of the GPT-2 architecture
    is its ability to perform well on text-generation tasks. The GPT-2 model is also
    a transformer-based model containing around 1.5 billion parameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还开源了一个名为**GPT-2**的架构，它在他们之前的架构GPT基础上进行了改进。GPT-2架构的核心特点是它在文本生成任务中表现出色。GPT-2模型同样基于Transformer架构，包含约15亿个参数。
- en: Note
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Readers interested in learning more can refer to the blogpost by OpenAI at https://blog.openai.com/better-language-models/.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣了解更多的读者可以参考OpenAI的博客：[Better Language Models](https://blog.openai.com/better-language-models/)。
- en: 'Activity 11: Build a Text Summarization Model'
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动11：构建文本摘要模型
- en: We will use the attention mechanism model architecture we built for neural machine
    translation to build a text summarization model. The goal of text summarization
    is to write a summary of a given large text corpus. You can imagine using text
    summarizers for the summarization of books or the generation of headlines for
    news articles.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们为神经机器翻译构建的注意力机制模型架构来构建文本摘要模型。文本摘要的目标是编写给定大规模文本语料的摘要。你可以想象使用文本摘要工具来总结书籍内容或为新闻文章生成标题。
- en: 'As an example, use the given input text:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个示例，使用给定的输入文本：
- en: '"Celebrating its 25th year, Mercedes-Benz India is set to redefine India''s
    luxury space in the automotive segment by launching the new V-Class. The V-Class
    is powered by a 2.1-litre BS VI diesel engine that generates 120kW power, 380Nm
    torque, and can go from 0-100km/h in 10.9 seconds. It features LED headlamps,
    a multi-functional steering wheel, and 17-inch alloy wheels."'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: “梅赛德斯-奔驰印度在庆祝其25周年之际，将通过发布全新V-Class重新定义印度汽车行业的豪华空间。V-Class搭载2.1升BS VI柴油发动机，产生120kW的功率和380Nm的扭矩，0-100km/h加速仅需10.9秒。它配备了LED前大灯、多功能方向盘和17英寸铝合金轮毂。”
- en: 'A good text summarization model should be able to produce a meaningful summary,
    such as:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的文本摘要模型应该能够生成有意义的摘要，例如：
- en: '"Mercedes-Benz India launches the new V-Class"'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: “梅赛德斯-奔驰印度发布全新V-Class”
- en: From an architectural viewpoint, a text summarization model is exactly the same
    as a translation model. The input to the model is text that is fed character by
    character (or word by word) to an encoder, while the decoder produces output characters
    in the same language as the source text.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构的角度来看，文本摘要模型与翻译模型完全相同。模型的输入是文本，它会按字符（或按词）逐个输入到编码器中，而解码器则输出与源文本相同语言的字符。
- en: Note
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The input text can be found at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2008.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本可以在此链接找到：[https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2008](https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2008)。
- en: 'The following steps will help you with the solution:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你解决问题：
- en: Import the required Python packages and make the human and machine vocab dictionaries.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的Python包，并制作人类与机器的词汇字典。
- en: Define the length of the input and output characters and the model functions
    (*Repeator*, *Concatenate*, *Densors*, and *Dotor*).
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入和输出字符的长度以及模型功能（*Repeator*，*Concatenate*，*Densors* 和 *Dotor*）。
- en: Define a one-step-attention function and the number of hidden states for the
    decoder and encoder.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个一步注意力函数，并为解码器和编码器定义隐状态的数量。
- en: Define the model architecture and run it to obtain a model.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构并运行它以获得模型。
- en: Define model loss functions and other hyperparameters. Also, initialize the
    decoder state vectors.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型的损失函数和其他超参数。同时，初始化解码器的状态向量。
- en: Fit the model to our data.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到我们的数据上。
- en: Run the inference step for the new text.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行新文本的推理步骤。
- en: '**Expected Output:**'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 8.17: Output for text summarization](img/C13783_08_17.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17：文本摘要输出](img/C13783_08_17.jpg)'
- en: 'Figure 8.17: Output for text summarization'
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.17：文本摘要输出
- en: Note
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 333.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 活动的解决方案可以在第333页找到。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the concept of attention mechanisms. Based
    on attention mechanisms, several architectures have been proposed that constitute
    the state of the art in the NLP world. We learned about one specific model architecture
    to perform a neural machine translation task. We also briefly mentioned other
    state-of-the-art architectures such as transformers and BERT.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了注意力机制的概念。基于注意力机制，已经提出了几种架构，这些架构构成了NLP领域的最新技术。我们学习了一种特定的模型架构，用于执行神经机器翻译任务。我们还简要提到了其他先进的架构，如Transformer和BERT。
- en: Up to now, we have seen many different NLP models. In the next chapter, we will
    look at the flow of a practical NLP project in an organization and related technology.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了许多不同的自然语言处理（NLP）模型。在下一章，我们将讨论一个实际NLP项目在组织中的流程以及相关技术。
