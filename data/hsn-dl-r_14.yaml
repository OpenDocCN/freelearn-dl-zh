- en: Deep Q-Learning for Maze Solving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 学习在迷宫求解中的应用
- en: In this chapter, you will learn how to use R to implement reinforcement learning
    techniques within a maze environment. In particular, we will create an agent to
    solve a maze by training the agent to perform actions and learn from failed attempts.
    We will learn how to define the maze environment and configure the agent to travel
    through it. We will also be adding neural networks to Q-learning. This provides
    us with an alternative way of getting the value for all the state-action pairs.
    We are going to iterate over our model numerous times to create the policy to
    get through the maze.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用 R 来实现强化学习技术，并将其应用于迷宫环境。特别地，我们将创建一个代理，通过训练代理执行动作并从失败中学习，来解决迷宫。我们将学习如何定义迷宫环境，并配置代理使其能够穿越迷宫。我们还将把神经网络添加到
    Q 学习中，这为我们提供了获取所有状态-动作对值的替代方法。我们将多次迭代我们的模型，以创建一个策略，帮助代理走出迷宫。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Creating an environment for reinforcement learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为强化学习创建环境
- en: Defining an agent to perform actions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个代理来执行动作
- en: Building a deep Q-learning model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度 Q 学习模型
- en: Running the experiment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行实验
- en: Improving performance with policy functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用策略函数提高性能
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files used in this chapter at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)
    上找到本章使用的代码文件。
- en: Creating an environment for reinforcement learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为强化学习创建环境
- en: In this section, we will define an environment for reinforcement learning. We
    could think of this as a typical maze where an agent needs to navigate the two-dimensional
    grid space to get to the end. However, in this case, we are going to use more
    of a physics-based maze. We will represent this using the mountain car problem.
    An agent is in a valley and needs to get to the top; however, it cannot simply
    go up the hill. It has to use momentum to get to the top. In order to do this,
    we need two functions. One function will start or reset the agent to a random
    point on the surface. The other function will describe where the agent is on the
    surface after a step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将定义一个强化学习的环境。我们可以把它想象成一个典型的迷宫，其中代理需要在二维网格空间中导航到达终点。然而，在这种情况下，我们将使用一个基于物理的迷宫。我们将使用山地车问题来表示这一点。代理处在一个山谷中，需要到达山顶；但是，它不能直接爬坡。它必须利用动量到达山顶。为此，我们需要两个函数。一个函数将启动或重置代理，将其放置在表面上的一个随机点。另一个函数将描述代理在一步之后在表面上的位置。
- en: 'We will use the following code to define the `reset` function to provide a
    place for the agent to start:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来定义 `reset` 函数，为代理提供一个起始位置：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can see that with this function, the first thing that happens is that the `position` variable
    is defined by taking one random value from a uniform distribution between `-0.6`
    and `-0.4`. This is the point on the surface where the agent will be placed. Next,
    the variable velocity is set to `0`, since our agent is not moving yet. The `reset`
    functions act merely to place the agent at a starting point. The `position` variable
    and the `velocity` variable are now added to a 1 x 2 matrix and this `matrix`
    variable is the starting spot and starting speed for our agent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用这个函数时，首先发生的事情是通过从 `-0.6` 到 `-0.4` 之间的均匀分布中随机选择一个值来定义 `position` 变量。这是代理将被放置在表面上的点。接下来，`velocity`
    变量被设置为 `0`，因为我们的代理尚未移动。`reset` 函数仅用于将代理放置在起始点。`position` 变量和 `velocity` 变量现在被加入到一个
    1 x 2 的矩阵中，这个 `matrix` 变量就是我们代理的起始位置和起始速度。
- en: 'The next function takes a value for every action and calculates the next step
    that the agent will take. To code this function, we use the following code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数获取每个动作的值，并计算代理将采取的下一步。为了编写这个函数，我们使用以下代码：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this function, the first part defines the position and velocity. In this
    case, this is taken from the `self` object, which we will cover next. The `self`
    variable contains details about the agent. Here, the `position` and `velocity`
    variables are taken from `self` and represent where the agent currently is on
    the surface and the current velocity. Then, the `action` argument is used to calculate
    the velocity. The next line constrains `velocity` between `-0.7` and `0.7`. After
    this, we calculate the next position by adding the velocity to the current position.
    Then, there is one more constraint line. If `position` goes past `-1.2`, then
    the agent is out of bounds and gets reset to the `-1.2` position with no velocity.
    Finally, a check is carried out to see whether the agent has reached its goal.
    If the state is greater than `0.5`, then the agent wins; otherwise, the agent
    keeps moving and attempts to reach the goal.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，第一部分定义了位置和速度。在这个案例中，位置和速度是从`self`对象中获取的，接下来我们将介绍`self`。`self`变量包含有关代理的详细信息。这里，`position`和`velocity`变量是从`self`中获取的，表示代理当前在表面上的位置以及当前的速度。然后，`action`参数用于计算速度。接下来的行将`velocity`限制在`-0.7`到`0.7`之间。之后，我们通过将速度加到当前位置来计算下一个位置。然后，还有一行约束代码。如果`position`超过`-1.2`，代理就会超出边界，并且会重置到`-1.2`位置，且速度为零。最后，会进行检查，看代理是否已达到目标。如果状态值大于`0.5`，代理就算获胜；否则，代理继续移动并尝试达到目标。
- en: 'When we finish with the two coding blocks, we will see that we have two functions
    defined in our **Environment** pane. Your **Environment** pane will appear as
    in the following screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成这两个代码块时，我们将看到在**环境**面板中定义了两个函数。你的**环境**面板将如以下截图所示：
- en: '![](img/8674eb48-130b-404b-bdb6-9f72c15a330d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8674eb48-130b-404b-bdb6-9f72c15a330d.png)'
- en: The combination of these two functions defines the shape of the surface and
    the location of the agent on the surface, as well as the placement of the target
    spot on that surface. The `reset` function is the initial placement of the agent
    and the `step` function defines the step the agent takes at every iteration. With
    these two functions, we have a way of defining the shape and boundaries of our
    environment and a mechanism for placing and moving our agent within this environment.
    Next, let's define our agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数的组合定义了表面的形状、代理在表面上的位置以及目标位置在表面上的设置。`reset`函数用于代理的初始位置，`step`函数定义了每次迭代时代理的步伐。通过这两个函数，我们定义了环境的形状和边界，并且为将代理放置和移动到环境中提供了机制。接下来，我们来定义我们的代理。
- en: Defining an agent to perform actions
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个代理来执行动作
- en: In this section, we will define our agent for deep Q-learning. We have already
    seen how the preceding environment functions define how the agent moves. Here,
    we define the agent itself. In the previous chapter, we used Q-learning and were
    able to apply the Bellman equation to the new state that was the result of a given
    action. In this chapter, we will augment that portion of Q-learning with a neural
    network, which is what takes standard Q-learning and makes it deep Q-learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义用于深度Q学习的代理。我们已经看到前面环境函数如何定义代理的移动方式。在这里，我们定义代理本身。在上一章中，我们使用了Q学习，并且能够将贝尔曼方程应用到由特定动作产生的新状态。在本章中，我们将通过神经网络增强Q学习的这一部分，这就是将标准Q学习转化为深度Q学习的关键。
- en: 'In order to add this neural network model to the process, we need to define
    a class. This is something that is often done in object-oriented programming;
    however, it is done less often in a programming language such as R. To accomplish
    this, we will use the `R6` package for class creation. We will break up the creation
    of this `R6` class into numerous parts to make it easier to understand. A class
    provides instructions for instantiating and operating on a data object. In this
    case, our class will use declared variables to instantiate the data object and
    a series of functions, which are referred to as methods within a class context,
    to operate on the data object. In the following steps, we will just look at the
    individual parts of our class one by one to make it easier to understand the parts
    of the class that we are creating. However, running parts of the code will result
    in errors. After walking through all the parts, we will wrap everything in a function
    to create our class and this final, longer `R6` code that includes everything
    that you will run. To get started, we will set up the initial values using the
    following code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个神经网络模型添加到过程中，我们需要定义一个类。这在面向对象编程中是常见的；然而，在像R这样的编程语言中则不太常见。为此，我们将使用`R6`包来创建类。我们将把`R6`类的创建分解为多个部分，以便更容易理解。类提供了实例化和操作数据对象的指令。在这种情况下，我们的类将使用声明的变量来实例化数据对象，并且一系列的函数（在类的上下文中称为方法）将用于操作数据对象。在接下来的步骤中，我们将逐一查看类的各个部分，以便更容易理解我们正在创建的类的构成部分。然而，运行代码的部分会导致错误。在详细讲解所有部分之后，我们将把所有内容包装在一个函数中来创建我们的类，并且这是你将要运行的最终、较长的`R6`代码。首先，我们将使用以下代码来设置初始值：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When creating a class for this purpose using R, we first set two options. First,
    we set `portable` to `FALSE`, which means other classes cannot inherit methods
    or functions from this class. However, it also means that we can use the `self`
    keyword. Second, we set `lock_objects` to `FALSE`, since we will need to modify
    objects within this class.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用R创建一个用于此目的的类时，我们首先设置两个选项。首先，我们将`portable`设置为`FALSE`，这意味着其他类不能继承此类的方法或函数。但这也意味着我们可以使用`self`关键字。其次，我们将`lock_objects`设置为`FALSE`，因为我们需要在此类中修改对象。
- en: Next, we define our initial values. We use `self` here, which is a special keyword
    that refers to the object created. Remember that a class is not an object—it is
    a constructor for creating an object. Here, we will create an instance of our
    class and this object will be the agent. The agent will initialize with the following
    values. The state size and action size will be passed in as arguments when creating
    the environment. The next memory is an empty deque. A **deque** is a special object
    type, which is double-ended so that values can be added to and removed from both
    sides. We will use this to store the steps the agent takes while trying to reach
    the goal. Gamma is the discount rate. Epsilon is the exploration rate. As we know,
    with deep learning, the goal is to balance exploration and exploitation, so we
    begin with an aggressive exploration rate. However, we then define an epsilon
    decay, which is how much the rate will be reduced, and an epsilon minimum, so
    the rate never reaches `0`. Lastly, the learning rate is just the constant value
    used when adjusting weights and the model takes the result of running our neural
    network model, which we will get to next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的初始值。我们在这里使用`self`，它是一个特殊的关键字，指代所创建的对象。记住，类不是对象——它是创建对象的构造器。在这里，我们将创建一个类的实例，这个对象将作为代理。该代理将使用以下值进行初始化。状态大小和动作大小将在创建环境时作为参数传入。下一个记忆是一个空的deque。**deque**是一种特殊的对象类型，它是双端的，因此可以从两端添加或移除值。我们将使用它来存储代理在试图达成目标时所采取的步骤。Gamma是折扣率。Epsilon是探索率。正如我们所知道的，深度学习的目标是平衡探索与利用，因此我们从一个激进的探索率开始。然而，我们随后定义了一个epsilon衰减，这是探索率将被减少的程度，以及一个epsilon最小值，以确保探索率永远不会达到`0`。最后，学习率就是在调整权重时使用的常数值，而模型则是运行我们的神经网络模型的结果，我们将在接下来的部分介绍。
- en: 'Next, we will give the class the power to act on variables by adding a function.
    In particular, we will add the `build_model` function to run the neural network:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过添加一个函数来赋予类操作变量的能力。特别地，我们将添加`build_model`函数来运行神经网络：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The model takes the current state as input and the output will be one of the
    actions available when we predict the model. However, this function just returns
    the model because we will pass a different state argument to the model, depending
    on what part of the deep Q-learning path we are on when it is called. The model
    is called in two different scenarios, which we will cover shortly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将当前状态作为输入，输出将是我们在预测模型时可用的动作之一。然而，这个函数只是返回模型，因为我们将在调用时根据我们在深度Q学习路径中的位置，传递一个不同的状态参数给模型。模型在两种不同的场景下被调用，我们稍后会详细讲解。
- en: 'Next, we include a function for memory. The memory portion of this class will
    be a function to store the state, action, reward, and next state details as the
    agent attempts to solve the maze. We store those values in the agent''s memory
    by adding them to the deque using the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加入一个用于记忆的函数。这个类的记忆部分将是一个存储状态、动作、奖励和下一状态的函数，随着智能体尝试解决迷宫，我们将这些值存储在智能体的记忆中，并通过以下代码将它们添加到双端队列（deque）中：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the `pushback` function to add a given value to the first position in
    the deque and move all the existing elements back by one. We do this for state,
    action, reward, and next state, and the flag that shows whether or not the puzzle
    is complete. This sequence is stored in the agent's memory, so it can exploit
    what it already knows by accessing this sequence in memory rather than continuing
    to explore when the exploration-versus-exploitation formula selects the exploitation
    option.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`pushback`函数将给定的值添加到双端队列的第一个位置，并将所有现有元素向后移动一个位置。我们对状态、动作、奖励、下一状态以及显示迷宫是否已完成的标志进行此操作。这些序列存储在智能体的记忆中，因此当探索与利用公式选择利用选项时，它可以通过访问记忆中的这个序列来利用已知的内容，而不是继续进行探索。
- en: 'Next, we will add some code to select the next action. To perform this task,
    we will use a check on the decaying epsilon value. Depending on whether or not
    the decaying epsilon is greater than a randomly selected value from a uniform
    distribution, one of two actions will take place. We set up the function for deciding
    on the next action by using the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一些代码来选择下一个动作。为完成此任务，我们将检查衰减的epsilon值。根据衰减的epsilon是否大于从均匀分布中随机选择的值，将发生两种可能的动作之一。我们通过以下代码设置了一个决定下一个动作的函数：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As noted previously, there are two possible outcomes from this action function.
    First, if the randomly selected value from the uniform distribution is less than
    or equal to epsilon, then a value will be selected between the full range of active
    movements. Otherwise, the current state is used to predict the next action using
    the model we defined earlier, which results in weighted probabilities that any
    of these actions are correct. The action with the highest probability is selected.
    This is a balance between two different forms of exploration in seeking the correct
    next step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个动作函数有两种可能的结果。首先，如果从均匀分布中随机选择的值小于或等于epsilon，那么将从活动动作的全部范围中选择一个值。否则，当前状态将用于使用我们先前定义的模型预测下一个动作，这会导致加权概率来判断这些动作中哪个是正确的。选择具有最高概率的动作。这是探索与利用之间的平衡，旨在寻找正确的下一步。
- en: 'Having covered exploration steps previously, we will now write our `replay()`
    function, which will exploit what the agent already knows and has stored in memory.
    We code this exploitation function using the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前讨论过探索步骤后，我们现在将编写我们的`replay()`函数，该函数将利用智能体已经知道并存储在记忆中的内容。我们使用以下代码来编写这个利用函数：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s break apart the function for leveraging what the agent already knows
    to help solve the puzzle. The first thing we will do is select a random sequence
    from the memory deque. We will then place each element from our sample into a
    given part of a sequence for the agent: state, action, target, next state, and
    the `done` flag to indicate whether the maze is solved. Next, we will add some
    code to change the way that our model predicts. We start by defining the target
    using the resulting state from the sequence, leveraging what we have already learned
    from attempting this sequence.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拆解这个函数，以利用智能体已经知道的内容来帮助解决难题。我们首先做的是从记忆双端队列（deque）中选择一个随机序列。接着，我们将从样本中提取的每个元素放入智能体序列中的特定部分：状态、动作、目标、下一状态和`done`标志，后者用来指示迷宫是否已解决。接下来，我们将添加一些代码来改变模型的预测方式。我们从使用序列结果中的状态来定义目标，利用我们从尝试此序列中学到的内容。
- en: Next, we predict on the state to get all the possible values the model would
    predict. We then insert the calculated value into that vector. When we run the
    model once more, we help train the model based on experience. This is also the
    step where epsilon is updated, which will result in more exploitation and less
    exploration during future iterations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们预测状态以获得模型可能预测的所有值。然后，我们将计算出的值插入到该向量中。当我们再次运行模型时，我们帮助模型基于经验进行训练。这也是更新 epsilon
    的步骤，这将导致在未来的迭代中更多的开发（exploitation）和更少的探索（exploration）。
- en: 'The very last step is to add a method or function for saving and loading our
    model. We add the means to save and load our model using the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是添加一个保存和加载我们模型的方法或函数。我们通过以下代码为保存和加载模型提供手段：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With these methods, we are now able to save the model that we defined earlier,
    as well as load the trained models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些方法，我们现在能够保存之前定义的模型，并加载训练好的模型。
- en: We will now need to take everything that we have covered and place it all in
    a overarching function that will take all the declared variables and functions
    and use them to create an `R6` class. To create our `R6` class, we will take all
    the code that we have just written and put it all together.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将我们所覆盖的所有内容放入一个总体函数中，该函数将接收所有声明的变量和函数，并用它们来创建一个 `R6` 类。为了创建我们的 `R6`
    类，我们将把刚才写的所有代码整合在一起。
- en: 'After running the complete code, we will have an `R6` class in our environment.
    It will have a class of `Environment`. If you click on it, you can see all the
    attributes of the class. You will notice that there are many attributes associated
    with creating a class that we did not specifically define; however, take a look
    at the following screenshot. We can see that the class is not portable, we can
    see the public fields where we will assign values, and we can see all the functions
    we defined, which are called methods when included as part of a class:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完整代码后，我们将会在环境中得到一个 `R6` 类。它将有一个 `Environment` 的类。如果你点击它，你可以看到类的所有属性。你会注意到有许多与创建类相关的属性，我们并没有特别定义；然而，看看以下截图。我们可以看到这个类不可移植，我们可以看到将要赋值的公共字段，还能看到我们定义的所有函数，这些函数在作为类的一部分时被称为方法：
- en: '![](img/5d74bda2-7d86-4252-b88d-ae67c317616c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d74bda2-7d86-4252-b88d-ae67c317616c.png)'
- en: With this step, we have completely created an `R6` class to act as our agent.
    We have provided it with various means to take an action based on the current
    state and an element of randomness, and we have also provided a way for our agent
    to explore the surface of this maze to find the target location. We have also
    provided a means for the agent to recall what it has already learned from past
    experience and use that to inform future decisions. Altogether, we have a complete
    reinforcement learning agent that learns through trial and error and, importantly,
    learns from past mistakes and from continually taking actions at random.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一步，我们已经完全创建了一个 `R6` 类，作为我们的代理。我们为它提供了基于当前状态和一定随机性的行动手段，并且还为我们的代理提供了一种方式来探索这个迷宫的表面，以找到目标位置。我们还提供了一种方式，让代理回顾它从过去经验中学到的内容，并用这些来指导未来的决策。总的来说，我们拥有了一个完整的强化学习代理，它通过试错学习，最重要的是，从过去的错误中学习，并不断地通过随机行动来学习。
- en: Building a deep Q-learning model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度 Q 学习模型
- en: 'At this point, we have defined the environment and our agent, which will make
    running our model quite straightforward. Remember that to get set up for reinforcement
    learning using R, we used a technique from object-oriented programming, which
    is not used very often in a programming language such as R. We created a class
    that describes an object, but is itself not an object. To create an object from
    a class, we must instantiate it. We set our initial values and instantiate an
    object using our `DQNAgent` class by using the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经定义了环境和我们的代理，这将使得运行我们的模型变得相当简单。记住，为了使用 R 进行强化学习，我们采用了面向对象编程中的一种技巧，而这在像
    R 这样的编程语言中并不常用。我们创建了一个描述对象的类，但它本身并不是一个对象。为了从类中创建对象，我们必须实例化它。我们设置了初始值，并通过以下代码使用我们的
    `DQNAgent` 类实例化一个对象：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After running this block of code, we will see an agent object in our environment.
    The agent has a class of `Environment`; however, if we click on it, we will see
    something similar to the following screenshot, which contains some differences
    compared with our class:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们将在环境中看到一个代理对象。该代理的类为 `Environment`；然而，如果我们点击它，我们将看到类似下面的截图，其中包含与我们类的一些差异：
- en: '![](img/2ef1b1d8-1d0d-4d94-ae68-dd27e6addb57.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ef1b1d8-1d0d-4d94-ae68-dd27e6addb57.png)'
- en: After running this line, we will now have an object that has inherited all the
    attributes defined in the class. We pass in a state size of `2` as an argument
    because, for this environment, the state is two-dimensional. The two dimensions
    are position and velocity. We see the value that we passed is reflected alongside
    the `state_size` field. We pass in an action size of `20` as an argument because
    for this game, we will allow the agent to use up to `20` units of force to propel
    forward or backward. We can see this value as well. We also can see all the methods;
    however, they are no longer nested under various methods—they are now all just
    inherited by the `agent` object.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这一行代码后，我们现在拥有了一个继承了类中所有属性的对象。我们将 `2` 作为参数传入状态大小，因为对于这个环境，状态是二维的。这两个维度是位置和速度。我们可以看到我们传入的值在
    `state_size` 字段旁边被反映出来。我们将 `20` 作为参数传入动作大小，因为对于这个游戏，我们将允许代理使用最多 `20` 单位的力量向前或向后推进。我们也能看到这个值。同样，我们可以看到所有的方法；不过它们不再嵌套在不同的方法下——它们现在都由
    `agent` 对象继承。
- en: 'To create our environment, we use the `makeEnvironment` function from the `reinforcelearn`
    package, which allows for custom environment creation. We use the following code
    to pass the `step` and `reset` functions as arguments to create the custom environment
    for the agent to navigate:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的环境，我们使用 `reinforcelearn` 包中的 `makeEnvironment` 函数，该函数允许自定义环境创建。我们使用以下代码将
    `step` 和 `reset` 函数作为参数传递，以创建代理导航的自定义环境：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After running the preceding line of code, you will see an `env` object in your
    **Environment** pane. Note that this object also has a class of `Environment`.
    When we click on this object, we will see the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码行后，你会在 **Environment** 面板中看到一个 `env` 对象。请注意，这个对象也具有 `Environment` 类。当我们点击这个对象时，我们会看到以下内容：
- en: '![](img/59c8dba6-55e5-4355-a3ce-16d19a18bb5d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59c8dba6-55e5-4355-a3ce-16d19a18bb5d.png)'
- en: The preceding line of code used the functions that we created earlier to define
    an environment. We now have an instance of the environment, which includes a means
    of initializing a game, while the `step` function defines the range of possible
    motions that the agent can make every turn. Note that this is also an `R6` class,
    just like our agent class.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码行使用了我们之前创建的函数来定义一个环境。现在我们拥有了一个环境实例，它包含了初始化游戏的方式，而 `step` 函数定义了代理每次行动时可能的动作范围。请注意，这也是一个
    `R6` 类，就像我们的代理类一样。
- en: 'Lastly, we include two additional initial values. We establish the remaining
    initial values to complete our model setup by running the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们加入了两个额外的初始值。通过运行以下代码，我们建立了剩余的初始值，以完成我们的模型设置：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first value of `FALSE` for `done` denotes that the objective is not yet
    complete. The batch size of `32` is the size of the exploration attempt or series
    of moves the agent will make before beginning to leverage what is already known
    before the next series of moves.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`FALSE` 作为 `done` 的第一个值表示目标尚未完成。`32` 的批量大小是代理在开始利用已知信息进行下一系列动作之前，将尝试进行的探索动作或系列动作的大小。'
- en: This is the complete model setup for deep Q-learning. We have an instance of
    our agent, which is an object created with the characteristics we established
    in the class earlier. We also have an environment defined with the parameters
    we set up when we created our `step` and `reset` functions. Lastly, we defined
    some initial values and now, everything is complete. The next step is just to put
    the agent in motion, which we will do next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度 Q 学习的完整模型设置。我们有一个代理实例，这是一个根据我们之前在类中定义的特征创建的对象。我们还有一个定义了我们在创建 `step` 和 `reset`
    函数时设置的参数的环境。最后，我们定义了一些初始值，现在，一切都已经完成。下一步就是让代理开始行动，我们将在接下来完成这一过程。
- en: Running the experiment
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行实验
- en: 'The last step in reinforcement learning is to run the experiment. To do this,
    we need to drop the agent into the environment and then allow the agent to take
    steps until it reaches the goal. The agent is constrained by a limited number
    of possible moves and the environment also places another constraint—in our case,
    by setting boundaries. We set up a `for` loop that iterates through rounds of
    the agent attempting a legal move and then sees whether the maze has been successfully
    accomplished. The loop stops when the agent reaches the goal. To begin our experiment
    with our defined agent and environment, we write the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的最后一步是运行实验。为此，我们需要将代理放入环境中，然后允许代理采取步骤，直到达到目标。代理受到可用移动次数的限制，环境也施加了另一个约束——在我们的例子中，就是通过设置边界来实现。我们设置了一个`for`循环，循环通过代理尝试合法移动的回合，然后查看迷宫是否已成功完成。当代理到达目标时，循环停止。为了开始我们的实验，使用我们定义的代理和环境，我们编写了以下代码：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code runs the experiment that sets our agent in motion. The agent
    is governed by the values and functions in the class that we defined and is furthered
    by the environment that we created. As we can see, quite a few steps take place
    when we run our experiment. We will review each step here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码运行的是设置代理运动的实验。代理的行为受我们定义的类中的值和函数的控制，并且进一步由我们创建的环境推动。如我们所见，运行实验时会进行很多步骤。我们将在这里回顾每个步骤：
- en: 'After running the first line in the preceding code, we will see a starting
    state for our agent. If you view the `state` object, it will look something like
    this, where the position value is between `-0.4` and `-0.6` and the velocity is
    `0`:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行前面的代码的第一行后，我们将看到代理的初始状态。如果查看`state`对象，它将类似于这样，其中位置值介于`-0.4`和`-0.6`之间，速度为`0`：
- en: '![](img/9bbec86f-5db7-48e7-be24-23f5415d686c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bbec86f-5db7-48e7-be24-23f5415d686c.png)'
- en: 'After running the remaining code block, we will see something like the following
    printed to the console, which shows the state at every tenth round:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行剩余的代码块后，我们将看到类似以下内容的输出打印到控制台，其中显示了每十轮的状态：
- en: '![](img/d7b2c0e8-6db0-43a5-8f9e-4b16da70a73e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7b2c0e8-6db0-43a5-8f9e-4b16da70a73e.png)'
- en: When we run this code, the first thing that happens is that the environment
    is reset and the agent is placed on the surface.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，首先发生的事情是环境被重置，代理被放置在表面上。
- en: 'Then, the loop is initiated. Every round in the loop has the following sequence
    of activities:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，启动循环。每一轮中的活动顺序如下：
- en: First, use the `act` function in the `agent` class to take an action. Remember,
    this function defines the allowable moves for the agent.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`agent`类中的`act`函数来执行一个动作。记住，这个函数定义了代理允许的移动。
- en: Next, we pass the action that the agent takes through to the `step` function
    to get the results.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将代理采取的动作传递给`step`函数，以获取结果。
- en: The output is the next state, which is where the agent lands after the action,
    as well as the reward based on whether the action led to a positive result, and
    finally the `done` flag, which indicates whether the target has been reached successfully.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出是下一个状态，这是代理在执行动作后所到达的位置，以及基于该动作是否带来了正面结果的奖励，最后是`done`标志，表示目标是否已经成功到达。
- en: These three elements are output from the function as a `list` object.
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这三个元素作为`list`对象从函数中输出。
- en: The next steps are to assign them to their own objects. For `reward` and `done`,
    we just extract them from the list and assign them to an integer and logical data
    type, respectively. For the next state, it is a little more difficult. We first
    use `unlist` to extract the two values and then we place them in a 2 x 1 matrix.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的步骤是将它们分配到各自的对象中。对于`reward`和`done`，我们仅从列表中提取它们，并将它们分别分配为整数和逻辑数据类型。至于下一个状态，这稍微复杂一些。我们首先使用`unlist`提取两个值，然后将它们放入一个2
    x 1的矩阵中。
- en: After all of the elements in an agent's move are moved to their own objects,
    the reward is calculated. In our case, there are no intermediate accomplishments
    that would lead to a reward short of reaching the target, so `reward` and `done`
    operate in a similar way. Here, we see that if the `done` flag is set to `TRUE`,
    then `reward` is set to `0`, as defined in the `step` function, when `reward`
    is `TRUE`.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将代理移动的所有元素转移到它们自己的对象后，奖励被计算出来。在我们的例子中，除非达到目标，否则没有中间成就会导致奖励，因此`reward`和`done`的操作方式相似。这里，我们看到，如果`done`标志被设置为`TRUE`，则当`reward`为`TRUE`时，`reward`被设置为`0`，如`step`函数中所定义的那样。
- en: Next, all the values that were output from the `step` function are added to
    the `memory` deque object. The `memorize` function takes each value and pushes
    it to the first element in the deque while pushing existing values back.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，所有从`step`函数输出的值都将添加到`memory`队列对象中。`memorize`函数将每个值推送到队列的第一个元素位置，并将现有值推回。
- en: After this, the `state` object is assigned the value of the next state. This
    is because the next state is now the new current state as the agent takes a new
    step.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此之后，`state`对象被赋值为下一个状态。这是因为下一个状态现在成为了新的当前状态，因为智能体采取了一个新的步骤。
- en: There is then a check to see whether the agent has reached the end of the maze.
    If so, the loop breaks and the epsilon value is printed to see how much was done
    through exploration and how much through exploitation. For all the other rounds,
    there is a secondary check that prints the current state and velocity for every
    tenth move.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后会检查智能体是否已经到达迷宫的终点。如果是，则跳出循环并打印出epsilon值，以查看通过探索完成了多少任务，探索和利用各占多少。对于其他回合，会有一个二次检查，打印每十步的当前状态和速度。
- en: The other conditional is the trigger for the `replay` function. After reaching
    the threshold, the agent pulls values from the memory deque and the process continues
    from there.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个条件是`replay`函数的触发条件。在达到阈值后，智能体从记忆队列中提取值，过程从那里继续。
- en: This is the entire process for running an experiment for reinforcement learning.
    With this process, we now have a method of reinforcement learning that is more
    robust than just using Q-learning. While using Q-learning is a good solution when
    the environment is limited and known, deep Q-learning is required when the environment
    scales up or changes dynamically. By iterating over a defined agent taking actions
    in a defined environment, we can see how well the defined agent can solve the
    problem presented in the environment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是执行强化学习实验的整个过程。通过这个过程，我们现在拥有了一种比单纯使用Q学习更为强大的强化学习方法。虽然在环境有限且已知时，使用Q学习是一个不错的解决方案，但当环境扩展或动态变化时，则需要深度Q学习。通过让定义好的智能体在定义好的环境中采取行动并进行迭代，我们可以看到该智能体在解决环境中提出的问题时的表现如何。
- en: Improving performance with policy functions
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用策略函数提升性能
- en: We have successfully coded an agent to use a neural network deep learning architecture
    to solve a problem. Let's now look at a few ways that we could improve our model.
    Unlike other machine learning, we cannot evaluate to a performance metric as usual,
    where we try to minimize some chosen error rate. Success in reinforcement learning
    is slightly more subjective. You may want an agent to complete a task as quickly
    as possible, to acquire as many points as possible, or to make the fewest mistakes
    possible. In addition, depending on the task, we may be able to alter the agent
    itself to see how it impacts results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地编写了一个智能体，使用神经网络深度学习架构来解决问题。现在让我们看一下可以改进模型的几种方法。与其他机器学习不同，我们无法像通常那样通过性能指标进行评估，通常我们会尝试最小化某个选择的错误率。强化学习的成功略带主观性。你可能希望智能体尽可能快地完成任务，尽可能多地获取积分，或者尽可能少地犯错。此外，根据任务的不同，我们可能能够改变智能体本身，看看它对结果有何影响。
- en: 'We will look at three possible methods for improving performance:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论三种可能的提高性能的方法：
- en: '**Action size**:At times, this will be an option and, at times, it will not.
    If you are trying to solve a problem where the agent rules and environment rules
    are set externally, such as trying to optimize performance in a game such as chess,
    then this will not be an option. However, you can imagine a problem such as setting
    up a self-driving car and in this case, you could change the agent, if it would
    work better in this environment. With our experiment, try changing the action
    size value from `20` to `10` and also to `40` to see what happens.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作大小**：有时这是一个选项，有时则不是。如果你正在尝试解决一个智能体规则和环境规则在外部设置的问题，例如尝试在棋类游戏中优化性能，那么这将不是一个选项。然而，你可以想象一个问题，例如设置一个自动驾驶汽车，在这种情况下，如果在该环境下效果更好，你可以改变智能体。通过我们的实验，尝试将动作大小值从`20`更改为`10`，并且再更改为`40`，看看会发生什么。'
- en: '**Batch size**: We can also adjust the batch size to see how it impacts performance.
    Remember that when the move count for the agent reaches the threshold for a batch,
    the agent then selects values from memory to begin to leverage what is already
    known. By raising or lowering this threshold, we provide a policy for the agent
    that, more or less, exploration should be conducted before using what is already
    known. Change the batch size to `16`, `64`, and `128` to see which option results
    in the agent completing the challenge the quickest.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：我们还可以调整批量大小，以观察它对性能的影响。请记住，当代理的移动次数达到批次的阈值时，代理就会从记忆中选择值，开始利用已有的知识。通过提高或降低这个阈值，我们为代理提供了一种策略，即在使用已有知识之前，应该进行更多或更少的探索。将批量大小更改为`16`、`64`和`128`，观察哪个选项能让代理最快完成任务。'
- en: '**Neural network**:The last part of the agent policy that we will discuss modifying
    is the neural network. In many ways, this is the brain for the agent. By making
    changes, we can allow our agent to make choices that will lead to a more optimized
    performance. Within the `AgentDQN` class, add some layers to the neural network
    and then run the experiment again to see what happens. Then, make some changes
    to the number of units in each layer and run those experiments to see what happens.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：我们将讨论的最后一个需要修改的代理策略部分是神经网络。在很多方面，它是代理的“大脑”。通过修改，我们可以让代理做出更多有利于优化性能的选择。在`AgentDQN`类中，添加一些神经网络层，然后重新运行实验，看看会发生什么。接着，改变每一层的单元数量，并再次运行实验，查看结果。'
- en: In addition to these changes, we could also make changes to the starting epsilon
    value, how quickly the epsilon decays, and the learning rate for the neural network
    model. All of these types of changes will impact the policy functions for the
    agent. When we change a value that alters the output of the act or replay function,
    then we modify the policy that the agent uses to solve the problem. We can make
    a policy for the agent to explore a wider or narrower number of actions if possible,
    or to use more or less time exploring the environment versus exploiting current
    knowledge, as well as adjust how quickly the agent learns from every move, how
    many times an agent may try a similar action to see whether it is always incorrect,
    and how drastically the agent tries to adjust after trying actions that lead towards
    failure.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些变化外，我们还可以对起始的epsilon值、epsilon衰减的速度以及神经网络模型的学习率进行调整。这些变化都会影响代理的策略函数。当我们更改一个值，这个值会改变动作或回放函数的输出时，我们就在修改代理用来解决问题的策略。我们可以为代理制定策略，让它探索更多或更少的动作，或者调整它在探索环境与利用当前知识之间所花费的时间，还可以调整代理从每一步中学习的速度，代理可能尝试多少次相似的动作以查看是否总是错误的，以及在采取导致失败的动作后，代理尝试调整的幅度。
- en: As with any type of machine learning, there are a number of parameters that
    can be tuned to optimize performance in reinforcement learning. Unlike other problems,
    there may not be a standard metric to help tune these, and deciding on the values
    that will work best may be more subjective and rely more on trial and error experimentation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何类型的机器学习一样，强化学习中有许多参数可以调节以优化性能。与其他问题不同，可能没有标准的度量来帮助调整这些参数，选择最合适的值可能更为主观，并且依赖于试验和错误的实验过程。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we wrote code to conduct reinforcement learning using deep
    Q-learning. We noted that while Q-learning is a simpler approach, it requires
    a limited and known environment. Applying deep Q-learning allows us to solve problems
    at a larger scale. We also defined our agent, which required creating a class.
    The class defined our agent and we instantiated an object with the attributes
    defined in our class to solve the reinforcement learning challenge. We then created
    a custom environment using functions that defined boundaries, as well as the range
    of moves the agent could take and the target or objective. Deep Q-learning involves
    adding a neural network to select actions, rather than relying on the Q matrix,
    as in Q-learning. We then added a neural network to our agent class.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，我们编写了代码，使用深度Q学习进行强化学习。我们注意到，尽管Q学习是一种更简单的方法，但它需要一个有限且已知的环境。而应用深度Q学习则使我们能够在更大范围内解决问题。我们还定义了我们的智能体，这需要创建一个类。该类定义了我们的智能体，并通过实例化一个对象，将类中定义的属性应用于解决强化学习的挑战。接着，我们创建了一个自定义环境，使用函数定义了边界，以及智能体可以采取的移动范围和目标或目的。深度Q学习涉及在选择动作时加入神经网络，而不是像Q学习那样依赖Q矩阵。随后，我们将神经网络添加到我们的智能体类中。
- en: Lastly, we put it all together by placing our agent object in our custom environment
    and letting it take various actions until it solved the problem. We further discussed
    some choices we could make to improve the agent's performance. With this framework,
    you are ready to apply reinforcement learning to any number of environments using
    any number of possible agents. The process will largely stay consistent; the changes
    will be in how the agent is programmed to act and learn and what the rules are
    in the environment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将智能体对象放入自定义环境，并让它采取各种行动，直到解决问题，从而将所有内容整合在一起。我们进一步讨论了一些可以采取的选择，以提高智能体的表现。有了这个框架，你已经准备好将强化学习应用于各种环境，并使用各种可能的智能体。这个过程基本上保持一致，变化将体现在智能体的编程方式以及环境中的规则。
- en: This completes *Hands-On Deep Learning with R*. Throughout this book, you have
    learned a wide variety of deep learning methods. In addition, we applied these
    methods to a diverse set of tasks. This book was written with a bias toward action.
    The goal of this book was to provide concise code that addresses practical projects.
    Using what you have learned in this book, I hope that I have prepared you well
    to begin solving real-world challenges using deep learning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了*Hands-On Deep Learning with R*。在本书中，你学习了多种深度学习方法。此外，我们还将这些方法应用于多种不同的任务。本书的编写偏向于实际操作。其目标是提供简洁的代码，解决实际项目中的问题。希望通过本书的学习，你已经准备好利用深度学习解决现实世界中的挑战。
