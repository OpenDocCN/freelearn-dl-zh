- en: Deep Neural Networks for Multi-Class Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于多类分类的深度神经网络
- en: 'When developing prediction and classification models, depending on the type
    of response or target variable, we come across two potential type of problems:
    the target variable is of categorical type (this is a classification type of problem)
    or the target variable is of a numeric type (this is a regression type of problem).
    It has been observed that about 70% of the data belongs to problems arising from
    classification categories and the remaining 30% are regression problems (here
    is the reference: [https://www.topcoder.com/role-of-statistics-in-data-science/](https://www.topcoder.com/role-of-statistics-in-data-science/)).
    In this chapter, we will provide steps for applying deep learning neural networks
    for classification problems. The steps are illustrated using the fetal cardiotocograms,
    or CTGs.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发预测和分类模型时，根据响应或目标变量的类型，我们会遇到两种潜在类型的问题：目标变量是类别型（这是一个分类问题）或目标变量是数值型（这是一个回归问题）。有研究表明，大约
    70% 的数据属于分类类别问题，剩下的 30% 是回归问题（参考文献：[https://www.topcoder.com/role-of-statistics-in-data-science/](https://www.topcoder.com/role-of-statistics-in-data-science/)）。在本章中，我们将提供使用深度学习神经网络解决分类问题的步骤。步骤将使用胎儿心电图（CTG）进行说明。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: A brief understanding of the fetal cardiotocogram (or CTG) dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对胎儿心电图（或 CTG）数据集的简要理解
- en: Steps for data preparation, including normalization, data partitioning, and
    one-hot encoding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备的步骤，包括归一化、数据分区和独热编码
- en: Creating and fitting a deep neural network model for classification problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分类问题创建并拟合深度神经网络模型
- en: Evaluating classification model performance and making predictions using the
    model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类模型性能并使用模型进行预测
- en: Fine-tuning the model for performance optimization and best practices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行微调，以优化性能并采用最佳实践
- en: Cardiotocogram dataset
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胎儿心电图数据集
- en: In this section, we will provide information about the data used for developing
    a multiclass classification model. We will use only one library, which is Keras.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供有关用于开发多类分类模型的数据的相关信息。我们将仅使用一个库，即 Keras。
- en: Dataset (medical)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集（医学）
- en: The dataset used in this chapter is publicly available at the UCI Machine Learning
    Repository maintained by the School of Information and Computer Science at the University
    of California. You can access this at [https://archive.ics.uci.edu/ml/datasets/cardiotocography](https://archive.ics.uci.edu/ml/datasets/cardiotocography).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集可以在由加利福尼亚大学信息与计算机科学学院维护的 UCI 机器学习库中公开访问。你可以通过[https://archive.ics.uci.edu/ml/datasets/cardiotocography](https://archive.ics.uci.edu/ml/datasets/cardiotocography)访问该数据集。
- en: It is to be noted that this URL enables you to download an Excel data file.
    This file can be easily converted to a `.csv` format by saving the file as a `.csv`
    file.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，该 URL 允许你下载 Excel 数据文件。你可以通过将文件另存为 `.csv` 文件，轻松将其转换为 `.csv` 格式。
- en: 'For data we should use the formatting which is used for `.csv`, as shown in
    the following code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据，我们应使用 `.csv` 格式的格式化方式，如以下代码所示：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This data consists of fetal CTGs, and the target variable classifies a patient
    into one of three categories: normal, suspect, and pathological. There are 2,126
    rows in this dataset. The CTGs are classified by three expert obstetricians, and
    a consensus classification label is assigned to each of them as normal (N) (represented
    by 1), suspect (S) (represented by 2), and pathological (P) (represented by 3).
    There are 21 independent variables, and the main objective is to develop a classification
    model to correctly classify each patient into one of the three categories represented
    by N, S, and P.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据包含胎儿的心电图（CTG），目标变量将患者分类为三类之一：正常、可疑和病理。该数据集共有 2,126 行数据。CTG 由三位专家产科医生分类，并为每个
    CTG 分配一致的分类标签：正常（N）（用 1 表示）、可疑（S）（用 2 表示）和病理（P）（用 3 表示）。数据集共有 21 个独立变量，主要目标是开发一个分类模型，将每个患者正确分类为
    N、S 和 P 之一。
- en: Preparing the data for model building
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: In this section, we will prepare the data for building the classification model.
    Data preparation will involve normalizing the data, partitioning the data into
    training and test data, and carrying out one-hot encoding of the response variable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将准备数据以构建分类模型。数据准备将涉及数据的归一化、将数据划分为训练数据和测试数据，以及对响应变量进行独热编码。
- en: Normalizing numeric variables
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值变量标准化
- en: 'For developing deep network models, we carry out the normalization of numeric
    variables to bring them to a common scale. When dealing with several variables,
    it is likely that different variables have different scales—for example, there
    could be a variable that shows revenues earned by a company and the values could
    be in millions of dollars. In another example, there could be a variable that
    shows the dimension of a product in centimeters. Such extreme differences in scale
    create difficulties when training a network, and normalization helps to address
    this issue. For normalization, we will use the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发深度网络模型时，我们对数值变量进行标准化，使其达到统一的尺度。在处理多个变量时，不同变量可能具有不同的尺度——例如，某个变量可能表示公司赚取的收入，数值可能是以百万美元为单位的。在另一个例子中，某个变量可能表示产品的尺寸，以厘米为单位。如此极端的尺度差异在训练网络时会造成困难，而标准化有助于解决这个问题。对于标准化，我们将使用以下代码：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see from the preceding code, we first change the data to matrix format,
    and then we remove the default names by assigning `NULL` to the dimension names.
    In this step, the names of 22 variables will be changed to `V1`, `V2`, `V3`,...,
    `V22`. If you run `str(data)` at this stage, you will notice the change in format
    of the original data. We normalize the 21 independent variables using the `normalize`
    function, which is a part of the Keras package. When you run this line of code,
    you will notice that it uses TensorFlow as a backend. We also change the target
    variable, NSP, to numeric from the default integer type. In addition, in the same
    line of code, we also change values from `1`, `2`, and `3` to `0`, `1`, and `2`
    respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们首先将数据转换为矩阵格式，然后通过将`NULL`赋值给维度名称来去除默认的名称。在这一步中，22个变量的名称将被更改为`V1`、`V2`、`V3`，一直到`V22`。如果你在此阶段运行`str(data)`，你会注意到原始数据的格式已经发生了变化。我们使用`normalize`函数对21个独立变量进行标准化，该函数是Keras包的一部分。当你运行这一行代码时，你会注意到它使用了TensorFlow作为后端。我们还将目标变量NSP从默认的整数类型更改为数值型。此外，在同一行代码中，我们还将`1`、`2`和`3`的值分别更改为`0`、`1`和`2`。
- en: Partitioning the data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据划分
- en: 'Next, we will partition this data into training and test datasets. To carry
    out data partitioning, we use the following code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把这些数据划分为训练集和测试集。为了进行数据划分，我们使用以下代码：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see from the preceding code, to obtain the same samples in the training
    and test datasets for repeatability purposes, we use `set.seed` with a specific
    number, which in this case is `1234`. This will ensure that the reader can also
    obtain the same samples in the training and test data. For data partitioning,
    a 70:30 split is used here, but any other ratio can be used too. In machine learning
    applications, this is a commonly used step to ensure that the prediction model
    works well with unseen data that is stored in the form of test data. Training
    data is used for developing the model and test data is used to assess the performance
    of the model. Sometimes, a prediction model may perform very well or even perfectly
    well with training data; however, when it is evaluated with test data that has
    not been seen by the model, the performance may turn out to be very disappointing.
    In machine learning, this problem is termed as over fitting the model. Test data
    helps to assess and ensure that the prediction model can be reliably implemented
    for making the appropriate decisions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，为了获得相同的训练集和测试集样本以保证可重复性，我们使用`set.seed`并指定一个特定的数字，在此案例中是`1234`。这将确保读者也能获得相同的训练数据和测试数据样本。在数据划分时，这里使用了70:30的比例，但也可以使用其他比例。在机器学习应用中，这是一个常用的步骤，旨在确保预测模型能够在未见过的数据（即测试数据）上表现良好。训练数据用于开发模型，而测试数据则用于评估模型的性能。有时，预测模型可能在训练数据上表现得非常好，甚至完美；然而，当用模型未见过的测试数据进行评估时，模型的表现可能会非常令人失望。在机器学习中，这个问题被称为模型的过拟合。测试数据有助于评估并确保预测模型能够可靠地用于做出正确的决策。
- en: We use `training` and `test` names to store independent variables and we use
    `trainingtarget` and `testtarget` names to store target variables stored in the
    22^(nd) column of the dataset. After data partitioning, we will have 1,523 observations
    in the training data and the remaining 603 observations will be in the test data.
    Note that although we use a 70:30 split here, the actual ratio after data partitioning
    may not be exactly 70:30.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`training`和`test`名称来存储自变量，使用`trainingtarget`和`testtarget`名称来存储目标变量，这些目标变量存储在数据集的第22列中。数据划分后，我们将在训练数据中获得1,523个观察值，剩余的603个观察值将位于测试数据中。请注意，尽管我们在这里使用70:30的划分比例，但实际的数据划分比例可能并不完全是70:30。
- en: One-hot encoding
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: One-hot编码
- en: 'After data partitioning, we will carry out a one-hot encoding of the response
    variable. One-hot encoding helps to represent a categorical variable in zeros
    and ones. The code and output for one-hot encoding is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据划分后，我们将对响应变量进行一-hot编码。One-hot编码有助于将分类变量表示为零和一。One-hot编码的代码和输出如下所示：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see in the preceding code, with the help of the `to_categorical`
    function from the Keras package, we convert the target variable to a binary class
    matrix, where the presence or absence of a class is simply represented by 1 or
    0 respectively. In this example, we have three classes for the target variable,
    which are converted to three dummy variables. This process is also called **one-hot
    encoding**. First, 10 rows from `testLabels` are printed. The first row indicates
    the normal category for the patient with (1,0,0), the sixth row indicates the
    suspect category for the patient with (0,1,0), and the fourth row provides an
    example of the pathologic category of a patient with (0,0,1).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，在Keras包中借助`to_categorical`函数，我们将目标变量转换为二进制类别矩阵，其中类别的存在与否分别由1或0表示。在这个示例中，我们有三个目标变量类别，这三个类别被转换为三个虚拟变量。这个过程也叫做**one-hot编码**。首先，打印了`testLabels`中的10行数据。第一行表示患者的正常类别，标记为（1,0,0）；第六行表示患者的可疑类别，标记为（0,1,0）；第四行则提供了一个患者病理类别的示例，标记为（0,0,1）。
- en: Once we complete these steps for data preparation, we move to the next step,
    where we create the classification model to classify a patient as normal, suspect,
    or pathological.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 完成数据准备步骤后，我们进入下一步，在这一阶段我们创建分类模型，将患者分类为正常、可疑或病理。
- en: Creating and fitting a deep neural network model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建并拟合深度神经网络模型
- en: In this section, we will develop the model architecture, compile the model,
    and then fit the model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将开发模型架构、编译模型，然后拟合模型。
- en: Developing model architecture
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发模型架构
- en: 'The code used for developing the model is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用于开发模型的代码如下：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As shown in the preceding code, we start by creating a sequential model using
    the `keras_model_sequential()` function, which allows a linear stack of layers
    to be added. Next, we add layers to the model using the pipe operator, `%>%`.
    This pipe operator takes information from the left as output and feeds that information
    as input to what is on the right. We use a fully connected or densely connected
    neural network using the `layer_dense` function and then specify various inputs.
    In this dataset, we have 21 independent variables, and as such, the `input_shape` function is
    specified as 21 neurons or units in the neural network. This layer is also termed
    as the input layer in the network. The first hidden layer has 8 units and the
    activation function that we use here is a rectified linear unit, or `relu`, which
    is the most popular activation function used in these situations. The first hidden
    layer is connected to the output layer with 3 units using the pipe operator. We
    use 3 units since our target variable has 3 classes. The activation function used
    in the output layer is `'softmax'`, which helps to keep the range of output values
    between 0 and 1\. Keeping the range of output values between 0 and 1 will help
    us to interpret results in the form of familiar probability values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们首先使用`keras_model_sequential()`函数创建一个顺序模型，它允许将层按线性堆叠的方式添加。接下来，我们使用管道操作符`%>%`向模型中添加层。这个管道操作符将左侧的输出信息作为输入，传递给右侧的操作。我们使用`layer_dense`函数构建一个全连接的神经网络，并指定各种输入。在这个数据集中，我们有21个独立变量，因此，`input_shape`函数被指定为21个神经元或单元。该层也被称为网络中的输入层。第一个隐藏层包含8个单元，我们在此使用的激活函数是修正线性单元`relu`，它是这种情况下最常用的激活函数。第一个隐藏层通过管道操作符与包含3个单元的输出层相连接。我们使用3个单元是因为我们的目标变量有3个类别。输出层使用的激活函数是`'softmax'`，它有助于将输出值的范围保持在0到1之间。将输出值的范围控制在0到1之间有助于我们将结果解释为熟悉的概率值。
- en: For typing the pipe operator, `%>%`, in RStudio, you can use the *Shift* + *Command*
    + *M* shortcut for Mac, and for Windows, *Shift* + *Ctrl* + *M*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在RStudio中输入管道操作符`%>%`时，对于Mac，可以使用*Shift* + *Command* + *M*快捷键，对于Windows，则可以使用*Shift*
    + *Ctrl* + *M*。
- en: 'To obtain a summary of the model architecture that we have created, we can
    run the `summary` function, as shown in the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取我们创建的模型架构的摘要，我们可以运行`summary`函数，如下代码所示：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Since the input layer has 21 units that are connected to each of the 8 units
    in the first hidden layer, we end up with 168 weights (21 x 8). We also obtain
    one bias term for each unit in the hidden layer, with a total of 8 such terms.
    So, at the first and only hidden layer stage, we have a total of 176 parameters
    (168 + 8). Similarly, 8 units in the hidden layer are connected to 3 units in
    the output layer, yielding 24 weights (8 x 3). This way, we have 24 weights and
    3 bias terms at the output layer that account for a total of 27 parameters. Finally,
    the total number of parameters for this neural network architecture will be 203.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入层有21个单元，每个单元与第一个隐藏层的8个单元相连接，因此我们得到了168个权重（21 x 8）。我们还为隐藏层中的每个单元获得一个偏置项，总共有8个这样的项。因此，在第一个也是唯一的隐藏层阶段，我们总共有176个参数（168
    + 8）。类似地，隐藏层中的8个单元与输出层的3个单元相连接，得到24个权重（8 x 3）。这样，输出层就有24个权重和3个偏置项，总共有27个参数。最后，这个神经网络架构的参数总数为203。
- en: Compiling the model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'To configure the learning process for the neural network, we compile the model
    by specifying the loss, optimizer, and metrics, as shown in the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置神经网络的学习过程，我们通过指定损失函数、优化器和评估指标来编译模型，如下代码所示：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We use `loss` for specifying the objective function that we want to optimize.
    As shown in the preceding code, for the loss, we use `'categorical_crossentropy'`,
    since our target variable has three categories. For situations where the target
    variable has two categories, we use `binary_crossentropy`. For the optimizer,
    we use the `'adam'` optimization algorithm, which is a popular algorithm for deep
    learning. Its popularity is mainly due to the fact that it gives good results
    faster than other stochastic optimization methods, such as the **adaptive gradient
    algorithm** (**AdaGrad**) and **root mean square propagation** (**RMSProp**).
    We specify the metrics for evaluating the model performance during training and
    testing. For `metrics`, we use `accuracy` to assess the classification performance
    of the model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`loss`来指定我们希望优化的目标函数。如前面的代码所示，对于损失函数，我们使用`'categorical_crossentropy'`，因为我们的目标变量有三个类别。对于目标变量有两个类别的情况，我们使用`binary_crossentropy`。对于优化器，我们使用`'adam'`优化算法，这是一个流行的深度学习优化算法。它之所以受欢迎，主要是因为它比其他随机优化方法（如**自适应梯度算法**（**AdaGrad**）和**均方根传播**（**RMSProp**））能够更快地得到好的结果。我们指定了用于评估模型训练和测试性能的度量标准。对于`metrics`，我们使用`accuracy`来评估模型的分类性能。
- en: Now we are ready to fit the model, which we will do in the next section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好拟合模型，接下来我们将在下一节中进行此操作。
- en: Fitting the model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'To fit the model, we make use of the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合模型，我们使用以下代码：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As seen from the preceding code, we see the following observations:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中可以看出，我们得到了以下观察结果：
- en: To fit the model, we provide training data that has data for 21 independent
    variables and `trainLabels`, which contains data for the target variable.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了拟合模型，我们提供了包含21个自变量数据的训练数据，以及包含目标变量数据的`trainLabels`。
- en: The number of iterations or epochs is specified as 200\. An epoch is a single
    pass of the training data followed by model assessment using validation data.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数或轮数设置为200。一个轮次是指训练数据的一次完整传递，随后使用验证数据进行模型评估。
- en: To avoid overfitting, we specified that the validation split is 0.2, which means
    that 20% of the training data will be used to assess the model performance as
    the training proceeds.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免过拟合，我们指定了验证数据的拆分比例为0.2，这意味着20%的训练数据将在训练过程中用于评估模型性能。
- en: Note that this 20% of data is the bottom 20% of the data points in the training
    data. We stored data on the loss and accuracy values for the training and validation
    data generated during the training of the model in `model_one` for later use.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，这20%的数据是训练数据中底部20%的数据点。我们将训练过程中生成的训练数据和验证数据的损失和准确度值存储在`model_one`中，供以后使用。
- en: For `batch_size`, we used the default value of 32, which represents the number
    of samples that will be used per gradient.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`batch_size`，我们使用默认值32，这表示每次梯度更新时所使用的样本数量。
- en: As the training of the model proceeds, we get a visual display of plots for
    loss and accuracy based on training and validation data after each epoch.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着模型的训练进行，我们会在每个轮次后基于训练和验证数据展示损失和准确度的可视化图表。
- en: For accuracy, we would like the model to have higher values, as accuracy is
    a t`he-higher-the-better` type of metric, whereas for loss, which is a `the-lower-the-better`
    type of metric, we would like the model to have lower values.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于准确率，我们希望模型的值越高越好，因为准确率是`越高越好`类型的指标，而对于损失，它是`越低越好`类型的指标，我们希望模型的值越低越好。
- en: In addition, we also obtained the numeric summary of the loss output based on
    the last 3 epochs, as shown in the preceding code output. For each epoch, we saw
    that 1,218 samples out of 1,523 samples of the training data (about 80%) were
    used for fitting the model. The remaining 20% of the data was used for calculating
    accuracy and loss values for the validation data.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们还获得了基于最后3个轮次的损失输出的数值摘要，如前面的代码输出所示。对于每个轮次，我们看到从1,523个训练数据样本中使用了1,218个样本（约80%）进行模型拟合。剩余的20%的数据用于计算验证数据的准确度和损失值。
- en: A word of caution. When using `validation_split`, note that the validation data
    is not selected randomly from the training data—for example, when `validation_split
    = 0.2`, the last 20% of the training data is used for validation and the first
    80% is used for training. Therefore, if the values of the target variable are
    not random, then `validation_split` may introduce bias in the classification model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，当使用`validation_split`时，请注意验证数据并不是从训练数据中随机选择的——例如，当`validation_split =
    0.2`时，训练数据的最后20%被用作验证，前80%用于训练。因此，如果目标变量的值不是随机的，那么`validation_split`可能会在分类模型中引入偏差。
- en: 'After the training process completes 200 epochs, we can plot the training progress
    in terms of loss and accuracy for training and validation data using the `plot`
    function, as shown in the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程完成200个纪元后，我们可以使用`plot`函数绘制训练和验证数据的损失和准确率进展，如下代码所示：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following graph provides a plot that has accuracy in the top window and
    loss in the lower window:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了一个图形，其中顶部窗口显示准确率，底部窗口显示损失：
- en: '![](img/e7d38d49-2ace-4e8d-9365-ba2e65201ed1.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7d38d49-2ace-4e8d-9365-ba2e65201ed1.png)'
- en: Accuracy and loss for training and validation data
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的准确率与损失
- en: 'From the preceding plot for loss and accuracy, we can make the following observations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的损失和准确率图表中，我们可以做出以下观察：
- en: From the plot for accuracy in the top graph, you can see that accuracy values
    increase significantly after about 25 epochs and then continue to increase gradually
    for the training data.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从顶部图表中准确率的图形可以看到，准确率值在大约25个纪元后显著增加，并随后持续逐渐增加，尤其是对于训练数据。
- en: For validation data, the progress is more uneven, with a drop in accuracy between
    the 25^(th) and 50^(th) epochs.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于验证数据，进展更加不均衡，在第25^(th)和第50^(th)个纪元之间，准确率出现下降。
- en: A somewhat similar pattern is observed in the opposite direction for loss values.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在损失值的反向方向上观察到一个类似的模式。
- en: Note that if the training data accuracy increases with the number of epochs,
    but the validation data accuracy decreases, that would suggest an overfitting
    of the model. We do not see any major pattern suggesting model overfitting from
    this plot.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，如果训练数据的准确率随着纪元数的增加而提高，但验证数据的准确率下降，这可能表明模型出现了过拟合。从这个图表中我们没有看到任何主要的模式表明模型过拟合。
- en: Model evaluation and predictions
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: In this section, we will use test data to evaluate the model performance. We
    can certainly calculate loss and accuracy values using the training data; however,
    the real test of a classification model is when it is used with unseen data. And
    since test data is kept separate from the model building process, we can now use
    it for model evaluation. We will first calculate loss and accuracy values with
    the test data and then develop a confusion matrix.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用测试数据来评估模型的性能。当然，我们可以使用训练数据来计算损失和准确率值；然而，分类模型的真正考验是它在未见过的数据上进行测试。由于测试数据与模型构建过程是分开的，因此我们现在可以使用它来进行模型评估。我们将首先使用测试数据计算损失和准确率值，然后构建混淆矩阵。
- en: Loss and accuracy calculation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失和准确率计算
- en: 'The code for obtaining loss and accuracy values using the test data along with
    the output is shown in the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用测试数据获取损失和准确率值的代码及输出：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see in the preceding code, using the `evaluate` function, we can
    obtain loss and accuracy values as `0.4439` and `0.8424` respectively. Using `colSums(testLabels)`,
    we can find that there are 460, 94, and 49 cases of normal, suspect, and pathological
    patients respectively in the test data. Converting these numbers to percentages
    using a total of 603 samples in the test data, we obtain 76.3%, 15.6%, and 8.1%
    respectively. The highest number of samples belongs to the normal category of
    patients, and we can use 76.3% as a benchmark for the model performance. If we
    do not use any model and simply classify all cases in the test data as belonging
    to the normal category of patients, then we will still be correct about 76.3%
    of the time since we will be right about all normal patients and incorrect about
    the other two categories.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的代码所示，使用`evaluate`函数，我们可以得到损失值和准确率值分别为`0.4439`和`0.8424`。通过使用`colSums(testLabels)`，我们可以发现测试数据中正常、可疑和病理类别的患者分别有460、94和49例。将这些数字转换为百分比，基于603个样本的总数，我们得到分别为76.3%、15.6%和8.1%的比例。样本数量最多的是正常类别的患者，我们可以将76.3%作为模型表现的基准。如果我们不使用任何模型，而是简单地将测试数据中的所有病例分类为正常患者类别，那么我们仍然能在76.3%的情况下正确分类，因为我们会正确分类所有正常患者，而其他两类则会被分类错误。
- en: In other words, the accuracy of our prediction will be as high as 76.3%; therefore,
    the model that we develop here should perform at least better than this benchmark
    number. If it functions below this number, then it is not likely to be of much
    practical use. Since we get an accuracy of 84.2% for the test data, we are definitely
    doing better than the benchmark value, but clearly we must also try to improve
    our model in order to perform even better. To do that, let's dig even deeper and
    learn about model performance for each category of the response variable with
    the help of a confusion matrix.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们的预测准确率将达到76.3%；因此，我们在这里开发的模型至少应该表现得比这个基准值更好。如果其表现低于这个值，那么它不太可能在实际应用中有太大用处。由于我们在测试数据上的准确率为84.2%，我们显然已经超过了基准值，但显然我们还必须进一步改进模型，以便取得更好的表现。为了做到这一点，我们需要深入了解每个响应变量类别的模型表现，借助混淆矩阵来分析。
- en: Confusion matrix
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'To obtain a confusion matrix, let''s start by making a prediction for the test
    data and save it in `pred`. We use `predict_classes` to make this prediction and
    then use the `table` function to create a summary of predicted versus actual values
    for the test data to create a confusion matrix, as shown in the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得混淆矩阵，首先让我们为测试数据做一个预测，并将其保存在`pred`中。我们使用`predict_classes`进行预测，然后使用`table`函数创建一个预测值与实际值的汇总，以此生成混淆矩阵，如下所示的代码：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding confusion matrix, shown as output", values `0`, `1`, and `2`
    represent normal, suspect, and pathological categories respectively. From the
    confusion matrix, we can make the following observations:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的混淆矩阵中，输出中显示的值`0`、`1`和`2`分别代表正常、可疑和病理类别。从混淆矩阵中，我们可以做出以下观察：
- en: There were `435` patients in the test data who were actually normal and the
    model also predicted them as being normal.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据中有`435`名患者实际上是正常的，且模型也预测他们为正常。
- en: Similarly, there were `51` correct predictions for the suspect group and `22`
    correct predictions for the pathological group.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，`51`个可疑组的正确预测和`22`个病理组的正确预测也被记录在案。
- en: If we add all the numbers on the diagonal of the confusion matrix, which are
    the correct classifications, we obtain 508 (435 + 51 + 22), or an accuracy level
    of 84.2% ((508 ÷ 603) x 100).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将混淆矩阵对角线上的所有数字相加（即正确分类的数量），我们得到508（435 + 51 + 22），即准确率为84.2%（（508 ÷ 603）×
    100）。
- en: In the confusion matrix, the off diagonal numbers indicate the number of patients
    who are misclassified. The highest number of misclassifications is 41, where the
    patients actually belong to the suspect group but the model incorrectly classified
    them in the normal category of patients.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在混淆矩阵中，非对角线上的数字表示被错误分类的患者数量。最高的错误分类数为41，表示这些患者实际上属于可疑组，但模型错误地将其分类为正常患者。
- en: The instance of misclassification with the lowest number involved one patient
    who actually belonged to the normal category, but the model incorrectly classified
    this patient in the pathological category.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误分类数量最少的一项涉及一名患者，他实际上属于正常类别，但模型错误地将这名患者分类为病理类别。
- en: 'Let''s also look at the predictions in terms of probabilities instead of only
    classes, which was the approach that we used previously. To predict probabilities,
    we can use the `predict_prob` function. We can then look at the first seven rows
    from the test data using the `cbind` function for comparison, as shown in the
    following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看一下基于概率的预测，而不是仅仅查看类别，这是我们之前使用的方法。要预测概率，我们可以使用`predict_prob`函数。然后，我们可以使用`cbind`函数查看测试数据中的前七行进行比较，如下所示的代码：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding output, we have probability values for three categories based
    on the model and we also have the predicted category represented by `pred` and
    the actual category represented by `testtarget` in the test data. From the preceding
    output, we can make the following observations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们有基于模型的三个类别的概率值，同时我们还拥有测试数据中由`pred`表示的预测类别和由`testtarget`表示的实际类别。从前面的输出中，我们可以得出以下观察结果：
- en: For the first sample, the highest probability of `0.993` is for the normal category
    of patients, and that is the reason the predicted class is identified as `0`.
    Since this prediction matches the actual result in the test data, we treat this
    as the correct classification.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第一个样本，最高的概率是`0.993`，对应的是正常类别的患者，这就是预测类别被识别为`0`的原因。由于这个预测与测试数据中的实际结果一致，我们认为这是正确的分类。
- en: Similarly, since the fourth sample shows the highest probability of `0.7197`
    for the third category, the predicted class is labeled as `2`, which turns out
    to be a correct prediction.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，由于第四个样本对于第三类的最高概率为`0.7197`，因此预测类别被标记为`2`，这证明是一个正确的预测。
- en: However, the sixth sample has the highest probability of `0.9466` for the first
    category represented by `0`, whereas the actual class is `1`. In this case, our
    model misclassifies the sample.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，第六个样本对于第一类（由`0`表示）有最高的概率`0.9466`，而实际类别是`1`。在这种情况下，我们的模型将样本分类错误。
- en: Next, we will explore the option of improving the classification performance
    of the model to obtain better accuracy. Two key strategies that we can follow
    are to increase the number of hidden layers for building a deeper neural network
    and to change the number of units in the hidden layer. We will explore these options
    in the next section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索提高模型分类性能以获得更好准确率的选项。我们可以遵循的两项关键策略是增加隐藏层的数量以构建更深的神经网络，以及改变隐藏层中单元的数量。我们将在下一节中探索这些选项。
- en: Performance optimization tips and best practices
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: In this section, we fine-tune the previous classification model to explore its
    functions and see whether its performance can be further improved.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对之前的分类模型进行了微调，以探索其功能并查看其性能是否能够进一步提高。
- en: Experimenting with an additional hidden layer
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加额外隐藏层的实验
- en: 'In this experiment, we will add an additional hidden layer to the previous
    model. The code and output of the summary of the model is given as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将在之前的模型中添加一个额外的隐藏层。模型的代码和输出摘要如下：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As shown in the preceding code and output, we have added a second hidden layer
    with 5 units. In this hidden layer too, we use `relu `as the activation function.
    Note that as a result of this change, we have increased the total number of parameters
    from 203 in the previous model to 239 in this model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码和输出所示，我们添加了一个具有5个单元的第二个隐藏层。在这个隐藏层中，我们同样使用`relu`作为激活函数。请注意，由于这个改变，我们将总参数数量从之前模型的203个增加到了这个模型的239个。
- en: 'Next, we compile and then fit the model using the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码编译并拟合模型：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As shown in the preceding code, we have compiled the model with same settings
    that we used earlier. We have also kept the setting for the `fit` function the
    same as earlier. The model-output-related information is stored in `model_two`.
    The following diagram provides the plot of accuracy and loss for `model_two`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们已使用与之前相同的设置编译模型。我们还保持了`fit`函数的设置与之前一致。与模型输出相关的信息存储在`model_two`中。下图展示了`model_two`的准确率和损失图：
- en: '![](img/81a4aa6a-1a94-4f86-a533-997d3a1f4cb0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81a4aa6a-1a94-4f86-a533-997d3a1f4cb0.png)'
- en: Accuracy and loss for training and validation data
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的准确率与损失
- en: 'From the preceding diagram, we can make the following observations:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下观察结果：
- en: The accuracy values based on training and validation data remain relatively
    constant for the first few epochs.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于训练数据和验证数据的准确率在前几个周期内保持相对稳定。
- en: After about 20 epochs, the accuracy for the training data starts to increase
    and then continues to increase for the remaining epochs. The rate of increase,
    however, slows down after about 100 epochs.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大约20个epoch后，训练数据的准确率开始上升，并在剩余的epoch中持续上升。然而，在大约100个epoch后，增长速度放缓。
- en: On the other hand, the accuracy based on validation data drops for approximately
    50 epochs, then starts to increase, and then becomes more or less constant after
    about 125 epochs.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，基于验证数据的准确率在大约50个epoch后下降，然后开始上升，并在大约125个epoch后变得或多或少保持恒定。
- en: Similarly, loss values initially drop significantly for training data, but after
    about 50 epochs, the rate of decrease drops.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，训练数据的损失值最初大幅下降，但在大约50个epoch后，下降的速度放缓。
- en: The loss values for the validation data drop during the initial few epochs and
    then increase and stabilize after about 25 epochs.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据的损失值在最初的几个epoch中下降，然后在大约25个epoch后增加并稳定。
- en: 'Using class predictions based on the test data, we can also obtain a confusion
    matrix to assess the performance of this classification model. The following code
    is used to obtain a confusion matrix:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于测试数据的类别预测，我们还可以获得一个混淆矩阵来评估这个分类模型的性能。以下代码用于获取混淆矩阵：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'From the preceding confusion matrix, we can make the following observations:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以得出以下观察结果：
- en: By comparing correct classifications for `0`, `1`, and `2` classes with the
    previous model, we notice that improvement is only seen for class `1`, whereas
    the correct classifications for classes `0` and `2` have, in fact, reduced.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将`0`、`1`和`2`类别的正确分类与之前的模型进行比较，我们注意到只有类别`1`有所改进，而类别`0`和`2`的正确分类实际上有所减少。
- en: The overall accuracy for this model is 82.1%, which is below the accuracy value
    of 84.2% that we obtained earlier. So, our attempt to make our model slightly
    deeper did not improve accuracy, in this case.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型的整体准确率为82.1%，低于我们之前获得的84.2%的准确率。因此，在这种情况下，我们尝试使模型稍微更深并没有提高准确率。
- en: Experimenting with a higher number of units in the hidden layer
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试在隐藏层中使用更多的单元
- en: 'Now, let''s fine-tune the first model by changing the number of units in the
    first and only hidden layer using the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过使用以下代码调整第一个模型的隐藏层中单元的数量来微调第一个模型：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As shown in the preceding code and output, we have increased the number of units
    in the first and only hidden layer from `8` to `30`. The total number of parameters
    for this model is `753`. We compile and fit the model with the same setting that
    we used earlier. We store the accuracy and loss values while fitting the model
    in `model_three`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码和输出所示，我们将第一个且唯一的隐藏层中的单元数从`8`增加到`30`。该模型的总参数数量为`753`。我们使用之前相同的设置编译并拟合模型。在拟合模型时，我们将准确率和损失值存储在`model_three`中。
- en: 'The following screenshot provides the plot for accuracy and loss for training
    and validation data based on the new classification model, as shown in the following
    graph:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了基于新分类模型，训练和验证数据的准确率和损失的图表，如下图所示：
- en: '![](img/469ca0d2-5a59-477c-b70e-e5e2c7c59d52.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/469ca0d2-5a59-477c-b70e-e5e2c7c59d52.png)'
- en: Accuracy and loss for training and validation data
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证数据的准确率和损失
- en: 'We can make the following observations from the preceding plot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下观察结果：
- en: There is no evidence of overfitting.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有发现过拟合的证据。
- en: After about 75 epochs, we do not see any major improvement in the model performance.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大约75个epoch后，我们没有看到模型性能的任何重大改进。
- en: 'The prediction of classes using the test data and confusion matrix is obtained
    using the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用测试数据和混淆矩阵预测类别，可以通过以下代码获得：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From the preceding confusion matrix, we can make the following observations:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以得出以下观察结果：
- en: We see improvements in the classification of 1 suspect and 2 pathological categories
    compared to the first model.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与第一个模型相比，我们在1类嫌疑人和2类病理类别的分类上看到了一些改进。
- en: The correct classifications for the `0`, `1`, and `2` categories are `424`,
    `55`, and `39` respectively.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`、`1` 和 `2` 类别的正确分类分别为 `424`、`55` 和 `39`。'
- en: The overall accuracy using the test data comes to 85.9%, which is better than
    the first two models.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试数据的整体准确率为85.9%，比前两个模型更好。
- en: We can also obtain percentages that show how often this model correctly classifies
    each class by dividing the number of correct classifications in each column by
    the total of that column. We find that this classification model correctly classifies
    normal, suspect, and pathological cases with percentages of about 92.2%, 58.5%,
    and 79.6% respectively. So the model performance is at its highest when correctly
    classifying normal patients; however, the model accuracy drops to just 58.5% when
    correctly classifying patients in the suspect category. From the confusion matrix,
    we can see that the highest number of samples associated with misclassification
    is 35\. Thus, there are 35 patients who actually belong to the suspect category,
    but the classification model incorrectly puts these patients in the normal category.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将每列中正确分类的数量除以该列的总数，获得展示该模型每个类别正确分类次数的百分比。我们发现该分类模型对于正常、可疑和病理病例的正确分类率分别约为
    92.2%、58.5% 和 79.6%。因此，模型在正确分类正常患者时表现最好；然而，当正确分类可疑类别的患者时，模型准确率下降至仅 58.5%。从混淆矩阵中，我们可以看到误分类的样本数量最高的是
    35。也就是说，有 35 名实际上属于可疑类别的患者被分类模型错误地归为正常类别。
- en: Experimenting using a deeper network with more units in the hidden layer
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用更深层且具有更多单元的网络进行实验
- en: 'After building three different neural network models with 203, 239, and 753
    parameters respectively, we will now build a deeper neural network model containing
    a larger number of units in the hidden layers. The code used for this experiment
    is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建了三个参数分别为 203、239 和 753 的不同神经网络模型之后，我们现在将构建一个更深的神经网络模型，其中隐藏层包含更多的单元。用于该实验的代码如下：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can see from the preceding code and output that to try and improve the classification
    performance, this model has a total of 2,793 parameters. This model has three
    hidden layers with 40, 30, and 20 units in the three hidden layers. After each
    hidden layer, we have also added a dropout layer with dropout rates of 40%, 30%,
    and 20% to avoid overfitting—for example, with a dropout rate of 0.4 (or 40%)
    after the first hidden layer, 40% of the units in the first hidden layer are dropped
    to zero at random at the time of training. This helps to avoid any overfitting
    that may occur because of the higher number of units in the hidden layers. We
    compile the model and then run the model with same settings that we used earlier.
    We also store the loss and accuracy values after each epoch in `model_four`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码和输出中可以看出，为了尝试提高分类性能，该模型共有 2,793 个参数。该模型包含三个隐藏层，分别有 40、30 和 20 个单元。每个隐藏层后，我们还添加了一个
    dropout 层，dropout 率分别为 40%、30% 和 20%，以避免过拟合——例如，在第一个隐藏层后，dropout 率为 0.4（或 40%），这意味着在训练时，第一隐藏层中的
    40% 的单元会被随机置零。这有助于避免由于隐藏层中单元数过多而可能导致的过拟合问题。我们编译该模型并使用之前相同的设置运行模型。我们还在每次训练周期后将损失值和准确率存储在
    `model_four` 中。
- en: 'A plot for accuracy and loss values for training and validation data is shown
    in the following graph:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了训练数据和验证数据的准确率和损失值：
- en: '![](img/ef6be428-c579-42c5-8a68-0580a8f217aa.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef6be428-c579-42c5-8a68-0580a8f217aa.png)'
- en: Accuracy and loss for training and validation data
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证数据的准确率与损失
- en: 'From the preceding plot, we can make the following observations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下结论：
- en: Training loss and accuracy values stay approximately constant after about 150
    epochs.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的损失值和准确率在大约 150 次训练周期后保持大致不变。
- en: Accuracy values for validation data are mainly flat after about 75 epochs.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据的准确率在大约 75 次训练周期后基本保持平稳。
- en: However, for loss, we see some divergence between training and validation data
    after about 75 epochs, with loss from validation data increasing gradually. This
    suggests the presence of overfitting after about 75 epochs.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，对于损失值，我们看到在大约 75 次训练周期后，训练数据和验证数据之间出现了一些分歧，验证数据的损失逐渐增加。这表明大约在 75 次训练周期后出现了过拟合现象。
- en: 'Let''s now make predictions using test data and review the resulting confusion
    matrix to assess model performance, as shown in the following code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用测试数据进行预测，并查看结果的混淆矩阵，以评估模型性能，代码如下所示：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From the preceding confusion matrix, the following observations can be made:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以得出以下结论：
- en: The correct classifications for the `0`, `1`, and `2` categories are `431`,
    `53`, and `40` respectively.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`0`、`1` 和 `2` 类别，正确分类的数量分别为 `431`、`53` 和 `40`。
- en: The overall accuracy comes to 86.9%, which is better than the first three models.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体准确率为86.9%，优于前三个模型。
- en: We can also find that this classification model correctly classifies normal,
    suspect, and pathological cases with percentages of about 93.7%, 56.4%, and 81.6%
    respectively.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以发现，这个分类模型能够正确分类正常、可疑和病理病例，分别为93.7%、56.4%和81.6%。
- en: Experimenting by addressing the class imbalance problem
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过解决类别不平衡问题进行实验
- en: In this dataset, the number of patients in the normal, suspect, and pathological
    categories is not the same. In the original dataset, the number of normal, suspect,
    and pathological patients are 1,655, 295, and 176, respectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在此数据集中，正常、可疑和病理类别的患者数量并不相同。在原始数据集中，正常、可疑和病理患者的数量分别为1,655、295和176。
- en: 'We will make use of the following code to develop a bar plot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来绘制条形图：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After running the preceding code, we obtain the following bar plot:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们得到以下的条形图：
- en: '![](img/290b5c4d-c759-4931-bd19-0893b0770a47.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/290b5c4d-c759-4931-bd19-0893b0770a47.png)'
- en: Proportion of samples in each of the three classes
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别中样本的比例
- en: In the preceding bar plot, the percentages of normal, suspect, and pathological
    patients are approximately 78%, 14%, and 8% respectively. When we compare these
    classes, we observe that the number of normal patients is about 5.6 times (1,655/295)
    greater than the number of suspect patients and about 9.4 times greater than the
    number of pathological patients. The dataset exhibiting a pattern where classes
    are not balanced but contain significantly different numbers of cases per class
    is described as having a class imbalance problem. The class that has a significantly
    higher number of cases may benefit from this at the time of training the model,
    but at the cost of the other classes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的条形图中，正常、可疑和病理患者的比例分别约为78%、14%和8%。当我们比较这些类别时，可以观察到正常患者的数量大约是可疑患者的5.6倍（1,655/295），也是病理患者的9.4倍。该数据集表现出类别不平衡的模式，其中每个类别的病例数差异显著，这种情况被称为类别不平衡问题。具有显著更多病例的类别在训练模型时可能会受益，但也会以牺牲其他类别为代价。
- en: As a result, a classification model may contain a bias toward the class that
    has a significantly higher number of cases, and provide results with higher classification
    accuracy for this class compared to the other classes. When data is influenced
    by such a class imbalance, it is important to address the issue to avoid bias
    in the final classification model. In such situations, we can make use of class
    weights to address the class imbalance issue in a dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个分类模型可能会对拥有显著更多样本的类别产生偏倚，并为该类别提供比其他类别更高的分类准确度。当数据受到此类类别不平衡的影响时，必须解决该问题，以避免最终分类模型的偏倚。在这种情况下，我们可以利用类别权重来处理数据集中的类别不平衡问题。
- en: Very often, datasets that are used for developing classification models have
    an unequal number of samples for each class. Such class imbalance issues can easily
    be handled using the `class_weight` function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 很多用于开发分类模型的数据集在每个类别中的样本数量是不均等的。这样的类别不平衡问题可以通过使用`class_weight`函数轻松处理。
- en: 'The code that includes `class_weight` to incorporate class imbalance information
    is shown in the following code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 包含`class_weight`以引入类别不平衡信息的代码如下所示：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see in the preceding code, we have specified a weight of `1` for
    the `normal` class, a weight of `5.6` for the suspect class, and a weight of `9.4`
    for the pathological class. Assigning these weights creates a level playing field
    for all three categories. We have kept all other settings the same as they were
    in the previous model. After training the network, the loss and accuracy values
    for each epoch are stored in `model_five`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们为`normal`类别指定了权重`1`，为可疑类别指定了权重`5.6`，为病理类别指定了权重`9.4`。分配这些权重为所有三个类别创造了一个公平的竞争环境。我们保持了其他设置与之前的模型一致。训练完网络后，每个周期的损失和准确率值存储在`model_five`中。
- en: 'The loss and accuracy plot for this experiment is shown in the following screenshot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本次实验的损失和准确率图如下所示：
- en: '![](img/c9b36df6-2112-4e24-9488-ac26dcab752e.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9b36df6-2112-4e24-9488-ac26dcab752e.png)'
- en: From the accuracy and loss plot based on training and validation data, we do
    not see any obvious pattern suggesting overfitting. After about 100 epochs, we
    do not see any major improvement in model performance in terms of loss and accuracy
    values.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据训练和验证数据的准确度和损失图表，我们没有看到任何明显表明过拟合的模式。大约在100个epochs之后，我们没有看到模型性能在损失和准确度值方面有任何重大改善。
- en: 'The code for the predictions from the model and the resulting confusion matrix
    is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 来自模型预测和生成的混淆矩阵的代码如下：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From the preceding confusion matrix, we can make the following observations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述混淆矩阵，我们可以得出以下观察结果：
- en: The correct classifications for the `0`, `1`, and `2` categories are `358`,
    `74`, and `41` respectively.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`0`、`1`和`2`类别的正确分类分别为`358`、`74`和`41`。
- en: The overall accuracy is now reduced to 78.4%, which is mainly due to the drop
    in accuracy for the normal class, since we increased the weights for the other
    two classes.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体准确度现在降至78.4%，这主要是由于正常类的准确率下降，因为我们增加了其他两类的权重。
- en: We can also find that this classification model correctly classifies normal,
    suspect, and pathological cases with percentages of about 77.8%, 78.7%, and 83.7%
    respectively.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以发现，这个分类模型正确分类了正常、嫌疑和病理情况，准确率分别约为77.8%、78.7%和83.7%。
- en: Clearly, the biggest gains are for the suspect class, which is now correctly
    classified at the rate of 78.7% versus the earlier rate of only 56.4%.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显然，最大的收益是对于嫌疑类别，现在的正确分类率为78.7%，而之前只有56.4%。
- en: In the pathological class, we do not see any major gain or loss in accuracy
    value.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在病理类中，我们并未看到准确度值有任何重大的增益或损失。
- en: These results clearly indicate the influence of using weights to address class
    imbalance problems, as now the classification performance across the three classes
    is more consistent.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些结果清楚地表明了使用权重来解决类别不平衡问题的影响，因为现在三个类别的分类性能更加一致。
- en: Saving and reloading a model
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的保存和加载
- en: 'We know that each time we run a model in Keras, the model starts with different
    starting points due to random initial weights*.* Once we arrive at a model with
    an acceptable level of performance and would like to reuse the same model in the
    future, we can save the model using the `save_model_hdf5` function. We can then
    load this same model using the `load_model_hdf5` function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在Keras中每次运行模型时，由于随机的初始权重，模型都会从不同的起点开始*。一旦我们得到了性能水平可接受的模型，并希望将来重复使用相同的模型，我们可以使用`save_model_hdf5`函数保存模型。然后，我们可以使用`load_model_hdf5`函数加载相同的模型：
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding code will allow us to save the model architecture and the model
    weights, and, if needed, will allow us to resume the training of the model from
    the previous training session.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将允许我们保存模型的架构和模型的权重，并且如果需要的话，将允许我们从先前的训练会话中恢复模型的训练。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how to develop a neural network model that helps to
    solve a classification type of problem. We started with a simple classification
    model and explored how to change the number of hidden layers and the number of
    units in the hidden layers. The idea behind exploring and fine-tuning a classification
    model was to illustrate how to explore and improve the performance of the classification
    model. We also saw how to dig deeper to understand the performance of a classification
    model with the help of a confusion matrix. We purposefully looked at a relatively
    smaller neural network model at the beginning of this chapter and finished with
    an example of a relatively deeper neural network model. Deeper networks involving
    several hidden layers can also lead to overfitting problems, where a classification
    model may have excellent performance with training data but doesn't do well with
    testing data. To avoid such situations, we can make use of dropout layers after
    each dense layer, as was illustrated previously. We also illustrated the use of
    class weights for situations where the class imbalance could cause a classification
    model to be more biased toward a specific class. Finally, we also saw how we can
    save the model details for future use when we don't need to rerun the model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何开发一个神经网络模型来帮助解决分类问题。我们从一个简单的分类模型开始，并探讨了如何更改隐藏层的数量和隐藏层中单元的数量。探索和调整分类模型的背后理念是为了说明如何探索和提升分类模型的性能。我们还展示了如何借助混淆矩阵深入理解分类模型的表现。我们有意在本章开始时使用了一个相对较小的神经网络模型，并以一个较深的神经网络模型作为例子来结束。本章介绍的更深层的网络包含多个隐藏层，这也可能导致过拟合问题，其中一个分类模型在训练数据上可能表现优异，但在测试数据上表现不佳。为避免这种情况，我们可以在每个全连接层后使用丢弃层（dropout
    layer），如前所示。我们还展示了在类别不平衡的情况下，如何使用类别权重，以避免分类模型偏向某个特定类别。最后，我们还介绍了如何保存模型的详细信息，以便未来使用，避免重新训练模型。
- en: For the models that we used in this chapter, there were certain parameters that
    we kept constant during the various experiments—for example, when compiling a
    model, we always used `adam` as an optimizer. One of the reasons for the popularity
    of using `adam` is that it doesn't require much tuning, and provides good results
    in less time; however, the reader is encouraged to try out other optimizers, such
    as `adagrad`, `adadelta`, and `rmsprop`, and observe the impact on the classification
    performance of the model. Another setting that we kept constant in this chapter
    is the batch size of 32 at the time of training the network. The reader is also
    encouraged to experiment with higher (such as 64) and lower (such as 16) batch
    sizes and observe what impact this has on the classification performance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章使用的模型中，我们在各种实验中保持了某些参数恒定——例如，在编译模型时，我们始终使用`adam`作为优化器。`adam`之所以广受欢迎，部分原因在于它不需要太多的调优，并且能在较短的时间内提供良好的结果；然而，建议读者尝试其他优化器，如`adagrad`、`adadelta`和`rmsprop`，并观察这些优化器对模型分类性能的影响。另一个我们在本章中保持恒定的设置是训练网络时的批量大小（batch
    size）为32。读者也可以尝试更大的批量（如64）或更小的批量（如16），并观察这些变化对分类性能的影响。
- en: As we go on to future chapters, we will gradually develop more and more complex
    and deeper neural network models. Having addressed a classification model where
    the response variables are categorical, in the next chapter, we will go over the
    steps for developing and improving the prediction model for the regression type
    of problems, where the target variable is numeric.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入后续章节，我们将逐渐开发更复杂、更深入的神经网络模型。在本章中，我们已经介绍了一个分类模型，其中响应变量是类别型的。在下一章中，我们将讲解如何开发和改进回归类型问题的预测模型，其中目标变量是数值型的。
