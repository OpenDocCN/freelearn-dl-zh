- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*'
- en: Convolutional Neural Networks for Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉中的卷积神经网络
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Explain how convolutional neural networks work
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释卷积神经网络的工作原理
- en: Construct a convolutional neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建卷积神经网络
- en: Improve the constructed model by using data augmentation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用数据增强来改进已构建的模型
- en: Use state-of-the-art models by implementing transfer learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实现迁移学习使用最先进的模型
- en: In this chapter, we will learn how to use probability distributions as a form
    of unsupervised learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用概率分布作为一种无监督学习的形式。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, we learned about how a neural network can be trained
    to predict values and how a **recurrent neural network (RNN)**, based on its architecture,
    can prove to be useful in many scenarios. In this chapter, we will discuss and
    observe how **convolutional neural networks (CNNs)** work in a similar way to
    dense neural networks (also called fully-connected neural networks, as mentioned
    in *Chapter 2*, *Introduction to Computer Vision*).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了神经网络如何通过训练预测值，并了解了基于其架构的 **递归神经网络（RNN）** 在许多场景中的应用价值。本章中，我们将讨论并观察
    **卷积神经网络（CNNs）** 如何与密集神经网络（也称为全连接神经网络，如*第2章*《计算机视觉导论》中所提到的）以类似的方式工作。
- en: CNNs have neurons with weights and biases that are updated during training time.
    CNNs are mainly used for image processing. Images are interpreted as pixels and
    the network outputs the class it thinks the image belongs to, along with loss
    functions that state the errors with every classification and every output.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 拥有具有权重和偏置的神经元，这些权重和偏置会在训练过程中进行更新。CNNs 主要用于图像处理。图像被解释为像素，网络输出它认为图像所属的类别，以及损失函数，后者描述每次分类和每次输出的错误。
- en: These types of networks make an assumption that the input is an image or works
    like an image, allowing them to work more efficiently (CNNs are faster and better
    than deep neural networks). In the following sections, you will learn more about
    CNNs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这类网络假设输入是图像或类似图像的形式，这使得它们可以更高效地工作（CNNs 比深度神经网络更快、更好）。在接下来的章节中，你将了解更多关于 CNNs
    的内容。
- en: Fundamentals of CNNs
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNs 基础
- en: In this topic, we will see how CNNs work and explain the process of convolving
    an image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本主题中，我们将看到 CNNs 如何工作，并解释卷积图像的过程。
- en: We know that images are made up of pixels, and if the image is in RGB, for example,
    it will have three channels where each letter/color (Red-Green-Blue) has its own
    channel with a set of pixels of the same size. Fully-connected neural networks
    do not represent this depth in an image in every layer. Instead, they have a single
    dimension to represent this depth, which is not enough. Furthermore, they connect
    every single neuron of one layer to every single neuron of the next layer, and
    so on. This in turn results in lower performance, meaning you would have to train
    a network for longer and would still not get good results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道图像是由像素组成的，如果图像是 RGB 格式，例如，它将有三个通道，每个字母/颜色（红-绿-蓝）都有自己的一组像素，每个通道的像素大小相同。全连接神经网络并不会在每一层表示图像的这种深度，而是通过一个单一的维度来表示这种深度，这显然是不够的。此外，它们将每一层的每个神经元与下一层的每个神经元连接，依此类推。这反过来导致性能较低，意味着你需要训练网络更长的时间，但仍然无法获得良好的结果。
- en: '**CNNs** are a category of neural networks that has ended up being very effective
    for tasks such as classification and image recognition. Although, they also work
    very well for sound and text data. CNNs consist of an input, hidden layers, and
    an output layer, just like normal neural networks. The input and hidden layers
    are commonly formed by **convolutional layers**, **pooling layers** (layers that
    reduce the spatial size of the input), and **fully-connected layers** (fully-connected
    layers are explained in *Chapter 2*, *Introduction to Computer Vision*). Convolutional
    layers and pooling layers will be explained later on in this chapter.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**CNNs** 是一类神经网络，在分类和图像识别等任务中非常有效。虽然它们也能很好地处理声音和文本数据。CNNs 由输入层、隐藏层和输出层组成，就像普通的神经网络一样。输入层和隐藏层通常由
    **卷积层**、**池化层**（用于减少输入的空间尺寸）和 **全连接层**（全连接层将在*第2章*《计算机视觉导论》中解释）构成。卷积层和池化层将在本章稍后进行详细讲解。'
- en: 'CNNs give depth to every layer, starting from the original depth of the image
    to deeper hidden layers as well. The following figure shows how a CNN works and
    what one looks like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）为每一层赋予了深度，从图像的原始深度到更深的隐藏层。下图展示了CNN的工作原理以及其结构：
- en: '![Figure 5.1: Representation of a CNN](img/C13550_05_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1：CNN的表示](img/C13550_05_01.jpg)'
- en: 'Figure 5.1: Representation of a CNN'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.1：CNN的表示
- en: In the preceding figure, the CNN takes an input image of 224 x 224 x 3, which
    by convolutional processes is transformed into the next layer, which compresses
    the size but has more depth to it (we will explain how these processes work later
    on). These operations continue over and over until the graphical representation
    is flattened and these dense layers are used to end up with the corresponding
    classes of the dataset as output.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，CNN接收一个224 x 224 x 3的输入图像，通过卷积处理后转化为下一个层级，这一过程压缩了尺寸，但深度增加（我们稍后会解释这些过程是如何工作的）。这些操作会不断进行，直到图形表示被拉平，并通过这些密集层来输出数据集对应的类别。
- en: '**Convolutional Layers:** Convolutional layers consist of a set of **filters**
    of fixed size (typically a small size), which are matrices with certain values/weights,
    that are applied all over the input (an image, for example), by computing the
    scalar product between the filters and the input, which is called convolution.
    Each of these filters produces a two-dimensional activation map, which is stacked
    along the depth of the input. These activation maps look for features in the input
    and will determine how well the network learns. The more filters you have, the
    deeper the layer is, thus, the more your network learns, but the slower it gets
    at training time. For instance, in a particular image say, you would like to have
    3 filters in the first layer, 96 filters in the next layer, 256 in the next, and
    so on. Note that, at the beginning of the network, there are usually fewer filters
    than at the end or in the middle of the network. This is because the middle and
    the end of the network have more potential features to extract, thus we need more
    filters, of a smaller size, toward the end of the network. This is because the
    deeper we advance into the network, the more we look at little details within
    an image, therefore we want to extract more features from those details to get
    a good understanding of the image.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积层：**卷积层由一组固定大小（通常较小）的**滤波器**组成，这些滤波器是具有特定值/权重的矩阵，它们会遍历输入（例如一张图像），通过计算滤波器与输入之间的标量积，这个过程称为卷积。每个滤波器会生成一个二维激活图，这些激活图会沿着输入的深度堆叠。激活图的作用是寻找输入中的特征，并决定网络学习的效果。滤波器的数量越多，层的深度越大，因此网络学习得越多，但训练时的速度会变慢。例如，在某个图像中，你可能希望在第一层使用3个滤波器，在下一层使用96个滤波器，在再下一层使用256个滤波器，依此类推。请注意，在网络的开始部分，滤波器通常比在中间或末端部分少。这是因为网络的中间和末端具有更多潜在的特征可以提取，因此我们需要更多、更小的滤波器来处理网络的后段。这是因为随着我们深入网络，我们更多地关注图像中的小细节，因此希望从这些细节中提取更多特征，以便更好地理解图像。'
- en: The sizes of the filters of convolutional layers often go from 2x2 to 7x7, for
    example, depending on whether you are at the beginning of the network (higher
    sizes) or toward the end (smaller sizes).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层滤波器的尺寸通常从2x2到7x7不等，具体取决于你处于网络的哪个阶段（开始部分使用较大尺寸，末端使用较小尺寸）。
- en: In Figure 5.1, we can see convolution being applied using filters (in light
    blue) and the output would be a single value that goes to the next step/layer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.1中，我们可以看到使用滤波器（浅蓝色）进行卷积操作，输出将是一个单一值，并传递到下一步/层。
- en: After performing convolution, and before another convolution is applied, a max
    pooling (**pooling layer**) layer is normally applied in order to reduce the size
    of the input so that the network can get a deeper understanding of the image.
    Nevertheless, lately, there is a tendency to avoid max pooling and instead encourage
    strides, which are naturally applied when performing convolution, so we are going
    to explain image reduction by naturally applying convolution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行卷积操作后，在应用另一次卷积之前，通常会使用最大池化（**池化层**）来减少输入的大小，以便网络能够对图像进行更深层次的理解。然而，近年来，逐渐避免使用最大池化，转而鼓励使用步幅，这是在进行卷积时自然应用的，因此我们将通过自然地应用卷积来解释图像尺寸的缩小。
- en: '**Strides:** This is the length, defined in pixels, for the steps of the filter
    being applied over the entire image. If a stride of one is selected, the filter
    will be applied, but one pixel at a time. Similarly, if a stride of two is selected,
    then the filter will be applied two pixels at a time, the output size is smaller
    than the input, and so on.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**步长（Strides）**：这是定义为像素的长度，用于描述滤波器在整个图像上应用的步伐。如果选择步长为 1，滤波器将每次应用一个像素。同样，如果选择步长为
    2，则滤波器将每次应用两个像素，输出大小会比输入小，依此类推。'
- en: 'Let''s look at an example. Firstly, Figure 5.2 will be used as the filter to
    convolve the image, which is a 2x2 matrix:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。首先，将使用图 5.2 作为滤波器对图像进行卷积，它是一个 2x2 的矩阵：
- en: '![Figure 5.2: Convolution filter](img/C13550_05_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2: 卷积滤波器](img/C13550_05_02.jpg)'
- en: 'Figure 5.2: Convolution filter'
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.2: 卷积滤波器'
- en: 'And the following could be the image (matrix) we are convolving:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下可能是我们正在卷积的图像（矩阵）：
- en: '![Figure 5.3: Image to convolve](img/C13550_05_03.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3: 要卷积的图像](img/C13550_05_03.jpg)'
- en: 'Figure 5.3: Image to convolve'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.3: 要卷积的图像'
- en: Of course, this is not a real image, but for the sake of simplicity, we are
    taking a matrix of 4x4 with random values to demonstrate how convolution works.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这不是一个真实的图像，但为了简化，我们使用一个 4x4 的随机值矩阵来演示卷积是如何工作的。
- en: 'Now, if we want to apply convolution with stride equal to 1, this would be
    the process, graphically:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们要应用步长为 1 的卷积，图形化的过程如下：
- en: '![Figure 5.4: Convolution process Stride=1](img/C13550_05_04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4: 卷积过程 Stride=1](img/C13550_05_04.jpg)'
- en: 'Figure 5.4: Convolution process Stride=1'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.4: 卷积过程 Stride=1'
- en: The preceding Figure shows a 2x2 filter being applied to the input image, pixel
    by pixel. The process goes from left to right and from top to bottom.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图显示了一个 2x2 的滤波器被逐像素地应用到输入图像上。过程是从左到右、从上到下进行的。
- en: The filter multiplies every value of every position in its matrix to every value
    of every position of the zone (matrix) where it's being applied. For instance,
    in the first part of the process, the filter is being applied to the first 2x2
    part of the image [1 2; 5 6] and the filter we have is [2 1; -1 2], so it would
    be 1*2 + 2*1 + 5*(-1) + 6*2 = 11.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器将矩阵中每个位置的每个值与它所应用区域（矩阵）中每个位置的每个值相乘。例如，在过程的第一部分，滤波器应用于图像的前 2x2 部分 [1 2; 5
    6]，而我们使用的滤波器是 [2 1; -1 2]，那么计算方式为 1*2 + 2*1 + 5*(-1) + 6*2 = 11。
- en: 'The resulting image, after applying the filter matrix, would be as shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 应用滤波器矩阵后，得到的图像如下所示：
- en: '![Figure 5.5: Convolution result Stride=1](img/C13550_05_05.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5: 卷积结果 Stride=1](img/C13550_05_05.jpg)'
- en: 'Figure 5.5: Convolution result Stride=1'
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.5: 卷积结果 Stride=1'
- en: As you can see, the resulting image is now one dimension smaller. This is because
    there is another parameter, called **padding**, which is set to "valid" by default,
    which means that the convolution will be applied normally; that is, applying the
    convolution makes the image one pixel thinner by nature. If it is set to "same,"
    the image will be surrounded by one line of pixels with a value equal to zero,
    thus the output matrix will have the same size as the input matrix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，处理后的图像现在变得比原来少了一维。这是因为存在一个名为**padding**的参数，默认设置为“valid”，这意味着卷积会按常规应用；也就是说，应用卷积后，图像自然会变得薄一像素。如果设置为“same”，图像周围将被一行像素包围，像素值为零，因此输出矩阵的大小将与输入矩阵相同。
- en: Now, we are going to apply a stride of 2, to reduce the size by 2 (just like
    a max pooling layer of 2x2 would do). Remember that we are using a padding equal
    to "valid."
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将应用步长为 2 的卷积，以将图像大小减少 2（就像一个 2x2 的最大池化层所做的那样）。请记住，我们使用的是“valid”类型的 padding。
- en: 'The process would have fewer steps, just like in the following figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程将会减少步骤，就像以下图所示：
- en: '![Figure 5.6: Convolution process Stride=2](img/C13550_05_06.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6: 卷积过程 Stride=2](img/C13550_05_06.jpg)'
- en: 'Figure 5.6: Convolution process Stride=2'
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.6: 卷积过程 Stride=2'
- en: 'And the output image/matrix would look like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 而输出的图像/矩阵看起来会是这样的：
- en: '![Figure 5.7:  Convolution result Stride=2](img/C13550_05_07.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7: 卷积结果 Stride=2](img/C13550_05_07.jpg)'
- en: 'Figure 5.7: Convolution result Stride=2'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.7: 卷积结果 Stride=2'
- en: The resulting image would be an image of 2x2 pixels. This is due to the natural
    process of convolution with stride equal to 2.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像将是一个 2x2 像素的图像。这是由于卷积过程的自然结果，步长（stride）为 2。
- en: These filters, which are applied on every convolutional layer, have weights
    that the neural network adjusts so that the outputs of those filters help the
    neural network learn valuable features. These weights, as explained, are updated
    by the process of backpropagation. As a reminder, backpropagation is the process
    where the network's loss (or the amount of errors) of the predictions made versus
    the expected results in a training step of the network is calculated, updating
    all the weights of the neurons of the network that have contributed to that error
    so that they do not make the same mistake again.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在每个卷积层上应用的过滤器，具有权重，神经网络会调整这些权重，使得这些过滤器的输出有助于神经网络学习有价值的特征。如前所述，这些权重通过反向传播过程更新。提醒一下，反向传播是网络计算训练步骤中预测结果与期望结果之间的误差（或损失），然后更新所有贡献于该误差的神经元权重，以避免再次犯同样的错误。
- en: Building Your First CNN
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建你的第一个CNN
- en: Note
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For this chapter, we are going to still use Keras on top of TensorFlow as the
    backend, as mentioned in *Chapter 2, Introduction to Computer Vision* of this
    book. Also, we will still use Google Colab to train our network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们仍然将在TensorFlow上使用Keras作为后端，正如在本书的*第二章《计算机视觉导论》*中所提到的。此外，我们仍然会使用Google Colab来训练我们的网络。
- en: Keras is a very good library for implementing convolutional layers, as it abstracts
    the user so that layers do not have to be implemented by hand.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个非常好的实现卷积层的库，因为它对用户进行了抽象，用户不需要手动实现各层。
- en: 'In *Chapter 2*, *Introduction to Computer Vision,* we imported the Dense, Dropout,
    and BatchNormalization layers by using the `keras.layers` package, and to declare
    convolutional layers of two dimensions, we are going to use the same package:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第二章《计算机视觉导论》*中，我们通过使用`keras.layers`包导入了Dense、Dropout和BatchNormalization层，而为了声明二维卷积层，我们将使用相同的包：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `Conv2D` module is just like the other modules: you have to declare a sequential
    model, which was explained in *Chapter 2, Introduction to Computer Vision* of
    this book, and we also add `Conv2D`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv2D`模块与其他模块类似：你必须声明一个顺序模型，这在本书的*第二章《计算机视觉导论》*中已做解释，我们还需要添加`Conv2D`：'
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the first layer, the input shape has to be specified, but after that, it
    is no longer needed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一层，必须指定输入形状，但之后不再需要。
- en: The first parameter that must be specified is the **number of filters** that
    the network is going to learn in that layer. As mentioned before, in the earlier
    layers, we will filter few layers which will be learned, rather than the layers
    deeper in the network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 必须指定的第一个参数是**过滤器的数量**，即网络在该层要学习的过滤器数量。如前所述，在较早的层中，我们会学习少量的过滤器，而不是网络中的深层过滤器。
- en: The second parameter that must be specified is the **kernel size**, which is
    the size of the filter applied to the input data. Usually, a kernel of size 3x3
    is set, or even 2x2, but sometimes when the image is large, a bigger kernel size
    is set.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 必须指定的第二个参数是**卷积核大小（kernel size）**，即应用于输入数据的滤波器大小。通常设置为3x3的卷积核，或者甚至是2x2的卷积核，但有时当图像较大时，会使用更大的卷积核。
- en: The third parameter is **padding**, which is set to "valid" by default, but
    it needs to be set to "same," as we want to preserve the size of the input in
    order to understand the behavior of down-sampling the input.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个参数是**padding**，默认设置为“valid”，但我们需要将其设置为“same”，因为我们希望保持输入的尺寸，以便理解输入的下采样行为。
- en: The fourth parameter is **strides**, which, by default, is set to (1, 1). We
    will be setting it to (2, 2), since there are two numbers here and it has to be
    set for both the x and y axes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个参数是**strides**，默认设置为(1, 1)。我们将其设置为(2, 2)，因为这里有两个数字，并且需要为x轴和y轴都设置该参数。
- en: 'After the first layer, we will apply the same methodology as was mentioned
    in *Chapter 2*, *Introduction to Computer Vision*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层之后，我们将采用与*第二章*《计算机视觉导论》中提到的相同方法：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As a reminder, the **BatchNormalization** layer is used to normalize the inputs
    of each layer, which helps the network converge faster and may give better results
    overall.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，**批量归一化（BatchNormalization）**层用于规范化每层的输入，帮助网络更快收敛，通常也能提高整体效果。
- en: The `activation` function is a function that takes the input and calculates
    a weighted sum of it, adding a bias and deciding whether it should be activated
    or not (outputting 1 and 0, respectively).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`激活`函数是一个接受输入并计算其加权和的函数，添加偏置后决定是否激活（分别输出1和0）。'
- en: The **Dropout** layer helps the network avoid overfitting, which is when the
    accuracy of the training set is much higher than the accuracy of the validation
    set, by switching off a percentage of neurons.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 层通过关闭一定比例的神经元，帮助网络避免过拟合，过拟合是指训练集的准确率远高于验证集的准确率。'
- en: We could apply more sets of layers like this, varying the parameters, depending
    on the size of the problem to solve.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样应用更多的层集，根据问题的大小调整参数。
- en: The last layers remain the same as those of dense neural networks, depending
    on the problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层和密集神经网络的层保持一致，具体取决于问题。
- en: 'Exercise 17: Building a CNN'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习17：构建一个 CNN
- en: Note
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This exercise uses the same packages and libraries as *Chapter 2, Introduction
    to Computer Vision*. These libraries are Keras, Numpy, OpenCV, and Matplotlib.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习使用了与*第2章，计算机视觉简介*相同的包和库。这些库包括 Keras、Numpy、OpenCV 和 Matplotlib。
- en: In this exercise, we are going to take the same problem as *Chapter 2*, *Activity
    2*, *Classify 10 Types of Clothes of the Fashion-MNIST Database*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用与*第2章*、*活动2*、*分类 Fashion-MNIST 数据库中的10种衣物类型*相同的问题。
- en: Remember that, in that activity, the neural network that was built was not capable
    of generalizing well enough to classify the unseen data that we passed to it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在那个活动中，构建的神经网络并不能很好地泛化，以至于无法对我们传递给它的未见数据进行分类。
- en: 'As a reminder, this problem is a classification problem, where the model has
    to classify 10 types of clothes correctly:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这个问题是一个分类问题，模型需要正确地分类10种类型的衣物：
- en: Open up your Google Colab interface.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Google Colab 界面。
- en: Create a folder for the book and download the `Datasets` folder from GitHub
    and upload it in the folder in your drive.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为书籍创建一个文件夹，并从 GitHub 下载`Datasets`文件夹并上传到你驱动器中的文件夹里。
- en: 'Import drive and mount it as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 挂载驱动器并按照以下方式进行：
- en: '[PRE3]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Every time you use a new collaborator, mount the drive to the desired folder.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每次使用新合作者时，将驱动器挂载到目标文件夹。
- en: 'Once you have mounted your drive for the first time, you will have to enter
    the authorization code mentioned by clicking on the URL given by Google and press
    the **Enter** key on your keyboard:![Figure 5.8: Mounting on Google Collab](img/C13550_05_08.jpg)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你首次挂载了你的驱动器，你将需要输入 Google 给出的授权代码，点击给出的 URL 并按下键盘上的**Enter**键：![图 5.8：在 Google
    Collab 上挂载](img/C13550_05_08.jpg)
- en: 'Figure 5.8: Mounting on Google Collab'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：在 Google Collab 上挂载
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经挂载了驱动器，需要设置目录的路径：
- en: '[PRE4]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The path mentioned in step 5 may change as per your folder setup on Google Drive.
    The path will always begin with `cd /content/drive/My Drive/`.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5步中提到的路径可能会根据你在 Google Drive 上的文件夹设置而有所变化。路径将始终以`cd /content/drive/My Drive/`开头。
- en: 'First, let''s import the data from Keras and initialize the random seed to
    42 for reproducibility:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们从 Keras 导入数据并将随机种子初始化为42，以确保结果可复现：
- en: '[PRE5]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We import NumPy in order to pre-process the data and Keras utils to one-hot
    encode the labels:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入了 NumPy 以便对数据进行预处理，并导入 Keras 工具来进行标签的独热编码：
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We declare the `Sequential` function to make a sequential model in Keras, the
    callbacks, and, of course, the layers:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们声明`Sequential`函数来创建 Keras 的顺序模型、回调函数，当然还有层：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We have imported a callback called **EarlyStopping**. What this callback does
    is stop the training after a number of epochs, where the metric that you choose
    (for example, validation accuracy) has dropped. You can set that number with the
    number of epochs that you want.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经导入了一个回调函数叫做**EarlyStopping**。这个回调的作用是，当你选择的度量（例如，验证准确率）下降时，在指定的轮次后停止训练。你可以通过设置想要的轮次数来确定这个数字。
- en: 'Now, we are going to build our first CNN. First, let''s declare the model as
    `Sequential` and add the first `Conv2D`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将构建我们的第一个 CNN。首先，让我们将模型声明为`Sequential`并添加第一个`Conv2D`：
- en: '[PRE8]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We add 32 filters as is the first layer, and a filter size of 3x3\. Padding
    is set to "`same`" and the strides are set to 2 to naturally reduce the dimensionality
    of the `Conv2D` module.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们添加了32个滤波器作为第一层，滤波器的大小为3x3。填充设置为“`same`”，步长设置为2，以自然地减少`Conv2D`模块的维度。
- en: 'We follow this layer by adding `Activation` and `BatchNormalization` layers:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过添加`Activation`和`BatchNormalization`层来继续这个层：
- en: '[PRE9]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are going to add another three layers with the same characteristics as before,
    applying dropout and jumping to another block:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加另外三层，保持之前相同的特性，应用 dropout，并跳到另一个模块：
- en: '[PRE10]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we apply dropout of 20%, which turns off 20% of the neurons in the network:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们应用20%的dropout，这会关闭网络中20%的神经元：
- en: '[PRE11]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We are going to do the same procedure one more time but with 64 filters:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将再次进行相同的操作，但这次使用64个过滤器：
- en: '[PRE12]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For the end of the network, we apply the `Flatten` layer to make the output
    of the last layer one-dimensional. We apply a `Dense` layer with 512 neurons.
    Where the logistics of the network occur, we apply the `Activation` layer and
    the `BatchNormalization` layer, before applying a `Dropout` of 50%:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于网络的最后部分，我们应用`Flatten`层将最后一层的输出转换为一维。我们应用一个包含512个神经元的`Dense`层。在网络的物流部分，我们应用`Activation`层和`BatchNormalization`层，然后应用50%的`Dropout`：
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And, finally, we declare the last layer as a `dense` layer with 10 neurons,
    which is the number of classes of the dataset, and a `Softmax` activation function,
    which establishes which class the image is more likely to be, and we return the
    model:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们声明最后一层为一个包含10个神经元的`dense`层，这是数据集的类别数，并应用`Softmax`激活函数，确定图像最可能属于哪个类别，最后返回模型：
- en: '[PRE14]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s declare the model along with the callbacks and compile it:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们声明模型并加上回调函数，然后进行编译：
- en: '[PRE15]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For compiling, we are using the same optimizer. For declaring the checkpoint,
    we are using the same parameters. For declaring `EarlyStopping`, we are using
    the validation loss as the main metric and we set a patience of five epochs.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于编译，我们使用相同的优化器。对于声明检查点，我们使用相同的参数。对于声明`EarlyStopping`，我们将验证损失作为主要度量，并设置耐心值为五个epoch。
- en: Let the training begin!
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让训练开始吧！
- en: '[PRE16]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We set the batch size to 128 because there are enough images and because this
    way, it will take less time to train. The number of epochs is set to 100, as `EarlyStopping`
    will take care of stopping the training.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将批量大小设置为128，因为图像数量足够，而且这样做会减少训练时间。epoch的数量设置为100，因为`EarlyStopping`会负责停止训练。
- en: The accuracy obtained is better than in the exercise in *Chapter 2*, *Introduction
    to Computer Vision* – we have obtained an accuracy of **92.72%**.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所得到的准确率比*第2章*中的练习*计算机视觉简介*中的结果更好——我们获得了**92.72%**的准确率。
- en: 'Take a look at the following output:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请查看以下输出：
- en: '![](img/C13550_05_09.jpg)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13550_05_09.jpg)'
- en: 'Figure 5.9: val_acc shown as 0.9240, which is 92.72%'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：显示的val_acc为0.9240，即92.72%
- en: Note
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The entire code for this exercise is available on GitHub: [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb).'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本次练习的完整代码可以在GitHub上找到：[https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb)。
- en: 'Let''s try with the same examples that we tried in *Activity 2*, *Classify
    10 Types of Clothes of the Fashion-MNIST Database* of *Chapter 2*, which is located
    in `Dataset/testing/`:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试使用我们在*第2章*的*活动2*，即*Fashion-MNIST数据库的10种服装分类*中尝试过的相同例子，数据位于`Dataset/testing/`：
- en: '[PRE17]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here''s the output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 5.10: Prediction of clothes using CNNs](img/C13550_05_10.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10：使用CNN进行服装预测](img/C13550_05_10.jpg)'
- en: 'Figure 5.10: Prediction of clothes using CNNs'
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.10：使用卷积神经网络（CNN）进行服装预测
- en: 'As a reminder, here is the table with the number of corresponding clothes:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这是对应服装数量的表格：
- en: '![Figure 5.11: The table with the number of corresponding clothes      ](img/C13550_05_11.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11：对应服装数量的表格](img/C13550_05_11.jpg)'
- en: 'Figure 5.11: The table with the number of corresponding clothes'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.11：对应服装数量的表格
- en: We can see that the model has predicted all the pictures well, so we can state
    that the model is far better than one with only dense layers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型已经很好地预测了所有图片，因此我们可以断定这个模型远比只有密集层的模型要好。
- en: Improving Your Model - Data Augmentation
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进您的模型 - 数据增强
- en: There are situations, at times, where you would not be able to improve the accuracy
    of your model by building a better model. Sometimes, the problem is not the model
    but the data. One of the most important things to consider when working with machine
    learning is that the data you work with has to be good enough for a potential
    model to generalize that data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你可能无法通过构建更好的模型来提高模型的准确性。有时，问题不在于模型，而在于数据。工作时最重要的事情之一是，使用的数据必须足够好，以便潜在的模型能够对这些数据进行泛化。
- en: Data can represent real-life things, but it can also include incorrect data
    that may perform badly. This can happen when you have incomplete data or data
    that does not represent the classes well. For those cases, data augmentation has
    become one of the most popular approaches.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以表示现实中的事物，但也可能包含表现不佳的错误数据。当数据不完整或数据无法很好地代表各类时，就可能会发生这种情况。对于这些情况，数据增强已成为最流行的方法之一。
- en: 'Data augmentation actually increases the number of samples of the original
    dataset. For computer vision, this could mean increasing the number of images
    in a dataset. There are several data augmentation techniques, and you may want
    to use a specific technique, depending on the dataset. Some of these techniques
    are mentioned here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强实际上是增加原始数据集的样本数量。对于计算机视觉，这意味着增加数据集中的图像数量。数据增强技术有很多种，你可能会根据数据集的不同选择特定的技术。这里提到了一些技术：
- en: '**Rotation**: The user sets the degree of rotation for images in the dataset.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋转**：用户为数据集中的图像设置旋转角度。'
- en: '**Flip**: To flip the images horizontally or vertically.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻转**：水平或垂直翻转图像。'
- en: '**Crop**: Crop a section from the images randomly.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁剪**：从图像中随机裁剪一部分。'
- en: '**Change color**: Change or vary the color of the images.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改变颜色**：更改或变化图像的颜色。'
- en: '**Add Noise**: To add noise to images.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加噪声**：向图像中添加噪声。'
- en: Applying these or other techniques, you end up generating new images that vary
    from the original ones.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些或其他技术，你将生成与原始图像不同的新图像。
- en: 'In order to implement this in code, Keras has a module called `ImageDataGenerator`,
    where you declare transformations that you want to apply to your dataset. You
    can import that module using this line of code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在代码中实现这一点，Keras有一个叫做`ImageDataGenerator`的模块，在其中声明你希望应用到数据集的变换。你可以通过以下代码行导入该模块：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In order to declare the variable that is going to apply all those changes to
    your dataset, you have to declare it as in the following code snippet:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了声明将应用所有这些更改的数据集变量，你需要像下面的代码片段一样声明它：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can see what attributes you can pass to `ImageDataGenerator` by looking
    at this documentation from Keras: [https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看Keras的这篇文档，了解可以传递给`ImageDataGenerator`的属性：[https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/)。
- en: 'After declaring `datagen`, you have to compute some calculations for feature-wise
    normalization by using the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在声明`datagen`之后，你需要使用以下方法进行特征-wise规范化计算：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, `x_train` is your training set.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`x_train`是你的训练集。
- en: 'In order to train the model using data augmentation, the following code should
    be used:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用数据增强训练模型，应使用以下代码：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Datagen.flow()` is used so that data augmentation can be applied. As Keras
    does not know when to stop applying data augmentation in the given data, `Steps_per_epoch`
    is the parameter that sets that limit, which should be the length of the training
    set divided by the batch size.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`Datagen.flow()`用于应用数据增强。由于Keras不知道何时停止对给定数据应用数据增强，`Steps_per_epoch`是设置该限制的参数，应该是训练集长度除以批量大小。'
- en: Now we are going to jump right into the second exercise of this chapter to observe
    the output. Data augmentation promises better results and better accuracy. Let's
    find out whether that is true or not.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将直接进入本章的第二个练习，观察输出。数据增强承诺更好的结果和更高的准确度。让我们来看看这是否成立。
- en: 'Exercise 18: Improving Models Using Data Augmentation'
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习18：使用数据增强改进模型
- en: 'In this exercise, we are going to use the The Oxford - III Pet dataset, which
    is RGB images, of varying sizes and several classes, of different cat/dog breeds.
    In this case, we will separate the dataset into two classes: cats and dogs, for
    simplicity. There are 1,000 images for each class, which is not much, but it will
    increment the effect of data augmentation. This dataset is stored in the `Dataset/dogs-cats/`
    folder, added on GitHub.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用牛津-III宠物数据集，该数据集包含RGB图像，大小不一，有多个类，包括不同品种的猫和狗。在这个例子中，我们将数据集分为两个类：猫和狗，为了简便。每个类有1,000张图片，虽然数量不多，但这将增强数据增强的效果。该数据集存储在GitHub上`Dataset/dogs-cats/`文件夹中。
- en: 'We will build a CNN and train it with and without data augmentation, and we
    will compare the results:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个CNN，并分别使用和不使用数据增强进行训练，然后比较结果：
- en: Note
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For this exercise, we are going to open another Google Colab notebook.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将打开另一个Google Colab笔记本。
- en: 'The entire code for this exercise can be found on GitHub: [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习的完整代码可以在GitHub上找到：[https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb)。
- en: Open up your Google Colab interface.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您的Google Colab界面。
- en: Create a folder for the book and download the `Datasets` folder from GitHub
    and upload it in the folder in your drive.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文件夹用于存放书籍，并从GitHub下载`Datasets`文件夹并上传到您驱动器中的该文件夹。
- en: 'Import drive and mount it as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入驱动器并按如下方式挂载：
- en: '[PRE22]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Every time you use a new collaborator, mount the drive to the desired folder.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每次使用新的协作者时，都要将驱动器挂载到所需的文件夹。
- en: Once you have mounted your drive for the first time, you have to enter the authorization
    code mentioned by clicking on the URL given by Google.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您第一次挂载驱动器后，您需要通过点击Google提供的URL并输入授权代码。
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您已经挂载了驱动器，接下来需要设置目录的路径：
- en: '[PRE23]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The path mentioned in step 5 may change as per your folder setup on Google Drive.
    The path will always begin with `cd /content/drive/My Drive/`.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤5中提到的路径可能会根据您在Google Drive上的文件夹设置而有所变化。路径将始终以`cd /content/drive/My Drive/`开头。
- en: 'First, let''s use these two methods, which we have already used before, to
    load the data from disk:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用之前已经使用过的这两种方法从磁盘加载数据：
- en: '[PRE24]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The size of the image is specified as 128x128\. This size is larger than the
    sizes used before, because we need more detail in these images, as the classes
    are more difficult to differentiate and the subjects are presented in varying
    positions, which makes the work even more difficult.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像的大小设置为128x128。这比之前使用的大小要大，因为我们需要图像中更多的细节，因为这些类别更难以区分，且主题呈现出不同的姿势，这使得工作变得更加困难。
- en: 'We load the corresponding images of dogs and cats as `X` for the images and
    `y` for the labels, and we print the shape of those:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载相应的狗和猫的图像作为`X`（图像）和`y`（标签），并打印它们的形状：
- en: '[PRE25]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Figure 5.12: Dogs-cats data shape](img/C13550_05_12.jpg)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.12：狗猫数据形状](img/C13550_05_12.jpg)'
- en: 'Figure 5.12: Dogs-cats data shape'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.12：狗猫数据形状
- en: 'Now we will import `random`, set the seed, and show some samples of the data:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将导入`random`，设置种子，并显示一些数据样本：
- en: '[PRE26]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Figure 5.13: Image samples of the Oxford Pet dataset](img/C13550_05_13.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.13：牛津宠物数据集的图像样本](img/C13550_05_13.jpg)'
- en: 'Figure 5.13: Image samples of the Oxford Pet dataset'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.13：牛津宠物数据集的图像样本
- en: 'To pre-process the data, we are going to use the same procedure as in *Exercise 17:
    Building a CNN*:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '为了预处理数据，我们将使用与*Exercise 17: 构建CNN*中相同的过程：'
- en: '[PRE27]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we separate `X` and `y` into `x_train` and `y_train` for the training
    set, and `x_test` and `y_test` for the testing set, and we print the shapes:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将`X`和`y`分别拆分为`x_train`和`y_train`（训练集），以及`x_test`和`y_test`（测试集），并打印它们的形状：
- en: '[PRE28]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 5.14: Training and testing set shapes](img/C13550_05_14.jpg)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.14：训练集和测试集形状](img/C13550_05_14.jpg)'
- en: 'Figure 5.14: Training and testing set shapes'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.14：训练集和测试集形状
- en: 'We import the corresponding data to build, compile, and train the model:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入相应的数据以构建、编译和训练模型：
- en: '[PRE29]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s build the model:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建模型：
- en: '[PRE30]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The model goes from 16 filters in the very first layer to 128 filters at the
    end, doubling the size in every 2 layers.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型的第一个层有16个滤波器，最后一个层有128个滤波器，每2层大小翻一倍。
- en: Because this problem is harder (we have bigger images with 3 channels and 128x128
    images), we have made the model deeper, adding another couple of layers with 16
    filters at the beginning (the first layer having a kernel size of 5x5, which is
    better in the very first stages) and another couple of layers with 128 filters
    at the end of the model.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这个问题更难（我们有更大的图像，且图像具有3个通道，尺寸为128x128），我们使模型更深，添加了另外几层，在开始时使用16个滤波器（第一层使用5x5的卷积核，这在最初阶段更为有效），并在模型的最后再添加了几层128个滤波器。
- en: 'Now, let''s compile the model:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们编译模型：
- en: '[PRE31]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have set the patience to 15 epochs for the EarlyStopping callback because
    it takes more epochs for the model to converge to the sweet spot, and the validation
    loss can vary a lot until then.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将`EarlyStopping`回调的耐心值设置为15个周期，因为模型需要更多的周期才能收敛到最佳位置，而验证损失在此之前可能会波动很大。
- en: 'Then, we train the model:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们训练模型：
- en: '[PRE32]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The batch size is also low as we do not have much data, but it could be increased
    to 16 easily.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小也较小，因为我们没有太多数据，但它可以轻松增加到16。
- en: 'Then, evaluate the model:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，评估模型：
- en: '[PRE33]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should see the following output:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![](img/C13550_05_15.jpg)'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13550_05_15.jpg)'
- en: 'Figure 5.15: Output showing the accuracy of the model'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.15：展示模型准确度的输出
- en: As you can see from the preceding figure, the accuracy achieved in this dataset
    with this model is **67.25%**.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前图所示，使用此模型在该数据集上实现的准确率为**67.25%**。
- en: 'We are going to apply data augmentation to this process. We have to import
    ImageDataGenerator from Keras and declare it with transformations that we are
    going to make:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对该过程应用数据增强。我们需要从Keras导入ImageDataGenerator，并声明它与我们将进行的转换：
- en: '[PRE34]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following transformations have been applied:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用了以下转换：
- en: We have set a rotation range of 15 degrees because dogs and cats within images
    can be positioned in slightly different ways (feel free to tweak this parameter).
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设置了15度的旋转范围，因为图像中的狗和猫可能以稍微不同的方式呈现（可以根据需要调整此参数）。
- en: We have set the width shift range and height shift range to 0.2 to shift the
    image horizontally and vertically, as an animal could be anywhere within the image
    (also tweakable).
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将宽度平移范围和高度平移范围设置为0.2，以便在水平和垂直方向上平移图像，因为动物可能出现在图像的任何位置（同样可以调整）。
- en: We have set the horizontal flip property to `True` because these animals can
    be flipped in the dataset (horizontally; with vertical flipping, it is much more
    difficult to find an animal).
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将水平翻转属性设置为`True`，因为这些动物在数据集中可以水平翻转（对于垂直翻转，找到动物会更加困难）。
- en: Finally, we set zoom range to 0.3 to make random zooms on the images as the
    dogs and cats may be farther in the image or closer.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们将缩放范围设置为0.3，以便对图像进行随机缩放，因为图像中的狗和猫可能离得更远或更近。
- en: 'We fit the `datagen` instance declared with the training data in order to compute
    quantities for feature-wise normalization and declare and compile the model again
    to make sure we are not using the previous one:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将声明好的`datagen`实例与训练数据拟合，以计算特征标准化所需的量，并重新声明并编译模型，以确保不使用之前的模型：
- en: '[PRE35]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we train the model with the `fit_generator` method of the model and
    the `flow()` method of the `datagen` instance generated:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用模型的`fit_generator`方法和`datagen`实例生成的`flow()`方法来训练模型：
- en: '[PRE36]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We set the `steps_per_epoch` parameter equal to the length of the training set
    divided by the batch size (8).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将`steps_per_epoch`参数设置为训练集长度除以批量大小（8）。
- en: 'We also set the number of workers to 4 to take advantage of the 4 cores of
    the processor:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将工作线程数设置为4，以便充分利用处理器的四个核心：
- en: '[PRE37]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should see the following output:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 5.16: Output showing the accuracy of the model](img/C13550_05_16.jpg)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.16：展示模型准确度的输出](img/C13550_05_16.jpg)'
- en: 'Figure 5.16: Output showing the accuracy of the model'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.16：展示模型准确度的输出
- en: As you can see from the preceding figure, with data augmentation, we achieve
    an accuracy of **81%**, which is far better.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前图所示，经过数据增强后，我们实现了**81%**的准确率，效果远好于之前。
- en: 'If we want to load the model that we just trained (dogs versus cats), the following
    code achieves that:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想加载我们刚刚训练的模型（狗与猫），以下代码可以实现这一点：
- en: '[PRE38]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s try the model with unseen data. The data can be found in the `Dataset/testing`
    folder and the code from *Exercise 17*, *Building a CNN* will be used (but with
    different names for the samples):'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用未见过的数据来试试这个模型。数据可以在`Dataset/testing`文件夹中找到，代码来自*练习 17*，*构建卷积神经网络（CNN）*（但样本的名称不同）：
- en: '[PRE39]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In these lines of code, we are loading an image, resizing it to the expected
    size (128 x 128), normalizing the image – as we did with the training set – and
    reshaping it to (1, 128, 128, 3) to fit as input in the neural network.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这些代码行中，我们加载了一张图片，将其调整为预期的大小（128 x 128），对图片进行了归一化处理——就像我们对训练集所做的那样——并将其重塑为（1,
    128, 128, 3）的形状，以适应神经网络的输入。
- en: 'We continue the for loop:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们继续执行for循环：
- en: '[PRE40]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/C13550_05_17.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13550_05_17.jpg)'
- en: 'Figure 5.17: Prediction of the Oxford Pet dataset with unseen data using CNNs
    and data augmentation'
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.17：使用卷积神经网络（CNNs）和数据增强对牛津宠物数据集进行预测，数据为未见过的数据
- en: We can see that the model has made all the predictions well. Note that not all
    the breeds are stored in the dataset, so not all the cats and dogs will be predicted
    properly. Adding more types of breeds would be necessary in order to achieve that.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型已经做出了所有正确的预测。请注意，并非所有的品种都存储在数据集中，因此并非所有猫狗都会被正确预测。为了实现这一点，需要添加更多品种类型。
- en: 'Activity 5: Making Use of Data Augmentation to Classify correctly Images of
    Flowers'
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 5：利用数据增强正确分类花卉图像
- en: 'In this activity, you are going to put into practice what you have learned.
    We are going to use a different dataset, where the images are bigger (150x150).
    There are 5 classes in this dataset: daisy, dandelion, rose, sunflower, and tulip.
    There are, in total, 4,323 images, which is fewer when compared to the previous
    exercises we performed. The classes do not have the same number of images either,
    but do not worry about that. The images are RGB, so there will be three channels.
    We have stored them in NumPy arrays of each class, so we will provide a way to
    load them properly.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，您将把所学内容付诸实践。我们将使用一个不同的数据集，其中图像更大（150x150）。该数据集包含 5 个类别：雏菊、蒲公英、玫瑰、向日葵和郁金香。该数据集总共有
    4,323 张图像，较我们之前进行的练习要少。各个类别的图像数量也不相同，但请不要担心。图像是 RGB 格式的，因此将有三个通道。我们已将它们存储在每个类别的
    NumPy 数组中，因此我们将提供一种方法来正确加载它们。
- en: 'The following steps will guide you through this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导您完成此过程：
- en: 'Load the dataset by using this code, as the data is stored in NumPy format:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码加载数据集，因为数据以 NumPy 格式存储：
- en: '[PRE41]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Show some samples from the dataset by importing `random` and `matplotlib`, using
    a random index to access the `X` set.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过导入`random`和`matplotlib`，并使用随机索引访问`X`集来显示数据集中的一些样本。
- en: Note
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The NumPy arrays were stored in BGR format (OpenCV format), so in order to
    show the images properly, you will need to use the following code to change the
    format to RGB (only to show the image): `image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)`.'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NumPy 数组是以 BGR 格式存储的（OpenCV 格式），因此为了正确显示图像，您需要使用以下代码将格式转换为 RGB（仅用于显示图像）：`image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)`。
- en: You will need to import `cv2`.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您需要导入`cv2`。
- en: Normalize the `X` set and set the labels to categorical (the `y` set).
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`X`集进行归一化，并将标签设置为类别（即`y`集）。
- en: Split the sets into a training and testing set.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分成训练集和测试集。
- en: Build a CNN.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个 CNN。
- en: Note
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As we have bigger images, you should consider adding more layers, thus reducing
    the image size, and the first layer should contain a bigger kernel (the kernel
    should be an odd number when it is bigger than 3).
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于图像较大，您应该考虑添加更多的层，从而减少图像的大小，且第一层应包含更大的卷积核（如果卷积核大于 3，则应为奇数）。
- en: Declare ImageDataGenerator from Keras with the changes that you think will suit
    the variance of the dataset.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Keras 中声明 ImageDataGenerator，并根据您认为适合数据集变化的方式进行调整。
- en: Train the model. You can either choose an EarlyStopping policy or set a high
    number of epochs and wait or stop it whenever you want. If you declare the Checkpoint
    callback, it will always save only the best validation loss model (if that is
    the metric you are using).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。您可以选择提前停止（EarlyStopping）策略，或设置较大的 epoch 数量，等待或随时停止它。如果您声明了 Checkpoint 回调，它将始终仅保存最佳的验证损失模型（如果您使用的是该指标）。
- en: 'Evaluate the model using this code:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码评估模型：
- en: '[PRE42]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This will print the accuracy of the model. Note that batch_size is the batch
    size you have set for your training sets and for `x_test` and `y_test`, which
    are your testing sets.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印模型的准确率。请注意，batch_size 是您为训练集、`x_test` 和 `y_test` 设置的批量大小，它们是您的测试集。
- en: 'You can use this code in order to evaluate any model, but first you need to
    load the model. If you want to load the entire model from a `.h5` file, you will
    have to use this code:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以使用以下代码来评估任何模型，但首先需要加载模型。如果您要从`.h5`文件加载整个模型，则必须使用以下代码：
- en: '`from keras.models import load_model`     `model = load_model(''model.h5'')`'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`from keras.models import load_model`     `model = load_model(''model.h5'')`'
- en: 'Try the model with unseen data. In the `Dataset/testing/` folder, you will
    find five images of flowers that you can load to try it out. Remember that the
    classes are in this order:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试用未见过的数据测试模型。在 `Dataset/testing/` 文件夹中，您将找到五张花卉图像，可以加载它们进行测试。请记住，类别的顺序如下：
- en: classes=['daisy','dandelion','rose','sunflower','tulip']
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: classes=['daisy','dandelion','rose','sunflower','tulip']
- en: 'So, the result should look like this:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所以，结果应如下所示：
- en: '![Figure 5.18: Prediction of roses using CNNs](img/C13550_05_18.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18：使用 CNN 预测玫瑰花](img/C13550_05_18.jpg)'
- en: 'Figure 5.18: Prediction of roses using CNNs'
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：使用 CNN 预测玫瑰花
- en: Note
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 313.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 313 页找到。
- en: State-of-the-Art Models - Transfer Learning
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最先进的模型 - 迁移学习
- en: Humans do not learn each and every task that they want to achieve from scratch;
    they usually take previous knowledge as a base in order to learn tasks much faster.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 人类并不是从零开始学习每一个想要实现的任务；他们通常会以之前的知识为基础，以便更快速地学习新任务。
- en: When training neural networks, there are some tasks that are extremely expensive
    to train for every individual, such as having hundreds of thousands of images
    for training and having to distinguish between two or more similar objects, ending
    up having a cost of days to achieve good performance, for example. These neural
    networks are trained to achieve this expensive task, and because neural networks
    are capable of saving that knowledge, then other models can take advantage of
    those weights to retrain specific models for similar tasks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，有些任务对于每个个体来说是非常昂贵的，例如需要数十万张图像进行训练，且必须区分两个或多个相似的物体，最终可能需要几天时间才能获得好的性能。这些神经网络经过训练以完成这个昂贵的任务，而由于神经网络能够保存这些知识，其他模型可以利用这些权重来重新训练特定的模型以执行类似的任务。
- en: '**Transfer learning** does just that – it transfers the knowledge of a pretrained
    model to your model, so you can take advantage of that knowledge.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**正是做了这件事——它将预训练模型的知识转移到你的模型中，这样你就可以利用这些知识。'
- en: So, for example, if you want to make a classifier that is capable of identifying
    five objects but that task seems too expensive to train (it takes knowledge and
    time), you can take advantage of a pretrained model (usually trained on the famous
    **ImageNet** dataset) and retrain the model adapted to your problem. The ImageNet
    dataset is a large visual database designed for use in visual object recognition
    research and has more than 14 million images with more than 20,000 categories,
    which is very expensive for an individual to train.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想创建一个能够识别五个物体的分类器，但这个任务训练起来似乎成本太高（需要知识和时间），你可以利用一个预训练的模型（通常是在著名的**ImageNet**数据集上训练的），并重新训练模型，使其适应你的问题。ImageNet数据集是一个大型视觉数据库，旨在用于视觉物体识别研究，包含超过1400万张图像和超过20000个类别，个人训练起来是非常昂贵的。
- en: Technically, you load the model with the weights of the dataset where it was
    trained, and if you want to achieve a different problem, you only have to change
    the last layer of the model. If the model is trained on ImageNet, it could have,
    1000 classes but you only have 5 classes, so you would change the last layer to
    a dense layer with only 5 neurons. You could add more layers before the last one,
    though.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你加载模型时会使用该数据集的权重，如果你想解决不同的问题，你只需更改模型的最后一层。如果模型是在ImageNet上训练的，它可能有1000个类别，但你只有5个类别，所以你需要将最后一层更改为一个只有5个神经元的全连接层。你还可以在最后一层之前添加更多的层。
- en: 'The layers of the model that you have imported (the base model) can be frozen
    so their weights do not reflect on the training time. Depending on this, there
    are two types of transfer learning:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你导入的模型层（基础模型）可以被冻结，这样它们的权重就不会影响训练时间。根据这一点，迁移学习有两种类型：
- en: '**Traditional**: Freeze all the layers of the base model'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统**：冻结基础模型的所有层'
- en: '**Fine-tuning**: Freeze only a part of the base model, typically the first
    layers'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：仅冻结基础模型的一部分，通常是前几层'
- en: In Keras, we can import famous pretrained models such as Resnet50 and VGG16\.
    You can import a pretrained model with or without weights (in Keras, there are
    only weights for ImageNet), which includes the top of the model or not. The input
    shape can only be only specified if the top is not included and with a minimum
    size of 32.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，我们可以导入一些著名的预训练模型，如Resnet50和VGG16。你可以选择导入带或不带权重的预训练模型（在Keras中，只有ImageNet的权重可用），并决定是否包含模型的顶部部分。输入形状只有在不包含顶部部分时才能指定，且最小大小为32。
- en: 'With the following lines of code, you would import the Resnet50 model without
    the top, with the `imagenet` weights and with a shape of 150x150x3:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码行，你可以导入Resnet50模型，不包括顶部部分，带有`imagenet`权重，并且输入形状为150x150x3：
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you have included the top of the model because you want to use the last
    dense layers of the model (let''s say your problem is similar to ImageNet but
    with different classes), then you should write this code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经包含了模型的顶部部分，因为你想使用模型的最后一层全连接层（假设你的问题类似于ImageNet但有不同的类别），那么你应该编写以下代码：
- en: '[PRE44]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This code gets rid of the classification layer (the last dense layer) and prepares
    the model so that you can add your own last layer. Of course, you could add more
    layers at the end before adding your classification layer.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码移除了分类层（最后的全连接层），并准备模型，以便你可以添加自己的最后一层。当然，你也可以在最后添加更多层，之后再添加分类层。
- en: 'If you have not added the top of the model, then you should add your own top
    with this code:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有添加模型的顶部层，你应该使用以下代码添加自己的顶部层：
- en: '[PRE45]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here, `GlobalAveragePooling2D` is like a type of max pooling.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`GlobalAveragePooling2D`类似于一种最大池化方法。
- en: 'With these kinds of models, you should preprocess the data just as you did
    with the data that trained those models (if you are using the weights). Keras
    has a `preprocess_input` method that does that for every model. For example, for
    ResNet50, it would be like this:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这类模型，你应该像训练这些模型的数据一样预处理数据（如果你正在使用权重）。Keras有一个`preprocess_input`方法，针对每个模型都会这样做。例如，对于ResNet50，应该是这样的：
- en: '[PRE46]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You pass your array of images to that function and then you will have your data
    ready for training.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你将图像数组传递给那个函数，然后你的数据就准备好用于训练了。
- en: The **learning rate** in a model is how fast it should convert the model to
    a local minimum. Usually, you do not have to worry about this but if you are retraining
    a neural network, this is a parameter that you have to tweak. When you are retraining
    a neural network, you should decrease the value of this parameter so that the
    neural network does not unlearn what it has already learned. This parameter is
    tweaked when declaring the optimizer. You can avoid tweaking this parameter, although
    the model may end up not ever converging or overfitting.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的**学习率**是指模型向局部最小值转换的速度。通常，你无需担心这一点，但如果你正在重新训练神经网络，这个参数需要调整。当你重新训练神经网络时，你应该降低该参数的值，以免神经网络忘记已经学到的内容。这个参数是在声明优化器时调整的。你可以选择不调整这个参数，尽管模型可能永远不会收敛或出现过拟合。
- en: With this kind of approach, you could train your network with very little data
    and get good results overall, because you take advantage of the weights of the
    model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，你可以用非常少的数据训练网络，并获得总体良好的结果，因为你利用了模型的权重。
- en: You can combine transfer learning with data augmentation as well.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将迁移学习与数据增强结合使用。
- en: 'Exercise 19: Classifying €5 and €20 Bills Using Transfer Learning with Very
    Little Data'
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习19：使用迁移学习和极少数据分类€5和€20账单
- en: 'This problem is about differentiating €5 bills from €20 bills with very little
    data. We have 30 images for every class, which is much less than we have had in
    previous exercises. We are going to load the data, declare the pretrained model,
    then declare the changes on the data with data augmentation and train the model.
    After that, we will check how well the model performs with unseen data:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题是关于用非常少的数据区分€5和€20账单。每个类别我们有30张图片，远少于之前的练习中的数据量。我们将加载数据，声明预训练模型，然后通过数据增强声明数据的变化，并训练模型。之后，我们将检查模型在未见数据上的表现：
- en: Open up your Google Colab interface.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的Google Colab界面。
- en: Note
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You would need to mount the `Dataset` folder on your drive and set the path
    accordingly.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你需要将`Dataset`文件夹挂载到你的驱动器上，并相应地设置路径。
- en: 'Declare functions to load the data:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明加载数据的函数：
- en: '[PRE47]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note that the data is resized to 224x224.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，数据被调整为224x224的大小。
- en: 'The data is stored in `Dataset/money/`, where you have both classes in subfolders.
    In order to load the data, you have to write the following code:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据存储在`Dataset/money/`中，其中有两个类别在子文件夹内。为了加载数据，你需要写出以下代码：
- en: '[PRE48]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The label for the €20 bill is 0 and it's 1 for the €5 bill.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: €20账单的标签是0，€5账单的标签是1。
- en: 'Let''s show the data:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们展示数据：
- en: '[PRE49]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Figure 5.21: Samples of bills](img/C13550_05_19.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.21：账单样本](img/C13550_05_19.jpg)'
- en: 'Figure 5.19: Samples of bills'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.19：账单样本
- en: 'Now we are going to declare the pretrained model:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将声明预训练模型：
- en: '[PRE50]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this case, we are loading the MobileNet model with the weights of imagenet.
    We are not including the top so we should build our own top. The input shape is
    224x224x3.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，我们正在加载MobileNet模型，并使用imagenet的权重。我们没有包括顶部层，因此我们应该构建自己的顶部层。输入形状是224x224x3。
- en: We have built the top of the model by taking the output of the last layer of
    MobileNet (which is not the classification layer) and start building on top of
    that. We have added `GlobalAveragePooling2D` for image reduction, a dense layer
    that we can train for our specific problem, a `Dropout` layer to avoid overfitting,
    and the classifier layer at the end.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过获取MobileNet最后一层（非分类层）的输出，开始构建模型的顶部。我们添加了`GlobalAveragePooling2D`用于图像压缩，一个我们可以针对特定问题训练的密集层，一个`Dropout`层以避免过拟合，最后是分类层。
- en: The dense layer at the end has two neurons, as we have only two classes, and
    it has the `Softmax` activation function. For binary classification, the Sigmoid
    function can also be used, but it changes the entire process as you should not
    make the labels categorical and the predictions look different.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后的密集层有两个神经元，因为我们只有两个类别，并且使用了`Softmax`激活函数。对于二分类问题，也可以使用Sigmoid函数，但这会改变整个过程，因为不应该将标签做成类别形式，并且预测结果也会有所不同。
- en: Afterward, we create the model that we are going to train with the input of
    MobileNet as input and the classification dense layer as output.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们创建了一个模型，输入为MobileNet，输出为分类密集层。
- en: 'We are going to do fine-tuning. In order to do that, we have to freeze some
    of the input layers and keep the rest of the trainable data, unchanged:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将进行微调。为了做到这一点，我们必须冻结一些输入层，并保持其余可训练的数据不变：
- en: '[PRE51]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s compile the model with the `Adadelta` optimizer:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`Adadelta`优化器编译模型：
- en: '[PRE52]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now we will use the `preprocess_input` method that we imported previously to
    preprocess the `X` set for MobileNet, and then we convert label `y` to one-hot
    encoding:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用之前导入的`preprocess_input`方法对MobileNet的`X`集进行预处理，然后将标签`y`转换为独热编码：
- en: '[PRE53]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We use the `train_test_split` method to split the data into a training set
    and testing set:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`train_test_split`方法将数据集分割为训练集和测试集：
- en: '[PRE54]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We are going to apply data augmentation to our dataset:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对数据集应用数据增强：
- en: '[PRE55]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As a bill can be at different angles, we choose to make a rotation range of
    90º. The other parameters seem reasonable for this task.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于账单可能处于不同角度，我们选择设置旋转范围为90º。其他参数对于此任务来说似乎合理。
- en: 'Let''s declare a checkpoint to save the model when the validation loss decreases
    and train the model:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们声明一个检查点，当验证损失减少时保存模型，并训练该模型：
- en: '[PRE56]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We have set the batch size to 4 because we have only a few samples of data and
    we do not want to pass all the samples to the neural network at once, but in batches.
    We are not using the EarlyStopping callback because the loss goes up and down
    due to the lack of data and the use of Adadelta with a high learning rate.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将批次大小设置为4，因为我们只有少量数据，并且不希望将所有样本一次性传递给神经网络，而是分批处理。由于数据不足，并且使用Adadelta优化器时学习率较高，所以我们没有使用EarlyStopping回调函数，因为损失值会上下波动。
- en: 'Check the results:![Figure 5.22: Showing the desired output](img/C13550_05_20.jpg)'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查结果：![图5.22：显示期望的输出](img/C13550_05_20.jpg)
- en: 'Figure 5.20: Showing the desired output'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.20：显示期望的输出
- en: In the preceding figure, we can see that, in the 7th epoch, we already achieve
    100% accuracy with low loss. This is due to the lack of data on the validation
    set, because with only 12 samples you cannot tell whether the model is performing
    well against unseen data.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到，在第7个周期时，模型已经达到了100%的准确率，并且损失较低。这是由于验证集数据的不足，因为只有12个样本，无法判断模型在未见数据上的表现。
- en: 'Let''s run the code to calculate the accuracy of this model:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行代码来计算该模型的准确率：
- en: '[PRE57]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output is as follows:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.23: Accuracy achieved of 100%](img/C13550_05_21.jpg)'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.23：达成100%准确率](img/C13550_05_21.jpg)'
- en: 'Figure 5.21: Accuracy achieved of 100%'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.21：达成100%准确率
- en: 'Let''s try this model with new data. There are test images in the `Dataset/testing`
    folder. We have added four examples of bills to check whether the model predicts
    them well:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用新数据测试这个模型。`Dataset/testing`文件夹中有测试图像。我们添加了四个账单示例来检查模型是否能准确预测：
- en: Note
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE58]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In this code, we have loaded the unseen examples as well, and we have clubbed
    the output image, which looks like this:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，我们也加载了未见过的示例，并且我们将输出图像合并，结果如下所示：
- en: '![Figure 5.24: Prediction of bills](img/C13550_05_22.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图5.24：账单预测](img/C13550_05_22.jpg)'
- en: 'Figure 5.22: Prediction of bills'
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.22：账单预测
- en: The model has predicted all the images precisely!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已精确预测所有图像！
- en: Congratulations! Now you are able to train a model with your own dataset when
    you have little data, thanks to transfer learning.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你可以在数据量较少的情况下，借助迁移学习训练一个自己的模型了。
- en: Note
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The complete code for this exercise is uploaded on GitHub: https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise19/Exercise19.ipynb.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的完整代码已上传到GitHub：https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise19/Exercise19.ipynb。
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: CNNs have shown much better performance than fully-connected neural networks
    when dealing with images. In addition, CNNs are also capable of accomplishing
    good results with text and sound data.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在处理图像时的表现明显优于全连接神经网络。此外，CNN也能够在处理文本和声音数据时取得良好结果。
- en: CNNs have been explained in depth, as have how convolutions work and all the
    parameters that come along with them. Afterward, all this theory was put into
    practice with an exercise.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: CNN已经被深入讲解，并且卷积的工作原理以及相关的所有参数也得到了详细说明。之后，所有这些理论通过一个练习进行了实践。
- en: Data augmentation is a technique for overcoming a lack of data or a lack of
    variation in a dataset by applying simple transformations to the original data
    in order to generate new images. This technique has been explained and also put
    into practice with an exercise and an activity, where you were able to experiment
    with the knowledge you acquired.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种通过对原始数据进行简单的转换来生成新图像，从而克服数据不足或数据集变异性不足的技术。此技术已经通过一个练习和活动进行了说明和实践，在其中你可以实验所学到的知识。
- en: Transfer learning is a technique used when there is a lack of data or the problem
    is so complex that it would take too long to train on a normal neural network.
    Also, this technique does not need much of an understanding of neural networks
    at all, as the model is already implemented. It can also be used with data augmentation.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种在数据不足或问题过于复杂，以至于使用常规神经网络训练会花费太长时间时使用的技术。此外，这种技术对神经网络的理解要求较低，因为模型已经实现。它也可以与数据增强结合使用。
- en: Transfer learning was also covered and put into practice with an exercise where
    the amount of data was very small.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习也已被涵盖，并通过一个练习进行实践，其中的数据量非常小。
- en: Learning how to build CNNs is very useful for recognizing objects or environments
    in computer vision. When a robot is using its vision sensors to recognize an environment,
    normally, CNNs are employed and data augmentation is used to improve the CNNs
    performance. In *Chapter 8*, *Object Recognition to Guide the Robot Using CNNs,*
    the CNN concepts you have learned about will be applied to a real-world application,
    and you will be able to recognize an environment using deep learning.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何构建CNN对于计算机视觉中的物体或环境识别非常有用。当机器人使用视觉传感器识别环境时，通常会使用CNN，并通过数据增强来提高CNN的性能。在*第八章*，*使用CNN进行物体识别以引导机器人*中，你将把所学的CNN概念应用到实际应用中，并能够利用深度学习识别环境。
- en: Before applying these techniques to recognize the environment, first you need
    to learn how to manage a robot that will be able to recognize an environment.
    In *Chapter 6, Robot Operating System (ROS)*, you will learn how to manage a robot
    using a simulator by taking advantage of software called ROS.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用这些技术来识别环境之前，首先你需要学习如何管理一个能够识别环境的机器人。在*第六章，机器人操作系统（ROS）*中，你将通过利用名为ROS的软件，学习如何使用模拟器来管理机器人。
