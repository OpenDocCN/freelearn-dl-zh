- en: Chapter 1. Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：深度学习简介
- en: Deep Neural networks are currently capable of providing human level solutions
    to a variety of problems such as image recognition, speech recognition, machine
    translation, natural language processing, and many more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络目前能够为许多问题提供接近人类水平的解决方案，例如图像识别、语音识别、机器翻译、自然语言处理等等。
- en: In this chapter, we will look at how neural networks, a biologically-inspired
    architecture has evolved throughout the years. Then we will cover some of the
    important concepts and terminology related to deep learning as a refresher for
    the subsequent chapters. Finally we will understand the intuition behind the creative
    nature of deep networks through a generative model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨神经网络这一生物启发式架构是如何在这些年中发展的。接下来，我们将回顾一些与深度学习相关的重要概念和术语，为后续章节做准备。最后，我们将通过生成模型理解深度网络的创造性本质背后的直觉。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Evolution of deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的演变
- en: Stochastic Gradient Descent, ReLU, learning rate, and so on
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降、ReLU、学习率等
- en: Convolutional network, Recurrent Neural Network and LSTM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络、递归神经网络和长短期记忆网络（LSTM）
- en: Difference between discriminative and generative models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别模型与生成模型的区别
- en: Evolution of deep learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的演变
- en: A lot of the important work on neural networks happened in the 80's and 90's,
    but back then computers were slow and datasets very tiny. The research didn't
    really find many applications in the real world. As a result, in the first decade
    of the 21st century neural networks have completely disappeared from the world
    of machine learning. It's only in the last few years, first in speech recognition
    around 2009, and then in computer vision around 2012, that neural networks made
    a big comeback (with LeNet, AlexNet, and so on). What changed?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多关于神经网络的重要研究发生在80年代和90年代，但那时计算机速度很慢，数据集非常小。这些研究在现实世界中并没有找到很多应用。因此，在21世纪的第一个十年，神经网络完全从机器学习的世界中消失了。直到最近几年，首先是在2009年左右的语音识别，然后是在2012年左右的计算机视觉领域，神经网络才迎来了一次大的复兴（比如LeNet、AlexNet等）。发生了什么变化？
- en: Lots of data (big data) and cheap, fast GPU's. Today, neural networks are everywhere.
    So, if you're doing anything with data, analytics, or prediction, deep learning
    is definitely something that you want to get familiar with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大量数据（大数据）和廉价、高速的GPU。如今，神经网络无处不在。因此，如果你从事任何与数据、分析或预测相关的工作，深度学习绝对是你需要熟悉的领域。
- en: 'See the following figure:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下图示：
- en: '![Evolution of deep learning](img/B08086_01_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习的演变](img/B08086_01_01.jpg)'
- en: 'Figure-1: Evolution of deep learning'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习的演变
- en: Deep learning is an exciting branch of machine learning that uses data, lots
    of data, to teach computers how to do things only humans were capable of before,
    such as recognizing what's in an image, what people are saying when they are talking
    on their phones, translating a document into another language, and helping robots
    explore the world and interact with it. Deep learning has emerged as a central
    tool to solve perception problems and it's state of the art with computer vision
    and speech recognition.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个令人兴奋的分支，它利用大量的数据来教会计算机执行以前只有人类能够完成的任务，比如识别图像中的内容、理解人们在打电话时说的话、将文档翻译成另一种语言，以及帮助机器人探索世界并与之互动。深度学习已成为解决感知问题的核心工具，并且在计算机视觉和语音识别领域处于最前沿。
- en: Today many companies have made deep learning a central part of their machine
    learning toolkit—Facebook, Baidu, Amazon, Microsoft, and Google are all using
    deep learning in their products because deep learning shines wherever there is
    lots of data and complex problems to solve.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多公司已经将深度学习作为其机器学习工具包的核心部分——Facebook、百度、亚马逊、微软和谷歌都在其产品中使用深度学习，因为深度学习在数据量大且问题复杂的场景中具有显著优势。
- en: 'Deep learning is the name we often use for "deep neural networks" composed
    of several layers. Each layer is made of nodes. The computation happens in the
    nodes, where it combines input data with a set of parameters or weights, that
    either amplify or dampen that input. These input-weight products are then summed
    and the sum is passed through the `activation` function, to determine to what
    extent the value should progress through the network to affect the final prediction,
    such as an act of classification. A layer consists of a row of nodes that that
    turn on or off as the input is fed through the network. The input of the first
    layer becomes the input of the second layer and so on. Here''s a diagram of what
    neural a network might look like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常是我们用来指代由多个层组成的“深度神经网络”的名称。每一层由节点组成。计算发生在节点中，它们将输入数据与一组参数或权重结合起来，这些参数或权重要么放大输入，要么减少输入。然后将这些输入权重的乘积求和，并通过`activation`函数传递，以确定数值应如何通过网络以影响最终预测，如分类等操作。一层由一行节点组成，这些节点在输入通过网络时打开或关闭。第一层的输入成为第二层的输入，依此类推。以下是神经网络可能看起来的图示：
- en: '![Evolution of deep learning](img/B08086_01_22.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习的演变](img/B08086_01_22.jpg)'
- en: Let's get familiarized with some deep neural network concepts and terminology.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们熟悉一些深度神经网络的概念和术语。
- en: Sigmoid activation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: sigmoid激活
- en: The sigmoid activation function used in neural networks has an output boundary
    of *(0, 1)*, and *α* is the offset parameter to set the value at which the sigmoid
    evaluates to 0\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中使用的sigmoid激活函数具有输出边界*(0, 1)*，*α*是偏移参数，用于设置sigmoid评估为0的值。
- en: The sigmoid function often works fine for gradient descent as long as the input
    data *x* is kept within a limit. For large values of *x*, *y* is constant. Hence,
    the derivatives *dy/dx* (the gradient) equates to *0*, which is often termed as
    the **vanishing gradient** problem.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降来说，sigmoid函数在输入数据*x*保持在限制范围内时通常效果良好。对于*x*的大值，*y*是常数。因此，导数*dy/dx*（梯度）等于*0*，这通常称为**梯度消失**问题。
- en: This is a problem because when the gradient is 0, multiplying it with the loss
    (actual value - predicted value) also gives us 0 and ultimately networks stop
    learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题，因为当梯度为0时，将其与损失（实际值-预测值）相乘也会得到0，最终网络停止学习。
- en: Rectified Linear Unit (ReLU)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: A neural network can be built by combining some linear classifiers with some
    non-linear functions. The **Rectified Linear Unit** (**ReLU**) has become very
    popular in the last few years. It computes the function *f(x)=max(0,x)*. In other
    words, the activation is simply thresholded at zero. Unfortunately, ReLU units
    can be fragile during training and can die, as a ReLU neuron could cause the weights
    to update in such a way that the neuron will never activate on any datapoint again,
    and so the gradient flowing through the unit will forever be zero from that point
    on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将一些线性分类器与一些非线性函数组合来构建神经网络。**修正线性单元**（ReLU）在过去几年中变得非常流行。它计算函数*f(x)=max(0,x)*。换句话说，激活在零处被阈值化。不幸的是，ReLU单元在训练过程中可能会变得脆弱并死亡，因为ReLU神经元可能导致权重更新的方式使得神经元永远不会在任何数据点上激活，因此从那时起通过单元的梯度将永远为零。
- en: 'To overcome this problem, a leaky `ReLU` function will have a small negative
    slope (of 0.01, or so) instead of zero when *x<0*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，一个带有小负斜率（大约0.01）的渗漏`ReLU`函数将在*x<0*时具有非零斜率：
- en: '![Rectified Linear Unit (ReLU)](img/B08086_01_24.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![修正线性单元（ReLU）](img/B08086_01_24.jpg)'
- en: where *αα* is a small constant.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*αα*是一个小常数。
- en: '![Rectified Linear Unit (ReLU)](img/B08086_01_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![修正线性单元（ReLU）](img/B08086_01_02.jpg)'
- en: 'Figure-2: Rectified Linear Unit'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图-2：修正线性单元
- en: Exponential Linear Unit (ELU)
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指数线性单元（ELU）
- en: The mean of ReLU activation is not zero and hence sometimes makes learning difficult
    for the network. The **Exponential Linear Unit** (**ELU)** is similar to ReLU
    activation function when the input *x* is positive, but for negative values it
    is a function bounded by a fixed value *-1*, for *α=1* (the hyperparameter *α*
    controls the value to which an ELU saturates for negative inputs). This behavior
    helps to push the mean activation of neurons closer to zero; that helps to learn
    representations that are more robust to noise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活的均值不为零，因此有时会使网络学习困难。**指数线性单元**（ELU）与ReLU激活函数类似，当输入*x*为正时，但对于负值，它是一个被固定值*-1*界定的函数，对于*α=1*（超参数*α*控制ELU对负输入饱和的值）。这种行为有助于将神经元的均值激活推向接近零的位置；这有助于学习更能抵抗噪声的表示。
- en: Stochastic Gradient Descent (SGD)
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）
- en: Scaling batch gradient descent is cumbersome because it has to compute a lot
    if the dataset is big, and as a rule of thumb, if computing your loss takes *n*
    floating point operations, computing its gradient takes about three times that
    to compute.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降的扩展比较繁琐，因为当数据集较大时，需要进行大量的计算。作为经验法则，如果计算损失需要 *n* 次浮点运算，那么计算梯度大约需要三倍的运算量。
- en: But in practice we want to be able to train lots of data because on real problems
    we will always get more gains the more data we use. And because gradient descent
    is iterative and has to do that for many steps, that means that in order to update
    the parameters in a single step, it has to go through all the data samples and
    then do this iteration over the data tens or hundreds of times.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实际操作中，我们希望能够训练大量的数据，因为在真实问题中，我们总是能从使用更多数据中获得更多的收益。而且因为梯度下降是迭代的，并且需要进行许多步更新，这意味着为了在单步中更新参数，必须遍历所有数据样本，然后对数据进行数十次或数百次的迭代。
- en: Instead of computing the loss over entire data samples for every step, we can
    compute the average loss for a very small random fraction of the training data.
    Think between 1 and 1000 training samples each time. This technique is called
    **Stochastic Gradient Descent** (**SGD**) and is at the core of deep learning.
    That's because SGD scales well with both data and model size.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在每一步都计算整个数据集的损失，我们可以计算一个非常小的随机训练数据子集的平均损失。每次选择的样本数在 1 到 1000 之间。这种技术叫做**随机梯度下降**（**SGD**），是深度学习的核心。这是因为
    SGD 在数据和模型大小上都能很好地扩展。
- en: SGD gets its reputation for being black magic as it has lots of hyper-parameters
    to play and tune such as initialization parameters, learning rate parameters,
    decay, and momentum, and you have to get them right.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 因其有很多超参数需要调整而被认为是黑魔法，例如初始化参数、学习率参数、衰减和动量，你必须正确调整这些参数。
- en: 'AdaGrad is a simple modification of SGD, which implicitly does momentum and
    learning rate decay by itself. Using AdaGrad often makes learning less sensitive
    to hyper-parameters. But it often tends to be a little worse than precisely tuned
    SDG with momentum. It''s still a very good option though, if you''re just trying
    to get things to work:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 是 SGD 的一个简单修改，它隐式地实现了动量和学习率衰减。使用 AdaGrad 通常使学习对超参数的敏感度降低。但它往往比精确调优的带动量的
    SGD 要稍微差一点。不过，如果你只是想让模型运行起来，它仍然是一个非常好的选择：
- en: '![Stochastic Gradient Descent (SGD)](img/B08086_01_04.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降（SGD）](img/B08086_01_04.jpg)'
- en: 'Figure-4a: Loss computation in batch gradient descent and SGD'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4a：批量梯度下降和SGD中的损失计算
- en: '*Source*: [https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
    descent](https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
    descent)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源*：[https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-descent](https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-descent)'
- en: '![Stochastic Gradient Descent (SGD)](img/B08086_01_05.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降（SGD）](img/B08086_01_05.jpg)'
- en: 'Figure 4b: Stochastic Gradient Descent and AdaGrad'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4b：随机梯度下降和AdaGrad
- en: You can notice from *Figure 4a* that in case of batch gradient descent the `loss`/`optimization`
    function is well minimized, whereas SGD calculates the loss by taking a random
    fraction of the data in each step and often oscillates around that point. In practice,
    it's not that bad and SGD often converges faster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图4a* 中可以看出，批量梯度下降的 `loss`/`优化` 函数已经得到很好的最小化，而 SGD 在每一步随机选择数据子集来计算损失，往往在该点附近震荡。在实际应用中，这并不算太糟，SGD
    往往会更快地收敛。
- en: Learning rate tuning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率调优
- en: The `loss` function of the neural network can be related to a surface, where
    the weights of the network represent each direction you can move in. Gradient
    descent provides the steps in the current direction of the slope, and the learning
    rate gives the length of each step you take. The learning rate helps the network
    to abandons old beliefs for new ones.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的 `loss` 函数可以与一个表面相关联，其中网络的权重表示可以移动的每个方向。梯度下降提供了当前坡度方向上的步长，而学习率则决定了每一步的长度。学习率帮助网络抛弃旧的信念，接受新的信念。
- en: Learning rate tuning can be very strange. For example, you might think that
    using a higher learning rate means you learn more or that you learn faster. That's
    just not true. In fact, you can often take a model, lower the learning rate, and
    get to a better model faster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调优可能非常奇怪。例如，你可能认为使用较高的学习率意味着学习更多，或者学习更快。实际上并非如此。事实上，你常常可以通过降低学习率，快速得到更好的模型。
- en: '![Learning rate tuning](img/B08086_01_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![学习率调整](img/B08086_01_03.jpg)'
- en: 'Figure-3: Learning rate'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图-3：学习率
- en: You might be tempted to look at the learning curve that shows the loss over
    time to see how quickly the network learns. Here the higher learning rate starts
    faster, but then it plateaus, whereas the lower learning rate keeps on going and
    gets better. It is a very familiar picture for anyone who has trained neural networks.
    *Never trust how quickly you learn*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想查看显示损失随时间变化的学习曲线，看看网络学习的速度有多快。这里较高的学习率一开始学习较快，但随后趋于平稳，而较低的学习率则持续进行并且变得更好。对于任何训练过神经网络的人来说，这都是一个非常熟悉的画面。*永远不要相信你学习的速度有多快*。
- en: Regularization
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: The first way to prevent over fitting is by looking at the performance under
    validation set, and stopping to train as soon as it stops improving. It's called
    early termination, and it's one way to prevent a neural network from over-optimizing
    on the training set. Another way is to apply regularization. Regularizing means
    applying artificial constraints on the network that implicitly reduce the number
    of free parameters while not making it more difficult to optimize.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过拟合的第一种方法是查看验证集上的表现，并在表现不再提升时停止训练。这被称为早停法，它是防止神经网络在训练集上过度优化的一种方法。另一种方法是应用正则化。正则化意味着对网络施加人工约束，这些约束在不增加优化难度的情况下，隐性地减少自由参数的数量。
- en: '![Regularization](img/B08086_01_06.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![正则化](img/B08086_01_06.jpg)'
- en: 'Figure 6a: Early termination'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图6a：早停法
- en: In the skinny jeans analogy as shown in *Figure 6b*, think stretch pants. They
    fit just as well, but because they're flexible, they don't make things harder
    to fit in. The stretch pants of deep learning are sometime called **L2 regularization**.
    The idea is to add another term to the loss, which penalizes large weights.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在瘦身牛仔裤类比中，如图*6b*所示，想象弹力裤。它们同样合身，但由于具有弹性，它们不会让物品更难放入其中。深度学习中的弹力裤有时被称为**L2正则化**。其思路是向损失函数中添加另一个项，从而惩罚较大的权重。
- en: '![Regularization](img/B08086_01_07.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![正则化](img/B08086_01_07.jpg)'
- en: 'Figure 6b: Stretch pant analogy of deep learning'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6b：深度学习的弹力裤类比
- en: '![Regularization](img/B08086_01_08.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![正则化](img/B08086_01_08.jpg)'
- en: 'Figure 6c: L2 tegularization'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6c：L2正则化
- en: Currently, in deep learning practice, the widely used approach for preventing
    overfitting is to feed lots of data into the deep network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在深度学习实践中，防止过拟合的广泛使用的方法是将大量数据输入到深度网络中。
- en: Shared weights and pooling
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享权重与池化
- en: Let say an image has a cat in it and it doesn't really matter where the cat
    is in the image, as it's still an image with a cat. If the network has to learn
    about cats in the left corner and about cats in the right corner independently,
    that's a lot of work that it has to do. But objects and images are largely the
    same whether they're on the left or on the right of the picture. That's what's
    called **translation invariance**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一张图像中有一只猫，而猫的位置不重要，因为它仍然是一张包含猫的图片。如果网络必须独立学习左上角和右上角的猫，那就需要做很多工作。但无论对象或图像位于图片的左侧还是右侧，它们在本质上是相同的。这就叫做**平移不变性**。
- en: 'The way of achieving this in networks is called **weight sharing**. When networks
    know that two inputs can contain the same kind of information, then it can share
    the weights and train the weights jointly for those inputs. It is a very important
    idea. Statistical invariants are things that don''t change on average across time
    or space, and are everywhere. For images, the idea of weight sharing will get
    us to study convolutional networks. For text and sequences in general, it will
    lead us to recurrent neural networks:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中实现这一点的方法叫做**权重共享**。当网络知道两个输入可以包含相同类型的信息时，就可以共享权重，并共同训练这些输入的权重。这是一个非常重要的概念。统计不变性是指在时间或空间上平均而言不发生变化的事物，并且它们无处不在。对于图像来说，权重共享的概念将引导我们研究卷积神经网络。对于文本和一般的序列，它将引导我们研究循环神经网络：
- en: '![Shared weights and pooling](img/B08086_01_09.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![共享权重与池化](img/B08086_01_09.jpg)'
- en: 'Figure 7a: Translation variance'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7a：平移不变性
- en: '![Shared weights and pooling](img/B08086_01_10.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![共享权重与池化](img/B08086_01_10.jpg)'
- en: 'Figure 7b: Weight sharing'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图7b：权重共享
- en: To reduce the spatial extent of the feature maps in the convolutional pyramid,
    a very small stride could run and take all the convolutions in a neighborhood
    and combine them somehow. This is known as **pooling**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少卷积金字塔中特征图的空间范围，可以使用非常小的步幅，并在邻域内进行所有卷积操作，并以某种方式将它们结合起来。这被称为**池化**。
- en: In max-pooling as shown in *Figure 7d*, at every point in the feature map, look
    at a small neighborhood around that point and compute the maximum of all the responses
    around it. There are some advantages to using max pooling. First, it doesn't add
    to your number of parameters. So, you don't risk an increasing over fitting. Second,
    it simply often yields more accurate models. However, since the convolutions that
    run below run at a lower stride, the model then becomes a lot more expensive to
    compute. Max-pooling extracts the most important feature, whereas average pooling
    sometimes can't extract good features because it takes all into account and results
    in an average value that may/may not be important for object detection-type tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大池化中，如*图7d*所示，在特征图的每个点上，查看该点周围的小邻域，并计算该邻域内所有响应的最大值。使用最大池化有一些优势。首先，它不会增加你的参数数量，因此不会导致过拟合的风险。其次，它通常能产生更准确的模型。然而，由于卷积操作在较低的步幅下运行，模型的计算成本会大幅增加。最大池化提取最重要的特征，而平均池化有时无法提取好的特征，因为它会考虑所有特征并产生一个平均值，而这个平均值可能对物体检测类任务来说并不重要。
- en: '![Shared weights and pooling](img/B08086_01_11.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![共享权重与池化](img/B08086_01_11.jpg)'
- en: 'Figure 7c: Pooling'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7c：池化
- en: '![Shared weights and pooling](img/B08086_01_12.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![共享权重与池化](img/B08086_01_12.jpg)'
- en: 'Figure 7d: Max and average pooling'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图7d：最大池化与平均池化
- en: Local receptive field
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部感受野
- en: 'A simple way to encode the local structure is to connect a submatrix of adjacent
    input neurons into one single hidden neuron belonging to the next layer. That
    single hidden neuron represents one local receptive field. Let''s consider CIFAR-10
    images that have an input feature of size [32 x 32 x 3]. If the receptive field
    (or the filter size) is 4 x 4, then each neuron in the convolution layer will
    have weights to a [4 x 4 x 3] region in the input feature, for a total of 4*4*3
    = 48 weights (and +1 bias parameter). The extent of the connectivity along the
    depth axis must be 3, since this is the depth (or number of channel: RGB) of the
    input feature.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的编码局部结构的方法是将相邻输入神经元的子矩阵连接成一个属于下一层的单一隐藏神经元。这个单一的隐藏神经元代表一个局部感受野。假设我们考虑CIFAR-10图像，其输入特征为[32
    x 32 x 3]。如果感受野（或滤波器大小）为4 x 4，那么卷积层中的每个神经元将有权重连接到输入特征中的[4 x 4 x 3]区域，总共有4*4*3
    = 48个权重（再加一个偏置参数）。沿深度轴的连接程度必须为3，因为这是输入特征的深度（或通道数：RGB）。
- en: Convolutional network (ConvNet)
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNet）
- en: '**Convolutional Networks** (**ConvNets**) are neural networks that share their
    parameters/weights across space. An image can be represented as a flat pancake
    that has width, height, and depth or number of channel (for RGB: having red, green,
    and blue channel the depth is 3, whereas for grayscale the depth is 1).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**ConvNets**）是通过在空间上共享其参数/权重的神经网络。一幅图像可以表示为一个平的煎饼，具有宽度、高度和深度或通道数（对于RGB图像，深度是3，表示红、绿、蓝三个通道，而对于灰度图像，深度是1）。'
- en: Now let's slide a tiny neural network with *K* outputs across the image without
    changing the weights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将一个输出为*K*的小型神经网络滑过图像，且不改变权重。
- en: '![Convolutional network (ConvNet)](img/B08086_01_13.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络（ConvNet）](img/B08086_01_13.jpg)'
- en: 'Figure 8a: Weight sharing across space'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图8a：空间上的权重共享
- en: '![Convolutional network (ConvNet)](img/B08086_01_14.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络（ConvNet）](img/B08086_01_14.jpg)'
- en: 'Figure 8b: Convolutional pyramid with layers of convolution'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8b：具有卷积层的卷积金字塔
- en: On the output, a different image will be drawn with different width, different
    height, and different depth (from just R, G, B color channels to *K* number of
    channels). This operation is known as convolution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出端，将绘制一幅不同的图像，具有不同的宽度、不同的高度和不同的深度（从仅有的RGB颜色通道到*K*个通道）。这个操作被称为卷积。
- en: A ConvNet is going to basically be a deep network with layers of convolutions
    that stack together to form a pyramid like structure. You can see from the preceding
    figure that the network takes an image as an input of dimension (width x height
    x depth) and then applys convolutions progressively over it to reduce the spatial
    dimension while increasing the depth, which is roughly equivalent to its semantic
    complexity. Let's understand some of the common terminology in convent.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNet）基本上是一个深度网络，包含多个卷积层，这些卷积层堆叠在一起形成类似金字塔的结构。从前面的图可以看到，网络将图像作为输入（维度为宽度
    x 高度 x 深度），然后逐步对其应用卷积操作，以减少空间维度，同时增加深度，这大致等同于其语义复杂度。让我们理解一下卷积神经网络中的一些常见术语。
- en: 'Each layer or depth in the image stack is called a feature map and patches
    or kernels are used for mapping three feature maps to *K* feature maps. A stride
    is the number of pixels that is shifted each time you move your filter. Depending
    on the type of padding a stride of 1 makes the output roughly the same size as
    the input. A stride of 2 makes it about half the size. In the case of valid padding,
    a sliding filter don''t cross the edge of the image, whereas in same-padding it
    goes off the edge and is padded with zeros to make the output map size exactly
    the same size as the input map:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图像堆栈中的每一层或深度叫做特征图（feature map），并且使用补丁或卷积核（kernels）将三个特征图映射到*K*个特征图。步幅（stride）是指每次移动滤波器时，像素的移动数量。根据填充方式，步幅为1时，输出的大小大致与输入相同；步幅为2时，输出约为输入的一半。对于有效填充（valid
    padding），滑动滤波器不会越过图像的边缘，而在同样填充（same-padding）情况下，滤波器会越过边缘，并且用零填充，使得输出图的大小与输入图的大小完全相同：
- en: '![Convolutional network (ConvNet)](img/B08086_01_15.jpg)![Convolutional network
    (ConvNet)](img/B08086_01_16.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络（ConvNet）](img/B08086_01_15.jpg)![卷积网络（ConvNet）](img/B08086_01_16.jpg)'
- en: 'Figure 8c: Different terminology related to convolutional network'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8c：与卷积网络相关的不同术语
- en: Deconvolution or transpose convolution
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反卷积或转置卷积
- en: In the case of a computer vision application where the resolution of final output
    is required to be larger than the input, deconvolution/transposed convolution
    is the de-facto standard. This layer is used in very popular applications such
    as GAN, image super-resolution, surface depth estimation from image, optical flow
    estimation, and so on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要最终输出分辨率大于输入的计算机视觉应用，反卷积/转置卷积是事实上的标准。这一层被广泛应用于如GAN、图像超分辨率、从图像估计表面深度、光流估计等热门应用中。
- en: CNN in general performs down-sampling, that is, they produce output of a lower
    resolution than the input, whereas in deconvolution the layer up-samples the image
    to get the same resolution as the input image. Note since a naive up-sampling
    inadvertently loses details, a better option is to have a trainable up-sampling
    convolutional layer whose parameters will change during training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常执行下采样，即输出分辨率低于输入，而在反卷积中，层会对图像进行上采样，使其获得与输入图像相同的分辨率。需要注意的是，由于简单的上采样不可避免地会丢失细节，因此一个更好的选择是使用可训练的上采样卷积层，其参数会在训练过程中发生变化。
- en: 'Tensorflow method: `tf.nn.conv2d_transpose(value, filter, output_shape, strides,
    padding, name)`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow方法：`tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding,
    name)`
- en: Recurrent Neural Networks and LSTM
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归神经网络和LSTM
- en: The key idea behind **Recurrent Neural Networks** (**RNN**) is to share parameters
    over time. Imagine that you have a sequence of events, and at each point in time
    you want to make a decision about what's happened so far in this sequence. If
    the sequence is reasonably stationary, you can use the same classifier at each
    point in time. That simplifies things a lot already. But since this is a sequence,
    you also want to take into account the past-everything that happened before that
    point.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）的关键思想是共享参数。假设你有一系列事件，在每个时刻你需要根据到目前为止发生的事件做出决策。如果序列是相对稳定的，你可以在每个时刻使用相同的分类器，这样就简化了很多。但由于这是一个序列，你还需要考虑过去——即在某个时刻之前发生的所有事情。'
- en: 'RNN is going to have a single model responsible for summarizing the past and
    providing that information to your classifier. It basically ends up with a network
    that has a relatively simple repeating pattern, with part of the classifier connecting
    to the input at each time step and another part called the recurrent connection
    connecting you to the past at each step, as shown in the following figure:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RNN将拥有一个单一模型，负责总结过去的内容并将这些信息提供给分类器。它基本上形成了一个具有相对简单重复模式的网络，其中分类器的一部分在每个时间步连接到输入，而另一部分叫做递归连接，在每一步将你与过去的内容连接，如下图所示：
- en: '![Recurrent Neural Networks and LSTM](img/B08086_01_18.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![递归神经网络和LSTM](img/B08086_01_18.jpg)'
- en: 'Figure 9a: Recurrent neural network'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9a：递归神经网络
- en: '![Recurrent Neural Networks and LSTM](img/B08086_01_19.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![递归神经网络和LSTM](img/B08086_01_19.jpg)'
- en: 'Figure-9b: Long short-term memory (LSTM)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图-9b：长短期记忆（LSTM）
- en: '**LSTM** stands for **long short-term memory**. Now, conceptually, a recurrent
    neural network consists of a repetition of simple little units like this, which
    take as an input the past, a new input, and produce a new prediction and connect
    to the future. Now, what''s in the middle of that is typically a simple set of
    layers with some weights and linearities.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**代表**长短期记忆**。从概念上讲，循环神经网络由简单的重复单元组成，这些单元以过去的信息、一个新的输入为输入，产生新的预测并与未来连接。其中心通常是一组简单的层，带有一些权重和线性激活。'
- en: In LSTM as shown in *Figure 9b*, the gating values for each gate get controlled
    by a tiny logistic regression on the input parameters. Each of them has its own
    set of shared parameters. And there's an additional hyperbolic tension sprinkled
    to keep the outputs between -1 and 1\. Also it's differentiable all the way, which
    means it can optimize the parameters very easily. All these little gates help
    the model keep its memory for longer when it needs to, and ignore things when
    it should.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM中，如*图9b*所示，每个门的门控值都由输入参数上的一个小型逻辑回归控制。每个门都有自己的一组共享参数。此外，还额外加入了双曲正切函数，以保持输出值在-1和1之间。并且它是可微分的，这意味着它可以很容易地优化参数。所有这些小门帮助模型在需要时保持更长时间的记忆，并在应该时忽略不重要的内容。
- en: Deep neural networks
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: The central idea of deep learning is to add more layers and make your model
    deeper. There are lots of good reasons to do that. One is parameter efficiency.
    You can typically get much more performance with fewer parameters by going deeper
    rather than wider.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的核心思想是增加更多的层次，使模型更深。这样做有很多好的理由。其中之一就是参数效率。通常，通过增加网络深度而非宽度，你可以用更少的参数获得更好的性能。
- en: Another one is that a lot of the natural phenomena that you might be interested
    in, tend to have a hierarchical structure, which deep models naturally capture.
    If you poke at a model for images, for example, and visualize what the model learns,
    you'll often find very simple things at the lowest layers, such as lines or edges.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是，许多你可能感兴趣的自然现象，通常具有层次结构，而深度模型自然能够捕捉这一点。例如，如果你探究一个图像模型，并可视化模型学习到的内容，你通常会发现在最底层学到的是非常简单的东西，比如线条或边缘。
- en: '![Deep neural networks](img/B08086_01_20.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![深度神经网络](img/B08086_01_20.jpg)'
- en: 'Figure 10a: Deep neural networks'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10a：深度神经网络
- en: '![Deep neural networks](img/B08086_01_21.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![深度神经网络](img/B08086_01_21.jpg)'
- en: 'Figure 10b: Network layers capturing hierarchical structure of image'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10b：网络层捕捉图像的层次结构
- en: A very typical architecture for a ConvNet is a few layers alternating convolutions
    and max pooling, followed by a few fully connected layers at the top. The first
    famous model to use this architecture was LeNet-5 designed by Yann Lecun for character
    recognition back in 1998.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNet）的一种典型架构是几层交替进行卷积和最大池化，最后顶部接几层全连接层。第一个使用这种架构的著名模型是1998年Yann Lecun为字符识别设计的LeNet-5。
- en: Modern convolutional networks such as AlexNet, which famously won the competitive
    ImageNet object recognition challenge in 2012, use a very similar architecture
    with a few wrinkles. Another notable form of pooling is average pooling. Instead
    of taking the max, just take an average over the window of pixels around a specific
    location.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现代卷积神经网络，如AlexNet，在2012年著名赢得了ImageNet物体识别挑战赛，采用了与之非常相似的架构，并进行了一些调整。另一种值得注意的池化方式是平均池化。与其选择最大值，不如在特定位置周围的像素窗口上取平均值。
- en: Discriminative versus generative models
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判别式模型与生成式模型
- en: A discriminative model learns the conditional probability distribution *p(y|x)*
    which could be interpreted as the *probability of y given x*. A discriminative
    classifier learns by observing data. It makes fewer assumptions on the distributions,
    but depends heavily on the quality of the data. The distribution *p(y|x)* simply
    classifies a given example x directly into a label *y*. For example, in logistic
    regression all we have to do is to learn weights and bias that would minimize
    the squared loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 判别式模型学习条件概率分布*p(y|x)*，这可以解释为*x给定y的概率*。判别式分类器通过观察数据来学习。它对分布的假设较少，但严重依赖数据的质量。分布*p(y|x)*简单地将给定的样本x直接分类到标签*y*中。例如，在逻辑回归中，我们所要做的就是学习能够最小化平方损失的权重和偏差。
- en: Whereas a generative model learns the joint probability distribution *p(x,y)*,
    where *x* is the input data and *y* is the label that you want to classify. A
    generative model can generate more samples by itself artificially, based on assumptions
    about the distribution of data. For example, in the Naive Bayes' model, we can
    learn *p(x)* from data, also *p(y)*, the prior class probabilities, and we can
    also learn *p(x|y)* from the data using say maximum likelihood.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 而生成模型学习的是联合概率分布*p(x,y)*，其中*x*是输入数据，*y*是你希望分类的标签。生成模型可以根据对数据分布的假设，自行生成更多样本。例如，在朴素贝叶斯模型中，我们可以从数据中学习*p(x)*，还可以学习*p(y)*，即先验类别概率，我们也可以使用最大似然估计从数据中学习*p(x|y)*。
- en: 'Once we have *p(x)*, *p(y)* and *p(x|y)*, *p(x, y)* is not difficult to find
    out. Now using Bayes'' rule, we can replace the *p(y|x)* with *(p(x|y)p(y))/p(x)*.
    And since we are just interested in the *arg max*, the denominator can be removed,
    as that will be the same for every *y*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了*p(x)*，*p(y)*和*p(x|y)*，那么*p(x, y)*就不难找出来了。现在使用贝叶斯定理，我们可以将*p(y|x)*替换为*(p(x|y)p(y))/p(x)*。由于我们只关心*arg
    max*，分母可以被去掉，因为对于每个*y*来说它都是相同的：
- en: '![Discriminative versus generative models](img/B08086_01_25.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![判别模型与生成模型](img/B08086_01_25.jpg)'
- en: This is the equation we use in generative models, as *p(x, y) = p(x | y) p(y)*,
    which explicitly models the actual distribution of each class.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在生成模型中使用的方程，*p(x, y) = p(x | y) p(y)*，它明确地建模了每个类别的实际分布。
- en: In practice, the discriminative models generally outperform generative models
    in classification tasks, but the generative model shines over discriminative models
    in creativity/generation tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，判别模型通常在分类任务中优于生成模型，但在创作/生成任务中，生成模型则胜过判别模型。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: So far you have refreshed various concepts related to deep learning and also
    learned how deep networks have evolved from the arena of supervised tasks of classifying
    an image, recognizing voice, text, and so on, towards the creative power through
    generative model. In the next chapter we will see how deep learning can be used
    for performing wonderful creativity tasks in the unsupervised domain using **Generative
    Adversarial Networks** (**GANs**).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经刷新了与深度学习相关的各种概念，并且学习了深度网络如何从监督任务的领域（例如分类图像、语音识别、文本等）演变到通过生成模型展现创作能力。在下一章中，我们将看到深度学习如何在无监督领域中利用**生成对抗网络**（**GANs**）执行精彩的创作任务。
