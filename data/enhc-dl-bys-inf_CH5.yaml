- en: Chapter 5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章
- en: Principled Approaches for Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习的原理方法
- en: Now that we’ve introduced the concept of **Bayesian Neural Networks** (**BNNs**),
    we’re ready to explore the various ways in which they can be implemented. As we
    discussed previously, ideal BNNs are computationally intensive, becoming intractable
    with more sophisticated architectures or larger amounts of data. In recent years,
    researchers have developed a range of methods that make BNNs tractable, allowing
    them to be implemented with larger and more sophisticated neural network architectures.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了**贝叶斯神经网络**（**BNN**）的概念，接下来我们准备探索实现它们的各种方法。正如我们之前讨论的，理想的BNN计算量巨大，随着更复杂的架构或更大数据量，变得不可处理。近年来，研究人员开发了一系列方法，使得BNN变得可处理，从而能够在更大且更复杂的神经网络架构中实现。
- en: 'In this chapter, we’ll explore two particularly popular methods: **Probabilistic**
    **Backpropagation** (**PBP**) and **Bayes by Backprop** (**BBB**). Both methods
    can be referred to as *probabilistic neural network models*: neural networks designed
    to learn probabilities over their weights, rather than simply learning point estimates
    (a fundamental defining feature of BNNs, as we learned in [*Chapter 4*](CH4.xhtml#x1-490004),
    [*Introducing Bayesian Deep Learning*](CH4.xhtml#x1-490004)). Because they explicitly
    learn distributions over the weights at training time, we refer to them as *principled*
    methods; in contrast to the methods we’ll explore in the next chapter, which more
    loosely approximate Bayesian inference with neural networks. We’ll cover these
    topics in the following sections of this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两种特别流行的方法：**概率反向传播**（**PBP**）和**贝叶斯反向传播**（**BBB**）。这两种方法都可以称为*概率神经网络模型*：旨在学习其权重的概率，而不仅仅是学习点估计（这是BNN的一个基本特征，正如我们在[*第4章*](CH4.xhtml#x1-490004)中学习的那样，[*介绍贝叶斯深度学习*](CH4.xhtml#x1-490004)）。因为它们在训练时显式地学习权重的分布，所以我们称之为*原理性*方法；与我们将在下一章探讨的方法相比，后者更加宽松地近似贝叶斯推断与神经网络的结合。我们将在本章的以下部分讨论这些主题：
- en: Explaining notation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号说明
- en: Familiar probabilistic concepts from deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中的常见概率概念
- en: Bayesian inference by backpropagation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过反向传播的贝叶斯推断
- en: Implementing BBB with TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现BBB
- en: Scalable Bayesian deep learning with PBP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PBP的可扩展贝叶斯深度学习
- en: Implementing PBP
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现PBP
- en: First, let’s quickly review the technical requirements for this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速回顾一下本章的技术要求。
- en: 5.1 Technical requirements
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 技术要求
- en: 'To complete the practical tasks in this chapter, you will need a Python 3.8
    environment with the Python SciPy stack and the following additional Python packages
    installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的实际任务，你将需要一个安装了Python 3.8环境以及Python SciPy栈和以下附加Python包的环境：
- en: TensorFlow 2.0
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0
- en: TensorFlow Probability
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow概率
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)。
- en: 5.2 Explaining notation
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 符号说明
- en: 'While we’ve introduced much of the notation used throughout the book in the
    previous chapters, we’ll be introducing more notation associated with BDL in the
    following chapters. As such, we’ve provided an overview of the notation here for
    reference:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在前几章中介绍了书中使用的大部分符号，但在接下来的章节中，我们将介绍与BDL相关的更多符号。因此，我们在这里提供了符号的概述，供参考：
- en: '*μ*: The mean. To make it easy to cross-reference our chapter with the original
    Probabilistic Backpropagation paper, this is represented as *m* when discussing
    PBP.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*μ*：均值。为了方便将本章与原始的概率反向传播论文进行交叉引用，讨论PBP时，这个符号用*m*表示。'
- en: '*σ*: The standard deviation.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*：标准差。'
- en: '*σ*²: The variance (meaning the square of the standard deviation). To make
    it easy to cross-reference our chapter with the paper, this is represented as
    *v* when discussing PBP.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*²：方差（即标准差的平方）。为了方便将本章与论文进行交叉引用，在讨论PBP时，使用*v*表示。'
- en: '**x**: A single vector input to our model. If considering multiple inputs,
    we’ll use **X** to represent a matrix comprising multiple vector inputs.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x**：输入模型的单一向量。如果考虑多个输入，我们将使用**X**表示由多个向量输入组成的矩阵。'
- en: '**x**: An approximation of our input **x**.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x**：我们输入的近似值**x**。'
- en: '*y*: A single scalar target. When considering multiple targets, we’ll use **y**
    to represent a vector of multiple scalar targets.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*：单个标量目标。当考虑多个目标时，我们将使用 **y** 来表示多个标量目标的向量。'
- en: '*ŷ*: A single scalar output from our model. When considering multiple outputs,
    we’ll use **ŷ** to represent a vector of multiple scalar outputs.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ*：来自我们模型的单个标量输出。当考虑多个输出时，我们将使用 **ŷ** 来表示多个标量输出的向量。'
- en: '**z**: The output of an intermediate layer of our model.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**z**：我们模型中间层的输出。'
- en: '*P*: Some ideal or target distribution.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*：某个理想或目标分布。'
- en: '*Q*: An approximate distribution.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q*：近似分布。'
- en: '*KL*[*Q*∥*P*]: The KL preergence between our target distribution *P* and our
    approximate distribution *Q*.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*KL*[*Q*∥*P*]：我们的目标分布 *P* 和近似分布 *Q* 之间的 KL 散度。'
- en: 'ℒ: The loss.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ℒ：损失。
- en: ': The expectation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ：期望。
- en: '*N*(*μ,σ*): A normal (or Gaussian) distribution parameterized by the mean *μ*
    and the standard deviation *σ*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*(*μ,σ*)：一个由均值 *μ* 和标准差 *σ* 参数化的正态（或高斯）分布。'
- en: '*𝜃*: A set of model parameters.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*𝜃*：一组模型参数。'
- en: 'Δ: A gradient.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Δ：梯度。
- en: '*∂*: A partial derivative.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*∂*：偏导数。'
- en: '*f*(): Some function (e.g. *y* = *f*(*x*) indicates that *y* is produced by
    applying function *f*() to input *x*).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*()：某个函数（例如 *y* = *f*(*x*) 表示 *y* 是通过对输入 *x* 应用函数 *f*() 得到的）。'
- en: We will encounter different variations of this notation, using different subscripts
    or variable combinations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遇到不同变体的符号，使用不同的下标或变量组合。
- en: 5.3 Familiar probabilistic concepts from deep learning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 深度学习中的常见概率概念
- en: While this book introduces many concepts that may be unfamiliar, you may find
    that some ideas discussed here are familiar. In particular, **Variational** **Inference**
    (**VI**) is something you may be familiar with due to its use in **Variational
    Autoencoders** (**VAEs**).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍了许多可能不熟悉的概念，但你可能会发现这里讨论的一些想法是你已经熟悉的。特别是，**变分推断**（**VI**）可能因为在**变分自编码器**（**VAE**）中的应用而为你所熟知。
- en: As a quick refresher, VAEs are generative models that learn encodings that can
    be used to generate plausible data. Much like standard autoencoders, VAEs comprise
    an encoder-decoder architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾一下，VAE 是一种生成模型，它学习可以用于生成合理数据的编码。与标准自编码器类似，VAE 也采用编码器-解码器架构。
- en: '![PIC](img/file107.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file107.png)'
- en: 'Figure 5.1: Illustration of autoencoder architecture'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：自编码器架构的示意图
- en: With a standard autoencoder, the model learns a mapping from the encoder to
    the latent space, and then from the latent space to the decoder.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准自编码器，模型学习从编码器到潜在空间的映射，然后从潜在空间到解码器的映射。
- en: 'As we see here, our output is simply defined as **x** = *f*[*d*](**z**), where
    our encoding **z** is simply: **z** = *f*[*e*](**x**), where *f*[*e*]() and *f*[*d*]()
    are our encoder and decoder functions, respectively. If we want to generate new
    data using values in our latent space, we could simply inject some random values
    into the input of our decoder; bypassing the encoder and randomly sampling from
    our latent space:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们的输出简单地定义为 **x** = *f*[*d*](**z**)，其中我们的编码 **z** 定义为：**z** = *f*[*e*](**x**)，其中
    *f*[*e*]() 和 *f*[*d*]() 分别是我们的编码器和解码器函数。如果我们想使用潜在空间中的值生成新数据，我们可以通过向解码器输入注入一些随机值；绕过编码器，直接从潜在空间中随机采样：
- en: '![PIC](img/file108.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file108.png)'
- en: 'Figure 5.2: Illustration of sampling from the latent space of a standard autoencoder'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：从标准自编码器的潜在空间中采样的示意图
- en: The problem with this approach is that a standard autoencoder doesn’t do a great
    job of learning the structure of the latent space. This means that while we’re
    free to randomly sample points in this space, there’s no guarantee that those
    points will correspond to something that can be processed by the decoder to generate
    plausible data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的问题在于，标准自编码器在学习潜在空间的结构上表现不佳。这意味着，虽然我们可以自由地在该空间中随机采样点，但无法保证这些点能够被解码器处理，生成合理的数据。
- en: In a VAE, the latent space is modeled as a distribution. Therefore, what was
    **z** = *f*[*e*](**x**) becomes **z** ≈𝒩(*μ*[x], *σ*[x]); that is to say, our
    latent space **z** now becomes a Gaussian distribution conditioned on our input
    **x**. Now, when we want to generate data using our trained network, we can do
    so simply by sampling from a normal distribution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VAE 中，潜在空间被建模为一个分布。因此，**z** = *f*[*e*](**x**) 变为 **z** ≈𝒩(*μ*[x], *σ*[x])；也就是说，我们的潜在空间
    **z** 现在变成了一个条件于输入 **x** 的高斯分布。现在，当我们想使用训练好的网络生成数据时，我们可以简单地从正态分布中采样。
- en: 'To achieve this, we need to ensure that the latent space approximates a Gaussian
    distribution. To do so, we use the **Kullback-Leibler divergence** (or KL divergence)
    during training by incorporating it as a regularization term:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们需要确保潜在空间近似一个高斯分布。为此，我们在训练过程中使用**Kullback-Leibler散度**（或KL散度），并将其作为正则化项加入：
- en: '![ 2 ℒ = ∥x− ˆx ∥ + KL [Q ∥P] ](img/file109.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 ℒ = ∥x− ˆx ∥ + KL [Q ∥P] ](img/file109.jpg)'
- en: 'Here, *P* is our target distribution (in this case, a multivariate Gaussian
    distribution), which we’re trying to approximate with *Q*, which is the distribution
    associated with our latent space, which in this case is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*P* 是我们的目标分布（在此情况下是多变量高斯分布），我们正试图用*Q*来逼近它，*Q*是与我们潜在空间相关的分布，在此情况下如下所示：
- en: '![Q = z ≈ 𝒩 (μ,σ) ](img/file110.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Q = z ≈ 𝒩 (μ,σ) ](img/file110.jpg)'
- en: 'So, our loss now becomes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的损失现在变为：
- en: '![ℒ = ∥x− ˆx ∥2 + KL [q(z|x)∥p(z )] ](img/file111.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![ℒ = ∥x− ˆx ∥2 + KL [q(z|x)∥p(z )] ](img/file111.jpg)'
- en: 'We can expand it as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下扩展：
- en: '![ℒ = ∥x − ˆx∥2 + KL [𝒩 (μ,σ)∥𝒩 (0,I)] ](img/file112.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![ℒ = ∥x − ˆx∥2 + KL [𝒩 (μ,σ)∥𝒩 (0,I)] ](img/file112.jpg)'
- en: 'Here, *I* is the identity matrix. This will allow our latent space to converge
    on our Gaussian prior, while also minimizing the reconstruction loss. The KL divergence
    can additionally be rewritten as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*I* 是单位矩阵。这将使我们的潜在空间能够收敛到我们的高斯先验，同时最小化重构损失。KL散度还可以重新写成如下形式：
- en: '![KL [q(z|x )∥p (z)] =q (z|x) logq(z|x )− q(z|x) log p(z) ](img/file113.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![KL [q(z|x )∥p (z)] =q (z|x) logq(z|x )− q(z|x) log p(z) ](img/file113.jpg)'
- en: The terms on the right hand side of our equation here are the expectation (or
    mean) of log *q*(**z**|**x**) and log *p*(**z**). As we know from [*Chapter 2*](CH2.xhtml#x1-250002),
    [*Fundamentals of* *Bayesian Inference*](CH2.xhtml#x1-250002) and [*Chapter 4*](CH4.xhtml#x1-490004),
    [*Introducing Bayesian Deep Learning*](CH4.xhtml#x1-490004), we can obtain the
    the expectation of a given distribution by sampling. Thus, as we can see that
    all terms of our KL divergence are expectations computed with respect to our approximate
    distribution *q*(**z**|**x**), we can approximate our KL divergence by sampling
    from *q*(**z**|**x**), which is exactly what we’re about to do!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方程右侧的项是对数*q*(**z**|**x**)和对数*p*(**z**)的期望（或均值）。正如我们从[*第2章*](CH2.xhtml#x1-250002)、[*贝叶斯推断基础*](CH2.xhtml#x1-250002)和[*第4章*](CH4.xhtml#x1-490004)、[*引入贝叶斯深度学习*](CH4.xhtml#x1-490004)中了解到的那样，我们可以通过采样来获得给定分布的期望。因此，正如我们所见，KL散度的所有项都是相对于我们近似分布*q*(**z**|**x**)计算的期望，我们可以通过从*q*(**z**|**x**)中采样来近似我们的KL散度，这正是我们接下来要做的！
- en: 'Now that our encoding is represented by the distribution shown in equation
    [5.3](#x1-63006r3), our neural network structure has to change. We need to learn
    the mean (*μ*) and standard deviation (*σ*) parameters of our distribution:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的编码由方程[5.3](#x1-63006r3)中显示的分布表示，我们的神经网络结构必须发生变化。我们需要学习我们分布的均值（*μ*）和标准差（*σ*）参数：
- en: '![PIC](img/file114.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file114.png)'
- en: 'Figure 5.3: Illustration of autoencoder architecture with mean and standard
    deviation weights'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：带有均值和标准差权重的自动编码器架构示意图
- en: The issue with constructing a VAE in this way is that our encoding *z* is now
    stochastic, rather than deterministic. This is a problem because we can’t obtain
    a gradient for stochastic variables – and if we can’t obtain a gradient, we have
    nothing to backpropagate – so we can’t learn!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式构建VAE的问题在于我们的编码*z*现在是随机的，而不是确定性的。这是一个问题，因为我们无法为随机变量获得梯度——如果我们无法获得梯度，就无法进行反向传播——因此我们无法进行学习！
- en: 'We can fix this using something called the **reparameterization trick**. The
    reparameterization trick involves modifying how we compute **z**. Instead of sampling
    **z** from our distribution parameters, we will define it as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一种叫做**重参数化技巧**的方法来解决这个问题。重参数化技巧涉及到修改我们计算**z**的方式。我们不再从分布参数中抽样**z**，而是将其定义为如下：
- en: '![z = μ + σ ⊙ 𝜖 ](img/file115.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![z = μ + σ ⊙ 𝜖 ](img/file115.jpg)'
- en: 'As you can see, we’ve introduced a new variable, *𝜖*, which is sampled from
    a Gaussian distribution with *μ* = 0 and *σ* = 1:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们引入了一个新的变量，*𝜖*，它是从一个均值为0，标准差为1的高斯分布中采样的：
- en: '![𝜖 = 𝒩 (0,1) ](img/file116.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![𝜖 = 𝒩 (0,1) ](img/file116.jpg)'
- en: 'Introducing *𝜖* has allowed us to move the stochasticity out of our backpropagation
    path. With the stochasticity residing solely in *𝜖*, we’re able to backpropagate
    through our weights as normal:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 引入*𝜖*使我们能够将随机性移出反向传播路径。由于随机性仅存在于*𝜖*中，我们能够像正常情况一样通过权重进行反向传播：
- en: '![PIC](img/file117.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file117.png)'
- en: 'Figure 5.4: Illustration of typical VAE architecture with mean and standard
    deviation weights, having moved the sampling component out of the backpropagation
    path'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：典型 VAE 架构的示意图，其中包含均值和标准差权重，并将采样组件移出了反向传播路径
- en: 'This means we’re able to represent our encoding as a distribution while still
    being able to backpropagate the gradient of *z*: learning the parameters *μ* and
    *σ*, and using *𝜖* to sample from the distribution. Being able to represent *z*
    as a distribution means we’re able to use it to compute the KL divergence, allowing
    us to incorporate our regularization term in equation 5.1, which in turn allows
    our embedding to converge towards a Gaussian distribution during training.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们能够将编码表示为一个分布，同时仍然能够反向传播 *z* 的梯度：学习 *μ* 和 *σ* 的参数，并使用 *𝜖* 从分布中进行采样。能够将
    *z* 表示为分布意味着我们能够利用它来计算 KL 散度，从而将正则化项纳入方程 5.1，这反过来又使我们的嵌入在训练过程中向高斯分布收敛。
- en: These are the fundamental steps in variational learning, and are what turn our
    standard autoencoder into a VAE. But this isn’t all about learning. Crucially
    for VAEs, because we’ve learned a normally distributed latent space, we can now
    sample effectively from that latent space, enabling us to use our VAE to generate
    new data according to the data landscape learned during training. Unlike the brittle
    random sampling we had with a standard autoencoder, our VAE is now able to generate
    *plausible* data!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是变分学习的基本步骤，它们将我们的标准自编码器转变为 VAE。但这不仅仅是关于学习的。对于 VAE 来说，至关重要的是，因为我们已经学到了一个正态分布的潜在空间，我们现在可以从这个潜在空间有效地进行采样，使得我们可以使用
    VAE 根据训练期间学到的数据景观生成新数据。与标准自编码器中的脆弱随机采样不同，我们的 VAE 现在能够生成*合理*的数据！
- en: To do this, we sample *𝜖* from a normal distribution and multiply *σ* by this
    value. This gives us a sample of *z* to pass through our decoder, obtaining our
    generated data, **x**, at the output.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们从正态分布中采样 *𝜖* 并将 *σ* 与该值相乘。这将给我们一个 *z* 的样本，传递给解码器，从而在输出端获得我们生成的数据，**x**。
- en: Now that we’re familiar with the fundamentals of variational learning, in the
    next section we’ll see how these principles can be applied to create BNNs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了变分学习的基础，在下一节中，我们将看到如何将这些原理应用于创建 BNN。
- en: 5.4 Bayesian inference by backpropagation
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 通过反向传播进行贝叶斯推断
- en: In their 2015 paper, *Weight Uncertainty in Neural Networks*, Charles Blundell
    and his colleagues at DeepMind introduced a method for using variational learning
    for Bayesian inference with neural networks. Their method, which learned the BNN
    parameters via standard backpropagation, was appropriately named **Bayes by Backprop**
    (**BBB**).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2015 年的论文《神经网络中的权重不确定性》中，Charles Blundell 及其在 DeepMind 的同事们提出了一种使用变分学习进行神经网络贝叶斯推断的方法。他们的方法通过标准反向传播来学习
    BNN 参数，并且这个方法被恰当地命名为**贝叶斯反向传播**（**BBB**）。
- en: 'In the previous section, we saw how we can use variational learning to estimate
    the posterior distribution of our encoding, *z*, learning *P*(*z*|*x*). For BBB,
    we’re going to be doing very much the same thing, except this time it’s not just
    the encoding we care about. This time we want to learn the posterior distribution
    over all of the parameters (or weights) of our network: *P*(*𝜃*|*D*).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到了如何使用变分学习来估计我们编码的后验分布 *z*，学习 *P*(*z*|*x*)。对于 BBB，我们将做非常类似的事情，只是这一次我们不仅仅关心编码。我们这次要学习的是所有参数（或权重）的后验分布：*P*(*𝜃*|*D*)。
- en: 'You can think of this as having an entire network made up of VAE encoding layers,
    looking something like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其看作是一个由 VAE 编码层组成的整个网络，类似于下面这样：
- en: '![PIC](img/file118.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file118.png)'
- en: 'Figure 5.5: Illustration of BBB'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：BBB 的示意图
- en: 'As such, it’s logical that the learning strategy is also similar to that which
    we used for the VAE. We again use the principle of variational learning to learn
    parameters for *Q*, and approximation of the true distribution *P*, but this time
    we’re looking for the parameters *𝜃*^(*⋆*) that minimize this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，学习策略也与我们为 VAE 使用的策略类似，这是合乎逻辑的。我们再次使用变分学习的原理来学习 *Q* 的参数，并近似真实分布 *P*，但这一次我们要寻找的是最小化以下内容的参数
    *𝜃*^(*⋆*)：
- en: '![𝜃⋆ = 𝜃 KL [q(w |𝜃)||P(w |D)] ](img/file119.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![𝜃⋆ = 𝜃 KL [q(w |𝜃)||P(w |D)] ](img/file119.jpg)'
- en: 'Here, *D* is our data, **w** is our network weights, and *𝜃* is the parameters
    of our distribution, e.g. *μ* and *σ* in the case of a Gaussian distribution.
    To do this we make use of an important cost function in Bayesian learning: the
    **Evidence Lower** **Bound**^([1](#footnote1)) , or **ELBO** (also referred to
    as the variational free energy). We denote this with the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*D* 是我们的数据，**w** 是我们的网络权重，*𝜃* 是我们的分布参数，例如在高斯分布中，*μ* 和 *σ* 是参数。为了做到这一点，我们使用了贝叶斯学习中的一个重要成本函数：**证据下界**（**ELBO**，也叫变分自由能）。我们用以下公式表示：
- en: '![ℒ(D,𝜃 ) = KL [q(w |𝜃)||P (w)]− q(w |𝜃) [log P(D |w)] ](img/file120.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![ℒ(D,𝜃 ) = KL [q(w |𝜃)||P (w)]− q(w |𝜃) [log P(D |w)] ](img/file120.jpg)'
- en: 'This looks rather complicated, but it’s really just a generalization of what
    we saw in equation 5.4\. We can break it down as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当复杂，但其实它只是我们在方程 5.4 中看到的内容的一种概括。我们可以按以下方式进行拆解：
- en: On the left-hand side, we have the KL divergence between our prior *P*(**w**)
    and our approximate distribution *q*(**w**|*𝜃*). This is similar to what we saw
    in equations 5.1-5.4 in the previous section. Incorporating the KL divergence
    in our loss allows us to tune our parameters *𝜃* such that our approximate distribution
    converges on our prior distribution.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左边，我们有先验 *P*(**w**) 和近似分布 *q*(**w**|*𝜃*) 之间的 KL 散度。这与我们在上一节的方程 5.1-5.4 中看到的内容类似。在损失中加入
    KL 散度使得我们可以调整参数 *𝜃*，使得我们的近似分布收敛到先验分布。
- en: On the right-hand side, we have the expectation of the negative log-likelihood
    of our data *D* given our neural network weights **w** with respect to the variational
    distribution. Minimizing this (because it’s the *negative log-likelihood*) ensures
    that we learn parameters that maximize the likelihood of our data given our weights;
    our network learns to map our inputs to our outputs.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右边，我们有关于给定神经网络权重 **w** 和变分分布的情况下，数据 *D* 的负对数似然的期望值。最小化这个（因为它是*负对数似然*）可以确保我们学习到的参数能最大化给定权重情况下数据的似然；我们的网络学会将输入映射到输出。
- en: 'Just as with VAEs, BBB makes use of the reparameterization trick to allow us
    to backpropagate gradients through our network parameters. Also as before, we
    sample from our distribution. Taking the form of the KL divergence introduced
    in equation 5.5, our loss becomes as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 VAE 一样，BBB 使用了重参数化技巧，使得我们能够通过网络参数反向传播梯度。和之前一样，我们从分布中采样。根据方程 5.5 中引入的 KL 散度形式，我们的损失函数变为如下：
- en: '![ ∑N ℒ (D,𝜃) ≈ logq(wi |𝜃)− log P(wi) − log P(D |wi) i=1 ](img/file121.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑N ℒ (D,𝜃) ≈ logq(wi |𝜃)− log P(wi) − log P(D |wi) i=1 ](img/file121.jpg)'
- en: '*N* is the number of samples, and *i* denotes a particular sample. While we’ll
    be using Gaussian priors here, an interesting property of this approach is that
    it can be applied to a wide range of distributions.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*N* 是样本的数量，*i* 表示某个特定样本。虽然我们这里使用的是高斯先验，但这个方法的一个有趣特点是，它可以应用于各种分布。'
- en: 'The next step is to use our weight samples to train our network:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用我们的权重样本来训练网络：
- en: 'First, just as with VAEs, we sample *𝜖* from a Gaussian distribution:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，就像在 VAE 中一样，我们从高斯分布中采样 *𝜖*：
- en: '![𝜖 ≈ 𝒩 (0,I) ](img/file122.jpg)'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![𝜖 ≈ 𝒩 (0,I) ](img/file122.jpg)'
- en: 'Next, we apply *𝜖* to the weights in a particular layer, just as with our VAE
    encoding:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将 *𝜖* 应用于某一层的权重，就像在 VAE 编码中一样：
- en: '![w = μ + log(1 + exp(ρ))⊙ 𝜖 ](img/file123.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![w = μ + log(1 + exp(ρ))⊙ 𝜖 ](img/file123.jpg)'
- en: Note that in BBB, *σ* is parameterized as *σ* = log(1 + exp(*ρ*)). This ensures
    that it is always non-negative (because a standard deviation cannot be negative!).
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在 BBB 中，*σ* 被参数化为 *σ* = log(1 + exp(*ρ*))。这确保了它始终是非负的（因为标准差不能是负数！）。
- en: 'With our parameters *𝜃* = (*μ,ρ*), we define our loss, following equation 3.10,
    as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的参数 *𝜃* = (*μ,ρ*)，我们根据方程 3.10 定义我们的损失函数如下：
- en: '![f(w, 𝜃) = logq(w |𝜃 )− log P (w )P (D |w ) ](img/file124.jpg)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![f(w, 𝜃) = logq(w |𝜃 )− log P (w )P (D |w ) ](img/file124.jpg)'
- en: 'Because our neural network is made up of weights for both means and standard
    deviations, we need to calculate the gradients for them separately. We first calculate
    the gradient with respect to the mean, *μ*:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们的神经网络包含均值和标准差的权重，所以我们需要分别计算它们的梯度。我们首先计算相对于均值 *μ* 的梯度：
- en: '![ ∂f(w,-𝜃) ∂f-(w,𝜃) Δ μ = ∂w + ∂μ ](img/file125.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![ ∂f(w,-𝜃) ∂f-(w,𝜃) Δ μ = ∂w + ∂μ ](img/file125.jpg)'
- en: 'Then we calculate the gradient with respect to the standard deviation parameter,
    *ρ*:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们计算相对于标准差参数 *ρ* 的梯度：
- en: '![ ∂f(w, 𝜃) 𝜖 ∂f (w, 𝜃) Δ ρ = --------------------+ -------- ∂w 1 + exp(− ρ)
    ∂ρ ](img/file126.jpg)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![ ∂f(w, 𝜃) 𝜖 ∂f (w, 𝜃) Δ ρ = --------------------+ -------- ∂w 1 + exp(− ρ)
    ∂ρ ](img/file126.jpg)'
- en: 'Now we have all the components necessary to update our weights via backpropagation,
    in a similar fashion to a typical neural network, except we update our mean and
    variance weights with their respective gradients:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经具备了通过反向传播更新权重所需的所有组件，这与典型的神经网络类似，唯一不同的是，我们使用各自的梯度更新均值和方差权重：
- en: '![μ ← μ − α Δμ ](img/file127.jpg)![ρ ← ρ − αΔ ρ ](img/file128.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![μ ← μ − α Δμ ](img/file127.jpg)![ρ ← ρ − αΔ ρ ](img/file128.jpg)'
- en: You may have noticed that the first terms of the gradient computations in equations
    5.14 and 5.15 are the gradients you would compute for backpropagation of a typical
    neural network; we’re simply augmenting these gradients with *μ*- and *ρ*-specific
    update rules.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，在方程5.14和5.15中的梯度计算的第一项，正是你会为典型神经网络的反向传播计算的梯度；我们只是通过*μ*和*ρ*特定的更新规则增强了这些梯度。
- en: 'While that was fairly heavy in terms of mathematical content, we can break
    it down into a few simple concepts:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这部分内容在数学方面相对较重，但我们可以将其分解为几个简单的概念：
- en: Just as with the encoding in VAEs, we are working with weights that represent
    the mean and standard deviation of a multivariate distribution, except this time
    they make up our entire network, not just the encoding layer.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与变分自编码器（VAE）中的编码类似，我们在这里使用表示多元分布均值和标准差的权重，唯一不同的是，这次这些权重构成了整个网络，而不仅仅是编码层。
- en: 'Because of this, we again use a loss that incorporates the KL divergence: we’re
    looking to maximize the ELBO.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们再次使用一个包含KL散度的损失函数：我们的目标是最大化ELBO。
- en: As we are dealing with mean and standard deviation weights, we update these
    separately with update rules that use the gradients for the respective set of
    weights.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们正在处理均值和标准差权重，我们将使用更新规则分别更新它们，规则使用的是各自权重集的梯度。
- en: Now that we understand the core principles behind BBB, we’re ready to see how
    it all comes together in code!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了BBB背后的核心原理，准备好看看它如何在代码中实现！
- en: 5.5 Implementing BBB with TensorFlow
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 使用TensorFlow实现BBB
- en: In this section, we’ll see how to implement BBB in TensorFlow. Some of the code
    you’ll see will be familiar; the core concepts of layers, loss functions, and
    optimizers will be very similar to what we covered in *Chapter 3, Fundamentals
    of* *Deep Learning*. Unlike the examples in [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003), we’ll see how we can
    create neural networks capable of probabilistic inference.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将看到如何在TensorFlow中实现BBB。你将看到一些你已经熟悉的代码；层、损失函数和优化器的核心概念与我们在*第3章 深度学习基础*中所涵盖的非常相似。与[*第3章*](CH3.xhtml#x1-350003)、[*深度学习基础*](CH3.xhtml#x1-350003)中的例子不同，我们将看到如何创建能够进行概率推断的神经网络。
- en: 'Step 1: Importing packages'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：导入包
- en: 'We start by importing the relevant packages. Importantly, we will import `tensorflow-probability`,
    which will provide us with the layers of the network that replace the point-estimate
    with a distribution and implement the reparameterization trick. We also set the
    global parameter for the number of inferences, which will determine how often
    we sample from the network later:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入相关包。重要的是，我们将导入`tensorflow-probability`，它将为我们提供网络的层，这些层用分布替代了点估计，并实现了重新参数化技巧。我们还设置了推理次数的全局参数，这将决定稍后我们从网络中采样的频率：
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Acquiring data'
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：获取数据
- en: 'We then download the MNIST Fashion dataset, which is a dataset that contains
    images of ten different clothing items. We also set the class names and derive
    the number of training examples and classes:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们下载MNIST Fashion数据集，这是一个包含十种不同衣物图像的数据集。我们还设置了类别名称，并推导出训练样本和类别的数量：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 3: Helper functions'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：辅助函数
- en: Next, we create a helper function that defines our model. As you can see, we
    use a very simple convolutional neural network structure for image classification
    that consists of a convolutional layer, followed by a max-pooling layer and a
    fully connected layer. The convolutional layer and the dense layer are imported
    from the `tensorflow-probability` package, as indicated by the prefix *tfp*. Instead
    of defining point-estimates for the weights, they will define weight distributions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个辅助函数来定义我们的模型。如你所见，我们使用了一个非常简单的卷积神经网络结构进行图像分类，其中包括一个卷积层，随后是一个最大池化层和一个全连接层。卷积层和全连接层来自`tensorflow-probability`包，如*tfp*前缀所示。与定义权重的点估计不同，这里我们定义的是权重分布。
- en: 'As the names `Convolution2DReparameterization` and `DenseReparameterization`
    suggest, these layers will use the reparameterization trick to update the weight
    parameters during backpropagation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如`Convolution2DReparameterization`和`DenseReparameterization`的名称所示，这些层将使用重参数化技巧在反向传播过程中更新权重参数：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also create another helper function that compiles the model for us, using
    `Adam` as our optimizer and a categorical cross-entropy loss. Provided with this
    loss and the preceding network structure, `tensorflow-probability` will automatically
    add the Kullback-Leibler divergence that is contained in the convolutional and
    dense layers to the cross-entropy loss. This combination effectively amounts to
    calculating the ELBO loss that we described in equation 5.9:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了另一个辅助函数来为我们编译模型，使用`Adam`作为优化器和分类交叉熵损失。提供了这个损失和前述的网络结构后，`tensorflow-probability`将自动将包含在卷积和密集层中的KL散度添加到交叉熵损失中。这种组合有效地相当于计算我们在方程式5.9中描述的ELBO损失：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: model training'
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：模型训练
- en: 'Before we can train the model, we first need to convert the labels of the training
    data from integers to one-hot vectors because this is what TensorFlow expects
    for the categorical cross-entropy loss. For example, if an image shows a t-shirt
    and the integer label for t-shirts is 1, then this label will be transformed like
    this: `[1,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0]`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练模型之前，我们首先需要将训练数据的标签从整数转换为独热向量，因为这是TensorFlow对于分类交叉熵损失的期望。例如，如果一张图像显示一件T恤，而T恤的整数标签为1，则该标签将被转换如下：`[1,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0,`` 0]`：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we are ready to train our model on the training data. We will train for
    ten epochs:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备在训练数据上训练我们的模型。我们将训练十个epoch：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 5: inference'
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 5：推断
- en: 'We can then use the trained model to perform inference on the test images.
    Here, we predict the class label for the first 50 images in the test split. For
    every image, we sample seven times from the network (as determined by `NUM_INFERENCES`),
    which will give us seven predictions for every image:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用训练好的模型对测试图像进行推断。在这里，我们预测测试集中前 50 张图像的类标签。对于每张图像，我们从网络中进行七次采样（由`NUM_INFERENCES`确定），这将为每张图像给出七个预测：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And that’s it: we have a working BBB model! Let’s visualize the first image
    in the test split and the seven different predictions for that image. First, we
    obtain the class predictions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样：我们有了一个可用的BBB模型！让我们可视化测试集中的第一张图像以及该图像的七个不同预测。首先，我们获取类别预测：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we visualize the image along with the predicted class for every inference:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可视化图像以及每次推断的预测类别：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Looking at the image in *Figure* [*5.7*](#x1-70136r7), on most samples the network
    predicts Ankle boot (which is the correct class). For two of the samples, the
    network also predicted Sneaker, which is somewhat plausible given the image is
    showing a shoe.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 查看*Figure* [*5.7*](#x1-70136r7)中的图像，在大多数样本中，网络预测为“踝靴”（这是正确的类别）。在两个样本中，网络还预测为“运动鞋”，这在图像显示鞋类时也是合理的。
- en: '![PIC](img/file129.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file129.png)'
- en: 'Figure 5.6: Class predictions across the seven different samples from the network
    trained with the BBB approach on the first test image in the MNIST fashion dataset'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：来自采用BBB方法训练的网络的七个不同样本中的第一张测试图像的类预测
- en: 'Given that we now have seven predictions per image, we can also calculate the
    mean variance across these predictions to approximate an uncertainty value:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们每张图像有了七个预测，我们还可以计算这些预测之间的平均方差，以近似不确定性值：
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For example, the uncertainty value for the first test image in the MNIST fashion
    dataset is 0*.*0000002\. To put this uncertainty value into context, let’s load
    some images from the regular MNIST dataset, which contains handwritten digits
    between 0 and 9, and obtain uncertainty values from the model we have trained.
    We load the dataset and then perform inference again and obtain the uncertainty
    values:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，时装MNIST数据集中第一张测试图像的不确定性值为0*.*0000002。为了将此不确定性值置于上下文中，让我们从常规MNIST数据集加载一些图像，其中包含介于0到9之间的手写数字，并从我们训练过的模型中获取不确定性值。我们加载数据集，然后再次进行推断并获取不确定性值：
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can then visualize and compare the uncertainty values between the first 50
    images in the fashion MNIST dataset and the regular MNIST dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以可视化比较时装MNIST数据集中前 50 张图像和常规MNIST数据集之间的不确定性值。
- en: In *Figure* [*5.7*](#x1-70136r7), we see that uncertainty values are a lot greater
    for images from the regular MNIST dataset than for the fashion MNIST dataset.
    This is expected, given that our model has only seen fashion MNIST images during
    training and the handwritten digits from the regular MNIST dataset are out-of-distribution
    for the model we trained.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图* [*5.7*](#x1-70136r7) 中，我们可以看到，来自常规 MNIST 数据集的图像的不确定性值远高于时尚 MNIST 数据集的图像。这是预期中的情况，因为我们的模型在训练时只看到了时尚
    MNIST 图像，而常规 MNIST 数据集中的手写数字对于我们训练的模型来说是超出分布的。
- en: '![PIC](img/file130.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file130.png)'
- en: 'Figure 5.7: Uncertainty values for images in the fashion MNIST dataset (left)
    versus the regular MNIST dataset (right)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：时尚 MNIST 数据集（左）与常规 MNIST 数据集（右）中图像的不确定性值
- en: BBB is perhaps the most commonly encountered highly principled Bayesian deep
    learning method, but it isn’t the only option for those concerned with better
    principled methods. In the next section, we’ll introduce another highly principled
    method and learn about the properties that differentiate it from BBB.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: BBB 可能是最常遇到的高度原则化的贝叶斯深度学习方法，但对于那些关注更好原则性方法的人来说，它并不是唯一的选择。在接下来的章节中，我们将介绍另一种高度原则化的方法，并了解它与
    BBB 的区别。
- en: 5.6 Scalable Bayesian Deep Learning with Probabilistic Backpropagation
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 可扩展的贝叶斯深度学习与概率反向传播
- en: 'BBB provided a great introduction to Bayesian inference with neural networks,
    but variational methods have one key drawback: their reliance on sampling at training
    and inference time. Unlike a standard neural network, we need to sample from the
    weight parameters using a range of *𝜖* values in order to produce the distributions
    necessary for probabilistic training and inference.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: BBB 为贝叶斯推断与神经网络的结合提供了很好的介绍，但变分方法有一个关键的缺点：它们依赖于训练和推理时的采样。与标准神经网络不同，我们需要使用一系列
    *𝜖* 值从权重参数中进行采样，以生成进行概率训练和推理所需的分布。
- en: 'At around the same time that BBB was introduced, researchers at Harvard University
    were working on their own brand of Bayesian inference with neural networks: **Probabilistic
    Backpropagation**, or **PBP**. Like BBB, PBP’s weights form the parameters of
    a distribution, in this case mean and variance weights (using variance, *σ*²,
    rather than *σ*). In fact, the similarities don’t end here – we’re going to see
    quite a few similarities to BBB but, crucially, we’re going to end up with a different
    approach to BNN approximation with its own advantages and disadvantages. So, let’s
    get started.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BBB 被引入的同时，哈佛大学的研究人员也在研究他们自己的贝叶斯推断与神经网络相结合的方法：**概率反向传播**，或称 **PBP**。像 BBB
    一样，PBP 的权重也构成了一个分布的参数，在这种情况下是均值和方差权重（使用方差 *σ*²，而不是 *σ*）。实际上，相似之处不仅限于此——我们将看到许多与
    BBB 相似的地方，但关键是，我们最终会采用一种不同的 BNN 近似方法，这种方法有其独特的优缺点。那么，让我们开始吧。
- en: 'To make things simpler, and for parity with the various PBP papers, we’ll stick
    with individual weights while we work through the core ideas of PBP. Here’s a
    visualization of how these weights are related in a small neural network:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，并与各种 PBP 论文保持一致，我们将在阐述 PBP 核心思想时坚持使用单个权重。以下是一个小型神经网络中这些权重如何关联的可视化：
- en: '![PIC](img/file131.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file131.png)'
- en: 'Figure 5.8: Illustration of neural network weights in PBP'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：PBP 中神经网络权重的示意图
- en: 'Just as before, we see that our network is essentially built from two sub-networks:
    one for the mean weights, or *m*, and one for the variance weights, or *v*. The
    core idea behind PBP is that, for each weight, we have some distribution *P*(*w*|*D*)
    that we’re trying to approximate:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所见，我们的网络本质上是由两个子网络组成：一个用于均值权重，或者说 *m*，另一个用于方差权重，或者说 *v*。PBP 的核心思想是，对于每个权重，我们都有一个分布
    *P*(*w*|*D*)，我们正在尝试对其进行近似：
- en: '![q(w) = 𝒩 (w|m, v) ](img/file132.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![q(w) = 𝒩 (w|m, v) ](img/file132.jpg)'
- en: This notation should be very familiar now, with *P*() being the true (intractable)
    distribution, and *q*() being the approximate distribution. In PBP’s case, as
    demonstrated in equation 5.18, this is a Gaussian distribution parameterized by
    mean *m* and variance *v*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个符号现在应该很熟悉了，其中 *P*() 是真实（不可解）分布，*q*() 是近似分布。在 PBP 中，如公式 5.18 所示，这是由均值 *m* 和方差
    *v* 参数化的高斯分布。
- en: In BBB, we saw how variational learning via the ELBO used the KL divergence
    to ensure our weight distribution converged towards our prior *P*(*w*). In PBP,
    we will again make use of the KL divergence, although this time we’ll do it indirectly.
    The way that we achieve this is through the use of a process called **Assumed
    Density Filtering** (**ADF**).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BBB 中，我们看到变分学习通过 ELBO 使用 KL 散度确保我们的权重分布收敛到我们的先验 *P*(*w*)。在 PBP 中，我们将再次使用 KL
    散度，尽管这一次我们是间接地实现的。我们通过使用一种叫做**假定密度滤波**（**ADF**）的过程来实现这一点。
- en: 'ADF was developed as a fast sequential method of minimizing the KL divergence
    between the true posterior *P*(*w*|*D*) and some approximation *q*(*w*|*D*). A
    key point here is the fact that it is a *sequential* algorithm: just like gradient
    descent, which we use with standard neural networks, ADF updates its parameters
    sequentially. This makes it particularly well suited for adapting to a neural
    network. The ADF algorithm can be described in two key steps:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ADF 是一种快速的顺序方法，用于最小化真实后验 *P*(*w*|*D*) 与某个近似 *q*(*w*|*D*) 之间的 KL 散度。这里的一个关键点是它是一个*顺序*算法：就像我们与标准神经网络一起使用的梯度下降一样，ADF
    也是顺序地更新其参数。这使得它特别适合适应神经网络。ADF 算法可以通过两个关键步骤来描述：
- en: Initialize our parameters, with *m* = 0 and *v* = 1; that is, we start with
    a unit Gaussian 𝒩(0*,*1).
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化我们的参数，*m* = 0，*v* = 1；也就是说，我们从单位高斯 𝒩(0*,*1) 开始。
- en: Next, we go through each data point *x*[*i*] ∈ **x** and update the parameters
    of our model using a set of specific update equations that update our model parameters
    *m* and *v* separately.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们遍历每个数据点 *x*[*i*] ∈ **x**，并使用一组特定的更新方程更新我们的模型参数 *m* 和 *v*，这两个参数分别进行更新。
- en: While it’s beyond the scope of this book to provide a full derivation of ADF,
    you should know that as we update our parameters through ADF, we also minimize
    the KL divergence.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本书的范围之外提供 ADF 的完整推导，但你应该知道，在通过 ADF 更新参数时，我们也在最小化 KL 散度。
- en: 'Thus, for PBP, we need to adapt typical neural network update rules so that
    the weights are updated along the lines of ADF instead. We do this using the following
    update rules, which are derived from the original ADF equations:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于 PBP，我们需要调整典型的神经网络更新规则，使得权重沿着 ADF 的方向进行更新。我们通过以下更新规则来实现这一点，这些规则是从原始 ADF
    方程推导出来的：
- en: '![mnew = m + v ∂logZ-- ∂m ](img/file133.jpg)![ [ ( ) ] new 2 ∂-log-Z- ∂-log-Z-
    v = v − v ∂m − 2 ∂v ](img/file134.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![mnew = m + v ∂logZ-- ∂m ](img/file133.jpg)![ [ ( ) ] new 2 ∂-log-Z- ∂-log-Z-
    v = v − v ∂m − 2 ∂v ](img/file134.jpg)'
- en: 'Here, log *Z* denotes the Gaussian marginal likelihood, which is defined as
    follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，log *Z* 表示高斯边际似然，其定义如下：
- en: '![ 2 logZ = − log p(y|m, v) = − 0.5 × log-v +-(y-−-m-) v ](img/file135.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 logZ = − log p(y|m, v) = − 0.5 × log-v +-(y-−-m-) v ](img/file135.jpg)'
- en: 'This is the **negative log-likelihood** (**NLL**). Equation 5.21 is crucial
    to how we learn the parameters of PBP, as this is the loss function that we’re
    trying to optimise - so let’s take some time to understand what’s going on. Just
    as with our loss for BBB (equation 5.9), we can see that our log *Z* loss incorporates
    a few important pieces of information:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**负对数似然**（**NLL**）。方程 5.21 对于我们学习 PBP 的参数至关重要，因为这是我们试图优化的损失函数——所以我们需要花些时间来理解其中的含义。就像我们在
    BBB 中的损失（方程 5.9）一样，我们可以看到我们的对数*Z*损失包含了几个重要的信息：
- en: In the numerator, we see (*y*−*m*)². This is similar to a typical loss we’re
    used to seeing in standard neural network training (the L2 loss). This incorporates
    the penalty between our target *y* and our mean estimate for the value, *m*.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分子中，我们看到 (*y*−*m*)²。这类似于我们在标准神经网络训练中常见的典型损失（L2 损失）。它包含了目标 *y* 和我们对该值的均值估计 *m*
    之间的惩罚。
- en: The whole equation gives us the NLL function, which describes the joint probability
    of our target *y* as a function of our distribution parameterised by *m* and *v*.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个方程给出了 NLL 函数，它描述了我们的目标 *y* 作为我们由 *m* 和 *v* 参数化的分布的联合概率。
- en: 'This has some important properties, which we can explore through a few simple
    examples. Let’s look at the loss for some arbitrary parameters *m* = 0*.*8 and
    *v* = 0*.*4 for a given target *y* = 0*.*6:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有一些重要的性质，我们可以通过几个简单的例子来探索。让我们看一下对于给定目标 *y* = 0*.*6，参数 *m* = 0*.*8 和 *v* =
    0*.*4 的损失：
- en: '![ 2 2 − 0.5× logv-+-(y −-m)- = − 0.5 × log(0.4)-+-(0.6-−-0.8)- = 1.095 v 0.4
    ](img/file136.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 − 0.5× logv-+-(y −-m)- = − 0.5 × log(0.4)-+-(0.6-−-0.8)- = 1.095 v 0.4
    ](img/file136.jpg)'
- en: Here, we can see that our typical error, in this case the squared error, is
    (0*.*6 − 0*.*8)² = 0*.*04, and we know that as *m* converges towards *y*, this
    will shrink. In addition to that, the log-likelihood scales our error. This is
    important, because a well-conditioned model for uncertainty quantification will
    be *more uncertain* when it’s wrong, and *more confident* when it’s right. The
    likelihood function gives us a way of achieving this, ensuring that our likelihood
    is greater if we’re uncertain about incorrect predictions and certain about correct
    predictions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的典型误差，在这种情况下是平方误差，(0*.*6 − 0*.*8)² = 0*.*04，并且我们知道，当*m*趋近于*y*时，这个误差会缩小。此外，似然函数会缩放我们的误差。这很重要，因为一个用于不确定性量化的良好条件模型在错误时会*更不确定*，在正确时会*更自信*。似然函数为我们提供了一种方法，确保在我们对错误预测不确定时，它的似然值较大，而对正确预测时，则较为确定。
- en: 'We can see this in action by substituting another value of *v* and seeing how
    this changes the NLL. For example, let’s increase our variance to *v* = 0*.*9:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过替换另一个*v*的值，查看它如何改变NLL，来观察这个过程。例如，我们可以将方差增加到*v* = 0*.*9：
- en: '![ 2 − 0.5× log(0.9)+-(0.6−-0.8)- = 0.036 0.9 ](img/file137.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 − 0.5× log(0.9)+-(0.6−-0.8)- = 0.036 0.9 ](img/file137.jpg)'
- en: 'This significant increase in variance produces a similarly significant reduction
    in NLL. Similarly, we’ll see our NLL increase again if we have high variance for
    a correct prediction *m* = *y*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 方差的显著增加会导致NLL的显著下降。类似地，如果我们对一个正确的预测*m* = *y*有很高的方差，我们会看到NLL再次增加：
- en: '![ log(0.9)+-(0.8−-0.8)2 − 0.5× 0.9 = 0.059 ](img/file138.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![ log(0.9)+-(0.8−-0.8)2 − 0.5× 0.9 = 0.059 ](img/file138.jpg)'
- en: 'Hopefully, with this example you can see how using the NLL loss translates
    to well calibrated uncertainty estimates over our outputs. In fact, this property
    – using the variance to scale to our objective function – is a fundamental component
    of all principled BNN methods: BBB also does this, although it’s a little more
    difficult to demonstrate on paper as it requires sampling.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过这个例子，你能看到使用NLL损失如何转化为我们输出的良好校准的不确定性估计。实际上，这个属性——利用方差来缩放目标函数——是所有有原则的BNN方法的基本组成部分：BBB也做了这件事，尽管由于需要采样，它在纸面上展示起来有点复杂。
- en: There are a few low-level details of PBP that we’ll encounter in the implementation.
    These relate to the ADF process, and we encourage you to take a look at the articles
    in the *Further reading* section for comprehensive derivations of PBP and ADF.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现过程中，我们将遇到一些PBP的低级细节。它们与ADF过程有关，我们鼓励你查看*进一步阅读*部分中的文章，以获得PBP和ADF的详细推导。
- en: Now that we’ve covered PBP’s core concepts, let’s take a look at how we implement
    it with TensorFlow.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了PBP的核心概念，接下来让我们看看如何使用TensorFlow实现它。
- en: 5.7 Implementing PBP
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 实现PBP
- en: Because PBP is quite complex, we’ll implement it as a class. Doing so will keep
    our example code tidy and allow us to easily compartmentalize our various blocks
    of code. It will also make it easier to experiment with, for example, if you want
    to explore changing the number of units or layers in your network.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PBP相当复杂，我们将把它实现为一个类。这样做可以使我们的示例代码更加简洁，并且方便我们将不同的代码块进行模块化。它还将使得实验变得更容易，例如，如果你想探索改变网络中单元或层的数量。
- en: 'Step 1: Importing libraries'
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：导入库
- en: 'We begin by importing various libraries. In this example, we will use scikit-learn’s
    California Housing dataset to predict house prices:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入各种库。在这个例子中，我们将使用scikit-learn的加利福尼亚住房数据集来预测房价：
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To make sure we produce the same output every time, we initialize our seeds:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保每次生成相同的输出，我们初始化我们的种子值：
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then load our dataset and create train and test splits:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以加载数据集并创建训练集和测试集：
- en: '[PRE13]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Step 2: Helper functions'
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二步：辅助函数
- en: 'Next, we define two helper functions that ensure that our data is in the correct
    format, one for the input and another one for the output data:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义两个辅助函数，确保我们的数据格式正确，一个用于输入，另一个用于输出数据：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will also create a short class to initialize a gamma distribution: `ReciprocalGammaInitializer`.
    This distribution is used as the prior for PBP’s precision parameter *λ* and the
    noise parameter *γ*.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一个简短的类来初始化一个伽玛分布：`ReciprocalGammaInitializer`。这个分布被用作PBP精度参数*λ*和噪声参数*γ*的先验。
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A thorough treatment of these variables is not required for a general understanding
    of PBP. For further details on this, please see the PBP paper listed in the *Further
    reading* section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些变量的深入处理对于理解PBP并不是必需的。如需进一步了解， 请参见*进一步阅读*部分中列出的PBP论文。
- en: 'Step 3: Data preparation'
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：数据准备
- en: 'With these prerequisites implemented, we can normalize our data. Here, we normalize
    to mean zero and unit standard deviation. This is a common pre-processing step
    that will make it easier for our model to find the right set of weights:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现这些先决条件后，我们可以对数据进行归一化处理。在这里，我们将数据归一化为均值为零，标准差为单位。这是一个常见的预处理步骤，有助于模型更容易找到合适的权重：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Step 4: Defining our model class'
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：定义我们的模型类
- en: 'We can now start to define our model. Our model will consist of three layers:
    two ReLU layers and one linear layer. We use Keras’ `Layer` to define our layers.
    The code for this layer is quite long, so we will break it into several subsections.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始定义我们的模型了。我们的模型将由三层组成：两层ReLU层和一层线性层。我们使用Keras的`Layer`来定义这些层。由于这一层的代码比较长，因此我们将其拆分成几个子部分。
- en: 'First, we subclass the `Layer` to create our own `PBPLayer` and define our
    `init` method. Our initialization method sets the number of units in our layer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们继承`Layer`类来创建我们自己的`PBPLayer`并定义`init`方法。我们的初始化方法设置了层中的单元数量：
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We then create a `build()` method that defines the weights of our layer. As
    we discussed in the previous section, PBP comprises both *mean* weights and *variance*
    weights. As a simple MLP is composed of a multiplicative component, or weight,
    and a bias, we’ll split both our weights and biases into mean and variance variables:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个`build()`方法，用于定义我们层的权重。正如我们在上一节中讨论的，PBP包含了*均值*权重和*方差*权重。由于一个简单的MLP由乘法组件或权重和偏置组成，我们将权重和偏置分解为均值和方差变量：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `weights_m` and `weights_v` variables are our mean and variance weights,
    forming the very core of our PBP model. We will continue our definition of `PBPLayer`
    when we work through our model fitting function. For now, we can subclass this
    class to create our ReLU layer:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`weights_m`和`weights_v`变量是我们的均值和方差权重，构成了PBP模型的核心。我们将在通过模型拟合函数时继续定义`PBPLayer`。现在，我们可以继承该类来创建我们的ReLU层：'
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can see that we overwrite two functions: our `call()` and `predict()` functions.
    The `call()` function calls our regular linear `call()` function and then applies
    the ReLU max operation we saw in [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep* *Learning*](CH3.xhtml#x1-350003). The `predict()` function calls our
    regular `predict()` function, but then also calls a new function, `get_bias_mean_variance()`.
    This function computes the mean and variance of our bias in a numerically stable
    way, as shown here:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们重写了两个函数：`call()`和`predict()`函数。`call()`函数调用我们常规的线性`call()`函数，然后应用我们在[*第3章*](CH3.xhtml#x1-350003)，《深度学习基础》[*Chapter
    3*](CH3.xhtml#x1-350003)中看到的ReLU最大操作。`predict()`函数调用我们常规的`predict()`函数，但随后也调用了一个新函数`get_bias_mean_variance()`。该函数以数值稳定的方式计算偏置的均值和方差，如下所示：
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With our layer definitions in place, we can build our network. We first create
    a list of all layers in our network:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义好层之后，就可以构建我们的网络。我们首先创建一个包含网络中所有层的列表：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then create a `PBP` class that contains the model’s `fit()` and `predict()`
    functions, similar to what you see in a model defined with Keras’s `tf.keras.Model`
    class. Next, we’ll see a number of important variables; let’s go through them
    here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`PBP`类，包含模型的`fit()`和`predict()`函数，类似于你在使用Keras的`tf.keras.Model`类定义的模型。接下来，我们将看到一些重要的变量；让我们在这里一起了解它们：
- en: '`alpha` and `beta` : These are parameters of our gamma distribution'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`和`beta`：这些是我们伽马分布的参数'
- en: '`Gamma` : An instance of the `tfp.distributions.Gamma()` class for our gamma
    distributions, which is a hyper-prior on PBP’s precision parameter *λ*'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gamma`：一个`tfp.distributions.Gamma()`类的实例，用于我们的伽马分布，它是PBP精度参数*λ*的超先验'
- en: '`layers` : This variable specifies the number of layers in the model'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers`：这个变量指定了模型中的层数'
- en: '`Normal` : Here, we instantiate an instance of the `tfp.distributions.Normal()`
    class, which implements a Gaussian probability distribution (in this case, with
    a mean of 0 and a standard deviation of 1):'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normal`：在这里，我们实例化了`tfp.distributions.Normal()`类，它实现了一个高斯概率分布（此处均值为0，标准差为1）：'
- en: '[PRE22]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `PBP` class `__init__` function creates a number of parameters but essentially
    initializes our *α* and *β* hyper-priors with a normal and a gamma distribution.
    Furthermore, we save the layers that we created in the previous step.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`PBP`类的`__init__`函数创建了多个参数，但本质上是通过正态分布和伽马分布初始化我们的*α*和*β*超先验。此外，我们还保存了在上一步创建的层。'
- en: 'The `fit()` function updates the gradients of our layers and then updates our
    *α* and *β* parameters. The function for updating gradients is defined as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`函数更新我们层的梯度，然后更新*α*和*β*参数。更新梯度的函数定义如下：'
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Before we can update our gradients, we need to propagate them forward through
    the network. To do so, we’ll implement our `predict()` method:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新梯度之前，我们需要通过网络传播梯度。为此，我们将实现我们的`predict()`方法：
- en: '[PRE24]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now that we can propagate values through our network, we are ready to implement
    our loss function. As we saw in the previous section, we use the NLL, which we’ll
    define here:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过网络传播值，我们准备好实现我们的损失函数了。正如我们在前一节所看到的，我们使用NLL（负对数似然），在这里我们将定义它：
- en: '[PRE25]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now propagate values through the network and obtain our gradient with
    respect to our loss (as we would with a standard neural network). This means we’re
    ready to update our gradients by applying the update rules we saw in equations
    5.19 and 5.20 for the mean weights and variance weights, respectively:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过网络传播值，并计算相对于损失的梯度（就像我们在标准神经网络中一样）。这意味着我们可以根据公式5.19和5.20中的更新规则，分别更新均值权重和方差权重：
- en: '[PRE26]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As discussed in the previous section, PBP belongs to the class of **Assumed**
    **Density Filtering** (**ADF**) methods. As such, we update the *α* and *β* parameters
    according to ADF’s update rules:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所讨论，PBP属于**假设****密度滤波**（**ADF**）方法的类别。因此，我们根据ADF的更新规则更新*α*和*β*参数：
- en: '[PRE27]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Step 5: Avoiding numerical errors'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤5：避免数值错误
- en: 'Finally, let’s define a few helper functions to ensure that we avoid numerical
    errors during fitting:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们定义一些辅助函数，以确保在拟合过程中避免数值错误：
- en: '[PRE28]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Step 6: Instantiating our model'
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤6：实例化我们的模型
- en: 'And there we have it: the core code for training PBP. Now we’re ready to instantiate
    our model and train it on some data. Let’s use a small batch size and a single
    epoch in this example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样：训练PBP的核心代码完成了。现在我们准备实例化我们的模型，并在一些数据上进行训练。在这个例子中，我们使用较小的批次大小和一个训练周期：
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Step 7: Using our model for inference'
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤7：使用我们的模型进行推理
- en: 'Now that we have our fitted model, let’s see how well it works on our test
    set. We first normalize our test set:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了拟合的模型，接下来看看它在测试集上的表现如何。我们首先对测试集进行标准化：
- en: '[PRE30]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then we get our model predictions: the mean and variance:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到模型预测结果：均值和方差：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then we post-process these values to make sure they have the right shape and
    are in the range of the original input data:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对这些值进行后处理，以确保它们具有正确的形状，并且在原始输入数据的范围内：
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that we’ve got our predictions, we can compute how well our model’s done.
    We’ll use a standard error metric, RMSE, as well as the metric we used for our
    loss: the NLL. We can compute them using the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了预测结果，可以计算我们的模型表现如何。我们将使用标准误差指标RMSE，以及我们在损失函数中使用的指标：NLL。我们可以使用以下公式计算它们：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Evaluating both of these metrics is good practice for any regression task for
    which you have model uncertainty estimates. The RMSE gives you your standard error
    metric, which allows you to compare directly with non-probabilistic methods. The
    NLL gives you an imdivssion of how well calibrated your method is by evaluating
    how confident your model is when it’s doing well versus doing poorly, as we discussed
    earlier in the chapter. Together, these metrics give you a comprehensive imdivssion
    of a Bayesian model’s performance, and you’ll see them used time and time again
    in the literature.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 评估这两个指标是任何回归任务的好做法，尤其是当你有模型不确定性估计时。RMSE给出了标准误差指标，它允许你直接与非概率方法进行比较。NLL则通过评估当模型表现好与表现差时模型的信心，来判断你的方法的校准程度，正如我们在本章前面讨论过的那样。总体来看，这些指标提供了贝叶斯模型性能的全面评估，你会在文献中反复看到它们的应用。
- en: 5.8 Summary
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 小结
- en: In this chapter, we learned about two fundamental, well-principled, Bayesian
    deep learning models. BBB showed us how we can make use of variational inference
    to efficiently sample from our weight space and produce output distributions,
    while PBP demonstrated that it’s possible to obtain predictive uncertainties *without*
    sampling. This makes PBP more computationally efficient than BBB, but each model
    has its pros and cons.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了两个基础的、原则明确的贝叶斯深度学习模型。BBB展示了如何利用变分推断高效地从权重空间进行采样并生成输出分布，而PBP则展示了通过不进行采样也能获得预测不确定性的可能性。这样，PBP在计算上比BBB更高效，但每个模型都有其优缺点。
- en: 'In BBB’s case, while it’s less computationally efficient than PBP, it’s also
    more adaptable (particularly with the tools available in TensorFlow for variational
    layers). We can apply this to a variety of different DNN architectures with relatively
    little difficulty. The price is incurred through the sampling required at both
    inference and training time: we need to do more than just a single forward pass
    to obtain our output distributions.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BBB 的情况下，虽然它在计算效率上不如 PBP，但它也更具适应性（特别是在 TensorFlow 中用于变分层的工具）。我们可以将其应用于各种不同的
    DNN 架构，且相对不费力。代价则是在推理和训练时所需的采样：我们需要进行的不仅仅是一次前向传递才能获得输出分布。
- en: Conversely, PBP allows us to obtain our uncertainty estimates with a single
    pass, but – as we’ve just seen – it’s quite complex to implement. This makes it
    awkward to adapt to other network architectures, and while it has been done (see
    the *Further reading* section), it’s not a particularly practical method to use
    given the technical overhead of implementation and the relatively marginal gains
    compared to other methods.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，PBP 允许我们通过一次传递获得不确定性估计，但正如我们刚才所见，它的实现相当复杂。这使得它在适应其他网络架构时显得有些笨拙，尽管已经有实现（参见*进一步阅读*部分），但鉴于实施的技术开销以及与其他方法相比相对较小的收益，它并不是一种特别实用的方法。
- en: In summary, these methods are excellent if you need robust, well-principled
    BNN approximations and aren’t constrained in terms of memory or computational
    overheads at inference. But what if you have limited memory and/or limited compute,
    such as running on edge devices? In these cases, you may want to turn to more
    practical methods of obtaining predictive uncertainties.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果你需要稳健且原则清晰的 BNN 近似，并且在推理时不受内存或计算开销的限制，这些方法非常优秀。但如果你有有限的内存和/或计算资源，比如在边缘设备上运行，怎么办？在这种情况下，你可能需要转向更实用的方法来获得预测的不确定性。
- en: In *Chapter 6, Bayesian Neural Network Approximation Using a Standard Deep*
    *Learning Toolbox*, we’ll see how we can use more familiar components in TensorFlow
    to create more practical probabilistic neural network models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第六章，使用标准深度学习工具箱的贝叶斯神经网络近似*中，我们将看到如何使用 TensorFlow 中更熟悉的组件来创建更实用的概率神经网络模型。
- en: 5.9 Further reading
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 进一步阅读
- en: '*Weight Uncertainty in Neural Networks*, Charles Blundell *et al.*: This is
    the paper that introduced BBB, and is one of the key pieces of BDL literature.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络中的权重不确定性*，Charles Blundell *等人*：这篇论文介绍了 BBB，并且是 BDL 文献中的关键文献之一。'
- en: '*Practical Variational Inference for Neural Networks*, Alex Graves *et al.*:
    An influential paper on the use of variational inference for neural networks,
    this work introduces a straightforward stochastic variational method that can
    be applied to a variety of neural network architectures.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络的实用变分推断*，Alex Graves *等人*：这是一篇关于神经网络中使用变分推断的有影响力的论文，介绍了一种简单的随机变分方法，可以应用于各种神经网络架构。'
- en: '*Probabilistic Backpropagation for Scalable Learning of Bayesian* *Neural Networks*,
    José Miguel Hernández-Lobato *et al.*: Another important work in BDL literature,
    this work introduced PBP, demonstrating how Bayesian inference can be achieved
    via more scalable means.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展贝叶斯神经网络学习的概率反向传播*，José Miguel Hernández-Lobato *等人*：BDL 文献中的另一项重要工作，介绍了
    PBP，展示了如何通过更具可扩展性的方法实现贝叶斯推断。'
- en: '*Practical Considerations for Probabilistic Backpropagation*, Matt Benatan
    *et al.*: In this work, the authors introduce methods for making PBP more practical
    for real-world applications.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概率反向传播的实用考虑*，Matt Benatan *等人*：在这项工作中，作者介绍了使 PBP 更适合实际应用的方法。'
- en: '*Fully Bayesian Recurrent Neural Networks for Safe Reinforcement* *Learning*,
    Matt Benatan *et al.*: This paper shows how PBP can be adapted to an RNN architecture,
    and shows how BNNs can be advantageous in safety-critical systems.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于安全强化学习的完全贝叶斯递归神经网络*，Matt Benatan *等人*：这篇论文展示了如何将 PBP 适应于 RNN 架构，并展示了 BNN
    在安全关键系统中的优势。'
- en: '**[1](#footref1)** It is beyond the scope of this book to guide the reader
    through the derivation of ELBO, but we encourage the reader to see the Further
    reading section for texts that provide a more comprehensive overview of ELBO.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1](#footref1)** 本书的范围不包括引导读者推导 ELBO，但我们鼓励读者参阅进一步阅读部分中的文本，以获得对 ELBO 更全面的概述。'
