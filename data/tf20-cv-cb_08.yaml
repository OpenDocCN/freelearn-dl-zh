- en: '*Chapter 8*: Fine-Grained Understanding of Images through Segmentation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：通过分割实现对图像的精细理解'
- en: Image segmentation is one of the biggest areas of study in computer vision.
    It consists of simplifying the visual contents of an image by grouping together
    pixels that share one or more defining characteristics, such as location, color,
    or texture. As is the case with many other subareas of computer vision, image
    segmentation has been greatly boosted by deep neural networks, mainly in industries
    such as medicine and autonomous driving.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是计算机视觉研究领域中最大的领域之一。它通过将共享一个或多个定义特征（如位置、颜色或纹理）的像素组合在一起，简化图像的视觉内容。与计算机视觉的许多其他子领域一样，图像分割也得到了深度神经网络的极大推动，特别是在医学和自动驾驶等行业。
- en: While it's great to classify the contents of an image, more often than not,
    it's not enough. What if we want to know exactly where an object is? What if we're
    interested in its shape? What if we need its contour? These fine-grained needs
    cannot be met with traditional classification techniques. However, as we'll discover
    in this chapter, we can frame an image segmentation problem in a very similar
    way to a regular classification project. How? Instead of labeling the image as
    a whole, we'll label each pixel! This is known as image segmentation and is what
    constitutes the recipes in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对图像的内容进行分类非常重要，但往往仅仅分类是不够的。假如我们想知道一个物体具体在哪里呢？如果我们对它的形状感兴趣呢？如果我们需要它的轮廓呢？这些精细的需求是传统分类技术无法满足的。然而，正如我们将在本章中发现的那样，我们可以用一种非常类似于常规分类项目的方式来框定图像分割问题。怎么做？我们不是给整张图像标注标签，而是给每个像素标注！这就是图像分割，也是本章食谱的核心内容。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Creating a fully convolutional network for image segmentation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个用于图像分割的全卷积网络
- en: Implementing a U-Net from scratch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始实现 U-Net
- en: Implementing a U-Net with transfer learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习实现 U-Net
- en: Segmenting images using Mask-RCNN and TensorFlow Hub
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Mask-RCNN 和 TensorFlow Hub 进行图像分割
- en: Let's get started!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In order to implement and experiment with the recipes in this chapter, it''s
    recommended that you have access to a GPU. If you have recourse to a cloud-based
    provider, such as AWS or FloydHub, that''s great, but keep the fees attached to
    them in mind as they might skyrocket if you''re not careful! In the *Getting ready*
    section of each recipe, you''ll find everything you''ll need to prepare for what
    lies ahead. The code for this chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现和实验本章的食谱，建议你拥有一台 GPU。如果你有访问基于云的服务提供商，如 AWS 或 FloydHub，那就太好了，但请注意相关费用，因为如果不小心的话，费用可能会飙升！在每个食谱的*准备工作*部分，你会找到你需要为接下来的内容做准备的所有信息。本章的代码可以在这里找到：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看《代码实践》视频：
- en: '[https://bit.ly/2Na77IF](https://bit.ly/2Na77IF).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/2Na77IF](https://bit.ly/2Na77IF)。'
- en: Creating a fully convolutional network for image segmentation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个用于图像分割的全卷积网络
- en: If you were to create your first network for image segmentation while knowing
    that, at its core, segmenting is just pixel-wise classification, what would you
    do? You would probably take a battle-tested architecture and swap the final layers
    (usually fully connected ones) with convolutions in order to produce an output
    volume, instead of an output vector.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在知道图像分割本质上就是像素级分类的情况下，创建你的第一个图像分割网络，你会怎么做？你可能会选择一个经过验证的架构，并将最后的层（通常是全连接层）替换为卷积层，以便生成一个输出体积，而不是输出向量。
- en: Well, that's exactly what we'll do in this recipe to build a **Fully Convolutional
    Network** (**FCN**) for image segmentation based on the famous **VGG16** network.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这正是我们在本食谱中要做的，基于著名的**VGG16**网络构建一个**全卷积网络**（**FCN**）来进行图像分割。
- en: Let's get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We need to install a couple of external libraries, starting with `tensorflow_docs`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装几个外部库，首先是 `tensorflow_docs`：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to install TensorFlow Datasets, `Pillow`, and `OpenCV`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要安装 TensorFlow Datasets、`Pillow` 和 `OpenCV`：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Regarding the data, we will segment images from `the Oxford-IIIT Pet` dataset.
    The good news is that we''ll access it using `tensorflow-datasets`, so we don''t
    really need to do anything in that respect here. Each pixel in this dataset is
    classified as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据，我们将从`the Oxford-IIIT Pet`数据集中分割图像。好消息是，我们将通过`tensorflow-datasets`来访问它，所以在这方面我们实际上不需要做任何事情。该数据集中的每个像素将被分类如下：
- en: '1: The pixel belongs to a pet (cat or dog).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 像素属于宠物（猫或狗）。'
- en: '2: The pixel belongs to the contour of a pet.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2: 像素属于宠物的轮廓。'
- en: '3: The pixel belongs to the surroundings.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3: 像素属于周围环境。'
- en: 'Here are some sample images from the dataset:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据集中的一些示例图像：
- en: '![Figure 8.1 – Sample images from the Oxford-IIIT Pet dataset'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 来自Oxford-IIIT Pet数据集的示例图像](img/B14768_08_001.jpg)'
- en: '](img/B14768_08_001.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_001.jpg)'
- en: Figure 8.1 – Sample images from the Oxford-IIIT Pet dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 来自Oxford-IIIT Pet数据集的示例图像
- en: Let's start implementing!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实现吧！
- en: How to do it…
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Follow these steps to complete this recipe:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成此配方：
- en: 'Import all the required packages:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的包：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`tf.data.experimental.AUTOTUNE`定义一个别名：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define a function that will normalize the images in the dataset to the range
    [0, 1]. Just for consistency''s sake, we''ll subtract one from each pixel in the
    mask so that they go from 0 all the way to 2:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于将数据集中的图像归一化到[0, 1]范围。为了保持一致性，我们将从掩膜中的每个像素减去1，这样它们的范围就从0扩展到2：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the `load_image()` function, which loads both the image and its mask,
    given a TensorFlow dataset element. We will seize the opportunity to resize the
    images to *256x256* here. Also, if the `train` flag is set to `True`, we can perform
    a bit of augmentation by randomly mirroring the image and its mask. Lastly, we
    must normalize the inputs:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`load_image()`函数，给定一个TensorFlow数据集元素，该函数加载图像及其掩膜。我们将借此机会将图像调整为*256x256*。另外，如果`train`标志设置为`True`，我们可以通过随机镜像图像及其掩膜来进行一些数据增强。最后，我们必须对输入进行归一化：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Implement the `FCN()` class, which encapsulates all the logic required to build,
    train, and evaluate our `RMSProp` as the optimizer and `SparseCategoricalCrossentropy`
    as the loss. Notice that `output_channels` is, by default, 3, because each pixel
    can be categorized into one of three classes. Also, notice that we are defining
    the path to the weights of the **VGG16** this model is based on. We'll use these
    weights to give our network a head start when training.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`FCN()`类，该类封装了构建、训练和评估所需的所有逻辑，使用`RMSProp`作为优化器，`SparseCategoricalCrossentropy`作为损失函数。请注意，`output_channels`默认值为3，因为每个像素可以被分类为三类之一。还请注意，我们正在定义基于**VGG16**的预训练模型的权重路径。我们将使用这些权重在训练时为网络提供一个良好的起点。
- en: 'Now, it''s time to define the architecture itself:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候定义模型的架构了：
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We started by defining the input and the first block of convolutions and max
    pooling layers. Now, define the second block of convolutions, this time with 128
    filters each:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义了输入和第一块卷积层以及最大池化层。现在，定义第二块卷积层，这次每个卷积使用128个滤波器：
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The third block contains convolutions with 256 filters:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三块包含256个滤波器的卷积：
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The fourth block uses convolutions with 512 filters:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四块使用了512个滤波器的卷积：
- en: '[PRE9]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The fifth block is a repetition of block four, again with 512 filter-deep convolutions:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第五块是第四块的重复，同样使用512个滤波器的卷积：
- en: '[PRE10]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The reason we''ve been naming the layers so far is so that we can match them
    with the pre-trained weights we''ll import next (notice `by_name=True`):'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们到目前为止命名层的原因是，为了在接下来导入预训练权重时能够与它们匹配（请注意`by_name=True`）：
- en: '[PRE11]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`output`, in a traditional **VGG16** architecture, is comprised of fully connected
    layers. However, we''ll be replacing them with transposed convolutions. Notice
    we are connecting these layers to the output of the fifth block:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在传统的**VGG16**架构中，`output`由全连接层组成。然而，我们将用反卷积层替换它们。请注意，我们正在将这些层连接到第五块的输出：
- en: '[PRE12]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a 1x1 convolution, followed by a transposed convolution, and connect
    it to the output of the fourth block (this is, indeed, a skip connection):'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个1x1卷积层，接着是一个反卷积层，并将其连接到第四块的输出（这实际上是一个跳跃连接）：
- en: '[PRE13]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Pass the output of the third block through a 1x1 convolution. Then, merge these
    three paths into one and pass them through a final transposed convolution. This
    will be activated with `Softmax`. This output constitutes the segmentation mask
    predicted by the model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第三块的输出通过一个1x1卷积层。然后，将这三条路径合并成一条，传递通过最后一个反卷积层。这将通过`Softmax`激活。这个输出即为模型预测的分割掩膜：
- en: '[PRE14]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s create a private helper method to plot the relevant training curves:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个私有辅助方法来绘制相关的训练曲线：
- en: '[PRE15]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `train()` method takes the training and validation datasets, as well as
    the number of epochs and training and validation steps to perform, in order to
    fit the model. It also saves the loss and accuracy plots to disk for later analysis:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train()` 方法接受训练和验证数据集，以及执行的周期数和训练、验证步骤数，用于拟合模型。它还将损失和准确率图保存到磁盘，以便后续分析：'
- en: '[PRE16]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Implement `_process_mask()`, which is used to make the segmentation masks compatible
    with OpenCV. What this function does is create a three-channeled version of a
    grayscale mask and upscale the class values to the [0, 255] range:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 `_process_mask()`，用于使分割掩膜与 OpenCV 兼容。这个函数的作用是创建一个三通道版本的灰度掩膜，并将类值上采样到 [0,
    255] 范围：
- en: '[PRE17]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `_save_image_and_masks()` helper method creates a mosaic of the original
    image, the ground truth mask, and the predicted segmentation mask, and then saves
    it to disk for later revision:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_save_image_and_masks()` 辅助方法创建了原始图像、真实标签掩膜和预测分割掩膜的马赛克图像，并将其保存到磁盘以便后续修订：'
- en: '[PRE18]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In order to pass the output volume produced by the network to a valid segmentation
    mask, we must take the index with the highest value at each pixel location. This
    corresponds to the most likely category for that pixel. The `_create_mask()` method
    does this:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将网络输出的体积转换为有效的分割掩膜，我们必须在每个像素位置选择值最高的索引。这对应于该像素最可能的类别。`_create_mask()` 方法执行此操作：
- en: '[PRE19]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `_save_predictions()` method uses the `_save_image_and_mask()` helper method,
    which we defined in *Step 18*:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_save_predictions()` 方法使用了我们在 *步骤 18* 中定义的 `_save_image_and_mask()` 辅助方法：'
- en: '[PRE20]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `evaluate()` method computes the accuracy of the **FCN** on the test set
    and generates predictions for a sample of images, which are then stored on disk:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`evaluate()` 方法计算 **FCN** 在测试集上的准确率，并为一部分图像生成预测结果，然后将其保存到磁盘：'
- en: '[PRE21]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Download (or load, if cached) `Oxford IIIT Pet Dataset`, along with its metadata,
    using **TensorFlow Datasets**:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **TensorFlow Datasets** 下载（或加载，如果已缓存）`Oxford IIIT Pet Dataset` 及其元数据：
- en: '[PRE22]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Use the metadata to define the corresponding number of steps the network will
    take over the training and validation datasets. Also, define the batch and buffer
    sizes:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用元数据定义网络在训练和验证数据集上的步数。此外，还要定义批处理和缓冲区大小：
- en: '[PRE23]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the training and testing datasets'' pipelines:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和测试数据集的管道：
- en: '[PRE24]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Instantiate the **FCN** and train it for 120 epochs:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 **FCN** 并训练 120 个周期：
- en: '[PRE25]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Lastly, evaluate the network on the test dataset:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在测试数据集上评估网络：
- en: '[PRE26]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As shown in the following graph, the accuracy on the test set should be around
    84% (specifically, I got 84.47%):'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如下图所示，测试集上的准确率应约为 84%（具体来说，我得到的是 84.47%）：
- en: '![Figure 8.2 – Training and validation accuracy curves'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 训练和验证准确率曲线'
- en: '](img/B14768_08_002.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_002.jpg)'
- en: Figure 8.2 – Training and validation accuracy curves
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 训练和验证准确率曲线
- en: 'The training curves display a healthy behavior, meaning that the network did,
    indeed, learn. However, the true test is to visually assess the results:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 训练曲线显示出健康的行为，意味着网络确实学到了东西。然而，真正的考验是通过视觉评估结果：
- en: '![Figure 8.3 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 原始图像（左），真实标签掩膜（中），预测掩膜（右）'
- en: '](img/B14768_08_003.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_003.jpg)'
- en: Figure 8.3 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 原始图像（左），真实标签掩膜（中），预测掩膜（右）
- en: 'In the preceding image, we can see that the mask that was produced by the network
    follows the shape of the ground truth segmentation. However, there''s an unsatisfying
    pixelated effect across the segments, as well as noise in the upper-right corner.
    Let''s take a look at another example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，我们可以看到，网络生成的掩膜跟真实标签分割的形状相符。然而，分割部分存在令人不满意的像素化效果，并且右上角有噪点。让我们看看另一个例子：
- en: '![Figure 8.4 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4 – 原始图像（左），真实标签掩膜（中），预测掩膜（右）'
- en: '](img/Image86700.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Image86700.jpg)'
- en: Figure 8.4 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 原始图像（左），真实标签掩膜（中），预测掩膜（右）
- en: In the preceding image, we can see a very deficient, spotty, and overall low-quality
    mask that proves that the network still needs a lot of improvement. This could
    be achieved by doing more fine-tuning and experimentation. However, in the next
    recipe, we'll discover a network that's best suited to performing image segmentation
    and capable of producing a really good mask with way less effort.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图像中，我们可以看到一个非常不完整、斑点状且总体质量较差的掩膜，这证明网络仍需要大量改进。这可以通过更多的微调和实验来实现。然而，在下一个食谱中，我们将发现一个更适合执行图像分割并能以更少的努力产生真正好掩膜的网络。
- en: We'll discuss what we've just done in the *How it works…* section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*它是如何工作的…*部分讨论我们刚刚做的事情。
- en: How it works…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we implemented an **FCN** for image segmentation. Even though
    we adapted a well-known architecture, **VGG16**, to our purposes, in reality,
    there are many different adaptations of **FCNs** that extend or modify other seminal
    architectures, such as **ResNet50**, **DenseNet**, and other variants of **VGG**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们实现了一个**FCN**用于图像分割。尽管我们将一个广为人知的架构**VGG16**进行了调整以适应我们的需求，实际上，**FCN**有许多不同的变种，它们扩展或修改了其他重要的架构，如**ResNet50**、**DenseNet**以及其他**VGG**的变体。
- en: What we need to remember is that `UpSampling2D()` with bilinear interpolation
    or `ConvTranspose2D()`). The achieved result is that instead of classifying the
    whole image with an output vector of probabilities, we produce an output volume
    that has the same dimensions as the input image, where each pixel contains a probability
    distribution of the classes it can belong to. Such an output volume of pixel-wise
    likelihood is known as a predicted segmentation mask.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要记住的是，`UpSampling2D()`配合双线性插值或`ConvTranspose2D()`的使用。最终的结果是，我们不再用一个输出的概率向量来对整个图像进行分类，而是生成一个与输入图像相同尺寸的输出体积，其中每个像素包含它可能属于的各个类别的概率分布。这种像素级的预测分割掩膜就被称为预测分割掩膜。
- en: See also
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can read more about `Oxford IIIT Pet Dataset`, visit the official site
    here: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以了解更多关于`Oxford-IIIT Pet Dataset`的信息，访问官方站点：[https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)。
- en: Implementing a U-Net from scratch
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始实现 U-Net
- en: It's difficult to talk about image segmentation without mentioning **U-Net**,
    one of the seminal architectures when it comes to pixel-wise classification.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要谈论图像分割，不能不提到**U-Net**，它是像素级分类的经典架构之一。
- en: A **U-Net** is a composite network comprised of an encoder and a decoder, whose
    layers, as the name suggests, are arranged in a U shape. It's intended for fast
    and precise segmentation, and in this recipe, we'll implement one from scratch.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**U-Net**是一个由编码器和解码器组成的复合网络，正如其名，网络层以U形排列。它旨在快速且精确地进行分割，在本食谱中，我们将从零开始实现一个。'
- en: Let's get started, shall we?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧，怎么样？
- en: Getting ready
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this example, we''ll rely on several external libraries, such as TensorFlow
    Datasets, TensorFlow Docs, `Pillow`, and `OpenCV`. The good news is that we can
    easily install them all with `pip`. First, install `tensorflow_docs`, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将依赖几个外部库，如 TensorFlow Datasets、TensorFlow Docs、`Pillow`和`OpenCV`。好消息是，我们可以通过`pip`轻松安装它们。首先，安装`tensorflow_docs`，如下所示：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, install the remaining libraries:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装其余的库：
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will be using `Oxford-IIIT Pet Dataset` in this recipe. However, we don''t
    need to do anything at this stage since we''ll download it and manipulate it using
    `tensorflow-datasets`. In this dataset, the segmentation mask (an image where
    each location contains the class of the corresponding pixel in the original image)
    contains pixels categorized into three classes:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将使用`Oxford-IIIT Pet Dataset`。不过，现阶段我们不需要做任何事情，因为我们将通过`tensorflow-datasets`下载并操作它。在这个数据集中，分割掩膜（一个图像，每个位置包含原始图像中相应像素的类别）包含分类为三类的像素：
- en: '1: The pixel belongs to a pet (cat or dog).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 像素属于宠物（猫或狗）。'
- en: '2: The pixel belongs to the contour of a pet.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2: 像素属于宠物的轮廓。'
- en: '3: The pixel belongs to the surroundings.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3: 像素属于周围环境。'
- en: 'Here are some sample images from the dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据集中的一些示例图像：
- en: '![Figure 8.5 – Sample images from the Oxford-IIIT Pet dataset'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5 – 来自 Oxford-IIIT Pet 数据集的示例图像'
- en: '](img/B14768_08_005.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_005.jpg)'
- en: Figure 8.5 – Sample images from the Oxford-IIIT Pet dataset
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 来自 Oxford-IIIT Pet 数据集的示例图像
- en: Great! Let's start implementing!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！让我们开始实现吧！
- en: How to do it…
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to implement your own **U-Net** so that you can segment
    images of your own pets:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现您自己的**U-Net**，这样您就可以对自己宠物的图像进行分割：
- en: 'Let''s import all the necessary dependencies:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入所有必需的依赖项：
- en: '[PRE29]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`tf.data.experimental.AUTOTUNE`的别名：
- en: '[PRE30]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define a function that will normalize the images in the dataset. We must also
    normalize the masks so that the classes are numbered from 0 through 2, instead
    of from 1 through 3:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于归一化数据集中的图像。我们还需要归一化掩膜，使得类别编号从0到2，而不是从1到3：
- en: '[PRE31]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define a function that will load an image, given an element from a TensorFlow
    dataset data structure. Note that we resize both the image and the mask to *256x256*.
    Also, if the `train` flag is set to `True`, we perform augmentation by randomly
    mirroring the image and its mask. Finally, we normalize the inputs:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，根据TensorFlow数据集结构中的元素加载图像。请注意，我们将图像和掩膜的大小调整为*256x256*。此外，如果`train`标志设置为`True`，我们会通过随机镜像图像及其掩膜来进行数据增强。最后，我们对输入进行归一化处理：
- en: '[PRE32]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, let's define a class, `UNet()`, that will contain all the logic necessary
    to build, train, and evaluate our `RMSProp` as the optimizer and `SparseCategoricalCrossentropy`
    as the loss. Note that `output_channels` is, by default, `3`, because each pixel
    can be categorized into one of three classes.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个`UNet()`类，它将包含构建、训练和评估所需的所有逻辑，使用`RMSProp`作为优化器，`SparseCategoricalCrossentropy`作为损失函数。请注意，`output_channels`默认为`3`，因为每个像素可以被分类为三类之一。
- en: 'Now, let''s define the `_downsample()` helper method, which builds a downsampling
    block. This is a convolution that can be (optionally) batch normalized and that''s
    activated with `LeakyReLU`:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义`_downsample()`助手方法，用于构建下采样块。它是一个卷积层，可以（可选地）进行批量归一化，并通过`LeakyReLU`激活：
- en: '[PRE33]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Conversely, the `_upsample()` helper method expands its input through a transposed
    convolution, which is also batch normalized and `ReLU` activated (optionally,
    we can add a dropout layer to prevent overfitting):'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相反，`_upsample()`助手方法通过转置卷积扩展其输入，该卷积也进行批量归一化，并通过`ReLU`激活（可选地，我们可以添加一个dropout层来防止过拟合）：
- en: '[PRE34]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Armed with `_downsample()` and `_upsample()`, we can iteratively build the
    full **U-Net** architecture. The encoding part of the network is just a stack
    of downsampling blocks, while the decoding portion is, as expected, comprised
    of a series of upsampling blocks:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凭借`_downsample()`和`_upsample()`，我们可以迭代地构建完整的**U-Net**架构。网络的编码部分只是一个下采样块的堆叠，而解码部分则如预期那样，由一系列上采样块组成：
- en: '[PRE35]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In order to shield the network against the vanishing gradient problem (a phenomenon
    where very deep networks forget what they''ve learned), we must add skip connections
    at every level:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了防止网络出现梯度消失问题（即深度网络遗忘已学内容的现象），我们必须在每个层级添加跳跃连接：
- en: '[PRE36]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output layer of the **U-Net** is a transposed convolution whose dimensions
    are the same as the input image''s, but it has as many channels as there are classes
    in the segmentation mask:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**U-Net**的输出层是一个转置卷积，其尺寸与输入图像相同，但它的通道数与分割掩膜中的类别数相同：'
- en: '[PRE37]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s define a `helper` method in order to plot the relevant training curves:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个`helper`方法，用于绘制相关的训练曲线：
- en: '[PRE38]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `train()` method takes the training and validation datasets, as well as
    the number of epochs and training and validation steps to perform, in order to
    fit the model. It also saves the loss and accuracy plots to disk for later analysis:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train()`方法接受训练和验证数据集，以及进行训练所需的轮次、训练和验证步数。它还会将损失和准确率图保存到磁盘，以供后续分析：'
- en: '[PRE39]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define a helper method named `_process_mask()`, which will be used to make
    the segmentation masks compatible with OpenCV. What this function does is create
    a three-channeled version of a grayscale mask and upscale the class values to
    the [0, 255] range:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`_process_mask()`的助手方法，用于将分割掩膜与OpenCV兼容。此函数的作用是创建一个三通道的灰度掩膜版本，并将类别值扩大到[0,
    255]的范围：
- en: '[PRE40]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `_save_image_and_masks()` helper method creates a mosaic of the original
    image, the ground truth mask, and the predicted segmentation mask, and saves it
    to disk for later revision:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_save_image_and_masks()`助手方法会创建一个由原始图像、真实掩膜和预测分割掩膜组成的马赛克，并将其保存到磁盘，供以后修订：'
- en: '[PRE41]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In order to pass the output volume produced by the network to a valid segmentation
    mask, we must take the index of the highest value at each pixel location, which
    corresponds to the most likely category for that pixel. The `_create_mask()` method
    does this:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将网络产生的输出体积传递到有效的分割掩膜，我们必须获取每个像素位置上最高值的索引，这对应于该像素最可能的类别。`_create_mask()`方法执行了这个操作：
- en: '[PRE42]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `_save_predictions()` method uses the `_save_image_and_mask()` helper method
    we defined in *Step 13*:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`_save_predictions()`方法使用了我们在*步骤13*中定义的`_save_image_and_mask()`辅助方法：'
- en: '[PRE43]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `evaluate()` method computes the accuracy of the **U-Net** on the test
    set, and also generates predictions for a sample of images, which are then stored
    on disk:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`evaluate()`方法计算**U-Net**在测试集上的准确度，并为一些图像样本生成预测，之后将其存储到磁盘上：'
- en: '[PRE44]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Download (or load, if cached) `Oxford IIIT Pet Dataset`, along with its metadata,
    using TensorFlow Datasets:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow Datasets下载（或加载，如果已缓存）`Oxford IIIT Pet Dataset`及其元数据：
- en: '[PRE45]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Use the metadata to define the corresponding number of steps the network will
    go over for the training and validation datasets. Also, define the batch and buffer
    sizes:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用元数据来定义网络在训练和验证数据集上将进行的相应步数。还需定义批量和缓冲区大小：
- en: '[PRE46]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the training and testing datasets'' pipelines:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和测试数据集的管道：
- en: '[PRE47]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Instantiate the **U-Net** and train it for 50 epochs:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化**U-Net**并训练50个epoch：
- en: '[PRE48]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Lastly, evaluate the network on the test dataset:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在测试数据集上评估网络：
- en: '[PRE49]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The accuracy on the test set should be around 83% (in my case, I got 83.49%):'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试集上的准确度应该在83%左右（在我的情况下，我得到了83.49%）：
- en: '![Figure 8.6 – Training and validation accuracy curves'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6 – 训练和验证准确度曲线'
- en: '](img/B14768_08_006.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_006.jpg)'
- en: Figure 8.6 – Training and validation accuracy curves
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 训练和验证准确度曲线
- en: Here, we can see that after about epoch 12, the gap between the training and
    validation accuracy curves slowly widens. This isn't a sign of overfitting, but
    an indication that we could do better. How does this accuracy translate to actual
    images?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，大约在第12个epoch之后，训练准确度曲线和验证准确度曲线之间的差距开始慢慢扩大。这不是过拟合的表现，而是表明我们可以做得更好。那么，这个准确度是如何转化为实际图像的呢？
- en: 'Take a look at the following image, which shows the original image, the ground
    truth mask, and the produced mask:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图片，展示了原始图像、地面真实掩膜和生成的掩膜：
- en: '![Figure 8.7 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7 – 原始图像（左侧）、地面真实掩膜（中间）和预测掩膜（右侧）'
- en: '](img/B14768_08_007.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_007.jpg)'
- en: Figure 8.7 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 原始图像（左侧）、地面真实掩膜（中间）和预测掩膜（右侧）
- en: 'Here, we can see that there''s a good resemblance between the ground truth
    mask (center) and the predicted one (right), although there is some noise, such
    as the small white region and the pronounced bump on the lower half of the dog''s
    silhouette, that could be cleaned up with more training:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，地面真实掩膜（中间）和预测掩膜（右侧）之间有很好的相似性，尽管存在一些噪声，比如小的白色区域和狗轮廓下半部分明显的隆起，这些噪声通过更多的训练可以清理掉：
- en: '![Figure 8.8 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8 – 原始图像（左侧）、地面真实掩膜（中间）和预测掩膜（右侧）'
- en: '](img/B14768_08_008.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_008.jpg)'
- en: Figure 8.8 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 原始图像（左侧）、地面真实掩膜（中间）和预测掩膜（右侧）
- en: The preceding image clearly shows that the network could use more training or
    fine-tuning. This is because even though it gets the overall shape and location
    of the dog right, there's really too much noise for this mask to be usable in
    a real-world application.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图片清楚地表明，网络可以进行更多的训练或微调。这是因为尽管它正确地获取了狗的整体形状和位置，但掩膜中仍有太多噪声，导致其无法在实际应用中使用。
- en: Let's head over to the *How it works…* section to connect the dots.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们前往*它是如何工作的……*部分，进一步连结各个环节。
- en: How it works…
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we implemented and trained a **U-Net** from scratch to segment
    the body and contour of household pets. As we saw, the network did learn, but
    still offers room for improvement.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们从头开始实现并训练了一个**U-Net**，用于分割家庭宠物的身体和轮廓。正如我们所看到的，网络确实学到了东西，但仍然有改进的空间。
- en: The ability to semantically segment the contents of an image is of paramount
    importance in several domains, such as in medicine, where what's more important
    than knowing if a condition, such as a malignant tumor, is present, is to determine
    the actual location, shape, and area of said ailment. The field of biomedicine
    is where **U-Net** made its debut. In 2015, it outperformed established methods
    for segmentation, such as sliding-windows convolutional networks, using far less
    data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个领域中，语义分割图像内容的能力至关重要，例如在医学中，比知道是否存在病症（如恶性肿瘤）更重要的是确定病变的实际位置、形状和面积。**U-Net**首次亮相于生物医学领域。2015年，它在使用远少于数据的情况下，超越了传统的分割方法，例如滑动窗口卷积网络。
- en: How does **U-Net** achieve such good results? As we learned in this recipe,
    the key is in its end-to-end nature, where both the encoder and decoder are comprised
    of convolutions that form a contracting path, whose job is to capture context
    and a symmetric expanding path, thereby enabling precise localization.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**U-Net**是如何取得如此好结果的呢？正如我们在本食谱中所学到的，关键在于其端到端的结构，其中编码器和解码器都由卷积组成，形成一个收缩路径，其任务是捕捉上下文信息，还有一个对称的扩展路径，从而实现精确的定位。'
- en: Both of the aforementioned paths can be as deep as needed, depending on the
    nature of the dataset. This depth customization is viable due to the presence
    of skip connections, which allow the gradients to flow farther down the network,
    thus preventing the vanishing gradient problem (this is similar to what **ResNet**
    does, as we learned in [*Chapter 2*](B14768_02_Final_JM_ePub.xhtml#_idTextAnchor053),
    *Performing Image Classification*).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两条路径的深度可以根据数据集的性质进行调整。这种深度定制之所以可行，是因为跳跃连接的存在，它允许梯度在网络中进一步流动，从而防止梯度消失问题（这与**ResNet**所做的类似，正如我们在[*第2章*](B14768_02_Final_JM_ePub.xhtml#_idTextAnchor053)中学到的，*执行图像分类*）。
- en: 'In the next recipe, we''ll combine a very powerful concept with this implementation
    of `Oxford IIIT Pet Dataset`: transfer learning.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个食谱中，我们将结合这个强大的概念与`Oxford IIIT Pet Dataset`的实现：迁移学习。
- en: See also
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'A great way to familiarize yourself with `Oxford IIIT Pet Dataset`, visit the
    official site here: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的方法来熟悉`Oxford IIIT Pet Dataset`，可以访问官方网站：[https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)。
- en: 'In this recipe, we mentioned the vanishing gradient problem a few times, so
    it''s a good idea to understand the concept by reading this article: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们提到过梯度消失问题几次，因此，阅读这篇文章以了解这一概念是个好主意：[https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。
- en: Implementing a U-Net with transfer learning
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现带迁移学习的U-Net
- en: Training a **U-Net** from scratch is a very good first step toward creating
    a performant image segmentation system. However, one of the biggest superpowers
    in deep learning that's applied to computer vision is being able to build solutions
    on top of the knowledge of other networks, which usually leads to faster and better
    results.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练一个**U-Net**是创建高性能图像分割系统的一个非常好的第一步。然而，深度学习在计算机视觉中的最大超能力之一就是能够在其他网络的知识基础上构建解决方案，这通常会带来更快且更好的结果。
- en: Image segmentation is no exception to this rule, and in this recipe, we'll implement
    a better segmentation network using transfer learning.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割也不例外，在本食谱中，我们将使用迁移学习来实现一个更好的分割网络。
- en: Let's begin.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'This recipe is very similar to the previous one (*Implementing a U-Net from
    scratch*), so we''ll only go into depth on the parts that are different. For a
    deeper explanation, I recommend that you complete the *Implementing a U-Net from
    scratch* recipe before attempting this one. As expected, the libraries we''ll
    need are the same as they were for that recipe, all of which can be installed
    using `pip`. Let''s start with `tensorflow_docs`, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱与前一个食谱（*从头开始实现U-Net*）非常相似，因此我们只会深入讨论不同的部分。为了更深入的理解，我建议在尝试本食谱之前，先完成*从头开始实现U-Net*的食谱。正如预期的那样，我们需要的库与之前相同，都可以通过`pip`安装。让我们首先安装`tensorflow_docs`，如下所示：
- en: '[PRE50]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, let''s set up the remaining dependencies:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置剩余的依赖项：
- en: '[PRE51]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Once again, we''ll work with `Oxford-IIIT Pet Dataset`, which can be accessed
    through `tensorflow-datasets`. Each pixel in this dataset falls within one of
    these classes:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用`Oxford-IIIT Pet Dataset`，可以通过`tensorflow-datasets`访问。该数据集中的每个像素都属于以下类别之一：
- en: '1: The pixel belongs to a pet (cat or dog).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1：该像素属于宠物（猫或狗）。
- en: '2: The pixel belongs to the contour of a pet.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2：该像素属于宠物的轮廓。
- en: '3: The pixel belongs to the surroundings.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3：该像素属于周围环境。
- en: 'The following image shows two sample images from the dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了数据集中的两张样本图像：
- en: '![Figure 8.9 – Sample images from the Oxford-IIIT Pet dataset'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.9 – 来自Oxford-IIIT Pet数据集的样本图像](img/B14768_08_009.jpg)'
- en: '](img/B14768_08_009.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_009.jpg)'
- en: Figure 8.9 – Sample images from the Oxford-IIIT Pet dataset
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 来自Oxford-IIIT Pet数据集的样本图像
- en: With that, we are good to go!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以开始了！
- en: How to do it…
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Complete these steps to implement a transfer learning-powered **U-Net**:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下步骤以实现一个基于转移学习的**U-Net**：
- en: 'Import all the needed packages:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有需要的包：
- en: '[PRE52]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`tf.data.experimental.AUTOTUNE`定义一个别名：
- en: '[PRE53]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define a function that will normalize the images and masks in the dataset:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于对数据集中的图像和掩码进行归一化处理：
- en: '[PRE54]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define a function that will load an image and its corresponding mask, given
    an element from a TensorFlow Datasets data structure. Optionally, it should perform
    image mirroring on training images:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，根据TensorFlow Datasets数据结构中的一个元素加载图像及其对应的掩码。可选地，函数可以对训练图像执行图像镜像操作：
- en: '[PRE55]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Define `UNet()`, a container class for the logic necessary to build, train,
    and evaluate our transfer learning-aided `RMSProp` as the optimizer and `SparseCategoricalCrossentropy`
    as the loss. Notice that `output_channels` is, by default, `3`, because each pixel
    can be categorized into one of three classes. The encoder will be a pre-trained
    `MobileNetV2`. However, we'll only use a select group of layers, defined in `self.target_layers`.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`UNet()`，这是一个容器类，包含了构建、训练和评估我们的转移学习辅助的`RMSProp`优化器和`SparseCategoricalCrossentropy`损失函数所需的逻辑。注意，`output_channels`默认值为`3`，因为每个像素可以归类为三种类别之一。编码器将使用预训练的`MobileNetV2`，但我们只会使用其中一部分层，这些层在`self.target_layers`中定义。
- en: 'Now, let''s define the `_upsample()` helper method, which builds an upsampling
    block:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们来定义`_upsample()`辅助方法，构建一个上采样模块：
- en: '[PRE56]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Armed with our pre-trained `MobileNetV2` and `_upsample()`, we can iteratively
    build the full `self.target_layers`, which are frozen (`down_stack.trainable =
    False`), meaning we only train the decoder or upsampling blocks of the architecture:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用我们预训练的`MobileNetV2`和`_upsample()`方法，我们可以逐步构建完整的`self.target_layers`，这些层被冻结（`down_stack.trainable
    = False`），这意味着我们只训练解码器或上采样模块：
- en: '[PRE57]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we can add the skip connections to facilitate the flow of the gradient
    throughout the network:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以添加跳跃连接，以促进梯度在网络中的流动：
- en: '[PRE58]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output layer of the **U-Net** is a transposed convolution that has the
    same dimensions as the input image, but has as many channels as there are classes
    in the segmentation mask:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**U-Net**的输出层是一个转置卷积，其尺寸与输入图像相同，但通道数与分割掩码中的类别数相同：'
- en: '[PRE59]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Define `_plot_model_history()`, a helper method that plots the relevant training
    curves:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`_plot_model_history()`，一个辅助方法，用于绘制相关的训练曲线：
- en: '[PRE60]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Define the `train()` method, which is in charge of fitting the model:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`train()`方法，负责拟合模型：
- en: '[PRE61]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define `_process_mask()`, a helper method that makes the segmentation masks
    compatible with OpenCV:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`_process_mask()`，一个辅助方法，使分割掩码与OpenCV兼容：
- en: '[PRE62]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Define the `_save_image_and_masks()` helper method to create a visualization
    of the original image, along with the real and predicted masks:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`_save_image_and_masks()`辅助方法，用于创建原始图像的可视化，以及真实和预测的掩码：
- en: '[PRE63]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Define `_create_mask()`, which produces a valid segmentation mask from the
    network''s predictions:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`_create_mask()`，该方法根据网络的预测生成有效的分割掩码：
- en: '[PRE64]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The `_save_predictions()` method uses the `_save_image_and_mask()` helper method,
    which we defined in *Step 13*:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_save_predictions()`方法使用了我们在*第13步*中定义的`_save_image_and_mask()`辅助方法：'
- en: '[PRE65]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The `evaluate()` method computes the accuracy of the **U-Net** on the test
    set, while also generating predictions for a sample of images. These are then
    stored on disk:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`evaluate()`方法计算**U-Net**在测试集上的准确度，并生成一组图像样本的预测。预测结果随后会被存储到磁盘上：'
- en: '[PRE66]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Download (or load, if cached) `Oxford IIIT Pet Dataset`, along with its metadata,
    using TensorFlow Datasets:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow Datasets下载（或加载缓存的）`Oxford IIIT Pet Dataset`及其元数据：
- en: '[PRE67]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Use the metadata to define the corresponding number of steps the network will
    take over the training and validation datasets. Also, define the batch and buffer
    sizes:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用元数据定义网络在训练和验证数据集上将执行的步骤数。还要定义批量大小和缓存大小：
- en: '[PRE68]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Define the training and testing datasets'' pipelines:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和测试数据集的管道：
- en: '[PRE69]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Instantiate the **U-Net** and train it for 30 epochs:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化**U-Net**并训练它30个周期：
- en: '[PRE70]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Evaluate the network on the test dataset:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据集上评估网络：
- en: '[PRE71]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The accuracy on the test set should be close to 90% (in my case, I obtained
    90.78% accuracy):'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在测试集上的准确率应该接近90%（在我的案例中，我得到了90.78%的准确率）：
- en: '![Figure 8.10 – Training and validation accuracy curves'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10 – 训练和验证准确率曲线'
- en: '](img/B14768_08_010.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_010.jpg)'
- en: Figure 8.10 – Training and validation accuracy curves
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 训练和验证准确率曲线
- en: The accuracy curves show that the network is not overfitting because both the
    training and validation plots follow the same trajectory, with a very thin gap.
    This also confirms that the knowledge the model is acquiring is transferrable
    and usable on unseen data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率曲线显示，网络没有发生过拟合，因为训练和验证图表遵循相同的轨迹，且差距非常小。这也确认了模型所获得的知识是可迁移的，并且可以用于未见过的数据。
- en: 'Let''s take a look at some of the outputs from the network, starting with the
    following image:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下网络的一些输出，从以下图像开始：
- en: '![Figure 8.11 – The original image (left), the ground truth mask (center),
    and the predicted mask (right)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.11 – 原始图像（左）、真实标签掩码（中）和预测掩码（右）'
- en: '](img/B14768_08_011.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_011.jpg)'
- en: Figure 8.11 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 原始图像（左）、真实标签掩码（中）和预测掩码（右）
- en: 'Compared to *Figure 8.7* in the *Implementing a U-Net from scratch* recipe,
    in the preceding image, we can see that the **U-Net** produces a much cleaner
    result, with the background (gray pixels), contour (white pixels), and pet (black
    pixels) clearly separated and almost identical to the ground truth mask (center):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 与*图 8.7*中*从头实现U-Net*一节中的结果相比，在前面的图像中，我们可以看到**U-Net**产生了一个更干净的结果，背景（灰色像素）、轮廓（白色像素）和宠物（黑色像素）被清晰地分开，并且几乎与真实标签掩码（中）完全相同：
- en: '![Figure 8.12 – The original image (left), the ground truth mask (center),
    and the predicted mask (right)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.12 – 原始图像（左）、真实标签掩码（中）和预测掩码（右）'
- en: '](img/B14768_08_012.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_012.jpg)'
- en: Figure 8.12 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 原始图像（左）、真实标签掩码（中）和预测掩码（右）
- en: The preceding image is a great improvement in comparison to *Figure 8.8* in
    the *Implementing a U-Net from scratch* recipe. This time, the predicted mask
    (right), although not perfect, presents less noise and is much closer to the actual
    segmentation mask (center).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与*从头实现U-Net*一节中的*图 8.8*相比，前面的图像是一个显著的改进。这次，预测掩码（右），虽然不是完美的，但呈现出更少的噪声，并且更接近实际的分割掩码（中）。
- en: We'll dig deeper in the *How it works…* section.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*它是如何工作的…*一节中深入探讨。
- en: How it works…
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we made a small yet substantial change to the `MobileNetV2`
    trained on the massive `ImageNet` dataset.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们对在庞大的`ImageNet`数据集上训练的`MobileNetV2`做了一个小但重要的改动。
- en: The reason transfer learning worked so well in this context is that there are
    hundreds of classes in `ImageNet` focused on different breeds of cats and dogs,
    meaning the overlap with `Oxford IIIT Pet` is very substantial. However, if this
    wasn't the case, this doesn't mean we should drop transfer learning entirely!
    What we should do in that situation is fine-tune the encoder by making some (or
    all) of its layers trainable.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在这个场景中效果如此出色的原因是，`ImageNet`中有成百上千的类别，专注于不同品种的猫和狗，这意味着与`Oxford IIIT Pet`数据集的重叠非常大。然而，如果情况并非如此，这并不意味着我们应该完全放弃迁移学习！我们在这种情况下应该做的是通过使编码器的某些（或全部）层可训练，来微调它。
- en: By leveraging the knowledge encoded in `MobileNetV2`, we were able to bump the
    accuracy on the test set from 83% up to 90%, an impressive gain that translated
    into better, cleaner prediction masks, even on challenging examples.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用`MobileNetV2`中编码的知识，我们将测试集上的准确率从83%提升到了90%，这是一项令人印象深刻的提升，带来了更好、更清晰的预测掩码，即使是在具有挑战性的例子上。
- en: See also
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can read the original `Oxford IIIT Pet Dataset`, please go to [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).
    To learn how to combat the vanishing gradient problem, read this article: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以阅读原始的`Oxford IIIT Pet Dataset`，请访问[https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)。想了解如何解决梯度消失问题，请阅读这篇文章：[https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。
- en: Segmenting images using Mask-RCNN and TensorFlow Hub
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Mask-RCNN和TensorFlow Hub进行图像分割
- en: '`COCO` dataset. This will help us perform out-of-the-box object detection and
    image segmentation.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`COCO`数据集。这将帮助我们进行开箱即用的物体检测和图像分割。'
- en: Getting ready
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, we must install `Pillow` and **TFHub**, as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须安装`Pillow`和**TFHub**，如下所示：
- en: '[PRE72]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We also need to install the `cd` to a location of your preference and clone
    the `tensorflow/models` repository:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将`cd`安装到你选择的位置，并克隆`tensorflow/models`仓库：
- en: '[PRE73]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, install the **TensorFlow Object Detection API**, like this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装**TensorFlow对象检测API**，方法如下：
- en: '[PRE74]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: That's it! Let's get started.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！让我们开始吧。
- en: How to do it…
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Follow these steps to learn how to segment your images using **Mask-RCNN**:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤学习如何使用**Mask-RCNN**进行图像分割：
- en: 'Import the necessary packages:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE75]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a function that will load an image into a NumPy array:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将图像加载到NumPy数组中：
- en: '[PRE76]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define a function that will make predictions with **Mask-RCNN** and save the
    results to disk. Start by loading the image and passing it through the model:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，使用**Mask-RCNN**进行预测，并将结果保存到磁盘。首先加载图像并将其输入到模型中：
- en: '[PRE77]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Convert the results into `NumPy` arrays:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果转换为`NumPy`数组：
- en: '[PRE78]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Extract both the detection masks and boxes from the model output and convert
    them into tensors:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型输出中提取检测掩膜和框，并将它们转换为张量：
- en: '[PRE79]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Reframe the box masks to image masks:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将框掩膜转换为图像掩膜：
- en: '[PRE80]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Create a visualization of the detections and their boxes, scores, classes,
    and masks:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可视化图，显示检测结果及其框、得分、类别和掩膜：
- en: '[PRE81]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Save the result to disk:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果保存到磁盘：
- en: '[PRE82]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Load the `COCO` dataset''s category index:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`COCO`数据集的类别索引：
- en: '[PRE83]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Load **Mask-RCNN** from **TFHub**:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**TFHub**加载**Mask-RCNN**：
- en: '[PRE84]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Run `output` folder. Let''s review an easy one:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`output`文件夹。让我们回顾一个简单的例子：
- en: '![Figure 8.13– Single instance of segmentation'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.13– 单实例分割'
- en: '](img/B14768_08_013.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_013.jpg)'
- en: Figure 8.13– Single instance of segmentation
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13– 单实例分割
- en: 'Here, we can see that the network correctly detected and segmented the dog
    with 100% accuracy! Let''s try a more challenging one:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到网络正确地检测并分割出了狗，准确率为100%！我们来尝试一个更具挑战性的例子：
- en: '![Figure 8.14 – Multiple instances of segmentation'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.14 – 多实例分割'
- en: '](img/B14768_08_014.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_014.jpg)'
- en: Figure 8.14 – Multiple instances of segmentation
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 多实例分割
- en: 'This image is much more crowded than the previous one, and even then, the network
    correctly identified most of the objects in the scene (cars, people, trucks, and
    so on) – even occluded ones! However, the model fails in some circumstances, as
    shown in the following image:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片比之前的更拥挤，尽管如此，网络仍然正确地识别出了场景中的大部分物体（如汽车、人、卡车等）——即使是被遮挡的物体！然而，模型在某些情况下仍然失败，如下图所示：
- en: '![Figure 8.15 – Segmentation with errors and redundancies'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15 – 带有错误和冗余的分割'
- en: '](img/B14768_08_015.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_08_015.jpg)'
- en: Figure 8.15 – Segmentation with errors and redundancies
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 带有错误和冗余的分割
- en: This time, the network correctly identified me and my dogs, as well as the coffee
    cup and the couch, but it threw duplicate and nonsensical detections, such as
    my leg being a person. This happened because I'm holding my dog, and parts of
    my body are disconnected in the photo, leading to incorrect or low confidence
    segmentations.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，网络正确地识别了我和我的狗，还有咖啡杯和沙发，但它却出现了重复和荒谬的检测结果，比如我的腿被识别为一个人。这是因为我抱着我的狗，照片中我的身体部分被分离，导致了不正确或置信度低的分割。
- en: Let's head over to the next section.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个部分。
- en: How it works…
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In this recipe, we learned how to detect objects and perform image segmentation
    using one of the most powerful neural networks in existence: **Mask-RCNN**. Training
    such a model is not an easy task, let alone implementing it from scratch! Fortunately,
    thanks to **TensorFlow Hub**, we were able to use all its predicting power with
    just a few lines of code.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇教程中，我们学习了如何使用现有最强大的神经网络之一：**Mask-RCNN**，来检测物体并执行图像分割。训练这样的模型并非易事，更不用说从零开始实现了！幸运的是，得益于**TensorFlow
    Hub**，我们能够通过仅几行代码使用它的全部预测能力。
- en: We must take into consideration that this pre-trained model will work best on
    images containing objects the network has been trained on. More precisely, the
    more the images that we pass to `COCO`, the better the results will be. Nevertheless,
    a degree of tweaking and experimentation is always needed in order to achieve
    the best detections possible because, as we saw in the previous example, the network,
    although great, isn't perfect.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须考虑到，这个预训练模型在包含网络已训练过的物体的图像上表现最好。更具体地说，我们传递给`COCO`的图像越多，结果越好。不过，为了实现最佳检测效果，仍然需要一定程度的调整和实验，因为正如我们在前面的例子中看到的，虽然网络非常强大，但并不完美。
- en: See also
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can learn more about the model we used here: [https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1).
    Also, reading the **Mask-RCNN** paper is a sound decision: [https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里了解我们使用的模型：[https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1)。此外，阅读**Mask-RCNN**的论文也是一个明智的决定：[https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)。
