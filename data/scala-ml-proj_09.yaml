- en: Fraud Analytics Using Autoencoders and Anomaly Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器和异常检测进行欺诈分析
- en: Detecting and preventing fraud in financial companies, such as banks, insurance
    companies, and credit unions, is an important task in order to see a business
    grow. So far, in the previous chapter, we have seen how to use classical supervised
    machine learning models; now it's time to use other, unsupervised learning algorithms,
    such as autoencoders.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融公司，如银行、保险公司和信用合作社，检测和防止欺诈是一个重要任务，这对于业务的增长至关重要。到目前为止，在上一章中，我们已经学习了如何使用经典的有监督机器学习模型；现在是时候使用其他的无监督学习算法，比如自编码器。
- en: In this chapter, we will use a dataset having more than 284,807 instances of
    credit card use and for each transaction, where only 0.172% transactions are fraudulent.
    So, this is highly imbalanced data. And hence it would make sense to use autoencoders
    to pre-train a classification model and apply an anomaly detection technique to
    predict possible fraudulent transactions; that is, we expect our fraud cases to
    be anomalies within the whole dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个包含超过284,807个信用卡使用实例的数据集，其中只有0.172%的交易是欺诈交易。因此，这是一个高度不平衡的数据。因此，使用自编码器来预训练分类模型并应用异常检测技术以预测可能的欺诈交易是有意义的；也就是说，我们预计欺诈案件将在整个数据集中表现为异常。
- en: 'In summary, we will learn the following topics through this end-to-end project:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，通过这个端到端项目，我们将学习以下主题：
- en: Outlier and anomaly detection using outliers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用异常值进行异常检测
- en: Using autoencoders in unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无监督学习中使用自编码器
- en: Developing a fraud analytics predictive model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发一个欺诈分析预测模型
- en: Hyperparameters tuning, and most importantly, feature selection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调优，最重要的是特征选择
- en: Outlier and anomaly detection
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值和异常检测
- en: Anomalies are the unusual and unexpected patterns in an observed world. Thus
    analyzing, identifying, understanding, and predicting anomalies from seen and
    unseen data is one of the most important task in data mining. Therefore, detecting
    anomalies allows extracting critical information from data which then can be used
    for numerous applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 异常是观察世界中不寻常和意外的模式。因此，分析、识别、理解和预测从已知和未知数据中的异常是数据挖掘中最重要的任务之一。因此，检测异常可以从数据中提取关键信息，这些信息随后可以用于许多应用。
- en: While anomaly is a generally accepted term, other synonyms, such as outliers,
    discordant observations, exceptions, aberrations, surprises, peculiarities or
    contaminants, are often used in different application domains. In particular,
    anomalies and outliers are often used interchangeably. Anomaly detection finds
    extensive use in fraud detection for credit cards, insurance or health care, intrusion
    detection for cyber-security, fault detection in safety critical systems, and
    military surveillance for enemy activities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然异常是一个广泛接受的术语，但在不同的应用领域中，通常会使用其他同义词，如异常值、背离观察、例外、偏差、惊讶、特异性或污染物。特别是，异常和异常值常常可以互换使用。异常检测在信用卡、保险或医疗保健的欺诈检测、网络安全的入侵检测、安全关键系统的故障检测以及敌方活动的军事监视等领域中得到了广泛应用。
- en: 'The importance of anomaly detection stems from the fact that for a variety
    of application domains anomalies in data often translate to significant actionable
    insights. When we start exploring a highly unbalanced dataset, there are three
    possible interpretation of your dataset using kurtosis.  Consequently, the following
    questions need to be answered and known by means of data exploration before applying
    the feature engineering:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测的重要性源于这样一个事实：在许多应用领域，数据中的异常通常转化为具有重要可操作性的信息。当我们开始探索一个高度不平衡的数据集时，可以使用峰度对数据集进行三种可能的解释。因此，在应用特征工程之前，以下问题需要通过数据探索来回答和理解：
- en: What is the percentage of the total data being present or not having null or
    missing values for all the available fields? Then try to handle those missing
    values and interpret them well without losing the data semantics.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有可用字段中，数据中存在或不存在空值或缺失值的比例是多少？然后，尝试处理这些缺失值，并在不丢失数据语义的情况下很好地解释它们。
- en: What is the correlation between the fields? What is the correlation of each
    field with the predicted variable? What values do they take (that is, categorical
    or on categorical, numerical or alpha-numerical, and so on)?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各个字段之间的相关性是什么？每个字段与预测变量之间的相关性是什么？它们取什么值（即，分类的或非分类的、数值的或字母数字的，等等）？
- en: 'Then find out if the data distribution is skewed or not. You can identify the
    skewness by seeing the outliers or long tail (slightly skewed to the right or
    positively skewed, slightly skewed to the left or negatively skewed, as shown
    in Figure 1). Now identify if the outliers contribute towards making the prediction
    or not. More statistically, your data has one of the 3 possible kurtosis as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后找出数据分布是否有偏。你可以通过查看异常值或长尾来识别偏度（如图 1 所示，可能是稍微偏向右侧或正偏，稍微偏向左侧或负偏）。现在确定异常值是否有助于预测。更准确地说，你的数据具有以下三种可能的峰度之一：
- en: Mesokurtic if the measure of kurtosis is less than but almost equal to 3
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果峰度值小于但接近 3，则为正态峰态（Mesokurtic）
- en: Leptokurtic if the measure of kurtosis is more than 3
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果峰度值大于 3，则为高峰态（Leptokurtic）
- en: Platykurtic if the measure of kurtosis is less than 3
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果峰度值小于 3，则为低峰态（Platykurtic）
- en: '![](img/1fa1b01a-9ef4-40c5-9b15-4fc6f24f7ff0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fa1b01a-9ef4-40c5-9b15-4fc6f24f7ff0.png)'
- en: 'Figure 1: Different kind of skewness in imbalance dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不平衡数据集中不同类型的偏度
- en: Let’s give an example. Suppose you are interested in fitness walking and you
    walked on a sports ground or countryside in the last four weeks (excluding the
    weekends). You spent the following time (in minutes to finish a 4 KM walking track):15,
    16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 25.15, 27.27, 25.24, 21.05, 21.65,
    20.92, 22.61, 23.71, 35, 39, and 50\. Compute and interpret the skewness and kurtosis
    of these values using R would produce a density plot as follows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子。假设你对健身步行感兴趣，在过去四周内（不包括周末），你曾在运动场或乡间步行。你花费的时间如下（完成 4 公里步行赛道所需的分钟数）：15，16，18，17.16，16.5，18.6，19.0，20.4，20.6，25.15，27.27，25.24，21.05，21.65，20.92，22.61，23.71，35，39，50。使用
    R 计算并解释这些值的偏度和峰度，将生成如下的密度图。
- en: The interpretation presented in *Figure 2* of the distribution of data (workout
    times) shows the density plot is skewed to the right so is leptokurtic. So the
    data points to the right-most position can be thought as the unusual or suspicious
    for our use case. So we can potentially identify or remove them to make our dataset
    balanced. However, this is not the purpose of this project but only the identification
    is.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2*中关于数据分布（运动时间）的解释显示，密度图右偏，因此是高峰态（leptokurtic）。因此，位于最右端的数据点可以被视为在我们使用场景中不寻常或可疑。因此，我们可以考虑识别或移除它们以使数据集平衡。然而，这不是该项目的目的，目的仅仅是进行识别。'
- en: '![](img/7b9c075a-e64b-4e9d-8ed8-591699664df1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b9c075a-e64b-4e9d-8ed8-591699664df1.png)'
- en: 'Figure 2: Histogram of the workout time (right-skewed)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：运动时间的直方图（右偏）
- en: Nevertheless, by removing the long tail, we cannot remove the imbalance completely.
    There is another workaround called outlier detection and removing those data points
    would be useful.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过去除长尾，我们不能完全去除不平衡问题。还有另一种方法叫做异常值检测，去除这些数据点可能会有帮助。
- en: 'Moreover, we can also look at the box-plots for each individual feature. Where
    the box plot displays the data distribution based on five-number summaries: **minimum**,
    **first quartile**, median, **third quartile**, and **maximum**, as shown in *Figure
    3*, where we can look for outliers beyond three (3) **Inter-Quartile Range** (**IQR**):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以查看每个单独特征的箱型图。箱型图根据五数概括显示数据分布：**最小值**、**第一个四分位数**、中位数、**第三个四分位数**和**最大值**，如*图
    3*所示，我们可以通过查看超出三倍四分位距（**IQR**）的异常值来判断：
- en: '![](img/e5b1e2a8-6ec6-4f45-af32-39e594a0f7d0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5b1e2a8-6ec6-4f45-af32-39e594a0f7d0.png)'
- en: 'Figure 3: Outliers beyond three (3) Inter-Quartile Range (IQR)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：超出三倍四分位距（IQR）的异常值
- en: Therefore, it would be useful to explore if removing the long tail could provide
    better predictions for supervised or unsupervised learning. But there is no concrete
    recommendation for this highly unbalanced dataset. In short, the skewness analysis
    does not help us in this regard.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，探索去除长尾是否能为监督学习或无监督学习提供更好的预测是有用的。但对于这个高度不平衡的数据集，暂时没有明确的建议。简而言之，偏度分析在这方面对我们没有帮助。
- en: Finally, if you observe your model cannot provide you the perfect classification
    but the **mean square error** (**MSE**) can provide some clue on finding the outlier
    or anomaly. For example, in our case,  even if our projected model cannot classify
    your dataset into fraud and non-fraud cases but the mean MSE is definitely higher
    for fraudulent transactions than for regular ones. So even it would sound naïve,
    still we can identify outlier instances by applying an MSE threshold for what
    we can consider outliers. For example, we can think of an instance with an MSE
    > 0.02 to be an anomaly/outlier.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你发现你的模型无法提供完美的分类，但**均方误差**（**MSE**）可以为你提供一些线索，帮助识别异常值或异常数据。例如，在我们的案例中，即使我们投影的模型不能将数据集分为欺诈和非欺诈案例，欺诈交易的均方误差（MSE）肯定高于常规交易。所以，即使听起来有些天真，我们仍然可以通过应用MSE阈值来识别异常值。例如，我们可以认为MSE
    > 0.02的实例是异常值/离群点。
- en: Now question would be how we can do so? Well, through this end-to-end project,
    we will see that how to use autoencoders and anomaly detection. We will also see
    how to use autoencoders to pre-train a classification model. Finally, we’ll see
    how we can measure model performance on unbalanced data. Let's get started with
    some knowing about autoencoders.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么问题是，我们该如何做到这一点呢？通过这个端到端的项目，我们将看到如何使用自编码器和异常检测。我们还将看到如何使用自编码器来预训练一个分类模型。最后，我们将看到如何在不平衡数据上衡量模型的表现。让我们从了解自编码器开始。
- en: Autoencoders and unsupervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器和无监督学习
- en: Autoencoders are artificial neural networks capable of learning efficient representations
    of the input data without any supervision (that is, the training set is unlabeled).
    This coding, typically, has a much lower dimensionality than the input data, making
    autoencoders useful for dimensionality reduction. More importantly, autoencoders
    act as powerful feature detectors, and they can be used for unsupervised pre-training
    of deep neural networks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是能够在没有任何监督的情况下（即训练集没有标签）学习输入数据高效表示的人工神经网络。这种编码通常具有比输入数据更低的维度，使得自编码器在降维中非常有用。更重要的是，自编码器充当强大的特征检测器，可以用于深度神经网络的无监督预训练。
- en: Working principles of an autoencoder
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器的工作原理
- en: 'An autoencoder is a network with three or more layers, where the input layer
    and the output layer have the same number of neurons, and the intermediate (hidden)
    layers have a lower number of neurons. The network is trained to simply reproduce
    in output, for each input data, the same pattern of activity in the input. The
    remarkable aspect of the problem is that, due to the lower number of neurons in
    the hidden layer, if the network can learn from examples, and generalize to an
    acceptable extent, it performs data compression: the status of the hidden neurons
    provides, for each example, a compressed version of the input and output common
    states.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一个包含三层或更多层的网络，其中输入层和输出层具有相同数量的神经元，而中间（隐藏）层的神经元数量较少。该网络的训练目标是仅仅将每个输入数据的输入模式在输出中再现。问题的显著特点是，由于隐藏层中的神经元数量较少，如果网络能够从示例中学习，并在可接受的范围内进行概括，它将执行数据压缩：隐藏神经元的状态为每个示例提供了输入和输出公共状态的压缩版本。
- en: 'The remarkable aspect of the problem is that, due to the lower number of neurons
    in the hidden layer, if the network can learn from examples, and generalize in
    an acceptable extent, it performs *data compression*: the status of the hidden
    neurons provides, for each example, a *compressed version* of the *input* and
    *output common states*. Useful applications of autoencoders are **data denoising**
    and **dimensionality** **reduction** for data visualization.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的一个显著特点是，由于隐藏层中的神经元数量较少，如果网络能够从示例中学习，并在可接受的范围内进行概括，它将执行*数据压缩*：隐藏神经元的状态为每个示例提供了*压缩版本*的*输入*和*输出公共状态*。自编码器的有用应用包括**数据去噪**和**数据可视化的降维**。
- en: 'The following schema shows how an autoencoder typically works. It reconstructs
    the received input through two phases: an encoding phase that corresponds to a
    dimensional reduction for the original input*,* and a decoding phase, capable
    of reconstructing the original input from the encoded (compressed) representation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了自编码器的典型工作原理。它通过两个阶段来重建接收到的输入：一个编码阶段，它对应于原始输入的维度缩减，*和*一个解码阶段，它能够从编码（压缩）表示中重建原始输入：
- en: '![](img/844e530e-9cd2-4222-95d4-a3e29a4cd44a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/844e530e-9cd2-4222-95d4-a3e29a4cd44a.png)'
- en: 'Figure 4: Encoder and decoder phases in autoencoder'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：自编码器中的编码器和解码器阶段
- en: 'As an unsupervised neural network, an autoencoders main characteristic is in
    its symmetrical structure. An autoencoder has two components: an encoder that
    converts the inputs to an internal representation, followed by a decoder that
    converts back the internal representation to the outputs. In other words, an autoencoder
    can be seen as a combination of an encoder, where we encode some input into a
    code, and a decoder, where we decode/reconstruct the code back to its original
    input as the output. Thus, a **Multi-Layer Perceptron** (**MLP**) typically has
    the same architecture as an autoencoder, except that the number of neurons in
    the output layer must be equal to the number of inputs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种无监督神经网络，自编码器的主要特点在于其对称结构。自编码器有两个组成部分：一个编码器将输入转换为内部表示，接着是一个解码器将内部表示转换回输出。换句话说，自编码器可以看作是编码器和解码器的组合，其中编码器将输入编码为代码，而解码器则将代码解码/重建为原始输入作为输出。因此，**多层感知机**（**MLP**）通常具有与自编码器相同的结构，除了输出层中的神经元数量必须等于输入数量。
- en: As mentioned earlier, there is more than one way to train an autoencoder. The
    first one is by training the whole layer at once, similar to MLP. Although, instead
    of using some labeled output when calculating the cost function (as in supervised
    learning), we use the input itself. So, the `cost` function shows the difference
    between the actual input and the reconstructed input.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练自编码器的方式不止一种。第一种方法是一次性训练整个层，类似于多层感知机（MLP）。不过，与使用一些标记输出计算代价函数（如监督学习中一样）不同的是，我们使用输入本身。因此，`代价`函数显示实际输入与重构输入之间的差异。
- en: The second way is by greedy-training one layer at a time. This training implementation
    comes from the problem that was created by the backpropagation method in supervised
    learning (for example, classification). In a network with a large number of layers,
    the backpropagation method became very slow and inaccurate in gradient calculation.
    To solve this problem, Geoffrey Hinton applied some pretraining methods to initialize
    the classification weight, and this pretraining method was done to two neighboring
    layers at a time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是通过贪心训练逐层进行。这种训练实现源自于监督学习中反向传播方法所带来的问题（例如，分类）。在具有大量层的网络中，反向传播方法在梯度计算中变得非常缓慢和不准确。为了解决这个问题，Geoffrey
    Hinton应用了一些预训练方法来初始化分类权重，而这种预训练方法是一次对两个相邻层进行的。
- en: Efficient data representation with autoencoders
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效的数据表示与自编码器
- en: 'A big problem that plagues all supervised learning systems is the so-called
    **curse of dimensionality**: a progressive decline in performance while increasing
    the input space dimension. This occurs because the number of necessary samples
    to obtain a sufficient sampling of the input space increases exponentially with
    the number of dimensions. To overcome these problems, some optimizing networks
    have been developed.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有监督学习系统面临的一个大问题是所谓的**维度诅咒**：随着输入空间维度的增加，性能逐渐下降。这是因为为了充分采样输入空间，所需的样本数随着维度的增加呈指数级增长。为了解决这些问题，已经开发出一些优化网络。
- en: 'The first are autoencoders networks: these are designed and trained for transforming
    an input pattern in itself, so that, in the presence of a degraded or incomplete
    version of an input pattern, it is possible to obtain the original pattern. The
    network is trained to create output data such as that presented in the entrance,
    and the hidden layer stores the data compressed, that is, a compact representation
    that captures the fundamental characteristics of the input data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类是自编码器网络：这些网络被设计和训练用来将输入模式转化为其自身，以便在输入模式的降级或不完整版本出现时，能够恢复原始模式。网络经过训练，可以生成与输入相似的输出数据，而隐藏层则存储压缩后的数据，也就是捕捉输入数据基本特征的紧凑表示。
- en: 'The second optimizing networks are **Boltzmann machines**: these types of networks
    consist of an input/output visible layer and one hidden layer. The connections
    between the visible layer and the hidden one are non-directional: data can travel
    in both directions, visible-hidden and hidden-visible, and the different neuronal
    units can be fully connected or partially connected.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类优化网络是**玻尔兹曼机**：这类网络由一个输入/输出可见层和一个隐藏层组成。可见层和隐藏层之间的连接是无方向的：数据可以双向流动，即从可见层到隐藏层，或从隐藏层到可见层，不同的神经元单元可以是完全连接的或部分连接的。
- en: 'Let''s see an example. Decide which of the following series you think would
    be easier to memorize:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。决定以下哪个序列你认为更容易记住：
- en: 45, 13, 37, 11, 23, 90, 79, 24, 87, 47
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 45, 13, 37, 11, 23, 90, 79, 24, 87, 47
- en: 50, 25, 76, 38, 19, 58, 29, 88, 44, 22, 11, 34, 17, 52, 26, 13, 40, 20
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50, 25, 76, 38, 19, 58, 29, 88, 44, 22, 11, 34, 17, 52, 26, 13, 40, 20
- en: Seeing the preceding two series, it seems the first series would be easier for
    a human, because it is shorter, containing only a few numbers compared to the
    second one. However, if you take a careful look at the second series, you would
    find that even numbers are exactly two times the following numbers. Whereas the
    odd numbers are followed by a number times three plus one. This is a famous number
    sequence called the **hailstone sequence**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 看完前面两个序列，似乎第一个序列对于人类来说更容易记住，因为它更短，包含的数字比第二个序列少。然而，如果仔细观察第二个序列，你会发现偶数正好是后一个数字的两倍，而奇数后面跟着一个数字，乘以三再加一。这是一个著名的数字序列，叫做**冰雹序列**。
- en: 'However, if you can easily memorize long series, you can also recognize patterns
    in the data easily and quickly. During the 1970s, researchers observed that expert
    chess players were able to memorize the positions of all the pieces in a game
    by looking at the board for just five seconds. This might sound controversial,
    but chess experts don''t have a more powerful memory than you and I do. The thing
    is that they can realize the chess patterns more easily than a non-chess player
    does. An autoencoder works such that it first observes the inputs, converts them
    to a better and internal representation, and can swallow similar to what it has
    already learned:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你能轻松记住长序列，你也能更轻松、更快速地识别数据中的模式。在1970年代，研究人员观察到，国际象棋高手能够仅仅通过看棋盘五秒钟，就记住游戏中所有棋子的摆放位置。听起来可能有些争议，但国际象棋专家的记忆力并不比你我更强大。问题在于，他们比非棋手更容易识别棋盘上的模式。自编码器的工作原理是，它首先观察输入，将其转化为更好的内部表示，并能够吸收它已经学习过的内容：
- en: '![](img/6b8afe47-97cc-4bc8-8cd9-12e01f78432d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b8afe47-97cc-4bc8-8cd9-12e01f78432d.png)'
- en: 'Figure 5: Autoencoder in chess game perspective'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：国际象棋游戏中的自编码器
- en: 'Take a look at a more realistic figure concerning the chess example we just
    discussed: the hidden layer has two neurons (that is, the encoder itself), whereas
    the output layer has three neurons (in other words, the decoder). Because the
    internal representation has a lower dimensionality than the input data (it is
    2D instead of 3D), the autoencoder is said to be under complete. An under complete
    autoencoder cannot trivially copy its inputs to the coding, yet it must find a
    way to output a copy of its inputs.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下一个更现实的图形，关于我们刚才讨论的国际象棋例子：隐藏层有两个神经元（即编码器本身），而输出层有三个神经元（换句话说，就是解码器）。因为内部表示的维度低于输入数据（它是2D而不是3D），所以这个自编码器被称为“欠完备”。一个欠完备的自编码器无法轻松地将其输入复制到编码中，但它必须找到一种方法输出其输入的副本。
- en: It is forced to learn the most important features in the input data and drop
    the unimportant ones. This way, an autoencoder can be compared with **Principal
    Component Analysis** (**PCA**), which is used to represent a given input using
    a lower number of dimensions than originally present.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它被迫学习输入数据中最重要的特征，并丢弃不重要的特征。通过这种方式，自编码器可以与**主成分分析**（**PCA**）进行比较，PCA用于使用比原始数据更少的维度来表示给定的输入。
- en: Up to this point, we know how an autoencoder works. Now, it would be worth knowing
    anomaly detection using outlier identification.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了自编码器是如何工作的。现在，了解通过离群值识别进行异常检测将会很有意义。
- en: Developing a fraud analytics model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发欺诈分析模型
- en: 'Before we fully start, we need to do two things: know the dataset, and then
    prepare our programming environment.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完全开始之前，我们需要做两件事：了解数据集，然后准备我们的编程环境。
- en: Description of the dataset and using linear models
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集的描述与线性模型的使用
- en: 'For this project, we will be using the credit card fraud detection dataset
    from Kaggle. The dataset can be downloaded from [https://www.kaggle.com/dalpozz/creditcardfraud](https://www.kaggle.com/dalpozz/creditcardfraud).
    Since I am using the dataset, it would be a good idea to be transparent by citing
    the following publication:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将使用Kaggle上的信用卡欺诈检测数据集。数据集可以从[https://www.kaggle.com/dalpozz/creditcardfraud](https://www.kaggle.com/dalpozz/creditcardfraud)下载。由于我正在使用这个数据集，因此通过引用以下出版物来保持透明性是一个好主意：
- en: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson, and Gianluca Bontempi,
    *Calibrating Probability with Undersampling for Unbalanced Classification*. In
    Symposium on **Computational Intelligence and Data Mining** (**CIDM**), IEEE,
    2015.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrea Dal Pozzolo、Olivier Caelen、Reid A. Johnson和Gianluca Bontempi，《用欠采样校准概率进行不平衡分类》，在IEEE计算智能与数据挖掘研讨会（CIDM）上发表于2015年。
- en: The datasets contain transactions made by credit cards by European cardholders
    in September 2013 over the span of only two days. There is a total of 285,299
    transactions, with only 492 frauds out of 284,807 transactions, meaning the dataset
    is highly imbalanced and the positive class (fraud) accounts for 0.172% of all
    transactions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含2013年9月欧洲持卡人的信用卡交易，仅为两天。总共有285,299笔交易，其中只有492笔是欺诈交易，占284,807笔交易的0.172%，表明数据集严重不平衡，正类（欺诈）占所有交易的0.172%。
- en: It contains only numerical input variables, which are the result of a PCA transformation.
    Unfortunately, due to confidentiality issues, we cannot provide the original features
    and more background information about the data. There are 28 features, namely
    `V1`, `V2`, ..., `V28`, that are principal components obtained with PCA, except
    for the `Time` and `Amount`. The feature `Class` is the response variable, and
    it takes value 1 in the case of fraud and 0 otherwise. We will see details later
    on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它只包含数值输入变量，这些变量是PCA转换的结果。不幸的是，由于保密问题，我们无法提供有关数据的原始特征和更多背景信息。有28个特征，即`V1`、`V2`、...、`V28`，这些是通过PCA获得的主成分，除了`Time`和`Amount`。特征`Class`是响应变量，在欺诈案例中取值为1，否则为0。我们稍后会详细了解。
- en: Problem description
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题描述
- en: Given the class imbalance ratio, we recommend measuring the accuracy using the
    **Area Under the Precision-Recall Curve** (**AUPRC**). Confusion matrix accuracy
    is not meaningful for imbalanced classification. Regarding this, use linear machine
    learning models, such as random forests, logistic regression, or support vector
    machines, by applying over-or under-sampling techniques. Alternatively, we can
    try to find anomalies in the data, since an assumption like only a few fraud cases
    being anomalies within the whole dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于类别不平衡比率，我们建议使用**精度-召回率曲线下面积**（**AUPRC**）来衡量准确性。对于不平衡分类，混淆矩阵准确性并不具有意义。关于此，可以通过应用过采样或欠采样技术使用线性机器学习模型，如随机森林、逻辑回归或支持向量机。或者，我们可以尝试在数据中找到异常值，因为假设整个数据集中只有少数欺诈案例是异常。
- en: When dealing with such a severe imbalance of response labels, we also need to
    be careful when measuring model performance. Because there are only a handful
    of fraudulent instances, a model that predicts everything as non-fraud will already
    achieve more than the accuracy of 99%. But despite its high accuracy, linear machine
    learning models won't necessarily help us find fraudulent cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理如此严重的响应标签不平衡时，我们在测量模型性能时也需要小心。由于欺诈案例很少，将所有预测为非欺诈的模型已经达到了超过99%的准确率。但尽管准确率很高，线性机器学习模型不一定能帮助我们找到欺诈案例。
- en: Therefore, it would be worth exploring deep learning models, such as autoencoders.
    Additionally, we need to use anomaly detection for finding anomalies. In particular,
    we will see how to use autoencoders to pre-train a classification model and measure
    model performance on unbalanced data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，值得探索深度学习模型，如自编码器。此外，我们需要使用异常检测来发现异常值。特别是，我们将看到如何使用自编码器来预训练分类模型，并在不平衡数据上测量模型性能。
- en: Preparing programming environment
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备编程环境
- en: 'In particular, I am going to use several tools and technologies for this project.
    The following is the list explaining each technology:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我将为这个项目使用多种工具和技术。以下是解释每种技术的列表：
- en: '**H2O/Sparking water**: For deep learning platform (see more in the previous
    chapter)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H2O/Sparking water**：用于深度学习平台（详见上一章节）'
- en: '**Apache Spark**: For data processing environment'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**：用于数据处理环境'
- en: '**Vegas**: An alternative to Matplotlib, similar to Python, for plotting. It
    can be integrated with Spark for plotting purposes'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vegas**：Matplotlib的替代品，类似于Python，用于绘图。它可以与Spark集成以进行绘图目的。'
- en: '**Scala**: The programming language for our project'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scala**：我们项目的编程语言'
- en: 'Well, I am going to create a Maven project, where all the dependencies will
    be injected into the `pom.xml` file. The full content of the `pom.xml` can be
    downloaded from the Packt repository. So let''s do it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我将创建一个Maven项目，所有依赖项都将注入到`pom.xml`文件中。`pom.xml`文件的完整内容可以从Packt仓库下载。所以让我们开始吧：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, Eclipse or your favorite IDE will pull all the dependencies. The first
    dependency will also pull all the Spark related dependencies compatible with this
    H2O version. Then, create a Scala file and provide a suitable name. Then we are
    ready to go.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Eclipse或你喜欢的IDE将拉取所有的依赖项。第一个依赖项也会拉取与该H2O版本兼容的所有Spark相关依赖项。然后，创建一个Scala文件并提供一个合适的名称。接下来，我们就准备好了。
- en: Step 1 - Loading required packages and libraries
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1 - 加载所需的包和库
- en: 'So let''s start by importing required libraries and packages:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从导入所需的库和包开始：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Step 2 - Creating a Spark session and importing implicits
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2 - 创建Spark会话并导入隐式转换
- en: 'We then need to create a Spark session as the gateway of our program:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要创建一个Spark会话作为我们程序的入口：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Additionally, we need to import implicits for spark.sql and h2o:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要导入spark.sql和h2o的隐式转换：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Step 3 - Loading and parsing input data
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3 - 加载和解析输入数据
- en: 'We load and get the transaction. Then we get the distribution:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载并获取交易数据。然后我们获得分布：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Step 4 - Exploratory analysis of the input data
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4 - 输入数据的探索性分析
- en: As described earlier, the dataset contains numerical input variables `V1` to
    `V28`, which are the result of a PCA transformation of the original features.
    The response variable `Class` tells us whether a transaction was fraudulent (value
    = 1) or not (value = 0).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集包含`V1`到`V28`的数值输入变量，这些变量是原始特征经过PCA转换后的结果。响应变量`Class`告诉我们交易是否是欺诈行为（值=1）或正常交易（值=0）。
- en: 'There are two additional features, `Time` and `Amount`. The `Time` column signifies
    the time in seconds between the current transaction and the first transaction.
    Whereas the `Amount` column signifies how much money was transferred in this transaction.
    So let''s see a glimpse of the input data (only `V1`, `V2`, `V26`, and `V27` are
    shown, though) in *Figure 6*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个额外的特征，`Time`和`Amount`。`Time`列表示当前交易和第一次交易之间的秒数，而`Amount`列表示本次交易转账的金额。所以让我们看看输入数据的一个简要展示（这里只显示了`V1`、`V2`、`V26`和`V27`）在*图
    6*中：
- en: '![](img/29ef9fd6-5183-47f2-8d32-6715d749d7cf.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29ef9fd6-5183-47f2-8d32-6715d749d7cf.png)'
- en: 'Figure 6: A snapshot of the credit card fraud detection dataset'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：信用卡欺诈检测数据集的快照
- en: 'We have been able to load the transaction, but the preceding DataFrame does
    not tell us about the class distribution. So, let''s compute the class distribution
    and think about plotting them:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经能够加载交易数据，但上面的DataFrame没有告诉我们类别的分布情况。所以，让我们计算类别分布并考虑绘制它们：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/df46daf3-9596-4e75-9ade-2d93f781f36e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df46daf3-9596-4e75-9ade-2d93f781f36e.png)'
- en: 'Figure 7: Class distribution in the credit card fraud detection dataset'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：信用卡欺诈检测数据集中的类别分布
- en: 'Now, let''s see if the time has any important contribution to suspicious transactions.
    The `Time` column tells us the order in which transactions were done, but doesn''t
    tell us anything about the actual times (that is, time of day) of the transactions.
    Therefore, normalizing them by day and binning those into four groups according
    to time of day to build a `Day` column from `Time` would be useful. I have written
    a UDF for this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看时间是否对可疑交易有重要的影响。`Time`列告诉我们交易发生的顺序，但并未提供任何关于实际时间（即一天中的时间）的信息。因此，将它们按天进行标准化，并根据一天中的时间将其分为四组，以便从`Time`构建一个`Day`列会很有帮助。我为此编写了一个UDF：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let''s plot it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制它：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/e01f7590-6be0-4115-acac-8410a5dbf6bf.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e01f7590-6be0-4115-acac-8410a5dbf6bf.png)'
- en: 'Figure 8: Day distribution in the credit card fraud detection dataset'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：信用卡欺诈检测数据集中的天数分布
- en: 'The preceding graph shows that the same number of transactions was made on
    these two days, but to be more specific, slightly more transactions were made
    in `day1`. Now let''s build the `dayTime` column. Again, I have written a UDF
    for it:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了这两天的交易数量相同，但更具体地说，`day1`的交易数量略多。现在让我们构建`dayTime`列。我为此编写了一个UDF：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we need to get the quantiles (`q1`, median, `q2`) and building time
    bins (`gr1`, `gr2`, `gr3`, and `gr4`):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要获取分位数（`q1`、中位数、`q2`）并构建时间区间（`gr1`、`gr2`、`gr3`和`gr4`）：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then let''s get the distribution for class `0` and `1`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们获取类别`0`和`1`的分布：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s plot the group distribution for class `0`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制类别`0`的组分布：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/29070829-7d9b-40e4-918c-7d4de5fe8913.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29070829-7d9b-40e4-918c-7d4de5fe8913.png)'
- en: 'Figure 9: Group distribution for class 0 in the credit card fraud detection
    dataset'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：信用卡欺诈检测数据集中类别0的组分布
- en: 'From the preceding graph, it is clear that most of them are normal transactions.
    Now let''s see the group distribution for `class 1`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表来看，显然大部分是正常交易。现在让我们看看 `class 1` 的分组分布：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/077d35cb-67f7-4002-82d8-9b53fb78d8ad.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/077d35cb-67f7-4002-82d8-9b53fb78d8ad.png)'
- en: 'Figure 10: Group distribution for class 1 in the credit card fraud detection
    dataset'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：信用卡欺诈检测数据集中类 1 的分组分布
- en: 'So, the distribution of transactions over the four **Time** bins shows that
    the majority of fraud cases happened in group 1\. We can of course look at the
    distribution of the amounts of money that were transferred:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，四个**Time**区间中的交易分布显示，大多数欺诈案件发生在组 1。我们当然可以查看转账金额的分布：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/491ab0c9-6738-411e-a8a2-fc6e00edba83.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/491ab0c9-6738-411e-a8a2-fc6e00edba83.png)'
- en: 'Figure 11: Distribution of the amounts of money that were transferred for class
    0'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：类 0 转账金额的分布
- en: 'Now let''s plot the same for `class 1`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为 `class 1` 绘制相同的图表：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/3dfd4828-7a0d-44e5-902e-4ab6e1c6dc8b.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3dfd4828-7a0d-44e5-902e-4ab6e1c6dc8b.png)'
- en: 'Figure 12: Distribution of the amounts of money that were transferred for class
    1'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：类 1 转账金额的分布
- en: 'So, from the preceding two graphs, it can be observed that fraudulent credit
    card transactions had a higher mean amount of money that was transferred, but
    the maximum amount was much lower compared to regular transactions. As we have
    seen in the `dayTime` column that we manually constructed, it is not that significant,
    so we can simply drop it. Let''s do it:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从前面这两张图表可以看出，欺诈性信用卡交易的转账金额均值较高，但最大金额却远低于常规交易。正如我们在手动构建的 `dayTime` 列中看到的那样，这并不十分显著，因此我们可以直接删除它。我们来做吧：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Step 5 - Preparing the H2O DataFrame
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 5 - 准备 H2O DataFrame
- en: 'Up to this point, our DataFrame (that is, `t4`) is in Spark DataFrame. But
    it cannot be consumed by the H2O model. So, we have to convert it to an H2O frame.
    So let''s do it:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的 DataFrame（即` t4`）是 Spark DataFrame，但它不能被 H2O 模型使用。所以，我们需要将其转换为 H2O
    frame。那么我们来进行转换：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We split the dataset to, say, 40% supervised training, 40% unsupervised training,
    and 20% test using H2O built-in splitter called FrameSplitter:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集划分为，假设 40% 为有监督训练，40% 为无监督训练，20% 为测试集，使用 H2O 内置的分割器 FrameSplitter：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the above code segment, `Key.make[Frame](_)` is used as a low-level task
    to split the frame based on the split ratio that also help attain distributed
    Key/Value pairs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，`Key.make[Frame](_)` 被用作低级任务，用于根据分割比例分割数据帧，同时帮助获得分布式的键/值对。
- en: Keys are very crucial in H2O computing. H2O supports a distributed Key/Value
    store, with exact Java memory model consistency. The thing is that Keys are a
    means to find a link value somewhere in the Cloud, to cache it locally, to allow
    globally consistent updates to a link Value.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 H2O 计算中，键非常关键。H2O 支持分布式的键/值存储，并且具有精确的 Java 内存模型一致性。关键点是，键是用来在云中找到链接值、将其缓存到本地，并允许对链接值进行全局一致的更新的手段。
- en: 'Finally, we need to convert the `Time` column from String to Categorical (that
    is, **enum**) explicitly:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将 `Time` 列从字符串类型显式转换为类别类型（即**枚举**）：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Step 6 - Unsupervised pre-training using autoencoder
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 6 - 使用自编码器进行无监督预训练
- en: 'As described earlier, we will be using Scala with the `h2o` encoder. Now it''s
    time to start the unsupervised autoencoder training. Since the training is unsupervised,
    it means we need to exclude the `response` column from the unsupervised training
    set:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用 Scala 和 `h2o` 编码器。现在是时候开始无监督自编码器训练了。由于训练是无监督的，这意味着我们需要将 `response`
    列从无监督训练集中排除：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next task is to define the hyperparameters, such as the number of hidden
    layers with neurons, seeds for the reproducibility, the number of training epochs
    and the activation function for the deep learning model. For the unsupervised
    pre-training, just set the autoencoder parameter to `true`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的任务是定义超参数，例如隐藏层的数量和神经元、用于重现性的种子、训练轮数以及深度学习模型的激活函数。对于无监督预训练，只需将自编码器参数设置为 `true`：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, we are applying a technique called **bottleneck** training,
    where the hidden layer in the middle is very small. This means that my model will
    have to reduce the dimensionality of the input data (in this case, down to two
    nodes/dimensions).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们应用了一种叫做**瓶颈**训练的技术，其中中间的隐藏层非常小。这意味着我的模型必须降低输入数据的维度（在这种情况下，降到两个节点/维度）。
- en: The autoencoder model will then learn the patterns of the input data, irrespective
    of given class labels. Here, it will learn which credit card transactions are
    similar and which transactions are outliers or anomalies. We need to keep in mind,
    though, that autoencoder models will be sensitive to outliers in our data, which
    might throw off otherwise typical patterns.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，自编码器模型将学习输入数据的模式，而不考虑给定的类别标签。在这里，它将学习哪些信用卡交易是相似的，哪些交易是异常值或离群点。不过，我们需要记住，自编码器模型对数据中的离群点非常敏感，这可能会破坏其他典型模式。
- en: 'Once the pre-training is completed, we should save the model in the `.csv`
    directory:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预训练完成，我们应该将模型保存在`.csv`目录中：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Reload the model and restore it for further use:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载模型并恢复以便进一步使用：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now let''s print the model''s metrics to see how the training went:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印模型的指标，看看训练的效果如何：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/5dc693c5-f3ed-46b4-8012-b902b7470df6.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5dc693c5-f3ed-46b4-8012-b902b7470df6.png)'
- en: 'Figure 13: Autoencoder model''s metrics'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：自编码器模型的指标
- en: Fantastic! The pre-training went very well, because we can see the RMSE and
    MSE are pretty low. We can also see that some features are pretty unimportant,
    such as `v16`, `v1`, `v25`, and so on. We will try to analyze it later on.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！预训练进展非常顺利，因为我们可以看到 RMSE 和 MSE 都相当低。我们还可以看到，一些特征是相当不重要的，例如`v16`、`v1`、`v25`等。我们稍后会分析这些。
- en: Step 7 - Dimensionality reduction with hidden layers
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 7 - 使用隐藏层进行降维
- en: Since we used a shallow autoencoder with two nodes in the hidden layer in the
    middle, it would be worth using the dimensionality reduction to explore our feature
    space. We can extract this hidden feature with the `scoreDeepFeatures()` method
    and plot it to show the reduced representation of the input data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了一个中间有两个节点的浅层自编码器，因此使用降维来探索我们的特征空间是值得的。我们可以使用`scoreDeepFeatures()`方法提取这个隐藏特征并绘制图形，展示输入数据的降维表示。
- en: The `scoreDeepFeatures()` method scores an auto-encoded reconstruction on-the-fly,
    and materialize the deep features of given layer. It takes the following parameters,
    frame Original data (can contain response, will be ignored) and layer index of
    the hidden layer for which to extract the features. Finally, a frame containing
    the deep features is returned. Where number of columns is the hidden [layer]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`scoreDeepFeatures()`方法会即时评分自编码重构，并提取给定层的深度特征。它需要以下参数：原始数据的框架（可以包含响应，但会被忽略），以及要提取特征的隐藏层的层索引。最后，返回一个包含深度特征的框架，其中列数为隐藏[层]。'
- en: 'Now, for the supervised training, we need to extract the Deep Features. Let''s
    do it from layer 2:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于监督训练，我们需要提取深度特征。我们从第 2 层开始：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The plotting for eventual cluster identification is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最终聚类识别的绘图如下：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/7408e0ac-c725-476f-b0d4-52d0a2c442a3.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7408e0ac-c725-476f-b0d4-52d0a2c442a3.png)'
- en: 'Figure 14: Eventual cluster for classes 0 and 1'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：类别 0 和 1 的最终聚类
- en: 'From the preceding figure, we cannot see any cluster of fraudulent transactions
    that is distinct from non-fraudulent instances, so dimensionality reduction with
    our autoencoder model alone is not sufficient to identify fraud in this dataset.
    But we could use the reduced dimensionality representation of one of the hidden
    layers as features for model training. An example would be to use the 10 features
    from the first or third hidden layer. Now, let''s extract the Deep Features from
    layer 3:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们无法看到任何与非欺诈实例明显区分的欺诈交易聚类，因此仅使用我们的自编码器模型进行降维不足以识别数据集中的欺诈行为。但我们可以使用隐藏层之一的降维表示作为模型训练的特征。例如，可以使用第一层或第三层的
    10 个特征。现在，让我们从第 3 层提取深度特征：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s do unsupervised DL using the dataset of the new dimension again:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次使用新维度的数据集进行无监督深度学习：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then save the model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们保存模型：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For measuring model performance on test data, we need to convert the test data
    to the same reduced dimensions as the training data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量模型在测试数据上的表现，我们需要将测试数据转换为与训练数据相同的降维形式：
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, this actually looks quite good in terms of identifying fraud cases: 93%
    of fraud cases were identified!'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从识别欺诈案例的角度来看，这实际上看起来相当不错：93%的欺诈案例已被识别！
- en: Step 8 - Anomaly detection
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 8 - 异常检测
- en: 'We can also ask which instances were considered outliers or anomalies within
    our test data. Based on the autoencoder model that was trained before, the input
    data will be reconstructed, and for each instance, the MSE between actual value
    and reconstruction is calculated. I am also calculating the mean MSE for both
    class labels:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以问一下哪些实例被认为是我们测试数据中的离群值或异常值。根据之前训练的自编码器模型，输入数据将被重构，并为每个实例计算实际值与重构值之间的 MSE。我还计算了两个类别标签的平均
    MSE：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/01c06441-36c9-4976-a9ea-c669408e55b3.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01c06441-36c9-4976-a9ea-c669408e55b3.png)'
- en: 'Figure 15: DataFrame showing MSE, class, and row ID'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：显示均方误差（MSE）、类别和行 ID 的数据框
- en: 'Seeing this DataFrame, it''s really difficult to identify outliers. But plotting
    them would provide some more insights:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这个数据框，确实很难识别出离群值。但如果将它们绘制出来，可能会提供更多的见解：
- en: '[PRE31]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/add43d6c-82c0-472e-b39f-5ebc1dfc40e4.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/add43d6c-82c0-472e-b39f-5ebc1dfc40e4.png)'
- en: 'Figure 16: Distribution of the reconstructed MSE, across different row IDs'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：不同行 ID 下重建的 MSE 分布
- en: As we can see in the plot, there is no perfect classification into fraudulent
    and non-fraudulent cases, but the mean MSE is definitely higher for fraudulent
    transactions than for regular ones. But a minimum interpretation is necessary.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图表中所看到的，欺诈与非欺诈案例之间并没有完美的分类，但欺诈交易的平均 MSE 确实高于常规交易。但是需要进行最低限度的解释。
- en: From the preceding figure, we can at least see that most of the **idRows** have
    an MSE of **5µ**. Or, if we extend the MSE threshold up to **10µ**, then the data
    points exceeding this threshold can be considered as outliers or anomalies, that
    is, fraudulent transactions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们至少可以看到，大多数**idRows**的 MSE 为**5µ**。或者，如果我们将 MSE 阈值提高到**10µ**，那么超出此阈值的数据点可以视为离群值或异常值，即欺诈交易。
- en: Step 9 - Pre-trained supervised model
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 9 - 预训练的监督模型
- en: 'We can now try using the autoencoder model as a pre-training input for a supervised
    model. Here, I am again using a neural network. This model will now use the weights
    from the autoencoder for model fitting. However, transforming the classes from
    Int to Categorical in order to train for classification is necessary. Otherwise,
    the H2O training algorithm will treat it as a regression:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试使用自编码器模型作为监督模型的预训练输入。这里，我再次使用神经网络。该模型现在将使用来自自编码器的权重进行模型拟合。然而，需要将类别从整数转换为类别型，以便进行分类训练。否则，H2O
    训练算法将把它当作回归问题处理：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that the training set (that is, `train_supervised`) is ready for supervised
    learning, let''s jump into it:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练集（即`train_supervised`）已经为监督学习准备好了，我们可以开始进行训练了：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Well done! We have now completed the supervised training. Now, to see the predicted
    versus actual classes:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！我们现在已经完成了监督训练。接下来，让我们看看预测类别与实际类别的对比：
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, this looks much better! We did miss 17% of the fraud cases, but we also
    did not misclassify too many of the non-fraudulent cases. In real life, we would
    spend some more time trying to improve the model by example, performing grid searches
    for hyperparameter tuning, going back to the original features and trying different
    engineered features and/or trying different algorithms. Now, what about visualizing
    the preceding result? Let''s do it using the `Vegas` package:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这看起来好多了！我们确实错过了 17% 的欺诈案例，但也没有错误地分类太多非欺诈案例。在现实生活中，我们会花更多的时间通过示例来改进模型，执行超参数调优的网格搜索，回到原始特征并尝试不同的工程特征和/或尝试不同的算法。现在，如何可视化前面的结果呢？让我们使用
    `Vegas` 包来实现：
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/f46c6d3a-8edc-489b-9250-cba98b3b7f58.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f46c6d3a-8edc-489b-9250-cba98b3b7f58.png)'
- en: 'Figure 17: Predicted versus actual classes using the supervised trained model'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：使用监督训练模型的预测类别与实际类别对比
- en: Step 10 - Model evaluation on the highly-imbalanced data
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 10 - 在高度不平衡的数据上进行模型评估
- en: Since the dataset is highly imbalanced towards non-fraudulent cases, using model
    evaluation metrics, such as accuracy or **area under the curve** (**AUC**), does
    not make sense. The reason is that these metrics would give overly optimistic
    results based on the high percentage of correct classifications of the majority
    class.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集在非欺诈案例上高度不平衡，因此使用模型评估指标（如准确率或**曲线下面积**（**AUC**））是没有意义的。原因是，这些指标会根据大多数类的高正确分类率给出过于乐观的结果。
- en: 'An alternative to AUC is to use the precision-recall curve, or the sensitivity
    (recall) -specificity curve. First, let''s compute the ROC using the `modelMetrics()`
    method from the `ModelMetricsSupport` class:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: AUC的替代方法是使用精确度-召回率曲线，或者敏感度（召回率）-特异度曲线。首先，让我们使用`ModelMetricsSupport`类中的`modelMetrics()`方法来计算ROC：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have the `precision_recall` DataFrame, it would be exciting to
    plot it. So let''s do it:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了`precision_recall`数据框，绘制它会非常令人兴奋。那么我们就这样做：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/f66e5dc0-0b53-4b60-9ba2-9186bfa32671.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f66e5dc0-0b53-4b60-9ba2-9186bfa32671.png)'
- en: 'Figure 18: Precision-recall curve'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：精确度-召回率曲线
- en: Precision is the proportion of test cases predicted to be fraudulent that were
    truly fraudulent, also called **true positive** predictions. On the other hand,
    recall, or sensitivity, is the proportion of fraudulent cases that were identified
    as fraudulent. And specificity is the proportion of non-fraudulent cases that
    are identified as non-fraudulent.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是预测为欺诈的测试案例中，真正是欺诈的比例，也叫做**真实正例**预测。另一方面，召回率或敏感度是被识别为欺诈的欺诈案例的比例。而特异度是被识别为非欺诈的非欺诈案例的比例。
- en: 'The preceding precision-recall curve tells us the relationship between actual
    fraudulent predictions and the proportion of fraudulent cases that were predicted.
    Now, the question is how to compute the sensitivity and specificity. Well, we
    can do it using standard Scala syntax and plot it using the `Vegas` package:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的精确度-召回率曲线告诉我们实际欺诈预测与被预测为欺诈的欺诈案例的比例之间的关系。现在，问题是如何计算敏感度和特异度。好吧，我们可以使用标准的Scala语法来做到这一点，并通过`Vegas`包绘制它：
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/d09bc5bc-db14-4d43-8cd8-69615e21e749.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d09bc5bc-db14-4d43-8cd8-69615e21e749.png)'
- en: 'Figure 19: Sensitivity versus specificity curve'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：敏感度与特异度曲线
- en: Now the preceding sensitivity-specificity curve tells us the relationship between
    correctly predicted classes from both labels—for example, if we have 100% correctly
    predicted fraudulent cases, there will be no correctly classified non-fraudulent
    cases, and vice versa).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，前面的敏感度-特异度曲线告诉我们两个标签下正确预测类别之间的关系——例如，如果我们有100%正确预测的欺诈案例，那么将没有正确分类的非欺诈案例，反之亦然。
- en: 'Finally, it would be great to take a closer look at this a little bit differently,
    by manually going through different prediction thresholds and calculating how
    many cases were correctly classified in the two classes. More specifically, we
    can visually inspect true positive, false positive, true negative, and false negative
    over different prediction thresholds—for example, 0.0 to 1.0:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过手动检查不同的预测阈值，并计算两个类别中正确分类的案例数量，从不同角度更深入地分析会很有帮助。更具体地说，我们可以直观地检查不同预测阈值下的真实正例、假正例、真实负例和假负例——例如，从0.0到1.0：
- en: '[PRE39]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'First, let''s draw the true positive one:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们绘制真实正例的图：
- en: '[PRE40]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/8744aaab-cff4-4f47-91a5-c79c408bd47e.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8744aaab-cff4-4f47-91a5-c79c408bd47e.png)'
- en: 'Figure 20: True positives across different prediction thresholds in [0.0, 1.0]'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：在[0.0, 1.0]范围内的真实正例数量随不同预测阈值变化
- en: 'Secondly, let''s draw the false positive one:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，让我们绘制假正例的图：
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/3a0053e8-ba09-438e-ba3a-95f59da4e307.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a0053e8-ba09-438e-ba3a-95f59da4e307.png)'
- en: 'Figure 21: False positives across different prediction thresholds in [0.0,
    1.0]'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：在[0.0, 1.0]范围内的假正例数量随不同预测阈值变化
- en: 'However, the preceding figure is not easily interpretable. So let''s provide
    a threshold of 0.01 for the `datum.th` and then draw it again:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的图形不容易解读。所以让我们为`datum.th`设置一个0.01的阈值，并重新绘制：
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/f4190313-1a84-41f5-8193-164149958df3.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4190313-1a84-41f5-8193-164149958df3.png)'
- en: 'Figure 22: False positives across different prediction thresholds in [0.0,
    1.0]'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：在[0.0, 1.0]范围内的假正例数量随不同预测阈值变化
- en: 'Then, it''s the turn for the true negative one:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，轮到绘制真实负例了：
- en: '[PRE43]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/4720dbe2-95f4-4b8c-a5ee-df4b4b9d61c9.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4720dbe2-95f4-4b8c-a5ee-df4b4b9d61c9.png)'
- en: 'Figure 23: False positives across different prediction thresholds in [0.0,
    1.0]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：在[0.0, 1.0]范围内的假正例数量随不同预测阈值变化
- en: 'Finally, let''s draw the false negative one, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们绘制假负例的图，如下所示：
- en: '[PRE44]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/bab24a7b-dd1a-4857-96dc-c82ae1521854.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bab24a7b-dd1a-4857-96dc-c82ae1521854.png)'
- en: 'Figure 24: False positives across different prediction thresholds in [0.0,
    1.0]'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：在[0.0, 1.0]范围内的假正例数量随不同预测阈值变化
- en: Therefore, the preceding plots tell us that we can increase the number of correctly
    classified non-fraudulent cases without losing correctly classified fraudulent
    cases when we increase the prediction threshold from the default 0.5 to 0.6.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的图表告诉我们，当我们将预测阈值从默认的0.5提高到0.6时，我们可以增加正确分类的非欺诈性案例的数量，而不会丧失正确分类的欺诈性案例。
- en: Step 11 - Stopping the Spark session and H2O context
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11步 - 停止Spark会话和H2O上下文
- en: 'Finally, stop the Spark session and H2O context. The following `stop()` method
    invocation will shut down the H2O context and Spark cluster, respectively:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，停止Spark会话和H2O上下文。以下的`stop()`方法调用将分别关闭H2O上下文和Spark集群：
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The first one, especially, is more important, otherwise it sometimes does not
    stop the H2O flow but still holds the computing resources.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种尤其重要，否则有时它不会停止H2O流动，但仍然占用计算资源。
- en: Auxiliary classes and methods
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 辅助类和方法
- en: 'In the preceding steps, we have seen some classes or methods that we should
    describe here, too. The first method, named `toCategorical()`, converts the Frame
    column from String/Int to enum; this is used to convert `dayTime` bags (that is,
    `gr1`, `gr2`, `gr3`, `gr4`) to a factor-like type. This function is also used
    to convert the `Class` column to a factor type in order to perform classification:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的步骤中，我们看到了一些类或方法，这里也应该进行描述。第一个方法名为`toCategorical()`，它将Frame列从String/Int转换为枚举类型；用于将`dayTime`袋（即`gr1`、`gr2`、`gr3`、`gr4`）转换为类似因子的类型。该函数也用于将`Class`列转换为因子类型，以便执行分类：
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This builds a confusion matrix for anomaly detection according to a threshold
    if an instance is considered anomalous (if its MSE exceeds the given threshold):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法根据一个阈值构建异常检测的混淆矩阵，如果实例被认为是异常的（如果其MSE超过给定阈值）：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Apart from these two auxiliary methods, I have defined three Scala case classes
    for computing precision, recall; sensitivity, specificity; true positive, true
    negative, false positive and false negative and so on. The signature is as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两个辅助方法外，我还定义了三个Scala案例类，用于计算精准度、召回率；灵敏度、特异度；真正例、假正例、真负例和假负例等。签名如下：
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Hyperparameter tuning and feature selection
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优和特征选择
- en: 'Here are some ways of improving the accuracy by tuning hyperparameters, such
    as the number of hidden layers, the neurons in each hidden layer, the number of
    epochs, and the activation function. The current implementation of the H2O-based
    deep learning model supports the following activation functions:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过调节超参数来提高准确度的一些方法，如隐藏层的数量、每个隐藏层中的神经元数、训练轮次（epochs）以及激活函数。当前基于H2O的深度学习模型实现支持以下激活函数：
- en: '`ExpRectifier`'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExpRectifier`'
- en: '`ExpRectifierWithDropout`'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExpRectifierWithDropout`'
- en: '`Maxout`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Maxout`'
- en: '`MaxoutWithDropout`'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxoutWithDropout`'
- en: '`Rectifier`'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Rectifier`'
- en: '`RectifierWthDropout`'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RectifierWithDropout`'
- en: '`Tanh`'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tanh`'
- en: '`TanhWithDropout`'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TanhWithDropout`'
- en: Apart from the `Tanh` one, I have not tried other activation functions for this
    project. However, you should definitely try.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`Tanh`之外，我没有尝试其他激活函数用于这个项目。不过，你应该尝试一下。
- en: One of the biggest advantages of using H2O-based deep learning algorithms is
    that we can take the relative variable/feature importance. In previous chapters,
    we have seen that, using the random forest algorithm in Spark, it is also possible
    to compute the variable importance. So, the idea is that if your model does not
    perform well, it would be worth dropping less important features and doing the
    training again.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于H2O的深度学习算法的最大优势之一是我们可以获得相对的变量/特征重要性。在前面的章节中，我们已经看到，使用Spark中的随机森林算法也可以计算变量的重要性。因此，如果你的模型表现不好，可以考虑丢弃不重要的特征，并重新进行训练。
- en: 'Let''s see an example; in *Figure 13*, we have seen the most important features
    in unsupervised training in autoencoder. Now, it is also possible to find the
    feature importance during supervised training. I have observed feature importance
    here:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子；在*图13*中，我们已经看到在自动编码器的无监督训练中最重要的特征。现在，在监督训练过程中也可以找到特征的重要性。我在这里观察到的特征重要性：
- en: '![](img/e5331bba-dac7-43af-a0b9-428c83482cad.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5331bba-dac7-43af-a0b9-428c83482cad.png)'
- en: 'Figure 25: False positives across different prediction thresholds in [0.0,
    1.0]'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：不同预测阈值下的假正例，范围为[0.0, 1.0]
- en: Therefore, from *Figure 25*, it can be observed that the features Time, `V21`,
    `V17`, and `V6` are less important ones. So why don't you drop them and try training
    again and observe whether the accuracy has increased or not?
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从*图25*中可以观察到，特征Time、`V21`、`V17`和`V6`的重要性较低。那么为什么不将它们去除，再进行训练，看看准确率是否有所提高呢？
- en: Nevertheless, grid searching or cross-validation techniques could still provide
    higher accuracy. However, I'll leave it up to you.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，网格搜索或交叉验证技术仍然可能提供更高的准确率。不过，我将把这个决定留给你。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have used a dataset having more than 284,807 instances of
    credit card use and for each transaction where only 0.172% transactions are fraudulent.
    We have seen how we can use autoencoders to pre-train a classification model and
    how to apply anomaly detection techniques to predict possible fraudulent transactions
    from highly imbalanced data—that is, we expected our fraudulent cases to be anomalies
    within the whole dataset.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了一个数据集，该数据集包含了超过284,807个信用卡使用实例，并且每笔交易中只有0.172%的交易是欺诈性的。我们已经看到如何使用自编码器来预训练分类模型，以及如何应用异常检测技术来预测可能的欺诈交易，尤其是在高度不平衡的数据中——也就是说，我们期望欺诈案件在整个数据集中是异常值。
- en: Our final model now correctly identified 83% of fraudulent cases and almost
    100% of non-fraudulent cases. Nevertheless, we have seen how to use anomaly detection
    using outliers, some ways of hyperparameter tuning, and, most importantly, feature
    selection.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终模型现在正确识别了83%的欺诈案件和几乎100%的非欺诈案件。然而，我们已经了解了如何使用异常检测来识别离群值，一些超参数调整的方法，最重要的是，特征选择。
- en: A **recurrent neural network** (**RNN**) is a class of artificial neural network
    where connections between units form a directed cycle. RNNs make use of information
    from the past. That way, they can make predictions in data with high temporal
    dependencies. This creates an internal state of the network that allows it to
    exhibit dynamic temporal behavior.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）是一类人工神经网络，其中单元之间的连接形成一个有向循环。RNN利用来自过去的信息，这样它们就能在具有高时间依赖性的数据中做出预测。这会创建一个网络的内部状态，使其能够展示动态的时间行为。'
- en: An RNN takes many input vectors to process them and output other vectors. Compared
    to a classical approach, using an RNN with **Long Short-Term Memory cells** (**LSTMs**)
    requires almost no feature engineering. Data can be fed directly into the neural
    network, which acts like a black box, modeling the problem correctly. The approach
    here is rather simple in terms of how much of the data was preprocessed.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: RNN接收多个输入向量进行处理，并输出其他向量。与经典方法相比，使用带有**长短期记忆单元**（**LSTM**）的RNN几乎不需要特征工程。数据可以直接输入到神经网络中，神经网络就像一个黑箱，能够正确建模问题。就预处理的数据量而言，这里的方法相对简单。
- en: 'In the next chapter, we will see how to develop an machine learning project
    using an RNN implementation called **LSTM** for **human activity recognition**
    (**HAR**), using a smartphones dataset. In short, our machine learning model will
    be able to classify the type of movement from six categories: walking, walking
    upstairs, walking downstairs, sitting, standing, and laying.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到如何使用名为**LSTM**的循环神经网络（RNN）实现，开发一个**人类活动识别**（**HAR**）的机器学习项目，使用的是智能手机数据集。简而言之，我们的机器学习模型将能够从六个类别中分类运动类型：走路、走楼梯、下楼梯、坐着、站立和躺下。
