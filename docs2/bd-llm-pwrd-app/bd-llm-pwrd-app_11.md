# 11

# 大型语言模型微调

到目前为止，我们已经探讨了“基础”形式下大型语言模型（LLMs）的功能和应用，这意味着我们使用从其基础训练中获得的参数来消费它们。我们实验了许多场景，即使在基础形式下，LLMs 也能适应各种场景。然而，可能存在极端特定领域的案例，其中通用 LLM 无法完全拥抱该领域的分类法和知识。如果是这种情况，您可能需要在特定领域的数据上微调您的模型。

**定义**

在语言模型微调的背景下，“分类法”指的是一种结构化的分类或分类系统，它根据特定领域内概念、术语和实体之间的关系和层次结构来组织它们。这个系统对于使模型对内容的理解和生成更加相关和准确，对于专业应用至关重要。

在特定领域的一个具体分类法示例是医疗领域。在这里，分类法可以将信息分类到结构化组中，如疾病、症状、治疗和患者人口统计信息。例如，在“疾病”类别中，可能会有关于疾病类型的子类别，如“心血管疾病”，这可以进一步细分为更具体的情况，如“高血压”和“冠状动脉疾病”。这种详细的分类有助于微调语言模型，使其在医疗咨询或文档中理解和生成更精确和上下文相关的响应。

在本章中，我们将介绍微调 LLM 的技术细节，从其背后的理论到使用 Python 和 Hugging Face 的实际操作实现。到本章结束时，您将能够使用自己的数据微调 LLM，从而可以构建由这些模型驱动的特定领域应用。

我们将深入探讨以下主题：

+   微调简介

+   理解何时需要微调

+   准备您的数据以微调模型

+   在您的数据上微调基础模型

+   您微调模型的托管策略

# 技术要求

要完成本章的任务，您需要以下内容：

+   Hugging Face 账户和用户访问令牌。

+   Python 3.7.1 或更高版本。

+   Python 包：请确保已安装以下 Python 包：`python-dotenv`、`huggingface_hub`、`accelerate>=0.16.0`、`<1 transformers[torch]`、`safetensors`、`tensorflow`、`datasets`、`evaluate` 和 `accelerate`。这些包可以通过在终端中运行 `pip install` 命令轻松安装。如果您想从最新版本安装所有内容，可以在终端中运行 `pip install git+https://github.com/huggingface/transformers.git` 来参考原始 GitHub。

你可以在本书的 GitHub 仓库中找到所有代码和示例：`github.com/PacktPublishing/Building-LLM-Powered-Applications`。

# 什么是微调？

微调是一种**迁移学习**技术，其中使用预训练神经网络的权重作为在新的不同任务上训练新神经网络的初始值。这可以通过利用从先前任务中学到的知识来提高新网络的表现，尤其是在新任务数据有限的情况下。

**定义**

迁移学习是机器学习中的一种技术，涉及使用从一项任务中学到的知识来提高相关但不同任务的表现。例如，如果你有一个可以识别汽车的模型，你可以使用其中的一些特征来帮助你识别卡车。通过重用现有模型而不是从头开始训练新模型，迁移学习可以节省你的时间和资源。

为了更好地理解迁移学习和微调的概念，让我们考虑以下示例。

想象一下，你想要训练一个计算机视觉神经网络来识别不同类型的花朵，例如玫瑰、向日葵和郁金香。你有很多花朵的照片，但不足以从头开始训练一个模型。

相反，你可以使用迁移学习，这意味着使用已经在不同任务上训练好的模型，并使用其中的一些知识来处理你的新任务。例如，你可以使用一个已经训练来识别许多车辆的模型，如汽车、卡车和自行车。这个模型已经学会了如何从图像中提取特征，例如边缘、形状、颜色和纹理。这些特征对任何图像识别任务都很有用，而不仅仅是原始任务。

你可以使用这个模型作为你的花朵识别模型的基础。你只需要在其上方添加一个新层，该层将学习如何将特征分类为花朵类型。这个层被称为分类器层，它是模型适应新任务所需的。在基础模型之上训练分类器层的过程称为**特征提取**。一旦完成这一步，你可以通过解冻基础模型的一些层并与分类器层一起训练来进一步调整你的模型，这允许你调整基础模型特征以更好地适应你的任务。

下图展示了计算机视觉模型示例：

![图片](img/B21714_11_01.png)

图 11.1：迁移学习和微调的示例

微调通常在特征提取之后进行，作为提高模型性能的最后一步。你可以根据你的数据大小和复杂性来决定解冻多少层。一种常见的做法是解冻基础模型中最后几层，这些层更具体于原始任务，而保留前几层冻结，这些层更通用且可重用。

总结来说，迁移学习和微调是允许您为新的任务使用预训练模型的技巧。迁移学习涉及在基础模型之上添加新的分类器层，并仅训练该层。微调涉及解冻基础模型的一些或所有层，并将它们与分类器层一起训练。

在生成式 AI 的背景下，微调是通过在特定任务或领域的数据集上更新其参数来调整预训练语言模型的过程。微调可以提高模型在目标任务上的性能和准确性。微调涉及以下步骤：

1.  **加载预训练的语言模型及其分词器**：分词器用于将文本转换为模型可以处理的数值标记。不同的模型具有独特的架构和需求，通常附带自己的专用分词器，用于处理其特定的输入格式。

例如，**BERT**（代表**从 Transformer 中提取的双向编码器表示**）使用 WordPiece 分词，而 GPT-2 采用**字节对编码**（**BPE**）。由于训练和推理过程中的内存限制，模型也设置了标记限制。

这些限制确定了模型可以处理的最大序列长度。例如，BERT 的最大标记限制为 512 个标记，而 GPT-2 可以处理更长的序列（例如，最多 1,024 个标记）。

1.  **准备特定任务的数据集**：数据集应包含与任务相关的输入-输出对。例如，对于情感分析，输入可以是文本评论，输出可以是情感标签（正面、负面或中性）。

1.  **定义特定任务的头部**：头部是在预训练模型之上添加的一层或一组层，用于执行特定任务。头部应匹配任务的输出格式和大小。例如，对于情感分析，头部可以是一个具有三个输出单元的线性层，这三个输出单元对应于三个情感标签。

**注意**

当处理专门为文本生成设计的 LLM 时，其架构与用于分类或其他任务的模型不同。事实上，与预测标签的分类任务不同，LLM 预测序列中的下一个单词或标记。这一层被添加到预训练的基于 Transformer 的模型之上，目的是将基础模型中的上下文化隐藏表示转换为词汇表上的概率。

1.  **在特定任务的数据集上训练模型**：训练过程包括将输入标记馈送到模型，计算模型输出与真实输出之间的损失，并使用优化器更新模型参数。训练可以是固定数量的 epoch，或者直到满足某个标准。

1.  **在测试或验证集上评估模型**：评估过程涉及使用适当的指标来衡量模型在未见数据上的性能。例如，对于情感分析，指标可以是准确率或 F1 分数（将在本章后面讨论）。评估结果可以用来比较不同的模型或微调策略。

即使微调在计算和时间成本上比完整训练要低，但微调一个 LLM 并不是一项“轻松”的活动。由于 LLM 本质上很大，它们的微调需要硬件要求，以及数据收集和预处理。

因此，在接近特定场景时，您想要问自己的第一个问题是：“我真的需要微调我的 LLM 吗？”

# 何时需要微调？

正如我们在前面的章节中看到的，良好的提示工程以及您可以通过嵌入添加到模型中的非参数知识是定制 LLM 的卓越技术，它们可以解释大约 90%的使用案例。然而，前面的断言往往适用于最先进的模型，如 GPT-4、Llama 2 和 PaLM 2。正如所讨论的，这些模型具有大量的参数，使它们变得沉重，因此需要计算能力；此外，它们可能是专有的，并且可能需要按使用付费。

从此以后，当您想利用一个轻量级且免费的 LLM，例如 Falcon LLM 7B，但又希望它在特定任务中表现与 SOTA 模型相当时，微调也可能很有用。

一些可能需要微调的例子包括：

+   当您想使用 LLM 对电影评论进行情感分析，但 LLM 是在维基百科文章和书籍上预训练的。微调可以帮助 LLM 学习电影评论的词汇、风格和语气，以及与情感分类相关的相关特征。

+   当您想使用 LLM 对新闻文章进行文本摘要，但 LLM 是在语言建模目标上预训练的。微调可以帮助 LLM 学习摘要的结构、内容和长度，以及生成目标和评估指标。

+   当您想使用 LLM 在两种语言之间进行机器翻译，但 LLM 是在不包含这些语言的跨语言语料库上预训练的。微调可以帮助 LLM 学习目标语言的词汇、语法和句法，以及翻译目标和对齐方法。

+   当您想使用 LLM 执行复杂的**命名实体识别**（NER）任务时。例如，财务和法律文件包含专业术语和实体，这些在通用语言模型中通常不会被优先考虑，因此微调过程在这里可能极为有益。

在本章中，我们将介绍利用 Hugging Face 模型和库的全代码方法。然而，请注意，Hugging Face 还提供了一个名为 AutoTrain 的低代码平台（你可以在[`huggingface.co/autotrain`](https://huggingface.co/autotrain)了解更多信息），如果你的组织更倾向于低代码策略，这可能是一个不错的选择。

# 开始微调

在本节中，我们将涵盖使用全代码方法微调一个 LLM 所需的所有步骤。我们将利用 Hugging Face 库，如`datasets`（从 Hugging Face 数据集生态系统加载数据）和`tokenizers`（提供最流行的分词器实现）。我们将要解决的问题是一个情感分析任务。我们的目标是微调一个模型，使其成为一个专家级的二元分类器，将情感分为“正面”和“负面”。

## 获取数据集

我们需要的第一个要素是训练数据集。为此，我将利用 Hugging Face 中可用的数据集库来加载一个名为 IMDB 的二分类数据集（你可以在[`huggingface.co/datasets/imdb`](https://huggingface.co/datasets/imdb)找到数据集卡片）。

数据集包含电影评论，这些评论被分类为正面或负面。更具体地说，数据集包含两列：

+   文本：原始的电影评论。

+   标签：该评论的情感。它被映射为“0”表示“负面”，而“1”表示“正面”。

由于这是一个**监督学习**问题，数据集已经包含了用于训练集的 25,000 行和用于验证集的 25,000 行。

**定义**

监督学习是一种机器学习方法，它使用有标签的数据集来训练算法以准确地对数据进行分类或预测结果。有标签的数据集是包含输入特征和期望输出值的示例集合，也称为标签或目标。例如，用于手写识别的有标签数据集可能包含手写数字的图像作为输入特征，以及相应的数值作为标签。

训练集和验证集是有标签数据集的子集，在监督学习过程中用于不同的目的。训练集用于拟合模型的参数，例如神经网络中连接的权重。验证集用于调整模型的超参数，例如神经网络中的隐藏单元数量或学习率。超参数是影响模型整体行为和性能的设置，但不是直接从数据中学习的。验证集通过比较不同候选模型在验证集上的准确度或其他指标来帮助选择最佳模型。

监督学习与另一种机器学习方法不同，即**无监督学习**。在后一种学习中，算法的任务是在没有标记输出或目标的情况下，在数据集中寻找模式、结构或关系。换句话说，在无监督学习中，算法没有提供具体的指导或标签来指导其学习过程。相反，它自行探索数据，并识别内在的模式或分组。

你可以通过运行以下代码下载 IMDB 数据集：

```py
from datasets import load_dataset
dataset = load_dataset("imdb")
dataset 
```

Hugging Face 数据集附带一个字典模式，如下所示：

```py
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
}) 
```

要访问特定数据集对象（例如，`train`）的一个观测值，你可以使用切片器，如下所示：

```py
dataset["train"][100] 
```

这给我们以下输出：

```py
{'text': "Terrible movie. Nuff Said.[…]
 'label': 0} 
```

因此，训练集的第 101 个观测值包含一个标记为负面的评论。

现在我们有了数据集，我们需要对其进行预处理，以便可以用于训练我们的 LLM。为此，我们需要对提供的文本进行分词，我们将在下一节中讨论这个问题。

## 数据分词

分词器是一个负责将文本分割成更小单元（如单词或子词）的组件，这些单元可以用作 LLM 的输入。分词器可以用来高效且一致地编码文本，以及添加一些特殊标记，如掩码或分隔符标记，这些标记是某些模型所必需的。

Hugging Face 提供了一个强大的实用工具，称为 AutoTokenizer，它包含在 Hugging Face Transformers 库中，为各种模型（如 BERT 和 GPT-2）提供分词器。它作为一个通用分词器类，根据你指定的预训练模型动态选择和实例化适当的分词器。

以下代码片段展示了我们如何初始化我们的分词器：

```py
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased") 
```

注意，我们选择了一个名为`bert-base-cased`的特定分词器。实际上，分词器和 LLM 之间存在联系，即分词器通过将文本转换为模型可以理解的数值 ID 来准备模型的输入。

**定义**

输入 ID 是与分词器词汇表中的标记相对应的数值 ID。当编码文本输入时，分词器函数会返回这些输入 ID。输入 ID 被用作模型的输入，模型期望的是数值张量而不是字符串。不同的分词器可能对相同的标记有不同的输入 ID，这取决于它们的词汇表和分词算法。

不同的模型可能使用不同的分词算法，如基于词、基于字符或基于子词。因此，为每个模型使用正确的分词器非常重要，否则模型可能表现不佳，甚至产生错误。让我们看看每种可能的场景：

+   基于字符的方法可能适合处理罕见单词或具有复杂形态结构语言的场景，或者当处理拼写纠正任务时。

+   基于单词的方法可能适合像命名实体识别（NER）、情感分析和文本分类这样的场景。

+   子词方法介于前两种方法之间，当我们需要平衡文本表示的粒度与效率时很有用。

在下一节中，我们将利用**BERT**模型进行此场景，因此我们加载了其预训练的标记器（这是一个由 WordPiece 算法驱动的基于单词的标记器）。

我们现在需要初始化`tokenize_function`，它将被用来格式化数据集：

```py
def tokenize_function(examples):
    return tokenizer(examples["text"], padding = "max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True) 
```

如您所见，我们还配置了`tokenize_function`的**填充**和**截断**，以确保输出适合我们 BERT 模型的大小。

**定义**

填充和截断是两种技术，用于使文本输入序列具有相同的长度。这对于一些期望固定长度输入的自然语言处理（NLP）模型，如 BERT 模型，通常是必需的。

填充意味着在序列的末尾或开头添加一些特殊标记，通常是零，以使其达到所需的长度。例如，如果我们有一个长度为 5 的序列，而我们想将其填充到长度为 8，我们可以在末尾添加 3 个零，如下所示：[1, 2, 3, 4, 5, 0, 0, 0]。这被称为后填充。或者，我们可以在开头添加 3 个零，如下所示：[0, 0, 0, 1, 2, 3, 4, 5]。这被称为前填充。填充策略的选择取决于模型和任务。

截断意味着从序列中删除一些标记以使其符合所需的长度。例如，如果我们有一个长度为 10 的序列，而我们想将其截断到长度为 8，我们可以从序列的末尾或开头删除 2 个标记。例如，我们可以删除最后的 2 个标记，如下所示：[1, 2, 3, 4, 5, 6, 7, 8]。这被称为后截断。或者，我们可以删除前 2 个标记，如下所示：[3, 4, 5, 6, 7, 8, 9, 10]。这被称为前截断。截断策略的选择也取决于模型和任务。

现在，我们可以将函数应用于我们的数据集并检查一个条目的数值 ID：

```py
tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets['train'][100]['input_ids'] 
```

这是我们的输出：

```py
[101,
 12008,
 27788,
...
 0,
 0,
 0,
 0,
 0] 
```

如您所见，由于传递给函数的`padding='max_length'`参数，向量的最后元素是零。

可选地，如果您想缩短训练时间，您可以决定减小数据集的大小。在我的情况下，我已经将数据集缩小如下：

```py
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(500))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(500)) 
```

因此，我有两个集合——一个用于训练，一个用于测试——每个集合包含 500 个观测值。现在我们已经预处理并准备好了数据集，我们需要微调模型。

## 微调模型

如前所述，我们将用于微调的 LLM 是 BERT 的基础版本。BERT 模型是由谷歌研究人员在 2018 年引入的基于 transformer 的、仅编码器的自然语言理解模型。BERT 是第一个通用 LLM 的例子，这意味着它是第一个能够同时处理多个 NLP 任务的模型，这与当时存在的特定任务模型不同。

现在，尽管这可能听起来有点“过时”（实际上，与今天的模型如 GPT-4 相比，它甚至不算“大”，其大型版本只有 3.4 亿个参数），鉴于过去几个月市场上涌现的所有新的 LLM，BERT 及其微调变体仍然是一种广泛采用的架构。事实上，正是由于 BERT，语言模型的标准得到了极大的提升。

BERT 模型有两个主要组件：

+   编码器：编码器由多个 transformer 块层组成，每个块都有一个自注意力层和一个前馈层。编码器以一个标记序列作为输入，标记是文本的基本单元，并输出一个隐藏状态序列，这些状态是高维向量，代表每个标记的语义信息。

+   输出层：输出层是特定于任务的，并且可能因 BERT 所用的任务类型而异。例如，对于文本分类，输出层可以是一个线性层，用于预测输入文本的类别标签。对于问答，输出层可以是两个线性层，用于预测输入文本中答案范围的开头和结尾位置。

+   模型的层数和参数数量取决于模型版本。实际上，BERT 有两种大小：BERTbase 和 BERTlarge。以下插图显示了这两个版本之间的差异：

![计算机屏幕截图  自动生成的描述](img/B21714_11_02.png)

图 11.2：BERTbase 和 BERTlarge 的比较（来源：[`huggingface.co/blog/bert-101`](https://huggingface.co/blog/bert-101))

后来，为了降低 BERT 的计算成本和内存使用，引入了其他版本，如 BERT-tiny、BERT-mini、BERT-small 和 BERT-medium。

该模型在约 33 亿个单词的异构语料库上进行了训练，这些语料库属于维基百科和谷歌的 BooksCorpus。训练阶段涉及两个目标：

+   **掩码语言模型**（**MLM**）：MLM 旨在教会模型预测输入文本中随机掩码（用特殊标记替换）的原始单词。例如，给定句子“他昨天买了一辆新的  **车**”，模型应该预测“车”、“自行车”或其他有意义的单词。这个目标有助于模型学习词汇、语法以及词语之间的语义和上下文关系。

+   **下一句预测**（**NSP**）：NSP 旨在教会模型预测两个句子在原文中是否连续。例如，给定句子“她喜欢读书”和“她最喜欢的类型是奇幻”，模型应该预测它们是连续的，因为它们很可能在文本中一起出现。然而，给定句子“她喜欢读书”和“他每个周末都踢足球”，模型应该预测它们不是连续的，因为它们不太可能相关。这个目标有助于模型学习文本的连贯性和逻辑，以及句子之间的语篇和语用关系。

通过使用这两个目标（模型同时训练），BERT 模型可以学习可以转移到特定任务（如文本分类、问答和命名实体识别）的通用语言知识。BERT 模型在这些任务上的表现优于仅使用一个方向上下文或根本不使用预训练的先前模型。事实上，它在许多基准和任务上已经实现了最先进的成果，例如**通用语言理解评估**（**GLUE**）、**斯坦福问答数据集**（**SQuAD**）和**多体裁自然语言推理**（**MultiNLI**）。

BERT 模型及其许多微调版本可在 Hugging Face Hub 中找到。您可以如下实例化模型：

```py
import torch
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2) 
```

注意，`AutoModelForSequenceClassification`是`AutoModel`的一个子类，它可以实例化一个适合序列分类的模型架构，例如文本分类或情感分析。它可以用于任何需要为每个输入序列提供一个标签或标签列表的任务。在我的情况下，我将输出标签的数量设置为两个，因为我们处理的是一个二元分类问题。

另一方面，`AutoModel`是一个通用类，可以根据预训练模型名称或路径从库中实例化任何模型架构。它可以用于任何不需要特定输出格式的工作，例如特征提取或语言建模。

在开始训练之前，定义我们将需要的评估指标是为了理解我们的模型一旦微调后表现如何。

## 使用评估指标

如我们在*第一章*中看到的，评估一个通用目的的 LLM 可能比较繁琐。因为这些模型是在未标记的文本上训练的，并且不是针对特定任务的，而是根据用户的提示具有通用性和适应性，传统的评估指标已经不再适用。评估一个 LLM 意味着，在众多事情中，测量其语言流畅性、连贯性以及根据用户请求模仿不同风格的能力。

然而，我们也看到了 LLM 如何用于非常具体的场景，就像我们的二分类任务一样。如果是这种情况，评估指标将简化为该场景常用的指标。

**注意**

当涉及到更会话化的任务，如摘要、问答和检索增强生成时，需要引入一套新的评估指标，这些指标通常由 LLM 提供支持。其中一些最受欢迎的指标如下：

+   流畅性：这评估生成文本的自然和流畅程度。

+   **一致性**：这评估文本中想法的逻辑流程和连通性。

+   相关性：这衡量生成内容与给定提示或上下文的一致性程度。

+   GPT 相似度：这量化了生成文本与人类撰写内容的相似程度。

+   **真实性**：这评估生成文本是否基于事实信息或上下文。

这些评估指标帮助我们了解 LLM 生成文本的质量、自然度和相关性，指导改进并确保可靠的 AI 辅助。

当涉及到二元分类时，评估二元分类器最基本的方法之一是使用混淆矩阵。混淆矩阵是一个表格，显示了预测标签与真实标签匹配的数量。它有四个单元格：

+   **真阳性** (**TP**): 当真实标签为 1 时，分类器正确预测 1 的案例数量。

+   **假阳性** (**FP**): 当真实标签为 0 时，分类器错误预测 1 的案例数量。

+   **真阴性** (**TN**): 当真实标签为 0 时，分类器正确预测 0 的案例数量。

+   **假阴性** (**FN**): 当真实标签为 1 时，分类器错误预测 0 的案例数量。

下面是我们将要构建的情感分类器的混淆矩阵的示例，知道标签 0 与“负面”相关，标签 1 与“正面”相关：

|  | **预测正面** | **预测负面** |
| --- | --- | --- |
| **正面** | 20 (TP) | 5 (FN) |
| **负面** | 3 (FP) | 72 (TN) |

混淆矩阵可以用来计算各种指标，这些指标衡量分类器性能的不同方面。其中一些最常见的指标是：

+   **准确度**：所有预测中正确预测的比例。它计算为 `(TP + TN) / (TP + FP + TN + FN)`。例如，情感分类器的准确度为 `(20 + 72) / (20 + 3 + 72 + 5) = 0.92`。

+   **精确度**：所有正预测中正确正预测的比例。它计算为 `TP / (TP + FP)`。例如，情感分类器的精确度为 `20 / (20 + 3) = 0.87`。

+   **召回率**：所有正案例中正确正预测的比例。它也被称为灵敏度或真阳性率。它计算为 `TP / (TP + FN)`。例如，情感分类器的召回率为 `20 / (20 + 5) = 0.8`。

+   **特异性**：所有负例中正确负预测的比例。它也被称为真正负率。计算公式为`TN / (TN + FP)`。例如，情感分类器的特异性为`72 / (72 + 3) = 0.96`。

+   **F1 分数**：精确率和召回率的调和平均值。它是精确率和召回率之间平衡的度量。计算公式为`2 * (precision * recall) / (precision + recall)`。例如，情感分类器的 F1 分数为`2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83`。

可以从混淆矩阵或其他来源（如决策分数或分类器的概率输出）导出许多其他指标。以下是一些例子：

+   **接收者操作特征**（**ROC**）**曲线**：召回率与假正率（`FP / (FP + TN)`）的图表，显示了分类器在不同阈值下区分正例和负例的能力。

+   **ROC 曲线下的面积**（**AUC**）：AUC 衡量分类器将正例排名高于负例的能力。它可以通过以下图表说明，其中显示了 ROC 曲线及其下的面积：

![](img/B21714_11_03.png)

图 11.3：ROC 曲线的示意图，突出显示完美分类器和曲线下的面积（AUC）

在我们的情况下，我们将简单地通过以下步骤使用准确度指标：

1.  您可以通过以下方式从`evaluate`库导入此指标：

    ```py
    import numpy as np
    import evaluate
    metric = evaluate.load("accuracy") 
    ```

1.  我们还需要定义一个函数，该函数根据训练阶段的输出计算准确度：

    ```py
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels) 
    ```

1.  最后，我们需要设置我们的评估策略，这意味着在训练过程中我们希望模型多久对测试集进行一次测试：

    ```py
    from transformers import TrainingArguments, Trainer
    training_args = TrainingArguments(output_dir="test_trainer", num_train_epochs = 2
    evaluation_strategy="epoch") 
    ```

在我们的情况下，我们将设置`epoch`作为评估策略，这意味着评估在每个 epoch 结束时进行。

**定义**

一个 epoch 是机器学习中用来描述整个训练数据集完整遍历的术语。它是一个可以调整以改进机器学习模型性能的超参数。在 epoch 期间，模型的权重根据训练数据和损失函数进行更新。一个 epoch 可以包含一个或多个批次，这些批次是训练数据的小子集。epoch 中批次的数量取决于批次大小，这也是另一个可以调整的超参数。

现在我们已经拥有了开始微调所需的所有成分，这将在下一节中介绍。

## 训练和保存

我们需要微调模型最后一个组件是一个`Trainer`对象。`Trainer`对象是一个类，它为 PyTorch 中模型的全功能训练和评估提供 API，针对 Hugging Face Transformers 进行了优化。您可以按照以下步骤进行：

1.  首先，让我们通过指定在之前步骤中已经配置好的参数来初始化我们的`Trainer`。更具体地说，`Trainer`需要一个模型、一些配置参数（例如，epoch 的数量）、一个训练数据集、一个评估数据集以及要计算的评估指标类型：

    ```py
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=small_train_dataset,
        eval_dataset=small_eval_dataset,
        compute_metrics=compute_metrics,
    ) 
    ```

1.  然后，你可以通过以下方式启动微调过程：

    ```py
    trainer.train() 
    ```

根据你的硬件，训练过程可能需要一些时间。在我的情况下，考虑到数据集规模较小和 epoch 数量较少（只有 2 个），我不期望有异常的结果。然而，仅就准确率而言，两个 epoch 的训练结果如下：

```py
{'eval_loss': 0.6720085144042969, 'eval_accuracy': 0.58, 'eval_runtime': 609.7916, 'eval_samples_per_second': 0.328, 'eval_steps_per_second': 0.041, 'epoch': 1.0}
{'eval_loss': 0.5366445183753967, 'eval_accuracy': 0.82, 'eval_runtime': 524.186, 'eval_samples_per_second': 0.382, 'eval_steps_per_second': 0.048, 'epoch': 2.0} 
```

如你所见，在两个 epoch 之间，模型提高了 41.38%的准确率，最终准确率达到 82%。考虑到上述因素，这已经很不错了！

1.  一旦模型训练完成，我们就可以将其保存在本地，指定路径如下：

    ```py
    trainer.save_model('models/sentiment-classifier') 
    ```

1.  要消费和测试模型，你可以使用以下代码加载它：

    ```py
    model = AutoModelForSequenceClassification.from_pretrained('models/sentiment-classifier') 
    ```

1.  最后，我们需要测试我们的模型。为此，让我们将一个句子传递给模型（首先进行分词），该模型可以对其进行情感分类：

    ```py
    inputs = tokenizer("I cannot stand it anymore!", return_tensors="pt")
    outputs = model(**inputs)
    outputs 
    ```

这将产生以下输出：

```py
SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467, -0.0041]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None) 
```

注意，模型输出是一个`SequenceClassifierOutput`对象，它是句子分类模型输出的基类。在这个对象中，我们感兴趣的是 logit **张量**，这是我们分类模型生成的与标签关联的原始（非归一化）预测向量。

1.  由于我们正在处理张量，我们需要利用 Python 中的`tensorflow`库。此外，我们将使用`softmax`函数来获取与每个标签关联的概率向量，以便我们知道最终结果对应于概率最大的标签：

    ```py
    import tensorflow as tf
    predictions = tf.math.softmax(outputs.logits.detach(), axis=-1)
    print(predictions) 
    ```

以下为获得的输出：

```py
tf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=float32) 
```

我们的模型告诉我们，句子“我再也忍受不了了”的情感是负面的，概率为 65.71%。

1.  注意，你还可以将模型保存在你的 Hugging Face 账户中。为此，你首先需要允许笔记本将代码推送到你的账户，如下所示：

    ```py
    from huggingface_hub import notebook_login
    notebook_login() 
    ```

1.  你将被提示到 Hugging Face 登录页面，在那里你必须输入你的访问令牌。然后，你可以保存模型，指定你的账户名和模型名：

    ```py
    trainer.push_to_hub('vaalto/sentiment-classifier') 
    ```

通过这样做，这个模型可以通过 Hugging Face Hub 轻松消费，就像我们在上一章中看到的那样，如下面的截图所示：

![计算机屏幕截图，描述自动生成](img/B21714_11_04.png)

图 11.4：Hugging Face Hub 空间中的模型卡片

此外，你也可以选择使模型公开，这样 Hugging Face 中的每个人都可以测试和消费你的创作。

在本节中，我们仅用几行代码就微调了一个 BERT 模型，这得益于 Hugging Face 库和加速器。再次强调，如果你的目标是减少代码量，你可以利用 Hugging Face 上托管的低代码 AutoTrain 平台来训练和微调模型。

Hugging Face 无疑是一个用于训练开源 LLM 的稳固平台。除此之外，还有其他平台你可能想要利用，因为专有模型也可以进行微调。例如，OpenAI 允许你使用自己的数据对 GPT 系列进行微调，并提供训练和托管定制模型的计算能力。

总体而言，微调可以是你 LLM 在特定用例中出类拔萃的点睛之笔。基于我们在一开始探讨的框架来决定微调的策略是构建成功应用的关键步骤。

# 摘要

在本章中，我们介绍了微调 LLM 的过程。我们从一个微调的定义和如果你必须决定微调你的 LLM 时需要考虑的一般性考虑开始。

我们随后亲自动手进行微调的实践部分。我们讨论了一个场景，即从基础 BERT 模型出发，我们希望得到一个强大的电影评论情感分析器。为此，我们使用 Hugging Face Python 库的全代码方法在 IMDB 数据集上对基础模型进行微调。

微调是一种强大的技术，可以进一步定制 LLM 以符合你的目标。然而，与 LLM 的许多其他方面一样，它在伦理和安全方面也存在一些担忧和考虑。在下一章中，我们将深入探讨这一点，分享如何与 LLM 建立安全边界，以及更普遍地，各国政府是如何从监管角度来处理这个问题的。

# 参考文献

+   训练数据集：[`huggingface.co/datasets/imdb`](https://huggingface.co/datasets/imdb)

+   HF AutoTrain: [`huggingface.co/docs/autotrain/index`](https://huggingface.co/docs/autotrain/index)

+   BERT 论文：*Jacob Devlin*，*Ming-Wei Chang*，*Kenton Lee*，*Kristina Toutanova*，2019，*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*: [`arxiv.org/abs/1810.04805`](https://arxiv.org/abs/1810.04805)

# 加入我们的 Discord 社区

加入我们的社区 Discord 空间，与作者和其他读者进行讨论：

[`packt.link/llm`](https://packt.link/llm)

![](img/QR_Code214329708533108046.png)
