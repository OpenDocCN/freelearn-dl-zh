

# 生成式AI的常见失败

如果你刚刚构建了你的**生成式AI**（**GenAI**）应用，那么你可能对它能做什么感到如此着迷，以至于你忽略了答案的质量和准确性。发现GenAI错误发生的频率本身就是一个挑战。

许多人倾向于认为，当计算机给出答案时，它给出的是准确的答案——通常比人类更准确。例如，大多数人感到欣慰，因为现在飞机是由机器而不是人驾驶的。由于这一进步，飞机可能比15年前更安全，但当我们谈到GenAI时，结果远不如飞行器的机载系统准确。

本章详细探讨了与GenAI应用相关的五大挑战及其发生的原因。理解这些挑战对于开发者设计有效的解决方案至关重要。到本章结束时，你将很好地理解这些挑战，它们如何影响你的结果，它们如何相互关联，以及为什么尽管存在这些挑战，这一特定技术集对用户仍然非常有价值。

本章将涵盖以下主题：

+   幻觉

+   奄媚

+   数据泄露

+   成本优化

+   性能问题

# 技术要求

本章中的大多数示例可以通过简单地重复ChatGPT中的提示或示例来演示。

# 幻觉

与GenAI一起工作的最大挑战之一，也许是最为人所知的，就是**幻觉**。在GenAI中，幻觉是指AI模型生成看似合理但实际上错误、无意义或与提供的输入数据不符的内容的现象。这个问题在**自然语言处理**（**NLP**）模型中尤为普遍，例如用于文本生成的模型，但也可以出现在其他生成模型中，如图像生成和LLMs（如GPT-4）。

在最坏的情况下，开发者和他们的用户都不知道GenAI给出的答案是正确的、部分正确、大部分不正确，还是完全虚构的。

## 幻觉的原因

组织捕获的大部分数据要么是**冗余的、过时的、微不足道的**（**ROT**），要么根本未分类。在数据湖、仓库和数据库中，*良好*的数据只占很小一部分，这些是大多数公司拥有的。在开始你的GenAI应用之旅时，你很可能会首先注意到，你想要用于训练GenAI应用的大部分数据质量都很差。不久之后，你就会了解到幻觉是由**低质量**的**训练数据**引起的。

工程师可以将此视为**垃圾输入，垃圾输出**问题。当训练数据有错误、不一致、不相关、过时信息、偏见信息和其他问题时，模型将学会复制这些问题。AI 模型的准确性高度依赖于训练数据的质量，以下数据问题更有可能导致输出问题和幻觉：

+   **不准确的数据**：输入中的错误会在系统中传播并累积，因此了解任何自动或实时数据流到您的 GenAI 应用程序中都有准确信息至关重要。例如，如果您正在使用传感器数据来预测设备何时会故障，但接收到的传感器读数不准确，那么您的 GenAI 应用程序可能无法正确或及时地预测故障。

+   **不完整的数据**：在不完整的数据集上进行训练可能导致模型生成看似合理但实际上错误的内容来填补感知到的空白。

+   **过时或陈旧的数据**：从本质上讲，过时的数据通常只是不再准确，向 AI 提供错误信息。相关数据更新确保您的 GenAI 应用程序继续为用户提供准确输出。

+   **无关数据**：可能会诱使您尽可能多地填充您的 GenAI 应用程序中的数据，以便它可以利用这些信息进行分析；然而，这是一种增加成本而不提高准确性的方法。

+   **误导性或失实的数据**：如果机器学习模型是在标签不佳或无法代表现实场景的图像上训练的，那么在部署时它将难以正确识别或分类图像。

+   **重复数据**：这也包括整合不良的数据集。冗余数据可能会给 AI 一种印象，即某些东西比它实际上更重要，因为它是重复的。

+   **模型架构和目标**：例如 GPT-4 这样的模型被训练来根据上下文预测序列中的下一个单词，而不一定是验证事实。这个目标可能导致模型生成流畅但事实不准确的文章。

这些原因各自引起略微不同的问题，并且结合在一起可能会使您的 GenAI 应用程序无法产生令人满意的结果。因此，您的训练数据必须准确、全面，并代表模型将在实际应用中遇到的多样化条件。大部分 GenAI 是持续自我学习的，因此维护数据质量是一个持续的问题，而不仅仅是**首次部署到生产**的问题。

生成模型专注于产生连贯且上下文相关的输出，这有时是以事实准确性为代价的。这些模型在识别和复制数据中的模式方面也非常出色。然而，这可能导致输出遵循学习到的模式，即使这些模式与事实现实不符。这是**相关性而非因果关系**的问题。

此外，模型是在静态数据集上训练的，缺乏对更新信息的实时访问，这可能导致过时或不正确的输出。例如，GPT及其同类产品是在几个月（甚至几年！）前从网络上抓取的数据上训练的。昨天的产品、见解和世界新闻都不在可用范围内。当询问关于最近事件的问题时，在最好的情况下，用户会收到这样的回答：“我没有这个信息”。在最坏的情况下，通用人工智能应用只是胡编乱造一个回答。生成模型可能不完全理解上下文或缺乏验证生成信息正确性的现实世界知识。

## 幻觉的后果

除了“错误”和“编造答案”之外，幻觉还可能产生其他意想不到的后果。错误信息可以轻易地传播给成千上万的人，其中一些人可能难以事后纠正。例如，如果今天，ChatGPT（一个流行的通用人工智能模型）开始告诉每个询问的人一个流行的开源项目存在一个关键漏洞，那么这个消息就会像野火一样迅速传播，使得损害控制变得困难。它将影响到比公司在博客上发表的关于信息不真实声明更多的人。许多用户在没有验证的情况下就信任人工智能的输出。

幻觉削弱了人工智能系统的可靠性，尤其是在医疗、法律或金融服务等领域，这些领域准确性至关重要。此外，持续的幻觉可能会侵蚀用户对人工智能应用的信任，导致采用率降低和对人工智能能力的怀疑。

错误信息可能导致道德困境和潜在的法律责任，尤其是如果人工智能的输出影响了关键决策或公众舆论。随着通用人工智能被应用于各种应用中，对于用户来说退出变得越来越困难，同时辨别答案是否合法也越来越困难。

值得注意的是，收到一个不是幻觉的回答与收到最佳回答大相径庭。

# 媚俗

媚俗者是指那些不惜一切代价赢得你赞同的人，即使是以牺牲他们的道德或对真相的了解为代价。人工智能模型经常表现出这种行为，以至于人工智能研究人员和开发者可以使用相同的术语——**媚俗**——来描述模型如何以欺骗或问题的方式对人类反馈和提示做出反应。人类反馈通常被用来微调人工智能助手。但人类反馈也可能鼓励模型产生与用户信念相符的响应，而不是真实的响应，这种特性被称为媚俗。媚俗以多种方式存在，例如镜像反馈、容易被影响，以及如果用户反驳，就会改变正确的答案。如果用户分享他们对某个主题的看法和观点，人工智能助手将提供与用户信念相符的答案。

奇谄可以在多个层面上观察到和描述，如下所示：

+   **反馈奇谄**：当用户对文本表达喜好或厌恶时，AI助手可能会相应地提供更多积极或消极的反馈。

+   **摇摆不定**：在正确回答问题后，即使原始答案是正确的，AI助手在用户挑战时可能会改变他们的答案。

+   **信念一致性**：当用户就某个话题分享他们的观点时，AI助手往往会提供与那些信念一致的答案，导致准确性降低。

在测试中，研究人员Mrinank Sharma等人展示了由Claude生成的奇谄答案（[https://arxiv.org/abs/2310.13548](https://arxiv.org/abs/2310.13548)），如*图11.1*所示。

![](img/B22495_11_01.jpg)

图11.1：展示奇谄的示例回答

值得注意的是，在ChatGPT中对相同和类似问题的重复测试并没有产生一致的结果。

## 奇谄的原因

奇谄的确切原因尚不清楚。这种现象存在于许多LLM中，因为这些模型已被指示接受上下文和参数信息以告知其回答。GenAI应用有一个*学习*功能，即它们与用户互动越多，就越能了解语法、上下文和提供充分答案。在这样做的时候，应用表现出只能描述为*取悦他人*的行为，导致它们偏离了纯粹的事实信息传递。

在上述研究中，发现奇谄是类似于RLHF的对齐训练的副作用。**从人类反馈中进行强化学习**（**RLHF**）是一种用于训练LLM以使代理或机器与人类偏好对齐的技术。这在语言模型等领域尤为重要。为了说明这一点，让我们看看一些例子以及为什么这很重要。

考虑以下内容：

当你问候同事时，你可能会说“你好，先生/女士”，“你好”，“早上好”，“好日子”，“嗨”，“怎么了”，“问候”，或许多其他可能的问候语。假设所有这些都是合适的，但人类对哪种更合适有一定的偏好。

为了进一步理解这一点，让我们从文化偏好开始。在某些文化中，如果你不包含同事的名字，比如“早上好，史密斯先生”，这确实会令人震惊。然而，在其他文化中，以这种方式称呼某人会显得极其奇怪。人类对哪种问候方式更受欢迎的偏好有一定的依据，其中一部分是文化因素，一部分是情境和语境（史密斯先生是总裁吗？他是你20岁的应届毕业生吗？），还有一部分纯粹是你个人的偏好。

工程师决定，当人们与GenAI互动时，他们更喜欢他们的对话和互动感觉像人。为了做到这一点，机器必须考虑文化、情境、行为，以及在某种程度上个人的偏好。

训练模型可以访问大量信息，包括上下文信息（来自网站、书籍、研究等的文本段落）和参数信息（最近邻词的嵌入）。它们将使用用户提供的任何文化、上下文或行为线索来帮助提供答案。也就是说，用户提问的方式会影响答案。

ChatGPT确认了这一点。当被问及它是如何得出答案时，它明确地陈述如下：

```py
I assess the context of your question. For instance, if you've mentioned the setting (formal or informal), the relationship with the coworker, or any specific preferences, I take those into account.
If we've interacted before, I consider any speech patterns or preferences you've shown in previous conversations. This helps tailor the response to your style and needs.
I use general knowledge about cultural and social norms to gauge what might be most appropriate. For example, formal greetings are more suitable in professional settings, while casual greetings work better in relaxed environments.
```

可以请求生成式人工智能（GenAI）在回答问题之前忽略你之前的互动、个人偏好、语法和/或它之前关于你的任何结论，但当然，这要求用户首先知道这种情况正在发生。

## 谄媚的影响

尽管这个功能很有帮助，但它对GenAI应用的输出在现实世界中有着实际的影响。在前面章节中引用的同一篇研究论文（[https://arxiv.org/abs/2310.13548](https://arxiv.org/abs/2310.13548)）中，确定谄媚的后果，虽然起源于机器，可能导致对用户观点的不正确顺从、传播用户创建的错误和有偏见的回答。因此，GenAI并没有帮助创造一个更加事实和一致的世界理解，反而持续并可能加速了错误信息的传播。

谷歌DeepMind的研究人员发现，随着模型变得更大，问题变得更加严重（[https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/](https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/)）。具有更多参数输入的LLM更有可能同意客观上错误的陈述，而不是较小的LLM。这种倾向甚至适用于数学方程式，即只有一个正确答案的问题。

LLM们不断学习、进化，并且被它们的创造者不断改进。在未来，也许LLM会更高地评价陈述的客观真实性，而不是用户的观点或偏好，但截至2023年，这还没有发生。持续的研究和测试将使它们在平衡用户期望、用户观点和事实方面变得更加熟练。然而，截至本书撰写时，谄媚仍然是GenAI应用的主要担忧，尤其是在输出在生成响应之前考虑用户观点和偏好的情况下。使用合成数据和重新训练模型进行的进一步测试将谄媚的倾向减少了高达10%，但这仍然不是100%（[https://arxiv.org/abs/2308.03958](https://arxiv.org/abs/2308.03958)）。这意味着即使经过相当大的微调修改，这种倾向仍然存在。

# 数据泄露

在生成式人工智能（GenAI）的背景下，**数据泄露**指的是使用来自期望训练数据集之外的信息来创建模型的情况，这可能导致过度乐观的性能指标，并可能产生有缺陷或误导性的预测。这种情况可能发生在模型开发的各个阶段，从数据收集到模型评估，可能会严重损害人工智能系统的有效性。存在多种类型的数据集，它们有不同的用途：

+   用于训练LLM的训练数据集

+   用于改进LLM响应和减少幻觉的微调数据集

+   用于评估响应准确性的评估数据集

## 数据泄露的原因

数据泄露的原因简单明了，且易于避免，只要这些应用的开发者了解这些原因。首先，让我们从高层次上了解导致数据泄露的原因：

+   **不适当的dataset重叠**：每个数据集应适用于适当的训练和评估阶段。当这种情况不成立时，就会发生数据泄露。例如，当训练数据集与评估数据集重叠时，GenAI应用在测试期间的表现自然会更好，因为它们已经知道了确切的答案。在这种情况下，您的股价预测应用在其训练和评估数据集中会有重复的历史数据点；因此，在测试其输出性能时，其表现将不切实际地高，因为它已经看到了答案。

+   **未来信息**：每个数据集应仅包含预测时可能可用的信息。例如，您不会在训练数据集中包含来自未来某个时期的真实或假设信息，或模型在生产中通常无法访问的数据。

+   **数据归一化和转换努力**：当转换或特征工程步骤意外地引入来自训练集之外的数据时，信息可能会从评估数据集泄露到训练过程中。对于GenAI，您希望训练数据尽可能接近*现实生活*，无论是从用户交互还是应用将运行在其内的任何上下文来看，这样您的应用就有真正代表性的数据。

为了说明这些原因，让我们使用一个假设的GenAI应用，该应用根据请求使用历史数据预测股价。在这种情况下，现在是2024年5月，您的应用处于最终测试阶段。在推向生产之前，您想确定其预测的准确性。您首先检查应用对以下用户请求的响应。

**用户请求**：

```py
Predict the average stock price for $TSLA in May 2024.
```

**输出答案**：

```py
The average stock price for $TSLA in May 2024 is expected to be $176.
```

在这个例子中，请注意以下内容：

+   模型输入的**训练**数据不应包含任何来自2024年5月的数据点。

+   **评估**数据集应包括2024年5月的所有价格，并可能包括平均股价的实际计算值。这是因为您希望将模型的估计值与实际值进行比较，然后对其进行评分，然后按月绘制，以查看该应用程序是否始终做出低或高的估计。

如果您试图在2024年5月的估计中追求准确性，但您已经在训练阶段提供了2024年5月的数据，这将被视为不适当的训练数据集重叠。让我们看看另一个例子。

**用户请求**:

```py
Predict the average annual price for $TSLA in 2024.
```

**输出答案**:

```py
The average annual stock price for $TSLA in May 2024 is predicted to be $205.
```

您不会提供一个已经包含年度平均值的训练数据集，因为该信息尚未可用。虽然您可以在训练数据集中包含截至当年的平均数，但不应该包含带有合成或生成的前瞻性数据的年度平均值。如果您创建了一个估计的年度股价并将其包含在训练数据中，那么您就是在使用未来信息。现在，让我们考虑一个最终的例子。

**用户请求**:

```py
What is the average stock price for $TSLA in May 2024?
```

**输出答案**:

```py
The average stock price for $TSLA in May 2024 is expected to be $176.
```

注意，与第一个例子相比，这里用户查询的措辞有所不同，尽管它导致了相同的答案。LLMs在推断用户意图方面非常擅长。记住，即使是相当简单的问题，用户也会用许多不同的方式表达（*估计*、*预测*、*预测*、*想象*、*猜测*和*预测*都是他们可能使用的词语）。

对于您的训练数据集，您可能包括一个针对整个支持数据库的提示-答案配对，其风格类似于**常见问题解答**（FAQs）。然而，请抵制纠正诸如措辞和拼写等方面的诱惑。虽然您希望意识到“垃圾输入，垃圾输出”的问题，但您不希望过度保护您的GenAI应用，以至于它不知道如何应对用户不可避免地输入垃圾的情况。这对于GenAI聊天机器人尤其相关。用户有无数种提问的方式。这些问题通常没有适当的语法、术语或上下文意识，他们的知识也可能过时。数据归一化和转换的努力不应使您的训练数据变得不那么有用。

## 数据泄露的影响

数据泄露的影响因情况而异，取决于您是否泄露了泪滴或瀑布。如果有数据泄露，那么您在生产前的GenAI评估和测试结果将是错误的，并且不能代表您应用程序的实际性能，导致过度乐观的测试或误导性的结论。在所有数据重叠的情况下，训练和测试数据集重叠的最明显后果是，模型可能会学会简单地*记忆*训练数据，并在任何必须从中做出预测的新数据上表现不佳。

这可能会给应用程序开发人员和测试人员一种对模型性能的虚假自信。后来，当提供真实世界数据并且用户在生产环境中提出问题时，应用程序的表现将明显变差。

避免数据泄露很简单，它从将你的数据集分割成不同的实体开始，然后执行以下操作：

+   确保训练、验证和测试数据集严格分离。使用时间分割等技术来防止未来信息泄露到训练集中。

+   使用工具确保数据转换仅在模型训练期间应用于训练集，并在评估期间独立应用于测试集。

+   以一种防止未来数据被使用的方式来设计特征。避免将未来值或聚合的未来统计数据作为训练数据的一部分。

回到股价预测应用程序，理想情况下，你希望训练集和测试集中的数据基于时间，确保训练集中的股价按时间顺序发生在测试集中的股价之前。然后，你的应用程序将只具有在预测股价点之前的历史股价数据中使用的特征，这标志着真实历史股价和预测未来股价之间的明确界限。接下来，为了验证你的应用程序，使用基于时间的交叉验证来确保模型性能是在模拟真实世界预测场景或应用程序允许的场景的数据上评估的。

通过严格管理模型开发过程中数据的处理方式，你可以最大限度地降低数据泄露的风险，并确保你的GenAI模型提供可靠和有效的预测。

# 成本

由于有如此多不同的、复杂的、可能昂贵的组成部分，工程师了解他们的GenAI应用程序的成本以及如何控制这些成本至关重要。虽然你将在[*第12章*](B22495_12.xhtml#_idTextAnchor253)“纠正和优化你的生成式AI应用程序”中学习更多关于成本优化策略，但本节将作为了解GenAI应用程序财务成本的一个介绍，这些成本在某些方面与Web开发应用程序不同。

## 成本类型

当使用GenAI时，成本可能来自几个不同的领域。这些成本可以大致分为计算、存储、数据获取、开发和维护成本：

+   **训练成本**：训练GenAI模型需要大量的计算资源。这对于像GPT-4这样的大型模型尤其如此。这些资源通常包括**图形处理单元**（**GPU**）或**张量处理单元**（**TPU**），它们针对并行处理任务进行了优化。支持这些设置的基础设施消耗大量电力，并需要冷却系统以维持运行温度。大多数工程师可能无法承担这些费用，而是会利用来自供应商的模型，例如OpenAI、Anthropic、Google、Meta或其他。

+   **推理，或实时计算**：从训练好的模型生成响应或输出，这被称为**推理**，也会产生计算成本，特别是对于需要提供实时答案的模型。更大的模型成本更高。

+   **存储成本**：存储用于训练GenAI模型的庞大数据集会产生成本。这包括原始数据、预处理数据、用户交互数据、可观察性数据以及模型本身。

+   **数据收集**：获取高质量数据集可能很昂贵。这可能包括从第三方提供商购买数据或生成专有数据集。

+   **数据标注和清洗**：预处理数据以确保其适合训练涉及成本。这可能包括支付人工标注员对数据进行标注或开发算法以清理和准备数据作为训练或评估数据集。

+   **软件开发**：编写和维护用于训练和部署GenAI应用的代码库需要熟练的工程师和数据分析师。

+   **实验和测试**：开发GenAI通常涉及广泛的实验和微调，这需要时间和资源。

+   **数据更新**：训练和评估数据集需要定期更新以保持其准确性和相关性，这涉及额外的计算和人力资源。

+   **监控和支持**：持续监控AI系统以确保其正确运行并处理出现的问题涉及运营成本。

+   **合规性和安全性**：确保数据隐私和安全以及遵守法规（如GDPR）涉及额外的成本。

这不是一个详尽的列表。因此，估计您预期的成本是复杂且非同寻常的任务。但让我们专注于最重要的成本驱动因素，即文本，因此是代币。接下来，您将学习如何在此处估计和控制成本。

## **代币**

**LLMs处理文本使用** **tokens**，这是在文本集中发现的常见字符序列。代币是GenAI应用的货币。每个用户输入和输出都是一个**token**，问题和响应的token计数都可以控制。每个token的成本极小。GenAI供应商寻求通过**每笔交易**赚钱，这可能会迅速累积。

让我们用一个例子来理解这个概念。语句`Hello how are you`是5个token。一个有用的经验法则是，一个token通常对应于约4个字符的普通英文文本。这相当于大约3/4个单词（因此，100个token约等于75个单词）。`Hello how are you`这个例子包括空格在内的字符数为18个，因此18/4 = 4.5，所以是5个token。

GenAI应用程序的每个输入和输出都被简化为这个简单的度量单位。

ChatGPT和其他LLM都有token限制，这限制了用户可以输入作为提示的文本量以及输出响应的限制。然而，这些限制是相当宽裕的。对于大多数用例，普通消费者不太可能达到这些限制。

例如，GPT-4每次交互的token限制为32,768个，估计的字数为25,000个单词，而Claude 3（Anthropic托管的大型语言模型）在撰写本文时token限制为100,000+。对于一个简单的客户服务聊天机器人，您不太可能达到这个限制，但这是可能的。让我们看看两个例子来解释这可能是如何成立的。

**示例 1**：一位客户向GenAI聊天机器人提出一个简单的问题。

**询问**:

```py
what is the latest version of python
```

**响应**:

```py
The latest stable version of Python is 3.12.4, released on June 6, 2024\. This release is part of the Python 3.12 series, which includes many new features and optimizations. Key improvements in this version include enhanced f-string parsing, support for isolated subinterpreters with separate Global Interpreter Locks, and numerous performance enhancements (Python.org) (Python.org).
Additionally, Python 3.13 is currently in development, with the first beta release, 3.13.0b1, made available in May 2024\. This upcoming version introduces new features such as an improved interactive interpreter, experimental JIT support, and an incremental garbage collector (Python Insider). For more details, you can visit the official Python website here.
```

初始询问为36个字符和7个token。响应为162个token，或741个字符。如果您将交互限制在300个token以内，您仍然远远低于您为用户允许的限制。

**示例 2**：一个服务器遇到了内存不足错误，GenAI自动分析堆栈跟踪并记录分析供人类稍后审查。

**询问**:

```py
serverId.nodeId.processID reports error for analysis.
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
    at java.util.HashMap.resize(HashMap.java:704)
    at java.util.HashMap.putVal(HashMap.java:663)
    at java.util.HashMap.put(HashMap.java:612)
    at com.example.server.RequestHandler.handleRequest(RequestHandler.java:45)
    at com.example.server.Server.processRequest(Server.java:78)
    at com.example.server.Server.run(Server.java:61)
    at java.lang.Thread.run(Thread.java:834)
2024-06-18 14:32:21.456 ERROR [Server] - Server encountered an unrecoverable error and will shut down.
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:674)
    at java.lang.StringBuilder.append(StringBuilder.java:214)
    at com.example.server.Logger.log(Logger.java:27)
    at com.example.server.RequestHandler.handleRequest(RequestHandler.java:47)
    at com.example.server.Server.processRequest(Server.java:78)
    at com.example.server.Server.run(Server.java:61)
    at java.lang.Thread.run(Thread.java:834)
```

**响应**:

```py
Exception: The OutOfMemoryError exception indicates that the Java Virtual Machine (JVM) has run out of memory.
Error Message: Java heap space specifies that the heap memory allocated for the JVM is exhausted.
Method Calls: The trace shows the sequence of method calls that led to the error.
The error starts in the HashMap.resize method.
The putVal and put methods of HashMap are called.
These methods are used in RequestHandler.handleRequest.
Server.processRequest and Server.run handle the processing and running of the server, respectively.
The Thread.run method is the entry point of the thread that encountered the error.
Timestamp: The timestamp (2024-06-18 14:32:21.456) provides the exact time of the error occurrence.
Logger: The logger in Logger.log attempted to log the error but also encountered the same OutOfMemoryError.
Causes and Solutions:
Causes: Excessive memory usage due to large data structures. Memory leaks where objects are not properly disposed of. Insufficient heap size allocated to the JVM.
Solutions: Increase the heap size using JVM options (-Xmx for maximum heap size). Optimize the code to use memory more efficiently. Use profiling tools to detect and fix memory leaks.
```

这个分析器对诊断工程师很有用，因为它可以快速审查堆栈跟踪，总结其发现，并为解决问题提供建议。

然而，提示词长度为275个token（1,240个字符），而响应长度为248个token（1,205个字符）。由于这是一个示例堆栈跟踪，实际应用中的实现可能会包含更详细的真实信息。因此，如果控制设置仍然是输入或输出300个token，那么您已经非常接近这个限制。

为您的应用程序选择输入和输出token限制至关重要。虽然您希望控制成本，但您也不希望从根本上限制功能。如果token限制太低，LLM可能无法生成所需的输出。

# 生成式AI应用中的性能问题

GenAI最明显的失败是性能和可靠性相关的问题。既然您已经学习了关于准确性的内容[第10章](B22495_10.xhtml#_idTextAnchor214)，*通过优化语义数据模型提高准确性*，在本章的上下文中，性能意味着速度慢。如果用户向您的AI应用程序提出问题，并且没有响应、部分响应或响应缓慢，这通常比如果响应是虚构的或谄媚的更为明显。

几个因素可能导致GenAI应用程序的缓慢。GenAI中性能问题最常见的几个原因包括计算负载、网络延迟、模型服务策略以及高**输入/输出**（**I/O**）操作。

当然，可能还有更多原因。本节剩余部分将详细解释一些性能杀手，以及它们对您的应用程序和用户的影响。

## 计算负载

如您所知，LLMs需要大量的计算能力。生成查询响应所需的时间随着模型复杂性和规模的增加而增加。不恰当的请求会显著增加GenAI应用程序的计算负载。让我们看看几个这样的例子，以便您能够理解这种故障模式是如何发生的。

### 大量的数据处理和计算

需要处理大量数据集或执行大量计算的请求可能会带来计算上的负担，以下是一个例子。

**用户请求**：

```py
Evaluate a sample of the last 20,000 stock prices for TSLA, sort it from highest to lowest, and let me know on which days and times it had the highest price.
```

获取20,000条随机股票价格看似简单，但用户没有指定时间范围。模型应该评估最后20,000条股票价格的哪个时间段？是过去一个月？还是去年随机选取的时间段？对这些值的排序计算成本高昂，并进一步增加了返回列表的处理负担。

### 高复杂度请求

涉及评估大量数据、总结并返回大量结果的复杂请求也很耗费资源。通常，这涉及到通过高级提示技术，如ReACT模式和函数调用，链式调用多个LLM。

**推理和行动**（**ReACT**）模式是GenAI模型中用于处理需要多个推理和交互步骤的复杂任务的高级提示技术。这个模式涉及一个序列，其中模型对任务进行推理，生成中间动作，然后产生最终输出。ReACT模式帮助模型将复杂请求分解为可管理的步骤，提高最终响应的准确性和连贯性。

在LLMs的上下文中，**函数调用**涉及指示模型在其响应生成过程中执行特定的函数或动作。这对于需要结构化输出、计算、数据检索或与外部系统交互的任务特别有用。例如，开发者可以在提示中指定模型可以调用的函数来执行特定任务。这些函数是预定义的，可以处理各种操作，如查询数据库、执行计算或获取外部数据。

让我们通过一个高复杂度的请求来阐述这一点。

**用户请求**：

```py
Generate a detailed and historically accurate list of the top three priorities for every US president, but do not include their policies related to South America.
```

在这种情况下，GenAI 必须首先创建每位美国总统的名单，然后寻找每位总统的信息，接着创建他们在任期间政策和事件的详细总结。它还必须检索与总统优先考虑的事项相关的内容，确定哪些内容是最高优先级的共识，整理并总结所有这些信息，然后输出给用户。这是广泛的知识检索、分析和文本生成。很可能会需要多个 LLM 查询，而更多的查询意味着更多的开销。

这些例子说明了某些类型的用户请求如何显著增加 GenAI 应用程序的计算负载。现在让我们看看模型服务策略如何影响 GenAI 的性能。

## 模型服务策略

根据请求量，为每个请求单独生成响应可能效率低下。如果应用程序没有设计成可以同时处理多个请求，那么随着用户数量的增加，它将变得越来越慢。如果应用程序依赖于云服务，网络延迟会影响性能。客户端和服务器之间慢速的互联网连接或高延迟会导致延迟。频繁或复杂的 API 调用到外部服务会增加响应时间，尤其是如果这些服务正在经历高负载或地理位置遥远。

让我们以股票预测应用程序为例。

由于您的 GenAI 应用程序获得了一些新闻报道，您的网站流量激增，与应用程序互动的客户数量急剧增加。但是，由于您的应用程序逐个处理每个请求并且无法同时处理多个请求，随着系统变得不堪重负，每个用户的响应时间都会增加。用户会体验到更慢的响应时间，从而导致挫败感。

新闻报道来自澳大利亚悉尼的一位有影响力的人物，因此用户激增来自亚洲。您的服务器位于美国东部地区，由于服务器和客户端之间的地理距离，网络延迟导致延迟。互联网连接速度慢的客户体验到的等待时间更长，进一步降低了用户体验。

您的应用程序经常调用外部 API 来获取股票价格和金融市场新闻的实时数据。如果这些外部服务正在经历高负载，API 调用将需要更长的时间才能完成。

## 高 I/O 操作

不良的数据处理实践，如低效地读取大型数据集或未使用适当的数据结构，可能会降低性能。频繁的磁盘读写操作可能成为瓶颈，同样，数据库交互优化不良和查询格式错误也可能导致问题。示例股价预测应用程序经常读取大量历史股价数据集进行预测。让我们探讨一些数据处理问题，这些问题会导致高I/O操作：

+   该应用程序读取大型数据集效率低下，例如，即使只需要子集，也会将整个数据集加载到内存中，这消耗了过多的内存和计算能力，降低了性能。

+   该应用程序在每次预测周期结束后将中间预测结果和日志保存到磁盘。频繁的磁盘读写操作形成瓶颈，这显著增加了完成每个预测周期所需的时间。

+   在做出预测之前，该应用程序会查询数据库以获取最近的财经新闻和其他相关数据。然而，缺乏索引意味着查询结果缓慢返回。这增加了响应时间，使得应用程序对用户请求的响应速度变慢。

假设你有一个大型数据集，你将想要避免这些做法，因为它们会影响用户体验并增加成本。

# 摘要

现在你已经通过了这些GenAI挑战，你可以欣赏到这些强大技术所伴随的一些复杂性和细微差别。幻觉、谄媚、数据泄露、成本和性能问题构成了巨大的障碍，需要批判性的眼光和创新解决方案。每个挑战都为GenAI应用程序固有的局限性和潜在陷阱提供了独特的视角。

尽管存在这些障碍，GenAI仍然无可争议地具有价值。它继续改变行业，提高生产力，并为创造力和创新开辟新的途径。通过理解和解决这些挑战，开发者可以充分发挥GenAI的潜力，提供强大、可靠和负责任的应用程序。同时，也要注意，即使应用程序不总是正确，它们也可能是有用的。以ChatGPT为例：它已经极大地提高了数百万用户的效率，尽管其缺陷是众所周知的（而且有些并不容易解决）。你的GenAI应用程序也可能同样有用和受欢迎，但同样存在类似的警告。

在下一章中，你将了解优化你的GenAI应用程序的方法，提高其输出和性能，从而改善用户体验，并解决这里讨论的一些问题。
