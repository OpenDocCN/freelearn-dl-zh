<html><head></head><body>
		<div><h1 id="_idParaDest-79" class="chapter-number"><a id="_idTextAnchor078"/><st c="0">4</st></h1>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/><st c="2">Components of a RAG System</st></h1>
			<p><st c="28">When you’re developing </st><a id="_idIndexMarker162"/><st c="52">with </st><strong class="bold"><st c="57">retrieval-augmented generation</st></strong><st c="87"> (</st><strong class="bold"><st c="89">RAG</st></strong><st c="92">), it is essential to understand the intricacies of each component, how they can be integrated, and the technologies that empower </st><st c="223">these systems.</st></p>
			<p><st c="237">In this chapter, we will cover the </st><st c="273">following topics:</st></p>
			<ul>
				<li><st c="290">Key </st><st c="295">component overview</st></li>
				<li><st c="313">Indexing</st></li>
				<li><st c="322">Retrieval </st><st c="333">and generation</st></li>
				<li><st c="347">Prompting</st></li>
				<li><st c="357">Defining </st><st c="367">your LLM</st></li>
				<li><st c="375">User </st><st c="381">interface (UI)</st></li>
				<li><st c="395">Evaluation</st></li>
			</ul>
			<p><st c="406">These topics should provide you with a comprehensive understanding of the key components representing a </st><st c="511">RAG application.</st></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/><st c="527">Technical requirements</st></h1>
			<p><st c="550">The code for this chapter is placed in the following GitHub </st><st c="611">repository: </st><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_04"><st c="623">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_04</st></a></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/><st c="720">Key component overview</st></h1>
			<p><st c="743">This chapter delves</st><a id="_idIndexMarker163"/><st c="763"> into the intricate components that make up a RAG system. </st><st c="821">Let’s start with an overview of the </st><st c="857">entire system.</st></p>
			<p><st c="871">In </st><a href="B22475_01.xhtml#_idTextAnchor015"><em class="italic"><st c="875">Chapter 1</st></em></a><st c="884">, we introduced the three main stages of the RAG system from a technical standpoint (see </st><em class="italic"><st c="973">Figure 4</st></em><em class="italic"><st c="981">.1</st></em><st c="983">):</st></p>
			<ul>
				<li><strong class="bold"><st c="986">Indexing</st></strong></li>
				<li><strong class="bold"><st c="995">Retrieval</st></strong></li>
				<li><strong class="bold"><st c="1005">Generation</st></strong></li>
			</ul>
			<div><div><img src="img/B22475_04_01.jpg" alt="Figure 4.1 – The three stages of a RAG system"/><st c="1016"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="1095">Figure 4.1 – The three stages of a RAG system</st></p>
			<p><st c="1140">We will continue to build off this concept, but we will also introduce the practical aspects of development that are required for building an application. </st><st c="1296">These include prompting, defining </st><a id="_idIndexMarker164"/><st c="1330">your </st><strong class="bold"><st c="1335">large language model</st></strong><st c="1355"> (</st><strong class="bold"><st c="1357">LLM</st></strong><st c="1360">), the UI, and an evaluation component. </st><st c="1401">Later</st><a id="_idIndexMarker165"/><st c="1406"> chapters will cover each of those areas even further. </st><st c="1461">All of this will be done with code so that you can tie the conceptual framework we’ll discuss directly with the implementation. </st><st c="1589">Let’s start </st><st c="1601">with indexing.</st></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/><st c="1615">Indexing</st></h1>
			<p><st c="1624">The first stage in</st><a id="_idIndexMarker166"/><st c="1643"> the RAG system we will examine more closely is indexing. </st><st c="1701">Note</st><a id="_idIndexMarker167"/><st c="1705"> that we are skipping the setup, where we install and import packages, as well as set up OpenAI and related accounts. </st><st c="1823">That is a typical step in every generative artificial intelligence (AI) project, not just RAG systems. </st><st c="1926">We provided a thorough setup guide in </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="1964">Chapter 2</st></em></a><st c="1973">, so jump back there if you want to review the libraries we’ve added to support these </st><st c="2059">next steps.</st></p>
			<p><st c="2070">Indexing occurs as the first main stage of RAG. </st><st c="2119">As </st><em class="italic"><st c="2122">Figure 4</st></em><em class="italic"><st c="2130">.2</st></em><st c="2132"> indicates, it is the step after the </st><st c="2169">user query:</st></p>
			<div><div><img src="img/B22475_04_02.jpg" alt="Figure 4.2 – The ﻿Indexing stage of RAG highlighted"/><st c="2180"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="2254">Figure 4.2 – The Indexing stage of RAG highlighted</st></p>
			<p><st c="2304">In our code from </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="2322">Chapter 2</st></em></a><st c="2331">, </st><em class="italic"><st c="2333">Indexing</st></em><st c="2341"> is the</st><a id="_idIndexMarker168"/><st c="2348"> first section of code you see. </st><st c="2380">This is the step where the data you are introducing to the RAG system is processed. </st><st c="2464">As you can see in the code, the </st><em class="italic"><st c="2496">data</st></em><st c="2500"> in this scenario is the web document that is being loaded by </st><code><st c="2562">WebBaseLoader</st></code><st c="2575">. This is</st><a id="_idIndexMarker169"/><st c="2584"> the beginning of that document (</st><em class="italic"><st c="2617">Figure 4</st></em><em class="italic"><st c="2626">.3</st></em><st c="2628">):</st></p>
			<div><div><img src="img/B22475_04_03.jpg" alt="Figure 4.3 – The web page that we process"/><st c="2631"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5490">Figure 4.3 – The web page that we process</st></p>
			<p><st c="5531">In </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="5535">Chapter 2</st></em></a><st c="5544">, you may have noticed that the code in the latter stages, </st><em class="italic"><st c="5603">Retrieval</st></em><st c="5612"> and </st><em class="italic"><st c="5617">Generation</st></em><st c="5627">, is used after the user query is passed to the chain. </st><st c="5682">This is done in </st><em class="italic"><st c="5698">real time</st></em><st c="5707">, meaning that it happens at the time that the user interacts with it. </st><st c="5778">Indexing, on the other hand, typically happens well before the user interacts with the RAG application. </st><st c="5882">This aspect of indexing makes it a very different step from the other two stages, with the flexibility of being run at a different time than when the application is used. </st><st c="6053">This is </st><a id="_idIndexMarker170"/><st c="6061">called </st><strong class="bold"><st c="6068">pre-processing offline</st></strong><st c="6090">, meaning that this step is done before the user has even opened the application. </st><st c="6172">There are instances where indexing can be done in real time as well, but that is much less common. </st><st c="6271">For now, we will focus on the much more common step of </st><st c="6326">pre-processing offline.</st></p>
			<p><st c="6349">The</st><a id="_idIndexMarker171"/><st c="6353"> following</st><a id="_idIndexMarker172"/><st c="6363"> code</st><a id="_idIndexMarker173"/><st c="6368"> is our </st><strong class="bold"><st c="6376">document extraction</st></strong><st c="6395">:</st></p>
			<pre class="source-code"><st c="6397">
loader = WebBaseLoader(
    web_paths=("https://kbourne.github.io/chapter1.html",)
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title",
                    "post-header")
       )
    ),
)
docs = loader.load()</st></pre>
			<p><st c="6602">In this extract, we are ingesting a web page. </st><st c="6649">But imagine if this was pulling data in from a PDF or Word document, or other forms of unstructured data. </st><st c="6755">As discussed in </st><a href="B22475_03.xhtml#_idTextAnchor056"><em class="italic"><st c="6771">Chapter 3</st></em></a><st c="6780">, unstructured data is a very popular data format in RAG applications. </st><st c="6851">Historically, unstructured data has been very difficult for companies to access relative to structured data (from SQL databases and similar applications). </st><st c="7006">But RAG has changed all of this, and companies are finally realizing how to significantly tap into this data. </st><st c="7116">We will review how to access other types of data</st><a id="_idIndexMarker174"/><st c="7164"> using </st><strong class="bold"><st c="7171">document loaders</st></strong><st c="7187"> in </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="7191">Chapter 11</st></em></a><st c="7201"> and how to do it </st><st c="7219">with LangChain.</st></p>
			<p><st c="7234">Regardless of what type of data you are pulling in, it all goes through a similar process, as shown in </st><em class="italic"><st c="7338">Figure 4</st></em><em class="italic"><st c="7346">.4</st></em><st c="7348">:</st></p>
			<div><div><img src="img/B22475_04_04.jpg" alt="Figure 4.4 – Creating a retriever in the Indexing stage of the RAG process"/><st c="7350"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7404">Figure 4.4 – Creating a retriever in the Indexing stage of the RAG process</st></p>
			<p><st c="7478">The document loader from the code fills the </st><strong class="bold"><st c="7523">Documents</st></strong><st c="7532"> component of this so that they can be retrieved later using the user query. </st><st c="7609">But in most RAG applications, you must turn that data into a more searchable format: vectors. </st><st c="7703">We will talk more about vectors in a moment, but first, to get your data into vector format, you must </st><a id="_idIndexMarker175"/><st c="7805">apply </st><strong class="bold"><st c="7811">splitting</st></strong><st c="7820">. In our code, that is </st><st c="7843">this section:</st></p>
			<pre class="source-code"><st c="7856">
text_splitter = SemanticChunker(OpenAIEmbeddings())
splits = text_splitter.split_documents(docs)</st></pre>
			<p><st c="7953">Splitting breaks your content into digestible chunks that can be vectorized. </st><st c="8031">Different vectorization algorithms have different requirements for the maximum size of the content</st><a id="_idIndexMarker176"/><st c="8129"> you can </st><a id="_idIndexMarker177"/><st c="8138">pass. </st><st c="8144">In this case, we are using the </st><code><st c="8175">OpenAIEmbeddings()</st></code><st c="8193"> vectorizer, which currently has a max input of </st><code><st c="8241">8191</st></code><st c="8245"> tokens.</st></p>
			<p class="callout-heading"><st c="8253">Note</st></p>
			<p class="callout"><st c="8258">In the OpenAI API, the text is tokenized using a byte-level </st><strong class="bold"><st c="8319">byte pair encoding</st></strong><st c="8337"> (</st><strong class="bold"><st c="8339">BPE</st></strong><st c="8342">) vocabulary. </st><st c="8357">This means the raw text is split</st><a id="_idIndexMarker178"/><st c="8389"> into subword tokens rather than individual characters. </st><st c="8445">The number of tokens that are consumed for a given input text depends on the specific content as common words and subwords are represented by single tokens while less common words may be split into multiple tokens. </st><st c="8660">On average, one token is approximately four characters for English text. </st><st c="8733">However, this is just a rough estimate and can vary significantly based on the specific text. </st><st c="8827">For example, short words such as </st><em class="italic"><st c="8860">a</st></em><st c="8861"> or </st><em class="italic"><st c="8865">the</st></em><st c="8868"> would be a single token, while a long, uncommon word might be split into </st><st c="8942">several tokens.</st></p>
			<p><st c="8957">These digestible chunks need to be smaller than that </st><code><st c="9011">8191</st></code><st c="9015"> token limit, and other embedding services have their token limits. </st><st c="9083">If you’re using a splitter that defines a chunk size and a chunk overlap, keep the chunk overlap in mind for that token limit as well. </st><st c="9218">You have to add that overlap to the overall chunk size to be able to determine how large that chunk is. </st><st c="9322">Here is an example o</st><a id="_idIndexMarker179"/><st c="9342">f using a </st><code><st c="9409">1000</st></code><st c="9413"> and the chunk overlap </st><st c="9436">is </st><code><st c="9439">200</st></code><st c="9442">:</st></p>
			<pre class="source-code"><st c="9444">
text_splitter = RecursiveCharacterTextSplitter(
   chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)</st></pre>
			<p><st c="9573">Expanding the chunk overlap is a common approach to ensuring that no context is lost between chunks. </st><st c="9675">For example, if a chunk happens to cut an address in half in a legal document, it is unlikely you will find that address if you search for it. </st><st c="9818">But with chunk overlap, you can account for issues like that. </st><st c="9880">We will review various splitter options, including the </st><a id="_idIndexMarker180"/><st c="9935">recursive</st><a id="_idIndexMarker181"/><st c="9944"> character </st><code><st c="9955">TextSplitter</st></code><st c="9967"> in LangChain in </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="9984">Chapter 11</st></em></a><st c="9994">.</st></p>
			<p><st c="9995">The last part of the </st><em class="italic"><st c="10017">Indexing</st></em><st c="10025"> stage is defining the vector store and adding the embeddings, built from your data splits to that vector store. </st><st c="10138">You see it here in </st><st c="10157">this code:</st></p>
			<pre class="source-code"><st c="10167">
vectorstore = Chroma.from_documents(
                   documents=splits,
                   embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()</st></pre>
			<p><st c="10291">In this case, we use </st><code><st c="10712">OpenAIEmbeddings</st></code><st c="10728"> API is just one of many vectorizing algorithms that can be used here as well. </st><st c="10807">We will dive into this topic in more detail in </st><em class="italic"><st c="10854">Chapters 7</st></em><st c="10864"> and </st><em class="italic"><st c="10869">8</st></em><st c="10870"> when we discuss vectors, vector stores, and </st><st c="10915">vector searches.</st></p>
			<p><st c="10931">Going back to our diagram of the </st><em class="italic"><st c="10965">Indexing</st></em><st c="10973"> process, </st><em class="italic"><st c="10983">Figure 4</st></em><em class="italic"><st c="10991">.5</st></em><st c="10993"> is an even more accurate representation of what it </st><st c="11045">looks like:</st></p>
			<div><div><img src="img/B22475_04_05.jpg" alt="Figure 4.5 – Vectors during the Indexing stage of the RAG process"/><st c="11056"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11246">Figure 4.5 – Vectors during the Indexing stage of the RAG process</st></p>
			<p><st c="11311">You might be wondering why we aren’t calling the step where we define the </st><em class="italic"><st c="11386">retriever</st></em><st c="11395"> a part of the </st><em class="italic"><st c="11410">Retrieval</st></em><st c="11419"> step. </st><st c="11426">This is because we are establishing this as the mechanism that we retrieve from, but we do not apply the retrieval until later during the retrieval step as a result of the user submitting their user query. </st><st c="11632">The </st><em class="italic"><st c="11636">Indexing</st></em><st c="11644"> step focuses on building the infrastructure that the other two steps work from, and we are indeed indexing the data so</st><a id="_idIndexMarker183"/><st c="11763"> that it can be retrieved later. </st><st c="11796">At the end of this part of the</st><a id="_idIndexMarker184"/><st c="11826"> code, you have a retriever ready and waiting to receive a user query when the process starts. </st><st c="11921">Let’s talk about the parts of the code that will use this retriever – the retrieval and </st><st c="12009">generation steps!</st></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/><st c="12026">Retrieval and generation</st></h1>
			<p><st c="12051">In our RAG application code, we have combined the </st><em class="italic"><st c="12102">Retrieval</st></em><st c="12111"> and </st><em class="italic"><st c="12116">Generation</st></em><st c="12126"> stages. </st><st c="12135">From a diagram standpoint, this looks like what’s shown in </st><em class="italic"><st c="12194">Figure 4</st></em><em class="italic"><st c="12202">.6</st></em><st c="12204">:</st></p>
			<div><div><img src="img/B22475_04_06.jpg" alt="Figure 4.6 – Vectors during the Indexing stage of the RAG process"/><st c="12206"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="12278">Figure 4.6 – Vectors during the Indexing stage of the RAG process</st></p>
			<p><st c="12343">While retrieval and generation are two separate stages serving two important functions of the RAG application, they are combined in our code. </st><st c="12486">When we invoke </st><code><st c="12501">rag_chain</st></code><st c="12510"> as the last step, it is stepping through both of these stages, making them difficult to separate when talking about the code. </st><st c="12637">But conceptually, we will separate them here, and then show how they pull them together to process the user query and provide an intelligent generative AI response. </st><st c="12802">Let’s start with the </st><st c="12823">retrieval step.</st></p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/><st c="12838">Retrieval focused steps</st></h2>
			<p><st c="12862">In the</st><a id="_idIndexMarker185"/><st c="12869"> complete </st><a id="_idIndexMarker186"/><st c="12879">code (which can be found in </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="12907">Chapter 2</st></em></a><st c="12916">), there are only two areas in this code where actual retrieval takes place or is processed. </st><st c="13010">This is </st><st c="13018">the first:</st></p>
			<pre class="source-code"><st c="13028">
# Post-processing
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</st></pre>
			<p><st c="13122">The second can be found as the first step within the </st><st c="13176">RAG chain:</st></p>
			<pre class="source-code"><st c="13186">
 {"context": retriever | format_docs, "question": RunnablePassthrough()}</st></pre>
			<p><st c="13258">When the code is initiated, it runs in </st><st c="13298">this order:</st></p>
			<pre class="source-code"><st c="13309">
rag_chain.invoke("What are the Advantages of using RAG?")</st></pre>
			<p><st c="13367">The chain is invoked with the user query and runs through the steps we defined in the </st><st c="13454">chain here:</st></p>
			<pre class="source-code"><st c="13465">
rag_chain = (
    {"context": </st><strong class="bold"><st c="13492">retriever | format_docs</st></strong><st c="13515">,
     "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)</st></pre>
			<p><st c="13588">With this chain, the user query is passed to the first link, which passes that user query into the retriever we defined earlier, where it performs a similarity search to match the user query against the other data in the vector store. </st><st c="13824">At this point, we have a retrieved list of content strings that is contextually similar to the </st><st c="13919">user query.</st></p>
			<p><st c="13930">However, as shown in </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="13952">Chapter 2</st></em></a><st c="13961">, there is a bit of a glitch in our retrieval steps due to the formatting of the tools we are using. </st><st c="14062">The </st><code><st c="14066">{question}</st></code><st c="14076"> and </st><code><st c="14081">{context}</st></code><st c="14090"> placeholders both expect strings, but the retrieval mechanism we use to fill in the context is a long list of separate content strings. </st><st c="14227">We need a mechanism to turn that list of content pieces</st><a id="_idIndexMarker187"/><st c="14282"> into</st><a id="_idIndexMarker188"/><st c="14287"> the string format that the prompt in the next chain link </st><st c="14345">is expecting.</st></p>
			<p><st c="14358">So, if you look closely at the code for the retriever, you may notice that the retriever is actually in a mini-chain (</st><code><st c="14477">retriever | format_docs</st></code><st c="14501">), indicated by the pipe (</st><code><st c="14528">|</st></code><st c="14530">) symbol, so the output of the retriever is passed right into the </st><code><st c="14596">format_docs</st></code><st c="14607"> function </st><st c="14617">shown here:</st></p>
			<pre class="source-code"><st c="14628">
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</st></pre>
			<p><st c="14704">Let’s consider this a post-processing step in the </st><em class="italic"><st c="14755">Retrieval</st></em><st c="14764"> stage. </st><st c="14772">The data has been retrieved, but it is not in the right format, so we are not done. </st><st c="14856">The </st><code><st c="14860">format_docs</st></code><st c="14871"> function completes the task and returns our content in the </st><st c="14931">proper format.</st></p>
			<p><st c="14945">However, this only provides us with </st><code><st c="14982">{context}</st></code><st c="14991">, one of the input variable placeholders. </st><st c="15033">The other placeholder we need to hydrate our prompt is the </st><code><st c="15092">{question}</st></code><st c="15102"> placeholder. </st><st c="15116">However, we do not have the same formatting problem with the </st><em class="italic"><st c="15177">question</st></em><st c="15185"> that we had with the </st><em class="italic"><st c="15207">context</st></em><st c="15214"> since the </st><em class="italic"><st c="15225">question</st></em><st c="15233"> is already a string. </st><st c="15255">So, we can use a convenient object called </st><code><st c="15297">RunnablePassThrough</st></code><st c="15316"> that, as its name suggests, passes the input (the </st><em class="italic"><st c="15367">question</st></em><st c="15375">) </st><st c="15378">through as-is.</st></p>
			<p><st c="15392">If you take the entire first chain link in its entirety, this is essentially performing the retrieval step, formatting its output, and pulling it all together in the proper format to pass on to the </st><st c="15591">next step:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="15601">{"context": retriever | format_docs, "question": RunnablePassthrough()}</st></strong></pre>
			<p><st c="15673">But wait a minute. </st><st c="15693">If you’re doing a vector search, you need to convert the user query into a vector, right? </st><st c="15783">Did we not say that we are taking the mathematical representation of the user query and measuring the distance to other vectors, finding which ones are closer? </st><st c="15943">So, where does that happen? </st><st c="15971">The retriever was created from a method of the </st><st c="16018">vector store:</st></p>
			<pre class="source-code"><st c="16031">
retriever = vectorstore.as_retriever()</st></pre>
			<p><st c="16070">The vector store that this was generated from is a Chroma vector database that was declared </st><a id="_idIndexMarker189"/><st c="16163">using</st><a id="_idIndexMarker190"/><st c="16168"> the </st><code><st c="16173">OpenAIEmbeddings()</st></code><st c="16191"> object as its </st><st c="16206">embedding function:</st></p>
			<pre class="source-code"><st c="16225">
vectorstore = Chroma.from_documents(
                   documents=splits,
                   embedding=OpenAIEmbeddings())</st></pre>
			<p><st c="16310">That </st><code><st c="16316">.as_retriever()</st></code><st c="16331"> method has all of the functionality built in to take that user query, convert it into an embedding that matches the embedding format of the other embeddings, and then run the </st><st c="16507">retrieval process.</st></p>
			<p class="callout-heading"><st c="16525">Note</st></p>
			<p class="callout"><st c="16530">Because this is using the </st><code><st c="16557">OpenAIEmbeddings()</st></code><st c="16575"> object, it sends your embeddings to the OpenAI API and you will incur charges from this. </st><st c="16665">In this case, it is just one embedding; with OpenAI, this currently costs $0.10 per 1M tokens. </st><st c="16760">So, for the </st><code><st c="16772">What are the Advantages of using RAG?</st></code><st c="16809"> input, which is ten tokens according to OpenAI, this is going to cost a whopping $0.000001. </st><st c="16902">That may not seem like a lot, but we want to be completely transparent when there is any </st><st c="16991">cost involved!</st></p>
			<p><st c="17005">That concludes our </st><em class="italic"><st c="17025">Retrieval</st></em><st c="17034"> stage, with an output that is properly formatted for the next step – the prompt! </st><st c="17116">Next, we’ll discuss the </st><em class="italic"><st c="17140">Generation</st></em><st c="17150"> stage, where we utilize the LLM to take the</st><a id="_idIndexMarker191"/><st c="17194"> final</st><a id="_idIndexMarker192"/><st c="17200"> step of generating </st><st c="17220">a response.</st></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/><st c="17231">Generation stage</st></h2>
			<p><st c="17248">The </st><em class="italic"><st c="17253">Generation</st></em><st c="17263"> stage is</st><a id="_idIndexMarker193"/><st c="17272"> the final stage and is where you will use</st><a id="_idIndexMarker194"/><st c="17314"> the LLM to generate the response to the user query based on the content you retrieved in the </st><em class="italic"><st c="17408">Retrieval</st></em><st c="17417"> stage. </st><st c="17425">But before we can do this, we have to do a little bit of preparation work. </st><st c="17500">Let’s walk </st><st c="17511">through this.</st></p>
			<p><st c="17524">Overall, the </st><em class="italic"><st c="17538">Generation</st></em><st c="17548"> stage is represented by two parts of the code, starting with </st><st c="17610">the prompt:</st></p>
			<pre class="source-code"><st c="17621">
prompt = hub.pull("jclemens24/rag-prompt")</st></pre>
			<p><st c="17664">Then, we have </st><st c="17679">the LLM:</st></p>
			<pre class="source-code"><st c="17687">
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)</st></pre>
			<p><st c="17740">With the prompt and LLM defined, these components are used in the </st><st c="17807">RAG chain:</st></p>
			<pre class="source-code"><st c="17817">
  | prompt
  | llm</st></pre>
			<p><st c="17832">Note that the </st><code><st c="17847">question</st></code><st c="17855"> section was bolded in both the </st><em class="italic"><st c="17887">Retrieval</st></em><st c="17896"> and the </st><em class="italic"><st c="17905">Generation</st></em><st c="17915"> stages. </st><st c="17924">We already noted how it is used in the </st><em class="italic"><st c="17963">Retrieval</st></em><st c="17972"> stage as the basis for what the similarity search is run against. </st><st c="18039">Now, we will show how it is used again when integrating it into a prompt that is fed to the LLM </st><st c="18135">for generation.</st></p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/><st c="18150">Prompting</st></h1>
			<p><strong class="bold"><st c="18160">Prompts</st></strong><st c="18168"> are a</st><a id="_idIndexMarker195"/><st c="18174"> fundamental part of any generative AI application, not</st><a id="_idIndexMarker196"/><st c="18229"> just RAG. </st><st c="18240">When you start talking about prompts, particularly with RAG, you know LLMs are going to be involved soon after. </st><st c="18352">But first, you must create and prepare a proper prompt for our LLM. </st><st c="18420">In theory, you could write your prompt, but I wanted to take this chance to teach you this very common development pattern and get you used to using it when you need it. </st><st c="18590">In this example, we’ll pull the prompt </st><a id="_idIndexMarker197"/><st c="18629">from the </st><strong class="bold"><st c="18638">LangChain Hub</st></strong><st c="18651">.</st></p>
			<p><st c="18652">LangChain describes its Hub as a place to “</st><em class="italic"><st c="18696">discover, share, and version control prompts.</st></em><st c="18742">” Other users of the hub have shared their polished prompts here, making it easier for you to build off common knowledge. </st><st c="18865">It is a good way to start with prompts, pulling down pre-designed prompts and seeing how they are written. </st><st c="18972">But you will eventually want to move on to writing your own, more </st><st c="19038">customized prompts.</st></p>
			<p><st c="19057">Let’s talk about what the purpose of this prompt is in terms of the retrieval process. </st><st c="19145">The “prompt” is the next link in the chain after the Retrieval stage we just discussed. </st><st c="19233">You can see it here </st><st c="19253">in </st><code><st c="19256">rag_chain</st></code><st c="19265">:</st></p>
			<pre class="source-code"><st c="19267">
rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
    | </st><strong class="bold"><st c="19356">prompt</st></strong><st c="19362">
    | llm
    | StrOutputParser()
)</st></pre>
			<p><st c="19390">Staying true to the LangChain pattern, the inputs of the prompt are the outputs of the previous step. </st><st c="19493">You can see these inputs at any time by printing them out </st><st c="19551">like this:</st></p>
			<pre class="source-code"><st c="19561">
prompt = hub.pull("jclemens24/rag-prompt")
prompt.input_variables</st></pre>
			<p><st c="19627">This results in the </st><st c="19648">following output:</st></p>
			<pre class="source-code"><st c="19665">
['context', 'question']</st></pre>
			<p><st c="19689">This matches what we defined in the </st><st c="19726">previous step:</st></p>
			<pre class="source-code"><st c="19740">
{"</st><strong class="bold"><st c="19743">context</st></strong><st c="19751">": retriever | format_docs,
 "</st><strong class="bold"><st c="19781">question</st></strong><st c="19790">": RunnablePassthrough()}</st></pre>
			<p><st c="19816">Printing out the entire prompt object, using </st><code><st c="19862">print(prompt)</st></code><st c="19875">, shows that there is much more than just the text prompt and </st><st c="19937">input variables:</st></p>
			<pre class="source-code"><st c="19953">
input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. </st><st c="20155">Use the following pieces of retrieved-context to answer the question. </st><st c="20225">If you don't know the answer, just say that you don't know.\nQuestion: {question} \nContext: {context} \nAnswer:"))]</st></pre>
			<p><st c="20341">Let’s unravel this a bit further, starting with the input variables. </st><st c="20411">These are the variables we just discussed, that this particular prompt takes as input. </st><st c="20498">These can vary depending on the prompt. </st><st c="20538">There is a </st><code><st c="20549">messages []</st></code><st c="20560"> list, but in this case, there is only one message in the list. </st><st c="20624">This message is an instance of </st><code><st c="20655">HumanMessagePromptTemplate</st></code><st c="20681">, which represents a specific type of message template. </st><st c="20737">It</st><a id="_idIndexMarker198"/><st c="20739"> is </st><a id="_idIndexMarker199"/><st c="20743">initialized with a </st><code><st c="20762">PromptTemplate</st></code><st c="20776"> object. </st><st c="20785">The </st><code><st c="20789">PromptTemplate</st></code><st c="20803"> object is created with the specified </st><code><st c="20841">input_variables</st></code><st c="20856"> and a template string. </st><st c="20880">Again, </st><code><st c="20887">input_variables</st></code><st c="20902"> are </st><code><st c="20907">context</st></code><st c="20914"> and </st><code><st c="20919">question</st></code><st c="20927">, and you can see where they are placed in the </st><code><st c="20974">template</st></code><st c="20982"> string:</st></p>
			<pre class="source-code"><st c="20990">
template="You are an assistant for question-answering tasks. </st><st c="21052">Use the following pieces of retrieved-context to answer the question. </st><st c="21122">If you don't know the answer, just say that you don't know.\nQuestion: </st><strong class="bold"><st c="21193">{question}</st></strong><st c="21203"> \nContext: </st><strong class="bold"><st c="21215">{context}</st></strong><st c="21224"> \nAnswer:"</st></pre>
			<p><st c="21235">The </st><code><st c="21240">{question}</st></code><st c="21250"> and </st><code><st c="21255">{context}</st></code><st c="21264"> placeholders will be replaced with the actual values of the </st><code><st c="21325">question</st></code><st c="21333"> and </st><code><st c="21338">context</st></code><st c="21345"> variables when the prompt is used in the chain. </st><st c="21394">The output of this chain link is the string template that was filled in with </st><code><st c="21471">{question}</st></code><st c="21481"> and </st><code><st c="21486">{context}</st></code><st c="21495"> from the previous </st><st c="21514">retrieval step.</st></p>
			<p><st c="21529">The last part is simply </st><code><st c="21554">Answer:</st></code><st c="21561"> with nothing after that. </st><st c="21587">This prompts the LLM for an answer and is a prevalent pattern that works well for LLM interactions to elicit </st><st c="21696">an answer.</st></p>
			<p><st c="21706">In short, a prompt is an object that is plugged into your LangChain chain with inputs to fill a prompt template, generating the prompt that you will pass to an LLM for inference. </st><st c="21886">This is essentially a</st><a id="_idIndexMarker200"/><st c="21907"> preparation stage for the </st><em class="italic"><st c="21934">Generation</st></em><st c="21944"> stage of the </st><st c="21958">RAG</st><a id="_idIndexMarker201"/><st c="21961"> system.</st></p>
			<p><st c="21969">In the next step, we will pull in the LLM, the brains behind the </st><st c="22035">whole operation!</st></p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/><st c="22051">Defining your LLM</st></h1>
			<p><st c="22069">With the prompt </st><a id="_idIndexMarker202"/><st c="22086">template selected, we can select an LLM, a central component for any RAG application. </st><st c="22172">The following code shows the LLM model as the next chain link </st><st c="22234">in </st><code><st c="22237">rag_chain</st></code><st c="22246">:</st></p>
			<pre class="source-code"><st c="22248">
rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
    | prompt
    | </st><strong class="bold"><st c="22346">llm</st></strong><st c="22349">
    | StrOutputParser()
)</st></pre>
			<p><st c="22371">As discussed previously, the output of the previous step, which was the </st><code><st c="22444">prompt</st></code><st c="22450"> object, is going to be the input of the next step, the LLM. </st><st c="22511">In this case, the prompt will </st><em class="italic"><st c="22541">pipe</st></em><st c="22545"> right into the LLM with the prompt we generated in the </st><st c="22601">previous step.</st></p>
			<p><st c="22615">Above </st><code><st c="22622">rag_chain</st></code><st c="22631">, we define the LLM we want </st><st c="22659">to use:</st></p>
			<pre class="source-code"><st c="22666">
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)</st></pre>
			<p><st c="22719">This is creating an instance of the </st><code><st c="22756">ChatOpenAI</st></code><st c="22766"> class from the </st><code><st c="22782">langchain_openai </st></code><st c="22799">module, which serves as an interface to OpenAI’s language models, specifically the GPT-4o model. </st><st c="22896">LLMs are typically fed a prompt using the invoke method, and you could call this directly in the code here by adding </st><st c="23013">the following:</st></p>
			<pre class="source-code"><st c="23027">
llm_only = llm.invoke("Answering in less than 100 words,
    what are the Advantages of using RAG?")
print(llm_only.content)</st></pre>
			<p><st c="23148">Doing it this way, you are asking the LLM directly for </st><st c="23204">the answer.</st></p>
			<p><st c="23215">If you run the preceding code, it will give you the response from GPT-4o, which will know about RAG. </st><st c="23317">But for comparison, what if we changed it to GPT3.5? </st><st c="23370">Here is the response I received when using </st><st c="23413">ChatGPT 3.5:</st></p>
			<pre class="source-code"><st c="23425">
RAG (Red, Amber, Green) status reporting allows for clear and straightforward communication of project progress or issues. </st><st c="23549">It helps to quickly identify areas that need attention or improvement, enabling timely decision-making. </st><st c="23653">RAG status also provides a visual representation of project health, making it easy for stakeholders to understand the current situation at a glance. </st><st c="23802">Additionally, using RAG can help prioritize tasks and resources effectively, increasing overall project efficiency and success.'</st></pre>
			<p><st c="23930">Uh-oh! </st><st c="23938">ChatGPT 3.5 doesn’t know about RAG! </st><st c="23974">At least not in the context we are talking about. </st><st c="24024">This highlights the value of using RAG to add your data. </st><st c="24081">The most recent cutoff date for ChatGPT 3.5 was January 2022. </st><st c="24143">The generative AI-focused concept of RAG must not have been popular enough for it to instantly know what I was referring to with the </st><st c="24276">RAG acronym.</st></p>
			<p><st c="24288">Using RAG, we can </st><a id="_idIndexMarker203"/><st c="24307">augment its knowledge and utilize the LLM’s other skills of summarizing and finding data to have a more successful result overall. </st><st c="24438">But try changing this to the question </st><code><st c="24476">answering in less than 100 words, what are the Advantages of using Retrieval Augmented Generation (RAG)?</st></code><st c="24580"> and see what results you can get. </st><st c="24615">Try it with a newer model that likely has more information about RAG applications in its training data. </st><st c="24719">You will likely get a better response because the data that the LLM was trained on has a more recent </st><st c="24820">cutoff date!</st></p>
			<p><st c="24832">But instead of calling the LLM directly, we pass it the prompt we have structured using the </st><em class="italic"><st c="24925">Retrieval</st></em><st c="24934"> stage and can get a much more informed answer. </st><st c="24982">You could end the chain here and the output of your chain would be what’s returned from the LLM. </st><st c="25079">In most cases, this is not just the text you might see when you type something into ChatGPT – it is in JSON format and has a lot of other data included with it. </st><st c="25240">So, if you want a nicely formatted string output reflecting the LLM’s response, you have one more chain link to pipe the LLM response into: the </st><code><st c="25384">StrOutputParser()</st></code><st c="25401"> object. </st><st c="25410">The </st><code><st c="25414">StrOutputParser()</st></code><st c="25431"> object is a utility class in LangChain that parses the key output of the language model into a string format. </st><st c="25542">Not only does it strip away all the information you did not want to deal with right now, but it ensures that the generated response is returned as </st><st c="25689">a string.</st></p>
			<p><st c="25698">And of course, the last line of code is the line that kicks </st><st c="25759">everything off:</st></p>
			<pre class="source-code"><st c="25774">
rag_chain.invoke("What are the Advantages of using RAG?")</st></pre>
			<p><st c="25832">After the </st><em class="italic"><st c="25843">Retrieval</st></em><st c="25852"> stage, this user query is used a second time as one of the input variables for the prompt that is passed to the LLM. </st><st c="25970">Here, </st><code><st c="25976">What are the advantages of using RAG?</st></code><st c="26013"> is the string that’s passed into </st><st c="26047">the chain.</st></p>
			<p><st c="26057">As we discussed</st><a id="_idIndexMarker204"/><st c="26073"> in </st><a href="B22475_02.xhtml#_idTextAnchor035"><em class="italic"><st c="26077">Chapter 2</st></em></a><st c="26086">, in the future, this prompt will include a query that comes from a UI. </st><st c="26158">Let’s discuss the UI as another important component of the </st><st c="26217">RAG system.</st></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/><st c="26228">UI</st></h1>
			<p><st c="26231">At some point, to make </st><a id="_idIndexMarker205"/><st c="26255">this application more professional and usable, you must add a way for regular users who do not have your code to enter their queries directly and see the results. </st><st c="26418">The UI serves as the primary point of interaction between the user and the system and therefore is a critical component when building a RAG application. </st><st c="26571">Advanced interfaces might</st><a id="_idIndexMarker206"/><st c="26596"> include </st><strong class="bold"><st c="26605">natural language understanding</st></strong><st c="26635"> (</st><strong class="bold"><st c="26637">NLU</st></strong><st c="26640">) capabilities to interpret the user’s intent more accurately, a form of </st><strong class="bold"><st c="26714">natural language processing</st></strong><st c="26741"> (</st><strong class="bold"><st c="26743">NLP</st></strong><st c="26746">) that focuses on the understanding part of natural language. </st><st c="26809">This component is crucial for ensuring that users can easily and effectively communicate their needs to </st><st c="26913">the system.</st></p>
			<p><st c="26924">This begins with replacing this last line with </st><st c="26972">a UI:</st></p>
			<pre class="source-code"><st c="26977">
rag_chain.invoke("What are the Advantages of using RAG?")</st></pre>
			<p><st c="27035">This line would be replaced with an entry field for the user to submit a text question, rather than a set string that we pass it in, as </st><st c="27172">shown here.</st></p>
			<p><st c="27183">This also includes displaying the resulting response from the LLM in a more user-friendly interface, such as in a nicely designed screen. </st><st c="27322">In </st><a href="B22475_06.xhtml#_idTextAnchor114"><em class="italic"><st c="27325">Chapter 6</st></em></a><st c="27334">, we will show this in code, but for now, let’s have a higher-level talk about adding an interface to your </st><st c="27441">RAG application.</st></p>
			<p><st c="27457">When an application is loaded for a user, they will have some way to interact with it. </st><st c="27545">This is typically facilitated through an interface that can range from simple text input fields on a web page to more complex voice recognition systems. </st><st c="27698">The key is to accurately capture the intent of the user’s query in a format that can be processed by the system. </st><st c="27811">One obvious advantage of adding a UI is that it allows your users to test the results of other queries. </st><st c="27915">A </st><a id="_idIndexMarker207"/><st c="27917">user could enter any query they want and see what the </st><st c="27971">result is.</st></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/><st c="27981">Pre-processing</st></h2>
			<p><st c="27996">As we discussed, even though </st><a id="_idIndexMarker208"/><st c="28026">the user just enters a question such as </st><code><st c="28066">What is Task Decomposition?</st></code><st c="28093"> in the UI, after that question is submitted, there is pre-processing that often occurs to make that query more LLM-friendly. </st><st c="28219">This is primarily done in the prompt, which also gets help from many of the other functions. </st><st c="28312">But all of this happens behind the scenes and not in the view of the user. </st><st c="28387">All they will see in this scenario is the final output displayed in a </st><st c="28457">user-friendly way.</st></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/><st c="28475">Post-processing</st></h2>
			<p><st c="28491">Even after the LLM</st><a id="_idIndexMarker209"/><st c="28510"> has returned the response, this response is often post-processed before it is shown to </st><st c="28598">the user.</st></p>
			<p><st c="28607">Here’s what an actual LLM output </st><st c="28641">looks like:</st></p>
			<pre class="source-code"><st c="28652">
AIMessage(content="The advantages of using RAG include improved accuracy and relevance of responses generated by large language models, customization and flexibility in responses tailored to specific needs, and expanding the model's knowledge beyond the initial training data.")</st></pre>
			<p><st c="28931">As a last step in the chain, we pass that through </st><code><st c="28982">StrOutput Parser()</st></code><st c="29000"> to parse out just </st><st c="29019">the string:</st></p>
			<pre class="source-code"><st c="29030">
'The advantages of using RAG (Retrieval Augmented Generation) include improved accuracy and relevance, customization, flexibility, and expanding the model's knowledge beyond the training data. </st><st c="29224">This means that RAG can significantly enhance the accuracy and relevance of responses generated by large language models, tailor responses to specific needs, and access and utilize information not included in initial training sets, making the models more versatile and adaptable.'</st></pre>
			<p><st c="29504">That is certainly better than the previous step’s output, but this is still displaying in your notebook. </st><st c="29610">In a more professional application, you will want to display this on a screen in a way that is friendly for the user. </st><st c="29728">You may want to display other information, such as the source document we showed in the </st><a href="B22475_03.xhtml#_idTextAnchor056"><em class="italic"><st c="29816">Chapter 3</st></em></a><st c="29825"> code. </st><st c="29832">This will depend on the intentions of your</st><a id="_idIndexMarker210"/><st c="29874"> application and will vary significantly across </st><st c="29922">RAG systems.</st></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/><st c="29934">Output interface</st></h2>
			<p><st c="29951">For a full UI, this string</st><a id="_idIndexMarker211"/><st c="29978"> will be passed to the interface that displays the message that’s returned to the chain. </st><st c="30067">This interface can be very simple, like what you can see with ChatGPT in </st><em class="italic"><st c="30140">Figure 4</st></em><em class="italic"><st c="30148">.7</st></em><st c="30150">:</st></p>
			<div><div><img src="img/B22475_04_07.jpg" alt="Figure 4.7 – The ChatGPT 4 interface"/><st c="30152"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="30654">Figure 4.7 – The ChatGPT 4 interface</st></p>
			<p><st c="30690">You could also build something more robust that is more suitable for your particular target user group. </st><st c="30795">If it is meant to be more conversational, the interface should also be designed to facilitate further interaction. </st><st c="30910">You could give users options to refine their queries, ask follow-up questions, or request </st><st c="31000">additional information.</st></p>
			<p><st c="31023">Another common feature in the UI is the collection of feedback on the usefulness and accuracy of the response. </st><st c="31135">This can be used to continuously improve the system’s performance. </st><st c="31202">By analyzing user interactions and feedback, the system can learn to better understand user intent, refine the vector search process, and enhance the relevance and quality of </st><a id="_idIndexMarker212"/><st c="31377">the generated responses. </st><st c="31402">This leads us to our last key </st><st c="31432">component: evaluation.</st></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/><st c="31454">Evaluation</st></h1>
			<p><st c="31465">The evaluation component is </st><a id="_idIndexMarker213"/><st c="31494">essential for assessing and </st><a id="_idIndexMarker214"/><st c="31522">improving the RAG system’s performance. </st><st c="31562">While there are many common practices for evaluation, the most effective evaluation system will focus on what is most important for your users and provide an evaluation for improving those features and capabilities. </st><st c="31778">Often, this involves analyzing the system’s outputs using various metrics, such as accuracy, relevance, response time, and user satisfaction. </st><st c="31920">This feedback is used to identify areas of improvement, and guide adjustments in the system’s design, data handling, and LLM integration. </st><st c="32058">Continuous evaluation is crucial for maintaining high-quality responses and ensuring that the system meets users’ </st><st c="32172">needs effectively.</st></p>
			<p><st c="32190">As mentioned previously, you can also collect user feedback in various ways, including qualitative data (entry forms with open-ended questions) or quantitative (true/false, ratings, or other numerical representations) on the usefulness and accuracy of the response. </st><st c="32457">A thumbs up/down is often used to get a quick feedback response from the user and gauge the general effectiveness of the application among </st><st c="32596">many users.</st></p>
			<p><st c="32607">We will go more in-depth about how to incorporate evaluation into your code in </st><a href="B22475_10.xhtml#_idTextAnchor218"><em class="italic"><st c="32687">Chapter 10</st></em></a><st c="32697">.</st></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/><st c="32698">Summary</st></h1>
			<p><st c="32706">This chapter hasn’t provided an exhaustive list of components for a RAG system. </st><st c="32787">However, these are components that tend to be in every successful RAG system. </st><st c="32865">Keep in mind that RAG systems are constantly evolving and new types of components are appearing every day. </st><st c="32972">The key aspect of your RAG system should be to add the components that will deliver what your users need. </st><st c="33078">This can be very specific to your project but is often an intuitive outgrowth of what your </st><st c="33169">company does.</st></p>
			<p><st c="33182">This chapter provided a comprehensive overview of the essential components that make up a successful RAG system. </st><st c="33296">It delved into the three main stages: </st><em class="italic"><st c="33334">Indexing</st></em><st c="33342">, </st><em class="italic"><st c="33344">Retrieval</st></em><st c="33353">, and </st><em class="italic"><st c="33359">Generation</st></em><st c="33369">, and explained how these stages work together to deliver enhanced responses to </st><st c="33449">user queries.</st></p>
			<p><st c="33462">In addition to the core stages, this chapter highlighted the importance of the UI and evaluation components. </st><st c="33572">The UI serves as the primary point of interaction between the user and the RAG system, allowing users to input their queries and view the generated responses. </st><st c="33731">Evaluation is crucial for assessing and improving the RAG system’s performance. </st><st c="33811">This involves analyzing the system’s outputs using various metrics and collecting user feedback. </st><st c="33908">Continuous evaluation helps identify areas for improvement and guides adjustments in the system’s design, data handling, and </st><st c="34033">LLM integration.</st></p>
			<p><st c="34049">While the components that were discussed in this chapter are not exhaustive, they form the foundation of most successful </st><st c="34171">RAG systems.</st></p>
			<p><st c="34183">However, there is a very important aspect of every RAG system that we didn’t cover in this chapter: security. </st><st c="34294">We will dedicate the entire next chapter to covering key aspects of security, particularly as it relates </st><st c="34399">to RAG.</st></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor094"/><st c="34406">References</st></h1>
			<p><st c="34417">LangChain’s prompt hub </st><st c="34441">information: </st><a href="https://docs.smith.langchain.com/old/category/prompt-hub"><st c="34454">https://docs.smith.langchain.com/old/category/prompt-hub</st></a><st c="34510">.</st></p>
		</div>
	<div></body></html>