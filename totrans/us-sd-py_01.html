<html><head></head><body>
		<div><h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Introducing Stable Diffusion</h1>
			<p>Stable Diffusion is a deep learning <a id="_idIndexMarker000"/>model that utilizes diffusion processes to generate high-quality artwork from guided instructions and images.</p>
			<p>In this chapter, we will introduce you to AI image generation technology, namely Stable Diffusion, and see how it evolved into what it is now.</p>
			<p>Unlike other deep learning image generation models, such as OpenAI’s DALL-E 2, Stable Diffusion works by starting with a random-noise latent tensor and then gradually adding detailed information to it. The amount of detail that is added is determined by a diffusion process, governed by a mathematical equation (we will delve into the details in <a href="B21263_05.xhtml#_idTextAnchor097"><em class="italic">Chapter 5</em></a>). In the final stage, the model decodes the latent tensor into the pixel image.</p>
			<p>Since its creation in 2022, Stable Diffusion has been used widely to generate impressive images. For example, it can generate images of people, animals, objects, and scenes that are indistinguishable from<a id="_idIndexMarker001"/> real photographs. Images are generated using specific instructions, such as <em class="italic">A cat running on the moon’s surface</em> or <em class="italic">a photograph of an astronaut riding </em><em class="italic">a horse.</em></p>
			<p>Here is a sample of a prompt to use with Stable Diffusion to generate an image using the given description:</p>
			<p><code>"a photograph of an astronaut riding </code><code>a horse".</code></p>
			<p>Stable Diffusion will generate an image like the following:</p>
			<div><div><img src="img/B21263_01_01.jpg" alt="Figure 1.1: A photograph of an astronaut riding a horse, generated by Stable Diffusion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1: A photograph of an astronaut riding a horse, generated by Stable Diffusion</p>
			<p>This image didn’t exist before I hit the <em class="italic">Enter</em> button. It was created collaboratively by me and Stable Diffusion. Stable Diffusion not only understands the descriptions we give it, but also adds more detail to the image.</p>
			<p>Apart from text-to-image<a id="_idIndexMarker002"/> generation, Stable Diffusion also facilitates editing photos using natural language. To illustrate, consider the preceding image again. We can replace the space background with a blue sky and mountains using an automatically generated mask and prompts.</p>
			<p>The <code>background</code> prompt can be used to generate the background mask, and the <code>blue sky and mountains</code> prompt is used to guide Stable Diffusion to transform the initial image into the following:</p>
			<div><div><img src="img/B21263_01_02.jpg" alt="Figure 1.2: Replace the background with a blue sky and mountains"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2: Replace the background with a blue sky and mountains</p>
			<p>No mouse-clicking or dragging is required, and there's no need for additional paid software such as Photoshop. You can achieve this using pure Python together with Stable Diffusion. Stable Diffusion can perform many other tasks using only Python code, which will be covered<a id="_idIndexMarker003"/> later in this book.</p>
			<p>Stable Diffusion is a powerful tool that has the potential to revolutionize the way we create and interact with images. It can be used to create realistic images for movies, video games, and other applications. It can also be used to generate personalized images for marketing, advertising, and decoration.</p>
			<p>Here are some of the key features of Stable Diffusion:</p>
			<ul>
				<li>It can generate<a id="_idIndexMarker004"/> high-quality images from text descriptions</li>
				<li>It is based on a diffusion process, which is a more stable and reliable way to generate images than other methods</li>
				<li>Many massive pre-trained publicly accessible models are available (10,000+), and keep on growing</li>
				<li>New research and applications are building on Stable Diffusion</li>
				<li>It is open source and can be <a id="_idIndexMarker005"/>used by anyone</li>
			</ul>
			<p>Before we proceed, let me provide a brief introduction to the evolution of the Diffusion model in recent yea<a id="_idTextAnchor017"/>rs.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/>Evolution of the Diffusion model</h1>
			<p>Diffusion hasn’t always been available, just as Rome was not built in a day. To have a high-level bird’s view of this technology, in this section, we will discuss the overall evolution of the Diffusion<a id="_idIndexMarker006"/> model in recent ye<a id="_idTextAnchor019"/>ars.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor020"/>Before Transformer and Attention</h2>
			<p>Not too long ago, <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>) and <strong class="bold">Residual Neural Networks</strong> (<strong class="bold">ResNets</strong>) dominated the field of computer vision in machine learning.</p>
			<p>CNNs and ResNets have proven to be highly <a id="_idIndexMarker007"/>effective in tasks such as guided object detection and face<a id="_idIndexMarker008"/> recognition. These models have been widely adopted across various industries, including self-driving cars and AI-driven agriculture.</p>
			<p>However, there is a significant <a id="_idIndexMarker009"/>drawback to CNNs and ResNets: they can only <a id="_idIndexMarker010"/>recognize objects that are part of their training set. To detect a completely new object, a new category label must be added to the training dataset, followed by retraining or fine-tuning the pre-trained models.</p>
			<p>This limitation stems from the models themselves, as well as the constraints imposed by hardware and the availability of training data at th<a id="_idTextAnchor021"/>at time.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor022"/>Transformer transforms machine learning</h2>
			<p>The Transformer <a id="_idIndexMarker011"/>model, developed by Google, has revolutionized the field of computer vision, starting with its <a id="_idIndexMarker012"/>impact on <strong class="bold">Natural Language </strong><strong class="bold">Processing </strong>(<strong class="bold">NLP</strong>).</p>
			<p>Unlike traditional approaches that rely on predefined labels to calculate loss and update neural network weights through backpropagation, the Transformer model, along with the Attention mechanism, introduced a pioneering concept. They utilize the training data itself for both training and labeling<a id="_idIndexMarker013"/> purposes.</p>
			<p>Let’s consider the following sentence as an example:</p>
			<p><em class="italic">“Stable Diffusion can generate images </em><em class="italic">using text”</em></p>
			<p>Let’s say we input the sequence of words into the neural network, excluding the last word <em class="italic">text</em>:</p>
			<p><em class="italic">“Stable Diffusion can generate </em><em class="italic">images using”</em></p>
			<p>Using this prompt, the model can predict the next word based on its current weights. Let’s say it predicts <em class="italic">apple</em>. The encoded embedding of the word <em class="italic">apple</em> is significantly different from <em class="italic">text</em> in terms of vector space, much like two numbers with a large gap between them. This gap value can be used as the loss value, which is then backpropagated to update the weights.</p>
			<p>By repeating this process millions or even billions of times during training and updating, the model’s weights gradually learn to produce the next reasonable words in a sentence.</p>
			<p>Machine learning models can now learn a wide range of tasks with a properly designed los<a id="_idTextAnchor023"/>s function.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor024"/>CLIP from OpenAI makes a big difference</h2>
			<p>Researchers and engineers quickly<a id="_idIndexMarker014"/> recognized the potential of the Transformer model, as mentioned in the concluding remarks of the well-known machine learning paper titled <em class="italic">Attention Is All You Need</em> [2]. The author states the following:</p>
			<p><em class="italic">We are excited about the future of Attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted Attention mechanisms to efficiently handle large inputs and outputs such as images, audio, </em><em class="italic">and video</em>.</p>
			<p>If you have read the paper and grasped the remarkable capabilities of Transformer- and Attention-based models, you might also be inspired to reimagine your own work and harness this extraordinary power.</p>
			<p>Researchers from OpenAI grasped this power and created a model called CLIP [1] that uses the Attention mechanism and Transformer model architecture to train an image classification model. The model has the ability to classify a wide range of images with no need for labeled data. It is the first large-scale image classification model trained on 400 million image-text pairs extracted from the internet.</p>
			<p>Although there were similar <a id="_idIndexMarker015"/>efforts prior to OpenAI’s CLIP model, the results were not deemed satisfactory according to the authors of the CLIP paper [1]:</p>
			<p><em class="italic">A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language </em><em class="italic">is scale.</em></p>
			<p>Indeed, scale plays a pivotal role in unlocking the remarkable superpower of universal image recognition. While other models utilized 200,000 images, the CLIP team trained their model using a staggering 400,000,000 images combined with text data from the public internet.</p>
			<p>The results are astonishing. CLIP enables image recognition and segmentation without the limitations of predefined labels. It can detect objects that previous models struggled with. CLIP has brought about a significant change through its large-scale model. Given the immense weight of CLIP, researchers have pondered whether <a id="_idIndexMarker016"/>it could also be employed for image generat<a id="_idTextAnchor025"/>ion from text.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor026"/>Generate images</h2>
			<p>Using only CLIP, we still cannot generate a realistic image based on a text description. For instance, if we ask CLIP to draw an apple, the model merges various types of apples, different shapes, colors, backgrounds, and so on. CLIP might generate an apple that is half green and half red, which might not be what we intended.</p>
			<p>You may be familiar with <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>), which are capable of generating highly photorealistic<a id="_idIndexMarker017"/> images. However, text prompts cannot be utilized in the generation process. GANs have become a sophisticated solution for image processing tasks such as face restoration and image upscaling. Nevertheless, a new innovative approach was needed to leverage models for image generation based on guided descriptions or prompts.</p>
			<p>In June 2020, a paper titled <em class="italic">Denoising Diffusion Probabilistic Models </em>[3] by Jonathan Ho et al. introduced a diffusion-based probabilistic model for image generation. The term <strong class="bold">diffusion</strong> is borrowed from thermodynamics. The original meaning of diffusion is the movement of particles from a region of high concentration to a region of low concentration. This idea of diffusion inspired machine learning researchers to apply it to denoising and sampling processes. In other words, we can start with a noisy image and gradually refine it by removing noise. The denoising process gradually transforms an image with high levels of noise into a clearer version of the original image. Therefore, this generative model is referred to<a id="_idIndexMarker018"/> as a <strong class="bold">denoising diffusion </strong><strong class="bold">probabilistic model</strong>.</p>
			<p>The idea behind this approach is ingenious. For any given image, a limited number of normally distributed noise images are added to the original image, effectively transforming it into a fully noisy image. What if we train a model that can reverse this diffusion process, guided by the CLIP model? Surprisingly, this app<a id="_idTextAnchor027"/>roach works [4].</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor028"/>DALL-E 2 and Stable Diffusion</h2>
			<p>In April 2022, OpenAI released DALL-E 2, accompanied by its paper titled <em class="italic">Hierarchical Text-Conditional Image Generation with CLIP Latents</em> [4]. DALL-E 2 garnered significant attention worldwide. It generated a massive collection of astonishing images that spread across social networks <a id="_idIndexMarker019"/>and mainstream media. People were not only amazed by the quality of the generated images but also by its ability to create images that<a id="_idIndexMarker020"/> had never existed before. DALL-E 2 was effectively producing works of art.</p>
			<p>Perhaps coincidentally, in April 2022, a paper titled <em class="italic">High-Resolution Image Synthesis with Latent Diffusion Models</em> [5] was published by CompVis, introducing another diffusion-based model for text-guided image generation. Building upon CompVis’s work, researchers and engineers from CompVis, Stability AI, and LAION collaborated to release an open source counterpart of DALL-E 2 called Stable Diffusi<a id="_idTextAnchor029"/>on in August 2022.</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor030"/>Why Stable Diffusion</h1>
			<p>While DALL-E 2 and other <a id="_idIndexMarker021"/>commercial image generation models such as Midjourney can produce remarkable images without requiring complex environment setups or hardware preparation, these models are closed-source. Consequently, users have limited control over the generation process, cannot use their own customized models, and are unable to add custom functions to the platform.</p>
			<p>On the other hand, Stable Diffusion is an open source model released under the CreativeML Open RAIL-M license. Users not only have the freedom to utilize the model but can also read the source code, add features, and benefit from the countless custom models share<a id="_idTextAnchor031"/>d by the<a id="_idIndexMarker022"/> community.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor032"/>Which Stable Diffusion to use</h1>
			<p>When we say Stable Diffusion, which Stable Diffusion are we really referring to? Here’s a list of the different Stable Diffusion tools and the differences between them:</p>
			<ul>
				<li><strong class="bold">Stable Diffusion GitHub repo</strong> (<a href="https://github.com/CompVis/stable-diffusion">https://github.com/CompVis/stable-diffusion</a>): This is the original implementation<a id="_idIndexMarker023"/> of Stable Diffusion from CompVis, contributed to by many great engineers <a id="_idIndexMarker024"/>and researchers. It is a PyTorch implementation that can be used to train and generate images, text, and other creative content. The library is now less active at the time of writing in 2023. Its README page also recommends users use Diffusers from Hugging Face to use and train Diffusion models.</li>
				<li><strong class="bold">Diffusers from Hugging Face</strong>: Diffusers is a library for training and using diffusion models developed by Hugging Face. It is the go-to library for state-of-the-art, pre-trained diffusion models<a id="_idIndexMarker025"/> for generating images, audio, and even the 3D structures of molecules. The library is well <a id="_idIndexMarker026"/>maintained and being actively developed at the time of writing. New code is added to its GitHub repository almost every day.</li>
				<li><strong class="bold">Stable Diffusion WebUI from AUTOMATIC1111</strong>: This might be the most popular web-based application currently <a id="_idIndexMarker027"/>that allows users to generate images and text using Stable Diffusion. It <a id="_idIndexMarker028"/>provides a GUI interface that makes it easy to experiment with different settings and parameters.</li>
				<li><strong class="bold">InvokeAI</strong>: InvokeAI was<a id="_idIndexMarker029"/> originally developed as a fork of the Stable Diffusion <a id="_idIndexMarker030"/>project, but it has since evolved into its own unique platform. InvokeAI offers a number of features that make it a powerful tool for creatives.</li>
				<li><strong class="bold">ComfyUI</strong>: ComfyUI is a node-based UI<a id="_idIndexMarker031"/> that utilizes Stable Diffusion. It allows users to construct tailored workflows, including image post-processing<a id="_idIndexMarker032"/> and conversions. It is a potent and adaptable graphical user interface for Stable Diffusion, characterized by its node-based<a id="_idIndexMarker033"/> design.</li>
			</ul>
			<p>In this book, when I use Stable Diffusion, I am referring to the Stable Diffusion model, not the GUI tools just listed. The focus of this book will be on using Stable Diffusion with plain Python. Our example code will use Diffusers’ pipelines and will leverage the code from Stable Diffusion WebUI and open source code from<a id="_idTextAnchor033"/> academic papers, et cetera.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor034"/>Why this book</h1>
			<p>While the Stable Diffusion GUI tool can generate fantastic images driven by the Diffusion model, its usability is limited. The presence of dozens of knobs (more sliders and buttons are being added) and specific terms sometimes makes generating high-quality images a guessing game. On the other hand, the open source Diffusers package from Hugging Face gives users full control over Stable Diffusion using Python. However, it lacks many key features such as loading custom LoRA and textual inversion, utilizing community-shared models/checkpoints, scheduling, and weighted prompts, unlimited prompt tokens, and high-resolution image fixing and upscaling (The Diffusers package does keep improving over time, however).</p>
			<p>This book aims to help you understand all the complex terms and knobs from the internal view of the Diffusion model. The book will also assist you in overcoming the limitations of Diffusers and implementing the missing functions and advanced features to create a fully customized Stable Diffusion application.</p>
			<p>Considering the rapid pace of AI technology evolution, this book also aims to enable you to quickly adapt to the upcoming changes.</p>
			<p>By the end of this book, you will not only be able to use Python to generate and edit images but also leverage the solutions provided in the book to build Stable Diffusion applications for your business and <a id="_idTextAnchor035"/>users.</p>
			<p>Let’s start the journey.</p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor036"/>References</h1>
			<ol>
				<li><em class="italic">Learning Transferable Visual Models From Natural Language </em><em class="italic">Supervision</em>: <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></li>
				<li><em class="italic">Attention Is All You </em><em class="italic">Need</em>: <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
				<li><em class="italic">Denoising Diffusion Probabilistic </em><em class="italic">Models</em>: <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a></li>
				<li><em class="italic">Hierarchical Text-Conditional Image Generation with CLIP </em><em class="italic">Latents</em>: <a href="https://arxiv.org/abs/2204.06125v1">https://arxiv.org/abs/2204.06125v1</a></li>
				<li><em class="italic">High-Resolution Image Synthesis with Latent Diffusion </em><em class="italic">Models</em>: <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></li>
				<li>DALL-E 2: <a href="https://openai.com/dall-e-2">https://openai.com/dall-e-2</a></li>
			</ol>
		</div>
	</body></html>