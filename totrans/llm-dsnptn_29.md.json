["```py\n    import numpy as np\n    from sklearn.metrics import ndcg_score\n    # Sample data\n    queries = [\n        \"What is the capital of France?\",\n        \"Who painted the Mona Lisa?\",\n        \"What is the highest mountain in the world?\"\n    ]\n    ground_truth = [\n        [0, 1, 2],  # Indices of relevant documents for query 1\n        [3, 4],     # Indices of relevant documents for query 2\n        [5, 6, 7]   # Indices of relevant documents for query 3\n    ]\n    retrieved = [\n        [1, 5, 0, 2, 8, 9, 3, 4, 6, 7],  # Ranked list of retrieved document indices for query 1\n        [4, 3, 0, 1, 2, 5, 6, 7, 8, 9],  # Ranked list of retrieved document indices for query 2\n        [6, 5, 7, 0, 1, 2, 3, 4, 8, 9]   # Ranked list of retrieved document indices for query 3\n    ]\n    ```", "```py\n    def calculate_recall_at_k(ground_truth, retrieved, k):\n        \"\"\"Calculates Recall@k for a set of queries.\"\"\"\n        recall_scores = []\n        for gt, ret in zip(ground_truth, retrieved):\n            num_relevant = len(gt)\n            retrieved_k = ret[:k]\n            num_relevant_retrieved = len(\n                set(gt).intersection(set(retrieved_k))\n            )\n            recall = (\n                num_relevant_retrieved / num_relevant\n                if num_relevant > 0 else 0\n            )\n            recall_scores.append(recall)\n        return np.mean(recall_scores)\n    ```", "```py\n    def calculate_precision_at_k(ground_truth, retrieved, k):\n        \"\"\"Calculates Precision@k for a set of queries.\"\"\"\n        precision_scores = []\n        for gt, ret in zip(ground_truth, retrieved):\n            retrieved_k = ret[:k]\n            num_relevant_retrieved = len(\n                set(gt).intersection(set(retrieved_k))\n            )\n            precision = num_relevant_retrieved / k if k > 0 else 0\n            precision_scores.append(precision)\n        return np.mean(precision_scores)\n    ```", "```py\n    def calculate_mrr(ground_truth, retrieved):\n        \"\"\"Calculates Mean Reciprocal Rank (MRR) for a set of queries.\"\"\"\n        mrr_scores = []\n        for gt, ret in zip(ground_truth, retrieved):\n            for i, doc_id in enumerate(ret):\n                if doc_id in gt:\n                    mrr_scores.append(1 / (i + 1))\n                    break\n            else:\n                mrr_scores.append(0)  # No relevant document found\n        return np.mean(mrr_scores)\n    ```", "```py\n    def calculate_ndcg_at_k(ground_truth, retrieved, k):\n        \"\"\"Calculates NDCG@k for a set of queries.\"\"\"\n        ndcg_scores = []\n        for gt, ret in zip(ground_truth, retrieved):\n            relevance_scores = np.zeros(len(ret))\n            for i, doc_id in enumerate(ret):\n                if doc_id in gt:\n                    relevance_scores[i] = 1\n            # sklearn.metrics.ndcg_score requires 2D array\n            true_relevance = np.array([relevance_scores])\n            retrieved_relevance = np.array([relevance_scores])\n            ndcg = ndcg_score(\n                true_relevance, retrieved_relevance, k=k\n            )\n            ndcg_scores.append(ndcg)\n        return np.mean(ndcg_scores)\n    ```", "```py\n    k_values = [1, 3, 5, 10]\n    for k in k_values:\n        recall_at_k = calculate_recall_at_k(ground_truth,\n            retrieved, k)\n        precision_at_k = calculate_precision_at_k(\n            ground_truth, retrieved, k\n        )\n        ndcg_at_k = calculate_ndcg_at_k(ground_truth, retrieved, k)\n        print(f\"Recall@{k}: {recall_at_k:.3f}\")\n        print(f\"Precision@{k}: {precision_at_k:.3f}\")\n        print(f\"NDCG@{k}: {ndcg_at_k:.3f}\")\n    mrr = calculate_mrr(ground_truth, retrieved)\n    print(f\"MRR: {mrr:.3f}\")\n    ```", "```py\n         pip install rouge-score\n          from rouge_score import rouge_scorer\n    ```", "```py\n    query = \"What is the capital of France?\"\n    answer = \"The capital of France is Paris.\"\n    retrieved_documents = [\n        \"Paris is the capital city of France.\",\n        \"France is a country in Europe.\",\n        \"The Eiffel Tower is a famous landmark in Paris.\",\n        \"London is the capital of the United Kingdom.\"\n    ]\n    ```", "```py\n         def calculate_rouge_scores(answer, documents):\n        \"\"\"Calculates ROUGE scores between the answer and each document.\"\"\"\n        scorer = rouge_scorer.RougeScorer(\n            ['rouge1', 'rouge2', 'rougeL'],\n            use_stemmer=True\n        )\n        scores = []\n        for doc in documents:\n            score = scorer.score(answer, doc)\n            scores.append(score)\n        return scores\n    ```", "```py\n    rouge_scores = calculate_rouge_scores(answer,\n        retrieved_documents)\n    for i, score in enumerate(rouge_scores):\n        print(f\"Document {i+1}:\")\n        print(f\"  ROUGE-1: {score['rouge1'].fmeasure:.3f}\")\n        print(f\"  ROUGE-2: {score['rouge2'].fmeasure:.3f}\")\n        print(f\"  ROUGE-L: {score['rougeL'].fmeasure:.3f}\")\n    ```", "```py\n    avg_rouge1 = sum([score['rouge1'].fmeasure\n        for score in rouge_scores]) / len(rouge_scores)\n    avg_rouge2 = sum([score['rouge2'].fmeasure\n        for score in rouge_scores]) / len(rouge_scores)\n    avg_rougeL = sum([score['rougeL'].fmeasure\n        for score in rouge_scores]) / len(rouge_scores)\n    print(f\"\\nAverage ROUGE Scores:\")\n    print(f\"  Average ROUGE-1: {avg_rouge1:.3f}\")\n    print(f\"  Average ROUGE-2: {avg_rouge2:.3f}\")\n    print(f\"  Average ROUGE-L: {avg_rougeL:.3f}\")\n    ```", "```py\n    pip install transformers torch\n    from transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification\n    )\n    import torch\n    ```", "```py\n     query = \"What is the capital of France?\"\n    answer = \"The capital of France is Paris. It is a global center for art, fashion, gastronomy, and culture.\"\n    context = \"\"\"\n    Paris is the capital city of France. It is situated on the River Seine, in northern France.\n    Paris has an area of 105 square kilometers and a population of over 2 million people.\n    France is a country located in Western Europe.\n    \"\"\"\n    ```", "```py\n    model_name = \"roberta-large-mnli\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = \\\n        AutoModelForSequenceClassification.from_\n    pretrained(\n        model_name\n    )\n    ```", "```py\n    def calculate_claim_groundedness(context, claim):\n        \"\"\"Calculates the entailment score for a single claim given the context.\"\"\"\n        inputs = tokenizer(context, claim, truncation=True,\n            return_tensors=\"pt\")\n        outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=1)\n        entailment_prob = probs[0][2].item()  # Assuming label 2 corresponds to entailment\n        return entailment_prob\n    ```", "```py\n    def calculate_groundedness(context, answer):\n        \"\"\"Calculates the overall groundedness score for the generated answer.\"\"\"\n        claims = answer.split(\". \")  # Simple sentence splitting\n        if not claims:\n            return 0\n        claim_scores = []\n        for claim in claims:\n            if claim:\n              score = calculate_claim_groundedness(context, claim)\n              claim_scores.append(score)\n        return (\n            sum(claim_scores) / len(claim_scores)\n            if claim_scores\n            else 0\n        )\n    ```", "```py\n    groundedness_score = calculate_groundedness(context, answer)\n    print(f\"Groundedness Score: {groundedness_score:.3f}\")\n    ```", "```py\n    pip install kilt==0.5.5\n    from kilt import kilt_utils as utils\n    from kilt import retrieval\n    from kilt.eval import answer_evaluation, provenance_evaluation\n    ```", "```py\n    # Download the WoW dataset\n    utils.download_dataset(\"wow\")\n    ```", "```py\n    # Load the dataset\n    wow_data = utils.load_dataset(\"wow\", split=\"test\")\n    ```", "```py\n    class DummyRetriever(retrieval.base.Retriever):\n        def __init__(self, k=1):\n              super().__init__(num_return_docs=k)\n              self.k = k\n        # retrieve some Wikipedia pages (or the entire dataset)\n        # based on the query\n        def retrieve(self, query, start_paragraph_id=None):\n            # Dummy retrieval: return the same set of pages for each query\n            dummy_pages = [\n                {\n                    \"wikipedia_id\": \"534366\",\n                    \"start_paragraph_id\": 1,\n                    \"score\": self.k,\n                    \"text\": \"Paris is the capital of France.\"\n                },\n                {\n                    \"wikipedia_id\": \"21854\",\n                    \"start_paragraph_id\": 1,\n                    \"score\": self.k-1,\n                    \"text\": \"The Mona Lisa was painted by Leonardo da Vinci.\"\n                },\n                {\n                    \"wikipedia_id\": \"37267\",\n                    \"start_paragraph_id\": 1,\n                    \"score\": self.k-2,\n                    \"text\": \"Mount Everest is the highest mountain in the world.\"\n                }\n              ]\n              return dummy_pages[:self.k]\n    # Example usage\n    retriever = DummyRetriever(k=2)\n    ```", "```py\n         def dummy_generate(query, retrieved_pages):\n            \"\"\"Simulates RAG generation by returning \n            a fixed answer for each query.\"\"\"\n            if \"capital of France\" in query:\n                return \"Paris\"\n            elif \"Mona Lisa\" in query:\n                return \"Leonardo da Vinci\"\n            elif \"highest mountain\" in query:\n                return \"Mount Everest\"\n            else:\n                return \"I don't know.\"\n    ```", "```py\n    predictions = []\n    for element in wow_data[:10]:\n        query = element[\"input\"]\n        retrieved_pages = retriever.retrieve(query)\n        # Add provenance information to the element\n        element[\"output\"] = [{\"provenance\": retrieved_pages}]\n        generated_answer = dummy_generate(query, retrieved_pages)\n        # Add the generated answer to the element\n        element[\"output\"][0][\"answer\"] = generated_answer\n        predictions.append(element)\n    ```", "```py\n    kilt_scores = {}\n    kilt_scores[\"provenance_MAP@k\"] = \\\n        provenance_evaluation.get_map_at_k(\n        predictions, verbose=False\n    )\n    kilt_scores[\"answer_EM\"] = answer_evaluation.get_exact_match(\n        predictions, verbose=False\n    )\n    kilt_scores[\"answer_F1\"] = answer_evaluation.get_f1(\n        predictions, verbose=False\n    )\n    kilt_scores[\"answer_ROUGE-L\"] = answer_evaluation.get_rouge_l(\n        predictions, verbose=False\n    )\n    print(kilt_scores)\n    ```"]