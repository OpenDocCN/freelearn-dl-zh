# 9

# 探索前沿：由LLM驱动的先进应用和创新

在自然语言处理（NLP）快速发展的领域中，**大型语言模型（LLM**）已经迈出了革命性的一步，重塑了我们与信息互动、自动化流程以及从大量数据池中提取洞察的方式。本章代表了我们在NLP方法和出现与发展过程中的旅程的总结。在这里，之前章节中建立的理论基础与实际、前沿的应用相结合，展示了当使用正确的工具和技术时，LLM的非凡能力。

我们深入探讨LLM应用中最新的和最激动人心的进展，通过详细的Python代码示例进行展示，旨在进行动手学习。这种方法不仅展示了LLM的力量，还使你具备在现实场景中实施这些技术的技能。本章涵盖的主题经过精心挑选，以展示一系列高级功能和应用的广度。

本章的重要性不容小觑。它不仅反映了NLP的最新水平，还作为通往未来的桥梁，在这些技术融入日常解决方案时变得无缝。到本章结束时，你将全面了解如何应用最新的LLM技术和创新，赋予你推动NLP及其超越可能性的边界的能力。加入我们，踏上这段激动人心的旅程，解锁LLM的全部潜力。

让我们回顾本章中涵盖的主要标题：

+   使用RAG和LangChain增强LLM性能——深入高级功能

+   基于链的高级方法

+   从各种网络来源自动检索信息

+   提示压缩和API成本降低

+   多个代理——形成一个LLM协作的团队

# 技术要求

对于本章，以下将是必需的：

+   **编程知识**：熟悉Python编程是必需的，因为开源模型、OpenAI的API和LangChain都是使用Python代码进行说明的。

+   **访问OpenAI的API**：探索封闭源模型需要OpenAI的API密钥。这可以通过在OpenAI创建账户并同意他们的服务条款来获得。

+   **开源模型**：访问本章中提到的特定开源模型将是必要的。这些模型可以通过各自的存储库或通过包管理器如pip或conda进行访问和下载。

+   **本地开发环境**：需要一个安装了Python的本地开发环境。可以使用**集成开发环境（IDE**）如**PyCharm**、**Jupyter Notebook**或简单的文本编辑器。请注意，我们推荐免费的**Google Colab**笔记本，因为它将这些要求封装在一个无缝的网页界面中。

+   **安装库的能力**：您必须拥有安装所需Python库（如**NumPy**、**SciPy**、**TensorFlow**和**PyTorch**）的权限。请注意，我们提供的代码中已包含所需的安装，因此您无需提前安装。我们只是强调您应该有权限这样做，我们预期您会这样做。具体来说，使用免费的Google Colab笔记本就足够了。

+   **硬件要求**：根据您所处理的模型复杂性和大小，您将需要一个具有足够处理能力（可能包括用于ML任务的良好GPU）和充足内存的计算机。这仅当您选择不使用免费的Google Colab时才相关。

现在我们已经使用API和本地设置好了LLM应用，我们终于可以部署LLM的高级应用，让我们能够利用其巨大的力量。

# 利用RAG和LangChain增强LLM性能——深入高级功能

**检索增强生成**（RAG）框架已成为针对特定领域或任务定制**大型语言模型**（LLM）的关键工具，它弥合了提示工程简单性和模型微调复杂性之间的差距。

提示工程是定制LLM的最初、最易访问的技术。它利用模型根据输入提示解释和响应用户查询的能力。例如，要查询Nvidia在其最新公告中是否超过了收益预期，可以在提示中直接提供收益电话会议内容，以弥补LLM缺乏即时、最新上下文的问题。这种方法虽然简单，但取决于模型在单个或一系列精心设计的提示中消化和分析提供的信息的能力。

当调查范围超出提示工程所能容纳的范围时——例如分析十年来的科技行业收益电话会议——RAG变得不可或缺。在采用RAG之前，替代方案是微调，这是一个资源密集型的过程，需要显著调整LLM的架构以纳入大量数据集。RAG通过预处理并将大量数据存储在向量数据库中简化了这一点。它智能地隔离和检索与查询相关的数据段，有效地将大量信息压缩成LLM可管理的、提示大小的上下文。这一创新大幅减少了此类广泛数据熟悉任务所需的时间、资源和专业知识。

在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中，我们介绍了RAG（检索增强生成）的一般概念，特别是LangChain，这是一个以其先进功能而区别于其他RAG框架的框架。

现在，我们将讨论LangChain为增强LLM应用提供的额外独特功能，为您提供其实施和复杂NLP任务中的实用见解。

## 使用Python的LangChain管道 – 通过LLM增强性能

在本节中，我们将从我们上一节在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中的最后一个例子继续进行。在这个场景中，我们处于医疗行业，在我们的医院中，我们的护理提供者表示需要能够根据对患者的粗略描述或其状况来快速检索患者的记录。例如，“去年我看到的那位怀有双胞胎的患者是谁？”“我是否曾经有过一位双亲都有癌症病史的患者，他们对临床试验感兴趣？”等等。

重要提示

我们强调，这些不是真实的医疗笔记，并且笔记中描述的人也不是真实的。

在我们例子中的[*第8章*](B18949_08.xhtml#_idTextAnchor440)，我们通过简单地利用临床笔记的嵌入向量数据库，将管道保持在了最小复杂度，然后我们应用了相似度搜索来根据简单的请求查找笔记。我们注意到其中一个问题，即第二个问题，在使用相似度搜索算法时得到了错误的答案。

现在，我们将增强这个管道。我们不会满足于相似度搜索的结果，并将这些结果展示给医生；我们将那些被认为与请求内容相似的结果，并使用LLM来检查这些结果，核实它们，并告诉我们哪些确实与医生相关。

### 支付的LLM与免费的

我们将使用这个管道来展示付费或免费类型的LLM的实用性。通过`paid_vs_free`变量，你可以选择使用OpenAI的付费GPT模型或一个免费的LLM。使用OpenAI的付费模型将利用他们的API，并需要一个API密钥。然而，免费的LLM被导入到运行Python代码的本地环境中，因此任何有互联网连接和足够计算资源的人都可以使用。

让我们开始动手实验代码。

### 应用高级LangChain配置和管道

参考以下笔记本：`Ch9_Advanced_LangChain_Configurations_and_Pipeline.ipynb`。

注意，笔记本的前一部分与[*第8章*](B18949_08.xhtml#_idTextAnchor440)中的笔记本相同，因此我们将跳过该部分的描述。

#### 安装所需的Python库

在这里，我们需要扩展已安装的库，并安装`openai`和`gpt4all`。此外，为了利用`gpt4all`，我们需要从网络上下载一个`.bin`文件。

这两个步骤通过笔记本执行都很简单。

#### 设置LLM – 在付费LLM（OpenAI的GPT）和免费LLM（来自Hugging Face）之间选择

如上所述，我们让你选择是否想要通过OpenAI的付费API或免费的LLM来运行这个例子。

记住，由于OpenAI的服务包括托管LLM和处理提示，它只需要最少的资源和时间以及基本的互联网连接。这也涉及到将我们的提示发送到OpenAI的API服务。提示通常包含在现实世界设置中可能是专有信息。因此，需要做出关于数据安全的行政决策。在过去的十年中，公司从本地计算向云迁移时，类似的考虑是核心的。

与该要求相反，使用免费的LLM，你将本地托管它，你将避免将任何信息导出至你的计算环境之外，但你需要承担处理工作。

另一个需要考虑的方面是每个LLM的使用条款，因为每个LLM可能有不同的许可条款。虽然LLM可能允许你免费实验，但它可能对你在商业产品中使用它施加限制。

在运行时间和计算资源受限的背景下，选择付费LLM进行此示例将产生更快的响应。

为了满足你想要免费实验LLM的愿望，并且因为我们渴望让你在Google Colab上快速免费运行代码，我们必须将LLM的选择限制在那些可以在Google免费账户所提供的有限RAM上运行的LLM。为了做到这一点，我们选择了一个精度降低的LLM，也称为量化LLM。

根据你选择基于API的LLM还是免费的本地LLM，LLM将被分配给`llm变量`。

#### 创建一个QA链

在这里，我们设置了一个RAG框架。它被设计用来接受各种文本文档并将它们设置为检索。

#### 当使用LLM作为“大脑”而不是嵌入相似性时，基于相同要求进行搜索。

我们现在将运行与[*第8章*](B18949_08.xhtml#_idTextAnchor440)中的示例完全相同的请求。这些请求将在相同的笔记和相同的向量数据库上执行，该数据库包含相同的嵌入。这一切都没有改变或增强。区别在于我们将让LLM监督答案的处理。

在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中，我们看到了第二个问题得到了错误的答案。问题是：“是否有任何原定于9月出生的孕妇？”

我们在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中看到的答案是关于一位原定于8月出生的病人。错误是由于相似性算法的不足。确实，那位病人的记录内容与问题相似，但分娩在不同月份的细微差别应该是使那些记录无关紧要的因素。

在这里，在我们的当前管道中，应用了OpenAI的LLM，它正确地告诉我们没有病人原定于9月出生。

注意，当选择免费 LLM 时，它可能会出错。这体现了该模型的次优方面，因为它被量化以节省 RAM 要求。

为了总结这个例子，我们建立了一个内部搜索机制，允许用户（在我们的例子中，是一位医生）根据某些标准搜索他们的患者记录以找到患者。这个系统设计的独特之处在于让 LLM 从外部数据源检索相关答案，而不仅限于它所训练的数据。这种范式是 RAG 的基础。

在下一节中，我们将展示 LLM 的更多用途。

# 带链的高级方法

在本节中，我们将继续探索一个人可以利用 LLM 管道的方式。我们将重点关注链。

请参考以下笔记本：`Ch9_Advanced_Methods_with_Chains.ipynb`。这个笔记本展示了一个链管道的演变，因为每一次迭代都展示了 LangChain 允许我们使用的另一个功能。

为了使用最少的计算资源、内存和时间，我们使用了 OpenAI 的 API。您可以选择使用免费的 LLM，并且可以以与我们在本章前一个示例中设置笔记本类似的方式这样做。

笔记本总是从基本配置开始，因此我们可以跳过审查笔记本的内容。

## 向 LLM 提出一个一般知识问题

在这个例子中，我们希望使用 LLM 告诉我们一个简单问题的答案，这个问题需要训练有素的 LLM 应该拥有的常识：

[PRE0]

然后，我们定义了一个简单的链，称为 `LLMChain`，并给它提供 `LLM` 变量和提示。

LLM 确实知道答案，并从其知识库中返回：

[PRE1]

## 请求输出结构 – 使 LLM 以特定的数据格式提供输出

这次，我们希望输出具有特定的语法，这可能允许我们以计算方式使用它进行下游任务：

[PRE2]

现在，我们添加了一个用于实现语法的功能。我们定义了 `output_parser 变量`，并使用不同的函数生成输出，`predict_and_parse()`。

输出如下：

[PRE3]

## 发展到流畅的对话 – 插入记忆元素，以便将先前交互作为后续提示的参考和上下文

这个功能为链带来了新的价值水平。到目前为止，提示没有上下文。LLM 独立处理每个提示。例如，如果你想提出一个后续问题，你做不到。管道没有你的先前提示及其响应作为参考。

为了从提出零散的问题转变为拥有持续的、滚动对话般的体验，LangChain 提供了 `ConversationChain()`。在这个函数中，我们有一个 `memory` 参数，它将链的先前交互映射到当前提示。因此，提示模板就是内存“居住”的地方。

而不是使用基本模板进行提示，例如

[PRE4]

现在模板已经适应了记忆功能：

[PRE5]

在这里，你可以将这个字符串视为类似于Python `f"…"`字符串的格式，其中`history`和`input`是字符串变量。`ConversationChain()`函数处理这个提示模板，并插入这两个变量以完成提示字符串。`input`变量由函数本身生成，当我们激活记忆机制时，然后我们运行以下内容：

[PRE6]

输出如下：

[PRE7]

现在，让我们提出一个只有在前一个请求和输出上下文中才能理解的后续请求：

[PRE8]

的确，我们得到了适当的输出：

[PRE9]

为了完成这个例子，让我们假设我们的意图是快速生成一个包含名称和描述的节日表格：

[PRE10]

现在，我们从链中得到了一个格式化的字符串：

[PRE11]

我们可以使用pandas将这个字符串转换为表格：

[PRE12]

在pandas将`dict`处理为DataFrame之后，我们可以在*表9.1*中观察到它：

|  | **名称** | **描述** |
| --- | --- | --- |
| **0** | 圣诞节 | 圣诞节是基督教节日，庆祝耶稣基督的诞生。每年12月25日庆祝。 |
| **1** | 感恩节 | 感恩节是一个人们聚集在一起表达对生活中祝福的感激之情的节日。在美国，感恩节庆祝于11月的第四个星期四。 |
| **2** | 新年 | 新年标志着格里高利日历年的开始。它通过各种传统和庆祝活动在1月1日庆祝。 |
| **3** | 复活节 | 复活节是基督教节日，纪念耶稣基督从死里复活。它是在春分后的第一个满月后的第一个星期日庆祝的。 |
| **4** | 情人节 | 情人节是庆祝爱情和亲情的日子。它传统上与浪漫爱情相关联，但也是表达对朋友和家人的感激之情的时候。 |
| **5** | 圣帕特里克节 | 圣帕特里克节是一个文化和宗教节日，纪念爱尔兰的守护圣人圣帕特里克。它在3月17日通过游行、穿绿色和其他节日活动来庆祝。 |

表9.1 – pandas将表从字典转换为DataFrame，从而适合下游处理

这总结了笔记本展示的各种链式功能。注意我们如何利用了这两个链和LLM带来的功能。例如，虽然记忆和解析功能完全由链的方面处理，但以特定格式（如JSON格式）呈现响应的能力则完全归功于LLM。

在我们的下一个例子中，我们将继续使用LLM和LangChain展示新的实用工具。

# 从各种网络来源自动检索信息

在这个例子中，我们将回顾如何简单地利用LLMs（大型语言模型）访问网络并提取信息。我们可能希望研究某个特定主题，因此我们希望从几个网页、几个介绍该主题的YouTube视频等中整合所有信息。这样的努力可能需要一段时间，因为内容可能非常庞大。例如，有时几个YouTube视频可能需要数小时才能审查。通常，一个人只有在观看视频的很大一部分后，才知道视频有多有用。

另一个用例是当需要实时跟踪各种趋势时。这可能包括跟踪新闻来源、YouTube视频等。在这里，速度至关重要。与之前示例中速度重要是为了节省我们个人时间不同，在这里，速度是使我们的算法对识别实时新兴趋势具有相关性的必要条件。

在本节中，我们将构建一个非常简单且有限的示例。

## 从YouTube视频中检索内容并进行总结

参考以下笔记本：`Ch9_Retrieve_Content_from_a_YouTube_Video_and_Summarize.ipynb` ([https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain))。我们将基于名为EmbedChain的库（[https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)）构建我们的应用程序。EmbedChain利用RAG框架，并通过允许向量数据库包含来自各种网络来源的信息来增强它。

在我们的例子中，我们将选择一个特定的YouTube视频（*Robert Waldinger: What makes a good life? Lessons from the longest study on happiness | TED*：[https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED](https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED)）。我们希望将该视频的内容处理成RAG框架。然后，我们将向LLM提出与该视频内容相关的疑问和任务，这样我们就可以提取我们想要了解的所有关于视频的信息，而无需观看它。

应该强调的是，这个方法依赖的一个关键特性是YouTube为其许多口头视频提供了书面字幕。这使得视频文本上下文的导入变得无缝。然而，如果有人希望将此方法应用于没有字幕的视频，这并不是问题。一个人需要选择一个语音转文字模型，其中许多是免费且质量极高的。视频的音频将被处理，提取出字幕，然后你可以将其导入到RAG（Retrieval-Augmented Generation，检索增强生成）过程中。

### 安装、导入和设置

与之前的笔记本一样，我们安装必要的包，导入所有相关包，并设置我们的OpenAI API密钥。

我们接下来要做以下几步：

1.  选择我们的模型。

1.  选择一个将服务于RAG向量数据库功能的嵌入模型。

1.  选择一个提示LLM。注意你可以设置更多控制模型输出的参数，例如返回的最大令牌数或温度。

1.  选择你想要应用此代码的YouTube视频，并使用视频的URL设置一个字符串变量。

### 设置检索机制

我们需要设置EmbedChain的RAG过程。我们指定我们正在传递一个YouTube视频的路径，并提供视频的URL。

我们可以打印出获取的文本，并验证它确实与我们要分析的视频对齐。

### 审阅、总结和翻译

我们现在将观察此代码产生的价值。

我们要求LLM审阅内容，整理一个总结，并以英语、俄语和德语呈现该总结：

[PRE13]

返回的输出非常准确，因为它完全捕捉了TED演讲的精髓。我们编辑它以移除分隔字符串，得到：

[PRE14]

现在，为了使内容对德语使用者来说简单易懂，我们要求LLM将德语总结形成几个最佳描述视频内容的要点。

它做得很好，输出如下：

[PRE15]

虽然此代码旨在作为一个基本的证明概念，但人们可以看到如何简单地向其中添加更多数据源，自动化它以持续运行，并根据发现采取行动。虽然一个可读的总结很有帮助，但人们可以将代码修改为根据确定的内容采取行动并执行下游应用。

既然我们已经观察到了LLMs可以执行的一些功能，我们可以退后一步，改进我们利用这些LLMs的方式。在下一节中，我们将举例说明如何减少LLM处理，从而节省API成本，或者当使用本地LLM时，减少推理计算。

# 提示压缩和API成本降低

这一部分专门介绍在采用基于API的LLMs（如OpenAI的服务）进行资源优化方面的最新进展。在考虑使用远程LLM作为服务与本地托管LLM之间的许多权衡时，一个关键指标是成本。特别是，根据应用和用途，API成本可能会累积到相当大的数额。API成本主要受发送到和从LLM服务发送的令牌数量驱动。

为了说明这种支付模式在商业计划中的重要性，考虑那些产品或服务依赖于API调用OpenAI的GPT的业务单元，其中OpenAI作为第三方供应商。作为一个特定例子，想象一个允许用户通过LLM助手评论帖子的社交网络。在这种情况下，用户想要评论一个帖子，而不是写一个完整的评论，一个功能允许用户用三到五个词描述他们对帖子的感受，而后端过程则增加一个完整的评论。

在这个特定的例子中，引擎收集用户的三个到五个单词，它还收集评论针对的帖子的内容，这意味着它还会收集社交网络专家认为与增强评论相关的所有其他相关信息。例如，用户的个人资料描述，他们过去的几条评论等等。

这意味着每次用户希望对评论进行增强时，都会从社交网络的服务器通过API发送一个详细的提示。

现在，这种类型的流程可能会积累很高的成本。

在本节中，我们将分析一种通过减少通过API发送给LLM的令牌数量来降低这种成本的方法。基本假设是，人们总是可以减少发送给LLM的单词数量，从而降低成本，但性能的降低可能是显著的。我们的动机是在保持高质量性能的同时减少这种数量。然后我们询问是否只发送“正确”的单词，忽略其他“非重要”的单词。这个概念让我们想起了文件压缩的概念，其中使用了一种智能且定制的算法来减少文件的大小，同时保持其目的和价值。

## 提示压缩

在这里，我们介绍**LLMLingua**，这是微软开发的一个旨在通过压缩来处理“信息稀疏”的提示的开发工具。

LLMLingua利用一个紧凑、训练有素的语料库模型，如LLaMA-7B，来识别和删除提示中的非必要令牌。这种方法使得使用LLM进行推理变得高效，实现了高达20倍压缩且性能损失最小（[https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua)）。

在他们的论文（[https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736) 和 https://arxiv.org/abs/2310.06839）中，作者解释了该算法及其提出的优势。值得注意的是，除了成本降低外，压缩还旨在聚焦剩余内容，作者指出这可以通过LLM的性能提升来实现，因为它避免了稀疏和嘈杂的提示。

让我们在一个真实世界的例子中尝试提示压缩，并评估其影响和各种权衡。

## 尝试提示压缩并评估权衡

为了进行这个实验，我们将展示一个真实世界的例子。

在我们当前的使用案例中，我们正在开发一个位于学术出版物数据库之上的功能。该功能允许用户选择特定的出版物并对其提出问题。后端引擎评估问题，审查出版物，并得出答案。

为了缩小该功能的范围以便进行一系列实验，出版物来自特定的AI出版物类别，用户提出的问题是以下内容：

[PRE16]

这个问题需要对每一篇出版物进行深入和有洞察力的审查，因为在某些情况下，一篇出版物讨论了一种新颖的算法，其中在出版物的任何地方都没有明确提到强化学习这个术语，但根据算法的描述，可以推断出它确实利用了强化学习的概念，并将其标记为强化学习。

请参考以下笔记本：`Ch9_RAGLlamaIndex_Prompt_Compression.ipynb`。

在这个代码中，我们运行了一系列实验，每个实验都按照上述特征描述进行。每个实验都是以一个完整的、端到端的RAG（Retrieval-Augmented Generation）任务的形式进行的。虽然我们在之前的RAG示例中使用了LangChain，但在这里，我们引入了LlamaIndex。LlamaIndex是一个开源的Python库，它采用了一个RAG框架（[https://docs.llamaindex.ai/en/stable/index.html](https://docs.llamaindex.ai/en/stable/index.html)）。LlamaIndex在这一点上与LangChain相似。

微软团队整合的LLMLingua代码栈与LlamaIndex集成。

让我们详细审查代码。

### 代码设置

与之前的笔记本类似，这里我们也使用以下方式设置了初始设置：

1.  在这个代码部分，我们首先定义了一些关键变量。

1.  我们设置了想要运行的实验数量。我们想要确保选择一个足够大的数字，以便能够得到压缩影响的良好统计代表性。

1.  我们设置了top-k，这是RAG框架为提示上下文检索的块的数量。

1.  我们预先定义了希望压缩减少的目标令牌数量。

1.  最后，就像之前的代码一样，我们设置了我们的OpenAI API密钥。

我们借此机会强调，在这个评估中，一些参数被固定是为了限制其复杂性，并使其适合教育目的。在商业或学术环境中进行此类评估时，应该有定性的或定量的推理来支持所选择的价值。定性可能的形式是“由于预算限制，我们将把期望的减少量固定为999个令牌”，而定量的可能寻求不固定它，而是将其作为其他权衡的一部分进行优化。在我们的案例中，我们将这个特定的参数固定为一个值，这个值被发现可以在保持两个评估方法之间良好的协议率的同时，实现令人印象深刻的压缩率。另一个例子是我们选择的实验数量，这是在运行时间、GPU内存分配和统计功效之间的权衡。

### 收集数据

我们需要收集出版物的数据集，并且我们也对其进行筛选，以便只保留属于AI类别的有限出版物群体。

### LLM配置

在这里，我们为将要使用的两个LLM（大型语言模型）奠定了基础。

压缩方法LLMLingua使用Llama2作为压缩LLM。它将获得LlamaIndex RAG管道检索到的上下文、用户的问题，并将上下文内容的大小压缩和减少。

OpenAI的GPT将被用作提示的下游LLM，这意味着它将获得关于强化学习的问题以及额外的相关上下文，并返回一个答案。

此外，在这里，我们定义了用户的问题。请注意，我们为OpenAI的GPT添加了如何呈现答案的说明。

### 实验

这是笔记本的核心。一个`for`循环遍历各种实验。在每次迭代中，评估两种场景：

1.  **第一种场景**：部署了一个普通的RAG任务，其中上下文在未压缩的情况下被检索。提示由检索到的上下文和用户的问题组成，记录LLM返回的答案以及发送的标记数和处理时间。

1.  **第二种场景**：使用LLMLingua。检索到的上下文被压缩。压缩后的上下文连同用户的问题一起发送给LLM。再次，记录返回的答案以及发送的标记数和处理时间。

当这个代码单元完成时，我们有一个字典`record`，它保存了每个迭代的有关值，这些值将被用于汇总和得出结论。

### 分析上下文压缩的影响——分类性能的降低与资源效率的提升

在这里，我们总结了实验的数值，并推断出提示压缩对LLM性能、处理时间和API成本的影响：

+   我们发现，上下文长度的减少产生了92%的一致率。

+   我们发现，压缩过程将处理时间延长了11倍。

+   我们发现，上下文长度的减少节省了发送标记总成本的92%！

注意，成本降低与一致率呈负相关，因为我们预计成本节省的增加将降低一致率。

这种降低是显著的，在某些情况下，甚至可能将服务从亏损转变为盈利。

这里有一些关于不一致的意义和额外权衡的注意事项。关于两种方法之间的一致率下降，虽然两种方法之间的一致性暗示它们都是正确的，但不一致可能有两种情况。可能是第二种场景中，压缩扭曲了上下文，因此模型无法正确分类它。然而，情况可能相反，因为压缩可能减少了无关内容，使LLM专注于内容的相关方面，从而使压缩上下文的场景得出正确答案。

关于额外的权衡，上述LLM性能、处理时间和API成本的指标并没有揭示出额外的考虑因素，例如压缩所需的计算资源。在我们的案例中，本地的压缩LLM（Llama2）需要本地托管和本地GPU。这些是非平凡的资源，普通笔记本电脑上并不存在。记住，原始方法，即第一种场景，并不需要这些。普通的RAG方法可以使用较小的LM进行嵌入，例如基于BERT的LM，或者甚至基于API的嵌入。根据我们的原始假设，提示LLM被选为远程和基于API的，从而使得部署环境具有最少的计算资源，就像普通笔记本电脑提供的资源一样。

这一评估证明，LLMLingua提示压缩方法作为成本降低手段非常具有影响力和实用性。

在本章接下来的最后一个代码演示中，我们将继续观察这一经验的结果，我们将通过组建一个由专家组成的团队来实现，每个专家由一个LLM扮演，以增强得出分析结论的过程。

# 多个代理——形成一个LLM团队进行协作

本节讨论了LLM领域中最近最激动人心的方法之一，即同时使用多个LLM。在本节的背景下，我们试图定义多个代理，每个代理都由一个LLM支持，并赋予不同的指定角色。与我们在ChatGPT中看到的用户直接与LLM工作不同，在这里，用户设置了多个LLM，并为每个LLM定义了不同的系统提示来设定它们的角色。

## 多个LLM代理同时工作的潜在优势

就像人们一起工作时一样，这里我们也看到了同时使用几个LLM的优势。

一些优势如下：

+   **增强验证和减少幻觉**：研究表明，当向大型语言模型（LLM）提供反馈并要求其进行推理或检查其回答时，其回答的可靠性会提高。在为团队中的各种LLM代理指定角色时，至少其中一方的系统提示可能包括批评和验证答案的要求。

+   **允许个人根据意愿参与或退出过程**：在指定各种角色时，用户可以将自己插入团队中，这样当轮到用户参与对话时，其他代理会等待用户输入。然而，如果用户希望的话，他们也可以完全退出，让LLM自动工作。

    在以下示例中，我们将看到后者的示例。

+   **允许不同的大型语言模型（LLM）得到最佳利用**：如今，我们有几种领先的LLM可供选择。有些是免费的，本地托管，有些是基于API的。它们在大小和能力上有所不同，其中一些在特定任务上比其他LLM更强。当组建一个每个代理都分配不同角色的团队时，可能需要设置一个最适合该角色的不同LLM。例如，在编码项目的背景下，其中一位代理是特定编程语言的程序员，用户可以选择为该代理设置一个在该特定编程语言中代码生成能力更强的LLM。

+   **优化资源 – 使用多个较小的LLM**：想象一个涉及技术功能和领域专业知识的项目。例如，在医疗领域构建一个用户平台。你希望有一个前端工程师、后端工程师、设计师和医学专家，所有这些都由项目经理和产品经理管理。如果你要使用多代理框架开发这个平台，你会定义代理，将各种角色分配给他们，并选择一个LLM来驱动他们。如果你要为所有代理使用相同的LLM，比如说OpenAI最新的GPT，那么这个模型将必须非常通用，因此可能需要非常大、可能非常昂贵，甚至可能很慢。然而，如果你可以访问到每个都预先训练以仅执行有限功能的单个LLM，例如，一个专注于医疗服务领域的LLM和一个专注于Python后端开发的LLM，那么你将把每个特定的LLM分配给相应的代理。

    这可能显著减少模型大小，因为几个专业LLM的组合架构可能比一个通用LLM的架构更小，假设两种情况下的性能相同。

+   **优化资源 – 单一LLM的最佳分配**：使用多个LLM的一个独特且特定的情况是我们寻求根据当前任务优化所选择的LLM。这种情况与上述所有情况都不同，因为它不涉及多个LLM同时工作的情况。在这种情况下，一个路由算法根据当前约束和变量的状态选择一个LLM。这些可能包括以下内容：

    +   计算系统各部分的当前负载

    +   成本限制，这些可能随时间变化

    +   提示的来源，因为不同的客户/地区可能有不同的优先级

    +   提示的目的，因为不同的业务场景可能被赋予不同的优先级

    +   提示的要求，作为一个代码生成任务，可能需要使用一个小型且高效的代码生成LLM来获得出色的响应，而审查法律文件并提出先例的建议可能需要完全不同的模型

### AutoGen

我们在本节中使用的特定框架称为AutoGen，由微软提供（GitHub仓库：[https://github.com/microsoft/autogen/tree/main](https://github.com/microsoft/autogen/tree/main)）。

*图9**.1* 展示了AutoGen框架。以下内容来自GitHub仓库中的说明：

*AutoGen是一个框架，它使开发者能够使用能够相互对话以解决任务的多个代理来开发LLM应用。AutoGen代理是可定制的、可对话的，并允许无缝地参与人类。它们可以在各种模式下运行，这些模式结合了LLM、人类输入和*工具*。

![图9.1 – AutoGen功能](img/B18949_09_1.jpg)

图9.1 – AutoGen功能

在*图9**.1* 的左侧，我们观察到为个体代理指定的角色和能力；在右侧，我们观察到一些可用的对话结构。

AutoGen在代码仓库中展示的关键功能：

+   AutoGen通过最小努力使基于多代理对话的下一代LLM应用成为可能。它简化了复杂LLM工作流程的编排、自动化和优化。它最大化了LLM模型的表现并克服了它们的弱点。

+   它支持复杂工作流程中的多样化对话模式。通过可定制和可对话的代理，开发者可以使用AutoGen构建关于对话自主性、代理数量和代理对话拓扑的广泛对话模式。

+   它提供了一系列具有不同复杂度的运行系统。这些系统涵盖了从各个领域和复杂度范围内的广泛应用。这展示了AutoGen如何轻松支持多样化的对话模式。

+   AutoGen提供了增强型LLM推理。它提供了API统一和缓存等实用工具，以及错误处理、多配置推理、上下文编程等高级使用模式。

AutoGen由微软、宾夕法尼亚州立大学和华盛顿大学的研究合作研究支持。

接下来，我们可以深入到代码中的实际示例。

### 完成复杂分析——可视化结果并得出结论

在这里，我们将展示一个由多个代理组成的团队，每个代理都有不同的指定角色，如何作为一个专业团队提供服务。我们选择的用例是之前运行代码的延续。在上次代码中，我们执行了对使用提示压缩的复杂评估，当该代码完成后，我们得到了两个结果项：一个包含实验的数值测量的`dict`，称为`record`，以及关于结果协议率、令牌和成本的减少以及处理时间的变化的口头陈述。

在之前的笔记本中，我们故意停了下来。我们没有可视化令牌和成本的减少，也没有形成关于是否倡导使用提示减少的观点。然而，在商业或学术环境中，人们需要提供这两者。当你向利益相关者、决策者或研究社区展示你的发现时，你期望在可行的情况下，可视化实验的统计显著性。作为NLP和ML的主题专家，你也期望提供你的建议，即是否采用实验方法。

我们将使用那个评估的结果，并将任务分配给一组代理来完成这项工作！

请参考以下笔记本：`Ch9_Completing_a_Complex_Analysis_with_a_Team_of_LLM_Agents.ipynb`。该笔记本从安装、导入和设置的常见方面开始。你会注意到AutoGen有一个特定的字典形式的设置格式。它们提供了详细信息，正如你可以在我们的笔记本中看到的那样。

现在，我们转向有趣的部分！

#### 创建实验重要性的可视化

`record.pickle`文件是一个`dict`变量。它是之前评估笔记本中数值结果的集合。我们的愿望是可视化每个实验的令牌计数分布。有原始提示的令牌计数和压缩提示的令牌计数，以及每个实验中两者的比率。

在本节中，我们将组建一个团队，编写代码以可视化三个分布。

#### 定义团队需要完成的任务

首先，我们定义团队需要完成的任务。我们告诉团队文件保存的位置、上下文以及`dict`中值的性质，从而为团队提供他们需要来构思任务解决方案的理解。然后，我们描述了创建图表和可视化分布的任务。所有这些细节都在描述任务的单一字符串中。请注意，在敏捷Scrum工作环境中，这个任务字符串类似于故事的目的。

现在我们已经形成了一个全面的描述，应该很清楚期望的是什么。例如，我们要求标注图表和坐标轴，但我们没有明确说明期望的标签是什么。代理会自己理解，就像我们自己会理解一样，因为标签是从任务和数据字段名称中推断出来的。

#### 定义代理和分配团队成员的角色

对于这个任务，我们需要三名团队成员：一名程序员编写代码，一名QA工程师运行代码并提供反馈，以及一名团队领导来验证任务是否完成。

对于每个角色，我们阐述一个系统提示。正如我们在 [*第8章*](B18949_08.xhtml#_idTextAnchor440) 中所学，这个系统提示对LLM的功能有重大影响。请注意，我们还为QA工程师和团队领导提供了运行代码的能力。这样，他们就能验证程序员的代码并提供客观反馈。如果我们让同一个代理编写代码并确认其正确性，我们可能会发现，在实践中，它可能会生成一个初稿，不会麻烦去运行和验证它，并且会在未经验证的情况下完成该任务。

#### 定义群组对话

这里，我们将对话定义为多代理对话；这是AutoGen的一个特性。这与定义一系列对话的情况略有不同，其中每个对话只涉及两个代理。群组对话涉及更多的代理。

当定义一个群组对话时，我们也会定义一个对话经理。

#### 部署团队

团队领导将我们定义的任务分配给经理。然后经理将工作委托给程序员和QA工程师。

下面是屏幕上显示的自动化对话的要点：

[PRE17]python

import pandas as pd

import matplotlib.pyplot as plt

# 从URL加载记录字典

import requests

import pickle

[...]

qa_engineer (向 manager_0):

exitcode: 0 (执行成功)

代码输出：

图(640x480)

programmer (向 manager_0):

终止

[PRE18]

lead (向 manager_1):

请参考下面打印的结果。

这些是从 [...] 得出的结果。

writer (向 manager_1):

使用LLMLingua进行提示压缩的实验产生了以下结果：

- 分类性能：

- [...] 的协议率

principal_engineer (向 manager_1):

[...]

[PRE19]

仔细考虑提示压缩带来的权衡是至关重要的，因为虽然它可能导致资源节约，但可能会对处理效率产生影响。采用提示压缩的决定应该基于对这些权衡的全面理解。

[PRE20]

总体而言，结果表明，虽然提示压缩可能导致成本节约和资源减少，但它是以降低分类性能和显著增加处理时间为代价的。

**建议：** 使用LLMLinguam进行提示压缩**不推荐**，因为它可能会对分类性能产生负面影响，并显著增加处理时间，超过了潜在的节约成本。

```

在这里，团队发现得出明确结论要容易得多。它没有进行任何人为干预，仅基于给出的数值结果。

## 对多代理团队的总结性思考

这种同时使用多个LLM的新兴方法在AI领域引起了兴趣和关注。在本节中我们展示的代码实验中，毫无疑问地证明了AutoGen的群组对话在专业环境中可以提供有形和可操作的价值。尽管设置这些代码实验需要一系列的尝试和错误来正确设置代理角色和描述任务，但它表明这个框架正朝着减少人类干预的方向发展。似乎仍然是一个重要组成部分的是，对那些代理团队*工作*产生的结果进行人类监督、反馈和评估。我们想向读者强调，在我们这本书中分享的各种应用和创新中，我们将多代理框架标记为最有可能增长并成为最受欢迎的框架。这是基于行业对AI自动化和展示类似人类的专业知识的压倒性期望，而像Autogen和Autodev这样的创新，后者由微软推出，都展示了日益增长的可行性和能力。

# 摘要

在这个关键章节的整个过程中，我们深入探讨了最近和最具突破性的LLM应用，这些应用通过全面的Python代码示例进行展示。我们首先通过使用RAG框架和LangChain来解锁高级功能，从而增强LLM在特定领域任务中的性能。旅程继续，我们探讨了用于复杂格式化和处理的链的高级方法，随后是来自不同网络来源的信息检索自动化。我们还解决了通过提示压缩技术优化提示工程的问题，显著降低了API成本。最后，我们探索了LLM的协作潜力，通过组建一个协同解决复杂问题的模型团队。

通过掌握这些主题，你现在已经掌握了一套强大的技能，使你能够利用LLM的力量应用于各种应用。这些新获得的能力不仅使你准备好应对NLP中的当前挑战，还为你提供了洞察力，以创新和推动该领域的可能性边界。从本章中获得的实际知识将赋予你应用高级LLM技术解决现实世界问题的能力，为效率、创造力和问题解决开辟新的机会。

随着我们翻过这一页，下一章将带我们进入AI和LLM技术新兴趋势的领域。我们将深入研究最新的算法发展，评估它们对各个商业部门的影响，并考虑AI的未来格局。这次即将到来的讨论承诺为您提供对领域未来走向的全面理解，以及如何保持技术创新的前沿。
