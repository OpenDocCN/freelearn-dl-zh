- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enhancing the GenAISys with DeepSeek
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *DeepSeek-V3 Technical Report* arrived in December 2024, followed a month
    later by the **DeepSeek-R1** paper and a full set of open source resources. The
    release sent a shockwave through the AI community: download counts on Hugging
    Face exploded, DeepSeek apps topped store charts, and new API providers sprang
    up overnight. Governments debated moratoriums while the major generative AI players—OpenAI,
    X (with Grok 3), and others—stepped on the gas. Within weeks, we saw o3 versions
    improve OpenAI models, a clear signal that the AI race had entered a new phase.
    At the same time, real-world AI production teams watched these dizzying innovations
    pile up, disrupting existing AI systems. Teams that spent months adapting their
    systems to one generative AI model found themselves caught in a gray area between
    systems that work but could still be improved.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what should we do? Should we upgrade a stable GenAISys to follow the latest
    trend in an accelerating AI market with the cost and risks entailed? Or should
    we ignore the latest models if our system is stable? If we ignore evolutions,
    our system may become obsolete. If we keep following the trends, our system will
    become unstable!
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shows how to strike a workable balance. Instead of rewriting entire
    environments for every model upgrade or new functionality, we introduce a **handler-selection
    mechanism** that routes user requests to the right tool at the right time. A **handler
    registry** stores every AI function we develop; the selection layer inspects each
    incoming message and triggers the appropriate handler. With this design, the GenAISys
    can evolve indefinitely without destabilizing the stack. We will begin the chapter
    by defining how a balanced approach can be found between model evolutions and
    real-world usage, illustrated through a product design and production use case.
    Next comes a concise look at DeepSeek-V3, DeepSeek-R1, and the distilled Llama
    model we’ll implement. Then, we’ll install **DeepSeek-R1-Distill-Llama-8B** locally
    with Hugging Face, wrap it in a reusable function, and then plug it into our GenAISys.
    At that point, we will develop the flexible, scalable environment of the handler-selection
    mechanism to allow us to activate the models and tasks we need for each project.
    By the end of the chapter, you will be able to have full control over the GenAISys
    and be ready for whatever the AI market throws at you.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The balance between AI acceleration and usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of DeepSeek-V3, R1, and distillation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing DeepSeek-R1-Distill-Llama-8B locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a function to run DeepSeek-R1-Distill-Llama-8B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying DeepSeek-R1-Distill-Llama-8B in the GenAISys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a handler registry for all the AI functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a handler-selection mechanism to select the handlers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading the AI functions to be handler-compatible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running product design and production examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by defining the balance between relentless AI evolution and day-to-day
    business usage.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing model evolution with project needs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before racing to adopt every new model, we must anchor our decisions on project
    needs. So far, our GenAISys has served mostly marketing functions for an online
    travel agency. Now, imagine that the agency has grown large enough to fund a line
    of branded merchandise—custom travel bags, booklets, and other goodies. To manage
    this new venture, the company hires a **product designer and production manager**
    (**PDPM**). The PDPM studies customer feedback and designs personalized kits but
    quickly sees that AI could boost both creativity and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in this chapter thus focus on product design and production workflows.
    Our goal is not to force DeepSeek (or any other model) into every task but to
    choose the model that best fits the need. To do that, we’ll extend the GenAISys
    with a handler-selection mechanism that responds to user choices in the IPython
    interface and to keywords in each message. Depending on the situation, the operations
    team can configure the system to route requests to GPT-4o, DeepSeek, or any future
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Before wiring DeepSeek into our GenAISys, let’s review the DeepSeek model family.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepSeek-V3, DeepSeek-V1, and R1-Distill-Llama: Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DeepSeek’s journey began with DeepSeek-V3, advanced to DeepSeek-R1—a reasoning-focused
    upgrade—and then branched into distilled variants built on Qwen and Llama architectures,
    as shown in *Figure 7.1*. V3 was responsible for putting the model on the map,
    and it was R1 that brought in robust reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: DeepSeek development cycle](img/B32304_07_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: DeepSeek development cycle'
  prefs: []
  type: TYPE_NORMAL
- en: According to DeepSeek-AI et al. (2024), V3 delivered striking efficiency gains.
    Its full training budget was only 2.788 million H800 GPU-hours (≈ USD 5.6 million
    at USD 2 per GPU-hour)—remarkably low for a modern frontier model. Even on a per-token
    basis, the cost is lean, needing just 180 K GPU-hours per trillion tokens. The
    cost is, therefore, very economical compared to what is typically reported for
    large-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: When we examine the list of authors of the DeepSeek-V3 Technical Report (2024)
    on arXiv, [https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437),
    we first notice that more than 150 specialists wrote the paper! In itself, this
    factor alone proves the efficiency of open source approaches that involve collective
    efforts to produce efficiency-driven architectures by opening ideas to every person
    willing to contribute. The list of *Contributions and Acknowledgements* in *Appendix
    A* is a tribute to open source developments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: DeepSeek-R1 is derived from DeepSeek-V3](img/B32304_07_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: DeepSeek-R1 is derived from DeepSeek-V3'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSeek-R1 grew straight out of DeepSeek-V3\. The team wanted V3’s punch, but
    with feather-weight inference, so they wired the model to activate only a minimal
    subset of experts during inference, as shown in *Figure 7.2*. Furthermore, training
    stayed just as lean. R1 jumped directly into reinforcement learning with no supervised
    fine-tuning. The reasoning was high but faced limitations for classic NLP tasks.
    Rule-based rewards were introduced to avoid the neural network’s training cycles.
    The training prompts were structured with neat `<think> … <answer>` tags, avoiding
    the smuggling of biases into the model’s final answer. Moreover, the reinforcement
    learning process began with *cold-start* data containing **chain of thought**
    (**CoT**) examples focusing on reasoning. This approach reduced training time
    and costs.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSeek evolved to R1 by refining MoE strategies and integrating multi-token
    prediction, significantly enhancing both accuracy and efficiency. Finally, DeepSeek-R1
    was used to enhance DeepSeek-V3 with reasoning features. DeepSeek-R1 was also
    distilled into smaller models such as Llama and Qwen. The technique used was knowledge
    distillation, where a smaller “student” model (in this chapter, Llama) learns
    from a “teacher” model (in this chapter, DeepSeek-R1). This approach is effective
    in that it teaches the student model to achieve performance similar to that of
    the teacher while being more efficient and suitable for deployment on resource-constrained
    devices, which will be the case in this chapter, as you’ll see.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s install and run DeepSeek-R1-Distill-Llama-8B and plug it into our GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with DeepSeek-R1-Distill-Llama-8B
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement DeepSeek-RAI-Distill-Llama-8B, a distilled
    version of DeepSeek-R1, as shown in *Figure 7.3*. We will install Hugging Face’s
    open-source `Transformers` library, an open framework for using and fine-tuning
    pre-trained transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: Installing DeepSeek-RAI-Distill-Llama-8B, a distilled version
    of DeepSeek-R1](img/B32304_07_3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Installing DeepSeek-RAI-Distill-Llama-8B, a distilled version of
    DeepSeek-R1'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the DeepSeek-RAI-Distill-Llama-8B documented by Hugging Face:
    [https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B).
    Hugging Face also provides recommendations for this model: [https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B#usage-recommendations](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B#usage-recommendations).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The version we will download is an open source distilled version of DeepSeek-R1
    provided by Unsloth, an LLM accelerator, on Hugging Face: [https://unsloth.ai/](https://unsloth.ai/).
    We will thus not use a DeepSeek API but only a locally installed open source version
    that does not interact with the web, leveraging Hugging Face’s SOC 2 Type 2 certification
    that complies with privacy and security constraints: [https://huggingface.co/docs/inference-endpoints/en/security](https://huggingface.co/docs/inference-endpoints/en/security).'
  prefs: []
  type: TYPE_NORMAL
- en: To install deepseek-ai/DeepSeek-R1-Distill-Llama-8B locally on a recent machine,
    it is recommended to have about 20 GB of RAM. A bit less is possible, but it is
    best to avoid the risk. About 20 GB of disk space is also recommended.
  prefs: []
  type: TYPE_NORMAL
- en: To install DeepSeek-R1-Distill-Llama-8B on Google Colab, it is recommended to
    use Google Colab Pro to obtain GPU memory and power. For this section, the Hugging
    Face model is downloaded on Google Drive, which is mounted through Google Colab.
    The disk space required will exceed the free version of Google Drive, and a minimal
    subscription to Google Drive may be required. Check the costs before installing
    on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb` within the Chapter07
    directory on GitHub ([https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main)).
    We will follow the standard procedure of the Hugging Face framework:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the notebook once to install DeepSeek-R1-Distill-Llama-8B locally:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the notebook with no installation and interact with the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the model in place, we can wrap it in a handler and plug it into our GenAISys
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the DeepSeek Hugging Face environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll begin by installing DeepSeek-R1-Distill-Llama-8B (locally or in Colab)
    and then run a quick inference to confirm everything works.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first install DeepSeek in the first session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The GPU needs to be activated, so let’s check it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are installing Google Colab, we can mount Google Drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We now set the cache directory in Google Drive and set the corresponding environment
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now install the Hugging Face `Transformers` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With that, we are ready to download the model.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading DeepSeek
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now download the model from `unsloth/DeepSeek-R1-Distill-Llama-8B` within
    the Hugging Face framework with the tokenizer and the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The download time will be displayed and also depends on your internet connection
    and Hugging Face’s download speed. Once installed, verify that everything is installed
    in your local directory. In this case, it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should show the files downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s run a DeepSeek session.
  prefs: []
  type: TYPE_NORMAL
- en: Running a DeepSeek-R1-Distill-Llama-8B session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make sure the model is correctly installed and also to avoid overwriting
    the installation when starting a new session, go back to the top of the notebook
    and set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now load the `DeepSeek-R1-Distill-Llama-8B` tokenizer and model locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The time it took to load the model is displayed and will depend on the configuration
    of your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can have a look at the configuration of the Llama model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows interesting information. The `LlamaConfig` readout confirms
    we are running a compact, well-scoped model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The distilled Llama model has 32 transformer layers and 32 attention heads
    per layer, totaling 1,024 attention heads. Also, it contains 8 billion parameters.
    By contrast, its teacher model, **DeepSeek-R1**, is an MoE giant with **61 layers**
    and a massive **671 billion parameters**, of which about **37 billion** are active
    on each forward pass. Let’s now run an example with a prompt for a production
    issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We first insert time measurement and tokenize the input using the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run the generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal of our parameters is to limit the repetitions and remain focused:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_new_tokens=1200`: To limit the number of output tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repetition_penalty=1.5`: To limit the repetitions (can be higher)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_repeat_ngram_size=3`: To prevent repeating n-grams of a particular size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature=0.6`: To reduce randomness and stay focused'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p=0.9`: Allows nucleus sampling for diversity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k=50`: Limits token selection to `top_k` to make the next token choice'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This set of tokens tends to limit repetitions while allowing diversity. We
    can now decode the generated text with the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the overall time it took the model to think and respond:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s wrap `generated_text` and display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides ideas as requested. It displays DeepSeek-R1’s thinking
    abilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Integrating DeepSeek-R1-Distill-Llama-8B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will add DeepSeek-R1-Distill-Llama-8B to our GenAISys in
    a few steps. Open `GenAISys_DeepSeek.ipynb`. You can decide to run the notebook
    with DeepSeek in the first cell, which will require a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also decide not to run DeepSeek in this notebook, in which case, you
    will not need a GPU and can change the runtime to CPU. If you decide on this option,
    OpenAI’s API will take over, confirming that no GPU is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, go to the *Setting up the DeepSeek Hugging Face environment* subsection
    of the notebook. We will simply transfer the following cells from `Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb`
    to this subsection. The following code will only be activated if `deepseek=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPU activation check: `!nvidia-smi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting the local cache of the model: `…os.environ[''TRANSFORMERS_CACHE'']
    =cache_dir…`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing the Hugging Face library: `!pip install transformers==4.48.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loading the tokenizer and the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The installation is now complete. The calls to the DeepSeek model will be made
    in the *AI Functions* section if `DeepSeek==True` with the parameters described
    in the *Running a DeepSeek-R1-Distill-Llama-8B session* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: With DeepSeek functioning, we’re ready to build the handler selection mechanism,
    which will route every user request to GPT-4o, DeepSeek, or any future model—without
    touching the rest of the stack.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the handler selection mechanism as an orchestrator of the GenAISys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PDPM at the online travel agency is experiencing increased demands, requiring
    the agency to design and produce large quantities of merchandise kits, including
    travel bags, booklets, and pens. The PDPM wants to be directly involved in the
    GenAISys development to explore how it can significantly boost productivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the growing complexity and variety of AI tasks in the system, the GenAISys
    development team has decided to organize these tasks using handlers, as illustrated
    in *Figure 7.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4: GenAISys data flow and component interaction](img/B32304_07_4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: GenAISys data flow and component interaction'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll, therefore, define, implement, and then invite the PDPM to run the enhanced
    GenAISys to evaluate functions aimed at improving productivity in merchandise
    design and production.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.4* describes the behavior of the handler pipeline we are going to
    implement:'
  prefs: []
  type: TYPE_NORMAL
- en: The **IPython interface** serves as the entry and exit point for user interactions,
    capturing user input, formatting it, and displaying responses returned by the
    handler mechanism.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **handler mechanism** interprets user inputs, directing data among the IPython
    interface, the handler registry, and the AI functions. It ensures tasks triggered
    by user messages execute smoothly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **handler registry** maintains a list of all available handlers and their
    corresponding functions. It supports system modularity and scalability by clarifying
    handler registration and retrieval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AI functions** perform core tasks such as natural language understanding
    and data analysis, executing instructions received from the handler mechanism,
    and returning outputs to the IPython Interface.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this setup, a user provides input through the IPython interface. This input
    is routed into a handler selection mechanism, which then evaluates the available
    handlers registered alongside specific conditions. Each entry in the registry
    is a (condition, handler) pair responsible for different operations such as reasoning,
    image generation, or data analysis. Once a matching condition is found, the corresponding
    AI function is activated. After processing, it returns the results to the interface.
    This structured pipeline—from user input through to the AI-generated response—is
    handled gracefully, with each handler clearly defined for readability and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Before coding, let’s clearly define what we mean by a “handler” in the GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: What is a handler?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A handler is essentially a specialized function responsible for addressing specific
    tasks or types of requests. Each handler is registered alongside a condition,
    typically a small function or lambda expression. When evaluated as `True`, this
    condition indicates that the associated handler should be invoked. This design
    neatly decouples the logic for deciding *which* handler should run from *how*
    the handler executes its task.
  prefs: []
  type: TYPE_NORMAL
- en: In our context, handlers are the orchestrator’s building blocks—conditional
    functions designed to process specific input types. When a user provides input,
    the handler selection mechanism evaluates it against the handler registry, which
    consists of pairs of conditions and handlers. Upon finding a match, the corresponding
    handler is triggered, invoking specialized functions such as `handle_generation`,
    `handle_analysis`, or `handle_pinecone_rag`. These handlers execute sophisticated
    reasoning, data retrieval, or content generation tasks, providing precise and
    targeted outputs.
  prefs: []
  type: TYPE_NORMAL
- en: But why exactly is a handler better for our GenAISys than a traditional list
    of `if…then` conditions?
  prefs: []
  type: TYPE_NORMAL
- en: Why is a handler better than a traditional if...then list?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using handlers improves maintainability and readability. Instead of scattering
    multiple `if...then` checks across the code, each handler is self-contained: it
    has its condition and a separate function that carries out the required action.
    This structure makes it easier to add, remove, or modify handlers without risking
    unintended interactions in a chain of lengthy conditionals. Additionally, since
    it separates the logic of “which handler do we need?” from “how does that handler
    actually work?” we’re left with a more modular design that makes scaling seamless.'
  prefs: []
  type: TYPE_NORMAL
- en: We will first go through the modifications to our IPython interface.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. IPython interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll start by reviewing the primary updates to our IPython interface, which
    remains the main interaction point, as shown in *Figure 7.5*. From a user perspective,
    the introduction of handlers doesn’t alter the interface significantly, but some
    underlying code adjustments are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: The IPython interface processes the user input and displays the
    output](img/B32304_07_5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: The IPython interface processes the user input and displays the
    output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The IPython interface calls `chat_with_gpt` as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, however, we can explicitly select either an OpenAI or a DeepSeek model
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To add the model to the `chat_with_gpt` call, we first add a drop-down model
    selector to the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The model selector is added to the `VBox` instances in the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The user can now choose their preferred model directly from the interface,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: Selecting a model](img/B32304_07_6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Selecting a model'
  prefs: []
  type: TYPE_NORMAL
- en: '![A magnifying glass on a black background  AI-generated content may be incorrect.](img/1.png)**Quick
    tip**: Need to see a high-resolution version of this image? Open this book in
    the next-gen Packt Reader or view it in the PDF/ePub copy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2.png)**The next-gen Packt Reader** is included for free with the purchase
    of this book. Scan the QR code OR go to [packtpub.com/unlock](http://packtpub.com/unlock),
    then use the search bar to find this book by name. Double-check the edition shown
    to make sure you get the right one.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A qr code on a white background  AI-generated content may be incorrect.](img/Unlock_Code2.png)'
  prefs: []
  type: TYPE_IMG
- en: An additional feature has been added to manage file displays.
  prefs: []
  type: TYPE_NORMAL
- en: File management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many ways to design file management. We will introduce a function
    here that can be expanded during a project’s implementation phase as needed. Our
    file management code has three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Manage user-triggered file deletion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete `c_image.png` when the checkbox is unchecked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use existence checks to prevent errors during deletion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will build the code to handle user interactions directly by observing changes
    in the checkbox widget of our interface within the Jupyter Notebook environment.
    The code will then delete a specific image file (`c_image.png`) when the user
    unchecks the checkbox named `files_checkbox`. This ensures that files are removed
    cleanly when they are no longer needed, preventing clutter and saving storage
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The event handler function defines a callback function named `on_files_checkbox_change`
    that will execute when the state of `files_checkbox` changes. `change` is provided
    by the observer, which contains information about the change event, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`old`: The previous state of the checkbox'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new`: The new state of the checkbox'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The code verifies whether the checkbox was previously checked (`True`) and
    has now been unchecked (`False`). This guarantees that the file deletion only
    occurs when the user explicitly unchecks the checkbox, preventing accidental file
    removal. We now remove the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to add an observer to inform the `on_files_checkbox_change` function
    when there is a file status change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `files_checkbox.observe()` function links the `on_files_checkbox_change`
    function to the `files_checkbox` widget. `names='value'` specifies that the function
    should be triggered when the value of the checkbox changes (i.e., when it is checked
    or unchecked).
  prefs: []
  type: TYPE_NORMAL
- en: We will now move on to the next part of the pipeline and implement the handler
    selection mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Handler selection mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The handler selection mechanismdynamically selects and executes the appropriate
    handler based on predefined conditions. It iterates through available handlers,
    evaluating conditions until it finds a match, ensuring efficient and structured
    processing of the user input. The handler selection mechanism is in the `chat_with_gpt`
    function we built in the previous chapters. However, it now contains an orchestration
    task, as shown in *Figure 7.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chat_with_gpt` remains a pivotal function within the GenAISys and now contains
    the handler mechanism'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It checks conditions sequentially to decide which handler to invoke
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It falls back to a memory-based handler if no conditions match
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ensures robustness with error handling for an uninterrupted user experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.7: The orchestration role of the handler mechanism](img/B32304_07_7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: The orchestration role of the handler mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: In the broader GenAISys workflow, the handler mechanism acts as an orchestrator.
    It processes user inputs and identifies which AI functions to activate. When the
    IPython interface captures user messages, the handler mechanism evaluates these
    inputs to determine the appropriate handler from the handler registry. If no specific
    handler matches, it defaults to a memory-based response, which is then returned
    to the IPython interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `chat_with_gpt` function encapsulates this logic. It iterates through a
    predefined list of handlers, each paired with a corresponding condition function.
    When a condition evaluates to true, the associated handler is executed. If none
    match, the fallback memory-based handler ensures a seamless response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through the parameters of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`messages`: The conversation history between the user and the AI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user_message`: The latest message from the user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`files_status`: Tracks the status of any files involved in the conversation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active_instruct`: Any instruction or mode that might influence how responses
    are generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models`: Specifies the active AI model in use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function uses `global memory_enabled` to access a global variable that determines
    whether memory should be applied to store/remember the full dialogue of a user.
    In this chapter, `global memory_enabled=True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function attempts to execute the appropriate handler based on the provided
    conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, `for condition, handler in handlers` iterates over a list called
    `handlers`, where each item is a tuple containing the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: A condition functionto check whether a handler should be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A handler function to execute whether the condition is satisfied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A generic `if` condition, `(...)`, toevaluate the condition function with the
    provided parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code returns the output of the corresponding handler if the condition is
    met, immediately exiting the function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now add a fallback if no handlers match the input conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`handle_with_memory` is called as a default handler that does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Uses the full conversation history (`messages`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considers memory if `memory_enabled` is true, which is the case in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the response directly if executed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, let’s add an exception to catch return errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With the handler selection mechanism defined, we can now proceed to build the
    handler registry that stores these handlers.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Handler registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **handler registry** is a structured collection of condition-handler pairs,
    where each condition is a lambda function that evaluates user messages and instructions
    to determine whether specific criteria are met. When a condition is satisfied,
    the corresponding handler is triggered and executed immediately, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: Creating the handler registry](img/B32304_07_8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Creating the handler registry'
  prefs: []
  type: TYPE_NORMAL
- en: All lambda functions have four parameters (`msg`, `instruct`, `mem`, and `models`).
    This ensures that the number of arguments matches when `chat_with_gpt()` calls
    a handler.
  prefs: []
  type: TYPE_NORMAL
- en: 'The handler registry has three main features:'
  prefs: []
  type: TYPE_NORMAL
- en: Is orchestrated by the handler mechanism and can be unlimited
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routes inputs based on keywords, instructions, or model selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guarantees a fallback response if no conditions match
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will design our handler registry with the following structure of four key
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handler registration**: Creates a list of handlers, each with a condition
    function and a corresponding handler function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specific handler conditions**: Sequentially checks whether an input meets
    any of the specific conditions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fallback handler**: Adds a default memory-based handler if none of the conditions
    match'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution**: When a condition is satisfied, the corresponding handler is
    executed immediately'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of `**kwargs` in the code provides a flexible way to interact with
    the AI functions. `**kwargs` is short for *keyword arguments* and is used in Python
    functions to allow passing a variable number of arguments to a function. In the
    context of our handler registry code, `**kwargs` plays a crucial role by allowing
    handlers to accept additional, optional parameters without explicitly defining
    them in the function. It makes the handlers extensible for future updates or new
    parameters without requiring modifications to existing function signatures.
  prefs: []
  type: TYPE_NORMAL
- en: We will now begin to build the handler registry with the Pinecone/RAG handler.
  prefs: []
  type: TYPE_NORMAL
- en: Pinecone/RAG handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Pinecone/RAG handler manages the **retrieval-augmented generation** (**RAG**)
    functions previously defined. It activates when detecting the `Pinecone` or `RAG`
    keyword within the user message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This handler checks whether the user message contains “Pinecone” or “RAG,” in
    which case `lambda:` returns `True`; otherwise, it returns `False`. We will now
    create the reasoning handler.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already built the reasoning function, but now we need a handler. The
    keywords that trigger the handler are `Use reasoning`, `customer`, and `activities`.
    Any additional text in the message provides context for the reasoning process.
    The handler uses `all()` to ensure all keywords are included in the message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Let’s move on and create the analysis handler.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The analysis handler has been used for memory analysis up to now and is triggered
    by the `Analysis` instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Time to create the generation handler.
  prefs: []
  type: TYPE_NORMAL
- en: Generation handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The generation handler takes memory analysis to another level by asking the
    generative AI model to generate an engaging text for a customer based on a memory
    analysis of the text. The `Generation` instruction triggers the generation handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now build the image creation handler.
  prefs: []
  type: TYPE_NORMAL
- en: Image handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The image creation handler is triggered by the `Create` and `image` keywords
    in the user message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We will now create the freestyle handler for when there is no keyword or instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Fallback memory handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This handler is a general-purpose handler when there is no instruction or keyword
    to trigger a specific function. Let’s append the fallback memory handler accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have replaced `user_memory` with `memory_enabled` to generalize
    memory management.
  prefs: []
  type: TYPE_NORMAL
- en: You can add as many handlers and AI functions as you wish to the handler registry.
    You can scale your GenAISys as much as you need to. You can also modify the keywords
    by replacing them with explicit instructions, as we did for the `Analysis` and
    `generation` functions. The handlers will then call all the AI functions you need.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now go through the new organization of the AI functions.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. AI functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now run the AI functions that are activated by the handler registry.
    The functions build on those from earlier chapters but are now managed by the
    handler-selection mechanism introduced in this chapter. Additionally, the examples
    used in this section are based on typical prompts related to product design and
    production scenarios. Keep in mind that, due to the stochastic (probabilistic)
    nature of generative AI models, outputs can vary each time we run these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9: AI functions call by the handler selection mechanism and registry](img/B32304_07_9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: AI functions call by the handler selection mechanism and registry'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now execute all AI functions currently available in our GenAISys, incorporating
    DeepSeek model calls where applicable. Let’s begin with the RAG functions.
  prefs: []
  type: TYPE_NORMAL
- en: Functions such as speech synthesis, file management, dialogue history, and summary
    generation remain unchanged from previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This RAG function can run with OpenAI or DeepSeek with the `Pinecone` keyword
    in the user message. The RAG function’s name has changed, but its process remains
    unchanged for the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the function now contains a DeepSeek distilled R1 call. The function
    first defaults to OpenAI if no model is provided or if DeepSeek is deactivated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If DeepSeek is activated, it will be called if chosen in the IPython interface
    for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We will first run a sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis (genaisys)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An example user input by the PDPM is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output with OpenAI selected (default) and `Agent` checked will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the output with DeepSeek selected (default) and `Agent`
    checked will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The sentiment score and explanation score are acceptable in both cases. Imagine
    receiving thousands of such customer feedback messages—the GenAISys filters the
    low scores and provides these outputs automatically, storing them in the customer
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the PDPM checks semantic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic analysis (genaisys)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider another example input by the PDPM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This RAG function can run with OpenAI or DeepSeek with a “Pinecone” keyword
    in the user message.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s output is acceptable and clearly outlines the semantic relationships
    within the message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'DeepSeek’s output is relevant as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: DeepSeek’s answer is longer and more complex. However, what would a team prefer?
    A shorter answer like OpenAI’s response or a longer one with more explanations?
    The decision can be reached through workshops and meetings.
  prefs: []
  type: TYPE_NORMAL
- en: Data retrieval (data01)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Both OpenAI and DeepSeek can be used for data retrieval. The user input for
    a product designer could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We thus have a flexible RAG system in our GenAISys that can run with the models
    we wish. However, we still have to evaluate the models for each set of tasks we
    want to perform.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CoT function operates with **Files** checked and defaults to **OpenAI**
    as the model provider. Its implementation remains consistent as it is built and
    run in the previous chapter. The key difference is that it is now integrated into
    the handler selection mechanism, which activates based on specific keywords in
    the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider anexample user input from the PDPM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output seems acceptable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now see how memory analysis will run with both model sources (OpenAI and
    DeepSeek).
  prefs: []
  type: TYPE_NORMAL
- en: Analysis (memory)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both OpenAI and DeepSeek models handle memory-based customer profiles using
    neuroscientific-style categorizations. The function has been adapted to the handler
    selection process and contains a DeepSeek call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'An example user input using the `Analysis` option in the `Reasoning` list could
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenAI’s output contains a useful segment highlighting the emotional dimension
    related to the customer’s wish for a personalized souvenir, which could help the
    product designer with their merchandise kit production endeavor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'DeepSeek’s output, however, goes off track. It first finds the right task to
    do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'But it then gets lost and seems to struggle with formatting and coherence,
    introducing irregular spacing and even foreign characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: DeepSeek can certainly do better, but improving this result would require additional
    iterations of prompt refinement or selecting a more robust DeepSeek variant or
    API. Investing time in refining prompts carries some risk, as even then, the outcome
    may not meet your expectations. Whether to refine the prompt, switch to a DeepSeek
    API, explore another DeepSeek variant, or default to OpenAI should ultimately
    be decided collaboratively within the team and based on your project’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now move on to running the generation function.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The generation function (select `Generation` in the `Reasoning` list), active
    by default with **OpenAI**, **Agent**, and **Files** checked, supports the creation
    of engaging, memory-based customer messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s consider a general user input as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenAI’s output is an appealing customer-facing message, blending nostalgia
    and merchandising suggestions, accompanied by an appropriate custom T-shirt image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 7.10: A personal image for a customer](img/B32304_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: A personal image for a customer'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now create an image.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This functionality utilizes DALL-E to generate images, with the **Files** box
    checked. The function does not change beyond being adapted to the handler-selection
    mechanism, which activates this feature with the `Create` and `image` keywords
    in the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The product designer could use it to ideate merchandising kits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a cool T-shirt that the production team could use and adapt for
    production:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11: Custom T-shirt design](img/B32304_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Custom T-shirt design'
  prefs: []
  type: TYPE_NORMAL
- en: We will now create freestyle prompts that are not triggered by any keywords
    or instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Fallback handler (memory-based)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This general-purpose handler activates when no specific instruction or keyword
    matches the input. `handle_with_memory` runs with OpenAI and DeepSeek, depending
    on the model selected. The memory of a user dialogue is set with a global variable,
    `memory_enabled`, that is initialized at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The function will return a message and stop if `memory_enabled` is set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'It will process the past messages of a user from the conversation history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the models are selected with OpenAI being the default model provider
    if no other model is selected in the IPython interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The response message is stored and returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'An example input by the PDPM could be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenAI’s answer is both acceptable and productive. Take your time to read the
    prompt and the response, which shows the transition of generative AI from NLP
    general tasks to zero-shot domain-specific tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'DeepSeek’s answer does not provide the same quality, although it contains some
    interesting points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtaining a better result would require further prompt design and output analysis
    cycles, evaluating DeepSeek models that are not distilled, such as DeepSeek-V3
    or DeepSeek-R1\. DeepSeek can surely do better, as demonstrated by using DeepSeek-R1
    on [https://chat.deepseek.com/](https://chat.deepseek.com/), which produced the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Read the prompts and outputs in this section carefully. For security and privacy
    reasons, we are using only a locally installed, distilled Hugging Face open source
    version of DeepSeek-R1\. However, you could use the online version of DeepSeek
    for certain tasks, such as the production example in this section, if you have
    the necessary permissions, just as you would with ChatGPT or any other online
    platform. Depending on your project’s specifications, you could also explore US-based
    DeepSeek APIs or alternative deployment approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, both OpenAI and DeepSeek are capable of delivering effective production
    solutions when correctly prompted and when the most appropriate model version
    is selected. Generative AI has clearly entered a new phase!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we moved further along our journey into generative AI systems.
    First, we took the time to digest the arrival of DeepSeek-R1, a powerful open
    source reasoning model known for innovative efficiency improvements in training.
    This development immediately raised a critical question for project managers:
    should we constantly follow real-time trends or prioritize maintaining a stable
    system?'
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, we developed a balanced solution by building a handler
    selection mechanism. This mechanism processes user messages, triggers handlers
    within a handler registry, and then activates the appropriate AI functions. To
    ensure flexibility and adaptability, we updated our IPython interface, allowing
    users to easily select between OpenAI and DeepSeek models before initiating a
    task.
  prefs: []
  type: TYPE_NORMAL
- en: This design allows the GenAISys administrator to introduce new experimental
    models or any other function(non-AI, ML, or DL) while maintaining access to proven
    results. For instance, when analyzing user comments, administrators can run tasks
    using the reliable OpenAI model while simultaneously evaluating the DeepSeek model.
    Administrators can also disable specific models when necessary, providing a practical
    balance between stability and innovation, which is crucial in today’s fast-paced
    AI environment.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this balance practically, we began by installing and running DeepSeek-R1-Distill-Llama-8B
    in an independent notebook, demonstrating its capabilities through production-related
    examples. We then integrated this distilled model into our GenAISys, creating
    a need for enhanced flexibility and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of the handler selection mechanism and the structured handler
    registry ensures that our system can scale effectively and indefinitely. Each
    handler follows a unified, modular format, enabling easy management, activation,
    or deactivation by administrators. We demonstrated these handlers through a series
    of practical prompts related to product design and production.
  prefs: []
  type: TYPE_NORMAL
- en: We are now positioned to expand and scale our GenAISys, adding new features
    within this adaptable framework. In the next chapter, we’ll continue this journey
    by connecting our GenAISys to the broader external world.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DeepSeek-V3 was trained with zero-shot examples. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepSeek-R1 is a reasoning model. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepSeek-R1 was first trained with RL-only. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepSeek-R1-Distill-Llama-8B is the teacher of DeepSeek-R1\. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepSeek-V3 was enhanced with DeepSeek-R1, which was derived from DeepSeek.
    (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A handler registry that contains a list of handlers for all the AI functions
    is scalable. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A handler selection mechanism that processes user messages makes the GenAISys
    highly flexible. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generative AI models such as OpenAI and DeepSeek reasoning models solve a wide
    range of problems with no additional training. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A GenAISys with a solid architecture is sufficiently flexible to be expanded
    in terms of models and tasks to perform. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*DeepSeek-V3 Technical Report*. [https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, Ł., & Polosukhin, I. (2017). *Attention Is All You Need*. Advances in
    Neural Information Processing Systems, 30, 5998–6008\. Available at: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepSeekAI, Daya Guo, Dejian Yang, et al: [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., & Lample, G. (2023). *LLaMA: Open and Efficient Foundation Language
    Models*. [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,
    Letman, A., Mathur, A., Schelten, A., Vaughan, et al. (2024). *The Llama 3 Herd
    of Models*. [https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/inference-endpoints/en/security](https://huggingface.co/docs/inference-endpoints/en/security)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsloth AI: [https://unsloth.ai/](https://unsloth.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Frantar, E., Ashkboos, S., Hoefler, T., & Alistarh, D. (2023). *GPTQ: Accurate
    Post-Training Quantization for Generative Pre-trained Transformers* [https://arxiv.org/abs/2210.01774](https://arxiv.org/abs/2210.01774))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA’s data center GPUs: [https://www.nvidia.com/en-us/data-center/](https://www.nvidia.com/en-us/data-center/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Unlock this book’s exclusive benefits now
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scan this QR code or go to [packtpub.com/unlock](http://packtpub.com/unlock),
    then search for this book by name. | ![A qr code on a white background  AI-generated
    content may be incorrect.](img/Unlock.png) |
  prefs: []
  type: TYPE_NORMAL
- en: '| *Note: Keep your purchase invoice ready before you start.* |'
  prefs: []
  type: TYPE_TB
