<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-167"><a id="_idTextAnchor366"/>10</h1>
<h1 id="_idParaDest-168"><a id="_idTextAnchor367"/>Conclusions and Reflections</h1>
<p>We have been on quite a journey throughout this book. I’ve learned a lot, and I hope you have as well. I've had the chance to revisit my love of robotics and spend a lot of time examining the <a id="_idIndexMarker801"/>state of the art of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) and robot design to try and find a way to explain the concepts to you in an easily digestible form.</p>
<p>In this chapter, we will discuss finishing our robot. I’ll provide you with some advice on careers in AI and robotics. We will also talk a little about the future of AI, at least as I see it, and finish with a discussion about risk.</p>
<p>The following main topics will be covered in this chapter:</p>
<ul>
<li>Learning when to stop</li>
<li>Careers in robotics</li>
<li>Exploring the current state of AI</li>
<li>Understanding risk in AI</li>
</ul>
<h1 id="_idParaDest-169"><a id="_idTextAnchor368"/>Learning when to stop</h1>
<p>Over the last nine chapters, we’ve worked on designing a specific robot to perform a specific task. We designed <em class="italic">Albert</em> the robot to pick up toys in an unstructured environment, namely a family home. To do this, it needed to be able to recognize toys with a camera, pick up <a id="_idIndexMarker802"/>toys with a robot arm and hand, navigate inside a house, and deposit toys in a toy box. We also added interaction, teaching the robot to listen and react to commands. Finally, it received an artificial personality and simulated emotions.</p>
<p>So, the next question is, are we finished with designing, building, and testing our robot? Sometimes, the most difficult part of designing, building, and testing a robot is determining when it is finished. Quite often, I see that some little thing might be improved, or some detail added, or some feature enhanced. Oh, the robot needs a spotlight. It would be nice if it could remember what toys are in the toy box. What if it had two arms? And so on. You can continue in this vein forever, tinkering and adding, never quite finishing anything.</p>
<p>The solution to this is to set specific goals and measure against them. For example, we want the robot to pick up toys. How many toys? All of the ones that are on the floor. So, we can run several trials and, if at the end of each test, all of the toys have been picked up, that is finished. But what if not all of the toys have been picked up? What is good enough? How about we say if we can pick up any remaining toys in one pass by holding them all in our hands, then that is acceptable. What level would not be acceptable? Well, picking up no <a id="_idIndexMarker803"/>toys at all is not acceptable. What about half? Would we consider that OK? I’d say probably not. You can continue this self-conversation until you say with some clarity where the finish line is. Then, once you’ve crossed it, you’re done.</p>
<p>Sometimes, we have to temper expectations when we’re trying something new or innovative. Expect to spend some time ironing the bugs out. I’ve had the experience of creating something and having the robot do something new and unexpected that ended up being far better than what I started with. For example, I created a robot that had a fairly sophisticated <em class="italic">follow me</em> function. It used body recognition to identify humans, and then, when commanded, followed behind one person, even through a crowd – so long as it could always keep the person it was following in its field of view. The robot was programmed to keep a six-foot distance from the person. This means if you walked toward it, instead of away, it backed up. You could then steer the robot backward in any direction simply by walking toward it – my <em class="italic">follow me</em> function became a <em class="italic">walk </em><em class="italic">ahead</em> function.</p>
<p>Other times, an innovation just does not work, and at some point, it must be abandoned. Generally, you can tell this when you have to keep adding <em class="italic">crutches</em> or workarounds over and over again. The software becomes more complex and fragile with each workaround. I had this problem with a sonar-based obstacle avoidance system – the sonar sensor I used was just too unreliable and was very sensitive to the surface involved – for example, it could not see (get any echoes) from polished wooden doors. After a few <a id="_idIndexMarker804"/>weeks of testing, we abandoned that sensor and went with another light-based sensor called an <strong class="bold">Infrared Proximity Detector</strong> (<strong class="bold">IRPD</strong>) that worked much more reliably.</p>
<p>In the next section, we’ll take a look at some career paths that you could take if you are interested in robotics and/or <a id="_idTextAnchor369"/><a id="_idTextAnchor370"/><a id="_idTextAnchor371"/>AI.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor372"/>Careers in robotics</h1>
<p>I am often asked what sorts of skills or degrees robot designers need to have, or what courses they <a id="_idIndexMarker805"/>should take. I meet a lot of young people at robot competitions, student events, conferences, and recruiting trips. A lot of the advice I give people I have put into this book <a id="_idIndexMarker806"/>already – especially now that AI, neural networks, <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>), expert systems, chatbots, navigation, and image processing are all important. You need to understand that <em class="italic">robotics</em> is an umbrella term that covers a lot of industries and a lot of skill <a id="_idTextAnchor373"/>sets.</p>
<p>Who uses robotics? The range of applications is continuing to grow every day. These include the following:</p>
<ul>
<li>In the medical field, there is robot-assisted surgery, robotic prosthetic limbs, exoskeletons helping paraplegics to walk, and implantable devices to help people hear and see.</li>
<li>We have self-driving cars and self-flying airplanes and helicopters in transportation (which is what I do).</li>
<li>We have robots delivering packages on the sidewalks in San Francisco, and a few companies testing parcel delivery by aerial drone in Africa, Switzerland, Texas, and other places.</li>
<li>We have self-driving cars being tested in several countries. Safety features that debuted in the <em class="italic">DARPA Grand Challenge</em> robot car race in the United States and were developed for autonomous cars – lane keeping, adaptive cruise control, driver assistance, and self-parking – are now common features on even base-level <a id="_idIndexMarker807"/>automobiles. There are over 80 companies currently developing some sort of electric <strong class="bold">vertical takeoff and landing</strong> (<strong class="bold">VTOL</strong>) manned vehicle, every one of which uses advanced automation and autonomy as part of its control system. In far western Australia, Rio Tinto Mining has developed the <em class="italic">Mine of the Future</em> in Pilbara, where 80 autonomous trucks are remotely operated from Perth, 1,500 kilometers away.</li>
</ul>
<p>The future of robotics is just being written, and you, reading this book, may play a part in determining the path it takes.</p>
<p>So, what skills are required to design and make robots like the ones I just described? The truth is that a modern robot company would employ just about every skill imaginable. Let’s look <a id="_idIndexMarker808"/>more closely at some of these skills:</p>
<ul>
<li>Even the simplest <a id="_idIndexMarker809"/>robot takes <strong class="bold">mechanical designers</strong> to develop the parts, gears, and levers to make robots move and help package the electronics.</li>
<li><strong class="bold">Electrical engineers</strong> work with batteries and motors.</li>
<li><strong class="bold">Radiofrequency</strong> (<strong class="bold">RF</strong>) <strong class="bold">engineers</strong> and <strong class="bold">technicians</strong> work with radios and datalinks <a id="_idIndexMarker810"/>that are used to connect mobile robots to their control stations (such as an <strong class="bold">unmanned aerial </strong><strong class="bold">vehicle</strong> (<strong class="bold">UAV</strong>)).</li>
<li><strong class="bold">Cognitive specialists</strong> design AI routines, develop robot emotions, and harness machine learning techniques, as well as design user interfaces.</li>
<li><strong class="bold">Writers</strong> and <strong class="bold">artists</strong> craft voice routines, write dialogue, design user interfaces, write manuals and documentation, and add creative touches to the inside and outside of the robot.</li>
<li><strong class="bold">Managers</strong> and <strong class="bold">supervisors</strong> track budgets and schedules.</li>
<li><strong class="bold">Supply specialists</strong> work with suppliers, parts, build-to-print shops, electronics warehouses, and salesmen to get the parts to put together the assembly line.</li>
<li>Industrial <a id="_idIndexMarker811"/>robots are managed by special types of <strong class="bold">programmers</strong> who use <strong class="bold">programmable logic arrays</strong> (<strong class="bold">PLAs</strong>) and <strong class="bold">ladder logic</strong> to control robot arms that paint and assemble components. This type of programming emulates how robots were originally designed using relays and switches.</li>
<li><strong class="bold">Bookkeepers</strong> and <strong class="bold">accountants</strong> make sure the bills, as well as the employees, are paid.</li>
<li><strong class="bold">Salespeople</strong>, <strong class="bold">marketing</strong>, and <strong class="bold">customer relations</strong> teams get the product sold and keep the custom<a id="_idTextAnchor374"/>ers happy.</li>
</ul>
<p>All of these skills have to be present in some form in a professional roboticist, particularly if you think you will run your own company. I’ve been part of every size of robot project, from one <a id="_idIndexMarker812"/>person to thousands. Each has its strengths and weaknesses, but as a robot designer, you can be sure that you will be the center of the storm, making the magic happen, solving problems, and turning ideas into physical form. To me, there is no more satisfying moment than seeing my creation driving or flying around, doing its job, and knowing that all the hard work, research, coding, mechanics, sleepless nigh<a id="_idTextAnchor375"/>ts, smashed fingers and toes, and skipped meals were worth this result.</p>
<p>Now, let’s talk a bit about the hype surrounding robotics and AI that you have probably seen in<a id="_idTextAnchor376"/> the news.</p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor377"/>Exploring the current state of AI</h1>
<p>There is a lot of hype going on right now in the intersecting worlds of AI and robotics. And a lot of it is just exaggeration.</p>
<p>One common <a id="_idIndexMarker813"/>myth is that robots are taking jobs away from people. In truth, robots and automation free up workers to do more productive tasks. The truth of this can be seen in job statistics – unemployment in the US is at a 50-year low (<a href="https://www.wsj.com/articles/january-jobs-report-unemployment-rate-economy-growth-2023-11675374490">https://www.wsj.com/articles/january-jobs-report-unemployment-rate-economy-growth-2023-11675374490</a>), despite massive improvements in factory automation. However, according to <em class="italic">The Harvard Business Review</em>, the improved productivity of robotics creates more jobs than it removes (<a href="https://hbr.org/2021/03/why-robots-wont-steal-your-job">https://hbr.org/2021/03/why-robots-wont-steal-your-job</a>). The overall level of employment has <em class="italic">increased</em>, not gone down because of automation and increased productivity.</p>
<p>I do recognize that the modern worker, even someone like myself, who works in technology, must be ready and willing – at any age – to retrain themselves and to learn and adapt to new ways of working, new economies, and new opportunities. I’ve had to completely retrain myself at least six times as new markets were invented and new technologies emerged. Sometimes, there is a <em class="italic">second wave</em> where some technology was invented but then disappeared when it was too expensive for the benefits it provided, or the proper hardware had not been invented yet. Neural networks fit into that category, as does virtual reality, which was a big deal in 1999, and has now re-emerged with very small high-resolution screens that were developed for cell phones.</p>
<h2 id="_idParaDest-172">Looking ahead in AI a<a id="_idTextAnchor378"/>nd robotics</h2>
<p>I’m quite interested in the long-term impact of what has been called the <em class="italic">sharing economy</em>, where <a id="_idIndexMarker814"/>companies such as Uber, Lyft, and Airbnb create value by connecting suppliers and consumers on a massive scale without owning any of the capital or resources to provide any services. All of this is enabled and made possible by <a id="_idIndexMarker815"/>the ubiquitous internet, which continues to grow and evolve at a rapid pace. The availability of the internet has allowed the general public or an individual student to have access to supercomputer-level capabilities to run AI programs such as ChatGPT online, which are too large to fit in a home computer, smartphone, or tablet. I often use the term, “<em class="italic">but that’s a decade in internet years</em>” while referring to some idea that is maybe 24 months old to indicate the rapid turnover in internet tech. This trend will continue. It will be interesting to see if anyone owns a car in 20 years, or only a subscription to a car service.</p>
<p>Another trend that has become very interesting is the lowering of barriers to entry in a lot of businesses. You used to have to have an enormous machine shop and giant machines to make precision machine parts – before 3D printers came and put that capability <a id="_idIndexMarker816"/>on your desktop. <strong class="bold">Generative AI</strong> is AI that can synthesize writing and drawing and can also directly create music, write programs and software, provide advice, and assist users in writing scripts, drawing pictures, and making animations powered by only a text prompt. Do you want to make movies? You can do so on an iPhone. Do you want to start a recording studio? The parts for professional results (with a large amount of effort) are available for less than $200 and you can use AI to generate lyrics, chord progressions, arpeggios, or even song ideas.</p>
<p>One item <a id="_idIndexMarker817"/>that fits into that category of lowering barriers to entry is drones or small UAVs. When I started making UAVs, a decent <strong class="bold">global positioning system</strong> (<strong class="bold">GPS</strong>) and <strong class="bold">inertial measurement unit</strong> (<strong class="bold">IMU</strong>) – the things that make unstable <a id="_idIndexMarker818"/>quadcopters possible to control – cost tens of thousands to hundreds of thousands of dollars. The real breakthrough in drone technology did not come from aviation, but rather from our cell phones. The developments in cell phones enabled companies to invest billions of dollars in making the next cell phone, smartphone, hand-held computer pacifier, or whatever you would want to call it. The convergence of very small radios, very small GPSs, and very, very small accelerometers enabled an entire world of unmanned flying objects – quadcopters, gliders, airships, airplanes, and hybrid VTOL craft – to emerge. That, along with higher density batteries that came from (you guessed it) cell phones and laptops, allowed people to discover that if you put enough power on it, you can make almost anything fly, including you.</p>
<p>The secret to the flying quadcopter’s amazing success is that the tiny accelerometers (which measure changes in movement) and tiny gyroscopes (which measure orientation changes) became cheap and readily available. Without these sensors, and the robotics algorithms <a id="_idIndexMarker819"/>that control them, quadcopters are unstable and impossible to control. Another reason for the quadcopter’s success is that it uses only the throttle setting – the speed of the motors – to control all its aspects of flight, including stability. This compares with the very complicated collective controls and cyclic pitch controls that make a helicopter work. You can see the difference between a radio-controlled helicopter, which is very expensive and only a few people can fly, and a quadcopter, which is quite cheap and can be flown by anyone, with the help of a computer and some sensors. You can add a drone autopilot to a collective/cyclic radio-controlled helicopter and end up with a very controllable drone helicopter. Quadcopters and more complex flying machines use AI for stabilization, adaptive flight control, object recognition, and obstacle avoidance.</p>
<p>The other side of these advances in AI and robotics has also resulted in a backlash from some people and can be describe<a id="_idTextAnchor379"/><a id="_idTextAnchor380"/><a id="_idTextAnchor381"/>d as robot phobia.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor382"/>Is AI phobia reasonable?</h2>
<p>You have probably seen some blazing headlines on the internet from various very credible <a id="_idIndexMarker820"/>sources saying some incredible things.</p>
<p>About a decade ago, Stephen Hawking, a leading scientist, stated that “<em class="italic">The development of full artificial intelligence could spell the end of the human race... It will take off on its own and re-design itself at an ever-increasing rate… Humans, who are limited by slow biological evolution, can’t compete, and will be superseded</em>” (<a href="https://www.bbc.com/news/technology-30290540">https://www.bbc.com/news/technology-30290540</a>). This quote is frequently used even today by critics of AI.</p>
<p>More recently, Elon Musk suggested that AI could lead to <em class="italic">civilization destruction</em>, although he has invested significantly in the growth of AI (<a href="https://edition.cnn.com/2023/04/17/tech/elon-musk-ai-warning-tucker-carlson/index.html">https://edition.cnn.com/2023/04/17/tech/elon-musk-ai-warning-tucker-carlson/index.html</a>).</p>
<p>Bill Gates, former chairman of Microsoft, takes a more middle ground, stating that AI presents both promise and concerns. In an open letter, he elaborated on AI’s potential but also discussed the risks of developing this tech. He wrote, “<em class="italic">The world needs to establish the rules of the road so that any downsides of artificial intelligence are far outweighed by its </em><em class="italic">benefits</em>” (<a href="https://www.forbes.com/sites/qai/2023/03/24/bill-gatess-open-letter-suggests-ais-potential-is-both-exciting-and-terrifying/">https://www.forbes.com/sites/qai/2023/03/24/bill-gatess-open-letter-suggests-ais-potential-is-both-exciting-and-terrifying/</a>).</p>
<p>First, let me clarify that I do not think I’m in the same league as these gentlemen. But what I am is someone who works every day at the leading edge of AI and unmanned vehicles and robotics, and who attends and speaks at a lot of AI conferences. The source of this fear of AI can be found in any bathroom, hanging on the wall – we are using our mirrors to reflect our motivations and desires onto our creations.</p>
<p>I would liken <a id="_idIndexMarker821"/>the state of the art of AI today to a teenager who has managed to strap a V-8 engine to a skateboard – an enormous amount of power with no way to use it. While computers, hardware, and networks have advanced and scaled at an amazing rate, we are still building software the same way today as we did 20 years ago, and many of the advances of AI were invented 40 or 50 years ago. For instance, neural networks were invented in the 1940s (<a href="https://link.springer.com/article/10.1007/BF02478259">https://link.springer.com/article/10.1007/BF02478259</a>). Genetic algorithms were suggested by Alan Turing in the 1950s. We have been able to make some advancements in applying learning theory now that we have lots of data (thank you, internet) and lots of bandwidth, but we have not created any startling new concepts or capabilities that show any sort of <a id="_idIndexMarker822"/>breakthrough in AI. Recent work in generative AI –such as <strong class="bold">ChatGPT</strong> and <strong class="bold">Dall-E</strong> image generation – has created what might be described as a computer version <a id="_idIndexMarker823"/>of an imagination (creating data stepwise out of noise), which indeed is a step forward, but still, a generative AI such as ChatGPT is doing what neural networks did before: predict the next word in a sentence.</p>
<p>What is generative AI? In this case, we can ask the AI itself for a definition. The following text was written by an AI program called ChatGPT4 in response to the prompt, “<em class="italic">Can you provide me with a definition of </em><em class="italic">generative AI?</em>”:</p>
<p class="uthor-quote">“Generative AI refers to a subset of artificial intelligence technologies that can generate new content, such as text, images, music, or code, based on the patterns and information it has learned from its training data. This type of AI uses machine learning algorithms, particularly generative models such as generative adversarial networks (GANs) or transformers, to understand and replicate the complexities of human-generated content.</p>
<p class="uthor-quote">“The “generative” aspect implies that the AI isn’t just analyzing or processing existing information but actually creating something new and previously unseen. For instance, in the case of text, generative AI can write essays, poems, or code; for images, it can create artworks or realistic photos of things that don’t exist; in music, it can compose new pieces in various styles.”</p>
<p>But is this <a id="_idIndexMarker824"/>what is referred to as <strong class="bold">general AI</strong> (software with human intellig<a id="_idTextAnchor383"/>ence)? I don’t think so. While the hardware and software are <a id="_idIndexMarker825"/>advancing, I simply don’t see any path forward that leads to the type of problems that the esteemed Mr Gates, Mr Musk, or Dr Hawking suggested. Why not? We’ll focus on this in the next section.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor384"/>Comparing the needs of humans and AI</h2>
<p>The reason why I don’t see significant problems is because robots don’t have needs. Humans have <a id="_idIndexMarker826"/>needs and ambitions to exist. We are encased in a frail flesh cell, what William Burroughs called the <em class="italic">Soft Machine</em>. We must provide air, food, water, shelter, and clothing to protect our fragile shells and <a id="_idIndexMarker827"/>interact with other soft machines (people) to reproduce and make more of ourselves. You can argue, as Richard Dawkins did in his book <em class="italic">The Selfish Gene</em>, that all of this is simply an evolved way for our DNA to perpetuate itself, and that we are simply the product of our biological programming. It is impossible to separate a human from their needs – if we don’t, we die in a matter of minutes. It is our needs that drive us forward, to come out of the trees, to learn to farm, to build cities, and to make civilizations.</p>
<p>Robots, on the other hand, do not have needs as a condition of their existence. They are just sets of instructions we have set down in electronics – golems with words in their heads that make them move (as described in the book <em class="italic">Feet of Clay</em>, by Terry Pratchett). If we don’t provide food to them – nothing happens. If we don’t use them – nothing happens. If we forget them for a week and check on them later, they are still the same.</p>
<p>First, let’s discuss what humans’ needs are. Maslow came up with the <strong class="bold">hierarchy of needs</strong> back in 1943, and he has been quoted ever since. Maslow says that we not just have needs, but they form a hierarchy – the more important needs are at the bottom while the more abst<a id="_idTextAnchor385"/>ract needs are at the top. We only worry about the need at any given level when all of the needs below it are satisfied, as shown in the following diagram:</p>
<div><div><img alt="Figure 10.1 – Hierarchy of needs for humans" src="img/B19846_10_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Hierarchy of needs for humans</p>
<p>Let’s look <a id="_idIndexMarker828"/>at this pyramid of needs in detail:</p>
<ul>
<li>At the <a id="_idIndexMarker829"/>bottom are the <em class="italic">physical needs</em> – air, food, water, and clothing.</li>
<li>The next level is <em class="italic">security</em> – we need to feel secure from predators or from other humans wanting to harm us.</li>
<li>Above the security needs are <em class="italic">social needs</em> – to be in a group or part of society. Humans want to belong to a family, a community, a nation. This drive is very strong, as our invention of war to protect our society attests.</li>
<li>Next, we have <em class="italic">ego needs</em> – the need to be recognized, to be special, to stand out from the crowd we fought so hard to be part of. Remember, we only get to express this once all the other needs are taken care of, so you only worry about recognition once you are part of a group.</li>
<li>Our final need is called <em class="italic">self-actualization</em>, as described by Maslow – we would call it self-improvement, or the drive to improve one’s self. This is where we get athletes, artists, musicians, <a id="_idTextAnchor386"/>and people who write books.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">It has been a running joke between my wife and me that every textbook we ever read in college contained some reference to Maslow and his hierarchy of needs. This was quite unusual since I studied math and engineering, and my wife’s degree is in human resources. I appreciate the irony that this book now adds to that list of books that reference Maslow.</p>
<p>Now, let’s look <a id="_idIndexMarker830"/>at machine intelligence and imagine <a id="_idIndexMarker831"/>for a moment what a set of robot needs might look like. I found this an interesting thought experiment – we make a baby robot that is fully capable of learning anything a baby human (or baby mouse, or baby cricket) can. What needs would it have? Let’s look at a modified version of Maslow’s hierarchy of needs:</p>
<div><div><img alt="Figure 10.2 – Hierarchy of needs for robots" src="img/B19846_10_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Hierarchy of needs for robots</p>
<p>We can break this down as follows:</p>
<ul>
<li>In contrast to a robot’s needs, hunger is built into humans’ biology. However, in the case of an AI system, we, the creators, would need to build it in. That would equate, as we did in our artificial personality, to <em class="italic">electrical power</em> or <em class="italic">battery life</em>.</li>
<li>The next level of needs would be the <em class="italic">goals</em> and <em class="italic">tasks</em> for which the AI was created.</li>
<li>The next level up could potentially be <em class="italic">curiosity</em> and the need to explore – our AI system would have a drive to acquire more data, or to get access to more resources. Once a robot has data, this gives it the basis to get more data, and so on.</li>
<li>The next level of needs we would endow to our AI would be the need for <em class="italic">friendship</em> or <em class="italic">communication</em>, either with other robots or with people.</li>
<li>Finally, we could give our robot the need for <em class="italic">learning</em>, or to acquire and learn new skills and techniques – to grow as a robot.</li>
</ul>
<p>You may have noticed that we did not cover these subjects in this book, nor any other. We did not <a id="_idIndexMarker832"/>talk about giving the robot needs, only how to simulate emotions and some rules for conversation that make no sense to <a id="_idIndexMarker833"/>the robot at all. It gets no joy from telling a terrible joke to a 7-year-old because it does not know what any of those concepts are. It just sends electrons down one path or another because we tell it to. The only intelligence here is the reflection and imagination of us, the robot designers. Everything else is an illusion – a magician’s trick.</p>
<p>I get this sort of question quite a lot and felt that I could give you some of my answers if that helps arm you against the critics of AI. The bottom line is that I simply do not worry about AI taking over the world. I don’t mean to say that there will never be a general AI, just I don’t see one coming about in the foreseeable future.</p>
<p>With this context under our belt, let’s discuss how to und<a id="_idTextAnchor387"/><a id="_idTextAnchor388"/>erstand and manage risk in AI.</p>
<h1 id="_idParaDest-175"><a id="_idTextAnchor389"/>Understanding risk in AI</h1>
<p>One subject I talk <a id="_idIndexMarker834"/>about frequently at conferences and in print is the risk of AI in terms of <em class="italic">trust</em> and <em class="italic">control</em>. I’m not talking about AI running amok here, but rather how to make AI dependable. It is quite interesting that the sort of AI we have been considering – specifically, ANNs – does something very few other computer software do. Given the same inputs and conditions, the output of an AI system is not always the same. Given the same inputs, the AI system will sometimes come up with a different answer. The formal name for th<a id="_idTextAnchor390"/>is behavior is <strong class="bold">non-determinism</strong>.</p>
<p>There is a second corollary to this. Given the same inputs, the AI process will sometimes take a different amount of time to complete its task. This is simply not normal behavior for a computer.</p>
<p>Admittedly, we are not using AI to get answers to math questions such as <em class="italic">2+2</em>, but rather how to do things such as diagnose a cancer tumor or recognize a pedestrian in a crosswalk for a self-driving car. How do we deal with a computer output that may be wrong? You can verify this for yourself – look at the examples we covered when we performed training on neural networks. Did we ever achieve 100% success from a training run, where we got all of the answers right? No, not once. This is because ANNs are <em class="italic">universal approximation functions</em> that map inputs – which can be quite complex – to outputs. They do this by dealing with probabilities and averages, which were developed over time. You can think of an artificial neuron as a probability engine that says, <em class="italic">45 out of the last 50 times I got this set of inputs, the output was supposed to be true. The odds are it will be true this time</em>. And it sets itself to true. We may have millions of little artificial neurons in our network, each of them making the same sort of calculation. The net result is making a very educated guess about the answer.</p>
<p>For most applications <a id="_idIndexMarker835"/>of our neural networks, this is acceptable behavior. We are classifying pictures, and it is acceptable if a few are wrong. We do a Google search for platypus, and we get one picture out of 100 Platypus brand tennis shoes. That is OK for a Google search, but what if we were doing something more serious, such as recognizing pedestrians in a self-driving car? Is it OK if we misidentify one pedestrian out of 100 and don’t avoid them? Of course not. That’s why, right now, we don’t allow AI systems in such critical functions. But people want to use AI in this way – in fact, quite a lot. It would be great to have an AI system that recognizes geese in flight and tells your airliner how to avoid them. It would be great to have an AI system recognize that a patient was misdiagnosed in the hospital and needed immediate attention. But we can’t do that until we come up with processes for dealing with the non-deterministic and thus non-reliable nature of AI.</p>
<p>Currently, we deal with non-deterministic elements in automobiles all of the time. They are called drivers. It is widely believed that the vast majority of car crashes are caused by the human element, which is why we need self-driving cars with a better percentage. How do we deal with human drivers? Let’s look at the necessary criteria for a driver:</p>
<ul>
<li>We require them to be a certain age, which means they have gained experience</li>
<li>They have to pass a test, demonstrating competency in accomplishing tasks</li>
<li>They have to demonstrate compliance with rules and regulations by passing a knowledge test</li>
<li>They have to get periodically re-certified by renewing their license</li>
<li>We also require seat belts and airbags to partially mitigate the risk of the human driver making mistakes by reducin<a id="_idTextAnchor391"/>g some of the resulting injuries</li>
</ul>
<p>We can apply these types of criteria to AI. We can require a certain amount of training cases. We can <a id="_idIndexMarker836"/>test and demonstrate a level of competency. We can predict the level of errors or mistakes in advance and put measures in place to mitigate that risk. Perhaps we can have two AI systems – one that detects obstacles and another that has been trained to recognize that the first AI has made a mistake. If we have a 90% chance of the first AI being right, and another 90% chance of the second AI being right, then we have a <em class="italic">90% + (90% of 10%) = 99%</em> chance of avoidance.</p>
<p>I think the key to using AI in safety-critical applications<a id="_idTextAnchor392"/> is being able to predict risk in advance, and designing in advance to mitigate either the<a id="_idTextAnchor393"/><a id="_idTextAnchor394"/> cause of the risk or the effect.</p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor395"/>Summary</h1>
<p>In this final chapter of this book, we summarized our journey through robotics and AI. We talked about robotics as a career and discussed AI robotics as a profession. I brought up some issues regarding the future of AI, both real and imaginary. Drones and self-driving cars are real; robots taking jobs from humans or taking over the world is imaginary, at least in my opinion. I talked about robots and AI not having needs, and thus lacking the motivation, pressure, or even capability to evolve. Finally, we talked about risk in AI and how to recognize it. I hope that this information gives you some guidance in your interest in robotics and AI and provides some <em class="italic">insider information</em> from a practitioner in this area.</p>
<p>Now that we’re almost at the end of this book, I want to thank you for coming on this journey with me. I hope you have learned something along the way, even if it is just to know more questions to ask. I encourage you to dive in and build your own robot, learn about AI, and become part of the community of people who contribute to robotics as a hobby or a profession.</p>
<p>I have to acknowledge a debt of gratitude to all of the robotics and AI open source communities for making all of this material, knowledge, and expertise available, and continuing to make AI the poster child for why open source, as a model for the advancement of human knowledge, works and works well. ROS, which is entirely run by volunteers, is a case in point as it make<a id="_idTextAnchor396"/><a id="_idTextAnchor397"/>s building robots so much easier.</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor398"/>Questions</h1>
<ol>
<li>Given that we started the chapter on a light note and ended up talking about robot phobia and philosophical questions about existence, do you feel that AI is a threat? Why or why not?</li>
<li>List five professions that would be necessary to turn our Albert robot into a product company.</li>
<li>If we imagine a company that was going to put Albert the robot into production, would it need a psychologist? For the robot, or for the humans?</li>
<li>What components found in cell phones or smartphones are also found in quadcopters?</li>
<li>Why are AI systems, specifically ANNs, naturally non-deterministic in terms of results and time?</li>
<li>What might be a practical application of an AI system that predictably makes mistakes?</li>
<li>If an AI system was picking stocks for you and predicted a winning stock 43% of the time, and then you had a second AI that was 80% accurate at determining when the first AI had <em class="italic">not</em> picked a good stock, what<a id="_idTextAnchor399"/> percent of the time would the AI com<a id="_idTextAnchor400"/>bination pick a profitable stock?</li>
</ol>
<h1 id="_idParaDest-178"><a id="_idTextAnchor401"/>Further reading</h1>
<ul>
<li><em class="italic">The Organization of Behavior</em>, by Donald Hebb, Wiley.</li>
<li><em class="italic">Computing Machinery and Intelligence</em>, by Alan M. Turing in the journal <em class="italic">Mind</em>. Vol. LIX (238).</li>
<li><em class="italic">Feet of Clay</em>, by Terry Pratchett, published by HarperCollins, London 2009. This book discusses the fictional concept of golems, which are clay creatures that are <em class="italic">programmed</em> by a set of instructions written on paper and put into their heads, an interesting analog to robots.</li>
<li><em class="italic">A Theory of Human Motivation</em>, by A.H. Maslow in the journal <em class="italic">Psychological Review</em>, vol. 50 (4).</li>
<li>US Dept. of Transportation. <em class="italic">National Motor Vehicle Crash Causation </em><em class="italic">Survey</em>. <a href="https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811059">https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811059</a>.</li>
<li><em class="italic">Attacking Faulty Reasoning: A Practical Guide to Fallacy- Free Arguments</em>, by T. Edward Damer, Cengage Learning.</li>
<li><em class="italic">Modern Generative AI with ChatGPT and OpenAI Models</em>, by Valentina Alito, Packt Publishing.</li>
</ul>
</div>


<div><h1 id="_idParaDest-179"><a id="_idTextAnchor402"/>Answers</h1>
<h1 id="_idParaDest-180"><a id="_idTextAnchor403"/><a href="B19846_01.xhtml#_idTextAnchor015">Chapter 1</a></h1>
<ol>
<li>What does the acronym PID stand for? Is this considered an AI software method?<p class="list-inset"><strong class="bold">PID</strong> stands for <strong class="bold">Proportional, Integral, Derivative</strong>, and is a type of closed-loop controller that does not require a model (simulation) to operate. PID is not an AI method because there is no learning or adaptation involved in the decision-making. PIDs are very useful control techniques and are widely used to control motors and thermostats.</p></li>
<li>What is the Turing Test? Do you feel that this is a valid method of assessing an artificial intelligence system?<p class="list-inset">The <strong class="bold">Turing Test</strong>, originally called <em class="italic">The Imitation Game</em> by Alan Turing, is an imaginary test, or thought experiment, in which a person communicates with someone or something via a <strong class="bold">teletype</strong> (or a text message, for you younger people). An AI would pass the Turing Test if the person was unable to tell whether the entity they were communicating with was a human or a robot. The Turing Test has been passed by modern AI-based chatbots and generative AI engines such as ChatGPT, and new intelligence tests are being created (<a href="https://www.nature.com/articles/d41586-023-02361-7">https://www.nature.com/articles/d41586-023-02361-7</a>).</p></li>
<li>Why do you think robots have a problem in general with negative obstacles such as stairs and potholes?<p class="list-inset">It is difficult to see negative obstacles (holes, drop-offs, stairs going down, etc.) using the robot’s sensors, which have an easier time with positive (going up) obstacles. Usually, cameras and LiDAR can only see part of a negative obstacle due to the bottom being obscured (not visible). It is sometimes easier to reason about negative obstacles by seeing their shadow – the area that the sensor cannot see. The following diagram shows how a robot perceives a hole:</p></li>
</ol>
<div><div><img alt="Figure 11.1 – How a robot perceives a hole or negative obstacle" src="img/B19846_11_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – How a robot perceives a hole or negative obstacle</p>
<p class="list-inset">The following diagram shows how a robot perceives stairs:</p>
<div><div><img alt="Figure 11.2 – How a robot sees stairs" src="img/B19846_11_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – How a robot sees stairs</p>
<ol>
<li value="4">In the OODA loop, what does the orient step do?<p class="list-inset">In the <em class="italic">orient</em> step, all of the data is put into the same reference frame, which is usually the robot’s central point of view. This allows the robot to determine which data is relevant to decision-making. It is the most important step in decision-making.</p></li>
<li>From the discussion of Python advantages, compute the following: You have a program that needs 50 changes tested. Assuming each change requires one run and a recompile step to test. A Make compile and build in C takes 450 seconds and a Python <code>run</code> command takes three seconds. How much time do you sit idle waiting for the C compiler?<p class="list-inset">Using Python as an interpreted language can save a lot of time on very complex builds, where a C/C++ compiler and link can take 20 minutes or more. The C program test cycle in question would take 6.25 hours to complete, while the Python test program would take 2.5 minutes.</p></li>
<li>What does RTOS stand for?<p class="list-inset"><strong class="bold">RTOS</strong> stands for <strong class="bold">real-time operating system</strong>. This is an operating system that enforces time limits and processing partitions in the OS itself. An RTOS is a deterministic operating system that always executes a task in the exact same amount of time.</p></li>
<li>Your robot has the following scheduled tasks: telemetry at 10 Hz, GPS at 5 Hz, inertial measurements at 50 Hz, and motor control at 20 Hz. At what frequency would you schedule the base task, and what intervals would you use for the slower tasks (i.e., 10 Hz base, motors every three frames, telemetry every two frames, etc.)?<p class="list-inset">You need a number that all of the rates (10, 5, 50, 20) divide into evenly. The smallest number that fits is 100 Hz. I would also accept 50 Hz if the student assumed that the 20 Hz would update two times in one frame and three times in the next frame, which is cheating a little but a common adaptation for a real-time system.</p></li>
<li>Given that a frame rate scheduler has the fastest task at 20 frames per second, how would you schedule a task that needs to run at 7 frames per second? How about one that runs at 3.5 frames per second?<p class="list-inset">As given in the previous questions, there does not have to be the same number of samples in each frame in order to come out with a constant frame rate as long as there is a multiple of the base frame rate that every sample divides into. In this case, <em class="italic">20 x 7 = 140</em>, so the 7 Hz can run at a 20 Hz base rate, and it will repeat patterns every 140 frames, or 7 seconds. Half of 7 is 3.5 and can run at the same base rate with a pattern that repeats every 70 frames, or 3.5 seconds.</p><p class="list-inset">Each update would be 5.7 frames apart, which gets rounded up to 6.</p></li>
<li>What is a blocking call function? Why is it bad to use blocking calls in a real-time system such as a robot?<p class="list-inset">A <strong class="bold">blocking call</strong> suspends the execution of your program until an interruption or event occurs, such as receiving a datagram or UDP packet. These are bad because you lose control of your program timing and cannot maintain a soft real-time execution. Use <strong class="bold">polling calls</strong> instead for serial ports and network interfaces. A polling call looks for data on the interface and then continues when no data is available.</p></li>
</ol>
<h1 id="_idParaDest-181"><a id="_idTextAnchor404"/><a href="B19846_02.xhtml#_idTextAnchor032">Chapter 2</a></h1>
<ol>
<li value="1">Name three types of robot sensors.<p class="list-inset">A sensor is anything that conveys data from the outside world to the robot. Sensors mentioned in the text include the following:</p><ul><li>Sonar sensors</li><li>Cameras</li><li>Microphone</li><li>Buttons</li><li>Analog-to-digital voltage sensors</li><li>Temperature via thermistors</li></ul></li>
<li>What does the acronym PWM stand for?<p class="list-inset"><strong class="bold">PWM</strong> stands for <strong class="bold">pulse width modulation</strong>, a type of digital-to-analog control scheme where pulses are sent out that get longer based on the amount of control desired. In other words, the pulse duty cycle (amount of time on/off) gets converted into a voltage to drive a motor. This is commonly used to control DC motors.</p></li>
<li>What is analog-to-digital conversion? What goes in and what comes out?<p class="list-inset">As the name says, <strong class="bold">analog-to-digital</strong> (<strong class="bold">A2D</strong>) conversion takes in an analog value, typically a voltage, and converts it into a digital value or number that the digital part of the computer can understand. A typical application is measuring battery voltage to determine the state of charge.</p></li>
<li>Who invented the subsumption architecture?<p class="list-inset">As stated in <em class="italic">Cambrian Intelligence: The Early History of the New AI</em> by Rodney Brooks, the subsumption architecture was originally described by Dr. Rodney Brooks, a professor at MIT who would later help found iRobot Corporation and invent the Baxter Robot. Rodney was trying to develop analogs of insect brains to understand how to program intelligent robots.</p></li>
<li>Compare my diagram of the three-layer subsumption architecture to the three laws of robotics postulated by Isaac Asimov. Is there a correlation? Why or why not?<p class="list-inset">No, not really. The three laws of robotics from Isaac Asimov are fictional, while the <strong class="bold">Subsumption Architecture</strong> (<strong class="bold">SA</strong>) is a real architecture that is used to make robots in the real world.</p><p class="list-inset">Asimov’s three laws:</p><ul><li>Robots will not harm a human being, or through inaction, allow a human to come to harm</li><li>Robots will obey orders from humans, except when that violates the first law</li><li>Robots will protect themselves from harm, except when that violates the first two laws</li></ul><p class="list-inset">Let’s look at the three layers in the SA:</p><ul><li>The bottom layer of the SA is the part that looks inside the robot and takes care of internal systems – I like to compare it to the autonomic nervous system. That protects the robot.</li><li>The second layer is the short-term manager – it tells the robot where to go, which includes obeying orders from users.</li><li>The top layer contains the strategic thinking and planning processes. The correlation is weak, to be truthful.</li></ul><p class="list-inset">And, readers, remember the final, or zeroth, law: A robot shall not harm humanity or allow humanity to come to harm. That was a later addition.</p></li>
<li>Do you think I should have given our robot project <em class="italic">Albert</em> a name? Do you name your robots individually or by model?<p class="list-inset">Roombas, being robotic vacuum cleaners, exhibit characteristics that people often associate with living entities, such as movement and the ability to navigate spaces autonomously. This behavior can trigger a human tendency to anthropomorphize or attribute human-like qualities to non-human entities. Naming is a natural extension of this anthropomorphism.</p></li>
<li>What is the importance of the <code>ROS_ROOT</code> environment variable?<p class="list-inset">The most important variables are <code>ROS_ROOT</code> and <code>ROS_PACKAGE_PATH</code>. These variables are used to define the filesystem paths for ROS packages and resources. They are essential for the ROS system to locate and use various packages and resources correctly.</p></li>
</ol>
<h1 id="_idParaDest-182"><a id="_idTextAnchor405"/><a href="B19846_03.xhtml#_idTextAnchor043">Chapter 3</a></h1>
<ol>
<li value="1">Describe some of the differences between a storyboard for a movie or cartoon and a storyboard for a software program.<p class="list-inset">A storyboard for a movie is used not only for advancing the plot, but also for showing what point of view will be used – in other words, it is used to plan camera angles, directions, and movements. In that the purpose of both storyboards is to “tell the story” of what happens, they are the same. The point of view of a computer software storyboard should be the user.</p></li>
<li>What are the five <em class="italic">W</em> questions? Can you think of any more questions that would be relevant in examining a use case?<p class="list-inset"><em class="italic">Who</em>, <em class="italic">What</em>, <em class="italic">When</em>, <em class="italic">Where</em>, <em class="italic">Why</em> (with <em class="italic">Why</em> being the most important). More relevant questions might be: How well? How often? How many or how much?</p></li>
<li>Complete this sentence: A use case shows what the robot does but not ________.<p class="list-inset">“How it does it.” Use cases are from the user’s perspective and never include implementation details.</p></li>
<li>Take the storyboard in <em class="italic">step 9</em>, where the robot is driving to the toybox, and break it down into more sequenced steps in your own storyboard. Think about everything that must happen between <em class="italic">frames 9</em> and <em class="italic">10</em>.<p class="list-inset">The robot has to do the following:</p><ol><li class="upper-roman">Determine a route to the toybox.</li><li class="upper-roman">Plan a path.</li><li class="upper-roman">Avoid obstacles along the way.</li><li class="upper-roman">Align itself with the front of the toybox.</li><li class="upper-roman">Drive up to the toybox.</li><li class="upper-roman">Move the robot arm to clear the top.</li></ol></li>
<li>Complete the reply form of the “knock-knock” joke, where the robot answers the user telling the joke. What do you think is the last step?<p class="list-inset">It is to compliment the teller of the joke – the robot should say “That is very funny” or “I am sorry, I am unable to groan”. Yes, that is my opinion and not an official joke writer’s idea. What do you think?</p></li>
<li>Look at the teleoperate operations. Would you add any more or does this look like a good list?<p class="list-inset">The robot needs to send video back to the operator so that the operator can see where they are going.</p></li>
<li>Write a specification for a sensor that uses distance measurement to prevent the robot from driving downstairs.<p class="list-inset">The robot shall have a sensor capable of detecting negative obstacles in the floor (i.e., stairs going downward, balconies) at a distance of at least six inches from the robot along the robot’s driving direction.</p></li>
<li>At which distance can a camera with 320x200 pixels and a 30-degree <strong class="bold">field of view</strong> (<strong class="bold">FOV</strong>) vertically and horizontally see a 6” stuffed animal, assuming we need 35 pixels for recognition?<p class="list-inset">To solve this problem, we first need to determine how many degrees per pixel we have and then use that to calculate the angular dimension of the target, which is 35 pixels tall:</p><p class="list-inset"><em class="italic">30 degrees / 320 pixels wide = </em><em class="italic">0.0937 deg/pixel</em></p><p class="list-inset">To find the number of degrees per pixel, we can perform the following calculation:</p><p class="list-inset"><em class="italic">35 pixels* deg/pixel = </em><em class="italic">3.28 degrees</em></p><p class="list-inset">This gives us an isosceles triangle, but we need a right triangle to do the math. Divide the base into two to make a right triangle; thus, the base of this triangle is now <em class="italic">3 inches</em>.</p><p class="list-inset">We also divide the angle in half:</p><p class="list-inset"><em class="italic">3.28/2 = </em><em class="italic">1.64 degrees</em></p><p class="list-inset">Then, to calculate the perpendicular height, we divide the length of the base by the value of <em class="italic">tan</em>:</p><p class="list-inset"><em class="italic">3 / tan(1.64) = </em><em class="italic">104.78 inches</em></p><p class="list-inset">This translates to 8.73 feet.</p><p class="list-inset">This can be illustrated by the following diagram:</p></li>
</ol>
<div><div><img alt="Figure 11.3 – Solving for pixels needed for recognition" src="img/B19846_11_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Solving for pixels needed for recognition</p>
<p class="list-inset">Thus, the required distance is 8.73 feet.</p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor406"/><a href="B19846_04.xhtml#_idTextAnchor126">Chapter 4</a></h1>
<ol>
<li value="1">We went through a lot in this chapter. You can use the framework provided to investigate the properties of neural networks. Make adjustments to the learning rate, batch size, number of epochs, and loss functions.<p class="list-inset">This is an exercise for the student. You should see different curves develop as these parameters are changed. Some will not produce an answer at all (which looks like random results – the curve stays at the same level as no learning is taking place). Some will learn faster or slower.</p></li>
<li>Draw a diagram of an artificial neuron and label the parts. Look up a natural, human biological neuron and compare.<p class="list-inset">See <em class="italic">Figure 4</em><em class="italic">.3</em> in the chapter. The artificial neuron has a number of inputs, a set of weights, one for each input, a bias, an activation, and a set of outputs.</p></li>
<li>Which features are the same in a real neuron and an artificial neuron?<p class="list-inset">Both have multiple inputs and multiple outputs and accept inputs, perform some processing, and then make an output. Both use some sort of activation function (the biological equivalent is the synapse) to determine when to <em class="italic">fire</em> or produce an output. Both are part of networks: it takes a lot of neurons to compose a neural network, and likewise an animal brain.</p></li>
<li>Which features of a real neuron and an artificial neuron are different?<p class="list-inset">The natural <strong class="bold">neuron</strong> is an analog device that can handle many levels or degrees of input, with no simple on/off binary representations like the computer neuron. Neurons use chemical paths that make pathways and connections easier the more they are used, which is the learning function of a neuron. This is simulated by the weights in an artificial neuron. The natural neuron has an axon, or connecting body, that extends out to the outputs that can be at a quite distance from the nerve inputs. Neurons are randomly connected to other neurons, while artificial neurons are connected in regular patterns. Some neural networks use <strong class="bold">dropout layers</strong> that randomly disconnect neurons, providing some randomness to the output that helps the network estimate non-linear solutions.</p></li>
<li>What relationship does the first layer of a neural network have to the input?<p class="list-inset">The first layer contains the number of inputs to the network. For instance, if you have five inputs, then the first layer must contain five neurons.</p></li>
<li>What relationship does the last layer of a neural network have to the output?<p class="list-inset">The last layer of an ANN is the output layer and has to have the same number of neurons as there are potential outputs.</p></li>
<li>Look up three kinds of loss functions and describe how they work. Include mean square loss and the two kinds of cross-entropy loss.<p class="list-inset"><strong class="bold">Loss functions</strong> in ANNs are the error functions that compare the expected output of the neuron with the actual output. Let’s look at them in detail:</p><ul><li><strong class="bold">Mean square loss</strong> (<strong class="bold">MSL</strong>): This is the most commonly used loss function. It is given by the sum of the squares of the distances between the output and the expected output. MSL amplifies the error the farther away from the desired solution it is.</li><li><strong class="bold">Cross entropy</strong> (<strong class="bold">XE</strong>): This is also called log loss and is used mostly for the classification of CNNs. As the predicted value approaches 1 (no error), XE slowly decreases. As the values diverge, XE increases rapidly. Two types of cross entropy are as follows:<ul><li>Binary (on/off, used for yes/no questions)</li><li>Sigmoid cross-entropy, which can handle multiple classes</li></ul></li></ul></li>
<li>What would you change if your network trained to 40% and got “stuck” or was unable to learn anything further?<p class="list-inset">You are probably <em class="italic">overfitting</em> and have too small a sample size or your network is not wide or deep enough.</p></li>
</ol>
<h1 id="_idParaDest-184"><a id="_idTextAnchor407"/><a href="B19846_05.xhtml#_idTextAnchor159">Chapter 5</a></h1>
<ol>
<li value="1">In Q-learning, what does the Q stand for (you will have to research this on the internet)?<p class="list-inset">The origin of <strong class="bold">Q-learning</strong> is the doctoral thesis of Christopher John Cornish Hellaby Watkins from King’s College, London, May, 1989 (<a href="https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards">https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards</a>). Evidently, the Q just stands for <em class="italic">Quantity</em>.</p></li>
<li>What could we do to limit the number of states that the Q-learning algorithm must search through?<p class="list-inset">Only pick the Q-states that are relevant and are follow-ons to the current state. If one of the states is impossible to reach from the current position, or state, then don’t consider it.</p></li>
<li>What effect does changing the learning rate have on the learning process?<p class="list-inset">If the learning rate is too small, the training can take a very long time. If the learning rate is too large, the system does not learn a path but instead overshoots and may miss the minimum or optimum solution. If the learning rate is too big, the solution may not converge or may suddenly drop off.</p></li>
<li>What function or parameter serves to penalize longer paths in the Q-learning equation? What effect does increasing or decreasing this function have?<p class="list-inset">The discount factor works by decreasing the reward as the path length gets longer. It is usually a value just short of 1.0, for example, 0.93. Making the discount factor higher may cause the system to reject valid longer paths and not find a solution. If the discount is too small, then the paths may be very long.</p></li>
<li>In the genetic algorithm, how would you go about penalizing longer paths so that shorter paths (fewer number of steps) would be preferred?<p class="list-inset">You would adjust the fitness function to consider path length as a factor in the fitness calculation.</p></li>
<li>What effect does changing the learning rate in the genetic algorithm change? What are the upper and lower bounds of the learning rate?<p class="list-inset">Generally, increasing the learning rate shortens the learning time in generations, up to a limit where the path jumps out of the valid range. For our example program, the lowest learning rate that returns a valid solution is five, and the highest value is 15.</p></li>
<li>In the genetic algorithm, what effect does lowering the population cause?<p class="list-inset">It causes the simulation to run much faster but take many more generations to find a solution.</p></li>
</ol>
<h1 id="_idParaDest-185"><a id="_idTextAnchor408"/><a href="B19846_06.xhtml#_idTextAnchor205">Chapter 6</a></h1>
<ol>
<li value="1">Do some internet research on why an open source voice assistant was named Mycroft. How many stories did you find and which one did you like?<p class="list-inset">I found at least three. My favorite is that Mycroft is Sherlock Holmes’ older, and some say smarter, brother. Sherlock Holmes is played on TV in the UK by Benedict Cumberbatch, who also played Alan Turing in the movie <em class="italic">The Imitation Game</em>, the original name of the Turing Test, a test of AI conversation, which is what Mycroft does.</p></li>
<li>In the discussion of intent, how would you design a neural network to predict command intent from natural language sentences?<p class="list-inset">One approach would be to gather a selection of commands, label the intent of the command, use the commands as input in a neural network, and use intent as the output label for the training.</p></li>
<li>Rewrite the “receive knock-knock jokes” program to remember the jokes told to the robot by adding them to the joke database used by the “tell knock-knock jokes” program. Is this machine learning?<p class="list-inset">It is fairly simple to add a program to just write to the knock-knock joke program database. You’ll find a version of this in the GitHub Repository. Is this machine learning? I would say definitely! The machine has a capability that it did not have before. It did not have to be reprogrammed to have the new capability, so it learned.</p></li>
<li>Modify <code>KnockKnock</code> (the program that tells knock-knock jokes) to play sounds from a <code>WAV</code> file, such as a music clip, as well as do text-to-speech.<p class="list-inset">Add these lines to the <code>KnockKnock</code> program:</p><pre class="source-code">
"play_wav_cmdline": "paplay %1 --stream-name=mycroft-voice",</pre><p class="list-inset">You can use this to play the audio. You can also add a tag to the joke file that indicates a <code>WAV</code> file <code>&lt;groan.wav&gt;</code>. Then, if you see this tag, call the <code>play_wav_cmdline</code> function above.</p></li> <li>The sentence structure used in this chapter is all based on English grammar. Other languages, such as French and Japanese, have different structures. How does that change the parsing of sentences? Would the program we wrote be able to understand Yoda?<p class="list-inset">In other languages, the object or the subject appears in a different order, just as in Yoda’s speech patterns. “Backwards, I talk,” Yoda would say. This does require us to change or add new sentence patterns to our <code>.</code><code>voc</code> files.</p><p class="list-inset">You can follow Mycroft’s instructions for changing the engine to understand French at <a href="https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/languages">https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/languages</a>.</p></li>
<li>Do you think that Mycroft’s Intent Engine is actually understanding intent or just pulling out keywords?<p class="list-inset">I do not place Mycroft in the category of construction AI chatbots, but it is rather a referential type that looks up answers in a database, which makes it more of an expert system than an AI program. It does use AI neural networks in the speech-to-text section.</p></li>
<li>Describe the voice commands necessary to instruct the robot to drive to an object and pick it up without the robot being able to recognize the object. How many commands do you need?<p class="list-inset">We need two commands:</p><ul><li><em class="italic">Can you see </em><em class="italic">any objects?</em></li><li><em class="italic">Drive to the </em><em class="italic">closest object</em></li></ul></li>
<li>From <em class="italic">Question 7</em>, work to minimize the number of commands. How many can you eliminate or combine?<p class="list-inset">The following are the voice commands to tell the robot to drive to the nearest object:</p><ul><li><em class="italic">Hey, Albert</em></li><li><em class="italic">Drive to the </em><em class="italic">nearest object</em></li></ul></li>
<li>From <em class="italic">Question 7</em>, how many unique keywords are involved? How many non-unique keywords?<p class="list-inset">The four keywords are <strong class="bold">see</strong>, <strong class="bold">objects</strong>, <strong class="bold">drive</strong>, and <strong class="bold">closest</strong>.</p><p class="list-inset">All of the words are unique except <strong class="bold">objects</strong>, so there are three unique words. They have not been otherwise defined in the Mycroft word database.</p></li>
</ol>
<h1 id="_idParaDest-186"><a id="_idTextAnchor409"/><a href="B19846_07.xhtml#_idTextAnchor221">Chapter 7</a></h1>
<ol>
<li value="1">Regarding SLAM, what sensor is most used to create the data that SLAM needs to make a map?<p class="list-inset"><strong class="bold">Light detection and ranging</strong> (<strong class="bold">LiDAR</strong>) sensors are the most common SLAM sensors used by a wide margin. The 3D data that LiDAR provides is perfect for SLAM’s mapping function.</p></li>
<li>Why does SLAM work better with wheel odometer data available?<p class="list-inset">The wheel odometers reduce the search space that the SLAM algorithm needs to look for the possible locations of the robot after moving. Thus, it increases information and reduces uncertainty in the map. How does it do this? By giving extra measurements about where the robot is located (how far it moved), we can then reduce our search to match where we are against our sensor readings.</p></li>
<li>In the Floor Finder algorithm, what does the Gaussian blur function do to improve the results?<p class="list-inset">The <strong class="bold">Gaussian blur function</strong> reduces noise and gets rid of stray single pixels in the image, making for a smoother result.</p></li>
<li>The final step in Floor Finder is to trace upwards from the robot’s position to the first red pixel. In what other way can this step be accomplished (referring to <em class="italic">Figure 7</em><em class="italic">.3</em>)?<p class="list-inset">Instead of using radial red lines, the program can just draw upwards from the bottom of the screen in a series of vertical lines.</p></li>
<li>Why did we cut the image in half horizontally before doing our neural network processing?<p class="list-inset">We just want to use the upper half of the room to train the network because the lower half has the toys on it and is subject to change. The upper half of the room does not change with the addition of toys.</p></li>
<li>What advantages does using the neural network approach provide that a technique such as SLAM does not?<p class="list-inset">We don’t have to have a map to successfully navigate the room. We are providing labeling of our training set by just driving the robot around and taking pictures at regular intervals. This approach is also far more resilient to changes in the room, such as the furniture being in slightly different positions. Please also see <a href="https://hackaday.com/2021/10/25/fast-indoor-robot-watches-ceiling-lights-instead-of-the-road/">https://hackaday.com/2021/10/25/fast-indoor-robot-watches-ceiling-lights-instead-of-the-road/</a> for someone else’s implementation of this idea, used for indoor car racing.</p></li>
<li>If we used just a random driving function (where you make random turns at random times) instead of the neural network, what new program or function would we have to add to the robot to achieve the same results?<p class="list-inset">We would need to have a navigation function that determined where in the room we were at – this would probably mean a SLAM algorithm. We would also need something to detect the stairs.</p></li>
<li>How did we end up avoiding the stairs in the approach presented in the chapter? Do you feel that this is adequate? Would you suggest any other means to accomplish this task?<p class="list-inset">We trained the robot to navigate by looking at the upper part of the room. We only drove the robot in safe areas and used that information to allow the robot to predict its next driving command based on where it was in the room. Since we did not drive the robot down the stairs in this process, the robot will never get a command to drive toward the stairs. If there is a toy near the stairs, the robot will still go pick it up but will drive away from the stairs afterward when it goes back to navigation mode. We have to be careful to get a good training result before letting the robot loose, however. I used a baby gate to block the stairs for early testing. As an additional safety measure, we can add a lookdown sensor to detect stairs. I would use an <strong class="bold">infrared proximity detector</strong> (<strong class="bold">IRPD</strong>) for this purpose.</p></li>
</ol>
<h1 id="_idParaDest-187"><a id="_idTextAnchor410"/><a href="B19846_08.xhtml#_idTextAnchor235">Chapter 8</a></h1>
<ol>
<li value="1">What are the three ways to traverse a decision tree?<p class="list-inset">From beginning to end (start to goal); from goal to start; and from both ends at once to meet in the middle.</p></li>
<li>In the fishbone diagram example, how do you go about pruning the branches of the decision tree?<p class="list-inset">By eliminating the effect of the item on a branch. For example, using our “robot does not move” fault, if the branch says “Arduino-no power” and you check to see if the Arduino has power and it does, you can prune that branch. If the branch is “motor stuck”, the effect of having a motor stuck is that the robot will drive in circles. As the robot is not driving in circles – it is not driving at all – you can prune that branch.</p></li>
<li>What is the role of the Gini coefficient in creating a classification?<p class="list-inset">It determines the amount of <em class="italic">impurity</em> in the sample or pool. When the Gini coefficient = 0, all of the members of the class have the same attributes, and no further subdivision is possible. This minimizes misclassification. The Gini coefficient is given by one minus the sum of the square of the probability of an item being in that class.</p></li>
<li>In the toy classifier example using the Gini coefficient, which attributes of the toy were not used by the decision tree? Why not?<p class="list-inset">Color, Noise, Soft, and Material were not useful for dividing the categories by labels as the labels and the items did not correlate. It does make sense that color is not useful for dividing toys by type.</p></li>
<li>Which color for the toys was used as a criterion by one of the classification techniques we tried?<p class="list-inset">The color white was used by the decision tree that used the Gini index and one hot encoding to separate the stuffed animals.</p></li>
<li>Give an example of label encoding and one hot encoding for menu items at a restaurant.<p class="list-inset">Let’s have three types of menu items: appetizer, entrée, and dessert. Label encoding would substitute 0 for appetizer, 1 for entrée, and 2 for dessert. One hot encoding would use 1 0 0 for appetizer, 0 1 0 for entrée, and 0 0 1 for dessert.</p></li>
<li>In the A* algorithm, discuss the different ways that <code>G()</code> and <code>H()</code> are computed.<p class="list-inset">The <code>G()</code> function is the distance along the path from the current position to the start. <code>H()</code> is the distance from the current position directly to the goal (the Euclidean distance). Note that <code>G()</code> follows the path and <code>H()</code> is the straight-line distance to the goal since we have not computed a path to the goal yet.</p></li>
<li>In the A* algorithm, why is <code>H()</code> considered heuristic and <code>G()</code> not? In the D* algorithm, heuristics are not used. Why not?<p class="list-inset"><code>H()</code> – the direct line distance to the goal – is an estimate and ignores any obstacles, it can’t be used directly but is just a way to compare one position to another. A major difference between D* and A* is that D* starts at the goal and works backward toward the start. This allows D* to know the exact cost to the target – it is using the actual path distance to the goal from the current position and not a heuristic approach or an estimate of the distance to go, as A* did.</p></li>
<li>In the D* algorithm, why is there a <code>RAISED</code> and a <code>LOWERED</code> tag and not just a <code>CHANGED</code> flag?<p class="list-inset">The <code>RAISED</code> squares or points are eliminated from consideration. The <code>LOWERED</code> squares may be added back into the queue for consideration to be a path. Keep in mind that lowering scores due to new sensor readings ripples through the path planner.</p></li>
</ol>
<h1 id="_idParaDest-188"><a id="_idTextAnchor411"/><a href="B19846_09.xhtml#_idTextAnchor294">Chapter 9</a></h1>
<ol>
<li value="1">What is your favorite movie robot? How would you describe its personality?<p class="list-inset">This is, of course, a subjective question. I’m a big R2D2 fan. R2 is feisty, determined, and stubborn as well as being a faithful companion and helper. R2 will get you out of a jam, fix your starfighter, provide cover from hostile fire, and hack Imperial computers. He is a Swiss army knife on wheels.</p></li>
<li>What techniques did the movie-makers use to express R2D2’s personality (body language, sounds, etc.)?<p class="list-inset">R2D2 owes his personality to a combination of his emotional beeps and squawks (provided by Ben Burtt) and his body movements provided by having a person inside his chassis (Kenny Baker). They were stuck with the not-very versatile chassis designed for the first <em class="italic">Star Wars</em> movie, which only has a head that moves. Most of R2’s persona comes through in his sounds, including his famous scream.</p></li>
<li>What are the two types of chatbots? List some of the strengths and weaknesses of each.<p class="list-inset">The two types of chatbots are as follows:</p><ul><li><strong class="bold">Retrieval-based chatbots</strong>: Retrieval-based chatbots look up responses in lists of scripts and choose from a number of phrases that are written in advance by humans. The strengths of these chatbots are that they are easy to program, allow more control over the outputs, and are much smaller and faster programs. The weaknesses are that they have limited responses and the use of keywords gives them a small vocabulary.</li><li><strong class="bold">Generative chatbots</strong>: Generative chatbots use the rules of grammar and models of sentences to create new sentences with proper meaning, are more flexible, and can handle a wider range of topics, but they are much harder to program and are complex and slow (comparatively speaking). Generative chatbots have now taken over, given the success of ChatGPT and other generative AI models.</li></ul></li>
<li>In <em class="italic">Figure 9</em><em class="italic">.2</em>, the illustration on modeling custom distributions (the airport example), the lower picture shows two standard distributions and two uniform distributions. Why don’t the curves go all the way to the top of the graph?<p class="list-inset">The two distributions will add together – the standard distributions sit <em class="italic">on top</em> of the uniform distributions, and the two combined go to the top of the graph.</p></li>
<li>Design your own robot emotions: Pick six contrasting emotions that can express the entire range of your robot’s personality. Why did you pick those?<p class="list-inset">This is another subjective question. My answers are in the text. I picked emotions that represented the range of capability of my robot and the situations it would be in. I kept to a friendly type of robot so that the only negative emotion was sadness – there was no anger, for instance.</p></li>
<li>If you were designing a robot to have the personality of an annoying little boy (think Bart Simpson, Cartman, or Dennis the Menace if you are that old), what traits would it have?<p class="list-inset">A small boy would be mischievous, have a short attention span, constantly change the subject, keep trying to bring up the same topic over and over, and repeat variations of the same questions. How might we represent mischievous? Perhaps by ignoring directives and generating random events or distractions that get the robot’s attention away from the task at hand.</p></li>
<li>Why is it important for the robot to have a backstory or biography?<p class="list-inset">To provide consistent answers to personal questions, such as “How old are you?”</p></li>
<li>For the following questions, pick a persona from my list to model (from <em class="italic">Integrating </em><em class="italic">Artificial Personality</em>).<ol><li class="upper-roman">Write six lines of dialogue for the robot to ask a human where they last went on vacation.<ul><li>So, where did you go on vacation last?</li><li>Summertime is coming up. Where did you go on vacation last year? Do you like to travel? Where have you been?</li><li>I never get to go on vacation. Where did you go last?</li><li>I have heard of this concept called vacation. Where do you like to go? Have you been to the beach?</li></ul></li><li class="upper-roman">Write six ways for the robot to express that it is tired and needs to recharge without sounding like a robot.</li></ol><ul><li>I’m tired – have you seen my recharger?</li><li>Wow, it is getting late. I’ve been at this for a long time.</li><li>Well, my battery is getting low. Must be about quitting time. I am starting to feel a bit run down.</li><li>Well, look at the time! My battery needs attending to. I’m getting hungry in here. Can I go charge now?</li></ul></li>
</ol>
<h1 id="_idParaDest-189"><a id="_idTextAnchor412"/> <a href="B19846_10.xhtml#_idTextAnchor366">Chapter 10</a></h1>
<ol>
<li value="1">Given that we started the chapter with knock-knock jokes and ended up talking about robot phobia and addressing philosophical questions about existence, do you feel that AI is a threat? Why or why not?<p class="list-inset">I do not feel that robots or AI are a threat in any way because the necessary and sufficient conditions for robots to be a threat do not exist, which is to say that the robots have to <em class="italic">want</em> to take over the world and must have a <em class="italic">need</em> to take over. Currently, robots and AI have no such wants or needs.</p></li>
<li>List any five professions that would be necessary to turn our Albert robot into a product.<p class="list-inset">We would need project managers, packaging designers, advertising and marketing experts, salespeople, engineers, technicians, artists, package designers, machinists, electricians, accountants, lawyers, a psychologist, and support staff, among others. You can select any five from among these options.</p></li>
<li>Why would we need a psychologist in our imaginary company that manufactures robots?<p class="list-inset">Psychologists study normal and abnormal mental states and cognitive processes, which is exactly what we are trying to simulate in an artificial personality. We want the robot to not trigger bad responses in people. I once had a robot with flashing red eyes that caused small children to have panic attacks. Psychologists would help avoid such errors.</p></li>
<li>What components found in cell phones or smartphones are also found in quadcopters?<p class="list-inset">GPS receivers, radios, Wi-Fi, Bluetooth, accelerometers, gyroscopes, and, these days, applications, or apps.</p></li>
<li>Why are artificial intelligence systems, specifically artificial neural networks, naturally non-deterministic in both result and time?<p class="list-inset">They are universal approximation systems that work in probabilities and averages, not in discrete numbers and logic. Artificial neural networks can take a different amount of time because a particular bit of data may take different paths at different times, going through a different number of neurons and thus not taking the same amount of time to process. It is true that if you provide the exact same input to a neural network, it will give you the exact same answer every time. In the real world that robots live in, however, the case that two inputs are identical in every way can be very rare.</p></li>
<li>What might be a practical application of an AI system that made predictable mistakes?<p class="list-inset">You can use a neural network-based system to model a bad human operator for a driving simulation to help teach other drivers (and self-driving cars) how to avoid bad drivers. The desired state is an unpredictable driver, so just train the neural network to 60% or so. Now the network will come up with the wrong answer 40% of the time, i.e., be statistically predictable. I actually did this on a project for the Navy that wanted a simulation of imperfect people misusing a system at a predictable level, so they could create a responsive control system that could handle those mistakes.</p></li>
<li>If an AI system was picking stocks for you and predicted winning stock 43% of the time and you had a second AI system that was 80% accurate at determining when the first AI had <em class="italic">not</em> picked good stock, what percent of the time would the AI combination pick profitable stock?<p class="list-inset">We have 100 stocks picked by our AI program. Of that set, an indeterminate number are winners and losers. There is a 43% chance the stock is correctly predicted to be a winner and a 57% chance it is predictably a loser. We have no way of judging the stocks as being winners or losers except by investing our money, which is what we are trying to avoid – investing in bad stocks. A 43% chance of winning is not good.</p><p class="list-inset">The second AI has an 80% chance of telling you that the first AI chose bad stock. Eighty times out of 100, you will know that the stock was not a winner. You are left with an 80% chance of correctly identifying one of the 57 bad stocks, which eliminates 45 stocks. That leaves you with 55 stocks, of which 43 are winners (on average), which raises your odds to 78%.</p><p class="list-inset">Bayes' theorem shows the combination of two independent probabilities (probability of <em class="italic">x</em> occurring given <em class="italic">c</em> has occurred):</p><p class="list-inset"><em class="italic">px</em> = probability of <em class="italic">x</em>, and <em class="italic">pc</em> = probability of <em class="italic">c</em>)</p><p class="list-inset"><em class="italic">p(x|c) = (px* pc) / (</em><em class="italic">px*pc)+(1-px)(1-pc)</em></p><p class="list-inset">Using this theorem, I recomputed the combined probabilities as 75.1%, so I’ll take either answer.</p></li>
</ol>
</div>


<div><h1 id="_idParaDest-190"><a id="_idTextAnchor413"/>Appendix</h1>
<p><strong class="bold">Robotic Operating System</strong> (<strong class="bold">ROS</strong>) was a<a id="_idIndexMarker837"/> framework designed to enable the development of software for complex robots and was developed by a company called <em class="italic">Willow Garage</em>, specifically for the control of the PR2 robot. The PR2 was a human-sized robot with two <strong class="bold">7-degree of freedom</strong> (<strong class="bold">7DOF</strong>) arms and an entire array of sensors. Controlling this very<a id="_idIndexMarker838"/> complex robot required the interaction of a multitude of sensors, motors, and communications. The ROS framework allowed the development of robot components to be done independently. While not an operation system in the traditional sense<a id="_idIndexMarker839"/> of the word, it is a <strong class="bold">Modular Open Source </strong><strong class="bold">Architecture</strong> (<strong class="bold">MOSA</strong>).</p>
<p>The primary tool of ROS is a robust <em class="italic">publish-subscribe</em> service that makes talking between processes — that <a id="_idIndexMarker840"/>is, <strong class="bold">Inter-Process Communications</strong> (<strong class="bold">IPC</strong>) — easy and flexible. It also standardized a lot of the interfaces between sensors, motors, and controls for robots.</p>
<p>We will be using ROS 2 throughout this book. ROS 2 is a new version of ROS. In this <em class="italic">Appendix</em>, we will discuss how to install ROS 2, use it to communicate, and briefly introduce some of the tools ROS provides. We will provide an in-depth introduction to ROS 2 and describe some of the hardware involved in the design of our toy-collecting robot, Albert. We will also cover some of the hardware referenced in the book when creating Albert as our example for this book. Albert V2, the robot in this second edition of this book, is my 30th or so robot design.</p>
<p>The topics covered in the appendix include the following:</p>
<ul>
<li>Introducing MOSA</li>
<li>A brief overview of ROS 2</li>
<li>Software requirements for the robot</li>
<li>Introducing the hardware for the robot</li>
<li>Robot safety tips</li>
</ul>
<h1 id="_idParaDest-191"><a id="_idTextAnchor414"/>Introducing MOSA</h1>
<p>ROS is an<a id="_idIndexMarker841"/> example of a <a id="_idIndexMarker842"/>MOSA. Why is this important? Imagine if every electrical appliance in your house had its own plug, a different voltage, and a different wire. It would make life very difficult for you. But all your electrical plugs are the same shape and put out the same voltage. They are standardized interfaces that allow you to plug many different types of appliances into them. A MOSA acts like that for software, standardizing interfaces and allowing <em class="italic">plug-and-play</em> compatibility.</p>
<p>The following are its advantages:</p>
<ul>
<li>A <a id="_idIndexMarker843"/>MOSA system architecture allows modularity – the ability to create software in sections or modules that can be developed, debugged, and operated independently. Before ROS, I created one major executable that ran everything on my robot. The problem with this is, first of all, that I could not take advantage of the multi-core nature of <a id="_idIndexMarker844"/>my <strong class="bold">Single Board Computer</strong> (<strong class="bold">SBC</strong>), which was the robot’s brain. I had all my code in one thread, in one program, and splitting out functions to run independently was difficult.</li>
<li>Then there were the interactions. If I changed the timing on my motor driver, it messed up the sensor timing for the camera. If I changed the path planner, then the steering needed adjusting. This sort of <a id="_idIndexMarker845"/>interaction is typical in a <strong class="bold">unitary architecture</strong>. However, in a MOSA, each section of the robot, for example, the robot arm controller, is independent and runs in its own program. They can be developed and debugged independently, and interactions are limited to the interfaces we create in those programs. This frees us from a lot of problems we would otherwise have.</li>
<li>The other feature we can take advantage of is the very large library of standard, already-created interfaces and programs that ROS provides. We don’t need to create a steering interface; ROS has one (the <code>Twist</code> command). We don’t need to create data types for camera imagery data; ROS has several to choose from. ROS also has USB<a id="_idIndexMarker846"/> camera drivers and viewers that we can use without writing any code at all.</li>
</ul>
<p>Now let’s talk a bit about how ROS 2 works.</p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor415"/>A brief overview of ROS 2</h1>
<p>As mentioned earlier, ROS 2 <a id="_idIndexMarker847"/>is the latest version of ROS, a widely used framework for developing robot applications. I’ve been using ROS for some time now and appreciate how much simpler it makes the integration of various components, sensors, and capabilities into my robots. I resisted moving to ROS for some time, but now that I have invested the time to learn what it can do, I can’t imagine developing a robot without it.</p>
<p>In this section, we will discuss some concepts that are foundational for our understanding of ROS 2, how to install ROS 2, and some basic commands that we can use with it.</p>
<p class="callout-heading">Note</p>
<p class="callout">Please use whatever is the latest version of ROS 2.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor416"/>Understanding the basic concepts</h2>
<p>ROS 2 works a bit differently than other programming paradigms. ROS is based on a publish/subscribe mechanism that allows different programs or processes to pass information from one to another without having to know in advance who is receiving the data. Let’s look at how this process works:</p>
<ul>
<li>Each program or code that communicates in ROS is called <a id="_idIndexMarker848"/>a <strong class="bold">node</strong>. Nodes each have names that uniquely identify them to the system.</li>
<li>Nodes publish data on <strong class="bold">topics</strong>, which<a id="_idIndexMarker849"/> represent an interface for messages.</li>
<li><strong class="bold">Messages</strong> are interfaces<a id="_idIndexMarker850"/> that have data types (string, float, fixed, array, etc.).</li>
</ul>
<p>For example, a joystick interface node publishes to the commanded velocity (<code>cmd_vel</code>) topic with a <code>Twist</code> message. On the receiver end of the message, the motor control interface subscribes to the <code>cmd_vel</code> topic to receive Twist messages that have speed and turn information for the robot.</p>
<p class="callout-heading">Note</p>
<p class="callout">Why is it called a Twist message? The reason for using the term “twist” is conceptual. Think of a robot’s movement as a combination of straightforward (linear) motion and rotation (twisting or turning). By combining linear and angular velocities, a Twist message effectively describes how the robot “twists” through space, which includes both translation (moving from one place to another) and rotation (changing orientation).</p>
<p>ROS has a few other useful features:</p>
<ul>
<li>There is a<a id="_idIndexMarker851"/> systemwide <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, or <code>FATAL</code>. Log messages are automatically displayed on the local output (usually the command line). They are collected centrally, and you can parse through log messages to debug problems. Periodically, you do need to purge old log files, as they do tend to pile up. Logs are put into the <code>~/.</code><code>log</code> directory.</li>
<li>One of my personal favorite<a id="_idIndexMarker852"/> features is <strong class="bold">parameters</strong>. These are external data values that can be created independently of source code and can be used to turn features on or off, or to set critical settings, such as image size, resolution, range, and other features. The ability to have external parameters goes a long way to make ROS portable and the interfaces reusable. Parameters can be specified in launch files, which are ways of starting multiple programs (nodes) all at once. Albert has five major subsystems (control, motor drive, arm control, vision, and speech), all of which have to be started together.</li>
</ul>
<h2 id="_idParaDest-194"><a id="_idTextAnchor417"/>Comparing ROS 2 and ROS</h2>
<p>ROS 2 introduced some very significant improvements over the original ROS. Some of them are as follows:</p>
<ul>
<li>The most <a id="_idIndexMarker853"/>notable<a id="_idIndexMarker854"/> improvement is the absence of the <strong class="bold">roscore</strong> <em class="italic">traffic cop</em> application. This central program acted as the Master node in any ROS implementation and directed the other nodes on where to communicate via sockets. It directed all the traffic on the network. Therefore, every node had to communicate with roscore to know where it was running. If roscore died, or was turned off, then the entire ROS set of applications stopped. Instead, ROS 2 <a id="_idIndexMarker855"/>uses <strong class="bold">Data Distribution Services</strong> (<strong class="bold">DDS</strong>) as its <em class="italic">middleware layer</em>. DDS is a standard for high-performance, scalable, and real-time data exchange, created by the <strong class="bold">Object Management Group</strong> (<strong class="bold">OMG</strong>) and <a id="_idIndexMarker856"/>managed by the DDS Foundation. It provides a decentralized discovery mechanism for nodes to find each other without needing a central master such as roscore. When a node starts, it advertises its presence to other nodes and also discovers existing nodes and topics. This process is managed by the underlying DDS implementation.</li>
<li>ROS 2 has more features<a id="_idIndexMarker857"/> for <strong class="bold">real-time processing</strong>, and does not have the weird workaround that ROS had, where we had to do things such as post-dating messages into the future. These features include using DDS (which is intended for real-time systems, the ability to work <a id="_idIndexMarker858"/>with <strong class="bold">real-time operating systems</strong> (<strong class="bold">RTOSs</strong>), and to use pre-emption to control processes.</li>
<li>One big improvement is that ROS 2 will <strong class="bold">run natively on Windows</strong> for the first time. You don’t need virtual machines to run ROS on Windows – you can build your own control panels and interfaces directly in Windows.</li>
<li>ROS 2 also improves <strong class="bold">discovery</strong> (the process of finding nodes on the network) and has some enhancements for cyber security.</li>
</ul>
<p>So, all in all, ROS 2 <a id="_idIndexMarker859"/>was well worth taking the time to upgrade, and I feel that it is far less fiddly than the old ROS, easier to set up on both Windows and Linux, and easier to manage.</p>
<p>Let’s look at software requirements next.</p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor418"/>Software requirements for the robot</h1>
<p>In this section, we will discuss the software requirements<a id="_idIndexMarker860"/> for the robot and how to install them on the robot’s CPU.</p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor419"/>Installing ROS 2</h2>
<p>The version of <a id="_idIndexMarker861"/>ROS 2 I installed on Albert currently is <em class="italic">Foxy</em>. Please feel free to use the latest version of ROS 2. Jetson tends to run behind Ubuntu upgrades and is several releases behind. We use the Jetson Nano because it has the required power and the <strong class="bold">Graphics Processing Units</strong> (<strong class="bold">GPUs</strong>) to <a id="_idIndexMarker862"/>run neural network software. My version of the Nano is running Ubuntu 20.04, but you should also be able to get it working with Ubuntu version 18.04.</p>
<p>I used the standard ROS 2 installation script<a id="_idIndexMarker863"/> that can be found at <a href="https://github.com/jetsonhacks/installROS2">https://github.com/jetsonhacks/installROS2</a>. This is a script that contains all of the steps found on the regular ROS 2 installation page<a id="_idIndexMarker864"/> at <a href="https://docs.ros.org/en/foxy/Installation/Alternatives/Ubuntu-Development-Setup.html">https://docs.ros.org/en/foxy/Installation/Alternatives/Ubuntu-Development-Setup.html</a>.</p>
<p>Note that we are doing an <em class="italic">Install from Source</em> setup since many of the programs need to be recompiled to run on the Jetson Nano’s ARM-based architecture. For me, this process took about 4 hours, so it does take some patience.</p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor420"/>Installing other packages</h2>
<p>You <a id="_idIndexMarker865"/>will <a id="_idIndexMarker866"/>also need to install <a id="_idIndexMarker867"/>the following Python packages: <strong class="bold">Scientific Python </strong>(<strong class="bold">SciPy</strong>), <strong class="bold">Numeric Python</strong> (<strong class="bold">NumPy</strong>), <strong class="bold">scikit-learn</strong>, <strong class="bold">Open Computer Vision</strong> (<strong class="bold">OpenCV</strong>), and <strong class="bold">PyTorch</strong>. Let’s<a id="_idIndexMarker868"/> look at the commands <a id="_idIndexMarker869"/>needed to install these packages:</p>
<ul>
<li>SciPy:<pre class="source-code">
<strong class="bold">python -m pip install scipy</strong></pre></li> <li>NumPy:<pre class="source-code">
<strong class="bold">pip install numpy</strong></pre></li> <li>scikit-learn:<pre class="source-code">
<strong class="bold">pip3 install -U scikit-learn</strong></pre></li> <li>OpenCV:<pre class="source-code">
<strong class="bold">sudo apt-get install python3-opencv</strong></pre><p class="list-inset">For more details on<a id="_idIndexMarker870"/> OpenCV, you can refer to <a href="https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html">https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html</a></p></li> <li>PyTorch:<pre class="source-code">
<strong class="bold">pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</strong></pre></li> </ul>
<p>Now let’s look at how <a id="_idIndexMarker871"/>we can get started with ROS 2.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor421"/>Basic ROS 2 commands</h2>
<p>The following <a id="_idIndexMarker872"/>commands <a id="_idIndexMarker873"/>relate to starting, controlling, and monitoring nodes in ROS 2:</p>
<ol>
<li>To start executing a package in ROS 2, use the following command:<pre class="source-code">
<code>turtlesim_node</code> from the <code>turtlesim</code> package in the background.</pre></li> <li>To check that this node is running, we type the following:<pre class="source-code">
<strong class="bold">ros2 node list</strong></pre><p class="list-inset">This gives us a list of currently running nodes:</p><pre class="source-code"><strong class="bold">/turtlesim</strong></pre></li> <li>We can get more information about the node by typing this command:<pre class="source-code">
<strong class="bold">ros2 node info /turtlesim</strong></pre><p class="list-inset">This results in the following:</p><pre class="source-code"><strong class="bold">/my_turtle</strong>
<strong class="bold"> Subscribers:</strong>
<strong class="bold"> /parameter_events: rcl_interfaces/msg/ParameterEvent</strong>
<strong class="bold"> /turtle1/cmd_vel: geometry_msgs/msg/Twist</strong>
<strong class="bold"> Publishers:</strong>
<strong class="bold"> /parameter_events: rcl_interfaces/msg/ParameterEvent</strong>
<strong class="bold"> /rosout: rcl_interfaces/msg/Log</strong>
<strong class="bold"> /turtle1/color_sensor: turtlesim/msg/Color</strong>
<strong class="bold"> /turtle1/pose: turtlesim/msg/Pose</strong></pre></li> </ol>
<p>Here, we can <a id="_idIndexMarker874"/>note the following key points:</p>
<ul>
<li>The <code>turtlesim</code> publishes <code>parameter_events</code> (which happen when you change parameter values), it has a logging interface to <code>/rosout</code>, and it publishes the color of the turtle robot and its position (pose)</li>
<li>For input, it<a id="_idIndexMarker875"/> subscribes to <code>cmd_vel</code>, which uses the <code>Twist</code> command to move the turtle, and to <code>parameter_events</code>, which allows the program to receive parameter changes</li>
</ul>
<p>The command for looking at topics is as follows:</p>
<pre class="console">
ros2 topic list</pre> <p>This shows topics that are active. For our <code>turtlesim</code> example, we get the following output:</p>
<pre class="console">
/parameter_events
/rosout
/turtle1/cmd_vel
/turtle1/color_sensor
/turtle1/pose</pre> <p>If you add <code>-t</code> to the end<a id="_idIndexMarker876"/> of that command, you also get the topic message type:</p>
<pre class="console">
/parameter_events [rcl_interfaces/msg/ParameterEvent]
/rosout [rcl_interfaces/msg/Log]
/turtle1/cmd_vel [geometry_msgs/msg/Twist]
/turtle1/color_sensor [turtlesim/msg/Color]
/turtle1/pose [turtlesim/msg/Pose]</pre> <p>You can find more details and the<a id="_idIndexMarker877"/> full tutorial at <a href="https://ros2-industrial-workshop.readthedocs.io/en/latest/_source/navigation/ROS2-Turtlebot.html">https://ros2-industrial-workshop.readthedocs.io/en/latest/_source/navigation/ROS2-Turtlebot.html</a>. Also, I recommend that you refer to the topic list in the tutorials available from the ROS 2 website: <a href="https://docs.ros.org/en/foxy/Tutorials.html">https://docs.ros.org/en/foxy/Tutorials.html</a>. This will give a more robust introduction to ROS 2. The rest of what you need to run the programs in this book is in the text.</p>
<p>Now, let’s look at the hardware that makes up Albert the Robot.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor422"/>Introducing the hardware for the robot</h1>
<p>I <a id="_idIndexMarker878"/>designed Albert the Robot to perform one manual task – picking up toys. As such, I chose a set of motors, a speaker, and a robot arm as effectors, and a camera and microphone as sensors. Here is a labeled diagram of what Albert looks like:</p>
<div><div><img alt="Figure 12.1 – Albert the Robot" src="img/B19846_12_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Albert the Robot</p>
<p>In the following sections, we will look at how I put Albert together.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor423"/>Effectors – base, motors, and wheels</h2>
<p>The robot base is a <a id="_idIndexMarker879"/>two-layer aluminum allow frame that I purchased at <a href="https://www.amazon.com/gp/product/B093WDD9N5">https://www.amazon.com/gp/product/B093WDD9N5</a>. This base uses <strong class="bold">Mecanum</strong> wheels, which<a id="_idIndexMarker880"/> have the unique ability to move the chassis not just forward and back, but sideways and at any angle. For video game fans, this sideways <a id="_idIndexMarker881"/>movement is sometimes <a id="_idIndexMarker882"/>called <strong class="bold">strafing</strong>. You will note that the wheels have smaller rollers mounted at 45-degree angles. These convert various inputs into multiple directions. Moving all four motors forward, not surprisingly, results in the platform moving forward. Moving the right wheels forward and the left wheels backward results in turning in place to the right.</p>
<p>When we move the left wheels<a id="_idIndexMarker883"/> away from each other and the right wheels towards each other (left front forward, right front backward, left rear backward, and right rear forward), the vehicle moves sideways – strafes – to the right. Reverse these directions and you strafe to the left. These are the motions we need to complete the exercises in the book.</p>
<p>I made some modifications to the base: I cut off the little white connectors to the four motors so I could wire them to the motor controller.</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor424"/>Battery</h2>
<p>I have used <a id="_idIndexMarker884"/>a 4,200 mAh <strong class="bold">Nickel Metal Hydride</strong> (<strong class="bold">NiMH</strong>) battery– no lithium battery here, so there’s less risk of fire. The <a id="_idIndexMarker885"/>output is 7.2 V. This should provide several hours of runtime to our robot, and it fits in the chassis. You can use any drone-type battery with the same specs. A larger battery won’t fit in the chassis, which is quite small. This battery is adequate for our needs. Here is an example of a battery you can get for your robot: <a href="https://www.amazon.com/gp/product/B08KXYY53G">https://www.amazon.com/gp/product/B08KXYY53G</a>.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor425"/>DC/DC power supply</h2>
<p>One of my favorite bits of kit is this <a id="_idIndexMarker886"/>DROK DC/DC adjustable power supply. This board provides the 5 V power to the main computer and the Arduino. You adjust the voltage level to 5 V with a small screwdriver. I really like to have this display on the robot to show that everything is working. Here is a link to the one I used: <a href="https://www.amazon.com/Converter-DROK-Adjustable-Stabilizer-Protective/dp/B01FQH4M82?th=1">https://www.amazon.com/Converter-DROK-Adjustable-Stabilizer-Protective/dp/B01FQH4M82?th=1</a>.</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor426"/>CPU – the brains of the outfit</h2>
<p>As we’ve mentioned throughout the <a id="_idIndexMarker887"/>book, the main computer for Albert is<a id="_idIndexMarker888"/> the <strong class="bold">Nvidia Jetson Nano</strong>. This is a rugged little single-board computer with a big heat sink. This CPU is specifically designed to run AI code, with<a id="_idIndexMarker889"/> 128 <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) and four Arm A57 <strong class="bold">central processing units</strong> (<strong class="bold">CPUs</strong>) running at 1.43 GHz, and packed with peripheral <a id="_idIndexMarker890"/>ports and external I/O capability. This is an ideal board for our needs for running a robot with perception and decision-making abilities. The Jetson sits on a development board, which provides an interface to all of its capabilities. Any of the other members of the Jetson family (TX2, Xavier, and AGX) will also work, but I’ve sized the code in the book to fit on the Nano specifically. This is a link to purchase the Jetson Nano computer: <a href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit">https://developer.nvidia.com/embedded/jetson-nano-developer-kit</a>.</p>
<p>I’ve added a Wi-Fi card, which is mounted underneath the CPU. This is the one I used: <a href="https://www.amazon.com/gp/product/B07SGDRG34">https://www.amazon.com/gp/product/B07SGDRG34</a>.</p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor427"/>Effectors – robot arm</h2>
<p>I can’t say enough about <a id="_idIndexMarker891"/>how much of an upgrade the digital servo arm used in this book (<a href="https://www.hiwonder.com.cn/store/learn/42.html">https://www.hiwonder.com.cn/store/learn/42.html</a>) is over the previous analog servo arm I used in the first edition of the book. Some of the advantages of this arm are as follows:</p>
<ul>
<li>This arm uses <strong class="bold">digital servos</strong>, and this simplifies the wiring of the arm, since we just plug one servo<a id="_idIndexMarker892"/> into the next in a series. Rather than being controlled by analog signals, these servos have a digital serial interface that gives us not just fine control, but the ability to determine where the motors actually are, rather than where they were commanded to be. Why is this important? If the arm hits something and can’t continue to move, it will stop. This is not a surprise, but then when you ask for the motor position, it tells you where it stopped. This means you can use the arm itself as a sensor! To determine where the floor is, you command the arm to a downward position, let it stop when it hits the floor, and read the motor positions to see where the floor actually is relative to the arm.</li>
<li>This arm also lets you move the arm manually – with your hands – and then read the position of the arm, which is very useful when designing poses.</li>
<li>Another useful feature<a id="_idIndexMarker893"/> is the <strong class="bold">self-programming mode</strong>. You can put the arm in <em class="italic">program</em> mode using the relevant button, then move the arm and push the other button (labeled <em class="italic">run</em>), and the program moves into the arm without the computer. Then you can push <em class="italic">program</em> again to store these moves, and then hit <em class="italic">run</em> and it will play back what you did. This is useful for testing out moves. I used it to prototype grasping positions for picking up toys.</li>
</ul>
<p>If you would like to <a id="_idIndexMarker894"/>control a simulated robot arm instead of a real one, there is a tutorial at <a href="https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot">https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot</a>.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor428"/>Arm controller</h2>
<p>There is an<a id="_idIndexMarker895"/> important note on wiring up the arm – the arm draws far more electrical power than the USB is capable of supporting. Do <em class="italic">not</em> run the arm without a power supply connected to the power port. The arm will run with the power straight from the battery, without using the DC/DC converter. I added a master power switch to the robot so I can turn it on and off safely. Without the power switch, the only way to shut down the robot is by physically unplugging the battery, which is problematic.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor429"/>Arduino microcontroller and motor controller</h2>
<p>The <a id="_idIndexMarker896"/>Arduino UNO is integrated into the motor controller, which is plugged into the Arduino’s <strong class="bold">General Purpose Input/Output</strong> (<strong class="bold">GPIO</strong>) ports. This is the simplest way to create a computer<a id="_idIndexMarker897"/> interface to the four motors that drive the base of the robot. The motor controller has to have power from the batteries (again without going through the DC/DC converter). The Arduino uses <strong class="bold">Pulse Width Modulation</strong> (<strong class="bold">PWM</strong>) to <a id="_idIndexMarker898"/>control the four brushed DC motors in the drive base. The motor controller is from Adafruit, and is available at this website: <a href="https://www.adafruit.com/product/1438">https://www.adafruit.com/product/1438</a>.</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor430"/>Sensor – USB camera</h2>
<p>The main sensor of<a id="_idIndexMarker899"/> Albert is the wide <strong class="bold">field-of-view</strong> (<strong class="bold">FOV</strong>) camera<a id="_idIndexMarker900"/> that has a USB interface. Any number of cameras will fill this need, including many webcams. My camera has a 170-degree FOV and a resolution of 1,024x768 pixels. You can use a camera with higher resolution but make the USB ROS camera driver downsample the image to 1,024x768 so that we know that the rest of the software can handle the bandwidth. I used a color camera with an<a id="_idIndexMarker901"/> RGB (three-color) output.</p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor431"/>Sensor and effector – audio interface</h2>
<p>I bought a <a id="_idIndexMarker902"/>USB audio card (<a href="https://www.amazon.com/gp/product/B08R95XJW8">https://www.amazon.com/gp/product/B08R95XJW8</a>) to support the voice input and output on the robot. This audio interface has both a microphone and some small speakers. This unit makes our interface to the voice interface for speech recognition and text-to-speech output. This unit is well constructed and sturdy and it does a good job with music as well.</p>
<p>Here is a handy wiring diagram of how Albert goes together:</p>
<div><div><img alt="Figure 12.2 – Block wiring diagram of Albert the Robot" src="img/B19846_12_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Block wiring diagram of Albert the Robot</p>
<p>Now that we have Albert the Robot assembled, we can use it for the examples in the book. Albert’s a very versatile platform with a lot of capability, as you will see.</p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor432"/>Robot safety tips</h1>
<p>Let’s quickly look at some safety tips<a id="_idIndexMarker903"/> related to working around robots:</p>
<ul>
<li>We are using fairly high-current batteries and drive systems. Be very careful with wiring and look out for shorts, where the positive and negative wires are touching. It is not a bad idea to put a fuse between the battery and the power supply of about 10 Amps to protect against accidental shorts.</li>
<li>Be careful when the robot is operating. It may suddenly change direction or get stuck. I have a policy of not sitting down, having my hands in my pockets, or using a cell phone when the robot’s motor drives are activated. You need to be paying attention.</li>
<li>Beware of the pinch points in the robot arm – you can get a finger caught in them quite easily (as I have learned). Don’t put your fingers inside the joints with the power turned on.</li>
<li>Have a checklist for setting up the robot, starting all the software, and turning on the hardware. This will stop you worrying that you have forgotten a step.</li>
<li>In general, here are the steps you should be following. Turn on the robot’s power. Wait for the computer to boot up. Connect to the onboard computer from your laptop or desktop <a id="_idIndexMarker904"/>using <strong class="bold">Virtual Network Computing</strong> (<strong class="bold">VNC</strong>). Start the onboard software, and then start giving commands.</li>
<li>Beware when <a id="_idIndexMarker905"/>charging lithium batteries; they can catch fire. It is best to charge them inside a metal box. I used NiCad batteries in my Albert prototype for this reason – no fire hazard. Lithium batteries are lighter and stronger, but they have their downsides as well, such as catching fire, being a hazardous chemical, and permanently losing charge when frozen.</li>
</ul>
</div>
</body></html>