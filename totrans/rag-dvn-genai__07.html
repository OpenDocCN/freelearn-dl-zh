<html><head></head><body>
  <div><h1 class="chapterNumber">7</h1>
    <h1 id="_idParaDest-183" class="chapterTitle">Building Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex</h1>
    <p class="normal">Scaled datasets can rapidly become challenging to manage. In real-life projects, data management generates more headaches than AI! Project managers, consultants, and developers constantly struggle to obtain the necessary data to get any project running, let alone a RAG-driven generative AI application. Data is often unstructured before it becomes organized in one way or another through painful decision-making processes. Wikipedia is a good example of how scaling data leads to mostly reliable but sometimes incorrect information. Real-life projects often evolve the way Wikipedia does. Data keeps piling up in a company, challenging database administrators, project managers, and users.</p>
    <p class="normal">One of the main problems is seeing how large amounts of data fit together, and <strong class="keyWord">knowledge graphs</strong> provide an <a id="_idIndexMarker445"/>effective way of visualizing the relationships between different types of data. This chapter begins by defining the architecture of a knowledge base ecosystem designed for RAG-driven generative AI. The ecosystem contains three pipelines: data collection, populating a vector store, and running a knowledge graph index-based RAG program. We will then build <em class="italic">Pipeline 1: Collecting and preparing the documents</em>, in which we will build an automated Wikipedia retrieval program with the Wikipedia API. We will simply choose a topic based on a Wikipedia page and then let the program retrieve the metadata we need to collect and prepare the data. The system will be flexible and allow you to choose any topic you wish. The use case to first run the program is a marketing knowledge base for students who want to upskill for a new job, for example. The next step is to build <em class="italic">Pipeline 2: Creating and populating the Deep Lake vector store</em>. We will load the data in a vector store leveraging Deep Lake’s in-built automated chunking and OpenAI embedding functionality. We will peek into the dataset to explore how this marvel of technology does the job.</p>
    <p class="normal">Finally, we will build <em class="italic">Pipeline 3: Knowledge graph index-based RAG</em>, where LlamaIndex will automatically build a knowledge graph index. It will be exciting to see how the index function churns through our data and produces a graph showing semantic relationships contained in our data. We will then query the graph with LlamaIndex’s in-built OpenAI functionality to automatically manage user inputs and produce a response. We will also see how re-ranking can be done and implement metrics to calculate and display the system’s performance.</p>
    <p class="normal">This chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Defining knowledge graphs</li>
      <li class="bulletList">Implementing the Wikipedia API to prepare summaries and content</li>
      <li class="bulletList">Citing Wikipedia sources in an ethical approach</li>
      <li class="bulletList">Populating a Deep Lake vector store with Wikipedia data</li>
      <li class="bulletList">Building a knowledge graph index with LlamaIndex</li>
      <li class="bulletList">Displaying the LlamaIndex knowledge graph</li>
      <li class="bulletList">Interacting with the knowledge graph</li>
      <li class="bulletList">Generating retrieval responses with the knowledge graph</li>
      <li class="bulletList">Re-ranking the order retrieval responses to choose a better output</li>
      <li class="bulletList">Evaluating and measuring the outputs with metrics</li>
    </ul>
    <p class="normal">Let’s begin by defining the architecture of RAG for knowledge-based semantic search.</p>
    <h1 id="_idParaDest-184" class="heading-1">The architecture of RAG for knowledge-graph-based semantic search</h1>
    <p class="normal">As established, we will build a<a id="_idIndexMarker446"/> graph-based RAG program in this chapter. The graph will enable us to visually map out the relationships between the documents of a RAG dataset. It can be created automatically with LlamaIndex, as we will do in the <em class="italic">Pipeline 3: Knowledge graph index-based RAG</em> section of this chapter. The program in this chapter will be designed for any Wikipedia topic, as illustrated in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_07_01.png" alt="A diagram of a graph  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.1: From a Wikipedia topic to interacting with a graph-based vector store index</p>
    <p class="normal">We will first implement a marketing agency for which a knowledge graph can visually map out the complex relationships between different marketing concepts. Then, you can go back and explore<a id="_idIndexMarker447"/> any topic you wish once you understand the process. In simpler words, we will implement the three pipelines seamlessly to:</p>
    <ul>
      <li class="bulletList">Select a Wikipedia topic related to <em class="italic">marketing</em>. Then, you can run the process with the topic of your choice to explore the ecosystem.</li>
      <li class="bulletList">Generate a corpus of Wikipedia pages with the Wikipedia API.</li>
      <li class="bulletList">Retrieve and store the citations for each page.</li>
      <li class="bulletList">Retrieve and store the URLs for each page.</li>
      <li class="bulletList">Retrieve and upsert the content of the URLs in a Deep Lake vector store.</li>
      <li class="bulletList">Build a knowledge base index with LlamaIndex.</li>
      <li class="bulletList">Define a user input prompt.</li>
      <li class="bulletList">Query the knowledge base index.</li>
      <li class="bulletList">Let LlamaIndex’s in-built LLM functionality, based on OpenAI’s embedding models, produce a response based on the embedded data in the knowledge graph.</li>
      <li class="bulletList">Evaluate the LLM’s response with a sentence transformer.</li>
      <li class="bulletList">Evaluate the LLM’s response with a human feedback score.</li>
      <li class="bulletList">Provide time metrics for the key functions, which you can extend to other functions if necessary.</li>
      <li class="bulletList">Run metric calculations and display the results.</li>
    </ul>
    <p class="normal">To attain our goal, we will implement three pipelines leveraging the components we have already built in the previous<a id="_idIndexMarker448"/> chapters, as illustrated in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_07_02.png" alt="A diagram of a graph  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.2: Knowledge graph ecosystem for index-based RAG</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 1: Collecting and preparing the documents</strong> will involve building a Wikipedia program using the Wikipedia API to retrieve links from a Wikipedia page and the metadata for all the pages (summary, URL, and citation data). Then, we will load and parse the URLs to prepare the data for upserting.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline 2: Creating and populating the Deep Lake vector store</strong> will embed and upsert parsed content of the Wikipedia pages prepared by <em class="italic">Pipeline 1</em> to a Deep Lake vector store.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline 3: Knowledge graph index-based RAG</strong> will build the knowledge graph index using embeddings with LlamaIndex and display it. Then, we will build the functionality to query the knowledge base index and let LlamaIndex’s in-built LLM generate the response based on the updated dataset.</li>
    </ul>
    <div><p class="normal">In this chapter’s scenario, we are directly implementing an augmented retrieval system leveraging OpenAI’s embedding models more than we are augmenting inputs. This implementation shows the many ways we can improve real-time data retrieval with LLMs. There are no conventional rules. What works, works!</p>
    </div>
    <p class="normal">The ecosystem of the<a id="_idIndexMarker449"/> three pipelines will be controlled by a scenario that will enable an administrator to either query the vector base or add new Wikipedia pages, as we will implement in this chapter. As such, the architecture of the ecosystem allows for indefinite scaling since it processes and populates the vector dataset one set of Wikipedia pages at a time. The system only uses a CPU and an optimized amount of memory. There are limits to this approach since the LlamaIndex knowledge graph index is loaded with the entire dataset. We can only load portions of the dataset as the vector store grows. Or, we can create one Deep Lake vector store per topic and run queries on multiple datasets. These are decisions to make in real-life projects that require careful decision-making and planning depending on the specific requirements of each project.</p>
    <p class="normal">We will now dive into the code, beginning a tree-to-graph sandbox.</p>
    <h2 id="_idParaDest-185" class="heading-2">Building graphs from trees</h2>
    <p class="normal">A graph is a collection <a id="_idIndexMarker450"/>of nodes (or vertices) connected by edges (or arcs). Nodes represent entities, and edges represent relationships or connections between these entities. For instance, in our chapter’s use case, nodes could represent various marketing strategies, and the edges could show how these strategies are interconnected. This helps new customers understand how different marketing tactics work together to achieve overall business goals, facilitating clearer communication and more effective strategy planning. You can play around with the tree-to-graph sandbox before building the pipelines in this chapter.</p>
    <p class="normal">You may open <code class="inlineCode">Tree-2-Graph.ipynb</code> on GitHub. The provided program is designed to visually represent relationships in a tree structure using NetworkX and Matplotlib in Python. It specifically creates a directed graph from given pairs, checks and marks friendships, and then <a id="_idIndexMarker451"/>displays this tree with customized visual attributes.</p>
    <p class="normal">The program first defines the main functions:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">build_tree_from_pairs(pairs)</code>: Constructs a directed graph (tree) from a list of node pairs, potentially identifying a root node</li>
      <li class="bulletList"><code class="inlineCode">check_relationships(pairs, friends)</code>: Checks and prints the friendship status for each pair</li>
      <li class="bulletList"><code class="inlineCode">draw_tree(G, layout_choice, root, friends)</code>: Visualizes the tree using <code class="inlineCode">matplotlib</code>, applying different styles to edges based on friendship status and different layout options for node positioning</li>
    </ul>
    <p class="normal">Then, the program executes the process from tree to graph:</p>
    <ul>
      <li class="bulletList">Node pairs and friendship data are defined.</li>
      <li class="bulletList">The tree is built from the pairs.</li>
      <li class="bulletList">Relationships are checked against the friendship data.</li>
      <li class="bulletList">The tree is drawn using a selected layout, with edges styled differently to denote friendship.</li>
    </ul>
    <p class="normal">For example, the program first defines a set of node pairs with their pairs of friends:</p>
    <pre class="programlisting code"><code class="hljs-code"># Pairs
pairs = [('a', 'b'), ('b', 'e'), ('e', 'm'), ('m', 'p'), ('a', 'z'), ('b', 'q')]
friends = {('a', 'b'), ('b', 'e'), ('e', 'm'), ('m', 'p')}
</code></pre>
    <p class="normal">Notice that <code class="inlineCode">('a', 'z')</code> are not friends because they are not on the <code class="inlineCode">friends</code> list. Neither are <code class="inlineCode">('b', 'q')</code>. You can imagine any type of relationship between the pairs, such as the same customer age, similar job, same country, or any other concept you wish to represent. For instance, the <code class="inlineCode">friends</code> list could contain relationships between friends on social media, friends living in the same country, or anything else you can imagine or need!</p>
    <p class="normal">The program then builds the tree and checks the relationships:</p>
    <pre class="programlisting code"><code class="hljs-code"># Build the tree
tree, root = build_tree_from_pairs(pairs)
# Check relationships
check_relationships(pairs, friends)
</code></pre>
    <p class="normal">The output shows which pairs are friends and which ones are not:</p>
    <pre class="programlisting con"><code class="hljs-con">Pair ('a', 'b'): friend
Pair ('b', 'e'): friend
Pair ('e', 'm'): friend
Pair ('m', 'p'): friend
Pair ('a', 'z'): not friend
Pair ('b', 'q'): not friend
</code></pre>
    <p class="normal">The output can be used to provide useful information for similarity searches. The program now draws the graph with the <code class="inlineCode">'spring'</code> layout:</p>
    <pre class="programlisting code"><code class="hljs-code"># Draw the tree
layout_choice = 'spring'  # Define your layout choice here
draw_tree(tree, layout_choice=layout_choice, root=root, friends=friends)
</code></pre>
    <p class="normal">The <code class="inlineCode">'spring'</code> layout <a id="_idIndexMarker452"/>attracts nodes attracted by edges, simulating the effect of springs. It also ensures that all nodes repel each other to avoid overlapping. You can dig into the <code class="inlineCode">draw_tree</code> function to explore and select other layouts listed there. You can also modify the colors and line styles.</p>
    <p class="normal">In this case, the pairs of friends are represented with solid lines, and the pairs that are not friends are represented with dashes, as shown in the following graph:</p>
    <figure class="mediaobject"><img src="img/B31169_07_03.png" alt="A diagram of a tree  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.3: Example of a spring layout</p>
    <p class="normal">You can play with this <a id="_idIndexMarker453"/>sandbox graph with different pairs of nodes. If you imagine doing this with hundreds of nodes, you will begin to appreciate the automated functionality we will build in this chapter with LlamaIndex’s knowledge graph index!</p>
    <p class="normal">Let’s go from the architecture to the code, starting by collecting and preparing the documents.</p>
    <h1 id="_idParaDest-186" class="heading-1">Pipeline 1: Collecting and preparing the documents</h1>
    <p class="normal">The code in this section retrieves the metadata we need from Wikipedia, retrieves the documents, cleans them, and <a id="_idIndexMarker454"/>aggregates them to be ready for insertion into the Deep Lake vector<a id="_idIndexMarker455"/> store. This process is illustrated in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_07_04.png" alt="A diagram of a document  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.4: Pipeline 1 flow chart</p>
    <p class="normal"><em class="italic">Pipeline 1</em> includes two notebooks:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Wikipedia_API.ipynb</code>, in which we will implement the Wikipedia API to retrieve the URLs of the pages related to the root page of the topic we selected, including the citations for each page. As mentioned, the topic is “marketing” in our case.</li>
      <li class="bulletList"><code class="inlineCode">Knowledge_Graph_Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb</code>, in which we will implement all three pipelines. In Pipeline 1, it will fetch the URLs provided by the <code class="inlineCode">Wikipedia_API</code> notebook, clean them, and load and aggregate them for upserting.</li>
    </ul>
    <p class="normal">We will begin by implementing the Wikipedia API.</p>
    <h2 id="_idParaDest-187" class="heading-2">Retrieving Wikipedia data and metadata</h2>
    <p class="normal">Let’s begin by building a<a id="_idIndexMarker456"/> program to interact with the Wikipedia API to retrieve information about a specific topic, tokenize the retrieved text, and manage citations from Wikipedia <a id="_idIndexMarker457"/>articles. You may open <code class="inlineCode">Wikipedia_API.ipynb</code> in the GitHub repository and follow along.</p>
    <p class="normal">The program begins by installing the <code class="inlineCode">wikipediaapi</code> library we need:</p>
    <pre class="programlisting code"><code class="hljs-code">try:
  import wikipediaapi
except:
  !pip install Wikipedia-API==0.6.0
  import wikipediaapi
</code></pre>
    <p class="normal">The next step is to define the tokenization function that will be called to count the number of tokens of a summary, as shown in the following excerpt:</p>
    <pre class="programlisting code"><code class="hljs-code">def nb_tokens(text):
    # More sophisticated tokenization which includes punctuation
    tokens = word_tokenize(text)
    return len(tokens)
</code></pre>
    <p class="normal">This function takes a string of text as input and returns the number of tokens in the text, using the NLTK library for sophisticated tokenization, including punctuation. Next, to start retrieving data, we need to set up an instance of the Wikipedia API with a specified language and user agent:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create an instance of the Wikipedia API with a detailed user agent
wiki = wikipediaapi.Wikipedia(
    language='en',
    user_agent='Knowledge/1.0 ([USER AGENT EMAIL)'
)
</code></pre>
    <p class="normal">In this case, English was defined with <code class="inlineCode">'en'</code>, and you must enter the user agent information, such as an email address, for example. We can now define the main topic and filename associated with the Wikipedia page of interest:</p>
    <pre class="programlisting code"><code class="hljs-code">topic="Marketing"     # topic
filename="Marketing"  # filename for saving the outputs
maxl=100
</code></pre>
    <p class="normal">The three parameters defined are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">topic</code>: The topic of the retrieval process</li>
      <li class="bulletList"><code class="inlineCode">filename</code>: The name of the topic that will customize the files we produce, which can be different from the topic</li>
      <li class="bulletList"><code class="inlineCode">maxl</code>: The maximum<a id="_idIndexMarker458"/> number of URL links of the pages we will retrieve</li>
    </ul>
    <p class="normal">We now need to <a id="_idIndexMarker459"/>retrieve the summary of the specified Wikipedia page, check if the page exists, and print its summary:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap # to wrap the text and display it in paragraphs
page=wiki.page(topic)
if page.exists()==True:
  print("Page - Exists: %s" % page.exists())
  summary=page.summary
  # number of tokens)
  nbt=nb_tokens(summary)
  print("Number of tokens: ",nbt)
  # Use textwrap to wrap the summary text to a specified width, e.g., 70 characters
  wrapped_text = textwrap.fill(summary, width=60)
  # Print the wrapped summary text
  print(wrapped_text)
else:
  print("Page does not exist")
</code></pre>
    <p class="normal">The output provides the control information requested:</p>
    <pre class="programlisting con"><code class="hljs-con">Page - Exists: True
Number of tokens:  229
Marketing is the act of satisfying and retaining customers.
It is one of the primary components of business management
and commerce. Marketing is typically conducted by the seller, typically a retailer or manufacturer…
</code></pre>
    <p class="normal">The information provided shows if we are on the right track or not before running a full search on the main page of the topic:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Page - Exists: True</code> confirms that the page exists. If not, the <code class="inlineCode">print("Page does not exist")</code> message will be displayed.</li>
      <li class="bulletList"><code class="inlineCode">Number of tokens: 229</code> provides us with insights into the size of the content we are retrieving for project management assessments.</li>
      <li class="bulletList">The output of <code class="inlineCode">summary=page.summary</code> displays a summary of the page.</li>
    </ul>
    <p class="normal">In this case, the page exists, fits <a id="_idIndexMarker460"/>our topic, and the summary makes sense. Before we continue, we <a id="_idIndexMarker461"/>check if we are working on the right page to be sure:</p>
    <pre class="programlisting code"><code class="hljs-code">print(page.fullurl)
</code></pre>
    <p class="normal">The output is correct:</p>
    <pre class="programlisting con"><code class="hljs-con">https://en.wikipedia.org/wiki/Marketing
</code></pre>
    <p class="normal">We are now ready to retrieve the URLs, links, and summaries on the target page:</p>
    <pre class="programlisting code"><code class="hljs-code"># prompt: read the program up to this cell. Then retrieve all the links for this page: print the link and a summary of each link.
# Get all the links on the page
links = page.links
# Print the link and a summary of each link
urls = []
counter=0
for link in links:
  try:
    counter+=1
    print(f"Link {counter}: {link}")
    summary = wiki.page(link).summary
    print(f"Link: {link}")
    print(wiki.page(link).fullurl)
    urls.append(wiki.page(link).fullurl)
    print(f"Summary: {summary}")
    if counter&gt;=maxl:
      break
  except page.exists()==False:
    # Ignore pages that don't exist
    pass
print(counter)
print(urls)
</code></pre>
    <p class="normal">The function is limited to <code class="inlineCode">maxl</code>, defined at the beginning of the program. The function will retrieve URL links up to <code class="inlineCode">maxl</code> links, or less if the page contains fewer links than the maximum requested. We then check the output before moving on to the next step and generating files:</p>
    <pre class="programlisting con"><code class="hljs-con">Link 1: 24-hour news cycle
Link: 24-hour news cycle
https://en.wikipedia.org/wiki/24-hour_news_cycle
Summary: The 24-hour news cycle (or 24/7 news cycle) is 24-hour investigation and reporting of news, concomitant with fast-paced lifestyles…
</code></pre>
    <p class="normal">We observe that we have<a id="_idIndexMarker462"/> the information we need, and the summaries are acceptable:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Link 1</code>: The link counter</li>
      <li class="bulletList"><code class="inlineCode">Link</code>: The actual link to the page retrieved from the main topic page</li>
      <li class="bulletList"><code class="inlineCode">Summary</code>: A summary of the link to the page</li>
    </ul>
    <p class="normal">The next step is to <a id="_idIndexMarker463"/>apply the function we just built to generate the text file containing citations for the links retrieved from a Wikipedia page and their URLs:</p>
    <pre class="programlisting code"><code class="hljs-code">from datetime import datetime
# Get all the links on the page
links = page.links
# Prepare a file to store the outputs
fname = filename+"_citations.txt"
with open(fname, "w") as file:
    # Write the citation header
    file.write(f"Citation. In Wikipedia, The Free Encyclopedia. Pages retrieved from the following Wikipedia contributors on {datetime.now()}\n")
    file.write("Root page: " + page.fullurl + "\n")
    counter = 0
    urls = []…
</code></pre>
    <p class="normal"><code class="inlineCode">urls = []</code> will be appended to have the full list of URLs we need for the final step. The output is a file containing the name of the topic, <code class="inlineCode">datetime</code>, and the citations beginning with the citation text:</p>
    <pre class="programlisting code"><code class="hljs-code">Citation. In Wikipedia, The Free Encyclopedia. Pages retrieved from the following Wikipedia contributors on {datetime.now()}\n")
</code></pre>
    <p class="normal">The output, in this case, is a file named <code class="inlineCode">Marketing_citations.txt</code>. The file was downloaded and uploaded to the <code class="inlineCode">/citations</code> directory of this chapter’s directory in the GitHub repository.</p>
    <p class="normal">With that, the citations page has been generated, displayed in this notebook, and also saved in the GitHub repository to respect Wikipedia’s citation terms. The final step is to generate the file<a id="_idIndexMarker464"/> containing the list of URLs we will use to fetch the content of the pages we<a id="_idIndexMarker465"/> need. We first display the URLs:</p>
    <pre class="programlisting code"><code class="hljs-code">urls
</code></pre>
    <p class="normal">The output confirms we have the URLs required:</p>
    <pre class="programlisting con"><code class="hljs-con">['https://en.wikipedia.org/wiki/Marketing',
 'https://en.wikipedia.org/wiki/24-hour_news_cycle',
 'https://en.wikipedia.org/wiki/Account-based_marketing',
…
</code></pre>
    <p class="normal">The URLs are written in a file with the topic as a prefix:</p>
    <pre class="programlisting code"><code class="hljs-code"># Write URLs to a file
ufname = filename+"_urls.txt"
with open(ufname, 'w') as file:
    for url in urls:
        file.write(url + '\n')
print("URLs have been written to urls.txt")
</code></pre>
    <p class="normal">In this case, the output is a file named <code class="inlineCode">Marketing_urls.txt</code> that contains the URLs of the pages we need to fetch. The file was downloaded and uploaded to the <code class="inlineCode">/citations</code> directory of the chapter’s directory in the GitHub repository.</p>
    <p class="normal">We are now ready to prepare the data for upsertion.</p>
    <h2 id="_idParaDest-188" class="heading-2">Preparing the data for upsertion</h2>
    <p class="normal">The URLs provided by the Wikipedia API in the <code class="inlineCode">Wikipedia_API.ipynb</code> notebook will be processed in the <code class="inlineCode">Knowledge_Graph_ Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb</code> notebook you can find in the<a id="_idIndexMarker466"/> GitHub directory of the chapter. The <em class="italic">Installing the environment</em> section of this notebook is almost the same section as its equivalent section in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, and <em class="chapterRef">Chapter 3</em>, <em class="italic">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</em>. In this chapter, however, the list of URLs was generated by the <code class="inlineCode">Wikipedia_API.ipynb</code> notebook, and we will retrieve it.</p>
    <p class="normal">First, go to the <em class="italic">Scenario</em> section of the notebook to define the strategy of the workflow:</p>
    <pre class="programlisting code"><code class="hljs-code">#File name for file management
graph_name="Marketing"
# Path for vector store and dataset
db="hub://denis76/marketing01"
vector_store_path = db
dataset_path = db
#if True upserts data; if False, passes upserting and goes to connection
pop_vs=True
# if pop_vs==True, overwrite=True will overwrite dataset, False will append it:
ow=True
</code></pre>
    <p class="normal">The parameters will determine the behavior of the three pipelines in the notebook:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">graph_name="Marketing"</code>: The prefix (topic) of the files we will read and write.</li>
      <li class="bulletList"><code class="inlineCode">db="hub://denis76/marketing01"</code>: The name of the Deep Lake vector store. You can choose the name of the dataset you wish.</li>
      <li class="bulletList"><code class="inlineCode">vector_store_path = db</code>: The path to the vector store.</li>
      <li class="bulletList"><code class="inlineCode">dataset_path = db</code>: The path to the dataset of the vector store.</li>
      <li class="bulletList"><code class="inlineCode">pop_vs=True</code>: Activates data insertion if <code class="inlineCode">True</code> and deactivates it if <code class="inlineCode">False</code>.</li>
      <li class="bulletList"><code class="inlineCode">ow=True</code>: Overwrites<a id="_idIndexMarker467"/> the existing dataset if <code class="inlineCode">True</code> and appends it if <code class="inlineCode">False</code>.</li>
    </ul>
    <p class="normal">Then, we can launch the <em class="italic">Pipeline 1: Collecting and preparing the documents</em> section of the notebook. The program will download the URL list generated in the previous section of this chapter:</p>
    <pre class="programlisting code"><code class="hljs-code"># Define your variables
if pop_vs==True:
  directory = "Chapter07/citations"
  file_name = graph_name+"_urls.txt"
  download(directory,file_name)
</code></pre>
    <p class="normal">It will then read the file and store the URLs in a list named <code class="inlineCode">urls</code>. The rest of the code in the <em class="italic">Pipeline 1: Collecting and preparing the documents</em> section of this notebook follows the same process as the <code class="inlineCode">Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb</code> notebook from <em class="italic">Chapter 3</em>. In <em class="italic">Chapter 3</em>, the URLs of the web pages were entered manually in a list.</p>
    <p class="normal">The code will fetch the content in the list of URLs. The program then cleans and prepares the data to populate<a id="_idIndexMarker468"/> the Deep Lake vector store.</p>
    <h1 id="_idParaDest-189" class="heading-1">Pipeline 2: Creating and populating the Deep Lake vector store</h1>
    <p class="normal">The pipeline in this section of <code class="inlineCode">Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb</code> was built with the code of <em class="italic">Pipeline 2</em> from <em class="italic">Chapter 3</em>. We can see that by creating pipelines as components, we can rapidly repurpose and adapt them to other applications. Also, Activeloop Deep Lake <a id="_idIndexMarker469"/>possesses in-built default chunking, embedding, and <a id="_idIndexMarker470"/>upserting functions, making it seamless to integrate various types of unstructured data, as in the case of the Wikipedia documents we are upserting.</p>
    <p class="normal">The output of the <code class="inlineCode">display_record(record_number)</code> function shows how seamless the process is. The output displays the ID and metadata such as the file information, the data collected, the text, and the embedded vector:</p>
    <pre class="programlisting con"><code class="hljs-con">ID:
['a61734be-fe23-421e-9a8b-db6593c48e08']
Metadata:
file_path: /content/data/24-hour_news_cycle.txt
file_name: 24-hour_news_cycle.txt
file_type: text/plain
file_size: 2763
creation_date: 2024-07-05
last_modified_date: 2024-07-05
…
Text:
['24hour investigation and reporting of news concomitant with fastpaced lifestyles This article is about the fastpaced cycle of news media in technologically advanced societies.
Embedding:
[-0.00040736704249866307, 0.009565318934619427, 0.015906672924757004, -0.009085721336305141, …]
</code></pre>
    <p class="normal">And with that, we have successfully repurposed the <em class="italic">Pipeline 2</em> component of <em class="italic">Chapter 3</em> and can now move on and build the graph knowledge index.</p>
    <h1 id="_idParaDest-190" class="heading-1">Pipeline 3: Knowledge graph index-based RAG</h1>
    <p class="normal">It’s time to create a knowledge<a id="_idIndexMarker471"/> graph index-based RAG pipeline and interact with it. As illustrated in the following figure, we have a lot of work to do:</p>
    <figure class="mediaobject"><img src="img/B31169_07_05.png" alt="A diagram of a graph  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.5: Building knowledge graph-index RAG from scratch</p>
    <p class="normal">In this section, we will:</p>
    <ul>
      <li class="bulletList">Generate the knowledge graph index</li>
      <li class="bulletList">Display the graph</li>
      <li class="bulletList">Define the user prompt</li>
      <li class="bulletList">Define the hyperparameters of LlamaIndex’s in-built LLM model</li>
      <li class="bulletList">Install the similarity score packages</li>
      <li class="bulletList">Define the similarity score functions</li>
      <li class="bulletList">Run a sample similarity comparison between the similarity functions</li>
      <li class="bulletList">Re-rank the output vectors of an LLM response</li>
      <li class="bulletList">Run evaluation samples and apply metrics and human feedback scores</li>
      <li class="bulletList">Run metric calculations and display them</li>
    </ul>
    <p class="normal">Let’s go through these steps and begin by generating the knowledge graph index.</p>
    <h2 id="_idParaDest-191" class="heading-2">Generating the knowledge graph index</h2>
    <p class="normal">We will create a knowledge graph index from a set of documents using the <code class="inlineCode">KnowledgeGraphIndex</code> class from the <code class="inlineCode">llama_index.core</code> module. We will also time the index creation process to evaluate performance.</p>
    <p class="normal">The function begins <a id="_idIndexMarker472"/>by recording the start time with <code class="inlineCode">time.time()</code>. In this case, measuring the time is important because it takes quite some time to create the index:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import KnowledgeGraphIndex
import time
# Start the timer
start_time = time.time()
</code></pre>
    <p class="normal">We now create a <code class="inlineCode">KnowledgeGraphIndex</code> with embeddings using the <code class="inlineCode">from_documents</code> method. The function uses the following parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">documents</code> is the set of documents to index</li>
      <li class="bulletList"><code class="inlineCode">max_triplets_per_chunk</code> is set to 2, limiting the number of triplets per chunk to optimize memory usage and processing time</li>
      <li class="bulletList"><code class="inlineCode">include_embeddings</code> is set to <code class="inlineCode">True</code>, indicating that embeddings should be included</li>
    </ul>
    <p class="normal">The graph index is thus created in a few lines of code:</p>
    <pre class="programlisting code"><code class="hljs-code">#graph index with embeddings
graph_index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=2,
    include_embeddings=True,
)
</code></pre>
    <p class="normal">The timer is stopped and the creation time is measured:</p>
    <pre class="programlisting code"><code class="hljs-code"># Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Index creation time: {elapsed_time:.4f} seconds")
print(type(graph_index))
</code></pre>
    <p class="normal">The output displays the time:</p>
    <pre class="programlisting con"><code class="hljs-con">Index creation time: 371.9844 seconds
</code></pre>
    <p class="normal">The graph type is displayed:</p>
    <pre class="programlisting code"><code class="hljs-code">print(type(graph_index))
</code></pre>
    <p class="normal">The output confirms the<a id="_idIndexMarker473"/> knowledge graph index class:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;class 'llama_index.core.indices.knowledge_graph.base.KnowledgeGraphIndex'&gt;
</code></pre>
    <p class="normal">We will now set up a query engine for our knowledge graph index and configure it to manage similarity, response<a id="_idIndexMarker474"/> temperature, and output length parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">#similarity_top_k
k=3
#temperature
temp=0.1
#num_output
mt=1024
graph_query_engine = graph_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</code></pre>
    <p class="normal">The parameters will determine the behavior of the query engine:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">k=3</code> sets the number of top similar results to take into account.</li>
      <li class="bulletList"><code class="inlineCode">temp=0.1</code> sets the temperature parameter, controlling the randomness of the query engine’s response generation. The lower it is, the more precise it is; the higher it is, the more creative it is.</li>
      <li class="bulletList"><code class="inlineCode">mt=1024</code> sets the maximum number of tokens for the output, defining the length of the generated responses.</li>
    </ul>
    <p class="normal">The query engine is then created with the parameters we defined:</p>
    <pre class="programlisting code"><code class="hljs-code">graph_query_engine = graph_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</code></pre>
    <p class="normal">The graph index and query engine are ready. Let’s display the graph.</p>
    <h2 id="_idParaDest-192" class="heading-2">Displaying the graph</h2>
    <p class="normal">We will create a<a id="_idIndexMarker475"/> graph instance, <code class="inlineCode">g</code>, with <code class="inlineCode">pyvis.network</code>, a Python library used for creating interactive network visualizations. The displayed parameters are similar to the ones we defined in the <em class="italic">Building graphs from trees</em> section of this chapter:</p>
    <pre class="programlisting code"><code class="hljs-code">## create graph
from pyvis.network import Network
g = graph_index.get_networkx_graph()
net = Network(notebook=True, cdn_resources="in_line", directed=True)
net.from_nx(g)
# Set node and edge properties: colors and sizes
for node in net.nodes:
    node['color'] = 'lightgray'
    node['size'] = 10
for edge in net.edges:
    edge['color'] = 'black'
    edge['width'] = 1
</code></pre>
    <p class="normal">A directed graph has been created, and now we will save it in an HTML file to display it for further use:</p>
    <pre class="programlisting code"><code class="hljs-code">fgraph="Knowledge_graph_"+ graph_name + ".html"
net.write_html(fgraph)
print(fgraph)
</code></pre>
    <p class="normal">The <code class="inlineCode">graph_name</code> was defined at the beginning of the notebook, in the <em class="italic">Scenario</em> section. We will now display the graph in the notebook as an HTML file:</p>
    <pre class="programlisting code"><code class="hljs-code">from IPython.display import HTML
# Load the HTML content from a file and display it
with open(fgraph, 'r') as file:
    html_content = file.read()
# Display the HTML in the notebook
display(HTML(html_content))
</code></pre>
    <p class="normal">You can now download the file to display it in your browser to interact with it. You can also visualize it in the notebook, as shown in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_07_06.png" alt="A diagram of marketing strategy  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.6: The knowledge graph</p>
    <p class="normal">We are all set to<a id="_idIndexMarker476"/> interact with the knowledge graph index.</p>
    <h2 id="_idParaDest-193" class="heading-2">Interacting with the knowledge graph index</h2>
    <p class="normal">Let’s now define the<a id="_idIndexMarker477"/> functionality we need to execute the query, as we have done in <em class="italic">Chapter 3</em> in the <em class="italic">Pipeline 3: Index-based RAG</em> section:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">execute_query</code> is the function we created that will execute the query: <code class="inlineCode">response = graph_query_engine.query(user_input)</code>. It also measures the time it takes.</li>
      <li class="bulletList"><code class="inlineCode">user_query="What is the primary goal of marketing for the consumer market?"</code>, which we will use to make the query.</li>
      <li class="bulletList"><code class="inlineCode">response = execute_query(user_query)</code>, which is encapsulated in the request code and displays the response.</li>
    </ul>
    <p class="normal">The output provides the best vectors that we created with the Wikipedia data with the time measurement:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 2.4789 seconds
The primary goal of marketing for the consumer market is to effectively target consumers, understand their behavior, preferences, and needs, and ultimately influence their purchasing decisions.
</code></pre>
    <p class="normal">We will now install similarity <a id="_idIndexMarker478"/>score packages and define the similarity calculation functions we need.</p>
    <h2 id="_idParaDest-194" class="heading-2">Installing the similarity score packages and defining the functions</h2>
    <p class="normal">We will first retrieve <a id="_idIndexMarker479"/>the Hugging Face <a id="_idIndexMarker480"/>token from the <strong class="screenText">Secrets</strong> tab on Google Colab, where it was stored in the settings of the notebook:</p>
    <pre class="programlisting code"><code class="hljs-code">from google.colab import userdata
userdata.get('HF_TOKEN')
</code></pre>
    <p class="normal">In August 2024, the token is optional for Hugging Face’s <code class="inlineCode">sentence-transformers</code>. You can ignore the message and comment the code. Next, we install <code class="inlineCode">sentence-transformers</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install sentence-transformers==3.0.1
</code></pre>
    <p class="normal">We then create a cosine similarity function with embeddings:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    return similarity[0][0]
</code></pre>
    <p class="normal">We import the libraries we need:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
import textwrap
import sys
import io
</code></pre>
    <p class="normal">We have a similarity function and can use it for re-ranking.</p>
    <h2 id="_idParaDest-195" class="heading-2">Re-ranking</h2>
    <p class="normal">In this section, the <a id="_idIndexMarker481"/>program re-ranks the response of a query by reordering the top results to select other, possibly better, ones:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">user_query=" Which experts are often associated with marketing theory?"</code> represents the query we are making.</li>
      <li class="bulletList"><code class="inlineCode">start_time = time.time()</code> records the start time for the query execution.</li>
      <li class="bulletList"><code class="inlineCode">response = execute_query(user_query)</code> executes the query.</li>
      <li class="bulletList"><code class="inlineCode">end_time = time.time()</code> stops the timer, and the query execution time is displayed.</li>
      <li class="bulletList"><code class="inlineCode">for idx, node_with_score in enumerate(response.source_nodes)</code> iterates through the response to retrieve all the nodes in the response.</li>
      <li class="bulletList"><code class="inlineCode">similarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)</code> calculates the similarity score between the user query and the text in the nodes retrieved from the response. All the comparisons are displayed.</li>
      <li class="bulletList"><code class="inlineCode">best_score=similarity_score3</code> stores the best similarity score found.</li>
      <li class="bulletList"><code class="inlineCode">print(textwrap.fill(str(best_text), 100))</code> displays the best re-ranked result.</li>
    </ul>
    <p class="normal">The initial response for the <code class="inlineCode">user_query</code> <code class="inlineCode">"Which experts are often associated with marketing theory?"</code> was:</p>
    <pre class="programlisting con"><code class="hljs-con">Psychologists, cultural anthropologists, and market researchers are often associated with marketing
theory.
</code></pre>
    <p class="normal">The response is acceptable. However, the re-ranked response goes deeper and mentions the names of marketing experts (highlighted in bold font):</p>
    <pre class="programlisting con"><code class="hljs-con">Best Rank: 2
Best Score: 0.5217772722244263
[…In 1380 the German textile manufacturer <strong class="hljs-con-slc">Johann Fugger</strong><strong class="hljs-con-slc">Daniel Defoe</strong>  travelled from Augsburg to Graben in order to gather information on the international textile industry… During this period   a
London merchant published information on trade and economic resources of England and Scotland…]
</code></pre>
    <p class="normal">The re-ranked response is longer and contains raw document content instead of the summary provided by LlamaIndex’s LLM query engine. The original query engine response is better from an LLM perspective. However, it isn’t easy to estimate what an end-user will prefer. Some users like short answers, and some like long documents. We can imagine many other ways of re-ranking documents, such as modifying the prompt, adding documents, and deleting documents. We can even decide to fine-tune an LLM, as we will do in <em class="chapterRef">Chapter 9</em>, <em class="italic">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</em>. We can also introduce human feedback scores as we did in <em class="chapterRef">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em>, because, in many cases, mathematical metrics will not <a id="_idIndexMarker482"/>capture the accuracy of a response (writing fiction, long answers versus short input, and other complex responses). But we need to try anyway!</p>
    <p class="normal">Let’s perform some of the possible metrics for the examples we are going to run.</p>
    <h2 id="_idParaDest-196" class="heading-2">Example metrics</h2>
    <p class="normal">To evaluate the knowledge<a id="_idIndexMarker483"/> graph index’s query engine, we will run ten examples and keep track of the scores. <code class="inlineCode">rscores</code> keeps track of human feedback scores while <code class="inlineCode">scores=[]</code> keeps track of similarity function scores:</p>
    <pre class="programlisting code"><code class="hljs-code"># create an empty array score human feedback scores:
rscores =[]
# create an empty score for similarity function scores
scores=[]
</code></pre>
    <p class="normal">The number of examples can be increased as much as necessary depending on the needs of a project. Each of the ten examples has the same structure:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">user_query</code>, which is the input text for the query engine</li>
      <li class="bulletList"><code class="inlineCode">elapsed_time</code>, which is the result of the time measurement of the system’s response</li>
      <li class="bulletList"><code class="inlineCode">response = execute_query(user_query)</code> executes the query</li>
    </ul>
    <p class="normal">The user query and output are the same as in the example used for the re-ranking function:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 1.9648 seconds
Psychologists, cultural anthropologists, and other experts in behavioral sciences are often
associated with marketing theory.
</code></pre>
    <p class="normal">However, this time, we will run a similarity function and also ask a human for a score:</p>
    <pre class="programlisting code"><code class="hljs-code">text1=str(response)
text2=user_query
similarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)
print(f"Cosine Similarity Score with sentence transformer: {similarity_score3:.3f}")
scores.append(similarity_score3)
human_feedback=0.75
rscores.append(human_feedback)
</code></pre>
    <p class="normal">In this function:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">text1</code> is the query<a id="_idIndexMarker484"/> engine’s response.</li>
      <li class="bulletList"><code class="inlineCode">text2</code> is the user query.</li>
      <li class="bulletList"><code class="inlineCode">similarity_score3</code> is the cosine similarity score.</li>
      <li class="bulletList"><code class="inlineCode">scores.append(similarity_score3)</code> appends the similarity score to scores.</li>
      <li class="bulletList"><code class="inlineCode">human_feedback</code> is the human similarity evaluation. We could replace this score with a document as we did in <em class="chapterRef">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em>, or we could replace the human score with a human text response, which will become the ground truth. In both cases, the similarity score is recalculated with human feedback content.</li>
      <li class="bulletList"><code class="inlineCode">rscores.append(human_feedback)</code> appends the human score to <code class="inlineCode">rscores</code>.</li>
    </ul>
    <p class="normal">Let’s review a few of the ten examples’ outputs and add a comment at the end of each one.</p>
    <div><p class="normal">LLMs are stochastic algorithms. As such, the responses and scores may vary from one run to another.</p>
    </div>
    <ul>
      <li class="bulletList"><strong class="keyWord">Example 1</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">User query</strong>: <code class="inlineCode">Which experts are often associated with marketing theory?</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Response</strong>: Psychologists, cultural anthropologists, and other experts in behavioral sciences are often associated with marketing theory.</li>
          <li class="bulletList level-2"><strong class="keyWord">Cosine similarity score</strong>: <code class="inlineCode">0.809</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Human feedback</strong>: <code class="inlineCode">0.75</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Comment</strong>: The response is acceptable, but it could be more specific and mention the names of experts. However, the prompt is ambiguous and only mentions experts in general.</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Example 3</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">User query</strong>: <code class="inlineCode">What is the difference between B2B and B2C?</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Response</strong>: B2B businesses sell products and services to other companies, while B2C businesses sell directly to customers.</li>
          <li class="bulletList level-2"><strong class="keyWord">Cosine Similarity score</strong>: <code class="inlineCode">0.760</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Human feedback</strong>: <code class="inlineCode">0.8</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Comment</strong>: The response is precise, but in some cases, users like examples.</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Example 7</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">User query</strong>: <code class="inlineCode">What commodity programs does the Agricultural Marketing Service (AMS) maintain?</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Response</strong>: The <strong class="keyWord">Agricultural Marketing Service</strong> (<strong class="keyWord">AMS</strong>) maintains programs<a id="_idIndexMarker485"/> in five commodity<a id="_idIndexMarker486"/> areas: cotton and tobacco, dairy, fruit and vegetable, livestock and seed, and poultry.</li>
          <li class="bulletList level-2"><strong class="keyWord">Cosine Similarity score</strong>: <code class="inlineCode">0.904</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Human feedback</strong>: <code class="inlineCode">0.9</code></li>
          <li class="bulletList level-2"><strong class="keyWord">Comment</strong>: This response is accurate and interesting because the information is contained in a page linked to the main page. Thus, this is information from a linked page to the main page. We could ask Wikipedia to search the links of all the linked pages to the main page and go down several levels. However, the main information we are looking for may be diluted in less relevant data. The decision on the scope of the depth of the data depends on the needs of each project. </li>
        </ul>
      </li>
    </ul>
    <p class="normal">We will now perform metric calculations on the cosine similarity scores and the human feedback scores.</p>
    <h3 id="_idParaDest-197" class="heading-3">Metric calculation and display</h3>
    <p class="normal">The cosine similarity scores of <a id="_idIndexMarker487"/>the examples are <a id="_idIndexMarker488"/>stored in <code class="inlineCode">scores</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">print(len(scores), scores)
</code></pre>
    <p class="normal">The ten scores are displayed:</p>
    <pre class="programlisting con"><code class="hljs-con">10 [0.808918, 0.720165, 0.7599532, 0.8513956, 0.5457667, 0.6963912, 0.9036964, 0.44829217, 0.59976315, 0.47448665]
</code></pre>
    <p class="normal">We could expand the evaluations to as many other examples, depending on the needs of each project. The human feedback scores for the same examples are stored in <code class="inlineCode">rscores</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">print(len(rscores), rscores)
</code></pre>
    <p class="normal">The ten human feedback scores are displayed:</p>
    <pre class="programlisting con"><code class="hljs-con">10 [0.75, 0.5, 0.8, 0.9, 0.65, 0.8, 0.9, 0.2, 0.2, 0.9]
</code></pre>
    <p class="normal">We apply metrics to evaluate the responses:</p>
    <pre class="programlisting code"><code class="hljs-code">mean_score = np.mean(scores)
median_score = np.median(scores)
std_deviation = np.std(scores)
variance = np.var(scores)
min_score = np.min(scores)
max_score = np.max(scores)
range_score = max_score - min_score
percentile_25 = np.percentile(scores, 25)
percentile_75 = np.percentile(scores, 75)
iqr = percentile_75 - percentile_25
</code></pre>
    <p class="normal">Each metric can provide several insights. Let’s go through each of them and the outputs obtained:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Central tendency (mean, median)</strong> gives us an idea of what a typical score looks like.</li>
      <li class="bulletList"><strong class="keyWord">Variability (standard deviation, variance, range, IQR)</strong> tells us how spread out the scores are, indicating the consistency or diversity of the data.</li>
      <li class="bulletList"><strong class="keyWord">Extremes (minimum, maximum)</strong> show the bounds of our dataset.</li>
      <li class="bulletList"><strong class="keyWord">Distribution (percentiles)</strong> provides insights into how scores are distributed across the range of values.</li>
    </ul>
    <p class="normal">Let’s go through these<a id="_idIndexMarker489"/> metrics calculated from the cosine <a id="_idIndexMarker490"/>similarity scores and the human feedback scores and display their outputs:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Mean (average)</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The mean is the sum of all the scores divided by the number of scores.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It gives us the central value of the data, providing an idea of the typical score.</li>
          <li class="bulletList level-2"><strong class="keyWord">Calculation</strong>:</li>
        </ul>
      </li>
    </ol>
    <p class="center"><a id="_idIndexMarker491"/><img src="img/B31169_07_001.png" alt=""/></p>
    <ul>
      <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Mean: 0.68</code></li>
    </ul>
    <ol>
      <li class="numberedList" value="2"><strong class="keyWord">Median</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The median is the middle value when the scores are ordered from smallest to largest.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It provides the central point of the dataset and is less affected by extreme values (outliers) compared to the mean.</li>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Median: 0.71</code></li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">Standard deviation</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The standard deviation measures the average amount by which each score differs from the mean.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It gives an idea of how spread out the scores are around the mean. A higher value indicates more variability.</li>
          <li class="bulletList level-2"><strong class="keyWord">Calculation</strong>:</li>
        </ul>
      </li>
    </ol>
    <p class="center"><a id="_idIndexMarker492"/><img src="img/B31169_07_002.png" alt=""/></p>
    <ul>
      <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Standard Deviation: 0.15</code></li>
    </ul>
    <ol>
      <li class="numberedList" value="4"><strong class="keyWord">Variance</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The variance is the square of the standard deviation.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It also measures<a id="_idIndexMarker493"/> the spread of the scores, showing how much they <a id="_idIndexMarker494"/>vary from the mean.</li>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Variance: 0.02</code></li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">Minimum</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The minimum is the smallest score in the dataset.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It tells us the lowest value.</li>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Minimum: 0.45</code></li>
        </ul>
      </li>
      <li class="numberedList">Maximum:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The maximum is the largest score in the dataset.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It tells us the highest value.</li>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Maximum: 0.90</code></li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">Range</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The range is the difference between the maximum and minimum scores.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It shows the span of the dataset from the lowest to the highest value.</li>
          <li class="bulletList level-2"><strong class="keyWord">Calculation</strong>:</li>
        </ul>
        <p class="normal"><em class="italic">Range</em> = <em class="italic">Maximum</em> - <em class="italic">Minimum</em></p>
        <ul>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Range: 0.46</code></li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">25</strong><sup class="superscript-bold" style="font-weight: bold;">th</sup><strong class="keyWord"> Percentile (Q1)</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The 25<sup class="superscript">th</sup> percentile is the value below which 25% of the scores fall.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It provides a point below which a quarter of the data lies.</li>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">25th Percentile (Q1): 0.56</code></li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">75</strong><sup class="superscript-bold" style="font-weight: bold;">th</sup><strong class="keyWord"> Percentile (Q3)</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The 75<sup class="superscript">th</sup> percentile is the value below which 75% of the scores fall.</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It gives a point below which three-quarters of the data lies.</li>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">75th Percentile (Q3): 0.80</code></li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">Interquartile Range (IQR)</strong>:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Definition</strong>: The IQR is the range between the 25<sup class="superscript">th</sup> percentile (Q1) and the 75<sup class="superscript">th</sup> percentile (Q3).</li>
          <li class="bulletList level-2"><strong class="keyWord">Purpose</strong>: It measures the middle 50% of the data, providing a sense of the data’s spread without being affected by extreme values.</li>
          <li class="bulletList level-2"><strong class="keyWord">Calculation</strong>:</li>
        </ul>
        <p class="normal"><em class="italic">IQR</em> = <em class="italic">Q3</em> – <em class="italic">Q1</em></p>
        <ul>
          <li class="bulletList level-2"><strong class="keyWord">Output</strong>: <code class="inlineCode">Interquartile Range (IQR): 0.24</code></li>
        </ul>
      </li>
    </ol>
    <p class="normal">We have built a<a id="_idIndexMarker495"/> knowledge-graph-based RAG system, interacted<a id="_idIndexMarker496"/> with it, and evaluated it with some examples and metrics. Let’s sum up our journey.</p>
    <h1 id="_idParaDest-198" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we explored the creation of a scalable knowledge-graph-based RAG system using the Wikipedia API and LlamaIndex. The techniques and tools developed are applicable across various domains, including data management, marketing, and any field requiring organized and accessible data retrieval.</p>
    <p class="normal">Our journey began with data collection in <em class="italic">Pipeline 1</em>. This pipeline focused on automating the retrieval of Wikipedia content. Using the Wikipedia API, we built a program to collect metadata and URLs from Wikipedia pages based on a chosen topic, such as marketing. In <em class="italic">Pipeline 2</em>, we created and populated the Deep Lake vector store. The retrieved data from <em class="italic">Pipeline 1</em> was embedded and upserted into the Deep Lake vector store. This pipeline highlighted the ease of integrating vast amounts of data into a structured vector store, ready for further processing and querying. Finally, in <em class="italic">Pipeline 3</em>, we introduced knowledge graph index-based RAG. Using LlamaIndex, we automatically built a knowledge graph index from the embedded data. This index visually mapped out the relationships between different pieces of information, providing a semantic overview of the data. The knowledge graph was then queried using LlamaIndex’s built-in language model to generate optimal responses. We also implemented metrics to evaluate the system’s performance, ensuring accurate and efficient data retrieval.</p>
    <p class="normal">By the end of this chapter, we had constructed a comprehensive, automated RAG-driven knowledge graph system capable of collecting, embedding, and querying vast amounts of Wikipedia data with minimal human intervention. This journey showed the power and potential of combining multiple AI tools and models to create an efficient pipeline for data management and retrieval. You are now all set to implement knowledge graph-based RAG systems in real-life projects. In the next chapter, we will learn how to implement dynamic RAG for short-term usage.</p>
    <h1 id="_idParaDest-199" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with yes or no:</p>
    <ol>
      <li class="numberedList" value="1">Does the chapter focus on building a scalable knowledge-graph-based RAG system using the Wikipedia API and LlamaIndex?</li>
      <li class="numberedList">Is the primary use case discussed in the chapter related to healthcare data management?</li>
      <li class="numberedList">Does <em class="italic">Pipeline 1</em> involve collecting and preparing documents from Wikipedia using an API?</li>
      <li class="numberedList">Is Deep Lake used for creating a relational database in <em class="italic">Pipeline 2</em>?</li>
      <li class="numberedList">Does <em class="italic">Pipeline 3</em> utilize LlamaIndex to build a knowledge graph index?</li>
      <li class="numberedList">Is the system designed to only handle a single specific topic, such as marketing, without flexibility?</li>
      <li class="numberedList">Does the chapter describe how to retrieve URLs and metadata from Wikipedia pages?</li>
      <li class="numberedList">Is a GPU required to run the pipelines described in the chapter?</li>
      <li class="numberedList">Does the knowledge graph index visually map out relationships between pieces of data?</li>
      <li class="numberedList">Is human intervention required at every step to query the knowledge graph index?</li>
    </ol>
    <h1 id="_idParaDest-200" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Wikipedia API GitHub repository: <a href="https://github.com/martin-majlis/Wikipedia-API">https://github.com/martin-majlis/Wikipedia-API</a></li>
      <li class="bulletList">PyVis Network: <em class="italic">Interactive Network Visualization in Python</em>.</li>
    </ul>
    <h1 id="_idParaDest-201" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Hogan, A., Blomqvist, E., Cochez, M., et al. <em class="italic">Knowledge Graphs</em>. <code class="inlineCode">arXiv:2003.02320</code></li>
    </ul>
    <h1 id="_idParaDest-202" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>