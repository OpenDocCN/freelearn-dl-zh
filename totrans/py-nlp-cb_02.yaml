- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Playing with Grammar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grammar is one of the main building blocks of language. Each human language,
    and programming language for that matter, has a set of rules that every person
    speaking it must follow, otherwise risking not being understood. These grammatical
    rules can be uncovered using NLP and are useful for extracting data from sentences.
    For example, using information about the grammatical structure of text, we can
    parse out subjects, objects, and relations between different entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to use different packages to reveal the
    grammatical structure of words and sentences, as well as extract certain parts
    of sentences. These are the topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Counting nouns – plural and singular nouns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the dependency parse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting noun chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the subjects and objects of the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding patterns in text using grammatical information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please follow the installation requirements given in [*Chapter 1*](B18411_01.xhtml#_idTextAnchor013)
    to run the notebooks in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Counting nouns – plural and singular nouns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will do two things: determine whether a noun is plural or
    singular and turn plural nouns into singular, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: You might need these two things for a variety of tasks. For example, you might
    want to count the word statistics, and for that, you most likely need to count
    the singular and plural nouns together. In order to count the plural nouns together
    with singular ones, you need a way to recognize that a word is plural or singular.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To determine whether a noun is singular or plural, we will use `spaCy` via
    two different methods: by looking at the difference between the lemma and the
    actual word and by looking at the `morph` attribute. To inflect these nouns, or
    turn singular nouns into plural or vice versa we will use the `textblob` package.
    We will also see how to determine the noun’s number using GPT-3 through the OpenAI
    API. The code for this section is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter02).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first use `spaCy`’s lemma information to infer whether a noun is singular
    or plural. Then, we will use the `morph` attribute of `Token` objects. We will
    then create a function that uses one of those methods. Finally, we will use GPT-3.5
    to find out the number of nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the code in the file and language utility notebooks. If you run into an
    error saying that the small or large models do not exist, you need to open the
    **lang_utils.ipynb** file, uncomment, and run the statement that downloads the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the **text** variable and process it using the **spaCy** small model
    to get the resulting **Doc** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we loop through the **Doc** object. For each token in the object,
    we check whether it’s a noun and whether the lemma is the same as the word itself.
    Since the lemma is the basic form of the word, if the lemma is different from
    the word, that token is plural:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will check the number of a noun using a different method: the **morph**
    features of a **Token** object. The **morph** features are the morphological features
    of a word, such as number, case, and so on. Since we know that token **3** is
    a noun, we directly access the **morph** features and get the **Number** to get
    the same result as previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we prepare to define a function that returns a tuple, **(noun,
    number)**. In order to better encode the noun number, we use an **Enum** class
    that assigns numbers to different values. We assign **1** to singular and **2**
    to plural. Once we create the class, we can directly refer to the noun number
    variables as **Noun_number.SINGULAR** and **Noun_number.PLURAL**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define the function. It takes as input the text, the **spaCy**
    model, and the method of determining the noun number. The two methods are **lemma**
    and **morph**, the same two methods we used in *steps 3* and *4*, respectively.
    The function outputs a list of tuples, each of the format **(<noun text>, <noun
    number>)**, where the noun number is expressed using the **Noun_number** class
    defined in *step 5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the preceding function and see its performance with different **spaCy**
    models. In this step, we use the small **spaCy** model with the function we just
    defined. Using both methods, we see that the **spaCy** model gets the number of
    the irregular noun **geese** incorrectly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s do the same using the large model. If you have not yet downloaded
    the large model, do so by running the first line. Otherwise, you can comment it
    out. Here, we see that although the **morph** method still incorrectly assigns
    singular to **geese**, the **lemma** method provides the correct answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now use GPT-3.5 to get the noun number. In the results, we see that GPT-3.5
    gives us an identical result and correctly identifies both the number for **geese**
    and the number for **road**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also change the nouns from plural to singular, and vice versa. We will
    use the `textblob` package for that. The package should be installed automatically
    via the Poetry environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **TextBlob** class from the package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a list of text variables and process them using the **TextBlob**
    class via a list comprehension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the **pluralize** function of the object to get the plural. This function
    returns a list and we access its first element. Print the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will do the reverse. We use the preceding **plurals** list to turn
    the plural nouns into **TextBlob** objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Turn the nouns into singular using the **singularize** function and print:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be the same as the list we started with in *step 2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Getting the dependency parse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dependency parse is a tool that shows dependencies in a sentence. For example,
    in the sentence *The cat wore a hat*, the root of the sentence is the verb, *wore*,
    and both the subject, *the cat*, and the object, *a hat*, are dependents. The
    dependency parse can be very useful in many NLP tasks since it shows the grammatical
    structure of the sentence, with the subject, the main verb, the object, and so
    on. It can then be used in downstream processing.
  prefs: []
  type: TYPE_NORMAL
- en: The `spaCy` NLP engine does the dependency parse as part of its overall analysis.
    The dependency parse tags explain the role of each word in the sentence. `ROOT`
    is the main word that all other words depend on, usually the verb.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use `spaCy` to create the dependency parse. The required packages are
    part of the Poetry environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will take a few sentences from the `sherlock_holmes1.txt` file to illustrate
    the dependency parse. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the file and language utility notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the sentence we will be parsing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will print the word, its grammatical function embedded
    in the **dep_** attribute, and the explanation of that attribute. The **dep_**
    attribute of the **Token** object shows the grammatical function of the word in
    the sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s use this function on the first sentence in our list. We can see
    that the verb **heard** is the **ROOT** word of the sentence, with all other words
    depending on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To explore the dependency parse structure, we can use the attributes of the
    **Token** class. Using the **ancestors** and **children** attributes, we can get
    the tokens that this token depends on and the tokens that depend on it, respectively.
    The function to print the ancestors is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s use this function on the first sentence in our list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output will be as follows. In the result, we see that `heard` has no ancestors
    since it is the main word in the sentence. All other words depend on it, and in
    fact, contain `heard` in their ancestor lists.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The dependency chain can be seen by following the ancestor links for each word.
    For example, if we look at the word `name`, we see that its ancestors are `under`,
    `mention`, and `heard`. The immediate parent of `name` is `under`, the parent
    of `under` is `mention`, and the parent of `mention` is `heard`. A dependency
    chain will always lead to the root, or the main word, of the sentence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see all the children, use the following function. This function prints out
    each word and the words that depend on it, its **children**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s use this function on the first sentence in our list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows. Now, the word `heard` has a list of words
    that depend on it since it is the main word in the sentence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also see left and right children in separate lists. In the following
    function, we print the children as two separate lists, left and right. This can
    be useful when doing grammatical transformations in the sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use this function on the first sentence in our list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also see the subtree that the token is in by using this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use this function on the first sentence in our list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows. From the subtrees that each word is part of,
    we can see the grammatical phrases that appear in the sentence, such as the `any
    other name`, and the `under any` `other name`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dependency parse can be visualized graphically using the `displaCy` package,
    which is part of `spaCy`. Please see [*Chapter*](B18411_08.xhtml#_idTextAnchor205)
    *7*, *Visualizing Text Data*, for a detailed recipe on how to do the visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting noun chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Noun chunks are known in linguistics as noun phrases. They represent nouns and
    any words that depend on and accompany nouns. For example, in the sentence *The
    big red apple fell on the scared cat*, the noun chunks are *the big red apple*
    and *the scared cat*. Extracting these noun chunks is instrumental to many other
    downstream NLP tasks, such as named entity recognition and processing entities
    and relations between them. In this recipe, we will explore how to extract named
    entities from a text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `spaCy` package, which has a function for extracting noun chunks,
    and the text from the `sherlock_holmes_1.txt` file as an example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following steps to get the noun chunks from a text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the file and language utility notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function that will print out the noun chunks. The noun chunks are
    contained in the **doc.noun_chunks** class variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the text from the **sherlock_holmes_1.txt** file and use the function
    on the resulting text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the partial result. See the output of the notebook at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter02/noun_chunks_2.3.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter02/noun_chunks_2.3.ipynb)
    for the full printout. The function gets the pronouns, nouns, and noun phrases
    that are in the text correctly:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Noun chunks are `spaCy` `Span` objects and have all their properties. See the
    official documentation at [https://spacy.io/api/token](https://spacy.io/api/token).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore some properties of noun chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define a function that will print out the different properties of noun
    chunks. It will print the text of the noun chunk, its start and end indices within
    the **Doc** object, the sentence it belongs to (useful when there is more than
    one sentence), the root of the noun chunk (its main word), and the chunk’s similarity
    to the word **emotions**. Finally, it will print out the similarity of the whole
    input sentence to **emotions**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the sentence to **All emotions, and that one particularly, were abhorrent
    to his cold, precise but admirably** **balanced mind**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the **explore_properties** function on the sentence using the small model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will also see a warning message similar to this one due to the fact that
    the small model does not ship with word vectors of its own:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s apply the same function to the same sentence with the large model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The large model does come with its own word vectors and does not result in
    a warning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see that the similarity of the `All emotions` noun chunk is high in relation
    to the word `emotions`, as compared to the similarity of the `his cold, precise
    but admirably balanced mind` noun chunk.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A larger **spaCy** model, such as **en_core_web_lg**, takes up more space but
    is more precise.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The topic of semantic similarity will be explored in more detail in [*Chapter
    3*](B18411_03.xhtml#_idTextAnchor067).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting subjects and objects of the sentence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we might need to find the subject and direct objects of the sentence,
    and that is easily accomplished with the `spaCy` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the dependency tags from `spaCy` to find subjects and objects.
    The code uses the `spaCy` engine to parse the sentence. Then, the subject function
    loops through the tokens, and if the dependency tag contains `subj`, it returns
    that token’s subtree, a `Span` object. There are different subject tags, including
    `nsubj` for regular subjects and `nsubjpass` for subjects of passive sentences,
    thus we want to look for both.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the `subtree` attribute of tokens to find the complete noun chunk
    that is the subject or direct object of the verb (see the *Getting the dependency
    parse* recipe). We will define functions to find the subject, direct object, dative
    phrase, and prepositional phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the file and language utility notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use two functions to find the subject and the direct object of the
    sentence. These functions will loop through the tokens and return the subtree
    that contains the token with **subj** or **dobj** in the dependency tag, respectively.
    Here is the subject function. It looks for the token that has a dependency tag
    that contains **subj** and then returns the subtree that contains that token.
    There are several subject dependency tags, including **nsubj** and **nsubjpass**
    (for the subject of a passive sentence), so we look for the most general pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the direct object function. It works similarly to **get_subject_phrase**
    but looks for the **dobj** dependency tag instead of a tag that contains **subj**.
    If the sentence does not have a direct object, it will return **None**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign a list of sentences to a variable, loop through them, and use the preceding
    functions to print out their subjects and objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows. Since the first sentence does not have a direct
    object, `None` is printed out. For the sentence `The big black cat stared at the
    small dog`, the subject is `the big black cat` and there is no direct object (`the
    small dog` is the object of the preposition `at`). For the sentence `Jane watched
    her brother in the evenings`, the subject is `Jane` and the direct object is `her
    brother`. In the sentence `Laura gave Sam a very interesting book`, the subject
    is `Laura` and the direct object is `a very` `interesting book`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can look for other objects, for example, the dative objects of verbs such
    as *give* and objects of prepositional phrases. The functions will look very similar,
    with the main difference being the dependency tags: `dative` for the dative object
    function, and `pobj` for the prepositional object function. The prepositional
    object function will return a list since there can be more than one prepositional
    phrase in a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dative object function checks the tokens for the **dative** tag. It returns
    **None** if there are no dative objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also combine the subject, object, and dative functions into one with
    an argument that specifies which object to look for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us now define a sentence with a dative object and run the function for
    all three types of phrases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows. The dative object is `Sam`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the prepositional object function. It returns a list of objects of
    prepositions, which will be empty if there are none:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s define a list of sentences and run the two functions on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is one prepositional phrase in each sentence. In the sentence `The big
    black cat stared at the small dog`, it is `at the small dog`, and in the sentence
    `Jane watched her brother in the evenings`, it is `in` `the evenings`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is left as an exercise for you to find the actual prepositional phrases with
    prepositions intact instead of just the noun phrases that are dependent on these
    prepositions.
  prefs: []
  type: TYPE_NORMAL
- en: Finding patterns in text using grammatical information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the `spaCy` `Matcher` object to find patterns in
    the text. We will use the grammatical properties of the words to create these
    patterns. For example, we might be looking for verb phrases instead of noun phrases.
    We can specify grammatical patterns to match verb phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the `spaCy` `Matcher` object to specify and find patterns.
    It can match different properties, not just grammatical. You can find out more
    in the documentation at [https://spacy.io/usage/rule-based-matching/](https://spacy.io/usage/rule-based-matching/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Your steps should be formatted like so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the file and language utility notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the **Matcher** object and initialize it. We need to put in the vocabulary
    object, which is the same as the vocabulary of the model we will be using to process
    the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list of patterns and add them to the matcher. Each pattern is a list
    of dictionaries, where each dictionary describes a token. In our patterns, we
    only specify the part of speech for each token. We then add these patterns to
    the **Matcher** object. The patterns we will be using are a verb by itself (for
    example, *paints*), an auxiliary followed by a verb (for example, **was observing**),
    an auxiliary followed by an adjective (for example, **were late**), and an auxiliary
    followed by a verb and a preposition (for example, **were staring at**). This
    is not an exhaustive list; feel free to come up with other examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the small part of the *Sherlock Holmes* text and process it using the
    small model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we find the matches using the **Matcher** object and the processed text.
    We then loop through the matches and print out the match ID, the string ID (the
    identifier of the pattern), the start and end of the match, and the text of the
    match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code finds some of the verb phrases in the text. Sometimes, it finds a partial
    match that is part of another match. Weeding out these partial matches is left
    as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use other attributes apart from parts of speech. It is possible to match
    on the text itself, its length, whether it is alphanumeric, the punctuation, the
    word’s case, the `dep_` and `morph` attributes, lemma, entity type, and others.
    It is also possible to use regular expressions on the patterns. For more information,
    see the spaCy documentation: [https://spacy.io/usage/rule-based-matching](https://spacy.io/usage/rule-based-matching).'
  prefs: []
  type: TYPE_NORMAL
