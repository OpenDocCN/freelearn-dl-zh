<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Decision Trees and Random Forests</h1>
                </header>
            
            <article>
                
<p>Decision trees and random forests are powerful techniques that you can use to add power to your applications. Let's walk through some concepts and some code and hopefully have you up and running in no time.</p>
<p>In this chapter, we are going to learn about decision trees and random forests. We will:</p>
<ul>
<li>Work through a lot of code samples to show you how you can add this powerful functionality to your applications</li>
<li>Discuss decision trees</li>
<li>Discuss random forests</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will be required to have Microsoft Visual Studio installed in your system. You may also need to refer to open source SharpLearning framework's GitHub repository at <a href="https://github.com/mdabros/SharpLearning">https://github.com/mdabros/SharpLearning</a>.</p>
<p><span>Check out the following video to see Code in Action: <a href="http://bit.ly/2O1Lbhr">http://bit.ly/2O1Lbhr</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p>Decision trees can be used for both classification and regression. Decision trees answer sequential questions with a yes/no, true/false response. Based upon those responses, the tree follows predetermined paths to reach its goal. Trees are more formally a version of what is known as a directed acyclic graph. Finally, a decision tree is built using the entire dataset and all features.</p>
<p>Here is an example of a decision tree. You may not know it as a decision tree, but for sure you know the process. Anyone for a doughnut?</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1219 image-border" src="assets/a372cb1a-d005-4c00-b232-ada277d070fa.png" style="width:51.00em;height:36.83em;"/></p>
<p>As you can see, the flow of a decision tree starts at the top and works its way downward until a specific result is achieved. The root of the tree is the first decision that splits the dataset. The tree recursively splits the dataset according to what is known as the <strong>splitting metric</strong> at each node. Two of the most popular metrics are <strong>Gini Impurity</strong> and <strong>Information Gain</strong>.</p>
<p>Here is another depiction of a decision tree, albeit without the great donuts!</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1413 image-border" src="assets/ee594bda-9688-4c4f-976b-acbd2cce21cc.png" style="width:17.58em;height:10.58em;"/></p>
<p>The <em>depth</em> of a decision tree represents how many questions have been asked so far. This is the deepest that the tree can go (the total number of questions that may be asked), even if some results can be achieved using fewer questions. For instance, using the preceding diagram, some results can be obtained after 1 question, some after 2. Therefore, the <em>depth</em> of that decision tree would be 2.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree advantages</h1>
                </header>
            
            <article>
                
<p>The following are some advantages from using decision trees:</p>
<ul>
<li>Easy interpretation.</li>
<li>Straightforward, self-explanatory visualizations.</li>
<li>Can be easily reproduced.</li>
<li>Can handle both numeric and categorical data.</li>
<li>Perform well on very large datasets.</li>
<li>Normally are very fast.</li>
<li>Depth-wise, the location of the tree allows easy visualization of which features are important. The importance is denoted by the depth of the tree.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree disadvantages</h1>
                </header>
            
            <article>
                
<p>The following are some disadvantages to using decision trees:</p>
<ul>
<li>At each node, the algorithm needs to determine the correct choice. The best choice at one node may not necessarily be the best choice for the entire tree.</li>
<li>If a tree is deep, it can be prone to what is known as overfitting.</li>
<li>Decision trees can memorize the training set.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When should we use a decision tree?</h1>
                </header>
            
            <article>
                
<p>The following are some examples of when to use a decision tree:</p>
<ul>
<li>When you want a simple and explainable model</li>
<li>When your model should be non-parametric</li>
<li>When you don’t want to worry about feature selection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p>We have talked about decision trees, and now it’s time to discuss random forests. Very basically, a random forest is a collection of decision trees. In random forests, a fraction of the number of total rows and features are selected at random to train on. A decision tree is then built upon this subset. This collection will then have the results aggregated into a single result.</p>
<p>Random forests can also reduce bias and variance. How do they do this? By training on different data samples, or by using a random subset of features. Let’s take an example. Let’s say we have 30 features. A random forest might only use 10 of these features. That leaves 20 features unused, but some of those 20 features might be important. Remember that a random forest is a collection of decision trees. Therefore, in each tree, if we utilize 10 features, over time most if not all of our features would have been included anyway simply because of the law of averages. So, it is this inclusion that helps limit our error due to bias and variance.</p>
<p>For large datasets, the number of trees can grow quite large, sometimes into the tens of thousands and more, depending on the number of features you are using, so you need to be careful regarding performance.</p>
<p>Here is a diagram of what a random forest might look like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1414 image-border" src="assets/fef193f7-c28b-492e-838e-7352b2518c59.png" style="width:36.33em;height:12.92em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest advantages</h1>
                </header>
            
            <article>
                
<p>The following are some advantages from using random forests:</p>
<ul>
<li>More robust than just a single decision tree</li>
<li>Random forests contain many decision trees and are therefore able to limit overfitting and error</li>
<li>Depth-wise, the location shows which features contribute to the classification or regression as well as their relative importance</li>
<li>Can be used for both regression and classification</li>
<li>Default parameters can be sufficient</li>
<li>Fast to train</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest disadvantages</h1>
                </header>
            
            <article>
                
<p><span>The following are some disadvantages to using random forests:</span></p>
<ul>
<li>Random forests need to be done in parallel in order to increase speed</li>
<li>Predictions can be slow to create once trained</li>
<li>More accuracy requires more trees, and this can result in a slower model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When should we use a random forest?</h1>
                </header>
            
            <article>
                
<p><span>The following are some examples of when to use random forests:</span></p>
<ul>
<li>When model interpretation is not the most important criterion. Interpretation will not be as easy as a single tree.</li>
<li>When model accuracy is most important.</li>
<li>When you want robust classification, regression, and feature selection analysis.</li>
<li>To prevent overfitting.</li>
<li>Image classification.</li>
<li>Recommendation engines.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SharpLearning</h1>
                </header>
            
            <article>
                
<p>Let’s now turn our attention to an incredible open source package, SharpLearning. <strong>SharpLearning</strong> is an excellent machine learning framework for individuals to learn about many aspects of machine learning, including decision trees and random forests as we described in the preceding sections. Let’s spend a few minutes getting familiar with a few things before we dive into some code samples and example applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Terminology</h1>
                </header>
            
            <article>
                
<p>Throughout this chapter you are going to see the following terms used. Here is the context for what each of them means:</p>
<ul>
<li><strong>Learner</strong>: This refers to a machine learning algorithm.</li>
<li><strong>Model</strong>: This refers to a machine learning model.</li>
<li><strong>Hyper-parameters</strong>: These are the parameters used to adjust and regulate (hopefully) the machine learning model.</li>
<li><strong>Targets</strong>: These are more commonly referred to as a dependent variable. In most notations, this will be <em>y</em>. These are the values that we are attempting to model.</li>
<li><strong>Observations</strong>: These are the feature matrix, which contains all the information we currently have about the targets. In most notations, this will be <em>x</em>.</li>
</ul>
<p>Throughout most of our examples we will be focusing on two namespaces within SharpLearning. They are:</p>
<ul>
<li><kbd>SharpLearning.DecisionTrees</kbd></li>
<li><kbd>SharpLearning.RandomForest</kbd></li>
</ul>
<p>With that behind us, let’s start digging into SharpLearning and show you a few concepts relative to how it works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading and saving models</h1>
                </header>
            
            <article>
                
<p>SharpLearning makes it very easy to load and save models to disk. This is a very important part of a machine learning library and SharpLearning is among the easiest to implement.</p>
<p>All models in SharpLearning have a <kbd>Save</kbd> and a <kbd>Load</kbd> method. These methods do the heavy lifting of saving and loading a model for us.</p>
<p>As an example, here we will save a model that we learned to disk:</p>
<pre>model.Save(() =&gt; StreamWriter(@"C:\randomforest.xml"));</pre>
<p>If we want to load this model back in, we simply use the <kbd>Load</kbd> method:</p>
<pre>varloadedModel = RegressionForestModel.Load(() =&gt; newStreamReader(@"C:\randomforest.xml"));</pre>
<p>Yep, it’s that easy and simple to load and save your data models. It is also possible for you to save models using serialization. This will allow us to choose between XML and a Binary format. Another very nice design feature of SharpLearning is that serializing models allows us to serialize to the <kbd>IPredictorModel</kbd> interface. This makes replacing your models much easier, if each conforms to that interface. Here’s how we would do that:</p>
<pre>varxmlSerializer = new GenericXmlDataContractSerializer();<br/>xmlSerializer.Serialize&lt;IPredictorModel&lt;double&gt;&gt;(model, <br/> () =&gt; new StreamWriter(@"C:\randomforest.xml"));<br/>var loadedModelXml = xmlSerializer<br/>.Deserialize&lt;IPredictorModel&lt;double&gt;&gt;(() =&gt; new StreamReader(@"C:\randomforest.xml"));</pre>
<table border="1" style="border-collapse: collapse;width: 100%">
<thead>
<tr>
<td>
<p><strong>Algorithm</strong></p>
</td>
<td>
<p><strong>Train Error</strong></p>
</td>
<td>
<p><strong>Test Error</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
<p class="CDPAlignLeft CDPAlign">RegressionDecisionTreeLearner(default)</p>
</td>
<td>
<p>0.0518</p>
</td>
<td>
<p>0.4037</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>And there you have it, instant training and testing errors.</p>
<div class="packt_infobox">When reporting the performance of your model, you should always use the test error even if the training error is lower, since that is an estimate of how well the model generalizes to new data.</div>
<p>Now, let's talk for a second about <strong>hyperparameters</strong>. Hyperparameters are parameters that affect the learning process of the machine learning algorithm. You can adjust them to tune the process and improve performance and reliability. At the same time, you can also incorrectly adjust parameters and have something that does not work as intended. Let's look at a few things that can happen with an incorrectly tuned hyperparameter:</p>
<ul>
<li>If the model is too complex, you can end up with what is known as high variance, or <strong>Overfitting</strong></li>
<li>If the model ends up being too simple, you will end up with what is known as high bias, or <strong>Underfitting</strong></li>
</ul>
<p>For those who have not done so, manually tuning hyperparameters, a process that happens in almost every use case, can take a considerable amount of your time. As the number of hyperparameters increases with the model, the tuning time and effort increase as well. The best way around this is to use an optimizer and let the work happen for you. To this end, SharpLearning can be a huge help to us due to the numerous optimizers that are available for it. Here is a list of just some of them:</p>
<ul>
<li>Grid search</li>
<li>Random search</li>
<li>Particle swarm (which we will talk about in <a href="34e7d6b8-85b9-42ff-865b-86f158138320.xhtml">Chapter 7</a>, <em>Replacing Back Propagation with PSO</em>)</li>
<li>Bayesian optimization</li>
<li>Globalized bounded nelder mead</li>
</ul>
<p>Let’s start with an example.</p>
<p>Let’s create a learner and use the default parameters, which, more than likely, will be good enough. Once we find our parameters and create the learner, we need to create the model. We then will predict the training and test set. Once all of that is complete, we will measure the error on the test set and record it:</p>
<pre>// create learner with default parameters<br/>var learner = new RegressionSquareLossGradientBoostLearner(runParallel: false);<br/>// learn model with found parameters<br/>var model = learner.Learn(trainSet.Observations, trainSet.Targets);<br/>// predict the training and test set.<br/>var trainPredictions = model.Predict(trainSet.Observations);<br/>var testPredictions = model.Predict(testSet.Observations);<br/>// since this is a regression problem we are using square error as metric<br/>// for evaluating how well the model performs.<br/>var metric = new MeanSquaredErrorRegressionMetric();<br/>// measure the error on the test set.<br/>var testError = metric.Error(testSet.Targets, testPredictions);</pre>
<p>And here is our test set error</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<thead>
<tr>
<td>
<p><strong>Algorithm</strong></p>
</td>
<td>
<p><strong>Test Error</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
<p>RegressionSquareLossGradientBoostLearner (default)</p>
</td>
<td>
<p>0.4984</p>
</td>
</tr>
</tbody>
</table>
<p>With that part complete, we now have our baseline established. Let’s use a <kbd>RandomSearchOptimizer</kbd> to tune the hyperparameters to see if we can get any better results. To do this we need to establish the bounds of the hyperparameters, so our optimizer knows how to tune. Let’s look at how we do this:</p>
<pre>var parameters = new ParameterBounds[]<br/>{<br/> new ParameterBounds(min: 80, max: 300, <br/> transform: Transform.Linear, parameterType: ParameterType.Discrete), <br/> new ParameterBounds(min: 0.02, max: 0.2, <br/> transform: Transform.Logarithmic, parameterType: ParameterType.Continuous), <br/> new ParameterBounds(min: 8, max: 15, <br/> transform: Transform.Linear, parameterType: ParameterType.Discrete), <br/> new ParameterBounds(min: 0.5, max: 0.9, <br/> transform: Transform.Linear, parameterType: ParameterType.Continuous), <br/> new ParameterBounds(min: 1, max: numberOfFeatures, <br/> transform: Transform.Linear, parameterType: ParameterType.Discrete), <br/>};</pre>
<p>Did you notice that we used a Logarithmic transform for the learning rate? Do you know why we did this? The answer is: to ensure that we had a more even distribution across the entire range of values. We have a large range difference between our minimum and maximum values (0.02 -&gt; 0.2), so the logarithmic transform will be best.</p>
<p>We now need a <em>validation set</em> to help us measure how well the model generalizes to unseen data during our optimization. To do this, we will need to further split the training data. To do this, we are going to leave our current test set out of the optimization process. If we don’t, we risk getting a positive bias on our final error estimate, and that will not be what we want:</p>
<pre>var validationSplit = new RandomTrainingTestIndexSplitter&lt;double&gt;(trainingPercentage: 0.7, seed: 24)<br/>.SplitSet(trainSet.Observations, trainSet.Targets);</pre>
<p>One more thing that the optimizer will need is an objective function. The function will take a double array as input (containing the set of hyperparameters) and return an <kbd>OptimizerResult</kbd> that contains the validation error and the corresponding set of hyperparameters:</p>
<pre>Func&lt;double[], OptimizerResult&gt; minimize = p =&gt;<br/> {<br/> var candidateLearner = new RegressionSquareLossGradientBoostLearner(<br/> iterations: (int)p[0],<br/>learningRate: p[1], <br/>maximumTreeDepth: (int)p[2], <br/>subSampleRatio: p[3], <br/>featuresPrSplit: (int)p[4],<br/>runParallel: false);<br/> var candidateModel = candidateLearner.Learn(validationSplit.TrainingSet.Observations,<br/>validationSplit.TrainingSet.Targets);<br/> var validationPredictions = candidateModel.Predict(validationSplit.TestSet.Observations);<br/> var candidateError = metric.Error(validationSplit.TestSet.Targets, validationPredictions);<br/> return new OptimizerResult(p, candidateError);<br/>};</pre>
<p>Once this <kbd>objective</kbd> function has been defined, we can now create and run the optimizer to find the best set of parameters. Let’s start out by running our optimizer for 30 iterations and trying out 30 different sets of hyperparameters:</p>
<pre>// create our optimizer<br/>var optimizer = new RandomSearchOptimizer(parameters, <strong>iterations: 30</strong>, runParallel: true);<br/>// find the best hyperparameters for use<br/>var result = optimizer.OptimizeBest(minimize);<br/>var best = result.ParameterSet;</pre>
<p>Once we run this, our optimizer should find the best set of hyperparameters. Let’s see what it finds:</p>
<ul>
<li><kbd>Trees</kbd>: 277</li>
<li><kbd>learningRate</kbd>: 0.035</li>
<li><kbd>maximumTreeDepth</kbd>: 15</li>
<li><kbd>subSampleRatio</kbd>: 0.838</li>
<li><kbd>featuresPrSplit</kbd>: 4</li>
</ul>
<p>Progress. Now that we have a set of best hyperparameters, which were measured on our validation set, we can create a learner with these parameters and learn a new model using the entire dataset:</p>
<pre>var learner = new RegressionSquareLossGradientBoostLearner(<br/> iterations: (int)best[0],<br/>learningRate: best[1], <br/>maximumTreeDepth: (int)best[2], <br/>subSampleRatio: best[3],<br/>featuresPrSplit: (int)best[4], <br/>runParallel: false);<br/>var model = learner.Learn(trainSet.Observations, trainSet.Targets);</pre>
<p>With our final set of hyperparameters now intact, we pass these to our learner and are able to reduce the test error significantly. For us to do that manually would have taken us an eternity and beyond!</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<thead>
<tr>
<td>
<p><strong>Algorithm</strong></p>
</td>
<td>
<p><strong>Test Error</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
<p>RegressionSquareLossGradientBoostLearner (default)</p>
</td>
<td>
<p>0.4984</p>
</td>
</tr>
<tr>
<td>
<p>RegressionSquareLossGradientBoostLearner (Optimizer)</p>
</td>
<td>
<p>0.3852</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example code and applications</h1>
                </header>
            
            <article>
                
<p>In the next few sections, we are going to look at some code samples without all the verbosity. This will be pure C# code so it should be something easily understood by all.</p>
<p>Let’s take a quick look at how we can use SharpLearning to predict observations. I’ll show you an entire code sample without the verbosity:</p>
<pre>var parser = new CsvParser(() =&gt;new StringReader(Resources.AptitudeData));<br/>var observations = parser.EnumerateRows(v =&gt; v != "Pass").ToF64Matrix();<br/>var targets = parser.EnumerateRows("Pass").ToF64Vector();<br/>var rows = targets.Length;<br/>var learner = new ClassificationDecisionTreeLearner(100, 1, 2, 0.001, 42);<br/>varsut = learner.Learn(observations, targets);<br/>var predictions = sut.Predict(observations);<br/>var evaluator = new TotalErrorClassificationMetric&lt;double&gt;();<br/>var error = evaluator.Error(targets, predictions);<br/>Assert.AreEqual(0.038461538461538464, error, 0.0000001);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving a model</h1>
                </header>
            
            <article>
                
<p>Here is a code sample that will show you how easy it is to save a model:</p>
<pre>var parser = new CsvParser(() =&gt;new StringReader(Resources.AptitudeData));<br/>var observations = parser.EnumerateRows(v =&gt; v != "Pass").ToF64Matrix();<br/>var targets = parser.EnumerateRows("Pass").ToF64Vector();<br/>var learner = new ClassificationDecisionTreeLearner(2);<br/>var sut = learner.Learn(observations, targets);<br/>var writer = new StringWriter();<br/>sut.Save(() =&gt; writer);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean squared error regression metric</h1>
                </header>
            
            <article>
                
<p><strong>Mean squared error</strong> (<strong>MSE</strong>) is a metric that measures the average of the squares of the errors. More concretely, it measures the average distance between the estimated values and what is estimated. A mean squared error is always non-negative, and values that are closer to zero are considered more acceptable. SharpLearning makes it incredibly easy to calculate this error metric, as depicted in the following code:</p>
<pre>var targets = new double[] { 1.0, 2.3, 3.1, 4.4, 5.8 };<br/>var predictions = new double[] { 1.0, 2.0, 3.0, 4.0, 5.0 };<br/>var sut = new MeanSquaredErrorRegressionMetric();<br/>var actual = sut.Error(targets, predictions);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">F1 score</h1>
                </header>
            
            <article>
                
<p>To talk about an f1 score we must first talk about <em>Precision</em> and <em>Recall</em>.</p>
<p><strong>Precision</strong> is the ratio of correctly predicted positive observations divided by the total predicted positive observations. Less formally, of all the people that said they were coming, how many came?</p>
<p><strong>Recall</strong> (sensitivity) is the ratio of correctly predicted positive observations to all observations in total.</p>
<p><strong>F1 score</strong> is then the weighted average of Precision and Recall.</p>
<p>Here’s how we calculate an f1 score using SharpLearning:</p>
<pre>var targets = new double[] { 0, 1, 1 };<br/>var predictions = new double[] { 0, 1, 1 };<br/>var sut = new F1ScoreMetric&lt;double&gt;(1);<br/>var actual = sut.Error(targets, predictions);<br/>Assert.AreEqual(0.0, actual);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizations</h1>
                </header>
            
            <article>
                
<p>The following is how you can use the <em>Particle Swarm Optimizer</em> to return the result that best minimizes the provided function:</p>
<pre>var parameters = new ParameterBounds[]<br/>{<br/>new ParameterBounds(-10.0, 10.0, Transform.Linear),<br/>new ParameterBounds(-10.0, 10.0, Transform.Linear),<br/>new ParameterBounds(-10.0, 10.0, Transform.Linear),<br/>}; <br/>var sut = new ParticleSwarmOptimizer(parameters, 100);<br/>var actual = sut.OptimizeBest(Minimize);</pre>
<p>The following is how you can use the <em>Grid Search Optimizer</em> to optimize by trying all combinations of the provided parameters:</p>
<pre>var parameters = new double[][] { new double[]{ 10.0, 20.0, 30.0, 35.0, 37.5, 40.0, 50.0, 60.0 } };<br/>var sut = new GridSearchOptimizer(parameters);<br/>var actual = sut.OptimizeBest(Minimize);</pre>
<p>The following is how you can use the <em>Random Search Optimizer</em> to initialize random parameters between the min and max provided. The result that best minimizes the provided function will be returned:</p>
<pre>var parameters = new ParameterBounds[] <br/>{<br/>new ParameterBounds(0.0, 100.0, Transform.Linear)<br/>};<br/>var sut = new RandomSearchOptimizer(parameters, 100);<br/>var actual = sut.OptimizeBest(Minimize);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample application 1</h1>
                </header>
            
            <article>
                
<p>With all this knowledge under our belt, let’s go ahead and write our first sample program. The program itself is very simple and meant to show how easy it is to implement such techniques in your applications with a minimal amount of code. To show you exactly what I mean, here is what the output of the program looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-162 image-border" src="assets/a5da7152-6cf1-4d74-a8e3-939463ceb358.jpg" style="width:31.17em;height:8.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The code</h1>
                </header>
            
            <article>
                
<p>Here is the code that implements our sample program and produces the previous output. As you can see, the code is very simple and everything in here (save the shuffling of indices) is code we have already walked through before. We’ll keep the verbosity to a minimum so that you can concentrate on the code itself. This sample will read in our data, parse it into observations and target samples, and then create a learner using 1,000 trees. From there we will use the learner to learn and create our model. Once this is complete we will calculate our mean squared error metric and display it on the screen:</p>
<pre>var parser = new CsvParser(() =&gt;new StringReader(Resources.Glass));<br/>var observations = parser.EnumerateRows(v =&gt; v != "Target").ToF64Matrix();<br/>var targets = parser.EnumerateRows("Target").ToF64Vector();<br/>int trees = 1000;<br/>var sut = new RegressionExtremelyRandomizedTreesLearner(trees, 1, 100, 1, 0.0001, 1.0, 42, false);<br/>var indices = Enumerable.Range(0, targets.Length).ToArray();<br/>indices.Shuffle(new Random(42));<br/>indices = indices.Take((int)(targets.Length * 0.7)).ToArray();<br/>var model = sut.Learn(observations, targets, indices);<br/>var predictions = model.Predict(observations);<br/>var evaluator = new MeanSquaredErrorRegressionMetric();<br/>var error = evaluator.Error(targets, predictions);<br/>Console.WriteLine("Error: " + error);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample application 2 – wine quality</h1>
                </header>
            
            <article>
                
<p>In our next application, we are going to use our knowledge to determine the most important features for wine based upon the model that is created. Here is what our output will look like when we complete it:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-879 image-border" src="assets/fe7ac6eb-9a76-4d5c-b02a-ad46e51e49a0.png" style="width:77.00em;height:25.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The code</h1>
                </header>
            
            <article>
                
<p>Here is the code for our application. As always, we first load and parse our data into observations and target sample sets. Since this is a regression sample, we’ll use a 70/30 split of our data sample: 70% for training, 30% for testing. From there, we create our random forest learner and create our model. After this, we calculate our training and test errors and print out the feature importance in order of importance, as found by our model:</p>
<pre>var parser = new CsvParser(() =&gt;new StreamReader(Application.StartupPath + "\\winequality-white.csv"));<br/>var targetName = "quality";<br/>// read in our feature matrix<br/>var observations = parser.EnumerateRows(c =&gt; c != targetName).ToF64Matrix();<br/>// read in our regression targets<br/>var targets = parser.EnumerateRows(targetName).ToF64Vector();<br/>// Since this is a regression problem, we use the random training/test set splitter. 30 % of the data is used for the test set. <br/>var splitter = new RandomTrainingTestIndexSplitter&lt;double&gt;(trainingPercentage: 0.7, seed: 24);<br/>var trainingTestSplit = splitter.SplitSet(observations, targets);<br/>var trainSet = trainingTestSplit.TrainingSet;<br/>var testSet = trainingTestSplit.TestSet;<br/>var learner = new RegressionRandomForestLearner(trees: 100);<br/>var model = learner.Learn(trainSet.Observations, trainSet.Targets);<br/>var trainPredictions = model.Predict(trainSet.Observations);<br/>var testPredictions = model.Predict(testSet.Observations);<br/>// since this is a regression problem we are using squared error as the metric<br/>// for evaluating how well the model performs.<br/>var metric = new MeanSquaredErrorRegressionMetric();<br/>var trainError = metric.Error(trainSet.Targets, trainPredictions);<br/>var testError = metric.Error(testSet.Targets, testPredictions);<br/>Trace.WriteLine($"Train error: {trainError:0.0000} - Test error: {testError:0.0000}");<br/>System.Console.WriteLine($"Train error: {trainError:0.0000} - Test error: {testError:0.0000}");<br/><br/>// the variable importance requires the featureNameToIndex from the dataset. <br/>// This mapping describes the relation from the column name to the associated <br/>// index in the feature matrix.<br/>var featureNameToIndex = parser.EnumerateRows(c =&gt; c != targetName).First().ColumnNameToIndex;<br/><br/>var importances = model.GetVariableImportance(featureNameToIndex);<br/><br/>var importanceCsv = new StringBuilder();<br/>importanceCsv.Append("FeatureName;Importance");<br/>System.Console.WriteLine("FeatureName\tImportance");<br/>foreach (var feature in importances)<br/>{<br/>importanceCsv.AppendLine();<br/>importanceCsv.Append($"{feature.Key};{feature.Value:0.00}");<br/>System.Console.WriteLine($"{feature.Key}\t{feature.Value:0.00}");<br/>}<br/>Trace.WriteLine(importanceCsv);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about decision trees and random forests. We also learned how to use the open source framework, SharpLearn, to add these powerful features to our applications. In the next chapter, we are going to learn about facial and motion detection and show you how you can enable your application with this exciting technology! You’ll meet Frenchie, my pet French Bulldog, who will demonstrate most of the samples we will show. Also, we have a guest poser you will just have to see!</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><a href="https://github.com/mdabros/SharpLearning">https://github.com/mdabros/SharpLearning</a></li>
</ul>


            </article>

            
        </section>
    </body></html>