- en: Dynamic Programming and the Bellman Equation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划与贝尔曼方程
- en: '**Dynamic programming** (**DP**) was the second major thread to influence modern
    **reinforcement learning** (**RL**) after trial-and-error learning. In this chapter,
    we will look at the foundations of DP and explore how they influenced the field
    of RL. We will also look at how the Bellman equation and the concept of optimality
    have interwoven with RL. From there, we will look at policy and value iteration
    methods to solve a class of problems well suited for DP. Finally, we will look
    at how to use the concepts we have learned in this chapter to teach an agent to
    play the FrozenLake environment from OpenAI Gym.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划（DP**）是继试错学习之后，对现代**强化学习（RL**）产生重大影响的第二个主要分支。在本章中，我们将探讨DP的基础，并研究它们如何影响RL领域。我们还将探讨贝尔曼方程和最优性概念如何与RL交织在一起。从那里，我们将探讨策略和值迭代方法来解决适合DP的一类问题。最后，我们将探讨如何使用本章学到的概念来教一个智能体玩OpenAI
    Gym中的FrozenLake环境。'
- en: 'Here are the main topics we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将在本章中涵盖的主要主题：
- en: Introducing DP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍动态规划（DP）
- en: Understanding the Bellman equation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解贝尔曼方程
- en: Building policy iteration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建策略迭代
- en: Building value iteration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建值迭代
- en: Playing with policy versus value iteration
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩策略迭代与值迭代
- en: For this chapter, we look at how to solve a finite **Markov decision process**
    (**MDP**) with DP using the Bellman equation of optimality. This chapter is meant
    as a history lesson and background to DP and Bellman. If you are already quite
    familiar with DP, then you may want to bypass this chapter since we will just
    explore entry-level DP, covering just enough background to see how it has influenced
    and bettered RL.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将探讨如何使用贝尔曼最优方程，用DP解决有限**马尔可夫决策过程（MDP**）。本章旨在作为DP和贝尔曼的历史课程和背景介绍。如果您已经非常熟悉DP，那么您可能希望跳过本章，因为我们只将探索入门级DP，涵盖足够的背景知识，以了解它如何影响和改进RL。
- en: Introducing DP
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DP
- en: DP was developed by Richard E. Bellman in the 1950s as a way to optimize and
    solve complex decision problems. The method was first applied to engineering control
    problems but has since found uses in all disciplines requiring the analytical
    modeling of problems and subproblems. In effect, all DP is about is solving subproblems
    and then finding relationships to connect those to solve bigger problems. It does
    all of this by first applying the Bellman optimality equation and then solving
    it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DP是由理查德·E·贝尔曼在20世纪50年代开发的，作为一种优化和解决复杂决策问题的方法。该方法最初应用于工程控制问题，但后来在所有需要分析和建模问题及其子问题的学科中找到了应用。实际上，所有DP都是关于解决子问题，然后找到将这些子问题连接起来以解决更大问题的关系。它通过首先应用贝尔曼最优方程，然后求解来实现这一点。
- en: Before we get to solving a finite MDP with DP, we will want to understand, in
    a little more detail, what it is we are talking about. Let's look at a simple
    example of the difference between normal recursion and DP in the next section.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们用DP解决有限MDP之前，我们希望更详细地了解我们正在讨论的内容。让我们在下一节中看看正常递归和DP之间的简单示例差异。
- en: Regular programming versus DP
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正规编程与DP的比较
- en: 'We will do a comparison by first solving a problem using regular methods and
    then with DP. Along the way, we will identify key elements that make our solution
    a DP one. What most experienced programmers find is that they likely have done
    DP in some capacity, so don''t be surprised if this all sounds really familiar.
    Let''s open up the `Chapter_2_1.py` example and follow the exercise:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过首先使用常规方法解决问题，然后使用DP来进行比较。在这个过程中，我们将识别出使我们的解决方案成为DP的关键元素。大多数经验丰富的程序员发现，他们可能已经在某种程度上做过DP，所以如果这一切听起来非常熟悉，请不要感到惊讶。让我们打开`Chapter_2_1.py`示例并跟随练习：
- en: 'The code is an example of finding the *n*^(th) number in the Fibonacci sequence
    using recursion and is shown as follows:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码是使用递归查找斐波那契序列中第*n*个数字的示例，如下所示：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Recall that we can resolve the *n*^(th) element in the Fibonacci sequence by
    summing the two previous digits in the sequence. We consider that when `n == 1`,
    the value is `0`, and when `n == 2`, the returned value is `1`. Hence, the third
    element in the sequence would be the sum of `Fibonacci(1)` and `Fibonacci(2)`
    and would return a value of `1`. This reflects in the code shown in the following
    line:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回想一下，我们可以通过将序列中的前两个数字相加来解决问题序列的第*n*个元素。我们考虑当`n == 1`时，值为`0`，当`n == 2`时，返回的值是`1`。因此，序列中的第三个元素将是`Fibonacci(1)`和`Fibonacci(2)`的和，并将返回一个值为`1`。这在以下代码行中得到了反映：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Hence, the solution for finding the fourth element using the linear programming
    version of recursion is shown in the following diagram:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，使用递归的线性规划版本找到第四个元素的解决方案如下所示：
- en: '![](img/73bd0b7b-9d48-4a87-bf49-535ef14ad31e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73bd0b7b-9d48-4a87-bf49-535ef14ad31e.png)'
- en: Solving the fourth element of the Fibonacci sequence
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解决斐波那契数列的第四个元素
- en: 'We can note the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意以下几点：
- en: The preceding diagram shows each recursive call to the `Fibonacci` function
    required to calculate previous elements. Notice how this method requires the solution
    to solve or call the `Fibonacci(2)` function twice. Of course, that additional
    call in this example is trivial but these additional calls can quickly add up.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述图表显示了计算前一个元素所需的每个递归调用`Fibonacci`函数。注意这种方法需要解决或调用`Fibonacci(2)`函数两次。当然，在这个例子中，额外的调用是微不足道的，但这些额外的调用可以迅速累积。
- en: Run the code as you normally would and see the printed result for the ninth
    element.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照常规方式运行代码，并查看第九个元素的打印结果。
- en: 'To appreciate how inefficient recursion can be, we have modified our previous
    example and saved it as `Chapter_2_2.py`. Open up that example now and follow
    the next exercise:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了欣赏递归可能有多么低效，我们已经修改了之前的示例并将其保存为`Chapter_2_2.py`。现在打开这个示例并跟随下一个练习：
- en: 'The modified code, with the extra line highlighted, is shown for reference:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改后的代码，带有高亮显示的额外行，如下所示供参考：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'All we are doing is printing out when we need to calculate two more sequences
    to return the sum. Run the code and notice the output, as shown in the following
    screenshot:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们所做的一切只是打印出我们需要计算两个更多序列以返回总和的时刻。运行代码并注意以下截图所示的输出：
- en: '![](img/3261baec-4253-4b54-aa74-1cc31741d5c0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3261baec-4253-4b54-aa74-1cc31741d5c0.png)'
- en: Output from example Chapter_2_1.py
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 来自示例`Chapter_2_1.py`的输出
- en: Notice, for just calculating the ninth element of the Fibonacci sequence using
    recursion, the number of times just `Fibonacci(3)` is called.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，仅使用递归计算斐波那契数列的第九个元素时，`Fibonacci(3)`被调用的次数。
- en: Now, the solution works and, as they say, it is pretty and clean. In fact, as
    a programmer, you may have been taught to worship this style of coding at one
    time. However, we can clearly see how inefficient this method is to scale and
    that is where DP comes in, which we will discuss in the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，解决方案有效，正如人们所说，它既美观又简洁。事实上，作为一个程序员，你可能曾经被教导要崇拜这种编程风格。然而，我们可以清楚地看到这种方法在扩展时的低效，这就是动态规划（DP）发挥作用的地方，我们将在下一节中讨论。
- en: Enter DP and memoization
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入动态规划和记忆化
- en: 'A key concept in DP is to break down larger problems into smaller subproblems,
    then solve said smaller problems and store the result. The fancy name for this
    activity is called **memoization** and the best way to showcase how this works
    is with an example. Open up `Chapter_2_3.py` and follow the exercise:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划（DP）的一个关键概念是将较大的问题分解成较小的子问题，然后解决这些较小的子问题并存储结果。这种活动的时髦名称叫做**记忆化**，展示这种工作方式的最佳方式是使用一个例子。打开`Chapter_2_3.py`并跟随练习：
- en: 'For reference, the entire block of code from our previous example has been
    modified as follows:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为参考，我们之前示例中的整个代码块已被修改如下：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Again, the highlighted lines denote code changes, but, in this case, we will
    go over each code change in more detail starting with the first change, as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，高亮显示的行表示代码更改，但在这个例子中，我们将更详细地逐个分析代码更改，从第一个更改开始，如下所示：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This new line creates a new starting Fibonacci list with our two base numbers,
    `0` and `1`. We will still use a recursive function, but now we also store the
    results of every unique calculation for later:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这条新线创建了一个新的斐波那契列表，包含我们的两个基数`0`和`1`。我们仍然使用递归函数，但现在我们还存储每次独特计算的每个结果以供以后使用：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next code change is where the algorithm returns a previously stored value,
    as in `0` or `1`, or as calculated and then stored in the `fibSequence` list:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个代码更改是在算法返回之前存储的值的地方，例如`0`或`1`，或者计算后存储在`fibSequence`列表中：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The last group of code changes now saves the recursive calculation, adding the
    new value to the entire sequence. This now requires the algorithm to only calculate
    each *n*^(th) value of the sequence once.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一批代码更改现在保存了递归计算，将新值添加到整个序列中。这现在要求算法只需计算序列的第*n*个值一次。
- en: 'Run the code as you normally would and look at the results shown in the following
    screenshot:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式运行代码，并查看以下截图所示的结果：
- en: '![](img/a270b391-9ecb-4969-a97d-2e3ae9f7833d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a270b391-9ecb-4969-a97d-2e3ae9f7833d.png)'
- en: Example output from example Chapter_2_3.py
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 来自示例`Chapter_2_3.py`的输出示例
- en: Notice how we are now only calculating the numbers in the sequence and not repeating
    any calculations. Clearly, this method is far superior to the linear programming
    example that we looked at earlier, and the code actually doesn't look bad either.
    Now, as we said, if this type of solution seems obvious, then you probably already
    understand more about DP than you realize.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们现在只计算序列中的数字，而不重复任何计算。显然，这种方法比我们之前看到的线性规划示例要好得多，而且代码看起来也不差。现在，正如我们所说的，如果这种解决方案看起来很显然，那么你可能比你意识到的更了解DP。
- en: We will only cover a very simple introduction to DP as it relates to RL in this
    book. As you can see, DP is a powerful technique that can benefit any developer
    who is serious about optimizing code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将仅简要介绍与强化学习相关的动态规划（DP）。正如你所见，DP是一种强大的技术，可以惠及任何认真优化代码的开发者。
- en: In the next section, we look further at the work of Bellman and the equation
    that was named after him.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将进一步探讨贝尔曼的工作以及以他的名字命名的方程。
- en: Understanding the Bellman equation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解贝尔曼方程
- en: Bellman worked on solving finite MDP with DP, and it was during these efforts
    he derived his famed equation. The beauty behind this equation—and more abstractly,
    the concept, in general—is that it describes a method of optimizing the value
    or quality of a state. In other words, it describes how we can determine the optimal
    value/quality for being in a given state given the action and choices of successive
    states. Before breaking down the equation itself, let's first reconsider the finite
    MDP in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼在解决有限MDP问题时使用了动态规划（DP），正是在这些努力中他推导出了他著名的方程。这个方程背后的美丽之处——以及更抽象地说，一般概念——在于它描述了一种优化状态价值或质量的方法。换句话说，它描述了在给定动作和后续状态的选择的情况下，我们如何确定处于某个状态的最优价值/质量。在分解方程本身之前，让我们首先在下一节重新审视有限MDP。
- en: Unraveling the finite MDP
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解开有限MDP
- en: 'Consider the finite MDP we developed in [Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding
    Rewards Learning*, that described your morning routine. Don''t to worry if you
    didn''t complete that exercise previously as we will consider a more specific
    example, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们在[第1章](5553d896-c079-4404-a41b-c25293c745bb.xhtml)，“理解奖励学习”中开发的有限MDP，它描述了你的早晨常规。如果你之前没有完成那个练习，不用担心，我们将考虑一个更具体的例子，如下所示：
- en: '![](img/aa23c01a-8b06-4551-899e-7a906e68aea9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aa23c01a-8b06-4551-899e-7a906e68aea9.png)'
- en: MDP for waking up and getting on the bus
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用于醒来和乘坐公交车的MDP
- en: 'The preceding finite MDP describes a possible routine for someone waking up
    and getting ready to get on a bus to go to school or work. In this MDP, we define
    a beginning state (**BEGIN**) and an ending state, that is, getting on the bus
    (**END**). The **R =** denotes the reward allotted when moving to that state and
    the number closest to the end of the action line denotes the probability of taking
    that action. We can denote the optimum path through this finite MDP, as shown
    in the following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的有限MDP描述了某人醒来并准备乘坐公交车去上学或上班的可能常规。在这个MDP中，我们定义了一个初始状态（**BEGIN**）和一个结束状态，即上车（**END**）。**R
    =** 表示移动到该状态时分配的奖励，而靠近动作行末尾的数字表示采取该动作的概率。我们可以用以下图示表示通过这个有限MDP的最优路径：
- en: '![](img/9710a44e-0116-493e-9cb3-32311140ee1b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9710a44e-0116-493e-9cb3-32311140ee1b.png)'
- en: The optimum solution to the MDP
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MDP的最佳解决方案
- en: 'Mathematically, we can describe the optimum result, therefore, as the sum of
    all rewards traversed through the environment to obtain the total reward. More
    formally, we can also write this mathematically like so:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以将最优结果描述为通过环境遍历获得的所有奖励的总和。更正式地说，我们也可以这样数学地写出：
- en: '![](img/076725cd-a610-4a0b-a86d-6601c01a0ee5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/076725cd-a610-4a0b-a86d-6601c01a0ee5.png)'
- en: 'However, and this is where Bellman comes in, we can''t consider all rewards
    as equal. Without exploring the math in more exhaustive detail, the Bellman equation
    introduced the concept that future rewards should be discounted. This is also
    quite intuitive when you think about it. The experience or effect we feel from
    future actions is weakened depending on how far in the future we need to decide.
    This is the same concept we apply with the Bellman equation. Hence, we can now
    apply a discounting factor (gamma) to the previous equation and show the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这正是贝尔曼介入的地方，我们不能将所有奖励视为相等。在不更深入地探讨数学细节的情况下，贝尔曼方程引入了未来奖励应该被折现的概念。当你这样思考时，这也是相当直观的。从未来的动作中感受到的经验或效果会根据我们需要决定的时间距离而减弱。这正是我们应用贝尔曼方程时所采用的概念。因此，我们现在可以对前面的方程应用一个折现因子（伽马）并展示以下内容：
- en: '![](img/6b648417-d51e-4680-a5be-3499d76264c7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b648417-d51e-4680-a5be-3499d76264c7.png)'
- en: Gamma (![](img/9906fcb0-b342-4e56-809a-bbd16b5926cc.png)), shown in the equation,
    represents a future rewards discount factor. The value can be from `0.0` to `1.0`.
    If the value is `1.0`, then we consider no discount of future rewards, whereas
    a value of `0.1` would heavily discount future rewards. In most RL problems, we
    keep this number fairly high and well above `0.9`. This leads us to the next section
    where we discuss how the Bellman equation can optimize a problem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 伽马（![](img/9906fcb0-b342-4e56-809a-bbd16b5926cc.png)），在方程中表示，代表未来奖励的折现因子。这个值可以从
    `0.0` 到 `1.0`。如果值为 `1.0`，则我们不考虑未来奖励的折现，而值为 `0.1` 将会大幅度折现未来奖励。在大多数强化学习问题中，我们保持这个数值相当高，并且远高于
    `0.9`。这引出了下一节，我们将讨论贝尔曼方程如何优化问题。
- en: The Bellman optimality equation
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼最优方程
- en: 'The Bellman equation shows us that you can solve any MDP by first finding the
    optimal policy that allows an agent to traverse that MDP. Recall that a policy
    defines the decisions for each action that will guide an agent through an MDP.
    Ideally, what we want to find is the optimal policy: a policy that can maximize
    the value for each state and determine which states to traverse for maximum reward.
    When we combine this with other concepts and apply more math wizardry and then
    combine it with the Bellman optimality equation, we get the following optimal
    policy equation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程表明，你可以通过首先找到允许智能体遍历 MDP 的最优策略来解决任何 MDP。回想一下，策略定义了指导智能体通过 MDP 的每个动作的决定。理想情况下，我们想要找到的是最优策略：一个可以最大化每个状态的价值并确定要遍历哪些状态以获得最大奖励的策略。当我们结合其他概念并应用更多的数学技巧，然后与贝尔曼最优方程结合，我们得到以下最优策略方程：
- en: '![](img/02370093-e53e-42ff-951f-2c3bd8fedd75.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02370093-e53e-42ff-951f-2c3bd8fedd75.png)'
- en: That strange term at the very beginning ([![](img/fb95776c-d075-47d2-bdd1-fc3aa828e7b3.png)])
    is a way of describing a function that maximizes the rewards given a set of states
    and actions considering that we discount future rewards by a factor called gamma.
    Notice how we also use ![](img/cd5e7dcf-31c0-49dd-b6ce-9a3d6dee4650.png) to denote
    the policy equation but we often think of this as a quality and may refer to this
    as *q* or *Q*. If you think back to our previous peek at the Q-learning equation,
    then now you can clearly see how the rewards' discount factor, gamma, comes into
    play.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 非常奇怪的开头术语（[![](img/fb95776c-d075-47d2-bdd1-fc3aa828e7b3.png)）是一种描述函数的方式，该函数在给定一组状态和动作的情况下最大化奖励，同时考虑到我们通过一个称为伽马（gamma）的因子对未来的奖励进行折现。注意我们如何也使用
    ![](img/cd5e7dcf-31c0-49dd-b6ce-9a3d6dee4650.png) 来表示策略方程，但我们通常认为这是一个质量，可能会将其称为
    *q* 或 *Q*。如果你回想起我们之前对 Q-learning 方程的简要了解，那么现在你可以清楚地看到奖励的折现因子伽马是如何起作用的。
- en: In the next section, we will look at methods to solve an MDP using DP and a
    method of policy iteration given our understanding of the Bellman optimality principle
    and resulting policy equation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用动态规划（DP）和基于我们对贝尔曼最优原则和结果策略方程的理解的策略迭代方法来解决 MDP 的方法。
- en: Building policy iteration
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建策略迭代
- en: 'For us to determine the best policy, we first need a method to evaluate the
    given policy for a state. We can use a method of evaluating the policy by searching
    through all of the states of an MDP and further evaluating all actions. This will
    provide us with a value function for the given state that we can then use to perform
    successive updates of a new value function iteratively. Mathematically, we can
    then use the previous Bellman optimality equation and derive a new update to a
    state value function, as shown here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳策略，我们首先需要一个方法来评估给定状态的政策。我们可以通过搜索 MDP 的所有状态并进一步评估所有动作来评估政策。这将为我们提供给定状态的价值函数，然后我们可以使用它来迭代地执行新值函数的连续更新。从数学上讲，我们可以使用前面的
    Bellman 最优性方程并推导出新的状态值函数更新，如下所示：
- en: '![](img/0c9818aa-db72-4266-8766-866221321deb.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0c9818aa-db72-4266-8766-866221321deb.png)'
- en: 'In the preceding equation, the [![](img/d19906d9-7bab-41e1-98ee-e739b3c8d6ff.png)]
    symbol represents an expectation and denotes the expected state value update to
    a new value function. Inside this expectation, we can see this dependent on the
    returned reward plus the previous discounted value for the next state given an
    already chosen action. That means that our algorithm will iterate over every state
    and action evaluating a new state value using the preceding update equation. This
    process is called backing up or planning, and it is helpful for us to visualize
    how this algorithm works using backup diagrams. The following is an example of
    the backup diagrams for action value and state value backups:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，[![图片](img/d19906d9-7bab-41e1-98ee-e739b3c8d6ff.png)] 符号代表期望值，表示期望状态值更新到新的值函数。在这个期望值内部，我们可以看到它依赖于返回的奖励加上给定已选择动作的下一个状态的先前折现值。这意味着我们的算法将遍历每个状态和动作，使用前面的更新方程评估新的状态值。这个过程称为备份或规划，使用备份图可视化这个算法的工作方式对我们很有帮助。以下是动作值和状态值备份的备份图示例：
- en: '![](img/817a2069-e71b-4177-9bc6-e1c9ecd6f6c5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/817a2069-e71b-4177-9bc6-e1c9ecd6f6c5.png)'
- en: Backup diagrams for action value and state value backups
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值和状态值备份的备份图
- en: Diagram **(a)** or [![](img/1feda0df-b44f-4614-9d34-598e29d447b8.png)]is the
    part of the backup or evaluation that tries each action and hence provides us
    with action values. The second part of the evaluation comes from the update and
    is shown in diagram **(b)** for [![](img/134a97e6-b70d-4a6e-a633-5f315102b150.png)].
    Recall, the update evaluates the forward states by evaluating each of the state
    actions. The diagrams represent the point of evaluation with a filled-in solid
    circle. Notice how the action value only focuses on the forward actions while
    the state value focuses on the action value for each forward state. Of course,
    it will be helpful to look at how this comes together in code. However, before
    we get to that, we want to do some housekeeping in the next section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图**(a)**或[![图片](img/1feda0df-b44f-4614-9d34-598e29d447b8.png)]是备份或评估中尝试每个动作的部分，因此为我们提供了动作值。评估的第二部分来自更新，并在图**(b)**中显示为[![图片](img/134a97e6-b70d-4a6e-a633-5f315102b150.png)]。回想一下，更新通过评估每个状态动作来评估前向状态。这些图用实心圆圈表示评估点。注意动作值只关注前向动作，而状态值关注每个前向状态的动作值。当然，查看代码中这些是如何结合起来的会有所帮助。然而，在我们到达那里之前，我们想要在下一节做一些整理工作。
- en: Installing OpenAI Gym
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 OpenAI Gym
- en: To help to encourage research and development in RL, the OpenAI group provides
    an open source platform for RL training called Gym. Gym, provided by OpenAI, comes
    with plenty of sample test environments that we can venture through while we work
    through this book. Also, other RL developers have developed other environments
    using the same standard interface Gym uses. Hence, by learning to use Gym, we
    will also be able to explore other cutting-edge RL environments later in this
    book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励强化学习（RL）的研究和开发，OpenAI 团队提供了一个开源的 RL 训练平台，称为 Gym。由 OpenAI 提供的 Gym 拥有大量的样本测试环境，我们可以在阅读本书的过程中探索这些环境。此外，其他
    RL 开发者也使用 Gym 相同的标准接口开发了其他环境。因此，通过学习使用 Gym，我们将在本书的后面部分也能够探索其他前沿的 RL 环境。
- en: The installation for Gym is quite simple, but, at the same time, we want to
    avoid any small mistakes that may cause you frustration later. Therefore, it is
    best to use the following instructions to set up and install an RL environment
    for development.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Gym 的安装相当简单，但同时我们想要避免任何可能让你感到沮丧的小错误。因此，最好使用以下说明来设置和安装一个用于开发的 RL 环境。
- en: It is highly recommended that you use Anaconda for Python development with this
    book. Anaconda is a free open source cross-platform tool that can significantly
    increase your ease of development. Please stick with Anaconda unless you consider
    yourself an experienced Python developer. Google `python anaconda` to download
    and install it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议您使用Anaconda进行本书的Python开发。Anaconda是一个免费的开源跨平台工具，可以显著提高您的开发便利性。除非您认为自己是一位经验丰富的Python开发者，否则请坚持使用Anaconda。通过Google搜索`python
    anaconda`下载并安装它。
- en: 'Follow the exercise to set up and install a Python environment with Gym:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按照练习设置和安装带有Gym的Python环境：
- en: Open a new Anaconda Prompt or Python shell. Do this as an admin or be sure to
    execute the commands as an admin if required.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Anaconda Prompt或Python shell。如果您需要，请以管理员身份执行这些命令。
- en: 'From the command line, run the following:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行运行以下命令：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will create a new virtual environment for your development. A virtual environment
    allows you to isolate dependencies and control your versioning. If you are not
    using Anaconda, you can use the Python virtual environment to create a new environment.
    You should also notice that we are forcing the environment to use Python 3.6\.
    Again, this makes sure we know what version of Python we are using.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将为您的开发创建一个新的虚拟环境。虚拟环境允许您隔离依赖项并控制版本。如果您不使用Anaconda，您可以使用Python虚拟环境来创建一个新环境。您还应该注意到，我们正在强制环境使用Python
    3.6。再次强调，这确保我们知道我们使用的是哪个版本的Python。
- en: 'After the installation, we activate the environment with the following:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，我们使用以下命令激活环境：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we install Gym with the following command:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下命令安装Gym：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Gym will install several dependencies along with the various sample environments
    we will train on later.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gym将安装几个依赖项，包括我们稍后将在其上训练的各种样本环境。
- en: Before we get too far ahead though, let's now test our Gym installation with
    code in the next section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们走得太远之前，现在让我们在下一节中使用代码测试我们的Gym安装。
- en: Testing Gym
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Gym
- en: 'In the next exercise, we will write code to test Gym and an environment called
    FrozenLake, which also happens to be our test environment for this chapter. Open
    the `Chapter_2_4.py` code example and follow the exercise:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将编写代码来测试Gym和一个名为FrozenLake的环境，这个环境也恰好是我们本章的测试环境。打开`Chapter_2_4.py`代码示例并按照练习进行：
- en: 'For reference, the code is shown as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了参考，代码如下所示：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: At the top, we have the imports to load the `system` modules as well as `gym`,
    `time`, and `numpy`. `numpy` is a helper library we use to construct tensors.
    Tensors are a math/programming concept that can describe single values or multidimensional
    arrays of numbers.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在顶部，我们有导入语句，用于加载`system`模块以及`gym`、`time`和`numpy`。`numpy`是一个辅助库，我们用它来构建张量。张量是数学/编程概念，可以描述单个值或数字的多维数组。
- en: 'Next, we build and reset the environment with the following code:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码构建和重置环境：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: After that, we have a `clear` function, which we use to clear rendering that
    is not critical to this example. The code should be self-explanatory as well.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们有一个`clear`函数，我们用它来清除对示例不重要的渲染。代码应该也是不言自明的。
- en: 'This brings us to the `for` loop and where all of the actions, so to speak,
    happen. The line of most importance is shown as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将带我们到`for`循环，也就是所有动作发生的地方。以下是最重要的那一行：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `env` variable represents the environment, and, in the line, we are letting
    the algorithm take a random action every step or iteration. In this example, the
    agent learns nothing and just moves at random, for now.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`env`变量代表环境，在那一行中，我们让算法每一步或迭代随机采取一个动作。在这个例子中，代理目前什么也没学到，只是随机移动。'
- en: 'Run the code as you normally would and pay attention to the output. An example
    of the output screen is shown in the following:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码，并注意输出。以下是一个输出屏幕的示例：
- en: '![](img/6fa318c8-8f0f-46ac-95f7-a85a95896e68.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6fa318c8-8f0f-46ac-95f7-a85a95896e68.png)'
- en: Example render from the FrozenLake environment
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从FrozenLake环境中提取的示例渲染
- en: 'Since the algorithm/agent moves randomly, it is quite likely to hit a hole,
    denoted by `H` and just stay there. For reference, the legend for FrozenLake is
    given here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法/代理随机移动，它很可能会撞到表示为`H`的洞，并停留在那里。为了参考，以下是FrozenLake的图例：
- en: '**S** = start: This is where the agent starts when reset is called.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S** = 开始：当调用重置时，代理从这里开始。'
- en: '**F** = frozen: This allows the agent to move across this area.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F** = 冻结：这允许代理在这个区域内移动。'
- en: '**H** = hole: This is a hole in the ice; if the agent moves here, it falls
    in.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H** = 洞：这是冰中的洞；如果智能体移动到这里，它会掉进去。'
- en: '**G** = goal: This is the goal the agent wants to reach, and, when it does,
    it receives a reward of 1.0.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G** = 目标：这是智能体想要达到的目标，当它达到时，它将获得 1.0 的奖励。'
- en: Now that we have Gym set up, we can move to evaluate the policy in the next
    section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了 Gym，我们可以进入下一节来评估策略。
- en: Policy evaluation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略评估
- en: 'Unlike the trial-and-error learning, you have already been introduced to DP
    methods that work as a form of static learning or what we may call planning. Planning
    is an appropriate definition here since the algorithm evaluates the entire MDP
    and hence all states and actions beforehand. Hence, these methods require full
    knowledge of the environment including all finite states and actions. While this
    works for known finite environments such as the one we are playing within this
    chapter, these methods are not substantial enough for real-world physical problems.
    We will, of course, solve real-world problems later in this book. For now, though,
    let''s look at how to evaluate a policy from the previous update equations in
    code. Open `Chapter_2_5.py` and follow the exercise:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与试错学习不同，你已经接触到了 DP 方法，它们作为一种静态学习形式或我们可能称之为规划的形式工作。在这里，规划是一个合适的定义，因为算法在评估整个 MDP
    以及所有状态和动作之前。因此，这些方法需要完全了解环境，包括所有有限状态和动作。虽然这对于我们在这个章节中玩的环境等已知有限环境有效，但这些方法对于现实世界的物理问题来说还不够充分。当然，我们将在本书的后面解决现实世界的问题。不过，现在让我们看看如何从之前的更新方程中在代码中评估策略。打开
    `Chapter_2_5.py` 并遵循练习：
- en: 'For reference, the entire block of code, `Chapter_2_5.py`, is shown as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了参考，整个代码块 `Chapter_2_5.py` 显示如下：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: At the beginning of the code, we perform the same initial steps as our test
    example. We load `import` statements and initialize and load the environment,
    then define the `clear` function.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码的开始，我们执行与我们的测试示例相同的初始步骤。我们加载 `import` 语句，初始化和加载环境，然后定义 `clear` 函数。
- en: Next, move to the very end of the code and notice how we are initializing the
    policy using `numpy as np` to fill a tensor of the size of the environment, `state`
    x `action`. We then divide the tensor by the number of actions in a state—`4`,
    in this case. This gives us a distributed probability of `0.25` per action. Remember
    that the combined action probability in a Markov property needs to sum up to `1.0`
    or 100%.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，移动到代码的末尾并注意我们是如何使用 `numpy as np` 初始化策略来填充一个大小为环境 `state` x `action` 的张量。然后，我们将张量除以状态中的动作数量——在这个例子中是
    `4`。这给出了每个动作的分布式概率 `0.25`。记住，在马尔可夫属性中，所有动作的概率总和需要达到 `1.0` 或 100%。
- en: 'Now, move up the `eval_policy` function and focus on the double loop, as shown
    in the following code block:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，向上移动到 `eval_policy` 函数并关注双重循环，如下面的代码块所示：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first `for` loop loops on the number of terms or iterations before termination.
    We set a limit here to prevent endless looping. In the inner loop, all of the
    states in the environment are iterated through and acted on using the `act` function.
    After that, we use our previous render code to show the updated values. At the
    end of the first `for` loop, we check whether the calculated total change in the
    `v` value is less than a particular threshold, `theta`. If the change in value
    is less than the threshold, the function returns the calculate value function,
    `V`.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个 `for` 循环遍历在终止之前的项数或迭代次数。在这里我们设置一个限制以防止无限循环。在内循环中，通过 `act` 函数对环境中的所有状态进行迭代并采取行动。之后，我们使用之前的渲染代码来显示更新的值。第一个
    `for` 循环结束时，我们检查计算出的 `v` 值的总变化是否小于特定的阈值，`theta`。如果值的变化小于阈值，函数返回计算出的值函数，`V`。
- en: 'At the core of the algorithm is the `act` function and where the update equation
    operates; the inside of this function is shown as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法的核心是 `act` 函数以及更新方程操作的地方；该函数内部的显示如下：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first `for` loop iterates through all of the actions in the policy for the
    given state. Recall that we start by initializing the policy to `0.25` for every
    `action` function, `action_prob = 0.25`. Then, we loop through every transition
    from the state and action and apply the update. The update is shown in the highlighted
    equation. Finally, the `value` function, `V`, for the current state is updated
    to `v`.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个 `for` 循环遍历给定状态下策略中的所有动作。回想一下，我们首先将策略初始化为每个 `action` 函数的 `0.25`，即 `action_prob
    = 0.25`。然后，我们遍历从状态和动作到每个转换并应用更新。更新在突出显示的方程中显示。最后，当前状态的价值函数 `V` 更新为 `v`。
- en: 'Run the code and observe the output. Notice how the `value` function is continually
    updated. At the end of the run, you should see something similar to the following
    screenshot:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码并观察输出。注意`value`函数是如何不断更新的。在运行结束时，你应该看到类似于以下截图的内容：
- en: '![](img/a8ef6f06-a5b9-492d-9aa7-b3aa4267b4aa.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8ef6f06-a5b9-492d-9aa7-b3aa4267b4aa.png)'
- en: Running example Chapter_2_5.py
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 运行示例Chapter_2_5.py
- en: If it seems off that the policy is not updated, that is actually okay, for now.
    The important part here is to see how we update the `value` function. In the next
    section, we will look at how we can improve the policy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果策略没有更新看起来有些不对劲，实际上这现在是完全可以接受的。这里的重要部分是看到我们是如何更新`value`函数的。在下一节中，我们将探讨如何改进策略。
- en: Policy improvement
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略改进
- en: 'With policy evaluation under our belt, it is time to move on to improving the
    policy by looking ahead. Recall we do this by looking at one state ahead of the
    current state and then evaluating all possible actions. Let''s look at how this
    works in code. Open up the `Chapter_2_6.py` example and follow the exercise:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握策略评估之后，是时候通过前瞻性思考来改进策略了。回想一下，我们是通过对当前状态之前的一个状态进行观察，然后评估所有可能的行为来做到这一点的。让我们看看在代码中是如何实现这一点的。打开`Chapter_2_6.py`示例并跟随练习：
- en: 'For brevity, the following code excerpt from `Chapter_2_6.py` shows just the
    new sections of code that were added to that last example:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了简洁起见，以下从`Chapter_2_6.py`中摘录的代码仅显示了添加到上一个示例中的新代码部分：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Added to the last example are three new functions: `improve_policy`, `lookahead`,
    and `evaluate`. `improve_policy` uses a limited loop to loop through the states
    in the current environment; before looping through each state, it calls `eval_policy`
    to update the `value` function by passing in the current `policy`, `environment`,
    and `gamma` parameters (discount factor). Then, it calls the `lookahead` function,
    which internally calls an `evaluate` function that updates the action values for
    the state. `evaluate` is a modified version of the `act` function.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个示例中添加了三个新的函数：`improve_policy`、`lookahead`和`evaluate`。`improve_policy`使用有限循环遍历当前环境中的状态；在遍历每个状态之前，它调用`eval_policy`通过传递当前的`policy`、`environment`和`gamma`（折扣因子）参数来更新`value`函数。然后，它调用`lookahead`函数，该函数内部调用一个`evaluate`函数来更新状态的动作值。`evaluate`是`act`函数的一个修改版本。
- en: 'While both functions, `eval_policy` and `improve_policy`, use limited terms
    for loops to prevent endless looping, they still use very large limits; in the
    example, the default is `1e09`. Therefore, we still want to determine a condition
    to hopefully stop the loop much earlier than the term''s limit. In policy evaluation,
    we controlled this by observing the change or delta in the value function. In
    policy improvement, we now look to improve the actual policy and, to do this,
    we assume a greedy policy. In other words, we want to improve our policy to always
    pick the highest value action, as shown in the following code:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然`eval_policy`和`improve_policy`这两个函数都使用有限循环的术语来防止无限循环，但它们仍然使用非常大的限制；在示例中，默认值是`1e09`。因此，我们仍然希望确定一个条件，以便在术语限制之前尽早停止循环。在策略评估中，我们通过观察价值函数的变化或delta来控制这一点。在策略改进中，我们现在要改进实际策略，为此，我们假设一个贪婪策略。换句话说，我们希望改进我们的策略，使其总是选择具有最高价值的动作，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding block of code first uses the `numpy` function—`np.argmax` on the
    list of `action_value` returns from the `lookahead` function. This returns the
    max or `best_action`, or in other words, the greedy action. We then consider whether `current_action`
    is not equal to `best_action`; if it is not, then we consider the policy is not
    stable by setting `stable` to `false`. Since the action is not the best, we also
    update `policy` with the identity tensor using `np.eye` for the shape defined.
    This step simply assigns the policy a value of `1.0` for the best/greedy actions
    and `0.0` for all others.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码块首先使用`numpy`函数——`np.argmax`在`lookahead`函数返回的`action_value`列表上返回最大值或`best_action`，换句话说，就是贪婪动作。然后我们考虑`current_action`是否不等于`best_action`；如果不等于，那么我们认为策略是不稳定的，将`stable`设置为`false`。由于动作不是最好的，我们还使用`np.eye`为定义的形状更新`policy`，使用单位张量。这一步只是将策略的值分配为`1.0`给最佳/贪婪动作，而其他所有动作的值为`0.0`。
- en: At the end of the code, you can see that we now just call `improve_policy` and
    print the results of the policy and value functions.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码的末尾，你可以看到我们现在只是调用`improve_policy`并打印策略和价值函数的结果。
- en: 'Run the code as you normally would and observe the output, as shown in the
    following screenshot:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码并观察输出，如下面的截图所示：
- en: '![](img/9e42541a-6250-41d1-93bd-c867138337d3.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9e42541a-6250-41d1-93bd-c867138337d3.png)'
- en: Example output for Chapter_2_6.py
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`Chapter_2_6.py`的示例输出'
- en: This sample will take a while longer to run and you should see the `value` function
    improve as the sample runs. When the sample completes, it will print the value
    function and policy. You can now see how the policy clearly indicates the best
    action for each state with a `1.0` value. The reason some states still have the
    `0.25` value for all actions is that the algorithm sees no need to evaluate or
    improve the policy in those states. They were likely states that were holes or
    were outside the optimal path.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将需要更长的时间来运行，你应该会看到随着示例的运行，`value`函数得到改善。当示例完成后，它将打印出值函数和策略。你现在可以看到策略如何清楚地指示每个状态的最佳动作，其值为`1.0`。一些状态的所有动作仍然具有`0.25`的值，原因在于算法认为在这些状态下没有必要评估或改进策略。这些状态可能是空缺状态或位于最优路径之外。
- en: Policy evaluation and improvement is one method we can use for planning with
    DP, but, in the next section, we will look at a second method called value iteration.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 政策评估和改进是我们可以使用DP进行规划的一种方法，但在下一节中，我们将探讨第二种方法，称为值迭代。
- en: Building value iteration
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建值迭代
- en: 'Iterating over values may seem a step back to what we referred to as policy
    iteration in the last section, but it is actually more of a side step or companion
    method. In value iteration, we loop through all states in the entire MDP looking
    for the best value for each state, and when we find that, we stop or break. However,
    we don''t stop there and we continue by looking ahead of all states and then assuming
    a deterministic probability of 100% for the best action. This yields a new policy
    that may perform better than the previous policy iteration demonstration. The
    differences between both methods are subtle and best understood with a code example.
    Open up `Chapter_2_7.py` and follow the next exercise:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在值迭代中，我们遍历整个MDP中的所有状态，寻找每个状态的最佳值，当我们找到时，就停止或中断。然而，我们并没有就此停止，而是继续向前查看所有状态，并假设最佳动作的概率为100%。这产生了一种新的策略，可能比之前的策略迭代演示表现得更好。这两种方法之间的差异很微妙，最好通过代码示例来理解。打开`Chapter_2_7.py`并跟随下一个练习：
- en: 'This code example builds on the previous example. New code changes in example
    `Chapter_2_7.py` are shown in the following code:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个代码示例建立在之前的示例之上。示例`Chapter_2_7.py`中的新代码变化如下：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The bulk of this code is quite similar to code we already reviewed in the previous
    examples, but there are some subtle differences worth noting.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的大部分与我们之前在示例中已经审查过的代码非常相似，但也有一些值得注意的细微差别。
- en: 'First, this time, inside the limited terms loop, we iterated through the states
    and performed a straight lookahead with the `lookahead` function. The details
    of this code are as follows:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，这次，在有限项循环内部，我们遍历状态并使用`lookahead`函数进行直接前瞻性查看。此代码的详细信息如下：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The slight difference in the preceding code versus policy evaluation and improvement
    is that, this time, we do an immediate lookahead and iterate over action values
    and then update the `value` function based on the best value. In this block of
    code, we also calculate a new `delta` value or amount of change from the previous
    best action value:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与政策评估和改进相比，前述代码的细微差别在于，这次我们立即进行前瞻性查看，遍历动作值，然后根据最佳值更新`value`函数。在这段代码块中，我们还计算了一个新的`delta`值或从先前最佳动作值的变化量：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After the loop, there is an `if` statement that checks whether the calculated
    `delta` value or the amount of action value change is below a particular threshold,
    `theta`. If `delta` is sufficiently small, we break the limited terms loop:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环之后，有一个`if`语句检查计算出的`delta`值或动作值变化量是否低于特定的阈值`theta`。如果`delta`足够小，我们就中断有限项循环：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: From there, we initialize `policy` this time to all zeros with the `numpy np.zeros`
    function. Then, we loop through all of the states again and perform another one-step
    lookahead using the `lookahead` function. This returns a list of action values,
    which we determine the max index of, `best_action`. We then set `policy` to `1.0`;
    we assume the best action is always the one chosen for the state. Finally, we
    return the new policy and the `value` function, `V`.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那里，我们使用`numpy np.zeros`函数将`policy`初始化为零。然后，我们再次遍历所有状态，并使用`lookahead`函数进行另一步前瞻性查看。这个函数返回一个动作值的列表，我们确定最大索引值，即`best_action`。然后我们将`policy`设置为`1.0`；我们假设最佳动作总是为该状态选择。最后，我们返回新的策略和`value`函数，`V`。
- en: 'Run the code as you have and examine the output as shown in the following screenshot:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码，并检查以下截图所示的输出：
- en: '![](img/ba298c25-3bc4-4d1e-99d5-7c319438e071.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba298c25-3bc4-4d1e-99d5-7c319438e071.png)'
- en: Example output from Chapter_2_8.py
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Chapter_2_8.py 生成的输出示例
- en: This time, we don't do any policy iteration or improvement so the sample runs
    faster. You should also note how the policy has been updated for all states. Recall,
    in policy iteration, only the relevant states the algorithm/agent could move through
    were evaluated.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们没有进行任何策略迭代或改进，因此样本运行得更快。你也应该注意策略是如何对所有状态进行更新的。回想一下，在策略迭代中，只有算法/智能体能够通过的相关状态才会被评估。
- en: In the next section, we turn an actual agent loose on the environment using
    the policy calculated with policy iteration and improvement versus value iteration
    in the next section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用策略迭代和改进与价值迭代计算出的策略，将实际智能体释放到环境中。
- en: Playing with policy versus value iteration
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略与价值迭代的比较
- en: 'Policy and value iteration methods are quite similar and looked at as companion
    methods. As such, to evaluate which method to use, we often need to apply both
    methods to the problem in question. In the next exercise, we will evaluate both
    policy and value iteration methods side by side in the FrozenLake environment:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 策略和价值迭代方法非常相似，被视为配套方法。因此，为了评估使用哪种方法，我们通常需要将两种方法都应用于所讨论的问题。在下一个练习中，我们将在 FrozenLake
    环境中同时评估策略和价值迭代方法：
- en: 'Open the `Chapter_2_8.py` example. This example builds on the previous code
    examples, so we will only show the new additional code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_2_8.py`示例。此示例基于之前的代码示例，因此我们只展示新的附加代码：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The additional code consists of a new function, `play`, and different test
    code at the end. At the code at the end, we first calculate a policy using the
    `improve_policy` function, which performs policy iteration:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附加代码包括一个新的函数`play`和末尾的不同测试代码。在末尾的代码中，我们首先使用`improve_policy`函数计算策略，该函数执行策略迭代：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Next, we evaluate the number of wins for `policy` by using the `play` function.
    After this, we print the number of wins.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`play`函数评估`policy`的获胜次数。之后，我们打印获胜次数。
- en: 'Then, we evaluate a new policy using value iteration, again using the `play`
    function to evaluate the number of wins and printing the result:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用价值迭代评估一个新的策略，再次使用`play`函数评估获胜次数，并打印结果：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Inside the `play` function, we loop through the number of episodes. Each episode
    is considered to be one attempt by the agent to move from the start to the goal.
    In this example, the termination of an episode happens when the agent encounters
    a hole or the goal. If it reaches the goal, it receives a reward of `1.0`. Most
    of the code is self-explanatory, aside from the moment an agent conducts an action
    and is shown again as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`play`函数中，我们遍历次数。每个回合都被认为是智能体从起点移动到目标的一次尝试。在这个例子中，回合的终止发生在智能体遇到洞或目标时。如果它达到目标，它将获得`1.0`的奖励。大部分代码都是自解释的，除了智能体执行动作并再次显示的时刻如下：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Recall, in our Gym environment test, we just randomly stepped the agent around.
    Now, in the preceding code, we execute a specific action set by the policy. The
    return from taking the action is `next_state`, `reward` (if any), `term` or termination,
    and an `info` variable. This line of code entirely controls the agent and allows
    it to move and interact with the environment:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回想一下，在我们的 Gym 环境测试中，我们只是随机移动智能体。现在，在前面代码中，我们执行由策略设定的特定动作。执行动作的回报是`next_state`、`reward`（如果有）、`term`或终止，以及一个`info`变量。这一行代码完全控制智能体，并允许其移动并与环境交互：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: After the agent takes a step, we then update `total_reward` and `state`. Then,
    we test to see whether the agent won, the environment was terminated, and the
    returned reward was `1.0`. Otherwise, the agent continues. The agent may also
    terminate an episode from falling into a hole.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 智能体移动一步后，我们更新`total_reward`和`state`。然后，我们测试智能体是否获胜，环境是否终止，以及返回的奖励是否为`1.0`。否则，智能体继续。智能体也可能因掉入洞而结束回合。
- en: 'Run the code as you normally would and examine the output, as shown in the
    following screenshot:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码，并检查以下截图所示的输出：
- en: '![](img/049500f1-b224-4bf4-b2e0-adc3c96c9331.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/049500f1-b224-4bf4-b2e0-adc3c96c9331.png)'
- en: Example output from example Chapter_2_8.py
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从示例 Chapter_2_8.py 生成的输出示例
- en: Notice the difference in results. This is the difference in policy iteration
    with value iteration over the FrozenLake problem. You can play with and adjust
    the parameters of `theta` and `gamma` to see whether you get better results for
    either method. Again, much like RL itself, you will need to perform a little trial
    and error on your own to determine the best DP method to use.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意结果之间的差异。这是在FrozenLake问题上策略迭代与值迭代的差异。你可以调整`theta`和`gamma`参数的值，看看是否可以得到更好的结果。同样，就像强化学习本身一样，你需要自己进行一些尝试和错误，以确定最佳的DP方法。
- en: In the next section, we look at some additional exercises that can help you further your
    understanding of the material.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一些可以帮助你进一步理解材料的附加练习。
- en: Exercises
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Completing the exercises in this section is entirely optional, but, hopefully,
    you can start to appreciate that we, as reinforcement learners ourselves, learn
    best by doing. Do your best and attempt to complete at least 2-3 exercises from
    the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本节中的练习完全是可选的，但希望你能开始欣赏到，作为强化学习者的我们，通过实践学习是最好的。尽力而为，并尝试完成以下至少2-3个练习：
- en: Consider other problems you could use DP with? How would you break the problem
    up into subproblems and calculate each subproblem?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑其他可以使用动态规划（DP）解决的问题？你将如何将问题分解为子问题并计算每个子问题？
- en: Code up another example that compares a problem programmed linearly versus dynamically.
    Use the example from *Exercise 1*. The code examples, `Chapter_2_2.py` and `Chapter_2_3.py`,
    are good examples of side-by-side comparisons.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写另一个示例，比较线性编程与动态编程的问题。使用*练习1*中的示例。代码示例`Chapter_2_2.py`和`Chapter_2_3.py`是并列比较的好例子。
- en: Look through the OpenAI documentation and explore other RL environments.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查阅OpenAI文档，并探索其他强化学习环境。
- en: Create, render, and explore other RL environments from Gym using the sample
    test code from `Chapter_2_4.py`.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Chapter_2_4.py`中的示例测试代码创建、渲染和探索Gym中的其他强化学习环境。
- en: Explain the process/algorithm of evaluating and improving a policy using DP.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释使用DP评估和改进策略的过程/算法。
- en: Explain the difference between policy iteration and value iteration.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释策略迭代与值迭代之间的区别。
- en: Open the `Chapter_2_5.py` policy iteration example and adjust the `theta` and
    `gamma` parameters. What effect do these have on learning rates and values?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_2_5.py`策略迭代示例，并调整`theta`和`gamma`参数。这些参数对学习率和值有什么影响？
- en: Open the `Chapter_2_6.py` policy improvement example and adjust the `theta`
    and `gamma` parameters. What effect do these have on learning rates and values?
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_2_6.py`策略改进示例，并调整`theta`和`gamma`参数。这些参数对学习率和值有什么影响？
- en: Open the `Chapter_2_7.py` value iteration example and adjust the `theta` and
    `gamma` parameters. What effect do these have on learning rates and values?
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_2_7.py`值迭代示例，并调整`theta`和`gamma`参数。这些参数对学习率和值有什么影响？
- en: Complete all of the policy and value iteration examples using the `FrozenLake
    8x8` environment. This a much larger version of the lake problem. Now, which method
    performs better?
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FrozenLake 8x8`环境完成所有策略和值迭代示例。这是湖泊问题的更大版本。现在，哪种方法表现更好？
- en: Use these exercises to strengthen your knowledge of the material we just covered
    in this chapter. In this next section, we will summarize what we covered in this
    chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些练习来加强你对本章所涵盖材料的理解。在下一节中，我们将总结本章所涵盖的内容。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we took an in-depth look at DP and the Bellman equation. The
    Bellman equation with DP has influenced RL significantly by introducing the concept
    of future rewards and optimization. We covered the contribution of Bellman in
    this chapter by first taking a deep look at DP and how to solve a problem dynamically.
    Then, we advanced to understanding the Bellman optimality equation and how it
    can be used to account for future rewards as well as determine expected state
    and action values using iterative methods. In particular, we focused on the implementation
    in Python of policy iteration and improvement. Then, from there, we looked at
    value iteration. Finally, we concluded this chapter by setting up an agent test
    against the FrozenLake environment using a policy generated by both policy and
    value iteration. For this chapter, we looked at a specific class of problems well
    suited to DP that also helped us to derive other concepts in RL such as discounted
    rewards.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了动态规划（DP）和贝尔曼方程。通过引入未来奖励和优化的概念，具有DP的贝尔曼方程对强化学习（RL）产生了显著影响。在本章中，我们首先深入研究了动态规划以及如何动态地解决问题，从而介绍了贝尔曼的贡献。然后，我们进一步理解了贝尔曼最优性方程及其如何通过迭代方法来考虑未来奖励以及确定期望的状态和动作值。特别是，我们关注了在Python中实现策略迭代和改进的实现。接着，我们从那里转向了价值迭代。最后，我们通过使用由策略和价值迭代生成的策略，在FrozenLake环境中设置了一个智能体测试，以此结束本章。对于本章，我们研究了一类非常适合DP的问题，这也有助于我们推导出强化学习中的其他概念，例如折现奖励。
- en: In the next chapter, we continue with this theme by looking at Monte Carlo methods.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探讨这一主题，通过研究蒙特卡洛方法。
