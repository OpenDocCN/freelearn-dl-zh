- en: Dynamic Programming and the Bellman Equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dynamic programming** (**DP**) was the second major thread to influence modern
    **reinforcement learning** (**RL**) after trial-and-error learning. In this chapter,
    we will look at the foundations of DP and explore how they influenced the field
    of RL. We will also look at how the Bellman equation and the concept of optimality
    have interwoven with RL. From there, we will look at policy and value iteration
    methods to solve a class of problems well suited for DP. Finally, we will look
    at how to use the concepts we have learned in this chapter to teach an agent to
    play the FrozenLake environment from OpenAI Gym.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Bellman equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building policy iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building value iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with policy versus value iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this chapter, we look at how to solve a finite **Markov decision process**
    (**MDP**) with DP using the Bellman equation of optimality. This chapter is meant
    as a history lesson and background to DP and Bellman. If you are already quite
    familiar with DP, then you may want to bypass this chapter since we will just
    explore entry-level DP, covering just enough background to see how it has influenced
    and bettered RL.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DP was developed by Richard E. Bellman in the 1950s as a way to optimize and
    solve complex decision problems. The method was first applied to engineering control
    problems but has since found uses in all disciplines requiring the analytical
    modeling of problems and subproblems. In effect, all DP is about is solving subproblems
    and then finding relationships to connect those to solve bigger problems. It does
    all of this by first applying the Bellman optimality equation and then solving
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to solving a finite MDP with DP, we will want to understand, in
    a little more detail, what it is we are talking about. Let's look at a simple
    example of the difference between normal recursion and DP in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Regular programming versus DP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will do a comparison by first solving a problem using regular methods and
    then with DP. Along the way, we will identify key elements that make our solution
    a DP one. What most experienced programmers find is that they likely have done
    DP in some capacity, so don''t be surprised if this all sounds really familiar.
    Let''s open up the `Chapter_2_1.py` example and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is an example of finding the *n*^(th) number in the Fibonacci sequence
    using recursion and is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that we can resolve the *n*^(th) element in the Fibonacci sequence by
    summing the two previous digits in the sequence. We consider that when `n == 1`,
    the value is `0`, and when `n == 2`, the returned value is `1`. Hence, the third
    element in the sequence would be the sum of `Fibonacci(1)` and `Fibonacci(2)`
    and would return a value of `1`. This reflects in the code shown in the following
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, the solution for finding the fourth element using the linear programming
    version of recursion is shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/73bd0b7b-9d48-4a87-bf49-535ef14ad31e.png)'
  prefs: []
  type: TYPE_IMG
- en: Solving the fourth element of the Fibonacci sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'We can note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows each recursive call to the `Fibonacci` function
    required to calculate previous elements. Notice how this method requires the solution
    to solve or call the `Fibonacci(2)` function twice. Of course, that additional
    call in this example is trivial but these additional calls can quickly add up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the code as you normally would and see the printed result for the ninth
    element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To appreciate how inefficient recursion can be, we have modified our previous
    example and saved it as `Chapter_2_2.py`. Open up that example now and follow
    the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified code, with the extra line highlighted, is shown for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'All we are doing is printing out when we need to calculate two more sequences
    to return the sum. Run the code and notice the output, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3261baec-4253-4b54-aa74-1cc31741d5c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Output from example Chapter_2_1.py
  prefs: []
  type: TYPE_NORMAL
- en: Notice, for just calculating the ninth element of the Fibonacci sequence using
    recursion, the number of times just `Fibonacci(3)` is called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the solution works and, as they say, it is pretty and clean. In fact, as
    a programmer, you may have been taught to worship this style of coding at one
    time. However, we can clearly see how inefficient this method is to scale and
    that is where DP comes in, which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Enter DP and memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A key concept in DP is to break down larger problems into smaller subproblems,
    then solve said smaller problems and store the result. The fancy name for this
    activity is called **memoization** and the best way to showcase how this works
    is with an example. Open up `Chapter_2_3.py` and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the entire block of code from our previous example has been
    modified as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the highlighted lines denote code changes, but, in this case, we will
    go over each code change in more detail starting with the first change, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This new line creates a new starting Fibonacci list with our two base numbers,
    `0` and `1`. We will still use a recursive function, but now we also store the
    results of every unique calculation for later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next code change is where the algorithm returns a previously stored value,
    as in `0` or `1`, or as calculated and then stored in the `fibSequence` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The last group of code changes now saves the recursive calculation, adding the
    new value to the entire sequence. This now requires the algorithm to only calculate
    each *n*^(th) value of the sequence once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and look at the results shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a270b391-9ecb-4969-a97d-2e3ae9f7833d.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from example Chapter_2_3.py
  prefs: []
  type: TYPE_NORMAL
- en: Notice how we are now only calculating the numbers in the sequence and not repeating
    any calculations. Clearly, this method is far superior to the linear programming
    example that we looked at earlier, and the code actually doesn't look bad either.
    Now, as we said, if this type of solution seems obvious, then you probably already
    understand more about DP than you realize.
  prefs: []
  type: TYPE_NORMAL
- en: We will only cover a very simple introduction to DP as it relates to RL in this
    book. As you can see, DP is a powerful technique that can benefit any developer
    who is serious about optimizing code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look further at the work of Bellman and the equation
    that was named after him.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bellman worked on solving finite MDP with DP, and it was during these efforts
    he derived his famed equation. The beauty behind this equation—and more abstractly,
    the concept, in general—is that it describes a method of optimizing the value
    or quality of a state. In other words, it describes how we can determine the optimal
    value/quality for being in a given state given the action and choices of successive
    states. Before breaking down the equation itself, let's first reconsider the finite
    MDP in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Unraveling the finite MDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the finite MDP we developed in [Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding
    Rewards Learning*, that described your morning routine. Don''t to worry if you
    didn''t complete that exercise previously as we will consider a more specific
    example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa23c01a-8b06-4551-899e-7a906e68aea9.png)'
  prefs: []
  type: TYPE_IMG
- en: MDP for waking up and getting on the bus
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding finite MDP describes a possible routine for someone waking up
    and getting ready to get on a bus to go to school or work. In this MDP, we define
    a beginning state (**BEGIN**) and an ending state, that is, getting on the bus
    (**END**). The **R =** denotes the reward allotted when moving to that state and
    the number closest to the end of the action line denotes the probability of taking
    that action. We can denote the optimum path through this finite MDP, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9710a44e-0116-493e-9cb3-32311140ee1b.png)'
  prefs: []
  type: TYPE_IMG
- en: The optimum solution to the MDP
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can describe the optimum result, therefore, as the sum of
    all rewards traversed through the environment to obtain the total reward. More
    formally, we can also write this mathematically like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/076725cd-a610-4a0b-a86d-6601c01a0ee5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, and this is where Bellman comes in, we can''t consider all rewards
    as equal. Without exploring the math in more exhaustive detail, the Bellman equation
    introduced the concept that future rewards should be discounted. This is also
    quite intuitive when you think about it. The experience or effect we feel from
    future actions is weakened depending on how far in the future we need to decide.
    This is the same concept we apply with the Bellman equation. Hence, we can now
    apply a discounting factor (gamma) to the previous equation and show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b648417-d51e-4680-a5be-3499d76264c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Gamma (![](img/9906fcb0-b342-4e56-809a-bbd16b5926cc.png)), shown in the equation,
    represents a future rewards discount factor. The value can be from `0.0` to `1.0`.
    If the value is `1.0`, then we consider no discount of future rewards, whereas
    a value of `0.1` would heavily discount future rewards. In most RL problems, we
    keep this number fairly high and well above `0.9`. This leads us to the next section
    where we discuss how the Bellman equation can optimize a problem.
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman optimality equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Bellman equation shows us that you can solve any MDP by first finding the
    optimal policy that allows an agent to traverse that MDP. Recall that a policy
    defines the decisions for each action that will guide an agent through an MDP.
    Ideally, what we want to find is the optimal policy: a policy that can maximize
    the value for each state and determine which states to traverse for maximum reward.
    When we combine this with other concepts and apply more math wizardry and then
    combine it with the Bellman optimality equation, we get the following optimal
    policy equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02370093-e53e-42ff-951f-2c3bd8fedd75.png)'
  prefs: []
  type: TYPE_IMG
- en: That strange term at the very beginning ([![](img/fb95776c-d075-47d2-bdd1-fc3aa828e7b3.png)])
    is a way of describing a function that maximizes the rewards given a set of states
    and actions considering that we discount future rewards by a factor called gamma.
    Notice how we also use ![](img/cd5e7dcf-31c0-49dd-b6ce-9a3d6dee4650.png) to denote
    the policy equation but we often think of this as a quality and may refer to this
    as *q* or *Q*. If you think back to our previous peek at the Q-learning equation,
    then now you can clearly see how the rewards' discount factor, gamma, comes into
    play.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at methods to solve an MDP using DP and a
    method of policy iteration given our understanding of the Bellman optimality principle
    and resulting policy equation.
  prefs: []
  type: TYPE_NORMAL
- en: Building policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For us to determine the best policy, we first need a method to evaluate the
    given policy for a state. We can use a method of evaluating the policy by searching
    through all of the states of an MDP and further evaluating all actions. This will
    provide us with a value function for the given state that we can then use to perform
    successive updates of a new value function iteratively. Mathematically, we can
    then use the previous Bellman optimality equation and derive a new update to a
    state value function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c9818aa-db72-4266-8766-866221321deb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the [![](img/d19906d9-7bab-41e1-98ee-e739b3c8d6ff.png)]
    symbol represents an expectation and denotes the expected state value update to
    a new value function. Inside this expectation, we can see this dependent on the
    returned reward plus the previous discounted value for the next state given an
    already chosen action. That means that our algorithm will iterate over every state
    and action evaluating a new state value using the preceding update equation. This
    process is called backing up or planning, and it is helpful for us to visualize
    how this algorithm works using backup diagrams. The following is an example of
    the backup diagrams for action value and state value backups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/817a2069-e71b-4177-9bc6-e1c9ecd6f6c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Backup diagrams for action value and state value backups
  prefs: []
  type: TYPE_NORMAL
- en: Diagram **(a)** or [![](img/1feda0df-b44f-4614-9d34-598e29d447b8.png)]is the
    part of the backup or evaluation that tries each action and hence provides us
    with action values. The second part of the evaluation comes from the update and
    is shown in diagram **(b)** for [![](img/134a97e6-b70d-4a6e-a633-5f315102b150.png)].
    Recall, the update evaluates the forward states by evaluating each of the state
    actions. The diagrams represent the point of evaluation with a filled-in solid
    circle. Notice how the action value only focuses on the forward actions while
    the state value focuses on the action value for each forward state. Of course,
    it will be helpful to look at how this comes together in code. However, before
    we get to that, we want to do some housekeeping in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help to encourage research and development in RL, the OpenAI group provides
    an open source platform for RL training called Gym. Gym, provided by OpenAI, comes
    with plenty of sample test environments that we can venture through while we work
    through this book. Also, other RL developers have developed other environments
    using the same standard interface Gym uses. Hence, by learning to use Gym, we
    will also be able to explore other cutting-edge RL environments later in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: The installation for Gym is quite simple, but, at the same time, we want to
    avoid any small mistakes that may cause you frustration later. Therefore, it is
    best to use the following instructions to set up and install an RL environment
    for development.
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended that you use Anaconda for Python development with this
    book. Anaconda is a free open source cross-platform tool that can significantly
    increase your ease of development. Please stick with Anaconda unless you consider
    yourself an experienced Python developer. Google `python anaconda` to download
    and install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the exercise to set up and install a Python environment with Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Anaconda Prompt or Python shell. Do this as an admin or be sure to
    execute the commands as an admin if required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the command line, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will create a new virtual environment for your development. A virtual environment
    allows you to isolate dependencies and control your versioning. If you are not
    using Anaconda, you can use the Python virtual environment to create a new environment.
    You should also notice that we are forcing the environment to use Python 3.6\.
    Again, this makes sure we know what version of Python we are using.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the installation, we activate the environment with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we install Gym with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Gym will install several dependencies along with the various sample environments
    we will train on later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we get too far ahead though, let's now test our Gym installation with
    code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next exercise, we will write code to test Gym and an environment called
    FrozenLake, which also happens to be our test environment for this chapter. Open
    the `Chapter_2_4.py` code example and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the code is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: At the top, we have the imports to load the `system` modules as well as `gym`,
    `time`, and `numpy`. `numpy` is a helper library we use to construct tensors.
    Tensors are a math/programming concept that can describe single values or multidimensional
    arrays of numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we build and reset the environment with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: After that, we have a `clear` function, which we use to clear rendering that
    is not critical to this example. The code should be self-explanatory as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This brings us to the `for` loop and where all of the actions, so to speak,
    happen. The line of most importance is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `env` variable represents the environment, and, in the line, we are letting
    the algorithm take a random action every step or iteration. In this example, the
    agent learns nothing and just moves at random, for now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and pay attention to the output. An example
    of the output screen is shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6fa318c8-8f0f-46ac-95f7-a85a95896e68.png)'
  prefs: []
  type: TYPE_IMG
- en: Example render from the FrozenLake environment
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the algorithm/agent moves randomly, it is quite likely to hit a hole,
    denoted by `H` and just stay there. For reference, the legend for FrozenLake is
    given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** = start: This is where the agent starts when reset is called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F** = frozen: This allows the agent to move across this area.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H** = hole: This is a hole in the ice; if the agent moves here, it falls
    in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G** = goal: This is the goal the agent wants to reach, and, when it does,
    it receives a reward of 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have Gym set up, we can move to evaluate the policy in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Policy evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike the trial-and-error learning, you have already been introduced to DP
    methods that work as a form of static learning or what we may call planning. Planning
    is an appropriate definition here since the algorithm evaluates the entire MDP
    and hence all states and actions beforehand. Hence, these methods require full
    knowledge of the environment including all finite states and actions. While this
    works for known finite environments such as the one we are playing within this
    chapter, these methods are not substantial enough for real-world physical problems.
    We will, of course, solve real-world problems later in this book. For now, though,
    let''s look at how to evaluate a policy from the previous update equations in
    code. Open `Chapter_2_5.py` and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the entire block of code, `Chapter_2_5.py`, is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: At the beginning of the code, we perform the same initial steps as our test
    example. We load `import` statements and initialize and load the environment,
    then define the `clear` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, move to the very end of the code and notice how we are initializing the
    policy using `numpy as np` to fill a tensor of the size of the environment, `state`
    x `action`. We then divide the tensor by the number of actions in a state—`4`,
    in this case. This gives us a distributed probability of `0.25` per action. Remember
    that the combined action probability in a Markov property needs to sum up to `1.0`
    or 100%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, move up the `eval_policy` function and focus on the double loop, as shown
    in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first `for` loop loops on the number of terms or iterations before termination.
    We set a limit here to prevent endless looping. In the inner loop, all of the
    states in the environment are iterated through and acted on using the `act` function.
    After that, we use our previous render code to show the updated values. At the
    end of the first `for` loop, we check whether the calculated total change in the
    `v` value is less than a particular threshold, `theta`. If the change in value
    is less than the threshold, the function returns the calculate value function,
    `V`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the core of the algorithm is the `act` function and where the update equation
    operates; the inside of this function is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first `for` loop iterates through all of the actions in the policy for the
    given state. Recall that we start by initializing the policy to `0.25` for every
    `action` function, `action_prob = 0.25`. Then, we loop through every transition
    from the state and action and apply the update. The update is shown in the highlighted
    equation. Finally, the `value` function, `V`, for the current state is updated
    to `v`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code and observe the output. Notice how the `value` function is continually
    updated. At the end of the run, you should see something similar to the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a8ef6f06-a5b9-492d-9aa7-b3aa4267b4aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Running example Chapter_2_5.py
  prefs: []
  type: TYPE_NORMAL
- en: If it seems off that the policy is not updated, that is actually okay, for now.
    The important part here is to see how we update the `value` function. In the next
    section, we will look at how we can improve the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Policy improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With policy evaluation under our belt, it is time to move on to improving the
    policy by looking ahead. Recall we do this by looking at one state ahead of the
    current state and then evaluating all possible actions. Let''s look at how this
    works in code. Open up the `Chapter_2_6.py` example and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For brevity, the following code excerpt from `Chapter_2_6.py` shows just the
    new sections of code that were added to that last example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Added to the last example are three new functions: `improve_policy`, `lookahead`,
    and `evaluate`. `improve_policy` uses a limited loop to loop through the states
    in the current environment; before looping through each state, it calls `eval_policy`
    to update the `value` function by passing in the current `policy`, `environment`,
    and `gamma` parameters (discount factor). Then, it calls the `lookahead` function,
    which internally calls an `evaluate` function that updates the action values for
    the state. `evaluate` is a modified version of the `act` function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While both functions, `eval_policy` and `improve_policy`, use limited terms
    for loops to prevent endless looping, they still use very large limits; in the
    example, the default is `1e09`. Therefore, we still want to determine a condition
    to hopefully stop the loop much earlier than the term''s limit. In policy evaluation,
    we controlled this by observing the change or delta in the value function. In
    policy improvement, we now look to improve the actual policy and, to do this,
    we assume a greedy policy. In other words, we want to improve our policy to always
    pick the highest value action, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding block of code first uses the `numpy` function—`np.argmax` on the
    list of `action_value` returns from the `lookahead` function. This returns the
    max or `best_action`, or in other words, the greedy action. We then consider whether `current_action`
    is not equal to `best_action`; if it is not, then we consider the policy is not
    stable by setting `stable` to `false`. Since the action is not the best, we also
    update `policy` with the identity tensor using `np.eye` for the shape defined.
    This step simply assigns the policy a value of `1.0` for the best/greedy actions
    and `0.0` for all others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the code, you can see that we now just call `improve_policy` and
    print the results of the policy and value functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and observe the output, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9e42541a-6250-41d1-93bd-c867138337d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output for Chapter_2_6.py
  prefs: []
  type: TYPE_NORMAL
- en: This sample will take a while longer to run and you should see the `value` function
    improve as the sample runs. When the sample completes, it will print the value
    function and policy. You can now see how the policy clearly indicates the best
    action for each state with a `1.0` value. The reason some states still have the
    `0.25` value for all actions is that the algorithm sees no need to evaluate or
    improve the policy in those states. They were likely states that were holes or
    were outside the optimal path.
  prefs: []
  type: TYPE_NORMAL
- en: Policy evaluation and improvement is one method we can use for planning with
    DP, but, in the next section, we will look at a second method called value iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Building value iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Iterating over values may seem a step back to what we referred to as policy
    iteration in the last section, but it is actually more of a side step or companion
    method. In value iteration, we loop through all states in the entire MDP looking
    for the best value for each state, and when we find that, we stop or break. However,
    we don''t stop there and we continue by looking ahead of all states and then assuming
    a deterministic probability of 100% for the best action. This yields a new policy
    that may perform better than the previous policy iteration demonstration. The
    differences between both methods are subtle and best understood with a code example.
    Open up `Chapter_2_7.py` and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code example builds on the previous example. New code changes in example
    `Chapter_2_7.py` are shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of this code is quite similar to code we already reviewed in the previous
    examples, but there are some subtle differences worth noting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, this time, inside the limited terms loop, we iterated through the states
    and performed a straight lookahead with the `lookahead` function. The details
    of this code are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The slight difference in the preceding code versus policy evaluation and improvement
    is that, this time, we do an immediate lookahead and iterate over action values
    and then update the `value` function based on the best value. In this block of
    code, we also calculate a new `delta` value or amount of change from the previous
    best action value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After the loop, there is an `if` statement that checks whether the calculated
    `delta` value or the amount of action value change is below a particular threshold,
    `theta`. If `delta` is sufficiently small, we break the limited terms loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From there, we initialize `policy` this time to all zeros with the `numpy np.zeros`
    function. Then, we loop through all of the states again and perform another one-step
    lookahead using the `lookahead` function. This returns a list of action values,
    which we determine the max index of, `best_action`. We then set `policy` to `1.0`;
    we assume the best action is always the one chosen for the state. Finally, we
    return the new policy and the `value` function, `V`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you have and examine the output as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ba298c25-3bc4-4d1e-99d5-7c319438e071.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_2_8.py
  prefs: []
  type: TYPE_NORMAL
- en: This time, we don't do any policy iteration or improvement so the sample runs
    faster. You should also note how the policy has been updated for all states. Recall,
    in policy iteration, only the relevant states the algorithm/agent could move through
    were evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we turn an actual agent loose on the environment using
    the policy calculated with policy iteration and improvement versus value iteration
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with policy versus value iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Policy and value iteration methods are quite similar and looked at as companion
    methods. As such, to evaluate which method to use, we often need to apply both
    methods to the problem in question. In the next exercise, we will evaluate both
    policy and value iteration methods side by side in the FrozenLake environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Chapter_2_8.py` example. This example builds on the previous code
    examples, so we will only show the new additional code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The additional code consists of a new function, `play`, and different test
    code at the end. At the code at the end, we first calculate a policy using the
    `improve_policy` function, which performs policy iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next, we evaluate the number of wins for `policy` by using the `play` function.
    After this, we print the number of wins.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we evaluate a new policy using value iteration, again using the `play`
    function to evaluate the number of wins and printing the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the `play` function, we loop through the number of episodes. Each episode
    is considered to be one attempt by the agent to move from the start to the goal.
    In this example, the termination of an episode happens when the agent encounters
    a hole or the goal. If it reaches the goal, it receives a reward of `1.0`. Most
    of the code is self-explanatory, aside from the moment an agent conducts an action
    and is shown again as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall, in our Gym environment test, we just randomly stepped the agent around.
    Now, in the preceding code, we execute a specific action set by the policy. The
    return from taking the action is `next_state`, `reward` (if any), `term` or termination,
    and an `info` variable. This line of code entirely controls the agent and allows
    it to move and interact with the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: After the agent takes a step, we then update `total_reward` and `state`. Then,
    we test to see whether the agent won, the environment was terminated, and the
    returned reward was `1.0`. Otherwise, the agent continues. The agent may also
    terminate an episode from falling into a hole.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and examine the output, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/049500f1-b224-4bf4-b2e0-adc3c96c9331.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from example Chapter_2_8.py
  prefs: []
  type: TYPE_NORMAL
- en: Notice the difference in results. This is the difference in policy iteration
    with value iteration over the FrozenLake problem. You can play with and adjust
    the parameters of `theta` and `gamma` to see whether you get better results for
    either method. Again, much like RL itself, you will need to perform a little trial
    and error on your own to determine the best DP method to use.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at some additional exercises that can help you further your
    understanding of the material.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Completing the exercises in this section is entirely optional, but, hopefully,
    you can start to appreciate that we, as reinforcement learners ourselves, learn
    best by doing. Do your best and attempt to complete at least 2-3 exercises from
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider other problems you could use DP with? How would you break the problem
    up into subproblems and calculate each subproblem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code up another example that compares a problem programmed linearly versus dynamically.
    Use the example from *Exercise 1*. The code examples, `Chapter_2_2.py` and `Chapter_2_3.py`,
    are good examples of side-by-side comparisons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look through the OpenAI documentation and explore other RL environments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create, render, and explore other RL environments from Gym using the sample
    test code from `Chapter_2_4.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the process/algorithm of evaluating and improving a policy using DP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the difference between policy iteration and value iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `Chapter_2_5.py` policy iteration example and adjust the `theta` and
    `gamma` parameters. What effect do these have on learning rates and values?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `Chapter_2_6.py` policy improvement example and adjust the `theta`
    and `gamma` parameters. What effect do these have on learning rates and values?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `Chapter_2_7.py` value iteration example and adjust the `theta` and
    `gamma` parameters. What effect do these have on learning rates and values?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete all of the policy and value iteration examples using the `FrozenLake
    8x8` environment. This a much larger version of the lake problem. Now, which method
    performs better?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use these exercises to strengthen your knowledge of the material we just covered
    in this chapter. In this next section, we will summarize what we covered in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took an in-depth look at DP and the Bellman equation. The
    Bellman equation with DP has influenced RL significantly by introducing the concept
    of future rewards and optimization. We covered the contribution of Bellman in
    this chapter by first taking a deep look at DP and how to solve a problem dynamically.
    Then, we advanced to understanding the Bellman optimality equation and how it
    can be used to account for future rewards as well as determine expected state
    and action values using iterative methods. In particular, we focused on the implementation
    in Python of policy iteration and improvement. Then, from there, we looked at
    value iteration. Finally, we concluded this chapter by setting up an agent test
    against the FrozenLake environment using a policy generated by both policy and
    value iteration. For this chapter, we looked at a specific class of problems well
    suited to DP that also helped us to derive other concepts in RL such as discounted
    rewards.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we continue with this theme by looking at Monte Carlo methods.
  prefs: []
  type: TYPE_NORMAL
